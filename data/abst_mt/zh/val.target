混合精度训练（MPT）正在成为一种实用的技术，通过利用现有GPU对IEEE半精度浮点的快速硬件支持，提高训练深度神经网络的速度和能源效率。 MPT通常与一种叫做损失比例的技术结合使用，其作用是在反向传播开始之前将损失值放大，以尽量减少数值下溢对训练的影响。不幸的是，现有的方法将这个损失比例值作为一个超参数，需要根据模型进行调整，而且单一的比例不能适应不同训练阶段的不同层。 我们引入了一种基于损失比例的训练方法，称为自适应损失比例，通过消除调整特定模型的损失比例超参数的需要，使MPT更容易和更实用。我们通过引入层级损失比例值，在训练期间自动计算，比现有方法更有效地处理欠流问题。
许多现实世界中的问题，例如物体检测，其输出自然地表达为实体的集合。这给传统的深度神经网络带来了挑战，这些网络自然地处理结构化的输出，如向量、矩阵或张量。我们提出了一种新的方法，学习使用深度神经网络预测具有未知排列和cardinality的集合。 具体来说，在我们的表述中，我们将排列组合作为不可观察的变量，并在学习过程中使用交替优化来估计其分布。我们在两个相关的视觉问题上证明了这种新表述的有效性：物体检测，我们的表述优于最先进的检测器，如Faster R-CNN和YOLO；以及复杂的验证码测试，我们观察到，令人惊讶的是，我们基于集合的网络获得了模仿算术的能力，而没有任何规则被编入。
凹陷是人类视觉的一个重要部分，一些深度网络也使用了凹陷。然而，在凹陷和非凹陷的深度网络之间，以及不同的可变分辨率下采样方法之间，很少有系统的比较。这里我们定义了几个这样的方法，并与Densenet-121网络比较了它们在ImageNet识别上的表现。
我们在神经网络验证的背景下探讨了协同设计的概念。具体来说，我们的目标是训练深度神经网络，它不仅对对抗性扰动具有鲁棒性，而且其鲁棒性可以更容易地被验证。为此，我们确定了网络模型的两个属性--权重稀疏性和所谓的ReLU稳定性--它们被证明对相应验证任务的复杂性有很大影响。 我们证明，仅提高权重稀疏度就能使我们把计算上难以解决的验证问题变成可解决的问题。然后，提高ReLU稳定性会使验证时间额外加快4-13倍。我们方法的一个重要特点是其 "普遍性"，即它可以用于广泛的训练程序和验证方法。
批量归一化（BatchNorm）已被证明对改善和加速深度神经网络的训练是有效的。然而，最近它被证明也容易受到对抗性扰动的影响。在这项工作中，我们旨在研究BatchNorm的对抗性脆弱性的原因。 我们假设，在训练和推理过程中使用不同的归一化统计（训练时使用小批量统计，推理时使用这些值的移动平均）是导致BatchNorm层的这种对抗性脆弱性的主要原因。我们通过在各种神经网络架构和数据集上的实验证明了这一点。
电子健康记录（EHR）包括纵向的临床观察，具有稀疏性、不规则性和高维度性，这成为得出可靠的下游结果的主要障碍。尽管有大量的归因方法被提出来解决这些问题，但大多数现有的方法忽略了相关的特征或时间动态，完全忽略了不确定性。特别是，由于缺失值的估计有不精确的风险，它促使我们对可靠和不确定信息给予不同的关注。 在这项工作中，我们提出了一种新的变异-复发归因网络（V-RIN），它将归因和预测网络统一起来，考虑到了相关的特征、时间动态，并进一步利用不确定性来减轻缺失值估计的风险。 具体来说，我们利用深度生成模型来估计基于变量间分布的缺失值，并利用递归网络来利用时间关系和利用不确定性。我们用公开的真实世界EHR数据集PhysioNet Challenge 2012验证了我们提出的模型的有效性，并将其结果与文献中其他最先进的竞争方法进行比较。
尽管深度神经网络（DNN）在各种分类问题上具有最先进的准确性，但由于其庞大的规模和复杂性，它们在资源有限的边缘计算设备上的部署仍然具有挑战性。最近的一些研究报告称，通过DNN模型的量化，在降低这种复杂性方面取得了显著成果。 然而，这些研究在进行量化时通常不考虑损失函数的变化，也不考虑DNN模型参数对准确性的不同重要性。我们在本文中通过提出一种新的方法来解决这些问题，这种方法被称为自适应量化，它通过为每个网络参数找到一个独特的、最佳的精度来简化训练的DNN模型，使损失的增加最小。 该方法的核心优化问题迭代地使用损失函数梯度来确定每个参数的误差范围，并相应地分配给它一个精度。由于这个问题使用线性函数，它的计算成本很低，而且正如我们将显示的，它有一个闭合的近似解。
我们研究了学习能够捕获包含关系的包络不变表示的问题。我们提出在一个新的任务上训练一个模型：预测多集对之间的对称差异的大小，这些集可能包含同一对象的多个副本。在模糊集理论的激励下，我们提出了多集表示以及如何在这些表示下预测对称差异大小。 我们将多集元素建模为标准单线上的向量，将多集建模为这些向量的总和，并将对称差异预测为多集表示之间的l1-距离。我们证明，我们的表示比基于DeepSets的无约束对象表示方法更有效地预测对称差异的大小。
收集可靠的训练样本$(x,y)$对于建立数据密集型的学习系统（如深度学习系统）是很重要的。在文献中，有一系列关于从持有相关信息的自利代理人那里引出分布信息的研究。 要求人们报告复杂的分布$p(x)$，虽然在理论上是可行的，但在实践中是具有挑战性的。这主要是由于人类代理推理和报告这种高维信息需要沉重的认知负荷。考虑到这样的例子，我们对通过首先收集某类高维图像数据来建立一个图像分类器感兴趣。虽然经典的诱导结果适用于诱导这个图像数据的复杂和生成（和连续）分布$p（x），我们对诱导代理的样本$x_i\sim p（x）$感兴趣。 本文介绍了一种深度学习辅助方法，以激励来自自私和理性的代理人的可信的样本贡献。这样做的挑战是设计一个激励兼容的评分函数，对每个报告的样本进行评分，以诱导真实的报告，而不是一个任意的甚至是对抗性的。 我们表明，通过对某个$f$-分歧函数的准确估计，我们能够在诱导真实样本时实现近似的激励相容性。然后，我们通过研究$f$-分歧函数的变异形式，提出了一个具有理论保证的高效估计器。 我们还展示了这个样本激发问题和$f$-GAN之间的联系，以及这种联系如何帮助重建一个基于收集的样本的分布估计。
著名的序列到序列学习（Seq2Seq）技术及其众多变体在许多任务上都取得了出色的表现。然而，许多机器学习任务的输入都自然地表示为图形；现有的Seq2Seq模型在实现从图形形式到适当序列的精确转换方面面临着巨大的挑战。为了解决这一挑战，我们引入了一个通用的端到端图形到序列的神经编码器-解码器架构，将输入图形映射为矢量序列，并使用基于注意力的LSTM方法从这些矢量解码出目标序列。 我们的方法首先使用一个改进的基于图的神经网络生成节点和图的嵌入，该神经网络具有新颖的聚合策略，可以将边缘方向信息纳入节点嵌入中。我们进一步引入一个注意力机制，使节点嵌入和解码序列保持一致，以更好地应对大型图。 在bAbI、最短路径和自然语言生成任务上的实验结果表明，我们的模型达到了最先进的性能，并明显优于现有的图神经网络、Seq2Seq和Tree2Seq模型；使用提出的双向节点嵌入聚合策略，该模型可以快速收敛到最佳性能。
我们解决了学习发现未见类别中物体的三维部分的问题。能够学习部分的几何先验并将此先验转移到未见类别中，对数据驱动的形状分割方法构成了基本挑战。我们提出了一个基于学习的迭代分组框架，该框架学习分组策略，以自下而上的方式逐步将小部分建议合并为大部分。 我们方法的核心是限制提取零件级特征的局部环境，这鼓励了对新类别的通用性。在最近提出的大规模细粒度三维零件数据集PartNet上，我们证明了我们的方法可以将从3个训练类别学到的零件知识转移到21个未见过的测试类别，而无需看到任何注释的样本。与四个强大的形状分割基线的定量比较表明，我们达到了最先进的性能。
本文介绍了弹道图神经网络。弹道图神经网络从运输的角度处理权重分布，与传统的图神经网络管道相比有许多不同的特性。弹道图神经网络不需要计算任何特征值。与传统的图神经网络（$/sigma^2 \sim T^2$）相比，过滤器的传播速度呈指数增长（$/sigma^2 \sim T$）。 我们使用扰动硬币算子来扰动和优化扩散速度。我们的结果表明，通过选择扩散速度，网络可以用较少的参数达到类似的精度。我们还表明，与纯弹道的过滤器相比，扰动的过滤器起到了更好的表示作用。
在本文中，我们提出了一个基于数据编程范式的神经排名任务的textit{弱监督}框架，它使我们能够利用来自不同来源的多个弱监督信号。从经验上看，我们考虑了两个弱监督信号来源，无监督的排名功能和语义特征相似性。 在我们的弱监督框架下，我们训练了一个基于BERT的段落排名模型（该模型在两个具有完全监督的基准数据集上实现了新的最先进的性能）。在不使用地面真实训练标签的情况下，BERT-PR模型在所有三个数据集上的表现都远远超过了BM25基线，甚至在其中两个数据集上击败了之前具有完全监督的最先进结果。
我们从傅里叶分析的角度研究了深度神经网络（DNNs）的训练过程。我们在高维基准数据集（如MNIST/CIFAR10）和深度网络（如VGG16）上证明了一个非常普遍的频率原则（F-Principle）--DNNs经常从低频到高频地拟合目标函数。通过一个天真的理论，我们说明了这个F-Principle是由常用的激活函数的规律性导致的。F-Principle意味着一个隐含的偏见，即DNN倾向于用低频函数来拟合训练数据。这种理解提供了一个解释，即DNN在大多数真实数据集上的良好泛化和DNN在奇偶函数或随机数据集上的不良泛化。
加速药物发现的问题在很大程度上依赖于优化前体分子的自动工具，以使其具有更好的生化特性。我们在本文中的工作大大扩展了先前用于分子优化的图-图转换方法的先进性。特别是，我们通过将子结构成分的编码与原始分子图的原子级编码交织在一起，实现连贯的多分辨率表示。 此外，我们的图解码器是完全自回归的，并将添加新的子结构的每一步与解决其与新出现的分子的连接过程交织在一起。我们在多个分子优化任务上评估了我们的模型，并表明我们的模型明显优于以前最先进的基线。
等价性是一个很好的属性，因为它能产生参数效率更高的神经架构，并通过特征映射保留输入的结构。即使一些变换的组合可能永远不会出现（例如，一个直立的脸和一个水平的鼻子），目前的等价性架构在学习特征表征时考虑变换组中所有可能的变换集合。相反，人类视觉系统能够注意到环境中发生的相关变换集合，并利用这些信息来帮助和改善物体识别。 基于这一观察，我们修改了传统的等值特征映射，使其能够关注数据中共同出现的变换集合，并将这一概念推广到由多个对称性组成的群体上。我们表明，我们提出的共同关注的等值神经网络在旋转的MNIST和CIFAR-10上始终优于传统的旋转等值和旋转与反射等值神经网络。
在这项研究中，我们训练生成对抗网络（GANs）来生成固定长度的全原子蛋白质骨架，目的是从现实的3-D骨架片段的分布中取样。我们通过所有骨架原子之间的成对距离来表示蛋白质结构，并提出了一种以可微分的方式直接恢复和完善相应骨架坐标的方法。 我们表明，生成器潜在空间的插值对应于输出骨干的平滑变形，并且测试集结构在训练期间没有被生成器看到，存在于它的图像中。最后，我们对生成的结构子集进行序列设计、松弛和非初始折叠，并表明在某些情况下，我们可以在正向折叠后恢复生成的折叠。这些结果共同表明了一种使用外部能量函数进行快速蛋白质结构完善和折叠的机制。
Few-Shot Learning（用有限的标记数据学习）旨在克服传统机器学习方法的局限性，这些方法需要成千上万的标记例子来训练一个有效的模型。被认为是人类智能的标志，社区最近见证了关于这个主题的一些贡献，特别是通过元学习，其中一个模型学习如何学习一个有效的模型来进行少数的学习。主要的想法是从一组训练任务中获得先验知识，然后用来执行（少数的）测试任务。 大多数现有的工作假设训练任务和测试任务都来自相同的分布，并且训练任务中有大量的标记数据。这是一个非常强大的假设，它限制了元学习策略在现实世界中的使用，因为在现实世界中可能没有充足的训练任务和测试任务的相同分布。 在本文中，我们提出了一种新的元学习范式，其中学习了少量的学习模型，通过对抗性领域适应，同时克服了训练和测试任务之间的领域转移。
通用概率编程系统（PPSs）为指定丰富和复杂的概率模型提供了一个强大的框架。然而，这种表现力的代价是使从模型中得出推论的过程大大复杂化。特别是，当模型的支持在执行之间变化时，推论会变得具有挑战性。Divide, Conquer, and Combine (DCC).DCC将程序分为独立的直线子程序，每个子程序都有一个固定的支持，允许更强大的推理算法在本地运行，然后以一种原则性的方式重新组合它们的输出。我们展示了DCC如何作为一个自动化和通用的PPS推理引擎来实现，并通过经验证实，它可以比以前的方法提供大量的性能改进。
检测社区或现实生活中的网络（如社交网络或产品购买网络）的模块化结构是一项重要的任务，因为网络的运作方式往往由其社区决定。社区检测的传统方法包括基于模块化的方法，一般来说，这些方法根据启发式方法构建分区，寻求分区内的边与分区间的边的比例最大化。 节点嵌入方法将图中的每个节点表示为al-valued向量，将图中的社区检测问题转变为对一组向量的聚类问题。现有的节点嵌入方法主要基于首先从每个节点发起均匀的随机行走来构建节点的背景，然后寻求使节点的向量表示接近其背景。 然而，标准的节点嵌入方法在构建每个节点周围的上下文时并没有直接考虑到网络的社区结构。为了缓解这一问题，我们探索了两种不同的工作思路。首先，我们研究了使用有偏见的随机行走（特别是基于最大熵的行走）来获得更多保存中心性的节点嵌入，我们假设这可能导致嵌入空间中更有效的集群。 第二，我们提出了一种社区结构意识到的节点嵌入方法，我们将基于模块化的分区启发式方法纳入节点嵌入的目标函数中。我们证明了我们提出的社区检测方法优于一些基于模块化的基线以及在标准的节点嵌入矢量空间（特别是node2vec）上的K-means，在各种不同规模和密度的现实生活网络中都是如此。
点云是一种敏捷的三维表征，有效地模拟了物体的表面几何形状。然而，这些以表面为中心的特性也给设计识别和合成点云的工具带来了挑战。这项工作提出了一个新的自回归模型，PointGrow，它从零开始生成现实的点云样本，或从给定的语义背景中获得条件。我们的模型是循环运行的，每个点都根据给定的先前生成的点的条件分布进行采样。 由于点云物体的形状通常是由长距离的点间依赖关系编码的，我们用专门的自我注意模块来增强我们的模型，以捕捉这些关系。广泛的评估表明，PointGrow在无条件和有条件的点云生成任务上都取得了令人满意的性能，包括保真度、多样性和语义保存。
强化学习和进化算法可用于创建复杂的控制解决方案。不幸的是，由于其 "黑箱 "性质，解释这些解决方案如何工作可能很困难。此外，控制算法的时间扩展性质往往阻碍了用于标准监督学习算法的可解释性技术的直接应用。） 函数分析,3) 单一时间步骤综合梯度,4) 基于语法的决策树，5）敏感性分析与LSTM的时间建模相结合，以及6）解释模板。这些技术在一个简单的2D领域中进行了测试，其中一个模拟的漫游者试图通过障碍物导航来达到目标。对于控制，这个漫游者使用一个进化的多层感知，将障碍物和目标传感器的8D领域映射到一个决定它在下一个时间步骤中应该去的行动。
视觉和语言导航（VLN）任务需要一个代理在照片现实的未知环境中遵循导航指令。这项具有挑战性的任务要求代理知道哪个指令已经完成，接下来需要哪个指令，走哪条路，以及它向目标的导航进展。在本文中，我们介绍了一个具有两个互补组件的自我监测代理。(1)视觉-文本共同定位模块，从周围的图像中找到过去完成的指令、下一个动作所需的指令以及下一个移动方向；(2)进度监测器，确保接地的指令正确反映导航的进度。 我们在一个标准的基准上测试了我们的自我监控代理，并通过一系列的消融研究分析了我们提出的方法，阐明了主要组成部分的贡献。使用我们提出的方法，我们以显著的幅度设定了新的技术状态（在未见过的测试集上，成功率绝对增加8%）。代码可在https://github.com/chihyaoma/selfmonitoring-agent。
强化学习（RL）中的环境通常只是部分可观察的。为了解决这个问题，一个可能的解决方案是向代理提供关于过去观察的信息。虽然常见的方法是使用循环神经网络（RNN）来表示这个历史，但在本文中，我们提出了一个替代的表示方法，它是基于在给定情节中观察到的过去事件的记录。在人类记忆的启发下，这些事件只描述环境的重要变化，在我们的方法中，使用自我监督自动发现。我们使用两个具有挑战性的RL基准来评估我们的历史表示方法：Atari-57套件的一些游戏和3D环境Obstacle Tower。使用这些基准，我们显示了我们的解决方案相对于常见的基于RNN的方法的优势。
无条件生成高保真图像是测试图像解码器性能的一个长期基准。自回归图像模型已经能够无条件地生成小图像，但将这些方法扩展到可以更容易评估保真度的大图像，仍然是一个开放的问题。 其中主要的挑战是编码庞大的先前背景的能力，以及学习一个既能保留全局语义一致性又能保留细节精确性的分布的巨大困难。为了解决前面的挑战，我们提出了子尺度像素网络（SPN），一个条件解码器架构，将图像生成为等大小的图像片的序列。 SPN紧凑地捕捉了图像的空间依赖性，并需要一小部分内存和计算。为了解决后一个挑战，我们建议使用多维升尺度，通过与不同的SPN相对应的中间阶段在尺寸和深度上增长图像。 我们在256大小的CelebAHQ和32至128大小的ImageNet的非条件生成中评估了SPNs。我们在多种设置中取得了最先进的似然结果，在以前未探索的设置中建立了新的基准结果，并且能够在两个数据集的基础上生成每一个高保真的大尺度样本。
现实世界的动态系统通常由多个相互作用的随机子系统组成。由于在理解其成分的复杂相互作用和演变方面存在固有的困难，对这种动态系统的行为进行建模和预测通常并不容易。本文介绍了关系状态空间模型（R-SSM），这是一个顺序分层的潜变量模型，利用图神经网络（GNNs）来模拟多个相关对象的联合状态转换。 通过让GNN与SSM合作，R-SSM提供了一种灵活的方式将关系信息纳入多对象动态的建模中。我们进一步建议用为顶点索引随机变量实例化的归一化流来增强模型，并提出两个辅助性的对比目标来促进学习。
自然语言是分层结构的：较小的单元（如短语）被嵌套在较大的单元（如分句）中。当一个较大的成分结束时，嵌套在其中的所有较小的成分也必须关闭。虽然标准的LSTM架构允许不同的神经元在不同的时间尺度上跟踪信息，但它没有对成分的层次结构建模的明确倾向。 本文提出通过对神经元进行排序来增加这种归纳偏向；一个主输入和遗忘门的矢量确保当一个给定的神经元被更新时，在排序中紧随其后的所有神经元也被更新。我们的新型循环架构，有序神经元LSTM（ON-LSTM），在四个不同的任务上取得了良好的性能：语言建模、无监督解析、目标句法评估和逻辑推理。
跳过连接使训练非常深的网络成为可能，并已成为各种神经结构中不可缺少的组成部分。这里，我们提出了一个新的解释，说明跳过连接在训练非常深的网络中的好处。 训练深度网络的困难部分是由于模型的不可识别性引起的奇异性。在以前的工作中，已经确定了几个这样的奇异性：(i)由特定层中节点的排列对称性引起的重叠奇异性，(ii)与消除，即一致停用相应的消除奇异性。我们认为，跳过连接通过打破节点的排列对称性、减少节点消除的可能性以及使节点的线性依赖性降低来消除这些奇异性。 此外，对于典型的初始化，跳过连接使网络远离了这些奇点的 "幽灵"，并在它们周围塑造了景观，以缓解学习速度的下降。这些假设得到了来自简化模型以及在真实世界数据集上训练的深度网络实验的支持。
表征学习是一系列机器学习领域的核心挑战。在强化学习中，有效的功能性表征有可能极大地加快学习进度，并解决更多具有挑战性的问题。大多数先前关于表征学习的工作都集中在生成方法上，学习表征以一种更加分散或有序的方式捕捉观察空间的所有潜在变化因素。 在本文中，我们的目标是学习功能突出的表征：这些表征在捕捉观察空间中的所有变化因素方面不一定是完整的，而是旨在捕捉那些对决策很重要的变化因素--"可操作的"。 这些表征意识到了环境的动态性，并且只捕捉决策所需的观察元素，而不是所有的变化因素，从而消除了显式重建的需要。我们展示了这些学习到的表征如何有助于改善稀疏奖励问题的探索，实现长水平的分层强化学习，以及作为学习下游任务策略的状态表征。我们在一些模拟环境中评估了我们的方法，并将其与先前的表征学习、探索和分层强化学习的方法进行比较。
我们探索了标准卷积神经网在以下环境中的行为：按顺序引入分类任务，并要求神经网掌握新的任务，同时保持对以前学习的任务的掌握。 这种设置相当于人类学习者在获得领域专业知识时面临的情况，例如，当一个人逐章阅读一本教科书时。通过涉及10个相关任务序列的模拟，我们发现有理由乐观地认为，当网络从拥有单一技能发展到成为领域专家时，它们会有良好的扩展。 首先，前向促进--在学习了n个先前的任务后，加速学习任务n+1--随着n的增长而增长。第二，后向干扰--在学习任务n+1时对n个先前任务的遗忘--随着n的增长而减少。
我们展示了一种不费吹灰之力的方法，即从现有的词嵌入中无监督地构建任务优化的嵌入，以获得有监督的最终任务的性能。这避免了额外的标记或建立更复杂的模型结构，而是提供更适合最终任务的专门嵌入。 此外，该方法可用于粗略估计特定类型的终端任务是否可以从一个给定的未标记的数据集中学习，或在其中得到体现，例如使用公开可用的探测任务。我们对我们的方法进行评估，以适应不同的单词嵌入探测任务和嵌入训练语料的大小--即探索其在减少（预训练资源）设置中的使用。
数据增强通常用于编码学习方法中的不变性。然而，这个过程通常以一种低效的方式进行，因为人工例子是通过对训练集中的所有点应用一些转换来创建的。由此产生的数据集大小的爆炸可能是存储和训练成本方面的问题，也是选择和调整应用的最佳转换集的问题。 在这项工作中，我们证明有可能大大减少数据增强中包含的数据点的数量，同时实现与增强整个数据集相同的精度和不变性的好处。我们提出了一套新的子采样策略，基于模型影响和损失，可以实现增强集大小减少90%，同时保持标准数据增强的精度收益。
在过去的几年里，深度生成模型方面令人振奋的工作产生了能够通过生成代表其结构的字符串、树和图来建议新的有机分子的模型。虽然这种模型能够生成具有理想特性的分子，但由于难以知道如何合成这些分子，它们在实践中的效用是有限的。 因此，我们提出了一个新的分子生成模型，反映了一个更真实的现实世界的过程，在这个过程中，反应物被选择并组合成更复杂的分子。更具体地说，我们的生成模型提出了一袋初始反应物（从商业上可用的分子库中选择），并使用一个反应模型来预测它们如何一起反应以生成新分子。 在生成过程中对构建分子的整个过程进行建模提供了许多优势。首先，我们表明，由于建模反应的有用归纳偏见，这样的模型有能力生成广泛、多样的有效和独特的分子。第二，对合成路线而不是最终分子进行建模，为那些不仅对新分子感兴趣而且对稳定和安全的合成路线有建议的化学家提供了实际优势。第三，我们证明了我们的模型也能解决一步逆合成问题，预测能产生目标产品的一组反应物的能力。
深度神经网络是复杂的非线性模型，被用作预测分析工具，在许多分类任务上表现出最先进的性能。 然而，他们没有内在的能力来识别他们的预测可能出错。最近有一些努力来检测自然错误，即错误分类的输入，但这些机制带来了额外的能量需求。 为了解决这个问题，我们提出了一个新的事后框架，以节能的方式检测自然错误。 我们通过在每个类别中附加基于相关特征的线性分类器来实现这一目标，该分类器被称为基于相关特征的辅助单元（RACs）。  所提出的技术利用附加在几个选定的隐藏层上的RACs之间的共识来区分正确分类的输入和错误分类的输入。 我们在各种图像分类数据集，如CIFAR10、CIFAR100和Tiny-ImageNet上证明了我们技术的有效性。我们的结果显示，对于在VGG16网络上训练的CIFAR100数据集，RACs可以检测到46%的错误分类的例子，与基线网络相比，能量减少了12%，而69%的例子被正确分类。
为了预测实体之间是否存在关系，它们的嵌入通常按照特定关系的映射在潜空间中进行比较。虽然链接预测已经稳步提高，但潜结构，以及为什么这样的模型能够捕获语义信息，仍然没有得到解释。 我们以最近对词嵌入的理论解释为基础，考虑实体之间关系的明确结构。对于可识别的关系类型，我们能够预测属性，并证明领先的知识图表示方法的相对性能，包括他们经常被忽视的独立预测能力。
许多现实世界的应用涉及多变量、地理标记的时间序列数据：在每个地点，多个传感器记录相应的测量值。例如，空气质量监测系统记录PM2.5、CO等。所产生的时间序列数据往往由于设备中断或通信错误而有缺失值。 为了弥补缺失值，最先进的方法是建立在递归神经网络（RNN）的基础上，它按顺序处理每个时间戳，禁止直接对遥远的时间戳之间的关系进行建模。最近，自我注意机制已被提出用于序列建模任务，如机器翻译，明显优于RNN，因为每两个时间戳之间的关系可以被明确建模。 在本文中，我们首次将自我关注机制用于多变量、地理标记的时间序列数据。为了共同捕捉不同维度（即时间、位置和传感器测量）的自我关注，同时保持关注图的合理大小，我们提出了一种称为跨维度自我关注（CDSA）的新方法，以顺序处理每个维度，但以独立于顺序的方式。 在三个真实世界的数据集上，包括我们新收集的纽约市交通数据集，广泛的实验证明了我们的方法与最先进的方法相比在估算和预测任务上的优越性。
使用光学字符识别（OCR）软件将扫描文件转换为数字形式。这项工作的重点是提高扫描文件的质量，以改善OCR输出。我们创建了一个端到端的文件增强管道，该管道接收一组噪声文件并产生干净的文件。
对抗性例子的存在，或者由正确预测的例子的微小变化所构建的故意的错误预测，是当今神经网络研究中最重要的挑战之一。具有讽刺意味的是，许多新的防御措施是基于一个简单的观察--对抗性输入本身并不稳健，对攻击性输入的微小扰动往往能恢复所需的预测。 虽然直觉有些清楚，但研究文献中缺少对这一现象的详细了解。本文对扰动防御何时以及为何起作用进行了全面的实验分析，并提出了可以解释其在不同环境中的有效性（或无效性）的潜在机制。
在这项工作中，我们通过定义一个优化器的可调性来填补 "易用性 "这一重要而模糊的概念：使用自动随机超参数搜索找到好的超参数配置有多容易？我们为优化器的可调性提出了一个实用而普遍的量化措施，可以形成一个公平的优化器基准。 在一套广泛的标准数据集和架构上评估各种优化器，我们发现亚当对大多数问题来说是最可调谐的，特别是在超参数调谐的低预算下。
衍射物理学中的相位问题是所有科学中最古老的逆向问题之一。任何解决这个逆向问题的方法必须克服的核心困难是，一半的信息，即衍射光束的相位，总是缺失的。在电子显微镜的背景下，相位问题通常是非线性的，由相位检索技术提供的解决方案是已知的电子与物质相互作用的物理学的不良近似。这里，我们表明，一个半监督学习方法可以有效解决电子显微镜/散射的相位问题。 特别是，我们引入了一个新的深度神经网络（DNN），Y-net，除了通过监督训练学习基于物理的正则化外，还同时通过监督训练学习重建算法。我们证明，这种受约束的半监督方法比以纯粹监督方式训练的相同模型的数据效率和准确性要高一个数量级。此外，Y-net模型的结构为推理过程中模型预测的一致性提供了一个直接的评估，并普遍适用于其他环境中的相问题。
词嵌入从大型文本数据集中提取词的语义特征。大多数嵌入方法依靠对数线性模型来预测一个词在其他词的上下文中的出现。这里我们提出了word2net，一种用神经网络取代其线性参数化的方法。 对于词汇中的每一个词，word2net提出了一个神经网络，将上下文作为输入，并输出出现的概率。此外，word2net可以利用其单词网络的分层组织，将额外的元数据，如语义特征，纳入嵌入模型。 我们用两个数据集研究了word2net，一个是维基百科的文章集，一个是美国参议院演讲的语料库。从数量上看，我们发现word2net在预测保留词方面优于流行的嵌入方法，而且基于语篇的参数共享进一步提高了性能。从质量上看，word2net学习了可解释的语义表征，与基于矢量的方法相比，更好地纳入了句法信息。
神经科学的一个关键目标是了解认知功能的大脑机制。一个新兴的方法是使用功能磁共振成像（fMRI）研究 "大脑状态 "的动态。到目前为止，在文献中，大脑状态的研究通常使用30秒或更多的fMRI数据，目前还不清楚在多大程度上可以从非常短的时间序列中可靠地识别大脑状态。 在这个项目中，我们应用图卷积网络（GCN）对任务fMRI数据集中的短时间窗口的大脑活动进行解码，即把给定的fMRI时间序列窗口与所使用的任务联系起来。从具有由大脑皮层的取消和从功能连接组中提取的相邻矩阵定义的节点的人口脑图开始，GCN把短系列的fMRI量作为输入，生成高层次的特定领域的图表示，然后预测相应的认知状态。 我们在人类连接组项目（HCP）数据库中研究了这种GCN "认知状态注释 "的性能，该数据库具有21种不同的实验条件，横跨7个主要的认知领域，以及高时间分辨率的任务fMRI数据。使用10秒的窗口，21种认知状态被识别，平均测试准确率为89%（偶然水平4. 8%）。由于HCP任务电池的设计是为了选择性地激活广泛的专门功能网络，我们预计GCN注释可以作为其他转移学习应用的基础模型，例如，适应新的任务领域。
现代深度神经网络（DNNs）需要高内存消耗和大的计算负荷。 为了在边缘或移动设备上有效地部署DNN算法，人们探索了一系列DNN压缩算法，包括因子化方法的一系列工作。因子化方法用两个或多个低秩矩阵的乘法来近似DNN层的重量矩阵。 然而，在训练过程中很难衡量DNN层的等级。以前的工作主要是通过隐式近似或在每个训练步骤上通过昂贵的奇异值分解（SVD）过程来诱导低等级。前一种方法通常会引起高精确度的损失，而后者则使DNN因式分解无法有效地达到高压缩率。 在这项工作中，我们提出了SVD训练，首先应用SVD分解DNN的层，然后在全等级分解的权重上进行训练。为了提高训练质量和收敛性，我们对奇异向量添加了正交性正则化，以确保SVD的有效形式并避免梯度消失/爆炸。 我们通过对每一层的奇异值应用疏散诱导正则化来鼓励低秩，在最后应用奇异值修剪来达到低秩模型。
近来，少量学习算法的流行使得模型能够在仅有的几个训练样本的基础上快速适应新的任务。以前的少量学习工作主要集中在分类和强化学习方面。在本文中，我们提出了一个专注于回归任务的几率元学习系统。我们的模型是基于这样的想法：如果未知函数被表示为一组适当的基础函数的线性组合，那么它的自由度就可以大大降低。 我们设计了一个特征提取器网络来编码任务分布的基础函数，并设计了一个权重生成器来生成新任务的权重向量。我们表明，我们的模型在各种回归任务中优于目前的元学习方法。
大多数分类和分割数据集都假设了一个封闭世界的场景，在这个场景中，预测被表达为在一组预先确定的视觉类别上的分布。然而，这样的假设意味着不可避免的，而且在存在分布外（OOD）输入的情况下，往往是无法注意到的失败。 与最近的方法不同，我们避免仅通过观察为解决所需的计算机视觉任务而训练的主要模型的训练数据集来做出任何决定，我们建议通过对输入数据中的OOD像素进行鉴别性检测来解决这个问题。 相反，我们训练一个专门的OOD模型，该模型从一个更大的 "背景 "数据集中分辨出主要的训练集，该数据集近似于视觉世界的多样性。我们在一个密集的预测设置中对高分辨率的自然图像进行了实验。 我们在WildDash测试中评估了我们的方法，这是目前唯一具有分布外图像的公共测试数据集。获得的结果表明，所提出的方法成功地识别了分布外的像素，同时以很大的优势超越了以前的工作。
网络量化是在低功耗移动设备上部署卷积神经网络（CNN）的最友好的硬件技术之一。最近的网络量化技术将卷积层中的每个权重核独立量化，以获得更高的推理精度，因为一个层中的权重核表现出不同的变异，因此具有不同的冗余量。 量化位宽或位数（QBN）直接决定了推理精度、延迟、能量和硬件开销。为了有效减少冗余并加速CNN推理，各种权重核应该用不同的QBN进行量化。然而，先前的工作只用一个QBN来量化每个卷积层或整个CNN，因为为每个权重核搜索QBN的设计空间太大。 手工制作的内核明智的QBN搜索的启发式非常复杂，以至于领域专家只能获得次优的结果。即使是基于深度强化学习（DRL）的DDPG代理也很难找到一个内核明智的QBN配置，以达到合理的推理精度。 在本文中，我们提出了一种基于分层DRL的内核明智网络量化技术--AutoQ，为每个权重内核自动搜索一个QBN，并为每个激活层选择另一个QBN。与最先进的基于DRL的方案量化的模型相比，平均而言，通过AutoQ量化的相同模型可以减少54.06%的推理延迟，并减少50.69%的推理能耗，同时达到相同的推理精度。
最近的可视化分析系统利用多个机器学习模型来更好地适应数据，而不是传统的单一的、预先定义的模型系统。然而，虽然多模型的可视化分析系统可能是有效的，但其增加的复杂性带来了可用性问题，因为用户需要与多个模型的参数进行交互。此外，各种模型算法和相关超参数的出现创造了一个详尽的模型空间来对模型进行采样。 在本文中，我们提出了Gaggle，一个多模型的可视化分析系统，使用户能够交互式地浏览模型空间。通过定性的用户研究，我们展示了我们的方法如何帮助用户为分类和排名任务找到一个最佳模型。
目前，中文文本分类已经受到越来越多的关注。然而，中文文本表示的问题仍然阻碍着中文文本分类的改进，尤其是社交媒体中的多音字和同音字。为了有效地应对这一问题，我们提出了一种新的结构，即基于注意力机制的提取器，并设计了新的注意力网络，命名为提取器-注意力网络（EAN）。 此外，与混合编码器方法相比，EAN具有更复杂的组合结构和更多的还原参数结构。因此，EAN可以利用来自多输入的大量信息，缓解效率问题。
最近，利用深度神经网络从演示中学习（LfD）的进展使得学习复杂的机器人技能成为可能，这些技能涉及高维的感知，如原始图像输入。LfD算法通常假设从单一任务演示中学习。然而，在实践中，教师在没有仔细的任务设置、标签和工程的情况下演示多种任务是更有效的。不幸的是，在这种情况下，传统的模仿学习技术不能代表数据的多模式性质，往往导致次优行为。在本文中，我们提出了一种从视觉数据学习多种行为模式的LfD方法。 我们的方法基于随机深度神经网络（SNN），它将演示中的潜在意图表示为网络中的随机激活。我们提出了一种高效的SNN训练算法，对于视觉输入的学习，我们还提出了一种架构，将意图与随机注意力模块联系起来。我们在真实的机器人视觉物体到达任务中演示了我们的方法，并表明它可以可靠地学习演示数据中的多种行为模式。视频结果可在https://vimeo.com/240212286/fd401241b9。
在自然语言处理中，这种类型的解释对于人类用户理解解释的含义并得出解释和模型预测之间的联系是具有挑战性的，特别是对于长文本。 在这项工作中，我们专注于检测特征之间的相互作用，并提出了一种新的方法来构建基于特征相互作用的解释的层次。
在本文中，我们通过利用卷积层计算的特征的重要性与输入高度相关这一事实来降低这一成本，并提出了特征提升和抑制（FBS），一种在运行时预测性地放大突出的卷积通道并跳过不重要的通道的新方法。 与永久删除通道的修剪方法相反，它保留了完整的网络结构，并通过动态跳过不重要的输入和输出通道来加速卷积。FBS增强的网络是用传统的随机梯度下降法训练的，这使得它可以随时用于许多先进的CNN。 我们将FBS与一系列现有的通道修剪和动态执行方案进行了比较，并证明了ImageNet分类的巨大改进。实验表明，FBS在VGG-16和ResNet-18上可以分别节省5％和2％的计算量，而前5名的准确性损失都小于0.6％。
我们提出了一种新的方法，即通过使用预先定义的稀疏性来减少神经网络中饥饿的全连接层的参数数量，其中大部分连接在开始训练前是不存在的。我们的结果表明，卷积神经网络可以在分类层连接密度低于0.5%，或整个网络连接密度低于5%的情况下运行而不损失任何准确性。 我们还研究了预先定义只有完全连接层的网络的稀疏性的影响。基于我们的稀疏化技术，我们引入了 "散点 "指标来描述特定连接模式的质量。作为概念证明，我们展示了在CIFAR、MNIST和一个新的莫尔斯编码符号分类数据集上的结果，这突出了一些有趣的趋势和稀疏连接模式的限制。
深度神经网络容易受到对抗性例子的影响，这成为深度学习发展中最重要的问题之一。虽然近年来已经做了很多努力，但对对抗性攻击和防御算法进行正确和完整的评估具有重要意义。在本文中，我们建立了一个全面、严格和连贯的基准来评估图像分类任务的对抗性鲁棒性。 在简要回顾了大量有代表性的攻击和防御方法后，我们以两条鲁棒性曲线作为公正的评价标准进行了大规模的实验，以充分了解这些方法的性能。基于评价结果，我们得出了几个重要的发现，并为未来的研究提供了启示。
我们提出了对传统人工神经网络（ANNs）的修改，为ANNs提供了以生物神经元为动力的新能力。 生物神经元的工作远远超出了对突触输入进行线性加和，然后对综合信息进行转换。 一个生物神经元会根据外围因素（如神经调节剂）和内在因素来改变发射模式。 我们的修改连接了一种新型的ANN节点，它模仿了生物神经调节器的功能，被称为调节器，使其他传统的ANN节点能够根据其输入模式在运行时调整其激活灵敏度。 通过这种方式，我们使激活函数的斜率取决于上下文。 在卷积神经网络和长短期记忆网络的背景下，这种修改与传统的ANN节点相比产生了统计学上的重大改进。
在这项工作中，我们研究了大规模预训练-微调框架如何改变神经语言生成器的行为。我们专注于开放域对话反应生成任务的变换器编码器-解码器模型。我们发现，在标准微调之后，该模型忘记了在大规模预训练期间获得的重要语言生成技能。 我们通过详细的行为分析，从语境敏感性和知识转移的角度证明了遗忘现象。采用数据混合的概念，我们提出了一个直观的微调策略，名为 "混合审查"。我们发现混合审查有效地规范了微调过程，遗忘问题在很大程度上得到了缓解。
将领域知识模型与神经模型相结合一直是个挑战。 端到端训练的神经模型往往比领域知识模型或领域/神经组合的表现更好（均方误差更低），而且组合的训练效率很低。 在本文中，我们证明了通过将领域模型与机器学习模型相结合，通过使用外推测试集和调用装饰性目标函数，我们创建的模型可以预测更复杂的系统。这些模型是可解释的、外推的、数据高效的，并且捕获了可预测的但复杂的非随机行为，如未建模的自由度和系统测量噪声。 我们将这种改进的建模范式应用于几个模拟系统和一个实际的物理系统的系统识别中。  在各种不同复杂度和非线性的系统上，研究了用神经模型组成领域模型的几种方式，包括时间序列、提升、分袋和自动编码。 尽管这项工作是初步的，但我们表明，组合模型的能力是神经建模的一个非常有前途的方向。
人类可以从互动经验中学习任务无关的先验，并将先验用于新的任务，而不需要任何微调。在本文中，我们提出了Scoring-Aggregating-Planning（SAP），这是一个可以从任意质量的互动中学习任务无关的语义和动力学先验以及相应的稀疏奖励的框架，然后在零射条件下对未见的任务进行规划。 该框架为局部区域的状态和行动对找到了一个神经评分函数，该函数可以被汇总以近似于完整轨迹的质量；此外，一个用自我监督学习的动力学模型可以被纳入规划。 以前许多利用交互式数据进行策略学习的工作要么需要大规模的政策性环境互动，要么假定可以获得专家数据，而我们可以用纯粹的非政策性不完美数据实现类似的目标。实验证明，所提出的方法可以在包括网格世界、机器人任务和视频游戏等广泛的应用中胜过基线方法。
基于粒子的推理算法是一种很有前途的方法，通过迭代更新一组粒子来有效地生成难以处理的目标分布的样本。作为一个明显的例子，Stein variational gradient descent（SVGD）提供了一个确定性的和计算效率高的更新，但它被称为低估了高维的方差，其机制还不太清楚。在这项工作中，我们通过Stein's lemma探索SVGD和基于MMD推理算法之间的联系。 通过比较这两种更新规则，我们发现SVGD的偏差来源是高方差和确定性偏差的结合，并通过经验证明，去除其中任何一个因素都会导致方差的准确估计。此外，对于学习高维高斯目标，我们通过分析得出两种算法的收敛方差，并确认只有SVGD受到 "维度诅咒 "的影响。
我们描述了一种理解深度神经网络奇特和反直觉的泛化特性的方法。 该方法涉及到超越近年来在机器学习中流行的最坏情况下的理论能力控制框架，重新审视神经网络统计力学中的旧观念。 在这种方法中，我们提出了一个典型的非常简单的深度学习（VSDL）模型，其行为由两个控制参数控制，一个描述了网络上的有效数据量，或负载（当噪声被添加到输入中时，会减少），另一个是有效的温度解释（当算法被提前停止时，会增加）。 利用这个模型，我们描述了如何非常简单地应用泛化的统计力学理论的思想，为最近观察到的关于深度神经网络不能过度拟合训练数据、不连续学习和学习算法的泛化特性的急剧转变等的经验结果提供了强有力的定性描述。
当输出类的数量很大时，神经网络模型中softmax函数的计算是昂贵的。这可能成为此类模型训练和推理中的一个重要问题。在本文中，我们提出了Doubly Sparse Softmax（DS-Softmax），即稀疏专家的稀疏混合物，以提高softmax推理的效率。在训练期间，我们的方法通过将整个输出类空间分为几个部分重叠的专家来学习两级类层次。 每个专家负责输出类空间的一个学习子集，每个输出类只属于这些专家中的一小部分。在推理过程中，我们的方法快速定位最可能的专家来计算小规模的softmax。我们的方法是基于学习的，不需要先验的输出类分区空间的知识。我们在几个真实世界的任务上实证评估了我们的方法，证明我们可以在不损失性能的情况下显著减少计算。
用于高价值计算机视觉应用（如医学图像分类）的监督机器学习模型通常需要由领域专家标记的大型数据集，这些数据集收集速度慢，维护成本高，而且在数据分布变化方面是静态的。在这种情况下，我们评估了观察监督的效用，我们利用被动收集的信号，如眼球跟踪或 "凝视 "数据，以减少模型训练所需的手工标记的数据量。 具体来说，我们利用凝视信息直接监督视觉注意力层，惩罚人类标记者看得最久的空间区域和那些对模型输出影响最大的空间区域之间的分歧。我们提出证据，以这种方式约束模型可以减少达到特定性能水平所需的标记例子的数量达50%，并且凝视信息对更困难的任务最有帮助。
我们研究了GNNs训练程序对对称标签噪声的鲁棒性。通过将非线性神经信息传递模型（如Graph Isomorphism Networks, GraphSAGE等）与损失校正方法相结合，我们提出了一种用于图分类任务的耐噪声方法。我们的实验表明，在人工对称噪声设置下，测试精度可以得到提高。
通过最近在图表示学习方面的许多进展，近年来在涉及图结构数据的任务上取得的性能已经大大增加---主要是在涉及节点级预测的任务上。 然而，对整个图的预测任务的设置（如分子的属性预测或药物的副作用预测）被证明是更具挑战性的，因为算法必须将有关图的几个结构上相关的斑块的证据结合到一个预测中。 在这里，我们提出了一个设置，在这个设置中，图形神经网络一次接收成对的图形，并通过一个共同注意层对其进行扩展，使节点表征能够轻松地在它们之间交换结构信息。我们首先表明，这样的设置在成对的图形分类任务（药物-药物相互作用预测）上提供了自然的好处，然后扩展到一个更通用的图形回归设置：加强对QM9的预测，一个标准的分子预测基准。我们的设置很灵活，功能强大，除了预测多个训练图形的存在，没有对基础数据集属性进行假设。
在本文中，我们研究了作为有条件的GAN训练的图像标题，提出了一个上下文感知的LSTM标题器和共同关注的判别器，它在图像和标题之间执行语义对齐。我们研究了两种离散的GAN训练方法的可行性。我们研究了两种离散GAN训练方法的可行性：自我批判序列训练（SCST）和Gumbel Straight-Through（ST），并证明SCST显示出更稳定的梯度行为，并且比Gumbel ST的结果更好。
我们提出了牛顿蒙特卡洛（NMC），这是一种通过分析目标密度的一阶和二阶梯度来提高马尔科夫链蒙特卡洛（MCMC）收敛性的方法，以确定每个点的合适的提议密度。现有的基于一阶梯度的方法存在着确定适当的步长的问题。 太小的步长将需要大量的步长来收敛，而非常大的步长将导致它过冲高密度区域。NMC类似于优化中的Newton-Raphson更新，其中二阶梯度被用来自动调整每个维度的步长。然而，我们的目标不是找到一个最大值，而是找到一个能够最好地匹配目标密度的局部曲率的参数化密度。 作为对一阶方法的进一步改进，我们表明具有约束支持的随机变量在采取梯度步骤之前不需要进行转换，NMC直接将约束随机变量与具有相同支持的建议密度相匹配，从而保持目标密度的曲率不变。 我们在一些不同的领域展示了NMC的效率。对于先验与似然共轭的统计模型，我们的方法只需一步就能恢复后验。然而，我们也展示了相当大的非共轭模型的结果，其中NMC比自适应一阶方法（如NUTS）或其他不精确的可扩展推理方法（如随机变异推理或引导）表现更好。
Neural Tangents是一个旨在实现无限宽神经网络研究的库。它提供了一个高级API，用于指定复杂和分层的神经网络架构，然后这些网络可以像往常一样在有限宽或在其无限宽极限下进行训练和评估。整个库在CPU、GPU或TPU上开箱即用。所有的计算都可以自动分布在多个加速器上，设备的数量可以近乎线性地扩展。Neural Tangents可在https://www.github.com/google/neural-tangentsWe，还提供了一个配套的交互式Colab笔记本：https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb。
在过去的几年里，深度神经网络在分类任务中取得了巨大的成功。然而，在通往人工智能的道路上，一个主要的问题是神经网络无法准确地检测新的类分布的样本，因此，大多数现有的分类算法都假设所有的类在训练阶段之前是已知的。 在这项工作中，我们提出了一种训练神经网络的方法，使其能够有效地检测出分布外（OOD）的例子，而不影响其对已知类别的测试例子的分类准确性。 基于Outlier Exposure (OE)技术，我们提出了一个新的损失函数，该函数在图像和文本分类任务中利用OE实现了最先进的分布外检测结果。此外，该方法的构建方式使其适用于任何基于最大似然法的分类算法的训练。
导航是动物行为的关键，并被认为需要外部环境的内部表征，称为认知地图。这种表征的确切形式通常被认为是空间的度量表征。 在这里，我们训练了一个循环神经网络，该网络控制着一个在简单环境中执行几个导航任务的代理人。为了关注内部表征，我们将学习分成了一个与任务无关的预训练阶段，该阶段修改了内部连接性，以及一个控制网络输出的特定任务Q学习阶段。 我们表明，预训练塑造了网络的吸引子景观，导致了连续吸引子、离散吸引子或无序状态。这些结构诱导了Q-学习阶段的偏见，导致了与度量和拓扑规律性相对应的跨任务的性能模式。我们的结果表明，在循环网络中，诱导性偏见采取吸引子景观的形式--可以通过预训练塑造，并使用动态系统方法进行分析。 
机器学习模型的形式验证最近引起了人们的注意，在证明简单的属性方面取得了重大进展，如对输入特征的小扰动的鲁棒性。在这种情况下，人们还注意到，将验证程序折叠到训练中，更容易训练出可验证的鲁棒模型。 在本文中，我们通过将验证训练扩展到（1）递归神经网络架构和（2）超越简单对抗性鲁棒性的复杂规范，特别是捕捉时间属性的规范，如要求机器人定期访问充电站或语言模型总是产生长度有限的句子。实验表明，虽然使用标准训练训练的模型经常违反所需的规范，但我们的验证训练方法产生的模型既表现良好（在测试误差或奖励方面），又可以被证明与规范一致。
神经网络（NN）在图像、语音和文本领域的许多任务中取得了最先进的性能。这种巨大的成功主要是由于特殊的结构设计，以适应特定的数据模式，如CNN捕捉空间定位和RNN建模的顺序依赖性。 由于这些不同的表格数据之间没有共享的模式，所以很难设计出适合它们的特定结构。如果没有基于领域知识的精心架构设计，NN在这些表格数据领域达到令人满意的性能是相当具有挑战性的。为了填补NN在表格数据学习方面的空白，我们提出了一个通用的神经网络解决方案，称为TabNN，为各种任务中的表格数据自动得出有效的NN架构。\由于GBDT已经经验性地证明了其在表格数据建模方面的优势，我们使用GBDT来驱动TabNN的实现。在各种表格数据集上的综合实验分析表明，TabNN可以实现比许多基准解决方案更好的性能。
知识库（KBs）正变得越来越大，越来越稀疏，越来越具有概率性。这些知识库通常被用来进行查询推理和规则挖掘，但是它们的功效只有在其完整性的情况下才会发挥出来。 对于任何给定的概率KB，它从其关系中学习概率一阶规则，以识别有趣的模式。但是，目前的概率规则学习技术执行接地，为评估候选规则做概率推理。它不能很好地扩展到大型KB，因为使用接地推理的时间复杂性是KB大小的指数。我们将SafeLearner与最先进的概率规则学习器ProbFOIL+以及其在NELL（Never-Ending Language Learner）和Yago的标准概率KB上的确定性的当代AMIE+进行了比较。我们的结果表明，在学习简单规则时，SafeLearner的扩展性与AMIE+一样好，而且明显比ProbFOIL+快。
最近在面向任务的对话的对话状态跟踪（DST）方面的努力已经朝着开放词汇或基于生成的方法发展，其中模型可以从对话历史本身生成槽值候选。(1)它们不允许模型明确地学习跨领域和槽的信号，以检测 \textit{(domain, slot)}对之间的潜在依赖关系；(2)现有的模型遵循自动回归的方法，当对话在多个领域和多个回合中演变时，会产生高的时间成本。 在本文中，我们提出了一个新的非自回归对话状态跟踪（NADST）框架，它可以考虑领域和槽之间的潜在依赖关系，以优化模型，更好地预测作为一个完整的集合而不是单独的槽的对话状态。 特别是，我们的方法的非自回归性质不仅可以实现并行解码，以大大降低实时对话响应生成的DST的延迟，而且还可以在槽和域的层面上检测槽之间的依赖性。我们的经验结果表明，我们的模型在MultiWOZ 2.1语料库上实现了所有领域的最先进的联合准确性，而且随着对话历史随时间的延长，我们模型的延迟比以前的技术状态低一个数量级。
三维变焦操作是相机在垂直于图像平面的Z轴上的正向平移。相比之下，光学变焦改变了焦距，而数字变焦则用于将图像的某个区域放大到原始图像的大小。在本文中，我们首次提出了一个无监督的三维变焦学习问题，其中可以从给定的单一图像生成具有任意变焦系数的图像。 一个无监督的框架是很方便的，因为获得自然场景的三维缩放数据集是一项具有挑战性的任务，因为需要特殊的设备来确保相机的移动被限制在Z轴上。此外，场景中的物体在被拍摄时不应该移动，这阻碍了户外场景大型数据集的构建。我们提出了一个新颖的无监督框架来学习如何生成单一图像的任意三维缩放版本，不需要三维缩放的基础真相，称为深度三维缩放网。 深度3D-Zoom网包括以下特点：(i)通过反投影重建损失从预先训练的差距估计网络中转移学习；(ii)一个完全卷积网络架构，对基于深度图像的渲染（DIBR）进行建模，考虑到高频细节而不需要估计中间差距；以及(iii)纳入一个判别器网络，作为对非自然渲染区域的无参考处罚。 即使没有基线来公平地比较我们的结果，我们的方法在大型摄像机基线上的逼真外观方面优于以前的新颖视图合成研究。我们在KITTI和Cityscapes数据集上进行了广泛的实验来验证我们方法的有效性。
通用逼近定理在其最一般的版本中说，如果我们只考虑连续激活函数Ïƒ，那么具有一个隐藏层的标准前馈神经网络能够逼近任何连续多变量函数f到任何给定的逼近阈值δ，当且仅当Ïƒ是非多项式的。在本文中，我们给出了该定理的直接代数证明。 具体来说，如果R^n中的X是紧凑的，那么一个具有n个输入单元、m个输出单元和一个具有{n+d选择d}个隐藏单元（与m和δ无关）的单一隐藏层的神经网络，可以均匀地逼近任何多项式函数f:X->R^m，其每个m坐标函数的总度最多为d。 在f是任何连续函数的一般情况下，我们表明在O(δ^{-n})中存在一些N(独立于m)，这样N个隐藏单元就足以近似f。我们还表明，即使在对权重施加看似强大的条件下，这种均匀近似属性(UAP)仍然成立。 (ii) 存在一些Î">0（仅取决于f和Ïƒ），这样，如果我们限制第一层的所有非偏见权重w满足|w|>Î"，则UAP仍然成立。 (iii) 如果第一层的非偏见权重是*固定的，并从一个合适的范围内随机选择，则UAP以1的概率成立。
在本文中，我们设计了一个通用的框架，用于学习一个稳健的文本分类模型，在测试时间预算的限制下，达到与标准完整模型相当的准确度。我们采取了与现有方法不同的方法，通过一个低复杂度的选择器来学习动态地删除一大部分不重要的词，从而使高复杂度的分类器只需要处理一小部分重要的词。 此外，我们提出了一种新的数据聚合方法来训练分类器，使其即使在碎片化的单词序列上也能做出准确的预测。我们的端到端方法实现了最先进的性能，而其计算复杂性与整个语料库中的小部分重要单词呈线性扩展。
微分架构搜索（DARTS）在寻找有效的网络架构方面提供了一个快速的解决方案，但在联合训练超级网络和搜索最佳架构方面存在大量的内存和计算开销。在本文中，我们提出了一种新的方法，即部分连接的DARTS，通过对超级网络的一小部分进行采样，减少探索网络空间的冗余，从而在不影响性能的情况下进行更有效的搜索。 特别是，我们在一个通道的子集上进行操作搜索，同时绕过被保留的部分，这是一条捷径。这种策略可能会因为对不同通道的抽样而在选择超网边缘时出现不希望出现的不一致。 我们通过引入边缘规范化来解决这个问题，边缘规范化增加了一组新的边缘级超参数，以减少搜索中的不确定性。由于内存成本的降低，PC-DARTS可以用更大的批处理量进行训练，因此，既能享受更快的速度，又能享受更高的训练稳定性。实验结果证明了所提方法的有效性。 具体来说，我们在CIFAR10上仅用0.1个GPU天就实现了2.57%的架构搜索错误率，在ImageNet上（移动设置下）用3.8个GPU天实现了最先进的24.2%的搜索错误率。我们的代码已在https://www.dropbox.com/sh/on9lg3rpx1r6dkf/AABG5mt0sMHjnEJyoRnLEYW4a?dl=0。
对话研究倾向于区分闲聊和以目标为导向的任务。虽然前者可以说是更自然的，对语言的使用也更广泛，但后者有更清晰的衡量标准和更直接的学习信号。人类毫不费力地将这两者结合起来，例如，参与闲聊的目的是交换信息或引起特定的反应。 具体来说，我们通过自我游戏来训练一个具有强化学习的目标导向模型，与具有两种新方法的模仿学习的闲聊模型进行对比：政策要么学习选择一个主题，要么学习在给定的前k个语料中选择一个语料。我们表明，这两个模型的表现都优于强逆向模型基线，可以与对话伙伴自然交谈，以实现目标。
我们考虑了当轨迹数据由多种行为政策产生时的非政策性政策评估。最近的工作表明，在无限水平的背景下，状态或状态-行动静止分布修正在非政策性政策评估中发挥了关键作用。我们提出了估计混合政策（EMP），这是一类新的部分政策无关的方法，以准确估计这些数量。 通过仔细分析，我们表明EMP在估计状态静止分布修正时产生了方差较小的估计值，同时它也为估计状态行动静止分布修正提供了有用的归纳偏差。在连续和离散环境的广泛实验中，我们证明我们的算法与最先进的方法相比具有明显的准确性。
我们为摊销推理引入了一个更有效的神经结构，它使用一个原则性的结构选择将连续和条件归一化流结合起来。我们的梯度流从其基础图形模型的最小忠实逆中获得其稀疏模式。 因此，训练时间和推理时间的吞吐量增加了，与无约束的流量相比，性能没有下降。通过将结构反演和流量构建表达为概率编程语言的编译通道，我们证明了它们适用于现实模型的随机反演，如卷积神经网络（CNN）。
我们提出了一种神经结构搜索算法，以构建紧凑的强化学习（RL）策略，通过将ENAS和ES以高度可扩展和直观的方式结合起来。通过将NAS的组合搜索空间定义为不同的边缘分区（着色）到相同重量的类别的集合，我们通过有效的学习边缘分区来表示紧凑的结构。 对于几个RL任务，我们设法学习着色，将其转化为有效的政策，其参数仅有17个权重参数，比香草政策压缩90%以上，比基于托普利茨矩阵的最先进的紧凑政策压缩6倍，同时仍然保持良好的奖励。我们相信，我们的工作是首次尝试提出一个严格的方法，为RL问题训练结构化神经网络架构，特别是在存储和计算资源有限的移动机器人领域。
异常检测的深度方法最近在大型和复杂的数据集上显示出比浅层方法更有前途的结果。异常检测的半监督方法旨在利用这种标记的样本，但大多数提出的方法仅限于包括标记的正常样本。只有少数方法利用标记的异常，现有的深度方法是特定领域的。 在这项工作中，我们提出了Deep SAD，一种用于一般半监督异常检测的端到端深度方法。使用异常检测的信息理论观点，我们推导出一种损失，其动机是正常数据的潜在分布的熵应该低于异常分布的熵。 我们在MNIST、Fashion-MNIST和CIFAR-10以及其他异常检测基准数据集上进行了广泛的实验，证明我们的方法与浅层、混合和深层的竞争者相当，甚至在只提供少量标记数据的情况下也能产生明显的性能改进。
为了分析深层ReLU网络，我们采用了学生-教师的设置，其中一个过度参数化的学生网络从相同深度的固定教师网络的输出中学习，采用随机梯度下降法（SGD）。我们的贡献有两个方面。首先，我们证明，当梯度在训练中的每个数据点都是零（或以一个小常数为界），这种情况称为 emph{插值设置}，在温和条件下，最低层的学生和教师节点之间存在多对一{对齐}现象。 第二，对2层网络的噪声恢复和训练动态的分析表明，强势的教师节点（具有大的扇出权重）首先被学习，而微妙的教师节点直到训练的后期才被学习。因此，可能需要很长时间才能收敛到这些小梯度临界点。(1)它是在临界点发生对齐的必要条件，(2)在训练动态中，它帮助学生节点以较少的迭代次数覆盖更多的教师节点，两者都能提高泛化。
我们研究了梯度下降（GD）和随机梯度下降（SGD）在训练$L$隐藏层线性残差网络（ResNets）时的收敛性。我们证明，对于在输入层和输出层有一定线性变换的深残差网络的训练，在整个训练过程中是固定的，所有隐藏权重初始化为零的GD和SGD都可以收敛到训练损失的全局最小值。此外，当专门针对适当的高斯随机线性变换时，GD和SGD证明可以优化足够宽的深线性ResNets。 与GD训练标准深度线性网络的全局收敛结果\citep{du2019width}相比，我们对神经网络宽度的条件更尖锐，为$O(\kappa L)$，其中$\kappa$表示训练数据协方差矩阵的条件数。此外，我们首次建立了SGD训练深度线性ResNets的全局收敛性，并证明了全局最小值为0$时的线性收敛率。
在本文中，我们实证研究了深度神经网络相对于完全训练的浅层机器学习模型的训练历程。我们观察到，深度神经网络（DNNs）在学习更难的例子之前，在早期的历时中通过学习对浅层可学习的例子进行正确分类来训练。 我们在这一观察的基础上，提出了一种将数据集划分为难易子集的方法，可用于改善整个训练过程。顺便说一下，我们还发现了在我们考虑的所有数据集中，有一个耐人寻味的例子子集，是可以浅度学习的，但不是深度学习的。为了帮助可重复性，我们还适当地发布了我们这项工作的代码：https://github.com/karttikeya/Shallow_to_Deep/
虽然最近的许多工作都是针对用变异推理学习深度离散潜伏模型，但这种设置仍然具有挑战性，而且在优化ELBO时往往需要利用潜在的高变异梯度估计。作为一种替代方法，我们建议优化一个非ELBO目标，该目标来自于对MRF分区函数的Bethe自由能近似。该目标引起了一个鞍点学习问题，我们训练推理网络来近似优化。 我们评估了在文本上学习高阶神经HMMs的拟议方法，并发现它在真实保持的对数似然方面往往优于其他近似推理方案。同时，我们发现所有基于近似推理的学习高阶神经HMMs的方法都比精确推理的学习效果差很多。
在解释生成问题中，一个代理需要识别并向另一个代理解释其决定的原因。这一领域的现有工作大多局限于基于规划的系统，这些系统使用自动规划方法来解决问题。在本文中，我们从一个新的角度来处理这个问题，我们提出了一个基于逻辑的解释生成的一般框架。 特别是，给定一个包含公式$phi$的知识库$KB_1$和第二个不包含$phi$的知识库$KB_2$，我们试图找出一个解释$epsilon$，它是$KB_1$的一个子集，使得$KB_2$和$epsilon$的结合包含$phi$。 我们定义了两种类型的解释，模型理论和证明理论的解释，并使用成本函数来反映解释之间的偏好。此外，我们提出了为命题逻辑实现的算法，计算这种解释，并在随机知识库和规划领域进行了经验评估。
最近的理论工作表明，深度神经网络的性能优于浅层网络，但其训练更加困难，例如。这个问题通常可以通过整流线性单元（ReLU）激活来解决。然而，在这里我们表明，即使是这样的激活，深层和狭义的神经网络（NNs）也会根据损失的情况，大概率地收敛到目标函数的错误的平均值或中值状态。 我们从数值和理论上证明了这种NN的崩溃，并提供了崩溃概率的估计。我们还构建了一个安全区域图，用于设计避免崩溃到错误状态的NN。最后，我们研究了可能避免崩溃问题的不同初始化和规范化方式。
我们从边际最大化的角度研究了神经网络的对抗性鲁棒性，其中边际被定义为从输入到分类器决策边界的距离。我们的研究表明，边际最大化可以通过在 "最短的成功扰动 "下最小化决策边界上的对抗性损失来实现，证明了对抗性损失和边际之间的密切联系。我们提出了最大边际对抗性（MMA）训练，直接最大化边际来实现对抗性的鲁棒。与固定的$epsilon$的对抗训练不同，MMA通过对每个数据点的 "正确"$epsilon$的自适应选择作为保证金，提供了一种改进。 此外，我们从边际最大化的角度严格分析了对抗训练，并为对抗训练提供了另一种解释，即最大化边际的下限或上限。我们的实验从经验上证实了我们的理论，并证明了MMA训练在MNIST和CIFAR10数据集上的功效，即$/ell_\infty$和$/ell_2$的鲁棒性。
许多异常检测方法在低维问题上表现良好，但对于高维空间，如图像，明显缺乏有效的方法。受最近深度学习的成功启发，我们提出了一种使用生成对抗网络进行异常检测的新方法。 我们在标准图像基准数据集上取得了最先进的性能，对最异常的样本的视觉检查显示，我们的方法确实返回了异常。
变量推理（VI）和马尔科夫链蒙特卡洛（MCMC）是近似的后验推理算法，通常被认为具有互补的优势，VI速度快但有偏，MCMC速度慢但渐近无偏。在本文中，我们分析了基于梯度的MCMC和VI程序，发现理论和经验证据表明这些程序并不像人们想象的那样不同。 特别是，对支配Langevin动力学（LD）MCMC程序的Fokker-Planck方程的仔细检查显示，LD隐含地遵循一个梯度流，它对应于基于优化非参数归一化流的变异推理程序。 这一结果表明，LD的瞬时偏差（由于预热步骤太少）可能会跟踪VI的瞬时偏差（由于优化步骤太少），直到VI的参数化和渐进偏差造成的差异。从经验上看，我们发现这些算法（和动量加速版本）的瞬时偏差确实在类似地发展。 这表明，时间预算有限的从业人员可能会通过运行MCMC程序（即使它远远没有被烧毁）比VI程序获得更准确的结果，只要MCMC估计器的方差可以被处理（例如。通过运行许多平行链）。
图卷积网络（GCN）最近被证明在建模图结构数据方面相当成功。然而，主要重点是处理简单的无定向图。多关系图是一种更普遍和流行的图的形式，其中每条边都有一个标签和方向与之相关。大多数现有的处理这种图的方法受到过度参数化的影响，只限于学习节点的表示。 在本文中，我们提出了CompGCN，一个新的图形卷积框架，它在关系图中共同嵌入了节点和关系。CompGCN利用了知识图谱嵌入技术中的各种实体-关系组成操作，并随着关系数量的增加而扩展，它还概括了现有的几种多关系GCN方法。我们在节点分类、链接预测和图分类等多个任务上评估了我们提出的方法，并取得了明显的优越结果。
最先进的神经机器翻译方法采用了大量的参数。在不影响性能的情况下大幅降低这些方法的计算成本，到目前为止还没有得到解决。在这项工作中，我们提出了一个针对Transformer架构的量化策略。 我们在WMT14 EN-FR和WMT14 EN-DE翻译任务上评估了我们的方法，并为Transformer取得了最先进的量化结果，与非量化基线相比，在BLEU分数上没有损失。我们进一步压缩Transformer，表明一旦模型被训练，编码器中很大一部分节点可以被移除而不会造成任何BLEU损失。
基于梯度的元学习技术适用范围广，并能熟练解决具有挑战性的少量学习和快速适应问题。然而，它们在极端低数据状态下的高维参数空间操作时有实际困难。我们表明，通过学习模型参数的数据依赖性潜生成表示，并在这个低维潜空间中进行基于梯度的元学习，可以绕过这些限制。 由此产生的方法，即潜伏嵌入优化（LEO），将基于梯度的适应程序与模型参数的底层高维空间解耦。我们的评估表明，LEO在竞争激烈的miniImageNet和tieredImageNet的少量分类任务中可以达到最先进的性能。进一步分析表明，LEO能够捕捉数据中的不确定性，并且可以通过潜伏空间的优化更有效地进行适应。
我们介绍了一种增强无模型的深度强化学习代理的方法，它具有对结构化表征进行关系推理的机制，从而提高了性能、学习效率、通用性和可解释性。我们的架构将图像编码为一组向量，并应用迭代式消息传递程序来发现和推理场景中的相关实体和关系。在七个《星际争霸II》学习环境小游戏中，我们的代理取得了最先进的性能，并在四个游戏中超过了人类大师级水平。 在一个新的导航和规划任务中，我们的代理的性能和学习效率远远超过了非关系基线，它能够概括到比它在训练期间所经历的更复杂的场景。此外，当我们检查其学习的内部表示时，它们反映了关于问题和代理意图的重要结构。 这项工作的主要贡献是通过关系归纳偏见在无模型的深度强化学习代理中引入了表示和推理状态的技术。我们的实验表明，这种方法可以在效率、泛化和可解释性方面提供优势，并且可以扩大规模以满足现代人工智能中一些最具挑战性的测试环境。
两个领域之间的图像翻译是一类旨在学习从源领域的输入图像到目标领域的输出图像的映射的问题。它已被应用于许多应用，如数据增强、领域适应和无监督训练。 我们用这样的假设来约束这个问题，即翻译后的图像需要在感知上与原始图像相似，而且似乎来自新的领域，并提出了一个简单而有效的图像翻译模型，该模型由一个用自律化项和对抗性项训练的单一生成器组成。 我们进一步注意到，现有的图像翻译技术对感兴趣的主题是不可知的，并且经常对输入引入不必要的变化或人工制品。因此，我们建议添加一个注意力模块来预测注意力图，以指导图像翻译过程。 预测的注意力图也为无监督的分割和显著性检测等应用打开了大门。大量的实验和评估表明，我们的模型虽然比较简单，但取得了比现有图像翻译方法更好的性能。
构建深度神经网络来控制必须与物理世界实时互动的自主代理，如机器人或汽车，需要将时间无缝整合到网络的架构中。这项工作的核心问题是，现实的时间性应该如何反映在深度神经网络及其组件的执行中。 大多数人工深度神经网络被划分为连接模块或层的有向图，而层本身由元素构建块组成，如单个单元。对于大多数深度神经网络，一个层的所有单元是同步和平行处理的，但层本身是以顺序方式处理的。相反，生物神经网络的所有元素是平行处理的。 这些网络以流式或同步的层间平行方式执行，解锁此类网络的各层进行平行处理。与标准的层间顺序深度网络相比，这些新的层间平行网络显示出根本不同的时间行为和信息流，特别是对于具有跳过或递归连接的网络。 我们认为，层间平行深度网络更适合未来深度神经网络设计的挑战，如大型功能模块化和/或递归架构，以及根据当前刺激和/或任务复杂性分配不同网络能力的网络。我们布置了基本属性，并讨论了层间平行网络的主要挑战。此外，我们提供了一个工具箱来设计、训练、评估和在线互动的层间平行网络。
众所周知，深度神经网络容易受到对抗性扰动的影响。在本文中，我们将神经网络的对抗性鲁棒性与动态系统的李亚普诺夫稳定性联系起来。从这个角度来看，训练神经网络等同于寻找离散动态系统的最优控制，这使得人们可以利用逐次逼近的方法，一种基于Pontryagin最大原理的最优控制算法来训练神经网络。 这种解耦的训练方法允许我们在优化中加入约束条件，这使得深度模型更加稳健。约束的优化问题可以被表述为半无限编程问题，因此可以被有效地解决。实验表明，我们的方法有效地提高了深度模型的对抗性稳健性。
在本文中，我们提出了一种名为维度重权图卷积网络（DrGCNs）的方法，以解决GCNs节点表示中维度信息的差异问题。我们证明DrGCNs可以通过将我们的问题与均值场的理论联系起来，减少节点表示的差异。 然而，实际上，我们发现DrGCNs的帮助程度在不同的数据集上有很大的不同。我们重新审视了这个问题，并开发了一个新的措施K来量化这个效果。这个措施指导我们何时应该在GCNs中使用维度重权，以及它能提供多大的帮助。 此外，它还提供了解释所提出的DrGCNs所获得的改进的见解。精心设计的实验，包括对众所周知的节点分类基准数据集的重复、信息泄露和错误标签的若干修正，证明了DrGCNs比现有的最先进的方法更出色的性能。
基于知识的对话是一项基于话语背景和外部知识产生信息反应的任务。由于我们专注于在多轮基于知识的对话中对知识选择进行更好的建模，我们提出了一个顺序潜变量模型作为解决这个问题的第一个方法。 该模型被命名为顺序知识转化器（SKT），可以跟踪知识的先验和后验分布；因此，它不仅可以减少对话中知识选择的多样性造成的模糊性，而且可以更好地利用响应信息来正确选择知识。我们的实验结果表明，所提出的模型提高了知识选择的准确性，随后提高了语料生成的性能。我们在作为最大规模和最具挑战性的基准之一的Wizard of Wikipedia（Dinan等人，2019）上实现了新的最先进的性能。我们在另一个基于知识的对话Holl-E数据集（Moghe等人，2018）中进一步验证了我们的模型对现有对话方法的有效性。
元学习，或学习到学习，已被证明是攻击监督学习和强化学习中涉及少量数据的问题的成功策略。最先进的解决方案涉及使用一组训练情节学习初始化和/或学习算法，以便元学习者能够快速概括到评估情节。这些方法表现良好，但往往缺乏对不确定性的良好量化，当数据缺乏时，这对实际应用至关重要。 我们提出了一种元学习方法，该方法在不同的任务中有效地摊销了层次化变异推理，学习了神经网络权重的先验分布，这样，通过Backprop的Bayes的几个步骤就会产生一个好的特定任务的近似后验。我们表明，我们的方法在上下文匪徒和少量学习基准上产生良好的不确定性估计。
  通常我们希望将表征性知识从一个神经网络转移到另一个神经网络。例子包括将一个大型网络提炼成一个较小的网络，将知识从一种感觉模式转移到另一种感觉模式，或者将一系列模型集合成一个单一的估计器。知识提炼，这些问题的标准方法，最小化教师和学生网络的概率输出之间的KL分歧。 实验表明，我们的新目标在各种知识转移任务上优于知识蒸馏，包括单一模型压缩、集合蒸馏和跨模式转移。当与知识蒸馏相结合时，我们的方法在许多转移任务中处于领先地位，有时甚至优于教师网络。
为深度神经网络开发有效的生物学上合理的学习规则，对于推进深度学习和神经科学之间的联系非常重要。迄今为止，像大脑所采用的局部突触学习规则未能与深度网络中的反向传播的性能相匹配。在这项工作中，我们采用元学习来发现使用反馈连接和局部生物学动机的学习规则的网络。 我们的实验表明，元训练网络有效地利用反馈连接在多层结构中进行在线学分分配。此外，我们根据经验证明，这个模型在回归和分类基准的持续学习中优于最先进的基于梯度的元学习算法。
在视觉系统中，神经元对被称为其经典感受野（RF）的一块输入作出反应，并且可以被周围的刺激所调控。这些相互作用通常由侧向连接所介导，从而产生经典外RF。 我们通过反向传播的监督学习来学习前馈连接，结合无监督学习规则来学习卷积神经网络中各单元之间的横向连接。这些连接允许每个单元整合来自其周围的信息，为我们新提出的模型（CNNEx）中的单元产生类外感受区。 我们证明，这些连接使网络更加稳健，并在MNIST和CIFAR-10数据集的噪声版本上取得了更好的性能。虽然MNIST和CIFAR-10的图像统计有很大的不同，但相同的无监督学习规则可以推广到这两个数据集。我们的框架有可能被应用到其他任务训练的网络中，当输入不可靠时，学习到的横向连接可以帮助前馈连接实现计算。
深度学习（DL）由于其卓越的性能，近年来被广泛用于自然语言处理（NLP）的应用中。本文提出了一个新的架构，通过利用张量产品代表（TPR）来弥补这一差距，TPR是过去20年中在认知科学中开发的一个结构化的神经符号框架，其目的是将DL与明确的语言结构和规则相结合。 TPGN的关键思想是：1）通过基于TPR的深度神经网络无监督地学习单词的角色解绑向量；2）将TPR与典型的DL架构（包括长短时记忆（LSTM）模型）整合。 我们的方法的新颖性在于它能够生成一个句子，并通过使用角色非绑定向量来提取句子的部分语法结构，这些向量是以无监督的方式获得的。
众所周知，分类器容易受到对抗性扰动的影响。为了抵御对抗性扰动，人们得出了各种认证的鲁棒性结果。然而，现有的认证鲁棒性仅限于top-1的预测。 我们采用随机平滑，因为它可以扩展到大规模的神经网络，并适用于任何分类器。当使用高斯噪声的随机平滑时，我们得出了top-$k$预测的$ell_2$规范的严格稳健性。我们发现，将认证的稳健性从top-1推广到top-$k$预测面临着重大的技术挑战。 我们还在CIFAR10和ImageNet上对我们的方法进行了实证评估。例如，当对抗性扰动的$ell_2$-norms小于0.5（=127/255）时，我们的方法可以得到一个ImageNet分类器，其认证的top-5准确率为62.8%。我们的代码可在以下网站公开获得。\url{https://github.com/jjy1994/Certify_Topk}。
最近的工作表明，人们对使用变异自动编码器（VAE）框架以无监督的方式发现数据的可解释性表征越来越感兴趣。这些方法主要集中在修改变异成本函数以实现这一目标。然而，我们表明，像β-VAE这样的方法简化了变异推理的欠拟合趋势，导致病态的过度修剪和学习成分的过度正交化。在本文中，我们采取了一种补充方法：修改概率模型以鼓励发现结构化潜变量表征。 具体来说，标准的VAE概率模型是不可识别的：参数的可能性在潜伏空间的旋转下是不变的。这意味着没有压力来识别每个真正的变异因素与潜伏变量。因此，我们采用丰富的先验分布，类似于ICA模型，打破了旋转对称性。 广泛的定量和定性实验表明，所提出的先验减轻了像β-VAE和TCVAE这样的修正成本函数在重建损失和解缠之间的权衡。所提出的先验允许在解缠和重建质量方面比现有的技术水平大大改善这些方法。
由于残差网络（resnets）和相关架构的成功，捷径连接已迅速成为构建卷积神经网络的标准工具。文献中对捷径的明显有效性的解释是多种多样的，而且往往是相互矛盾的。我们假设捷径的作用主要是因为它们作为非线性层的线性对应物。 我们通过使用标准残差块的几种变体，以及不同类型的线性连接，来构建小型（100k--1.2M参数）图像分类网络，来检验这一假设。我们的实验表明，其他类型的线性连接甚至比身份捷径更有效。
亚当类型的优化器，作为一类具有指数移动平均方案的自适应矩估计方法，已经成功地用于深度学习的许多应用中。这种方法对于大规模稀疏数据集的能力是有吸引力的。 AdamT不是应用简单的指数加权平均，而是在用自适应步长和梯度更新参数时包括趋势信息。新增加的项有望有效地捕捉成本面上的非水平移动模式，从而更快速地收敛。
随着机器学习方法在医疗图像诊断等高风险应用中得到更多的采用和实施，对模型的可解释性和解释的需求变得更加关键。我们提出了一种方法，通过逐渐夸大给定类别的语义效果来解释分类黑箱的结果。 给定一个输入到分类器的查询，我们的方法产生一套渐进式的该查询的合理变化，这些变化逐渐改变后验概率，从它的原始类别到它的否定。这些反事实产生的样本保留了与分类决策无关的特征，这样，用户可以使用我们的方法作为一个 "调谐旋钮 "来穿越数据流形，同时跨越决策边界。 我们的方法与模型无关，只需要预测器的输出值和相对于其输入的梯度。
我们研究了解释深度神经网络丰富的行为属性的问题。我们的影响导向解释通过窥视网络内部，使用公理上合理的影响度量来识别对感兴趣的属性具有高影响力的神经元，然后为这些神经元代表的概念提供解释。我们通过在Pubfig、ImageNet和糖尿病视网膜病变数据集上训练卷积神经网络来评估我们的方法。 我们的评估表明，以影响为导向的解释（1）定位了网络使用的特征，（2）隔离了区分相关实例的特征，（3）帮助提取了网络学到的关于类别的本质，以及（4）协助调试错误分类。
标准的深度学习系统需要数以千计或数以百万计的例子来学习一个概念，并且不能轻易地整合新的概念。例如，仅仅从听到一句话中使用的一个词，人类就可以通过利用周围词语的语法和语义来推断出大量的信息。 在这里，我们从中得到启发，强调了一种简单的技术，通过这种技术，深层递归网络可以类似地利用其先前的知识，从很少的数据中为一个新词学习一个有用的表示。
最近开发具有外部记忆的神经网络架构的研究经常使用基准的bAbI问题和回答数据集，该数据集提供了大量需要推理的挑战性任务。这里我们采用了人类神经科学文献中的经典关联推理任务，以便更仔细地探测现有记忆增强的架构的推理能力。 在一个更复杂的任务上也得到了类似的结果，该任务涉及寻找路径中节点之间的最短路径。因此，我们开发了一个新的架构，MEMO，它被赋予了推理长距离的能力。 首先，它引入了存储在外部记忆中的记忆/事实与外部记忆中构成这些事实的项目之间的分离。其次，它利用了一种自适应的检索机制，允许在产生答案之前进行可变数量的 "记忆跳跃"。
深度可分离卷积减少了卷积操作中使用的参数和计算的数量，同时提高了表示效率。它们已被证明在图像分类模型中是成功的，既获得了比以前在给定参数数下可能获得的更好的模型（Xception架构），又大大减少了在给定水平上执行所需的参数数量（MobileNets系列架构）。最近，卷积序列到序列网络已被应用于机器翻译任务，并取得了良好的效果。在这项工作中，我们研究深度可分离卷积如何应用于神经机器翻译。 我们引入了一个受Xception和ByteNet启发的新架构，称为SliceNet，它能够显著减少获得像ByteNet那样的结果所需的参数数和计算量，并且在参数数相似的情况下，取得更好的结果。 除了表明深度可分离卷积在机器翻译中表现良好外，我们还研究了它们所带来的架构变化：我们观察到，由于深度可分离性，我们可以增加卷积窗口的长度，消除对滤波器扩张的需求。我们还引入了一个新的超分离卷积操作，进一步减少了模型的参数数量和计算成本。
将生成对抗网络（GAN）训练解释为近似发散最小化在理论上很有见地，引发了讨论，并导致了理论上和实践上有趣的扩展，如f-GANs和Wasserstein GANs。 非饱和方案通常被认为是处理优化问题的简单修改，但我们表明事实上GANs的非饱和方案有效地优化了一个类似KL的反向f发散。我们还开发了一些理论工具来帮助比较和分类f发散。我们希望这些结果可以帮助澄清一些围绕GAN训练的发散最小化观点的理论讨论。
我们介绍了一种将文本数据转换为抽象图像表示的新方法，这使得基于图像的处理技术（如图像分类网络）可以应用于基于文本的比较问题。我们将该技术应用于美国专利中发明人姓名的实体消歧。该方法涉及将两个发明人姓名记录之间每一对比较的文本转换为二维RGB（叠加）图像表示。 然后，我们训练一个图像分类神经网络来区分这种成对的比较图像，并使用训练好的网络将每对记录标记为匹配（相同的发明人）或不匹配（不同的发明人），获得高度准确的结果（F1：99.09%，精度：99.41%，召回率：98.76%）。我们新的文本到图像表示方法有可能更广泛地用于其他NLP比较问题，如学术出版物的消歧义，或用于需要同时分类文本和图像的问题。
我们提出了一种新的算法--求差生成对抗网络（DSGAN），它是从传统的GAN发展而来的。DSGAN考虑了目标分布的训练样本$p_{t}$难以收集的情况。假设有两个分布$p_{bar{d}}$和$p_{d}$，这样目标分布的密度可以是$p_{bar{d}$和$p_{d}$的密度之差。 我们展示了如何仅通过$p_{d}$和$p_{bar{d}}$的样本来学习目标分布$p_{t}$（相对容易获得）。DSGAN可以灵活地产生各种目标分布的样本（例如分布外）。
最近，生成对抗网络（GAN）和它的一些变种被广泛用于解决图像到图像的翻译问题，并在监督和无监督的情况下取得了非凡的成果。然而，大多数基于GAN的方法在实践中受到生成器和判别器之间的不平衡问题的影响。 更具体地说，我们用一种注意力机制来武装鉴别器，这样它不仅可以估计其输入是真实的概率，而且还可以创建一个注意力图，突出这种预测的关键特征。
根据给定的证据验证一个文本假设是否成立的问题，也被称为事实验证，在自然语言理解和语义表示的研究中起着重要的作用。然而，现有的研究主要限于处理非结构化证据（如自然语言句子和文件、新闻等），而在结构化证据下的验证，如表格、图形和数据库，仍然没有得到探索。本文特别旨在研究以半结构化数据为证据的事实验证。 为此，我们构建了一个名为TabFact的大规模数据集，用16k维基百科的表格作为118k人类注释的自然语言声明的证据，这些声明被标记为ENTAILED或REFUTED。TabFact具有挑战性，因为它同时涉及软语言推理和硬符号推理。Table-BERT和Latent Program Algorithm (LPA).Table-BERT利用最先进的预训练语言模型将线性化的表格和语句编码为连续矢量进行验证.LPA将语句解析为类似LISP的程序，并针对表格执行这些程序以获得返回的二进制值进行验证.这两种方法取得了相似的准确性，但仍然远远落后于人类的表现.我们还进行了全面分析以展示未来的巨大机会。
这项工作提出了一个两阶段的神经架构，用于学习和完善图之间的结构对应关系。首先，我们使用图神经网络计算的局部节点嵌入来获得节点之间软对应关系的初始排名。其次，我们采用同步消息传递网络来迭代地重新排列软对应关系，以在图之间的局部邻域达成匹配的共识。 我们从理论和经验上表明，我们的消息传递方案为相应的邻域计算了一个有根据的共识度量，然后用来指导迭代重排过程。我们的纯局部和疏散意识的架构可以很好地扩展到大的、真实世界的输入，同时仍然能够一致地恢复全局对应关系。我们在计算机视觉和知识图之间的实体对齐领域的真实世界任务上证明了我们方法的实际效果，在这些任务上我们改进了目前的最先进水平。
本文将欧几里得空间上的连续（甚至可测）函数空间中的神经网络密度证明扩展到概率度量的紧凑集合上的函数。通过这样做，这项工作与十多年前关于再现核希尔伯特空间中概率度量的平均映射嵌入的结果相似。 这项工作对多实例学习有广泛的实际影响，它在理论上证明了最近提出的一些构造。然后，该结果被扩展到笛卡尔产品，产生了树状结构域的普遍近似定理，这在JSON、XML、YAML、AVRO和ProtoBuffer等数据交换格式中自然出现。这具有重要的实际意义，因为它能够自动创建一个处理结构化数据的神经网络架构（AutoML范式），正如一个用于JSON格式的伴随库所示。
诸如句子中的双重否定和图像中的场景互动是最先进的机器学习模型所捕获的复杂依赖关系的常见形式。我们提出了MahÃ©，这是一种新颖的方法，提供对强大的机器学习模型（如深度神经网络）如何捕获这些互动的模型无知的层次解释，这些互动要么依赖于数据实例的上下文，要么不受其影响。 具体来说，Mahé通过一种新的局部解释算法提供依赖于上下文的解释，该算法有效地捕捉了任何顺序的互动，并通过泛化依赖于上下文的互动来获得无上下文的解释，以解释全局行为。实验结果表明，Mahé获得了比最先进的方法更好的局部互动解释，并成功提供无上下文的互动解释。
为了实现无处不在的嵌入式深度网络推理的承诺，必须寻求能量和面积效率的极限。 为此，低精度网络提供了巨大的前景，因为能量和面积都会随着精度的降低而呈四次方扩展。 在这里，我们首次在ImageNet分类基准上展示了ResNet-18、ResNet-34、ResNet-50、ResNet-152、Inception-v3、densenet-161和VGG-16bn网络，在8位精度下，经过一个历时的微调，其精度超过了全精度基线网络，从而充分利用了预训练模型的可用性。我们还展示了ResNet-18、ResNet-34和ResNet-50 4位模型，与全精度基线网络的精度相当，是迄今为止的最高得分。 令人惊讶的是，低精度网络的权重与相应基线网络的权重非常接近（余弦相似度），因此没有必要从头开始训练。我们发现，训练期间量化导致的梯度噪声随着精度的降低而增加，并寻求克服这种噪声的方法。随机梯度下降法为达到一定的训练误差所需的迭代次数与（a）初始解与最终解的距离加上（b）梯度估计的最大差异的平方有关。 从这一观察中得到启发，我们(a)通过从预训练的fp32精度基线网络开始并进行微调来减少解的距离，以及(b)通过使用较大的批次和匹配的学习率退火来对抗训练期间量化权重和激活所引入的噪声。 敏感性分析表明，这些技术，加上适当的激活函数范围校准，提供了一个有希望的启发式方法来发现低精度网络，如果它们存在的话，接近fp32精度基线网络。
  随着深度学习成为NLP的主流方法，我们越来越需要能够让我们更好地理解语言神经模型的表征和功能的分析方法。这里我们提出了两种基于表征相似性分析（RSA）和树核（TK）的方法，这使得我们能够直接量化在神经激活模式中编码的信息与语法树等符号结构所代表的信息的对应程度。 我们首先在具有明确语法和语义的算术表达式的简单合成语言的情况下验证了我们的方法，并表明它们表现出预期的结果模式。
监督下的深度学习需要大量带有注释的训练样本（例如，分类任务的标签类，分割任务的像素或体素化标签图），而这些注释的获取是昂贵而耗时的。在深度神经网络的训练过程中，注释样本以小批量的方式被送入网络，在这里它们通常被认为是同等重要的。 然而，在训练过程中，一些样本的信息量可能会变小，因为这些样本的梯度大小开始消失。同时，其他具有较高效用或硬度的样本可能更需要训练过程的进行，需要更多的利用。 为了解决昂贵的注释和样本信息量损失的挑战，我们在这里提出了一个新的训练框架，它可以自适应地选择信息量大的样本，并将其输入到训练过程中。自适应选择或抽样是基于生成模型构建的潜在空间中的硬度感知策略进行的。 为了评估所提出的训练框架，我们在三个不同的数据集上进行了实验，包括用于图像分类任务的MNIST和CIFAR-10，以及用于生物物理模拟任务的医学图像数据集IVUS。在所有三个数据集上，所提出的框架都优于随机采样方法，这证明了我们框架的有效性。
现有的人工智能生成艺术品的方法仍然难以生成高质量的风格化内容，其中高层语义被保留，或者从不同的艺术家那里分离出细粒度的风格。我们提出了一种新颖的生成对抗性解纠网络，它可以在一般情况下只对其中一个因素进行标记的情况下分离出两个互补的变化因素，特别是将复杂的动漫插图完全分解为风格和内容。 我们的方法分为两个阶段，一个是将输入图像编码为独立于风格的内容，另一个是基于双条件生成器。我们展示了使用单一的端到端网络生成具有固定内容和超过一千名艺术家的大量风格的高保真动漫肖像的能力，反之亦然，并在风格转移方面有所应用。
最近的研究表明，CNN通常对高频纹理模式过于敏感。受人类对低频（大尺度）模式更敏感这一直觉的启发，我们设计了一个正则化方案，惩罚每个卷积核内相邻组件之间的巨大差异。 我们将我们的正则化应用于几种流行的训练方法，表明具有所提议的平滑核的模型享有更好的对抗性鲁棒性。此外，在最近建立对抗性鲁棒性和可解释性之间联系的工作基础上，我们表明我们的方法似乎提供了更多感知上一致的梯度。
尽管关于强化学习算法和应用的文献越来越多，但对它们的统计推断却知之甚少。在本文中，我们研究了Q值估计的大样本行为，并对渐进方差进行了闭式表征。这使我们能够有效地构建Q值和最优值函数的置信区，并制定政策以最小化其估计误差。这也导致了一种政策探索策略，它依赖于估计Q值估计之间的相对差异。
瑕疵向量是在向量中编码哪些信息是已知的，哪些是未知的一种原则性方法。 它们被设计成一种关系模型，其中一个向量应该包括另一个向量的所有信息，这被称为必然性。 本文研究了在无监督的情况下学习词的语义的尾随向量。 我们使用简单的基于赋值的文本语义模型（分布式语义），诱导出赋值向量的词嵌入，在无监督和半监督的超义词实验中，其结果优于以往预测词之间的赋值的最佳效果。
我们描述了一个简单的方案，允许代理以无监督的方式学习其环境。我们的方案让同一代理的两个版本Alice和Bob相互竞争。Alice提出一个任务让Bob完成；然后Bob尝试完成该任务。 在这项工作中，我们将专注于两种环境。(爱丽丝将通过做一连串的动作来 "提出 "任务，然后鲍勃必须分别撤销或重复这些动作。 通过一个适当的奖励结构，Alice和Bob自动产生一个探索课程，使代理的无监督训练成为可能。当Bob被部署在环境中的RL任务上时，这种无监督训练减少了学习所需的监督情节的数量，并且在某些情况下收敛到一个更高的奖励。
许多现实世界的数据集被表示为图，如引文链接、社交媒体和生物互动。易变的图结构使得采用卷积神经网络（CNN）进行图数据处理成为非难事。 最近，图关注网络（GAT）通过将图神经网络与关注机制相结合，实现了任意结构的图中的按摩传递，证明了一种有希望的尝试。然而，GAT中的关注主要是基于节点内容之间的相似性来计算的，而图的结构在很大程度上仍然是不存在的（除了掩盖一跳邻居的关注）。 在本文中，我们提出了一个`````````````````````````````"ADaptive Structural Fingerprint"（ADSF）模型，以充分利用图的拓扑细节和节点的内容特征。其关键思想是用一个加权的、可学习的接受域来编码丰富多样的局部图结构，从而使每个节点都有背景。 此外，我们的模型为不同的节点特征子空间和不同规模的图结构提供了一个有用的平台，通过多头注意力的学习，可以相互 "交叉对话"，在处理复杂的真实世界数据时特别有用。 在一些节点分类的基准数据集上观察到令人鼓舞的性能。
虽然贝叶斯最优理论上是黄金标准，但现有的算法不能很好地扩展到连续状态和行动空间。 我们将挑战分成两个简单的部分。首先，我们获得一个千里眼专家的集合，并融合他们的建议来计算一个基线政策。第二，我们训练一个贝叶斯剩余政策，以改善集合的建议，并学习减少不确定性。
神经网络成功的奥秘之一是随机初始化的一阶方法，如梯度下降可以实现零训练损失，即使目标函数是非凸的和非平滑的。本文对两层全连接ReLU激活的神经网络解密了这个令人惊讶的现象。 对于一个具有ReLU激活的$m$隐藏节点的浅层神经网络和$n$训练数据，我们表明只要$m$足够大，并且没有两个输入是平行的，随机初始化梯度下降会以二次损失函数的线性收敛率收敛到一个全局最优解。 我们的分析依赖于以下观察：过度参数化和随机初始化共同限制了每个权重向量在所有迭代中都接近其初始化，这使得我们能够利用一个类似于强对流的属性来表明梯度下降以全局线性速率收敛到全局最优。我们相信这些见解在分析深度模型和其他一阶方法时也是有用的。
对于许多应用，特别是在自然科学领域，任务是从一组测量值中确定隐藏的系统参数。通常，从参数空间到测量空间的前向过程是明确的，而逆向问题是模糊的：多个参数集可能导致相同的测量结果。 与试图直接解决模糊反向问题的经典神经网络不同，INNs专注于学习正向过程，使用额外的潜在输出变量来捕获其他失去的信息。 我们从理论上证明并通过实验验证，在医学和天体物理学的人工数据和现实世界的问题上，INN是一个强大的分析工具，可以找到参数空间的多模式，发现参数的相关性，并识别不可恢复的参数。
机器学习系统做出的决定对世界的影响越来越大。然而，机器学习算法普遍认为不存在这种影响。一个例子是在线学习中使用i.i.d.假设，用于内容推荐等应用，其中显示的（选择）内容可以改变用户的看法和偏好，甚至驱赶他们，导致用户的分布发生转变。一般来说，算法有可能改变其自身输入的分布。我们引入术语自我诱导分布转变（SIDS）来描述这种现象。 强化学习和因果机器学习中的大量工作旨在处理因部署先前离线训练的学习系统而引起的分布转移。我们的目标类似，但又不同：我们指出学习算法的变化，如元学习的引入，可以揭示分布转移的隐藏激励（HIDS），并旨在诊断和预防与隐藏激励相关的问题。 我们设计了一个简单的环境作为HIDS的 "单元测试"，以及一个内容推荐环境，使我们能够区分不同类型的SIDS。我们证明了HIDS在这些环境中可能导致意外或不良行为，并提出和测试了一个缓解策略。 
在单类学习任务中，只有正常情况可以用数据建模，而所有可能的异常情况的变化太大，无法用样本充分描述。因此，由于缺乏代表性的数据，广泛的判别方法不能覆盖这样的学习任务，而是使用生成模型，试图学习正常情况的输入密度。然而，生成模型受到大输入维度（如图像）的影响，通常是低效的学习者。 我们建议用多假设自动编码器更有效地学习数据分布。此外，该模型被一个判别器所批判，它可以防止没有数据支持的人为数据模式，并强制执行不同假设的多样性。 这个基于一致性的异常检测（ConAD）框架允许可靠地识别非分布样本。对于CIFAR-10的异常检测，它比以前报告的结果提高了3.9%的分数。在一个真实的异常检测任务中，该方法将基线模型的误差从6.8%降低到1.5%。
生成对抗网络（GAN）在不同类型的数据上学习复杂的数据分布时可以取得很好的性能。在本文中，我们首先表明，现有GAN算法的直接扩展不适用于点云，因为对于集合数据来说，判别器所需的约束是未定义的。 我们对GAN算法提出了一个两方面的修改，以便能够生成点云（PC-GAN）。首先，我们通过学习一个分层的、可解释的采样过程，结合了层次贝叶斯建模和隐含生成模型的思想。我们方法的一个关键组成部分是，我们为隐藏变量训练一个后验推理网络。 其次，PC-GAN定义了一个通用框架，可以纳入许多现有的GAN算法。我们进一步提出了一个夹层目标，与WGAN中常用的对偶形式相比，它能产生更紧密的Wasserstein距离估计。我们在ModelNet40基准数据集上验证了我们的主张，观察到通过夹层目标训练的PC-GAN在测试数据上取得了比现有方法更好的结果。 我们还对几个任务进行了研究，包括对未见过的点云的泛化、潜在空间插值、分类和图像到点云的转换，以证明所提出的PC-GAN算法的通用性。
现有的注意力机制大多是基于项目的，即一个模型被训练来关注一个集合（记忆）中的单个项目，其中每个项目都有一个预定义的、固定的颗粒度，例如。直观地说，记忆中由多个项目组成的区域可以作为一个整体值得关注。我们提出了区域注意：一种关注记忆区域的方法，其中每个区域包含一组项目，当记忆具有二维结构时，这些项目在空间上是相邻的，如图像，或在时间上相邻的一维记忆，如自然语言句子。重要的是，一个区域的大小，即一个区域内的项目数量或聚合水平，是通过学习动态确定的，它可以根据相邻项目的学习一致性而变化。通过让模型选择关注一个区域的项目，而不是只关注单个项目，模型可以关注具有不同颗粒度的信息。 我们在两个任务上评估了区域注意：神经机器翻译（字符和符号级）和图像字幕，并在所有情况下都比强大的（最先进的）基线有所改进。除了提出区域注意的新概念外，我们还通过利用总和区域表的技术贡献了计算它的有效方法。
我们发现了一种现象，我们称之为*多模型遗忘*，这种现象在连续训练多个具有部分共享参数的深度网络时发生；由于共享参数被覆盖，在优化后续模型时，先前训练的模型的性能会下降。 为了克服这个问题，我们引入了一个统计学上合理的权重可塑性损失，根据其对先前模型的重要性来规范模型共享参数的学习，并证明其在连续训练两个模型和神经架构搜索时的有效性。在神经架构搜索中添加权重可塑性，可以将最佳模型保留到搜索结束，并在自然语言处理和计算机视觉任务中产生更好的结果。
揭示数据中的潜在结构是一个活跃的研究领域，它引入了令人兴奋的技术，如变异自动编码器和对抗网络，对于推动机器学习走向无监督的知识发现至关重要。然而，一个主要的挑战是缺乏合适的基准来客观和定量地评价所学的表征。为了解决这个问题，我们引入了Morpho-MNIST框架，旨在回答："我的模型在多大程度上学会了代表数据中的特定变化因素？"我们通过增加形态分析来扩展流行的MNIST数据集，使其能够定量比较训练的模型，识别潜在变量的作用，并描述样本的多样性。
在奖励稀少的环境中进行探索是强化学习的一个关键挑战。我们如何设计具有通用归纳偏见的代理，以便它们能够以一致的方式进行探索，而不是仅仅使用像ε-greedy这样的局部探索方案？我们提出了一个无监督的强化学习代理，它学习了一个离散的像素分组模型，保留了传感器的空间几何，同时也隐含了环境的空间几何。 我们使用这种表示法来推导几何学上的内在奖励函数，如中心点坐标和面积，并学习政策，用非政策学习来控制其中的每一个。这些政策形成了行为（选项）的基础集，使我们能够以一致的方式进行探索，并在分层强化学习设置中使用它们来解决外在定义的奖励。我们表明，我们的方法可以扩展到各种领域，具有竞争力的性能，包括3D环境中的导航和具有稀疏奖励的Atari游戏。
组合优化是计算机科学中的一个常见主题。虽然一般来说，这类问题是NP-Hard，但从实用的角度来看，局部最优解可能是有用的。然而，在一些组合问题中，可能很难定义有意义的解邻域，连接搜索空间的大部分，从而阻碍了直接搜索这个空间的方法。我们建议利用政策梯度算法来规避这种情况，该算法将问题转换为连续域，并优化一个新的代理目标，使前者成为通用随机优化器。 这是通过产生一个代理目标来实现的，这个代理目标的分布是固定的和预先确定的，因此不需要以个案的方式微调各种超参数。由于我们对能够成功恢复局部最优解的方法感兴趣，我们使用寻找局部最大的悬崖问题作为一个具有挑战性的实验基准，我们报告了在一个大型图形数据集上的结果，该数据集是用来测试悬崖寻找算法。 值得注意的是，我们在这个基准中表明，固定代理的分布是持续恢复局部最优解的关键，而且我们的代理目标导致了一个在许多措施上优于我们所测试的其他方法的算法。
确定性神经网络（NN）越来越多地被部署在安全关键领域，其中校准的、稳健的和有效的不确定性措施是至关重要的。虽然有可能通过最大化高斯似然函数来训练回归网络以输出概率分布的参数，但产生的模型仍然对其预测的基本信心视而不见。在本文中，我们提出了一种新方法来训练确定性的NN，不仅要估计期望的目标，而且要支持该目标的相关证据。 我们通过在我们的原始高斯似然函数上放置证据先验，并训练我们的NN来推断我们的证据分布的超参数来实现这一目标。我们在训练过程中施加先验，这样当模型的预测证据与正确的输出不一致时，就会受到惩罚。 因此，该模型不仅估计了我们目标的概率均值和方差，而且还估计了与这些参数中的每个参数相关的潜在不确定性。我们观察到，我们的证据回归方法在各种基准上学习了很好的校准的不确定性措施，可扩展到复杂的计算机视觉任务，并且对对抗性输入扰动具有鲁棒性。
Frankle & Carbin（2019）提出的彩票假说猜想，对于典型大小的神经网络，有可能找到小的子网络，其训练速度和产生的性能优于原来的对应网络。所提出的搜索这种子网络（中奖彩票）的算法，迭代幅度修剪（IMP），一直找到参数少90-95%的子网络，确实比它们所提取的过度参数化模型训练速度和性能更好，为转移学习等问题创造潜在应用。 在本文中，我们提出了一种新的搜索中奖彩票的算法--连续稀疏化（Continuous Sparsification），该算法在训练过程中不断地从网络中删除参数，并用基于梯度的方法来学习子网络的结构，而不是依靠修剪策略。我们通过经验表明，我们的方法能够找到优于迭代幅度修剪所学的彩票，同时，如果用训练历时数来衡量，搜索速度可提高5倍。
在大多数实际环境和理论分析中，人们假设一个模型可以被训练到收敛。然而，机器学习数据集和模型的复杂性不断增加，可能会违反这种假设。事实上，目前的超参数调整和神经结构搜索方法往往受到实际资源限制。我们分析了以下问题："给定一个数据集、算法和固定的资源预算，什么是可以实现的最佳性能？"我们专注于优化迭代次数作为代表资源。在这样的设置下，我们表明，根据给定的预算调整学习率计划是至关重要的。 我们通过在ImageNet（图像分类）、Kinetics（视频分类）、MS COCO（物体检测和实例分割）和Cityscapes（语义分割）上对最先进的模型进行广泛的实验来支持我们的主张。我们还分析了我们的结果，发现一个好的时间表的关键是预算收敛，一种梯度在每个允许的预算结束时消失的现象。
我们提出了一种有效探索的新方法，该方法利用基于模型和无模型目标的组合来学习环境的低维编码。我们的方法使用基于低维表示空间中最近邻居的加权距离的内在奖励来衡量新奇度。 我们的方法的一个关键因素是，我们在每个环境步骤之间执行更多的梯度步骤，以确保模型的准确性。我们在一些迷宫任务以及控制问题上测试了我们的方法，并表明我们的探索方法与强大的基线相比更具有样本效率。
神经网络很容易受到小的对抗性扰动的影响。虽然现有的文献主要集中在学习模型的脆弱性上，但我们展示了一个有趣的现象，即对抗性鲁棒性，与干净的准确性不同，对输入数据分布很敏感。即使是对输入数据分布进行语义保全的转换，也会导致在新分布上训练和评估的对抗性训练模型的鲁棒性明显不同。 我们通过为MNIST和CIFAR10分别构建语义相同的变体来证明这一点，并表明标准训练的模型在它们身上实现了相似的清洁精度，但对抗性训练的模型实现了明显不同的鲁棒性精度。这种反直觉的现象表明，仅输入数据分布就能影响训练的神经网络的对抗性鲁棒性，而不一定是任务本身。最后，我们讨论评估对抗性鲁棒性的实际影响，并初步尝试理解这一复杂的现象。
采样效率低下是强化学习（RL）中一个长期存在的问题。 最先进的方法是使用行动值函数来推导策略，而这通常涉及到对状态-行动空间的广泛搜索和不稳定的优化。为了实现样本效率高的RL，我们提出了排名政策梯度（RPG），这是一种政策梯度方法，可以学习一组离散行动的最佳排名。 为了加速政策梯度方法的学习，我们建立了收益下限最大化和模仿接近最优政策之间的等价关系，而不需要访问任何神谕。这些结果导致了一个通用的非政策学习框架，它保留了最优性，减少了方差，并提高了样本效率。
我们介绍了MultiGrain，这是一个神经网络架构，可以生成紧凑的图像嵌入向量，解决不同粒度的多个任务：类别、实例和副本识别。MultiGrain通过优化交叉熵损失来联合训练分类，并通过优化自我监督的排名损失来训练实例/副本识别，自我监督的损失只使用数据增强，因此不需要额外的标签。 当送入线性分类器时，使用ResNet-50的MultiGrain在ImageNet上达到了79.4%的最高准确率，比目前最先进的AutoAugment方法的绝对值提高了+1.8%。同样的嵌入在中等分辨率的图像上与最先进的实例检索性能相当。
 在本文中，我们研究了将词网中的hyonymy关系映射到特征向量。 我们的目标是对词汇知识进行建模，使其能够被用作通用机器学习模型的输入，如短语关联预测器。 我们提出了两个模型。第一个模型利用现有的词到特征向量的映射（fasttext），并试图将这些向量分类为每个类别的内部或外部。第二个模型是完全监督的，只使用词网作为基础真理。 在第一个模型上，我们接近，但没有达到最先进的性能。第二个模型可以达到接近完美的准确性。
递归神经网络（RNN）是强大的自回归序列模型，用于学习自然语言中的普遍模式。  然而，由RNNs生成的语言往往显示出人类语言中不常见的几个退化特征；虽然流畅，但RNN的语言生成可能过度泛化、重复，甚至是自相矛盾的。 我们推测，由RNN语言模型优化的目标函数，相当于文本的整体困惑度，没有足够的表现力来捕捉良好的生成的抽象品质，如Grice's Maxims.在本文中，我们介绍了一个通用的学习框架，可以构建一个更适合生成的解码目标。从一个生成训练的RNN语言模型开始，我们的框架通过结合几个可共同解决RNN生成限制的鉴别训练模型，学习构建一个大大加强的生成器。 人工评估表明，由所产生的生成器生成的文本比基线的文本要好得多，并大大增强了生成文本的整体连贯性、风格和信息内容。
近年来，随着服务器异质性水平的提高和云服务与应用规模的不断扩大，传统的负载平衡策略的效率甚至可行性都受到了挑战。在这样众多的软件负载平衡异质系统中，传统的解决方案，如JSQ，会产生越来越多的通信开销，而低通信替代方案，如JSQ（d）和最近提出的JIQ方案不是不稳定就是性能不佳。 我们认为，通过允许每个调度员对系统有不同的看法并继续使用JSQ，而不是贪婪地试图在每个决策的基础上避免饥饿，可以建立一个更好的低通信负载平衡方案。 我们正式确立了任何Loosely-Shortest-Queue策略的强稳定性，并提供了一个易于验证的充分条件来验证一个策略是Loosely-Shortest-Queue的。 最后，通过广泛的模拟，考虑到同质、异质和高度倾斜的异质系统，在有单个调度员和多个调度员的情况下，我们表明所研究的松散最短队列示例政策总是稳定的，正如理论所规定的那样。此外，它表现出吸引人的性能，在使用类似通信预算的情况下，明显优于著名的低通信政策，如JSQ（d）和JIQ。
我们提出了一个新的定量指标来预测深度神经网络分类器的性能，该指标完全来自于网络的图结构。我们希望该指标是开发评估新网络结构的方法的基本第一步，并减少对模型选择中涉及的计算上昂贵的试错或 "蛮力 "优化过程的依赖性。 该措施是在多层感知器（MLPs）的背景下得出的，但其定义被证明在深度卷积神经网络（CNN）的背景下也是有用的，它能够估计和比较不同类型的神经网络的相对性能，如VGG、ResNet和DenseNet。 我们的措施也被用来研究DenseNet架构中一些重要的 "隐藏 "超参数的影响，如DenseNet-BC中的层数、增长率和1x1卷积的维度。最终，我们的措施促进了DenseNet设计的优化，与基线相比，其结果有所改善。
在大规模机器学习实践中使用的学习率计划与随机逼近理论中规定的可接受的学习率计划之间存在着明显的差距。最近的结果，如使用振荡学习率的 "超级收敛 "方法，更有助于强调这一点。 一个合理的解释是，非凸的神经网络训练程序更适合使用根本不同的学习率计划，比如 "每隔恒定次数削减学习率 "的方法（它更类似于指数衰减的学习率计划）；请注意，这种广泛使用的计划与随机逼近文献中规定的多项式衰减方案形成鲜明对比，这些方案确实被证明对凸优化问题类别是（最坏情况）最优的。 这项工作的主要贡献表明，情况要细微得多，我们甚至不需要转移到非凸优化中去，就可以显示出其他学习率方案的有效性。 事实上，即使是在固定时间范围的随机线性回归的简单情况下，任何多项式衰减方案实现的速率与统计上的minimax速率相比都是次优的（以条件数为系数）；相反，与任何多项式衰减方案相比，"每隔恒定次数削减学习速率 "提供了指数级的改进（只取决于条件数的对数）。 最后，重要的是要问，我们的理论见解是否在某种程度上与二次损失最小化有根本的联系（在这里我们规避了更普遍的凸优化问题的最小下限）？在这里，我们猜想，最近的结果使梯度规范以接近最优的速度变小，对于凸和非凸优化，也可能对实践中使用的学习率计划提供更多的见解。
我们提出了价值传播（VProp），这是一套建立在价值迭代基础上的参数有效的可微分规划模块，可以成功地使用强化学习来训练解决未见的任务，有能力泛化到更大的地图尺寸，并且可以学习在动态环境中导航。 我们表明，当环境也包括随机元素时，这些模块能够学习规划，提供了一个具有成本效益的学习系统，为各种交互式导航问题建立低水平的尺寸不变的规划器。我们在MazeBase网格世界的静态和动态配置上进行了评估，随机生成了几种不同尺寸的环境，并在StarCraft导航场景上进行了评估，动态更复杂，像素作为输入。
学习高质量的词嵌入对于在许多下游学习任务中实现更好的性能具有重要意义。一方面，传统的词嵌入是为通用任务在大规模的语料库上训练的，这对于许多特定领域的任务往往是次优的。 另一方面，许多特定领域的任务没有足够大的领域语料库来获得高质量的嵌入。我们观察到，领域不是孤立的，一个小的领域语料库可以利用从过去许多领域学到的知识来增强该语料库，以产生高质量的嵌入。在本文中，我们将词嵌入的学习表述为一个终身学习过程。 考虑到从以前的许多领域学到的知识和一个小的新领域语料库，所提出的方法可以通过利用一个简单而有效的算法和元学习器来有效地生成新的领域嵌入，其中元学习器能够提供领域级的词汇上下文相似度信息。 实验结果表明，所提出的方法能够有效地从小型语料库和过去的领域知识中学习新的领域嵌入，我们将在最终修改后发布代码。
参数修剪是一种很有前途的CNN压缩和加速方法，通过消除多余的模型参数，并有可容忍的性能损失。尽管它很有效，但现有的基于正则化的参数修剪方法通常以大而恒定的正则化因子驱动权重趋于零，这忽略了CNN的表现力是脆弱的，需要一种更温和的正则化方式来使网络在修剪期间适应。 为了解决这个问题，我们提出了一种新的基于正则化的修剪方法（名为IncReg），根据不同权重组的相对重要性，逐步为其分配不同的正则化因子，与最先进的方法相比，其有效性在流行的CNN上得到了验证。
基于动量的随机梯度方法，如重球（HB）和Nesterov的加速梯度下降（NAG）方法在实践中被广泛用于训练深度网络和其他监督学习模型，因为它们通常比随机梯度下降（SGD）有明显的改进。严格地说，快速梯度方法只有在梯度精确的确定性情况下才有比梯度下降可证明的改进。 在随机情况下，对其广泛适用性的流行解释是，当这些快速梯度方法应用于随机情况时，它们部分地模仿了精确梯度的对应方法，导致了一些实际收益。这项工作通过证明存在一些简单的问题实例，这些方法不能超越SGD，尽管其参数的设置是最好的。 这些负面的问题实例在非正式意义上是通用的；它们看起来并不像精心构建的病态实例。这些结果表明（连同经验证据），HB或NAG的实际性能提升是minibatching的副产品。此外，这项工作提供了一个可行的（和可证明的）替代方案，在相同的问题实例集上，显著改善HB、NAG和SGD的性能。 这种算法被称为加速随机梯度下降（ASGD），是一种易于实现的随机算法，基于Nesterov加速法的一个相对不太流行的变体。本文中广泛的经验结果表明，ASGD比HB、NAG和SGD有性能上的提高。实现ASGD算法的代码可以在https://github.com/rahulkidambi/AccSGD。
超限规划（OSP）是一个寻找计划的问题，该计划在保持在指定的成本约束范围内的情况下使其最终状态的效用值最大化。最近，有研究表明OSP问题可以被重新表述为具有多个成本函数但没有效用的经典规划问题。 在这里，我们利用这种重述的优势，表明OSP问题可以用A*搜索算法进行优化求解，与之前使用分支和边界搜索的方法不同。这使得许多为经典规划开发的强大技术可以应用于OSP问题。 我们还引入了新的边界敏感启发式方法，这些方法能够推理出解决方案的主要成本，同时考虑到次要成本函数和边界，与不考虑这些边界的启发式方法相比，能够提供卓越的指导。我们实现了现有经典规划启发式方法的两个这样的边界敏感变体，并通过实验表明，所产生的搜索明显比不考虑边界的可比启发式方法更明智。
以前关于对抗性稳健神经网络的工作需要大量的训练集和计算上昂贵的训练程序。 另一方面，少量的学习方法非常容易受到对抗性例子的影响。 我们工作的目标是产生既能很好地完成少量任务又能对对抗性例子具有鲁棒性的网络。 我们为元学习调整了对抗性训练，我们为元学习的小型网络调整了稳健的结构特征，我们测试了预处理防御作为元学习对抗性训练的替代方案，我们还研究了稳健的元学习相对于稳健的转移学习在几率任务方面的优势。 这项工作对元学习背景下的对抗性鲁棒方法进行了彻底的分析，我们为未来关于少数任务的防御工作奠定了基础。
我们关于神经网络如何运作的许多核心假设仍然没有得到经验上的检验。一个常见的假设是，卷积神经网络需要对小的平移和变形稳定，以解决图像识别任务。多年来，这种稳定性通过加入交错池层被烘托到CNN架构中。然而，最近，交错池在很大程度上被放弃了。我们对变形稳定性的直觉到底对不对？它重要吗？池化对变形不变性是必要的吗？如果不是，在没有池化的情况下，如何实现变形不变性？在这项工作中，我们严格测试了这些问题，并发现卷积网络中的变形稳定性比它最初看起来更细微。(1) 变形不变性不是一个二元属性，而是不同的任务在不同的层需要不同程度的变形稳定性。(2) 变形稳定性不是一个网络的固定属性，在训练过程中，主要通过卷积滤波器的平滑性进行大量的调整。(3) 交错池层对于实现自然图像分类的最佳变形稳定性形式既不需要也不充分。 (4) 池化在初始化时为图像分类赋予了变形稳定性，在训练过程中，网络必须学会反击这种归纳偏见。这些发现共同为交错池化和变形不变性在CNN中的作用提供了新的见解，并证明了对我们关于神经网络工作的最基本假设进行严格的经验测试的重要性。
深度神经网络（DNN）已被证明在用噪声标签训练足够长的时间时，会过度拟合数据集。为了克服这个问题，我们提出了一个简单有效的方法--自集合标签过滤（SELF），在训练中逐步过滤掉错误的标签。我们的方法通过逐步只允许来自潜在的非噪声（干净）标签的监督，并停止对过滤的噪声标签的学习，来提高任务性能。 我们表明，在整个训练过程中，这些集合估计比网络在最近训练历时的单一估计更准确地识别不一致的预测。虽然过滤的样本被完全从监督训练损失中删除，但我们在无监督损失中通过半监督学习动态地利用它们。 我们证明了这种方法在对称和非对称标签噪声和不同的噪声比率下对各种图像分类任务的积极影响。它在不同的数据集上大大超过了所有以前的噪声感知学习的工作，并且可以应用于广泛的网络架构。
深度神经网络的漫长训练时间是机器学习研究中的一个瓶颈。快速训练的主要障碍是密集层和卷积层的内存和计算需求相对于其信息带宽的二次增长。 最近，训练 "先验 "稀疏网络被作为一种方法提出来，允许层保留高信息带宽，同时保持低内存和计算量。 首先，我们推导出一种新的稀疏神经网络初始化方案，使我们能够探索非常深的稀疏网络空间。接下来，我们评估了几种拓扑结构，并表明看似相似的拓扑结构在可实现的准确性方面往往会有很大的差异。为了解释这些差异，我们开发了一种无数据启发式方法，可以独立于网络将被训练的数据集来评估一种拓扑结构。然后我们推导出一组构成良好拓扑结构的要求，并得出一个满足所有这些要求的单一拓扑结构。
深度学习模型需要广泛的架构设计探索和超参数优化，以便在给定的任务上表现良好。模型设计空间的探索通常是由人类专家进行的，并使用网格搜索和搜索启发式方法的组合在大量可能的选择空间上进行优化。神经架构搜索（NAS）是一种强化学习方法，已被提出用于自动化架构设计。 然而，NAS需要取样、构建和训练成百上千的模型来实现性能良好的架构，这个过程需要为每个新任务从头开始执行。 在本文中，我们提出了多任务神经模型搜索（MNMS）控制器。我们的目标是学习一个可归纳的框架，该框架可以以先前看到的任务的成功模型搜索为条件构建模型，从而大大加快新任务的搜索速度。我们证明MNMS可以同时为多个任务进行自动架构搜索，同时还可以为每个任务学习表现良好的专门模型。 然后我们表明，预先训练好的MNMS控制器可以将学习转移到新的任务上。通过利用以前的搜索知识，我们发现预先训练好的MNMS模型从搜索空间的一个更好的位置开始，减少了对未见过的任务的搜索时间，同时仍然发现了优于人类设计的公开模型的模型。
这项工作通过利用深度生成架构来学习观察序列的线性高斯模型，研究了非线性视觉过程的建模问题。我们提出了一个联合学习框架，结合了多变量自回归模型和深度卷积生成网络。 在对内化的理论假设进行论证后，我们提出了一个架构，允许变异自动编码器和生成对抗网络同时学习非线性观察以及来自观察帧序列的线性状态转换模型。最后，我们在概念性的玩具例子和动态纹理上展示了我们的方法。
偏微分方程（PDEs）在许多学科中发挥着突出的作用，如应用数学、物理学、化学、材料科学、计算机科学等，PDEs通常是基于物理规律或经验观察而推导出来的。 受深度学习中神经网络设计的最新发展启发，我们提出了一种新的前馈深度网络，称为PDE-Net，以同时实现两个目标：准确预测复杂系统的动力学和揭示隐藏的PDE模型。所提出的PDE-Net的基本思想是通过学习卷积核（滤波器）来学习微分算子，并应用神经网络或其他机器学习方法来近似未知的非线性响应。 与现有的方法相比，我们的方法要么假定非线性响应的形式是已知的，要么固定微分算子的某些有限差分近似值，我们的方法通过学习微分算子和非线性响应而具有最大的灵活性。拟议的PDE-Net的一个特点是所有的滤波器都有适当的约束，这使我们能够轻松地识别治理PDE模型，同时仍然保持网络的表达和预测能力。 这些约束是通过充分利用微分算子的阶数与滤波器的和规则的阶数之间的关系（一个源于小波理论的重要概念）而精心设计的。我们还讨论了PDE-Net与计算机视觉中一些现有网络的关系，如网络中的网络（NIN）和剩余神经网络（ResNet）。
变量自动编码器（VAE）的每个训练步骤都需要我们从近似后验中取样，因此我们通常选择简单的（如因子化的）近似后验，其中取样是一种有效的计算，可以充分地利用GPU的并行性。 然而，这种简单的近似后验往往是不够的，因为它们消除了后验中的统计依赖性。 虽然有可能对连续潜质使用归一化流近似后验，但对离散潜质却没有类似的方法。对离散依赖关系建模的最自然方法是自回归分布，但从这种分布中取样本身就是顺序的，因此速度很慢。 我们开发了一种基于定点迭代的自回归分布的快速并行抽样程序，可以在离散状态空间模型中进行高效准确的变分推理。 为了优化变异约束，我们考虑了两种评估概率的方法：将松弛的样本直接插入离散分布的pmf中，或者转换为连续逻辑潜变量，并将K步定点迭代解释为归一化流。 我们发现，转换为连续潜变量给真实后验和近似后验之间的不匹配提供了相当大的额外空间，这导致了有偏见的推断，因此我们采用了前一种方法。 我们在神经科学问题上测试了我们的方法，即从嘈杂的钙成像数据中推断出离散的尖峰活动，发现它能在一个数量级的时间内得到准确的连接性估计。
深度神经网络（DNN）在NLP任务上取得了巨大的成功，如语言建模、机器翻译和某些问题回答（QA）任务。然而，在更多的知识密集型任务上，如来自大语料库的QA，成功是有限的。现有的端到端深度QA模型（Miller等人，2016；Weston等人，2014）需要在观察问题后阅读整个文本，因此他们回答问题的复杂性与文本大小成线性关系。我们建议通过使用符号意义表示来解决这个可扩展性问题，这些符号意义表示可以被索引并有效检索，其复杂性与文本大小无关。 更具体地说，我们使用序列到序列的模型对知识进行符号化编码，并从编码的知识中生成程序来回答问题。我们将我们的方法，称为N-Gram Machine（NGM），应用于bAbI任务（Weston等人。我们的实验表明，N-Gram Machine（NGM）可以准确有效地解决这两个任务。与完全可分的记忆模型不同，NGM的时间复杂性和回答质量不受故事长度的影响。 NGM的整个系统是用REINFORCE（Williams，1992）进行端到端训练的。为了避免离散潜变量模型中典型的梯度估计的高变异，我们使用波束搜索而不是采样。为了解决指数级大的搜索空间，我们使用稳定的自动编码目标和结构调整程序来反复减少和完善搜索空间。
我们建议使用一个元学习目标，使修改后的分布上的转移速度最大化，以学习如何将获得的知识模块化。特别是，我们专注于如何将一个联合分布纳入适当的条件，与因果方向一致。 我们解释了这在什么情况下能起作用，使用的假设是分布中的变化是局部的（例如，对其中一个边际的变化，例如由于对其中一个变量的干预）。我们证明，在这种因果机制局部变化的假设下，正确的因果图将倾向于只有少数参数具有非零梯度，即。 我们证明，在这种因果机制局部变化的假设下，正确的因果图往往只有少数参数的梯度不为零，即需要调整的参数（那些被修改的变量）。我们论证并在实验中观察到，这将导致更快的适应，并利用这一特性定义一个元学习代用分数，除了图的连续参数化之外，这将有利于正确的因果图。 最后，在人工智能代理观点的激励下（如机器人自主发现其环境），我们考虑同一目标如何发现因果变量本身，作为没有因果意义的观察到的低级变量的转化。在双变量情况下的实验验证了所提出的想法和理论结果。
持续学习是人工智能的一个长期目标，但经常受到灾难性遗忘的困扰，这种遗忘使神经网络无法按顺序学习任务。以前的持续学习方法已经证明了如何缓解灾难性遗忘，并在学习新任务的同时保留以前任务的性能。 我们从分类器可能性变化的角度分析了灾难性遗忘，并提出了一个简单的L1最小化标准，该标准可以适应不同的使用情况。我们进一步研究了通过该标准量化的两种最小化遗忘的方式，并提出了实现对遗忘更精细控制的策略。最后，我们在3个不同难度的数据集上评估了我们的策略，并证明了比以前已知的缓解灾难性遗忘的L2策略的改进。
我们提出了一种构建逼真的三维面部可变形模型（3DMM）的方法，允许直观的面部属性编辑工作流程。目前使用3DMM的面部建模方法存在缺乏局部控制的问题。 我们的基于局部PCA的方法使用一种新的方法从局部3DMM中选择最佳的特征向量，以确保组合的3DMM具有表现力，同时允许精确的重建。 在一大批可能的人体测量中，我们筛选出那些在脸部数据集上具有有意义的生成能力的测量值。我们通过从我们的面部扫描数据集中提取的映射矩阵，将这些测量值与基于部位的3DMM结合起来。 我们的基于部位的3DMM是紧凑而准确的，与其他3DMM方法相比，它在局部和全局控制之间提供了一个新的权衡。我们在用于推导3DMM的135张扫描的数据集上测试了我们的方法，另外还有19张用于验证的扫描。
我们回顾了八种机器学习分类算法，以分析脑电图（EEG）信号，从而区分与五个基本教育任务相关的EEG模式。 以前的EEG实验在同一实验中使用了几个分类器，或在不同实验的数据集上审查了不同的算法，而我们的方法侧重于在同一数据集上审查八个分类器类别，包括线性分类器、非线性贝叶斯分类器、近邻分类器、集合方法、自适应分类器、张量分类器、转移学习和深度学习。此外，我们打算找到一种可以在当前主流个人电脑和智能手机上顺利运行的方法。 实证评估表明，随机森林和LSTM（长短期记忆）的表现优于其他方法。我们使用了一个数据集，其中用户正在进行五个经常进行的学习相关的任务，包括阅读、写作和打字。结果显示，这两个最好的算法可以正确分类不同的用户，准确率提高了5%至9%，独立使用每个任务。 在每个主题内，与其他方法相比，任务可以被识别，准确率增加4%到7%。这项工作表明，随机森林可以成为目前主流硬件的推荐方法（快速和准确），而当主流计算机和智能手机可以在更短的时间内处理更多数据时，LSTM有可能成为首选方法。
多代理强化学习提供了一种方法来研究在需要解决特定问题的代理社区中如何出现通信。在本文中，我们研究了谈判环境中通信的出现，这是一个代理互动的半合作模型。我们引入了两个通信协议--一个基于游戏的语义，另一个则是先验的非基础的。 我们表明，自利的代理人可以使用先验的沟通渠道来进行公平的谈判，但无法有效地使用无基础的、廉价的谈话渠道来做同样的事情。 然而，亲社会的代理人确实学会了使用廉价谈话来寻找最佳的谈判策略，这表明合作对于语言的出现是必要的。我们还研究了一个代理人与社区中具有不同亲社会程度的代理人互动的环境中的交流行为，并展示了代理人的可识别性如何帮助谈判。
最近引入的元学习方法通过在大量的多类分类任务中学习通用分类器并将模型推广到新的任务中来解决这个问题。然而，即使有这样的元学习，新分类任务中的低数据问题仍然存在。 在本文中，我们提出了Transductive Propagation Network（TPN），这是一个新型的元学习框架，用于一次性对整个测试集进行分类，以缓解低数据问题。具体来说，我们建议通过学习利用数据中的流形结构的图构建模块，来学习将标签从有标签的实例传播到无标签的测试实例。 我们在多个基准数据集上验证了TPN，它在很大程度上超过了现有的几次学习方法，达到了最先进的效果。
我们描述了使用自动调度系统进行观测策略设计，并对NASA（美国国家航空和航天局）空间站ECOSystem星载热辐射仪实验（ECOSTRESS）的操作进行调度。 我们描述了压缩大型活动调度器和计划器（CLASP）调度系统对ECOSTRESS调度问题的适应性，强调了自动调度的多个用例和调度技术的几个挑战：处理信息不断变化的长期活动、大容量存储单元环形缓冲器操作的挑战和轨道的不确定性。所述调度系统自ECOSTRESS仪器的名义操作开始于2018年7月，预计将运行到2019年夏季任务结束。
对抗性例子是保留原始图像结构但偏离分类器的修改过的样本。研究者们努力开发生成对抗性例子的方法，并找出其起源。过去的研究把大部分注意力放在这些方法引起的决策边界变化上。本文则从更底层的知识表示角度讨论对抗性例子的起源。 人类可以学习和分类原型以及物体的变换。而神经网络以一种更混合的方式存储所学的知识，将所有的原型和变换作为一个整体的分布。混合存储可能导致不同类别之间的距离较低，因此小的修改会误导分类器。 一个一步到位的分布模仿方法被设计为模仿最近的不同类别邻居的分布。实验表明，仅仅通过模仿训练集的分布，而不需要分类器的任何知识，仍然可以导致对深度网络的分类结果的明显影响。这也意味着，对抗性例子的形式可能比小扰动更多。 从表征的角度讨论了缓解对抗性例子的潜在方法。第一条路径是改变发送到训练步骤的数据的编码。更加原型化的训练数据可以帮助抓住更强大和准确的结构知识。第二条路径需要构建具有改进表征的学习框架。
与流行的深度Q网络（DQN）学习不同，交替Q学习（AltQ）在每次迭代时并不完全适合目标Q函数，而且通常被认为是不稳定和低效的。AltQ的有限应用大多依赖于大幅改变算法架构以提高其性能。 更具体地说，我们在一批Atari 2600游戏上测试了所提出的算法，并表现出比DQN学习方法更优越的性能。在线性函数近似下，所提出的算法的略微修改版本的收敛率被描述出来。
为了寻找更准确的预测模型，我们为学习诊断问题定制了胶囊网络。我们还提出了光谱胶囊网络，这是胶囊网络的一个新的变体，它比带有EM路由的胶囊网络收敛得更快。光谱胶囊网络由空间重合滤波器组成，它根据一维线性子空间上提取的特征的排列来检测实体。在一个公共基准学习诊断数据集上的实验不仅显示了胶囊网络在这个任务上的成功，而且还证实光谱胶囊网络的收敛更快。
机器学习应用中的一大挑战是训练数据可能与算法所面临的真实世界的数据不同。在语言建模中，用户的语言（如私人信息）可能在一年内发生变化，与我们在公开数据中观察到的完全不同。同时，公共数据可用于获得一般知识（即英语的一般模型）。我们研究了在用户私人数据上对一般模型进行分布式微调的方法，其额外要求是保持一般数据的质量和通信成本最小化。 我们提出了一种新的技术，与一般模型相比，它能显著提高对用户语言的预测质量，并在通信效率方面优于梯度压缩方法。所提出的程序是快速的，并导致在非正式英语文本上的困惑减少近70%，击键节省率提高8.7个百分点。最后，我们提出了一个实验框架，用于评估语言模型分布式训练的差异隐私，并表明我们的方法具有良好的隐私保证。
我们提出近似贝叶斯算法应该优化一个新的标准，直接从损失中得出，以计算它们的近似后验，我们称之为伪后验。与标准的变异推理不同，它优化了对数边际似然的下限，新的算法可以被分析为用伪后验对预测提供损失保证。我们的标准可以被用来导出新的稀疏高斯过程算法，其误差保证适用于各种似然。
在本文中，我们提出了一种新的正则化方法，RotationOut，用于神经网络。与独立处理每个神经元/通道的Dropout不同，RotationOut将其输入层视为整个向量，并通过随机旋转向量引入正则化。RotationOut也可以在卷积层和递归层中使用，只需稍作修改。我们进一步使用噪声分析方法来解释RotationOut和Dropout在共同适应性减少方面的区别。利用这种方法，我们还展示了如何将RotationOut/Dropout与Batch Normalization一起使用。我们在视觉和语言任务中进行了大量的实验，以显示所提出的方法的有效性。将提供代码。
在概率推理的框架内制定强化学习（RL）问题，不仅提供了一个关于RL的新视角，而且还产生了更稳健和更容易训练的实用算法。虽然RL和概率推理之间的这种联系在单代理设置中得到了广泛的研究，但在多代理设置中还没有被充分理解。 在本文中，我们将多代理强化学习的问题看作是在一个特定的图形模型中进行推理的问题。我们使用单独但相关的马尔科夫决策过程对环境进行建模，正如每个代理所看到的。 我们推导出一种实用的非政策性最大熵行为人批判算法，我们称之为多行为人软行为人批判（MA-SAC），用于在拟议的模型中使用变异推理进行近似推理。 通过实验，我们证明了MA-SAC在几个多Agent场景中的表现优于一个强大的基线。虽然MA-SAC是一个可以从拟议的概率框架中得出的多Agent RL算法，但我们的工作提供了一个多Agent环境中最大熵算法的统一观点。
在这项工作中，我们提出了NeuralSort，这是一个通用的连续松弛，将排序算子的输出从包络矩阵变成了单模行-ochastic矩阵集，其中每一行的和为1，并且有一个明显的argmax。 此外，我们利用这种松弛，通过推导出Plackett-Luce系列分布在排列组合上的重新参数化梯度估计器，使基于梯度的随机优化在排列组合的大空间中得以实现。我们在三个需要学习高维对象语义排序的任务上证明了我们框架的有用性，包括K-近邻算法的完全可微分、参数化扩展
我们提出了一种新的转移学习方法，以便在贝叶斯优化的既定框架内获得定制的优化器，使我们的算法能够利用高斯过程的公认的泛化能力。 利用强化学习对一组相关任务的获取函数（AF）进行元训练，所提出的方法学会提取隐含的结构信息，并利用它来提高数据效率。我们在一个模拟到真实的转移任务以及几个模拟函数和两个超参数搜索问题上进行了实验。 结果表明，我们的算法（1）从可用的源任务或模拟中自动识别目标函数的结构属性，（2）在源数据稀少和丰富的情况下表现良好，（3）如果没有结构，则会回落到一般自动对焦的性能水平。
我们研究了深度神经网络（DNN）训练过程中内部表征的演变，旨在揭开信息瓶颈理论中压缩方面的神秘面纱。该理论认为，DNN训练包括一个快速拟合阶段和一个较慢的压缩阶段，其中输入X和内部表征T之间的相互信息I（X；T）减少。 有几篇论文观察到不同DNN模型上估计的相互信息的压缩，但在这些网络上真正的I(X;T)可以证明是恒定的（离散X）或无限的（连续X）.这项工作解释了理论和实验之间的差异，并澄清了过去这些工作实际测量的内容。 这个嘈杂的框架被证明是原始（确定性）DNN在性能和学习表征方面的一个很好的代理。然后，我们为嘈杂的DNN中的I(X;T)开发了一个严格的估计器，并观察各种模型中的压缩。 最后，我们回到过去工作中采用的I(X;T)估计器，并证明虽然它不能捕捉到真正的（空洞的）相互信息，但它确实可以作为聚类的一个措施。
在这项工作中，我们研究了如何利用策略正则化促进代理间的协调，并讨论了两种可能的途径，分别是基于代理间建模和同步子策略选择。我们在四个具有稀疏奖励的挑战性连续控制任务中测试了每种方法，并将它们与包括MADDPG（一种最先进的多代理强化学习算法）在内的三种基线进行比较。 为了确保公平的比较，我们依靠彻底的超参数选择和训练方法，允许每种算法和环境有固定的超参数搜索预算。我们因此评估了每种学习方法的超参数敏感性、样本效率和渐进性能。
多模态情感分析是一个核心研究领域，研究从语言、视觉和声音模态表达的说话人的情感。多模态学习的核心挑战涉及推断联合表征，可以处理和联系来自这些模态的信息。然而，现有的工作使用多种模态作为输入来学习联合表征，在测试时可能对噪声或缺失的模态很敏感。 随着最近机器翻译中序列到序列模型的成功，有机会探索学习联合表征的新方法，在测试时可能不需要所有的输入模态。 我们的方法基于这样一个重要的观点：从源模式到目标模式的翻译提供了一种只使用源模式作为输入来学习联合表征的方法。我们用循环一致性损失来增强模式翻译，以确保我们的联合表征保留所有模式的最大信息。 一旦我们的翻译模型用成对的多模态数据进行了训练，我们在测试时只需要来自源模态的数据进行预测。这确保了我们的模型对扰动或丢失的目标模态保持稳健。额外的实验表明，我们的模型在更多的输入模态下学习了越来越多的鉴别性联合表征，同时保持了对所有其他模态的扰动的鲁棒性。
损失面的几何属性，如解决方案的局部平坦性，与深度学习中的泛化有关。我们研究了在经验数据集上评估的神经网络Hessian的特征值，即经验Hessian，与数据生成分布下的Hessian的特征值之间的差异，我们称之为真实Hessian。 在温和的假设下，我们使用随机矩阵理论来证明真实黑森的特征值比经验黑森的绝对值要小。我们在110层ResNet和VGG-16上对不同的SGD计划支持这些结果。为了进行这些实验，我们提出了一个光谱可视化的框架，基于GPU加速的随机兰佐斯正交。
将长序列总结成一个简洁的语句是自然语言处理中的一个核心问题，需要对输入进行非微观的理解。基于图神经网络在高度结构化数据上的有希望的结果，我们开发了一个框架来扩展现有的序列编码器，其中有一个图组件，可以推理弱结构化数据（如文本）中的长距离关系。在一个广泛的评估中，我们表明，所产生的混合序列-图模型在一系列总结任务中都优于纯序列模型和纯图模型。
在概率分类中，基于高斯混合物的判别模型表现出灵活的拟合能力。然而，很难确定成分的数量。我们提出了一种基于判别高斯混合物模型（GMM）的稀疏分类器，它被称为稀疏判别高斯混合物（SDGM）。 这种学习算法通过获得稀疏解来提高泛化能力，并通过去除冗余成分来自动确定成分的数量。SDGM可以嵌入到神经网络（NN）中，如卷积NN，并可以以端到端方式进行训练。实验结果表明，所提出的方法通过获得稀疏性来防止过度拟合。此外，我们证明，在某些情况下，当它被用作深度NN的最后一层，所提出的方法优于具有softmax函数的全连接层。
我们最近观察到，使用现成的预计算的格拉斯曼尼子空间打包编码簿初始化的卷积过滤器在许多数据集上的表现令人惊讶。通过这篇短文，我们想传播这方面的一些初步结果，希望能激发深度学习社区的好奇心，将经典的格拉斯曼尼子空间打包结果作为更有效的初始化策略的新思路来源。
在新的、未见过的环境中，领域适应是成功的关键。应用于特征空间的对抗性适应模型发现了领域不变的表征，但很难可视化，有时无法捕捉到像素级和低级领域的转变。 最近的工作表明，生成式对抗网络与周期一致性约束相结合，在领域之间的图像映射方面令人惊讶地有效，即使不使用对齐的图像对。我们提出了一个新的鉴别性训练的周期一致性对抗领域适应模型。CyCADA在像素级和特征级适应表征，在利用任务损失的同时执行周期一致性，并且不需要对齐的对。 我们的模型可以应用于各种视觉识别和预测环境。我们展示了跨越多个适应性任务的新的最先进的结果，包括数字分类和道路场景的语义分割，证明了从合成领域到现实世界的转移。
词根化是一个去除词缀（即前缀、后缀和后缀）的过程，可以提高信息检索系统的准确性和性能。本文介绍了将阿姆哈拉语单词还原为相应的词根，目的是保留语义信息。 本文提出了将阿姆哈拉语单词还原成相应的词干的方法，目的是保留语义信息。本文提出的方法可以有效地从阿姆哈拉语单词中去除词缀，从单词中去除这些词缀（前缀、下缀和后缀）的过程被称为干化。 在本文中，我们设计了一个轻量级的阿姆哈拉语词干程序，它依靠的规则是接收一个阿姆哈拉语单词，然后找到一个与可能的前缀相匹配的词头和与可能的后缀相匹配的词尾，最后检查它是否有后缀。如果有任何前缀、后缀或/和后缀，最终结果就是词干，否则它仍然是早期状态之一。 该技术不依赖任何额外的资源（如字典）来验证生成的词干。使用手工注释的阿姆哈拉语单词来评估生成的词干的性能。
众所周知，地点细胞和网格细胞有助于动物和人类的导航。与概念细胞一起，它们允许人类形成外部世界的内部表征，即概念空间。我们通过绘制隐藏层神经元的激活曲线来研究深度神经网络中这种空间的存在。 虽然发现了类似于位置单元和概念单元的属性，但却没有类似于网格单元的发射模式，从而表明在训练的网络中缺乏路径整合或特征转换功能。总之，我们提出了当前深度学习实践中的一个合理的不足之处，限制了深度网络执行类比推理和记忆检索的任务。
我们从机器学习的角度出发，对Fristo（2010）提出的主动推理框架进行了全面的描述。从生物的灵感和自动编码的原则出发，提出了一个认知架构的草图，它应该提供实现面向估计的控制策略的方法。 虽然优化行动集的未来后验熵已被证明足以达到局部最优行动选择，但使用特定类别的突出性地图的离线计算被证明是更好的，因为它通过囊状路径预处理节省了处理成本，对识别/压缩率的影响可忽略不计。
图具有奇特的特征，如大小不一和没有自然的节点排序，这使得它们难以分析和比较。虽然最先进的方法通过强大的图神经网络来提高表达能力，但我们建议利用图的自然光谱特性来研究一个简单的图特征：图拉普拉斯谱（GLS）。 我们分析了这个同时满足同构不变量、表现力和变形一致性的对象的表现力。特别是，我们提出了一个基于图形扰动的理论分析，以了解我们在比较GLS时对图形进行了什么样的比较。 为此，我们推导出GLS之间的距离界限，它与同构分歧有关，是一种标准的计算昂贵的图分歧。最后，我们通过一致性测试和分类任务实验GLS作为图表示，并表明它是一个强大的图特征表示基线。
对抗性训练是一种学习鲁棒性深度网络的方法，通常被认为比传统的训练更昂贵，因为必须通过投影梯度衰减（PGD）这样的一阶方法来构建对抗性例子。 在本文中，我们有一个令人惊讶的发现，即可以使用一个更弱、更便宜的对手来训练经验上的鲁棒模型，这种方法以前被认为是无效的，使得该方法在实践中并不比标准训练更昂贵。 具体来说，我们表明用快速梯度符号法（FGSM）进行对抗性训练，当与随机初始化相结合时，与基于PGD的训练一样有效，但成本明显降低。 此外，我们表明FGSM对抗训练可以通过使用标准技术进一步加速深度网络的有效训练，使我们能够在6分钟内学习一个鲁棒的CIFAR10分类器，在epsilon=8/255时具有45%的鲁棒准确性，在12小时内学习一个鲁棒的ImageNet分类器，在epsilon=2/255时具有43%的鲁棒准确性，而过去基于 "自由 "对抗训练的工作需要10和50小时才能达到相同的阈值。
在寻求稀疏和高效的神经网络模型方面，以前的许多工作研究了在训练过程中强制执行L1或L0正则器以鼓励权重稀疏性。L0正则器直接测量参数稀疏性，并对参数值的缩放不产生影响，但它不能提供有用的梯度，因此需要复杂的优化技术。 受传统压缩传感问题中使用的Hoyer度量（L1和L2规范之间的比率）的启发，我们提出了DeepHoyer，这是一套几乎无处不在的可微化和尺度不变量的疏散性诱导正则。
自我监督，即在没有外部监督的情况下改进目标任务，主要是在假设有额外数据的情况下进行探索。然而，在许多情况下，特别是在医疗保健方面，人们可能无法获得额外的数据（标记或其他）。我们将这种方法称为有限的自我监督，因为我们只限于手头的数据。我们在三个序列级分类任务上证明了有限的自我监督的效用，其中两个与真实的临床数据有关，一个使用合成数据。在这个框架内，我们引入了新的自我监督形式，并证明了它们在提高目标任务性能方面的效用。 我们的结果表明，在一系列领域中，有限的自我监督导致了比监督基线的一致改进。特别是，对于从少量心电图数据中识别心房颤动的任务，我们观察到相对于基线（AUC-ROC=0.55 vs. AUC-ROC=0.62），接收器操作特征曲线下的面积提高了近13%。有限的自我监督应用于连续数据可以帮助学习中间表示，使其特别适用于数据收集困难的环境。
神经网络是否偏向于简单的功能？深度是否总是有助于学习更复杂的特征？训练网络的最后一层是否和训练所有的层一样好？这些问题从表面上看似乎不相关，但在这项工作中，我们从光谱的角度给所有这些问题一个共同的处理。 我们将研究*共轭核，CK，*（也称为*神经网络-高斯过程核*）和*神经切线核，NTK*的频谱。大致上，CK和NTK分别告诉我们 "网络在初始化时是什么样子 "和 "网络在训练期间和之后是什么样子。 "它们的谱系就编码了关于神经网络的初始分布以及训练和泛化特性的宝贵信息。通过分析特征值，我们对一开始提出的问题提出了新的见解，我们通过对神经网络的广泛实验来验证这些见解。 我们相信，我们在这里开发的用于分析CK和NTK光谱的计算工具可以作为未来研究深度神经网络的坚实基础。我们已经在github.com/jxVmnLgedVwv6mNcGCBy/NNspectra上公开了它的代码和本文中生成的图表。
为了交流，为了假设的基础，为了分析数据，神经科学家经常提到大脑的划分。这里我们考虑在研究大脑功能时用于划分大脑的图谱。我们从概念的角度以及通过对流行的大脑功能划分运行各种分析任务来讨论这些划分的意义和有效性。
高维稀疏奖励任务对强化学习代理提出了重大挑战。 在这项工作中，我们使用模仿学习来解决其中的两个挑战：如何从像素中学习世界的有用表征，以及如何在奖励信号稀少的情况下进行有效探索？我们表明，即使在这种高维观察空间中，对抗性模仿也能很好地工作。 我们的方法消除了大多数当代模仿方法中存在的限制：不需要演示者的行动（只有视频），没有特殊的初始条件或热启动，也没有对任何单一演示的明确跟踪。所提出的代理可以解决一个具有挑战性的机器人操纵任务，即只通过视频演示和稀疏的奖励进行块状堆叠，在这种情况下，非模仿代理不能完全学习。 此外，我们的代理比依赖手工制作的、分阶段的密集奖励函数的竞争性方法学习得更快，与标准的GAIL基线相比也更好。最后，我们开发了一个新的对抗性目标识别器，在某些情况下允许代理在没有任何任务奖励的情况下学习堆叠，纯粹来自模仿。
在本文中，我们展示了轻松识别用生成式对抗网络框架生成的假样本的策略。一个策略是基于原始像素值和从中提取的特征的统计分析和比较。另一个策略是从真实数据中学习形式上的规范，并表明假样本违反了真实数据的规范。我们表明用GANs生成的假样本有一个通用的签名，可以用来识别假样本。我们在MNIST、CIFAR10、音乐和语音数据上提供结果。
为降低深度学习训练中计算的数值精度所做的努力产生了积极量化权重和激活的系统，但在内积运算中对部分和采用了广泛的高精度累加器，以保持收敛的质量。由于缺乏任何框架来分析部分和累加的精度要求，导致了保守的设计选择。这对降低乘法累加单元的复杂性施加了上限。 我们提出了一种统计方法来分析降低积累精度对深度学习训练的影响。观察到积累精度的错误选择会导致信息的损失，表现为部分和的集合中方差的减少，我们推导出一组方程，将这种方差与积累的长度和积累所需的最小比特数联系起来。我们将分析应用于三个基准网络。在每一种情况下，按照我们提出的方程设置积累精度，网络都成功地收敛到单精度浮点基线。我们还表明，降低积累精度会进一步降低训练网络的质量，证明我们的方程产生了严格的界限。
无监督领域适应是一个有前途的途径，可以提高目标领域的深度神经网络的性能，只使用来自源域的标签。然而，当源域和目标域不共享一个共同的标签空间时，两个主要的方法，域差异减少学习和半监督学习，并不容易适用。 本文通过学习一个在（有标签的）源域和（无标签的）目标域上都能保持判别能力的表示空间，同时保持两个域的表示良好分离，来解决上述情况。在理论分析的启发下，我们首先将源域和目标域对应于不重叠的类标签的不相干分类任务重新表述为验证任务。 为了处理域内和跨域验证，我们提出了一个特征转移网络（FTN），将目标特征空间与原始源空间分离，同时与转换后的源空间对齐。此外，我们提出了一个非参数性的多类熵最小化损失，以进一步提高FTN在目标域的判别能力。 在实验中，我们首先说明了FTN是如何在从MNIST-M到MNIST的受控环境中工作的，这两个领域之间的数字类别是不相干的，然后通过在跨民族的人脸识别问题上的最先进的表现来证明FTN的有效性。
在本文中，我们考虑了训练神经网络（NN）的问题。为了促进具有特定结构的NN，我们明确考虑了非光滑正则化（如L1-norm）和约束（如区间约束）。这被表述为一个约束的非光滑非凸优化问题，我们提出了一个收敛的近似型随机梯度下降（Prox-SGD）算法。 我们表明，在适当选择的学习率下，动量最终类似于未知的真实梯度，因此在分析收敛性时至关重要。我们建立了概率为1的，由所提出的Prox-SGD生成的序列的每个极限点都是静止点。然后，Prox-SGD被定制用于训练稀疏神经网络和二元神经网络，理论分析也得到了大量数字测试的支持。
大脑中几个神经元的损失很少会导致任何可见的功能损失。然而，对 "少数 "在这种情况下的含义的洞察力并不清楚。要有多少个随机的神经元故障才会导致可见的功能损失？ 在本文中，我们解决了随机神经元子集的崩溃对神经网络的整体计算和它产生的输出错误的影响这一基本问题。我们研究了神经网络的容错性，该网络在概率环境下受到小的随机神经元/权重崩溃的影响。 我们的主要贡献是通过使用连续极限中的泰勒扩展，证明了在小的随机伯努利崩溃下网络输出的误差，其中一层的相邻神经元是相似的。我们在模型中采用的故障模式是神经形态硬件的特征，这是一种有希望加速人工神经网络的技术，也是生物网络的特征。 我们表明，我们的理论界限可以用来比较不同架构的容错性，并设计一个正则器来提高一个给定架构的容错性。我们设计了一个使用合理数量的神经元实现容错的算法。除了理论证明，我们还对我们的结果进行了实验验证，并提出与泛化能力问题的联系。
真正的智能代理需要捕捉他们所有感官的相互作用，以建立对其世界的丰富的物理理解。在机器人学中，我们已经看到了在使用视觉和触觉感知方面的巨大进步；然而，我们往往忽略了一个关键的感官：声音。这主要是由于缺乏捕捉行动和声音的相互作用的数据。在这项工作中，我们对声音和机器人行动之间的相互作用进行了首次大规模研究。 为此，我们创建了最大的声音-行动-视觉数据集，使用我们的机器人平台Tilt-Bot对60个物体进行了15000次互动。通过倾斜物体，让它们撞向机器人托盘的墙壁，我们收集了丰富的四通道音频信息。其次，声音也包含了关于行动的因果关系的信息，也就是说，鉴于所产生的声音，我们可以预测在物体上应用了什么行动。最后，从音频嵌入中得到的物体表征表明了隐含的物理属性。
分层标签结构广泛存在于许多机器学习任务中，从有明确标签层次的任务，如图像分类，到有潜在标签层次的任务，如语义分割。不幸的是，最先进的方法经常利用交叉熵损失，它明确地假设了类标签之间的独立性。 在HCOT中，除了最大化基础真理类的概率外，我们还以分层的方式中和其他类的概率，使模型明确地利用标签层次。我们在图像分类和语义分割上都进行了我们的方法。结果显示，HCOT在CIFAR100、Imagenet和PASCAL-context中的表现优于最先进的模型。我们的实验还证明，HCOT可以应用于具有潜在标签层次的任务，这是许多机器学习任务的共同特征。
人们对自动神经结构搜索（NAS）的兴趣越来越大。为了提高NAS的效率，以前的方法采用了权重共享的方法，迫使所有模型共享同一组权重。 在本文中，我们从贝叶斯的角度分析了现有的权重共享一次性NAS方法，并确定了后验褪色问题，这损害了共享权重的有效性。为了缓解这个问题，我们提出了一个实用的方法来引导参数后验向其真实分布。 所产生的方法，即后验聚合NAS（PC-NAS），在标准的GPU延迟约束下，在ImageNet上实现了最先进的性能。在我们的小搜索空间，我们的模型PC-NAS-S达到了76.8%的最高1级精度，比相同延迟的MobileNetV2（1.4倍）高2.1%。当采用到我们的大搜索空间时，PC-NAS-L在11ms内达到了78.1%的最高1级精度。所发现的架构也很适合于其他计算机视觉应用，如物体检测和人物重新识别。
噪声标签在现实世界的训练数据中非常常见，由于对噪声标签的过度拟合，导致测试数据的泛化效果很差。在本文中，我们声称这种过度拟合可以通过在噪声标签被严重记忆之前 "提前停止 "训练深度神经网络来避免。然后，我们使用 "最大安全集 "继续训练提前停止的网络，它在提前停止点之后的每个历时中保持一个几乎肯定为真实标签的样本集合。 把它们放在一起，我们新颖的两阶段训练方法，称为Prestopping，实现了在任何类型的标签噪声下的实际使用的无噪声训练。使用四个图像基准数据集进行的广泛实验证实，我们的方法在存在真实世界的噪声下，测试误差明显优于四个最先进的方法0.4-8.2个百分点。
最近的研究表明，连续通信允许在多Agent场景中使用反向传播进行有效的训练，但仅限于完全合作的任务。在本文中，我们提出了个体化控制连续通信模型（IC3Net），它比简单的连续通信模型具有更好的训练效率，并且可以应用于半合作和竞争环境以及合作环境。 IC3Net用门控机制控制连续通信，并对每个代理使用个性化奖励，以获得更好的性能和可扩展性，同时解决信用分配问题。使用各种任务，包括StarCraft BroodWars探索和战斗场景，我们表明，随着规模的增加，我们的网络产生比基线更好的性能和收敛率。
神经序列到序列模型是最近提出的用于文本文件抽象总结的一系列方法，有助于产生源文本叙述的浓缩版本，而不局限于只使用原文中的单词。尽管抽象总结取得了进展，但自定义的总结生成（例如，针对用户的偏好）仍未得到探索。 在本文中，我们提出了CATS，一个抽象的神经总结模型，它以序列到序列的方式总结内容，但也引入了一种新的机制来控制所产生的总结的潜在话题分布。我们在著名的CNN/DailyMail数据集上的实验结果表明，我们的模型达到了最先进的性能。
我们提出了一个基于学习-压缩算法思想的软件框架，它允许人们通过不同的压缩机制（剪枝、量化、低秩等）来压缩任何神经网络。通过设计，神经网络的学习（由SGD处理）与它的参数压缩（由信号压缩函数处理）是解耦的，因此该框架可以很容易地扩展到处理不同的神经网络和压缩类型的组合。 此外，它还有其他优点，如容易与深度学习框架集成，高效的训练时间，在损失-压缩权衡中具有竞争力的实际表现，以及合理的收敛保证。我们的工具包是用Python和Pytorch编写的，我们计划在研讨会前提供它，并最终开放给社区贡献。
这项工作寻求在没有任何人类标记注释的情况下，完全基于视听数据从语音中生成人脸的可能性。为此，我们提出了一个多模态学习框架，将推理阶段和生成阶段联系起来。首先，推理网络被训练来匹配两种不同模态的说话人身份。然后，预训练的推理网络通过提供关于语音的条件信息与生成网络合作。
我们提出了一个简单的神经模型，给定一个公式和一个属性，试图回答该公式是否具有给定的属性，例如一个命题公式是否总是真的。公式的结构由一个前馈神经网络以自上而下的方式为给定的公式递归构建。
尽管在深度强化学习（RL）领域取得了重大进展，但今天的算法仍然无法在一系列不同的任务（如Atari 2600游戏）中持续学习人类水平的策略。我们发现任何算法都需要掌握三个关键挑战，以便在所有游戏中表现良好：处理不同的奖励分布，在长时间范围内进行推理，以及有效地探索。 在本文中，我们提出了一种算法，它可以解决这些挑战中的每一个，并且能够在几乎所有的Atari游戏中学习人类水平的政策。一个新的转换贝尔曼算子使我们的算法能够处理不同密度和规模的奖励；一个辅助的时间一致性损失使我们能够使用0.999的折扣系数稳定地训练。 999（而不是0.99）的折现系数将有效的规划期限延长了一个数量级；我们通过使用人类的示范来缓解探索问题，引导代理走向奖励状态。当在一组42个Atari游戏上测试时，我们的算法超过了使用一组普通的超参数的40个游戏的平均人类性能。
人类对一个问题所掌握的知识往往远远超出了一组训练数据和输出标签。虽然深度学习的成功主要依赖于监督训练，但重要的属性不能仅仅从端到端的注释中有效地推断出来，例如因果关系或特定领域的不变性。我们提出了一种通用技术，用表达为训练实例之间关系的先验知识补充监督训练。 我们在视觉问题回答的任务上说明了该方法，以利用各种辅助注释，包括问题之间的等价关系和逻辑必然关系。现有的使用这些注释的方法，包括辅助损失和数据增强，不能保证将这些关系严格纳入模型，因为它们需要与端到端目标进行仔细的平衡。 我们的方法使用这些关系来塑造模型的嵌入空间，并将它们视为对其所学表征的严格约束。%由此产生的模型编码的关系可以更好地跨实例进行概括。在VQA的背景下，这种方法带来了准确性和稳健性的显著改善，特别是比将约束作为一个软正则器的常见做法。 我们还表明，将这种类型的先验知识与我们的方法结合起来会带来一致的改进，这与所使用的监督数据的数量无关。
近年来，人工神经网络彻底改变了计算机科学的许多领域，因为它们为许多以前未解决的问题提供了解决方案。另一方面，对于许多问题，存在经典算法，这些算法通常超过了神经网络的准确性和稳定性。为了结合这两个概念，我们提出了一种新的神经网络--算法神经网络（AlgoNets）。这些网络将经典算法的平滑版本整合到神经网络的拓扑结构中。
与边界框相比，在物体高度非结构化的应用中，如在医学领域，点状定位允许更精确的定位和准确的可解释性。在这项工作中，我们专注于弱监督定位（WSL），其中一个模型被训练来对图像进行分类，并仅使用全局图像注释在像素级定位感兴趣的区域。 为了缓解这个问题，我们为WSL提出了一种新的深度学习方法，由一个定位器和一个分类器组成，其中定位器被限制使用条件熵（CE）来确定相关和不相关区域，目的是减少假阳性区域。 在一个公共医疗数据集和两个自然数据集上使用Dice指数的实验结果表明，与最先进的WSL方法相比，我们的建议可以在图像级分类和像素级定位（低误报）方面提供显著的改进，并对过拟合具有鲁棒性。
基于模型的强化学习已经被经验证明是提高样本效率的成功策略。特别是Dyna架构，作为一个优雅的基于模型的架构，集成了学习和规划，提供了使用模型的巨大灵活性。Dyna中最重要的组件之一被称为搜索控制，它指的是生成状态或状态动作对的过程，我们从中查询模型以获得模拟经验。搜索控制对于提高学习效率至关重要。 我们的主要直觉建立在信号处理中的香农采样定理上，该定理表明高频信号需要更多的样本来重构。 我们开发了一种简单的策略，通过梯度规范来局部测量函数的频率，并为这种方法提供了理论依据。然后，我们将我们的策略应用于Dyna的搜索控制，并在基准领域进行实验，以显示其特性和有效性。
我们提出了一个新的架构，用于从一组分布式数据源中进行分布式图像压缩。这项工作的动机是数据驱动的编解码器设计、低功耗、鲁棒性和数据隐私等实际需求。拟议的架构，我们称之为可扩展图像压缩的分布式递归自动编码器（DRASIC），能够在相关的数据源上训练分布式编码器和一个联合解码器。 它的压缩能力比单独训练编解码器的方法要好得多。同时，对于10个分布式数据源，我们的分布式系统的峰值信噪比（PSNR）明显低于用所有数据源训练的单一编解码器的2dB。 我们实验了具有不同相关性的分布式源，并展示了我们的方法如何与分布式源编码（DSC）中的Slepian-Wolf定理相匹配。我们的方法也被证明对来自一些分布式源的编码数据的不存在具有鲁棒性。
长短时记忆网络（LSTM）被引入，以应对简单的递归神经网络（S-RNN）中的梯度消失，方法是用门控制的加法递归连接来增强它们。我们提出另一种观点来解释LSTM的成功：门本身是强大的递归模型，提供比以前更多的表征能力。 我们通过展示LSTM的门可以从嵌入的S-RNN中解耦，产生一类受限的RNN，其中主要递归计算输入的元素加权和与上下文无关的函数。在一系列具有挑战性的NLP问题上的实验表明，简化的基于门的模型比S-RNN的效果好很多，而且往往与原始LSTM一样好，强烈表明门在实践中的作用远远超过缓解梯度消失。
在这项工作中，我们对最近发表的100多篇ML4H研究论文进行了系统的评估，这些论文与我们确定的与可重复性有关的几个方面相比较，我们发现ML4H领域与更成熟的机器学习领域相比很差，特别是在数据可及性和代码可及性方面。 最后，借鉴其他科学领域的成功经验，我们向数据提供者、学术出版商和ML4H研究界提出建议，以促进可重复研究的发展。
我们提出了一个评估数学表达的解决方案。然而，我们没有设计一个单一的端到端模型，而是提出了一个乐高积木式的架构。在这个架构中，不是训练一个复杂的端到端神经网络，而是可以独立地训练许多小网络，每个网络完成一个特定的操作，扮演一个乐高积木。 这种自下而上的策略不仅引入了可重用性，我们还表明它允许对涉及n位数的计算进行泛化，并且我们展示了多达7位数的结果，与现有的方法不同，我们的解决方案对正数和负数都有泛化作用。
在标准生成对抗网络（SGAN）中，鉴别器估计输入数据是真实的概率。生成器被训练成增加假数据是真实的概率。我们认为，它也应该同时减少真实数据是真实的概率，因为1）这将解释先验知识，即小批量中的一半数据是假的，2）这将被观察到发散最小化，3）在最佳设置中，SGAN将等同于积分概率公制（IPM）GANs。我们表明，这一特性可以通过使用相对论判别器来诱导，该判别器估计给定的真实数据比随机抽样的假数据更真实的概率。我们还提出了一个变体，其中判别器估计给定的真实数据比假数据更真实的概率，平均而言。 我们将这两种方法概括为非标准的GAN损失函数，并将它们分别称为相对论GAN（RGAN）和相对论平均GAN（RaGAN）。我们表明，基于IPM的GAN是使用身份函数的RGAN的一个子集。从经验上看，我们发现：1）RGANs和RaGANs比它们的非相对论对应物明显更稳定，产生的数据样本质量更高；2）带有梯度惩罚的标准RaGAN比WGAN-GP产生的数据质量更好，而每次生成器更新只需要一次判别器更新（将达到最先进水平所需时间减少400%）；3）RaGANs能够从非常小的样本（N=2011）产生可信的高分辨率图像（256x256），而GAN和LSGAN不能。这些图像的质量明显好于WGAN-GP和SGAN在光谱归一化后生成的图像。 该代码可在https://github.com/AlexiaJM/RelativisticGAN 上免费获取。
深度强化学习在离散和连续控制的挑战性领域的一些最成功的应用是在政策设置中使用政策梯度方法。然而，政策梯度可能遭受大的方差，可能会限制性能，并且在实践中需要仔细调整熵的正则化以防止政策崩溃。 我们表明，在多任务设置中，V-MPO超过了以前报告的Atari-57和DMLab-30基准套件的分数，并且在没有重要性加权、熵正则化或基于群体的超参数调整的情况下，也能可靠地做到这一点。在单个DMLab和Atari水平上，所提出的算法可以实现大大高于以前报告的分数。 V-MPO也适用于具有高维、连续行动空间的问题，我们在学习控制具有22个自由度的全状态观察和56个自由度的像素观察的模拟人形动物的背景下，以及在OpenAI Gym任务的例子中，证明了V-MPO取得的渐近分数大大高于以前的报告。
图灵完全计算和推理通常被认为是一般智能的必要前提。已经有大量的工作在研究模仿一般计算的神经网络，但这些网络不能概括到其训练集之外的数据分布。我们通过计算机科学基本问题的视角研究这个问题：排序和图形处理。 我们修改了变压器的屏蔽机制，以使它们能够实现具有强大泛化能力的基本功能。我们称这种模型为神经执行引擎，并表明它通过监督学会了以近乎完美的精度数值计算包括这些算法的基本子程序。此外，它在泛化到未见过的数据和训练分布之外的长序列时保持了这种精度水平。
元学习是一种很有前途的学习策略，可以利用从任务分布中收集的数据在新的任务中有效地学习。然而，迄今为止，元学习的文献主要集中在任务分割的环境中，在训练时，离线数据被假定为根据基础任务分割，在测试时，算法被优化为在单一任务中学习。在这项工作中，我们使通用元学习算法应用于无法获得这种任务分割的环境，如具有时间变化的任务的持续在线学习。 我们提出了通过在线变化点分析（MOCA）进行元学习的方法，这种方法用可区分的贝叶斯变化点检测方案来增强元学习算法。该框架允许直接在时间序列数据上进行训练和测试，而无需将其分割成离散的任务。
患有高频听力损失的人依赖采用降频算法的助听器。这些算法将一些声音从高频段转移到低频段，在那里声音对患者来说变得更容易被感知。 因此，及时（零延迟）和准确的fricative音素检测是高质量助听器的一个关键问题。在本文中，我们提出了一个基于深度学习的fricative音素检测算法，该算法具有零检测延迟，并在TIMIT语音库上实现了最先进的fricative音素检测精度。所有报告的结果都是可重复的，并配有易于使用的代码，可作为未来研究的基准。
具有软注意力的序列到序列模型已经成功地应用于各种问题，但是它们的解码过程会产生二次的时间和空间成本，并且不适用于实时序列转换。为了解决这些问题，我们提出了单调分块注意力（MoChA），它自适应地将输入序列分割成小块，在上面计算软注意力。 我们表明，利用MoChA的模型可以用标准的反向传播进行有效的训练，同时允许在测试时进行在线和线性时间解码。当应用于在线语音识别时，我们获得了最先进的结果，并与使用离线软注意力机制的模型的性能相匹配。在我们不期望单调对齐的文档总结实验中，与基于单调注意力的基线模型相比，我们显示出明显的性能提高。
我们提出了一个自动排序图像补丁的框架，能够深入分析数据集与使用卷积神经网络的分类任务的可学习性之间的关系。我们的初步实验结果表明，在样本层面上对补丁进行知情的智能洗牌可以通过在训练的早期阶段暴露重要的特征来加速训练。此外，我们进行了系统的实验并提供证据表明CNN的概括能力与训练样本中存在的人类可识别特征不相关。 我们利用该框架不仅表明样本内特征的空间定位与泛化不相关，而且在实现类似的泛化性能的同时加速收敛。使用多种网络架构和数据集，我们表明使用相邻斑块之间的相互信息度量对图像区域进行排序，使CNN在没有斑块排序的情况下训练相同网络所需总步骤的三分之一就能收敛。
在强化学习中，产生能够泛化到各种环境的代理是一个重大的挑战。克服这个问题的一个方法是领域随机化，在每个训练集的开始，环境的一些参数被随机化，这样代理就会接触到许多可能的变化。然而，领域随机化是非常低效的，可能导致跨领域的高差异政策。 在这项工作中，我们对领域随机化问题进行了形式化处理，并表明相对于随机化参数而言，最小化策略的Lipschitz常数会导致所学策略的低方差。我们提出了一种方法，即代理只需要在环境的一个变体上进行训练，并且在训练期间对其学习的状态表示进行规范化处理，以最小化这个常数。我们进行的实验表明，我们的技术比标准领域随机化导致了更有效和强大的学习，同时实现了平等的通用分数。
来自网络神经科学和连接组学领域的主张表明，涉及复杂网络的大脑拓扑模型具有特殊的用途和意义。深度神经网络领域大多没有从这些主张中得到启发。在本文中，我们提出了三个架构，并使用它们中的每一个来探索网络神经科学和深度学习的交叉点，试图弥合这两个领域之间的差距。 利用网络神经科学和连接组学的教导，我们展示了对ResNet架构的改进，我们展示了早期训练和网络的光谱特性之间的可能联系，我们展示了基于C.Elegans的神经元网络的DNN的可训练性。
尽管在自动化知识库建设方面做了大量工作，但创建一个准确、最新和完整的知识库仍然是一个重大挑战。 在本文中，我们提出了Alexandria--一个用于无监督、高精度知识库构建的系统。Alexandria使用一个概率程序来定义将知识库事实转换为非结构化文本的过程。 使用概率推理，我们可以反转这个程序，从而从网络文本中检索出事实、模式和实体。使用概率程序可以将文本中的不确定性传播到所检索的事实中，从而提高准确性并帮助合并来自多个来源的事实。由于亚历山大系统不需要标记的训练数据，因此可以用最少的人工输入构建知识库。
最近的进展使得创建深度复值神经网络成为可能。尽管取得了这一进展，但许多具有挑战性的学习任务还没有利用复值表征的力量。在最近的进展基础上，我们提出了一种新的深度复值方法，用于频域的信号检索和提取。 我们的新方法基于复值版本的特征线性调制（FiLM），并作为我们提出的信号提取方法的基石。我们还引入了一个新的明确的幅度和相位感知损失，它是尺度和时间不变的，考虑到了频谱图的复值成分。使用华尔街日报数据集，我们将我们的相位感知损失与其他几个在时域和频域运行的损失进行了比较，证明了我们提出的信号提取方法和建议的损失的有效性。
我们提出了一种GNN的实现方法，它可以从观察到的蜂群轨迹数据中预测和模仿运动的情况。通过转移学习，我们证明了该网络捕捉蜂群中互动动态的能力。
嵌入层通常用于将离散的符号映射到反映其语义的连续嵌入向量中。尽管它们很有效，但嵌入层中的参数数量随着符号数量的增加而线性增加，对内存和存储限制构成了关键挑战。在这项工作中，我们提出了一个通用的和端到端的可学习压缩框架，称为可微分产品量化（DPQ）。 我们提出了DPQ的两个实例，利用不同的近似技术来实现端到端学习中的可分性。我们的方法可以随时作为任何现有嵌入层的替代方案。
对于多值函数--比如当给定输入的目标的条件分布是多模态的时候--标准回归方法并不总是可取的，因为它们提供的是条件平均值。 这样的方法很难扩展，也很难受益于参数化的函数近似，比如神经网络，它可以学习输入和目标之间的复杂关系。在这项工作中，我们提出了一个参数化的模态回归算法，通过使用隐含函数定理来开发一个学习输入和目标的联合参数化函数的目标。 我们在几个合成问题上实证了我们的方法(i)可以学习多值函数并产生条件模式，(ii)对高维输入有很好的扩展性，(iii)对某些单模问题甚至更有效，特别是对高频数据，输入和目标上的联合函数可以更好地捕捉它们之间的复杂关系。我们最后表明，我们的方法在两个目标上有不对称分布的回归数据集上提供小的改进。
深度强化学习算法需要大量的经验来学习一个单独的任务。虽然原则上元强化学习（meta-RL）算法使代理人能够从少量的经验中学习新的技能，但几个主要的挑战排除了其实用性。 目前的方法严重依赖政策上的经验，限制了它们的样本效率。它们也缺乏在适应新任务时推理任务不确定性的机制，限制了它们在稀疏奖励问题上的有效性。在本文中，我们通过开发一种非政策元RL算法来解决这些挑战，该算法将任务推理和控制分开。 在我们的方法中，我们对潜在的任务变量进行在线概率过滤，以便从少量的经验中推断出如何解决一个新的任务。我们展示了如何将这些任务变量与非政策性RL算法相结合，以实现元训练和适应效率。
知识库是关于不同主题的大量事实（RDF三元组）的集合，支持重要的现代应用。然而，与网络上丰富的信息相比，现有的知识库所包含的数据非常少。这是因为知识库创建和增强的行业标准受到了严重的瓶颈：它们依赖于领域专家来确定适当的网络资源来提取数据。 将知识提取完全自动化的努力未能改善这一标准：这些自动化系统能够从更广泛的来源中检索到更多的数据，但它们的精确度和召回率都很低。因此，这些大规模的提取仍然没有得到利用。 在本文中，我们提出了MIDAS，一个利用自动知识提取管道的结果来修复工业知识创造和增强过程中的瓶颈的系统。MIDAS自动建议高质量的网络资源，并描述在增强现有知识库方面应该提取什么。 第一，我们引入了一个新的概念，即网络资源片，来描述网络资源的内容。第二，我们定义了一个利润函数来量化网络资源片在增强现有知识库方面的价值。第三，我们开发了有效的和高可扩展的算法来得出高利润的网络资源片。
我们探讨了匹配预测问题，在这个问题上，人们试图根据部分组的比较数据来估计一组M项比另一组更受欢迎的可能性。 更糟糕的是，我们对某一特定场景的基础模型没有先验知识。这些都需要一个统一的方法，可以普遍适用于广泛的场景，并取得一致的高性能。为此，我们结合了深度学习架构，以反映大多数最先进的算法的关键结构特征，其中一些算法在某些情况下是最佳的，它们有共同点。 这使我们能够推断出隐藏在给定数据集下的模型，这些模型支配着组内互动和比较的统计模式，因此能够设计出适合手头数据集的最佳算法。通过在合成和真实世界数据集上的广泛实验，我们将我们的框架与最先进的算法进行了比较。 事实证明，我们的框架在所有数据集的交叉熵损失和预测准确率方面始终保持最佳性能，而最先进的算法在不同的数据集上表现不一致。此外，我们表明，它可以很容易地扩展到等级聚合任务中获得满意的性能，这表明它也可以适应其他任务。
递归神经网络（RNNs）被设计用来处理连续数据，但受到梯度消失或爆炸的影响。 最近关于单元递归神经网络（uRNNs）的工作被用来解决这个问题，在某些情况下，超过了长短期记忆网络（LSTMs）的能力。 我们提出了一个更简单和新颖的更新方案，在不使用复值矩阵的情况下保持正交的递归权重矩阵。这是用Cayley变换的斜对称矩阵来实现的。 所提出的训练方案涉及一个直接的梯度计算和更新步骤。在几个实验中，所提出的缩放的Cayley正交递归神经网络（scoRNN）以比其他单元式RNN更少的可训练参数取得了优异的结果。
存在大量的自然语言处理任务来分析人类语言的语法、语义和信息内容。这些看似非常不同的任务通常由专门设计的架构来解决。在本文中，我们提供了一个简单的见解，即大量的任务可以用一个由标签跨度和跨度之间的关系组成的统一格式表示，因此一个独立于任务的模型可以用于不同的任务。 我们在10个不同的任务上进行了广泛的实验来测试这一见解，这些任务包括依赖性解析（语法）、语义角色标注（语义）、关系提取（信息内容）、基于方面的情感分析（情感）以及其他许多任务，取得了与最先进的专门模型相当的性能。我们进一步证明了多任务学习的好处。我们将这些数据集转换成统一的格式来建立一个基准，这为评估未来通用自然语言分析的模型提供了一个整体测试平台。
大规模的矩阵反转经常被认为是扩展高斯过程（GP）模型的主要障碍。随着GP作为越来越复杂的贝叶斯深度学习模型的构建模块的使用，消除这些障碍是实现大规模结果的一个必要步骤。 我们的约束直接将一个自由矩阵作为参数，这是一个额外的变异参数。在约束的局部最大值，这个矩阵等于矩阵逆。我们证明了我们的约束给出了与早期变异近似相同的保证。我们通过实验证明了约束的一些有益特性，尽管显著的挂钟时间速度改进需要未来在优化和实施方面的改进。
已有研究表明，使用非零曲率的几何空间而不是零曲率的普通欧几里得空间可以提高一系列机器学习任务的性能，以学习表征。 ~我们开发了混合曲率可变自动编码器，这是一种训练自动编码器的有效方法，其潜空间是恒定曲率黎曼流形的乘积，其中每个分量的曲率可以被学习。
用于生成分子结构的机器学习算法为药物发现提供了一种有希望的新方法。我们将分子优化作为一个翻译问题，目标是将输入的化合物映射到具有改进的生化特性的目标化合物。值得注意的是，我们观察到，当生成的分子被反复反馈到翻译器中时，分子化合物的属性每一步都会得到改善。 我们称这种方法为黑匣子递归翻译（BBRT），这是一种用于分子属性优化的新推理方法。这种简单而强大的技术严格按照任何翻译模型的输入和输出进行操作。我们使用我们简单的滴入式替换与众所周知的基于序列和图的模型，获得了分子属性优化任务的最新结果。
深度神经网络（DNN）由于其卓越的性能，越来越多地被部署在云服务器和自主代理中。部署的DNN要么在白盒环境（模型内部是公开的），要么在黑盒环境（只有模型输出是已知的）中被利用，这取决于应用。在急于采用DNN的过程中，一个实际问题是保护模型免受知识产权（IP）的侵犯。 我们提出了BlackMarks，这是第一个适用于黑箱场景的端到端多比特水印框架。BlackMarks将预先训练好的无标记模型和所有者的二进制签名作为输入，其输出是相应的带有特定密钥的标记模型，以后可用于触发嵌入的水印。 为此，BlackMarks首先设计了一个依赖模型的编码方案，将任务中所有可能的类别映射为比特-0和比特-1。鉴于所有者的水印签名（二进制字符串），使用有针对性的对抗性攻击设计了一组密钥图像和标签对。 为了提取水印，BlackMarks用水印的关键图像查询模型，并使用设计的编码方案从相应的预测中解码所有者的签名。
对抗性训练为训练鲁棒神经网络提供了一种原则性的方法。从优化的角度来看，对抗性训练本质上是在解决一个minmax的鲁棒优化问题。外部最小化是试图学习一个鲁棒的分类器，而内部最大化是试图产生对抗性样本。不幸的是，由于缺乏凸凹结构，这样一个minmax问题很难解决。 具体来说，我们不是应用现有的手工设计算法来解决内部问题，而是学习一个优化器，它被参数化为卷积神经网络。同时，学习一个稳健的分类器来防御由学习优化器产生的对抗性攻击。 从生成学习的角度来看，我们提出的方法可以被看作是学习一个用于生成对抗性样本的深度生成模型，该模型对鲁棒性分类具有适应性。我们的实验表明，我们提出的方法在CIFAR-10和CIFAR-100数据集上明显优于现有对抗性训练方法。
在这项工作中，我们引入了一个新的框架，用于在存在不确定性的情况下进行时间预测。它是基于一个简单的想法，将未来状态中可预测的成分与那些本质上不可预测的成分分开，并将不可预测的成分编码为一个低维度的静态变量，并将其输入到前向模型中。 我们在多个数据集的视频预测背景下对该方法进行了评估，结果表明它能够准确地产生不同的预测结果，而不需要在潜在空间上进行交替最小化或对抗性训练。
进行强化学习实验是一个复杂而及时的过程。一个完整的实验管道通常包括一个环境的模拟、一个或多个学习算法的实现、各种旨在促进代理-环境相互作用的额外组件，以及任何必要的分析、绘图和记录。 鉴于这种复杂性，本文介绍了simple rl，这是一个新的开源库，用于在Python 2和3中进行强化学习实验，重点是简单性。本文概述了该软件包的核心设计理念，它与现有库的区别，并展示了其核心功能。
Wasserstein GAN(WGAN)是一个最小化数据分布和样本分布之间的Wasserstein距离的模型。最近的研究提出了稳定WGAN的训练过程和实现Lipschitz约束。在这项研究中，我们证明了在关于均衡和惩罚措施$mu$的适当假设下，优化简单梯度惩罚$mu$-WGAN(SGP $\mu$-WGAN)的局部稳定性。 我们采用了度量值微分的概念来处理惩罚项的导数，这有助于处理具有低维支持的抽象奇异度量。基于这种分析，我们声称对数据流形或样本流形进行惩罚是用梯度惩罚来规范原始WGAN的关键。
我们提出了随机分区放松（RPR），这是一种将卷积神经网络的参数强量化为二元（+1/-1）和三元（+1/0/-1）值的方法。从一个预训练的模型开始，我们首先将权重量化，然后将它们的随机分区放松到它们的连续值进行再训练，再将它们再次量化并切换到另一个权重分区以进一步适应。 我们实证评估了RPR与ResNet-18、ResNet-50和GoogLeNet在ImageNet分类任务中二元和三元权重网络的性能。我们显示出二元和三元权重的GoogLeNet的准确度超过了最先进的水平，而ResNet-18和ResNet-50的性能则具有竞争力，使用基于SGD的训练方法可以很容易地集成到现有框架。
学习长期依赖性是递归神经网络（RNNs）的一个关键的长期挑战。层次递归神经网络（HRNNs）被认为是一种有希望的方法，因为长期依赖性是通过层次结构上下的捷径来解决的。然而，截断反向传播时间（TBPTT）的内存要求仍然阻碍了在很长的序列上进行训练。 在本文中，我们通过经验表明，在（深度）HRNN中，从高层次到低层次的梯度传播可以被局部可计算的损失所取代，而不损害网络的学习能力，在广泛的任务中。这种通过局部损失的解耦，与标准TBPTT相比，训练的内存需求在层次深度上呈指数级下降。
在计算机视觉任务的典型深度学习方法中，卷积神经网络（CNN）被用来从图像中提取不同层次的抽象特征，并通过一系列的转换将高维输入压缩到低维决策空间。 这些变化被形式化为嵌入的有效维度。我们考虑了类内各层的有效维度是如何变化的。我们表明，在不同的数据集和架构中，一类的有效维度在进一步进入网络前会增加，这表明某种初始的白化转换。此外，在网络深处的有效维度的减少率与模型的训练性能相对应。
深度学习方法在声音识别任务中取得了很高的性能。决定如何提供训练数据对进一步提高性能很重要。我们提出了一种新的深度声音识别学习方法。我们的策略是通过将类间音识别为类间音来学习一个鉴别性的特征空间。我们通过将属于不同类的两个声音以随机比例混合来产生类间音，然后将混合后的声音输入到模型中，训练模型输出混合比例。 实验结果表明，BC学习改善了各种声音识别网络、数据集和数据增强方案的性能，其中BC学习被证明总是有益的。此外，我们构建了一个新的深度声音识别网络（EnvNet-v2），并用BC学习对其进行训练。
时空预测由于其广泛的应用，如气候建模、交通预测、视频缓存预测等，已经成为机器学习和统计学中越来越重要的预测任务。虽然已经进行了大量的研究，但大多数现有的工作都假设来自不同来源或不同地点的数据是同样可靠的。由于成本、可及性或其他因素，数据质量不可避免地会有差异，这就给模型引入了明显的偏差，导致不可靠的预测结果。 在黑箱预测模型中，如深度神经网络，这个问题可能会加剧。在本文中，我们提出了一个新的解决方案，可以通过没有明确标签的时空信号的局部变化来自动推断不同来源的数据质量水平。此外，我们将数据质量水平的估计与图卷积网络相结合，利用其有效结构。
人类对三维形状的感知超越了将它们重建为一组点或几何基元的组成：我们也毫不费力地理解更高层次的形状结构，如物体部分的重复和反射对称性。相比之下，最近在三维形状传感方面的进展更注重低层次的几何学，但较少关注这些高层次的关系。在本文中，我们提出三维形状程序，将自下而上的识别系统与自上而下的符号程序结构相结合，以捕获三维形状的低层次几何学和高层次结构先验。 由于没有真实形状的形状程序注释，我们开发的神经模块不仅可以学习从原始的、没有注释的形状中推断出3D形状程序，还可以执行这些程序进行形状重建。在初始引导之后，我们的端到端可分模型通过自我监督的方式重建形状来学习3D形状程序。 实验证明，我们的模型可以准确地推断和执行各种类别的高度复杂的形状的3D形状程序。它还可以与图像到形状模块集成，直接从RGB图像中推断出3D形状程序，从而使3D形状重建更准确，更具有物理合理性。
深度强化学习（Deep Reinforcement Learning，简称RL）由于其在各种控制任务上令人鼓舞的表现而受到越来越多的关注。然而，训练神经网络的传统正则化技术（例如。在这项工作中，我们首次全面研究了连续控制任务上的多种策略优化算法的正则化技术。有趣的是，我们发现策略网络上的常规正则化技术往往能给任务性能带来很大的改善，而且当任务难度较大时，改善通常更为显著。 我们还与广泛使用的熵正则化进行了比较，发现L_2$正则化通常更好。我们的发现被进一步证实对训练超参数的选择是稳健的。我们还研究了不同组件的正则化效果，发现只对策略网络进行正则化通常就足够了。我们希望我们的研究为未来策略优化算法的正则化实践提供指导。
我们介绍了FigureQA，这是一个视觉推理语料库，其中有超过100,000张图片中的100万个问题-答案对。这些图片是合成的、科学风格的五类数字：线图、点线图、垂直和水平条形图以及饼图。 我们通过从15个模板中生成问题来制定我们的推理任务；问题涉及绘图元素之间的各种关系，并检查诸如最大值、最小值、曲线下的面积、平滑度和交叉点等特征。要解决这些问题，往往需要参考多个绘图元素并综合分布在整个图形中的空间信息。 为了方便机器学习系统的训练，该语料库还包括可用于制定辅助目标的侧面数据。特别是，我们提供了用于生成每个图的数字数据，以及所有绘图元素的边界盒注释。我们通过训练几个模型来研究拟议的视觉推理任务，包括最近提出的关系网络作为强基线。
在本文中，我讨论了智能代理中可能出现的一些解释的种类。我区分了过程说明和偏好说明，前者涉及启发式搜索过程中的详细决定，后者阐明了备选方案的排序，而不考虑它们是如何产生的。此外，我讨论了多步骤决策的三个方面--概念推理、计划生成和计划执行--其中可能出现解释。
生成式深度学习引发了新一轮的超级分辨率（SR）算法，这些算法以令人印象深刻的美学效果增强了单一图像，尽管是想象中的细节。多帧超级分辨率（MFSR）通过对多个低分辨率视图进行调节，为这个不理想的问题提供了一个更有基础的方法。 这对于依赖可靠图像的人类对地球影响的卫星监测非常重要--从森林砍伐到侵犯人权。为此，我们提出了HighRes-net，这是第一个针对MFSR的深度学习方法，它以端到端的方式学习其子任务：(i) 共同注册，(ii) 融合，(iii) 上采样，以及(iv) 损失时注册。 低分辨率视图的协同注册是通过参考帧通道隐式学习的，没有明确的注册机制。我们学习了一个全局融合算子，该算子被递归地应用于任意数量的低分辨率对。我们引入了注册损失，通过ShiftNet学习将SR输出与地面实况对齐。
大型微型并行SGD通常用于深度网络的分布式训练。使用基于AllReduce的紧耦合精确分布式平均的方法对慢速节点和高延迟通信很敏感。在这项工作中，我们展示了随机梯度推演（SGP）对分布式训练的适用性。SGP使用一种称为PushSum的流言算法进行近似分布式平均，允许更松散的耦合通信，在高延迟或高可变性场景中是有益的。 其代价是近似分布式平均法会在梯度中注入额外的噪声，从而影响训练和测试的准确性。 例如，使用32个节点，每个节点8个GPU，在ImageNet上训练ResNet-50，节点通过10Gbps以太网通信，SGP在1.5小时左右完成90个epochs，而AllReduce SGD需要5个多小时，SGP的top-1验证精度保持在使用AllReduce SGD的1.2%以内。
在本文中，我们通过修改最先进的hredGAN架构，将基于角色的序列到序列（Seq2Seq）神经网络对话模型扩展到多轮对话场景中，以同时捕获语料属性，如说话人身份、对话主题、说话人情绪等。所提出的系统，phredGAN有一个基于角色的HRED生成器（PHRED）和一个条件判别器。(1) $phredGAN_a$，一个将属性表示作为额外输入到传统的对抗性判别器的系统，和(2) $phredGAN_d$，一个双判别器系统，除了对抗性判别器外，还合作预测产生输入语料的属性。 为了证明phredGAN的性能优于角色SeqSeq模型，我们用两个对话数据集进行了实验，即Ubuntu对话语料库（UDC）和来自《生活大爆炸》和《朋友》的电视剧记录。 我们还探索了在具有许多弱属性模式的数据集（如《生活大爆炸》和《朋友》）和具有少数强属性模式的数据集（Ubuntu数据集中的客户-代理人互动）上使用$phredGAN$的任何一个变体的权衡。
我们引入了受生物启发的人工神经网络，该网络由神经元组成，并以空间位置为特征。为了模拟生物系统的属性，我们在二维空间中增加了惩罚长连接和神经元接近的成本。我们的实验表明，在网络执行两个不同任务的情况下，神经元自然地分裂成集群，每个集群负责处理不同的任务。这种行为不仅对应于生物系统，还可以进一步了解可解释性或持续学习。
变换器已经成为许多NLP任务的核心模型，从翻译到语言建模再到表征学习。它的成功证明了堆叠注意力作为许多任务的递归的替代品的有效性。理论上，注意力也提供了对模型内部决策的更多洞察力；然而，在实践中，当堆叠时，它很快就变得几乎与递归模型一样完全连接。在这项工作中，我们提出了一个替代的变换器架构，离散变换器，其目的是更好地分离出内部模型决策。 该模型使用硬注意来确保每一步只取决于一个固定的上下文。此外，该模型使用一个单独的 "语法 "控制器来分离出网络结构和决策。最后，我们表明这种方法可以通过直接正则化进一步稀疏化。
深度预测编码网络是神经科学启发的无监督学习模型，可以学习预测未来的感觉状态。我们在Lotter、Kreiman和Cox（2016）的PredNet实现的基础上，研究预测编码表征是否对预测视觉皮层的大脑活动有用。我们使用表征相似性分析（RSA），将PredNet表征与Algonauts项目（Cichy等人）的功能磁共振成像（fMRI）和脑磁图（MEG）数据进行比较。2019年）。与之前的文献发现（Khaligh-Razavi & Kriegeskorte，2014年）相比，我们报告的经验数据表明，为预测视频帧而训练的无监督模型在与空间（fMRI）和时间（MEG）数据的相关性方面可能超过监督图像分类基线。
将先前的知识纳入学习对于实现基于小规模噪声样本的良好性能是至关重要的。这种知识通常是通过与当前感兴趣的领域和任务类似的相关数据的可用性来纳入的。理想情况下，我们希望允许当前任务和先前相关任务的数据以这样一种方式自我组织学习系统，即以数据驱动的方式学习任务之间的共同点和差异。 我们开发了一个同时学习多个任务的框架，其基础是分享所有任务的共同特征，通过使用模块化的深度前馈神经网络来实现，该网络由处理所有任务的共同特征的共享分支和学习每个任务的特定独特方面的私有分支组成。该方法以统一的方式处理元学习（如领域适应、转移和多任务学习），并能轻松处理来自不同类型的数据。数值实验证明了在领域适应和转移学习设置中学习的有效性，并为网络中出现的灵活和面向任务的表征提供了证据。
深度神经网络和决策树在很大程度上是分开操作的；通常，前者用预先指定的架构进行表征学习，而后者的特点是用数据驱动的架构在预先指定的特征上学习层次。我们通过自适应神经树（ANTs）将两者结合起来，这种模型将表征学习纳入决策树的边缘、路由函数和叶子节点，同时采用基于反向传播的训练算法，从原始模块（例如。我们在分类和回归任务中证明了这一点，在MNIST和CIFAR-10数据集上实现了超过99%和90%的准确性，在SARCOS数据集上的表现优于标准神经网络、随机森林和梯度提升树。
虽然自然语言处理系统通常专注于单一语言，但多语言迁移学习有可能提高性能，特别是对于低资源语言。我们介绍了XLDA，即跨语言数据增强，这是一种用另一种语言的翻译替换输入文本片段的方法。XLDA提高了跨语言自然语言推理（XNLI）基准的所有14种测试语言的性能，用XLDA训练的希腊语、土耳其语和乌尔都语的性能提高了4.8倍，达到了最先进的水平。 在SQuAD问题回答任务中，我们看到XLDA在英语评估集上提供了1.0的性能提升。综合实验表明，大多数语言作为跨语言增强器是有效的，XLDA对广泛的翻译质量是稳健的，XLDA对于随机初始化模型甚至比预训练模型更有效。
在条件性生成潜变量模型的训练中，如果条件性信号非常强，而解码器的表达能力足以在只给定条件的情况下产生一个合理的输出，那么训练条件性生成模型就很有挑战性；生成模型往往会忽略潜变量，出现后验塌陷的情况。我们发现，并通过经验表明，后验崩溃背后的主要原因之一是生成模型的条件化方式，即通过对潜变量和条件的连接。为了缓解这个问题，我们建议通过统一条件和潜变量采样，明确地使潜变量依赖于条件，从而将它们耦合起来，以防止模型抛弃变化的根源。为了实现这一点，我们开发了一个条件变异自动编码器架构，它不仅学习潜变量的分布，而且学习条件的分布，后者作为前者的先验。我们在有条件的人体运动预测和图像说明的挑战性任务上的实验证明了我们的方法在避免后验塌陷方面的有效性。我们的方法的视频结果以匿名方式提供在http://bit.ly/iclr2020。
我们提出了一项研究，在控制随机种子的同时，研究几种少数学习算法在超参数和优化方案变化下的稳定性。 我们提出了一种方法来测试几个复制下的模型性能的统计差异。为了研究这个具体的设计，我们试图复制三篇著名论文的结果。我们对miniImagenet数据集的标准分类任务进行了分析，在测试时间的5路5次学习设置中，我们发现所选的实现在随机种子和重复中表现出稳定性。
我们研究了目标条件下的分层强化学习中的表征学习问题。在这种分层结构中，上层控制器通过迭代传达目标来解决任务，而下层策略则被训练来达到这些目标。因此，表征的选择--观察空间到目标空间的映射--是至关重要的。为了研究这个问题，我们发展了一个表征的次优性概念，它是以使用该表征的最佳分层策略的预期报酬来定义的。 我们推导出了约束次优性的表达式，并展示了这些表达式如何转化为在实践中可能被优化的表示学习目标。在一些困难的连续控制任务上的结果显示，与现有的方法相比，我们的表示学习方法产生了质量上更好的表示以及数量上更好的层次政策。
启发式搜索研究通常涉及寻找离线规划的算法，其目的是最小化扩展节点的数量或规划时间。在在线规划中，实时搜索或死线感知搜索的算法以前已经被考虑过。然而，在本文中，我们对{em situated temporal planning}的问题感兴趣，其中一个代理的计划可能取决于外部世界的外生事件，因此在规划过程中考虑到时间的流逝变得很重要。 以前关于位置时间规划的工作提出了简单的修剪策略，以及相关元推理问题的简化版本的复杂方案。在本文中，我们提出了一种简单的元推理技术，称为粗略的贪婪方案，它可以应用于位置式时间规划器。我们的经验评估表明，粗略的贪婪方案优于基于成本-去向估计的标准启发式搜索。
神经网络很容易受到小的对抗性扰动的影响。现有的文献主要集中在理解和减轻学习模型的脆弱性上。在本文中，我们展示了关于文献中最流行的鲁棒性训练方法的一个有趣的现象，即对抗性训练。即使是对输入数据分布进行语义保全的转换，也会使在新分布上训练和评估的对抗性训练模型产生明显不同的鲁棒性。我们对数据分布的这种敏感性的发现是基于一项研究，该研究将贝叶斯分类器的清洁准确性和鲁棒准确性的行为分开。 我们分别为MNIST和CIFAR10构建了语义相同的变体，并表明标准训练的模型在它们身上实现了可比的清洁精度，但对抗训练的模型实现了明显不同的鲁棒性精度。这种反直觉的现象表明，仅输入数据分布就能影响训练的神经网络的对抗鲁棒性，而不一定是任务本身。最后，我们讨论了评估对抗鲁棒性的实际意义，并初步尝试理解这一复杂的现象。
 自然语言处理中的许多任务涉及比较两个句子，以计算一些相关性、必然性或相似性的概念。通常这种比较是在词的层面或句子的层面上进行的，没有尝试利用句子的固有结构。当句子结构被用于比较时，它是在一个不可区分的预处理步骤中获得的，导致错误的传播。我们引入了一个句子间结构化对齐的模型，展示了如何通过匹配其潜在的结构来比较两个句子。 使用结构化注意机制，我们的模型将第一个句子中的可能跨度与第二个句子中的可能跨度相匹配，同时发现每个句子的树状结构并进行比较，在一个完全可分的模型中，只对比较目标进行训练。我们在两个句子比较任务中评估这个模型：斯坦福自然语言推理数据集和TREC-QA数据集。我们发现，比较跨度的结果比单独比较单词的性能更优越，而且学到的树与实际语言结构一致。
在本文中，我们提出了信息最大化自动编码器（InfoAE），其中编码器通过最大化表示和给定信息之间的相互信息，以无监督的方式学习强大的分解表示。我们在MNIST数据集上评估了我们的模型，在使用完全无监督的训练时取得了大约98.9%的测试准确率。
神经网络的有效训练需要大量的数据。在低数据制度下，参数是不确定的，学习的网络泛化能力很差。该模型基于图像条件生成式对抗网络，从源域中获取数据，并学习从任何数据项中泛化出其他类内数据项。由于该生成过程不依赖于类本身，它可以应用于未见过的新数据类。 我们展示了数据增强生成对抗网络（DAGAN）可以很好地增强标准的香草分类器。我们还展示了DAGAN可以增强少数学习系统，如匹配网络。 在我们的实验中，我们可以看到在Omniglot（从69%到82%）、EMNIST（从73.9%到76%）和VGG-Face（从4.5%到12%）的低数据体制实验中，准确性增加了13%以上；在Omniglot的匹配网络中，我们看到了0.5%的增加（从96.9%到97.4%）和EMNIST的1.8%增加（从59.5%到61.3%）。
回答关于数据的问题可能需要了解输入X的哪些部分会影响响应Y。通过机器学习模型测试变量之间的关系，可以找到这样的理解。例如，条件随机化测试有助于确定一个变量是否与给定其他变量的响应有关。然而，随机化测试需要用户指定测试统计数据。 我们正式确定了一类适当的测试统计量，当一个特征提供关于响应的信息时，即使其余的特征是已知的，也能保证选择该特征。 我们提供了一个例子来说明完美的预测模型对于实例的特征选择是不够的。我们在几个模拟实验、基因组数据集、医院再入院的临床数据集以及ImageNet的一个类的子集上评估了我们的方法。 我们的方法在各种模拟数据集中的表现优于一些基线，能够识别具有生物学意义的基因，能够选择医院再入院事件的最重要的预测因素，并且能够识别图像分类任务中的显著特征。
监督学习依赖于注释的例子，这些例子被认为是基础真理。但这些标签往往来自于嘈杂的众包平台，如Amazon Mechanical Turk。从业者通常为每个例子收集多个标签，并汇总结果以减少噪音（典型的众包问题）。(1)我们如何才能最好地从嘈杂的工人中学习？(2)我们应该如何分配我们的标签预算，以最大限度地提高分类器的性能？我们提出了一种新的算法，用于从嘈杂的众包数据中联合建模标签和工人质量。交替最小化分几轮进行，从与当前模型的分歧中估计工人质量，然后通过优化损失函数更新模型，考虑当前工人质量的估计。 与以前的方法不同，即使每个例子只有一个注释，我们的算法也能估计工人的质量。我们为用我们的算法学习的模型建立了一个泛化误差界限，并在理论上确定，当工人的质量超过一个阈值时，对许多例子进行一次标注（而不是多次标注）更好。在ImageNet（使用模拟的噪声工人）和MS-COCO（使用真实的众包标签）上进行的实验证实了我们算法的好处。
神经网络会犯错。犯错的原因常常是一个谜，因此神经网络常常被认为是一个黑盒子。在本文中，我们开发了一种方法来解释分类器模型的错误，通过直观地显示必须在图像上添加什么来使其正确分类。 我们的工作结合了对抗性例子、生成模型和基于差异目标传播的修正技术，创造了一种技术，对图像被错误分类的原因进行解释。在本文中，我们解释了我们的方法，并在MNIST和CelebA上进行了演示。
在多任务学习的背景下，具有分支结构的神经网络经常被用来共同处理手头的任务。这种分支网络通常从一些共享层开始，之后不同的任务被分支到自己的层序列中。 可以理解的是，由于可能的网络配置的数量是组合性的，决定共享哪些层和在哪里分支变得很麻烦。以前的工作要么是依靠临时的方法来确定层的共享程度，这是次优的，要么是利用神经结构搜索技术来建立网络设计，这是相当昂贵的。 在本文中，我们超越了这些限制，提出了一种原则性的方法，通过利用所采用的任务的亲和力来自动构建分支的多任务网络。给定一个特定的预算，即可学习参数的数量，所提出的方法产生了架构，其中浅层是任务无关的，而深层则逐渐变得更加针对任务。
最近典型的神经网络设计主要是卷积层，但使结构化高效线性层（SELLs）的技巧还没有适应卷积层的设置。我们提出了一种使用对角矩阵、离散余弦变换（DCTs）和可以使用标准随机梯度方法优化的排列组合来表达卷积层中的权重张量的方法。由这种结构化高效卷积层（SECL）组成的网络优于现有低秩网络，显示出有竞争力的计算效率。
盲目的文档去模糊是文档处理和修复领域的一项基本任务，在光学字符识别系统、法医等方面有广泛的增强应用。由于这个问题是高度不确定的，监督和非监督学习方法都很适合这个应用。 然而，这些提取的特征并不适合于文档图像。我们提出了SVDocNet，一个基于空间递归神经网络（RNN）的端到端可训练的U-Net，用于盲目的文档去污，其中RNN的权重由不同的卷积神经网络（CNN）决定。这个网络在定量措施和定性结果方面都达到了最先进的性能。
与当今计算机视觉深度学习中使用的单体深度架构不同，视觉皮层通过两个功能不同但相互关联的网络来处理视网膜图像：腹侧通路用于处理物体相关信息，背侧通路用于处理运动和变换。 受这种皮层分工以及Magno-和parvocellular系统特性的启发，我们探索了一种无监督的特征学习方法，从自然视频中共同学习物体特征及其变换。我们提出了一种新的卷积双线性稀疏编码模型，（1）允许独立的特征变换，（2）能够处理大型图像。 我们的学习程序利用了自然视频中的平滑运动。我们的结果表明，我们的模型可以以完全无监督的方式直接从自然视频中学习特征组及其变换。学习的 "动态过滤器 "表现出某些等价特性，类似于皮质时空过滤器，并捕获视频帧之间过渡的统计数据。我们的模型可以被视为展示无监督学习初级 "胶囊"（由Hinton及其同事提出用于监督学习）的首批方法之一，并与视觉感知的Lie组方法有密切联系。
 众所周知，基于变异自动编码器或随机网络蒸馏法（RND）的传统分布外（OOD）检测方案为OOD数据赋予了比目标分布更低的不确定性。在这项工作中，我们发现这种传统的新颖性检测方案对模糊的图像也很脆弱。 基于这一观察，我们构建了一个基于RND的新型OOD检测器--SVD-RND，它在训练过程中利用了模糊的图像。我们的检测器很简单，在测试时间上很有效，并且在不同领域都优于基线OOD检测器。进一步的结果表明，SVD-RND比基线学习了更好的目标分布表示。最后，SVD-RND与几何变换相结合，在CelebA领域实现了接近完美的检测精度。
在海量数据集上训练大型深度神经网络在计算上是非常具有挑战性的。最近，人们对使用大批量随机优化方法来解决这个问题的兴趣大增。这个研究领域最突出的算法是LARS，它通过采用分层自适应学习率在几分钟内对ImageNet的ResNet进行训练。 然而，LARS对于像BERT这样的注意力模型表现不佳，这表明它在不同任务中的性能提升是不一致的。在本文中，我们首先研究了一种原则性的层级适应策略，以加速使用大型迷你批次的深度神经网络的训练。 利用这一策略，我们开发了一种新的分层自适应大批量优化技术，称为LAMB；然后我们提供了LAMB以及LARS的收敛分析，表明在一般的非凸设置中收敛到静止点。我们的经验结果表明，LAMB在各种任务中具有优越的性能，如BERT和ResNet-50训练，超参数调整非常少。特别是，对于BERT训练，我们的优化器可以使用32868的非常大的批次大小，而性能没有任何下降。通过将批次大小增加到TPUv3 Pod的内存极限，BERT训练时间可以从3天减少到仅76分钟（表1）。
模型诊断元学习（MAML）被称为强大的元学习方法。然而，由于存在两个学习率，MAML因很难训练而闻名。 因此，在本文中，我们推导出内部学习率$alpha$和元学习率$beta$必须满足的条件，以便MAML通过一些简化收敛到最小值。我们发现$beta$的上限取决于$alpha$，与使用正常梯度下降法的情况不同。 此外，我们还表明，$\beta$的阈值随着$\alpha$接近其自身的上限而增加。这一结果通过对各种少量任务和结构的实验得到了验证；具体而言，我们用多层感知器和卷积神经网络对Omniglot和MiniImagenet数据集进行了正弦回归和分类。 基于这一结果，我们提出了一个确定学习率的准则：首先，寻找最大可能的$alpha$；接下来，根据$alpha$的选择值来调整$beta$。
我们提出了一个神经框架，用于学习相互关联的词组之间的关联，例如在主语-动词-宾语（SVO）结构中发现的关联。我们的模型诱导出一个联合功能特定的词向量空间，在这个空间中，例如合理的SVO构成的向量紧挨着。 该模型甚至在联合空间中也保留了关于词组成员的信息，因此可以有效地应用于许多推理SVO结构的任务。结果表明，用我们的任务独立模型学习的表征组合优于先前工作中的特定任务架构，同时将参数的数量减少了95%。
半导体的制造涉及到蚀刻过程，以去除晶圆上的选定区域。然而，在显微照片中对蚀刻结构的测量严重依赖于耗时的手工程序。传统的图像处理通常需要大量的注释数据，而且性能仍然很差。我们将这一挑战视为分割问题，并使用深度学习方法来检测晶圆蚀刻结构中的物体掩码。 我们尝试用生成对抗网络（GAN）来生成更多的数据，以克服数据集非常有限的问题。我们从互联网上下载了10张4种类型的SEM（扫描电子显微镜）图像，在此基础上进行了实验。我们基于深度学习的方法与图像处理方法相比显示出优越性，测量的平均精度达到96%以上，与地面实况相比，我们所知道的，这是深度学习首次被应用于半导体行业的自动测量。
当考虑到消耗性资源和复杂的资源相互作用（如随时间变化的资源使用）时，活动的产生和安排特别具有挑战性。我们提出了三种方法来确定在存在这种约束的情况下，以时间为基础的计划中活动的有效时间安排间隔。 我们将这些技术应用于行星探测器的唤醒安排问题，在这个问题上，唤醒时间受到现有活动的影响。 我们展示了Probe和Linear算法在经验上优于Max Duration算法，然后我们以经验为基础展示了三种算法的运行时间差异。
一个数据集的分解表示应该能够恢复产生它的基本因素。出现的一个问题是，当基本产生因素具有某种几何结构时，使用欧几里得空间的潜变量模型是否能够产生分解表示。 我们如何解决这个问题？提交给NeurIPS2019 Disentanglement挑战赛第一阶段的作品包括一个具有超球形潜伏空间的扩散变异自动编码器（$Delta$VAE），例如，它可以恢复周期性的真实因素。$Delta$VAE的训练通过纳入证据下限（ELBO）的修改版本而得到加强，用于定制后验近似的编码能力。
最近，基于分类的方法被证明在这项任务上取得了优异的成绩。在这项工作中，我们提出了一个统一的观点，并提出了一个开放集方法来放松当前的泛化假设。此外，我们使用随机仿射变换将基于变换的方法的适用性扩展到非图像数据。
最近大规模语言模型的改进推动了为许多现实世界的应用自动生成句法和语义一致的文本的进展，其中许多进展利用了大型语料的可用性。 本文旨在量化和减少语言模型表现出的偏见。给定一个条件背景（如写作提示）和一个语言模型，我们分析生成的文本的情感是否（以及如何）受到敏感属性（如国名、职业和其他）值变化的影响。 我们通过改编公平机器学习文献中的个人和群体公平性指标来量化这些偏见。在两个不同的语料库（新闻文章和维基百科）上的广泛评估表明，最先进的基于Transformer的语言模型表现出从数据中学习的偏见。 我们提出了嵌入相似性和情感相似性正则化方法，在不牺牲困惑性和语义相似性的情况下提高了个体和群体的公平性指标--这是为开发和部署更公平的语言模型在现实世界的应用迈出的积极一步。
文本文档的主题建模是表示学习中最重要的任务之一。在这项工作中，我们提出了iTM-VAE，它是一个带有变异自动编码器的贝叶斯非参数（BNP）主题模型。 一方面，作为一个BNP主题模型，iTM-VAE可能有无限的主题，并且可以根据数据自动调整主题数量。另一方面，与其他BNP主题模型不同，iTM-VAE的推理是由神经网络建模的，它具有丰富的表示能力，并且可以以简单的前馈方式进行计算。 本文还提出了iTM-VAE的两个变体，其中iTM-VAE-Prod以专家产品的方式对生成过程进行建模，以获得更好的性能；iTM-VAE-G对浓度参数设置先验，使模型能够根据数据自动调整合适的浓度参数。在20News和路透社RCV1-V2数据集上的实验结果表明，提出的模型在困惑度、主题一致性和文档检索任务方面优于现有技术。
在最近的深度学习研究中，知识蒸馏（KD）是一种广泛使用的技术，以获得小型和简单的模型，其性能与大型和复杂的对应模型相当。标准的知识蒸馏往往是耗时的，因为获得教师模型的训练时间，然后为学生模型提供指导。 为了改善这一点，我们提出了一个新的知识蒸馏框架，利用整个训练集的暗知识。在这个框架中，我们提出了一个简单而有效的实现，名为利用同伴样本蒸馏（DUPS）的一代。我们在许多实验中验证了我们的算法。与现代架构上的标准训练相比，DUPS在各种任务上实现了1%-2%的平均改进，而且几乎没有额外成本。考虑到一些典型的知识蒸馏方法更耗时，我们使用DUPS也可以得到相当甚至更好的性能。
我们开发了一种用于学习分层结构政策的金属学习方法，通过使用共享基元--即在大量时间步长中执行的政策，提高了对未见任务的采样效率。具体而言，一组基元在任务分布中被共享，并由特定任务的政策在两者之间切换。 然后，我们提出了一种算法，通过使用任何现成的强化学习方法，通过重复采样新任务和重置特定任务的策略，端到端地解决这个问题。我们成功地发现了四条腿的机器人定向运动的有意义的运动基元，仅仅通过与迷宫的分布进行互动。我们还证明了基元的可转移性，以解决长时间尺度的稀疏奖励障碍课程，我们使三维仿人机器人能够用相同的策略稳健地行走和爬行。
本文提出了一种新的文档嵌入模型。现有的方法要么需要复杂的推理，要么使用难以并行化的递归神经网络。我们采取了不同的路线，利用语言建模的最新进展来开发卷积神经网络嵌入模型。这使我们能够训练完全可并行化的更深的架构。 将各层堆叠在一起，增加了感受性，使每个连续的层能够在文件中建立越来越长范围的语义依赖模型。从经验上看，我们在两个公开的基准上证明了卓越的结果。
我们证明了卷积网络泛化误差的界限。这些界限是以训练损失、参数数量、损失的Lipschitz常数和权重与初始权重的距离为条件的。它们与输入的像素数以及隐藏特征图的高度和宽度无关。我们用CIFAR-10以及一个深度卷积网络的不同参数进行了实验，将我们的界限与实际的泛化差距进行比较。
近年来，计算机视觉神经网络MobileNets系列在设计和组织资源效率架构方面取得了巨大的进展。在高度受限的设备中，具有严格实时性要求的新应用需要进一步压缩类似MobileNets的已计算效率网络。 模型量化是一种广泛使用的技术，用于压缩和加速神经网络推理，先前的工作已经将MobileNets量化到4-6位，尽管精度略有下降。虽然量化到子字节值（即精度为8位）已经很有价值，但甚至进一步将MobileNets量化到二进制或三元值是必要的，以实现显著的能源节约和可能在专用硬件，如ASIC和FPGA上的运行速度。 根据深度神经网络每一层的卷积滤波器可能对三元量化有不同的反应这一关键观察，我们提出了一种新的量化方法，该方法为MobileNets生成由全精度和三元权重滤波器组成的每层混合滤波器库。 使用这种拟议的量化方法，我们将MobileNets的大部分权重过滤器量化为三元值，从而节省了27.98%的能量，并将模型大小减少了51.07%，同时与基线全精度MobileNets相比，在专门的硬件上实现了相当的精度和不降低的吞吐量。
在噪声数据上进行控制性实验，对于彻底了解不同噪声水平的深度学习至关重要。由于缺乏合适的数据集，以前的研究只在受控的合成噪声上研究深度学习，而真实世界的噪声从未在受控环境中进行系统研究。为此，本文在10个受控噪声水平上建立了真实世界噪声标签的基准。由于真实世界的噪声拥有独特的属性，为了了解其差异，我们在各种噪声水平和类型、架构、方法和训练设置上进行了大规模研究。我们的研究表明。(1) 深度神经网络(DNN)在真实世界的噪声中的概括性要好得多。(2) DNN可能不会在真实世界的噪声数据中首先学习模式。(3) 当网络被微调时，ImageNet架构在噪声数据中的概括性很好。 (4)真实世界的噪声似乎危害较小，但对于鲁棒的DNN方法来说，它更难改善。(5)在合成噪声上效果好的鲁棒学习方法可能在真实世界的噪声上效果不好，反之亦然。我们希望我们的基准以及我们的发现将促进噪声数据的深度学习研究。
设计RNA分子最近在医学、合成生物学、生物技术和生物信息学方面引起了兴趣，因为许多功能性RNA分子被证明参与了转录、表观遗传学和翻译的调节过程。 由于RNA的功能取决于它的结构特性，RNA设计问题就是要找到一个满足给定结构约束的RNA序列。LEARNA使用深度强化学习来训练一个策略网络，以便在给定的目标结构下按顺序设计整个RNA序列。 通过在20个CPU核心上对65000个不同的RNA设计任务进行元学习一小时，我们的扩展Meta-LEARNA构建了一个RNA设计策略，可以开箱即用，以解决新的RNA设计任务。在方法上，我们认为这是第一次在策略网络、训练程序的超参数和决策过程的表述的丰富架构空间上进行联合优化。 在两个广泛使用的RNA设计基准以及我们引入的第三个基准上的综合经验结果表明，我们的方法在前者上实现了新的最先进的性能，同时在达到以前的最先进的性能方面也快了几个数量级。在一项消减研究中，我们分析了我们方法的不同组成部分的重要性。
修剪是一种流行的压缩神经网络的技术：一个大型的预训练网络被微调，同时连接被连续删除。然而，修剪的价值在很大程度上逃避了审查。在这个扩展摘要中，我们检查了通过Fisher-pruning获得的残余网络，并提出了两个有趣的意见。 第二，通过修剪过程获得的架构--而不是学习到的权重--被证明是有价值的。这样的架构在从头开始训练时是强大的。此外，这些架构很容易近似，不需要进一步修剪：我们可以修剪一次，获得一系列新的、可扩展的网络架构，以满足不同的内存需求。
监督学习问题--特别是那些涉及社会数据的问题--往往是主观的。也就是说，人类读者在看同样的数据时，可能会根据他们的个人经验得出合法但完全不同的结论。然而，在机器学习环境中，来自多个人类注释者的反馈往往被简化为一个单一的 "地面真相 "标签，从而隐藏了在社会光谱中发现的对数据的真实、潜在的丰富和多样化的解释。我们探索发现和学习大量人类群体的标签意见的代表性分布的回报和挑战。 这种方法的一个主要的关键成本是需要人类提供足够的标签，不仅是为了获得有代表性的样本，也是为了训练机器在未标记的数据上预测有代表性的分布。我们建议将标签分布聚集在一起，不仅是个人，还有数据项，以便最大限度地减少人类在循环中的成本。我们在最先进的深度学习模型上测试了不同的聚集方法。我们的结果表明，仔细的标签聚集方法可以大大减少获得代表性分布所需的样本数量。
最近深度学习技术的进步，如卷积神经网络（CNN）和生成对抗网络（GAN）在语义图像绘制问题上取得了突破性进展，这是重建给定图像中缺失像素的任务。虽然比传统方法更有效，但深度学习模型需要大型数据集和大量计算资源进行训练，而且当训练数据的大小和多样性不同时，绘制质量也有很大差异。 为了解决这些问题，我们在本文中提出了一种inpainting策略，即textit{Comparative Sample Augmentation}，它通过过滤掉不相关的图像，并利用要inpainting的图像周围区域的信息构建额外的图像，来提高训练集的质量。在多个数据集上的实验表明，我们的方法将深度inpainting模型的适用性扩展到不同规模的训练集，同时对一大类深度模型保持由定性和定量指标衡量的inpainting质量，几乎不需要针对模型进行考量。
生成对抗网络（GANs）是一个生成模型系列，它不需要最小化单一的训练标准。与其他生成模型不同，数据分布是通过生成器（生成模型）和判别器（提供训练信号的教师）之间的博弈来学习的，它们各自最小化自己的成本。GANs被设计为达到一个纳什均衡，在这个均衡中，每个玩家在不改变其他玩家的参数的情况下不能降低他们的成本。 GANs理论的一个有用的方法是表明训练分布和模型分布之间的发散在均衡时获得其最小值。最近的几个研究方向是由这种发散是学习过程的主要指导，每一步学习都应该减少发散的想法所激发的。 在GAN训练过程中，鉴别器在分布之间的发散梯度没有用的情况下提供了学习信号。我们为GAN训练为发散最小化的观点提供了经验上的反例。具体来说，我们证明GAN能够在发散最小化观点预测他们会失败的情况下学习分布。 我们还表明，从发散最小化的角度出发的梯度惩罚在应用于发散最小化观点所预测的其他情况下也同样有帮助。这为越来越多的证据做出了贡献，即GAN训练可能被视为通过轨迹接近纳什均衡，而不一定在每一步都使特定的发散最小化。
测量来自观察样本的高维、连续、随机变量之间的相互信息（MI）具有广泛的理论和实践应用。最近的工作通过可证明的低偏差近似和紧密的变异下限开发了准确的MI估计，假设有大量的样本供应，但需要不现实的样本数量来保证估计的统计意义。 在这项工作中，我们专注于提高数据效率，并提出了一个数据高效的MINE估计器（DEMINE），通过在MINE下限中加入交叉验证，可以在有限的数据下提供MI的紧密下限置信区间（Belghazi et al, 2018）。采用了超参数搜索，并开发了一种具有任务增强功能的新型元学习方法，以提高对超参数的鲁棒性，减少过拟合并提高准确性。随着数据效率的提高，我们的DEMINE估计器能够在实际数据集大小上进行依赖性的统计测试。
在目前的工作中，语言和视觉被当作两种不同的模式来处理，然而，最近关于超级字符方法的工作显示了二维词汇嵌入的有效性，它将文本分类问题转换为图像分类问题。 在本文中，我们提出了SuperCaptioning方法，它借用了Super Characters方法中的二维词汇嵌入的思想，并在一个单一的CNN模型中共同处理语言和视觉信息。在Flickr30k数据上的实验结果表明，所提出的方法能给出高质量的图像标题。
确定训练期间向深度神经网络提交数据实例的最佳顺序是一个非难事。 在本文中，我们提出了一个融合了自定进度学习（SPL）的深度度量学习（DML）框架，我们称之为自适应进度的学习嵌入（LEAP）。我们的方法是根据突出特征表示空间中的样本的textit{easiness}和textit{true diverseness}来动态地设置迷你批次的参数。在LEAP中，我们训练了一个卷积神经网络（CNN），通过使用磁损失（Magnet Loss）的自适应密度判别来学习一个有表现力的表示空间。CNN分类器根据交叉熵损失中的 "简单性 "和 "真实多样性"，从由 "嵌入 "CNN雕刻的表示空间中动态地选择样本，形成一个小批量。我们使用深度CNN架构评估了LEAP在MNIST、FashionMNIST、CIFAR-10、CIFAR-100和SVHN上的监督图像分类任务。我们表明，LEAP框架在每个数据集上实现可比或更好的测试性能所需的迷你批次更新数量方面收敛得更快。
传统的深度强化学习通常在每个时间点确定一个适当的原始行动，这需要大量的时间和精力来学习有效的策略，特别是在大型和复杂的环境中。为了从根本上解决这个问题，我们将宏观行动（定义为原始行动的序列）纳入原始行动空间，形成一个增强的行动空间。 使用适当的增强行动空间的代理能够跳到更远的状态，从而加快探索过程，并促进学习过程。在以前的研究中，宏观行动是通过挖掘最常用的行动序列或重复以前的行动来开发的。 然而，最常用的动作序列是从过去的政策中提取的，这可能只会加强该政策的原始行为。另一方面，重复动作可能会限制代理人行为的多样性。相反，我们建议通过遗传算法构建宏观动作，这消除了宏观动作推导程序对代理人过去政策的依赖。 我们的方法是每次将一个宏观行动附加到原始行动空间中，并评估增强后的行动空间是否能带来有希望的性能。  我们进行了广泛的实验，并表明所构建的宏观行动能够加速各种深度强化学习方法的学习过程。我们的实验结果还表明，我们的方法所建议的宏观行动在深度强化学习方法和类似环境中是可以转移的。我们进一步提供了一套全面的消融分析来验证我们的方法。
神经科学和生命科学中的一个关键问题是，数据生成过程通常最好被认为是动态系统的层次结构。其中一个例子是体内钙成像数据，观察到的钙瞬态是由电化学动力学的组合驱动的，其中围绕流形的假设轨迹决定了这些瞬态的频率。 最近一种使用顺序变异自动编码器的方法表明，有可能从被建模为泊松过程的尖峰数据中学习到达到行为的潜在动态结构。在这里，我们使用阶梯方法来推断驱动钙瞬变的尖峰事件以及更深层次的潜在动态系统，从而扩展了这种方法。
尽管最近神经机器翻译（NMT）在标准基准中取得了成功，但对于许多语言对来说，缺乏大型的平行语料库是一个主要的实际问题。已经有一些建议来缓解这个问题，例如三角化和半监督学习技术，但它们仍然需要一个强大的跨语言信号。 我们的模型建立在最近关于无监督嵌入映射的工作基础上，由一个稍加修改的注意力编码器-解码器模型组成，可以单独使用去噪和回译的组合对单语语料进行训练。 尽管方法简单，但我们的系统在WMT 2014法译英和德译英中分别获得了15.56和10.21的BLEU分。该模型还可以从小型平行语料库中获益，当与100,000个平行句子结合时，分别获得了21.81和15.24分。我们的实现作为一个开源项目发布。
我们描述了一种新的生成式对抗网络的训练方法。关键的想法是逐步增长生成器和鉴别器：从低分辨率开始，随着训练的进行，我们增加了新的层来模拟越来越精细的细节。我们还提出了一个简单的方法来增加生成图像的变化，并在无监督的CIFAR10中取得了8.80分的创纪录得分。此外，我们描述了几个实施细节，这对阻止生成器和判别器之间的恶性竞争很重要。
为球形神经网络设计卷积需要在效率和旋转等值之间进行微妙的权衡。DeepSphere是一种基于离散球体的图形表示的方法，在这两个要求之间取得了可控的平衡。 首先，我们从理论上和经验上研究了等值性如何受到像素和邻居数量方面的基础图的影响。其次，我们在相关问题上评估了DeepSphere。实验显示了最先进的性能，证明了这种表述的效率和灵活性。
静态平衡集合的概念在统计力学中发挥了核心作用。在机器学习中也是如此，训练作为广义的平衡，促使模型参数的概率分布向静止性发展。这里，我们推导出静止的波动-消散关系，将可测量的数量和随机梯度下降算法中的超参数联系起来。 这些关系对任何静止状态都是准确的，特别是可以用来自适应地设置训练计划。我们可以进一步利用这些关系来有效地提取与损失函数景观有关的信息，如其Hessian的大小和非谐波性。我们的主张得到了经验验证。
递归神经网络（RNN）很难在序列处理任务上进行训练，这不仅是因为输入噪声可能通过反馈被放大，而且还因为权重中的任何不准确都会产生与输入噪声类似的后果。我们描述了一种在训练期间对隐藏状态进行去噪的方法，以实现更稳健的表征，从而提高泛化性能。 这种状态去噪的循环神经网络（SDRNN）对每个外部序列步骤进行多步骤的内部处理。在一系列任务中，我们表明SDRNN优于一般的RNN以及SDRNN的一个变体，该变体在隐藏状态上有吸引子动态，但没有辅助损失。 我们认为，吸引子动力学---以及相应的连接性约束---是深度学习武器库的一个重要组成部分，不仅应该被用于递归网络，还应该被用于改进深度前馈网络和任务间转移。
我们考虑在输入驱动的环境中进行强化学习，其中一个外生的、随机的输入过程会影响系统的动态。输入过程出现在许多应用中，包括排队系统、有干扰的机器人控制和物体跟踪。由于状态动态和奖励取决于输入过程，仅状态就能为预期的未来回报提供有限的信息。 我们推导出一个无偏差的、依赖输入的基线来减少这种变异，并分析表明它比依赖状态的基线更有优势。我们随后提出了一种元学习方法来克服学习依赖长序列输入的基线的复杂性。我们的实验结果表明，在排队系统、计算机网络和MuJoCo机器人运动等环境中，依赖输入的基线不断提高训练稳定性，导致最终政策更好。
深度网络在分类任务中表现出很好的性能。然而，分类器网络学习的参数通常会抛弃输入的风格信息，而选择与分类严格相关的信息。 我们还展示了如何将这样一个神经网络训练成一个深度多层自动编码器，共同将分类和重构的损失降到最低。我们网络的生成能力表明，当分类正确时，风格记忆神经元与分类器神经元的组合会产生良好的输入重构。我们进一步研究风格记忆的性质，以及它与数字和字母组成的关系。
路由模型是一种条件计算的形式，其中例子通过一个更大的网络中的组件子集进行路由，在最近的工作中显示出有希望的结果。令人惊讶的是，迄今为止，路由模型缺乏重要的属性，如架构多样性和大量的路由决策。架构多样性和路由深度都可以增加路由网络的表示能力。 我们讨论了路由模型中架构多样性的意义，并解释了增加路由深度时容量和优化之间的权衡。在我们的实验中，我们发现在路由模型中增加架构多样性可以显著提高性能，在Omniglot设置上将强基线的错误率降低35%。然而，当扩大路由深度时，我们发现现代路由技术在优化方面很困难。
在众多的应用中，预测依赖于偏微分方程（PDEs）的数值解算器。虽然已经提出了使用深度学习技术，但由于训练数据是通过PDE解算器获得的，所以使用受到了限制。因此，使用仅限于PDE解算器适用的领域，但没有更进一步。我们提出了在小域上进行训练的方法，同时将训练好的模型应用到更大的域上，并通过一致性约束确保解决方案即使在小域的边界上也是有物理意义的。我们在爱尔兰都柏林的空气污染预测模型上展示了这些结果。
我们解决了训练生成对抗网络中的极限循环行为问题，并提出使用乐观镜面体面（OMD）来训练Wasserstein GANs.最近的理论结果表明，乐观镜面体面（OMD）在零和博弈的背景下可以享受更快的后悔率。WGANs正是一个解决零和博弈的背景，同时没有后悔的动力。 此外，我们表明乐观的镜像体面解决了WGANs训练中的极限循环问题。我们正式表明，在双线性零和博弈的情况下，OMD动力学的最后一个迭代收敛到均衡，与GD动力学相反，它必然会循环。我们还通过玩具实例描绘了GD和OMD动力学之间巨大的质量差异，即使GD被修改为最近文献中提出的许多适应方法，如梯度惩罚或动力。 我们将OMD WGAN训练应用于生成DNA序列的生物信息学问题。我们观察到，相对于真实的基础分布，用OMD训练的模型实现了持续较小的KL发散，而不是用GD变体训练的模型。最后，我们介绍了一种新的算法，乐观的亚当，它是亚当的一个乐观变体。
学习用户和项目的良好表征对隐性反馈的推荐至关重要。矩阵分解是通过分解给定的交互矩阵来获得用户和项目的表征的基本思路。然而，现有的基于矩阵分解的方法有一个局限性，即用户嵌入和项目嵌入之间的交互只是通过拟合给定的个人评级值来弱化，这可能会失去潜在的有用信息。 在本文中，我们提出了一种新的增强广义矩阵分解（AGMF）方法，它能够结合用户和项目的历史交互信息来学习用户和项目的有效表征。尽管我们提出的方法很简单，但在四个公共隐性反馈数据集上的广泛实验表明，我们的方法优于最先进的同行。此外，消融研究表明，通过使用多热编码来丰富广义矩阵分解的用户嵌入和项目嵌入，可以实现更好的性能，更快的收敛，以及更低的训练损失。
我们提出了一种无监督的方法来建立序列数据的动态表征，特别是观察到的相互作用。该方法同时获得了输入数据的表征和它的动态，它基于一个由两个层次组成的分层生成模型。在第一层，模型学习表征来生成观察到的数据。在第二层，表征状态编码了低层的动态。该模型被设计成一个贝叶斯网络，在高层次中代表了切换变量，并生成了过渡模型。 该方法在其知识和不确定性的指导下积极探索潜空间，这是通过更新潜变量从预测错误信号反向传播到潜空间来实现的。因此，没有使用编码器或推理模型，因为生成器也作为它们的逆向转换。对于视频，结果表明，该系统在与所观察到的动作的基本事实高度相关的状态下提取了数据的动态。
激活是一个非线性函数，在深度神经网络的收敛和性能方面起着主导作用。虽然整流线性单元（ReLU）是最成功的激活函数，但它的衍生物在基准数据集上显示出卓越的性能。在这项工作中，我们探索了多项式作为激活函数（阶数为2），可以在给定区间内近似连续实值函数。利用这一特性，主要想法是学习非线性，接受后续的函数可能不是单调的。 虽然有能力学习更合适的非线性，但我们不能忽视这样一个事实，即由于梯度的爆炸，实现稳定的性能是一个挑战--这一点随着顺序的增加而突出。为了处理这个问题，我们引入了动态输入缩放，输出缩放，以及较低的多项式权重学习率。
我们介绍了CBF，一种在没有奖励或剧情结束信号的情况下工作的探索方法。CBF是基于内在的奖励，来自于在特征空间中运行的动力学模型的误差。它受到（Pathak等人，2017）的启发，很容易实现，并且可以实现诸如通过四个级别的超级马里奥兄弟，导航VizDoom迷宫和通过两个级别的SpaceInvaders的结果。我们调查了该方法与几个辅助任务相结合的效果，但发现比CBF基线的改进不一致。
本文关注VAEs对对抗性攻击的鲁棒性。我们强调，传统的VAEs在攻击下是很脆弱的，但最近引入的解除纠缠的方法，如δ-TCVAE（Chen等人。2018）提高了鲁棒性，这一点通过之前提出的各种对抗性攻击得到了证明（Tabacof等人（2016）；Gondim-Ribeiro等人（2018）；Kos等人（2018））。这促使我们开发了Seatbelt-VAE，一种新的分层拆分的VAE，旨在比现有方法对对抗性攻击具有明显的鲁棒性，同时保留高质量的重建结果。
逆传播算法是人工神经网络中信用分配的事实标准，因为它的经验结果。自其诞生以来，逆传播算法的变体已经出现。更具体地说，变体利用逆传播方程中的函数变化来满足其特定的要求。反馈对准就是这样一个例子，它用一个随机矩阵取代了逆传播方程中的权重转置矩阵，以寻求一种更符合生物原理的信用分配算法。 在这项工作中，我们表明反向传播程序中的函数变化相当于给人工神经网络增加了一个隐性学习率。此外，我们在反向传播方程中学习激活函数导数，以证明这些人工神经网络的早期收敛。我们的工作报告了在MNIST和CIFAR10上足够大的深度神经网络架构上具有早期收敛的竞争性能。
无监督文本风格转移是指在不使用源风格和目标风格句子的平行语料库进行训练的情况下，将给定风格的文本改写成目标风格的任务。风格转移系统被评估为生成以下句子的能力：1）拥有目标风格，2）发音流畅自然，3）保留源句的非风格部分（内容）。 我们训练了一个基于强化学习（RL）的无监督风格转换系统，该系统包含了对上述措施的奖励，并描述了新的奖励塑造方法。我们的方法并不试图将风格和内容分开，而是利用大规模预训练的语言模型以及转换器的力量。我们的系统在各种数据集上，基于人类和自动评估目标风格、流畅性和内容保留以及风格转换的整体成功，明显优于现有的先进系统。
尽管生成对抗网络（GANs）在图像合成方面取得了成功，但人们对网络在深度生成表征中所学习到的东西以及如何从随机噪声中构成照片般真实的图像缺乏足够的了解。 在这项工作中，我们表明高度结构化的语义层次从生成表征中出现，作为合成场景的变化因素。通过在不同的抽象层次上用广泛的视觉概念探测层级表征，我们能够量化激活和输出图像中出现的语义之间的因果关系。 这样的量化确定了由GANs学习的人类可理解的变化因素，以构成场景。定性和定量的结果表明，由GAN学习的生成表征专门用于合成不同层次的语义：早期层倾向于确定空间布局和配置，中间层控制分类对象，后期层最终呈现场景属性以及色彩方案。确定这样一组可操作的潜在语义有利于语义场景的操作。
在分子的SMILES字符串和基于图的表示上定义的变异自动编码器（VAEs）有望改善分子特性的优化，从而彻底改变制药和材料行业。然而，这些VAEs受到SMILES字符串的非唯一性和图卷积的计算成本的阻碍。 为了有效地沿着分子图的所有路径传递信息，我们使用一组堆叠的递归神经网络对单个分子的多个SMILES字符串进行编码，在SMILES表征之间协调每个原子的隐藏表征，并使用注意集合来建立最终的固定长度的潜在表征。 通过解码到分子的SMILES字符串的不连续集合，我们的All SMILES VAE在先验的高概率质量子空间附近学习了分子和潜表征之间的几乎双向映射。我们的SMILES衍生的但基于分子的潜表征在各种完全和半监督的属性回归和分子属性优化任务中大大超过了最先进的水平。
我们提出了一种简单而高效的方法，解决了条件生成对抗网络（cGAN）中的模式崩溃问题。虽然条件分布在实践中是多模式的（即有许多模式），但大多数cGAN方法倾向于学习一个过于简化的分布，即输入总是映射到一个单一的输出，而不考虑潜伏代码的变化。为了解决这个问题，我们建议明确规范化生成器，以根据潜伏代码产生不同的输出。 此外，对生成器的显式正则化使我们的方法能够控制视觉质量和多样性之间的平衡。我们在三个条件生成任务上证明了我们的方法的有效性：图像到图像的翻译、图像绘画和未来视频预测。我们表明，将我们的正则化简单地添加到现有模型中会导致令人惊讶的多样性生成，大大超过了以前专门为每个单独任务设计的多模式条件生成方法。
转化器是一个最先进的神经翻译模型，它使用注意力来迭代完善词汇表征，并从周围环境中获取信息。词汇特征被送入第一层，并通过隐藏层的深度网络传播。 这使模型能够动态地访问相关的词汇内容，而不需要花费有限的资源将其存储在中间状态中。我们表明，拟议的修改在标准的WMT翻译任务上产生了一致的改进，并减少了沿着隐藏层传递的词汇信息量。我们进一步评估了将词汇连接整合到转化器架构中的不同方式，并提出了探索拟议捷径对模型行为影响的消融实验。
概率密度估计是一个经典的、研究得很好的问题，但标准的密度估计方法在历史上缺乏对复杂和高维图像分布的建模能力。 最近的生成模型利用神经网络的力量来隐含地学习和表示复杂图像的概率模型。 我们描述了从GANs中提取明确的概率密度估计的方法，并探讨了这些图像密度函数的属性。 我们进行了理智检查实验，以提供证据表明这些概率是合理的。 然而，我们也表明，自然图像的密度函数很难解释，因此在使用上受到限制。 我们研究了这种缺乏可解释性的原因，并建议我们可以通过对图像的潜在表征进行密度估计来获得更好的可解释性。 
卷积神经网络（CNN）由多个卷积层组成，在视觉任务中表现出优雅的性能。常规卷积的设计是基于感受场（RF），在这里处理特定区域内的信息。从常规卷积的RF来看，具有较小RF的低层神经元的输出被捆绑在一起，形成具有较大RF的高层神经元。因此，即使低层的神经元只能看到局部信息，高层的神经元也能捕捉到全局背景。然而，在生物脑的低层，RF以外的信息会改变神经元的属性。在这项工作中，我们扩展了常规卷积，并提出了空间随机卷积（ss卷积）。 在ss卷积中，常规卷积能够通过空间洗牌使用其RF之外的信息，这是一个简单而轻量级的操作。我们在CIFAR-10和ImageNet-1k数据集上进行了实验，结果显示ss卷积提高了各种CNN的分类性能。
我们提出了一个框架，对来自已知拓扑结构的图中连接的一组实体的顺序数据分布进行建模。该方法基于共享隐马尔可夫模型（HMMs）的混合物，这些模型的训练是为了利用图结构的知识，并以这样的方式使得到的混合物趋于稀疏。在不同应用领域的实验证明了该方法的有效性和通用性。
为了在多代理场景中获得高回报，有时需要了解其他代理并做出相应的最优决策。我们可以通过首先为其他代理建立模型，然后用这些模型找到最优策略来解决这些任务。为了得到一个准确的模型，需要许多观测数据，这可能导致样本效率低下。 在许多实际情况下，我们面临的每个代理可以被视为来自具有固定但未知分布的群体的样本。因此，我们可以将针对某些特定代理的任务视为从任务分布中抽取的任务。我们应用元学习方法来建立模型和学习策略。
我们描述了与标准二维多通道卷积层相关的线性变换的奇异值，使其能够有效计算。 这一特征也导致了一种将卷积层投射到算子规范球上的算法。我们表明这是一个有效的规范化器；例如，它将CIFAR-10上使用批量规范化的深度残差网络的测试误差从6.2%提高到5.3%。
在未知环境中进行探索和利用的交易是在学习过程中实现预期收益最大化的关键。贝叶斯最优策略，这样做是最理想的，它的行动不仅取决于环境状态，还取决于代理人对环境的不确定性。 在本文中，我们介绍了variational Bayes-Adaptive Deep RL（variBAD），这是一种在未知环境中进行近似推理的元学习方法，并在行动选择过程中直接纳入任务的不确定性。在一个网格世界领域，我们说明了variBAD如何作为任务不确定性的函数进行结构化的在线探索。
在一个持续学习的环境中，新的类别可能会随着时间的推移而被引入，一个理想的学习系统应该在原始类别和新类别上都有良好的表现。虽然深度神经网络在经典环境中取得了巨大的成功，但如果在当前的学习情节中遇到的例子与之前的情节中遇到的例子有很大的不同，它们就会忘记在之前的学习情节中获得的知识。 在本文中，我们提出了一个新的模型，它既能利用深度神经网络的表达能力，又能在引入新的类别时对遗忘有弹性。我们证明了与普通的深度神经网络相比，在原始类别上的准确性有所提高。
生物医学知识库在现代数据驱动的生物医学科学中至关重要，但自动匹配的生物医学知识库构建仍然具有挑战性。在本文中，我们考虑了疾病实体规范化的问题，这是构建生物医学知识库的一项重要任务。 我们提出了NormCo，一个深度一致性模型，它考虑了实体提及的语义，以及单个文档中提及的主题一致性。NormCo使用一个简单的语义模型来模拟实体提及，该模型由词嵌入组成短语表示，并使用RNN将一致性视为疾病概念共同提及序列，而不是对文档中所有概念的联合概率进行建模，这需要NP-hard推理。 为了克服数据稀少的问题，我们使用了远距离监督的数据和从BioASQ数据集衍生出来的先验数据生成的合成数据。 我们的实验结果表明，NormCo在两个疾病规范化语料库上的预测质量和效率都优于最先进的基线方法，并且在标签文档上的准确性和F1得分方面至少有同样的表现。
我们探索了乘法交互的作用，将其作为一个统一的框架来描述一系列经典和现代的神经网络架构模式，如门控、注意层、超网络和动态卷积等。乘法交互层作为原始操作在文献中早已存在，尽管这往往没有被强调，因此也没有被充分重视。 我们首先表明，这种层严格地丰富了神经网络的可表示函数类。我们猜想，当融合多个信息流或需要条件计算时，乘法交互提供了一个特别强大的归纳偏向。 最后，我们支持我们的主张，并通过在大规模复杂的RL和序列建模任务中的应用来证明乘法交互的潜力，在这些任务中，乘法交互的使用使我们能够提供最先进的结果，从而提供新的证据，支持乘法交互在设计新的神经网络架构时发挥更突出的作用。
开发用于文本到视频合成的条件生成模型是机器学习中一个极具挑战性但又很重要的研究课题。在这项工作中，我们通过引入文本过滤调节生成对抗网络（TFGAN）来解决这个问题，TFGAN是一个具有新型调节方案的GAN模型，有助于改善文本-视频关联。 通过这种调节方案和深度GAN架构的结合，TFGAN在非常具有挑战性的现实世界视频数据集上从文本中生成了照片般真实的视频。此外，我们构建了一个移动形状的基准合成数据集来系统地评估我们的调节方案。广泛的实验表明，TFGAN明显优于现有的方法，并且还可以生成训练期间未见的新类别视频。
过度参数化在如今的神经网络训练中无处不在，以利于寻求全局最优的优化和减少预测误差的泛化。然而，在许多现实世界的应用中需要压缩网络，直接训练小网络可能会陷入局部最优。 在本文中，我们提出了一种基于逆尺度空间差分内含的新方法，而不是修剪或提炼过度参数化的模型，该方法通过耦合梯度下降和镜像下降来生成从简单到复杂的模型系列，以探索模型结构的稀疏性。 它有一个简单的离散化，称为分裂线性化布雷格曼迭代（SplitLBI），其在深度学习中的全局收敛分析是建立在从任何初始化，算法迭代收敛到经验风险的临界点。实验证据表明，SplitLBI可能在ImageNet-2012数据集等大规模训练中取得最先进的性能。实验证明，SplitLBI在ImageNet-2012数据集的大规模训练中可以达到最先进的性能，而使用emph{early stopping}，在重新训练而不是修剪训练好的模型后，它可以揭开有效的子网架构，具有与密集模型相当的测试准确率。
在本文中，我们研究了用于解决稀疏编码问题的学习型迭代收缩阈值算法（LISTA）。 遵循先前工作的假设，我们首先发现其估计中的代码成分可能低于预期，即需要增益，为了解决这个问题，然后引入了一个可供理论分析的门控机制。门控的具体设计受到机制收敛分析的启发，因此其有效性可以得到正式的保证。
用于图像分类的分层表征的学习经历了一系列令人印象深刻的成功，部分原因是可以获得大规模的标记数据进行训练。另一方面，训练好的分类器传统上是在少数测试图像上进行评估的，这些图像被认为在所有自然图像的空间中分布极为稀疏。 因此，最近在过度重复使用的测试集上的性能改进是否能推广到具有更丰富内容变化的真实世界的自然图像上是值得怀疑的。此外，关于对抗性学习的研究表明，不费吹灰之力就能构造出几乎愚弄所有图像分类器的对抗性例子，为现有模型的相对性能比较增加了更多的复杂性。 这项工作提出了一个比较图像分类器的有效框架，我们将其命名为最大差异（MAD）竞赛。我们不是在固定的测试集上比较图像分类器，而是从一个任意大的未标记图像的语料库中自适应地抽取一个测试集，以便使分类器之间的差异最大化，该差异由WordNet层次结构的距离来衡量。 在由此产生的小型和依赖模型的图像集上的人类标签揭示了竞争分类器的相对性能，并提供了关于改进它们的潜在方法的有用见解。我们报告了11个ImageNet分类器的MAD竞争结果，同时指出该框架是容易扩展的，并具有成本效益，可以将未来的分类器加入竞争中。
神经网络的鲁棒性最近被对抗性例子所强调，也就是说。在本文中，我们设计了一个新的CNN架构，它本身具有良好的鲁棒性。我们引入了一个简单而强大的技术--随机掩码，来修改现有的CNN结构。 我们表明，带有随机掩码的CNN在不应用任何对抗性训练的情况下，实现了最先进的对抗黑盒对抗性攻击的性能。接下来，我们研究了 "愚弄 "带有随机掩码的CNN的对抗性例子。
有监督的深度学习方法需要干净标记的大规模数据集，但收集这样的数据很困难，有时甚至不可能。存在两个流行的框架来缓解这个问题：半监督学习和对标签噪声的鲁棒性学习。虽然这些框架放松了监督学习的限制，但它们是独立研究的。因此，当只有少量干净标记的数据可用时，适合的训练方案仍然未知。 在这项研究中，我们认为从双质量数据中学习是这些研究的概括，其中一小部分数据是干净的标签，其余的是腐败的。在这个框架下，我们比较了最近的半监督和稳健学习的算法。结果表明，半监督学习优于有噪声标签的稳健学习。我们还提出了一种混合混合技术的训练策略，以有效地从这种双质量数据中学习。
分层稀疏编码（HSC）是一个强大的模型，可以有效地表示多维、结构化的数据，如图像。解决这个计算困难的问题的最简单方案是将其分解为独立的分层子问题。然而，神经科学证据表明，这些子问题的相互连接，如预测编码（PC）理论，它在连续层之间增加了自上而下的连接。 在这项研究中，引入了一个名为稀疏深度预测编码（SDPC）的新模型来评估这种层间反馈连接的影响。特别是，SDPC与由一连串拉索层组成的层次拉索（Hi-La）网络进行了比较。 我们在3个不同的数据库上训练了一个2层的SDPC和一个Hi-La网络，每层都有不同的稀疏参数。首先，我们表明，由于反馈机制在各层之间传递预测误差，SDPC产生的整体预测误差较低。 第二，我们证明SDPC的推理阶段比Hi-La模型的收敛速度更快。第三，我们表明SDPC也加速了学习过程。最后，在激活概率的支持下，对两种模型字典的定性分析表明，SDPC的特征更通用，信息更丰富。
解释一个深度学习模型可以帮助用户理解它的行为，并让研究人员辨别它的缺点。最近的工作主要集中在为图像分类或视觉问题回答等任务解释模型。 在本文中，我们为图像相似度模型引入了一种解释方法，模型的输出是衡量两个输入的相似度的分数，而不是分类。 在这个任务中，一个解释取决于两个输入的图像，所以标准的方法并不适用。我们提出了一个解释方法，将识别重要图像区域的显著性图与最能解释匹配的属性配对。 我们发现，我们的解释提供了通常不被单单的显著性地图所捕获的额外信息，并且还能提高属性识别这一经典任务的性能。我们的方法的通用能力在来自不同领域的两个数据集上得到了证明，即Polyvore服装和带有属性的动物2。
对抗性例子已被证明是评估神经序列到序列（seq2seq）模型鲁棒性的有效方法，通过对模型的输入施加扰动导致性能的大幅下降。然而，如果这些扰动没有以改变预期输出的方式改变输入的语义，那么它们只能说明模型的弱点。 以机器翻译（MT）为例，我们为seq2seq模型的对抗性攻击提出了一个新的评估框架，将意义保留考虑在内，并证明现有的方法可能一般不会保留意义。基于这些发现，我们为基于词的MT系统的攻击提出了新的约束，并通过人工和自动评估表明，它们产生了更多语义相似的对抗性输入。此外，我们表明，用意义保留的攻击进行对抗性训练对模型在对抗性稳健性方面是有益的，而不会损害测试性能。
我们引入了一种新的归一化技术，该技术表现出批量归一化的快速收敛特性，使用的是层权重的转换，而不是层输出。
我们提出了一个建立实体及其组成的无监督表征的框架，其中每个实体被看作是一个概率分布，而不是一个固定长度的向量。特别是，这种分布在与实体共同出现的上下文中得到支持，并被嵌入一个合适的低维空间。 这使我们能够从最优传输的角度考虑表示学习的问题，并利用其众多工具，如Wasserstein距离和Wasserstein中心点。所提出的方法的主要优点包括：(a)通过将实体建模为分布来捕捉不确定性和多义性，(b)利用特定任务的底层几何（有地面成本），(c)同时提供可解释性，有上下文之间的最佳传输概念，(d)易于在现有的点嵌入方法之上应用。 从本质上讲，该框架可用于任何无监督或有监督的问题（关于文本或其他模式）；并且只需要许多问题所固有的共生结构。代码以及预先建立的直方图可在https://github.com/context-mover。
     同时，不那么令人惊讶的是，图像分类器在随机损坏的图像上缺乏人类水平的性能，例如带有加性高斯噪声的图像。在这项工作中，我们表明这是同一潜在现象的两种表现。我们通过几种方式建立这种联系。首先，我们发现，对抗性例子存在于我们期望在损坏的图像上具有相同性能的线性模型的距离尺度。 接下来，我们表明在训练过程中高斯数据增强提高了对小的对抗性扰动的鲁棒性，并且对抗性训练提高了对几种类型的图像损坏的鲁棒性。最后，我们提出了一个与模型无关的上限，即从损坏的图像到其最近的错误的距离给定的测试性能，并表明在实践中我们已经接近实现该界限，因此，进一步提高损坏的图像分布的鲁棒性需要大大减少测试错误。 所有这些都表明，提高对抗性的鲁棒性应该与提高在更普遍和更现实的图像损坏情况下的性能同时进行。这就产生了一个可计算的评价指标，供防御性考虑：噪声图像分布的测试误差。
自然语言表征的最新发展伴随着大型和昂贵的模型，这些模型通过自我监督的预训练来利用大量的一般领域的文本。由于将这种模型应用于下游任务的成本，已经提出了几个关于预训练语言表征的模型压缩技术（Sun et al, 然而，令人惊讶的是，仅仅预训练和微调紧凑模型的简单基线被忽视了。在本文中，我们首先表明，在较小的架构背景下，预训练仍然很重要，而且微调预训练的紧凑模型可以与同期工作中提出的更复杂的方法竞争。 通过广泛的实验，我们更普遍地探索了预训练和蒸馏在两个研究不足的变量下的相互作用：模型大小和未标记任务数据的属性。一个令人惊讶的观察是，即使在同一数据上依次应用，它们也有复合效应。为了加速未来的研究，我们将公开提供我们的24个预训练的微型BERT模型。
在本文中，我们研究了通过权重量化和无损源编码对深度神经网络（DNN）进行有损压缩，以实现内存效率的部署。以前的工作涉及DNN权重的非通用标量量化和熵编码，而我们首次通过通用矢量量化和通用源编码引入通用DNN压缩。 特别是，我们研究了DNN的通用随机晶格量化，它在晶格量化之前通过均匀的随机抖动使DNN权重随机化，并且可以在不依赖概率分布知识的情况下对任何源进行接近最优的执行。 此外，我们提出了一种微调矢量量化DNN的方法，以恢复量化后的性能损失。我们的实验结果表明，所提出的通用DNN压缩方案对32层ResNet（在CIFAR-10上训练）和AlexNet（在ImageNet上训练）的压缩率分别为47.1美元和42.5美元。
本文试图从理论上初步解决VAE的内在维度、真实因子、分解和指标问题，并在现实情况下通过噪声建模的角度实际解决实施问题。 在本征维度问题上，由于信息守恒，理想化的VAE学习并且只学习本征因子维度。此外，根据相互信息分离特性的建议，高斯先验对VAE目标的约束鼓励维度上的信息稀疏性。 在指标问题上，讨论了当前反切指标的行为，随后提出了几个关于反切和生成影响的性能指标，以评估VAE模型的性能和监督所使用的因子。在实施问题上，在噪声建模和约束条件下的实验实证了理论分析，也显示了它们在追求反切方面的特点。
权重衰减是神经网络工具箱中的标准技巧之一，但对其正则化效应的原因了解不多，最近的结果对传统的L_2$正则化解释产生了怀疑。事实证明，对于两者不同的优化器来说，字面的权重衰减优于L_2$正则化。我们对三种优化算法（SGD、Adam和K-FAC）和各种网络结构的权重衰减进行了实证研究。我们确定了三种不同的机制，根据特定的优化算法和结构，权重衰减发挥了正则化作用。(1)增加有效的学习率，(2)使输入-输出雅各布准则近似正规化，以及(3)减少二阶优化的有效阻尼系数。我们的结果为如何改进神经网络的正则化提供了启示。
在本文中，我们提出了第一个用于开发和评估领域适应方法的免费数据集，用于声音事件检测任务。该数据集包含从100美元不同的合成声音事件轨道中提取的40个对数mel-band能量，带有来自九个不同声学场景（来自室内、室外和车辆环境）的附加噪声，在六个不同的声噪比（SNR）（从-12到-27dB，步长为-3dB）下混合，总计5400（9*100*6）个声音文件，总长度为30 564分钟。 我们提供了原样的数据集，重新创建数据集的代码，以及重新混合不同信噪比的声音事件轨道和声学场景的代码，并提供了一个基线方法，以测试拟议数据集的适应性，并建立了一些初步的结果。
本文旨在解决基于变异优化的互信息估计器的局限性。通过使用非广义统计力学的广义函数重新定义成本，我们提高了以前的估计器的上限，并能够控制偏差方差的权衡。基于变异的估计器优于以前的方法，特别是在机器学习设置中发现的高依赖性高维场景。 我们的方法受到非广义统计力学的启发，对分区函数中的对数和指数进行了不同的概括，这使得估计器能够在更大的维度和输入变量的相关性范围内捕获相互信息的变化，而以前的估计器则使其饱和。
生成对抗网络（GANs）是一个广泛使用的学习生成模型的框架。Wasserstein GANs（WGANs）是GANs最成功的变体之一，需要解决一个minmax问题以达到全局最优，但在实践中，用随机梯度下降法成功训练。在本文中，我们表明，当生成器是一个单层网络时，随机梯度下降法在多项式时间和样本复杂性中收敛到一个全局解决方案。
深度神经网络等分类器已被证明在高维输入空间的问题上容易受到对抗性扰动的影响。虽然对抗性训练提高了分类器对这种对抗性扰动的鲁棒性，但它使分类器在不可忽略的一部分输入上对其敏感。 我们认为有两种不同的对抗性扰动：在许多输入上愚弄分类器的共享扰动和只在一小部分数据上愚弄分类器的奇异扰动。我们发现对抗性训练提高了分类器对共享扰动的鲁棒性。 此外，它在消除通用扰动方面特别有效，通用扰动可以被看作是共享扰动的极端形式。不幸的是，对抗性训练并没有持续增加对未见输入的奇异扰动的稳健性。 然而，我们发现，对抗性训练降低了其余扰动对图像变换的鲁棒性，如对比度和亮度的变化或高斯模糊。因此，它使得在物理世界中成功攻击分类器的可能性降低。最后，我们表明，即使是奇异扰动也可以很容易地被检测到，因此必须表现出可概括的模式，即使扰动对某些输入是特定的。
我们解决了高效深度学习模型部署的挑战性问题，其目标是设计能够适应不同硬件平台约束的神经网络架构。大多数传统方法要么是手动设计，要么是使用神经架构搜索（NAS）来寻找专门的神经网络，并为每个案例从头开始训练，这在计算上是昂贵的，而且无法扩展。 我们的关键想法是将模型训练与架构搜索脱钩，以节省成本。为此，我们建议训练一个支持不同架构设置（深度、宽度、核大小和分辨率）的一次性网络（OFA），给定一个部署场景，然后我们可以通过从OFA网络中选择快速获得一个专门的子网络，而无需额外训练。 为了防止训练过程中许多子网络之间的干扰，我们还提出了一种新颖的渐进式收缩算法，它可以同时训练数量惊人的子网络（$> 10^{19}$）。在各种硬件平台（CPU、GPU、mCPU、mGPU、FPGA加速器）上进行的大量实验表明，OFA始终优于SOTA NAS方法（最高4。 0%的ImageNet top1准确率比MobileNetV3高），同时减少了数量级的GPU时间和$CO_2$排放。特别是，OFA在移动设置（$<$600M FLOPs）下实现了新的SOTA 80.0%的ImageNet top1准确率。代码和预训练模型发布在https://github.com/mit-han-lab/once-for-all。
深度生成模型是一种学习数据分布的强大方法，它在许多场景中取得了巨大的成功。然而，单一的生成模型要忠实地捕捉复杂数据的分布是不容易的，如具有复杂结构的图像。任何隐藏变量元模型都可以被利用，只要它能支持似然评价。我们推导出提升模型的可分解变异下限，它允许每个元模型被单独和贪婪地训练。
诸如ELMo（Peters等人，2018a）和BERT（Devlin等人。2018）最近在各种下游NLP任务上取得了最先进的成果。在最近的标记级探测工作的基础上，我们引入了一个新的边缘探测任务设计，并构建了一套广泛的来自传统结构化NLP管道的子句子任务。 我们从最近的四个模型中探测词级上下文表征，并研究它们如何在一系列句法、语义、局部和长程现象中编码句子结构。我们发现，在语言建模和翻译上训练的现有模型对句法现象产生了强大的表征，但在语义任务上只比非语境基线提供了相对较小的改进。
深度强化学习在复杂的游戏中取得了成功，如Atari、Go等。然而，现实世界的决策往往需要用从复杂的视觉观察中提取的部分信息进行推理。本文提出了判别粒子过滤器强化学习（DPFRL），这是一个用于部分和复杂观察的新的强化学习框架。DPFRL在神经网络中用学习的过渡和观察模型编码了一个可区分的粒子过滤器，这允许在多个时间步骤中用部分观察进行推理。 标准的粒子过滤器依赖于生成性观察模型，而DPFRL则学习了一个判别性参数化的模型，直接进行决策训练。我们表明，判别性参数化的结果是性能的显著提高，特别是对于具有复杂视觉观察的任务，因为它规避了明确建立观察模型的困难。 在大多数情况下，DPFRL在Flickering Atari Games（一种现有的POMDP RL基准）和Natural Flickering Atari Games（我们引入的一种新的、更具挑战性的POMDP RL基准）中的表现优于最先进的POMDP RL模型。我们进一步表明，DPFRL在现实世界数据的视觉导航中表现良好。
用辅助潜变量扩展模型是提高模型表现力的著名技术。Bachman & Precup（2015）；Naesseth等人（2018）；Cremer等人（2017）；Domke & Sheldon（2018）表明，重要性加权自动编码器（IWAE）（Burda et al, 2015）可以被看作是对具有辅助潜变量的变异家族的扩展。同样，我们表明这一观点包含了变异界线的许多最新发展（Maddisonet al., 2017; Naesseth et al., 2018; Le et al., 2017; Yin & Zhou, 2018; Molchanovet al, 2018；Sobolev & Vetrov, 2018）。用辅助潜变量丰富变异族的成功促使我们将同样的技术应用于生成模型。我们开发了一个类似于IWAE边界的生成模型，并通过经验表明，它优于最近提出的学习接受/拒绝采样算法（Bauer & Mnih, 2018），同时大大简化了实现。 ; Ma & Collins, 2018）和对比性预测编码（Oord等人，2018）。
随机梯度下降或SGD是大规模问题中最流行的优化算法。SGD通过样本大小为1的均匀抽样来估计梯度。还有一些工作建议通过使用加权非均匀抽样来实现更快的迭代收敛，以获得更好的梯度估计。不幸的是，维持这种梯度估计的自适应分布的每迭代成本比计算全部梯度要高。 在本文中，我们打破了这一障碍，首次展示了一种抽样方案，它导致了卓越的梯度估计，同时保持每迭代的抽样成本与均匀抽样相似。这样的算法是可能的，因为最近出现了局部敏感哈希（LSH）的抽样观点。作为卓越和快速估计的结果，我们减少了所有现有梯度下降算法的运行时间。
近年来，我们在识别支撑神经功能的计算原则方面取得了重大进展。虽然尚未完成，但我们有足够的证据表明，这些想法的综合可以导致对神经计算如何从先天动态和可塑性的组合中产生的理解，并有可能用于构建具有独特能力的新人工智能技术。我讨论了相关原则，它们对计算的优势，以及它们如何有利于人工智能。
最近的工作表明，预测性建模如何赋予代理人对其周围环境的丰富知识，提高他们在复杂环境中的行动能力。我们提出将问题解答作为解码和理解这种代理人开发的表征的一般范式，将我们的方法应用于最近的两种预测性建模方法--行动条件CPC（Guo等人，2018）和SimCore（Gregor等人。在一个视觉丰富的三维环境中，用各种物体、颜色、形状和空间配置来训练代理的预测目标后，我们用大量的合成（英语）问题来探测他们的内部状态表征，而不把梯度从回答问题的解码器反向传播到代理。 我们的方法是直观的，即人类可以很容易地解释模型的反应，而不是检查连续的向量，并且是模型无关的，即适用于任何建模方法。通过揭示代理人在学习过程中获得的关于物体、数量、属性和关系的隐性知识，问题条件代理人探测可以刺激设计和开发更强大的预测性学习目标。
在大多数现实世界的场景中，训练数据集是高度班级不平衡的，深层神经网络在平衡的测试标准中受到影响。在本文中，我们探索了一种新颖而简单的方法，通过用其他班级的对抗性例子来合成不太频繁的班级来缓解这个问题。令人惊讶的是，我们发现这种反直觉的方法可以通过转移和利用大多数信息的多样性来有效地学习少数班级的可概括特征。 我们在图像分类和自然语言处理的各种类型的类平衡数据集上的实验结果表明，与其他重新采样或重新加权的方法相比，所提出的方法不仅显著提高了少数类的泛化能力，而且在类平衡分类方面也超过了其他先进水平的方法。
活性物质由活性剂组成，它们将从周围环境中提取的能量转化为动力，产生各种集体现象。一个模型，由蛋白质马达驱动的微管聚合物组成的合成活性系统自发地形成了一个液晶向列相。蛋白质马达产生的拉伸应力沉淀了微管的连续弯曲和折叠，产生了运动的拓扑缺陷和湍流。 缺陷运动由材料的流变特性决定；然而，这些特性在很大程度上仍未被量化。测量缺陷动态可以产生对活性线状体的基本见解，这类材料包括细菌薄膜和动物细胞。目前的缺陷检测方法缺乏稳健性和精确性，需要对具有不同视觉质量的数据集进行微调。 在这项研究中，我们应用深度学习来训练一个缺陷检测器，以自动分析微管活性线粒体的显微镜视频。 实验结果表明，我们的方法是稳健和准确的。预计它将大大增加可处理的视频数据量。
在这项工作中，我们研究了零点学习（ZSL）的学习表征方面的位置性和组成性。为了很好地隔离这些属性在学习表征中的重要性，我们施加了额外的约束，与最近在ZSL方面的工作不同，没有在不同的数据集（如ImageNet）上进行预训练。我们的实验结果表明，就输入的小部分而言，位置性和组成性，即学习的表征如何能够被表达为较小词汇的函数，都与泛化有深刻的关系，并促使我们在未来表征学习的研究方向上关注更加局部的感知模型。
越来越明显的是，许多机器学习分类器容易受到对抗性例子的影响。在试图解释对抗性例子的起源时，以前的研究通常集中在神经网络在高维数据上运行，他们过度拟合，或者他们太过线性。 这导致对抗性错误具有普遍的缩放性，作为一个幂律，与对抗性扰动的大小有关。我们表明，这种普遍性对广泛的数据集（MNIST、CIFAR10、ImageNet和随机数据）、模型（包括最先进的深度网络、线性模型、对抗性训练的网络和在随机洗牌的标签上训练的网络）和攻击（FGSM、步骤l.l。最后，我们研究了网络架构对对抗性敏感度的影响。为此，我们使用带有强化学习的神经架构搜索，在CIFAR10上寻找对抗性强的架构。
    近年来，强化学习（RL）导致了越来越复杂的外观行为。然而，这种复杂性可能是误导性的，并隐藏着过度拟合。我们发现，视觉表征可能是一个有用的复杂性指标，并且既能很好地关联客观优化，又能因果地影响奖励优化。 然后，我们提出了好奇心表征学习（CRL），它允许我们使用更好的视觉表征学习算法，通过内在目标在模拟环境和转移到真实图像上相应地增加政策中的视觉表征。最后，我们表明由CRL诱导的更好的视觉表征使我们在Atari上获得比其他好奇心目标更好的性能，没有任何奖励。
本文介绍了语义实例完成的任务：从一个场景的不完整的RGB-D扫描中，我们的目标是检测构成场景的单个物体实例，并推断出它们完整的物体几何形状。这使得扫描的场景在语义上被分解为单个完整的三维物体，包括隐藏的和未观察到的物体部分。 这将为与场景中的物体互动开辟新的可能性，例如虚拟或机器人代理。为了解决这个问题，我们提出了3D-SIC，一个新的数据驱动的方法，联合检测物体实例并预测其完整的几何形状。 3D-SIC的核心思想是一种新型的端到端3D神经网络架构，它利用了颜色和几何特征的联合学习。我们的3D网络的完全卷积性质使我们能够在一次前向传递中对大型室内环境的三维扫描的语义实例完成情况进行有效推断。在一个系列评估中，我们对真实和合成扫描基准数据进行了评估，我们在ScanNet上的表现比最先进的方法高出15倍以上，mAP@0.5，在SUNCG上高出18倍。
风格转换通常是指将特定风格图像的颜色和纹理信息应用于给定的内容图像，同时保留后者的结构。这里我们解决的是更通用的语义风格转换问题：给定两个未配对的图像集合，我们旨在学习每个集合的语料库级风格之间的映射，同时保留两个领域中共享的语义内容。 我们引入了XGAN（"Cross-GAN"），一个双重对抗性的自动编码器，它以无监督的方式捕获了共同领域语义内容的共享表示，同时在两个方向上共同学习领域到领域的图像翻译。 我们利用领域适应性文献中的想法，定义了一个语义一致性损失，鼓励模型在学习的嵌入空间中保留语义。我们报告了脸部到卡通翻译任务的有希望的定性结果。我们为此目的收集的卡通数据集也将作为语义风格转移的新基准发布。
在大型数据集上训练神经网络可以通过在机器网络上分配工作量来加速。随着数据集越来越大，由数百或数千台机器组成的网络在经济上是可行的。交流梯度的时间成本限制了使用这种大型机器数量的有效性，网络故障的机会也可能增加。 我们探索了一种特别简单的算法，用于稳健的、具有通信效率的学习--signSGD。工人只向服务器传输他们梯度向量的符号，整体更新由多数投票决定。 在通过实验验证的自然条件下，我们证明signSGD在大批量和小批量设置中收敛，作为副产品建立了Adam参数体系的收敛性。 在实践方面，我们在Pytorch中建立了我们的分布式训练系统。以最先进的集体通信库（NCCL）为基准，我们的框架--参数服务器完全放在一台机器上--导致在使用15台AWS p3.2xlarge机器时，在Imagenet上训练resnet50的时间减少25%。
从显微镜成像中分析细胞表型可以提供由影响细胞的各种因素产生的有意义的生物信息。一个激励性的应用是药物开发：可以从图像中捕捉到细胞的形态特征，从中可以量化不同剂量下应用的不同药物之间的相似度。 一般的方法是找到一个函数，将图像映射到一个可管理维度的嵌入空间，该空间的几何形状可以捕捉到输入图像的相关特征。例如，在同一星期培养和成像的细胞的嵌入向量往往比不同星期的细胞更相关，尽管在这两种情况下使用的药物化合物相同。 在这种情况下，一组实验的特定批次构成了数据的领域；理想的图像嵌入集应该只包含相关的生物信息（如药物效应）。我们开发了一个调整图像嵌入的一般框架，以便在保留相关生物信息的同时 "忘记 "特定领域信息。 为了做到这一点，我们根据每个重复处理的嵌入的边际分布之间的距离（如Wasserstein距离）来最小化损失函数。我们发现，对于我们转换后的嵌入，（1）基本的几何结构不仅被保留，而且嵌入还携带改进的生物信号（2）存在较少的领域特定信息。
本文提出了一种互信息神经估计器（MINE），它在维度以及样本大小上都是线性可扩展的。MINE是可逆的，我们证明它是强一致的。我们说明了少数应用，其中MINE被成功地应用于增强无监督和监督环境下生成模型的属性。我们应用我们的框架来估计信息瓶颈，并将其应用于与监督分类问题有关的任务。
 强化学习方法最近在广泛的控制问题上取得了令人印象深刻的结果。然而，特别是对于复杂的输入，他们仍然需要大量的训练数据，以便收敛到一个有意义的解决方案。这个限制在很大程度上禁止他们用于复杂的输入空间，如视频信号，并且它仍然不可能用于现实世界环境中的一些复杂问题，包括许多基于视频的控制。 在这篇文章中，我们提出了一种无模型的控制方法，它将强化学习和监督学习结合起来用于自主控制，为在现实环境中实现基于策略的控制铺平了道路。我们使用SpeedDreams/TORCS视频游戏来证明我们的方法与最先进的强化学习技术相比，在类似的数据上需要更少的样本（数十万对数百万或数千万），同时在质量上克服了监督和强化学习方法。
研究认知功能的典型实验是训练动物执行任务，同时研究人员记录动物神经元的电活动。当使用这种类型的电生理实验来揭示复杂行为背后的电路机制时，面临的主要障碍是我们无法完全访问大脑中的相关电路。 最近，强化学习模型被用来理解皮质-基底神经节电路的功能，因为在哺乳动物的大脑中发现了基于奖励的学习。在本文中，我们提出了一个生物上可信的演员-批评者与外显记忆（B-ACEM）框架来模拟前额叶皮质-基底神经节-海马（PFC-BG）电路，它被验证为捕获了一个著名的感知决策任务的行为结果，即。这个B-ACEM框架将神经计算与行为联系起来，在此基础上，我们可以探索应该如何考虑外显记忆来支配未来的决策。我们使用不同的外显记忆设置进行了实验，结果表明，所有的外显记忆模式都可以加速学习。
了解深层神经网络（DNNs）的表现力以及它们的结构特性（如：深度、宽度、激活类型）。在一篇开创性的论文中，Telgarsky通过提出一系列函数（基于模拟三角波）来强调深度的好处，对于这些函数，DNNs实现了零分类错误，而节点数少于指数级的浅层网络则会产生恒定错误。 尽管Telgarsky的工作揭示了浅层神经网络的局限性，但它并没有告诉我们为什么这些函数难以表示，事实上，他说这是一个诱人的开放性问题，以确定那些不能被较小深度很好地接近的函数。 在这项工作中，我们指出了DNN的表现力和来自动力系统的Sharkovsky定理之间的新联系，这使我们能够根据固定点的广义概念，即周期点（固定点是周期为1的点）的存在，来描述ReLU网络代表函数的深度-宽度折衷。 我们观察到Telgarsky工作中使用的三角波包含周期为3的点，这一周期很特别，因为它意味着基于Li-Yorke的著名结果的混沌行为。
我们研究了低位量化，以减少基于深度神经网络（DNN）的关键词发现（KWS）的计算成本。我们提出了通过将量化整合到关键词发现模型训练中来进一步减少量化位的方法，我们称之为量化感知训练。 我们在大型数据集上的实验结果表明，量化感知训练可以恢复量化为较低比特表示的模型的性能。通过结合量化感知训练和权重矩阵因子化，我们能够大大减少模型的大小和计算，用于小尺寸的关键词发现，同时保持性能。
单细胞RNA测序（scRNA-seq）是分析生物系统的强大工具。然而，由于生物和技术噪音，量化多种实验条件的影响是一个分析挑战。为了克服这一挑战，我们开发了MELD：潜在维度的增强。MELD利用图信号处理的工具，在数据中学习一个潜在维度，对每个数据点在实验或控制条件方面的原型进行评分。 MELD通过过滤图频域中嘈杂的分类实验标签来学习EES，以恢复具有连续值的平滑信号。这种方法可用于识别不同条件下的标志性基因，并确定哪些细胞类型受特定扰动的影响最大。
用户行为模型是许多规定性环境中的关键输入，可以被看作是将用户可用的状态信息转化为行动的决策规则。高斯过程（GPs）以及其非线性扩展提供了一个灵活的框架，与近似贝叶斯推理一起学习用户模型。 我们提出了决策规则GPs（DRGPs），在一个由决策规则定义的转换空间中应用GPs，这些决策规则对从业者来说具有直接的可解释性。我们在一个实际应用中说明了这种建模工具，并表明结构变异推理技术可以与DRGPs一起使用。
虽然贝叶斯优化（BO）在优化昂贵的评估黑箱函数方面取得了巨大的成功，特别是调整神经网络的超参数，但诸如随机搜索（Li et al, 2016）和多保真BO（如Klein等人（2017）），利用廉价的近似，如在较小的训练数据上训练或用较少的迭代，可以超越只使用全保真观测的标准BO方法。本文中，我们提出了一种新的贝叶斯优化算法，即连续保真知识梯度（cfKG）方法，当保真度由一个或多个连续设置（如训练数据大小和训练迭代次数）控制时，可以使用。 cfKG表征了在给定保真度下对某一点进行采样所获得的信息价值，选择在单位成本价值最大的点和保真度上进行采样。此外，按照Wu等人（2017）的说法，cfKG可以被推广到优化过程中可以使用导数的环境中，例如。 数值实验表明，在优化合成函数、调整CIFAR-10和SVHN上的卷积神经网络（CNN）以及大规模内核学习时，cfKG优于最先进的算法。
只为优化训练精度而训练的神经网络往往会被对抗性例子所愚弄----轻微扰动的输入被高置信度地错误分类。网络的验证使我们能够衡量它们对这种对抗性例子的脆弱性。 在寻找最小的对抗性扭曲的代表性任务上，我们的验证器比最先进的要快两到三个数量级。我们通过对非线性的严格计算，以及充分利用所有可用信息的新型预解算法来实现这一计算速度。 计算速度的提高使我们能够验证具有超过100,000个ReLU的卷积和残差网络的属性---比以前任何完整的验证器所验证的网络多几个数量级。特别是，我们首次确定了MNIST分类器对有界l-âˆž规范δ=0的扰动的确切对抗精度。 1：对于这个分类器，我们为4.38%的样本找到了对抗性例子，并为其余的样本找到了对规范约束扰动的鲁棒性证书。在所有考虑的鲁棒性训练程序和网络结构中，对于MNIST和CIFAR-10数据集，我们能够比最先进的技术认证更多的样本，并找到比强一阶攻击更大的对抗性例子。
不确定性估计是计算机视觉中深度学习模型鲁棒性评估的一个重要步骤，特别是当应用于风险敏感领域时。然而，大多数最先进的深度学习模型要么无法获得不确定性估计，要么需要进行重大修改（例如，制定适当的贝叶斯处理方法）以获得不确定性估计，以前的方法都无法从架子上取下一个任意的模型并产生不确定性估计，而无需重新训练或重新设计它。我们提出了三种简单的、可扩展的方法来分析在可容忍的扰动下训练过的网络的输出方差：infer-transformation、infer-noise和infer-dropout。它们只在推理过程中运行，而不需要重新训练、重新设计或微调模型，这是其他最先进的不确定性估计方法通常要求的。 令人惊讶的是，即使不涉及训练中的这种扰动，与其他需要训练的最先进的方法相比，我们的方法也能产生相当甚至更好的不确定性估计。最后但同样重要的是，我们证明了来自我们所提出的方法的不确定性可以用来改善神经网络的训练。
捕捉时空动态是视频识别中的一个重要课题。在本文中，我们提出了可学习的高阶运算，作为从高维输入视频空间中捕捉高阶关联的通用构件系列。我们证明了几个成功的视觉分类任务的架构都属于高阶神经网络系列，理论和实验分析证明了它们的基本机制是高阶的。 在视频识别任务上，即使只使用RGB而不使用其他视频数据集进行微调，我们的高阶模型也能在Something（V1和V2）和Charades数据集上取得与现有最先进方法相当或更好的结果。
目前最成功的半监督学习方法是基于一致性正则化，即模型被训练成对其输入和参数的小扰动具有鲁棒性。为了理解一致性正则化，我们从概念上探讨了损失几何与训练程序的相互作用。 在这些观察的激励下，我们提议用随机权重平均法（SWA）来训练基于一致性的方法，这是一种最新的方法，通过修改学习率计划，沿着SGD的轨迹平均权重。我们还提出了快速SWA，它通过在循环学习率计划的每个周期内平均多个点来进一步加速收敛。 通过权重平均，我们在CIFAR-10和CIFAR-100上取得了已知的最好的半监督结果，在许多不同数量的标签训练数据上。例如，我们在CIFAR-10上仅用4000个标签就取得了5.0%的误差，而之前文献中的最佳结果是6.3%。
在本文中，我们发现通过设计一个名为 "跟踪损失 "的新型损失函数，基于卷积神经网络（CNN）的物体检测器可以成功地转换为性能良好的视觉跟踪器，而没有任何额外的计算成本。 它还避免了以往研究中提出的特征工程和特征聚合等额外的机械。跟踪损失通过利用检测网络中特征图的内部结构和区别对待不同的特征点来实现这一特性。 这样的结构使我们能够同时考虑判别质量和边界框的准确性，而这一点被认为是成功的关键。我们还提出了一种网络压缩方法，在不降低性能的情况下加快跟踪速度。这也验证了即使网络被大幅压缩，跟踪损失也会保持高度有效。 此外，如果我们采用精心设计的跟踪损失合集，跟踪器将更加稳健和准确。评估结果显示，我们的跟踪器（包括合集跟踪器和两个基线跟踪器），在VOT 2016挑战赛上的预期平均重叠（EAO）和稳健性方面超过了所有最先进的方法。我们将公开提供代码。
我们研究的是语义代码修复的问题，可以广义地定义为自动修复源代码中的非语义错误。过去在语义代码修复方面的大多数工作都假定可以获得单元测试，而候选修复可以得到验证。 相比之下，这里的目标是开发一个强大的统计模型，以准确地预测错误的位置和确切的修复，而不需要获得关于程序的预期正确行为的信息。实现这样的目标需要一个强大的上下文修复模型，我们在一个真实世界源代码的大型语料库上进行训练，该语料库已经被合成注入的错误所增强。 我们的框架采用了一个两阶段的方法，首先由基于规则的处理器生成大量的修复候选者，然后由一个统计模型对这些候选者进行评分，该模型采用了一种新型的神经网络架构，我们称之为 "分享、专业和竞争"。 具体来说，该架构（1）使用抽象语法树上的RNN生成源代码的共享编码，（2）使用专门的网络模块对每个候选修复进行评分，以及（3）然后将这些分数归一，以便它们可以在可比的概率空间中相互竞争。我们在从GitHub收集的真实世界测试集上评估我们的模型，其中包含四类常见的错误。
深度网络最近被认为面临着准确性（在干净的自然图像上）和鲁棒性（在对抗性扰动的图像上）之间的几率（Tsipras等人，2019）.这样的困境被证明是根源于固有的较高的样本复杂性（Schmidt等人。2018）和/或模型容量（Nakkiran，2019），用于学习一个高准确率和稳健的分类器。有鉴于此，给一个分类任务，增加模型容量似乎有助于在准确率和稳健性之间取得双赢，但以牺牲模型大小和延迟为代价，因此对资源有限的应用构成挑战。 本文研究了与输入适应性高效推理相关的多出口网络，显示了它们在共同优化模型准确性、稳健性和效率方面实现 "甜蜜点 "的强大前景。 我们提出的解决方案被称为鲁棒动态推理网络（RDI-Nets），允许每个输入（无论是干净的还是对抗性的）自适应地选择多个输出层中的一个（早期分支或最后一个）来输出其预测。这种多损失的自适应性为对抗性攻击和防御增加了新的变化和灵活性，对此我们进行了系统的调查。 我们通过实验表明，通过给现有的骨干网络配备这种稳健的自适应推理，所产生的RDI-Nets可以实现更好的准确性和稳健性，但与被防御的原始模型相比，可以节省30%以上的计算量。
虽然深度卷积网络在许多自然语言任务中取得了更好的性能，但由于它们难以解释，所以被当作黑盒子。特别是，人们对它们在中间层如何表示语言知之甚少。为了理解在语言任务中训练的深度卷积网络的表示，我们表明个别单元对特定的语素、单词和短语有选择性的反应，而不是对任意的和不可解释的模式作出反应。 为了定量分析这种耐人寻味的现象，我们提出了一种基于单元如何响应复制文本的概念对齐方法。我们在分类和翻译任务的多个数据集上用不同的架构进行了分析，并对深度模型如何理解自然语言提供了新的见解。
我们研究的问题是建立能够分解独立变化因素的模型.这种模型编码的特征可以有效地用于分类，并在图像合成中的不同图像之间转移属性.作为数据，我们使用弱标记的训练集，其中标签表明两个数据样本之间有什么单一的因素发生了变化，尽管变化的相对值是未知的.这种标签是特别有趣的，因为它可能是现成的，没有注释成本.我们引入了一个自动编码器模型，通过对图像对和三联体的约束进行训练。 我们从理论上和实验上展示了特征维度和对抗性训练的作用。我们正式证明了参照物模糊性的存在，当使用弱标记数据时，这种模糊性在分离任务中是固有的。一个因子的数值在不同的参照物框架中具有不同的意义。
在信息检索中，学习排名构建了一个基于机器的排名模型，该模型给定一个查询，按其与查询的相关程度或重要性对搜索结果进行排序。神经网络已经成功地应用于这个问题，在本文中，我们提出了一个基于注意力的深度神经网络，它更好地将查询和搜索结果的不同嵌入与基于注意力的机制相结合。 这个模型还应用了解码器机制，以列表的方式学习搜索结果的等级。嵌入是用卷积神经网络或word2vec模型训练的。我们用图像检索和文本查询数据集证明了这个模型的性能。
计算神经科学旨在适应体内神经活动的可靠模型，并将其解释为抽象的计算。最近的工作表明，神经元的功能多样性可能仅限于相对较少的细胞类型；其他工作表明，将约束条件纳入人工神经网络（ANN）可以提高其模仿神经数据的能力。 在这里，我们开发了一种算法，将神经活动的记录作为输入，并按细胞类型返回神经元的集群和受这些集群约束的神经活动模型。所产生的模型既更具预测性，也更容易解释，揭示了功能细胞类型对神经计算的贡献，并最终为未来神经网络的设计提供信息。
图形神经网络（GNN）是解决图形结构输入问题的强大代表工具。然而，到目前为止，几乎在所有情况下，它们都被应用于直接从原始输入中恢复最终解决方案，而没有明确指导如何构建他们的问题解决。相反，我们专注于算法空间的学习：我们训练几个最先进的GNN架构来模仿经典图形算法的单个步骤，平行（广度优先搜索，贝尔曼-福德）以及顺序（普里姆算法）。 由于图算法通常依赖于在邻域内做出离散的决定，我们假设基于最大化的信息传递神经网络最适合此类目标，并在经验上验证了这一说法。我们还证明了在算法空间中的学习如何产生任务间积极转移的新机会--显示了在同时学习可达性算法时，学习最短路径算法可以得到大幅改善。
预测是人类提出新任务计划的一个重要部分，但在机器人领域还没有深入探索。预测多个任务级别是一个具有挑战性的问题，涉及到捕捉任务语义和世界状态的连续可变性。理想情况下，我们将结合机器学习的能力，利用大数据来学习任务的语义，同时使用任务规划的技术来可靠地归纳到新环境。 我们学习了一个神经网，它编码了来自给定世界的高水平行动的K个最可能的结果。我们的方法创造了可理解的任务计划，使我们能够预测未来许多时间步骤的环境变化。我们通过应用于杂乱环境中的堆叠任务来证明这种方法，在这种环境中，机器人必须在不同颜色的块之间进行选择，同时避开障碍，以执行任务。
自适应梯度算法利用梯度的历史进行基于梯度的更新，在训练深度神经网络中无处不在。虽然自适应梯度方法的理论在最小化问题上得到了很好的理解，但在GANs等最小-最大问题上驱动其经验成功的基本因素仍不清楚。在本文中，我们旨在从理论和经验的角度弥补这一差距。 首先，我们分析了在~\citep{daskalakis2017training}中提出的优化随机梯度（OSG）的一个变体，用于解决一类非凸非凹的最小最大值问题，并建立了寻找$epsilon$一阶静止点的$O（\epsilon^{-4}）复杂度。其中，该算法只需要调用一个随机一阶神谕，同时享受随机外梯度方法实现的最先进的迭代复杂度~citep{iusem2017extragradient}。 然后，我们提出了一个OSG的自适应变体，名为Optimistic Adagrad（OAdagrad），并揭示了一个/emph{improved}自适应复杂度$/widetilde{O}/left(/epsilon^{-frac{2}{1-alpha}}/right)$~footnote{这里$/widetilde{O}(/cdot)$压缩了$/epsilon$的一个对数因子。 }，其中$alpha$表征累积随机梯度的增长率，$0\leq \alpha\leq 1/2$。 据我们所知，这是第一项在非凸非凹最小-最大优化中建立自适应复杂性的工作。从经验上看，我们的实验表明，在GAN训练中，自适应梯度算法确实优于非自适应算法。此外，这一观察可以用累积随机梯度的缓慢增长率来解释，正如经验观察到的那样。
我们考虑的问题是无监督地学习包含运动物体的视频的低维、可解释的潜在状态。从像素中提炼动态的问题已经通过图形/状态空间模型的视角得到了广泛的考虑，这些模型利用马尔科夫结构进行廉价计算，并利用结构化图形模型先验来强制执行潜在表示的可解释性。我们通过抛弃马尔科夫结构向扩展这些方法迈出了一步；相反，重新利用最近提出的高斯过程先验变量自动编码器来学习复杂的潜在轨迹。 我们描述了该模型，并在一个合成数据集上进行了实验，发现该模型可靠地重建了表现为U型转弯和循环的平滑动力学。我们还观察到，该模型可以在没有任何β-退火或训练参数冻结的情况下进行训练。
梦境和我们回忆梦境的能力是睡眠研究中最令人困惑的问题之一。具体来说，对梦境回忆率高和低的个体之间的大脑网络动态的推定差异仍然知之甚少。在这项研究中，我们将这个问题作为一个分类问题来解决，我们将深度卷积网络（CNN）应用于睡眠EEG记录，预测受试者是属于高或低梦境回忆组（HDR和LDR对应）。 我们的模型在所有的睡眠阶段都达到了明显的准确度，从而表明了睡眠微观结构中梦境回忆的微妙特征。我们还将特征空间可视化，以检查所学特征的主体特异性，从而确保网络捕捉到群体水平的差异。除了是第一个将深度学习应用于睡眠EEG以分类HDR和LDR的研究，引导反向传播使我们能够在每个睡眠阶段将最有区别的特征可视化。
本文考虑了网络系统控制中的多代理强化学习（MARL）。具体来说，每个代理根据本地观察和来自相连邻居的信息学习分散的控制策略。我们将这样的网络MARL（NMARL）问题表述为时空马尔可夫决策过程，并引入空间折扣因子来稳定每个本地代理的训练。 此外，我们提出了一个新的可微分的通信协议，称为NeurComm，以减少NMARL中的信息损失和非平稳性。根据在现实的NMARL场景中的适应性交通信号控制和合作适应性巡航控制的实验，适当的空间折扣因素有效地提高了非通信MARL算法的学习曲线，而NeurComm在学习效率和控制性能方面都优于现有的通信协议。
变量贝叶斯推断是一种流行的方法，用于近似贝叶斯神经网络权重的后验分布。最近开发这类方法的工作探索了近似后验的更丰富的参数化，希望能提高性能。相反，这里我们分享一个奇怪的实验发现，建议将变量分布限制在一个更紧凑的参数化。 对于使用高斯均值场变异推理训练的各种深度贝叶斯神经网络，我们发现后验标准差在收敛后始终表现出强烈的低秩结构。这意味着通过将这些变异参数分解为低秩因子，我们可以使我们的变异近似更加紧凑而不降低模型的性能。此外，我们发现这种因子化的参数化改善了变异下限的随机梯度估计的信噪比，导致更快地收敛。
在线产品评论中的方面提取是情感分析和意见挖掘中的一项关键任务。如果没有地面真相方面的标签，就不可能为方面提取训练有监督的神经网络，而无监督的神经主题模型则无法捕捉到感兴趣的特定方面。我们的主要贡献如下：首先，我们通过与一个简单的词包分类器的比较表明，目前的弱监督网络不能利用现有的种子词的预测能力。 第二，我们提出了一种提取方面的提炼方法，其中种子词由词包分类器（教师）考虑，并提炼成神经网络（学生）的参数。第三，我们表明，正则化鼓励学生考虑非种子词进行分类，因此，学生的表现优于教师，教师只考虑种子词。 最后，我们的经验表明，在亚马逊产品评论的六个领域中，我们提出的蒸馏方法优于以前的弱监督方法（F1分数高达34.4%），用于方面提取。
在视觉场景中形成知觉群体和个体化物体是实现视觉智能的一个重要步骤。这种能力被认为是在大脑中通过神经元之间自下而上、水平和自上而下的连接实现的计算。 我们表明，增加任何一项任务的难度都会使仅仅依靠自下而上处理的网络的学习受到限制。水平连接通过支持活动的增量空间传播来解决有格式塔线索的任务的这种限制，而自上而下的连接通过修改对目标物体位置的粗略预测来拯救有高级物体线索的任务的学习。我们的发现将自下而上、水平和自上而下连接的计算作用分开，并表明一个具有所有这些互动的模型如何能够更灵活地学习形成知觉群。
生成对抗网络近年来发展迅速，在图像的生成建模方面取得了显著的进步。然而，它们在音频领域的应用受到的关注有限，自回归模型，如WaveNet，仍然是音频信号（如人类语音）生成建模的最先进技术。 为了解决这个问题，我们引入了GAN-TTS，一个用于文本到语音的生成对抗网络。我们的架构由一个产生原始语音音频的条件前馈发生器和一个在不同大小的随机窗口上操作的鉴别器组合组成。鉴别器从一般的真实性以及音频与应该发音的语料的对应程度两方面来分析音频。 为了衡量GAN-TTS的性能，我们采用了人类的主观评价（MOS--平均意见得分），以及新的定量指标（Fréchet DeepSpeech Distance和Kernel DeepSpeech Distance），我们发现它们与MOS有很好的相关性。我们表明，GAN-TTS能够生成高保真语音，其自然度可与最先进的模型相媲美，而且与自回归模型不同，由于有一个高效的前馈发生器，它具有高度的并行性。听GAN-TTS读这个摘要，http://tiny.cc/gantts。
本文提出了一个在训练中修剪（PiT）的学习框架，以减少网络的参数大小。与现有的工作不同，我们的PiT框架采用了稀疏的惩罚来训练网络，从而帮助对权重和过滤器的重要性进行排序。我们的PiT算法可以直接修剪网络，无需任何微调。修剪后的网络仍然可以达到与原始网络相当的性能。 我们在MNIST、Cifar-10和miniImageNet上进行了广泛的实验，结果验证了我们提出的方法的有效性。值得注意的是，在MNIST数据集上，我们的PiT框架可以为LeNet-5节省17.5%的参数大小，实现了98.47%的识别精度。
我们首先提出了无监督持续学习（UCL）的问题：从非稳定的无标签数据流中学习突出的表征，其中物体类别的数量随时间变化。在推理之前给定有限的标签数据，这些表征也可以与特定的物体类型相关联以进行分类。 STAM模块的层次基于Hebbian学习、在线聚类、检测新模式和遗忘异常值以及自上而下预测的组合进行学习。我们在每类只有3-12个标记的例子的情况下，以持续学习手写数字的方式说明了STAM的操作。
最近的进展使得创建深度复值神经网络成为可能。尽管有这样的进展，但在许多具有挑战性的学习问题上，完全复杂的中间计算和表征的潜在力量还没有被开发出来。在最近的进展基础上，我们提出了一种在频域中提取信号的新机制。作为一个案例研究，我们在傅里叶域中进行音源分离。我们的提取机制可以被视为一种局部集合方法，结合复值卷积版的特征明智线性调制（FiLM）和信号平均操作。 我们还引入了一个新的明确的振幅和相位感知损失，它是尺度和时间不变的，考虑到了频谱图的复值成分。使用华尔街日报数据集，我们将我们的相位感知损失与其他几个在时域和频域操作的损失进行了比较，证明了我们提出的信号提取方法和建议的损失的有效性。 当在复值频域运行时，我们的深度复值网络大大超过了它的实值对应网络，即使深度只有一半，参数只有三分之一。
在本文中，我们提出了一个新的框架，以完全无监督的方式学习这种分离的表征。我们在一个双分支的自动编码器框架中解决这个问题。对于结构内容分支，我们将潜在因素投射到一个软结构点张量中，并用来自先验知识的损失来约束它。 我们在四个图像数据集上评估了我们的方法，在这些数据集上，我们证明了在合成数据和真实世界数据中卓越的分离和视觉类比质量。我们能够生成256x256分辨率的照片般真实的图像，这些图像在内容和风格上都有明显的分离。
我们开发了Y-learner，用于估计实验和观察研究中的异质性治疗效果。Y-learner被设计为利用神经网络优化多个目标和持续更新的能力，这使得治疗组和控制组之间的基本特征信息得到更好的汇集。我们在三个测试问题上评估Y-learner。(1) 一组来自文献的六个模拟数据基准。(2) 一个关于选民说服的真实世界的大规模实验。(3) 一个来自文献的任务，估计在MNIST didgits上人为产生的治疗效果。Y-learner在三个任务中的两个上取得了最先进的结果。
随着物联网设备的迅速扩散，我们的网络空间如今被数十亿的低成本计算节点所支配，这给我们的计算系统带来了前所未有的异质性。动态分析是寻找软件漏洞的最有效方法之一，但由于缺乏能够运行各种先前未见过的固件的通用仿真器，它已陷入瘫痪。 近年来，我们目睹了针对物联网设备的破坏性安全漏洞。这些安全问题极大地阻碍了物联网技术的进一步发展。在这项工作中，我们提出了Laelaps，一个专门设计用于在低成本物联网设备上运行各种软件的设备模拟器。 我们没有将设备的任何具体信息编码到我们的模拟器中。相反，Laelaps通过符号执行辅助的外设仿真推断出固件的预期行为，并产生适当的输入来引导具体的执行。 这一独特的设计特点使Laelaps成为第一个能够在没有关于目标设备的先验知识的情况下运行各种固件的通用设备模拟器。为了证明Laelaps的能力，我们在模拟器上部署了两种流行的动态分析技术--模糊测试和动态符号执行。
深度神经模型，如卷积和递归网络，在图像和文本等空间数据上取得了惊人的成果。然而，在考虑表格数据时，决策树的梯度提升（GBDT）仍然是首选的方法。(DNF)--一种结合了决策树和密集残余连接元素的新型架构。我们提出了广泛的实证研究结果，其中我们考察了GBDTs、DNFs和（深度）全连接网络的性能。这些结果表明，DNF在表格数据上取得了与GBDT相当的结果，并为多模态数据的端到端神经建模打开了大门。为此，我们介绍了DNF作为混合架构的一部分在多模态驾驶场景理解分类任务中的成功应用。
超参数调整是深度学习中最耗时的工作量之一。最先进的优化器，如AdaGrad、RMSProp和Adam，通过自适应调整每个变量的单独学习率来减少这种劳动。最近，研究人员对更简单的方法如动量SGD表现出了新的兴趣，因为它们可能产生更好的结果。 基于这些见解，我们设计了YellowFin，它是SGD中动量和学习率的自动调整器。YellowFin可以选择使用负反馈回路来补偿异步设置中的动态动态。 我们的经验表明，YellowFin在图像识别、语言建模和选区解析方面，可以比Adam在ResNets和LSTM上以更少的迭代次数收敛，在同步设置中速度提高到3.28$x，在异步设置中速度提高到2.69$x。
机器学习（ML）系统的鲁棒性和安全性是交织在一起的，其中非鲁棒性的ML系统（分类器、回归器等）可能会受到使用各种漏洞的攻击。随着可扩展的深度学习方法的出现，人们对监督、无监督和强化学习算法的鲁棒性给予了很大重视。 在这里，我们研究了深度变异自动编码器（dVAE）的潜空间的鲁棒性，这是一个无监督的生成框架，表明确实有可能扰乱潜空间，翻转类预测，并在攻击前后保持分类概率大致相等。这意味着，一个查看解码器输出的代理将保持对攻击的无知。
基于图的依赖性解析包括两个步骤：首先，编码器为输入句子的每个解析子结构产生一个特征表示，然后用来计算子结构的得分；其次，解码器}找到子结构具有最大总分的解析树。 在过去的几年中，强大的神经技术被引入到编码步骤中，这大大增加了解析的准确性。然而，先进的解码技术，特别是高阶解码，已经看到了使用率的下降。人们普遍认为，由神经编码器产生的上下文特征可以帮助捕捉高阶解码信息，从而减少对高阶解码器的需求。 在本文中，我们实证评估了不同的神经和非神经编码器与一阶和二阶解码器的组合，并对这些组合在不同训练数据规模下的有效性进行了全面分析。 我们发现：首先，当有大量的训练数据时，一个强大的神经编码器与一阶解码器足以达到较高的解析精度，只是略微落后于神经编码与二阶解码的组合；其次，在小的训练数据下，一个非神经编码器与二阶解码器在大多数情况下胜过其他组合。  
元学习将是创造终身的、可泛化的人工智能的关键。然而，在实践中，很难定义用于训练元学习者的元训练任务分布。如果做得太小，任务过于相似，模型就无法进行有意义的泛化；如果做得太大，泛化就会变得难以置信地困难。 我们认为，这两个问题都可以通过引入一个控制元学习者被训练的任务序列的教师模型来缓解。这个教师模型被激励在简单的任务上开始学生的元学习者，然后根据学生的进步自适应地增加任务难度。
使用高阶知识来减少训练数据已经成为一个流行的研究课题。然而，现有方法绘制有效决策边界的能力仍然有限：当训练集很小时，神经网络会偏向于某些标签。基于这一观察，我们认为约束输出概率分布是高阶领域知识。我们设计了一种新的算法，在聚类嵌入空间上共同优化输出概率分布，使神经网络绘制有效的决策边界。 虽然直接应用概率约束是无效的，但用户需要提供额外的非常弱的监督：标记一些输出分布与目标概率分布有很大差异的批次。我们用实验来实证证明，我们的模型可以收敛到比其他先进的半监督学习模型更高的准确度，而这些模型的标注训练实例质量不高。
我们引入了一种新型的深度语境化词汇表征，该表征既能模拟（1）词汇使用的复杂特征（如语法和语义），又能模拟（2）这些使用如何在不同的语言环境中变化（即模拟多义词）。 我们的词向量是深度双向语言模型（biLM）内部状态的学习函数，该模型在一个大型文本语料库上进行了预训练。我们表明，这些表征可以很容易地添加到现有模型中，并在六个具有挑战性的NLP问题上显著提高技术水平，包括问题回答、文本相关性和情感分析。 我们还提出了一项分析，表明暴露预训练网络的深层内部结构是至关重要的，允许下游模型混合不同类型的半监督信号。
这项工作解决了在训练数据中存在时间上错位的标签的情况下鲁棒性事件定位的长期问题。我们提出了一个新颖的多功能损失函数，它将一些训练制度从标准的完全监督交叉熵概括为基于计数的弱监督学习。与经典的模型不同，这些模型在训练期间被约束为严格符合注释，我们的软定位学习方法反而放松了对标签的确切位置的依赖。 用这种新的损失函数进行训练，对标签的时间错位表现出很强的鲁棒性，从而减轻了对时间序列进行精确注释的负担。我们在一些具有挑战性的实验中针对标准基准展示了最先进的性能，并进一步表明，对标签噪声的鲁棒性不是以牺牲原始性能为代价实现的。
深度网络背后的驱动力是它们紧凑地表示丰富的函数类别的能力。正式推理这一现象的主要概念是表达效率，它指的是一种情况，即一个网络必须增长到令人难以置信的大，才能复制另一个网络的功能。 迄今为止，表达效率的分析集中在深度的架构特征上，显示出深层网络在表示上优于浅层网络。在本文中，我们研究了连接性带来的表达效率，其动机是观察到现代网络以精心设计的方式将其层连接起来。 通过引入和分析混合张量分解的概念，我们证明了扩张卷积网络的互连可以带来表达效率。特别是，我们表明，即使是中间层之间的单一连接也可以导致几乎二次的差距，这在大规模环境中通常是一个模型实用与否的区别。 实证评估表明，连接的表达效率，与深度的表达效率类似，如何转化为准确性的提高。这使我们相信，表达效率可能在开发深度网络设计的新工具中起到关键作用。
我们将多任务学习应用于类似MNIST的数据集上的图像分类任务。MNIST数据集被称为机器学习的{果蝇}，一直是许多学习理论的试验田。NotMNIST数据集和FashionMNIST数据集是以MNIST数据集为参考创建的。 在没有多任务学习的情况下，MNIST、NotMNIST和FashionMNIST的识别准确率分别为99.56%、97. 用多任务学习对网络进行预训练，识别精度分别为99.70%、97.46%和95.25%。结果再次证明，多任务学习框架，即使是不同类型的数据，也能带来明显的改善。
最近的工作表明，神经网络很容易受到对抗性例子的影响，也就是说。为了解决这个问题，我们通过稳健优化的视角来研究神经网络的对抗性稳健性。这种方法为我们提供了一个广泛和统一的观点，让我们了解到关于这个主题的许多先前工作。它的原则性也使我们能够确定训练和攻击神经网络的方法，这些方法是可靠的，在某种意义上是通用的。 特别是，他们指定了一个具体的安全保证，可以防止一个定义明确的对手类别。这些方法让我们训练的网络对广泛的对手攻击的抵抗力有显著提高。他们还建议把对一阶对手的鲁棒性作为一个自然的安全保证。我们相信，对这种定义明确的对手类别的鲁棒性是走向完全抵抗的深度学习模型的重要垫脚石。
近年来，图结构数据的分类方法迅速增加。无论是在图核还是图神经网络中，成功的最先进的模型的隐含假设之一是将图的同构特征纳入架构，导致更好的经验性能。然而，正如我们在这项工作中发现的，常用的图分类数据集有重复的实例，导致同构偏差的问题，即。 我们分析了以前广泛用于图相关任务的54个数据集，分析了同构偏差的存在，给机器学习从业者提出了一系列建议，以正确设置他们的模型，并为未来的实验开放新的数据集。
模仿学习，再加上强化学习算法，是一种有前途的范式，可以有效地解决复杂的控制任务。然而，从示范中学习往往受到协变量转移问题的影响，这导致了所学策略的级联错误。我们引入了一个保守推断的价值函数的概念，这可以证明导致具有自我修正的策略。 我们设计了一种带有负采样的价值迭代（VINS）算法，可以实际学习这种带有保守外推的价值函数。我们表明，VINS可以纠正模拟机器人基准任务上的行为克隆策略的错误。
在物体、关系和层次方面对我们世界的结构化理解是人类认知的一个重要组成部分。作为实现这一目标的一个步骤，我们引入了对比训练的结构化世界模型（C-SWMs）。 我们对C-SWMs在涉及多个可由代理人独立操纵的互动物体的组成环境、简单的Atari游戏和多物体物理模拟进行了评估。我们的实验表明，C-SWMs可以克服基于像素重建的模型的局限性，在高度结构化的环境中优于该模型类别的典型代表，同时学习可解释的基于物体的表征。
神经机器翻译（NMT）模型学习含有大量语言信息的表征。然而，目前还不清楚这种信息是否完全分布，或者其中一些信息是否可以归于单个神经元。我们开发了无监督的方法来发现NMT模型中的重要神经元。 我们的方法依赖于不同模型学习类似属性的直觉，并且不需要任何昂贵的外部监督。我们通过实验表明，翻译质量取决于所发现的神经元，并发现其中许多神经元捕获了常见的语言现象。最后，我们展示了如何通过修改单个神经元的激活，以可预测的方式控制NMT的翻译。
当输出类的数量很大时，神经网络模型中softmax函数的计算是昂贵的。这可能成为此类模型训练和推理中的一个重要问题。在本文中，我们提出了Doubly Sparse Softmax（DS-Softmax），即稀疏专家的稀疏混合物，以提高softmax推理的效率。在训练期间，我们的方法通过将整个输出类空间分为几个部分重叠的专家来学习两级类层次结构。 每个专家负责输出类空间的一个学习子集，每个输出类只属于这些专家中的一小部分。在推理过程中，我们的方法快速定位最可能的专家来计算小规模的softmax。我们的方法是基于学习的，不需要事先了解输出类分区空间。
我们的工作提出了经验性的证据，即层旋转，即每个层的权重向量与其初始化之间的余弦距离在训练中的演变，构成了一个令人印象深刻的泛化性能的指标。 与以前研究的泛化指标相比，我们表明，层旋转有一个额外的好处，即容易监测和控制，以及有一个与网络无关的最佳状态：在训练过程中，所有层的权重与初始化的余弦距离达到1，一直优于其他配置--测试精度高达20%。最后，我们的结果还表明，对层旋转的研究可以提供一个统一的框架来解释权重衰减和自适应梯度方法对泛化的影响。
代码的模型可以学习程序的语法和语义的分布式表示，以预测程序的许多非琐碎的属性。最近最先进的模型利用程序的高度结构化表示，如树、图和其中的路径（如数据流关系），这对代码来说是精确和丰富的。这对有语义的关系提供了强大的归纳偏向，产生了比经典的基于序列的模型更可概括的表示。 不幸的是，这些模型主要依靠基于图的消息传递来表示代码中的关系，由于消息传递步骤的高成本，这使得它们事实上是局部的，这与现代的、基于序列的全局模型（如Transformer）形成了鲜明的对比。在这项工作中，我们通过引入两个新的混合模型系列来弥合全局和结构化模型之间的鸿沟，它们既是全局的，又包含结构化的偏见。Graph Sandwiches，它将传统的（门控）图信息传递层包裹在顺序信息传递层中；以及Graph Relational Embedding Attention Transformers（简称GREAT），它用图边缘类型的关系信息对传统的Transformer进行偏置。 通过研究一个流行的、非微不足道的程序修复任务--变量误用识别，我们探索了传统和混合模型系列在代码表示方面的相对优势。从一个基于图的模型开始，该模型已经在这个任务的现有最先进水平上提高了20%，我们表明，我们提出的混合模型又提高了10-15%，同时训练速度更快，使用的参数也更少。
递归神经网络（RNN）特别适合于为连续数据中的长期依赖关系建模，但由于在时间上反向传播的误差要么消失，要么以指数速度爆发，所以是出了名的很难训练。 虽然许多工作试图通过门控递归单元、跳过连接、参数约束和设计选择来减轻这种影响，但我们提出了一种新的增量RNN（iRNN），其中隐藏的状态向量跟踪增量变化，因此近似于Rosenblatt（1962）的连续时间RNN的状态向量增量。 我们表明，我们的方法在计算上是高效的，克服了许多试图改善RNN训练的现有方法的开销，同时没有遭受性能下降。我们通过广泛的实验证明了我们的方法的效用，并在LTD和其他非LTD任务上显示了与标准LSTM的竞争性能。
最近关于过度参数化的深度网络的实证结果的特点是典型的U型测试误差曲线的缺失：测试误差在更广泛的网络中不断减少。研究人员正积极努力通过提出更好的复杂性测量方法来弥补这一差异。 为了更好地理解优化的作用，我们将总方差分解为训练集抽样引起的方差和初始化引起的方差。在参数不足的情况下，初始化引起的方差是很重要的。在参数过多的情况下，总方差要低得多，由抽样引起的方差所主导。
现实世界的图像通常包含大量的私人/敏感信息，应该在不降低其效用的情况下小心保护。在本文中，我们提出了一个具有可学习混淆器的隐私保护的深度学习框架，用于图像分类任务。 为了最好地保护用户在图像中的隐私，我们为我们的框架设计了一种对抗性训练方法，以优化混淆器。通过对现实世界数据集的广泛评估，数字指标和可视化结果都表明，我们的框架有资格保护用户的隐私，并在图像分类任务上达到相对较高的准确性。
比特币是一个虚拟的硬币系统，使用户能够在几乎没有中央信任机构的情况下进行交易。比特币区块链上的所有交易都可以公开查看，然而由于比特币主要是为了安全而建立的，它的原始结构不允许对地址交易进行直接分析。现有的比特币区块链分析方法可能很复杂，计算成本很高或不准确。我们提出了一个计算效率高的模型来分析比特币区块链地址，并允许它们与现有的机器学习算法一起使用。我们将我们的方法与多级序列学习者（MLSLs）进行比较，后者是比特币地址数据上表现最好的模型之一。
尽管在经验上取得了显著的成功，但对生成对抗网络（GAN）的训练动力学仍然知之甚少，它涉及到使用随机梯度解决一个最小的游戏。在这项工作中，我们分析了在凸凹性假设下同步梯度下降（simGD）及其变体的最后迭代收敛，并以微分方程的连续时间分析为指导。 首先，我们表明，在原始变量严格的凸性条件下，simGD是以随机子梯度收敛的。其次，我们概括了乐观的simGD，以适应独立于学习率的乐观率，并表明其以全梯度收敛。最后，我们提出了锚定的simGD，一种新方法，并表明以随机子梯度收敛。
小型航天器现在有商业化的精确姿态控制系统，允许它们在3个自由度内回转，并在短时间内捕获图像。在与适当的软件结合时，这种灵活性可以大大增加响应速度、重访时间和覆盖范围。在以前的工作中，我们已经展示了一个算法框架，它结合了轨道力学、姿态控制和调度优化，以规划一个星座中敏捷的小型航天器的时间变化、全身方向。 建议的时间表优化将在地面站自主运行，并将结果时间表上传到航天器上执行。该算法在小型可转向航天器、控制能力、传感器规格、成像要求和感兴趣的区域上是可通用的。 在这篇文章中，我们对该算法进行了修改，以便在小型航天器上运行，这样，该星座可以在没有地面控制的情况下，自主地做出时间敏感的决定，进行回转和捕捉图像。我们已经开发了一个基于延迟/容错网络（DTN）的通信模块，用于卫星间的机载数据管理和路由，它将与其他模块一起工作，优化敏捷通信和转向的时间表。 然后，我们将这一初步框架应用于有代表性的星座，模拟对偶发降水事件和随后的城市洪水的定向测量。我们的敏捷算法的指挥和控制效率与非敏捷（提高11.3倍）和非DTN（提高21%）星座进行了比较。
重要性抽样（IS）是一种标准的蒙特卡洛（MC）工具，用于计算随机变量的信息，如未知分布的时刻或定量。 IS在渐进上是一致的，因为MC样本的数量，以及因此确定密度估计参数的deltas（粒子）的数量都是无穷大的。然而，保留无限多的粒子是难以做到的。我们提出了一个方案，只保留粒子的一个/emph{无限的代表子集}和它们增强的重要性权重，是/emph{接近一致的}。为了以{在线方式}做到这一点，我们以两种方式对重要性采样进行近似。 首先，我们用核子来代替德尔塔，产生核子密度估计（KDEs）。 第二，我们依次将KDEs投射到附近的低维子空间上。我们描述了这个方案的渐进偏差，它由压缩参数和核带宽决定，产生了一致性和内存之间的可调控的权衡。在实验中，我们观察到内存和准确性之间的有利权衡，首次提供了任意后验分布的近乎一致的压缩。
我们研究了以下三个关于岭回归的基本问题。(1)估计器的结构是什么？(2)如何正确使用交叉验证来选择正则化参数？(3)如何在不损失太多精确度的情况下加速计算？我们在一个统一的大数据线性模型中考虑这三个问题。我们研究了K$-fold交叉验证在选择正则化参数时的偏差，并提出了一个简单的偏差修正。我们分析了脊回归的primal和dual sketching的准确性，显示它们是惊人的准确。我们的结果通过模拟和分析经验数据得到了说明。
在本文中，我们通过研究基于注意力的神经网络的群体和经验损失函数的景观来解决这个问题。我们的结果表明，在温和的假设下，两层全局注意力模型的每个局部最小值都具有较低的预测误差，而且注意力模型比不采用注意力的模型需要更低的样本复杂性。 然后，我们将我们的分析扩展到流行的自我注意模型，证明它们以更有表现力的函数类提供一致的预测。此外，我们的理论结果为设计注意机制提供了一些指导。我们的发现被MNIST和IMDB评论数据集上令人满意的实验结果所验证。
深度学习技术的最新进展表明，深度神经网络在提取执行手头任务所需的特征方面非常有用。然而，这些学到的特征只对最初的任务特别有帮助。这是因为学到的特征是非常具体的任务，没有捕捉到输入的最一般和任务无关的特征。 最近，变异自动编码器（VAE）已被证明是在生成意义上捕获潜在变量的事实模型。由于这些潜在特征可以表示为连续和/或离散变量，这表明我们使用VAE与连续和离散变量的混合物作为潜在空间。
深度学习的信息瓶颈理论提出，神经网络通过压缩其表征来忽略与任务无关的信息，从而实现良好的泛化。 相反，使用非饱和激活函数的网络实现了相当水平的任务表现，但没有显示出压缩。在本文中，我们开发了更稳健的互信息估计技术，它适应神经网络的隐藏活动，并对所有函数的激活产生更敏感的测量，特别是无界函数。 使用这些自适应估计技术，我们探索了具有一系列不同激活函数的网络中的压缩。通过两种改进的估计方法，首先，我们表明激活函数的饱和度不是压缩的必要条件，而且不同激活函数之间的压缩量是不同的。我们还发现，不同网络初始化之间的压缩量有很大的变化。其次，我们看到L2正则化导致压缩量明显增加，同时防止过拟合。最后，我们表明，只有最后一层的压缩与泛化呈正相关关系。
在这项工作中，我们解决了音乐音色转移的问题，目标是操纵一种乐器的声音样本的音色以匹配另一种乐器，同时保留其他音乐内容，如音高、节奏和响度。原则上，人们可以将基于图像的风格转移技术应用于音频信号的时频表示，但这取决于是否有一个允许独立操纵音色的表示，以及高质量的波形生成。 我们介绍了TimbreTron，这是一种音乐音色转换的方法，它将 "图像 "领域的风格转换应用于音频信号的时频表示，然后使用一个有条件的WaveNet合成器产生高质量的波形。 我们表明，恒定Q变换（CQT）表示由于其近似的音高等效性而特别适合卷积架构。根据人类的感知评估，我们确认TimbreTron在保留音乐内容的同时，可识别地转移了单音和复音样本的音色。我们在这里制作了一个配套的演示视频：https://www.cs.toronto.edu/~huang/TimbreTron/index.html，我们强烈建议你在阅读论文之前观看。
神经形态的硬件往往对可以在其上运行的深度网络的连通性构成限制。但深度学习的通用硬件和软件实现对稀疏网络的运行更有效。在没有连通性限制的情况下训练神经网络后，存在几种修剪连接的方法。 DEEP R在监督训练过程中自动重构网络，使连接出现在任务最需要的地方，而其总数一直是严格受限的。我们证明，DEEP R可以用来在标准基准任务上训练非常稀疏的前馈和递归神经网络，而在性能上只有轻微的损失。
深度学习的成功导致了越来越大的模型来处理越来越复杂的任务；训练的模型可以包含数百万个参数。这些大型模型是计算和内存密集型的，这使得以最小的延迟、吞吐量和存储需求来部署它们成为一个挑战。一些模型压缩方法已经成功地应用于图像分类和检测或语言模型，但在压缩生成式对抗网络（GAN）执行复杂任务方面的工作很少。 在本文中，我们展示了一种标准的模型压缩技术，即权重修剪，不能用现有的方法应用于GAN。然后，我们开发了一种自我监督的压缩技术，使用训练好的鉴别器来监督压缩生成器的训练。
大规模的分布式训练需要大量的通信带宽来进行梯度交换，这限制了多节点训练的可扩展性，并且需要昂贵的高带宽网络基础设施。在移动设备上进行分布式训练（联合学习）时，情况变得更加糟糕，因为它存在更高的延迟、更低的吞吐量以及间歇性的不良连接。 在本文中，我们发现分布式SGD中99.9%的梯度交换是多余的，并提出了深度梯度压缩（DGC）来大大降低通信带宽。为了在压缩过程中保持精度，DGC采用了四种方法：动量校正、局部梯度剪裁、动量因子屏蔽和热身训练。 我们将深度梯度压缩应用于图像分类、语音识别和语言建模的多个数据集，包括Cifar10、ImageNet、Penn Treebank和Librispeech Corpus.在这些场景上，深度梯度压缩实现了270倍到600倍的梯度压缩率，而不损失精度，将ResNet-50的梯度大小从97MB削减到0。 35MB，DeepSpeech的梯度大小从488MB降至0.74MB。深度梯度压缩实现了在廉价的商品1Gbps以太网上的大规模分布式训练，促进了移动端的分布式训练。
由于深度学习的进展，图像到图像的翻译最近受到了极大的关注。大多数工作都集中在以无监督的方式学习一对一的映射，或者以有监督的方式学习多对多的映射。然而，更实际的设置是以无监督的方式进行多对多的映射，由于缺乏监督和复杂的内部和跨域变化，这更难。 为了缓解这些问题，我们提出了模范引导和语义一致的图像到图像翻译（EGSC-IT）网络，该网络将翻译过程置于目标域的模范图像上。我们假设图像包括跨域共享的内容部分和每个域特有的风格部分。 在目标域典范的指导下，我们将自适应实例规范化应用于共享内容组件，这使得我们能够将目标域的风格信息转移到源域。 为了避免翻译过程中由于巨大的内部和跨域变化而自然出现的语义不一致，我们引入了特征掩码的概念，在不需要使用任何语义标签的情况下提供粗略的语义指导。在各种数据集上的实验结果表明，EGSC-IT不仅将源图像翻译成目标域的不同实例，而且在这个过程中保留了语义一致性。
深度神经网络可以学习有意义的数据表征。然而，这些表征很难解释。例如，可视化的潜伏层通常最多只能在三个维度上实现。神经网络能够学习并受益于更高维度的表征，但这些表征在视觉上无法解释，因为节点在一个层内有任意的排序。 在这里，我们利用人类观察者在结构化表征中识别模式的能力来可视化更高的维度。为此，我们提出了一类正则，我们称之为图谱正则（Graph Spectral Regularizations），将图结构强加于潜伏层。这通过将激活视为预定图上的信号并使用图滤波器（如低通和小波滤波器）限制这些激活来实现。 这个框架允许任何种类的图以及过滤器来实现广泛的结构化正则化，这取决于数据的推理需求。首先，我们展示了一个合成的例子，图结构化层可以揭示数据的拓扑特征。接下来，我们展示了一个平滑的正则化在应用于胶囊网时可以对节点进行语义上的一致排序。 此外，我们表明，图形结构层使用类似小波的空间定位滤波器，可以形成定位的感受野，以改善图像和生物医学数据的解释。换句话说，由于激活的定位，潜伏层、神经元和输出空间之间的映射变得清晰。最后，我们表明，当结构为网格时，表示创建连贯的图像，允许使用图像处理技术，如卷积。
文本生成在许多NLP任务中无处不在，从总结到对话和机器翻译。主要的参数化方法是基于局部归一化模型，每次预测一个词。 为了使训练具有可操作性，我们首先在预训练的局部归一化语言模型的残差中工作，其次我们使用噪声对比估计进行训练。此外，由于EBM在序列水平上工作，我们可以利用预训练的双向上下文表征，如BERT和RoBERTa。我们在两个大型语言建模数据集上的实验表明，与局部归一化基线相比，残差EBM产生较低的复杂度。此外，根据人类评估，通过重要性采样生成非常有效，而且质量高于基线模型。
我们研究了图像识别模型的鲁棒性特性，这些模型配备了两个受人类视觉启发的特征，一个是明确的偶发记忆，一个是形状偏差，在ImageNet规模上。正如以前的工作所报告的，我们表明明确的偶发记忆改善了图像识别模型在某些威胁模型下对小规范对抗性扰动的鲁棒性。 然而，它并没有提高对更自然的、通常是更大的扰动的鲁棒性。在训练期间学习更多的鲁棒性特征似乎对这第二种意义上的鲁棒性是必要的。我们表明，从一个被鼓励学习全局的、基于形状的表征的模型中得到的特征（Geirhos et al, 2019年）不仅提高了对自然扰动的鲁棒性，而且当与外显记忆结合使用时，它们还提供了对对抗性扰动的额外鲁棒性。最后，我们解决了外显记忆的三个重要设计选择：记忆的大小、记忆的维度和检索方法。我们表明，为了使外显记忆更加紧凑，最好是通过聚类来减少记忆的数量，而不是减少它们的维度。
群卷积神经网络（G-CNNs）可以通过给它们配备群的几何结构来改进经典的CNNs.G-CNNs成功的核心是将特征图提升到更高维的分解表征，其中数据特征被有效地学习，几何数据增强被淘汰，并且通过群理论保证几何变换下的可预测行为（等值性）。 然而，目前G-CNN的实际实现仅限于离散群（使网格不受影响）或连续紧凑群，如旋转（使傅里叶理论得以使用）。在本文中，我们解除了这些限制，提出了一个模块化框架，用于设计和实现任意李群的G-CNN。 在我们的方法中，李群的微分结构被用来扩展在李代数上定义的B-splines的通用基础上的卷积核。这导致了一个灵活的框架，使G-CNNs中的局部的、无序的和可变形的卷积分别通过局部的、稀疏的和非均匀的B-spline扩展。 我们在两个基准数据集上研究了我们的方法的影响和潜力：组织病理切片中的癌症检测（PCam数据集），其中旋转等值起着关键作用；面部地标定位（CelebA数据集），其中尺度等值很重要。在这两种情况下，G-CNN架构的表现都优于其经典的2D对应物，并且详细研究了无痕和本地化组卷积的附加价值。
 全局特征池是特征池的一个现代变体，提供了更好的可解释性和正则化。虽然存在其他的池化方法（如最大、lp norm、随机），但平均操作仍然是流行模型中占主导地位的全局池化方案。由于细粒度识别需要学习微妙的、有区别的特征，我们考虑的问题是：平均池化是最佳策略吗？我们首先问："通过全局平均和最大池化学习的特征之间有区别吗？可视化和定量分析表明，最大集合鼓励学习不同空间尺度的特征。我们接着问："是否有一个单一的全局特征集合变体最适合于细粒度识别？对九种有代表性的池化算法的全面评估发现：最大池化在不同的模型、数据集和图像分辨率下都优于平均池化；它通过减少泛化差距来做到这一点；广义池化的性能在从平均到最大的变化中几乎是单调的。我们最后问："什么是结合两种异质池化方案的最佳方式？常见的策略由于潜在的梯度冲突而陷入困境，但 "冻结和训练 "的技巧效果最好。我们还发现，全局后的批处理归一化有助于更快的收敛，并持续改善模型性能。
我们提出了一种技术，通过引入自我监督任务作为辅助损失函数来改善在小型标记数据集上学习的深度表征的泛化。虽然最近的研究显示了自我监督学习（SSL）在大型无标记数据集上的好处，但它在小型数据集上的效用是未知的。我们发现SSL将少数镜头元学习者的相对错误率降低了4%-27%，即使数据集很小并且只利用了数据集中的图像。 虽然SSL的好处可能会随着训练集的增大而增加，但我们观察到，当用于元学习和SSL的图像分布之间存在领域转移时，SSL会对性能产生负面影响。基于这种分析，我们提出了一种技术，即使用领域分类器从一个大的、通用的未标记图像池中为SSL自动选择图像，从而提供进一步的改进。我们使用几个元学习者和自我监督的任务在具有不同程度的领域转移和标签大小的数据集上展示结果，以描述SSL对少量学习的有效性。
马尔科夫决策过程的抽象化是解决复杂问题的有用工具，因为它可以忽略环境中不重要的方面，简化学习最优策略的过程。在本文中，我们提出了一种新的算法，在具有连续状态空间的环境中寻找抽象的MDP。 它基于MDP同构，是MDP之间的结构保护映射。我们展示了我们的算法从收集的经验中学习抽象的能力，并展示了如何重新使用抽象来指导代理人遇到的新任务的探索。我们的新型任务转移方法击败了基于深度Q网络的基线。
最近一些理解神经网络的方法都集中在量化单个特征的作用上。 其中一种方法，NetDissect使用视觉语义标签（颜色、材料、纹理、物体和场景）的Broden数据集来确定模型的可解释特征。 鉴于最近一些动作识别数据集的兴起，我们建议扩展Broden数据集以包括动作，从而更好地分析学习到的动作模型。 我们描述了注释过程，在扩展的Broden数据集上解释动作识别模型的结果，并研究了可解释的特征路径，以帮助我们理解用于分类动作的概念层次。
自动生成流行音乐的旋律一直是人工智能研究人员和音乐家的长期愿望。然而，由于一些因素，学习生成婉转的旋律已被证明是非常具有挑战性的。 在本文中，我们建议将每个音符及其属性表示为一个独特的 "词"，从而减少了属性之间错位的可能性，同时也降低了学习的复杂性。我们还对音符的范围执行规则化政策，从而鼓励生成的旋律接近于人类会发现容易遵循的东西。 此外，我们生成的旋律以歌曲部分信息为条件，从而复制了一首完整的歌曲的整体结构。实验结果表明，我们的模型可以生成悦耳的歌曲，与以前的模型相比，与人类写的歌曲更难区分。
深度是深度神经网络（DNNs）的一个关键组成部分，然而，设计深度是启发式的，需要很多人的努力。我们提出了AutoGrow来自动发现DNNs的深度：从一个浅的种子架构开始，如果增长提高了准确性，AutoGrow就会增长新的层；否则就停止增长，从而发现深度。 我们的实验表明，通过对不同的网络结构应用相同的策略，AutoGrow总能在MNIST、FashionMNIST、SVHN、CIFAR10、CIFAR100和ImageNet等各种数据集上发现接近最佳的深度。例如，在精度-计算权衡方面，AutoGrow在ResNets中发现的深度组合比人类专家更好。
鉴于遥感的重要性，令人惊讶的是，表示学习界很少关注它。为了解决这个问题，并加快这个领域的创新，我们以标准化的形式提供了对5个不同的遥感数据集的简化访问。我们特别探索了域内表示学习，并解决了 "一个数据集应该有什么特征才能成为遥感表示学习的一个好来源 "的问题。
生成式seq2seq对话系统被训练来预测已经发生的对话中的下一个词。它们可以从大型无标记的对话数据集中学习，建立对对话背景的深刻理解，并产生各种各样的反应。这种灵活性是以控制为代价的。 训练数据中不理想的反应将在推理时被模型重现，而且较长的生成往往没有意义。我们不是一次生成一个词的反应，而是训练一个分类器从预定义的完整反应列表中进行选择。 在推理时，我们生成与预测的反应类相关的示范性反应。专家可以随着时间的推移编辑和改进这些示范性反应，而不需要重新训练分类器或使旧的训练数据无效。对775个未见过的医生/病人对话的人工评估表明，这种权衡改进了反应。在相同的对话背景下，我们的判别性方法的反应只有12%比医生的反应差，而生成性模型的反应只有18%。
例如，这种等价性使得从完全贝叶斯的、无限宽的训练过的FCN所产生的测试集预测可以在不实例化FCN的情况下被计算出来，而是通过评估相应的GP。 在这项工作中，我们为多层卷积神经网络（CNN）推导了一个类似的等价物，包括有池层和无池层，并在CIFAR10上实现了没有可训练核的GP的最新结果。我们还引入了一种蒙特卡洛方法来估计对应于给定神经网络架构的GP，即使在分析形式有太多条款而在计算上可行的情况下。令人惊讶的是，在没有池化层的情况下，对应于具有和不具有权重共享的CNN的GP是相同的。因此，在用随机梯度下降法（SGD）训练的有限通道CNN中，翻译等值是有益的，它保证在无限通道极限的贝叶斯处理中不发挥作用--这是两种制度的质的区别，在FCN情况下不存在。 我们通过实验证实，虽然在某些情况下，随着通道数的增加，SGD训练的有限CNN的性能接近于相应的GPs，但通过仔细的调整，SGD训练的CNN可以大大超过其相应的GPs，这表明与完全贝叶斯参数估计相比，SGD训练具有优势。
贝叶斯推理有望为深度神经网络提供基础并提高其性能。它有望对过拟合具有鲁棒性，简化训练程序和超参数空间，并提供一个校准的不确定性措施，可以提高决策、代理探索和预测的公平性。 马尔科夫链蒙特卡洛（MCMC）方法通过从模型参数的后验分布中生成样本来实现贝叶斯推断。尽管贝叶斯推断的理论优势以及MCMC和优化方法之间的相似性，但到目前为止，在大规模深度学习任务中，采样方法的性能落后于优化方法。 我们旨在填补这一空白，并引入ATMC，这是一种自适应噪声MCMC算法，它估计并能够从神经网络的后验中取样。ATMC动态地调整应用于每个参数更新的动量和噪声量，以补偿随机梯度的使用。 我们在Cifar10基准和大规模ImageNet基准上使用没有批量归一化的ResNet架构来测试ATMC，结果表明，尽管没有批量归一化，ATMC在分类精度和测试对数可能性方面都优于强大的优化基线。我们表明，ATMC对训练数据的过度拟合具有内在的鲁棒性，与优化基线相比，ATMC提供了更好的不确定性校准测量。
现在，GANs可以生成越来越逼真的人脸图像，可以轻松骗过人类。 相比之下，一个普通的卷积神经网络（CNN），例如ResNet-18，如果训练和测试的人脸来自同一来源，可以达到99.9%以上的识别假/真人脸的准确率。本文中，我们同时进行了人类研究和CNN实验，这使我们有了两个重要的发现。一个发现是假人脸的纹理与真人脸有很大不同。CNN可以捕捉局部图像纹理信息来识别假/真人脸，而这些线索很容易被人类所忽略。 另一个发现是，全局图像纹理信息对图像编辑更加稳健，并且可以从不同的GAN和数据集中通用于假人脸。基于上述发现，我们提出了一个新颖的架构，被称为Gram-Net，它在多个语义层面上结合了 "Gram Block "来提取全局图像纹理表示。 特别是，我们的Gram-Net对图像编辑（如降采样、JPEG压缩、模糊和噪音）更加稳健。 更重要的是，我们的Gram-Net在检测训练阶段未见的GAN模型的假脸方面的通用性明显更好。
Wasserstein概率指标受到了机器学习界的广泛关注。与严格衡量概率变化的Kullback-Leibler发散不同，Wasserstein指标反映了结果之间的基本几何形状。对这种几何形状的敏感性的价值已经在序数回归和生成模型以及最近的强化学习中得到证实。 在本文中，我们描述了概率发散的三个自然属性，我们认为它们反映了机器学习的要求：和不变性、规模敏感性和无偏的样本梯度。 我们提供的经验证据表明，这在实践中是一个严重的问题。利用从概率预测中得到的启示，我们提出了一个替代Wasserstein度量的方法，即Cramér距离。 为了说明Cramér距离的实际意义，我们设计了一种新的算法，即Cramér生成对抗网络（GAN），并表明它比相关的Wasserstein GAN具有许多理想的特性。
我们人类对时间的不对称进展有一种天生的理解，我们用它来有效和安全地感知和操纵我们的环境。我们从中得到启发，在马尔科夫（决策）过程中学习时间箭头的问题。我们说明了学习的时间箭头如何能捕捉到关于环境的突出信息，这反过来又可以用来衡量可及性、检测副作用和获得内在的奖励信号。 最后，我们提出了一种简单而有效的算法来对手头的问题进行参数化，并通过一个函数近似器（这里是一个深度神经网络）来学习时间之箭。我们的实证结果跨越了离散和连续环境的选择，并证明了对于一类随机过程，学习的时间之箭与Jordan, Kinderlehrer和Otto（1998）提出的一个众所周知的时间之箭的概念相当一致。
我们将随机梯度下降法（SGD）表述为一个新的因子化贝叶斯过滤问题，其中每个参数都是单独推断的，以相关的反向传播梯度为条件。 在这种情况下，推断自然产生了BRMSprop和BAdam：RMSprop和Adam的贝叶斯变体。 值得注意的是，贝叶斯方法恢复了最先进的自适应SGD方法的许多特征，其中包括均方根归一化、Nesterov加速和AdamW。 因此，贝叶斯方法为最先进的自适应SGD算法的经验有效性提供了一种解释。 在MNIST上将BRMSprop和BAdam与天真的RMSprop和Adam进行经验性比较，我们发现贝叶斯方法有可能大大减少测试损失和分类误差。
数据增强（DA）已被广泛用于提高训练深度神经网络的泛化能力。最近，人类设计的数据增强已逐渐被自动学习的增强策略所取代。通过在精心设计的数据增强搜索空间中寻找最佳策略，AutoAugment（Cubuk et al, 本文中，我们开发了一种对抗性方法，以得出一种计算上可承受的解决方案，称为对抗性AutoAugment，它可以同时优化目标相关对象和增强策略的搜索损失。增强策略网络试图通过生成对抗性增强策略来增加目标网络的训练损失，而目标网络可以从更难的例子中学习更强大的特征来提高概括性。 与之前的工作相比，我们将目标网络训练中的计算重新用于策略评估，并免除了对目标网络的再训练。与AutoAugment相比，这使得ImageNet的计算成本降低了约12倍，时间开销缩短了11倍。 我们展示了我们的方法在CIFAR-10/CIFAR-100和ImageNet上的实验结果，并证明了比最先进的方法有明显的性能改进。在CIFAR-10上，我们实现了1.36%的顶级测试误差，这是目前表现最好的单一模型。在ImageNet上，我们实现了领先的性能，在ResNet-50上的顶级精度为79.40%，在ResNet-50-D上为80.00%，无需额外数据。
在这项研究中，我们专注于一阶元学习算法，该算法旨在学习一个网络的参数初始化，该网络可以在给定的几个例子中快速适应新的概念。我们研究了两种方法来提高此类算法的泛化和学习速度，特别是在Reptile（Nichol等人。2018）算法。我们引入了一种新的正则化技术，称为元步骤梯度修剪，还研究了在一阶元学习中增加网络架构深度的效果。我们对这两种方法进行了实证评估，我们使用Mini-ImageNet数据集以10倍的迭代次数匹配了基准的几张图片分类结果，而使用更深的网络，我们达到的精度超过了目前使用Omniglot数据集的几张图片分类基准。
在本文中，我们提出使用训练中矩阵分解来减少神经机器翻译的模型大小。使用训练中矩阵分解，参数矩阵可以被分解成更小的矩阵的乘积，这可以通过极大地减少可学习参数的数量来压缩大型机器翻译架构。 我们将训练中矩阵因子化应用于标准神经架构的不同层，并表明训练中因子化能够减少近50%的可学习参数，而没有任何相关的BLEU分数损失。此外，我们发现训练中矩阵因子化在嵌入层上特别强大，提供了一种简单而有效的方法来减少参数数量，对模型性能的影响最小，有时还能提高性能。
虽然最先进的句子表示模型可以执行需要大量语法知识的任务，但如何最好地评估它们的语法知识是一个开放的问题。我们探索了五种实验方法，这些方法受到先前评估预训练句子表示模型工作的启发。我们引入了一个人工生成的数据集，该数据集为实验操纵了NPI许可的关键特征。我们发现BERT对这些特征有重要的了解，但它的成功在不同的实验方法中差异很大。我们的结论是，各种方法对于揭示一个模型在特定领域的语法知识的所有相关方面是必要的。
灵长类动物的视觉系统建立了稳健的、多用途的外部世界表征，以支持几种不同的下游皮质过程。这种表征需要对动态变化的光照、局部纹理失真等引起的感觉不一致的情况保持不变。 在这项工作中，我们探索了将这种水平连接引入标准的深度卷积网络；我们提出了V1Net--一种新型的卷积-递归单元，它受灵长类视觉皮层连接的启发，对线性和非线性水平抑制和兴奋连接进行建模。我们介绍了纹理化挑战--一种评估知觉噪声下物体识别性能的新基准，我们用它来评估V1Net与一系列精心挑选的有/无递归处理的控制模型。 此外，我们还展示了V1Net的消融研究结果，证明不同的神经启发的水平连接对于最先进的人工智能系统从自然图像中检测物体边界的任务是有用的。我们还展示了几个生物学上合理的水平连接模式的出现，即中心-环绕-关闭、关联场和边界-所有权连接模式在V1Net模型中的训练，以对伯克利分割数据集500（BSD500）的自然图像进行边界检测。 我们的研究结果表明，V1Net和生物视觉系统之间的表征相似性增加了，并强调了神经启发的递归背景处理原则对学习视觉表征的重要性，这些表征对知觉噪声具有鲁棒性，并推动了计算机视觉的发展。
人类通过对核心语言成分的意义和作用进行组合来理解新的句子。相反，当需要这种组合概括时，用于自然语言建模的神经网络模型就会失败。本文的主要贡献是假设语言的组合性是一种群权变量的形式。 基于这一假设，我们提出了一套构建等价序列到序列模型的工具。通过对SCAN任务的各种实验，我们分析了现有模型在等价视角下的行为，并证明我们的等价架构能够实现人类语言理解中所需要的类型的组成概括。
变分推理（VI）是一种流行的近似贝叶斯推理方法，对于深度神经网络等高参数化的模型来说，它特别有前途。 在这项工作中，我们提出了一种训练高度灵活的变异分布的方法，即从粗略的近似开始，迭代细化它。每个细化步骤都进行廉价的局部调整，只需要优化简单的变异族。 在实验中，我们的方法在对数可能性和ELBO方面一直优于最近用于深度学习的变量推理方法。 我们看到，在更大规模的模型上，收益被进一步放大，在CIFAR10上明显优于标准VI和深度组合的残差网络。
在本文中，我们提出了一个用于高质量图像修复的残差非局部注意力网络。在不考虑损坏图像中信息分布不均的情况下，以前的方法受限于局部卷积操作和空间和通道方面特征的平等处理。 为了解决这个问题，我们设计了局部和非局部注意块，以提取捕捉像素之间的长距离依赖性的特征，并对具有挑战性的部分给予更多的关注。具体来说，我们在每个（非）局部注意块中设计了主干分支和（非）局部掩码分支。主干分支用于提取分层特征。 我们提出的方法可以推广到各种图像修复应用中，如图像去噪、去马赛克、减少压缩伪影和超分辨率。实验证明，与最近领先的方法相比，我们的方法在数量上和视觉上获得了可比或更好的结果。
大多数学习行动规划模型的方法都严重依赖于大量的训练样本或计划观测。在本文中，我们采用了一种不同的方法，基于从特定领域的知识中进行演绎学习，特别是来自于逻辑公式，这些公式规定了关于特定领域的可能状态的约束。
我们发布了用于表征学习的最大的连续原始信号的公共心电图数据集，其中包含超过11000名患者和20亿个标记的节拍。我们的目标是使半监督的心电图模型得以建立，并发现未知的心律失常亚型和异常的心电图信号事件。为此，我们提出一个无监督的表征学习任务，以半监督的方式进行评估。 我们为不同的特征提取器提供了一套基线，可以在此基础上进行扩展。 此外，我们对来自PCA嵌入的结果进行了定性评估，在那里我们确定了一些已知亚型的聚类，表明在心律失常亚型发现中进行表示学习的潜力。
作为卷积神经网络（CNN）的基本构件，卷积层被设计为提取局部模式，在本质上缺乏对全局背景的建模能力。最近，人们做出了许多努力来补充CNN的全局建模能力，特别是由一系列关于全局特征交互的作品。 然而，神经科学的研究表明，除了影响改变我们神经元的输入外，神经元根据上下文动态修改其功能的能力对感知任务至关重要，而这一点在大多数CNN中都被忽略了。 基于此，我们提出了一种新的上下文导向卷积（CGC），在全局上下文的指导下，明确地修改卷积层的权重。因此，由于意识到全局上下文，我们提出的CGC的调制卷积核可以更好地提取有代表性的局部模式，并组成判别性的特征。此外，我们提出的CGC是轻量级的，适合于现代CNN架构，并根据图像分类、动作识别和机器翻译的广泛实验，不断提高CNN的性能。
我们分析了低精度网络中量化噪声和剪切失真之间的权衡。我们确定了各种张量的统计数据，并推导出剪切导致的均方误差退化的精确表达式。通过优化这些表达式，我们显示出比通常避免剪切的标准量化方案的明显改进。 例如，仅仅通过选择准确的剪切值，VGG-16量化到4比特精度时，就可以获得超过40％的精度改进。我们的结果在训练和推理时对神经网络的量化有许多应用。
批量归一化（BN）是深度学习领域最广泛使用的技术之一。但它的性能会随着批量大小的不足而严重下降。这个缺点限制了BN在许多计算机视觉任务中的使用，如检测或分割，由于内存消耗的限制，批量大小通常很小。 因此，人们提出了许多修正的归一化技术，这些技术要么不能完全恢复BN的性能，要么必须在推理过程中引入额外的非线性操作并增加巨大的消耗。 基于我们的分析，我们提出了一种新的归一化方法，名为移动平均批归一化（MABN）。MABN可以完全恢复香草BN在小批量情况下的性能，而不需要在推理过程中引入任何额外的非线性操作。我们通过理论分析和实验来证明MABN的好处。我们的实验证明了MABN在多个计算机视觉任务中的有效性，包括ImageNet和COCO。代码已经发布在https://github.com/megvii-model/MABN。
我们提出了一个简单的证明，证明深度在多层前馈网络整流激活（"深度分离"）中的好处。具体来说，我们提出了一串分类问题f_i，这样（a）对于任何固定的深度整流网络，我们可以找到一个指数m，使得指数>m的问题需要指数级的网络宽度来完全表示函数f_m；并且（b）对于家族中的任何问题f_m，我们提出一个具有线性深度和有限宽度的具体神经网络，完全表示它。 虽然以前有一些工作显示了类似的结果，但我们的证明使用的工具和技术要简单得多，而且计算机科学专业的本科生和有类似背景的人应该也能接受。
丰富的、可获得的标记数据为深度学习的革命性成功提供了动力。然而，大量的监督对于许多实际的应用来说仍然是一种奢侈，这促进了人们对标签稀少的技术的极大兴趣，如少数次学习（FSL）。FSL的一个直观可行的方法是通过合成额外的训练样本进行数据增强。 在本文中，我们提出了一种新的FSL模型，称为$\textrm{D}^2$GAN，它基于生成对抗网络（GAN）合成多样性和鉴别性特征。$\textrm{D}^2$GAN通过限制合成的特征与同一类别的真实特征具有高相关性，而与不同类别的特征具有低相关性，从而保证了合成特征的鉴别性。 根据观察，在潜伏代码空间中比较接近的噪声向量在映射到特征空间时更有可能被折叠成相同的模式，$textrm{D}^2$GAN包含了一个新的反折叠正则化项，它通过惩罚两个合成特征的对数相似性和生成它们的潜伏代码的对数相似性的比率来鼓励特征多样性。 在三个常见的基准数据集上的实验通过与最先进的数据集相比较，验证了$textrm{D}^2$GAN的有效性。
这里，我们首先证明了结构化数据集的效果，通过实验比较两个不同数据集上训练的两层网络的动态和性能：(i)包含随机i.i.d.输入的非结构化合成数据集，以及(ii)简单的典型数据集如MNIST图像。 我们的分析揭示了两个与网络的动态和泛化能力有关的现象，这些现象只有在结构化数据集上训练时才会出现。其次，我们为数据集引入了一个生成模型，其中高维输入位于一个低维流形上，其标签只取决于它们在这个流形中的位置。我们称之为*隐藏流形模型*，我们通过实验证明，在从这个模型得出的数据集上训练网络会重现在MNIST上训练时出现的两种现象。
在本文中，我们研究了深度对角环形神经网络，即权重矩阵是对角和环形矩阵的乘积的深度神经网络。除了对其表现力进行理论分析，我们还介绍了训练这些模型的原则性技术：我们设计了一个初始化方案，并提出了一个聪明地使用非线性函数来训练深度对角环形网络。此外，我们表明这些网络的性能优于最近推出的具有其他类型结构层的深度网络。我们进行了全面的实验研究，将深度对角环形网络的性能与基于结构化矩阵的最新模型和密集模型进行了比较。我们表明，我们的模型比其他结构化方法取得了更好的准确性，同时需要的权重比下一个最佳方法少2倍。最后我们训练深度对角环形网络，在一个具有超过380万训练实例的真实世界视频分类数据集中建立一个紧凑和准确的模型。
可解释性在很大程度上集中在局部解释上，即解释为什么一个模型对一个样本做出了特定的预测。这些解释由于其简单性和局部的保真度而具有吸引力。 这些全局解释采取特征形状的形式，比特征归属更有表现力。通过仔细的实验，我们从质量和数量上表明，全局加法解释能够描述模型行为，并产生关于神经网络等模型的洞察力。我们的方法应用于神经网络的可视化训练，可在https://youtu.be/ErQYwNqzEdc。
最近在自然语言处理（NLP）方面的许多成功是由以无监督方式在大量文本上训练的单词的分布式矢量表征推动的。这些表征通常被用作跨越一系列NLP问题的单词的通用特征。 最近的工作探索了具有不同训练目标的无监督和监督学习技术，以学习通用的固定长度的句子表征。在这项工作中，我们提出了一个简单、有效的句子表征的多任务学习框架，在一个模型中结合了不同训练目标的归纳偏见。我们在多个数据源上对超过1亿个句子进行了多种训练目标的训练。广泛的实验表明，在弱相关的任务中共享一个单一的递归句子编码器会比以前的方法有一致的改进。我们在转移学习和低资源环境中使用我们学到的通用表征提出了实质性的改进。
在神经网络越来越多地被用于敏感应用的时代，算法偏见已经成为一个具有道德影响的问题。虽然一个系统有无数种可能被偏见破坏的方式，但系统地隔离和评估现有系统的这种情况是不难的，即。偏见可能是微妙的、自然的，而且本质上是难以量化的。为此，本文提出了第一个针对有偏见的场景对最先进的神经模型进行基准测试的系统研究。总而言之，我们的框架为有原则的量化和评估模型对有偏见的数据集提供了一种新的方法。因此，我们发现最先进的NLP模型（如BERT、RoBERTa、XLNET）很容易受到有偏见数据的影响。
在这种情况下，我们假设用户预先知道她希望模型学习的主题子集，并且能够为这些主题提供一些示范性文档。此外，虽然每个文档通常由多个主题组成，但我们并不假设用户会详尽地识别所有主题。     最近最先进的主题模型，如NVDM，在这里被称为神经主题模型（NTMs），属于变异自动编码器框架。我们通过在训练目标中使用信息性先验，将NTMs扩展到弱半监督环境中。在分析了信息性先验的影响后，我们提出了一个使用logit-normal后验的NVDM模型的简单修改，我们表明与其他NTM模型相比，它实现了对用户所需主题的更好匹配。
通过信息平面（IP）理论分析深度神经网络（DNNs）最近获得了极大的关注，因为它是深入了解其泛化能力等的工具。然而，如何估计每个隐藏层和输入/期望输出之间的相互信息（MI）来构建IP并不明显。 例如，具有许多神经元的隐藏层需要对与这些层相关的高维度具有鲁棒性的MI估计器。MI估计器也应该能够自然地处理卷积层，同时在计算上可扩展到大型网络。 g.\ VGG-16.在本文中，我们提出了一种使用新的基于矩阵的R\'enyi's entropy与卷积层的张量核的IP分析，利用核方法的力量来表示独立于数据维度的概率分布的特性。 获得的结果对以前有关小规模DNN的文献有了新的认识，但是使用了一种全新的方法。重要的是，新的框架使我们能够对当代大规模DNN和CNN提供第一个全面的IP分析，调查不同的训练阶段，并对大规模神经网络的训练动力学提供新的见解。
开发能够学习遵循自然语言指令的代理一直是一个新兴的研究方向。虽然自然语言指令是可访问的和灵活的，但有时甚至对人类来说也是模糊的。为了解决这个问题，我们建议利用以正式语言结构的程序，作为指定任务的精确和表达方式。 然后，我们设计了一个模块化的框架，可以学习执行由程序指定的任务--"由于不同的情况会产生不同的方式来完成任务，我们的框架可以感知它目前所处的情况，并相应地指示一个多任务策略来完成整个任务的每个子任务。 在二维Minecraft环境中的实验结果不仅证明了所提出的框架能够可靠地完成程序指令，并实现了对更复杂指令的零点泛化，而且还验证了所提出的调制机制在学习多任务策略方面的效率。我们还进行了分析，比较了以端到端方式从程序和自然语言指令中学习的各种模型。
我们分析了（随机）梯度下降算法的收敛性，该算法用于学习具有整流线性单元（ReLU）激活函数的卷积滤波器。我们的分析不依赖于任何特定形式的输入分布，我们的证明只使用ReLU的定义，与之前的工作相比，这些工作仅限于标准高斯输入。 我们表明，具有随机初始化的（随机）梯度下降可以在多项式时间内学习卷积滤波器，收敛率取决于输入分布的平滑性和斑块的接近性。据我们所知，这是第一个基于梯度的算法在非高斯输入分布上的卷积滤波器的恢复保证。我们的理论也证明了深度神经网络中的两阶段学习率策略。虽然我们的重点是理论上的，但我们也提出了证明我们理论发现的实验。
深度神经网络（DNNs）因其高精确度而被广泛采用于现实世界的认知应用中。然而，DNN模型的鲁棒性最近受到了对抗性攻击的挑战，输入样本上的小干扰可能导致错误分类。 目前最先进的防御算法，如对抗性训练或鲁棒性优化，通过支付高额的计算成本来提高DNN对对抗性攻击的弹性。此外，这些方法通常只为防御一种或几种已知的攻击技术而设计。 这项工作的目的是在对抗性攻击下提高DNN模型的鲁棒性。特别是，我们提出了Bamboo--第一个为提高DNN的一般鲁棒性而设计的数据增强方法，而不需要对攻击算法做任何假设。Bamboo在每个训练数据周围的固定半径球上均匀采样的少量数据来增强训练数据集，从而有效地增加自然数据点和决策边界之间的距离。 我们的实验表明，奔步大大改善了对任意类型的攻击和噪音的一般鲁棒性，与以前的对抗性训练方法、鲁棒性优化方法和其他具有相同数据量的数据增强方法相比，取得了更好的结果。
合成真实的神经活动模式的能力对于研究神经信息处理至关重要。这里我们使用生成对抗网络（GANs）框架来模拟神经元群体的协同活动。 我们调整了Wasserstein-GAN的变体，以促进无约束的神经群体活动模式的生成，同时仍然受益于时域的参数共享。我们证明了我们提出的GAN，我们称之为Spike-GAN，生成的尖峰列车与几十个神经元的数据集的一阶和二阶统计数据准确匹配，也很接近它们的高阶统计数据。 我们将Spike-GAN应用于从蝾螈视网膜记录的真实数据集，并表明它的表现与基于最大熵和二分高斯框架的最先进的方法一样好。重要的是，Spike-GAN不需要预先指定模型要匹配的统计数据，因此构成了一个比这些替代方法更灵活的方法。Spike-GAN提供了一种强大的、易于使用的技术，用于生成真实的尖峰神经活动，并描述现代系统神经科学中研究的大规模神经群体记录的最相关特征。
由于(Kingma & Welling 2013, Rezende et al. 2014)引入的可扩展学习算法，深度潜变量模型已经成为一种流行的模型选择。这些方法对观察数据的难以解决的对数似然性的变异下限进行了最大化。Burda et al. (2015)引入了一个多样本变异约束，即 IWAE，它至少和标准变异下限一样紧，并且随着样本数的增加而变得越来越紧。反过来说，IWAE约束的典型推理网络梯度估计器随着样本数的增加表现很差（Rainforth等人，2018，Le等人，2018）。 我们表明它实际上是有偏的，并且可以通过第二次应用重参数化技巧有效地估计偏向。双重重参数化梯度（DReG）估计器不会随着样本数量的增加而受到影响，解决了之前提出的问题。 特别是，我们表明，这个估计器减少了IWAE梯度的方差，重新加权的唤醒-睡眠更新（RWS）（Bornschein & Bengio 2014），以及jackknife variational inference（JVI）梯度（Nowozin 2018）。最后，我们表明，这种计算效率高、落入式估计器转化为在几个建模任务上改善所有三个目标的性能。
Zeroth-order优化是最小化目标$f(x)$的过程，给定神谕访问在自适应选择的输入$x$的评价。在本文中，我们提出了两个简单而强大的无梯度下降（GLD）算法，不依赖于底层梯度估计，并且数值稳定。 我们从一个新的几何学角度分析了我们的算法，我们表明，对于{it any monotone transform}的光滑强凸目标，潜伏维度为$k\ge n$，我们提出了一个新的分析，表明在$O(kQ\log(n)\log(R/\epsilon))$的评估中收敛于$epsilon$球的最优，其中输入维度为$n$，$R$是输入空间的直径，$Q$是条件数字。 我们的速率是第一个既1）对维度的多对数依赖，又2）在单调变换下不变的速率。我们进一步利用我们的几何视角来证明我们的分析是最优的。单调不变性及其利用低潜伏维度的能力是我们算法的经验成功的关键，这一点在合成和MuJoCo基准上得到了证明。
许多过程可以简明地表示为从开始状态到结束状态的一系列事件。给定原料和成品蛋糕，一个有经验的厨师可以推测出食谱。当修改为目标条件时，不仅现有的前向预测方法可以合成更好更长的视频，而且GCP模型还可以利用在时间上不是线性的结构，完成分层预测。为此，我们研究了自动回归的GCP模型和新的树状结构的GCP模型，这些模型递归地产生帧，将视频迭代地分割成由子目标划分的越来越细的片段。在模拟和真实数据集的实验中，我们的GCP方法在较长的时间内生成高质量的序列。 树状结构的GCP也比自动回归的GCP更容易并行化，使训练和推理非常有效，并允许模型在长度为数千帧的序列上进行训练。最后，我们证明了GCP方法在无法获得专家行动的情况下对模仿学习的效用。视频见补充网站：https://sites.google.com/view/video-gcp
计算技术和传感器设计的最新进展使得从病人身上收集纵向或时间序列数据变得更加容易，从而产生了大量的可用医疗数据。大多数医疗时间序列缺乏注释，或者即使有注释，也可能是主观的，容易出现人为错误。 早期的工作已经开发了自然语言处理技术，从医生笔记中提取概念注释和/或临床叙述。然而，这些方法很慢，而且没有使用伴随的医疗时间序列数据。我们提出了关系型多实例学习（RMIL）--一个基于递归神经网络的深度多实例学习框架，它使用池函数和注意力机制来完成概念标注任务。
嵌入层将输入词转化为真实的向量，是自然语言处理中使用的深度神经网络的关键组成部分。然而，当词汇量很大时，相应的权重矩阵可能是巨大的，这使得它们无法在有限的资源环境中部署。我们引入了一种基于张量列车（TT）分解的嵌入层参数化的新方法，它允许以可忽略不计的性能下降甚至轻微增加为代价大幅压缩模型。 我们在自然语言处理的各种基准上评估了我们的方法，并分析了从MLPs到LSTMs和Transformer等各种架构的性能和压缩率之间的权衡。
我们注意到，自适应梯度算法的常见实现，如Adam，限制了权重衰减正则化的潜在好处，因为权重不是乘法衰减（如标准权重衰减所预期的那样），而是通过一个加法常数因子衰减。我们提出了一个简单的方法来解决这个问题，即把权重衰减和损失函数的优化步骤脱钩。我们提供的经验证据表明，我们提出的修改(i)把权重衰减因子的最佳选择与标准SGD和Adam的学习率设置脱钩，(ii)大幅提高Adam的泛化性能，使其能够在图像分类数据集上与SGD竞争（在这些数据集上，它以前通常表现得比后者好）。 我们还证明，较长的优化运行需要较小的权重衰减值以获得最佳结果，并引入了权重衰减的归一化变体以减少这种依赖性。最后，我们提出了一个带有暖重启的Adam版本（AdamWR），该版本在CIFAR-10和ImageNet32x32上取得了最先进的结果，同时具有强大的随时性能。我们的源代码将在审查过程后提供。
终身学习是以连续的方式学习多个连续任务的问题，从以前的任务中获得的知识被保留下来并用于未来的学习。在这项工作中，我们专注于生成式建模的终身学习方法，我们不断将新观察到的流分布纳入我们学习的模型中。 通过引入一个新的跨模型正则器，学生模型利用教师学到的信息，作为到目前为止所看到的一切的总结。正则器的额外好处是减少灾难性干扰的影响，当我们通过流式数据学习时，这种干扰会出现。我们证明了它在流式分布上的功效，以及它在复杂的迁移学习场景中学习一个共同的潜在表示的能力。
三维几何数据为研究表征学习和生成模型提供了一个很好的领域。在本文中，我们研究了以点云为代表的几何数据。我们引入了一个深度自动编码器（AE）网络，具有出色的重建质量和泛化能力。所学的表征在三维识别任务中的表现优于目前的技术水平，并通过简单的代数操作实现基本的形状编辑应用，如语义部分编辑、形状类推和形状插值。 我们还对不同的生成模型进行了彻底的研究，包括在原始点云上操作的GANs，在我们的AEs的固定潜空间中训练的明显改进的GANs，以及高斯混合模型（GMM）。有趣的是，在我们的AEs的潜空间中训练的GMMs产生了最佳保真度和多样性的样本。
尽管深度神经网络（DNNs）在各种任务上有显著的表现，但它们很容易受到对抗性扰动的影响，这使得它们很难部署在现实世界的安全关键应用中。在本文中，我们旨在通过稀疏化DNN的潜在特征来获得对对抗性扰动敏感的健壮网络。 具体来说，我们定义了潜伏特征空间的脆弱性，然后提出了一个贝叶斯框架，根据其对原始损失和对抗性损失的贡献，对特征进行优先排序/调整。我们还建议在训练期间对特征的脆弱性进行规范化，以进一步提高鲁棒性。 虽然这种网络稀疏化在文献中主要是为了计算效率和DNN的正则化效果而研究的，但我们确认它也有助于通过定量评估和定性分析来设计一种防御机制。 我们在多个基准数据集上验证了我们的方法，即Adversarial Neural Pruning (ANP)}，该方法提高了测试精度并带来了最先进的鲁棒性。ANP还解决了同时获得稀疏和鲁棒网络的实际问题，这对于确保部署在计算和内存有限设备上的轻量级网络的对抗性鲁棒性至关重要。
在异常检测（AD）中，人们试图在给定正常样本的数据集的情况下，识别测试样本是否异常。  最近一种很有前途的AD方法依赖于深度生成模型，如变异自动编码器（VAEs），用于对正常数据分布的无监督学习。在半监督AD（SSAD）中，数据还包括一小部分标记的异常样本。 这两种方法的直观想法是训练编码器在正常数据和异常数据的潜伏向量之间进行 "分离"。我们表明，这一想法可以从问题的原则性概率公式中得到，并提出了简单有效的算法。 我们的方法可以应用于各种数据类型，正如我们在从自然图像到天文学和医学的SSAD数据集上所展示的那样，并且可以与任何VAE模型架构相结合。当与不针对特定数据类型的最先进的SSAD方法相比较时，我们在离群点检测方面获得了明显的改善。
我们引入了动态实例硬度（DIH），以促进机器学习模型的训练。DIH是每个训练样本的属性，被计算为在训练历史中测量的样本瞬时硬度的运行平均值。我们使用DIH来评估一个模型在一段时间内对每个训练样本的知识的保留程度。我们发现，对于深度神经网络（DNN），一个样本在相对早期训练阶段的DIH反映了它在后期阶段的DIH，因此，DIH可以有效用于减少未来历时的训练样本集。 具体来说，在每个历时中，只有高DIH的样本被训练（因为它们在历史上是很难的），而低DIH的样本可以被安全地忽略。DIH在每个历时中只对选定的样本进行更新，所以它不需要额外的计算。 另外，由于模型集中在历史上更有挑战性的样本上，结果模型更准确。以上，当被表述为一种算法时，可以被看作是一种课程学习的形式，所以我们称我们的框架为DIH课程学习（或DIHCL）。与其他课程学习方法相比，DIHCL的优点是。(1) DIHCL不需要在每个历时中对未被DIHCL选择的数据进行额外的推理步骤，(2) 动态实例硬度，与静态实例硬度（例如。在某些数学假设下，我们将DIHCL的问题表述为寻找一个使多集函数$f(\cdot)$最大化的课程，并得出DIH产生的课程相对于最优课程的近似约束。 从经验上看，DIHCL训练的DNN在效率、早期收敛性和最终性能方面明显优于随机小批量SGD和其他最近开发的课程学习方法，这一点在11个现代数据集上训练几个最先进的DNN时得到了证明。
本文探讨了自适应控制和机器学习之间的许多直接联系，既通过共同的更新定律，也通过共同的概念。自适应控制作为一个领域，侧重于数学的严谨性和保证收敛性。另一方面，机器学习的快速发展带来了大量的新技术和学习问题。本文阐明了这两个领域之间的许多共同联系，从而可以利用两者的结果来解决新问题。
递归卷积（RC）共享相同的卷积核并多次展开，最初是为了对时空信号进行建模而提出的。我们认为RC可以被看作是深度卷积神经网络的模型压缩策略。RC减少了跨层的冗余，是对大多数现有模型压缩方法的补充。然而，RC网络的性能不能与其相应的标准网络的性能相匹配，即具有相同深度但独立的卷积核。 这降低了RC在模型压缩方面的价值。在本文中，我们提出了一个简单的变体，改善了RC网络。一个RC模块的批归一化层在不同的展开步骤中是独立学习的（不是共享的）。我们提供了关于为什么这样做的见解。在CIFAR上的实验表明，将一个卷积层展开几个步骤可以提高性能，从而间接地在模型压缩中起作用。
视觉世界是巨大而多样的，但它的变化分为结构化和非结构化因素。结构化因素，如比例和方向，承认清晰的理论和有效的表征设计。非结构化因素，如是什么让一只猫看起来像一只猫，太复杂了，无法分析建模，因此需要自由形式的表示学习。 我们将结构化的高斯滤波器和自由形式的滤波器组合在一起，进行端到端的优化，将表征因子化，以实现高效而通用的学习。我们对动态结构的实验，其中结构化的滤波器随输入而变化，相当于动态推理的精度，有更多的自由度，同时提高了效率。（完整版请见https://arxiv.org/abs/1904.11487）。
众所周知，精心设计的扰动可以使最先进的机器学习分类器对图像进行错误的标记，足够小的扰动是人眼无法察觉的。然而，通过检测图像和错误标签之间的不一致，人类观察者会被提醒攻击。在本文中，我们旨在设计攻击，不仅使分类器产生错误的标签，而且使错误的标签无法被人类观察者察觉。 为了实现这一目标，我们提出了一种名为LabelFool的算法，该算法可以识别出一个与基础事实标签相似的目标标签，并为这个目标标签找到图像的扰动。我们首先通过一个概率模型找到输入图像的目标标签，然后在特征空间中向目标标签移动输入。在ImageNet上的主观研究表明，在标签空间中，我们的攻击更难被人类观察者识别，而ImageNet上的客观实验结果表明，我们在图像空间中保持了与最先进攻击算法类似的性能以及攻击率。
本文介绍了建筑物中产生的各种撞击声的噪声类型/位置分类，这是公寓楼中一个严重的冲突问题。在这项研究中，用一个麦克风记录了一组地板撞击声数据集。噪声类型/位置是根据韩国环境公司下属的地板管理中心的报告选择的。
大脑神经回路的记录显示了非凡的动态丰富性和高变异性。同时，降维技术通常发现了这些动态背后的低维结构。什么决定了神经回路活动的维度？维度在行为和任务学习中的功能作用是什么？ 在这项工作中，我们使用循环神经网络（RNN）模型来解决这些问题。我们发现，根据初始网络的动态，RNN学习增加和减少维度的方式与任务需求相匹配。这些发现揭示了基本的动态机制，神经网络通过这些机制来解决具有强大表征的任务，并可推广到新的情况。
最近提出的领域对抗方法包括调整源编码和目标编码，通常将这种方法作为目标误差理论约束中的两个（三个）项的最小化。 不幸的是，这种最小化可能会导致第三个项的任意增加，例如，在标签分布变化的情况下，它们可能会崩溃。我们提出了不对称放松分布对齐，这是一种新的方法，克服了标准领域对抗算法的一些限制。此外，我们描述了精确的假设，在这些假设下，我们的算法在理论上是有原则的，并在合成和真实数据集上展示了经验优势。
在本文中，我们探讨了{摘要到文章的生成}：给定一个简短的摘要生成长文章的任务，这为生成的文本提供了更精细的内容控制。为了防止序列到序列（seq2seq）模型退化为语言模型，并更好地控制要生成的长文本，我们提出了一个分层的生成方法，首先根据摘要生成一个中间长度的草图，然后通过充实生成的草图完成文章。 为了缓解训练期间使用的 "奇迹 "草图和推理期间生成的噪声草图之间的差异，我们提出了一个基于多代理强化学习的端到端联合训练框架。 为了进行评估，我们通过颠倒输入和输出来使用文本总结语料库，并引入一种新颖的评估方法，采用总结系统对生成的文章进行总结，并测试其与原始输入总结的匹配度。实验表明，我们提出的分层生成方法可以根据给定的总结生成一个连贯的相关文章，在传统的seq2seq模型上产生了显著的改进。
在训练深度神经网络进行监督下的图像分类时，我们可以大致区分两类图像的潜在特征，它们将驱动Y类的分类。(2016)，我们可以将特征大致分为两类：(i) "核心 "或 "条件不变 "特征X^ci，其分布P(X^ci | Y)不会在不同领域中发生重大变化；(ii) "风格 "或 "正交 "特征X^orth，其分布P（X^orth | Y）可以在不同领域中发生重大变化。 这些正交特征通常包括位置、旋转、图像质量或亮度等特征，但也包括更复杂的特征，如人物图像的头发颜色或姿态。然而，我们确实假设我们有时可以观察到一个所谓的标识符或ID变量。例如，我们可能知道两张图片显示的是同一个人，ID指的是这个人的身份。在数据增强中，我们从同一张原始图片中生成几张图片，ID指的是相关原始图片。 该方法只需要一小部分图像具有ID变量。我们通过将ID变量加入到Gong等人（2016）的模型中，为该问题提供了一个因果框架。然而，我们对不能直接观察领域的设置感兴趣，我们将领域视为一个潜在的变量。 如果两个或更多的样本共享相同的类别和标识符，（Y，ID）=（y，i），那么我们将这些样本视为正交或风格特征的不同风格干预下的反事实。使用这种按ID分组的方法，我们将网络正规化，通过惩罚适当的图拉普拉斯，为共享相同ID的样本提供接近恒定的输出。 在图像质量、亮度、颜色变化和更复杂的变化（如运动和姿势的变化）等方面的领域变化中，这被证明可以大幅提高性能。我们展示了与可解释性、公平性和转移学习问题的联系。
基于梯度的元学习算法需要几个梯度下降的步骤来适应新进入的任务。这个过程随着样本数量的增加而变得更加昂贵。 此外，梯度更新受到几个噪声源的影响，导致性能下降。  在这项工作中，我们提出了一种元学习算法，配备了GradiEnt Component COrrections，简称GECCO单元，它产生了一个乘法矫正的低秩矩阵，（在矢量化之后）修正了估计的梯度。GECCO包含一个简单的类似解码器的网络，具有可学习的参数，一个注意力模块和一个所谓的背景输入参数。GECCO的上下文参数被更新以生成网络梯度的低秩纠正项。  因此，元学习只需要少量的梯度更新来吸收新的任务（在少数情况下，通常一次更新就足够了）。以前的方法是通过改变学习率、分解网络参数或直接从特征和/或梯度中学习特征修正来解决这个问题，而GECCO是一个现成的类似于发电机的单元，它可以执行元素的梯度修正，而不需要直接观察特征和/或梯度。 我们表明，我们的GECCO(i)加速了学习，(ii)对被噪声破坏的梯度进行了稳健的修正，(iii)导致了对现有的基于梯度的元学习算法的明显改进。
鉴别性的问题回答模型可以过度适应数据集中的表面偏差，因为当任何线索使答案变得可能时，他们的损失函数就会饱和。 我们引入了问题和答案联合分布的生成模型，这些模型被训练来解释整个问题，而不仅仅是回答问题。我们的问题回答（QA）模型是通过学习答案的先验和条件语言模型来实现的，以生成给定答案的问题--允许在问题被逐字生成时进行可扩展和可解释的多跳推理。 我们的模型在SQUAD和CLEVR基准上取得了与专门的判别模型竞争的性能，表明它是一个比以前的工作更通用的语言理解和推理架构。
在本文中，我们将注意力转向激活函数和批处理归一化之间的相互作用，这是目前训练深度网络的一个几乎强制性的技术。我们提出了激活函数位移整流器线性单元（DReLU），猜想将ReLU的身份函数扩展到第三象限，可以增强与批处理归一化的兼容性。 此外，我们使用统计测试来比较使用不同的激活函数（ReLU、LReLU、PReLU、ELU和DReLU）对标准化的VGG和残差网络最先进模型的学习速度和测试准确性表现的影响。 这些卷积神经网络在CIFAR-100和CIFAR-10（最常用的深度学习计算机视觉数据集）上进行了训练。结果显示DReLU加快了所有模型和数据集的学习速度。此外，统计学意义上的性能评估（P<0.05）显示DReLU提高了ReLU在所有场景下的测试准确性。 此外，在所有的实验中，DReLU显示出比任何其他测试的激活函数更好的测试精度，只有一个例外，在这种情况下，它呈现出第二好的性能。因此，这项工作表明，有可能通过一个增强的激活函数来提高替代ReLU的性能。
将输入尺度信息明确地编码到卷积神经网络（CNN）学习的表征中，对许多视觉任务是有益的，特别是在处理多尺度输入信号时。在本文中，我们研究了一个具有跨空间和缩放组联合卷积的尺度等值CNN结构，这被证明是实现尺度等值表征的充分和必要条件。 为了降低模型的复杂性和计算负担，我们将卷积滤波器分解在两个预先固定的可分离基下，并将扩展截断到低频分量。截断滤波器扩展的另一个好处是提高了等值表示的变形鲁棒性。数值实验证明，所提出的带有分解卷积滤波器的尺度-等值神经网络（ScDCFNet）在多尺度图像分类中取得了明显改善的性能，在降低模型大小的情况下比普通CNN有更好的可解释性。
在本文中，我们对三维点云处理的深度神经网络进行了诊断，以探索不同网络架构的效用。我们就特定网络架构对DNNs的表示能力的影响提出了一些假设。为了证明这些假设，我们设计了五个指标，从以下角度诊断各种类型的DNNs，即信息丢弃、信息集中、旋转稳健性、对抗性稳健性和邻域不一致性。 我们根据这些指标进行比较研究，以验证这些假设，这可能为神经网络的架构设计带来新的启示。实验证明了我们方法的有效性。
在这项工作中，我们从低维条件半隐式分布中构建了灵活的联合分布。明确定义近似的结构可以使变异下限更紧，从而使推断更准确。
来自人类专家示范的模仿学习已被证明对具有稀疏环境奖励的挑战性强化学习问题有很大帮助。然而，不依靠专家示范而取得类似的成功是非常困难的。 最近关于自我模仿学习的工作表明，模仿代理人自己过去的良好经验可以间接地推动在某些环境中的探索，但这些方法往往会导致次优和近视行为。为了解决这个问题，我们认为通过模仿不同的轨迹在不同的方向进行探索，而不是专注于有限的良好轨迹，对于困难的探索任务是更理想的。 我们提出了一种新的方法，即从代理人自己的过去经验中学习轨迹条件的策略来模仿不同的轨迹，并表明这种自我模仿有助于避免近视行为，并增加找到硬探索任务的全局最优解的机会，特别是在有误导性奖励的时候。 我们的方法在各种具有局部最优的硬探索任务上明显优于现有的自我模仿学习和基于计数的探索方法。特别是，我们报告了在不使用专家示范或重设为任意状态的情况下，在Montezumas Revenge上的最先进得分超过20,000分。
我们提出了一种训练大容量神经网络的方法，大大提高了准确性并降低了动态计算成本。这是在细粒度上对深度学习架构进行控制实现的。 为了实现这一目标，我们引入了一个新的剩余块架构，以细粒度的方式对卷积通道进行门控。我们还引入了一个普遍适用的工具批量整形，将神经网络中特征的边际聚合后验与预先指定的先验分布相匹配。我们使用这种新技术来迫使门控更多地以数据为条件。 我们在CIFAR-10和ImageNet数据集上展示了图像分类的结果，在Cityscapes上展示了语义分割的结果。我们的结果显示，我们的方法可以有条件地缩小大型架构，从而使数据的平均计算成本与小型架构相当，但具有更高的准确性。 特别是在ImageNet上，我们的ResNet50和ResNet34门控网络获得了74.60%和72.55%的最高准确率，而基线ResNet18模型的准确率为69.76%，复杂度相似。我们还表明，所产生的网络自动学习对困难的例子使用更多的特征，对简单的例子使用更少的特征。
为了弥合深度学习和符号人工智能之间的差距，我们提出了一个新的端到端神经网络架构，它可以学习从原始像素数据中形成具有明确关系结构的命题表征。为了评估和分析该架构，我们引入了一系列不同复杂度的简单视觉关系推理任务。 我们表明，与一些基线架构相比，拟议的架构在对这些任务的课程进行预训练时，能够学习生成可重复使用的表征，从而更好地促进对先前未见过的任务的后续学习。
在自然语言推理中，一些词的语义并不影响推理。这样的信息被认为是肤浅的，会带来过度的拟合。在本文中，我们使用一阶逻辑（FOL）--一种来自意义表示语言的经典技术--来解释对于一个给定的句子对来说什么是肤浅的信息。这种解释也根据其属性提出了两种归纳偏向。我们提出一种基于神经网络的方法，利用这两种归纳偏向。
我们提出了一种训练机器学习模型的方法，其公平性在于其性能在特征的某些扰动下是不变的。例如，简历筛选系统的性能在申请人姓名的变化下应该是不变的。我们通过将其与Dwork等人提出的个人公平性的原始概念联系起来，将这种直观的公平性概念正式化，并表明所提出的方法实现了这种公平性概念。我们还在两个容易受性别和种族偏见影响的机器学习任务上证明了该方法的有效性。
在本文中，我们提出了一个种子-增强-训练-转移（SAT）框架，该框架包含一个合成种子图像数据集的生成程序，用于使用免费提供的开放字体文件数据集的不同数字系统的语言。这个种子图像数据集然后被增强以创建一个纯粹的合成训练数据集，这又被用来训练一个深度神经网络并在持有的真实世界手写数字数据集上进行测试，该数据集涵盖五个印度文字，卡纳达语、泰米尔语、古吉拉特语、马拉雅拉姆语和瓦那加里语。 我们通过训练一个能生成五种语言的真实数字图像的寻界GAN（BGAN），以及通过在现实世界的数据集上测试一个在合成数据上训练的CNN，从质量上展示了这种方法的功效。这不仅在字体数据集世界和迁移学习之间建立了一个有趣的联系，而且为任何文字的通用数字分类提供了一个配方。
机器学习的一个重要研究方向是围绕着开发元学习算法来解决少数学习问题。一个特别成功的算法是模型不可知元学习（MAML），这种方法由两个优化循环组成，外循环找到一个元初始化，内循环可以从中有效地学习新任务。 尽管MAML很受欢迎，但一个基本的开放性问题仍然存在--MAML的有效性是由于元初始化为快速学习做了准备（表征的大量有效变化），还是由于特征重用，因为元初始化已经包含了高质量的特征？我们通过消融研究和潜在表征的分析来研究这个问题，发现特征重用是主导因素。 这导致了ANIL（几乎没有内循环）算法，这是MAML的一个简化，我们删除了所有的内循环，但底层神经网络的（特定任务）头部除外。ANIL与MAML在基准的几张图片分类和RL上的性能相匹配，并提供了比MAML更好的计算能力。 我们进一步研究了网络头部和主体的精确贡献，表明测试任务的性能完全由所学特征的质量决定，我们甚至可以移除网络的头部（NIL算法）。我们最后讨论了更广泛的元学习算法的快速学习与特征重用问题。
模型训练仍然是机器学习应用中最主要的财务成本和时间投资。开发和调试模型往往涉及反复训练，进一步加剧了这一问题。虽然增量训练可以通过在一小部分数据上训练现有模型来节省大量时间和成本，但很少有工作探讨确定增量训练何时提供足够的模型性能与完全重新训练的政策。 我们提供了一种决定何时进行增量训练与全面训练的方法无关的算法。我们把这种非确定的全面或增量训练的设置称为 "混合设置训练"。在对填槽任务进行评估时，我们发现这种算法提供了一个有约束的误差，避免了灾难性的遗忘，并导致比总是全面训练的政策有显著的速度提高。
神经网络在许多推理任务中都取得了成功。从经验上看，这些任务需要专门的网络结构，例如：。图形神经网络（GNN）在许多这样的任务中表现良好，而结构较差的网络则失败了。理论上，人们对一个网络结构为什么以及何时能比其他同样具有表现力的网络结构概括得更好理解有限。我们开发了一个框架，通过研究一个网络的结构与相关推理程序的算法结构的吻合程度，来表征哪些推理任务可以学得好。 我们正式定义了算法一致性，并推导出一个样本复杂度约束，该约束随着一致性的提高而降低。这个框架解释了流行推理模型的经验成功，并提出了它们的局限性。我们通过一个强大的算法范式--动态编程（DP）的视角，统一了看似不同的推理任务，如直觉物理学、视觉问题回答和最短路径。我们表明，GNN可以学习DP，从而解决这些任务。
细胞与细胞之间的相互作用在肿瘤发生中起着不可或缺的作用，因为它们是管理免疫反应的关键。因此，调查特定的细胞与细胞之间的相互作用不仅有可能扩大对肿瘤发生的理解，而且还能指导病人对癌症免疫疗法的临床管理。 最近一种探索细胞-细胞相互作用的成像技术--飞行时间多重离子束成像（MIBI-TOF），可以在原位以高分辨率多重图像的方式对36种不同的蛋白质标记进行量化。 通过语义分割图对细胞类型、大小和邻域的图像生成进行调节，我们能够观察到这些因素如何同时影响不同蛋白质通道中的细胞-细胞相互作用。此外，我们设计了一套指标，并首次提供了对细胞空间方向、细胞蛋白质表达和细胞邻域的见解。 我们的模型，细胞-细胞相互作用GAN（CCIGAN），在所有传统的衡量标准上优于或匹配现有的图像合成方法，并在生物学动机的衡量标准上明显优于。据我们所知，我们是第一个通过图像合成系统地模拟多种细胞蛋白行为和模拟条件下的相互作用。
问题回答（QA）的机器学习模型，即给定一个问题和一段话，学习者必须在这段话中选择一些跨度作为答案，已知是很脆的。通过在这段话中插入一个讨厌的句子，对手可以欺骗模型选择错误的跨度。 一种有希望的新的质量保证方法将任务分解为两个阶段：(i)从段落中选择相关的句子；(ii)在这些句子中选择一个跨度。直觉上，如果句子选择器排除了违规的句子，那么下游的跨度选择器将是稳健的。 虽然最近的工作暗示了两阶段质量保证的潜在鲁棒性，但据我们所知，这些方法从未明确地与对抗性训练相结合。本文对对抗性鲁棒性进行了彻底的实证调查，表明尽管两阶段方法落后于单阶段跨度选择，但对抗性训练大大改善了其性能，导致F1得分比对抗性训练的单阶段模型提高了22分以上。
本研究的目的是引入一个用于分析和综合驾驶辅助系统的形式化框架。它将形式化方法用于验证使用认知架构ACT-R建立的随机人类驾驶模型，然后通过设计可证明正确的高级驾驶辅助系统来引导半自动车辆的安全性。 主要贡献包括在半自动系统的形式分析中整合概率ACT-R模型，以及一种抽象技术，该技术能够以马尔科夫模型的形式有限地表示一个大维的连续系统。该方法的有效性在不同条件下的几个案例研究中得到了说明。
与19世纪的旧书写系统相比，现代夏威夷正字法采用了长元音和喉塞的字符。这些额外的字符约占夏威夷语中音素的三分之一，因此包括它们对阅读理解和发音有很大的影响。然而，在手动执行时，在旧文本和新文本之间进行音译是一项艰巨的任务。鉴于没有足够的数据来训练一个端到端的深度学习模型，我们引入两种相关方法来帮助自动解决这种音译问题。 我们发现，混合方法优于端到端FST，因为它将原始问题划分为一个可以手工建模的部分，使用FST，而另一个部分则可以通过在现有数据上训练的RNN轻松解决。
在许多现实世界的环境中，学习模型必须进行少数次分类：学习从未见过的类别中对例子进行分类，每个类别只使用少数标记的例子。此外，为了安全地部署，它应该有能力检测出分布外的输入：不属于任何类别的例子。 在这项工作中，我们提出了在少数情况下进行分布外检测的任务，并在四个流行的少数情况分类数据集基础上建立了基准数据集。 总而言之，我们在新的基准数据集上使用标准指标建立了分布外检测的基线结果，并展示了我们提出的方法的改进结果。
虽然现代生成模型能够合成高保真、有视觉吸引力的图像，但成功生成对识别任务有用的例子仍然是一个难以实现的目标。为此，我们的关键见解是，例子应该被合成，以恢复分类器的决策边界，这些决策边界将从大量的真实例子中学到。更具体地说，我们把在合成例子上训练的分类器视为 "学生"，把在真实例子上训练的分类器视为 "老师"。 通过将知识提炼引入元学习框架，我们鼓励生成模型以一种使学生分类器模仿教师行为的方式产生例子。为了缓解学生和教师分类器之间的潜在差距，我们进一步提议以渐进的方式提炼知识，或者逐渐加强教师，或者削弱学生。我们展示了使用我们的模型无关的提炼方法来处理数据稀缺问题，大大改善了miniImageNet和ImageNet1K基准上的少量学习性能。
深度神经网络为许多感兴趣的应用提供了最先进的性能。不幸的是，它们已知容易受到对抗性例子的影响，这些例子是通过对原始输入进行小规模但恶意的扰动而形成的。此外，扰动可以跨模型转移：为特定模型生成的对抗性例子往往会误导其他未见过的模型。因此，对抗者可以利用它来攻击部署的黑盒系统。在这项工作中，我们证明了对抗性扰动可以被分解成两个部分：特定模型和数据依赖的部分，而后者才是造成可转移性的主要原因。在这种理解的激励下，我们建议通过利用接近数据依赖部分的降噪梯度（NRG）来制作对抗性例子。 在ImageNet上训练的各种分类模型的实验表明，新方法极大地提高了可转移性。我们还发现，在测试性能相当的条件下，低容量模型比高容量模型具有更强大的攻击能力。 这些见解产生了一种原则性的方式来构建具有高成功率的对抗性例子，并有可能为我们设计有效的防御方法来对抗黑箱攻击提供指导。
我们提出了迭代的双通道分解流程，以加速现有的卷积神经网络（CNN）。 我们提出的等级选择算法可以有效地确定低等级近似的目标卷积层的适当等级。我们的双程CP分解有助于防止不稳定问题。
我们介绍了LiPopt，这是一个多项式优化框架，用于计算神经网络的Lipschitz常数的越来越严格的上界。底层优化问题归结为线性（LP）或半无限（SDP）编程。 我们对具有随机权重的网络以及在MNIST上训练的网络进行了实验，表明在$\ell_infty$-Lipschitz常数的特殊情况下，与文献中的其他基线相比，我们的方法产生了卓越的估计。
尽管在元学习的帮助下，几率学习的研究进展迅速，但其实际用途仍然有限，因为大多数研究都假定所有的元训练和元测试的例子都来自于单一领域。我们提出了一种简单而有效的几率分类方法，其中任务分布跨越了多个领域，包括在元训练期间以前未见过的领域。 这将复杂任务分布上的特定任务适应简化为一个简单的选择问题，而不是在元测试时用一些参数来修改模型。受常见的多任务学习技术的启发，我们让池中的所有模型共享一个基础网络，并为每个模型添加一个单独的调制器，以自己的方式完善基础网络。这种结构允许池保持表示的多样性，每个模型也有领域不变的表示。实验表明，当目标任务可能来自许多不同的领域时，我们的选择方案优于其他少数的分类算法。他们还发现，对于来自未见过的领域的任务，聚合所有组成模型的输出是有效的，显示了我们框架的有效性。
在2019年，仍有许多扫描文件以非数字格式进入企业。要从现实世界的文件中提取的文本往往依偎在丰富的格式中，如表格结构或带有填空框或下划线的表格，其墨水经常触及甚至击穿文本本身的墨水。 我们设计了一种方法，以程序化的方式用真实的人工制品增加文本图像，并使用它们以弱监督的方式训练分割网络。除了高分割精度外，我们还表明，我们清洗过的图像在下游识别精度上得到了明显的提升，如Tesseract 4.0等流行的OCR软件。我们在国税局退税表扫描的非分布式数据集（NIST SDB）上测试了DeepErase，在印刷和手写文本的识别精度上都比基线有两位数的提高。
黑箱对抗性攻击需要大量的尝试，然后才能找到成功的对抗性例子，这些例子在视觉上与原始输入无法区分。目前依靠替代模型训练、梯度估计或遗传算法的方法往往需要过多的查询次数。因此，它们不适合现实世界的系统，因为最大查询次数由于成本而受到限制。 我们提出了一种查询效率高的黑盒攻击，它使用贝叶斯优化与贝叶斯模型选择相结合，对对抗性扰动和搜索空间降维的最佳程度进行优化。我们通过经验证明，与以前最先进的黑盒攻击相比，我们的方法能以2-5倍的查询次数达到相当的成功率。
由于多种异质信息源的存在，学习多模态表征从根本上说是一个复杂的研究问题。虽然多种模态的存在提供了额外的有价值的信息，但在从多模态数据中学习时，有两个关键的挑战需要解决：1）模型必须学习复杂的模内和跨模态的相互作用以进行预测；2）模型必须对测试期间意外缺失或噪声模态具有鲁棒性。 在本文中，我们建议对多模态数据和标签的生成-鉴别联合目标进行优化。我们引入一个模型，将表征分解为两组独立的因素：多模态鉴别因素和特定模态生成因素。 多模态判别因素是所有模态所共有的，包含判别任务（如情感预测）所需的联合多模态特征。特定模态生成因素对每个模态都是独特的，包含生成数据所需的信息。实验结果表明，我们的模型能够学习有意义的多模态表征，在六个多模态数据集上达到最先进或有竞争力的性能。 我们的模型通过对独立因素进行调节，展示了灵活的生成能力，并能在不显著影响性能的情况下重建缺失的模式。最后，我们解释了我们的因素化表征，以了解影响多模态学习的相互作用。
在现实世界的机器人应用中，灵活、通用的学习算法的成功应用往往受到其糟糕的数据效率的限制。为了应对这一挑战，具有多个感兴趣的主导任务的领域鼓励跨任务的信息共享，以限制所需的实验时间。为此，我们研究了分层策略形式的组合性归纳偏见，作为强化学习（RL）中跨任务的知识转移机制。 此外，我们证明了有效分解任务解决方案的额外激励措施的好处。我们的实验表明，这些激励措施是在多任务学习中自然给出的，并且可以很容易地引入到单一目标中。我们设计了一种RL算法，能够稳定和快速地学习结构化策略，并在非政策设置中有效地重复使用行为组件和跨任务的过渡数据。最后，我们在模拟环境以及物理机器人实验中评估了我们的算法，证明在数据数据效率方面比竞争基线有了很大的改善。
在本文中，我们研究了深度神经网络（DNN）的表示能力，这些网络属于片断-线性（PWL）函数家族，基于PWL激活单元，如整流器或最大值。我们通过研究PWL函数的线性区域数量来研究此类网络的复杂性。(2014), MontÂ´ufar (2017), and Raghu et al. (2017)的工作基础上，完善了整流和maxout网络的线性区域数量的上下限。除了实现更严格的界限，我们还开发了一种新方法，用混合整数线性公式对线性区域的数量进行精确计数或计算，将输入空间映射到输出。我们利用这种新能力来可视化线性区域的数量在训练DNNs时如何变化。 
卷积神经网络会记忆部分训练数据，这就是为什么要采用数据增强和剔除等策略来缓解过度拟合。本文考虑了 "成员推断 "的相关问题，其目标是确定在训练期间是否使用了一幅图像。 首先，我们展示了如何检测一个数据集是否被用于训练模型，特别是在训练时是否使用了一些验证图像。然后，我们介绍了一种新的方法，在少数顶层无法使用或被微调时推断成员资格，并表明较低层仍然携带关于训练样本的信息。为了支持我们的发现，我们在Imagenet和YFCC-100M的子集上进行了大规模的实验，并采用了VGG和Resnet等现代架构。
虽然生成对抗网络（GANs）在学习复杂的现实世界分布方面产生了令人印象深刻的结果，但最近的工作表明，它们缺乏多样性或模式崩溃。Arora等人（2017a）的理论工作提出了关于GANs's统计特性的困境：强大的判别器导致过度拟合，而弱的判别器无法检测到模式崩溃。 相比之下，我们在本文中表明，如果鉴别器类对特定的生成器类（而不是对所有可能的生成器）具有很强的鉴别能力，GANs原则上可以用多项式样本复杂性学习Wasserstein距离（或许多情况下的KL-分歧）中的分布。 对于各种生成器类，如高斯坦混合物、指数族、可逆和注入式神经网络生成器，我们设计了相应的判别器（通常是特定结构的神经网络），这样由判别器引起的积分概率度量（IPM）可以证明近似于Wasserstein距离和/或KL-分歧。 这意味着，如果训练成功，那么学到的分布就接近于Wasserstein距离或KL发散的真实分布，因此不能丢掉模式。我们的初步实验表明，在合成数据集上，测试IPM与KL发散或Wasserstein距离有很好的相关性，表明GANs缺乏多样性可能是由优化中的次优性而不是统计低效率造成的。
我们展示了随机梯度下降法的超参数是如何影响梯度的协方差（K）和训练损失的Hessian（H）的。基于一个理论模型，我们预测在训练的早期阶段使用高学习率或小批处理量会使SGD进入参数空间的区域：（1）减少K的谱准则，和（2）改善K和H的调节。我们表明，在训练过程中，这些效果保持的轨迹点，我们称之为盈亏平衡点，是在训练的早期达到的。最后，我们将我们的分析应用于具有批量规范化（BN）层的网络，并发现有必要使用高的学习率来实现以前仅归因于BN的损失平滑效果。
图形卷积网络（GCN）被认为是半监督学习中最有效的图形模型之一，但它仅仅通过信息传播来提取一阶或几阶邻域信息，对于更深层次的结构来说，性能会有所下降。 因此，我们提出了一种无监督的方法来描述这种相似性，并通过Lasso自动学习高阶邻居的权重矩阵，使一阶和高阶邻居之间的特征损失最小化，在此基础上，我们为GCN制定了新的卷积滤波器来学习更好的节点表征。我们的模型，称为高阶加权GCN（HWGCN），在Cora、Citeseer和Pubmed数据集的一些节点分类任务上取得了最先进的结果。
深度神经网络的性能通常归功于其自动化的、与任务相关的特征构建。不过，这仍然是一个开放性的问题，为什么这会导致具有良好泛化能力的解决方案，即使在参数数量大于样本数量的情况下。早在90年代，Hochreiter和Schmidhuber就观察到，局部最小值周围损失表面的平整度与低泛化误差相关。 然而，最近有研究表明，现有的平坦度测量方法在理论上不能与泛化相关：如果一个网络使用ReLU激活，网络函数可以被重新参数化，而不改变其输出的方式，平坦度几乎任意改变。本文提出了现有平坦度测量方法的自然修改，导致对重新参数化的不变性。 本文提出的措施意味着网络对输入和隐藏层变化的鲁棒性。将这种特征鲁棒性与泛化联系起来，导致了对数据代表性的概括性定义。有了这个定义，在代表性数据上训练的模型的泛化误差可以由其特征鲁棒性来约束，这取决于我们新颖的平坦性措施。
贝叶斯方法已经成功地应用于神经网络的权重稀疏化，并从网络中去除结构单元，如神经元。具体来说，除了单个权重和神经元的稀疏化，我们建议稀疏化门的预激活和LSTM中的信息流。它使一些门和信息流组件恒定，加速前向传递并改善压缩。
提高数值方法的精度仍然是许多学科的核心挑战，对于非线性模拟问题尤其重要。这类问题的一个代表性例子是流体流动，为了达到对复杂流动现象的有效模拟，已经对其进行了深入研究。 然后，我们采用了一个神经网络，推断出一个修正，以使粗略的从而快速获得的结果更接近参考数据。我们提供了对不同学习方法的目标学习问题的见解：带有天真和优化数据采集的完全监督学习方法，以及带有可微分的纳维-斯托克斯求解器的无监督学习方法。虽然我们的方法非常普遍，适用于任意偏微分方程模型，我们特别强调流体流动模拟精度的提高。
患者的健康信息通常是分散在不同的筒仓中的。尽管以一种支持快速学习医疗系统的方式将数据联合起来进行分析在技术上是可行的，但隐私问题和监管障碍限制了数据的集中化。机器学习可以以联合的方式在具有相同变量集的患者数据集上进行，但在不同的医疗地点是分开的。 但联合学习不能处理一个特定病人的不同数据类型在不同组织中垂直分离的情况。我们把能够在两个或更多程度分离的数据上进行机器学习模型训练的方法称为 "联合机器学习"。我们建立并评估了一个联合机器学习模型，对老年人意外跌倒的风险进行分层。
现有的神经网络很容易受到 "对抗性例子 "的影响--通过在输入中添加恶意设计的小扰动来诱发网络的错误分类。最值得研究的防御策略是对抗性训练，用对抗性例子来增加训练数据。 然而，在对抗训练中应用单步对抗并不支持网络的鲁棒性，相反，它们甚至会使网络过拟合。与单步对抗相比，多步训练在MNIST和CIFAR10上的表现是最先进的，但它需要大量的时间。 因此，我们提出了一种方法，随机量化激活（SQA），解决了单步对抗性训练中的过拟合问题，并快速实现了与多步训练相媲美的鲁棒性。SQA通过为激活函数提供随机选择性来削弱对抗性影响，并允许网络仅通过单步训练来学习鲁棒性。 在整个实验中，我们的方法对最强的白盒攻击之一表现出最先进的鲁棒性，就像PGD训练一样，但计算成本要低得多。最后，我们用SQA可视化网络的学习过程，以处理强敌，这与现有方法不同。
神经活动对重复刺激的反应是高度可变的。我们使用一个开放的数据集，即艾伦大脑观察站，来量化对重复的自然电影演示的反应分布。很大一部分反应是由对数正态分布或具有两个成分的高斯混合物最适合的。这些分布与来自深度神经网络中的单元的分布相似，有辍学现象。 我们使用一组单独的电生理记录，构建了一个群体耦合模型，作为对状态依赖性活动波动的控制，并发现模型残差也显示出非高斯分布。我们随后分析了来自不同电影片段多个部分的试验的反应，并观察到皮质中的噪声与片段内与片段外的刺激变化更一致。
近年来，无监督领域的适应性得到了极大的关注。大多数现有的工作都是处理封闭的场景，假设源域和目标域共享完全相同的类别。在本文中，我们通过对最先进的领域适应技术--Self-Ensembling的增强来解决这个问题，该技术在目标域中具有类别无关的群集。 具体来说，我们提出了带有类别无关的集群的Self-Ensembling（SE-CC）--一种新颖的架构，在目标领域特定的类别无关的集群的额外指导下引导领域适应。这些集群信息提供了特定领域的视觉线索，促进了Self-Ensembling在封闭场景和开放场景中的普及。 从技术上讲，聚类首先在所有未标记的目标样本上进行，以获得类别无关的聚类，这些聚类揭示了目标领域特有的底层数据空间结构。聚类分支被用来确保学习到的表示保留了这种底层结构，即把聚类上的估计分配与每个目标样本的固有聚类分布相匹配。 此外，SE-CC通过互信息最大化增强了学习到的表示。我们在Office和VisDA数据集上进行了广泛的实验，用于开放集和封闭集的领域适应，与最先进的方法相比，报告了卓越的结果。
我们提出了光谱推理网络，这是一个通过随机优化学习线性算子特征函数的框架。光谱推理网络将慢速特征分析推广到一般的对称算子，并与计算物理学中的变异蒙特卡洛方法密切相关。因此，它们可以成为从视频或图形结构数据中进行无监督表示学习的有力工具。 我们将训练光谱推理网络作为一个二层优化问题，允许在线学习多个特征函数。我们展示了在量子力学问题上训练光谱推理网络的结果，以及在合成数据集上进行视频特征学习的结果。
Tensor-Train factorization（TTF）是一种有效的方法，可以压缩循环神经网络（RNN）中全连接层和循环层的大权重矩阵。然而，所有参数的核心张量的高Tensor-Train等级需要从元素上固定，这导致了模型参数的不必要的冗余。这项工作应用黎曼尼随机梯度下降（RSGD）来训练黎曼尼流形中参数的核心张量，然后找到参数的低Tensor-Train等级向量。 本文首先介绍了RSGD算法的收敛分析，然后在更高级的张量训练RNN上进行了测试，如双向GRU/LSTM和具有张量训练注意力模型的编码器-解码器RNN。在数字识别和机器翻译任务上的实验表明，RSGD算法对张量训练RNN的有效性。
在本文中，我们考虑了学习控制策略的问题，这些策略在优化向量函数的同时，由于安全、公平或其他成本的考虑而满足约束条件。我们提出了一种新的算法--基于投影的约束策略优化（PCPO），这是一种在两步过程中优化策略的迭代方法--第一步执行无约束的更新，而第二步通过将策略投射回约束集上来协调约束的违反。 我们从理论上分析了PCPO，并提供了奖励改进的下限，以及每次政策更新的约束违反的上限。我们根据两个不同的指标--L2规范和Kullback-Leibler分歧，进一步描述了PCPO与投影的收敛性。
深度网络面临的挑战是如何确保它们对那些不能被从训练数据中学到的信息有效表示的输入的鲁棒性。为了补充从基于激活的表示中学到的信息，我们建议利用基于梯度的表示，明确关注缺失信息。 为了验证所提出的方法的有效性，我们比较了基于梯度的表示和基于激活的表示的异常检测性能。我们表明，在CIFAR-10和CURE-TSR数据集中，就所有类别的平均AUROC而言，基于梯度的表示比基于激活的表示高出0.093和0.361。 同时，我们提出了一种使用基于梯度表示的异常检测算法，称为GradCon，并在三个基准数据集上验证了其性能。所提出的方法在CIFAR-10、MNIST和fMNIST数据集上的平均AUROC分别为0.664、0.973和0.934，超过了大多数最先进的算法。
医学图像可能包含各种类型的人工制品，具有不同的模式和混合物，这取决于许多因素，如扫描设置、机器条件、患者的特征、周围环境等。然而，现有的基于深度学习的人工制品减少方法受到其训练集的限制，具有特定的预先确定的人工制品类型和模式。 具体来说，我们利用图像的低内部视觉熵，训练一个轻量级的图像特定的伪影减少网络，在测试时减少图像中的伪影。我们使用计算机断层扫描（CT）和磁共振成像（MRI）作为载体，表明ZSAR可以在质量和数量上比最先进的减少伪影，同时使用更短的执行时间。
归因方法为机器学习模型（如人工神经网络）的决策提供了见解。对于一个给定的输入样本，他们为每个单独的输入变量（如图像的像素）分配一个相关性分数。在这项工作中，我们将信息瓶颈的概念适应于归因。通过向中间特征图添加噪音，我们限制了信息的流动，并可以量化（比特）图像区域提供多少信息。 我们在VGG-16和ResNet-50上使用三种不同的指标将我们的方法与10个基线进行了比较，发现我们的方法在6个设置中的5个中优于所有基线。该方法的信息理论基础为归因值（比特）提供了一个绝对的参考框架，并保证得分接近零的区域对网络的决策是不必要的。
递归神经网络（RNN）被用于语音识别、机器翻译和语言建模等领域的最先进的模型中。稀疏是一种减少深度学习模型的计算和内存要求的技术。稀疏RNN更容易部署在设备和高端服务器处理器上。 即使稀疏操作相对于密集操作需要更少的计算和内存，但在不同的硬件平台上，通过使用稀疏操作观察到的速度提升低于预期。为了解决这个问题，我们研究了两种不同的方法来诱导RNN中的块状稀疏性：修剪层中的权重块和使用组套索正则化与修剪来创建带零的权重块。 使用这些技术，我们可以创建具有80%到90%稀疏度的块状稀疏RNN，并且在精度上有小的损失。这个技术允许我们将模型的大小减少大约10倍。此外，我们可以修剪一个更大的密集网络来恢复精度上的损失，同时保持高块状稀疏度并减少整体参数数。
值迭代网络是用卷积神经网络实现的值迭代（VI）算法的近似，以使VI完全可分。在这项工作中，我们在机器人运动规划的背景下研究这些网络，重点是行星车的应用。基于学习的运动规划的关键挑战任务是学习从地形观测到合适的导航奖励函数的转换。为了处理复杂的地形观测和策略学习，我们提出一个值迭代递归，被称为软值迭代网络（SVIN）。 SVIN旨在通过价值迭代网络产生更有效的训练梯度。它依赖于软政策模型，其中政策用所有可能的行动的概率分布来表示，而不是只返回最佳行动的确定性政策。我们在机器人运动规划场景中证明了所提方法的有效性。特别是，我们研究了SVIN在行星探测器导航中非常具有挑战性的问题上的应用，并在目前正在火星上运行的好奇号探测器收集的数据上展示了早期训练结果。
变形金刚网络在语言建模和机器翻译方面取得了重要进展。这些模型包括两个连续的模块，一个前馈层和一个自我注意层。后者允许网络捕捉长期的依赖关系，通常被视为变形金刚成功的关键因素。 更确切地说，我们用持久性记忆向量来增强自我注意层，这些向量发挥着与前馈层类似的作用。由于这些向量，我们可以在不降低变形器性能的情况下去除前馈层。我们的评估显示了我们的模型在标准字符和词级语言建模基准上带来的好处。
这项工作将神经网络视为数据生成系统，并在该数据上应用异常模式检测技术，以检测网络何时在处理一组异常输入。 检测异常是多个机器学习问题的关键组成部分，包括检测输入中是否存在对抗性噪声。更广泛地说，这项工作是向赋予神经网络检测非分布样本组的能力迈出的一步。 这项工作将异常模式检测领域的 "子集扫描 "方法引入检测神经网络异常输入的任务中。 子集扫描法使我们能够回答这个问题。"哪些输入子集在哪些节点子集有大于预期的激活？"  以这种方式框定对抗性检测问题，使我们能够识别激活空间中的系统模式，这些模式横跨多个对抗性噪声图像。 这样的图像是 "怪异的"。 利用这种常见的异常模式，我们显示出随着测试集中有噪声图像比例的增加，检测能力也在增加。  我们提供了使用基本迭代法攻击的20层ResNet上的CIFAR-10图像的目标对抗性噪声的检测能力和准确性结果。
稳定性是动态系统的一个基本属性，但迄今为止，它对递归神经网络的实践几乎没有影响。在这项工作中，我们对稳定的递归模型进行了彻底的调查。理论上，我们证明稳定的递归神经网络在推理和梯度下降训练方面都能被前馈网络近似。 经验上，我们证明了稳定的递归模型在基准序列任务上的表现往往与不稳定的对应模型一样好。总之，这些发现揭示了递归网络的有效能力，并表明大部分序列学习都发生在或可以发生在稳定的体系中。此外，我们的结果有助于解释为什么在许多情况下，实践者成功地用前馈模型取代了递归模型。
权重共享在许多深度神经网络的成功中发挥了重要作用，它提高了内存效率，并将有关问题的有用的归纳先验纳入网络中。但了解如何在一般情况下有效使用权重共享是一个尚未被广泛研究的话题。Chen等人（2015）提出了HashedNets，它用哈希表增强了多层感知器，作为一种神经网络压缩方法。 我们将这种方法概括为一个框架（ArbNets），允许有效的任意分权，并利用它来研究神经网络中分权的作用。我们表明，常见的神经网络可以用不同的哈希函数表示为ArbNets。我们还提出了两个新的哈希函数，Dirichlet哈希和Neighborhood哈希，并利用它们来实验证明，平衡和确定性的分权有助于神经网络的性能。
我们介绍了神经马尔科夫逻辑网络（NMLNs），这是一个借用马尔科夫逻辑思想的统计关系学习系统。与马尔科夫逻辑网络（MLNs）一样，NMLNs是一个指数族模型，用于对可能世界的分布进行建模，但与MLNs不同，它们不依赖于明确规定的一阶逻辑规则。相反，NMLNs学习这种规则的隐含表示，作为神经网络，在关系结构的片段上充当潜在的函数。 有趣的是，任何MLN都可以表示为NMLN.与最近提出的神经定理证明器（NTPs）（Rocktaschel at al. 2017）类似，NMLNs可以利用常数的嵌入，但与NTPs不同，NMLNs在没有常数的情况下也能很好地工作，这对于在过渡性环境之外的预测极为重要。
使用变异贝叶斯神经网络，我们开发了一种算法，能够将来自多个不同任务的知识积累到先验中。这导致了丰富的先验能够在新任务上进行少量学习。 对玩具任务的分析表明，它可以从明显不同的任务中学习，同时找到它们之间的相似性。在Mini-Imagenet上的实验达到了最先进的水平，5次学习的准确率为74.5%。最后，我们提供了两个新的基准，每个都显示了现有元学习算法的失败模式，如MAML和原型网络。
为了学习稳健的跨环境的序列描述，我们引入了disentangled状态空间模型（DSSM）。 我们的经验表明，这种分离能够实现稳健的预测、序列操作和环境表征。我们还提出了一个基于VAE的无监督训练程序来学习DSSM作为贝叶斯过滤器。在我们的实验中，我们展示了在不同的重力影响下控制生成和预测弹跳球视频序列的最先进性能。
在这项工作中，我们将单次和少数次学习问题作为为每一类寻找好的原型的方法，其中这些原型可以推广到新的数据样本和类别。我们提出了一个度量学习器，通过学习其基础凸函数来学习布雷格曼发散。布雷格曼发散是这个框架的良好候选者，因为它们是唯一一类具有一组点的最佳代表是由其平均值给出的发散。 我们提出了对原型网络的灵活扩展，以实现嵌入和发散的联合学习，同时保持计算效率。我们的初步结果与之前在Omniglot和Mini-ImageNet数据集上的工作相当，这两个标准的基准是一次性学习和少量学习。我们认为我们的模型可以用于其他涉及度量学习的任务或需要近似凸性的任务，如结构化预测和数据完成。
在生物神经网络的灵活性的激励下，我们引入了非限制性递归网络（URN），并证明它可以在通过梯度下降训练期间表现出类似的灵活性。 这些不同的结构可以通过在一个单一的通用损失函数上的梯度下降而得到，其中数据的结构和各种调节器项的相对强度决定了出现的网络的结构。我们表明，当考虑到网络的对称性以及输入数据的几何特性时，这个损失函数和调节器是自然产生的。
我们提出了CROSSGRAD，一种使用多领域训练数据学习分类器的方法，该分类器可以泛化到新的领域。CROSSGRAD不需要通过标记或未标记数据的适应阶段，也不需要新领域的领域特征。大多数现有的领域适应方法试图使用领域对抗训练等技术来消除领域信号。 相比之下，CROSSGRAD可以自由地使用领域信号来预测标签，如果它能防止训练领域的过度拟合的话。我们在贝叶斯设置中对任务进行了概念化，其中采样步骤被实现为数据增强，基于领域指导的输入实例的扰动。 CROSSGRAD联合训练了一个标签和一个领域分类器，这些例子被彼此目标的损失梯度所扰动。这使我们能够直接扰动输入，而不需要分离和重新混合领域信号，同时做出各种分布假设。 在三个不同的应用中，这种设置是自然的，其经验评估表明：（1）与一般的实例扰动方法相比，领域指导的扰动对未见过的领域提供持续的更好的泛化；（2）数据增强是一种比领域对抗性训练更稳定和准确的方法。
我们提出了sketch-rnn，一个能够构建基于笔画的普通物体的循环神经网络。该模型是在代表许多不同类别的人类绘画图像的数据集上训练的。我们概述了一个有条件和无条件的草图生成框架，并描述了用于生成矢量格式的连贯草图的新稳健训练方法。
Wilson等人（2017）表明，当步长计划设计得当时，随机梯度的泛化效果比ADAM（Kingma & Ba, 2014）更好。鉴于最近关于超梯度方法的工作（Baydin et al, 2018），我们重新审视这些说法，看看这种方法是否缩小了最流行的优化器之间的差距。作为副产品，我们分析了这些超梯度方法与更经典的时间表相比的真正好处，如Wilson等人（2017）的固定衰减。特别是，我们观察到他们的帮助是微不足道的，因为在调整他们的超参数时，他们的性能变化很大。最后，由于稳健性是优化器的一个关键质量，我们提供了这些基于梯度的优化器的敏感性分析，评估其调整的挑战性。
尽管关于强化学习算法和应用的文献越来越多，但对它们的统计推断却知之甚少。在本文中，我们研究了Q值估计的大样本行为，并对渐进方差进行了闭式表征。这使我们能够有效地构建Q值和最优值函数的置信区，并制定政策来最小化它们的估计误差。这也导致了一种政策探索策略，它依赖于估计Q值估计之间的相对差异。
我们在源域$X$和目标域$Y$之间进行完全无监督的单边图像转换，这样我们就可以保留相关的基础共享语义（例如，类别、大小、形状等）。特别是，我们对一种比文献中通常涉及的情况更困难的情况感兴趣，即源域和目标域足够 "遥远"，以至于重建式或像素式的方法都失败了。我们认为，转移（即，emph{翻译}）所述的相关信息应该包括放弃源域的特定信息，同时纳入目标域的特定信息，后者我们用一个噪声先验分布建模。为了避免生成的样本仅由先验分布解释的退化情况，我们建议最小化生成的样本和先验分布中的样本之间的相互信息的估计。我们展示了在MNIST到SVHN任务中无监督的图像到图像翻译的最新结果。
识别图像中的突出点是视觉运动学、运动结构学或SLAM算法的重要组成部分。最近，一些学习关键点的方法在具有挑战性的基准上表现出令人信服的性能。 然而，为自然图像中的兴趣点检测生成一致和准确的训练数据仍然具有挑战性，特别是对人类注释者而言。我们介绍了IO-Net（即InlierOutlierNet），这是一个用于关键点检测、描述和匹配的自我监督的新型代理任务。 通过在关键点学习框架内使来自点对对应的inlier-outlier集的抽样完全微分化，我们表明能够同时自我监督关键点描述和改善关键点匹配。第二，我们介绍了KeyPointNet，一个特别适合于强大的关键点检测和描述的关键点网络架构。 我们设计的网络允许局部关键点聚集，以避免因该任务常用的空间离散而产生的伪影，我们通过利用高效的亚像素卷积将描述器特征图上采样到更高的操作分辨率来改善细粒度的关键点描述器性能。通过广泛的实验和消融分析，我们表明，所提出的自监督关键点学习方法在挑战性的基准上比最先进的方法大大提高了特征匹配和同质性估计的质量。
我们研究了内在动机在稀疏奖励协同任务中作为强化学习的探索偏向的作用，协同任务是指多个代理必须一起工作以实现他们单独无法实现的目标。我们的关键想法是，协同任务中内在动机的一个良好指导原则是采取影响世界的行动，如果代理单独行动则无法实现。 我们研究了这个想法的两个实例，一个是基于遇到的真实状态，另一个是基于与政策同时训练的动力学模型。虽然前者比较简单，但后者的好处是对所采取的行动可以进行分析。 我们在具有稀疏奖励的机器人双臂操作任务中验证了我们的方法；我们发现我们的方法产生了比以下两种方法更有效的学习：1）只用稀疏奖励进行训练；2）使用典型的基于惊喜的内在动机的表述，它不偏向于协同行为。视频可在项目的网页上找到：https://sites.google.com/view/iclr2020-synergistic。
一个一般的图结构神经网络架构通过两个核心部分在图上运行。(1)足够复杂的信息函数；(2)固定的信息聚合过程。在本文中，我们提出了政策信息传递算法，该算法从概率的角度出发，将整个信息聚合重新表述为随机的顺序过程。该算法在更大的搜索空间上工作，利用推理历史来进行推理，并且对噪声边缘具有鲁棒性。我们将我们的算法应用于多个复杂的图推理和预测任务，显示我们的算法始终以显著的优势优于最先进的图结构模型。
深度多任务网络，其中一个神经网络产生多个预测输出，比其单任务对应的网络更具有可扩展性，并且通常具有更好的正则化。这些优势有可能导致速度和性能的提高，但多任务网络也很难在任务之间找到正确的平衡点进行训练。 我们提出了一种新的梯度归一化（GradNorm）技术，它通过直接调整梯度来平衡任务训练率，从而自动平衡多任务损失函数。我们表明，对于各种网络架构，对于回归和分类任务，以及在合成和真实数据集上，GradNorm提高了准确性，减少了对单一网络、静态基线和其他自适应多任务损失平衡技术的过度。 GradNorm还匹配或超过了详尽的网格搜索方法的性能，尽管只涉及一个单一的不对称超参数$alpha$。因此，曾经是一个乏味的搜索过程，每增加一个任务就会产生指数级的计算量，现在可以在几次训练中完成，无论任务的数量如何。最终，我们希望证明梯度操作为我们提供了对多任务网络训练动态的巨大控制，可能是释放多任务学习潜力的关键之一。
图像分割的目的是将属于同一物体或区域的像素分组。图像分割的核心问题是确定一个像素是在区域内还是在区域外，我们将其称为 "内部性 "问题。许多深度神经网络（DNNs）变体在分割基准中表现出色，但关于内部性，它们还没有被很好地可视化或理解。DNNs使用什么表征来解决内部性的长距离关系？架构选择如何影响这些表征的学习？在本文中，我们采取还原主义的方法，分析DNNs孤立地解决内部性问题，即确定封闭（约旦）曲线的内部。我们通过分析证明，最先进的前馈和递归架构可以实现对任何特定曲线的内部性问题的解决。 然而，当训练强制执行能够打破长距离关系的特定 "常规 "时，只有递归网络能够学习这些一般的解决方案。我们的结果强调了对新的训练策略的需求，该策略将学习分解为适当的阶段，并导致DNNs理解内部性所需的一般解决方案类别。
我们解决了深层表征学习的挑战性问题--将预先训练好的深层网络有效地适应不同的任务。具体来说，我们建议探索基于梯度的特征。这些特征是模型参数相对于给定输入样本的特定任务损失的梯度。我们的关键创新是设计了一个线性模型，它同时包含了梯度特征和网络的激活。 我们表明，我们的模型提供了一个底层深度模型的局部线性近似，并讨论了重要的理论见解。此外，我们提出了一个高效的算法，用于训练和推断我们的模型，而不计算实际梯度。
从单一图像中恢复三维几何形状、反照率和光照在许多领域都有广泛的应用，这也是一个典型的难题。为了消除模糊性，在重建过程中经常采用像从有限的扫描数据中学习的线性三维可变形模型（3DMM）这样的脸部先验知识。 然而，基于线性参数模型的方法不能很好地概括野外不同年龄、种族、表情、姿势和光线的面部图像。最近的方法旨在使用卷积神经网络（CNN）学习非线性参数模型，直接回归面部形状和纹理。然而，这些模型只在由线性3DMM生成的数据集上训练。 此外，在这些模型中，身份和表情表征是纠缠在一起的，这阻碍了许多面部编辑的应用。在本文中，我们以半监督的方式在混合批次的无标签和有标签的面部图像上训练我们的模型，以利用来自无约束的照片集的大量无标签的面部图像的价值。 我们引入了一种新的中心损失，以确保来自同一个人的不同面部图像具有相同的身份形状和反照率。此外，我们提出的模型将身份、表情、姿势和照明表征分开，这提高了整体重建性能，有利于面部编辑应用，例如。综合实验表明，与最先进的方法相比，我们的模型能够产生高质量的重建，并且对各种表情、姿势和光照条件具有鲁棒性。
人类的对话自然地围绕着相关的实体和连接的概念发展，同时也可能从一个话题转移到另一个话题。本文提出了ConceptFlow，它利用常识知识图来明确地模拟这种对话流，以更好地生成对话响应。ConceptFlow将对话输入作为潜在的概念空间的基础，并将潜在的对话流表示为沿着常识关系的概念流。 概念是由一个图注意机制引导的，该机制模拟了对话向不同概念发展的可能性。然后，对话响应使用语料文本和概念流的编码进行解码，将学到的对话结构整合到概念空间中。我们对Reddit对话的实验证明了ConceptFlow比以前的常识意识对话模型和微调的GPT-2模型的优势，同时使用的参数少得多，但对对话结构进行了明确的建模。
生物神经网络面临着限制连接权重的允许配置的稳态和资源约束。如果一个约束是严格的，它定义了一个非常小的解决方案空间，这些约束空间的大小决定了它们与计算任务的解决方案的潜在重叠。我们研究了对神经元的总突触权重和单个突触权重的约束的解决方案空间的几何，描述了使这些解决方案空间的大小最大化的连接度（伙伴的数量）。 然后，我们假设约束条件的解空间的大小可以作为管理神经回路发展的成本函数。我们为这些成本函数下的最大熵度分布的模型证据开发了分析性的近似值和界限。我们在已发表的苍蝇大脑联想学习中心的电子显微镜连接组上测试了这些，发现了回路结构发展进程的证据。
在这项初步工作中，我们研究了无限宽的神经网络的无限集合的泛化特性。 令人惊奇的是，这个模型系列对许多信息理论量都有可行的计算方法。 我们报告了在寻找与泛化相关的信号方面的分析和经验调查。
学习文本的多语言表征已被证明是许多跨语言迁移学习任务的成功方法，学习这种表征有两个主要范式。(1)对齐，将不同的独立训练的单语表征映射到一个共享的空间；(2)联合训练，直接使用单语和跨语目标联合学习统一的多语表征。在本文中，我们首先对使用这两种方法在不同的跨语任务中学习的表征进行了直接比较。 根据这一分析，我们提出了一个简单而新颖的框架，将这两种以前相互排斥的方法结合起来。在各种任务上的广泛实验表明，我们提出的框架减轻了两种方法的局限性，并在MUSE双语词汇归纳（BLI）基准上优于现有方法。我们进一步表明，我们提出的框架可以推广到上下文表示，并在CoNLL跨语言NER基准上取得最先进的结果。
深度神经网络中大量的权重使模型难以部署在低内存环境中，如移动电话、物联网边缘设备以及云端的 "推理即服务 "环境。之前的工作已经考虑通过压缩技术，如权重修剪、过滤器修剪等，或通过卷积层的低秩分解来减少模型的大小。在本文中，我们展示了使用多种技术来实现更高的模型压缩，同时也减少了推理过程中所需要的计算资源。 我们在使用塔克分解进行模型压缩的同时进行了滤波修剪，然后进行低秩分解。我们表明，与塔克分解或单独的滤波修剪相比，我们的方法实现了高达57%的模型压缩率，而GoogleNet的准确率也相差无几，而且它还减少了高达48%的Flops，从而使推理更快。
我们回顾了BLEU和ROUGE的局限性，它们是用来评估参考摘要和假设摘要的最流行的指标，并介绍了JAUNE：一套好的指标应该表现的标准，并提出了使用最近的基于变形的语言模型来评估参考摘要和假设摘要的具体方法。
许多标准的GNN变体通过计算 "消息 "沿图的边缘传播信息，只基于每条边缘的源的表示，在GNN-FiLM中，边缘的目标节点的表示被额外用于计算可应用于所有传入消息的变换，允许对传递的信息进行特征调制。 基于对基线方法的重新实现，介绍了在文献中的三个任务上比较不同GNN架构的实验结果。所有方法的超参数都是通过广泛的搜索找到的，产生了一些令人惊讶的结果：基线模型之间的差异比文献中报道的要小。尽管如此，GNN-FiLM在分子图的回归任务上优于基线方法，在其他任务上的表现也具有竞争力。
为了同时处理归因于网络嵌入和聚类，我们提出了一个新的模型。它利用了内容和结构信息，充分利用了它们的同时使用。所提出的模型依赖于用真正的离散聚类解决方案来逼近放松的连续嵌入解决方案。 实验结果表明，所提出的算法在聚类和嵌入方面比最先进的算法表现得更好，包括对具有不同属性的网络数据集致力于类似任务的深度学习方法。
我们提出了一种学习型图像引导渲染技术，它结合了基于图像的渲染和基于GAN的图像合成的优点。我们的方法的目标是为虚拟和增强现实应用生成重建物体的照片般真实的再渲染（例如。我们工作的一个核心部分是处理视线依赖的效果。具体来说，我们直接训练一个针对物体的深度神经网络来合成物体的视线依赖的外观。 这个视频被用来通过多视图立体重建物体的代理几何。基于这个3D代理，捕获的视图的外观可以被扭曲成一个新的目标视图，就像经典的基于图像的渲染一样。 为此，我们提出了EffectsNet，这是一个深度神经网络，可以预测依赖视图的效果。基于这些估计，我们能够将观察到的图像转换成漫反射图像。这些漫反射图像可以被投射到其他视图中。 为了将多个重新投影的图像合成为最终的输出，我们学习了一个能输出照片般真实结果的合成网络。使用这种图像引导的方法，网络不必在 "记忆 "物体外观上分配能力，相反，它学习如何将捕获的图像的外观结合起来。我们在合成和真实数据上定性和定量地证明了我们方法的有效性。
我们通过在合成数据集上测试生成式对抗网络的分布学习能力。这些数据集包括$R^n$空间中常见的点的分布和包含各种形状和尺寸的多边形的图像。 我们发现，总的来说，GANs不能忠实地再现包含不连续支持或带有噪声的急剧弯曲的点数据集。此外，在图像数据集上，我们发现GANs似乎不能学会计算图像中同类物体的数量。我们还强调了GANs中泛化和学习之间的明显矛盾。
本文提出了一种梯度方法中步长适应的新方法。所提出的方法被称为步长优化（SSO），它将步长适应表述为一个优化问题，在给定的模型参数和梯度下，损失函数相对于步长最小。 SSO不需要二阶信息或任何概率模型来适应步长，因此它是高效且易于实现的。此外，我们还为随机学习环境引入了随机SSO。在实验中，我们将SSO集成到香草SGD和Adam中，它们在大量基准数据集上的表现超过了最先进的自适应梯度方法，包括RMSProp、Adam、L4-Adam和AdaBound。
尽管生成模型在实践中非常成功，但这一现象的基础理论才开始跟上实践。在这项工作中，我们解决了生成模型的普遍性问题：神经网络是否真的可以任意很好地近似任何数据流形？ 我们为这个问题提供了一个肯定的答案，并表明在激活函数的温和假设下，人们总能找到一个前馈神经网络，将潜伏空间映射到与所需数据流形的特定Hausdorff距离内的集合上。我们还为多类生成模型和循环生成模型的情况证明了类似的定理，这些模型被训练为将样本从一个流形映射到另一个，反之亦然。
基于模型的强化学习（RL）被认为是一种有前途的方法，可以减少阻碍无模型RL的样本复杂性。然而，对这种方法的理论理解相当有限。 该元算法基于估计的动态模型和样本轨迹，迭代地建立了预期报酬的下限，然后在策略和模型上共同最大化下限。 将我们的框架简化实例化，得到了基于模型的RL算法的变体--随机下限优化（SLBO）。实验证明，在一系列连续控制基准任务上只允许1M或更少的样本时，SLBO达到了最先进的性能。
我们研究了使用知识蒸馏来压缩U型网结构。我们表明，虽然标准蒸馏不足以可靠地训练压缩的U型网，但在知识蒸馏中引入其他正则化方法，如批量正常化和类再加权，可显著改善训练过程。这使我们能够将U型网压缩1000倍以上，即压缩到其原始参数数的0.1%，而性能的下降可以忽略不计。
在一长串的任务中用梯度下降法学习神经网络是有问题的，因为他们对新任务的微调覆盖了对以前的任务很重要的网络权重，这导致了在旧任务上的糟糕表现--这种现象被称为灾难性遗忘。 虽然早期的方法使用了任务排练和成长网络，但它们都限制了任务序列的可扩展性，而正交方法则建立在正则化之上。 基于Fisher信息矩阵（FIM），与旧任务相关的参数变化受到惩罚，这迫使任务被映射到网络的可用剩余容量中。 与以前的工作相比，我们利用了损失表面的大多数区域是平坦的这一事实，因此只计算与当前任务相关的表面周围的Hessian-向量-乘积。
最近，人们对提高简单模型的性能产生了兴趣，原因有很多，如可解释性、小数据的稳健学习、在内存受限的环境中部署以及环境因素。我们提出了一种新的方法SRatio，它可以利用高性能复杂模型（即深度神经网络、提升树、随机森林）的信息，为潜在的低性能简单模型（如决策树或浅层网络）重新加权训练数据集，提高其性能。我们的方法还利用了简单模型的每个样本硬度估计，这与之前主要考虑复杂模型的置信度/预测的工作不同，因此在概念上是新颖的。 此外，我们对将探针附加到神经网络的中间层的概念进行了概括和正式化，这是以前工作中的主要想法之一citep{profweight}，并将其纳入我们的方法中。这些贡献的好处在实验中得到了证明，在6个UCI数据集和CIFAR-10中，我们在大多数情况下（27个中的16个）超过了竞争对手，在剩下的情况下并列获得最佳性能。 事实上，在一些情况下，我们甚至接近复杂模型的性能。我们还进行了进一步的实验，以验证断言，并直观地了解我们的方法为什么能起作用。理论上，我们通过表明简单模型使用我们的加权损失最小化的加权损失上限来激励我们的方法。
我们提出了一种核学习的原则性方法，该方法依赖于翻译不变量或旋转不变量核的傅里叶分析特征。我们的方法产生了一连串的特征图，迭代地完善了SVM余量。
我们详细阐述了使用重要性抽样进行因果推理，特别是反事实推理。我们展示了如何在概率编程中原生地实现这一点。
我们考虑的问题是代表大群体的集体行为和预测离散状态空间上群体分布的演变。离散时间平均场博弈（MFG）被认为是建立在博弈论基础上的可解释模型，用于理解个人行为的总体效果和预测群体分布的时间演变。 我们通过表明一个特殊的MFG可以还原为MDP，实现了MFG和马尔科夫决策过程（MDP）的综合。这使我们能够扩大平均场博弈理论的范围，并通过深度反强化学习推断出大型现实世界系统的MFG模型。我们的方法从真实数据中学习MFG的奖励函数和前向动力学，我们报告了对现实世界社交媒体群体的平均场博弈模型的首次经验测试。
我们研究了训练顺序生成模型以捕捉协调的多Agent轨迹行为的问题，如进攻性篮球游戏。 在对这类环境进行建模时，设计能够利用中间变量捕捉长期协调的分层模型往往是有益的。 此外，这些中间变量应该以可解释和可操作的方式捕捉有趣的高层行为语义。我们提出了一个层次化的框架，可以有效地学习这种顺序生成模型。 我们的方法受到最近关于利用程序化产生的弱标签的工作的启发，我们将其扩展到时空制度。除了合成设置，我们展示了如何实例化我们的框架，以有效地模拟篮球运动员之间的复杂互动，并在很长一段时间内生成现实的篮球游戏轨迹。我们使用定量和定性评估来验证我们的方法，包括与专业体育分析师进行的用户研究比较。
许多自动机器学习方法，如超参数和神经结构优化的方法，由于涉及到训练许多不同的模型配置，所以计算成本很高。在这项工作中，我们提出了一种新的方法，通过在训练早期终止不良配置来节省计算预算。 我们定性地表明，通过优化成对的排名损失和利用其他数据集的学习曲线，我们的模型能够有效地对学习曲线进行排名，而不需要观察许多或很长的学习曲线。我们进一步证明，我们的方法可以用来加速神经架构的搜索，其倍数可达100，而发现的架构没有明显的性能下降。在进一步的实验中，我们分析了排名的质量，不同模型组件的影响，以及模型的预测行为。
持续学习是学习新的任务或知识的问题，同时保护旧的知识，最好是从旧的经验中归纳出更快地学习新的任务。通过随机梯度下降训练的神经网络在连续训练具有不同数据分布的新任务时，常常在旧的任务上退化。 这种现象被称为灾难性遗忘，被认为是在非平稳数据或新任务序列中学习的主要障碍，并阻止网络不断积累知识和技能。我们在强化学习的背景下研究这个问题，在一个代理暴露于任务序列的环境中。与大多数其他工作不同，我们没有为任务边界模型提供明确的指示，这是暴露于连续经验的学习代理的最一般情况。 虽然最近提出了各种对抗灾难性遗忘的方法，但我们探索了一个直接的、普遍的、似乎被忽视的解决方案--即使用经验回放缓冲器来处理所有过去的事件--混合使用政策性学习和非政策性学习，利用行为克隆。 我们表明，这种策略仍然可以快速地学习新的任务，而且在Atari和DMLab领域都可以大大减少灾难性的遗忘，甚至与需要任务身份的方法的性能相匹配。当缓冲区的存储受到限制时，我们证实，一个简单的随机丢弃数据的机制允许有限大小的缓冲区的性能几乎与无限制的一样好。
我们提出了一种方法，该方法可以学习整合时间信息（来自学习的动力学模型）和模糊的视觉信息（来自学习的视觉模型），在互动代理的背景下。我们的方法是基于一个图结构的变异递归神经网络，它被训练成端到端的推断（部分观察）世界的当前状态，以及预测未来状态。我们表明，我们的方法在两个体育数据集上表现优于各种基线，一个基于真实篮球轨迹，一个由足球游戏引擎生成。
在本文中，我们研究了学习深度卷积神经网络的权重问题。我们考虑了一个网络，其中卷积是在非重叠的斑块上进行的，每一层都有一个内核。我们开发了一种算法，用于同时从训练数据中学习所有的内核。我们的方法被称为深度张量分解（DeepTD），是基于等级1的张量分解。 我们从理论上研究了训练数据的可实现模型下的DeepTD，其中输入是从高斯分布中选择的i.i.d.，而标签是根据种植的卷积核生成的。我们表明DeepTD是数据高效的，并且只要样本量超过网络中卷积权重的总数，就可以证明它的有效性。
神经网络剪枝技术可以将训练过的网络的参数数减少90%以上，减少存储需求，提高推理的计算性能而不影响准确性。然而，当代的经验是，通过剪枝产生的稀疏架构从一开始就很难训练，这同样会提高训练性能。 我们发现，一个标准的剪枝技术自然地发现了子网络，其初始化使它们能够有效地训练。基于这些结果，我们阐述了 "彩票假说"：密集的、随机初始化的、前馈网络包含子网络（"中奖彩票"），当单独训练时，在类似的迭代次数中达到与原始网络相当的测试精度。 我们发现的中奖彩票赢得了初始化彩票：它们的连接具有使训练特别有效的初始权重。我们提出了一种识别中奖彩票的算法和一系列支持彩票假说的实验，以及这些偶然初始化的重要性。我们一直发现中奖彩票的大小不到MNIST和CIFAR10的几个全连接和卷积前馈架构的10-20%。
我们研究了训练稀疏神经网络的困难，并对稀疏制度内的优化动态和能量景观进行了新的观察。最近，citep{Gale2019, Liu2018}的工作表明，在ImageNet-2012数据集上训练的稀疏ResNet-50架构收敛到的解决方案明显比修剪发现的解决方案差。我们表明，尽管优化器失败，但从初始化到 "好的 "解决方案有一条目标单调下降的线性路径。 此外，我们试图在稀疏子空间中找到一条从 "坏 "解到 "好 "解的递减目标路径的努力也失败了。然而，如果我们允许路径穿越密集子空间，那么我们始终能在两个解之间找到一条路径。这些发现表明，可能需要穿越额外的维度来摆脱稀疏子空间中发现的静止点。
神经网络的训练取决于底层损失景观的结构，即局部最小值、鞍点、平坦的高原和损失障碍。关于景观的结构，我们研究了深度神经网络每一层的神经元的排列对称性，这不仅产生了损失函数的多个等价全局最小值，也产生了伙伴最小值之间的关键点。 在一个由$d-1$隐藏层和$n_k$神经元组成的网络中，$k=1, \ldots, d$，我们构建了等效全局最小值之间的连续路径，这些路径通过一个 "互变点"，其中同一隐藏层$k$中两个神经元的输入和输出权重向量发生碰撞并互换。 我们表明，这种互换点是位于损失相等的高维子空间内的临界点，有助于景观的全局平坦性。我们还发现，神经元$i$和$j$交换的互换点过渡到一个平坦的高维高原，使特定层$k$中的所有$n_k！$神经元在同一损失值下进行互换。此外，我们通过利用神经网络损失景观中的层次结构引入高阶排列点，并发现K$-th阶排列点的数量远远大于（已经很庞大的）等效全局最小值的数量--至少是K$阶的多项式系数。在两个任务中，我们用我们的路径寻找方法用数字证明了伙伴最小值之间的连续路径的存在：首先，在一个函数逼近任务的单隐层玩具网络中，其次，在MNIST任务的多层网络中。我们的几何方法产生了由权重空间对称性产生的临界点数量的下限，并在以前的理论结果和数字观察之间提供了一个简单的直观联系。
我们研究了通过连续代理网络训练具有二进制（$pm1$）权重和激活的随机神经网络模型的问题。 这些方程显示，根据代用模型的选择，网络可能会或不会表现出从有序到混乱的过渡，以及限制最大可训练深度的深度尺度的存在。具体来说，在解决混乱边缘条件的方程时，我们表明使用高斯局部重参数化技巧得出的代用模型没有临界初始化，而基于分析性高斯积分的确定性代用模型有临界初始化。 该理论适用于一系列二元神经元和权重设计的选择，如不同的神经元噪声模型，允许在初始化时对算法的行为进行分类。 此外，我们从理论上预测并在数字上确认，在标准连续网络中使用的普通权重初始化方案，当应用于随机二元权重的平均值时，会产生较差的训练性能。这项研究表明，与通常的直觉相反，随机二元权重的平均值应该被初始化到接近$/pm 1$，以使更深的网络可以被训练。
语义依赖分析旨在寻找丰富的双词关系，它允许单词有多个依赖头，从而形成图结构的表示。我们提出了一种基于CRF自动编码器框架的半监督学习语义依赖分析器的方法。我们的编码器是一个判别性的神经语义依赖分析器，可以预测输入句子的潜在解析图。 我们的解码器是一个生成性神经模型，它以潜在的解析图为条件重建输入句子。我们的模型是弧形因素，因此解析和学习都是可操作的。实验表明，我们的模型比监督基线取得了显著和一致的改进。
在这项工作中，我们描述了一种新的方法，即DeFINE，用于有效地学习深层次的词级表征。我们的架构使用了一种具有新颖的跳过连接的分层结构，允许使用低维的输入和输出层，减少了总参数和训练时间，同时提供了与现有方法类似或更好的性能。 在WikiText-103上，DeFINE将Transformer-XL的总参数减少了一半，对性能的影响最小。在Penn Treebank上，DeFINE将AWD-LSTM提高了4个点，参数减少了17%，实现了与参数较少的最先进方法相媲美的性能。
在本文中，我们提出了对Bertinetto等人[2019]的论文 "Meta-learning with differentiable closed-form solvers "的复制，作为ICLR 2019年可复制性挑战的一部分。在成功复制该论文最关键的部分时，我们在两个基准上达到了与原论文相当或优于几个设置的性能。我们评估了新的基线结果，使用论文中提出的一个新数据集。然而，我们也提供关于可复制性和可比性的多种意见和建议。 在我们提请作者注意我们的可重复性工作后，他们已经更新了本工作所基于的原始论文，并发布了代码。我们的贡献主要包括复制他们原始论文中最重要的结果，在可重复性方面给出见解，并提供第一个开源的实现。
网络修剪已经成为减少深度神经网络规模的强大技术。修剪通过使用训练好的密集网络并逐渐去除不重要的连接来发现高性能的子网络。最近，出现了一些替代技术来直接训练稀疏网络，而不需要事先训练一个大型密集模型，从而在训练和推理期间实现小的内存足迹。 我们研究了这些技术中最新的一种，并进行了额外的实验，以阐明其在训练稀疏的深度卷积网络中的行为。动态参数重新分配在训练期间早期收敛为一个高度可训练的子网络。 因此，动态参数重新分配提高了深度卷积网络的可训练性，起到了与过度参数化类似的作用，而不会产生后者的内存和计算成本。
本文介绍了使用监督和半监督技术在视觉发展的三个方向上取得的进展。第一个是使用Soft At- tention和生成对抗网络（GANs）的原则实现半监督的物体检测和识别。 第二个和第三个是监督网络，分别使用卷积神经网络（CNN）学习空间位置和数量的基本概念。这三个重点是基于以前出版物中介绍的机器人经验学习方法。
对深层网络中间层所学到的表征的描述，可以为任务的性质提供有价值的洞察力，并可以指导开发适合的学习策略。在这里，我们研究了自动语音识别背景下基于卷积神经网络的声学模型。(1)前两层完全可以在语言之间转移，(2)第2-8层也是高度可转移的，但我们发现了一些语言特异性的证据，(3)随后的全连接层更具语言特异性，但可以成功地微调到目标语言。为了进一步探测权重冻结的效果，我们使用冻结训练[Raghu等人，2017]进行了后续实验。我们的结果与CCN在训练期间 "自底向上 "收敛的观察一致，证明冻结训练的好处，特别是对于转移学习。
当政策的变化被限制在一个小的Kullback-Leibler分歧时，政策梯度方法通常会取得更好的性能。我们推导出政策梯度，其中政策的变化被限制在一个小的Wasserstein距离（或信任区域）。这是在离散和连续多臂匪徒设置与熵正则化中完成的。 我们表明，在关于Wasserstein距离$W_2$的小步数限制中，政策动态受热方程的支配，遵循Jordan-Kinderlehrer-Otto结果。这意味着政策经历了扩散和平流，集中在具有高回报的行动附近。这有助于阐明概率匹配设置中收敛的性质，并为高斯政策先验和加性梯度噪声等经验做法提供理由。
softmax函数被广泛用于训练多类分类的深度神经网络。尽管它在分类任务中表现突出，但在一些特征空间中适用欧氏距离的情况下，从softmax监督中得到的特征通常是次优的。为了解决这个问题，我们提出了一个新的损失，被称为各向同性的损失，即数据点的整体分布被规范化以接近各向同性的正常分布。 结合vanilla softmax，我们正式提出了一个新的标准，称为各向同性softmax，简称isomax，用于深度神经网络的监督学习。凭借isomax，类内特征被各向同性损失惩罚，而类间距离被原始softmax损失很好地保持。
强化学习中的一个基本问题是无模型算法是否具有样本效率。最近，Jin等人（2018）提出了一种具有UCB探索策略的Q-learning算法，并证明它对有限期偶发MDP具有近乎最优的遗憾约束。本文中，我们将具有UCB-探索奖励的Q-learning调整为无限期的MDP，其折扣奖励\emph{without}访问生成模型。 我们表明，我们算法的{textit{sample complexity of exploration}被$tilde{O}({frac{SA}{\epsilon^2(1-\gamma)^7}}$所约束。这提高了以前已知的最佳结果$tilde{O}({frac{SA}{\epsilon^4(1-\gamma)^8}}$，在这种情况下，延迟Q-learning实现了（Strehlet al, 2006），并且与$$epsilon$以及$S$和$A$的下限相匹配，直到对数系数。
Backpropagation驱动着今天的人工神经网络（ANNs）.然而，尽管进行了广泛的研究，仍然不清楚大脑是否实现了这种算法.在神经科学家中，强化学习（RL）算法通常被视为一种现实的替代方法：神经元可以随机引入变化，并使用非特定的反馈信号来观察其对成本的影响，从而近似其梯度.然而，这种学习的收敛率随着参与的神经元数量的增加而变差.在这里我们提出一个混合学习方法。 每个神经元使用一个RL型策略来学习如何近似反向传播所提供的梯度。我们提供了证明，对于某些类别的网络，我们的方法收敛于真正的梯度。在前馈和卷积网络中，我们的经验表明，我们的方法可以学习到近似的梯度，并且可以匹配基于梯度的学习的性能。学习反馈权重提供了一个生物上合理的机制来实现良好的性能，而不需要精确、预先指定的学习规则。
本文提出并证明了神经网络训练中的一个令人惊讶的模式：在训练运行中（任何一点）产生的模型的任何一对损失（如交叉熵、平均平方误差、0/1误差等）的值之间存在着一对一的关系。这种模式是普遍的，因为这种一对一的关系在不同的架构（如VGG、Resnet、Densenet等）、算法（SGD和SGD与动力）以及训练损失函数（交叉熵和平均平方误差）中是相同的。
