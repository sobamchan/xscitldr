由于深度学习在解决各种具有挑战性的机器学习任务方面的成功，人们对从理论方面理解训练神经网络的损失函数的兴趣越来越大。特别是，临界点的属性和它们周围的景观对确定优化算法的收敛性能非常重要。 我们表明，临界点的分析形式表征了相应损失函数的值以及实现全局最小值的必要和充分条件。此外，我们利用临界点的分析形式来表征线性神经网络和浅层ReLU网络损失函数的景观特性。一个特别的结论是。虽然线性网络的损失函数没有虚假的局部最小值，但具有ReLU激活函数的单隐层非线性网络的损失函数确实有局部最小值，而不是全局最小值。
Backpropagation（BP）算法通常被认为在大脑中在生物学上是不可信的。其中一个主要原因是BP要求前馈和反馈通路中的权重矩阵是对称的。为了解决这个 "权重运输问题"（Grossberg，1987），Liao等人（2016）和Lillicrap等人提出了两个生物学上可信的算法。(2016)，放宽了BP的权重对称性要求，并在小数据集上表现出与BP相当的学习能力。然而，Bartunov等人（2018）最近的一项研究发现，尽管反馈对齐（FA）和目标传播（TP）的一些变体在MNIST和CIFAR上表现良好，但它们在ImageNet上的表现明显比BP差。在这里，我们额外评估了符号对称（SS）算法（Liao等人，2016），它与BP和FA的不同之处在于反馈和前馈权重不共享大小，而是共享符号。我们使用不同的网络架构（ResNet-18和AlexNet用于ImageNet；RetinaNet用于MS COCO）考察了符号对称和反馈对齐的性能。 令人惊讶的是，用符号对称性训练的网络可以达到接近BP训练的网络的分类性能。这些结果补充了Bartunov等人（2018）的研究，并为未来在更困难的数据集和更复杂的架构上的生物可信的学习算法建立了新的基准。
我们介绍了2-simplicial Transformer，这是Transformer的一个扩展，其中包括一种概括了点积注意力的高维注意力，并使用这种注意力来更新实体表征与价值向量的张量积。我们表明，这种架构是深度强化学习背景下逻辑推理的一个有用的归纳偏向。
我们提出了张量-训练RNN（TT-RNN），这是一个新的神经序列架构系列，用于在具有非线性动态的环境中进行多变量预测。这种系统的长期预测是非常具有挑战性的，因为存在长期的时间依赖性、高阶关联性和对错误传播的敏感性。我们提出的张量循环架构通过直接使用高阶矩和高阶状态转换函数学习非线性动态来解决这些问题。 此外，我们使用张量-训练（TT）分解高阶结构，以减少参数的数量，同时保持模型的性能。我们在理论上建立了张量-训练RNNs对一般序列输入的近似特性，而这种保证对通常的RNNs是不存在的。我们还证明了在一系列具有非线性动态的模拟环境以及真实世界的气候和交通数据上，长期预测能力比一般RNN和LSTM架构有显著提高。
最近将深度模型与概率图形模型相结合的努力在提供灵活的模型方面很有希望，同时也很容易解释。我们提出了一种变异信息传递算法，用于此类模型的变异推理。我们有三个贡献。首先，我们提出了结构化推理网络，将图形模型的结构纳入变异自动编码器（VAE）的推理网络中。 其次，我们建立了这样的推理网络能够实现类似于VAE的快速摊销推理的条件。最后，我们推导出一种变异消息传递算法，以执行高效的自然梯度推理，同时保留了摊销推理的效率。通过同时为深度结构化模型实现结构化、摊销和自然梯度推理，我们的方法简化并普及了现有的方法。
现代深度神经网络有大量的权重，这使得它们很难部署在计算受限的设备上，如手机。一个常见的减少模型大小和计算成本的方法是使用低秩因子化来近似权重矩阵。然而，用小秩执行标准的低秩因子化会损害模型的表现力并大大降低性能。 在这项工作中，我们提出使用多个低秩因式分解的混合物来模拟一个大的权重矩阵，并且根据其输入动态地计算混合物系数。我们在语言建模和图像分类任务上证明了所提出的方法的有效性。实验表明，我们的方法不仅提高了计算效率，而且与全秩对应的方法相比，保持（有时优于）其准确性。
深度学习训练需要高速访问大量的数据，这给通过商品网络和存储设备检索的数据集带来了挑战。我们引入了一种方法，通过我们称之为渐进式压缩记录（PCRs）的方法来动态地减少获取和运输训练数据的开销。 具有相似保真度的训练实例被分组在一起，这就减少了训练模型所需的系统开销和数据带宽。我们表明，模型可以在训练数据的积极压缩表示上进行训练，并且仍然保持高精确度，而且PCRs可以使平均速度比使用JPEG压缩的基线格式提高2倍。我们的结果适用于各种深度学习架构的数据集：ImageNet、HAM10000、Stanford Cars和CelebA-HQ。
当存在语义异常的例子时，训练强大而准确的深度神经网络（DNN）是最基本的，也是最具挑战性的。虽然已经取得了很大的进展，但仍有一个关键的研究问题还没有被彻底探索出来。在这项工作中，我们研究了这个问题，并提出了梯度重构（GR）来解决这个问题。GR修改了logit向量的梯度大小，当噪声变得更加严重时，强调相对容易的训练数据点，这就像明确的强调正则化一样，提高DNN的泛化性能。 除了正则化，我们还将GR与实例加权和设计稳健的损失函数联系起来。我们的经验表明，GR具有高度的异常稳健性，并以很大的幅度优于最先进的技术，例如。此外，我们提出了全面的消减研究，以探索GR在不同情况下的行为，这对GR在现实世界场景中的应用是有参考价值的。
生成对抗网络（GANs）在生成真实自然图像的任务中取得了显著的成果。在大多数应用中，GAN模型有两个共同点：一方面，GANs训练涉及解决一个具有挑战性的鞍点优化问题，被解释为生成器和鉴别器函数之间的对抗性博弈；另一方面，生成器和鉴别器以深度卷积神经网络为参数。 本文的目标是分解这两个因素对GANs成功的贡献。特别是，我们引入了生成性潜伏优化（GLO），这是一个训练深度卷积生成器的框架，不使用鉴别器，从而避免了对抗性优化问题的不稳定性。 通过各种实验，我们表明GLO享有GAN的许多理想特性：从大数据中学习，合成视觉上吸引人的样本，在样本之间有意义地插值，并对噪声向量进行线性运算。
在本文中，我们提出了一种新的内核，即随机森林内核，以提高MMD GAN的经验性能。与普通森林的确定性路由不同，在我们创新的随机森林内核中使用了一种概率性路由变体，它可以与CNN框架合并。从随机森林的角度来看，GAN判别器的输出可以被看作是森林的特征输入，每棵树只获得一部分特征，因此整个森林从集合学习中受益。在核方法方面，随机森林核被证明是有特点的，因此适合MMD结构。 此外，作为一个非对称核，我们的随机森林核在捕捉分布差异方面更加灵活。分享CNN、核方法和集合学习的优势，我们基于随机森林核的MMD GAN在CIFAR-10、CelebA和LSUN卧室数据集上获得了理想的经验表现。此外，为了完整起见，我们还提出了全面的理论分析来支持我们的实验结果。
在演员批评设置中的强化学习依赖于批评者的准确价值估计。然而，函数近似、时间差异（TD）学习和非政策训练的结合会导致高估价值函数。一个解决方案是使用Clipped Double Q-learning（CDQ），它用于TD3算法，计算TD-目标中两个批评者的最小值。我们表明，CDQ会引起低估偏差，并提出了一种新的算法，通过使用CDQ的目标和来自单一评论家的目标的加权平均数来说明这一点。加权参数在训练期间调整，使价值估计与最近的事件的实际折现回报相匹配，并通过它来平衡过度和低估。
我们引入了一个系统的框架来量化分类器对视频中发现的图像的自然发生的扰动的鲁棒性。作为这个框架的一部分，我们构建了ImageNet-Vid-Robust，这是一个由22668张图像组成的人类专家评审的数据集，这些图像被归类为1145组来自ImageNet视频物体检测数据集中的帧的感知相似图像。 此外，我们还评估了用于检测的Faster R-CNN和R-FCN模型，并显示自然扰动会引起分类和定位错误，导致检测MAP的中位数下降14点。我们的分析表明，现实世界中的自然扰动对当前的CNN来说有很大的问题，对它们在需要可靠、低延迟预测的安全关键环境中的部署构成重大挑战。
根据Kaggle ML和DS调查，结构化的表格数据是工业界最常用的数据形式。梯度提升树、支持向量机、随机森林和逻辑回归通常用于表格数据的分类任务。最近，使用二维单词嵌入的超级字符方法在文本分类任务中取得了最先进的结果，展示了这种新方法的前景。 在本文中，我们提出了SuperTML方法，它借用了超级字符法和二维嵌入的思想来解决表格数据的分类问题。对于每一个输入的表格数据，首先将特征投射到二维嵌入中，就像一个图像，然后将这个图像送入微调的ImageNet CNN模型进行分类。
从没有标签的预测学习中学习丰富的表征一直是机器学习领域的一个长期挑战。到目前为止，生成性预训练在为原始图像的表征建模方面还不如对比性方法成功。在本文中，我们提出了一个用于对原始图像进行自我监督表征学习的神经架构，称为PatchFormer，它可以学习对原始图像中各斑块的空间依赖关系进行建模。 我们通过在低数据量的分类任务上对预训练模型进行微调来评估所学表征的效用。具体来说，我们在半监督的ImageNet分类上对我们的模型进行了基准测试，ImageNet最近已经成为半监督和自监督学习方法的流行基准。我们的模型在只使用ImageNet上1%和10%的标签进行训练时，能够达到30.3%和65.5%的最高1级准确率，显示了生成性预训练方法的前景。
自适应正则化方法通过预处理矩阵对下降方向进行预处理。由于机器学习问题的参数很多，全矩阵预处理方法非常昂贵。我们展示了如何修改全矩阵自适应正则化，以使其实用和有效。 我们还提供了新的理论分析，用于非凸优化设置中的自适应正则化。我们算法的核心，称为GGT，包括低秩矩阵平方根的高效逆向计算。我们的初步实验强调了GGT在各种合成任务和标准深度学习基准中的改进收敛率。
对话系统需要大量不同但互补的专业知识来协助、通知和娱乐人类。在本文中，我们建议学习一个对话系统，该系统独立地对不同的对话技能进行参数化，并通过对参数的关注（AoP）来学习选择和组合它们中的每一个。实验结果表明，这种方法在MultiWOZ（Budzianowski et al, 2018）、车内助理（Eric等人，2017）和Persona-Chat（Zhang等人，2018）的组合数据集上取得了有竞争力的性能。最后，我们证明每个对话技能都是有效学习的，并且可以与其他技能相结合，产生选择性的反应。
模型提炼的目的是将复杂模型的知识提炼成更简单的模型。在本文中，我们考虑了另一种被称为数据集提炼的表述：我们保持模型的固定性，而是试图将大型训练数据集的知识提炼成小型数据集。 我们的想法是合成少量的数据点，这些数据点不需要来自正确的数据分布，但是当给学习算法作为训练数据时，将接近在原始数据上训练的模型。例如，我们表明有可能将60,000张MNIST训练图像压缩到仅仅10张合成的蒸馏图像（每类一张），并且在固定的网络初始化条件下实现接近原始性能。 在MNIST、CIFAR10、PASCAL-VOC和CUB-200等多个数据集上的实验表明，与其他方法相比，我们的方法具有优势。 最后，我们包括了数据集提炼在持续学习环境中的实际应用：我们表明，将提炼过的图像作为以前任务的偶发记忆来存储，可以比真实图像更有效地缓解遗忘。
我们将生成式对抗网络（GANs）的最小化游戏与寻找凸优化问题的拉格朗日函数的鞍点联系起来，其中判别器输出和生成器输出的分布分别扮演着原始变量和对偶变量的角色。这种表述显示了标准GAN训练过程与凸优化的原始-对偶次梯度方法之间的联系。这种内在联系不仅为函数空间中的GANs训练提供了理论收敛证明，而且还激发了训练的新目标函数。 修改后的目标函数迫使生成器输出的分布按照原始-双重子梯度方法沿方向更新。一个玩具例子表明，所提出的方法能够解决模式崩溃，在这种情况下，标准GAN或Wasserstein GAN无法避免。在高斯混合合成数据和真实世界图像数据集上的实验证明了所提出的方法在生成多样化样本上的性能。
指定奖励函数是困难的，这促使奖励推理领域：从人类行为中学习奖励。该领域的起始假设是，鉴于所需的奖励函数，人类行为是最优的，但在现实中，人们有许多不同形式的非理性，从噪音到近视到风险厌恶等等。 这一事实似乎对奖赏推断有严格的危害：从理性行为中推断出奖赏已经很困难了，而噪音和系统性偏见使行动与奖赏的关系不那么直接。我们在这项工作中的见解是，与预期相反，非理性实际上可以帮助而不是阻碍奖赏推断。 对于某些类型和数量的非理性，与理性行为相比，专家现在产生了更多不同的政策，这有助于区分不同的奖励参数--那些本来对应于相同的理性行为的参数。我们通过系统分析非理性对奖励推理的影响来检验这一点。 我们从覆盖非理性空间作为贝尔曼更新的偏差开始，模拟专家行为，测量推理的准确性，以对比不同的类型并研究收益和损失。我们对我们的发现提供了一个基于相互信息的分析，并通过讨论准确模拟非理性的必要性，以及我们可能期望（或能够训练）真人在向学习者教授奖励时表现出有用的非理性的程度来结束。
自然语言处理模型缺乏统一的鲁棒性测试方法。在本文中，我们介绍了WildNLP--一个在自然环境中测试模型稳定性的框架，在自然环境中会出现键盘错误或拼写错误等文本损坏。我们比较了4个流行的NLP任务的模型的鲁棒性。我们比较了4个流行的NLP任务的模型的稳健性：Q&A、NLI、NER和情感分析，通过测试它们在该框架中引入的方面的性能。特别是，我们专注于最近最先进的文本表示法和非语境化的词嵌入之间的比较。 为了提高鲁棒性，我们对这些方面进行对抗性训练，并检查其可转移性，以改进具有各种cor-ruption类型的模型。我们发现，尽管现代嵌入技术有助于提高模型的鲁棒性，但模型的高绩效并不能确保足够的鲁棒性。我们为社区发布cor-rupted数据集和WildNLP框架的代码。
训练生成模型如生成对抗网络（GAN）对噪声数据来说是一个挑战。本文提出了一种与聚类有关的新型课程学习算法来解决这个问题。课程建设是基于数据点中潜在聚类的中心度。 为了使我们的算法可以扩展到大规模数据，设计了主动集，即每轮训练只在一个主动子集上进行，该子集包含一小部分已经训练过的数据和中心度较低的增量数据。 此外，还提出了几何分析，以解释生成模型的聚类课程的必要性。在猫和人脸数据上的实验验证了我们的算法能够学习最佳的生成模型（如ProGAN），而这些模型的质量指标是针对噪声数据的。一个有趣的发现是，最佳聚类课程与论文中提出的几何渗流过程的临界点密切相关。
后门攻击旨在通过注入对抗性触发器来操纵训练数据的子集，这样在被篡改的数据集上训练的机器学习模型将在嵌入相同触发器的测试集上做出任意（有针对性）的错误预测。虽然联合学习（FL）能够聚合不同方提供的信息以训练更好的模型，但其分布式学习方法和各方固有的异质数据分布可能带来新的漏洞。 除了最近对FL的集中式后门攻击，即每一方在训练过程中嵌入相同的全局触发器，我们提出了分布式后门攻击（DBA）------一种新型的威胁评估框架，它是通过充分利用FL的分布式特性开发的。DBA将全局触发器模式分解成独立的局部模式，并分别嵌入到不同的对抗方的训练集中。 与标准的集中式后门相比，我们表明DBA在金融和图像数据等不同的数据集上对FL的持久性和隐蔽性大大增强。我们进行了大量的实验，表明在不同的设置下，DBA的攻击成功率明显高于集中式后门。此外，我们发现分布式攻击确实更加隐蔽，因为DBA可以躲避两种最先进的针对集中式后门的鲁棒FL算法。 为了进一步探索DBA的特性，我们通过改变不同的触发因素来测试攻击性能，包括局部触发器的变化（大小、间隙和位置）、FL中的缩放因子、数据分布以及毒药比例和间隔。我们提出的DBA和全面的评估结果揭示了FL的鲁棒性特点。
图形网络最近引起了相当大的兴趣，特别是在半监督学习的背景下。这些方法通常通过生成节点表征，在整个给定的加权图中传播。这里我们认为，对于半监督学习来说，考虑在图中传播标签反而更自然。 这种表述可用于学习边缘权重，与其他权重被启发式设置的方法不同。从实现LP单次迭代的层开始，我们通过增加几个重要的非线性步骤，大大增强了标签传播机制。在两个不同的环境中的实验证明了我们方法的效用。
神经结构搜索（NAS）在计算机视觉方面取得了快速的进展，在一系列任务中，通过自动搜索的神经网络（NN）架构取得了新的最先进的结果.相反，NAS在自然语言理解（NLU）方面没有取得类似的进展。 对应于NLU任务的典型神经网络模型的编码器-聚合器元架构（Gong等人，2018），我们重新定义了搜索空间，将其分为两部分：编码器搜索空间和聚合器搜索空间。编码器搜索空间包含基本操作，如卷积、RNNs、多头关注及其稀疏变体、星形变换器。 我们的搜索算法是通过DARTS，一个可微调的神经结构搜索框架来实现的。我们每隔几个epochs逐步减少搜索空间，从而进一步减少搜索时间和资源成本。
网络嵌入（NE）方法旨在学习网络节点的低维表征，通常是在欧几里得空间，然后将这些表征用于各种下游预测任务。 然而，链接预测的复杂性需要一个精心设计的评估管道来提供一致的、可重复的和可比较的结果，我们认为这在最近的工作中没有得到充分的考虑。 我们介绍了EvalNE，一个透明地评估和比较NE方法在链接预测上的性能的评估框架。EvalNE为超参数调整、模型验证、边缘采样、边缘嵌入的计算和模型验证等任务提供了自动化和抽象化。 该框架整合了边缘和非边缘采样的有效程序，可用于轻松评估任何现成的嵌入方法。最后，为了证明EvalNE在实践中的实用性，我们进行了一项实证研究，尝试复制和分析几篇有影响力的论文的实验部分。
深度学习模型可以通过随机梯度下降进行有效优化，但几乎没有理论证据支持。优化中的一个关键问题是了解神经网络的优化景观何时适合基于梯度的优化。我们专注于一个简单的神经网络两层ReLU网络，有两个隐藏单元，并表明所有局部最小化器是全局的。这与Lee等人（2017）的最新工作相结合；Lee等人（2016）表明，梯度下降收敛到全局最小器。
Dropout是一种简单而有效的技术，可以提高泛化性能，防止深度神经网络（DNNs）中的过度拟合。在本文中，我们讨论了关于Dropout的三个新的观察，以更好地理解具有整流线性单元（ReLU）激活的DNNs的泛化。1）dropout是一种平滑技术，它鼓励DNN的每个局部线性模型在附近区域的数据点上进行训练；2）恒定的dropout率会导致有效的神经失活率，对于具有不同比例激活的神经元的层来说是明显不同的；3）当批量归一化也被使用时，dropout的重新缩放因子会导致训练和测试条件中的归一化不一致。 上述情况导致了我们提出的 "Jumpout "方法中对辍学的三个简单但非微不足道的改进。"Jumpout "使用单调的递减分布（如截断的高斯的右部）对辍学率进行采样，因此每个数据点的局部线性模型被训练，很可能对来自附近的数据点比来自较远区域的数据点效果更好。 而不是为每一层调整一个辍学率并将其应用于所有样本，跳出更自适应地将每一层和每个训练样本/批次的辍学率正常化，因此应用于激活的神经元的有效辍学率保持不变。 此外，我们对跳出的输出进行了重新划分，以获得更好的权衡，使神经元的方差和平均值在训练和测试阶段更加一致，这就缓解了跳出和批量归一化之间的不相容性。与原始跳出相比，跳出在CIFAR10、CIFAR100、Fashion MNIST、STL10、SVHN、ImageNet-1k等方面显示出明显的性能改善。同时引入了可忽略不计的额外内存和计算成本。
对可解释性、计算资源和原则性归纳先验的关注，促使人们努力为NLP任务设计稀疏的神经模型。如果稀疏性对NLP很重要，训练有素的神经模型是否会自然地变得大致稀疏？ 使用Taxi-Euclidean规范来衡量稀疏性，我们发现频繁的输入词与集中或稀疏的激活有关，而频繁的目标词与分散的激活有关，但梯度集中。我们发现与功能词有关的梯度比内容词的梯度更集中，即使控制了词频。
将知识库（KB）整合到神经对话代理中是对话式人工智能的关键挑战之一。事实证明，记忆网络可以有效地将KB信息编码到外部存储器中，从而产生更流畅和知情的反应。不幸的是，这种存储器在训练过程中会充满潜在的表征，所以最常见的策略是随机覆盖旧的存储器条目。在本文中，我们对这种方法提出了质疑，并提供了实验证据，表明传统的记忆网络产生了许多冗余的潜在向量，导致过度拟合和需要更大的记忆。我们引入了记忆剔除作为一种自动技术，鼓励潜在空间的多样性：1）老化冗余的记忆，增加它们在训练中被覆盖的概率；2）对新的记忆进行采样，总结冗余记忆获得的知识。 这种技术使我们能够在斯坦福大学多轮对话数据集中加入知识库以实现最先进的对话生成。考虑到相同的架构，它的使用为自动生成反应提供了+2.2个BLEU点的改进，在识别命名实体方面增加了+8.1%。
Su-Boyd-Candes（2014）在Nesterov的方法和常微分方程（ODE）之间建立了联系。 我们表明，如果在Su-Boyd-Candes（2014）的ODE中加入Hessian阻尼项，那么Nesterov方法就会作为修改后的ODE的直接离散化出现。类似地，在强凸情况下，在Polyak的ODE中加入Hessian阻尼项，然后将其离散化，得到强凸函数的Nesterov方法。  尽管有Hessian项，这两个二阶ODE都可以表示为一阶系统。既定的Liapunov分析被用来恢复连续和离散时间中的加速收敛率。 此外，利亚普诺夫分析可以扩展到随机梯度的情况，这使得全梯度情况可以被视为随机情况的一个特殊情况。 其结果是，在连续和离散时间以及随机和全梯度情况下，都有一个统一的凸型加速方法。
我们提出了转移学习（L2TL），通过明智地从源数据集中提取信息来改善目标数据集上的转移学习。L2TL考虑了源任务和目标任务的模型之间巨大的共享权重的联合优化，并采用了自适应权重来缩放组成损失。 我们展示了L2TL在固定模型下的最先进的性能，在各种数据集上一直优于微调基线。在小规模的目标数据集和源和目标数据集之间的显著标签不匹配的情况下，L2TL以更大的幅度优于以前的工作。
在许多部分可观察的场景中，强化学习（RL）代理必须依靠长期记忆来学习最佳策略。我们证明，由于来自环境和探索的随机性，使用NLP和监督学习的技术在RL任务中是失败的。利用我们对RL中传统记忆方法的局限性的见解，我们提出了AMRL，一类能够以更高的样本效率学习更好的策略，并对噪声输入有弹性的模型。 具体来说，我们的模型使用一个标准的记忆模块来总结短期背景，然后从标准模型中聚集所有的先验状态，而不考虑顺序。我们表明，这在梯度衰减和信噪比方面都有优势。在测试长期记忆的Minecraft和迷宫环境中进行评估，我们发现我们的模型比具有相同参数数量的基线提高了19%的平均回报率，比具有更多参数的更强基线提高了9%。
流形上的优化已被广泛用于机器学习，以处理有约束的优化问题。然而，在实践中，优化问题涉及一个以上的约束（每个约束对应一个流形）是很常见的。一般来说，如何在多个流形上进行有效的、可证明的优化并不清楚，特别是当多个流形的交点不是流形或不容易计算时。 我们提出了一个统一的算法框架来处理多流形上的优化。具体来说，我们整合了来自多个流形的信息，并通过将每个流形的信息看作是一个漂移并将它们加在一起，沿着一个集合的方向前进。我们证明了所提出的算法的收敛特性。我们还将该算法应用于具有批量规范化层的训练神经网络，并取得了较好的经验结果。
长期以来，人们一直认为高维连续控制问题不能通过离散化行动空间的各个维度来有效解决，因为必须在这些维度上学习策略，其数量是指数级的。在本文中，我们从最近针对结构化预测问题的序列到序列模型的成功中得到启发，以开发离散化空间的策略。 具体来说，我们展示了如何使用离散维度上的下一步预测模型对连续空间上的Q值和政策进行建模。通过这种参数化，可以在学习过程中利用行动空间的组成结构，以及计算行动空间的最大值（大约）。 在一个简单的例子任务中，我们通过经验证明，我们的方法可以进行全局搜索，这有效地解决了困扰DDPG的局部优化问题。我们将该技术应用于非政策（Q-learning）方法，并表明我们的方法可以在几个连续控制任务中达到非政策方法的最先进水平。
基于模型的强化学习（MBRL）旨在学习一个动态模型，以减少与现实世界环境的交互次数。然而，由于估计错误，所学模型中的滚动，尤其是长跨度的滚动，无法与现实世界环境中的滚动相匹配。这种不匹配严重影响了MBRL的样本复杂性。这一现象可归因于以前的工作采用监督学习来学习单步过渡模型，这在保证多步滚动的分布匹配方面存在固有的困难。 基于这一主张，我们提出通过WGAN将从综合模型中采样的多步展开的分布与真实的分布进行匹配来学习综合模型。我们在理论上表明，两者的匹配可以使真实的过渡和学习的过渡之间的累积奖励的差异最小化。我们的实验也表明，所提出的模型模仿方法在样本复杂度和平均回报方面优于最先进的方法。
批量归一化（BN）及其变体在深度学习界被广泛采用，因为它们改善了深度神经网络的训练。关于这种归一化为何如此有效的讨论仍未得到解决。 我们明确了普通最小二乘法和通过BN进行反向传播时计算的部分导数之间的关系。我们将BN的反向传播重塑为最小二乘法拟合，它将归一化激活的部分导数归零并进行装饰。这种观点，我们称之为{em gradient-least-squares}，是对BN的一种可扩展和算术上的准确描述。
批量归一化（BN）已经成为不同架构的深度学习的基石，似乎有助于优化和泛化。虽然这个想法有直观的意义，但一直缺乏对其有效性的理论分析。这里为其猜想的属性之一提供了理论支持，即允许梯度下降在学习率调整较少的情况下取得成功的能力。实验表明，即使我们将标度不变的参数（如BN的各层权重）的学习率固定为一个常数（如0.3），梯度下降仍然以T^{-1/2}的速率在T次迭代中接近静止点（即梯度为零的解），渐进地符合梯度下降的最佳约束，并具有良好的学习率调控。
自然图像的生成模型通过对规模的有力利用，在高保真样本方面取得了进展。我们试图将这一成功带到视频建模领域，表明在复杂的Kinetics-600数据集上训练的大型生成对抗网络能够产生比以往工作更高的复杂度和保真度的视频样本。 我们提出的模型，双视频判别器GAN（DVD-GAN），通过利用其判别器的计算效率分解，可以扩展到更长和更高分辨率的视频。我们对视频合成和视频预测的相关任务进行了评估，对Kinetics-600的预测实现了新的最先进的Fréchet Inception Distance，对UCF-101数据集的合成实现了最先进的Inception Score，同时在Kinetics-600上建立了强大的合成基线。
理解程序性语言需要预测行动的因果效应，即使它们没有被明确说明。在这项工作中，我们引入了神经过程网络，通过行动动态的（神经）模拟来理解程序性文本。  我们的模型通过明确地将行动建模为状态变换器来补充现有的具有动态实体跟踪功能的记忆架构。实证结果表明，我们提出的模型可以推理行动的未说明的因果效应，使其能够为理解和生成程序性文本提供更准确的背景信息，同时提供比现有替代方案更可解释的内部表示。
最近，在训练神经网络以取代手工制作的数据结构方面出现了一种趋势，目的是为了更快的执行，更好的准确性，或更大的压缩。 在这种情况下，神经数据结构的实例化是通过对网络的输入进行多次历时训练直到收敛。在许多应用中，这种昂贵的初始化是不实际的，例如流式算法---输入是短暂的，只能被检查少量的次数。 在本文中，我们探索了通过元学习在一次数据流中学习近似的集合成员。我们提出了一种新的记忆结构，即神经布鲁姆过滤器，我们表明在倾斜数据或结构化集合的情况下，它比布鲁姆过滤器和几个现有的记忆增强的神经网络更具压缩性。
我们利用最近从神经网络的二阶优化中得到的启示，构建了一个Kronecker派生的拉普拉斯近似值，即训练后的网络权重。我们的近似值不需要修改训练程序，使实践者能够估计他们目前在生产中使用的模型的不确定性，而不需要重新训练它们。 我们证明了我们的克朗克因子方法在分布之外的数据上导致了更好的不确定性估计，并且对简单的对抗性攻击更加稳健。我们的方法只需要为每一层计算两个正方形曲率因子矩阵，其大小等于该层的输入和输出大小的各自平方，使得该方法在计算和内存使用方面都很高效。
频谱嵌入是一种流行的图数据表示技术。一些正则化技术已经被提出，以提高嵌入的质量，如聚类的下游任务。在本文中，我们解释了一个简单的块模型的完整图正则化的影响，其中一个常数被添加到邻接矩阵的所有条目。 具体来说，我们表明正则化迫使频谱嵌入集中在最大的块上，使表示对噪声或异常值不那么敏感。我们在合成数据和真实数据上都说明了这些结果，表明正则化如何改善标准聚类分数。
暴露偏差问题是指在自动回归神经网络语言模型（LM）的最大似然估计（MLE）训练中由教师强迫造成的训练-推理差异。它被认为是自然语言生成（NLG）模型训练的一个核心问题。尽管已经提出了很多算法来避免教师强迫，从而缓解暴露偏差，但很少有工作表明暴露偏差问题有多严重。 在这项工作中，我们首先确定了MLE训练的LM的自动恢复能力，这使我们对暴露偏差的严重性产生了怀疑。然而，根据我们在受控实验中的测量，当训练-推理差异被完全消除时，只有大约3%的性能增益。我们的结果表明暴露偏差问题可能比目前假设的要严重得多。
在语言进化文献中，算法进化或学习（组成）通信协议的能力传统上是通过使用突发通信任务来研究的。在这里，我们通过使用当代深度学习方法和在参考性通信游戏上训练强化学习神经网络代理来扩大这一研究。 我们通过开发能够从原始像素数据中学习的代理来扩展以前的工作，原始像素数据是一种更具挑战性和现实性的输入表示。我们发现，在输入数据中发现的结构程度会影响出现的协议的性质，从而证实了这样的假设：当代理认为世界是有结构的，结构化的组成语言最有可能出现。 
对于理解通用文档来说，像字体大小、列的布局以及一般情况下单词的定位等信息可能带有语义信息，这对于解决下游的文档智能任务至关重要。我们新颖的BERTgrid是基于Katti等人（2018）的Chargrid，它将文档表示为上下文的词片嵌入向量的网格，从而使其空间结构和语义可以被处理神经网络访问。 上下文的嵌入向量是从BERT语言模型中检索出来的。我们在语义实例分割任务上使用BERTgrid与全卷积网络相结合，从发票中提取字段。我们在表格行项目和文档标题字段提取上展示了它的性能。
众所周知，深度强化学习（RL）策略容易受到其观测值的对抗性扰动的影响，类似于分类器的对抗性例子。然而，攻击者通常不能直接修改另一个代理的观测值。这可能导致人们怀疑：是否有可能仅仅通过选择一个在多代理环境中行动的对抗性策略来攻击RL代理，从而创造出具有对抗性的自然观测值？ 我们证明了在具有本体观察的模拟人形机器人与通过自我游戏训练的最先进的受害者之间的零和游戏中存在对抗性政策。对抗性政策可靠地赢得了受害者，但产生了看似随机和不协调的行为。我们发现，这些政策在高维环境中更加成功，并在受害者政策网络中引起了与受害者与正常对手游戏时大不相同的激活。视频可在https://attackingrl.github.io。
GloVe和Skip-gram词嵌入方法通过将去噪的词共现矩阵分解为低秩矩阵的乘积来学习词向量。在这项工作中，我们提出了一种迭代算法，用于计算基于通用低秩模型的词共现矩阵的词向量。 我们的算法概括了Skip-gram和GloVe，以及基于指定的共现矩阵、共现分布和迭代算法中的迭代次数而产生的其他嵌入方法。 例如，使用Tweedie分布与一次迭代的结果是GloVe，而使用多叉分布与完全收敛模式的结果是Skip-gram.实验结果表明，在谷歌单词类比相似性任务上，我们算法的多次迭代比GloVe方法的结果有所改善。
确定性模型是对现实的近似，通常比随机性的替代品更容易建立和解释。 不幸的是，由于自然界是反复无常的，在实践中，观察到的数据永远不可能被确定性的模型完全解释。 观察和过程噪声需要被添加，以适应确定性模型的随机行为，这样它们就能够解释和推断出有噪声的数据。在确定性模拟器中添加过程噪声会在模拟器中引起故障，导致某些输入没有返回值--我们把这种特性描述为 "脆弱"。 我们表明，在这个空间进行推理可以被看作是拒绝抽样，并训练一个条件归一化流作为对噪声值的提议，这样模拟器崩溃的概率就很低，当被用作近似推理算法的提议时，在固定的样本预算下增加了计算效率和推理的保真度。
多跳问题回答需要模型从文本的不同部分收集信息来回答问题。目前的大多数方法都是通过神经网络以端到端的方式来学习解决这个任务，而没有保持推理过程的明确表示。我们提出一种方法来提取文本上的离散推理链，它由一系列导致答案的句子组成。 重要的是，我们不依赖于黄金注释的推理链或 "支持性事实"：在训练时，我们使用基于命名实体识别和核心推理解决的启发式方法得出假黄金推理链。在测试时，我们也不依赖于这些注释，因为我们的模型只学习从原始文本中提取推理链。 我们在最近提出的两个大型多跳问题回答数据集上测试我们的方法。我们的分析显示了对高性能至关重要的链的属性：特别是，按顺序建模提取是很重要的，就像以上下文感知的方式处理每个候选句子一样。此外，人类评估显示，我们提取的链允许人类以高置信度给出答案，表明这些是这项任务的强大中间抽象。
归一化常数（也称为分区函数、贝叶斯证据或边际似然）是贝叶斯推理的核心目标之一，然而大多数现有的方法既昂贵又不准确。在这里，我们开发了一种新的方法，从用标准马尔科夫链蒙特卡罗（MCMC）获得的后验样本开始。 我们应用一种新的归一化流（NF）方法从这些样本中获得一个分析密度估计值，然后用最优桥式采样（OBS）来获得归一化常数。我们将我们的方法（我们称之为高斯化桥式采样（GBS））与现有的方法如嵌套采样（NS）和退火重要性采样（AIS）在几个例子上进行比较，显示我们的方法比这些方法明显更快、更准确，并且有一个可靠的误差估计。
我们提出了一项关于现代深度神经网络（DNN）模型中灾难性遗忘（CF）的大规模实证研究，这些模型进行顺序（或：增量）学习。提出了一个新的实验方案，考虑到应用场景中遇到的典型限制。 由于调查是经验性的，我们在迄今为止最大数量的视觉分类数据集上评估了CF行为，从每一个数据集中，我们构建了具有代表性的顺序学习任务（SLTs），与之前关于CF的工作紧密结合。我们的结果清楚地表明，在所有调查的数据集和SLTs的应用条件下，没有模型能够避免CF。
联合学习（FL）是指在分散的数据存储基础上学习高质量的全局模型，而不需要复制原始数据。 在这项工作中，我们指出，模型不可知元学习（MAML）的设置，即优化快速的、基于梯度的、少量的适应异质分布的任务，与FL的个性化目标有很多相似之处。 ) 流行的FL算法，联合平均法，可以被解释为一种元学习算法。2) 仔细的微调可以产生一个具有较高准确性的全局模型，同时更容易个性化。
深度神经网络中的数据记忆已经成为一个备受关注的研究课题。在本文中，我们将深度卷积自动编码器中的图像记忆与通过分层卷积的下采样联系起来。 为了在更简单的环境中分析这一机制，我们训练了线性卷积自动编码器，并表明当使用下采样时，训练数据的线性组合被存储为对应于网络的线性算子中的特征向量。 另一方面，没有下采样的网络不会记忆训练数据。 我们提供了进一步的证据，证明同样的效果发生在非线性网络中。 此外，非线性网络中的降采样使模型不仅记忆了图像的线性组合，而且记忆了单个训练图像。 由于卷积自动编码器组件是深度卷积网络的构件，我们设想我们的发现将阐明过度参数化的深度网络中记忆这一重要现象。 
强化学习为决策和控制提供了一个强大的通用框架，但其在实践中的应用往往由于需要大量的特征和奖励工程而受到阻碍。深度强化学习方法可以消除对政策或价值特征的明确工程的需要，但仍然需要手动指定奖励函数。 在这项工作中，我们提出了AIRL，一种基于对抗性奖励学习公式的实用、可扩展的逆向强化学习算法，与直接模仿学习算法相比具有竞争力。此外，我们表明AIRL能够恢复对动态变化具有鲁棒性的可移植奖励函数，使我们能够在训练期间环境发生重大变化时学习政策。
我们考虑了机器学习的两个核心问题：我们如何预测一个最小值是否能推广到测试集，以及为什么随机梯度下降法能找到推广性好的最小值？ 我们的工作回应了\citet{zhang2016understanding}，他表明深度神经网络可以很容易地记住随机标记的训练数据，尽管在相同输入的真实标签上泛化得很好。我们表明，同样的现象发生在小型线性模型中。这些观察是由贝叶斯证据解释的，它惩罚了尖锐的最小值，但对模型参数化是不变的。 我们还证明，当一个人保持固定的学习率时，有一个最佳的批次大小，它能最大限度地提高测试集的准确性。我们提出，小的迷你批次引入的噪声促使参数走向证据大的最小值。 将随机梯度下降解释为一个随机微分方程，我们确定了 "噪声尺度"$g = \epsilon (\frac{N}{B} - 1) \approx \epsilon N/B$，其中$epsilon$是学习率，$N$是训练集大小，$B$是批次大小。\我们用经验来验证这些预测。
在工业领域，正电子湮灭不受复杂环境的影响，伽马射线光子穿透力强，可以实现工业零件的无损检测。由于正电子过程中伽马射线光子散射、衰减和采样时间短导致图像质量差，我们提出结合深度学习的思路，通过对抗网生成质量好、细节清晰的正电子图像。 本文的结构如下：首先，我们基于迁移学习编码得到医学CT图像的隐藏向量，并利用PCA提取正电子图像特征。其次，我们构建了基于注意力机制的正电子图像存储器，作为对抗网的整体输入，对抗网以医学隐藏变量作为查询。最后，我们联合训练整个模型并更新输入参数直到收敛。实验证明了利用对抗网生成工业无损检测的稀有正电子图像的可能性，并取得了良好的成像效果。
我们从主动信息采样的角度重新审视了递归注意模型（RAM，Mnih等人（2014）），一个用于视觉注意的递归神经网络。我们借用了神经科学关于主动信息采样在视觉注意和凝视背景下的作用的研究思路（Gottlieb，2018），作者提出了主动信息采样策略的三类动机。我们发现原来的RAM模型只实现了其中的一个。 我们确定了原始RAM的三个关键弱点，并通过在目标函数上添加两个额外的条款提供了一个简单的解决方案。修改后的RAM1）实现了更快的收敛，2）允许在不损失准确性的情况下对每个样本进行动态决策，3）与原始RAM相比，在没有训练的较长序列的瞥见上，概括得更好。
图形神经网络（GNNs）用于预测任务，如节点分类或边缘预测，在最近的机器学习中，从图形结构的数据中得到了越来越多的关注。然而，大量的标记图很难获得，这大大限制了GNNs的真正成功。虽然主动学习已经被广泛研究，以解决其他数据类型如文本、图像等的标签稀疏问题，但如何使其在图形上有效是一个有待研究的问题。 在本文中，我们介绍了关于主动学习与GNNs的节点分类任务的研究。 具体来说，我们提出了一种新的方法，在主动学习中使用节点特征传播和节点的K-Medoids聚类来选择实例。
连续归一化流（CNFs）由于其可逆性和精确的似然估计，已经成为广泛的任务中很有前途的深度生成模型。然而，由于模型生成的高维潜伏代码需要与输入数据的大小相同，对CNFs进行条件图像生成和下游预测任务的调节是低效的。 在本文中，我们提出了InfoCNF，这是一个高效的条件CNF，它将潜伏空间划分为一个特定于类的监督代码和一个非监督代码，在所有类之间共享，以有效地使用标记的信息。 由于分区策略（略）增加了函数评估（NFEs）的数量，InfoCNF也采用了门控网络来学习其常微分方程（ODE）求解器的容错率，以获得更好的速度和性能。我们通过经验表明，InfoCNF比基线提高了测试精度，同时在CIFAR10上获得了相当的似然分数并减少了NFEs。此外，在时间序列数据上应用InfoCNF的相同分区策略有助于提高推断性能。
无监督学习的一个核心目标是从未标记的数据或经验中获取表征，这些表征可用于从适量的标记数据中更有效地学习下游任务。许多先前的无监督学习工作旨在通过开发基于重建、分离、预测和其他指标的代理目标来实现这一目标。 相反，我们开发了一种无监督的元学习方法，明确优化了从少量数据中学习各种任务的能力。为此，我们以自动的方式从未标记的数据中构建任务，并对构建的任务运行元学习。 令人惊讶的是，我们发现，当与元学习相结合时，相对简单的任务构建机制，如聚类嵌入，导致在各种下游的、人类指定的任务上的良好表现。我们在四个图像数据集上的实验表明，我们的无监督元学习方法在没有任何标记数据的情况下获得了一种学习算法，适用于广泛的下游分类任务，改善了之前四个无监督学习方法学到的嵌入。
领域转移是机器学习的一个令人兴奋和具有挑战性的分支，因为模型必须学会在领域之间顺利转移，保留局部变化，并在没有标签的情况下捕捉到许多方面的变化。然而，迄今为止，大多数成功的应用要求两个领域密切相关（例如，图像-图像，视频-视频），利用类似或共享的网络来转换领域的特定属性，如纹理、着色和线条形状。在这里，我们证明了通过首先用潜在生成模型抽象数据，然后学习潜在空间之间的转换，可以实现跨模式的转换（例如，图像到音频）。我们发现，一个简单的变异自动编码器能够学习一个共享的潜势空间，以无监督的方式在两个生成模型之间建立桥梁，甚至在不同类型的模型之间（例如变异自动编码器和生成对抗网络）。我们可以在共享的潜空间中用线性分类器进一步施加所需的属性语义对齐。最后，分层结构将训练基础生成模型和语义排列的成本解耦，使个性化映射函数的计算效率和数据效率得到重新训练。
我们提出了对抗性归纳转移学习（AITL），一种解决源域和目标域之间输入和输出空间差异的方法。AITL利用对抗性领域适应和多任务学习来解决这些差异。 我们的激励性应用是药物基因组学，其目标是利用患者的基因组信息预测药物反应。挑战是具有药物反应结果的临床数据（即患者）非常有限，这就需要转移学习来弥补大型临床前药物基因组学数据集（如癌症细胞系）和临床数据之间的差距。 据我们所知，AITL是第一个解决输入和输出差异的对抗性归纳迁移学习方法。实验结果表明，AITL优于最先进的药物基因组学和迁移学习基线，可以更准确地指导精准肿瘤学。
命名实体识别（NER）和关系提取（RE）是信息提取和检索（IE和IR）中的两个重要任务。最近的工作表明，联合学习这些任务是有益的，这避免了基于管道的系统中固有的错误传播，提高了性能。然而，最先进的联合模型通常依赖于外部自然语言处理（NLP）工具，如依赖性分析器，限制了它们在这些工具表现良好的领域（如新闻）中的作用。 在本文中，我们提出了一种不依赖外部NLP工具的联合提取实体及其关系的神经端到端模型，该模型集成了一个大型的、预先训练好的语言模型。由于我们模型的大部分参数是预先训练好的，而且我们摒弃了递归而采用自关注，因此我们的模型训练起来很快。
在这项工作中，我们为循环神经网络探索了一种简单的变异贝叶斯方案。首先，我们表明，通过时间对截断反向传播的简单调整可以产生高质量的不确定性估计和卓越的正则化，而在训练过程中只需少量的额外计算成本，同时也减少了80/%的参数量。其次，我们证明了一种新型的后验近似如何产生对贝叶斯RNN性能的进一步改善。 我们展示了这种技术如何不局限于递归神经网络，可以更广泛地应用于训练贝叶斯神经网络。我们还实证证明了贝叶斯RNN在语言建模基准和图像字幕任务上如何优于传统的RNN，并展示了这些方法中的每一种如何比其他各种训练它们的方案改善我们的模型。我们还引入了一个研究语言模型不确定性的新基准，因此未来的方法可以很容易地进行比较。
随着时间的推移，无人自主飞行器（UAVs），特别是自主飞行的无人机在人工智能领域抢占了大量的注意力。由于电子技术越来越小，越来越便宜，越来越有效，最近观察到无人机的研究取得了巨大进展。 从监测洪水、辨别水体中藻类的蔓延到探测森林的踪迹，它们的应用范围很广。我们的工作主要集中在自主飞行的无人机上，我们建立了一个案例研究，对无人机的效率、稳健性和准确性进行了研究，我们通过实验证明了我们的结果。 我们进一步讨论了我们的实现算法，并提出了三个不同的最先进的算法，即TrailNet、InceptionResnet和MobileNet在准确性、稳健性、功耗和推理时间方面的比较实验。在我们的研究中，我们表明MobileNet以非常少的计算要求和功耗产生了更好的结果。
音乐在很大程度上依靠重复来建立结构和意义。 自我参照发生在多个时间尺度上，从主题到短语到整个音乐部分的重复使用，例如在具有ABA结构的作品中。 The Transformer（Vaswani等人，2017）是一个基于自我注意的序列模型，在许多需要保持长距离一致性的生成任务中取得了令人信服的结果，这表明自我注意可能也很适合对音乐进行建模。 在Transformer中代表相对位置信息的现有方法是根据成对的距离来调节注意力（Shaw等人，2018）。 这对于音乐作品等长序列来说是不切实际的，因为它们的记忆复杂性是序列长度的二次方。 我们提出了一种算法，将中间的记忆要求降低到与序列长度成线性关系。这使我们能够证明，带有我们修改过的相对注意力机制的转化器可以生成具有引人注目的结构的分钟长（数千步）的作品，生成连贯地阐述给定主题的延续，并在seq2seq设置中生成以旋律为条件的伴奏。  我们在JSB合唱团和钢琴比赛这两个数据集上用我们的相对注意力机制评估了Transformer，并在后者上获得了最先进的结果。
现实世界应用中的顺序决策问题往往需要实时解决，这就要求算法在有限的计算预算下表现良好。基于宽度的lookaheads在经典的规划问题以及预算紧张的Atari游戏中表现出最先进的性能。 我们分析了为什么基于宽度的算法在SSP问题上表现不佳，并克服了这些缺陷，提出了一种估计成本的方法。我们将基于宽度的lookaheads形式化为rollout算法的一个实例，给出了SSP问题的宽度定义，并解释了其样本复杂性。我们在各种SSP基准上的实验结果表明，该算法优于其他最先进的rollout算法，如UCT和RTDP。
深度神经网络（DNNs）在分类等有监督的任务中表现出色。卷积神经网络（CNNs）尤其可以学习有效的特征并建立高水平的表征，可用于分类，也可用于查询和近邻搜索。然而，CNNs也被证明在数据的分布从训练数据到测试数据发生变化时，会出现性能下降。 在本文中，我们分析了CNN的内部表征，并观察到与训练数据的表征相比，每一类中未见过的数据的表征在CNN的嵌入空间中分布更广（方差更大）。更重要的是，如果未见过的数据来自移位分布，这种差异会更加极端。基于这种观察，我们通过对CNN内部表征的类内协方差进行特征值分解，客观地评估每一类中表征的方差程度，观察到同样的行为。 这可能是有问题的，因为如果样本越过其类别的决策边界，较大的方差可能会导致错误的分类。我们在表征上应用近邻分类，经验表明，具有高方差的嵌入实际上具有明显更差的KNN分类性能，尽管这无法从其端到端的分类结果中预见到。 为了解决这个问题，我们提出了深度类内协方差分析（DWCCA），这是一个深度神经网络层，它大大降低了DNN表示的类内协方差，提高了对来自移位分布的未见测试数据的性能。 我们在两个声学场景分类的数据集（DCASE2016和DCASE2017）上对DWCCA进行了实证评估。我们证明，DWCCA不仅大大改善了网络的内部表示，还提高了端到端的分类精度，特别是当测试集表现出轻微的分布偏移时。通过在VGG神经网络中加入DWCCA，我们在分布不匹配的情况下实现了大约6个百分点的改进。
生成模型已被证明是代表高维概率分布和生成逼真图像的杰出工具。生成模型的一个基本特征是它们产生多模式输出的能力。然而，在训练时，它们往往容易受到模式塌陷的影响，这意味着模型在将输入噪声映射到真实数据分布的少数模式时受到限制。 在本文中，我们从决定性点过程（DPP）中获得灵感，设计了一个生成模型，在产生更高质量的样本的同时缓解了模式塌陷。DPP是一个优雅的概率测量，用于模拟子集内的负相关关系，从而量化其多样性。 我们使用DPP核对真实数据以及合成数据中的多样性进行建模。然后，我们设计了一个生成惩罚项，鼓励生成器合成具有与真实数据类似的多样性的数据。与之前倾向于使用额外的可训练参数或复杂的训练范式的最先进的生成模型相比，我们的方法不改变原始训练方案。 嵌入对抗性训练和变异自动编码器，我们的生成式DPP方法在各种合成数据和自然图像数据集（包括MNIST、CIFAR10和CelebA）上表现出一致的抗模式崩溃能力，同时在数据效率、收敛时间和生成质量方面优于最先进的方法。
尽管在确保神经网络的泛化方面已有一些工作，如规范、边际和锐度等规模敏感的复杂性措施，但这些复杂性措施并不能解释为什么神经网络在过度参数化的情况下泛化得更好。在这项工作中，我们提出了一种基于单位能力的新型复杂性措施，从而使两层ReLU网络的泛化约束更加严格。 我们的容量约束与测试误差随网络规模的增加而增加的行为相关（在实验中报告的范围内），并且可以部分地解释泛化与过度参数化的改善。我们进一步提出了一个匹配的Rademacher复杂性的下限，比以前的神经网络的容量下限有所改善。
我们引入了三个通用的点云处理块，这些处理块改善了多个最先进的网络的准确性和内存消耗，从而可以设计出更深、更准确的网络。促进有效信息流的新型处理块是一个用于点集的卷积型操作块，它以一种内存效率高的方式混合邻域信息；一个多分辨率点云处理块；以及一个在低分辨率和高分辨率处理分支之间有效分享信息的交叉链接块。 结合这些模块，我们设计了明显更宽更深的架构。我们在多个点分割基准（ShapeNetPart、ScanNet、PartNet）上广泛评估了所提出的架构，并报告了通过使用我们的通用模块和多个最新架构（PointNet++、DGCNN、SpiderCNN、PointCNN）在准确性和内存消耗方面的系统改进。我们报告了在PartNet数据集（这是最复杂的）上IoU增加9.7%，同时内存占用减少了57%。
端到端声学到词的语音识别模型最近得到了普及，因为它们容易训练，能很好地扩展到大量的训练数据，而且不需要词库。此外，词模型也可能更容易与下游任务（如口语理解）整合，因为与音素、字符或任何其他类型的子词单元相比，推理（搜索）要简化得多。在本文中，我们描述了直接从监督的序列到序列声学到词的语音识别模型构建上下文声学词嵌入的方法，使用学习的注意力分布。 在16个标准句子的评估任务中，我们的嵌入显示出与在语音转录上训练的word2vec模型相比具有竞争力的性能。此外，我们在口语理解任务中评估了这些嵌入，并观察到我们的嵌入与首先进行语音识别然后从转录中构建单词嵌入的管道中基于文本的嵌入性能相匹配。
在深度学习参与后，无监督单眼深度估计取得了很大的进展。用双眼立体图像进行训练被认为是很好的选择，因为数据很容易获得。然而，深度或差异预测的结果显示，物体边界的性能很差。 在本文中，我们提出了一种新的方法来克服这个问题。利用差异图的特性，我们生成了一个闭塞掩码来阻止图像扭曲过程中闭塞区域的反向传播，我们还设计了具有翻转立体图像的新网络来诱导网络学习闭塞的边界。
图分类目前由图核主导，虽然功能强大，但有一些明显的局限性。卷积神经网络（CNN）提供了一个非常有吸引力的替代方案。然而，用CNN处理图并非易事。 在本文中，我们反其道而行之：我们没有提出另一个图形CNN模型，而是引入了一种新的方法，将图形表示为类似于多通道图像的结构，使它们能够被普通的二维CNN处理。尽管我们的方法很简单，但事实证明它对最先进的图形内核和图形CNN非常有竞争力，并在一些数据集上以很大的优势胜过它们，它在时间复杂性方面也优于图形内核。
促使现代递归神经网络（RNN）在涉及连续数据的学习任务上取得空前成功的关键属性，是其不断提高的对复杂的长期时间依赖性进行建模的能力。然而，目前还缺乏对RNN长期记忆能力的成熟衡量标准，因此对其在整个时间内的数据关联能力的正式理解是有限的。 虽然卷积网络的深度效率现在已经很成熟了，但为了说明深度RNN在不同长度的输入上的成功，并需要解决它们的 "时间序列表达能力"。在本文中，我们分析了深度对递归网络表达长时间尺度的相关性的能力的影响。为了满足上述需要，我们引入了一个可以由网络支持的跨时间的信息流的度量，被称为起点-终点分离等级。 从本质上讲，这个度量反映了递归网络实现的函数与输入序列的开始和结束之间没有任何相互作用的函数之间的距离。我们证明，深度递归网络支持的开始-结束分离等级比浅层对应网络支持的等级要高得多。此外，我们表明，深度递归网络将输入序列的不同部分联系起来的能力随着输入序列的延长而呈指数增长，而香草式浅层递归网络则完全不适应序列长度。 因此，我们确定深度在递归网络模拟长期依赖关系的能力方面带来了压倒性的优势，并提供了一个量化这一关键属性的范例，它可以很容易地扩展到其他感兴趣的RNN架构，例如LSTM网络的变体。我们通过考虑一类被称为递归算术电路（RACs）的递归网络来获得结果，它通过乘法整合操作将隐藏状态与输入合并。
由于基础信号的复杂性，从整体上探索动物交流的感知和神经表征历来是非常困难的。我们在这里提出了一套新的技术，将整个交流剧目投射到可以系统采样的低维空间，探索感知表征、神经表征和机器学习算法学习的潜在表征空间之间的关系。 我们在一个正在进行的实验中展示了这种方法，该实验研究了音节的神经和知觉表征中上下文的顺序和时间维护。我们进一步讨论了研究鸟鸣中存在的长程信息内容的维护所依据的神经机制如何能够为机器序列建模提供信息和参考。
信息瓶颈原则（Shwartz-Ziv & Tishby，2017）表明，从信息理论的角度来看，基于SGD的深度神经网络训练会导致隐藏层的优化压缩。然而，这种说法是在玩具数据上建立的。我们在此提出的工作目标是在现实环境中使用更大、更深的卷积架构--ResNet模型来测试这些说法。 我们将PixelCNN++模型训练成反向表示解码器，以测量ResNet的隐藏层和输入图像数据之间的相互信息，当训练（1）分类和（2）自动编码时。我们发现，两个阶段的学习都发生在训练制度中，而且即使对于自动编码器，也会发生压缩。
我们研究了安全适应的问题：给定一个在过去的各种经验中训练的模型，这个模型能否学会在新的情况下执行该任务，同时避免灾难性的失败？这个问题设置经常出现在现实世界的强化学习场景中，如车辆适应新城市的驾驶，或机器人无人机适应仅在模拟中训练的策略。 虽然在没有灾难性失败的情况下学习是非常困难的，但先前的经验可以让我们学习模型，使之更容易。这些模型可能不会直接转移到新的环境中，但可以使谨慎的适应比na"{i}ve适应和从头学习要安全得多。 基于这种直觉，我们提出了风险规避领域适应（RADA）。RADA分两步工作：首先在源领域的群体中训练基于概率模型的RL代理，以获得经验并捕捉环境动态的认识不确定性。 然后，当被投放到一个新的环境中时，它采用悲观的探索策略，选择具有概率模型预测的最佳最坏情况的行动。我们表明，这种简单的最大化策略加速了在具有不同车辆尺寸的安全关键驾驶环境中的领域适应。
我们提出了神经符号概念学习者（NS-CL），这是一个学习视觉概念、单词和句子语义解析的模型，不需要对其中任何一个进行明确的监督；相反，我们的模型通过简单地看图像和阅读配对的问题和答案来学习。我们的模型建立了一个基于对象的场景表示，并将句子翻译成可执行的、符号化的程序。 为了连接两个模块的学习，我们使用了一个神经符号推理模块，在潜在的场景表征上执行这些程序。与人类的概念学习类似，感知模块根据被提及的对象的语言描述学习视觉概念。 同时，学习到的视觉概念有助于学习新的单词和解析新的句子。我们使用课程学习来指导在图像和语言的大组成空间上的搜索。广泛的实验证明了我们的模型在学习视觉概念、单词表征和句子的语义解析上的准确性和效率。此外，我们的方法允许轻松泛化到新的物体属性、组成、语言概念、场景和问题，甚至是新的程序领域。
贝叶斯推理为训练神经网络提供了一种有理论基础的通用方法，并有可能给出校准的不确定性。然而，在网络参数上指定一个有意义的、可操作的先验，并处理后验中的权重关联，这是一个挑战。 为此，本文介绍了两项创新：(i)基于最近引入的单元嵌入的网络参数的高斯过程层次模型，该模型可以灵活地编码权重结构；(ii)权重先验的输入依赖性上下文变量，可以通过使用核子提供方便的方法来规范网络所建模的函数空间。我们表明这些模型提供了理想的测试时间不确定性估计，展示了用核子为神经网络建模的归纳偏差案例，并在主动学习基准上展示了有竞争力的预测性能。
我们对自我注意模型在字符级神经机器翻译中的适用性进行了深入调查。我们测试了标准的变换器模型，以及一个新的变体，其中编码器块使用卷积结合了附近字符的信息。我们在WMT和UN数据集上进行了广泛的实验，使用多达三种输入语言（法语、西班牙语和汉语）测试了英语的双语和多语翻译。我们的变换器变体在字符级始终优于标准变换器，并且在学习更多稳健的字符级对齐时收敛更快。
医学诊断领域包含了大量与经典机器学习问题密切相关的挑战；然而，实际的限制使得将这些终点天真地转化为经典架构变得复杂。例如，放射学中的许多任务主要是多标签分类的问题，其中医学图像被解释为表明多种存在的或可疑的病症。临床环境促使人们必须同时对多种病理结果有较高的准确性，并大大限制了只考虑子集的工具的效用。 这一问题因训练数据的普遍匮乏而加剧，并最大限度地满足了从现有样本中提取临床相关特征的需要--最好是不使用预先训练的模型，因为这些模型可能会从切身相关的任务中带出不良的偏见。 我们提出并评估了这些限制因素的部分解决方案，即使用LSTM来利用目标标签之间的相互依赖性，从胸部X光片中预测14种病理模式，并在没有预训练的情况下，在NIH最大的公开胸部X光片数据集上建立最先进的结果。此外，我们提出并讨论了其他评价指标及其在临床实践中的相关性。
Semmelhack等人（2014年）使用支持向量机（SVM）在区分斑马鱼的游泳阵容方面取得了很高的分类精度。卷积神经网络（CNN）在各种图像识别任务中达到了比SVM更高的性能，但这些强大的网络仍然是一个黑盒子。达到更好的透明度有助于建立对其分类的信任，使学到的特征可以被专家解释。使用最近开发的一种叫做深度泰勒分解的技术，我们生成热图来突出与预测高度相关的输入区域。 我们发现，我们的CNN通过分析尾巴树干的稳定性来进行预测，这与Semmelhack等人（2014）使用的手动提取的特征明显不同。我们进一步发现，网络注意到了实验的假象。去除这些假象确保了预测的有效性。在校正后，我们最好的CNN击败了SVM 6.12%，实现了96.32%的分类精度。我们的工作因此证明了AI可解释性对于CNN的效用。
当交流时，人类依赖于内部一致的语言表征。也就是说，作为演讲者，我们期望听众的行为与我们听时的行为相同。这项工作提出了几种方法，以鼓励对话代理在新兴的交流环境中的这种内部一致性。 我们考虑了关于内部一致性约束效果的两个假设：1）它们提高了代理对未见过的指称的能力，以及2）它们提高了代理跨交际角色的概括能力（例如，尽管只被训练为听众，但作为说话者的表现）。
神经网络（NNs）能够执行依赖于组成结构的任务，即使它们缺乏明显的机制来表示这种结构。为了分析使这种成功得以实现的内部表示，我们提出了ROLE，一种检测这些表示是否隐含编码符号结构的技术。 ROLE通过学习符号组成结构和将该结构嵌入E的表示向量空间来学习近似目标编码器E的表示。然后，我们分析了一个为执行更复杂的合成任务（SCAN）而训练的seq2seq网络，其中没有可用的基础真理角色方案。对于这个模型，ROLE成功地发现了一个可解释的符号结构，该模型隐含地用于执行SCAN任务，提供了一个关于表征和一个众所周知的难以解释的模型类型的行为之间的全面说明。 我们验证了所发现的符号结构的因果重要性，表明当我们系统地操纵基于这种符号结构的隐藏嵌入时，模型的输出也会以我们的分析所预测的方式发生变化。最后，我们用ROLE来探索流行的句子嵌入模型是否捕捉到了组成结构，并发现了它们没有的证据；最后我们讨论了如何利用ROLE的见解来传授新的归纳偏见，从而提高这种模型的组成能力。
脊椎动物的视觉系统是分层组织的，在连续的阶段中处理视觉信息。神经表征在视觉处理的第一阶段有很大的不同：在视网膜的输出端，神经节细胞感受野（RFs）表现出明显的中心-周围结构，而在初级视觉皮层（V1），典型的RFs被锐利地调整到一个精确的方向。 目前还没有统一的理论来解释这些跨层表征的差异。在这里，我们用一个在图像识别上训练的深度卷积神经网络作为视觉系统的模型，表明这种表征的差异可以作为视网膜和皮质网络上不同神经资源约束的直接后果而出现，并且我们第一次发现一个单一的模型，在视觉处理的适当阶段两种几何形状都会自发地出现。 第二，我们发现，对于简单的下游皮质网络，视网膜输出端的视觉表征是作为非线性和有损的特征检测器出现的，而对于更复杂的皮质网络，它们是作为线性和忠实的视觉场景编码器出现的。这一结果预测小型脊椎动物（如蝾螈、青蛙）的视网膜应该进行复杂的非线性计算，提取与行为直接相关的特征，而大型动物如灵长类动物的视网膜应该主要对视觉场景进行线性编码，并对更广泛的刺激作出反应。这些预测可以调和两种似乎不相容的观点，即视网膜要么进行特征提取，要么对自然场景进行有效编码，这表明所有脊椎动物位于这两个目标之间的光谱上，取决于分配给其视觉系统的神经资源的程度。
虽然它尚未被证明，但经验证据表明，模型的泛化与可通过Hessian描述的优化的局部属性有关。我们将模型的泛化与PAC-Bayes范式下解决方案的局部属性联系起来。 特别是，我们证明了模型的泛化能力与Hessian、以Hessian的Lipschitz常数为特征的高阶 "平滑性 "项以及参数的尺度有关。在该证明的指导下，我们提出了一个对模型的泛化能力进行评分的指标，以及一个相应优化扰动模型的算法。
无监督学习是关于捕捉变量之间的依赖关系，并由这些变量的可能与不可能的配置之间的对比所驱动，通常是通过一个只对可能的配置进行采样的生成模型或用一个能量函数（非归一化的对数密度），对可能的配置来说是低的，对不可能的配置来说是高的。 生成对抗网络（GANs）中的批判者（或判别者）学习分离数据和生成器样本，而在生成器上引入熵最大化正则器可以将批判者的解释变成能量函数，它将训练分布与其他东西分开，因此可以用于异常或新颖性检测等任务。本文的动机是在潜伏空间而不是数据空间中采样的旧想法，因为在潜伏空间中运行蒙特卡洛马尔科夫链（MCMC）已被发现更容易和更有效，而且类似GAN的发生器可以将潜伏空间的样本转换为数据空间的样本。为此，我们展示了如何在潜伏空间中运行马尔科夫链，其样本可以被映射到数据空间，产生更好的样本。 这些样本也被用于估计数据空间能量函数的对数似然梯度所需的负相位梯度。为了最大限度地提高发生器输出端的熵，我们利用了最近引入的相互信息的神经估计器。我们发现，除了产生一个有用的异常检测的评分函数，所产生的方法产生尖锐的样本（像GANs），同时很好地覆盖模式，导致高Inception和Fréchet的分数。
在这项工作中，我们尝试在时域中进行长时间尺度的高质量音频传输和纹理合成，捕捉与音乐风格有关的和声、节奏和音色元素，使用的例子可能有不同的长度和音乐键。 我们展示了使用随机初始化的卷积神经网络将音乐风格的这些方面从一个作品转移到另一个作品的能力，使用的是三种不同的音频表征：短时傅里叶变换（STFT）的对数、梅尔谱图和恒定Q变换谱图。 我们通过精心设计补充音乐音频性质的神经网络结构来证明每种表征的缺点和优势。最后，我们表明，最引人注目的 "风格 "转移实例利用这些表征的组合来帮助捕捉音频信号的不同预期特征。
为了在新的背景下与新的伙伴交流，人类迅速形成了新的语言习惯。最近用深度神经网络训练的语言模型能够理解并产生其训练数据中存在的现有习惯，但不能像人类那样灵活地、交互地在飞行中调整这些习惯。 我们引入了一个重复的参考任务作为交流中适应性模型的基准，并提出了一个规律化的持续学习框架，使初始化了通用语言模型的人工代理能够随着时间的推移更准确和有效地理解他们的伙伴。
传统的集合预测模型由于一个我们称之为责任问题的问题而在简单的数据集上举步维艰。我们为特征向量的集合引入了一种基于对集合中各元素的特征进行排序的集合方法，这可以用来构建一个避免这种责任问题的包络变异自动编码器。 在一个多边形的玩具数据集和MNIST的集合版本上，我们表明这样的自动编码器能产生相当好的重建和表示。用FSPool替换现有集合编码器中的集合函数，能提高各种数据集的准确性和收敛速度。
我们提出了一种在室内环境中进行导航的策略学习方法。我们采用了一种分层的策略方法，在这种方法中，两个代理被训练成相互配合来完成复杂的导航任务。一个计划者代理在较高的层次上操作，为一个执行者代理提出子目标。执行者在其一系列操作结束时，将嵌入摘要报告给计划者，作为计划者下一个子目标建议的额外的侧面信息。 最终目标由环境产生并暴露给计划者，然后由计划者决定向执行者提出哪一组子目标。我们表明，这种计划者-执行者的设置大大增加了我们方法的样本效率，超过了传统的单一代理方法，有效地缓解了具有稀疏奖励信号的长系列行动所带来的困难。在要求导航各种现实室内环境的具有挑战性的Habitat环境中，我们证明我们的方法比以前的导航工作有了很大的改进。
显著性方法旨在解释深度神经网络的预测。当解释对那些无助于模型预测的因素敏感时，这些方法就缺乏可靠性。我们使用一个简单而常见的预处理步骤--给输入数据添加一个平均移动--来证明一个对模型没有影响的转换可以导致许多方法的错误归因。 我们将输入不变性定义为要求突出显示方法反映模型对输入变换的敏感性。我们通过几个例子表明，不满足输入不变性的突出显示方法是不可靠的，会导致误导和不准确的归因。
大型变形器模型通常在一些任务上取得最先进的结果，但是训练这些模型的成本过高，特别是在长序列上。我们引入了两种技术来提高变形器的效率。 此外，我们使用可逆的残差层而不是标准的残差层，这允许在训练过程中只存储一次激活，而不是N次，其中N是层的数量。所产生的模型，Reformer，与Transformer模型性能相当，同时内存效率更高，在长序列上更快。
在这项工作中，我们证明了通过阅读策略学习者的语言理解是一个有希望的工具，可以概括到新的环境中。我们提出了一个有基础的策略学习问题，阅读打怪（RTFM），其中代理必须共同推理语言目标、文件中描述的相关动态和环境观察。 我们按程序生成环境动态和相应的动态语言描述，这样，代理人必须通过阅读来理解新的环境动态，而不是记忆任何特定的信息。此外，我们提出了txt2π，一个捕捉目标、文件和观察之间三方互动的模型。 在RTFM上，txt2π可以泛化到新环境中的动态，而这些动态是在通过阅读训练时没有看到的。此外，我们的模型在RTFM上优于FiLM和语言条件CNN等基线。通过课程学习，txt2π产生的策略在复杂的RTFM任务上表现出色，需要几个推理和核心推理步骤。
深度学习社区的一个开放性问题是，为什么用梯度下降法训练的神经网络在真实数据集上有很好的泛化能力，尽管它们能够拟合随机数据。我们提出了一种方法来回答这个问题，它基于关于梯度下降动态的假设，我们称之为相干梯度。来自相似例子的梯度是相似的，因此整体梯度在某些方向上更强，这些梯度会相互加强。因此，在训练过程中，网络参数的变化偏向于那些（局部）同时有利于许多例子，当这种相似性存在时。我们用启发式论证和扰动实验支持这一假设，并概述了这如何解释关于深度学习的几个常见经验观察。
 深度学习的最新进展在许多低层次的视觉任务中显示出有希望的结果。然而，解决基于单幅图像的视图合成仍然是一个开放的问题。特别是，在给定单一输入图像的平行相机视图上生成新的图像是非常有趣的，因为它可以使二维输入景物的三维可视化。我们提出一个新颖的网络架构，在沿X轴的任意相机位置进行立体视图合成，或深三维平移，使用配备全局和局部自适应扩张的 "t形 "自适应核。 我们提出的网络架构，即怪物网络，设计了一个具有全局和局部自适应扩张的新型T形自适应内核，它可以有效地将全局相机的移动纳入目标图像像素的局部三维几何形状，以便在给定二维输入图像时合成自然的三维平移视图。 我们在KITTI、CityScapes和我们的VXXLXX_STEREO室内数据集上进行了广泛的实验，以证明我们方法的有效性。我们的怪物网络在RMSE、PSNR和SSIM等所有指标上都大大超过了最先进的方法SOTA。 此外，在无监督的单眼深度估计任务中，从 "t形 "核中提取的差异信息比SOTA更可靠，证实了我们方法的有效性。
深度中立网络（DNN）在现代图像/视频数据库上进行训练时需要巨大的GPU内存。不幸的是，GPU内存作为一种硬件资源总是有限的，这限制了图像分辨率、批量大小和学习速率，可以用来提高DNN的性能。在本文中，我们提出了一种新的训练方法，称为Re-forwarding，可以大大减少训练中的内存使用。我们的方法在DNN计算图中自动找到一个顶点子集，并在第一次前进时只在这些顶点存储张力。 在后退过程中，进行额外的局部前向（称为再前向过程），以计算子集顶点之间缺失的张量。总的内存成本成为（1）子集顶点的内存成本和（2）局部再前向的最大内存成本之和。 我们提出了理论和算法，为具有线性或任意计算图的DNN实现了最佳的内存解决方案。实验表明，Re-forwarding在流行的DNN（如Alexnet、VGG、ResNet、Densenet和Inception net）上削减了高达80%的训练内存。
压缩是在资源有限的平台上部署大型神经网络的关键步骤。作为一种流行的压缩技术，量化限制了不同权重值的数量，从而减少了表示和存储每个权重所需的比特数。在本文中，我们研究了量化神经网络的表示能力。 首先，我们证明了量化ReLU网络在一大类函数上的普遍可接近性。然后，我们提供了权重数量的上界，以及在给定的接近误差边界和权重位宽的情况下，与函数无关和与函数有关的结构的存储器大小。 我们的结果显示，为了达到$epsilon$的近似误差边界，量化网络所需的权重数量不超过$/mathcal{O}\left(\log^5(1/epsilon)\right)$是未量化网络的两倍。 这个开销比误差边界所需的权重数量的下限要低得多，支持了各种量化技术的经验性成功。就我们所知，这是第一个关于量化神经网络复杂性边界的深入研究。
基于价值的强化学习（RL）方法（如Q-learning）已在各种领域显示出成功，如游戏和推荐系统（RS）。当行动空间是有限的，这些算法隐含地通过学习最优价值函数找到一个政策，这通常是非常有效的。然而，将Q-learning扩展到处理连续行动RL问题的一个主要挑战是，获得最佳Bellman备份需要解决连续行动最大化（max-Q）问题。虽然通常限制Q-函数的参数化为行动中的凹，以简化max-Q问题，但这样的限制可能导致性能下降。 在这项工作中，我们提出了CAQL方法，该方法使用Q-learning和几个即插即用的行动优化器中的一个来最小化贝尔曼残差。特别是，利用深度NN中优化理论的进步，我们表明max-Q问题可以用混合整数编程（MIP）来优化解决--当Q-函数有足够的表示能力时，这种基于MIP的优化会产生更好的策略，并且比同行（例如）更稳健。为了加快CAQL的训练，我们开发了三种技术，即(i)动态容忍，(ii)双重过滤和(iii)聚类。为了加快CAQL的推理，我们引入了同时学习最佳策略的行动函数。为了证明CAQL的效率，我们在具有不同程度行动约束的基准连续控制问题上将其与最先进的RL算法进行比较，结果表明CAQL在严重约束的环境中明显优于基于策略的方法。
生成对抗网络（GANs）已被证明是学习从复杂分布中抽取样本的强大框架。然而，GANs也是出了名的难以训练，模式崩溃和振荡是常见的问题。我们假设这至少部分是由于生成器分布的演变和神经网络的灾难性遗忘倾向，这导致鉴别器失去记忆来自生成器先前实例的合成样本的能力。 认识到这一点，我们的贡献是双重的。首先，我们表明GAN训练是一个比一些更典型的数据集更有趣和现实的持续学习方法评估的基准。
许多具有大规模标记训练数据的问题已经通过深度学习得到了令人印象深刻的解决。然而，未见过的类别分类（UCC）与目标类别提供的最小信息是工业中最常遇到的设置，这仍然是机器学习中具有挑战性的研究问题。以前的UCC方法要么不能生成一个强大的判别特征提取器，要么不能学习一个灵活的分类器，可以轻松适应未见过的类别。本文中，我们建议通过网络重新参数化来解决这些问题， `textit{i。 我们建议通过网络重参数化来解决这些问题，即把网络的可学习权重作为其他变量的函数进行重参数化，通过重参数化，我们把深度分类模型的特征提取部分和分类部分解耦，以适应UCC的特殊设置，确保强大的辨别能力和出色的适应性。在零次学习和多次学习的设置下，在几个广泛使用的基准数据集上进行UCC的广泛实验表明，我们的网络重参数化方法取得了最先进的性能。
蛋白质是无处不在的分子，其在生物过程中的功能是由其三维结构决定的。对蛋白质结构的实验鉴定可能很耗时，成本太高，而且并不总是可能。另外，可以用计算方法对蛋白质折叠进行建模，但不能保证总是产生最佳结果。GraphQA是一种基于图形的方法，用于评估蛋白质模型的质量，它具有有利的特性，如表征学习、顺序和三维结构的明确建模、几何不变性和计算效率。在这项工作中，我们展示了手工设计和表征学习方法的重大改进，并仔细评估了GraphQA的个别贡献。
我们研究了使用主动学习在不完美或有噪声的情况下逐步训练机器学习模型的问题。我们特别考虑了批量主动学习的设置，其中选择了多个样本，而不是经典设置中的单个样本，以减少训练的开销。 在基准图像分类数据集（MNIST、SVHN和CIFAR10）上的实验表明，我们比现有的主动学习策略有了很大的改进。我们在深度网络中引入了一个额外的去噪层，使主动学习对标签噪声具有鲁棒性，并显示出明显的改进。
艺术风格转移是合成一个内容与给定图像相似、风格与另一图像相似的图像的问题。虽然最近的前馈神经网络可以实时生成风格化的图像，但这些模型在给定一对风格/内容图像的情况下产生单一的风格化，而且用户对合成的输出没有控制权。 此外，风格转换取决于模型的超参数，对于不同的输入图像具有不同的 "最佳值"。因此，如果风格化的输出对用户没有吸引力，她/他必须尝试多个模型，或者用不同的超参数重新训练一个模型，以获得最喜欢的风格化。 在本文中，我们通过提出一种新颖的方法来解决这些问题，该方法允许在训练后实时调整关键的超参数，通过一组可手动调整的参数，使用户能够修改来自同一对风格/内容图像的合成输出，以寻找最喜欢的风格化图像。
最近的工作表明，深度强化学习代理可以从不经常出现的环境奖励中学习遵循类似语言的指令。然而，这将设计语言条件奖励函数的责任放在了环境设计者身上，随着环境和语言复杂性的增加，这可能不容易或难以实现。为了克服这个限制，我们提出了一个框架，在这个框架中，指令条件RL代理使用不是从环境中获得的奖励，而是从专家实例中共同训练的奖励模型来训练。 随着奖励模型的改进，它们学会准确地奖励代理人完成环境配置的任务--以及专家数据中不存在的指令。这个框架有效地将指令要求的表示与如何执行分开。在一个简单的网格世界中，它使代理人能够学习一系列的指令，这些指令需要与区块互动，理解空间关系和不明确的抽象安排。
我们提出了多任务软选项学习（MSOL），这是一个基于规划即推理的分层多任务框架。MSOL扩展了选项的概念，为每个任务使用单独的变异后验，由一个共享的先验规范化。学到的软选项是时间上的扩展，允许一个更高级别的主策略通过低频率的决策在新任务上进行更快的训练。 此外，MSOL允许对新任务的软选择进行微调，而不会取消对以前有用行为的学习，并避免了多任务训练中的局部最小值问题。我们通过经验证明，MSOL在具有挑战性的多任务环境中明显优于分层和平面转移学习基线。
我们提出了一种算法，即引导式变异自动编码器（Guided-VAE），它能够通过执行潜表征解构学习来学习可控的生成模型。学习目标是通过向VAE中的潜编码/嵌入提供信号而实现的，而不改变其主要骨干结构，因此保留了VAE的理想特性。 在无监督策略中，我们通过引入一个学习潜在几何变换和主成分的轻量级解码器来指导VAE的学习；在监督策略中，我们使用一个对抗性的激励和抑制机制来鼓励潜在变量的解缠。
神经语言模型（NLMs）是生成性的，它们对语法句子的分布进行建模。在巨大的语料库上进行训练，NLMs正在推动建模精度的极限。此外，它们也被应用于解码文本的监督学习任务，例如。然而，NLM的生成性可能无法保证对 "好 "和 "坏"（在特定任务的意义上）的句子进行区分，从而导致次优的性能。 这项工作提出了一种将生成性NLM调整为判别性NLM的方法。与常用的最大似然目标不同，所提出的方法旨在扩大 "好 "和 "坏 "句子之间的幅度。它是端到端的训练，可广泛用于涉及解码文本重新评分的任务。
传统上，卷积神经网络（CNN）用同一组滤波器处理不同的图像。然而，图像的变化给这种方式带来了挑战。在本文中，我们建议在前向传递中为卷积层生成特定样本的滤波器。由于滤波器是即时生成的，模型变得更加灵活，与传统CNN相比可以更好地适应训练数据。 由于滤波器通常是高维的，我们建议学习一组系数，而不是一组滤波器。这些系数被用来线性组合滤波器库中的基础滤波器，以生成CNN的最终滤波器。在MNIST、MTFL和CIFAR10数据集上评估了所提出的方法。实验结果表明，通过使用所提出的滤波器生成方法，可以提高基线模型的分类精度。
我们提出了一种新的随时神经网络，它允许通过具有不同宽度和深度的子网络进行部分评估。与传统的只有深度可控的随时网络相比，在各种动态资源预算下，架构多样性的增加导致了更高的资源利用率和随之而来的性能改善。我们强调架构特征，使我们的方案可行且高效，并展示了其在图像分类任务中的有效性。
我们提出了一个新的模型，用于进行通用的和多样化的逆合成反应预测。给定一个目标化合物，任务是预测可能产生目标的化学反应物。这个生成任务可以通过使用分子的SMILES表示，被框定为一个序列到序列的问题。在流行的Transformer架构的基础上，我们提出两个新的预训练方法，为我们的问题构建相关的辅助任务（可信的反应）。 此外，我们在架构中加入了一个离散的潜在变量模型，以鼓励模型产生多样化的替代预测。在美国专利文献（USPTO-50k）基准数据集的50k反应实例子集上，我们的模型比基线的性能有了很大提高，同时也产生了更加多样化的预测。
Disentanglement-PyTorch库的开发是为了促进新的变异算法的研究、实施和测试。在这个模块化库中，神经结构、潜空间的维度和训练算法是完全解耦的，允许在不同的变异方法中进行独立和一致的实验。 到目前为止，该库包括以下无监督算法的实现：VAE、Beta-VAE、Factor-VAE、DIP-I-VAE、DIP-II-VAE、Info-VAE和Beta-TCVAE，以及CVAE和IFCVAE等条件方法。 该库与AICrowd上主办的2019年NeurIPS的Disentanglement挑战赛兼容，并被用来参加挑战赛的第一和第二阶段的比赛，在那里它被排在最好的几个参与者中。
信任区域方法，如TRPO，通常用于稳定强化学习（RL）中的策略优化算法。虽然目前的信任区域策略对连续控制是有效的，但它们通常需要与环境进行大量的政策上的互动。为了解决这个问题，我们提出了一种非政策信任区域方法，Trust-PCL，它利用了一个观察，即具有相对熵正则器的最大奖励目标的最佳政策和状态值沿任何路径满足一组多步骤路径一致性。 相对熵正则化的引入使得Trust-PCL能够保持优化的稳定性，同时利用非政策数据来提高样本效率。当对一些连续控制任务进行评估时，Trust-PCL显著提高了TRPO的解决方案质量和样本效率。
深度强化学习最近取得了许多成功，但我们对其优势和局限性的理解受到了阻碍，因为我们缺乏丰富的环境，在这些环境中，我们可以完全描述最优行为，并相应地针对这种描述诊断个别行动。在这里，我们考虑了一系列组合游戏，这些游戏产生于Erdos、Selfridge和Spencer的工作中，我们建议将它们作为评估和比较不同强化学习方法的环境。这些游戏有许多吸引人的特点：它们对当前的学习方法具有挑战性，但它们形成了（i）低维的、简单参数化的环境，其中（ii）从任何状态都有一个线性封闭形式的最优行为解决方案，以及（iii）游戏的难度可以通过改变环境参数的可解释方式来调整。 我们不仅使用这些Erdos-Selfridge-Spencer游戏来比较不同的算法，而且还比较了基于监督和强化学习的方法，分析了多代理方法在提高性能方面的力量，并评估了对训练集以外环境的概括。
在安全关键系统中采用深度学习提高了理解深度神经网络不理解的需求。已经提出了一些估计模型不确定性的方法，但这些方法限制了神经网络的训练或构建方式。我们提出了神经网络中的离群点检测（ODIN），这是一种在预测期间检测离群点观测的无假设方法，基于广泛用于制造过程监控的原则。 通过使用隐层流形的线性近似，我们在不改变结构或训练的情况下，将预测时的离群点检测添加到模型中。我们证明了ODIN在Fashion-MNIST、ImageNet-synsets和语音命令识别的预测过程中有效地检测离群点。
这项工作介绍了一个简单的网络，用于产生字符感知的单词嵌入。位置不可知和位置感知的字符嵌入相结合，为每个单词产生一个嵌入向量。学习到的单词表征被证明是非常稀疏的，有利于提高语言建模任务的结果，尽管使用的参数明显减少，而且不需要应用dropout。
我们提出了极低精度神经网络的第三个好处：提高了对某些对抗性攻击的鲁棒性，在最坏的情况下，性能与全精度模型相当。 我们专注于权重和激活都被量化为$pm$1的超低精度情况，并注意到仅在一层中随机量化权重就可以大大减少迭代攻击的影响。 我们观察到，非规模化的二进制神经网络表现出类似于原始的\emph{defensive distillation}程序的效果，导致了\emph{梯度掩蔽}，以及错误的安全概念。我们通过对不人为掩蔽梯度的二进制模型进行黑箱和白箱实验来解决这个问题。
无监督的双语字典归纳法（UBDI）对于无监督的机器翻译和模型向低资源语言的跨语言转移非常有用。UBDI的一种方法是使用生成对抗网络（GANs）与线性生成器对齐不同语言的单词向量空间，在一些语言对中实现了最先进的性能，然而对于一些语言对，基于GAN的归纳不稳定或完全不能对齐向量空间。 我们专注于证明存在线性转换的情况，但基于GAN的UBDI的性能在很大程度上取决于模型的初始化。我们表明，不稳定性取决于矢量集的形状和密度，但不取决于噪声；它是局部优化的结果，但无论是过度参数化还是改变批次大小或学习率都不能持续减少不稳定性。然而，我们可以通过基于无监督停止标准的最佳N模型选择，稳定基于GAN的UBDI。
由于计算机软件无处不在，软件漏洞检测（SVD）已经成为软件行业和计算机安全领域的一个重要问题。SVD中最关键的问题之一是应对项目中标签漏洞的稀缺性，这需要软件安全专家对代码进行费力的手工标签。 一个可能的解决方法是采用深度领域适应，最近在将学习从结构性标记的数据源转移到未标记的数据源方面取得了巨大的成功。一般的想法是将源数据和目标数据都映射到一个联合特征空间，并在这个联合特征空间中缩小这些数据的差异差距。 生成对抗网络（GAN）是一种试图弥补差异差距的技术，也是开发具有最先进性能的深度领域适应方法的基石。然而，使用GAN原理来弥补差异差距的深度领域适应方法会受到模式崩溃问题的影响，对预测性能产生负面影响。 我们在本文中的目的是提出双生成器-判别器深层代码域适应网络（Dual-GD-DDAN），用于解决在SVD背景下从有标签的软件项目到无标签的软件项目的转移学习问题，以解决以前的方法所面临的模式塌陷问题。在真实世界的软件项目上的实验结果表明，我们提出的方法在很大程度上超越了最先进的基线。
表征学习是深度学习的基础之一，允许对一些机器学习任务进行重要的改进，如神经机器翻译、问题回答和语音识别。最近的工作提出了学习图中节点和边的表征的新方法。 在本文中，我们提出了一种有效的也是高效的方法来生成图中的节点嵌入，该方法采用对节点紧邻的有限数量的排列组合作为上下文来生成其表示，因此是以自我为中心的表示。我们提出了一个全面的评估，表明我们的方法在与链接预测和节点分类问题有关的六个不同的数据集中优于最先进的方法，在为非常大的图生成节点嵌入时比基线快一到三个数量级。
正交递归神经网络通过使用正交矩阵对递归连接进行参数化来解决梯度消失问题。这类模型对于解决需要记忆长序列的任务特别有效。 我们展示了一个最近提出的递归架构，即线性记忆网络，由一个非线性前馈层和一个单独的线性递归组成，可以用来解决困难的记忆任务。我们提出了一个初始化方案，将递归架构的权重设置为近似于输入序列的线性自动编码器，可以用一个闭式解法找到这个初始化方案。   我们认为这种方法优于随机正交初始化，因为自动编码器允许在训练前记忆长序列。实证分析表明，我们的方法在顺序MNIST、混合MNIST和TIMIT上取得了与其他正交模型和LSTM竞争的结果。
本文改进了将命名实体识别（NER）表述为序列标签问题的研究思路。我们使用所谓的黑盒长短期记忆（LSTM）编码器来实现最先进的结果，同时对自动回归模型通过并行的自我注意机制学习的内容提供了深刻的理解。具体而言，我们将NER的序列标签问题解耦为实体分块，例如。具体来说，我们将NER的序列标签问题解耦为实体分块，例如：Barack_B Obama_E was_O elected_O，和实体打字，例如：Barack_PERSON Obama_PERSON was_NONE elected_NONE，并分析该模型如何学习或难以捕捉每个子任务的文本模式。我们获得的洞察力导致我们探索一个更复杂的深度交叉Bi-LSTM编码器，鉴于经验结果和理论依据，它被证明能更好地捕捉全球互动。
知识图谱嵌入（KGE）是为给定的知识图谱共同学习实体和关系嵌入的任务。现有的学习KGE的方法可以被视为一个两阶段的过程，其中（a）知识图谱中的实体和关系用一些线性代数结构（嵌入）表示，（b）定义一个评分函数，用相应的关系和实体嵌入来评估两个实体之间的关系强度。 不幸的是，先前关于第一步中的评分函数的建议都是启发式的，而且不清楚KGE中的评分函数与底层知识图谱的生成过程有什么关系。具体来说，给定一个由一组关系三联体（h，R，t）表示的知识图谱，其中语义关系R在两个实体h（头）和t（尾）之间存在，我们对随机行走模型（Arora等人。我们推导出联合概率p（h，R，t）与h、R和t的嵌入之间的理论关系。此外，我们表明，边际损失最小化，这是一个在KGE中被广泛使用的目标，根据我们的理论关系，在从KGE中估计的概率下，自然会出现对数似然比最大化。 我们提出了一个以理论分析为动机的学习目标，从一个给定的知识图谱中学习KGE。我们提出的方法学习的KGE在FB15K237和WN18RR基准数据集上获得了最先进的性能，为理论的支持提供了经验证据。
目前，共享深度学习模型治理的唯一技术是同态加密和安全的多方计算。不幸的是，由于其巨大的计算和通信开销，这些技术都不适用于大型神经网络的训练。作为共享模型治理的可扩展技术，我们建议在多方之间分割深度学习模型。 给定整个训练数据集或环境模拟器，以及一个训练好的深度学习模型的参数子集，需要多少训练才能恢复模型的原始性能？ 我们定义了一个评估模型完成问题硬度的指标，并在ImageNet上的监督学习以及Atari和DeepMind Lab上的强化学习中对其进行了实证研究。我们的实验表明：（1）强化学习中的模型完成问题比监督学习中更难，因为训练过的代理的轨迹不可用；（2）其硬度主要不是取决于缺失部分的参数数量，而是更取决于其类型和位置。 我们的结果表明，在一些训练成本很高的环境中，模型拆分可能是一种可行的共享模型治理技术。
本文提出了变异域适应，这是一个统一的、可扩展的、简单的框架，用于通过变异推理学习多个分布。与现有的通过深度生成模型进行域转移的方法不同，如StarGAN（Choi等人，2017）和UFDN（Liu等人。首先，不需要来自目标的样本。相反，该框架需要一个已知的源作为先验$p(x)$和二元判别器，$p(mathcal{D}_i|x)$，将目标域$mathcal{D}_i$与其他域进行判别。 因此，该框架将目标视为可以通过贝叶斯推理明确制定的后验，$p(x|\mathcal{D}_i) \propto p(\mathcal{D}_i|x)p(x)$，正如进一步提出的双变量自动编码器（DualVAE）模型所显示的那样。 正如VAE将样本$x$编码为潜空间上的模式：$/mu(x) \in \mathcal{Z}$，DualVAE将域$/mathcal{D}_i$编码为双潜空间$/mu^*(\mathcal{D}_i) \in \mathcal{Z}^*$的模式，命名为域嵌入。 它用一个自然的解析$langle, \rangle来重新表述后验。\mathcal{Z} \times \mathcal{Z}^* \rightarrow \Real$，它可以扩展到不可数的无限域，如连续域以及插值。第三，与GANs相比，DualVAE无需复杂的自动/手动超参数搜索就能快速收敛，因为它只需要VAE的一个额外参数。 通过数值实验，我们用CelebA上多达60个域的多域图像生成任务证明了这三个优点，并表明DualVAE记录了最先进的性能，超过了StarGAN和UFDN。
我们提出了一种基于对抗性训练和可证明防御的新方法来训练神经网络。关键的想法是将训练建模为一个包括验证者和对抗者的程序。在每个迭代中，验证者的目标是使用凸松弛来认证网络，而对抗者则试图在凸松弛内找到导致验证失败的输入。 我们的实验表明，这种训练方法是有希望的，并实现了两全其美--它在具有挑战性的CIFAR-10数据集上产生了具有最先进的准确性（74.8%）和认证鲁棒性（55.9%）的模型，并具有2/255 L-无限扰动。这比目前已知的68.3%准确性和53.9%认证鲁棒性的最佳结果有了显著改善，这些结果是使用比我们工作大5倍的网络实现的。
源代码的学习任务（即。例如，在遥远的地方使用相同的变量或函数所引起的长距离依赖性往往不被考虑。 我们建议使用图来表示代码的语法和语义结构，并使用基于图的深度学习方法来学习推理程序结构。在这项工作中，我们介绍了如何从源代码中构建图，以及如何将门控图神经网络训练扩展到这种大型图。我们在两个任务上评估了我们的方法：VarNaming和VarMisuse，在前者中，网络试图根据变量的用途预测其名称；在后者中，网络学习推理选择应该在给定程序位置使用的正确变量。我们与使用结构较少的程序表示方法的比较显示了对已知结构建模的优势，并表明我们的模型学习推断有意义的名称并在许多情况下解决VarMisuse的任务。
过度拟合是神经网络训练中一个普遍存在的问题，通常使用保持数据集来缓解。在这里，我们挑战这个理由，并研究在不使用保持数据集的情况下过度拟合的标准。具体来说，我们用不同比例的随机标签和一系列的正则化强度，多次训练一个固定数量的epochs的模型。一个经过适当训练的模型不应该达到大于正确标记的数据点的准确度，否则模型就会过度拟合。我们引入了两个检测过拟合的标准和一个检测欠拟合的标准。我们分析了早期停止、正则化因子和网络深度。在安全关键应用中，我们对表现良好且不可能过拟合的模型和参数设置感兴趣。
部分可观察的马尔科夫决策过程（POMDPs）是一个广泛使用的框架，用于模拟环境的不确定性和随机结果下的决策。在传统的POMDP模型中，代理人收到的观察结果来自于固定的已知分布。 由于这种选择过程的组合性，将感知决策与规划决策结合起来在计算上是难以实现的。为了防止行动空间的这种扩展，我们提出了一种贪婪的观察选择策略，旨在使状态的不确定性最小化。我们开发了一种新型的基于点的价值迭代算法，该算法结合了贪婪策略，以实现采样的信念点的近似最佳的不确定性降低。
深度神经网络，特别是卷积神经网络，已经成为压缩图像和解决逆向问题的非常有效的工具，包括去噪、绘画和从少数和嘈杂的测量中重建。这种成功可以部分归因于它们能够很好地表示和生成自然图像。在本文中，我们提出了一个未经训练的简单图像模型，称为深度解码器，它是一个深度神经网络，可以从非常少的权重参数中生成自然图像。深度解码器有一个简单的架构，没有卷积，权重参数比输出维度少。这种欠参数化使深度解码器能够将图像压缩成一组简洁的网络权重，我们表明这与基于小波的阈值处理是一样的。 深度解码器很简单，每一层都有一个相同的结构，只包括一个上采样单元、通道的像素级线性组合、ReLU激活和通道级归一化。
在本文中，我们研究了可由具有整流线性单元（ReLU）的深度神经网络（DNN）表示的函数系列。我们给出了一种算法来训练具有一个隐藏层的ReLU DNN到{em global optimality}，其运行时间与数据大小成多项式，但与输入维度成指数关系。 此外，我们改进了已知的用较浅的ReLU网逼近ReLU深层网函数的尺寸下限（从指数级到超指数级）。我们的差距定理对平滑参数化的 "硬 "函数系列成立，与文献中已知的可数离散系列相反。 我们的差距定理的一个例子是：对于每个自然数$k$，存在一个可由具有$k^2$隐藏层和总规模$k^3$的ReLU DNN表示的函数，这样，任何具有最多$k$隐藏层的ReLU DNN将至少需要$frac12k^{k+1}-1$的总节点。 最后，对于具有ReLU激活的$R^n\ to \R$ DNN家族，我们展示了一个新的仿生件数量的下限，在网络结构的某些阶段，它比以前的构造更大，最独特的是我们的下限是通过明确构造一个达到这个比例的函数家族/emph{smoothly parameterized}来证明的。我们的构造利用了多面体理论中的宗法理论。
误差反向传播算法（BP）通常被认为不可能在真实的大脑中实现。然而，最近深度网络在机器学习和人工智能中的成功，激发了许多关于理解大脑如何跨层学习，从而如何实现或接近BP的建议。 到目前为止，这些建议都没有在BP指导的深度学习任务中得到严格的评估，或者在比简单的全连接网络更有结构性的架构中得到评估。 对于CIFAR-10，我们展示了我们的算法，一个直接的、无权重传输的差分目标传播（DTP）的变体，经过修改以去除倒数第二层的反向传播，在训练具有无权重的局部定义的感受野的深度网络方面与BP有竞争力。 对于ImageNet，我们发现DTP和我们的算法的表现都比BP差很多，这就提出了关于是否需要不同的架构或算法来扩展这些方法的问题。我们的结果和实施细节有助于为未来的生物动机深度学习方案建立基线。
深度神经网络（DNNs）通常包含数百万甚至数十亿的参数/权重，这使得存储和计算都非常昂贵。 另一个著名的控制DNN复杂性的方法是参数共享/配对，其中某些权重组被迫共享一个共同的值。有些形式的权重共享是硬性规定的，以表达某些in-variance，一个明显的例子是卷积层的shift-invariance。然而，在学习过程中，可能还有其他权重组被捆绑在一起，从而进一步重塑了网络的复杂性。 在本文中，我们采用了一个最近提出的诱导稀疏性的正则器，名为GrOWL（group ordered weighted l1），它鼓励稀疏性，并模拟地学习哪些参数组应该共享一个共同的值。GrOWL在线性回归中被证明是有效的，能够识别和处理强相关的协变量。l1 a.k.a.Lasso）不同，GrOWL不仅通过将所有相应的权重设置为零来消除不重要的神经元，而且还通过将相应的权重绑定到一个共同的值来明确识别强相关的神经元。GrOWL的这种能力促使了以下两阶段的程序。(i) 在训练过程中使用GrOWL正则化，以同时识别重要的神经元和应该绑在一起的参数组；(ii) 重新训练网络，强制执行前一阶段所揭示的结构，即只保留重要的神经元并强制执行学到的绑缚结构。我们在几个基准数据集上评估了所提出的方法，表明它可以极大地压缩网络而对泛化性能没有任何损失。
权重共享--使用相同的参数同时优化多个神经网络--已经成为最先进的神经架构搜索的一个关键组成部分。然而，它的成功很少被理解，而且经常被发现是令人惊讶的。我们认为，权重共享方法不仅仅是一个优化技巧，而是由结构化的假设空间的放松引起的，并引入了新的算法和理论挑战，以及神经架构搜索以外的应用。 在算法上，我们展示了在设计基于梯度的最小化方法时，分担权重的ERM的几何形状需要更加谨慎，并应用非凸的非欧几里得优化工具来给出适应基本结构的通用算法。 我们进一步分析了实用分权方法所解决的二层优化的学习理论行为。接下来，使用内核配置和NLP特征选择作为案例研究，我们证明了分权如何适用于NAS的架构搜索泛化，并有效地优化所产生的二层目标。最后，我们使用我们的优化分析，为NAS开发一个简单的指数化梯度方法，与基础优化几何相一致，并与CIFAR-10上的最先进的方法匹配。
深层潜伏变量模型最近在许多数据领域取得了成功。无损压缩是这些模型的一种应用，尽管有可能非常有用，但尚未以实际方式实施。我们提出了 "Bits Back with ANS"（BB-ANS），这是一种以接近最佳速率对潜伏变量模型进行无损压缩的方案。 我们通过使用变异自动编码器模型（VAE）压缩MNIST数据集来证明这一方案，实现了优于仅使用简单VAE的标准方法的压缩率。鉴于该方案非常适于并行化，我们得出结论，如果有足够高质量的生成模型，该方案可用于在可接受的运行时间内实现压缩率的大幅提高。我们将我们的实现开源在https://github.com/bits-back/bits-back。
超参数调整可以说是在深度网络中获得最先进性能的最重要因素。 我们专注于与优化算法相关的超参数，例如学习率，它对训练速度和结果的准确性有很大的影响。通常，在训练期间采用固定的学习率计划。我们提出Hyperdyn是一种动态的超参数优化方法，在每个历时结束时选择新的学习率。  我们在CIFAR和Imagenet数据集上获得了最先进的准确性结果，但与最好的人工调整的网络相比，训练速度明显加快。
基于多跳的文本问题回答是目前机器理解中的一个挑战。在本文中，我们提出了一个新的架构，称为 "潜在问题重构网络"（LQR-net），这是一个多跳和平行的关注网络，是为需要推理能力的问题回答任务而设计的。LQR-net由textbf{阅读模块}和textbf{重构模块}的关联组成。 我们在旨在评估多跳推理能力的\hotpotqa问题解答数据集上评估了我们的架构。我们的模型在公共排行榜上取得了有竞争力的结果，在精确匹配（EM）和$F_1$得分方面优于当前最好的\textit{published}模型。
我们提出了一种方法来自动计算时间序列中每个观测点的特征的重要性，通过模拟给定以前的观测点的反事实轨迹。我们把每个观测点的重要性定义为用生成的观测点代替模型输出的变化。我们的方法可以应用于任意复杂的时间序列模型。我们把生成的特征重要性与现有的方法如敏感性分析、特征闭塞和其他解释基线进行比较，表明我们的方法产生了更精确的解释，对输入信号的噪声不那么敏感。
本文讨论了无监督领域的适应性，即在源域上有标记的训练数据，但目标是在只有未标记数据的目标域上有良好的性能。与以前的许多工作一样，我们试图在保留可辨别性的同时，使源域和目标域的学习表征一致。 每个自我监督任务都使两个领域沿着与该任务相关的方向更紧密地结合在一起。 我们在七个标准基准中的四个上取得了最先进的结果，并在分割适应上取得了有竞争力的结果。我们还证明了我们的方法与另一种流行的像素级适应方法有良好的结合。
我们介绍了计算概率分布的MinHash的简单有效的算法，适用于稀疏和密集的数据，其运行时间与两种情况下的技术水平相当。这些算法的碰撞概率是我们详细研究的正向量的相似性的新措施。 我们描述了这种碰撞概率对于任何基于抽样的位置敏感哈希的意义。我们认为这种相似性度量对于概率分布比其他算法追求的加权MinHash的相似性更有用，而且是Jaccard指数的自然概括。
最近，机器学习领域在改善关系推理方面取得了进展。在现有的模型中，图神经网络（GNNs）是多跳关系推理最有效的方法之一。事实上，多跳关系推理在许多自然语言处理任务中是不可缺少的，如关系提取。本文中，我们提出根据自然语言句子生成图神经网络（GP-GNNs）的参数，这使得GNNs可以处理非结构化文本输入的关系推理。 我们验证了GP-GNNs在文本中的关系提取。在一个人类注释的数据集和两个远距离监督的数据集上的实验结果表明，与基线相比，我们的模型取得了明显的改进。我们还进行了定性分析，证明我们的模型可以通过多跳关系推理发现更准确的关系。
通常，批评者的行动价值函数使用时间差进行更新，而批评者反过来为行为者提供一个损失，训练它采取具有更高预期收益的行动。在本文中，我们引入了一个新的和灵活的元批评者，观察学习过程并为行为者元学习一个额外的损失，加速并改善行为者批评者的学习。 与vanilla批评家相比，元批评家网络是明确训练的，以加速学习过程；与现有的元学习算法相比，元批评家是针对单一任务快速在线学习的，而不是在一系列任务中缓慢学习。关键是，我们的元批评家框架是为基于Off-Policy的学习者设计的，目前提供最先进的强化学习样本效率。我们证明，在线元批评家学习与当代Off-PAC方法DDPG、TD3和最先进的SAC相结合，导致各种连续控制环境中的改进。
现代神经网络是高度超参数化的，有能力对训练数据进行大幅度的过度拟合。然而，这些网络在实践中往往具有良好的泛化能力。 特别是，我们为应用于ImageNet分类问题的现实架构提供了第一个非空洞的泛化保证。此外，我们表明倾向于过度拟合的模型的可压缩性是有限的。经验结果表明，过度拟合的增加增加了描述一个训练过的网络所需的比特数。
对抗性例子可以被定义为诱发错误的模型的输入--模型的输出与神谕的输出不同，也许是以令人惊讶或恶意的方式。虽然在自然语言处理（NLP）的设置中已经提出了一些攻击，但它们在定义攻击的参数和成功的攻击会是什么样子方面往往有所不同。 我们定义了对抗性增益的概念：基于控制理论，它是一个系统输出相对于提交给学习者的输入扰动（由所谓的对抗者引起）的变化的度量。正如我们所展示的，这个定义可以在不同的特征空间和距离条件下用于确定不同直观流形的攻击或防御效果。 这个对抗性增益的概念不仅为评估对抗和防御提供了一个有用的方法，而且由于其植根于稳定性和流形理论的性质，可以作为未来对抗下鲁棒性工作的一个基石。
我们提出了一个翘曲的残余网络（WarpNet），使用一个可并行的翘曲算子来向前和向后传播到遥远的层，其训练速度比原始的残余神经网络更快。我们在残余网络上应用扰动理论，并将残余单元之间的相互作用解耦。我们通过广泛的性能研究证明，在参数数量相同的情况下，所提出的网络实现了与原始残差网络相当的预测性能，同时在总的训练时间上实现了显著的加速。由于WarpNet在残差网络训练中执行模型并行，其中权重分布在不同的GPU上，与原始残差网络相比，它具有加速和训练更大网络的能力。
可解释人工智能（XAI）社区提出了大量试图解释黑盒模型预测的方法。然而，衡量生成的解释的质量在很大程度上还没有被探索，这使得定量比较变得非同寻常。在这项工作中，我们提出了一套多方面的指标，使我们能够根据生成的解释的正确性、一致性以及信心来客观地比较解释者。 这些指标在计算上是便宜的，不需要模型再训练，并且可以在不同的数据模式中使用。我们在常见的解释器上评估了这些指标，如Grad-CAM、SmoothGrad、LIME和Integrated Gradients。我们的实验表明，所提出的指标反映了早期工作中报告的定性观察。
众所周知，神经网络在远离训练分布的输入上会产生意想不到的结果。解决这个问题的一种方法是检测训练过的网络不能可靠地回答的样本。ODIN是最近提出的一种分布外检测方法，它不修改训练过的网络，在各种图像分类任务中取得了良好的性能。在本文中，我们将ODIN适应于句子分类和单词标签任务。
最近的一些工作显示了深度2和深度3神经网络的表达能力之间的分离。这些分离结果是通过构建函数和输入分布来显示的，因此函数可以被一个多项式大小的深度3神经网络很好地近似，但它不能被任何多项式大小的深度2神经网络在选定的输入分布下很好地近似。 这些结果并不稳健，需要精心选择的函数以及输入分布。我们展示了在一大类输入分布上，只要权重是多项式约束的，深度-2和深度-3的sigmoidal神经网络的表达能力之间的类似分离。在这样做的同时，我们还展示了具有小宽度和小权重的深度-2 sigmoidal神经网络可以被低度多元多项式很好地近似。
在强化学习（RL）中，加权图可以被解释为由作用于环境的行为策略引起的状态转换过程，对拉普拉斯的特征向量进行逼近为状态表示学习提供了一种有希望的方法。 首先，它们的计算成本很高，经常需要对大矩阵进行操作。其次，这些方法在简单的、表格的、有限状态的设置之外缺乏足够的理由。在本文中，我们提出了一种完全通用的、可扩展的方法，用于在无模型的RL背景下对拉普拉斯的特征向量进行逼近。 最后，我们展示了在实现目标的RL任务中使用我们的方法学到的拉普拉斯表示的潜在好处，提供证据表明我们的技术可以用来显著提高RL代理的性能。
我们的工作为从语义标签图和计算机图形（CG）模拟边缘图图像到照片现实图像的领域转换提供了一种新的方法。我们以条件方式训练生成对抗网络（GAN），以生成给定CG场景的照片现实版本。 现有的GAN架构仍然缺乏训练DNN用于计算视觉任务所需的照片写实能力，我们通过嵌入边缘图和以对抗模式训练来解决这个问题。我们还提供了对我们模型的扩展，使用我们的GAN架构来创建视觉上有吸引力的和时间上连贯的视频。
深度神经网络被广泛应用于各个领域，但过高的计算复杂性阻碍了它们在移动设备上的部署。许多模型压缩算法已被提出，然而，选择适当的超参数以获得有效的压缩模型往往是困难和耗时的。在本文中，我们提出了一个模型压缩和加速的自动化框架，即PocketFlow。 这是一个易于使用的工具包，它集成了一系列的模型压缩算法，并嵌入了一个超参数优化模块，以自动搜索超参数的最佳组合。此外，压缩后的模型可以转换为TensorFlow Lite格式，并很容易部署在移动设备上，以加快推理速度。PocketFlow现在是开源的，可在https://github.com/Tencent/PocketFlow。
生成模型为复杂分布中的结构建模提供了一种方法，并被证明对许多实际感兴趣的任务是有用的。然而，目前训练生成模型的技术需要获得完全观察的样本。在许多情况下，获得完全观察的样本是昂贵的，甚至是不可能的，但获得部分的、噪声的观察是经济的。 我们表明，即使在存在每样本信息损失的情况下，真实的基础分布也可以被证明是可以恢复的。基于此，我们提出了一种训练生成对抗网络（GAN）的新方法，我们称之为AmbientGAN。在三个基准数据集上，对于各种测量模型，我们展示了实质性的定性和定量改进。
随机矩阵理论（RMT）被应用于分析深度神经网络（DNN）的权重矩阵，包括生产质量、预训练的模型，如AlexNet和Inception，以及从头开始训练的小型模型，如LeNet5和一个微型AlexNet。 经验和理论结果清楚地表明，DNN层矩阵的经验谱密度（ESD）显示出传统正则化统计模型的特征，即使在没有外生指定传统正则化形式的情况下，如Dropout或Weight Norm约束。 在RMT的最新成果的基础上，特别是它对重尾矩阵的普遍性类的扩展，我们开发了一个理论来识别5+1个训练阶段，对应于越来越多的隐式自我正则化。 对于较小和/或较老的DNN来说，这种隐性自我规范化就像传统的Tikhonov规范化，因为有一个 "大小尺度 "来区分信号和噪声。 然而，对于最先进的DNN，我们发现了一种新的重尾自规整形式，类似于无序系统的统计物理学中的自组织。 这种隐性的自律化可能强烈依赖于训练过程中的许多旋钮。 通过利用泛化差距现象，我们证明了我们可以使一个小模型仅仅通过改变批次大小就表现出训练的所有5+1阶段。
我们引入了一种注意力机制，以改善半监督环境下的深度主动学习（AL）的特征提取。所提出的注意力机制是基于最近的方法来直观地解释DNN所做的预测。我们将所提出的基于解释的注意力应用于MNIST和SVHN分类。所进行的实验显示，与基于不确定性的方法相比，在相同数量的训练实例下，原始和类平衡数据集的准确性得到了改善，长尾收敛速度更快。
我们应用梯度复合体（barcodes）的典型形式来探索神经网络的损失面。我们提出了一种计算目标函数的最小值barcodes的算法。 我们的实验证实了两个主要的观察结果。(1)最小值的条码位于目标函数值范围的较低部分；(2)神经网络深度的增加会降低最小值的条码。这对神经网络的学习和泛化能力有自然的影响。
正在开发和进入市场的新型计算硬件有希望以像GPU一样深刻的方式革新深度学习。然而，现有的深度学习的软件框架和训练算法还没有发展到完全利用新一波硅的能力。特别是，通过复杂和依赖实例的控制流利用结构化输入的模型，很难使用现有的算法和硬件来加速，这些算法和硬件通常依赖于minibatching。 我们提出了一种异步模型并行（AMP）的训练算法，该算法是以在互连设备网络上的训练为特别动机的。通过在多核CPU上的实现，我们表明AMP训练在类似的历时数中收敛到与传统同步训练算法相同的精度，但更有效地利用了可用的硬件，甚至对于小规模的迷你批次，导致更短的整体训练时间。我们的框架为扩大今天无法有效训练的新一类深度学习模型打开了大门。
在合作式多代理强化学习（MARL）中，如何设计一个合适的奖励信号来加速学习和稳定收敛是一个关键问题。全局奖励信号为所有代理分配相同的全局奖励，而局部奖励信号仅根据个人行为为每个代理提供不同的局部奖励。这两种奖励分配方法都有一些缺点：前者可能鼓励懒惰的代理，而后者可能产生自私的代理。本文中，我们研究基于分组路由环境的合作式MARL的奖励设计问题。 首先，我们表明上述两种奖励信号容易产生次优策略。然后，在一些观察和考虑的启发下，我们设计了一些混合的奖励信号，这些信号是现成的，可以学习更好的策略。最后，我们把混合的奖励信号变成了自适应的对应信号，在我们的实验中取得了最佳效果。
最近的进展表明，使用训练数据学习解决成像中的线性逆向问题通常是可能的，它可以胜过更传统的正则化最小二乘法的解决方案。沿着这些思路，我们提出了Neumann网络的一些扩展，这是一个最近推出的端到端学习架构，其灵感来自于正则化最小二乘法问题的解图的截断Neumann序列扩展。 在这里，我们总结了诺伊曼网络的方法，并表明它的形式与给定的逆问题的最佳重建函数兼容。我们还研究了诺伊曼网络的一个扩展，它包含了一个更有效的基于样本的正则化方法。
在我们的模型中，提出了一个全局记忆编码器和一个局部记忆解码器来共享外部知识。编码器对对话历史进行编码，修改全局上下文表示，并生成一个全局记忆指针。 接下来，它通过全局内存指针来过滤外部知识的相关信息，然后通过本地内存指针来实例化插槽。我们的经验表明，我们的模型可以提高复制的准确性，并减轻常见的词汇外问题。因此，GLMP能够在模拟的bAbI对话数据集和人与人之间的斯坦福大学多领域对话数据集的自动和人工评估中比以前最先进的模型有所提高。
棋盘现象是计算机视觉领域中众所周知的视觉伪影之一。像素空间中棋盘伪影的起源和解决方案已经被研究了很久，但它们对梯度空间的影响却很少被研究。在本文中，我们重新审视了梯度空间中的棋盘伪影，它被证明是网络架构的弱点。我们探索了梯度棋盘伪影的图像无关性，并通过利用伪影提出了一种简单而有效的防御方法。 我们介绍了我们的防御模块，被称为人工棋盘增强器（ACE），它在指定的像素上诱导对抗性攻击。这使模型能够通过在图像中只移动一个像素来转移攻击，具有显著的防御率。我们提供了广泛的实验来支持我们的工作对各种攻击场景的有效性，使用最先进的攻击方法。此外，我们表明ACE甚至适用于大规模的数据集，包括ImageNet数据集，可以轻松转移到各种预训练的网络。
由于深度生成模型和对抗方法的兴起，Min-max公式在ML界引起了极大的关注，而理解用于解决此类公式的（随机）梯度算法的动态性一直是一个巨大的挑战。 作为第一步，我们限制了双线性零和博弈，并对流行的梯度更新进行了系统分析，包括同步和交替版本。
广义零点学习的大多数方法依赖于图像特征空间和类嵌入空间之间的跨模态映射，或者依赖于生成人工图像特征。然而，通过对齐特定模态自动编码器的潜在空间来学习共享的跨模态嵌入，被证明在（广义）零点学习中是有希望的。 在遵循同一方向的同时，我们还将人工特征生成向前推进了一步，并提出了一个模型，即通过对齐的变异自动编码器来学习图像特征和类嵌入的共享潜在空间，目的是生成潜在特征来训练一个softmax分类器。我们在传统的基准数据集上评估了我们学到的潜在特征，并在广义零点学习和少数点学习上建立了新的技术状态。
我们提出并分析了三种方法，即Shuffle Conv、GAP+FC和1x1 Conv，它们在训练和测试阶段都会破坏空间信息。 我们在几个物体识别数据集（CIFAR100、Small-ImageNet、ImageNet）上对这些方法进行了广泛的评估，并采用了多种CNN架构（VGG16、ResNet50、ResNet152、MobileNet、SqueezeNet）。有趣的是，我们一直观察到，空间信息可以从相当多的层中完全删除，而性能没有下降或只下降很小。
在本文中，我们介绍了两种新的分解方法。我们的第一种方法，无标签分解GAN（UD-GAN，无监督），通过生成相似/不相似的图像对来分解潜在的噪声，它通过连体网络和对比损失在这些图像对上学习距离度量，这种成对的方法为相似的数据点提供一致的表示。 我们的第二种方法（UD-GAN-G，弱监督）用用户定义的指导函数修改了UD-GAN，这些指导函数限制了进入连体网络的信息。
我们提出了预测变量，这是一种使机器学习（ML）成为编程语言中一等公民的方法。在构建系统的方法中存在着越来越大的分歧：一方面是使用人类专家（如编程），另一方面是使用从数据中学习的行为（如ML）。我们描述了PVars及其接口，如何在编程中使用它们，并在三个算法问题上证明了我们方法的可行性：二进制搜索、QuickSort和缓存。我们通过实验表明，PVars能够改进常用的启发式算法，并导致比原始算法更好的性能。 我们的PVars实现目前依赖于标准的强化学习（RL）方法。为了更快地学习，PVars使用它们所取代的启发式函数作为初始函数。我们表明，PVars很快就能掌握初始函数的行为，然后在此基础上提高性能，而不会表现得很差--允许在关键应用中安全部署。
最近的许多研究都致力于视频预测和生成，但大多是针对短尺度的时间范围。Villegas等人（2017）的分层视频预测方法是长期视频预测的最先进方法的一个例子。 然而，他们的方法在实际环境中的适用性有限，因为它需要在训练时有一个地面真实姿势（例如，人类的关节姿势）。  本文提出了一个没有这种限制的长期分层视频预测模型。我们表明，该网络学习了自己的高层次结构（例如，姿势等效的隐藏变量），在地面真实姿势不能完全捕获预测下一帧所需的所有信息的情况下，效果更好。  这种方法比其他不需要地面真实姿势的视频预测方法提供了更清晰的结果，其效率在人类3.6M和机器人推举数据集上得到了体现。
结合来自不同感官模式的信息来执行有目标的行动是人类智能的一个关键方面。具体来说，人类代理人能够非常容易地将一个感官领域（例如视觉）中传达的任务转化为一种表示，使他们能够在只能使用单独的感官模式（例如触摸）来感知环境时完成这一任务。为了建立具有类似能力的代理人，在这项工作中，我们考虑从抽屉里取出一个目标物体的问题。 代理人得到了一个以前未见过的物体的图像，它只用触觉来探索抽屉里的物体，在没有收到任何视觉反馈的情况下找回图像中的物体。我们提出了一种在模拟环境中使用拟人的手执行这一任务的方法。
局部敏感的散列方案，如Simhash，提供了多集的紧凑表示，从中可以估计出相似性。然而，在某些应用中，我们需要估计动态变化的集的相似性。 在这种情况下，我们需要表示法是同构的，这样集合的联合和差异的哈希值就可以直接从操作数的哈希值计算出来。 我们提出了两种具有这种属性的余弦相似性的表示法（an extension of \simhash and angle-preserving random projections），并在第三个用于Jaccard相似性的表示法（an extension of \minhash）上取得了实质性的进展。我们采用这些哈希值来压缩条件随机场（CRF）核心推理模型的充分统计，并研究这种压缩如何影响我们在推理期间实体分裂和合并时计算相似性的能力。 \我们在条件随机场（CRF）分层核心推理模型中研究这些哈希值，以计算推理过程中实体合并和分裂时的相似性。}我们还提供了新的统计分析，以帮助证明它作为CRF内部的估计器是合理的，表明偏差和方差随着比特数的增加而迅速减少。 在作者核对问题上，我们发现，只要我们采用至少128或256比特，我们的/simhash方案可以将分层核对算法扩大一个数量级，而不会降低其统计性能或模型的核对精度。 保角随机投影进一步提高了核心推理的质量，有可能允许使用更少的维度。
最近的分析表明，相互信息的天真kNN估计器有严重的统计局限性，这促使我们采用更精细的方法。 我们还特别分析了KL发散的Donsker-Varadhan下限，并表明，当考虑到简单的统计因素时，这个下限永远不能产生大于$ln N$的高置信度值。虽然大的高置信度下限是不可能的，但在实践中可以使用没有正式保证的估计器。我们观察到，尽管交叉熵只是一个熵的上限，但交叉熵估计值以1/sqrt{N}$的速度收敛到真实的交叉熵。
在本文中，我们提出了一个称为神经元层次网络（NHN）的神经网络框架，它的发展超越了层中的层次，而集中在神经元的层次上。我们观察到手工制作和随机搜索的架构的权重中存在大量冗余。受人类大脑发展的启发，我们修剪了模型中的低敏感神经元，并向图中添加新的神经元，单个神经元之间的关系被强调，层的存在被弱化。 我们提出了一个通过随机架构搜索发现最佳基础模型的过程，并通过进化搜索发现新增神经元的最佳位置和连接。实验结果表明，NHN在Cifar-10上取得了比最先进的手工制作和随机搜索架构更高的测试精度，同时需要更少的参数和搜索时间。
在机器学习模型的训练数据注释成本高或甚至难以获得的情况下，模拟是一种有用的工具。在这项工作中，我们提出了一种基于强化学习的方法，用于自动调整任何（无差异的）模拟器的参数，从而控制合成数据的分布，以使在该数据上训练的模型的准确性最大化。 与手工制作这些模拟参数或只调整部分可用参数的现有技术相比，我们的方法完全控制模拟器，其实际的基本目标是最大限度地提高准确性，而不是模仿真实的数据分布或随机产生大量的数据。我们发现，我们的方法(i)在受控实验中迅速收敛到最佳模拟参数，(ii)在实际计算机视觉应用中确实可以为图像渲染模拟器发现良好的参数集。
条件密度估计（CDE）旨在从数据中学习完整的条件概率密度。虽然表达能力很强，但基于神经网络的CDE模型在用最大似然目标训练时可能会出现严重的过拟合。 为了解决这个问题，我们为CDE开发了一种与模型无关的噪声正则化方法，该方法在训练过程中向数据添加随机扰动。在我们的实验中，噪声正则化在七个数据集和三个CDE模型中明显且持续地优于其他正则化方法。
无监督的表征学习有希望利用大量可用的无标签数据来学习一般的表征。一个有希望的无监督学习技术是变异自动编码器（VAEs）的框架。 我们从中层表征发现工作中得到启发，提出了PatchVAE，它在斑块层面上对图像进行推理。我们的主要贡献是在VAE框架中的瓶颈表述，鼓励中层风格的表征。我们的实验证明，与虚无的VAE学到的表征相比，用我们的方法学到的表征在识别任务中表现得更好。
梯度消失和爆炸是训练深度神经网络的两个主要障碍，特别是在捕捉递归神经网络（RNN）的长距离依赖方面。在本文中，我们提出了一个RNN过渡矩阵的有效参数化，使我们能够稳定其训练中出现的梯度。 具体来说，我们通过奇异值分解（SVD）对过渡矩阵进行参数化，这使我们能够明确地跟踪和控制其奇异值。我们通过使用数值线性代数中常见的工具，即表示SVD中出现的正交矩阵的Householder反射器来实现效率。 通过明确控制奇异值，我们提出的svdRNN方法允许我们轻松解决爆炸梯度问题，并且我们观察到它在很大程度上解决了梯度消失问题。我们注意到SVD参数化可以用于任何矩形权重矩阵，因此它可以很容易地扩展到任何深度神经网络，如多层感知器。 理论上，我们证明了我们的参数化并没有失去任何表达能力，并展示了它如何潜在地使优化过程变得更容易。我们广泛的实验结果也证明了所提出的框架收敛得更快，并具有良好的通用性，特别是当深度较大时。
最近的图像风格转移方法通过输入内容和风格图像实现了任意风格化。为了将任意图像的风格转移到内容图像上，这些方法使用了带有最低尺度特征变换器的前馈网络或带有相应尺度特征变换器的级联网络。然而，他们的方法既没有考虑其单尺度特征变换器中的多尺度风格，也没有考虑级联网络中变换的特征统计之间的依赖性。 首先，我们的方法将内容图像的多尺度特征图转化为目标风格图像的特征图，同时考虑每个单一尺度特征图的通道间相关性和多尺度特征图之间的尺度间相关性。 其次，每个转换后的特征图都被插入到使用跳过连接的相应比例的解码层中。最后，跳过连接的多比例特征图通过我们训练的解码网络被解码成风格化的图像。
除了理解正在讨论的内容之外，人类的交流还需要意识到某人的感受。对话代理的一个挑战是识别对话伙伴的感受并作出相应的回答，这是一种关键的交流技能，对人类来说是微不足道的。这一领域的研究由于缺乏合适的公开可用的情感和对话的数据集而变得困难。 我们的实验表明，与仅仅在大规模互联网对话数据上训练的模型相比，使用我们的数据集的对话模型被人类评价者认为是更有同情心的，同时在其他指标上也有改善（例如，感知到的反应的相关性，BLEU分数）。我们还提出了几种方法的经验比较，通过利用现有的模型或数据集来提高一个特定模型的性能，而不需要对整个模型进行长时间的重新训练。
格兰杰因果关系是分析大规模网络中相互作用的一个广泛使用的标准。由于大多数物理相互作用本身是非线性的，我们考虑从非线性相互作用的随机过程的时间序列测量中推断其成对的格兰杰因果关系的存在问题。 我们提出的方法依赖于使用基于统计递归单元（SRU）的分量明确的时间序列预测模型对测量中的嵌入式非线性进行建模。我们提出了一个案例，即格兰杰因果关系的网络拓扑结构可以直接从为预测过程的时间序列测量而训练的SRU网络的内部参数的结构化稀疏估计中推断出来。 我们提出了一种SRU的变体，称为经济型SRU，根据设计，它的可训练参数要少得多，因此不容易出现过拟合。经济型SRU以随机投影的形式计算其高维隐藏状态的低维草图，以生成其循环处理的反馈。 此外，经济-SRU的内部权重参数以分组的方式被战略性地规范化，以促进拟议的网络提取有意义的预测特征，这些特征具有高度的时间定位，以模仿现实世界的因果事件。进行了广泛的实验，证明拟议的基于经济-SRU的时间序列预测模型优于以前考虑的MLP、LSTM和基于注意力的CNN时间序列模型，以推断格兰杰因果关系。
图卷积网络（GCNs）是强大的图结构数据的深度神经网络。然而，GCN从它们的邻居递归地计算节点的表示，使得接受域的大小随着层数的增加而呈指数增长。 以前通过对邻居进行子采样来减少感受野大小的尝试没有任何收敛保证，他们的每个节点的感受野大小仍然是数百个。在本文中，我们开发了一个预处理策略和两个基于控制变量的算法来进一步减少感受野大小。 我们的算法保证收敛到GCN的局部最优，无论邻居采样大小如何。实证结果表明，我们的算法与精确算法的每历时收敛速度相似，即使每个节点只使用两个邻居。我们的算法在Reddit数据集上的时间消耗只有以前邻居采样算法的五分之一。
低位宽的整数权重和激活对高效推理非常重要，特别是在较低的功耗方面。我们建议应用蒙特卡洛方法和重要性抽样对预训练的神经网络进行稀疏化和量化，而不需要任何再训练。我们获得了稀疏的、低位宽的整数表示，接近全精度权重和激活。 我们的方法被称为蒙特卡洛量化（MCQ），在时间和空间上都是线性的，而产生的量化稀疏网络与原来的全精度网络相比，显示出最小的精度损失。我们的方法在各种具有挑战性的任务上的表现或取得的结果与那些需要额外训练的方法相竞争。
我们提出了信息最大化自动编码器（IMAE），这是一种在无监督环境下同时学习连续和离散表示的信息理论方法。与变异自动编码器框架不同，IMAE从一个随机编码器开始，寻求将每个输入数据映射到一个混合的离散和连续表示，目标是最大化数据和其表示之间的相互信息。    我们表明，所提出的目标在理论上是有效的，并提供了一个原则性的框架来理解关于每个表征因素的信息量、表征的分离和解码质量的权衡。
神经网络的学习规则必然包括某种形式的正则化。大多数正则化技术是在参数空间中概念化和实施的。然而，也有可能在函数空间中进行正则化。这里，我们建议在$L^2$希尔伯特空间中测量网络，并测试一种学习规则，使网络在每次更新时能在$L^2$空间中的距离正则化。 这种方法受到梯度下降在参数空间中缓慢移动的启发，也受到自然梯度的启发，自然梯度可以从函数变化时的正则化项中得到。因此，所产生的学习规则，我们称之为希尔伯特约束梯度（HCGD），与自然梯度密切相关，但在函数空间中正则化一个不同的、更可计算的尺度。
随机梯度下降（SGD）可以追溯到20世纪50年代，是执行随机优化的最流行和有效的方法之一。最近，SGD的研究在机器学习中重新出现，用于优化凸损失函数和训练非凸深度神经网络。最近，Chen等人（2018）提出使用一致梯度估计器作为一种经济的替代方案。在经验成功的鼓励下，我们在一般情况下表明，一致估计器导致与无偏估计器相同的收敛行为。 我们的分析涵盖了强凸、凸和非凸目标。我们通过对合成和真实世界数据的说明性实验来验证结果。这项工作开辟了几个新的研究方向，包括开发更有效的SGD更新与一致估计器和设计大规模图形的有效训练算法。
我们考虑了（非贝叶斯）深度神经分类背景下的不确定性估计问题。在这种情况下，所有已知的方法都是基于从训练好的网络中提取不确定性信号，以解决手头的分类问题。我们证明，这种技术往往会对预测应该是高度自信的实例引入有偏见的估计。我们认为，这种缺陷是类似SGD优化器的训练动态的一个工件，它有一些类似过拟合的特性。 基于这一观察，我们开发了一种不确定性估计算法，该算法利用训练过的模型的早期快照，在其估计值被抖动之前（并且在其准备用于实际分类之前），有选择地估计高度自信的点的不确定性。
现有的公共人脸图像数据集对高加索人脸有强烈的偏见，而其他种族（如拉丁裔）的代表性则明显不足。我们定义了7个种族组：白人、黑人、印度人、东亚人、东南亚人、中东人和拉美人。图像从YFCC-100M Flickr数据集中收集，并标注了种族、性别和年龄组。在现有的脸部属性数据集和新型图像数据集上进行评估，以衡量泛化性能。 我们发现，从我们的数据集训练出来的模型在新的数据集上的准确度要高得多，而且准确度在不同的种族和性别群体中是一致的。我们还比较了几个商业计算机视觉API，并报告了它们在性别、种族和年龄群体中的平衡准确度。
生成模型的巨大进步使得人工渲染的人脸、动物和自然界中的其他物体的质量接近于照片质量。尽管有这些进步，对视觉和图像的更高层次的理解并不来自于对一个物体的详尽建模，而是识别最能概括一个物体各个方面的更高层次的属性。  在这项工作中，我们试图通过建立矢量图形的连续生成模型来对字体的绘制过程进行建模。 这个模型的好处是为图像提供了一个尺度不变的表征，其潜在的表征可以被系统地操作和利用来进行风格传播。我们在一个大型的字体数据集上展示了这些结果，并强调了这样一个模型如何捕捉到这个数据集的统计依赖性和丰富性。我们设想我们的模型可以作为设计师的工具来促进字体设计。
我们能从大规模的神经活动记录中了解到什么是大脑皮层微循环的功能组织？ 为了获得神经元之间随时间变化的功能连接的明确和可解释的模型，并建立皮质信息流的动态，我们开发了 "动态神经关系推理"（dNRI）。我们研究了合成和真实世界的神经尖峰数据，并证明所开发的方法能够比现有基线更可靠地揭示神经元之间的动态关系。
DeePa是一个深度学习框架，它在所有可并行的维度上探索并行性，以加速卷积神经网络的训练过程。我们提出了一种基于消除的算法，为每一层找到最佳的并行性配置。我们的评估表明，与最先进的深度学习框架相比，DeePa实现了高达6.5倍的速度提升，并将数据传输减少了23倍。
我们可以用内核机器替代任何神经网络中的每个神经元，并获得一个由内核机器驱动的对应网络。新网络继承了原始网络的表达能力和架构，但以更直观的方式工作，因为每个节点都享有作为超平面的简单解释（在再现内核希尔伯特空间）。此外，以内核多层感知器为例，我们证明在分类中，可以为每个隐藏层描述一个使网络风险最小化的最佳表示。 这个结果消除了模型学习中的反向传播的需要，并且可以推广到任何前馈核网络。此外，与反向传播不同的是，反向传播将模型变成了黑盒子，最佳隐藏表示享有直观的几何解释，使深度核网络的学习动态变得简单易懂。
许多公平的概念可以表示为线性约束，由此产生的约束目标通常通过将问题转化为其拉格朗日对偶的加法线性惩罚来优化。在非凸的环境中，所产生的问题可能很难解决，因为拉格朗日不能保证有一个确定性的鞍点均衡。 在本文中，我们建议将线性罚则修改为二阶罚则，我们认为这将在非凸的大数据环境下产生更实用的训练程序。首先，使用二阶罚则可以用固定的罚则系数值训练被罚目标，从而避免了不稳定和潜在的与双人最小最大化游戏有关的收敛性。 其次，我们推导出一种在随机小批量设置中有效计算与二阶惩罚相关的梯度的方法。我们得到的算法在经验上表现良好，在一些标准的基准上学习了一个适当的公平分类器。
深度网络的训练方法主要是随机梯度下降的变种。 使用（近似的）二阶信息的技术很少被使用，因为在深度学习的背景下，这些方法有计算成本和噪音。 然而，在本文中，我们展示了前馈深度网络如何表现出低秩导数结构。 这种低秩结构使得使用二阶信息成为可能，而不需要近似，也不会产生比梯度下降法大得多的计算成本。 为了证明这种能力，我们在一个具有随机梯度下降的前馈深度网络和它的两个变种上实现了立方正则化（CR）。 在那里，我们使用CR在MNIST和CIFAR-10数据集上训练时，以每迭次为基础计算学习率。 事实证明，CR在摆脱目标函数的高原区域方面特别成功。 我们还发现，这种方法比其他一阶方法需要更少的特定问题信息（如最佳的初始学习率），以便表现良好。
最小化最大对抗性损失的最坏情况训练原则，也被称为对抗性训练（AT），已被证明是增强对抗性鲁棒性以对抗规范球约束的输入扰动的最先进方法。尽管如此，在对抗性攻防研究中，超越AT目的的最小最大化优化还没有被严格探索过。 特别是，给定一组风险源（域），从域集诱发的最大损失最小化可以被重新表述为一个不同于AT的一般最小最大化问题。这种一般表述的例子包括攻击模型集合，设计多输入或数据转换下的通用扰动，以及不同类型攻击模型上的通用AT。我们表明，这些问题可以在一个统一的、理论上有原则的最小最大化优化框架下解决。 我们还表明，从我们的方法中学到的自我调整的领域权重提供了一种手段来解释多个领域的攻防难度。大量的实验表明，我们的方法导致了比传统的平均策略更多的性能改进。
大多数深度学习模型依靠富有表现力的高维表征来实现分类等任务的良好性能。然而，这些表征的高维度使它们难以解释，并容易过度拟合。我们提出了一个简单、直观和可扩展的降维框架，考虑到了标准深度模型的软概率解释，用于分类。 当把我们的框架应用于可视化时，我们的表征比标准的可视化技术（如t-SNE）更准确地反映了类间距离。我们通过实验表明，我们的框架提高了零次学习中对未见类别的泛化性能。我们还为该方法提供了有限样本误差上界保证。
内在动机的目标探索算法使机器能够发现在复杂环境中产生多种效果的政策复制品。这些探索算法已被证明允许现实世界的机器人获得技能，如在高维连续状态和行动空间中使用工具。然而，到目前为止，他们假设自我生成的目标是在专门设计的特征空间中采样的，限制了他们的自主性。 这是一种发展性的2阶段方法：首先，在感知学习阶段，深度学习算法使用被动的原始传感器对世界变化的观察来学习相应的潜空间；然后在第二阶段通过在这个潜空间中对目标进行采样来进行目标探索。我们用一个模拟的机器人手臂与一个物体进行交互的实验，我们表明，使用这种学习表征的探索算法可以密切匹配，甚至有时可以提高使用工程化表征获得的性能。
深度学习方法的主要挑战之一是选择适当的训练策略。特别是，额外的步骤，如无监督的预训练，已被证明可以大大提高深度结构的性能。在这篇文章中，我们提出了一个额外的训练步骤，称为后训练，它只优化网络的最后一层。 我们表明，这个过程可以在核理论的背景下进行分析，第一层计算数据的嵌入，最后一层是一个统计模型，以解决基于这个嵌入的任务。这个步骤确保数据的嵌入，或表示，是以最佳方式用于考虑的任务。
自然语言处理（NLP）模型通常需要大量的参数用于单词嵌入，从而导致大量的存储或内存占用。将神经NLP模型部署到移动设备上需要在不明显牺牲性能的情况下压缩单词嵌入。 每个编码由多个离散数组成，如（3，2，1，8），其中每个分量的值被限制在一个固定的范围内。我们建议通过应用Gumbel-softmax技巧在端到端神经网络中直接学习离散编码。实验表明，在情感分析任务中压缩率达到98%，在机器翻译任务中达到94%~99%，没有性能损失。 在这两项任务中，所提出的方法可以通过略微降低压缩率来提高模型性能。与其他方法如字符级分割相比，所提出的方法与语言无关，不需要修改网络结构。
在部署机器学习系统时，检测异常输入是很重要的。在深度学习中使用更大、更复杂的输入，放大了区分异常和分布中的例子的难度。同时，多样化的图像和文本数据数量巨大。我们建议利用这些数据来改善深度异常检测，针对异常值的辅助数据集训练异常检测器，这种方法我们称之为异常值暴露（OE）。 我们还观察到，在CIFAR-10上训练的前沿生成模型可能会给SVHN图像分配比CIFAR-10图像更高的可能性；我们使用OE来缓解这个问题。我们还分析了Outlier Exposure的灵活性和稳健性，并确定了提高性能的辅助数据集的特征。
虽然生成神经网络可以学习将特定的输入数据集转化为特定的目标数据集，但它们需要有这样一组成对的输入/输出数据集。例如，为了骗过判别器，一个专门训练将黑发*男人的图像转化为金发*男人的生成对抗网络（GAN）在给定黑发*女人的图像作为输入时，需要改变性别相关的特征以及头发颜色。 这是有问题的，因为通常有可能获得**对（源，目标）分布，但有第二个源分布，而目标分布是未知的。计算上的挑战是，生成模型擅长在它们被训练的数据流形内生成。然而，在流形外生成新的样本或推断 "样本外 "是一个更难的问题，研究得不多。 为了解决这个问题，我们引入了一种叫做*神经元编辑*的技术，学习神经元如何对潜空间中的特定变换进行编码编辑。我们使用自动编码器将数据集内的变化分解为不同神经元的激活，并通过对这些神经元定义编辑变换来生成变换的数据。 通过在潜伏的训练有素的空间中进行转换，我们对数据进行相当复杂的非线性转换，并对神经元的激活进行简单得多的分布转移。我们的技术是通用的，可用于各种数据领域和应用。我们首先在图像转换上进行演示，然后转向我们的两个主要生物应用：去除代表不需要的噪声的批量伪影和对药物治疗效果进行建模以预测药物间的协同作用。
在本文中，我们建议通过使用神谕的奖励塑造的想法来结合模仿和强化学习。 我们研究了近乎最优的成本-去向神谕对规划范围的有效性，并证明成本-去向神谕缩短了学习者的规划范围，是其准确性的函数：一个全局最优的神谕可以将规划范围缩短到1，导致一个单步贪婪的马尔科夫决策过程，这更容易优化，而一个远离最优的神谕需要在更长的范围内规划以达到接近最优的性能。 因此，我们的新见解在模仿学习和强化学习之间架起了一座桥梁。在上述见解的激励下，我们提出了截断的HORizon策略搜索（THOR），这种方法主要是在神谕是次优的情况下，在有限的规划范围内搜索总重塑奖励最大化的策略。我们通过实验证明，与RL基线和IL基线相比，即使神谕是次优的，基于梯度的THOR的实现也能获得更优的性能。
最近，生成对抗网络（GANs）已经成为复杂的高维分布建模的流行选择。大多数现有的工作隐含地假设来自目标分布的干净样本很容易获得。 在本文中，我们考虑了目标分布的样本是由两个结构化分量的叠加给出的观察环境，并利用GANs来学习分量的结构。我们提出了一个新颖的框架，即demixing-GAN，它可以同时学习两个分量的分布。通过大量的数值实验，我们证明了所提出的框架可以从未知分布中产生干净的样本，这些样本可以进一步用于未见的测试图像的解混。
归一化流（NFs）是一类基于似然的生成模型，最近得到了普及。它们基于将简单的密度转化为数据密度的想法。我们试图更好地理解这类模型，以及它们与以前提出的生成建模和无监督表示学习技术的比较。为此，我们在变异自动编码器（VAEs）的框架内重新解释了NFs，并提出了一种新形式的VAE，它概括了归一化流。 使用我们的统一模型，我们在MNIST手写数字的初步实验中，系统地研究了流量、变异自动编码器和变异自动编码器之间的模型空间。实验阐明了这些模型中隐含的建模假设，并为该领域的未来研究提出了多个新方向。
我们研究了在强化学习中有效学习不同策略的方法，用于生成性结构预测问题：查询重构。在提出的框架中，一个代理由多个专门的子代理和一个元代理组成，元代理学习汇总子代理的答案以产生最终答案。 子代理是在训练数据的不连续分区上训练的，而元代理是在完整的训练集上训练的。我们的方法使学习更快，因为它是高度可并行的，并且比强大的基线，如在完整数据上训练的代理集合，有更好的泛化性能。我们对文档检索和问题回答的任务进行评估。 我们在文档检索和问题回答的任务上进行了评估。性能的提高似乎是由于改写策略的多样性增加了。这表明多代理、分层方法可能在这类结构化预测任务中发挥重要作用。
我们研究了连续行动强化学习问题，在这些问题中，关键是代理人只通过安全策略与环境互动，即~在训练期间和收敛时使代理人处于理想状态的策略。我们将这些问题表述为{/em constrained}马尔科夫决策过程（CMDP）。我们将这些问题表述为{em约束}马尔科夫决策过程（CMDPs），并提出了基于李亚普诺夫方法的安全策略优化算法来解决这些问题。我们的算法可以使用任何标准的策略梯度（PG）方法，如深度确定性策略梯度（DDPG）或近似策略优化（PPO），来训练神经网络策略，同时通过将策略参数或所选行动投射到由状态相关的线性化李亚普诺夫约束所引起的可行方案集上，保证每次策略更新都能接近约束满足。 与现有的约束性PG算法相比，我们的算法数据效率更高，因为它们能够利用政策内和政策外的数据。此外，我们的行动投射算法通常导致不太保守的政策更新，并允许自然地整合到端到端PG训练管道中。我们评估了我们的算法，并在几个模拟（MuJoCo）任务以及现实世界的机器人避障问题上与最先进的基线进行比较，证明了它们在平衡性能和约束满足方面的有效性。
众所周知，生成网络是很难评估的。最近关于生成模型的工作，特别是关于生成对抗网络的工作，产生了不同类别图像的漂亮样本。 本文提出了一种评估生成器的新方法。我们的方法是基于用真实和生成的混合样本来训练分类器。我们在一个标记的训练集上训练生成模型，然后我们用这个生成模型对新的数据点进行采样，并与原始训练数据混合。 这种真实数据和生成数据的混合物被用来训练一个分类器，然后在给定的标记测试数据集上进行测试。我们将这一结果与在混合有噪声的真实训练数据上训练的同一分类器的得分进行比较。通过计算来自两个分布（真实和生成）的样本的不同比例的分类器的准确性，我们能够估计生成器是否成功适合并能够概括数据集的分布。
我们提出了自动化科学新闻（ASJ），即从科学论文中制作新闻稿的过程，作为一项新的任务，可以作为神经抽象总结的新基准。ASJ是一项具有挑战性的任务，因为它需要将长的源文本总结为长的目标文本，同时还要将复杂的科学概念转述给普通观众理解。为此，我们为ASJ引入了一个专门的数据集，包含科学论文和来自科学日报的新闻稿。 虽然最先进的序列到序列（seq2seq）模型可以很容易地为ASJ生成令人信服的新闻稿，但这些新闻稿通常是非事实性的，并偏离了来源。为了解决这个问题，我们通过转移学习来改进seq2seq的生成，对新的目标进行联合训练：（i）来源的科学摘要和（ii）分割的新闻稿。 我们进一步设计了一个事实性的衡量标准，对我们的seq2seq模型下的新闻稿与科学论文的相关程度进行评分。我们的定量和定性评估显示，与强大的基线相比，有相当大的改进，这表明所提出的框架可以改进seq2seq的总结，超过ASJ。
人工智能代理行为的可解释性对于有效的人机交互至关重要。为此，人们对描述和生成代理的可解释性行为越来越感兴趣。保证代理生成可解释性行为的另一种方法是设计代理的环境，使不可解释的行为要么过于昂贵，要么对代理不可用。 在这篇立场文件中，我们对可解释行为和环境再设计的所有不同风味进行了考察。具体来说，我们专注于三种特定类型的可解释行为--可解释性、可读性和可预测性--并提出了一个环境设计问题的一般框架，可以通过实例化来实现这三种可解释行为中的每一种。我们还讨论了这个框架的具体实例如何对应于先前的环境设计工作，并为未来工作确定了令人兴奋的机会。
推理模型，用学习到的模型取代基于优化的推理程序，在推进贝叶斯深度学习方面起到了基础作用，最显著的例子是变异自动编码器（VAEs）。在本文中，我们提出了迭代推理模型，它通过重复编码梯度来学习如何优化变异下限。 我们的方法在某些条件下概括了VAEs，通过在迭代推理的背景下看待VAEs，我们对最近的几个经验发现提供了进一步的见解。我们展示了迭代推理模型的推理优化能力，探索了这些模型的独特方面，并表明它们在典型的基准数据集上比标准推理模型更出色。
在用梯度下降法训练的人工神经网络中，用于处理刺激的权重在后向传递过程中也被用来计算梯度。对于真实的大脑来说，梯度信息必须被分开传播，比如一组突触权重被用于处理，另一组被用于后向传递。 这就产生了所谓的生物学习模型的 "权重传递问题"，用于计算梯度的后向权重需要反映用于处理刺激的前向权重。这个权重传递问题被认为非常困难，以至于流行的生物学习建议假定后向权重是简单的随机的，如反馈排列算法。 然而，这样的随机权重对于大型网络来说似乎并不奏效。这里我们展示了在尖峰系统中引入的不连续性如何导致这一问题的解决。所产生的算法是计量经济学中用于因果推断的估计器的一个特例，即回归不连续性设计。 随着后向权重变得正确，这比在Fashion-MNIST和CIFAR-10等任务上的反馈对准提高了学习性能。我们的结果表明，在尖峰网络中一个简单的学习规则可以使神经元产生正确的后向连接，从而解决权重传输问题。
变量推理（VI）方法，特别是变量自动编码器（VAEs）指定了可扩展的生成模型，它享有与流形学习的直观联系------在许多默认的先验中，后验/似然对$q(z|x)$/$p(x|z)$可以被视为数据流形和潜在欧氏空间之间的近似同构（及其逆）。 然而，这些近似在训练中会变得退化。除非仔细选择主观先验，否则先验和数据分布的拓扑结构往往不匹配。 相反，扩散图（DM）可以自动地对数据拓扑进行推理，并与流形学习有严格的联系，但不容易扩展，也不能提供反向的同构性。在本文中，我们提出了一个用于识别数据和潜在分布之间不匹配的原则性措施和一个结合变异推理和扩散图的优势来学习同构生成模型的方法。 衡量标准，即textit{locally bi-Lipschitz属性}，是同构的充分条件，易于计算和解释。方法，即textit{variational diffusion autoencoder}（VDAE）。(VDAE)是一种新型的生成算法，它首先推断出数据分布的拓扑结构，然后在数据上建立扩散随机行走模型。为了实现VDAE的高效计算，我们使用随机版本的变异推理和流形学习优化。 我们证明了VDAEs的维度依赖性的近似理论结果，潜伏空间的局部各向同性抽样导致了重建流形上的随机行走。最后，我们在各种真实和合成数据集上证明了我们方法的效用，并表明它表现出优于其他生成模型的性能。
虽然深度学习和深度强化学习系统在图像分类、游戏和机器人控制等领域展示了令人印象深刻的结果，但数据效率仍然是一个重大挑战，特别是当这些算法从头开始学习单个任务时。多任务学习已经成为一种有前途的方法，在多个任务之间共享结构，以实现更有效的学习。 在梯度干扰导致优化挑战的启发下，我们开发了一种简单而通用的方法来避免来自不同任务的梯度之间的干扰，通过我们称之为 "梯度手术 "的技术来改变梯度。我们提出了一种梯度手术的形式，将一个任务的梯度投射到任何其他有冲突梯度的任务的法线上。 此外，它可以有效地与之前提出的多任务架构相结合，以一种不受模型影响的方式提高性能。
在本文中，我们建议将Metropolis-Hastings算法的接受率看作是学习从目标分布中取样的一个普遍目标--以一组样本或未归一化密度的形式给出。 为了揭示这种联系，我们推导出接受率的下限，并将其作为学习显性和隐性采样器的目标。下限的形式允许在目标分布因子化（即在数据点上）的情况下进行双重随机梯度优化。我们在神经网络的贝叶斯推理和图像的生成模型上实证了我们的方法。
本文提出了一种针对视频特征的自监督学习方法，与现有方法相比，该方法在下游任务（如视频分类、字幕和分割）上的性能得到了明显改善。我们的方法将文本序列的BERT模型扩展到实值特征向量序列的情况，用噪声对比估计（NCE）取代softmax损失。我们还展示了如何从视觉特征序列和来自ASR（自动语音识别）的单词序列中学习表示，并表明这种跨模式训练（如果可能）甚至有助于。
我们提出了一个通用的动态架构，它采用了针对具体问题的可分叉机制，以利用关于问题数据结构的离散逻辑信息。我们将我们的模型调整并应用于CLEVR视觉问题回答，产生了DDRprog架构；与以前的方法相比，我们的模型在一半的历时中实现了更高的准确性，而可学习的参数却少了五倍。 我们的模型使用一个联合预测和执行功能神经模块的递归控制器直接模拟潜在的问题逻辑；它明确地分叉子进程以处理逻辑分支。虽然FiLM和其他竞争模型是静态架构，监督较少，但我们认为包含程序标签能够学习更高层次的逻辑操作--我们的架构在需要计数和整数比较的问题上实现了特别高的性能。我们通过DDRstack进一步证明了我们方法的通用性--我们的方法在反向波兰符号表达式评估中的应用，其中包含的堆栈假设使我们的方法能够通用于长表达式，明显优于具有十倍可学习参数的LSTM。
我们提出了支持引导的对抗性模仿学习（SAIL），这是一个通用的模仿学习框架，它将专家政策的支持估计与对抗性模仿学习（AIL）算法家族统一起来。SAIL解决了AIL的两个重要挑战，包括隐性奖励偏差和潜在的训练不稳定性。 我们还表明，SAIL至少与标准AIL一样有效。在广泛的评估中，我们证明了所提出的方法有效地处理了奖励偏差，并在广泛的基准控制任务上取得了比其他基线方法更好的性能和训练稳定性。
我们考虑的是少量的链接预测任务，其目标是用少量的已知边缘来预测多个图中的缺失边缘。我们表明，目前的链接预测方法通常没有能力处理这个任务---因为它们不能有效地在多图环境中的图之间转移知识，也不能有效地从非常稀疏的数据中学习。 为了应对这一挑战，我们引入了一个新的基于梯度的元学习框架Meta-Graph，该框架利用高阶梯度和一个学习到的图特征函数，有条件地生成一个图神经网络初始化。使用一组新颖的、为数不多的链接预测基准，我们表明Meta-Graph不仅能够快速适应，而且最终收敛性更好，仅使用少量真实边缘样本就能有效学习。
生成式神经网络将一个标准的、可能的分布映射到一个复杂的高维分布，这代表了现实世界的数据集。然而，一个确定的输入分布以及神经网络的特定结构可能对捕捉高维目标空间的多样性造成限制。 为了解决这个问题，我们提出了一个训练框架，它可以贪婪地产生一系列生成式对抗网络，逐步捕捉目标空间的多样性。我们从理论和经验上表明，我们的训练算法收敛到理论上的最佳分布，即真实分布在网络分布空间凸壳上的投影。
生成式先验在解决逆向问题方面变得非常有效，包括去噪、绘画和从少数和嘈杂的测量中重建。在压缩传感的背景下，如果未知图像属于预训练生成式网络的范围，那么我们可以通过从现有的测量中估计潜在的紧凑的潜代码来恢复图像。 这些方法在保持潜伏码固定的情况下更新网络权重，以便从给定的测量值中重建目标图像。在本文中，我们对网络权重和潜伏码进行了优化，将未经训练的生成网络作为视频压缩感应问题的先验。 我们表明，通过对潜伏码的优化，我们可以额外获得保留视频帧结构相似性的简明表示。我们还对潜伏码应用了低秩约束，以便在更低维的潜伏空间中表示视频序列。我们的经验表明，与现有的方法相比，我们提出的方法提供了更好或相当的准确性和低计算复杂性。
基于量纲的修剪是修剪神经网络的最简单的方法之一。尽管它很简单，但基于量纲的修剪及其变体在修剪现代结构方面表现出了显著的性能。 基于观察到基于数量的修剪确实最小化了对应于单层的线性算子的Frobenius失真，我们开发了一种简单的修剪方法，即lookahead修剪，通过将单层优化扩展到多层优化。我们的实验结果表明，所提出的方法在包括VGG和ResNet在内的各种网络上一直优于数量修剪，特别是在高稀疏制度下。
最近的文献表明，通过采用一组判别器对生成对抗网络进行训练取得了可喜的成果，而不是传统的涉及一个生成器对单一对手的游戏。这些方法对一些简单的损失合并进行单目标优化，例如平均数。在这项工作中，我们通过将不同模型提供的损失同时最小化作为一个多目标优化问题来重新审视多判别器方法。 具体来说，我们在一些不同的数据集上评估了多重梯度下降和超容积最大化算法的性能。此外，我们认为以前提出的方法和超容积最大化都可以被看作是多重梯度下降的变体，其中更新方向的计算可以有效地完成。我们的结果表明，超容积最大化在样本质量和多样性以及计算成本之间提出了比以前的方法更好的折衷方案。
搜索空间的设计是神经结构搜索（NAS）算法的一个关键问题。我们提出了一个由原子块组成的细粒度的搜索空间，这个最小的搜索单元比最近的NAS算法中使用的要小得多。 此外，我们提出了一种资源感知的架构搜索算法，在训练过程中动态选择原子块。该算法通过动态网络收缩技术进一步加速。我们的方法在ImageNet的几个FLOPS配置下取得了最先进的性能，搜索成本可以忽略不计。我们在以下网站开放了我们的整个代码库：https://github.com/meijieru/AtomNAS。
我们介绍了Lyceum，一个用于机器人学习的高性能计算生态系统。  Lyceum建立在Julia编程语言和MuJoCo物理模拟器之上，结合了高级编程语言的易用性和本地C语言的性能。Lyceum与其他流行的抽象语言如OpenAI的Gym和Deep-Mind的dm-control相比，速度快10-20倍。 这大大减少了各种强化学习算法的训练时间；也快到足以支持物理模拟器的实时模型预测控制。  Lyceum有一个直接的API，并支持跨多个核心或机器的并行计算。代码库、教程和演示视频可以在以下网站找到：https://sites.google.com/view/lyceum-anon。
现有的领域适应（DA）方法并不适合实际的DA场景，因为它们依赖于源-目标标签集关系的知识（例如，封闭集、开放集或部分DA）。 此外，几乎所有先前的无监督DA工作都需要源和目标样本共存，甚至在部署过程中也是如此，这使得它们不适合增量、实时适应。最初，在采购阶段，我们的目标是为未来的无源部署配备模型，假设事先不知道即将到来的类别差距和领域转变。 为了实现这一目标，我们通过利用现有的源数据，在一个新的生成分类器框架中增强了模型拒绝源外分布样本的能力。随后，在部署阶段，目标是设计一个统一的适应算法，能够在广泛的类别差距中运行，而不需要访问先前看到的源样本。 为了实现这一目标，与复杂的对抗性训练机制的使用相反，我们通过利用一种新的实例级权重机制，即源相似度指标（SSM），定义了一个简单而有效的无源适应目标。一个全面的评估显示了所提出的学习框架的实用性，甚至比最先进的依赖源的方法有更好的DA性能。
人工智能中学习目标导向行为的长期挑战之一是建立一个可以解决多个任务的单一代理。最近在目标导向序列问题的多任务学习中取得的进展是基于蒸馏的学习，其中一个学生网络通过模仿专家网络的特定任务策略从多个特定任务专家网络中学习。 虽然这些方法为多任务学习问题提供了一个有希望的解决方案，但它们需要大型专家网络的监督，而这些专家网络需要大量的数据和计算时间来进行训练。在这项工作中，我们提出了一个高效的多任务学习框架，在一个在线设置中解决了多个目标导向的任务，而不需要专家监督。 我们的工作使用主动学习原则，通过对更难的任务比更容易的任务进行抽样来实现多任务学习。在我们的主动抽样框架下，我们提出了三种不同的模型：一种具有极具竞争力的多任务性能的自适应方法；一种基于UCB的元学习器，它将挑选下一个任务进行训练的问题视为一个多臂匪徒问题。 我们在Atari 2600领域的七个多任务实例上展示了结果：三个6任务实例，一个8任务实例，两个12任务实例和一个21任务实例。
众多的机器阅读理解（MRC）数据集往往涉及人工注释，需要巨大的人力，因此数据集的规模仍然明显小于可用于无监督学习的数据规模。最近，研究人员提出了一种从维基百科等大型语料库生成合成问答数据的模型。 这个模型被用来在使用原始的MRC数据集进行微调之前，生成用于训练MRC模型的合成数据。这种技术比其他一般的预训练技术如语言建模显示出更好的性能，因为生成的数据的特征与下游的MRC数据相似。然而，很难有高质量的合成数据与人类注释的MRC数据集相媲美。 为了解决这个问题，我们提出了含答案的句子生成（ASGen），这是一种新颖的预训练方法，用于生成合成数据，涉及两种先进的技术：（1）动态确定K答案；（2）在含答案的句子生成任务上预训练问题发生器。 我们通过与现有方法比较BLEU得分来评估我们方法的问题生成能力，并在合成数据训练后在下游MRC数据上微调MRC模型来测试我们的方法。实验结果表明，我们的方法优于现有的生成方法，并在一系列MRC数据集（如SQuAD-v1.1、SQuAD-v2.0、KorQuAD和QUASAR-T）上提高了最先进的MRC模型的性能，而无需对原始MRC模型进行任何架构修改。
深度神经网络（DNN）在目前的机器学习研究中占主导地位。由于大规模的GPU并行化，DNN训练不再是一个瓶颈，具有许多参数和高计算量的大型模型在常见的基准表中占主导地位。因此，如果DNN要在嵌入式设备上实现合适的性能，模型大小和推理时间都必须大大减少。我们提出了一种软量化方法来训练DNN，可以使用纯定点运算来评估。 通过利用位移机制，我们得出了所有重要组件的定点量化约束，包括批量归一化和ReLU.与浮点算术相比，定点计算大大减少了计算量，而低位表示则立即降低了内存成本。我们在常见的基准数据集上用不同的架构评估了我们的方法，并与最近的量化方法进行了比较。
	我们提出了一种新的方法和新的架构，称为WSNet，用于学习紧凑和高效的深度神经网络。现有的方法是独立学习完整的模型参数，然后通过模型修剪或滤波器因子化等临时处理来压缩它们。另外，WSNet建议通过从一个紧凑的可学习参数集中采样来学习模型参数，这在整个学习过程中自然地执行{参数共享}。 通过采用这种方法，我们可以更有效地学习更小的网络，与具有相同数量卷积滤波器的基线网络相比，具有竞争力的性能。具体来说，我们考虑学习紧凑和高效的一维卷积神经网络进行音频分类。 在多个音频分类数据集上进行的广泛实验验证了WSNet的有效性。结合权重量化，所得到的模型比成熟的基线小到textbf{180$/times$}，理论上快到textbf{16$/times$}，而没有明显的性能下降。
最近的对抗性机器学习工作开始关注自主驾驶中的视觉感知，并研究了物体检测模型的对抗性例子（AE）。然而，在这种视觉感知管道中，检测到的物体也必须被追踪，在一个被称为多物体追踪（MOT）的过程中，建立周围障碍物的移动轨迹。 由于MOT被设计成对物体检测中的错误具有鲁棒性，它对现有的盲目针对异议检测的攻击技术提出了普遍的挑战：我们发现，要想真正影响跟踪结果，需要超过98%的成功率，这是现有攻击技术无法满足的要求。 在本文中，我们首次研究了针对自动驾驶中完整的视觉感知管道的对抗性机器学习攻击，并发现了一种新的攻击技术--跟踪器劫持，它可以有效地愚弄使用物体检测上的AE的MOT。 使用我们的技术，只要一帧成功的AE就可以将现有的物体移入或移出自动驾驶汽车的车头，从而造成潜在的安全隐患。我们使用伯克利深度驾驶数据集进行评估，发现平均在3帧被攻击时，我们的攻击可以有近100%的成功率，而盲目针对物体检测的攻击最多只有25%。
自我监督学习（Self-supervised learning，SlfSL），旨在通过巧妙设计的借口任务学习特征表征，而不需要人类注释，在过去几年中取得了引人注目的进展。最近，SlfSL也被认为是半监督学习（SemSL）的一个有前途的解决方案，因为它提供了一个新的范式来利用未标记的数据。 我们的见解是，SemSL中的预测目标可以被建模为SlfSL目标的预测器中的潜在因子。对潜在因子进行边际化处理，自然会得出一个新的表述，将这两个学习过程的预测目标结合起来。 通过通过一个简单而有效的SlfSL方法--旋转角度预测来实现这个框架，我们创建了一个新的SemSL方法，称为条件旋转角度预测（CRAP）。具体来说，CRAP的特点是采用一个模块来预测图像旋转角度\textbf{以候选图像类别为条件}。 通过实验评估，我们发现CRAP比其他现有的结合SlfSL和SemSL的方式取得了更高的性能。此外，提出的SemSL框架具有高度的可扩展性。通过用简单的SemSL技术和对旋转角度预测任务的修改来增强CRAP，我们的方法已经达到了最先进的SemSL性能。
排名是机器学习和信息检索中的一项核心任务。在这项任务中，向用户展示一个整体上具有吸引力的项目板块尤为重要。这反过来又需要考虑到项目之间的相互作用，因为从直觉上讲，将一个项目放在板块上会影响到哪些其他项目应该与它一起被选择的决定。在这项工作中，我们提出了一个名为seq2slate的序列到序列的排名模型。 在每一步，该模型预测下一个要放在石板上的项目，考虑到已经选择的项目。该模型的循环性质允许以一种灵活和可扩展的方式直接捕捉项目之间的复杂依赖关系。
虽然基于动量的方法，结合随机梯度下降法，在训练机器学习模型时被广泛使用，但对这种方法的泛化误差几乎没有理论上的理解。在实践中，动量参数的选择往往是启发式的，没有什么理论指导。 在这项工作中，我们利用算法稳定性的框架，在温和的技术假设下，为强凸损失函数类提供了泛化误差的上界。我们的界线随着训练集的大小反过来衰减到零，并且随着动量参数的增加而增加。
我们提出了一种基于模型的模仿学习方法，该方法可以只从专家的状态轨迹中学习特定环境的最优行动。 随后，我们通过监督学习从专家的状态轨迹中学习最佳行动，同时通过建模的环境动态反向传播误差梯度。实验评估表明，我们提出的方法成功地实现了与基于（状态，行动）轨迹的传统模仿学习方法类似的性能，即使在没有行动信息的情况下，与传统的无模型强化学习方法相比，我们的方法也能从专家代理的视频演示中学习行动，并能以较少的迭代次数达到预期性能。
最近的研究提出了彩票假说，表明对于一个深度神经网络来说，存在着可训练的子网络，在相应的训练步骤下，其性能与原始模型相同或更好。虽然这一发现很有见地，但找到合适的子网络需要反复的训练和修剪。 所产生的高成本限制了彩票假说的应用。我们表明存在一个上述子网络的子集，在训练过程中收敛速度明显加快，从而可以缓解成本问题。我们进行了广泛的实验，表明在超参数的限制性设置下（例如，精心选择的学习率、修剪率和模型容量），这种子网络在各种模型结构中持续存在。 作为我们研究结果的一个实际应用，我们证明这种子网络可以帮助减少对抗性训练的总时间，这是提高鲁棒性的一个标准方法，在CIFAR-10上可以达到最先进的鲁棒性，最多可减少49%。
分解表征，即较高层次的数据生成因素反映在不相干的潜在维度上，提供了一些好处，如容易推导出不变的表征，可转移到其他任务，可解释性等。我们考虑从大量未标记的观察数据中无监督地学习分解表征的问题，并提出了一种基于变异推理的方法来推断分解的潜在因素。 我们对观察到的数据的近似后验的期望值引入了一个正则，以鼓励解缠。我们还提出了一个新的解缠指标，它与解码器输出中观察到的定性解缠更好地保持一致。
在多代理系统（MASs）中，每个代理做出单独的决定，但他们都对系统的进化做出了全面的贡献。在MASs中的学习是困难的，因为每个代理的行动选择必须在其他共同学习的代理的存在下进行。此外，环境随机性和不确定性随着代理数量的增加而呈指数增长。 在本文中，我们提出了一种新的网络架构，名为行动语义网络（ASN），它明确表示了代理人之间的这种行动语义。ASN根据不同行动之间的行动语义，使用神经网络来描述不同行动对其他代理人的影响。ASN可以很容易地与现有的深度强化学习（DRL）算法相结合，以提高其性能。在《星际争霸II》微管理和神经MMO的实验结果表明，与几种网络架构相比，ASN显著提高了最先进DRL方法的性能。
尖峰神经网络正在被研究，它既是生物上可信的神经计算模型，也是一种潜在的更有效的神经网络类型。虽然卷积尖峰神经网络已经被证明可以达到接近最先进的性能，但到目前为止，只有一个解决方案被提出来转换门控递归神经网络。 在涉及序列识别或生成的问题领域中，门控记忆单元网络形式的递归神经网络一直是最先进的解决方案的核心。这里，我们设计了一个模拟门控LSTM单元，其神经元可以被替换为高效的随机尖峰神经元。 对于这样的神经元，我们对有效的激活函数进行了近似，它类似于一个sigmoid。我们展示了具有这种激活函数的模拟神经元如何被用来创建一个模拟LSTM单元；然后这些单元的网络可以用标准的反向传播法进行训练。 我们在Hochreiter & Schmidhuber (1997)的原始序列预测任务的有噪声和无噪声版本上训练这些LSTM网络，也在经典工作记忆强化学习任务的有噪声和无噪声版本上训练这些LSTM网络，即T-Maze.将模拟神经元替换成相应的自适应尖峰神经元，然后我们表明几乎所有产生的尖峰神经网络等价物都能正确计算原始任务。
CNN在识别视频中的人类动作方面取得了广泛的成功，尽管有很大的计算成本。在长距离动作的情况下，这种成本明显更高，一个视频平均可以跨越几分钟。 我们提出了VideoEpitoma，一个由两个模块组成的神经网络架构：一个时间戳选择器和一个视频分类器。给定一个由数千个时间段组成的长距离视频，选择器学会只选择视频中少数但最具代表性的时间段。这个选择器驻扎在一个轻量级的CNN（如MobileNet）之上，使用一个新颖的门控模块来做出二元决定：考虑或丢弃一个视频时间段。 这个决定是以时间段级的特征和视频级的共识为条件的。一个重量级的CNN模型，如I3D，将选定的帧作为输入并进行视频分类。使用现成的视频分类器，VideoEpitoma将计算量减少了50％，而不影响准确性。 此外，我们表明，如果进行端对端训练，选择器会学会做出更好的选择，使分类器受益，尽管选择器和分类器位于两个不同的CNN上。特别是，我们用不到一半的计算量就达到了I3D的准确性。
我们提出的一个问题是，除了利用文本来获得更多的通用词汇特征（即超越词汇嵌入）外，是否可以利用丰富的未标记文本来改进句法分析器。 由于精确推理是难以实现的，我们引入了一个可微调的松弛，以获得近似的样本并计算与解析器参数有关的梯度。我们的方法（可微调的扰动和解析）依赖于对随机扰动的边缘得分的可微调动态编程。
DeConvNet、Guided BackProp、LRP是为了更好地理解深层神经网络而发明的。我们表明，这些方法并不能为线性模型产生理论上正确的解释。然而，它们被用于具有数百万参数的多层网络。 我们认为，神经网络的解释方法应该在简单的极限，即线性模型中可靠地工作。基于我们对线性模型的分析，我们提出了一个概括，产生了两种解释技术（PatternNet和PatternAttribution），它们在理论上对线性模型是合理的，并对深度网络产生改进的解释。
图形神经网络在表示和分析不同的图形结构数据上显示出了很好的效果，如社会、引文和蛋白质相互作用网络。现有的方法普遍存在过度平滑的问题，无论政策是基于边缘还是基于节点的邻域聚合。 大多数方法还专注于固定图的反演场景，导致对未见过的图的泛化性能较差。为了解决这些问题，我们提出了一个新的图神经网络模型，考虑了基于边缘的邻域关系和基于节点的实体特征，即通过随机行走的图实体与步骤混合（GESM）。 GESM采用了通过随机行走的各种步骤的混合物来缓解过度平滑问题，并注意明确使用节点信息。这两种机制允许考虑实体和关系的属性的加权邻域聚合。通过密集的实验，我们表明，拟议的GESM在四个基准图数据集上实现了最先进或可比的性能，包括过渡性和归纳性学习任务。
Basis pursuit是一种压缩传感优化，其中l1-norm在模型误差约束下最小化。在这里，我们使用深度神经网络先验，而不是l1-regularization。使用已知的噪声统计，我们联合学习先验和重建图像，而不访问地面真实数据。 在推理时，我们固定权重并通过网络传递测量结果。我们比较了无监督和有监督（即有地面实况）方法的重建性能。我们假设，当地面实况数据不可用时，这种技术可用于学习重建，例如在高分辨率动态MRI中。
深度学习方法通常需要大量的标记数据来归纳。然而，人类只能通过几个样本来学习一个新的概念。人类的高认知能力之一是同时学习几个概念。在本文中，我们解决了通过只看到每个类别的几个样本来对多个物体进行分类的任务。 据作者所知，目前还没有专门为少量多类分类设计的数据集。我们设计了一个多物体少量多类分类的任务，并为这个任务设计了一个易于创建可控数据集的环境。我们用一种原型网络的扩展方法证明了所提出的数据集是合理的。
我们提出了一种定义序列损失函数的新方法，通过使用二级编码器-解码器作为损失函数来训练总结器，缓解了序列输出的词级训练的缺陷。该技术基于这样的直觉：如果一个总结是好的，它应该包含来自原文的最基本的信息，因此本身应该是一个好的输入序列，代替原文，从中可以生成一个总结。 我们提出了实验结果，将这一额外的损失函数应用于新闻摘要数据集上的一般抽象摘要器。结果是ROUGE度量的改进和人类评价的特别大的改进，表明增强的性能可以与专门的最先进的模型竞争。
现有的无监督视频到视频的翻译方法不能产生符合帧的现实、保留语义信息和视频级别一致的翻译视频。我们的模型分解了风格和内容，使用专门的编码器-解码器结构，并通过双向循环神经网络（RNN）单元传播帧间信息。风格-内容分解机制使我们能够实现长期风格一致的视频翻译结果，并为我们提供了一个灵活翻译模式的良好接口。 此外，通过改变输入帧和纳入我们翻译的风格代码，我们提出了一个视频插值损失，它捕捉了序列中的时间信息，以自我监督的方式训练我们的构件。我们的模型可以以多模态的方式产生照片般真实的、时空一致的翻译视频。主观和客观的实验结果验证了我们的模型比现有方法的优越性。
提供人工智能规划系统的透明度对其在实际应用中的成功至关重要。为了创建一个透明的系统，用户必须能够查询它对其输出的解释。我们认为，这一点的关键基本原则是在规划模型中使用因果关系，而论证框架提供了这种因果关系的直观表示。在本文中，我们讨论了论证如何帮助提取计划和模型中的因果关系，以及他们如何从中创建解释。
点云，作为拉格朗日的一种表示形式，允许在大量的计算学科中进行强大而灵活的应用。我们提出了一种新颖的深度学习方法，为随时间变化的点云学习稳定的、时间上一致的特征空间。 我们发现这些方法存在一系列固有的问题：在不了解时间维度的情况下，推断出的解决方案会表现出强烈的闪烁，而抑制这种闪烁的简单解决方案会导致不良的局部最小值，表现为晕轮结构。我们提出了一个新的时间损失函数，考虑到点位置的更高的时间导数，并鼓励混合，即。我们将这些技术结合到超分辨率方法中，并采用截断方法来灵活地调整生成位置的大小。我们表明我们的方法适用于来自不同来源的大型变形点集，以证明我们方法的灵活性。
我们研究了在黑箱环境中生成对抗性例子的问题，其中只有对模型的损失或acle访问是可用的。我们引入了一个框架，从概念上统一了许多关于黑箱攻击的现有工作，并证明目前最先进的方法在自然意义上是最优的。尽管有这种最优性，我们显示了如何通过将一个新元素引入问题来改进黑箱攻击：梯度优先权。 我们给出了一个基于匪徒优化的算法，使我们能够无缝地整合任何这样的先验因素，并且我们明确地识别和整合了两个例子。由此产生的方法使用的查询次数比目前最先进的方法少2到4倍，失败次数也少2到5倍。复制我们工作的代码可在https://git.io/fAjOJ。
收集高质量、大规模的数据集通常需要大量的资源。本工作的目的是通过对未标记数据的自我监督任务的多任务学习，提高在音频数据上操作的大型神经网络的标记效率。为此，我们训练了一个基于WaveNet的端到端音频特征提取器，并将其输入简单但通用的特定任务神经网络。 我们证明，在标注训练数据有限的情况下，通过同时训练这些额外的自我监督任务，可以显著提高监督分类任务的性能。我们表明，当与多达三个不同的自我监督任务联合训练时，可以将不同声音事件分类任务的性能提高近6/%。
人类的信息需求本质上是多模态的，能够最大限度地利用所处的环境。我们引入了一个数据集，用于从烹饪领域的图像中生成连续的程序性（如何做）文本。(1) 解码器中的脚手架结构(SSiD) (2) 损失中的脚手架结构(SSiL)。这些模型在经验和人类评估中显示出了改进。我们表现最好的模型（SSiL）达到了0.31的METEOR分数，比基线模型改进了0.6。 我们还对生成的基础食谱进行了人类评估，结果显示，61%的人认为我们提出的（SSiL）模型在整体食谱方面优于基线模型，72.5%的人在连贯性和结构方面更喜欢我们的模型。我们还讨论了对输出的分析，强调了未来方向的关键重要NLP问题。
隐式模型允许生成样本，但不允许对概率进行点评估，在机器学习处理的实际问题中无处不在，也是目前研究的热点。一些例子包括广泛用于工程和科学研究的数据模拟器、用于图像合成的生成对抗网络（GANs），以及依靠隐式分布的热门近似推理技术。 大多数现有的学习隐式模型的方法都依赖于近似难以处理的分布或基于梯度的优化目标，这很容易产生不准确的更新，从而产生糟糕的模型。本文通过提出emph{Stein梯度估计器}，直接估计隐式定义的分布的得分函数，从而减轻了对这种近似的需要。所提出的估计器的功效由实例证明，包括近似推理的元学习和熵正则GANs，提供改进的样本多样性。
噪声注入是数据增强的一个基本工具，但目前还没有广泛接受的程序将其纳入学习框架。 以不同密度函数分布的噪声模型通过结构相似性（SSIM）度量被赋予共同的幅度水平，以创造一个适当的比较基础。基本结果符合机器学习中最常见的概念，同时也引入了一些新的启发式方法和噪声注入建议。
最先进的无监督领域适应（UDA）方法通过最小化源域和目标域之间的特征分布差异来学习可转移的特征。与这些没有对特征分布进行明确建模的方法不同，在本文中，我们探索了UDA的明确特征分布建模。 特别是，我们提出了分布匹配原型网络（DMPN），将每个领域的深层特征建模为高斯混合分布。在DMPN中，我们提出了两个新的具有概率解释的领域差异损失。 第一个是最小化源数据和目标数据的相应高斯分量均值之间的距离。第二个是最小化从源特征分布生成目标特征的伪负对数似然。为了学习鉴别性和领域不变的特征，DMPN通过最小化标记源数据的分类损失和领域差异损失一起进行训练。 在两个UDA任务中进行了广泛的实验。我们的方法在数字图像传输任务中比最先进的方法产生了很大的优势。更值得注意的是，DMPN在VisDA 2017数据集上获得了81.4%的平均精度。超参数敏感性分析表明，我们的方法在超参数变化方面是稳健的。
有效地学习解决复杂环境中的任务是强化学习（RL）代理的一个关键挑战。我们建议使用任务无关的世界图来分解复杂的环境，这种抽象的世界图使代理人能够集中探索环境的一个子空间，从而加速学习。 我们的框架有两个学习阶段。1）通过训练轨迹数据上的二元递归变异自动编码器（VAE）来识别世界图的节点和边缘；2）一个分层的RL框架，利用从学习的世界图中获得的结构和连接性知识，使探索偏向与任务相关的航点和区域。 我们表明，我们的方法大大加速了一套具有挑战性的二维网格世界任务的RL：与基线相比，世界图集成在较简单的任务（如MultiGoal）上实现了双倍的奖励，并设法解决更具挑战性的任务（如Door-Key），而基线是失败的。
我们引入了属性签名的概念，这是一种程序和程序规范的表示方法，旨在供机器学习算法使用。给定一个具有输入类型τ_in和输出类型τ_out的函数，属性是一个类型的函数。(τ_in, τ_out) → Bool，它（非正式地）描述了所考虑的函数的一些简单属性。例如，如果τ_in和τ_out都是相同类型的列表，一个属性可能会问 "输入列表和输出列表的长度是否相同"。 最重要的是，我们可以 "猜测 "一个函数的属性签名，只需给定一组旨在指定该函数的输入/输出对。我们讨论了属性签名的几个潜在应用，并通过实验表明，它们可以用来改进一个基线合成器，使其在不到十分之一的时间内发出两倍的程序。
我们展示了如何修改现代强化学习方法来构建代理人的行为方式，即简单易懂、友好（从合作开始）、可挑衅（试图避免被利用）和宽容（试图回到相互合作）。 我们在理论上和实验上都表明，这样的代理可以在马尔可夫社会困境中保持合作。我们的构造除了修改自我游戏之外，不需要训练方法，因此，如果一个环境是这样的，在零和情况下可以构建良好的策略（例如，Atari），那么我们可以构建在这个环境中解决社会困境的代理。
重参数化技巧已经成为变分推理领域最有用的工具之一。然而，重参数化技巧是基于标准化变换的，它将这种方法的应用范围限制在具有可操作的反累积分布函数的分布上，或者可表达为这种分布的确定性变换。在本文中，我们通过允许一般的变换来概括重参数化技巧。 我们发现，所提出的模型是控制变量的一个特例，表明所提出的模型可以结合CV和广义重参数化的优点。基于所提出的梯度模型，我们提出了一个新的基于多项式的梯度估计器，在某些条件下比重参数化技巧具有更好的理论性能，并可以应用于更大类的变量分布。在合成和真实数据的研究中，我们表明，我们提出的梯度估计器比其他先进的方法具有明显低的梯度方差，从而使推理程序更快。
现代通信系统的故障诊断在传统上被认为是困难的，甚至对于纯粹的数据驱动的机器学习方法来说是不切实际的，因为它是一个由密集知识组成的人为系统。从故障档案中提取的一些标记的原始数据包流很难足以推断出基础协议的复杂逻辑。 为了将其固有的知识转移到目标领域，我们构建了一个有向信息流图，其节点是由两个生成器、三个判别器和一个分类器组成的神经网络组件，其每个前向路径代表一对对抗性优化目标，符合半监督和转移学习的要求。 多头网络可以用另一种方法进行训练，在每次迭代时，我们选择一个目标来更新上游路径上的权重，并将残差层层刷新到下游的所有输出。实际结果表明，它在对传输控制协议（TCP）流进行分类时可以达到相当的准确性，而不需要刻意的专家特征。该解决方案将操作工程师从理解和维护规则的大量工作中解脱出来，并提供了一个独立于特定协议的快速解决方案。
我们的工作解决了递归神经网络的两个重要问题。(1)它们被过度参数化，(2)递归权重矩阵条件不良。前者增加了学习的样本复杂性和训练时间，后者导致梯度消失和爆炸问题。 我们在七个标准数据集上的实验结果表明，与现有的循环模型相比，KRU可以将循环权重矩阵中的参数数量减少三个数量级，而不影响统计性能。这些结果特别表明，虽然有高维循环空间的优势，但模型的循环部分的容量可以大大减少。
本文研究了深度网络学习的表征对数据中语义不相关的变化过于敏感的不良现象。我们在经典的变异自动编码器（VAE）目标中找到了这种缺陷的原因，即证据下限（ELBO）。 我们表明，ELBO不能控制编码器在经验数据分布的支持范围之外的行为，VAE的这种行为会导致学习到的表征出现极端错误。为了解决这个问题，我们建议用规范来增加数据，强制执行表征对转换系列的不敏感性。 为了纳入这些规范，我们提出了一种基于选择机制的正则化方法，通过明确地扰动观察到的真实数据点来创建一个假的数据点。对于某些参数的选择，我们的表述自然会导致表示之间的熵正则化Wasserstein距离最小化。 我们在标准数据集上说明了我们的方法，并通过实验表明，通过完全以无监督的方式学习稳健的表征，无需参考特定的下游任务，也无需昂贵的有监督的对抗训练程序，就可以实现下游对抗准确性的显著提高。
我们提出了一种新的基于分数的方法，从观察数据中学习有向无环图（DAG）。我们调整了最近提出的连续约束优化公式，以允许使用神经网络的变量之间的非线性关系。这种扩展允许对复杂的相互作用进行建模，同时与其他贪婪的方法相比，其搜索更加全面。 除了将我们的方法与现有的连续优化方法进行比较外，我们还提供了与非线性贪婪搜索方法的缺失经验比较。在合成和真实世界的数据集上，这种新方法在大多数任务上优于当前的连续方法，同时在因果推理的重要指标上与现有的贪婪搜索方法具有竞争力。
我们研究了设计可证明的最优对抗性噪声算法的问题，这些算法在学习者聚集了来自多个分类器的决定的情况下会引起错误分类。鉴于最先进的模型对对抗性例子的脆弱性，最近在鲁棒机器学习领域的努力集中在使用集合分类器作为提高单个模型的鲁棒性的方式。 在本文中，我们设计了针对一组分类器的可证明的最优攻击。我们证明了这个问题如何被框定为在学习者和对手之间的双人零和游戏中寻找平衡策略，并因此说明了在对抗性攻击中需要随机化。 我们考虑的主要技术挑战是设计最佳响应信标，这些信标可以在乘法权重更新框架中实现，以寻找零和博弈中的均衡策略。我们为深度神经网络开发了一系列可扩展的噪声生成算法，并表明它在各种图像分类任务中的性能优于最先进的攻击。 虽然深度学习一般没有保证，但我们表明这是一个原则性很强的方法，因为它对线性分类器来说是可证明的最佳方法。主要的见解是决策空间的几何特征，它将设计最佳响应口令的问题减少为在一组凸多边形上最小化一个二次函数。
我们研究了这些博弈的一个子类，即马尔科夫势能博弈（MPG），当代理共享一些共同的资源时，这些博弈经常出现在经济和工程应用中。我们考虑MPG具有连续的状态-行动变量、耦合的约束和非凸的奖励。 以前的分析采用的是变分法，只适用于非常简单的情况（奖励凸、动态可逆、无耦合约束）；或者考虑确定性动态，提供开环（OL）分析，研究由预定的行动序列组成的策略，这对随机环境来说不是最优的。 我们提出了MPG的闭环（CL）分析，并考虑了取决于当前状态的参数化政策，以及代理人适应随机转换的情况。我们为随机游戏成为MPG提供了容易验证的充分和必要条件，甚至对于复杂的参数化函数（例如。这很有用，因为解决一个OCP--这是一个单目标问题--通常比解决形成游戏的原始耦合OCP--这是一个多目标控制问题--要简单得多。 这比以前对MPG的CL分析的标准方法有了相当大的改进，如果没有NE属于所选择的参数族，就没有近似的解决方案，而且这种方法只对简单的参数形式实用。我们用一个例子来说明理论上的贡献，将我们的方法应用于一个非合作的通信工程游戏。
我们做出了以下惊人的观察：在32x32的ImageNet上训练的完全卷积VAE模型可以很好地泛化，不仅可以泛化到64x64，而且可以泛化到更大的照片，而不需要改变模型。 我们利用这一特性，将完全卷积模型应用于无损压缩，展示了一种将基于VAE的 "Bits-Back with ANS "算法无损压缩到大型彩色照片的方法，并实现了对全尺寸ImageNet图像的压缩。我们发布了Craystack，这是一个开源库，可以方便地使用概率模型进行无损压缩的原型设计，以及我们所有压缩结果的完整实现。
    最先进的计算机视觉模型已被证明容易受到输入的小的对抗性扰动的影响。换句话说，数据分布中的大多数图像既被模型正确分类，又非常接近于视觉上类似的错误分类的图像。尽管有大量的研究兴趣，但对这种现象的原因仍然知之甚少，仍然没有解决。 我们假设这种反直觉的行为是数据流形的高维几何的自然发生的结果。作为探索这一假设的第一步，我们研究了一个简单的合成数据集，在两个同心的高维球体之间进行分类。 对于这个数据集，我们展示了测试错误量和平均距离最近的错误之间的基本权衡。特别是，我们证明了任何错误分类一个小的恒定部分的球体的模型将容易受到大小为$O(1/sqrt/{d})$的对抗性扰动。令人惊讶的是，当我们在这个数据集上训练几个不同的架构时，他们所有的错误集都自然接近这个理论界限。 作为理论的结果，神经网络对小的对抗性扰动的脆弱性是观察到的测试错误量的逻辑结果。我们希望我们对这个非常简单的案例的理论分析将为探索复杂的现实世界数据集的几何形状如何导致对抗性例子指明方向。
已经确定的是，跨越马尔科夫决策过程的可控子空间的各种行为可以通过奖励与其他政策相区别的政策来训练。然而，这种表述的一个局限性是很难在明确学习的有限行为集之外进行泛化，因为在后续任务中可能需要这样做。 为了做到这一点，我们引入了变异内在继任特征（VISR），这是一种新的算法，它可以学习可控制的特征，通过继任特征框架提供增强的泛化和快速的任务推断。我们在一个新的设置中对VISR进行了经验验证，在这个设置中，奖励只是在长期的无监督阶段后短暂地暴露出来。在12个游戏上实现了人类水平的性能，并击败了所有的基线，我们相信VISR代表了向代理从有限反馈快速学习迈出了一步。
预测结构化的输出，如语义分割，依赖于昂贵的每像素注释来学习强大的监督模型，如卷积神经网络。 为了避免劳动密集型的注释过程，我们开发了一种领域适应方法，以使源数据适应未标记的目标领域。为此，我们建议通过构建一个分离的空间，学习基于源领域中标签直方图的斑块的鉴别性特征表示。 有了这样的表征作为指导，我们再使用对抗性学习方案，将目标斑块中的特征表征推向源斑块中更接近的分布。此外，我们表明，我们的框架可以将全局对齐过程与提议的斑块级对齐相结合，并在语义分割上达到最先进的性能。在众多基准数据集上进行了广泛的消融研究和实验，这些数据集具有不同的设置，如合成到真实和跨城市的场景。
许多机器学习算法容易受到其输入的几乎难以察觉的扰动的影响。到目前为止，还不清楚对抗性扰动对现实世界机器学习应用的安全有多大的风险，因为大多数用于产生这种扰动的方法要么依赖于详细的模型信息（基于梯度的攻击），要么依赖于信心分数，如类概率（基于分数的攻击），这两者在大多数现实世界的场景中都无法获得。 在许多这样的情况下，人们目前需要退回到基于转移的攻击，这些攻击依赖于繁琐的替代模型，需要访问训练数据，并且可以被防御。这里我们强调仅依赖于最终模型决策的攻击的重要性。这种基于决策的攻击（1）适用于现实世界的黑箱模型，如自动驾驶汽车，（2）需要较少的知识，比基于转移的攻击更容易应用，（3）比梯度或基于分数的攻击对简单防御更强大。 以前这类攻击仅限于简单的模型或简单的数据集。这里我们介绍了边界攻击，这是一种基于决策的攻击，从一个大的对抗性扰动开始，然后寻求减少扰动，同时保持对抗性。该攻击在概念上很简单，几乎不需要超参数调整，不依赖替代模型，在标准计算机视觉任务（如ImageNet）中与基于梯度的最佳攻击具有竞争力。 我们在Clarifai.com的两个黑盒算法上应用了该攻击。特别是边界攻击和一般的基于决策的攻击类别，为研究机器学习模型的鲁棒性开辟了新的途径，并提出了关于部署的机器学习系统的安全性的新问题。该攻击的实现可作为Foolbox（https://github.com/bethgelab/foolbox）的一部分。
我们证明有可能在用户级别的差异性隐私保证下训练大型的递归语言模型，而在预测准确性方面只需付出可忽略不计的代价。 我们的工作建立在最近在用户分区数据上训练深度网络和随机梯度下降的隐私核算方面的进展。特别是，我们将用户级隐私保护添加到联合平均算法中，该算法从用户级数据进行大步更新。 我们的工作表明，给定一个有足够多用户的数据集（即使是小型互联网规模的数据集也很容易满足这一要求），实现不同的隐私是以增加计算量为代价的，而不是像大多数先前的工作那样降低效用。我们发现，当在大型数据集上训练时，我们的私有LSTM语言模型在数量和质量上都与无噪声模型相似。
卷积神经网络（CNN）通常使用为特定模型预先确定的固定空间图像尺寸进行训练。虽然在特定尺寸的图像上进行了训练，但已经确定CNN可以通过调整中间特征图的大小，在测试时用于评估广泛的图像尺寸。在这项工作中，我们描述并评估了一种新型的混合尺寸训练机制，该机制在训练时混合了几种图像尺寸。我们证明，使用我们的方法训练的模型对图像尺寸的变化更有弹性，即使在小图像上也能很好地概括。 例如，在图像大小为160的情况下，我们使用ResNet50获得了76.43%的最高准确率，这与基线模型的准确率相当，而计算量减少了2倍。此外，对于测试时使用的特定图像大小，我们表明这种方法可以被利用来加速训练或最终的测试准确率。例如，我们能够用288空间大小的模型评估达到79.27%的准确率，相对于基线的改进为14%。
我们提出了一个简单的技术来鼓励生成性RNN提前计划。我们训练一个 "后向 "的递归网络，以反向顺序生成一个给定的序列，我们鼓励前向模型的状态来预测后向模型的同时状态。后向网络只在训练期间使用，在采样或推理期间不发挥作用。 我们假设，我们的方法通过隐含地迫使前向状态持有关于长期未来的信息（如包含在后向状态中的信息）来简化长期依赖关系的建模。我们通过经验表明，我们的方法在语音识别任务中取得了9%的相对改进，并在COCO字幕生成任务中取得了重大改进。
深度生成模型试图恢复观察到的数据产生的过程。它们可以被用来合成新的样本或随后提取表征。在图像领域的成功方法是由几个核心的归纳偏见驱动的。然而，考虑到人类以物体的方式构造视觉场景的组成方式的偏见经常被忽视。 我们在几个多物体图像数据集上评估了我们的方法，并发现生成器学会了识别和分离与不同物体相对应的表征层次的信息。
目前机器学习的文献认为，不结盟的、自利的代理不会学习使用突发的通信渠道。我们引入了一个新的发送者-接收者游戏来研究这个频谱的部分竞争性场景的突发通信，并对评估给予特别关注。我们发现，在部分竞争性场景中确实可以出现通信，而且我们发现有三件事与改善它有关。 第一，自私的沟通与合作成正比，它自然会出现在合作多于竞争的情况下。第二，通过使用LOLA（Foerster等人，2018），特别是在竞争更激烈的场景中，稳定性和性能得到改善。第三，离散协议比连续协议更适合于学习合作沟通。
深度神经网络（DNNs）通常有足够的能力通过蛮力来适应随机数据，即使是在传统的专注于特征几何的数据依赖性正则化的情况下。我们发现，其原因是强制的几何和标准的softmax交叉熵损失之间的不一致。 为了解决这个问题，我们提出了一个新的依赖数据的DNN正则化框架，即Geometrically-Regularized-Self-Validating neural Networks（GRSVNet）。在训练过程中，对一批特征强制实施的几何图形同时在另一批特征上使用与几何图形一致的验证损失进行验证。 我们研究了GRSVNet的一个特殊情况，即正交-低秩嵌入（OLE）-GRSVNet，它能够产生驻留在正交低秩子空间中的高分辨特征。数值实验表明，OLE-GRSVNet在真实数据上训练时优于传统正则化的DNN。 更重要的是，与传统的DNN不同，OLE-GRSVNet拒绝记忆随机数据或随机标签，这表明它只是通过减少基线DNN的记忆能力来学习内在模式。
端到端自动语音识别（ASR）通常将音频信号转录成字符序列，而其性能是通过测量单词错误率（WER）来评估的。然而，由于每个标签类的例子稀少，用单词级别的监督来训练可能更困难。 我们表明它提高了误码率，并通过实证分析每个模型组件的学习归纳偏好偏差，研究词级模型如何从字符级监督中受益。我们发现，通过添加字符级监督，MTL模型在识别更频繁的单词（词级模型偏好）和更短的单词（字符级模型偏好）之间进行插值。
对浮点向量进行离散化是现代索引方法的一个基本步骤。最先进的技术在训练数据上学习量化器的参数以获得最佳性能，从而使量化器适应数据。 在这项工作中，我们建议扭转这种模式，使数据适应量化器：我们训练一个神经网络，其最后几层形成一个固定的无参数量化器，如预先定义的球形点。作为一个代理目标，我们设计和训练一个神经网络，有利于球形潜在空间的统一性，同时在映射后保留邻域结构。 为此，我们提出了一个源自Kozachenko-Leonenko差分熵估计器的新的正则器，并将其与一个局部感知的三连环损失相结合。实验表明，我们的端到端方法优于大多数学习量化方法，并且在广泛采用的基准上与技术水平相竞争。此外，我们表明，没有量化步骤的训练在准确性上几乎没有差异，但产生了一个通用的催化器，可以应用于任何后续的量化技术。
时差（TD）学习是强化学习中政策评估的一种流行算法，但香草TD会受到固有的优化方差的影响。Korda和La（2015）提出了一种方差减少的TD（VRTD）算法，它将方差减少技术直接应用于具有马尔科夫样本的在线TD学习。 在这项工作中，我们首先指出了Korda和La（2015）对VRTD分析中的技术错误，然后对VRTD的非渐进收敛性及其方差降低性能进行了数学上的坚实分析。 我们表明VRTD保证以线性收敛率收敛到TD的固定点解的附近。此外，与vanilla TD相比，VRTD的方差误差（对于i.i.d.和Markovian采样）和偏差误差（对于Markovian采样）通过方差减少的批量大小显著减少。
我们通过考虑不同的领域可能需要不同的处理方式来达到有效识别的共同特征表示这一事实来解决无监督的领域适应问题。为此，我们引入了一个深度学习框架，每个领域经历不同的操作序列，允许一些可能更复杂的领域比其他领域经历更多的计算。 这与最先进的领域适应技术形成鲜明对比，后者强迫所有领域以相同的操作序列进行处理，甚至在使用参数不共享的多流架构时也是如此。正如我们的实验所证明的那样，我们的方法的更大灵活性转化为更高的准确性。
强化学习代理的实际使用往往受到训练时间的瓶颈。为了加速训练，从业者往往转向分布式强化学习架构来并行化和加速训练过程。 然而，可扩展的强化学习（RL）的现代方法往往在RL代理可以学习的样本吞吐量（样本吞吐量）和每个样本的学习质量（样本效率）之间进行权衡。在这些可扩展的RL架构中，随着样本吞吐量的增加（即IMPALA（Espeholt等人。为了解决这个问题，我们提出了一种新的分布式强化学习算法IMPACT。IMPACT用三个变化扩展了PPO：用于稳定代理目标的目标网络、循环缓冲器和截断重要性采样。在离散行动空间环境中，我们表明IMPACT获得了更高的奖励，同时比IMBALA的训练壁时间减少了30%。
在本文中，我们展示了一个简单的着色方案可以在理论上和经验上改善消息传递神经网络（MPNN）的表达能力。更具体地说，我们引入了一个称为彩色局部迭代程序（CLIP）的图神经网络，它使用颜色来区分相同的节点属性，并表明这种表示是具有节点属性的图上连续函数的通用近似器。 我们的方法依赖于可分离性，这是一个关键的拓扑特征，允许将精心选择的神经网络扩展为普遍的表示。最后，我们通过实验表明，CLIP能够捕捉到传统MPNN无法区分的结构特征，同时在基准图分类数据集上是最先进的。
在本文中，我们提出了一种使用无递归组件的生成对抗网络的算法旋律生成方法。音乐生成已经成功地使用了递归神经网络，该模型学习序列信息，可以帮助创建真实的声音的旋律。 在这里，我们使用DCGAN架构与扩张卷积和塔来捕捉序列信息作为空间图像信息，并在固定长度的旋律形式中学习长距离的依赖关系，如爱尔兰传统卷轴。
神经机器翻译（NMT）系统在翻译文本方面已经达到了最先进的性能并被广泛部署。 然而，人们对这些系统的功能或故障了解甚少。 在这里，我们表明，NMT系统很容易产生高度病态的翻译，完全不与源材料相联系，我们称之为幻觉。 这种病态的翻译是有问题的，因为它们深深地干扰了用户的信任，而且很容易被发现。 我们描述了一种产生幻觉的方法，并表明NMT架构的许多常见变化都容易受到幻觉的影响。我们研究了各种减少幻觉频率的方法，包括数据增强、动态系统和正则化技术，并表明数据增强能显著减少幻觉频率。最后，我们分析了产生幻觉的网络，并在注意力矩阵和解码器的稳定性措施中显示了幻觉的特征。
为了使人工智能系统获得广泛的公众认可，我们必须开发出能够解释神经网络等黑箱模型决策的方法。在这项工作中，我们发现了当前解释方法的两个问题。首先，我们表明，关于解释的两个普遍观点--特征加法和特征选择--导致了根本不同的实例解释。 在文献中，来自不同角度的解释器目前被直接比较，尽管它们的解释目标不同。第二个问题是，目前的事后解释器只在简单的模型上得到了彻底的验证，如线性回归，当应用于现实世界的神经网络时，解释器通常是在假设学到的模型行为合理的情况下被评估。 然而，神经网络经常依赖于不合理的相关性，即使在产生正确的决策时也是如此。我们为特征选择视角下的解释方法引入了一个验证框架。我们的框架基于一个在现实世界任务中训练的非微妙的神经网络架构，并且我们能够为其内部工作提供保证。我们通过展示当前解释器的失败模式来验证我们评价的有效性。我们的目标是，当需要从特征选择视角进行解释时，这个框架能够提供一个公开可用的、1现成的评价。
在高维空间的规划仍然是一个具有挑战性的问题，即使最近在算法和计算能力方面取得了进展。我们的灵感来自于神经科学中的功效复制和感觉重合理论。 我们的目的是让代理人形成其环境的心理模型以进行规划。 小脑是用一个双流的、完全连接的预测网络来模拟的。该网络接收功效以及当前状态的特征作为输入。基于从知识提炼方法中获得的见解，我们选择一个预先训练好的网络的输出作为我们的特征，产生一个当前状态的压缩表示。 该表示法的选择使其能够使用经典的图搜索算法进行快速搜索。我们使用修改过的最佳优先搜索算法在一个观点匹配任务上展示了我们方法的有效性。
实验证据表明，在许多无监督的相似性任务上，简单的模型优于复杂的深度网络。引入最优表示空间的概念，我们为这个明显的悖论提供了一个简单的理论解决方案。此外，我们提出了一个直接的程序，在没有任何重新训练或架构修改的情况下，允许深度递归模型与浅层模型相比表现同样好（有时更好）。 为了验证我们的分析，我们进行了一系列一致的经验评估，并在这个过程中引入了几个新的句子嵌入模型。尽管这项工作是在自然语言处理的背景下提出的，但其见解很容易适用于其他依赖分布式表征的转移任务领域。
在本文中，我们提出了第一个大规模的由人类设计的cloze测试数据集CLOTH，其中的问题被用于初中和高中的语言考试。 我们显示人类的表现明显优于专门设计的基线模型，即使该模型是在足够大的外部数据上训练的。我们研究了性能差距的来源，将模型的缺陷追溯到CLOTH的一些独特的属性，并确定理解长期上下文的有限能力是关键瓶颈。此外，我们发现，与自动生成的数据相比，人类设计的数据导致了模型的性能和人类的性能之间更大的差距。
最近的工作表明，目标驱动的神经网络训练可以用来模拟大脑中的神经活动。虽然人工神经网络中的神经元的反应特性与大脑中的神经元有相似之处，但网络架构往往被约束得不同。在这里，我们问神经网络是否可以同时恢复神经表征，如果架构不受约束和优化，也可以恢复神经回路的解剖学特性。 我们在一个系统中证明了这一点，该系统的连接性和功能组织已经被定性，即啮齿动物和果蝇的头部方向电路。我们训练了循环神经网络（RNN），通过整合角速度来估计头部方向。 我们发现，在头部方向系统中观察到的两类不同的神经元，即环形神经元和移位神经元，作为训练的结果在人工神经网络中自然出现。此外，连接分析和仿真神经生理学揭示了人工网络和头部方向系统之间的结构和机制的相似性。 总的来说，我们的结果表明，在目标驱动的任务中，RNNs的优化可以重现生物电路的结构和功能，这表明人工神经网络可以用来在神经活动和解剖组织的水平上研究大脑。
卷积神经网络（CNN）是计算密集型的，这限制了它们在移动设备上的应用。它们的能量被执行卷积所需的乘法数量所支配。Winograd的最小过滤算法（Lavin，2015）和网络修剪（Han等人）。2015）可以减少操作数，但这两种方法不能直接结合起来--应用Winograd变换填补了权重和激活的稀疏性。我们对基于Winograd的CNN提出了两个修改，使这些方法能够利用稀疏性。 对于CIFAR-10、CIFAR-100和ImageNet数据集上的模型，我们的方法分别减少了10.4倍、6.8倍和10.8倍的乘法次数，精度损失小于0.1%，比以前的基线高出2.0倍至3.0倍。
在本文中，我们提出了一种叫做Advanced Neuroevolution的新型优化算法。该算法的目的是训练深度神经网络，并最终在需要时作为随机梯度下降（SGD）及其变体的替代品。我们在MNIST数据集以及几个全局优化问题（如Ackley函数）上评估了我们的算法。我们发现该算法在这两种情况下表现都比较好，超过了其他全局优化算法，如粒子群优化（PSO）和进化策略（ES）。
随机神经网的权重被用于各种情况，包括正则化、贝叶斯神经网、强化学习中的探索和进化策略。不幸的是，由于权重的数量很大，迷你批中的所有例子通常共享相同的权重扰动，从而限制了大型迷你批的方差减少效果。我们介绍了flipout，这是一种有效的方法，通过隐式采样每个例子的伪独立权重扰动来装饰迷你批中的梯度。 根据经验，flipout对全连接网络、卷积网络和RNNs实现了理想的线性方差减少。我们发现在训练具有乘法高斯扰动的神经网络时速度明显加快。 Flipout还使我们能够将进化策略矢量化：在我们的实验中，一个带有Flipout的GPU可以处理与使用现有方法的至少40个CPU核心相同的吞吐量，相当于在亚马逊网络服务上减少4倍的成本。
深度生成模型，如变异自动编码器（VAE）和生成对抗网络（GAN）在机器学习和计算机视觉中发挥着越来越重要的作用。然而，有两个基本问题阻碍了它们在现实世界的应用：在VAE中进行变异推理的困难和在GAN中编码真实世界样本的功能缺失。 在本文中，我们提出了一种名为Latently Invertible Autoencoder（LIA）的新算法，以在一个框架中解决上述两个问题。一个可逆网络及其逆向映射被对称地嵌入VAE的潜空间中。 因此，部分编码器首先将输入转化为特征向量，然后这些特征向量的分布被可逆网络重塑以适应先验。解码器以编码器的复合映射的相反顺序进行。 设计了一个两阶段的无随机性训练方案，通过对抗性学习来训练LIA，也就是说，LIA的解码器首先被训练成一个带有可逆网络的标准GAN，然后通过从LIA中分离出可逆网络，从自动编码器中学习部分编码器。 在FFHQ人脸数据集和三个LSUN数据集上进行的实验验证了LIA推理和生成的有效性。
神经程序是高度精确和结构化的策略，通过控制计算机制的行为来执行算法任务。尽管有可能提高人工代理行为的可解释性和构成性，但仍然很难从代表计算机程序的演示神经网络中学习。将算法领域与其他模仿学习领域区分开来的主要挑战是对高精度的需求、对特定数据结构的参与以及极其有限的可观察性。为了解决这些挑战，我们建议将程序建模为参数化层次程序（PHPs）。 一个PHP是一连串的条件操作，使用程序计数器和观察结果来选择采取一个基本的行动，调用另一个PHP作为子程序，并返回给调用者。 我们开发了一种算法，用于从一组监督者演示中训练PHP，其中只有部分演示被注释了内部调用结构，并将其应用于多级PHP的有效逐级训练。我们在两个基准中显示，NanoCraft和长手加法，PHP可以从较小数量的被注释和未被注释的演示中更准确地学习神经程序。
深度强化学习在直接从原始像素学习控制策略方面取得了最先进的成果。然而，尽管它取得了显著的成功，但它却不能泛化，这是一个稳定的人工智能系统所需要的基本组成部分。使用Atari游戏Breakout，我们证明了一个训练有素的代理很难适应原始图像的简单修改，这些修改是人类可以轻易适应的。 在迁移学习中，目标是利用从源任务中获得的知识，使目标任务的训练更快、更好。我们表明，使用各种形式的微调，一种常见的迁移学习方法，对于适应这种小的视觉变化是无效的。事实上，从头开始重新训练代理往往比微调训练的代理更容易。 我们建议，在某些情况下，可以通过添加一个专门的组件来改善迁移学习，该组件的目标是学习在已知领域和新领域之间进行视觉映射。具体来说，我们使用无对齐生成对抗网络（GANs）来创建一个映射函数，将目标任务中的图像转换为源任务中的相应图像。 这些映射函数使我们能够在Breakout游戏的各种变体之间，以及在任天堂游戏《Road Fighter》的不同级别之间进行转换。我们表明，学习这种映射比重新训练的效率要高得多。一个经过训练的代理在玩Breakout和Road Fighter时的可视化，有无GAN转移，可以在\url{https://streamable.com/msgtm}和\url{https://streamable.com/5e2ka}看到。
机器学习界的一个普遍看法是，使用自适应梯度方法会损害泛化。我们根据近年来的见解和趋势，从理论上和实验上重新审视这一看法。我们更深入地重温了以前经常引用的一些实验和理论说法，并在更大规模、最先进的设置中提供了一组新的实验。 我们的结论是，通过适当的调整，自适应优化器的训练性能提高一般不会带来过拟合惩罚，特别是在当代深度学习中。最后，我们综合了自适应优化器的 "用户指南"，包括对AdaGrad的一些修改建议，以减轻其一些经验上的不足之处。
在本文中，我们介绍了APL，一种通过记忆它所遇到的最令人惊讶的观察结果来逼近概率分布的算法。这些过去的观察结果被从一个外部内存模块中调用，并由一个解码器网络处理，它可以结合来自不同内存插槽的信息来概括出超出直接调用的信息。我们表明，这种算法可以在内存占用较小的情况下，在少量分类基准上表现得与最新的基线相同。 此外，它的内存压缩使它可以扩展到成千上万的未知标签。 最后，我们引入了一个比直接分类更具挑战性的元学习推理任务。在这种情况下，APL能够通过演绎推理在每类少于一个例子的情况下进行概括。
递归神经网（RNN）的最新进展在自然语言处理的许多应用中显示了很大的前景。对于大多数这些任务，如客户评论的情感分析，递归神经网模型在形成决策前会解析整个评论。我们认为，在实践中，阅读整个输入并不总是必要的，因为很多评论往往容易分类，即。在本文中，我们提出了一种快速阅读文本分类的方法。受几个著名的人类阅读技术的启发，我们的方法实现了一个智能循环代理，它评估当前片段的重要性，以决定是否进行预测，或跳过一些文本，或重新阅读部分句子。 我们的代理使用RNN模块来编码来自过去和当前标记的信息，并应用策略模块来形成决策。通过一个基于策略梯度的端到端训练算法，我们在几个文本分类数据集上训练和测试了我们的代理，与以前的方法相比，取得了更高的效率和更好的准确性。
强化学习代理需要探索他们的未知环境，以解决给他们的任务。对于复杂的环境来说，贝叶斯的最优探索方案是难以实现的，虽然已经提出了一些探索方法作为近似值，但仍然不清楚现有的探索方法正在优化什么基本目标，或者如何改变它们以纳入关于任务的先验知识。 此外，目前还不清楚如何获得一个对解决多个下游任务有用的单一探索策略。我们通过学习一个单一的探索策略来解决这些缺陷，该策略可以在多任务环境下快速解决一系列下游任务，摊薄学习探索的成本。 我们将探索重塑为一个状态边际匹配（SMM）的问题，我们的目标是学习一个状态边际分布与给定的目标状态分布相匹配的策略，这可以包含关于任务的先验知识。我们通过将其简化为一个状态密度模型和参数化策略之间的双人零和游戏来优化目标。 我们对这一方法的理论分析表明，先前的探索方法并没有学习一个进行分布匹配的策略，而是获得了一个进行分布匹配的重放缓冲器，这一观察可能解释了这些先前的方法在单任务设置中的成功。在模拟和现实世界的任务中，我们证明了我们的算法比先前的方法探索得更快，适应得也更快。
卷积神经网络的有效性在很大程度上源于其利用许多学习问题中固有的翻译不变性的能力。最近，有研究表明，CNN可以通过使用分组卷积而不是平面卷积来利用其他不变性，如旋转不变性。 然而，由于性能和易于实现的原因，有必要将组卷积限制在可以应用于滤波器而不需要插值的变换上。因此，对于具有方形像素的图像，只有整数平移、90度的倍数旋转和反射是可以接受的。而方形瓷砖提供了4倍的旋转对称性，平面的六边形瓷砖具有6倍的旋转对称性。 在本文中，我们展示了如何通过重新使用现有的高度优化的卷积程序，在六边形格子上有效地实现平面卷积和群卷积。我们发现，由于六边形滤波器的各向异性减少，在固定的参数预算下，平面HexaConv比平面卷积的方形滤波器提供更好的精度。 此外，我们发现六边形网格的对称程度增加，通过允许更多的参数共享，增加了组卷积的有效性。我们表明，我们的方法在AID航空场景分类数据集上明显优于传统的CNN，甚至优于ImageNet预训练的模型。
深度卷积神经网络（CNN）已被反复证明在图像分类任务中表现良好，当给定足够的训练数据时，能成功识别广泛的物体。然而，物体定位的方法仍然需要大量的改进。 一般来说，这些方法很耗时，需要进行许多分类计算。在本文中，我们提供了一种根本不同的方法来定位图像中的识别物体。我们的方法是基于这样的想法：能够识别物体的深度CNN必须在其连接权重中隐含有关物体位置的知识。 我们提供了一种简单的方法来解释单个分类图像中的分类器权重。该方法涉及计算网络生成的激活模式的导数，如输出类标签单元的激活，与每个输入像素有关，进行敏感性分析，以确定在局部意义上对内部表示和物体识别影响最大的像素。 这些导数可以通过深度CNN分类器的一次后向传递有效地计算出来，产生图像的灵敏度图。我们证明，可以从灵敏度图学习到边界框坐标的简单线性映射，对识别的物体进行定位。我们的实验结果，使用已知地面真实定位信息的真实世界数据集，显示了我们的快速技术具有竞争力的准确性。
我们提出了trellis网络，一种用于序列建模的新架构。一方面，trellis网络是一种具有特殊结构的时间卷积网络，其特点是跨深度的权重捆绑和直接将输入注入深层。另一方面，我们表明截断的递归网络等同于在其权重矩阵中具有特殊稀疏结构的trellis网络，因此具有一般权重矩阵的trellis网络可以概括截断的递归网络。 我们利用这些联系来设计高性能的trellis网络，从递归和卷积模型中吸收结构和算法元素。实验证明，trellis网络在各种挑战性的基准上的表现优于目前的技术水平，包括单词级语言建模和字符级语言建模任务，以及旨在评估长期记忆保留的压力测试。代码可在https://github.com/locuslab/trellisnet。
我们提出了一个训练特定领域模型（DSMs）的端到端框架，以获得物体检测任务的高精确度和计算效率。DSMs是用蒸馏法训练的，重点是在有限的领域（如交叉口的固定视图）实现高精确度。我们认为，即使模型尺寸很小，DSMs也能很好地捕获基本特征，实现比传统技术更快的精确度和效率。 此外，我们通过从训练集中剔除容易分类的图像来减少数据集的大小，从而提高训练效率。对于有限的领域，我们观察到紧凑的DSMs明显超过了相同大小的COCO训练模型的准确性。通过在紧凑的数据集上训练，我们表明，在准确性仅下降3.6%的情况下，训练时间可以减少93%。
我们通过神经网络对策略、Q$函数和动力学的表达能力来比较无模型强化学习和基于模型的方法。 我们从理论和经验上表明，即使是一维连续状态空间，也有许多MDPs的最优$Q$函数和策略比动力学复杂得多。我们假设许多现实世界的MDPs也有类似的特性。对于这些MDPs，基于模型的规划是一种有利的算法，因为产生的策略比神经网络参数化能更好地接近最优策略，而无模型或基于模型的策略优化依赖于策略参数化。 在该理论的激励下，我们应用一个简单的多步骤基于模型的引导计划器（BOOTS）将一个弱的$Q$-函数引导成一个更强的策略。实证结果表明，在测试时将BOOTS应用于基于模型或无模型的策略优化算法之上，提高了MuJoCo基准任务的性能。
常识性的物理推理是任何在现实世界中运行的智能代理的基本要素。例如，它可以用来模拟环境，或推断世界上目前没有观察到的部分的状态。为了匹配现实世界的条件，这种因果知识必须在没有监督数据的情况下学习。为了解决这个问题，我们提出了一种新的方法，它可以学习从原始视觉图像中发现物体并以纯无监督的方式建立它们的物理互动模型。 在弹跳球的视频中，我们展示了我们的方法与其他不包含这种先验知识的无监督神经方法相比所具有的卓越的建模能力。
神经网络可能表现出对简单性的偏爱的想法由来已久。简单性偏爱提供了一种量化这种直觉的方法。 它预测，对于描述科学和工程中许多系统的一类广泛的输入-输出图来说，简单的输出比复杂的输出在输入的均匀随机抽样中出现的可能性要大得多。 从RNA序列到二级结构图，到耦合微分方程系统，再到植物生长模型，都观察到了这种简单性偏向行为。  深度神经网络可以被视为从参数空间（权重）到函数空间（输入如何被网络转化为输出）的映射。 我们表明，这种参数-函数映射服从于简单性偏差的必要条件，并在数值上表明，它对低描述复杂性的函数有很大的偏差。 我们还证明了一个类似Zipf的幂律概率等级关系。  对简单性的偏爱可能有助于解释为什么神经网络的概括性如此之好。
模仿学习（IL）是学习理想的自主行为的一种吸引人的方法。然而，指导IL实现任意的目标是很困难的。相反，基于规划的算法使用动力学模型和奖励函数来实现目标，然而，唤起理想行为的奖励函数往往很难指定。 模仿模型是理想行为的概率预测模型，能够规划可解释的类似专家的轨迹，以实现指定的目标。我们推导出灵活的目标系列，包括有约束的目标区域、无约束的目标集和基于能量的目标。 我们还表明，我们的方法对不明确的目标具有鲁棒性，例如在道路的错误一侧的目标。
为了解决这个问题，我们引入了一个完全可区分的通信和推理框架，使代理人能够在部分可观察的环境中解决合作任务。该框架旨在促进代理人之间的明确推理，通过一个新的基于记忆的注意力网络，可以有选择地从其过去的记忆中学习。 该模型通过一系列推理步骤进行交流，这些步骤将每个代理的意图分解成学习的表征，首先用于计算交流信息的相关性，其次从记忆中提取新收到的信息。
在本文中，我们介绍了Symplectic ODE-Net（SymODEN），这是一个深度学习框架，可以从观察到的状态轨迹中推断出物理系统的动力学。为了用较少的训练样本实现更好的泛化，SymODEN通过以物理学信息的方式设计相关的计算图，纳入了适当的归纳偏见。特别是，我们用控制来执行哈密尔顿动力学，以透明的方式学习基础动力学，然后可以利用它来了解系统的相关物理方面，如质量和潜在能量。 此外，我们提出了一个参数化，即使广义坐标数据被嵌入到高维空间中，或者我们只能访问速度数据而不是广义动量，也能强制执行这个哈密尔顿形式主义。这个框架通过为物理系统提供可解释的、物理上一致的模型，为综合基于模型的控制策略提供了新的可能性。
联合学习，即通过对本地计算的更新参数进行迭代平均来训练全局模型，是一种很有前途的深度网络分布式训练方法；它提供了很高的通信效率和隐私保护性，可以很好地适应分散的数据环境，如移动云生态系统。然而，尽管有这些优势，基于联合学习的方法在处理本地设备（即学习者）的非IID训练数据时仍然有挑战。在这方面，我们研究了非IID环境下各种超参数条件的影响，以回答实际实施中的重要问题：(i) 我们首先研究了本地更新的参数分歧，以解释非IID数据的性能下降。 (ii) 然后，我们重新审视了优化器、网络深度/宽度和正则化技术的影响；我们的观察表明，众所周知的超参数优化策略的优势在非IID数据下反而会产生递减的回报。 (iii) 最后，我们以分类的方式提供了失败案例的原因，主要基于参数发散的度量。
众所周知，深度学习模型容易受到对抗性例子的影响。一个实用的对抗性攻击应该尽可能不需要对被攻击模型T的了解。目前的替代性攻击需要预先训练的模型来产生对抗性例子，其攻击成功率严重依赖于对抗性例子的可转移性。 目前基于分数和决策的攻击需要对T进行大量的查询。在这项研究中，我们提出了一种新型的对抗性模仿攻击。首先，它通过像生成性对抗网络（GANs）一样的双人游戏产生T的复制品。 生成模型G的目标是产生导致D返回与T不同的输出的例子。判别模型D的目标是在相同的输入下输出与T相同的标签。然后，D产生的对抗性例子被用来欺骗T。与目前的替代攻击相比，模仿攻击可以使用更少的训练数据来产生T的复制品，并提高对抗性例子的可转移性。实验证明，我们的模仿攻击比黑盒替代攻击需要更少的训练数据，但在没有查询的未见过的数据上取得的攻击成功率接近白盒攻击。
使用随机选择的批次的随机梯度下降（SGD）方法被广泛用于训练神经网络（NN）模型。执行设计探索以找到特定任务的最佳NN，通常需要在大数据集上用不同的模型进行广泛的训练，这在计算上是非常昂贵的。加快这种计算的最直接的方法是将SGD的批次分布在多个处理器上。然而，大批次训练有时会导致准确性下降，泛化能力差，甚至对对抗性攻击的鲁棒性差。 为了解决这个问题，我们提出了一种新的大批量训练方法，它结合了最近在对抗性训练（针对 "尖锐的最小值 "进行正则化）和二阶优化（使用曲率信息在训练中自适应地改变批量大小）方面的成果。我们在Cifar-10/100、SVHN、TinyImageNet和ImageNet数据集上使用多个NN，包括剩余网络和压缩网络，如SqueezeNext，广泛地评估我们的方法。 我们的新方法在准确性和SGD迭代次数方面都超过了现有解决方案的性能（分别高达1/%和3/times$）。我们强调，这是在没有任何额外的超参数调整的情况下实现的，以使我们的方法适合任何这些实验。
受哺乳动物大脑中导航细胞的神经生理学发现的启发，我们引入了第一个用于模拟以自我为中心的空间记忆（ESM）的深度神经网络架构。它可以学习估计代理人的姿势，并在空间扩展的环境中从自我为中心的视角逐步构建自上而下的二维全局地图。 在探索过程中，我们提出的ESM网络模型使用一个递归神经网络，根据本地观察更新全局地图的信念。它还用一个新的外部存储器来增强本地映射，根据它们在自我中心坐标中的相应位置来编码和存储所访问的地方的潜在表征，这使代理人能够进行循环封闭和映射校正。 这项工作在以下几个方面做出了贡献：首先，我们提出的ESM网络提供了准确的映射能力，这对具身代理导航到目标位置是非常重要的。在实验中，我们通过与几个有竞争力的基准线和最先进的同步定位和映射（SLAM）算法进行比较，证明了ESM网络在复杂的三维迷宫中随机行走的功能。 第二，我们忠实地假设了大脑中导航细胞的功能和工作机制。对我们的模型的综合分析表明了各个模块在我们提出的架构中的重要作用，并证明了这些模块之间的通信效率。我们希望这项工作能够推动计算机科学和计算神经科学两个领域的合作和通信研究。
我们表明，如果通常的训练损失被一个Lipschitz正则化项所增强，那么网络就会泛化。 我们首先通过建立一个更强的收敛结果以及收敛率来证明普适性。  第二个结果解决了Zhang等人（2016）提出的一个问题：一个模型如何区分干净标签和随机标签的情况？ 我们的答案是，使用干净数据的Lipschitz常数的Lipschitz正则化可以进行这种区分。 在这种情况下，模型学习了一个不同的函数，我们假设它正确地没有学习到脏标签。 
为了在资源有限的嵌入式硬件上快速和节能地部署训练有素的深度神经网络，每个学习的权重参数最好用一个比特来表示和存储。 在这里，我们报告了在多个数据集上错误率的大幅改善，用于部署每个权重为1比特的深度卷积神经网络。使用宽残差网络作为我们的主要基线，我们的方法简化了现有的方法，通过在训练中应用符号函数对权重进行二进制化；我们为每个层应用缩放因子，其未学习的恒值等于用于初始化的特定层标准偏差。 对于CIFAR-10、CIFAR-100和ImageNet，以及需要少于10MB参数内存的每权重1比特的模型，我们实现的错误率分别为3.9%、18.5%和26.0% / 8.5%（Top-1 / Top-5）。我们还考虑了MNIST、SVHN和ImageNet32，实现每权重1比特的测试结果为0。 对于CIFAR，我们的错误率是之前报道的数值的一半，并且与我们对同一网络的全精度权重的错误率相差约1%。对于过度拟合的网络，我们也显示出通过不学习批量归一化比例和偏移参数，错误率有明显的改善。 使用一个温暖的重启学习率时间表，我们发现1位每权重的训练和全精度网络一样快，而且比标准时间表有更好的准确性，在CIFAR-10/100中仅用62个训练历时就达到了约98%-99%的峰值性能。关于MATLAB、Keras和PyTorch的完整训练代码和训练模型，见https://github.com/McDonnell-Lab/1-bit-per-weight/ 。
本文介绍了一个使用实时光线追踪的非欧几里得空间的沉浸式可视化系统。它利用了基于英伟达图灵架构的新一代GPU的能力，以开发新的方法来直观地探索虚拟现实中具有非微观几何和拓扑结构的景观。
我们提出了集合自动编码器，这是一种用于元素集合的无监督表示学习的模型。它与序列到序列模型密切相关，后者为序列学习固定大小的潜在表示，并被应用于一些具有挑战性的监督序列任务，如机器翻译，以及序列的无监督表示学习。 与序列不同的是，集合是不变的。所提出的集合自动编码器考虑了这一事实，既考虑了模型的输入，也考虑了模型的输出。在输入端，我们使用基于内容的注意机制，调整了最近引入的循环神经结构。 在输出端，我们使用稳定的联姻算法，在学习阶段将预测与标签对齐。我们在点云的合成数据集上训练模型，并显示学习到的表征随着输入中的平移而平滑变化，保留输入中的距离，并且直接表示集合的大小。 我们将该模型应用于点云上的监督任务，使用固定大小的潜在表征。对于一些困难的分类问题，其结果比不考虑包络不变性的模型要好。特别是对于小的训练集，集合意识模型得益于无监督的预训练。
深度强化学习算法可以学习复杂的行为技能，但这些方法的实际应用需要代理人收集相当多的经验。在实际环境中，如机器人，这涉及到反复尝试一项任务，在每次尝试之间重新设置环境。然而，并非所有的任务都是容易或自动可逆的。实际上，这个学习过程需要相当多的人为干预。 在这项工作中，我们提出了一种自主的安全有效的强化学习方法，它同时学习前向和后向策略，后向策略为下一次尝试重置环境。通过学习后向策略的价值函数，我们可以自动确定前向策略何时即将进入非可逆状态，提供不确定性感知的安全中止。我们的实验表明，正确使用后向策略可以大大减少学习任务所需的人工重置次数，并可以减少导致非可逆状态的不安全行为。
有人认为，目前的机器学习模型没有常识，因此必须对先验知识进行硬编码（Marcus，2018）。这里我们展示了令人惊讶的证据，即语言模型已经可以学习捕捉某些常识性知识。我们的关键观察是，语言模型可以计算任何语句的概率，而这种概率可以用来评估该语句的真实性。 在Winograd Schema挑战赛上（Levesque等人，2011），语言模型的准确率比以前最先进的监督方法高11%。语言模型还可以针对在ConceptNet上挖掘常识性知识的任务进行微调，达到0.912和0.824的F1分数，超过了以前的最佳结果（Jastrzebskiet al.，2018）。 进一步的分析表明，语言模型可以发现Winograd Schema语境的独特特征，在没有明确监督的情况下决定正确答案。
在许多现实世界的学习场景中，特征只能在预算下的成本约束下获得。在本文中，我们提出了一种新颖的方法，用于在预测时间获得成本敏感的特征。建议的方法基于上下文感知的特征价值函数，逐步获得特征。我们在强化学习范式中制定问题，并根据每个特征的效用引入奖励函数。 具体来说，MC辍学抽样被用来衡量模型不确定性的预期变化，该模型被用作特征值函数。此外，我们建议在类预测器和价值函数估计器网络之间共享表示。雅虎学习排名和医疗领域的糖尿病分类数据集。根据结果，所提出的方法能够有效地获得特征并做出准确的预测。
本文重新审视了使用卷积架构进行序列建模的问题。虽然卷积和递归架构在序列预测中都有很长的历史，但目前大部分深度学习社区的 "默认 "心态是，通用序列建模最好用递归网络来处理。本文的目标是质疑这一假设。具体来说，我们考虑了一个简单的通用时间卷积网络（TCN），它采用了现代ConvNet架构的特征，如扩张和剩余连接。我们表明，在各种序列建模任务上，包括许多经常被用作评估递归网络的基准，TCN优于基线RNN方法（LSTMs、GRU和vanilla RNNs），有时甚至优于高度专业化的方法。我们进一步表明，RNNs相对于TCNs的潜在的 "无限内存 "优势在实践中基本不存在。TCNs确实表现出比它们的递归对应物更长的有效历史尺寸。 总的来说，我们认为现在可能是时候（重新）考虑将ConvNets作为序列建模的默认 "首选 "架构。
当提供数据并通过梯度下降方法进行训练时，深度神经网络在逼近复杂函数方面表现良好。同时，有大量的现有函数以精确的方式编程解决不同的任务，消除了训练的需要。 在许多情况下，可以将一个任务分解为一系列的函数，对于其中的一些函数，我们可能更倾向于使用神经网络来学习功能，而对于另一些函数，首选的方法是使用现有的黑盒函数。我们提出了一种对基础神经网络进行端到端训练的方法，它整合了对现有黑盒函数的调用。 我们通过用一个可微分的神经网络来接近黑盒功能，在端到端优化过程中驱动基础网络遵守黑盒函数接口。在推理时，我们用外部黑盒的非可微分对应物替换可微分估计器，从而使基础网络的输出与黑盒函数的输入参数匹配。 使用这种 "估计和替换 "的范式，我们训练一个神经网络，从头到尾计算黑箱功能的输入，同时消除了对中间标签的需求。我们表明，通过在推理过程中利用现有的精确黑箱功能，综合模型比完全可分模型的泛化效果更好，并且与基于RL的方法相比学习效率更高。
本文提出了一种新的演员批判式算法，称为双演员批判或Dual-AC。 它是以一种原则性的方式从贝尔曼最优方程的拉格朗日对偶形式推导出来的，它可以被看作是演员和一个类似批评家的函数之间的双人游戏，这个函数被命名为双批评家。 与它的行为者-批评者的亲戚相比，Dual-AC有一个理想的属性，即行为者和双重批评者合作更新以优化相同的目标函数，为学习批评者提供了一种更透明的方式，直接与行为者的目标函数相关。然后我们提供了一个具体的算法，可以有效地解决最小优化问题，使用多步引导、路径正则化和随机双重上升算法的技术。
训练一个模型来执行一个任务通常需要大量的数据，这些数据来自于任务将被应用的领域。然而，通常的情况是，某些领域的数据很丰富，但其他领域的数据却很匮乏。领域适应性处理的挑战是，使一个从数据丰富的源领域训练出来的模型在一个数据匮乏的目标领域表现良好。 CycleGAN是一个强大的框架，它通过对抗性训练和循环一致性约束有效地学习将输入从一个领域映射到另一个领域。然而，在一个或多个领域的训练数据有限的情况下，通过重建来强制执行循环一致性的传统方法可能过于严格。在本文中，我们提出了一个增强的循环对抗性学习模型，通过外部任务特定模型强制执行循环一致性约束，鼓励保留任务相关内容，而不是精确重建。 这个特定的任务模型既放松了循环一致性约束，又补充了训练期间判别器的作用，作为学习映射的一个增强的信息源。 我们的方法将TIMIT数据集中女性说话者的语音识别绝对性能提高了2%，其中大部分训练样本来自男性声音。在低资源的视觉领域适应中，结果显示我们的方法在将SVHN适应到MNIST和反之亦然时，绝对性能分别提高了14%和4%，这超过了需要高资源未标记的目标领域的无监督领域适应方法。
深度强化学习的流行算法，如策略梯度和Q-learning，其成功在很大程度上依赖于在顺序决策过程的每个时间步的信息奖励信号的可用性。当奖励在一个情节中只有稀疏的可用，或者奖励反馈只在情节终止后提供，这些算法由于信用分配的困难而表现得不太理想。 另外，基于轨迹的策略优化方法，如交叉熵法和进化策略，不需要每个时间段的奖励，但被发现由于完全放弃了问题的时间性而受到高样本复杂性的影响。因此，提高RL算法在具有稀疏或偶发奖励的现实世界问题中的效率是一个迫切需要。 在这项工作中，我们引入了一种自我模仿学习算法，该算法在稀疏和偶发的奖励设置中得到了很好的利用和探索。我们将每个策略视为状态-动作访问分布，并将策略优化表述为发散最小化问题。我们表明，通过Jensen-Shannon发散，这个发散最小化问题可以被简化为一个策略梯度算法，该算法具有从经验复制中学习到的成型的奖励。 实验结果表明，在奖励密集的环境中，我们的算法与现有的算法相当，而在奖励稀少和偶发的环境中，我们的算法明显更好。然后，我们讨论了自我模仿学习的局限性，并提出通过使用斯坦恩变异政策梯度下降与詹森-香农内核来学习多种不同的政策来解决这些问题。
我们研究了允许自动编码器编码和解码一个简单的几何形状--圆盘的精确机制。在这个精心控制的环境中，我们能够描述训练步骤中最小化问题的最优解的具体形式。我们表明，自动编码器在训练期间确实接近了这个解。 最后，我们探讨了几种正则化方案来解决泛化问题。鉴于最近对神经网络生成能力的高度关注，我们认为深入研究简单的几何案例可以对生成过程有所启发，并可以为更复杂的架构提供一个最低要求的实验设置。
我们提出了一个简单的想法，允许用一种给定的语言记录说话者，并用他们可能不知道的其他语言合成他们的声音。这些技术开启了广泛的潜在应用，如跨语言交流、语言学习或自动视频配音。我们称这个一般问题为多语言说话者条件的语音合成，我们提出了一个简单但强大的基线。 主要区别在于，我们不是以特定语言的字符或音素为条件，而是以所有语言通用的共享语音表征为条件。这种跨语言的文本语音表征允许我们在保留原始说话人的声乐特征的同时合成任何语言的语音。
模仿学习（IL）的目标是使学习者能够在专家的示范下模仿专家的行为。最近，生成对抗性模仿学习（GAIL）在复杂的连续任务的IL方面取得了重大进展。然而，GAIL及其扩展在训练期间需要大量的环境互动。 在现实世界的环境中，一个IL方法越是需要学习者与环境交互以获得更好的模仿，它所需要的训练时间就越多，对环境和学习者本身造成的损害也越大。我们认为，如果能够减少交互的数量，IL算法可以更适用于现实世界的问题。在本文中，我们提出了一种用于连续控制的无模型IL算法。我们的算法主要由对现有对抗性模仿学习（AIL）方法的三个变化组成--（a）采用非政策性行为者批评（Off-PAC）算法来优化学习者政策，（b）使用非政策性样本估计状态动作值，而不学习奖励函数，以及（c）代表随机政策函数，使其输出有界。 实验结果表明，我们的算法取得了与GAIL竞争的结果，同时大大减少了环境的相互作用。
在文本中进行无监督 "风格转移 "的主流方法是基于学习一个潜在表征的想法，该表征与指定其 "风格 "的属性无关。在本文中，我们表明这个条件不是必要的，而且在实践中也不一定能满足，即使有明确旨在学习这种分离表征的领域对抗性训练。 因此，我们提出了一个新的模型，控制文本数据中的几个变化因素，在这个模型中，用一个基于回译的更简单的机制来取代这个脱钩条件。我们的方法允许控制多个属性，如性别、情感、产品类型等。我们的方法允许对多个属性进行控制，如性别、情感、产品类型等，并对内容保存和风格改变之间的权衡进行更精细的控制，在潜空间中使用一个池化操作符。我们的实验表明，完全纠缠模型产生了更好的生成，即使在新的和更具挑战性的基准测试中，包括具有多个句子和多个属性的评论。
最近试图保留这种简单性，同时缓解梯度问题的做法是基于适当的初始化方案或对RNN的递归矩阵的正交性/单数约束，然而，这对其在动态系统现象（如混沌或多稳定性）方面的表达能力有限制。 在这里，我们提出了一个正则化方案，将RNN的部分潜伏子空间推向一个线吸引子配置，以实现长短期记忆和任意慢的时间尺度。我们表明，我们的方法在一些基准上表现出色，如连续的MNIST或乘法问题，并能够重建动态系统，这些系统包涵了广泛不同的时间尺度。
在生成对抗网络（GANs）领域，如何设计一个稳定的训练策略仍然是一个开放的问题。Wasserstein GANs通过引入Wasserstein距离，在很大程度上提高了原始GANs的稳定性，但仍然是不稳定的，容易出现各种故障模式。 在本文中，我们提出了一个名为Wasserstein-Bounded GAN（WBGAN）的通用框架，它通过简单地在Wasserstein项上增加一个上界约束来改进一大批基于WGAN的方法。此外，我们表明WBGAN可以合理地测量几乎没有交集的分布的差异。实验证明，WBGAN可以稳定并加速一系列基于WGAN变体的训练过程的收敛。
生成模型，如变异自动编码器（VAE）和生成对抗网络（GAN），通常是针对潜伏空间中的固定先验分布进行训练，如均匀分布或高斯分布。在得到一个训练好的模型后，人们可以以各种形式对生成器进行采样，用于探索和理解，如在两个样本之间进行插值，在一个样本的附近采样，或探索一对样本应用于第三个样本的差异。 在本文中，我们表明到目前为止，文献中使用的潜空间操作会引起结果输出和模型训练的先验分布之间的分布不匹配。为了解决这个问题，我们建议使用分布匹配运输图来确保这种潜空间操作保留先验分布，同时尽量减少对原始操作的修改。我们的实验结果验证了所提出的操作与原始操作相比给出了更高质量的样本。
最近，神经程序嵌入在各种程序分析任务中显示出很大的前景，包括程序合成、程序修复、代码完成和故障定位。然而，大多数现有的程序嵌入是基于程序的语法特征，如标记序列或抽象的语法树。 与图像和文本不同，程序具有明确的语义，仅考虑其语法是难以捕捉的（即语法相似的程序可以表现出巨大的运行时行为），这使得基于语法的程序嵌入从根本上受到限制。我们提出了一种新的语义程序嵌入，它是从程序执行轨迹中学习的。 我们的主要观点是，以现场变量值的顺序图表示的程序状态不仅能更准确地捕捉到程序语义，而且还能为循环神经网络提供更自然的模型。我们在对学生向编程入门课和CodeHunt教育平台提交的错误类型进行分类的任务上评估了不同的语法和语义程序嵌入。 我们的评估结果显示，语义程序嵌入明显优于基于标记序列和抽象语法树的句法程序嵌入。此外，我们用语义嵌入的预测来增强基于搜索的程序修复系统，并证明搜索效率有明显提高。
在本文中，我们提出了一种BN算法的泛化，即递减批量规范化（DBN），我们以递减移动平均的方式更新BN参数。批量规范化（BN）在加速神经网络训练阶段的收敛方面非常有效，它已经成为一种常见的做法。我们提出的DBN算法保持了原始BN算法的整体结构，同时对一些可训练的参数引入了加权平均的更新方式。我们对DBN算法的收敛性进行了分析，该算法在可训练参数方面收敛到一个静止点。我们的分析可以通过将一些参数设置为常数而轻松地推广到原始BN算法。该分析的主要挑战是，一些参数是通过梯度更新的，而另一些则不是。收敛分析适用于任何满足我们共同假设的激活函数。为了分析，我们还展示了确保收敛的步长和权重递减的充分和必要条件。在数值实验中，我们使用了具有更多层和ReLU激活的更复杂的模型。我们观察到，在Imagenet、MNIST、NI和CIFAR-10数据集上，DBN在合理的复杂FNN和CNN模型上优于原始BN算法。
生成模型，如变异自动编码器（VAEs）和生成对抗网络（GANs），通常是针对潜在空间中的固定先验分布进行训练，如均匀分布或高斯分布。 在得到一个训练好的模型后，人们可以以各种形式对生成器进行采样，以进行探索和理解，如在两个样本之间进行插值，在一个样本附近进行采样，或探索一对样本应用于第三个样本之间的差异。然而，到目前为止，文献中常用的潜空间操作会在结果输出和模型训练的先验分布之间引起分布不匹配。 在本文中，我们提出了一个修改潜空间操作的框架，以便完全消除分布不匹配。我们的方法基于最优传输图，它调整潜空间操作，使其与先验分布完全匹配，同时尽量减少对原始操作的修改。
构建一个具有适当话语和覆盖面的连贯和非单调的对话代理的问题仍然是一个开放的研究领域。目前的架构只考虑了给定查询的语义和上下文信息，而没有完全考虑句法和外部知识，这些知识对于在闲聊系统中产生反应至关重要。 为了克服这个问题，我们提出了一个端到端的多流深度学习架构，该架构通过利用记忆网络的上下文信息和句法信息，在其依赖性解析上纳入图卷积网络（GCN），为查询-响应对学习统一的嵌入。 这个网络的一个流还利用转移学习，通过预先训练一个双向转化器来为每个输入句子提取语义表示，并通过知识库（KB）中的实体的邻接来纳入外部知识。我们在下一个句子预测任务中对这些嵌入进行了基准测试，并在现有技术的基础上进行了显著改进。
我们研究了将广义策略与搜索算法相结合的技术，以便在解决概率规划问题时利用各自的优势并克服其弱点。行动模式网络（ASNet）是最近对规划的贡献，它使用深度学习和神经网络来学习概率规划问题的广义策略。 蒙特卡洛树搜索（MCTS）是一种用于优化决策的前向链式状态空间搜索算法，它通过模拟来逐步建立搜索树并估计每个状态的值。 虽然MCTS在与特定领域的知识搭配时可以取得最先进的结果，但如果没有这些知识，MCTS需要大量的模拟才能在搜索树中获得可靠的估计值。通过将ASNets与MCTS相结合，我们能够提高ASNet的能力，使其泛化到它所训练的问题分布之外，并加强MCTS在搜索空间的导航。
当训练分布和测试分布完全相同时，现代深度神经网络可以达到很高的精度，但这个假设在实践中经常被违反。当训练分布和测试分布不匹配时，精度会急剧下降。目前，很少有技术可以提高部署过程中遇到的不可预见的数据转移的鲁棒性。在这项工作中，我们提出了一种技术来提高图像分类器的鲁棒性和不确定性估计。 我们提出了AugMix，这是一种数据处理技术，实现起来很简单，增加的计算开销有限，并有助于模型抵御不可预见的损坏。AugMix在具有挑战性的图像分类基准上大大改善了鲁棒性和不确定性措施，在某些情况下将以前的方法与最佳性能的差距缩小了一半以上。
自动钢琴指法是一项艰巨的任务，计算机可以利用数据进行学习。由于数据收集是困难和昂贵的，我们建议通过使用计算机视觉技术从公共视频和MIDI文件中自动提取指法来实现这一过程的自动化。在90个视频上运行这一过程，产生了最大的钢琴指法数据集，其中有超过15万个音符。我们表明，当在我们的数据集上运行先前提出的自动钢琴指法模型，然后在人工标注的钢琴指法数据上进行微调，我们获得了最先进的结果。 除了指法提取方法，我们还介绍了一种新的方法，通过在生成对抗网络（GAN）提出的域外增强上进行微调，将深度学习计算机视觉模型转移到域外数据上工作。为了演示，我们在https://youtu.be/Gfs1UWQhr5Q 上匿名发布了我们对单个视频的输出的可视化信息。
领域适应是指利用源域中的标签数据，在标签稀少或不可用的目标域中学习一个准确的模型。最近一种寻找两个领域的共同表示的方法是通过领域对抗训练（Ganin & Lempitsky，2015），它试图诱导一个特征提取器，在一些特征空间中匹配源和目标特征分布。 然而，领域对抗训练面临着两个关键的限制：1）如果特征提取函数具有高容量，那么特征分布匹配是一个弱约束，2）在非保守的领域适应（没有一个分类器可以在源域和目标域都表现良好），训练模型在源域表现良好会损害目标域的性能。我们提出了两个新的相关模型：1）虚拟对抗性领域适应（VADA）模型，它将领域对抗性训练与惩罚违反集群假设的惩罚项相结合；2）带教师的决策边界迭代精化训练（DIRT-T）模型，它将VADA模型作为初始化，并采用自然梯度步骤来进一步减少违反集群假设。 广泛的实证结果表明，这两个模型的组合大大改善了数字、交通标志和Wi-Fi识别领域适应性基准的最先进性能。
在本文中，我们提出了连续图流，一种基于生成的连续图流方法，旨在为图结构数据的复杂分布建模。 一旦学会，该模型可以应用于任意图，在图所代表的随机变量上定义一个概率密度。它被表述为一个普通微分方程系统，具有共享和可重复使用的函数，在图上操作。 这导致了一种新型的神经图信息传递方案，随着时间的推移执行连续的信息传递。这类模型有几个优点：灵活的表示方法，可以通用于可变的数据维度；能够对复杂的数据分布中的依赖关系进行建模；可逆的和高效的记忆；以及精确和有效地计算数据的可能性。我们在不同领域的一组不同的生成任务上证明了我们模型的有效性：图形生成、图像拼图生成和来自场景图的布局生成。
在这项工作中，我们研究了深度学习的信息瓶颈（IB）理论，该理论提出了三个具体的主张：第一，深度网络经历了两个不同的阶段，包括初始拟合阶段和随后的压缩阶段；第二，压缩阶段与深度网络的优秀泛化性能有因果关系；第三，压缩阶段的发生是由于随机梯度下降的类似扩散行为。 在这里，我们表明这些说法在一般情况下都不成立。通过分析结果和模拟的结合，我们证明了信息平面轨迹主要是所采用的神经非线性的函数：像tanh这样的双面饱和非线性在神经激活进入饱和状态时产生压缩阶段，但线性激活函数和单面饱和非线性如广泛使用的ReLU事实上没有。 此外，我们发现压缩和泛化之间没有明显的因果关系：不压缩的网络仍然能够泛化，反之亦然。接下来，我们通过证明我们可以使用全批梯度下降法而不是随机梯度下降法来复制IB的发现，表明压缩阶段，如果它存在，并不是由训练中的随机性引起。 最后，我们表明，当输入域由任务相关和任务不相关的信息子集组成时，隐藏表征确实压缩了任务不相关的信息，尽管关于输入的整体信息可能随着训练时间单调地增加，而且这种压缩与拟合过程同时发生，而不是在随后的压缩期。
在过去的四年里，神经网络已经被证明容易受到对抗性图像的影响：有针对性的但不易察觉的图像扰动会导致截然不同的预测。我们表明，当被视为输入的一个函数时，对抗性的脆弱性随着训练目标的梯度而增加。 对于目前的大多数网络架构，我们证明这些梯度的L1-norm值随着输入大小的平方根而增长。因此，这些网络随着图像大小的增长而变得越来越脆弱。我们的证明依赖于网络初始化时的权重分布，但广泛的实验证实，我们的结论在常规训练后仍然成立。
神经网络的有效性能在很大程度上取决于优化超参数的有效调整，特别是学习率（以及其时间表）。我们提出了摊销近似优化（APO），它采取的观点是每个优化步骤应该近似地最小化一个近似目标（类似于用于激励自然梯度和信任区域政策优化的目标）。优化超参数被调整为在一次权重更新后最佳地最小化近似目标。 我们表明，理想化版本的APO（其中一个神谕精确地最小化了近似目标）实现了对神经网络的全局收敛到静止点和局部二阶收敛到全局最优。 我们尝试使用APO在训练过程中在线调整各种优化超参数，包括（可能是特定层的）学习率、阻尼系数和梯度方差指数。对于各种网络架构和优化算法（包括SGD、RMSprop和K-FAC），我们表明，通过最小的调整，APO的表现与精心调整的优化器有竞争力。
在过去的几年里，密集的词向量已经证明了它们在许多下游NLP任务中的价值。然而，这种嵌入的维度不容易解释。在一个词向量的d维中，我们将无法理解高值或低值意味着什么。以前解决这个问题的方法主要集中在训练稀疏/非负约束的词嵌入，或后处理标准的预训练词嵌入。 另一方面，我们分析了用单值分解法训练的传统词嵌入，并揭示了类似的可解释性。我们使用了一种受随机矩阵理论启发的新型特征向量分析方法，并表明语义连贯的群体不仅在行空间形成，而且在列空间也形成。这使我们能够将单个词向量维度视为人类可理解的语义特征。
大脑和神经形态芯片中的神经网络赋予系统执行多种认知任务的能力。然而，这两种网络都会经历广泛的物理扰动，从网络边缘的损坏到完全的节点删除，最终可能导致网络失败。 一个关键的问题是了解神经网络的计算特性在响应节点损伤时是如何变化的，以及是否存在修复这些网络以补偿性能下降的策略。在这里，我们研究了两类神经网络的损伤响应特性，即多层感知器（MLPs）和卷积神经网络（CNNs），它们分别被训练用于MNIST和CIFAR-10数据集的图像分类。 我们还提出了一个新的框架，以发现有效的修复策略来拯救受损的神经网络。该框架包括定义损伤和修复操作者，以动态地穿越神经网络的损失景观，目的是映射其突出的几何特征。 我们还发现，一个动态的恢复方案，即网络不断地被损坏和修复，会产生一组对损坏有弹性的网络，因为它可以被迅速抢救。广义上，我们的工作表明，我们可以通过在损坏期间持续应用在线再训练来设计容错的网络，用于生物学和机器学习的实时应用。
从段落中自动生成问题是一个重要且具有挑战性的问题，特别是由于段落的上下文很长。在本文中，我们为从段落中生成问题的任务提出并研究了两个分层模型。具体来说，我们提出了（a）一个具有选择性注意的新型分层BiLSTM模型和（b）一个新型分层Transformer架构，这两个模型都能学习段落的分层表示。我们以一个段落的组成句子为模型，以一个句子的组成词为模型。虽然注意力机制的引入有利于分层BiLSTM模型，但分层Transformer凭借其固有的注意力和位置编码机制也比平面Transformer模型表现得更好。我们使用标准指标对广泛使用的SQuAD和MS MARCO数据集进行了实证评估。结果证明了分层模型比平面模型的整体有效性。从质量上看，我们的分层模型能够产生流畅的相关问题。
许多部署的学习模型是黑盒子：给定输入，返回输出。关于模型的内部信息，如架构、优化程序或训练数据，没有明确披露，因为它可能包含专有信息或使系统更加脆弱。 一方面，我们的工作暴露了黑盒神经网络对不同类型攻击的脆弱性--我们表明，暴露的内部信息有助于产生针对黑盒模型的更有效的对抗性例子。另一方面，这项技术可用于更好地保护私人内容，使其免受使用对抗性例子的自动识别模型的影响。我们的论文表明，实际上很难在白盒和黑盒模型之间划出一条界限。
反向强化学习（IRL）被用来从运行马尔科夫决策过程（MDP）的专家的行动中推断出奖励函数。使用这种技术，连续潜变量（本例中的奖励函数）的难以处理的后验分布被分析性地逼近，以显得尽可能接近先验信念，同时试图重建以当前状态和行动为条件的未来状态。 奖励函数是使用一个著名的深度生成模型，即带有Wasserstein损失函数的条件变异自动编码器（CVAE），因此被称为条件Wasserstein自动编码器-IRL（CWAE-IRL），它可以被分析为后向和前向推理的结合。 这样就可以形成对以前的IRL方法的有效替代，同时对代理人的系统动力学一无所知。在标准基准（如objectworld和pendulum）上的实验结果表明，提出的算法可以在复杂的高维环境中有效地学习潜在的奖励函数。
旅行推销员问题（TSP）是一个著名的组合优化问题，在现实生活中有着广泛的应用。更确切地说，搜索过程被认为是一个马尔科夫决策过程（MDP），其中2-opt局部搜索被用来在一个小的邻域内搜索，而蒙特卡洛树搜索（MCTS）方法（通过模拟、选择和反向传播步骤进行迭代），被用来在一个扩大的邻域内对一些目标行动进行采样。 这种新的范式明显区别于现有的基于机器学习（ML）的解决TSP的范式，后者要么使用端到端的ML模型，要么只是在ML之后应用传统技术进行后期优化。基于两个公共数据集的实验表明，我们的方法在性能上明显优于所有现有的基于学习的TSP算法，证明了其在TSP上的高潜力。更重要的是，作为一个没有复杂手工规则的通用框架，它可以随时扩展到许多其他组合优化问题。
近年来，在设计更好的生成模型方面取得了重大进展。然而，尽管取得了这一进展，最先进的方法在很大程度上仍然无法捕捉数据中复杂的全局结构。例如，建筑物的图像通常包含空间模式，如窗户以固定的间隔重复；最先进的生成方法不容易再现这些结构。此外，我们提出了一个框架，通过利用程序合成生成训练数据来学习这些模型。在合成和真实世界的数据上，我们证明我们的方法在生成和完成包含全局结构的图像方面大大优于最先进的方法。
在机器人任务中学习控制策略需要大量的互动，这是因为学习率小、更新的界限或未知的约束。相反，人类可以在一次失败或意外观察后推断出保护性和安全的解决方案。为了达到类似的性能，我们开发了一种分层贝叶斯优化算法，它复制了在运动控制任务中避免失败的认知推理和记忆过程。一个高斯过程实现了建模和获取函数的采样。这使得快速学习具有较大的学习率，而心理重放阶段确保导致失败的策略区域在采样过程中被抑制。   分层贝叶斯优化方法的特点在一个模拟的和生理的人形姿态平衡任务中得到了评估。我们通过评估训练期间质心的偏差来定量比较人类的学习性能和我们的学习方法。 我们的结果表明，我们可以重现人类受试者在姿势控制任务中的高效学习，这为未来的生理运动控制任务提供了一个可测试的模型。在这些姿势控制任务中，我们的方法在解决任务的交互次数、计算需求和观察到的失败频率上都优于标准贝叶斯优化。
深度强化学习在许多以前困难的强化学习任务中取得了巨大的成功，然而最近的研究表明，深度RL代理也不可避免地容易受到对抗性扰动的影响，类似于分类任务中的深度神经网络。以前的工作大多侧重于无模型对抗性攻击和具有离散行动的代理。 在这项工作中，我们研究了有对抗性攻击的深度RL中的连续控制代理问题，并提出了第一个基于学习模型动态的两步算法。在各种MuJoCo领域（Cartpole、Fish、Walker、Humanoid）的广泛实验表明，在降低代理性能以及将代理推向不安全状态方面，我们提出的框架比基于无模型的攻击基线更有效和高效。
神经网络令人惊讶的泛化的一个主要假设是，梯度下降的动力学使模型偏向于简单的解决方案，通过以复杂度的递增顺序搜索解决方案空间。我们正式定义了递增学习动力学的概念，并推导出深度和初始化的条件，在深度线性模型中出现了这种现象。 我们的主要理论贡献是动态深度分离结果，证明虽然浅层模型可以表现出增量学习动态，但它们需要初始化是指数级的小，这些动态才会呈现出来。然而，一旦模型变得更深，依赖性就会变成多项式，增量学习可以在更自然的环境中出现。我们通过对深度矩阵感应、二次元神经网络和使用对角和卷积线性网络的二元分类进行实验，来补充我们的理论发现。
像图像这样的高维数据的生成模型是一个众所周知的困难和不明确的问题。特别是，如何评估一个学习的生成模型是不明确的。在本文中，我们认为*对抗学习*，由生成对抗网络（GANs）开创，提供了一个有趣的框架，为无监督任务隐含地定义更有意义的任务损失，如生成 "视觉逼真 "的图像。 通过在统计决策理论框架下将GANs和结构化预测联系起来，我们揭示了结构化预测理论的最新进展与GANs中发散的选择之间的联系。我们认为，关于 "难 "和 "易 "学习损失概念的见解可以类推到对抗性发散上。
实验的可重现性和可复制性是机器学习中的关键话题。作者们经常对科学出版物中缺乏可重现性提出关切，以提高该领域的质量。最近，图表示学习领域吸引了广泛研究界的注意，从而产生了大量的作品。因此，已经开发了一些图神经网络模型来有效解决图分类问题。然而，实验程序往往缺乏严谨性，而且很难重现。 为了应对这种令人不安的趋势，我们在一个受控的统一框架内进行了47000多次实验，在9个常见的基准上重新评估了5个流行的模型。 此外，通过将GNN与结构无关的基线进行比较，我们提供了令人信服的证据，即在一些数据集上，结构信息还没有被利用。我们相信这项工作可以为图学习领域的发展做出贡献，为图分类模型的严格评估提供了急需的基础。
数据增强（DA）是防止大型卷积神经网络过度拟合的基础，尤其是在训练数据集有限的情况下。在图像中，DA通常基于启发式变换，如几何或颜色变换。我们的工作不是使用预定义的变换，而是直接从训练数据中学习数据增强，用编码器-解码器结构结合空间变换器网络来变换图像。 变换后的图像仍然属于同一类别，但对于分类器来说是新的、更复杂的样本。我们的实验表明，我们的方法比以前的生成性数据增强方法更好，在训练图像分类器时可与预定义的变换方法相比。
我们考虑了高维数据的信息压缩问题。许多研究考虑了通过不可逆转的转换进行压缩的问题，我们强调了可逆转压缩的重要性。我们引入了新一类基于似然的自动编码器，具有伪双射结构，我们称之为伪不可逆转编码器。
我们利用最近得出的任意深度神经网络的反转方案来开发一个新的半监督学习框架，适用于广泛的系统和问题。 该方法在MNIST上达到了当前最先进的方法，并在SVHN和CIFAR10上提供了合理的性能。通过引入的方法，残差网络首次被应用于半监督任务。对一维信号的实验突出了该方法的通用性。
深度学习已经成为许多计算和分类问题中广泛使用的工具。然而，获得和标记数据，这是强大的结果所需要的，往往是昂贵的，甚至是不可能的。在本文中，我们评估并比较了三种不同的算法方法来处理数据的有限获取。我们展示了每种方法的缺点和优点。一种成功的方法，特别是在一次或几次的学习任务中，是在分类任务中使用外部数据。另一种成功的方法，在半监督学习（SSL）基准中取得了最先进的结果，是一致性正则化。特别是虚拟对抗训练（VAT）已经显示出强大的效果，本文将对此进行研究。一致性正则化的目的是迫使网络在输入或网络本身受到干扰时不改变输出。生成对抗网络（GANs）也显示出强大的经验结果。在许多方法中，GAN架构被用来创建额外的数据，从而提高分类网络的泛化能力。此外，我们考虑使用未标记的数据来进一步提高性能。对GANs和VAT来说，未标记的数据的使用都被研究过。
本文介绍了一种新的神经结构FusionNet，它从三个方面扩展了现有的注意力方法。首先，它提出了一个新的概念 "词的历史 "来描述从最低的词级嵌入到最高的语义级表示的注意力信息。第二，它确定了一个注意力评分函数，更好地利用了 "词的历史 "概念。第三，它提出了一个完全感知的多层次注意力机制，捕捉一个文本（如一个问题）中的完整信息并逐层利用其对应的（如背景或段落）。 我们将FusionNet应用于斯坦福大学问题回答数据集（SQuAD），在撰写本文时（2017年10月4日），它在SQuAD的官方排行榜上取得了单一和集合模型的第一位置。 同时，我们用两个对抗性的SQuAD数据集验证了FusionNet的泛化能力，它在两个数据集上都创造了新的最先进水平：在AddSent上，FusionNet将最佳F1指标从46.6%提高到51.4%；在AddOneSent上，FusionNet将最佳F1指标从56.0%提升到60.7%。
我们提出了一个具有简单马尔科夫结构的新型分层生成模型和一个相应的推理模型，生成模型和推理模型都是使用对抗性学习范式进行训练的。我们证明，分层结构支持学习逐渐抽象的表征，并提供具有不同忠实度的语义重建。由此产生的有语义的分层潜在结构发现在CelebA数据集上得到了体现。 在那里，我们表明，我们的模型以无监督的方式学习的特征优于最佳手工制作的特征。此外，在CelebA的属性预测任务中，与最近的几种深度监督方法相比，提取的特征仍然具有竞争力。最后，我们利用该模型的推理网络，在MNIST数字分类任务的半监督变体中取得了最先进的性能。
守恒定律被认为是自然界的基本定律，它在物理学、化学、生物学、地质学和工程学等许多领域都有广泛的应用，解决与守恒定律相关的微分方程是计算数学的一个重要分支。最近机器学习，特别是深度学习在计算机视觉和自然语言处理等领域的成功，吸引了计算数学界的大量关注，并激发了许多将机器学习与传统方法相结合的有趣工作。 在本文中，我们首次探索了使用深度强化学习解决非线性守恒定律的可能性和好处。作为一个概念证明，我们专注于一维标量守恒定律。我们部署了深度强化学习的机制来训练一个策略网络，它可以决定如何以顺序和空间-时间自适应的方式来逼近数值解决方案。我们将表明，解决守恒定律的问题可以自然地被视为一个顺序决策过程，以这种方式学习的数值方案可以轻松地执行长期准确性。此外，学习到的策略网络被精心设计，以根据解决方案的当前状态确定一个好的局部离散近似，这基本上使提出的方法成为一个元学习方法。换句话说，提出的方法能够模仿人类专家学习如何在特定情况下进行离散。最后，我们将提供关于策略网络如何训练的细节，与一些最先进的数值求解器如WENO方案相比，它的表现如何，以及它的概括性如何。我们的代码在\url{https://github.com/qwerlanksdf/L2D}上被非正常地发布。
我们提出了一个神经渲染架构，帮助变异自动编码器（VAEs）学习解离表征。代替通常用于VAEs解码器的解卷积网络，我们将潜伏向量在整个空间中分层（广播），连接固定的X和Y "坐标 "通道，并应用一个具有1x1步长的完全卷积网络。 这为分离潜伏空间中的位置特征和非位置特征提供了一个架构上的先决条件，但没有为此提供任何明确的监督。我们表明，这种架构，我们称之为空间广播解码器，改善了分离、重建精度和对数据空间中保留区域的概括。 我们表明，空间广播解码器是对最先进的（SOTA）解缠技术的补充，并且在纳入后可以提高它们的性能。
与人类和动物类似，深层人工神经网络表现出关键时期，在此期间，暂时的刺激缺失会损害技能的发展。损害的程度取决于缺失窗口的开始和长度，就像在动物模型中一样，也取决于神经网络的大小。不影响低层次统计的缺失，如图像的垂直翻转，对性能没有持久的影响，可以通过进一步训练克服。 为了更好地理解这一现象，我们使用权重的费希尔信息来衡量训练期间网络各层之间的有效连接。 与此相反的是，信息在训练的早期阶段迅速上升，然后下降，阻止了信息资源的重新分配，我们称之为 "信息可塑性 "的丧失。 我们的分析表明，最初的几个历时对于建立相对于输入数据分布来说是最理想的强连接是至关重要的。这些发现表明，与渐进行为相比，最初的学习瞬态未被充分考虑，在决定训练过程的结果方面起着关键作用。 我们的发现，结合最近文献中的理论结果，也表明遗忘（权重信息的减少）对于在表征学习中实现不变性和脱钩至关重要。最后，关键期不限于生物系统，而是可以在学习系统中自然出现，无论是生物还是人工，由于学习动力学和信息处理产生的基本约束。
我们提出了一种方法，在网络架构的领域上增量学习一个嵌入空间，以便在压缩架构搜索过程中能够仔细选择架构进行评估。给定一个教师网络，我们通过使用贝叶斯优化（BO）来搜索一个压缩的网络架构，在我们提出的嵌入空间上定义一个核函数来选择架构进行评估。我们证明了我们的搜索算法可以显著超越各种基线方法，如随机搜索和强化学习（Ashok et al, 2018）。我们的方法发现的压缩架构也优于最先进的手工设计的紧凑型架构ShuffleNet（Zhang等人，2018）。我们还证明，学到的嵌入空间可以转移到架构搜索的新设置，例如更大的教师网络或不同架构家族的教师网络，而不需要任何培训。
强化学习（RL）问题可以通过两种不同的方式来解决--基于价值函数的方法和基于策略优化的方法--最终得出一个针对给定环境的最优策略。强化学习的最新突破之一是使用深度神经网络作为函数近似器来近似强化学习方案中的价值函数或Q函数。这导致代理人自动学习如何玩像alpha-go这样的游戏的结果，显示出比人类更好的性能。 深度Q-学习网络（DQN）和深度确定性策略梯度（DDPG）是近来显示出最先进成果的两种方法。在RL的许多变体中，一类重要的问题是状态和行动空间是连续的--自主机器人、自主车辆、最优控制都是这类问题的例子，它们可以自然地借给基于强化的算法，并具有连续的状态和行动空间。 在本文中，我们以新颖的方式调整并结合了DQN和DDPG等方法，使其在连续状态和行动空间问题上的表现优于先前的结果。 我们相信这些结果是对快速增长的强化学习成果的宝贵补充，尤其是对连续状态和行动空间问题。
自然语言推理（NLI）任务要求代理人确定自然语言前提和自然语言假设之间的逻辑关系。我们介绍了互动推理网络（IIN），这是一类新型的神经网络架构，能够通过从交互空间分层提取语义特征来实现对句子对的高水平理解。 我们表明，交互张量（注意权重）包含解决自然语言推理的语义信息，而密集的交互张量包含更丰富的语义信息。这种架构的一个实例，密集交互推理网络（DIIN），在大规模NLI copora和大规模NLI alike语料库上展示了最先进的性能。值得一提的是，相对于最强的已发表系统，DIIN在具有挑战性的多流派NLI（MultiNLI）数据集上实现了大于20%的错误减少。
确定性点过程（DPPs）提供了一种优雅和通用的方法来对项目集进行抽样，以平衡所选项目的点状质量和集状多样性。由于这个原因，它们在许多依赖子集选择的机器学习应用中获得了突出地位。 然而，从大小为N的基础集上的DPP采样是一个昂贵的操作，一般需要O(N^3)的预处理成本和O(Nk^3)的子集采样成本。 我们开发了一种基于变换器网络的抑制性注意机制，它捕捉了特征向量之间的不相似性概念。 我们从理论上表明，这样的近似是合理的，因为它保持了抑制或不相似的保证，使DPP如此强大和独特。 在经验上，我们证明了在更昂贵的DPP替代方案下，来自我们模型的样本获得了高的可能性。
本文介绍了一个网络架构，通过特征-度量捆绑调整（BA）来解决结构-运动（SfM）问题，它以特征-度量误差的形式明确地强制执行多视图几何约束。整个管道是可微分的，因此网络可以学习合适的特征，使BA问题更具有可操作性。此外，这项工作引入了一个新的深度参数化，以恢复密集的每像素深度。 整个系统很好地结合了领域知识（即硬编码的多视图几何约束）和深度学习（即特征学习和基础深度图学习）来解决富有挑战性的密集SfM问题。
众所周知，带有函数近似的时差学习是不稳定的。以前的工作，如{citet{sutton2009fast}和{citet{sutton2009convergent}提出的替代目标是稳定的最小化。然而，在实践中，用神经网络进行TD-学习需要各种技巧，如使用更新缓慢的目标网络{citep{mnih2015human}。 在这项工作中，我们提出了一种对TD更新的约束，使目标值的变化最小化。这种约束可以应用于任何TD目标的梯度，并且可以很容易地应用于非线性函数近似。我们通过将我们的技术应用于深度Q-learning，以及没有目标网络的训练来验证这种更新。我们还表明，在Baird的反例上添加这种约束可以使Q-learning不发生分歧。
我们提出了DuoRC，这是一个新的阅读理解（RC）数据集，它激发了现有RC数据集以外的语言理解神经方法的几个新挑战。DuoRC包含186,089个独特的问题-答案对，这些问题-答案对是从7680对电影情节的集合中创建的，其中每一对集合反映了同一电影的两个版本，一个来自维基百科，另一个来自IMDb，由两个不同作者编写。 我们要求众包工作者从一个版本的情节中创建问题，而另一组工作者则从另一个版本中提取或合成相应的答案。DuoRC的这一独特特点是，问题和答案是从叙述同一基本故事的文件的不同版本中创建的，这在设计上确保了从一个版本中创建的问题与另一个版本中包含答案的片段之间很少有词汇重叠。此外，由于两个版本的情节细节、叙述风格、词汇等不同，回答第二个版本的问题需要更深入的语言理解，并纳入给定文本中没有的背景知识。此外，电影情节产生的段落的叙述风格（与现有数据集中的典型描述性段落相反）表现出对多个句子中的事件进行复杂推理的需要。 事实上，我们观察到，最先进的神经RC模型在SQuAD数据集上取得了接近人类的表现，即使与传统的NLP技术相结合来解决DuoRC的挑战，其表现也非常差（DuoRC的F1分数为37.42%，SQuAD数据集为86%）。
我们考虑了带有强化学习（RL）的弱监督结构化预测（SP）问题--例如，给定一个数据库表和一个问题，对该表进行一连串的计算操作，产生一个响应并获得一个二进制的成功-失败奖励。  这一研究方向通过利用RL直接优化SP任务的预期指标而获得成功--例如，问题回答的准确性或机器翻译的BLEU得分。 然而，与常见的RL设置不同，SP中的环境动态是确定的，这一点没有被通常应用的无模型RL方法所充分利用。由于SP模型通常可以完全访问环境动态，我们建议应用基于模型的RL方法，它依赖于规划作为主要的模型组件。 我们用神经程序规划器（NPP）证明了基于规划的SP的有效性，它从预训练的搜索策略中给出一组候选程序，考虑到执行这些程序产生的所有信息，决定哪个程序是最有希望的。我们通过堆叠学习基于预训练的搜索策略的规划模块，对NPP从自然语言（语义解析）的弱监督程序合成进行了评估。
深度学习算法在实现高分类精度的同时，也付出了巨大的计算成本。为了解决这一成本，人们提出了许多量化方案--但这些技术大多集中在量化权重上，而权重的大小相对于激活来说是比较小的。本文提出了一种在训练期间激活的新型量化方案--使神经网络能够在超低精度的权重和激活的情况下很好地工作，而没有任何明显的精度下降。 这项技术，即PArameterized Clipping acTi-vation (PACT)，使用了一个激活剪切参数α，该参数在训练过程中被优化，以找到正确的量化尺度。PACT允许将激活量化到任意的比特精度，同时相对于已发表的最先进的量化方案，取得更好的精度。 我们首次表明，权重和激活都可以量化到4比特的精度，同时在一系列流行的模型和数据集上仍然可以达到与全精度网络相当的精度。我们还表明，在硬件中利用这些降低精度的计算单元可以使推理性能得到超线性的改善，这是因为加速器计算引擎的面积大大减少，同时能够在片上存储器中保留量化的模型和激活的数据。
修剪神经网络参数通常被看作是压缩模型的一种手段，但修剪的动机也是为了防止过度拟合。这一动机特别相关，因为我们发现，尽管有时参数数量大量减少，但各种修剪方法都能提高测试精度，这也许是令人惊讶的。 为了更好地理解这一现象，我们分析了训练过程中修剪的行为，发现修剪对泛化的影响更多地依赖于它所产生的不稳定性（定义为修剪后测试精度的下降），而不是修剪后模型的最终大小。我们证明，即使修剪不重要的参数也会导致这种不稳定性，并显示修剪和通过注入噪声进行规范化之间的相似性，表明基于修剪的泛化改进机制与最近在过度参数化网络中观察到的强泛化是一致的。
用反向传播（backpropagation）训练的神经网络是深度学习的标准算法，它使用权重传输，很容易被现有的基于梯度的对抗性攻击所愚弄，这类攻击是基于输入的某些小扰动来使网络错误分类。 在MNIST上测试，不使用权重传输训练的深度神经网络（1）具有98%的对抗准确性，而使用反向传播训练的神经网络只有0.03%，并且（2）产生不可转移的对抗例子。然而，这一差距在CIFAR-10上有所下降，但特别是对于小于1⁄2的小扰动量级，仍然很重要。
化学反应可以被描述为分子中电子的逐步再分配。因此，反应经常使用 "推箭 "图来描述，该图将这种运动显示为一连串的箭头。我们提出一个电子路径预测模型（ELECTRO）来直接从原始反应数据中学习这些序列。 与直接从反应物分子中预测产物分子不同，学习电子运动模型的好处是：(a)便于化学家解释；(b)包含化学的约束条件，如反应前后的平衡原子数；(c)自然编码化学反应的稀疏性，通常只涉及反应物中少量原子的变化。 我们设计了一种方法，从任何原子映射的反应SMILES字符串的数据集中提取近似的反应路径。我们的模型在美国专利商标局反应数据集的一个重要子集上取得了出色的性能，与最强的基线相比，我们的模型可以恢复化学的基本知识，而不需要明确的训练。
当机器学习模型被用于高风险的决策时，它们应该准确、公平和负责任地进行预测。为了满足这三个要求，当模型没有资格进行预测时，它必须能够输出一个拒绝选项（即说 "我不知道"）。 我们提出了一种学习算法，它考虑了管道中较晚的决策者所持有的潜在偏见。在真实世界的数据集上的实验表明，学习推迟不仅可以使模型更准确，而且可以减少偏见。
分层任务网络（HTN）使用由额外的领域知识指导的分解过程来生成计划，以引导搜索走向计划任务。虽然许多HTN计划器可以在分解过程中调用外部进程（例如调用模拟器接口），但这是一个计算昂贵的过程，因此计划器的实现经常使用非常专业的领域知识以限制调用的数量，以一种临时的方式使用这种调用。 相反，少数能够在规划过程中使用外部调用（通常称为语义附件）的经典规划器通过在问题落地时生成固定数量的落地运算符，以更有限的方式进行调用。 在本文中，我们使用半共同程序为HTN规划开发了语义附件的概念，允许这种程序性定义的谓词将规划过程与规划器之外的自定义统一联系起来。然后，所产生的规划器可以使用这种共同程序作为其回溯机制的一部分，通过状态空间的平行维度（例如，通过数字变量）进行搜索。
最近的文献表明，在语义文本相似性任务中，经过简单后处理的平均词向量的表现优于许多深度学习方法。此外，当平均词向量在大型语料库的监督下进行训练时，它们在标准STS基准上取得了最先进的结果。 在这些见解的启发下，我们进一步推动了词嵌入的极限。我们提出了一种新的模糊词袋（FBoW）的文本表示，它同时包含了词汇中的所有单词，但具有不同的成员程度，这是从词向量之间的相似性得出的。 我们表明，最大集合词向量只是模糊BoW的一个特例，应该通过模糊Jaccard指数而不是余弦相似度进行比较。最后，我们提出了DynaMax，一个完全无监督和非参数的相似度测量方法，根据句子对动态地提取和最大集合好的特征。这个方法既有效又容易实现，但在STS任务上比目前的基线要好很多，甚至与直接优化余弦相似度训练的监督词向量有竞争力。
目前，模仿学习中最先进的结果是由对抗性方法保持的，这些方法迭代地估计学生和专家政策之间的分歧，然后最小化这个分歧，使模仿政策更接近专家行为。然而，仅从观察（没有专家行动标签）中模仿学习的类似技术并没有获得同样普遍的成功。最近在生成模型的对抗性方法方面的工作表明，用于判断真实样本和合成样本之间差异的措施是一种算法设计选择，不同的选择会导致模型性能的显著差异。 包括Wasserstein距离和各种$f$-分歧在内的选择已经在对抗性网络的文献中得到了探索，而最近后一类的选择被研究用于模仿学习。不幸的是，我们发现在实践中，这个现有的使用$f$-分歧的模仿学习框架受到了源于函数逼近和策略梯度强化学习的数值不稳定性的影响。 在这项工作中，我们缓解了这些挑战，并提供了对抗性模仿学习的重新参数化，即f$-分歧最小化，然后进一步扩展该框架以处理仅来自观测的模仿问题。 此外，我们发现，在适当选择f$发散的情况下，我们可以获得从观察中模仿的算法，这些算法优于基线方法，并且在低维观察空间的连续控制任务中更接近专家的表现。在高维观察中，我们仍然观察到有和没有行动标签的明显差距，为未来工作提供了一个有趣的途径。
我们使用基于稳态视觉诱发电位（SSVEPs）的实时认知脑机接口（cBMI）来研究这种联系：枕部EEG电位由有节奏的闪光刺激诱发。 我们实时跟踪SSVEP功率的瞬间波动，当该功率达到（预定的）高或低阈值时，我们就会呈现刺激。我们观察到，在SSVEP功率高（与低）的时间段内，在提示注意的位置触发刺激时，辨别精度（d'）有明显增加。
最近的研究已经证明了深度卷积神经网络对对抗性例子的脆弱性。受图像数据的内在维度远小于其像素空间维度以及神经网络的脆弱性随输入维度增长的观察启发，我们提出将高维输入图像嵌入低维空间以进行分类。 然而，任意地将输入图像投射到低维空间而不进行正则化并不能提高深度神经网络的鲁棒性。我们提出了一个新的框架，即嵌入正则化分类器（ER-Classifier），它通过嵌入正则化提高分类器的对抗性鲁棒性。在几个基准数据集上的实验结果表明，我们提出的框架在对抗强对抗性攻击方法时取得了最先进的性能。
我们研究了在有大量不同任务的情况下，基于深度学习的多任务和少量学习的任务聚类。我们的方法使用跨任务转移性能矩阵衡量任务的相似性。为了克服这些限制，我们提出了一种新的任务聚类算法，以估计基于矩阵完成理论的相似性矩阵。所提出的算法可以在部分观察到的相似性矩阵上工作，只基于具有可靠分数的采样任务对，确保其效率和稳健性。 我们的理论分析表明，在温和的假设下，重建的矩阵以压倒性的概率与潜在的 "真实 "相似性矩阵完全匹配。我们的结果表明，新的任务聚类方法可以发现任务聚类，这对情感分类和对话意图分类任务的多任务学习和少量学习设置都有好处。
在研究量子多体物理学和机器学习中使用的方法之间的相似性已经引起了相当大的关注。特别是，张量网络（TNs）和深度学习架构具有惊人的相似性，以至于TNs可以用于机器学习。以前的结果在图像识别中使用一维TNs，显示出有限的可扩展性和对高键维的要求。在这项工作中，我们训练二维分层TNs来解决图像识别问题，使用从多部分纠缠重正化定理（MERA）衍生的训练算法。 这种方法克服了可扩展性问题，并意味着量子多体物理学、量子信息论和机器学习之间新的数学联系。在训练阶段保持TN的单元性的同时，可以定义TN状态，将图像的每一类优化地编码为量子多体状态。 我们研究了TN状态的量子特征，包括量子纠缠和保真度。我们认为这些量可能是描述图像类别以及机器学习任务的新特性。我们的工作可以进一步应用于识别某些人工智能方法的可能的量子特性。
预测结果和规划与物理世界的互动是机器学习的长期目标。各种这样的任务涉及到连续的物理系统，可以用具有许多自由度的偏微分方程（PDEs）来描述。现有的旨在控制此类系统动态的方法通常局限于相对较短的时间框架或少量的互动参数。 我们建议将问题分成两个不同的任务：规划和控制。为此，我们引入了一个规划最佳轨迹的预测器网络和一个推断相应控制参数的控制网络。我们证明我们的方法成功地发展了对复杂物理系统的理解，并学会了对涉及PDEs（如不可压缩的Navier-Stokes方程）的任务进行控制。
过度参数化的深度网络具有良好的泛化能力，这与随机梯度下降法（SGD）找到的解决方案位于训练损失中平坦、宽阔的最小值有关，在这个最小值中，网络的输出对添加到其参数中的小的随机噪声具有弹性。到目前为止，这个观察结果只被用来为参数为 \textit{stochastic} 或 \textit{compressed} 的神经网络提供泛化保证。在这项工作中，我们提出了一个一般的PAC-Bayesian框架，利用这个观察结果为所学的原始网络提供一个约束--一个确定性的和未压缩的网络。 使我们能够做到这一点的是我们方法中的一个关键的创新点：我们的框架使我们能够表明，如果在训练数据上，权重矩阵之间的相互作用满足某些条件，意味着一个宽的训练损失最小值，这些条件本身{em generalize}到测试数据上矩阵之间的相互作用，从而意味着一个宽的测试损失最小值。 然后，我们在一个假设网络的预激活值不会太小的设置中应用我们的一般框架（尽管我们只在训练数据上假设这一点）。在这个设置中，我们为原始（确定的、未压缩的）网络提供了一个泛化保证，它不随权重矩阵的谱系准则的乘积而变化--这是一个在以前的方法中不可能实现的保证。
训练神经网络使其具有可认证的鲁棒性，对于确保其在对抗性攻击中的安全性至关重要。然而，目前要训练一个既准确又可认证的鲁棒性的神经网络非常困难。在这项工作中，我们向解决这一挑战迈出了一步。 我们证明，对于每一个连续函数$f$，都存在一个网络$n$，使得：(i)$n$可以任意接近$f$，并且(ii)通过$n$对一个区域$B$进行简单的区间约束传播，得到的结果可以任意接近$f$在$B$上的最优输出。我们的结果可以看作是区间认证ReLU网络的通用逼近定理。
在本文中，我们提出了一个有效的框架来加速卷积神经网络。我们利用两种加速方法：剪枝和提示。剪枝可以通过删除层的通道来减少模型的大小。提示可以通过转移教师模型的知识来提高学生模型的性能。我们证明剪枝和提示是相互补充的。 另一方面，从教师网络中修剪出来的模型对学生模型来说是一个很好的初始化，这增加了两个网络之间的可转移性。我们的方法迭代地执行修剪阶段和提示阶段，以进一步提高性能。
对于典型的序列预测问题，如语言生成，最大似然估计（MLE）通常被采用，因为它鼓励预测的序列与地面真实序列最一致，具有最高的发生概率。 然而，MLE侧重于预测序列和黄金标准之间的一次匹配，因此将所有不正确的预测视为同样不正确。 为了解决这个问题，我们通过引入一个额外的Kullback--Leibler发散项来增强MLE的损失，这个发散项是通过比较数据依赖的高斯先验和详细的训练预测得出的。拟议的数据依赖的高斯先验目标（D2GPo）是通过先验的标记拓扑顺序定义的，与通常在平滑MLE训练中采用的数据无关的高斯先验（L2正则化）是两极。 实验结果表明，所提出的方法有效地利用了数据中更详细的先验，并在典型的语言生成任务中提高了性能，包括有监督和无监督的机器翻译、文本总结、讲故事和图像说明。
我们提出了一种自然语言查询的交互式分类方法。我们不是只对给定的自然语言查询进行分类，而是使用一连串的二元和多选问题要求用户提供额外的信息。在每个回合，我们使用一个策略控制器来决定是提出问题还是向用户提供最终答案，并通过最大化系统信息增益来选择最佳问题。 我们的方案可以在没有任何交互数据的情况下启动系统，而是依靠非交互式众包和注解任务。我们的评估显示，交互有助于系统提高其准确性和处理模糊的查询，而我们的方法有效地平衡了问题的数量和最终的准确性。
卷积神经网络（CNN）在识别和表示音频、图像、视频和三维体积方面取得了最先进的性能；也就是说，在输入可以由规则图结构来描述的领域。然而，将CNN推广到像3D网格这样的不规则领域是具有挑战性的。此外，3D网格的训练数据往往是有限的。在这项工作中，我们将卷积自动编码器推广到网格表面。 此外，我们使用最大池化并在网络中引入上采样，以在低维空间中表示网格。我们构建了一个由20,466个具有极端面部表情的高分辨率网格组成的复杂数据集，并使用我们的卷积网格自动编码器对其进行编码。尽管训练数据有限，但我们的方法优于最先进的面部PCA模型，误差降低50%，同时使用的参数减少75%。
计算实例之间的距离是许多时间序列学习算法的核心。因此，在设计有效的时间序列距离度量方面做了大量的工作。我们提出了Jiffy，一个用于多变量时间序列的简单和可扩展的距离度量。 我们的方法是将该任务重塑为一个表征学习问题---而不是设计一个复杂的距离函数，我们使用一个CNN来学习一个嵌入，从而使欧氏距离有效。
前额叶皮层（PFC）是大脑中负责行为重现的部分。受PFC功能和连接性以及人类行为形成过程的启发，我们提出了一种具有行为模块（BM）和相应的端到端训练策略的新型神经网络模块化架构。 这种方法允许有效地学习行为和偏好表示。这种特性对用户建模（如对话代理）和推荐任务特别有用，因为可以学习不同用户状态的个性化表示。 在视频游戏的实验中，结果显示，所提出的方法允许在不同的BMs之间分离主要任务的目标和行为。实验还显示了通过独立学习新的行为模式的网络扩展性。
在无模型强化学习（RL）中，过拟合的一个主要组成部分涉及到这样一种情况，即代理可能错误地将奖励与马尔科夫决策过程（MDP）产生的观察中的某些虚假特征相关联。我们提供了一个分析这种情况的一般框架，我们用它来设计多个合成基准，只修改MDP的观察空间。 当代理人过度适应不同的观察空间时，即使底层的MDP动力学是固定的，我们也称之为观察过度。我们的实验揭示了有趣的特性，特别是关于隐性正则化，也证实了以前RL泛化和监督学习（SL）方面的工作结果。
我们提出了一种能够进行无监督句法结构归纳的神经语言模型。该模型利用结构信息形成更好的语义表征和更好的语言建模。标准的递归神经网络受到其结构的限制，不能有效地利用句法信息。另一方面，树状结构的递归网络通常需要额外的结构监督，其代价是人类专家注释。 在本文中，我们提出了一种新的神经语言模型，称为解析-阅读-预测网络（PRPN），它可以同时从未注释的句子中诱导出句法结构，并利用推断出的结构来学习更好的语言模型。在我们的模型中，梯度可以直接从语言模型损失反向传播到神经解析网络中。实验表明，提出的模型可以发现潜在的句法结构，在单词/字符级语言模型任务中达到最先进的性能。
无监督嵌入学习的目的是在不使用人类注释的标签的情况下从数据中提取良好的表征。由于监督学习所需的大规模标签的收集存在挑战，这类技术显然受到了关注。本文提出了一种全面的方法，称为Super-AND，它是基于锚邻接发现模型。 在Super-AND中定义的多种损失使类似的样本即使在低密度空间内也能聚集起来，并保持特征对增强的不变性。结果，我们的模型在各种基准数据集中优于现有的方法，并在CIFAR-10中使用Resnet18骨干网络达到了89.2%的准确率，比最先进的方法高出2.9%。
在数字组织病理学分析的情况下，训练有素的病理学家必须在多个缩放级别上审查极端数字分辨率（100,000^2像素）的大量整张幻灯片图像，以便从数百万个细胞中找到异常区域，或者在某些情况下找到单个细胞。 深度学习在这个问题上的应用不仅受到小样本量的阻碍，因为典型的数据集只包含几百个样本，而且还受到用于训练可解释的分类和分割模型的地面真实的本地化注释的影响。 我们提出了一种在训练过程中可用的疾病方法。即使没有像素级的注释，我们也能在Camelyon-16淋巴结转移检测挑战中表现出与用强注释训练的模型相媲美的性能。我们通过使用预训练的深度卷积网络、特征嵌入以及通过顶级实例和负面证据学习来实现这一点，这是一种来自语义分割和物体检测领域的多实例学习技术。
大规模多标签预测/分类问题出现在医疗保健或生物学等环境中，在这些环境中进行非常精确的预测是非常有用的。大规模多标签问题的一个挑战是，标签往往有一个长尾的频率分布，导致稀有标签的正面例子很少。我们提出了一个解决这个问题的方法，修改神经网络的输出层，创建一个贝叶斯网络的sigmoids，利用标签之间的本体关系，帮助在稀有标签和更常见标签之间分享信息。 我们将这种方法应用于疾病预测（ICD-9代码）和蛋白质功能预测（基因本体术语）这两个大规模的多标签任务，并在每个标签的AUROC和平均精度方面获得了显著的改善。
生成式对抗网络在各种使用情况下使数据生成成为可能，但在复杂的高维分布情况下，由于收敛问题和模式崩溃的出现，训练它们是很困难的。 切片Wasserstein GANs，特别是Max-Sliced Wasserstein距离的应用，使得在训练过程中以有效和稳定的方式近似Wasserstein距离成为可能，并有助于缓解这些架构的收敛问题。该方法将样本分配和距离计算转变为对样本的一维投影进行排序，从而充分近似高维Wasserstein距离。在本文中，我们将证明通过对样本进行排序来逼近Wasserstein距离并不总是最佳方法，对真假样本的贪婪分配可以导致更快的收敛和对原始分布的更好逼近。
终身机器学习的重点是在不忘记旧任务的情况下适应新任务，而少量学习则是在少量数据的情况下努力学习单一任务。这两个不同的研究领域对人工通用智能至关重要，然而，它们现有的研究在训练模型时都以某种方式假设了一些不切实际的设置。对于终身学习，推理时间内传入任务的性质（或数量）在训练时被假设为已知。 另一方面，人类可以在不考虑上述假设的情况下完成这些学习任务。受人脑工作方式的启发，我们提出了一种新的模型，称为慢速思考学习（STL），它通过在运行时迭代考虑当前任务和以前看到的任务之间的相互作用来进行复杂（和稍慢）的预测。在进行实验后，结果实证证明了STL对于更现实的终身学习和少数次学习设置的有效性。
超网络是一种元神经网络，它以端到端的可区分方式为主神经网络生成权重。尽管从多任务学习到贝叶斯深度学习都有广泛的应用，但到目前为止，优化超网络的问题还没有被研究。 我们观察到经典的权重初始化方法，如Glorot & Bengio（2010）和He等人（2015），当直接应用于超网络时，不能为主网产生正确规模的权重。我们开发了超网络中权重初始化的原则性技术，并表明它们导致更稳定的主网权重，更低的训练损失，以及更快的收敛。
对于双向的图像-文本联合建模，我们开发了变异异构编码器（VHE）随机生成对抗网络（GAN），这是一个多功能的深度生成模型，将概率文本解码器、概率图像编码器和GAN整合到一个连贯的端到端多模态学习框架中。VHE随机GAN（VHE-GAN）对图像进行编码以解码其相关文本，并将变异后验作为随机性来源送入GAN图像发生器。 我们将三个现成的模块，包括一个深度主题模型、一个梯形结构的图像编码器和StackGAN++，插入到VHE-GAN中，它已经取得了有竞争力的性能。这进一步激励了VHE-raster-scan-GAN的发展，它不仅以多尺度低到高分辨率的方式，而且以分层语义的粗到细的方式生成照片般的真实图像。 通过捕捉层次语义和视觉概念并将其与端到端的训练联系起来，VHE-raster-scan-GAN在各种图像-文本多模态学习和生成任务中实现了最先进的性能。
目前的经典规划器在寻找（非最佳）计划方面非常成功，即使是大的规划实例也是如此。为了解决这个问题，我们引入了一种部分接地的方法，在完全接地不可行的情况下，只对任务的一个投影进行接地。我们提出了一种指导机制，对于一个给定的领域，通过使用现成的机器学习方法来识别任务的相关部分，从而找到一个计划。
在本文中，我们引入了一种新的方法来解释递归神经网络（RNN），特别是细胞水平的长短时记忆网络（LSTM）。我们提出了一个系统的管道，使用响应表征方法来解释网络中的单个隐藏状态动态。通过分析它们的解耦阶跃和正弦响应的一组可解释指标来计算单个细胞对网络输出的排名贡献。 因此，我们的方法能够独特地识别具有深刻动态的神经元，通过消融分析量化动态特性和测试准确性之间的关系，并解释网络容量对网络动态分布的影响。最后，我们通过评估一系列不同的基准序列数据集，证明了我们方法的通用性和可扩展性。
几十年来对空间导航的神经编码的研究揭示了一系列不同的神经反应特性。哺乳动物大脑的Entorhinal Cortex（EC）包含了丰富的空间关联物，包括使用方块图案编码空间的网格细胞。然而，这些空间表征的机制和功能意义在很大程度上仍然是神秘的。作为了解这些神经表征的一种新方法，我们训练循环神经网络（RNNs）在2D舞台上根据速度输入执行导航任务。 令人惊讶的是，我们发现在训练的网络中出现了网格状的空间反应模式，同时还有表现出其他空间相关性的单元，包括边界细胞和带状细胞.所有这些不同功能类型的神经元都在实验中被观察到.网格状和边界细胞出现的顺序也与发育研究的观察结果一致.总之，我们的结果表明，网格细胞、边界细胞和其他在EC中观察到的细胞可能是有效表示空间的自然解决方案，因为神经回路中主要是循环连接。
音乐的音源分离是隔离来自不同乐器的贡献或音干的任务，这些乐器单独录制并排列在一起形成一首歌曲。虽然直接生成波形的端到端模型在许多音频合成问题上是最先进的，但最好的多乐器音源分离模型在幅度谱上生成掩码，并实现远高于当前端到端、波形到波形模型的性能。 我们提出了对一个新架构的深入分析，我们将其称为Demucs，该架构基于一个（转置的）卷积自动编码器，在瓶颈层有一个双向LSTM和U-Networks（Ronneberger等人）中的跳过连接。2015）。与最先进的波形对波形模型Wave-U-Net（Stoller等人，2018）相比，我们的方法除了双向LSTM之外，主要特点是使用反演卷积层而不是上采样卷积块，使用门控线性单元，通道数量随深度呈指数增长，并对权重进行新的仔细初始化。 MusDB数据集上的结果显示，我们的架构实现了比最好的波形对波形的竞争者高出近2.2个点的信噪比（从3.2到5.4个SDR）。这使得我们的模型在这个数据集上与最先进的性能相匹配，弥补了在谱图上操作的模型和端到端方法之间的性能差距。
尽管具有挑战性，但大型连接学习者网络中的策略配置文件评估对于实现下一波机器学习应用至关重要。最近，$alpha$-Rank，一种进化算法，被提出来作为多代理系统中联合策略配置文件排名的解决方案。 在本文中，我们正式证明了这种说法是没有根据的。事实上，我们表明$alpha$-Rank在代理数量上表现出指数级的复杂性，阻碍了它在有限数量的联合配置文件之外的应用。 意识到这种限制，我们提出了一个可扩展的评估协议，我们称之为$alpha^{alpha}$-Rank。我们的方法结合了进化动力学与随机优化和双神谕，以获得具有线性（代理数量）时间和内存复杂性的可扩展排名。 我们的贡献使我们第一次对多代理系统进行了大规模的评估实验，我们在规模为$/mathcal{O}(2^{25})$的大型联合策略配置文件上显示了成功的结果（即。$approx \text{$3300万个策略}$）--这是一个使用现有技术无法评估的环境。
在这项工作中，我们提出了DivideMix，一个通过利用半监督学习技术在有噪声标签的情况下学习的新框架。 特别是，DivideMix用混合模型对每个样本的损失分布进行建模，以动态地将训练数据分为带有干净样本的标记集和带有噪声样本的未标记集，并以半监督的方式对标记和未标记的数据进行模型训练。 为了避免确认偏差，我们同时训练两个分歧网络，其中每个网络使用来自另一个网络的数据集划分。在半监督训练阶段，我们通过在有标签和无标签的样本上分别进行标签共同精炼和标签共同猜测来改进MixMatch策略。在多个基准数据集上的实验表明比最先进的方法有很大的改进。代码可在https://github.com/LiJunnan1992/DivideMix 。
我们提出了一种新的算法来训练一个强大的神经网络来对抗对抗性攻击。我们的算法是由以下两个想法促成的。首先，尽管最近的工作表明，融合随机性可以提高神经网络的鲁棒性（Liu 2017），但我们注意到，盲目地将噪声添加到所有层中并不是纳入随机性的最佳方式。相反，我们在贝叶斯神经网络（BNN）的框架下对随机性进行建模，以可扩展的方式正式学习模型的后验分布。其次，我们在BNN中制定了mini-max问题，以学习对抗性攻击下的最佳模型分布，导致对抗性训练的贝叶斯神经网络。 实验结果表明，所提出的算法在强攻击下实现了最先进的性能。在带有VGG网络的CIFAR-10上，我们的模型在PGD攻击下，与对抗性训练（Madry 2017）和随机自组装（Liu，2017）相比，在0.035失真的情况下，准确率提高了14%，在ImageNet的一个子集上差距甚至更大。
联合学习将模型训练分布在众多代理之间，这些代理在隐私问题的指导下，使用其本地数据进行训练，但只共享模型参数更新，以便在服务器上进行迭代汇总。 在这项工作中，我们探讨了模型中毒攻击对联合学习的威胁，这种攻击是由单一的、非联合的恶意代理发起的，其目的是使模型以高置信度对一组选定的输入进行错误分类。我们探讨了一些策略来进行这种攻击，首先是简单地提高恶意代理的更新，以克服其他代理更新的影响。 为了提高攻击的隐蔽性，我们提出了一种交替最小化策略，交替优化训练损失和对抗性目标。最后，我们使用一套可解释性技术来生成良性和恶意模型决策的视觉解释，并表明这些解释在视觉上几乎是不可区分的。 我们的结果表明，即使是一个高度受限的对手也可以进行模型中毒攻击，同时保持隐蔽性，从而强调了联合学习环境的脆弱性和开发有效防御策略的必要性。
尽管在语音识别方面取得了快速的进展，但目前的模型对其输入的表面扰动仍然很脆弱。少量的噪声可以破坏一个原本最先进的模型的性能。 在本文中，我们假设一个干净的例子和其表面干扰的对应物不应该仅仅映射到同一类别---它们应该映射到相同的表示。在每个训练迭代中，对于每个训练实例，我们都会对一个有噪声的对应实例进行采样。然后，我们在每一层（在某个选定的层之上）应用一个惩罚项来强迫匹配的表征。我们在LibriSpeech数据集上的主要结果如下：(i) IRL在 "干净的"（3. 3% vs 6.5%）和 "其他"（11.0% vs 18.1%）的测试集；(ii) 在几个域外噪声设置（不同于训练时的设置）上，IRL的优势更加明显。仔细的消减证实了我们的结果并不仅仅是由于所选层的激活收缩。
在本文中，我们设计了一个用于音高检测的谐波声学模型。这个模型以一种方式安排了传统卷积和稀疏卷积，使得由稀疏卷积捕获的全局谐波模式是由传统卷积层捕获的足够数量的局部模式组成。 最令人印象深刻的是，当在MAPS上进行简单的数据增强训练时，上面有一个LSTM层的谐波模型超过了在MAESTRO数据集上训练的最新的、更复杂的音高检测系统，该数据集的训练分割比MAPS的训练分割大一个数量级。
学习领域不变性表示是领域泛化的主流方法。然而，以前基于领域不变性的方法忽略了类对领域的基本依赖性，这是造成分类准确性和不变性之间的权衡的原因。 本研究提出了一种新的方法{em adversarial feature learning under accuracy constraint (AFLAC)}，该方法在不影响准确性的范围内最大限度地提高领域不变性。实证验证表明，AFLAC的性能优于基线方法，支持考虑依赖性的重要性和所提方法克服问题的功效。
深度生成模型的最新进展导致了在合成高质量图像方面的显著进步。在它们成功应用于图像处理和表示学习之后，下一个重要步骤是考虑视频。学习视频的生成模型是一项更难的任务，除了物体的视觉呈现之外，还需要一个模型来捕捉场景的时间动态。 虽然最近的视频生成模型取得了一些成功，但由于缺乏考虑视觉质量、时间连贯性和样本多样性的定性指标，目前的进展受到了阻碍。为此，我们提出了弗雷谢视频距离（FVD），一个基于FID的视频生成模型的新指标。
尽管在深度学习方面取得了进展，但人工神经网络的学习方式与人类不同。今天，神经网络在对多个任务进行联合训练时可以学习这些任务，但在一次呈现一个任务时却不能保持对所学任务的表现--这种被称为灾难性遗忘的现象是神经网络能够从传入数据中持续学习之前需要克服的基本挑战。在这项工作中，我们从人类记忆中获得灵感，开发了一个能够从连续传入的任务中持续学习的架构，同时避免了灾难性遗忘。 具体来说，我们的模型包括一个双存储器架构，以模拟人脑中互补的学习系统（海马体和新皮层），并通过对过去经验的生成性重放来保持巩固的长期记忆。我们（i）证实了我们的主张，即重放应该是生成性的，（ii）通过实验显示生成性重放和双存储器的好处，以及（iii）证明即使对于低容量的小模型，性能保持也有改善。我们的架构显示了人类记忆的许多重要特征，并对人类的睡眠和学习之间的联系提出了见解。
大多数关于终身学习的研究适用于图像或游戏，但不适用于语言。我们提出了LAMOL，一种简单而有效的基于语言建模的终身语言学习（LLL）方法。LAMOL复制以前任务的伪样本，同时不需要额外的内存或模型容量。具体来说，LAMOL是一个语言模型，同时学习解决任务和生成训练样本。 当模型为一个新的任务进行训练时，它产生以前任务的伪样本，与新任务的数据一起进行训练。结果表明，LAMOL可以防止灾难性的遗忘，而没有任何不妥协的迹象，只用一个模型就可以连续完成五个非常不同的语言任务。总的来说，LAMOL比以前的方法要好很多，只比多任务处理差2-3%，这通常被认为是LLL的上限。源代码可在https://github.com/jojotenya/LAMOL。
深度学习自然语言处理模型通常使用矢量词嵌入，如word2vec或GloVe，来表示单词。如果一个离散的单词序列被表示为连续矢量序列，那么它可以更容易地与下游神经层整合。 然而，存储和访问字典中所有单词的嵌入向量需要大量的空间，并可能使GPU内存有限的系统受到影响。这里，我们使用了受量子计算启发的方法，提出了两种相关的方法，即word2ket和word2ketXS，用于在训练和推理过程中以高效的方式存储单词嵌入矩阵。我们的方法在实际自然语言处理任务中实现了存储嵌入所需空间的百倍或更多的减少，而准确性几乎没有相对下降。
人类语言的一个突出方面是它的构成性，这使我们能够用有限的词汇来描述复杂的环境。以前，已经证明神经网络代理可以学习用一种高度结构化的、可能是构成性的语言来交流，基于分解的输入（如手工设计的特征）。然而，人类不会学习基于良好的总结的特征来交流。在这项工作中，我们训练神经代理同时从原始图像像素发展视觉感知，并学习用一串离散符号交流。 代理人玩了一个图像描述游戏，其中图像包含诸如颜色和形状等因素。我们使用obverter技术训练代理人，其中代理人自省以产生最大化其自身理解的信息。通过定性分析、可视化和零点测试，我们表明代理人可以从原始图像像素中发展出一种具有组成特性的语言，给与来自环境的适当压力。
预测一组可能的但不同的代理的未来行为的能力（例如，行人的未来轨迹）对于安全关键的感知系统（例如，自主车辆）是至关重要的。特别是，由系统产生的一组可能的未来行为必须是多样化的，以考虑到所有可能的结果，从而采取必要的安全预防措施。保持一组最可能的未来结果是不够的，因为这组结果可能只包含一个主导的单一结果（主要模式）的扰动。） 虽然生成模型，如变异自动编码器（VAE）已被证明是学习未来轨迹分布的有力工具，但从所学的隐含似然模型中随机抽取的样本可能并不多样--似然模型来自训练数据分布，样本将集中在数据的主要模式周围。 在这项工作中，我们建议学习多样性抽样函数（DSF），该函数可以生成一组多样化但可能的未来轨迹。具体来说，识别多样化样本集的过程被称为DSF参数估计。为了学习DSF的参数，轨迹样本的多样性通过基于决定性点过程（DPP）的多样性损失进行评估。） 梯度下降是在DSF参数上进行的，这反过来又会移动样本集的潜伏代码，以找到一组最佳的多样化但可能的轨迹。我们的方法是DPP的一个新的应用，在连续空间中优化一组项目（预测轨迹）。我们在低维二维轨迹数据和高维人类运动数据上证明了我们的方法产生的轨迹的多样性。
越来越多的证据表明，预训练对神经网络语言理解模型是有价值的，但我们还没有清楚地了解预训练目标的选择如何影响模型学习的语言信息的类型。考虑到这一点，我们比较了四个目标--语言建模、翻译、跳过思考和自动编码--它们诱导句法和语篇信息的能力，保持训练数据的类型和数量不变。 我们发现，来自语言模型的表征在我们的句法辅助预测任务中始终表现最好，即使在相对较少的数据量上进行训练，这表明语言建模可能是需要句法信息的迁移学习应用的最佳数据丰富的预训练任务。我们还发现，一个随机初始化的冻结模型可以在我们的辅助任务中表现得非常好，但当辅助任务的训练数据量减少时，这种效果就会消失。
我们考虑的问题是生成满足物理约束的配置，以实现最佳的材料纳米图案设计，其中需要同时满足多种（而且往往是相互冲突的）特性。 例如，考虑在热阻、导电性和机械稳定性之间进行权衡，以设计一个具有最佳热电效率的纳米多孔模板。 为此，我们利用后验正则化框架，表明这个约束满足问题可以被表述为从吉布斯分布中取样。 主要的挑战来自于这些物理约束的黑箱性质，因为它们是通过解决高度非线性PDEs获得的。为了克服这些困难，我们为黑箱采样引入了基于代理的约束朗文动力学。 第二种方法是用深度神经网络逼近Langevin动力学中的梯度，使我们能够使用代用模型的有效采样策略。我们证明了当目标分布是对数凹陷和平滑时，这两种方法的收敛性。我们展示了这两种方法在设计最佳纳米多孔材料配置中的有效性，目标是产生具有低导热性和合理机械稳定性的纳米图案模板。
人们对几何启发的嵌入学习层次结构、部分秩序和格子结构的兴趣越来越大，自然而然地应用于递归关系数据，如entailment图。最近的工作将这些想法从确定的层次结构扩展到概率校准的模型，这使得从不确定的监督中学习并推断出概念之间的软包容性，同时保持层次嵌入模型的几何归纳偏见。 我们建立在Vilnis等人（2018）的箱格模型上，该模型在通过重叠的层级集合建模软内涵方面显示出有希望的结果，参数为高维超矩形（箱）。然而，箱的硬边缘给基于梯度的标准优化带来了困难；该工作采用了一个特殊的代理函数来处理不相交的情况，但我们发现这种方法很脆弱。 在这项工作中，我们提出了一个新的分层嵌入模型，其灵感来自于使用高斯卷积在盒子上将盒子嵌入放宽为参数化密度函数。我们的方法提供了一个替代原始格子度量的代用函数，提高了在不相交情况下的优化的稳健性，同时也保留了与原始格子有关的理想属性。 我们展示了在WordNet hypernymy预测、Flickr标题嵌套和基于MovieLens的市场篮子数据集上的增加或匹配性能。我们显示在稀疏数据的情况下有特别明显的改进，其中许多条件概率应该很低，因此盒子应该是几乎不相交的。
我们提出了一种弱监督的数据增强方法，以改善一个具有挑战性的领域中的命名实体识别（NER）：从科学文献中提取生物医学实体（如蛋白质）。最后，我们在增强的训练集上反复训练我们的NNER模型，包括种子、参考集的例子和弱标签的例子，从而得到精炼的标签。我们通过经验表明，这种增强的引导过程显著提高了NER的性能，并讨论了影响该方法有效性的因素。
量子机器学习方法有可能促进使用极其庞大的数据集进行学习。虽然用于训练机器学习模型的数据的可用性正在稳步增加，但通常情况下，收集特征向量比获得相应的标签要容易得多。解决这个问题的方法之一是使用半监督学习，它不仅利用标记的样本，而且还利用未标记的特征向量。 在这里，我们提出了一种用于训练半监督核支持向量机的量子机器学习算法。该算法使用基于量子样本的哈密尔顿模拟的最新进展来扩展现有的量子LS-SVM算法以处理损失中的半监督项，同时保持与量子LS-SVM相同的量子速度。
深度神经网络已经成为众多机器学习任务中最先进的模型。然而，仍然缺乏对网络架构设计的一般指导。在我们的工作中，我们将深度神经网络设计与数值微分方程联系起来。我们表明，许多有效的网络，如ResNet、PolyNet、FractalNet和RevNet，可以被解释为微分方程的不同数值离散化。 我们可以利用数值分析的丰富知识来指导我们设计新的和潜在的更有效的深度网络。作为一个例子，我们提出了一个线性多步骤架构（LM-架构），其灵感来自于解决常微分方程的线性多步骤方法。 尤其是在CIFAR和ImageNet上，LM-ResNet/LM-ResNeXt（即在ResNet和ResNeXt上分别应用LM架构得到的网络）在可训练参数数量相当的情况下，可以达到明显高于ResNet和ResNeXt的精度。 最后，我们还在训练过程中建立了随机控制和噪声注入之间的联系，这有助于提高网络的通用性。此外，通过将随机训练策略与随机动态系统联系起来，我们可以很容易地将随机训练应用于具有LM架构的网络。作为一个例子，我们将随机深度引入LM-ResNet，并在CIFAR10上实现了对原始LM-ResNet的显著改善。
将设计师创建的图形用户界面截图转化为计算机代码是开发人员为了构建定制软件、网站和移动应用程序而进行的一项典型任务。在本文中，我们表明可以利用深度学习方法来训练一个模型，从单一的输入图像中自动生成代码，在三个不同的平台（即iOS、Android和基于网络的技术）上，准确率超过77%。
计算机视觉任务，如图像分类、图像检索和少量学习，目前主要由欧氏和球形嵌入主导，因此关于类的归属或相似度的最终决定是使用线性超平面、欧氏距离或球形测地距离（余弦相似度）。在这项工作中，我们证明在许多实际场景中，双曲嵌入提供了一个更好的选择。
高维时间序列在许多领域都很常见。由于人类的认知没有被优化到可以在高维空间工作，这些领域可以从可解释的低维表示中受益。然而，大多数时间序列数据的表示学习算法都难以解释。这是因为从数据特征到表示的突出属性的非直观映射和随时间变化的非平稳性。 为了解决这个问题，我们提出了一个新的表征学习框架，该框架建立在可解释的离散降维和深度生成建模的思想之上。这个框架使我们能够学习时间序列的离散表征，从而产生具有卓越聚类性能的平滑和可解释嵌入。 我们引入了一种新的方法来克服离散表征学习中的非差异性，并提出了一种基于梯度的传统自组织图算法，其性能比原来的算法更强。此外，为了允许对我们的方法进行概率解释，我们在表征空间中整合了马尔科夫模型。这个模型揭示了时间过渡结构，进一步提高了聚类性能，并提供了额外的解释见解以及对不确定性的自然表示。 我们在静态（Fashion-）MNIST数据、线性插值（Fashion-）MNIST图像的时间序列、具有两个宏观状态的混沌Lorenz吸引子系统以及eICU数据集上具有挑战性的现实世界医疗时间序列应用中评估了我们的模型的聚类性能和可解释性。
我们提出了意义偏移卷积神经网络，这是一个用于回归多变量异步时间序列的深度卷积网络架构。 该模型受到标准自回归（AR）模型和循环神经网络中使用的门控机制的启发。 它涉及一个类似于AR的加权系统，其中最终的预测器是作为调整后的回归器的加权和得到的，而加权是通过卷积网络学习的与数据有关的函数。该架构是为异步时间序列的应用而设计的，并在这样的数据集上进行了评估：一个对冲基金的信贷衍生指数的超过200万个报价的专有数据集，一个人工生成的噪声自回归序列和家庭电力消耗数据集。  与卷积和递归神经网络相比，所提出的架构取得了有希望的结果。数值实验的代码和架构的实现将在网上共享，以使研究可重复。
MixUp是一种数据增强方案，其中一对训练样本及其相应的标签使用线性系数进行混合。没有标签混合，MixUp成为一个更传统的方案：输入样本被移动，但其原始标签被保留。 我们定义了未绑定的MixUp（UMixUp），它是MixUp的超集，其中训练标签与相应的样本的线性系数不同。我们表明，在相同的温和条件下，未绑定的MixUp收敛于整个DAT方案类别。 出于对UMixUp既是MixUp的泛化又是对抗性训练形式的理解，我们用不同的数据集和损失函数进行了实验，表明UMixUp提供了比MixUp更好的性能。总之，我们对MixUp提出了一个新的解释，即它属于一个高度类似于对抗性训练的类别，在此基础上我们引入了一个简单的泛化，它优于MixUp。
计划识别的目的是寻找目标计划，以最好地解释基于计划库和/或领域模型的观察到的行动。尽管以前的计划识别方法很成功，但它们大多依赖于正确的行动观察。最近在视觉活动识别方面的进展有可能使自动视频监控等应用成为可能。这种问题的有效方法需要有能力从视频信息中识别出代理人的计划。传统的计划识别算法依赖于对详细的计划领域模型的访问。最近一个有希望的方向是直接从观察到的活动序列中学习近似的（或浅层）领域模型。 这样的计划识别方法希望将观察到的行动序列作为输入。然而，视觉推理结果往往是嘈杂和不确定的，通常表示为可能行动的分布。在这项工作中，我们开发了一个视觉计划识别框架，通过从不确定的视觉数据中学习的近似领域模型来识别计划。
我们考虑使用语料库作为虚拟知识库（KB）来回答复杂的多跳问题的任务。特别是，我们描述了一个神经模块，DrKIT，它像一个虚拟的KB一样遍历文本数据，软性地遵循语料库中实体的提及关系的路径。在每一步操作中，使用稀疏矩阵TFIDF指数和最大内积搜索（MIPS）在上下文表示的特殊索引上的组合。 这个模块是可微分的，因此整个系统可以使用基于梯度的方法，从自然语言输入开始，进行完全的端到端训练。我们还描述了一个索引提及编码器的预训练方案，通过使用现有的知识库生成硬的负面例子。我们表明，DrKIT在MetaQA数据集的3跳问题上提高了9个点的准确性，将基于文本和基于KB的最先进技术之间的差距减少了70%。DrKIT也非常高效，每秒处理的查询次数比现有最先进的QA系统多10倍。
尽管取得了巨大的成功，传统的因式分解算法通常不支持特征（如。矩阵因式分解），或者它们的复杂度随着特征数量的增加而呈四级变化（如因式分解机）。另一方面，神经方法允许大型特征集，但往往是为特定的应用而设计的。我们提出了新的深度因式分解方法，允许高效和灵活的特征表示。 例如，我们能够用自然语言描述项目，其复杂度与词汇量成线性关系--这使得对未见过的项目的预测成为可能，并避免了冷启动问题。我们表明，我们的架构可以概括以前发表的一些单一用途的神经架构。我们的实验表明，与浅层方法相比，训练时间和准确性都有所提高。
增强现实（AR）可以通过使用 "情景指示 "来协助物理任务，如物体组装。这些指示可以是视频、图片、文本或指导性动画的形式，其中最有用的媒体在很大程度上取决于用户和任务的性质。我们的工作支持为组装任务编写AR教程，除了简单地执行任务本身，几乎没有开销。 所提出的系统，AuthAR，通过在作者组装物理部件时自动生成AR教程的关键部件，减少了建立交互式AR教程所需的时间和精力。此外，该系统指导作者在教程中添加视频、图片、文本和动画的过程。这种同时组装和生成教程的方法允许编写适合不同终端用户偏好的便携式教程。
监测ICU中的病人是一项具有挑战性和高成本的任务。因此，预测病人在ICU住院期间的状况有助于提供更好的急性护理和规划医院的资源。机器学习在ICU管理方面的研究一直在不断进步，这些工作大多集中在使用ICU仪器记录的时间序列信号。 在我们的工作中，我们表明增加临床笔记作为另一种方式，可以提高模型在三个基准任务中的性能：院内死亡率预测、失代偿模型和住院时间预测，这在ICU管理中起着重要作用。虽然时间序列数据是定期测量的，但医生笔记是在不定期的情况下记录的，这使得它们一起建模具有挑战性。我们提出一种方法对它们进行联合建模，与基线时间序列模型相比，在各基准任务中取得相当大的改进。
现有的深度多代理强化学习（MARL）的工作主要集中在协调合作代理，共同完成某些任务。因此，领导者，即公司或联盟的经理，需要向追随者提供奖金以实现有效的协调，我们称之为昂贵的协调。昂贵的协调的主要困难在于：i）领导者在分配奖金时必须考虑长期效果并预测追随者的行为；ii）追随者之间复杂的相互作用使得训练过程难以收敛，特别是当领导者的政策随时间变化时。在这项工作中，我们通过基于事件的深度RL方法解决这个问题，我们的主要贡献有三个方面。 (1) 我们将领导者的决策过程建模为半马尔科夫决策过程，并提出了一种新颖的基于事件的政策梯度来学习领导者的长期政策。 (2) 我们利用领导者与追随者的一致性方案，设计了一个追随者感知模块和一个追随者特定注意力模块来预测追随者的行为并对其行为做出准确反应。 (3) 我们提出了一种基于行动抽象的策略梯度算法，以减少追随者的决策空间，从而加速追随者的训练过程。在资源收集、导航和捕食者-猎物游戏中的实验显示，我们的方法大大超过了最先进的方法。
最近的工作研究了深度强化学习代理中语言的出现，这些代理必须合作解决一个任务，特别令人感兴趣的是导致语言是组成性的因素--即。进化语言学家发现，除了像在深度学习中已经研究过的那些结构性先验之外，语言的代际传递动态也大大促进了构成性的出现。在本文中，我们将这些文化进化动态引入语言的出现，通过定期替换群体中的代理来创造一个知识缺口，隐性地诱导语言的文化传递。
基于我们的观察，全连接层或卷积层的单一特征图的奇异值存在急剧下降，而且串联的特征向量的维数几乎等于每个特征图上的维数之和，我们提出了一种基于奇异值分解（SVD）的方法来估计典型卷积神经网络VGG19的深度流形的维数。 我们从ImageNet中选择了三个类别，即波斯猫、集装箱船和火山，并通过目标图像的切线空间确定深层流形的局部维度。 通过几种增强方法，我们发现高斯噪声法更接近内在维度，因为通过向图像添加随机噪声，我们在任意维度上移动，当增强图像的特征矩阵的等级不增加时，我们就非常接近流形的局部维度。我们还根据每个maxpooling层的切线空间来估计深度流形的维度。 我们的结果表明，不同类别的维度相互接近，并沿着卷积层和全连接层快速下降。此外，我们表明，维度在Conv5层内快速下降。我们的工作为深度神经网络的内在结构提供了新的见解，有助于揭开深度神经网络黑箱的内部组织。
像BERT这样的大型预训练变形器在许多NLP任务中都有巨大的效果。然而，这些大容量模型的推理速度太慢，成本太高。变形器本质上是一堆自我注意层，它使用整个输入序列作为其上下文对每个输入位置进行编码。然而，我们发现，可能没有必要在所有层上应用这种昂贵的全序列自我注意。基于这一观察，我们提出了对一个预训练的转化器的分解，允许低层独立处理输入的片段，以实现并行和缓存。我们表明，由于这种分解造成的信息损失可以在微调期间通过辅助监督在上层恢复。 我们在问题回答、句子相似性和自然语言推理等五个不同的成对输入任务上用预先训练好的BERT模型评估了去分解。 结果显示，分解能够使推理速度加快（达4倍），内存大幅减少（达70%），同时保留了大部分（达99%）的原始性能。我们将在<匿名的网址>上发布代码。
在学习表征的同时进行探索是深度强化学习（DeepReinforcementLearning，DRL）目前面临的主要挑战之一。由于学到的表征依赖于观察到的数据，探索策略具有至关重要的作用。流行的DQN算法大大改善了强化学习（RL）算法从原始数据中学习状态表征的能力，然而，它使用了一种天真的探索策略，在统计上是低效的。另一方面，它通过线性参数化的值函数有效地探索和泛化。然而，它是基于手工设计的状态表示，需要为每个环境进行优先工程工作。在本文中，我们提出了RLSVI的深度学习适应性。 由于在学习过程中对表征进行了优化，建议的方法的一个关键部分是似然匹配机制，它可以适应不断变化的表征。我们在一个玩具问题上证明了我们算法的各种属性的重要性，并表明我们的方法在五个Atari基准中表现出DQN，达到与Rainbow算法竞争的结果。
大规模神经网络的复杂性可能导致对其内部细节的不了解。我们表明，这种不透明性为对手提供了一个机会，使其能够以特洛伊木马攻击的形式将非预期的功能嵌入到网络中。我们的新型框架将恶意网络的存在隐藏在一个良性的传输网络中。 我们的攻击是灵活的，容易执行的，而且难以检测。我们从理论上证明了恶意网络的检测在计算上是不可行的，并从经验上证明了运输网络不会破坏其伪装。我们的攻击暴露了一个重要的、以前未知的漏洞，揭示了机器学习安全的新方向。
在本文中，我们介绍了随机路径生成对抗网络（RPGAN）---一种GAN的替代方案，可以作为生成模型分析的工具。典型GAN的潜空间由输入向量组成，从标准高斯分布中随机采样，而RPGAN的潜空间由生成器网络中的随机路径组成。 正如我们所展示的，这种设计允许将生成器的不同层与潜空间的不同区域联系起来，提供其自然的可解释性。通过在标准基准上的实验，我们证明RPGAN揭示了关于不同层在图像生成过程中所扮演角色的几个有趣的见解。
深度人工神经网络可以在相同分布的训练和测试集上实现训练和测试准确率之间的极小差异，这是概括性的标准衡量标准。然而，训练和测试集可能不足以代表经验样本集，而经验样本集由真实世界的输入样本组成。 在推理过程中，当样本来自代表性不足或没有代表性的子集时，训练和推理准确率之间的差距可能会很大。为了解决这个问题，我们首先将分类算法重新表述为搜索将输入特征映射到类的源代码的程序。然后，我们使用一个通用的认知相似性指标，即信息距离，基于Kolmogorov复杂性，得出泛化的必要和充分条件。 利用这个条件，我们制定了一个优化问题来学习一个更普遍的分类函数。为了达到这个目的，我们通过对输入特征的串联编码来扩展输入特征，然后在扩展的特征上训练分类器。作为这个想法的一个说明，我们专注于图像分类，在这里我们在输入特征上使用通道编码作为一种系统的方法来提高训练和测试集对经验样本集的代表程度。 为了展示我们的理论发现，考虑到被破坏或扰动的输入特征属于经验样本集，但通常不属于训练集和测试集，我们通过广泛的系统实验证明，作为学习一个更普遍的分类函数的结果，在编码的输入特征上训练的模型对常见的破坏有明显的鲁棒性，例如：。高斯噪声和射击噪声，以及对抗性扰动，例如那些通过投影梯度下降发现的扰动，比在未编码的输入特征上训练的模型要强得多。
许多因果发现的方法因其无法在仅有观察数据的情况下区分马尔科夫等价图而受到限制。我们将因果发现表述为基于边际似然的贝叶斯模型选择问题。我们采用基于因果机制独立性概念的参数化，使马尔科夫等价图得以区分。我们用经验贝叶斯方法来设置先验，使实际的基本因果图被赋予比其替代品更高的边际似然。 采用贝叶斯方法也允许对未观察到的混杂变量进行直接建模，为此我们提供了一种变异算法来近似边际似然，因为这种理想的特性使边际似然的计算变得难以实现。 我们认为，贝叶斯的因果发现方法既能使贝叶斯推理的丰富方法用于这个问题的各种困难方面，又能为因果发现研究提供一个统一的框架。我们在真实数据的实验中展示了有希望的结果，支持我们的建模方法和推理方法。
  压缩传感的目标是从有限数量的噪声线性测量值$y/approx Ax$中学习结构化信号$x$。 在传统的压缩传感中，"结构 "是由一些已知的基础的稀疏度来表示的。 受深度学习在图像建模方面的成功启发，最近的工作从~\cite{BDJP17}开始，反而认为结构来自生成模型$G: \R^k\ to \R^n$。 我们提出了两个结果，确定了后一项任务的难度，表明现有的界限是严格的。 首先，我们提供了一个与$L$-Lipschitz生成模型$G$的压缩传感的~cite{BDJP17}的上界相匹配的下界。 特别是，存在这样一个函数，它需要大约$Omega(k \log L)$的线性测量，才能实现稀疏恢复。 这甚至对于更宽松的 "非均匀性 "恢复目标也是成立的。 其次，我们表明生成模型将稀疏性概括为结构的一种表示。 特别是，我们构建了一个基于ReLU的神经网络$G: `R^{2k}到`R^n}。\to \R^n$，有O(1)$层，每层有O(k)$激活，这样，$G$的范围包含所有$k$稀疏向量。
我们假设，端到端的神经图像标题系统工作得似乎很好，因为它们利用并学习了多模态特征空间中的 "分布相似性"，通过将测试图像映射到该空间中的类似训练图像，并从同一空间生成标题。 为了验证我们的假设，我们专注于图像字幕的 "图像 "方面，并改变输入图像的表示方法，但保持CNN-RNN的RNN文本生成模型不变。我们提出了一个稀疏的物体袋向量作为可解释的表示方法来研究我们的分布相似性假设。 我们发现，图像标题模型（i）能够从嘈杂的输入表征中分离出结构；（ii）当高维表征被压缩到低维空间时，几乎没有明显的性能损失；（iii）将具有类似视觉和语言信息的图像聚集在一起；（iv）严重依赖具有与训练集相似分布的测试集；（v）通过匹配图像和在视觉-文本联合空间中 "检索 "标题，重复生成相同的标题。 我们的实验都指向一个事实：我们的分布相似性假设是成立的。我们的结论是，无论图像的表现形式如何，图像标题系统似乎都能在一个学习到的图像-文本联合语义子空间中匹配图像并生成标题。
我们为自然语言到SQL的任务提出了一种序列到行动的解析方法，该方法从预先定义的清单中用可行的行动逐步填充SQL查询的槽。为了说明通常有多个具有相同或非常相似语义的正确SQL查询的事实，我们从句法解析技术中得到启发，并提议用非决定性的口令训练我们的序列到行动模型。 我们在WikiSQL数据集上评估了我们的模型，并在测试集上实现了83.7%的执行准确率，比用传统的静态口令训练的模型（假设有一个正确的目标SQL查询）绝对提高了2.1%。当进一步与执行指导的解码策略相结合时，我们的模型创造了新的最先进的性能，执行准确率为87.1%。
我们提出了高效神经结构搜索（ENAS），这是一种比以前的方法更快、更便宜的自动模型设计方法。在ENAS中，一个控制器通过在一个更大的模型中搜索最优路径来学习发现神经网络结构。控制器通过策略梯度训练来选择一条在验证集上预期回报最大化的路径。 同时，与所选路径相对应的模型被训练成交叉熵损失最小。在Penn Treebank数据集上，ENAS可以发现一个新的架构，实现了57.8的测试困惑度，这在Penn Treebank的自动模型设计方法中是最先进的。在CIFAR-10数据集上，ENAS可以设计新的架构，实现2.89%的测试误差，接近标准NAS的2.65%（Zoph等人。最重要的是，我们的实验表明，ENAS比NAS快10倍以上，资源需求少100倍。
如今，深度神经网络（DNNs）已经成为广泛领域内机器学习任务的主要工具，包括视觉、NLP和语音。同时，在异质表格数据的重要案例中，DNNs相对于浅层对应物的优势仍然值得怀疑。 特别是，没有足够的证据表明深度学习机制可以构建出优于梯度提升决策树（GBDT）的方法，而GBDT通常是表格问题的首选。在本文中，我们介绍了神经遗忘决策群（NODE），一种新的深度学习架构，旨在处理任何表格数据。 简而言之，拟议的NODE架构概括了遗忘决策树的集合，但受益于基于梯度的端到端优化和多层分层表示学习的力量。通过在大量表格数据集上与领先的GBDT包进行广泛的实验比较，我们证明了拟议的NODE架构的优势，它在大多数任务上都优于竞争对手。我们开源了NODE的PyTorch实现，相信它将成为表格数据机器学习的通用框架。
人身重新识别（re-ID）的目的是在不同的相机中识别相同的人的图像。然而，不同数据集之间的领域多样性对将在一个数据集上训练的重新识别模型适应另一个数据集构成了明显的挑战。 为了减轻嘈杂的伪标签的影响，我们提出了一个无监督的框架，即MMT（Mutual Mean-Teaching），通过离线改进的硬伪标签和在线改进的软伪标签，以另一种训练方式从目标域学习更好的特征。 此外，通常的做法是同时采用分类损失和三联体损失来实现人的再识别模型的最佳性能。 为了解决这个问题，我们提出了一种新型的软性最大三倍体损失，以支持使用软性伪三倍体标签进行学习，从而实现最佳的领域适应性。所提出的MMT框架在Market-to-Duke、Duke-to-Market、Market-to-MSMT和Duke-to-MSMT无监督领域适应性任务上实现了14.4%、18.2%、13.1%和16.4%的显著改善。
我们提出了第一个音频分类器的端到端验证器。与现有的方法相比，我们的方法能够分析整个音频处理阶段以及递归神经网络架构（如LSTM）。音频处理是使用针对音频中使用的特征提取操作的新型凸松弛来验证的（如。我们表明验证器可以扩展到大型网络，同时在常见的音频分类基准上计算出比现有方法更严格的界限：在具有挑战性的谷歌语音命令数据集上，我们比区间近似法（之前唯一的可扩展方法）多证明了95%的输入，扰动为-90dB。
由于深度神经网络是过度参数化的，它们可以记忆嘈杂的例子。我们解决了在注释噪声存在的情况下的这种记忆问题。从深度神经网络不能概括通过记忆获得的特征的邻域的事实来看，我们假设在一定的扰动下，嘈杂的例子不会一直对网络造成小的损失。 基于此，我们提出了一种新的训练方法，称为 "集合共识学习"（LEC），通过使用扰动网络的集合共识来消除噪声实例，从而防止过度拟合。所提出的LEC之一，LTEC在噪声的MNIST、CIFAR-10和CIFAR-100上以有效的方式胜过目前最先进的方法。
从数据中恢复稀疏条件独立图是机器学习中的一个基本问题，具有广泛的应用。该问题的一个流行的表述是$ell_1$正则化最大似然估计。许多凸优化算法被设计用来解决这个表述以恢复图结构。最近，人们对直接基于数据学习算法的兴趣激增，在这种情况下，学习将经验协方差映射到稀疏精度矩阵。 然而，在这种情况下，这是一个具有挑战性的任务，因为矩阵的对称正确定性（SPD）和稀疏性不容易在学习的算法中执行，而且从数据到精度矩阵的直接映射可能包含许多参数。我们提出了一个深度学习架构，GLAD，它使用交替最小化（AM）算法作为我们的模型归纳偏见，并通过监督学习学习模型参数。我们表明，GLAD学习一个非常紧凑和有效的模型，从数据恢复稀疏图。
反事实遗憾最小化（CFR）是解决不完全信息博弈（IIG）的基本有效技术。然而，最初的CFR算法只适用于离散的状态和行动空间，并且所产生的策略被保持为表格表示。这种表格表示限制了该方法直接应用于大型博弈。在本文中，我们提出了IIG的双重神经表示，其中一个神经网络表示累计遗憾，另一个表示平均策略。 为了使学习高效，我们还开发了几种新技术，包括稳健的抽样方法和小型批量蒙特卡洛反事实后悔最小化（MCCFR）方法，这可能是独立的兴趣。 从经验上看，在可以用表格方法处理的游戏中，用我们的算法训练的神经策略与表格中的策略收敛程度相当，并明显优于基于深度强化学习的策略。 在具有数十亿决策节点的极端大型游戏中，我们的方法取得了强大的性能，同时使用的内存比表格CFR少几百倍。在手游无限制德州扑克的头对头比赛中，我们的神经代理以每局9.8美元/pm4.1美元的筹码击败了强大的代理ABS-CFR。
我们首次验证了用于感知任务的神经网络对每一个感兴趣的输入都能在规定的容许范围内产生正确的输出。我们对正确性的定义是相对于一个规范而言的，该规范确定了1）由世界上所有相关状态组成的状态空间和2）从世界的状态中产生神经网络输入的观察过程。 将状态空间和输入空间用有限数量的瓦片拼接起来，从状态瓦片中获得基础真相界线，从输入瓦片中获得网络输出界线，然后比较基础真相和网络输出界线，为任何感兴趣的输入提供网络输出误差的上限。两个案例研究的结果强调了我们的技术能够为所有感兴趣的输入提供严格的误差界线，并显示误差界线在状态和输入空间中的变化。
近年来，深度生成模型取得了令人瞩目的进展。尽管如此，生成模型的定量评估和比较仍然是重要的挑战之一。 虽然直接计算对数似然可能很困难，但最近表明，一些最有趣的生成模型的对数似然，如变异自动编码器（VAE）或生成对抗网络（GAN），可以使用退火重要性采样（AIS）有效地估计。 在这项工作中，我们认为对数似然指标本身不能代表生成模型的所有不同性能特征，并建议使用速率失真曲线来评估和比较深层生成模型。我们表明，我们可以使用AIS的一次运行来接近整个速率失真曲线，计算成本与单一对数似然估计大致相同。 我们在MNIST和CIFAR10上评估了不同的深度生成模型的有损压缩率，如VAEs、GANs（及其变体）和对抗性自动编码器（AAE），并得出了一些仅从对数似然中无法获得的见解。
虽然强化学习方法可以在模拟中取得令人印象深刻的结果，但现实世界中存在两大挑战：生成样本的成本非常高，而意外的扰动或未见的情况会导致熟练但专门的政策在测试时失败。鉴于训练单独的政策以适应代理在现实世界中可能看到的所有情况是不切实际的，这项工作建议学习如何快速有效地在线适应新任务。 我们的方法使用元学习来训练动力学模型先验，这样，当与最近的数据相结合时，这个先验可以迅速适应当地的环境。我们的实验证明了模拟和现实世界的代理上的连续控制任务的在线适应。我们首先展示了模拟代理在线适应新的地形、残缺的身体部分和高度动态的环境的行为。我们还通过将我们的方法应用于一个真正的动态腿部毫机器人，说明了将在线适应纳入在现实世界运作的自主代理的重要性。我们展示了代理的学习能力，即快速在线适应缺失的腿，适应新的地形和坡度，考虑姿势估计中的误判或错误，以及补偿拉动有效载荷。
无模型的深度强化学习方法在模拟环境中显示出超人的性能（例如。在训练过程中，这些方法往往隐含地构建了一个包含决策关键信息的潜空间。在本文中，我们在这个潜空间上学习了一个前向模型，并将其应用于不完全信息的微型实时战略游戏（MiniRTS）中基于模型的规划。） 我们首先表明，从现有的演员批评模型中构建的潜空间包含游戏的相关信息，并设计了学习前向模型的训练程序。我们还表明，我们学习的前向模型可以预测有意义的未来状态，并可用于潜空间蒙特卡洛树搜索（MCTS），在对基于规则的代理的胜利率方面。
几个最先进的卷积网络依赖于不同层的相互连接，以缓解其输入层和输出层之间的信息和梯度流动。这些技术使从业者能够成功地训练具有数百个层的深度卷积网络。特别是，一种新颖的层间连接方式被介绍为密集卷积网络（DenseNet），并在相关的图像识别任务上取得了最先进的性能。尽管它们在经验上取得了显著的成功，但其理论理解仍然有限。 在这项工作中，我们通过分析层间连接对卷积网络整体表现力的影响来解决这个问题。特别是，DenseNet中使用的连接与其他类型的层间连接进行比较。 我们对卷积运算电路（ConvACs）上的表现力互连进行了张量分析，并将我们的结果与标准卷积网络联系起来。该分析导致了性能界限和设计ConvACs的实用指南。
我们考虑了深度强化学习（DRL）领域的以下核心问题：我们如何使用隐含的人类反馈来加速和优化DRL算法的训练？"最先进的方法依赖于任何明确提供的人类反馈，需要人类的积极参与（例如。在这项工作中，我们研究了另一种范式，即非专家的人默默地观察（和评估）与环境互动的代理人。通过在人的头皮上放置电极和监测所谓的事件相关电势，人对代理人行为的内在反应被感知为隐性反馈。 隐性反馈随后被用来增强代理人在RL任务中的学习。我们开发了一个系统来获取并准确解码阿塔里类型环境中的状态-动作对的人类隐性反馈（特别是与错误相关的事件电位）。 作为基线贡献，我们证明了使用脑电图（EEG）帽捕捉人类观察者学习玩几个不同的Atari游戏的错误电位的可行性，然后对信号进行适当的解码，并将其作为DRL算法的辅助奖励函数，目的是加速游戏的学习。 在基线的基础上，我们在工作中做出了以下新的贡献：（i）我们认为错误电位的定义在不同的环境中是可以通用的；具体来说，我们表明观察者的错误电位可以为一个特定的游戏学习，而定义可以原样用于另一个游戏，而不需要重新学习错误电位。 (ii) 我们提出了两个不同的框架，将DRL的最新进展以一种有效的方式结合到基于错误电位的反馈系统中，允许人类在循环训练时或在RL代理训练之前提供隐性反馈。 (iii) 最后，我们将基于隐性人类反馈（通过ErrP）的RL扩展到合理的复杂环境（游戏），并通过合成和真实用户实验证明我们方法的重要性。
深度学习已经展示了学习复杂结构的能力，但它们可能受到可用数据的限制。最近，共识网络（CNs）被提出来，通过利用多种模式的特征来缓解数据的稀缺性，但它们也受到了标记数据规模的限制。 在本文中，我们将共识网络扩展到过渡性共识网络（TCN），适用于半监督学习。在TCN中，不同模式的输入被压缩成潜在的表征，我们鼓励它们在迭代对抗训练中变得不可区分。 为了理解TCN的两种机制，即共识和分类，我们在对这些机制的消减研究中提出了它的三种变体。为了进一步研究TCN模型，我们将潜伏表征视为概率分布，并以负的相对Jensen-Shannon分歧来衡量其相似性。 我们表明，一个有利于分类的共识状态需要一个稳定但不完美的表征之间的相似性。总的来说，在银行营销和DementiaBank数据集上，TCN的表现优于或与给定的20至200个标记样本的最佳基准算法一致。
一些常用于欧几里得领域的一阶随机优化方法，如随机梯度下降法（SGD）、加速梯度下降法或方差减少法，已经被改编为某些黎曼设置。然而，这些优化工具中最流行的一些--即Adam、Adagrad和最近的Amsgrad--仍有待普及到黎曼流形。 我们讨论了将这种适应性方案推广到最不可知的黎曼设置中的困难，然后在黎曼流形的乘积的特殊情况下，为几何凸目标提供算法和收敛证明，其中适应性是在卡氏乘积中跨流形实施的。 我们的概括是严格的，因为选择欧几里得空间作为黎曼流形会产生与标准算法相同的算法和遗憾界限。实验中，我们显示了黎曼自适应方法比其相应的基线在现实任务中嵌入Poincare球中的WordNet分类法的更快收敛和更低的训练损失值。
我们研究了保护图像分类的深度神经网络方法免受物理上可实现的攻击的问题。首先，我们证明了两种最可扩展和最有效的学习稳健模型的方法，带有PGD攻击的对抗性训练和随机平滑，对三种最引人注目的物理攻击表现出非常有限的有效性。 接下来，我们提出了一个新的抽象对抗模型--矩形遮挡攻击，在这个模型中，对抗者在图像中放置一个小的对抗性的矩形，并开发了两种方法来有效地计算所产生的对抗性例子。最后，我们证明，使用我们的新攻击的对抗性训练产生的图像分类模型对我们研究的物理上可实现的攻击表现出高度的鲁棒性，提供了第一个针对此类攻击的有效通用防御。
持续学习是在保护以前获得的知识的同时连续学习新的任务或知识的问题。然而，灾难性的遗忘对执行这种学习过程的神经网络构成了巨大的挑战。因此，部署在现实世界中的神经网络经常在数据分布非稳定（概念漂移）、不平衡或不总是完全可用，即。我们提出了一个可微分的Hebbian Consolidation模型，它由一个可微分的Hebbian Plasticity（DHP）Softmax层组成，在Softmax输出层的固定（缓慢变化）参数上增加了一个快速学习的可塑性成分（压缩的事件记忆）；使学到的表征能够保留更长的时间尺度。 我们通过整合著名的特定任务突触巩固方法来惩罚对每个目标任务都很重要的慢速权重的变化，从而证明了我们方法的灵活性。我们在Permuted MNIST、Split MNIST和Vision Datasets Mixture基准上评估了我们的方法，并引入了Permuted MNIST的不平衡变体--一个结合了类不平衡和概念漂移挑战的数据集。我们提出的模型不需要额外的超参数，通过减少遗忘而优于可比基线。
为了选择一个对特定建模问题有效的神经网络架构，我们必须了解每个潜在选项所带来的限制。这些限制通常用信息论的界限来描述，或者通过比较不同架构之间近似例子函数所需的相对复杂性来描述。 在本文中，我们研究了神经网络的结构对其能够近似的所有函数的水平集所施加的拓扑约束。这种方法的新颖之处在于限制的性质，以及它们与广泛的激活函数系列的网络深度无关的事实。
程序验证提供了一个框架，以确保程序的正确性，从而系统地消除不同类别的错误。推断循环不变量是现实世界程序自动验证背后的主要挑战之一，这些程序通常包含许多循环。 我们开发了一种新的健全和完整的语义映射，用于将SMT公式分配给连续真值，从而使CLNs得到有效的训练。我们使用CLNs来实现一个新的循环不变式推理系统CLN2INV，它在流行的Code2Inv数据集上的表现明显优于现有方法。 CLN2INV是第一个解决Code2Inv数据集中所有124个理论上可解决的问题的工具。此外，CLN2INV对每个问题平均只需要1.1秒，比现有方法快40倍。我们进一步证明，CLN2INV甚至可以学习12个明显比Code2Inv数据集所需的更复杂的循环不变式。
单细胞RNA测序（scRNAseq）技术可以量化癌症内单个细胞的基因表达谱。降维方法通常用于细胞聚类分析和数据的可视化。目前的降维方法往往过度消除表达变化对应的不太重要的特征，因此我们无法找到癌症发展的同质性。 在本文中，我们提出了一种新的scRNAseq数据的聚类分析方法，即BBSC，通过实现基因表达谱的二进制化为布尔矩阵因子化的开/关频率变化，BBSC恢复的表达矩阵的低等级表示增加了识别不同细胞类型或功能的分辨率。
虽然归一化流在建模高维连续分布方面取得了重大进展，但其对离散分布的适用性仍然未知。在本文中，我们表明流实际上可以扩展到离散事件---而且是在一个简单的变量变化公式下，不需要对数决定性-雅各宾计算。 离散流有许多应用。我们展示了两种流架构下的概念证明：离散自回归流实现了双向性，例如，允许文本中的标记在精确的语言模型中同时依赖于从左到右和从右到左的上下文；以及离散的双点流（即。与RealNVP的层结构）使平行生成成为可能，如精确的非递归文本建模。
我们提出了一个具有尖峰辅助特征提取的深度神经网络（SAFE-DNN），以提高输入随机扰动下的分类稳健性。拟议的网络增强了DNN的无监督学习，使用具有尖峰-时间-依赖-弹性（STDP）的尖峰神经元网络（SNN）的低级特征。 完整的网络在进行全局特征检测和分类的同时学会了忽略局部扰动。在CIFAR-10和ImageNet子集上的实验结果表明，多个DNN架构的噪声鲁棒性得到了改善，而不会牺牲干净图像的准确性。
神经嵌入已经在自然语言处理（NLP）中取得了巨大的成功，它们提供了封装单词相似性的紧凑表示，并在一系列语言任务中取得了最先进的性能。神经嵌入的成功促使人们对语言以外的领域的应用进行了大量的研究。 对于NLP和基于图的任务，已经学习了高维欧氏空间的嵌入。然而，最近的工作表明，用于嵌入复杂网络的适当等距空间不是平坦的欧氏空间，而是负弯曲的双曲空间。
国际计划与调度知识工程竞赛（ICKEPS）在促进新的知识工程（KE）工具的发展和强调所有不同的知识工程方面的原则性方法的重要性方面发挥了关键作用，这些方法对于在现实世界中成功地长期使用计划是必要的。 在本文中，作为一个综合练习，并为了激发思考和讨论，我们回顾了以前的ICKEPS的格式，为未来的比赛提出了替代的格式，最好是能激励有人站出来组织下一次的比赛。
我们表明，生成英语维基百科的文章可以作为源文件的多文档总结来处理。我们使用提取式总结来粗略地识别突出的信息，并使用神经抽象模型来生成文章。对于抽象模型，我们引入了一个仅有解码器的架构，它可以可扩展地处理非常长的序列，比序列转换中使用的典型编码器-解码器架构长很多。 我们表明，这个模型可以生成流畅、连贯的多句话段落，甚至是整个维基百科的文章。当给定参考文件时，我们表明它可以提取相关的事实信息，这反映在困惑度、ROUGE分数和人类评价上。
摘要 随机梯度下降（SGD）和Adam通常用于优化深度神经网络，但选择一个通常意味着在速度、准确性和稳定性之间做出权衡。这里我们提出了一个关于为什么存在权衡的直觉，以及一个以连续方式统一两者的方法。
使用模仿学习来学习一个具有多种模式或层次结构的复杂任务的单一策略是具有挑战性的。事实上，以前的工作表明，当模式是已知的，为每个模式或子任务学习单独的策略可以大大提高模仿学习的性能。在这项工作中，我们使用一个有向图模型从它们产生的状态动作轨迹序列中发现子任务之间的互动。 我们提出了一种基于生成式对抗模仿学习框架的新算法，该算法从未分割的演示中自动学习子任务策略。我们的方法在图形模型中最大化了子任务潜在变量和其生成轨迹之间的有向信息流。我们还展示了我们的方法如何与现有的选项框架相连接，该框架通常用于学习分层策略。
卷积神经网络（CNN）在最近几十年里成功地应用于许多领域；然而，在处理许多现实问题时，它缺乏利用先前领域知识的能力。我们提出了一个称为几何算子卷积神经网络（GO-CNN）的框架，它使用领域知识，其中第一个卷积层的内核被替换为由几何算子函数生成的内核。这个框架整合了许多传统的几何算子，这使它能够适应不同的问题范围。 在一定条件下，我们从理论上分析了GO-CNN和普通CNN的收敛性和泛化误差的界限。虽然几何算子卷积核的可训练参数比普通卷积核少，但实验结果表明，GO-CNN在CIFAR-10/100上的表现比普通CNN更准确。此外，GO-CNN减少了对训练实例数量的依赖，增强了对抗稳定性。
在深度学习框架下，DPP通常是通过近似来优化的，这并不直接，而且与多样性要求有一些冲突。 在本文中，我们设计了一种简单而有效的算法来解决这个问题，以优化DPP项，直接用谱域中的L-ensemble在克矩阵上表示，这比在参数化核上学习更灵活。 通过进一步考虑一些几何约束，我们的算法试图在DPP克矩阵不可倒置的情况下（在这种情况下不存在梯度）生成DPP项的有效子梯度。在这个意义上，我们的算法可以很容易地与多种深度学习任务结合起来。
机器翻译系统的质量在很大程度上取决于是否有可观的平行语料库。对于最近流行的神经机器翻译（NMT）框架，数据稀少问题可能变得更加严重。 在本文中，我们主张将每个句子对作为两组相似的句子进行广播，以纳入更多的语言表达的多样性，我们将其命名为平行集群。然后我们定义了一个更普遍的集群与集群之间的对应分数，并训练我们的模型来最大化这个分数。 由于直接最大化是困难的，我们推导出它的下限作为我们的替代目标，发现它可以概括点对点的最大似然估计（MLE）和点对群的奖励增强最大似然（RAML）算法作为特例。基于这个新的目标函数，我们划定了四个潜在的系统来实现我们的群对群框架，并在三个公认的翻译任务中测试它们的性能，每个任务都有正向和反向翻译方向。 在六次实验中的每一次，我们提出的四个平行系统都被证明明显优于MLE基线、RL（强化学习）和RAML系统。最后，我们进行了案例研究，实证分析了集群到集群的NMT框架的优势。
在这项工作中，我们研究了在归纳逻辑编程（ILP）范围内解释问题的学习。我们提出了神经逻辑归纳学习（NLIL），这是一个高效的可分化ILP框架，它可以学习能够解释数据中模式的一阶逻辑规则。 在实验中，与最先进的模型相比，我们发现NLIL能够搜索出X10倍的规则，同时保持X3倍的速度。我们还表明NLIL可以扩展到大型图像数据集，即视觉基因组，有100万个实体。
基于神经网络的分类器在许多常见的任务上与人类水平的准确性平行或超越，并被用于实际的系统中。然而，神经网络很容易受到对抗性例子的影响，这些精心扰乱的输入会导致网络以任意选择的方式出现错误行为。当用标准方法生成时，由于视点偏移、相机噪声和其他自然变换的组合，这些例子在物理世界中并不总是能够欺骗分类器。 我们介绍了第一种构建真实世界三维物体的方法，这些物体在广泛的角度和视点分布中一致地愚弄神经网络。我们提出了一种通用的算法，用于生成对抗性例子，这些例子在任何选定的变换分布中都是稳健的。 我们展示了它在二维空间的应用，产生了对噪声、扭曲和仿生变换具有鲁棒性的对抗性图像。最后，我们应用该算法来产生任意的物理3D打印对抗性物体，证明我们的方法在现实世界中是端到端的。
集的表示是具有挑战性的，因为对集的操作应该是互换不变的。为此，我们提出了一个互换优化模块，它可以端到端地学习如何互换一个集。互换后的集可以被进一步处理以学习该集的互换不变的表示，避免了传统集模型的瓶颈。 我们在四个数据集上展示了我们的模型在显性或隐性监督下学习排列组合和集合表示的能力，在这些数据集上我们取得了最先进的结果：数字排序、图像马赛克、从图像马赛克分类和视觉问题回答。
机器人的物理设计和控制其运动的策略在本质上是耦合的。然而，现有的方法在很大程度上忽略了这种耦合，而是选择在独立的设计和控制阶段之间交替进行，这需要专家的直觉，并有可能收敛到次优设计。在这项工作中，我们提出了一种方法，以无模型的方式联合优化机器人的物理设计和相应的控制策略，不需要任何专家监督。 给定一个任意的机器人形态，我们的方法保持一个设计参数的分布，并使用强化学习来训练一个神经网络控制器。在整个训练过程中，我们完善了机器人的分布，以最大化预期的奖励。
深度学习能够以大量数据为代价，从头开始训练大型灵活的函数近似器。神经网络的应用通常考虑在单一任务的背景下进行学习。然而，在许多场景下，我们希望学习的不仅仅是单一任务，而是一个可以用来解决多个不同任务的模型。这种多任务学习设置有可能通过在不同任务间共享数据和表征来提高数据效率和概括性。 然而，在一些具有挑战性的多任务学习设置中，特别是在强化学习中，很难学习一个能够解决所有任务的单一模型，同时实现数据效率和性能优势。从头开始独立学习每个任务，在这种设置中实际上可以表现得更好，但它不能从多任务学习可能提供的表征共享中受益。 在这项工作中，我们开发了一种方法，赋予单一模型代表两种极端情况的能力：联合训练和独立训练。为此，我们引入了矩阵交错（Mint），这是一种对标准神经网络模型的修改，将每个任务的激活投射到一个不同的学习子空间，由每个任务和每个层的矩阵代表。 通过与其他模型参数共同学习这些矩阵，优化器本身可以决定在任务之间共享多少表征。在三个具有挑战性的多任务监督学习和强化学习问题上，由于共享任务结构的程度不同，我们发现这个模型始终与联合训练和独立训练相匹配，甚至优于两者的最佳要素。
训练代理在一个环境中操作，往往会产生过拟合的模型，无法概括该环境的变化。然而，由于现实世界中可能出现的众多变化，代理往往被要求具有鲁棒性，以便发挥作用。在本文中，我们研究了视觉导航任务中RL代理对训练环境的过拟合问题。我们的实验表明，即使在多个环境中同时训练，深度RL代理也会过拟合。我们提出了一种正则化方法，该方法将RL与监督学习方法结合起来，在RL目标中加入一个项，鼓励策略对观察中的变化保持不变，这些变化不应该影响所采取的行动。这种方法的结果，称为不变性正则化，显示了策略对训练期间未见的环境的概括性的改善。
尽管视觉信息已被引入以增强神经机器翻译（NMT），但其有效性在很大程度上依赖于大量带有人工图像注释的双语平行句子对的可用性。在本文中，我们提出了一种通过带有图像注释的单语语料库学习的通用视觉表示，它克服了大规模双语句子-图像对的缺乏，从而扩展了图像在NMT中的适用性。 具体来说，一组与源句子主题相似的图像将从一个通过现有句子-图像对学习的轻型主题-图像查询表中检索出来，然后由一个预先训练好的ResNet编码为图像表示。 在四个广泛使用的翻译数据集，包括WMT'16英译罗，WMT'14英译德，WMT'14英译法和Multi30K上的实验表明，所提出的方法比强大的基线取得了明显的改进。
本文介绍了一个新的学习算法框架，以解决在线组合优化问题。为了实现这一目标，我们从传统算法和复杂性理论中引入了一些关键思想。 接下来，我们引入了对抗性分布（通用和高熵训练集）的概念，这些分布鼓励学习者找到在最坏情况下工作良好的算法。我们在一些优化问题上测试了我们的新想法，如AdWords问题、在线knapsack问题和秘书问题。我们的结果表明，这些模型的学习行为与这些问题的传统最佳算法一致。
尽管深度神经网络很受欢迎并取得了成功，但在理论上对其了解甚少，并被视为 "黑箱 "系统。使用这些网络的功能视图给了我们一个有用的新视角来理解它们。 一个关键的结果是，泛化的结果是函数近似的平滑性与平坦的初始近似相结合。这种平滑性随着单元数的增加而增加，解释了为什么大规模的超参数化网络继续泛化良好。
众所周知，较深的神经网络比较浅的神经网络更难训练。在这篇短文中，我们使用Hessian的（完整）特征值谱来探索当网络变得更深时，以及当残余连接被添加到架构中时，损失情况如何变化。 我们计算了一系列关于Hessian频谱的量化措施，表明在更深的网络中，Hessian特征值分布的尾部明显更重（相当于，更多的离群的特征值），这使得网络更难用一阶方法进行优化。
在优化的背景下，神经网络的梯度表示一个特定的权重相对于损失应该改变的量。因此，小的梯度表示一个好的权重值，不需要改变，可以在训练期间保持冻结。本文提供了一个关于神经网络权重的重要性的实验研究，以及它们需要更新到什么程度。我们希望表明，从第三个历时开始，冻结没有信息梯度的权重，在训练期间不太可能被改变，导致整体准确性非常轻微的下降（有时更好）。 我们在MNIST、CIFAR10和Flickr8k数据集上使用几种架构（VGG19、ResNet-110和DenseNet-121）进行实验。在CIFAR10上，我们表明从第三个历时开始冻结80%的VGG19网络参数会导致0.24%的准确性下降，而冻结50%的Resnet-110参数会导致0.9%的准确性下降，最后冻结70%的Densnet-121参数会导致0. 此外，为了体验现实生活中的应用，我们使用LSTM网络在Flickr8k数据集上训练了一个带有注意力机制的图像标题模型，从第三个历时开始冻结了60%的参数，结果比完全训练的模型获得了更好的BLEU-4分数。我们的源代码可以在附录中找到。
像人类一样，当样本以有意义的顺序或课程被组织和引入时，深度网络的学习效果更好。虽然传统的课程学习方法强调样本的难度是核心的增量策略，但它迫使网络从小的数据子集中学习，同时引入预计算的开销。 在这项工作中，我们提出了带增量标签和自适应补偿的学习（LILAC），它引入了一种新的课程学习方法。LILAC强调增量学习标签，而不是增量学习困难的样本。它在两个不同的阶段工作：首先，在增量标签引入阶段，我们在训练期间以固定的增量揭开真实标签，以改善网络学习的起点。 在自适应补偿阶段，我们通过自适应地将目标向量改变为更平滑的分布来补偿失败的预测。我们在三个标准图像基准，CIFAR-10、CIFAR-100和STL-10中，将LILAC与批处理和课程学习以及标签平滑中最接近的可比方法进行评估。 我们表明，我们的方法优于批处理学习，在所有基准中都有较高的平均识别准确率和较低的标准偏差。我们进一步扩展LILAC，使其在CIFAR-10中的表现达到最先进的水平，同时表现出标签顺序不变性和其他重要特性。
自然语言中的词遵循Zipfian分布，其中一些词是频繁出现的，但大多数是罕见的。学习这个分布中 "长尾 "的词的表示需要大量的数据。我们提供了一种方法，通过少量的辅助数据和为下游任务进行端对端训练的网络来预测稀有词的嵌入。我们表明，这比在阅读理解、识别文本嵌套和语言建模的终端任务上训练嵌入的基线提高了效果。
可靠地检测分布外样本的能力是部署一个好的分类器的关键因素之一，因为在大多数现实世界的应用中，测试分布总是与训练分布不匹配。在这项工作中，我们提出了一个深度生成分类器，通过将高斯判别分析的概念整合到深度神经网络中，可以有效地检测分布外样本并对分布内样本进行分类。 与只关注决策边界将其潜伏空间划分为多个区域的判别式（或softmax）分类器不同，我们的生成式分类器旨在明确地将类条件分布建模为可分离的高斯分布，因此，我们可以通过测试样本与每个分布中心的距离来定义置信度分数。
老年人口中最普遍的症状之一--痴呆症，可以通过从叙事记录中提取的语言特征训练出来的分类器来检测。然而，这些语言特征受到正常老化过程的影响，虽然相似但却不同。因此，老化是一个混杂因素，其影响对机器学习分类器来说很难隔离。在本文中，我们表明深度神经网络（DNN）分类器可以从语言特征中推断出年龄，这是一种纠缠，可能导致不同年龄组之间的不公平。我们表明这个问题是由因果关系图中的V型结构的不希望的激活引起的，它可以通过公平的表示学习来解决。我们建立的神经网络分类器可以学习反映痴呆症的影响的低维表示，但要摒弃年龄的影响。 为了评估这些分类器，我们指定了一个与模型无关的分数$Delta_{eo}^{(N)}$来衡量分类器的结果如何与年龄脱离关系。我们最好的模型在脱离关系方面优于基线神经网络分类器，而在DementiaBank和名人数据集上的准确率分别低至2.56%和2.25%。
深度神经网络设计的重点是提高准确率，这导致了更强大但高度复杂的网络架构，在实际场景中难以部署。 因此，最近人们对设计用于评估深度神经网络的定量指标产生了兴趣，这些指标不仅仅是将模型的准确性作为网络性能的唯一指标。 在这项研究中，我们通过介绍NetScore，一个专门为深度神经网络的准确性、计算复杂性和网络结构复杂性之间的平衡提供定量评估的新指标，继续讨论评估深度神经网络在实际设备边缘使用的性能的通用指标。 在文献中最大规模的深度神经网络之间的比较分析中，NetScore指标、top-1准确性指标和流行的信息密度指标在60个不同的深度卷积神经网络中进行了比较，用于ImageNet大规模视觉识别挑战（ILSVRC 2012）数据集上的图像分类。 本研究介绍了这组不同网络的三个指标的评估结果，作为该领域从业者的参考指南。 
深度神经网络（DNN）很容易受到对抗性攻击，特别是白盒目标攻击。本文研究了白盒目标攻击如何超越广泛使用的Top-1攻击的问题。我们建议学习有序的Top-k攻击（k>=1），它强制要求对抗性例子的Top-k预测标签是k个（随机）选择和有序的标签（地面真实标签是排他的）。 我们提出了两种方法。首先，我们扩展了vanilla Carlini-Wagner（C&W）方法，并将其作为一个强大的基线。其次，我们提出了一个由两部分组成的对抗性提炼框架：（i）为任何给定的Top-$k$目标标签计算一个对抗性概率分布。 (ii) 通过最小化对抗分布和预测分布之间的Kullback-Leibler（KL）分歧以及扰动能量惩罚来学习对抗实例。在计算对抗分布时，我们探索如何利用标签的语义相似性，从而实现面向知识的攻击。 在实验中，我们在ImageNet-1000 val数据集中测试了Top-k（k=1,2,5,10）攻击，使用两个流行的DNNs，即ResNet-50和DenseNet-121，用干净的ImageNet-1000训练数据集训练。总的来说，对抗性提炼方法获得了最好的结果，特别是在计算预算有限的情况下有很大的优势。 它在所有四个k的攻击成功率相同的情况下持续减少扰动能量，并在k=10的情况下以较大的幅度提高了攻击成功率，而不是改进的C&W方法。  
用于图上半监督分类的神经信息传递算法最近取得了巨大的成功。然而，对于分类一个节点，这些方法只考虑距离几个传播步骤的节点，而且这个利用的邻域的大小很难扩展。在本文中，我们利用图卷积网络（GCN）和PageRank之间的关系，得出一个基于个性化的PageRank的改进传播方案。 我们利用这个传播程序构建了一个简单的模型--神经预测的个性化传播（PPNP），以及它的快速近似模型--APPNP.我们的模型的训练时间与以前的模型相当或更快，它的参数数量与以前的模型相当或更低。它利用一个大的、可调整的邻域进行分类，可以很容易地与任何神经网络相结合。
最近在跨语言词嵌入方面的大部分工作都是以英语为中心的。绝大多数的词汇归纳评估字典都是在英语和另一种语言之间，而且在多语言环境下学习时，英语嵌入空间被默认为中心。 其次，我们扩大了目前的评估字典集，以包括所有使用三角法的语言对，同时也为代表性不足的语言创建了新的字典。对所有这些语言对的既定方法进行评估，揭示了它们的适用性，并为该领域提出了新的挑战。最后，在我们的分析中，我们确定了强大的跨语言嵌入基线的一般准则，而不仅仅是基于英语中心的实验。
将生成对抗网络（GAN）训练解释为近似发散最小化在理论上很有见地，激发了讨论，并导致了理论上和实践上有趣的扩展，如f-GANs和Wasserstein GANs。对于经典GANs和f-GANs，有一个原始训练变量和一个 "非饱和 "变量，使用替代形式的发生器更新。 替代生成器更新通常被认为是处理优化问题的简单修改，而且这似乎是一个常见的误解，即这两个变体最小化了相同的发散。在这个简短的说明中，我们得出了GAN和f-GAN训练的原始和替代变体近似最小化的发散。 例如，我们表明KL-GAN训练的替代变体实际上最小化了反向KL发散，而传统GAN训练的替代变体最小化了反向KL的 "软化 "版本。我们希望这些结果可以帮助澄清围绕GAN训练的发散最小化观点的一些理论讨论。
REINFORCE可用于在结构化预测设置中训练模型，以直接优化测试时间目标。然而，每个数据点（输入）抽样一个预测的常见情况是数据效率低下的。我们表明，通过每个数据点抽取多个样本（预测），我们可以用更少的数据进行学习，因为我们可以自由获得REINFORCE基线来减少方差。 此外，我们还得出了一个基于无替换抽样的REINFORCE估计器。结合最近使用随机光束搜索的无替换抽样序列技术，这改善了预测旅行推销员问题解决方案的序列模型的训练程序。
强化学习（RL）是一种强大的技术，可以训练一个代理来执行任务。 然而，使用RL训练的代理只能够实现通过其奖励函数指定的单一任务。  这种方法不能很好地扩展到代理需要执行各种任务的环境中，如导航到房间里的不同位置或将物体移动到不同的位置。 相反，我们提出一种方法，允许代理自动发现它在环境中能够执行的任务范围。 我们使用一个生成器网络来为代理人提出试图实现的任务，每个任务被指定为达到状态空间的某个参数化子集。 生成器网络使用对抗性训练进行优化，以产生对代理人来说始终处于适当难度的任务。 因此，我们的方法自动产生了一个供代理人学习的任务课程。 我们表明，通过使用这个框架，一个代理可以有效地自动学习执行广泛的任务，而不需要事先了解其环境（视频和代码见：https://sites.google.com/view/goalgeneration4rl）。我们的方法还可以学习实现具有稀疏奖励的任务，这给传统的RL方法带来了重大挑战。
已经提出了广泛的防御措施，以加强神经网络对抗对抗性攻击。然而，已经出现了一种模式，即大多数对抗性防御措施很快就被新的攻击所打破。 鉴于在生成强大的防御措施方面缺乏成功，我们不得不提出一个基本问题。 本文从理论角度分析了对抗性例子，并确定了分类器对对抗性攻击的敏感性的基本界限。  我们表明，对于某些类别的问题，对抗性例子是不可避免的。 通过实验，我们探讨了理论保证对现实世界问题的影响，并讨论了诸如维度和图像复杂性等因素如何限制分类器对对抗性例子的鲁棒性。
对于计算机视觉的应用，先前的工作显示了在深度神经网络中降低模型参数（网络权重）的数值精度的功效。然而，激活图在使用小批量输入时，在训练和推理步骤中占据了大量的内存足迹。 我们降低了激活图的精度（连同模型参数），并增加了一个层中过滤图的数量，发现这个方案与基线全精度网络的精度相匹配甚至超过了它。我们将我们的方案称为WRPN--宽减精网络。我们报告了结果，并表明WRPN方案在ILSVRC-12数据集上比以前报告的精度更好，同时与以前报告的减精网络相比计算成本更低。
我们研究了用于命名实体识别（NER）的神经线性链条件随机场（CRF）的半监督学习（SSL）方法，将标记器视为给定标记的文本生成模型中的摊销变异后验。我们首先说明了如何将CRF纳入VAE中，实现半监督数据的端到端训练。 然后，我们研究了一系列由端到端优化实现的越来越复杂的给定标签的深度生成模型，在Ontonotes5 NER数据集上将所提出的模型与监督和强CRF SSL基线进行了比较。我们发现，我们提出的最佳模型在低度和中度资源制度下持续提高了性能$approx 1/%$ F1，并在更困难的部分监督环境下轻松解决退化的模型行为。
为了使深度神经网络在资源受限的环境（如移动设备）中可行，通过使用低精度权重对模型进行量化是有益的。量化神经网络的一种常见技术是直通梯度法，它通过量化映射实现反向传播。 基于一个新的观察，即直通梯度法实际上与著名的Nesterov在量化约束优化问题上的双平均算法相同，我们提出了一个更有原则的替代方法，称为ProxQuant，它将量化网络训练制定为一个正则化学习问题，并通过近似梯度法进行优化。 ProxQuant在底层全精度向量上进行反向传播，并在随机梯度步骤之间应用高效的prox-operator来鼓励量化。对于量化ResNets和LSTMs，ProxQuant在二进制量化上的表现优于最先进的结果，在多比特量化上与最先进的结果相当。 对于二进制量化，我们的分析表明，在理论和实验上，ProxQuant比直通梯度法（即BinaryConnect）更稳定，挑战了直通梯度法的不可或缺性，提供了一个强大的替代方案。
深度神经网络对对抗性样本的脆弱性已经成为在敏感领域部署这些模型的一个重要问题。事实证明，设计一个明确的防御措施来对抗这种攻击是具有挑战性的，而依靠检测对抗性样本的方法只有在攻击者对检测机制视而不见的情况下才有效。在本文中，我们在鲁棒优化框架下考虑对抗性检测问题。我们将输入空间划分为子空间，使用非对称对抗性训练（AAT）训练对抗性鲁棒子空间检测器。 分类器和检测器的整合提出了一种检测机制，为它所考虑的对抗者提供了性能保证。我们证明了AAT促进了对类条件分布的学习，这进一步产生了生成性检测/分类方法，这些方法既稳健又更容易解释。我们对上述方法进行了全面评估，并证明了它们在对抗性检测和稳健分类问题上的竞争性能和引人注目的特性。
探索是成功的强化学习的一个关键组成部分，但最佳方法在计算上是难以实现的，因此研究人员集中于手工设计基于探索奖金和内在奖励的机制，有些是受自然系统中好奇行为的启发。 在这项工作中，我们提出了一种策略，将好奇心算法编码为特定领域语言的程序，并在元学习阶段搜索使RL代理在新领域表现良好的算法。 我们丰富的程序语言，可以将神经网络与包括最近邻模块在内的其他构件结合起来，并可以选择自己的损失函数，从而能够表达高度通用的程序，这些程序在不同的领域表现良好，如带有图像输入的网格导航、杂技机器人、月球着陆器、蚂蚁和跳蚤。  为了使这一方法可行，我们开发了一些修剪技术，包括学习预测一个程序的成功，基于其句法特性。  我们通过经验证明了该方法的有效性，发现了与已发表文献中的策略相似的好奇心策略，以及与之竞争并具有良好概括性的新策略。
许多机器学习算法用矢量嵌入或离散代码表示输入数据。当输入表现出组成结构（例如，从部件中建立的对象或从子程序中建立的程序）时，自然会问这种组成结构是否反映在输入的学习表示中。虽然语言中的组成性评估在语言学和邻近领域得到了极大的关注，但机器学习文献缺乏通用的工具来产生对更一般（例如矢量值）组成结构的分级测量。 我们描述了一个评估构成性的程序，通过测量真正的表示产生的模型能在多大程度上被一个明确构成推断的表示基元集合的模型所近似。我们使用该程序在各种环境下提供构成性结构的形式和经验特征，探索构成性与学习动态、人类判断、表示相似性和泛化之间的关系。
在本文中，我们提出了一个端到端的深度学习模型，称为E2Efold，用于RNA二级结构预测，可以有效地考虑到问题中的固有约束。E2Efold的关键思想是直接预测RNA碱基配对矩阵，并使用unrolled约束编程算法作为架构中的构建块来执行约束。 通过对基准数据集的综合实验，我们证明了E2Efold的优越性能：与以前的SOTA相比，它预测的结构明显更好（在某些情况下，F1得分提高了29.7%，对假结点结构的提高甚至更大），并且在推理时间上与最快的算法一样高效。
递归神经网络（RNN）中的学习最常通过梯度下降实现，使用通过时间的反向传播（BPTT），但BPTT不能准确模拟大脑的学习方式。相反，许多关于突触可塑性的实验结果可以总结为三因素学习规则，涉及局部神经活动的资格痕迹和第三个因素。 我们在此提出了资格传播（e-prop），这是RNNs中损失梯度的一个新的因子化，当为生物物理学尖峰神经元模型推导时，它符合三因素学习规则的框架。当在TIMIT语音识别基准测试时，它与BPTT在训练人工LSTM网络和尖峰RNN方面都具有竞争力。进一步分析表明，学习信号的多样性和考虑缓慢的内部神经动力学对e-prop的学习效率是决定性的。
循环神经网络（RNN）是广泛的强化和模仿学习问题的控制策略的有效表示。然而，RNN策略由于使用连续值的记忆向量和观察特征而特别难以解释、理解和分析。在本文中，我们引入了一种新的技术，量化瓶颈插入，来学习这些向量和特征的有限表示。 结果是RNN的量化表示，可以通过分析来提高我们对内存使用和一般行为的理解。我们在合成环境和六个Atari游戏上展示了这种方法的结果。所产生的有限表示在某些情况下是惊人的小，对于一个完美的Pong策略，只用了3个离散的内存状态和10个观察。
在最近深度强化学习方法成功的基础上，我们研究了通过重复使用几个连续政策的数据来改进政策上强化学习的可能性。政策上的方法带来了许多好处，如评估每个结果的能力。 为了实现这一点，所提出的算法对来自多个政策的数据的Q-、值和优势函数进行了概括。该方法使用信任区域优化，同时避免了TRPO或ACKTR等算法的一些常见问题：它使用超参数来取代信任区域选择启发式方法，以及使用可训练的协方差矩阵取代固定的协方差矩阵。 在许多情况下，该方法不仅改善了与最先进的信任区域政策性学习算法（如ACKTR和TRPO）相比的结果，而且还改善了与非政策性学习算法DDPG的对比。 
卷积神经网络（CNN）在处理空间域中均匀采样的数据信号方面取得了成功（例如，图像）。然而，大多数数据信号本身并不存在于网格上，在被采样到一个统一的物理网格的过程中，会出现明显的混叠误差和信息损失。此外，信号可以存在于不同的拓扑结构中，例如，点、线、面和体。分析混合拓扑结构的信号（例如，点云与面网）一直是一个挑战。） 为此，我们开发了非均匀傅里叶变换（NUFT）的数学公式，以直接和最优化的方式将定义在简单网格上的不同拓扑结构的非均匀数据信号取样到谱域中，并且没有空间取样误差。谱变换是在欧几里得空间进行的，这消除了图谱工作中的翻译模糊性。(1)该过程在初始采样时不会引起空间采样误差，(2)该方法的通用性为使用CNN分析混合拓扑结构的信号提供了一个统一的框架，(3)它允许我们利用最先进的骨干CNN架构进行有效的学习，而不必为特定的数据结构临时设计一个特定的架构，以及(4)该表示允许加权网格，每个元素有不同的权重（即。我们在三维形状检索任务中取得了与最先进水平相当的良好结果，在点云到表面重建任务中取得了新的最先进水平。
发现一组变量之间的因果结构是许多经验科学中的一个基本问题。传统的基于分数的随机发现方法依赖于各种局部启发式方法，根据预定的分数函数搜索有向无环图（DAG）。贪婪的等价搜索，在无限的样本和某些模型假设下可能有吸引人的结果，但由于有限的数据和可能违反的假设，它们在实践中不太令人满意。 奖励包含了预定义的分数函数和两个惩罚项，用于强制执行非周期性。与典型的RL应用相比，其目标是学习一个策略，我们将RL作为一种搜索策略，我们的最终输出将是在训练期间生成的所有图中获得最佳奖励的图。我们在合成数据集和真实数据集上进行了实验，并表明所提出的方法不仅具有改进的搜索能力，而且还允许在非周期性约束下使用灵活的分数函数。
主题建模发现了给定文本文档的潜在主题概率。为了生成更能代表给定文档的更有意义的主题，我们提出了一种可以在数据预处理阶段使用的通用方法。该方法包括三个步骤。首先，它从每一个文档中生成词/词对。第二，它对词/词对应用双向平行TF-IDF算法进行语义过滤。第三，它使用k-means算法来合并具有相似语义的词对。 我们在开放电影数据库（OMDb）、路透社数据集和20NewsGroup数据集上进行了实验，并使用平均精度分数作为评价指标。将我们的结果与其他最先进的主题模型，如Latent Dirichlet分配和传统的受限玻尔兹曼机进行比较。
多任务学习已经成功地用大型的、精心策划的标签数据集对多个相关的任务进行建模。通过利用不同任务之间的关系，多任务学习框架可以显著提高性能。然而，大多数现有的工作是在假设预定义的任务是相互关联的。 沿着这个思路，我们提出了一个新颖的多任务学习框架--通过多级任务依赖关系建模进行学习转移，该框架构建了不同任务之间基于注意力的依赖关系。同时，依赖关系可以用来指导哪些知识应该被转移，因此我们模型的性能也得到了提高。为了证明我们模型的有效性和考虑多级依赖关系的重要性，我们在几个公共数据集上进行了实验，在这些数据集上我们获得了比当前方法更多的改进。
 我们在MNIST数据集上训练的深度学习模型中设计了简单的、可量化的全局翻译不变性测试。卷积和胶囊神经网络的实验表明，两种模型在处理全局翻译不变性方面的性能都很差；但是，通过使用数据增强，性能有所提高。虽然胶囊网络在MNIST测试数据集上的表现更好，但卷积神经网络在翻译不变性上的表现普遍更好。
高斯过程在自然界和工程中无处不在。一个典型的例子是无限带宽极限下的一类神经网络，其先验参数与高斯过程相对应。这里我们将这种对应关系微扰地扩展到有限带宽神经网络，产生非高斯过程作为先验参数。 这里开发的方法使我们能够通过逐步整合从低层到高层的随机变量来跟踪预激活分布的流动，让人联想到重正化组的流动。我们进一步开发了一个微扰处方，以弱非高斯预设执行贝叶斯推理。
蒸馏是一种将知识从一个模型转移到另一个模型的方法，通常可以在相同的容量下实现更高的精度。在本文中，我们旨在提供一个理论上的理解，即主要是什么有助于蒸馏。我们的答案是 "早期停止"。 这可以用一个新的概念--各向异性检索（Anisotropic In- formation Retrieval，AIR）来证明，这意味着神经网络倾向于先适应信息性信息，而后适应非信息性信息（包括噪声）。在理论上分析过度参数化神经网络的最新发展的激励下，我们可以通过神经切线核（NTK）的特征空间来描述AIR。 我们提出了一种自我蒸馏的算法，从网络中连续蒸馏出前一个训练周期的知识，以避免记忆错误的标签。我们还从理论和经验上证明，自我蒸馏不仅可以从早期停止中获益。 理论上，我们证明了所提出的算法在l2距离方面对随机初始化的超参数化神经网络的收敛性，而之前的结果是在0-1损失方面的收敛性。理论上的结果保证了学习的神经网络在训练数据上享有一定的余量，从而导致更好的泛化。
终身学习在有效性（最小化所有任务的预测误差）和实时性能的整体计算可操作性方面带来了相当大的挑战。 本文通过在每一轮联合重新估计任务间关系（textit{输出}核）和每个任务的模型参数来解决连续的终身多任务学习，假设数据以流的方式到达。(为了避免内存爆炸，我们提出了一种稳健的预算有限的算法，有效地利用任务之间的关系来约束支持集中的代表性例子的总数。 此外，我们提出了一个两阶段的预算方案，以有效地解决终身学习中特定任务的预算约束。我们在三个数据集上的经验结果表明，OOKLA及其预算有限的表兄弟的AUC性能优于强基线。
Minecraft是一款电子游戏，它为人工智能系统提供了许多有趣的挑战。在本文中，我们专注于建筑场景，其中代理必须建立一个由单个块组成的复杂结构。由于高级对象是由低级对象组成的，建筑可以自然地被建模为分层任务网络。
对自然语言模型的攻击很难比较，因为他们对什么是成功的攻击有不同的定义。我们提出了一个约束的分类法来对这些攻击进行分类。对于每个约束，我们提出了一个现实世界的用例和一种方法来衡量生成的样本执行约束的程度。 然后，我们采用我们的框架来评估两种最先进的攻击，它们用同义词替换来愚弄模型。这些攻击声称它们的对抗性扰动保留了输入的语义和句法的正确性，但我们的分析表明这些约束没有得到强有力的执行。 对于这些对抗性例子中的很大一部分，语法检查器检测到了错误的增加。此外，人类研究表明，许多这些对抗性例子在语义上与输入有分歧，或者看起来不是人类写的。最后，我们强调需要对共享约束的攻击进行标准化评估。如果没有共享的评估指标，就要由研究人员来设置阈值，确定攻击质量和攻击成功之间的权衡。
为了使机器学习在许多应用中得到部署和信任，能够可靠地解释为什么机器学习算法会做出某些预测是至关重要的。例如，如果一个算法将给定的病理图像分类为恶性肿瘤，那么医生可能需要知道图像的哪些部分导致了算法的这种分类。因此，如何解释黑盒预测器是一个重要而活跃的研究领域。 一个基本问题是：我们能在多大程度上相信解释本身？在本文中，我们表明深度学习预测的解释在以下意义上是非常脆弱的：两个感知上无法区分的输入，具有相同的预测标签，可以被赋予非常不同的}解释。我们在ImageNet和CIFAR-10上系统地描述了几个广泛使用的特征-重要性解释方法（突出度地图、综合梯度和DeepLIFT）产生的解释的脆弱性。 我们的实验表明，即使是小的随机扰动也能改变特征的重要性，而新的系统扰动可以在不改变标签的情况下导致极大的不同解释。我们将这些结果扩展到表明，基于典范的解释（如影响函数）也同样脆弱。
在本文中，我们考虑了以深度神经网络作为预测模型的随机AUC最大化问题。基于AUC的代用损失的鞍点重构，该问题可以被投射到{/it非凸凹}最小最大值问题。 本文的主要贡献是使随机AUC最大化在深度神经网络和大数据中更加实用，同时也有理论上的见解。特别是，我们建议探索Polyak-\L{}ojasiewicz（PL）条件，该条件在深度学习中已经被证明和观察到，这使我们能够开发新的随机算法，具有更快的收敛率和更实用的步骤大小方案。
为强化学习（RL）设计奖励是具有挑战性的，因为它需要传达所需的任务，有效地进行优化，并易于计算。后者在将RL应用于机器人时尤其成问题，因为检测是否达到所需的配置可能需要大量的监督和仪器。此外，我们通常对能够达到广泛的配置感兴趣，因此每次设置不同的奖励可能是不切实际的。 在这项工作中，我们研究了不同的方法来结合演示，以大大加快收敛到能够达到任何目标的策略，也超过了用其他模仿学习算法训练的代理的性能。此外，我们的方法可以在只有没有专家行动的轨迹时使用，这可以利用运动学或第三人称演示。
贝叶斯神经网络，既使用负对数似然损失函数，又使用对参数的学习后验对其预测进行平均，已经在许多科学领域成功使用，部分原因是它们能够 "轻松 "地从许多大规模数据集中提取所需的表征。 在本文中，我们提出了一个新的PAC-Bayesian的负对数似然损失的泛化约束，它利用对数索博列夫不等式的emph{Herbst Argument}来约束学习者风险的矩生成函数。
数据增强技术，例如翻转或裁剪，通过明确地产生更多的训练样本来系统地扩大训练数据集，可以有效地提高深度神经网络的泛化性能。在有监督的设置中，数据增强的常见做法是给同一来源的所有增强样本分配相同的标签。为了解决这个问题，我们提出了一个简单而有效的想法，即学习增强样本的原始标签和自我监督标签的联合分布。联合学习框架更容易训练，并且能够结合不同增强样本的预测进行聚合推理以提高性能。 此外，为了加快聚合过程，我们还提出了一种知识转移技术，即自我蒸馏，将增强的知识转移到模型本身。我们在各种完全监督的情况下证明了我们的数据增强框架的有效性，包括少量和不平衡的分类情况。
长短时记忆（LSTM）网络允许表现出具有反馈连接的时间动态行为，似乎是学习三维网格序列的自然选择。我们介绍了一种用于汽车碰撞数值模拟的动态网格表示方法。 为了绕过使用三维网格的复杂性，我们将表面网格序列转化为有效编码形状的光谱描述符。我们选择了一个基于LSTM的两分支网络架构来学习模拟过程中碰撞的表现和动态。 它使用一个编码器LSTM将输入序列映射成一个固定长度的向量表示。在这个表示上，一个解码器LSTM执行输入序列的重建，而另一个解码器LSTM通过接收序列的初始步骤作为种子预测未来的行为。 对模型的时空误差行为进行了分析，以研究该模型能将学到的光谱描述符推断到未来的程度，也就是说，它学会了如何代表潜在的动态结构力学。考虑到只有少数训练实例可用，这是数值模拟的典型情况，该网络的表现非常好。
编码模型的目的是预测给定刺激的大脑活动。在这个贡献中，我们试图在自然刺激环境中估计听觉的全脑编码模型。我们分析了一个开放数据集的数据，其中16名受试者观看了一部短片，同时他们的大脑活动正在用功能磁共振成像进行测量。 我们提取了与电影中的音频时间相一致的特征向量，在深度神经网络的不同层次上对听觉场景的分类进行了预训练。fMRI数据使用分层聚类对500个包裹进行了划分，并使用具有一个隐藏层的全连接神经网络对编码模型进行了估计，该网络被训练为从DNN特征预测每个包裹的信号。 单个编码模型被成功训练，并预测了位于上颞叶以及背外侧前额叶区域的未见数据的大脑活动，这些区域通常被认为是参与听觉和语言处理的区域。总之，这一贡献扩展了之前关于估计编码模型的尝试，显示了使用通用DNN（即不是专门为此目的而训练）来提取听觉特征的大脑活动模型的能力，表明内部DNN表示和自然环境下的大脑活动之间有一定的相似性。
在本文中，我们描述了 "隐式自动编码器"（IAE），这是一个生成式自动编码器，其中生成路径和识别路径都是由隐式分布参数化的。我们使用两个生成式对抗网络来定义隐式自动编码器的重建和正则化成本函数，并根据最大似然学习推导出学习规则。 学习富有表现力的条件似然分布使潜伏代码只捕获数据的抽象和高级信息，而其余的信息则由隐性条件似然分布捕获。例如，我们表明隐性自动编码器可以拆分全局和局部信息，并对图像进行确定性或随机性重建。我们进一步表明，隐性自动编码器可以以无监督的方式从连续因素中拆分出离散的基本变化因素，并进行聚类和半监督性学习。
强烈的归纳偏见使儿童能够以快速和可适应的方式学习。儿童使用相互排斥性（ME）偏见来帮助区分单词如何映射到指称，假设如果一个对象有一个标签，那么它不需要另一个标签。在本文中，我们研究了标准神经架构是否有ME偏见，证明它们缺乏这种学习假设。 此外，我们表明它们的归纳偏见与分类和翻译的终身学习公式不匹配。我们证明有一个令人信服的理由来设计通过互斥性推理的神经网络，这仍然是一个公开的挑战。
皮质神经元在多个时间尺度上处理和整合信息。此外，这些时间尺度或时间感受野显示出功能和层次组织。例如，对工作记忆（WM）很重要的区域，如前额皮层，利用具有稳定时间感受野和长时间尺度的神经元来支持刺激的可靠表示。 尽管最近在实验技术方面取得了进展，但足以支持WM的神经元时标的出现的基本机制还不清楚，而且在实验上的调查也具有挑战性。在这里，我们证明了为执行WM任务而设计的尖峰递归神经网络（RNNs）再现了以前观察到的实验结果，这些模型可以在未来被用来研究WM特有的神经元时标是如何出现的。
传统的深度学习分类器是静态的，因为它们是在一组预定义的类别上训练的，学习分类一个新的类别通常需要重新训练。在这项工作中，我们解决了低速网络扩展学习的问题。 我们引入了一个学习框架，当新的类别的例子数量特别少时，可以扩展一个预先训练好的（基础）深度网络来对新的类别进行分类。我们提出了一个简单而强大的蒸馏方法，基础网络被增加了额外的权重来分类新的类别，同时保持基础网络的权重不变。 我们称这种学习为硬蒸馏，因为我们保留了网络对旧类的反应，使其在基础网络和扩展的网络中都是相同的。我们发现，由于只需要训练少量的权重，硬蒸馏法在低剂量的训练场景中表现出色。 最后，我们表明，通过使用基础类训练数据的紧凑生成模型，可以以非常小的内存足迹完成低镜头网络的扩展，相对于使用完整训练集的学习，其退化程度可以忽略不计。
我们提出了一个用于模拟人类提问的神经程序生成框架，该框架将问题表示为形式化程序，并通过基于编码器-解码器的深度神经网络生成程序。通过使用信息搜索游戏的大量实验，我们表明我们的方法可以在合成环境中提出最佳问题，并预测人类在无约束环境中可能提出的问题。
在除空气之外的特殊成像环境中拍摄的图像分类是扩展深度学习应用的第一个挑战。我们报告了一个UW-Net（水下网络），一个新的基于卷积神经网络（CNN）的网络，用于水下图像分类。在这个模型中，我们通过构建一个初始注意力（I-A）模块，模拟了背景注意力与特殊环境（如雾和水下）图像理解的视觉相关性。 此外，我们证明了所提出的IA模块可以用来提高现有物体识别网络的性能。通过用I-A模块替代inception模块，Inception-ResnetV2网络在ILSVRC-2012的子集上实现了10.7%的top1错误率和0%的top5错误率，这进一步说明了背景注意力在图像分类中的作用。
深度网络在各种重要的任务中都取得了令人印象深刻的结果。然而，一个已知的弱点是，当对与训练分布不同的数据进行评估时，即使这些差异非常小，也不能表现得很好，如对抗性例子的情况。 我们提出了 "强化网络"（Fortified Networks），这是一种对现有网络的简单改造，它通过识别隐藏状态偏离数据流形的时间来 "强化 "深度网络中的隐藏层，并将这些隐藏状态映射回网络表现良好的数据流形部分。 我们的主要贡献是表明强化这些隐藏状态可以提高深度网络的鲁棒性，我们的实验（i）证明了在黑盒和白盒威胁模型中对标准对抗性攻击的鲁棒性有所提高；（ii）表明我们的改进主要不是由于梯度信号质量下降导致的欺骗性好结果的问题（梯度掩蔽问题）；（iii）表明在隐藏层而不是在输入空间进行强化的优势。 我们在三个数据集（MNIST、Fashion MNIST、CIFAR10）上展示了对抗性鲁棒性的改进，跨越了几个攻击参数，包括白盒和黑盒设置，以及最广泛研究的攻击（FGSM、PGD、Carlini-Wagner）。 我们表明，这些改进是在各种各样的超参数下实现的。 
神经网络可能对与其训练数据略有不同的输入进行错误分类，这表明其决策边界与训练数据集之间的余量很小。在这项工作中，我们研究了线性可分离数据集的二元分类，并表明如果使用交叉熵损失进行训练，线性分类器的决策边界也可能位于其训练数据集附近。 特别是，我们表明，如果训练数据集的特征位于低维仿生子空间，并且使用梯度方法使交叉熵损失最小化，那么训练点和决策边界之间的余量可能比最优值小得多。这一结果与最近的相关工作的结论相反，如（Soudry et al, 2018），我们找出了这种矛盾的原因。为了提高余量，我们引入了差分训练，这是一种训练范式，使用定义在每个类的点对上的损失函数。我们表明，用差分训练训练的线性分类器的决策边界确实达到了最大的余量。结果揭示了使用交叉熵损失是对抗性例子的隐藏罪魁祸首之一，并引入了一个新的方向，使神经网络对它们具有鲁棒性。
单元演化矩阵和关联记忆的概念推动了循环神经网络（RNN）领域在各种序列任务中的最先进性能。 然而，RNN在操纵长期记忆方面的能力仍然有限。 为了绕过这个弱点，RNN最成功的应用是使用外部技术，如注意力机制。在本文中，我们提出了一个新的RNN模型，统一了最先进的方法。RUM的核心是它的旋转操作，这自然是一个单元矩阵，通过克服梯度消失和爆炸问题，为架构提供了学习长期依赖关系的能力。 此外，旋转单元也可作为联想记忆。我们在合成记忆、问题回答和语言建模任务上评估了我们的模型。  RUM完全学会了复制记忆任务，并在回忆任务中提高了最先进的结果。 RUM在bAbI问题回答任务中的表现与具有注意力机制的模型相当。我们还在字符级Penn Treebank（PTB）任务中把最先进的结果提高到1.189比特/字符（BPC）的损失，这是为了标志着RUM在现实世界的顺序数据中的应用。我们的构造的普遍性，在RNN的核心，确立了RUM作为语言建模、语音识别和机器翻译的一个有前途的方法。
虽然最近在深度强化学习方面的许多进展都依赖于无模型方法，但基于模型的方法仍然是一个诱人的前景，因为它们有可能利用无监督数据来学习环境动态。一个前景是追求混合方法，如AlphaGo，它将蒙特卡洛树搜索（MCTS）--一种基于模型的方法--与深Q网络（DQNs）--一种无模型的方法相结合。 MCTS需要生成滚动，这在计算上是很昂贵的。在本文中，我们建议模拟滚动，利用图像到图像转换的最新突破，即Pix2Pix GANs，来预测环境的动态。我们提出的算法，生成式对抗树搜索（GATS），使用基于GAN的动态模型和奖励预测器模拟指定深度的滚动。 GATS采用MCTS对模拟样本进行规划，并使用DQN来估计叶子状态下的Q函数。我们的理论分析确定了GATS相对于偏差-变异权衡的一些有利特性，经验结果表明，在5个流行的Atari游戏中，动力学和奖励预测器迅速收敛到准确的解决方案。 然而，GATS在5个游戏中的4个游戏中未能超过DQNs。值得注意的是，在这些实验中，MCTS只有很短的滚动（高达4的树深），而MCTS以前的成功涉及数百个树深。我们提出了一个假设，说明为什么即使给定完美的建模，短滚动的树搜索也会失败。
我们提出了一种新的GAN架构，用于分解表征学习。新的模型架构受到信息瓶颈（IB）理论的启发，因此被命名为IB-GAN。IB-GAN的目标与InfoGAN相似，但有一个关键的区别；采用了相互信息的容量正则化，由于IB-GAN的发生器可以以分解和可解释的方式利用潜在的表征。 通过在CelebA、3DChairs和dSprites数据集上的实验，我们证明了由IB-GAN生成的样本的视觉质量通常优于β-VAEs生成的样本。此外，IB-GAN在dSprites数据集上取得了比β-VAEs或InfoGAN高得多的解缠指标得分。
我们研究了使用最大平均差异（MMD）作为批判者的生成对抗网络的训练和性能，称为MMD GANs.作为我们的主要理论贡献，我们澄清了最近的工作提出的GAN损失函数中的偏差情况：我们表明，在MMD GANs和Wasserstein GANs的优化过程中使用的梯度估计器是无偏的，但基于样本学习判别器会导致生成器参数的梯度有偏。 我们还讨论了MMD判别器的核选择问题，并对用于Cramér GAN判别器的能量距离对应的核进行了描述。作为一个整体概率度量，MMD受益于最近为Wasserstein GANs开发的训练策略。 在实验中，MMD GAN能够采用比Wasserstein GAN更小的批评者网络，从而产生一个更简单、更快速的训练算法，并具有匹配的性能。我们还提出了一个改进的GAN收敛措施，即内核初始距离，并展示了如何使用它在GAN训练期间动态调整学习速率。
我们将共识网络框架扩展到过渡性共识网络（TCN），一个半监督的多模式分类框架，并确定其两个机制：共识和分类。通过提出三个变体作为消融研究，我们表明这两个机制应该一起运作。总体而言，当只有20到200个标记的数据点时，TCN的表现优于或与最佳基准算法一致。
分离混合分布是机器学习和信号处理的一个长期挑战。应用包括：单通道多扬声器分离（鸡尾酒会问题）、唱歌的声音分离和从图像中分离反射。目前的大多数方法要么依赖于对源分布的强烈假设（如稀疏性、低等级、重复性），要么依赖于拥有混合物中每个源的训练样本。在某些情况下，神经蛋分离对初始化很敏感，因此我们引入了GLO屏蔽，以确保良好的初始化。广泛的实验表明，我们的方法优于目前使用相同水平监督的方法，并经常达到与完全监督相似的性能。
在健康领域，机器学习越来越普遍，然而神经网络嵌入（表征）学习在生理信号方面可以说是利用不足。 这种不足与更传统的计算机科学领域，如计算机视觉（CV）和自然语言处理（NLP）形成了鲜明的对比。 对于生理信号来说，学习特征嵌入是解决病人隐私问题造成的数据不足的一个自然解决方案--研究人员可以分享信息嵌入模型（即表示模型），将病人数据映射到输出嵌入，而不是分享数据。  在这里，我们提出了PHASE（PHysiologicAl Signal Embeddings）框架，它由三个部分组成：i）学习生理信号的神经网络嵌入，ii）根据学到的嵌入预测结果，以及iii）通过估计 "堆叠 "模型（即特征嵌入模型之后是预测模型）中的特征归属解释预测结果。 PHASE在三个方面是新颖的。1）据我们所知，PHASE是第一个转移神经网络以创建生理信号嵌入的实例。2）我们提出了一个可操作的方法，通过堆叠模型获得特征归属。 我们证明了我们的堆叠模型归属可以近似于Shapley值--已知的具有理想属性的归属--对于任意的模型集合。 在我们的实验中，我们表明PHASE明显优于目前使用的其他嵌入--如原始、指数移动平均/变量和自动编码器。此外，我们提供证据表明，在不同的医院之间转移神经网络嵌入/表示学习者仍然可以产生高性能的嵌入，并在转移无效时提供建议。
我们考虑字典学习问题，其目的是将给定的数据建模为一个被称为字典的矩阵的几列的线性组合，其中形成线性组合的稀疏权重被称为系数。由于字典和系数，参数化的线性模型是未知的，相应的优化是固有的非凸的。 这是一个重大的挑战，直到最近，有人提出了字典学习的可证明算法。然而，这些算法只对字典的恢复提供了保证，而对系数没有明确的恢复保证。此外，字典中的任何估计错误都会对成功定位和估计系数的能力产生负面影响。 这潜在地限制了现有的可证明的字典学习方法在对系数恢复感兴趣的应用中的效用。为此，我们开发了NOODL：一个简单的神经可信的基于交替优化的在线字典学习算法，在适当初始化的情况下，以几何速率精确地恢复字典和系数。 我们的算法，NOODL，也是可扩展的，适合在神经结构中的大规模分布式实现，我们的意思是，它只涉及简单的线性和非线性操作。最后，我们通过对所提出的算法与当前最先进的技术进行实验评估来证实这些理论结果。
在这项工作中，我们提出了数据增强关系提取（DARE），这是一种简单的方法，通过适当地微调GPT2来增加训练数据，以生成特定关系类型的例子。 在一系列的实验中，我们展示了我们的方法的优势，与强大的基线相比，我们的方法导致了高达11个F1得分点的改进。同时，DARE在三个广泛使用的生物医学RE数据集中取得了新的先进性，平均超过了以前的最佳结果4.7个F1点。
本文讨论了使用多变量正态分布（MND）表示系统信念的问题，其中底层模型是基于深度神经网络（DNN）的。DNN的主要挑战是使用MND获得模型不确定性所需的计算复杂性。为了实现可扩展的方法，我们提出了一种新的方法，以稀疏信息形式表达参数后验。 我们的推理算法是基于一种新型的拉普拉斯近似方案，它涉及到克朗克因子特征基的对角线修正。由于这使得信息矩阵的反转难以实现--这是完全贝叶斯分析所需要的操作，我们设计了这种特征基的低等级近似和一种内存高效的采样方案。
现代数据集的规模不断扩大，加上获取标签信息的难度，使得半监督学习在现代机器学习应用中具有重要的实际意义。与监督学习相比，半监督学习的关键困难在于如何充分利用未标记的数据。 这两部分相互补充，共同沿着两个不同的方向执行平滑性，这对半监督学习至关重要。一个是沿着数据流形的切线空间应用，目的是在流形上执行分类器的局部不变性，而另一个是在与切线空间正交的法线空间上执行，目的是对分类器施加鲁棒性，使其免受噪声导致的观察数据偏离基础数据流形的影响。 我们的方法在人工数据集和实际数据集的半监督学习任务上都取得了最先进的性能。
神经网络的通用逼近特性是在各种实际问题中使用这些模型的动机之一。然而，这一特性并不是使神经网络独特的唯一特征，因为还有很多其他具有类似特性的方法。另一个使这些模型有趣的特性是它们可以用反向传播算法进行训练，这使得有效的梯度计算成为可能，并使这些通用逼近器具有从不同领域的大量数据中有效学习复杂流形的能力。 尽管神经网络在实践中被大量使用，但人们对它的理解仍然不深，正在进行的广泛研究是研究神经网络的可解释性。另一方面，拓扑数据分析（TDA）依赖于强大的（代数）拓扑学理论框架以及其他数学工具来分析可能复杂的数据集。 在这项工作中，我们利用源自代数拓扑学的普遍近似定理，在TDA和普通神经网络训练框架之间建立了联系。我们引入了自动细分的概念，并设计了一种特殊类型的神经网络用于回归任务。简单复杂网络（SCNs）。SCN的架构是由一组偏置函数以及前向传递过程中的特定政策来定义的，它替代了神经网络中常见的架构搜索框架。我们认为SCNs的观点可以作为建立可解释的深度学习模型的一个步骤。最后，我们在一组回归问题上验证了它的性能。
网络表征学习的目标是学习低维节点嵌入，以捕捉图的结构并对解决下游任务有用。然而，尽管这种方法激增，但目前还没有研究它们对对抗性攻击的鲁棒性。 我们推导出有效的对抗性扰动，这些扰动会毒害网络结构，并对嵌入的质量和下游任务产生负面影响。我们进一步表明，我们的攻击是可转移的，因为它们可以推广到许多模型，甚至在攻击者受到限制时也能成功。
分层强化学习方法提供了在复杂领域规划灵活行为的强大手段。然而，学习一个领域的适当分层分解为子任务仍然是一个巨大的挑战。我们提出了一个新的子任务发现算法，基于最近引入的多任务线性可解马尔科夫决策过程（MLMDP）框架。 MLMDP可以执行从未见过的任务，将它们表示为先前学到的任务基础集的线性组合。在这种情况下，子任务发现问题可以自然地被设定为寻找代理人在某一领域将面临的任务集的最佳低等级近似。 我们使用非负矩阵分解来发现这个最小的基础任务集，并表明该技术在各种领域中学习了直观的分解。我们的方法有几个定性的理想特征：它不限于学习具有单一目标状态的子任务，而是学习首选状态的分布模式；它在同一领域中学习了定性的不同层次的分解，取决于代理人将面临的任务集合；它可以直接迭代以获得更深的层次分解。
本文介绍了一个通过从优化问题的输入输出例子中学习来解决组合优化问题的框架。我们引入了一个新的内存增强神经模型，其中的内存是不可重置的（即处理完一个输入例子后存储在内存中的信息会保留到下一个看到的例子中）。 我们使用深度强化学习来训练记忆控制器代理来存储有用的记忆。我们的模型能够在二元线性编程（Binary LP）上胜过手工制作的求解器。所提出的模型在具有大量变量（多达1000个变量）和约束（多达700个约束）的不同二元LP实例上进行测试。
具有注意力的序列到序列（Seq2Seq）模型在涉及生成自然语言句子的任务中表现出色，如机器翻译、图像说明和语音识别。通过利用未标记的数据，通常以语言模型的形式，进一步提高了性能。 在这项工作中，我们提出了Cold Fusion方法，该方法在训练过程中利用预先训练好的语言模型，并展示了它在语音识别任务上的有效性。我们表明，带有Cold Fusion的Seq2Seq模型能够更好地利用语言信息，享有i）更快的收敛性和更好的概括性，ii）几乎完全转移到一个新的领域，同时使用不到10%的标记训练数据。
元学习（Meta-learning）认为这个问题是对模型参数的先验学习，可以在新的任务上进行快速适应，但通常假设一组任务是作为一个批次一起提供的。相反，在线（基于后悔的）学习考虑了一个连续的设置，其中问题是一个接一个被揭示的，但传统上只训练一个模型，没有任何特定的任务适应。 这项工作引入了一个在线元学习环境，它融合了上述两种范式的思想，以更好地捕捉持续终身学习的精神和实践。我们提出了跟随元领导（FTML）算法，它将MAML算法扩展到这个环境。
我们研究了在预训练的转化器语言模型（如BERT和RoBERTa）中单个注意力头隐含地捕获句法依赖关系的程度。我们采用两种方法--采取最大注意力权重和计算最大跨度树--从每层/头的注意力权重中提取隐含的依赖关系，并将它们与真实的通用依赖（UD）树进行比较。 我们表明，对于某些UD关系类型，存在着能够在解析的英语文本上恢复依赖关系类型的头，明显优于基线，这表明一些自我关注的头作为句法结构的代理。 我们还在两个数据集--面向句法的CoLA和面向语义的MNLI上分析了BERT的微调，以研究微调是否影响其自我关注的模式，但我们没有观察到使用我们的方法提取的整体依赖关系的实质性差异。 我们的结果表明，这些模型有一些跟踪个别依赖关系类型的专业注意头，但没有一个执行整体解析的通用头，明显优于琐碎的基线，而且直接分析注意权重可能不会揭示许多BERT式模型已知学习的句法知识。
最先进的人脸超级分辨率方法采用深度卷积神经网络，通过探索局部外观知识来学习低分辨率和高分辨率面部模式之间的映射。然而，这些方法大多没有很好地利用面部结构和身份信息，并且难以处理表现出巨大姿势变化和错位的面部图像。在本文中，我们提出了一种新型的人脸超级分辨率方法，明确地纳入了掌握尖锐面部结构的三维面部先验因素。 首先，建立三维面部渲染分支，以获得突出的面部结构和身份知识的三维先验。其次，空间注意机制被用来更好地利用这种层次信息（即强度相似性、三维面部结构、身份内容）来解决超分辨率问题。广泛的实验表明，所提出的算法实现了卓越的面部超分辨率结果，并超过了最先进的算法。
我们提出了跨视图训练（CVT），这是一种简单而有效的深度半监督学习方法。在有标签的例子上，模型是用标准的交叉熵损失来训练的。 然后，模型从这些软目标中学习（充当 "学生"）。我们与之前的工作不同，在模型中加入了多个辅助学生预测层。学生可以从教师（完整模型）那里学习，因为教师看到了更多的例子。同时，学生在学习用有限的数据进行预测的过程中，提高了教师使用的表征的质量。当与虚拟对抗训练相结合时，CVT改善了目前最先进的半监督CIFAR-10和半监督SVHN。 我们还应用CVT在五项自然语言处理任务上训练模型，使用的是数以亿计的无标签数据。在所有任务上，CVT都大大超过了单独的监督学习，所产生的模型比目前最先进的模型更有优势。
我展示了用与统一[0,1]值的比较来表达Metropolis的接受/拒绝决定是如何有益的，然后作为马尔科夫链状态的一部分不可逆地更新这个统一值，而不是每次迭代都独立采样。 当使用具有持续动量的Langevin更新时，它产生了更大的改进，其性能可与具有长轨迹的Hamilton Monte Carlo（HMC）相比。 当一些变量被其他方法更新时，这具有重要意义，因为如果使用HMC，这些更新只能在轨迹之间进行，而使用Langevin更新则可以更频繁地进行。 这一点在贝叶斯神经网络模型中可以看到，其中连接权重通过持续的Langevin或HMC更新，而超参数则通过Gibbs采样更新。
我们介绍了ES-MAML，这是一个基于进化策略（ES）的解决模型不可知元学习（MAML）问题的新框架。MAML的现有算法是基于策略梯度的，当试图使用随机策略的反向传播来估计二阶导数时，会遇到很大困难。 我们展示了如何将ES应用于MAML以获得一种算法，该算法避免了估计二阶导数的问题，并且在概念上也很简单，易于实现。此外，ES-MAML可以处理新型的非光滑适应算子，其他用于改善ES方法的性能和估计的技术也变得适用。
在贝叶斯推理和机器学习中，将一个概率分布转换为另一个概率分布是一个强大的工具。一些突出的例子是分布的有约束到无约束的转换，用于哈密尔顿蒙特卡洛和构建灵活和可学习的密度，如归一化流。我们介绍了Bijectors.jl，一个用Julia实现分布转换的软件包，可在github.com/TuringLang/Bijectors.jl.该软件包提供一个灵活和可组合的方式来实现分布的转换，而不被一个计算框架所约束。我们展示了Bijectors.jl在改进变异推理方面的应用，它通过使用归一化流将已知的统计依赖性编码到变异后验中，提供了一种放宽变异推理中通常使用的均值场假设的通用方法。
多域学习（MDL）的目的是获得一个跨多个域的平均风险最小的模型。我们的经验动机是自动显微镜数据，其中培养的细胞在暴露于已知和未知的化学扰动后被成像，每个数据集都显示出明显的实验偏差。本文提出了一个多域对抗性学习方法，MuLANN，在半监督的环境下，利用具有重叠但不同类别集的多个数据集。 我们的贡献包括：i）使用H-发散得到的MDL中平均和最差领域风险的约束；ii）适应半监督多领域学习和领域适应的新损失；iii）该方法的实验验证，在两个标准图像基准和一个新的生物图像数据集Cell上改善了技术状态。
我们介绍了我们的分布回归网络（DRN），它从输入概率分布到输出概率分布进行回归。与现有的方法相比，DRN用较少的模型参数进行学习，并且容易扩展到多个输入和多个输出分布。
现有的序列预测方法大多关注与时间无关的序列，其中事件之间的实际时间跨度是不相关的，事件之间的距离只是它们在序列中的顺序位置之间的差异。虽然这种与时间无关的序列观点适用于自然语言等数据，例如，处理句子中的单词，但对于许多现实世界的事件来说，它是不合适的，也是低效的，因为它们是在不平等的时间间隔点上观察和收集的，因为它们自然产生，例如，。在这项工作中，我们提出了一套在序列预测中使用时间的方法。由于神经序列模型如RNN更适合于处理标记式输入，我们提出了两种与时间相关的事件表示方法，基于日常生活中时间如何被标记的直觉和以前关于嵌入上下文的工作。 我们还介绍了两种使用下一个事件持续时间作为训练序列预测模型的正则化的方法。我们讨论了这些基于递归神经网络的方法。我们在五个类似于各种序列预测任务的数据集上评估了这些方法以及基线模型。
非政策性强化学习算法承诺适用于只有固定的环境互动数据集（批处理）且不能获得新经验的环境。 它允许使用由任意行为策略产生的数据，并使用学习到的先验--优势加权行为模型（ABM）--使RL策略偏向于以前执行过的、在新任务中可能成功的行动。我们的方法可以被看作是最近在批处理RL方面工作的扩展，它能够从冲突的数据源中进行稳定学习。
在基因交互数据上应用图卷积神经网络的主要挑战之一是对它们所属的向量空间缺乏了解，以及在一个明显较低的维度（即欧氏空间）上表示这些交互所涉及的固有困难。 我们介绍了一种系统的、通用的方法，称为iSOM-GSN，用于将具有较高维度的 "多原子 "数据转化为二维网格。之后，我们应用卷积神经网络来预测各种类型的疾病状态。基于Kohonen的自组织图的思想，我们为每个样本生成一个二维网格，代表一组基因的相似性网络。 我们利用基因表达、DNA甲基化和拷贝数改变测试了该模型对乳腺癌和前列腺癌的预测，对乳腺癌的肿瘤分期产生了94-98%的预测准确率，对这两种情况只用11个输入基因就计算出了前列腺癌的格里森分数。该方案不仅输出了近乎完美的分类准确率，而且还为表示学习、可视化、降维和结果解释提供了一个增强方案。
在语言理解任务上的最先进的表现是由巨大的语言模型在大量的无标签文本语料库上进行预训练而实现的，随后以特定任务的监督方式进行非常轻微的微调。似乎预训练程序为进一步训练各种自然语言理解任务学习了一个非常好的共同初始化，这样在参数空间中只需要采取少数步骤来学习每个任务。 在这项工作中，以来自变形器的双向编码器表示法（BERT）为例，我们通过显示特定任务的微调语言模型在参数空间中与预训练的模型高度接近来验证这一假设。 利用这种观察，我们进一步表明，这些巨大的模型的微调版本，具有10^8美元的浮点参数，可以在计算上变得非常高效。(1)学习执行特定的任务，(2)通过为每个任务只存储某些层的二进制掩码来节省内存，以及(3)通过对模型参数进行稀疏化操作来节省适当硬件的计算。 
我们提出了一种简单有效的算法，旨在解决模仿学习中的协变量转移问题。它通过在专家示范数据上训练一组策略，并将其预测的方差作为成本，用RL和监督的行为克隆成本来最小化。 我们证明了该算法在表格设置中的遗憾约束，该约束在时间范围内是线性的，并乘以一个系数，我们表明在行为克隆失败的某些问题上，该系数很低。我们在多个基于像素的Atari环境和连续控制任务中对我们的算法进行了经验评估，并表明它与行为克隆和生成式对抗性模仿学习相匹配或明显优于后者。
我们提出并讨论了一种简单的图像预处理方法，用于学习分解的潜在因素。特别是，我们利用在ImageNet数据库上预训练的网络的特征中所包含的隐性归纳偏见。我们通过明确地在对NeurIPS2019年disentanglement挑战有用的任务上对这种预训练网络进行微调来增强这种偏见，例如角度和位置估计或颜色分类。此外，我们在区域聚集的特征图上训练VAE，并使用最近文献中提出的指标讨论其disentanglement性能。
一个需要在不同情节中追求不同目标的强化学习代理需要一个目标条件政策。除了它们有可能将理想的行为概括为未见过的目标外，这种政策还可以实现基于子目标的更高层次的规划。在稀疏的奖励环境中，利用关于一个任意目标的实现程度的信息的能力，而另一个目标的意图似乎对实现有效的样本学习至关重要。 然而，强化学习代理只是在最近才被赋予这种事后诸葛亮的能力。在本文中，我们展示了如何将事后诸葛亮引入政策梯度方法，将这一想法推广到一类广泛的成功算法中。我们在稀疏奖励环境的不同选择上的实验表明，事后诸葛亮导致了样本效率的显著提高。
计算机视觉在性能上经历了一场巨大的革命，这在很大程度上是通过在大规模监督数据集上训练的深度特征驱动的。然而，这些改进大多集中在静态图像分析上；视频理解方面的改进相当有限。 在这项工作中，我们建立了一个具有完全可观察和可控制的物体和场景偏差的视频数据集，它确实需要时空理解才能解决。我们的数据集被命名为CATER，使用标准3D物体库进行合成渲染，并测试识别需要长期推理的物体运动组合的能力。 除了是一个具有挑战性的数据集，CATER还提供了大量的诊断工具，通过完全可观察和可控制来分析现代时空视频架构。利用CATER，我们提供了对一些最新的深度视频架构的洞察力。
我们解决了最近出现的联合学习中由散兵游勇效应引起的效率问题，联合学习在大规模工人设备上对分散的非i.i.d.（非独立和相同分布）数据进行协同训练，而不需要在不可靠的异构网络中交换训练数据。我们对一般联合学习的误差边界提出了一个新颖的两阶段分析，这为优化提供了实际见解。 因此，我们提出了一种新型的易于实现的联合学习算法，该算法使用异步设置和策略来控制全局模型和延迟模型之间的差异，并通过对呆滞性的估计来调整局部历时的数量，以加速收敛并抵抗由散兵游勇引起的性能下降。实验结果表明，我们的算法在存在大量散兵游勇的情况下能快速收敛并保持稳定。
长短期记忆（LSTM）单元有能力记忆并使用输入之间的长期依赖关系来产生对时间序列数据的预测。我们引入了使用由一组新的可训练权重参数化的旋转矩阵来修改LSTM单元状态（记忆）的概念。
我们解决了定义在包络矩阵集合上的指数族的边际推断问题。众所周知，随着包络的大小增加，这个问题很快就变得难以解决，因为它涉及到矩阵的永久计算，这是一个#P-hard问题。 我们将Sinkhorn变异边际推理作为一种可扩展的替代方法，这种方法的有效性最终由所谓的Sinkhorn永久值的近似值来证明。我们在虫子C.elegans的神经元的概率识别问题上证明了我们方法的有效性。
在本文中，我们提供了将鲁棒性分析转换为局部Lipschitz常数估计问题的理论依据，并建议使用极值理论进行有效的评估。我们的分析产生了一个新的鲁棒性指标，称为CLEVER，这是Cross Lipschitz Extreme Value for nEtwork Robustness的缩写。 在各种网络上的实验结果，包括ResNet、Inception-v3和MobileNet，表明(i)CLEVER与由来自强大攻击的对抗性例子的$\ell_2$和$\ell_\infty$规范衡量的鲁棒性指示一致，(ii)使用防御性蒸馏或有界ReLU的防御网络确实给出了更好的CLEVER得分。 据我们所知，CLEVER是第一个独立于攻击的鲁棒性指标，可以应用于任何神经网络分类器。
尽管实际系统通常采用分布式设置，但局部范围的通信和信息聚合对于完成复杂的任务仍然很重要。然而，现有的模型通常存在着僵化的通信结构，例如，它们中的大多数通过为代理指定固定的时间频率和空间范围来预先定义特定的通信模式，而不考虑必要性。这种设计无法处理多代理场景的任性和复杂，特别是当只有部分信息可用时。 特别是，它使每个代理能够根据其观察到的状态自发地决定何时以及向谁发送消息。通过这种方式，以在线和自组织的方式建立了一个动态的代理间通信通道。
我们研究了BERT语言表示模型和带有BERT编码器的序列生成模型，用于多标签文本分类任务。我们对这两种模型进行了实验，并探索了它们在这一环境中的特殊性。我们还引入了混合模型，这是一个多标签BERT和序列生成BERT模型的组合，并进行了实验。我们的实验表明，基于BERT的模型和混合模型，特别是在三个经过充分研究的英语文本多标签分类数据集和两个俄罗斯文本的私人Yandex出租车数据集上，在几个指标上超过了当前的基线，取得了最优秀的结果。
点击率（CTR）预测是工业应用中的一项关键任务，尤其是在线社交和商业应用。我们提出了一种用于CTR任务的新型模型，称为带有编码器增强因子机（DeepEnFM）的深度神经网络。 编码器生成的嵌入有利于进一步的特征交互。特别是，DeepEnFM利用双线性方法来生成不同领域对的不同相似度函数。此外，最大集合方法使DeepEnFM能够捕获不同注意力头之间的补充和抑制信息。我们的模型在Criteo和Avazu数据集上得到验证，并取得了最先进的性能。
对于自主代理在现实世界中成功运作，预测未来场景状态的能力是一项关键能力。在现实世界的场景中，未来状态变得越来越不确定和多模式，特别是在长时间范围内。 基于辍学的贝叶斯推理提供了一种计算上可操作的、理论上有根有据的方法，以学习不同的假设/模型来处理不确定的未来，并做出与观察结果完全一致的预测--经过很好的校准。然而，事实证明，这种方法在捕捉复杂的真实世界场景方面存在不足，与普通的确定性方法相比，甚至在准确性方面也有所下降。 在这项工作中，我们提出了一种新的贝叶斯公式来预测未来的场景状态，它利用合成似然来鼓励学习不同的模型，以准确捕捉未来场景状态的多模式性质。
条件生成对抗网络（cGAN）已经导致了条件图像生成任务的巨大改进，这也是计算机视觉的核心。到目前为止，主要的焦点是性能的提高，而在使cGAN对噪声更加稳健方面几乎没有努力。 在这项工作中，我们引入了一个新的条件GAN模型，称为RoCGAN，它利用模型的目标空间的结构来解决这个问题。我们的模型用一个无监督的途径来增强生成器，这促进了生成器的输出来跨越目标流形，即使在强烈的噪声存在下。
虽然深度神经网络在视觉分类方面已经取得了最先进的性能，但最近的研究表明，它们都很容易受到对抗性例子的攻击。在对抗性训练中，ATLPA使用了Tolerant Logit，它由前k类的置信度分布组成，并在图像层面上捕获类间的相似性。具体来说，除了最小化经验损失，ATLPA还鼓励对例子的注意力图进行相似。 当应用于干净的例子和它们的对抗性对应物时，ATLPA比对抗性训练提高了对抗性例子的准确性。我们用最先进的算法评估ATLPA，实验结果表明我们的方法以更高的准确性胜过这些基线。与以前的工作相比，我们的工作是在高挑战性的PGD攻击下评估的：最大扰动$/epsilon$是64和128，攻击迭代10到200。
智能的一个基本特征是在面对新情况时实现目标的能力。在这项工作中，我们解决了这样一个问题，即需要用一组新的行动来解决一个任务。赋予机器这种能力需要在代理感知其可用行动的方式以及使用这些行动来解决任务的方式上进行概括。 具体来说，代理人通过对反映该行动不同属性的数据样本集合进行无监督的表征学习来解释一个行动的行为。我们采用了一个强化学习架构，该架构在这些行动表征上工作，并提出了对在策略中实现泛化至关重要的正则化指标。我们说明了表征学习方法和策略的泛化性，以实现对以前未见过的行动在挑战性的连续决策环境中的零次泛化。我们的结果和视频可以在 sites.google.com/view/action-generalization/找到。
时间点过程是对不规则间隔发生的事件序列进行建模的主要范式。在这种模型中学习的标准方式是估计条件强度函数。 然而，强度函数的参数化通常会产生一些折衷。我们展示了如何通过直接对事件间时间的条件分布进行建模来克服基于强度的方法的局限性。 我们借鉴了关于流量归一化的文献，设计了灵活有效的模型。我们还提出了一个简单的混合模型，它与基于流量的模型的灵活性相匹配，但也允许以闭合形式采样和计算时刻。 所提出的模型在标准预测任务中实现了最先进的性能，并适用于新的应用，如学习序列嵌入和估算缺失数据。
我们提出了一种新颖而简单的神经网络结构用于话题建模。该方法基于训练一个自动编码器结构，其中瓶颈代表话题分布的空间，解码器输出代表话题上的词分布空间。 有效的话题建模方法的一个关键特征是具有稀疏的话题和词的分布，在话题和词的稀疏程度之间有一个权衡。这个特征在我们的模型中通过L-2正则化实现，模型的超参数照顾到了这个权衡。 我们在实验中表明，尽管我们的模型结构简单，训练过程简单，但与最先进的深度模型相比，我们的模型取得了有竞争力的结果。
语音转换（VC）是一项将感知到的说话人身份从源说话人转换到特定目标说话人的任务。文献中早期的方法主要是在给定的源-目标说话人对之间寻找映射。 大多数 "多对多 "的VC架构需要来自所有目标说话人的训练数据，我们想为他们转换声音。在本文中，我们提出了一个新的风格转换架构，它也可以扩展到生成声音，即使目标说话人的数据没有被用于训练（即。特别是，我们提出了自适应生成对抗网络（AdaGAN），新的架构训练程序有助于学习归一化的与说话人无关的潜在表征，这将被用来在VC的背景下生成具有不同说话风格的语音。 AdaGAN比StarGAN-VC在语音质量和说话人相似性的MOS测试中分别实现了31.73%和10.37%的相对改善。AdaGAN的关键优势在于它以较低的计算复杂性获得了这些结果。 
基于自我注意的Transformer在一些自然语言处理任务中表现出最先进的性能。自我注意能够对长期依赖关系进行建模，但它可能会受到提取上下文中不相关信息的影响。为了解决这个问题，我们提出了一个称为稀疏Transformer的新模型。 Sparse Transformer能够通过明确选择最相关的片段来提高对全局环境的注意力。在一系列自然语言处理任务上的大量实验结果，包括神经机器翻译、图像字幕和语言建模，都证明了Sparse Transformer在模型性能上的优势。  Sparse Transformer在IWSLT 2015英译越和IWSLT 2014德译英中达到了最先进的性能。此外，我们还进行了定性分析来说明Sparse Transformer的卓越性能。
人类观察者可以从少量的例子中学习识别新类别的物体，但在机器感知中这样做仍然是一个公开的挑战。我们假设，正如最近的感知证据所建议的那样，数据有效的识别是由使自然信号中的变异性更可预测的表示来实现的。因此，我们重新审视并改进Contrastive Predictive Coding，一个最近提出的无监督学习框架，并得出一个能够从少量标记数据中进行概括的表示。 当只提供1%的ImageNet标签（即每类13个）时，这个模型保持了强大的分类性能，73%的前5名准确率，比监督网络高出28%（相对提高65%），比最先进的半监督方法高出14%。我们还发现这个表示是PASCAL-VOC 2007数据集上物体检测的有用基底，接近于用完全注释的ImageNet数据集训练的表示。
我们展示了我们所说的稀疏学习的可能性：加速训练深度神经网络，在整个训练过程中保持稀疏的权重，同时达到密集的性能水平。我们通过开发稀疏动量来实现这一目标，这种算法使用指数平滑梯度（动量）来识别有效减少误差的层和权重。稀疏动量根据每个层的平均动量大小重新分配各层的修剪权重。 我们在MNIST、CIFAR-10和ImageNet上展示了最先进的稀疏性能，与其他稀疏算法相比，平均误差降低了8%、15%和6%。此外，我们表明，稀疏动量可靠地再现了密集性能水平，同时提供高达5.61倍的训练速度。
为了提供设计适当的深度神经网络（DNN）模型的原则性方法，理解现实假设下的DNN的损失面是至关重要的。我们介绍了理解损失面的局部最小值和整体结构的有趣方面。损失面的参数域可以分解为激活值（整流线性单元为零或一）一致的区域。 我们发现，在每个区域，损失面都有类似于线性神经网络的属性，其中每个局部最小值都是全局最小值.这意味着每个可微调的局部最小值都是相应区域的全局最小值.我们证明，对于在现实假设下使用整流线性单元的具有一个隐藏层的神经网络，存在着导致局部最小值的不良区域，我们解释了为什么即使在过度参数化的DNN中也存在这种区域。
在这项工作中，我们解决了神经语言表征中语义和结构之间的无监督分离的任务：我们的目标是学习语境化向量的转换，放弃词汇语义，但保留结构信息。 为此，我们自动生成结构相似但语义不同的句子组，并使用度量学习方法来学习一种转换，强调矢量中编码的结构成分。
我们引入了一种新的记忆结构，用于在以前未见过的环境中进行导航，其灵感来自于动物中基于地标的导航。拟议的半参数拓扑记忆（SPTM）包括一个（非参数）图，其节点对应于环境中的位置，以及一个（参数）深度网络，能够基于观察从图中检索节点。 我们将SPTM作为导航系统中的规划模块。鉴于只有5分钟的以前未见过的迷宫的录像，基于SPTM的导航代理可以建立一个环境的拓扑图，并利用它自信地朝着目标导航。SPTM代理在测试环境中的目标导向导航的平均成功率比表现最好的基线高三倍。
在我们的视觉世界中，可用的分辨率是非常高的，甚至是无限的。现有的CNN可以以完全卷积的方式应用于任意分辨率的图像，但随着输入尺寸的增加，它们无法捕捉到上下文信息。 此外，计算要求与输入像素的数量成线性关系，而且无论不同的图像区域的信息量有多大，资源都是在整个输入中统一分配的。我们试图通过提出一个新颖的架构来解决这些问题，该架构以自上而下的方式遍历图像金字塔，同时它使用硬注意机制来选择性地只处理信息量最大的图像部分。 我们在MNIST和ImageNet数据集上进行了实验，结果表明，当输入的分辨率大到基线的接受域不能充分覆盖感兴趣的对象时，我们的模型可以明显地超过完全卷积的对应模型。由于我们遵循的选择性处理，性能的提高是以较少的FLOPs实现的。此外，我们的注意力机制使我们的预测更容易解释，并在准确性和复杂性之间建立一个权衡，可以在训练和测试时间进行调整。
超参数优化可以被表述为一个二层优化问题，其中训练集上的最佳参数取决于超参数。我们旨在通过拟合最佳响应函数的紧凑近似来调整神经网络的正则化超参数，该函数将超参数映射到最佳权重和偏差。 我们展示了如何为神经网络构建可扩展的最佳响应近似，方法是将最佳响应建模为一个单一的网络，其隐藏单元有条件地被正则化门控。我们通过展示一个具有L2正则化雅各布的浅层线性网络的确切最佳响应可以被类似的门控机制所代表来证明这种近似。 我们使用基于梯度的超参数优化算法来拟合这个模型，该算法在围绕当前超参数近似最佳响应和使用近似最佳响应函数优化超参数之间交替进行。与其他基于梯度的方法不同，我们不需要区分相对于超参数的训练损失，允许我们调整离散超参数、数据增强超参数和辍学概率。 由于超参数是在线调整的，我们的方法发现了超参数计划，可以优于固定的超参数值。从经验上看，我们的方法在大规模深度学习问题上优于竞争的超参数优化方法。
条件生成对抗网络（cGANs）在许多应用领域得到了越来越广泛的应用。尽管取得了突出的进展，但这种模型的定量评估往往涉及多个不同的指标来评估不同的理想属性，如图像质量、条件一致性和条件内多样性。 在本文中，我们提出了Frechet联合距离（FJD），它被定义为图像和条件联合分布之间的Frechet距离，使其能够在一个单一的指标中隐含地捕获上述属性。我们在一个可控的合成数据集上进行了概念验证实验，与目前建立的指标相比，这些实验不断强调FJD的好处。 此外，我们使用新引入的指标来比较现有的基于cGAN的模型的各种调节模式（如类标签、对象掩码、边界框、图像和文字说明）。我们表明，FJD可以作为一个有前途的单一指标用于模型基准测试。
将无模型的深度强化学习与在线规划结合起来，是建立在深度RL成功基础上的一种有前途的方法。在过渡模型事先已知的环境中，使用前瞻树的在线规划被证明是成功的。 TreeQN通过在学习到的抽象状态空间中递归应用一个过渡模型来动态地构建一棵树，然后使用树状备份来估计Q值，聚集预测的奖励和状态值。我们还提出了ATreeC，一个行为批评的变体，用一个softmax层来增强TreeQN，形成一个随机的策略网络。 这两种方法都是端到端的训练，因此学到的模型是针对其在树中的实际使用而优化的。我们表明，TreeQN和ATreeC在推箱子的任务上优于n步DQN和A2C，以及n步DQN和价值预测网络（Oh et al, 2017）在多个Atari游戏上的表现。此外，我们提出了消融研究，证明了不同的辅助损失对学习过渡模型的影响。
多标签分类（MLC）是为给定样本分配一组目标标签的任务。在MLC中对组合标签的相互作用进行建模一直是一个长期的挑战。基于循环神经网络（RNN）的编码器-解码器模型在解决MLC方面表现出最先进的性能。然而，通过RNN建模标签依赖的顺序性限制了其在并行计算、预测密集标签和提供可解释性结果的能力。 在本文中，我们提出了消息传递编码器-解码器（MPED）网络，旨在提供快速、准确和可解释的MLC。MPED网络通过用消息传递机制取代编码器-解码器架构中的所有RNN，对标签的联合预测进行建模，并完全免除了自回归推理。 所提出的模型是简单、快速、准确、可解释和结构无关的（可用于已知或未知的结构化数据）。在七个真实世界的MLC数据集上的实验表明，所提出的模型在五个不同的指标上优于自回归RNN模型，在训练和测试时间上有明显的加速。
最近的几次学习算法使模型能够在仅有的几个训练样本的基础上快速适应新的任务。以前的几次学习工作主要集中在分类和强化学习上。在本文中，我们提出了一个专门关注回归任务的几次元学习系统。 我们设计了一个基函数学习者网络来编码任务分布的基函数，以及一个权重生成器网络来生成新任务的权重向量。我们表明，我们的模型在各种回归任务中优于目前最先进的元学习方法。
大规模的预训练语言模型，如BERT，最近在广泛的语言理解任务中取得了巨大的成功。然而，如何将BERT用于文本生成任务仍然是一个开放性的问题。教师）然后被利用作为额外的监督，以改善传统的Seq2Seq模型（即。通过利用BERT的特异性双向性质，提炼从BERT学到的知识可以鼓励自动回归的Seq2Seq模型提前计划，为连贯的文本生成施加全局序列级的监督。实验表明，所提出的方法在多个文本生成任务（包括机器翻译（MT）和文本总结）上明显优于Transformer的强大基线。
许多研究表明，人类视觉的这一特征来自于视觉皮层自下而上通路的前馈信号和自上而下通路提供的反馈信号之间的互动。在这种互动的激励下，我们提出了一个新的神经启发模型，即带有反馈的卷积神经网络（CNN-F）。 我们表明，CNN-F的迭代推理允许跨层解除潜变量。我们验证了CNN-F相对于基线CNN的优势。我们的实验结果表明，CNN-F对图像退化（如像素噪声、遮挡和模糊）更加强大。 此外，我们表明CNN-F能够从退化的图像中恢复原始图像，并具有较高的重建精度，同时引入可忽略不计的伪影。
我们通过通道大小、滤波器大小和宽度固定的ResNet型结构开发了卷积神经网络（CNN）的新近似和统计学习理论。结果表明，ResNet型CNN是一个通用的近似器，即使CNN中每层的大小固定，其表达能力也不比全连接的神经网络（FNN）差。 我们的结果具有普遍性，即我们可以自动将块稀疏FNNs实现的任何近似率转化为CNNs实现的近似率。由于一般的理论，我们表明CNNs上的学习满足几个重要函数类的近似和估计的最优性。 作为应用，我们考虑了两类需要估计的函数：Barron类和H/"olden类。我们证明了剪切的经验风险最小化（ERM）估计器可以达到与FNNs相同的速率，即使CNNs的通道大小、过滤器大小和宽度相对于样本大小是恒定的。 我们的证明是基于对CNN覆盖数的复杂评估和控制待建CNN的Lipschitz常数的非微观参数重构技术。
少数图像分类旨在从有限的标记数据中学习分类器。生成分类权重由于其简单性和有效性，已被应用于许多少数图像分类的元学习方法中。然而，我们认为，从很少的训练样本中为所有不同的查询样本生成精确和通用的分类权重是很困难的。 i) AWGIM通过让每个查询样本关注整个支持集，为不同的查询样本生成不同的分类权重。ii) 为了保证生成的权重适应不同的查询样本，我们重新表述问题，使生成的权重和查询以及支持数据之间的相互信息下限最大化。
对话式问题回答（CQA）是一项新颖的QA任务，需要对对话背景进行理解。与传统的单轮机器阅读理解（MRC）不同，CQA是一项综合任务，包括段落阅读、核心推理解决和背景理解。在本文中，我们提出了一个创新的基于背景注意的深度神经网络SDNet，将背景融合到传统MRC模型中。 我们的模型利用相互注意和自我注意来理解对话和段落。此外，我们展示了一种新颖的方法，将BERT上下文模型作为一个子模块整合到我们的网络中。实证结果显示了SDNet的有效性。在CoQA排行榜上，它比之前的最佳模型的F1得分高出了1.6%。
生成对抗网络（GANs）是训练生成模型时最流行的方法之一，其中Wasserstein GANs的变体被认为在学习稳定性和样本质量方面优于标准的GAN表述。然而，Wasserstein GANs要求批评者是1-Lipschitz，这通常是通过惩罚其梯度的规范来隐含地执行的，或者通过权重归一化技术来全面限制其Lipschitz常数。 受虚拟对抗训练的启发，我们提出了一种被称为对抗性Lipschitz正则化的方法，并表明当应用于Wasserstein GANs时，使用明确的Lipschitz惩罚确实是可行的，并导致有竞争力的性能，突出了Lipschitz正则化和对抗性训练之间的重要联系。
多任务学习承诺比训练单独的单任务模型使用更少的数据、参数和时间。但在实践中实现这些好处是具有挑战性的。特别是，很难定义一个合适的架构，它有足够的能力来支持许多任务，同时又不需要为每个单独的任务进行过多的计算。 我们定义了特征共享策略的参数化，以便有效地覆盖和抽查架构。我们还提出了一种利用特征提炼快速评估这种架构的方法。我们在Visual Decathlon上进行了基准测试，证明我们可以自动搜索和识别架构，有效地在任务资源要求之间进行权衡，同时保持高水平的最终性能。
随着自然语言语义学的分布式方法的发展和多样化，大于单词的语言单位（如句子）的嵌入器已经发挥了越来越重要的作用。 到目前为止，这种嵌入器是通过基准任务（如GLUE）和语言探测来评估的。 我们提出了一种比较方法，即近邻重叠（N2O），它以一种与任务无关的方式量化嵌入者之间的相似性。 N2O只需要一个例子的集合，而且很容易理解：如果对于相同的输入集合，输入的近邻之间有更大的重叠，那么两个嵌入者就更相似。 我们用N2O来比较21个句子嵌入器，并展示了不同设计选择和架构的效果。
生成对抗网络（GANs）可以在生成建模任务中实现最先进的样本质量，但受到模式崩溃问题的影响。另一方面，变异自动编码器（VAE）明确最大化基于重建的数据对数似然，迫使它覆盖所有模式，但受到较差的样本质量的影响。 这是因为VAE目标强迫在数据对数似然和对潜伏先验的发散之间进行权衡，合成似然比项在训练过程中也显示出不稳定性。我们提出了一个新的目标，具有 "最佳多样本 "重建成本和合成似然的稳定直接估计。
为了模仿人类在不同任务中不断获取和转移知识的能力，学习系统需要有终身学习的能力，有效地利用以前获得的技能。因此，关键的挑战是将从一个任务中学到的知识转移和概括到其他任务中，避免以前知识的干扰，提高整体性能。在本文中，在持续学习的范式中，我们引入了一种方法，在不同任务中不断有效地遗忘不太有用的数据样本。 该方法使用统计杠杆分数信息来衡量数据样本在每个任务中的重要性，并采用频繁的方向方法来实现终身学习的特性。我们首先为该方法提供了一些数学上的直觉，然后通过对MNIST和CIFAR100数据集变体的实验来证明其有效性。
卷积神经网络(CNN)对平移具有固有的等价性，嵌入其他形式的等价性的努力只集中在旋转方面。我们通过极地变换器网络(PTN)扩展CNN中的等价性概念。 PTN是端到端的训练，由三个不同的阶段组成：极地原点预测器、新引入的极地变换器模块和分类器。PTN在旋转的MNIST和新引入的SIM2MNIST数据集上达到了最先进的水平，MNIST的变化是通过添加杂波和用平移、旋转和缩放来扰乱数字的。
元学习允许智能代理利用先前的学习事件作为快速提高新任务性能的基础。贝叶斯层次模型提供了一个理论框架，将元学习正规化为一组跨任务共享的参数的推理。 与之前通过层次贝叶斯进行元学习的方法相比，MAML通过使用可扩展的梯度下降程序进行后验推断，自然适用于复杂的函数近似器。 此外，将MAML识别为层次贝叶斯提供了一种理解该算法作为元学习程序运行的方法，以及利用计算策略进行有效推理的机会。我们利用这一机会提出了对MAML算法的改进，该算法利用了近似推理和曲率估计的技术。
这项工作提供了一个名为Autostacker的自动机器学习（AutoML）建模架构。Autostacker通过利用创新的分层堆叠架构和高效的参数搜索算法，提高了机器学习基线的预测精度，既不需要关于数据的先前领域知识，也不需要特征预处理。 通过关注建模过程，Autostacker打破了遵循固定顺序管道的传统，不仅探索了单一的模型管道，还探索了创新的组合和结构。
代用模型可用于加速近似贝叶斯计算（ABC）。在一个这样的框架中，模拟和观察数据之间的差异是用高斯过程来模拟的。到目前为止，有原则的策略只被提出来用于顺序选择模拟位置。
随机初始化的一阶优化算法是解决机器学习中许多高维非凸问题的首选方法，然而一般的理论保证不能排除收敛到目标值较差的临界点。然而对于一些高度结构化的非凸问题，梯度下降的成功可以通过研究目标的几何学来理解。 我们研究了这样一个问题--完全正交字典学习，并为随机初始化梯度下降到全局最优附近提供了收敛保证。即使目标拥有指数级数量的鞍点，所产生的速率也是维度上的低阶多项式。
编码理论是支撑有线和无线调制解调器的核心学科，这些调制解调器是信息时代的主力军。编码理论的进展在很大程度上是由人类个人的聪明才智推动的，在过去的一个世纪里有零星的突破。 我们表明，精心设计和训练的RNN架构可以解码众所周知的顺序码，如卷积码和涡轮码，在加性白高斯噪声（AWGN）信道上具有接近最佳的性能，这本身是由我们这个时代的突破性算法（Viterbi和BCJR解码器，代表动态编程和前向后向算法）实现的。我们展示了强大的遗传性，即我们在特定的信噪比和区块长度下进行训练，但在这些数量的广泛范围内进行测试，以及对偏离AWGN设置的稳健性和适应性。
研究人员最近提出了几种算法来避免Adam的不收敛问题，但其效率在实践中并不令人满意。在本文中，我们对Adam以及其他自适应学习率方法的不收敛问题提供了新的见解。 我们认为，在Adam中，梯度$g_t$和第二时刻项$v_t$之间存在不恰当的关联（$t$是时间步长），这导致大梯度可能有小步长，而小梯度可能有大步长。 我们证明，这种不平衡的步长是Adam不收敛的根本原因，我们进一步证明，对$v_t$和$g_t$进行装饰将导致每个梯度的无偏步长，从而解决Adam的不收敛问题。最后，我们提出了AdaShift，一种新型的自适应学习率方法，通过时间上的转移，即用时间上的转移来装饰$v_t$和$g_t$。实验结果表明，AdaShift能够解决Adam的非收敛问题，同时在训练速度和泛化方面仍然保持与Adam有竞争力的性能。
在本文中，我们提出了第一个基于生成对抗网络的多源领域适应（MSDA）的方法。我们的方法受到这样的启发：一个给定图像的外观取决于三个因素：领域、风格（以低层次特征变化为特征）和内容。 为此，我们建议将图像特征投射到一个只保留内容依赖性的空间中，然后使用目标领域和风格将这种不变的表示重新投射到像素空间中。
推断一个联合分布的变量子集的最有可能的配置--我们称之为联合生成--是一个重要的挑战，除了最简单的设置外，对计算要求很高。这项任务得到了相当多的关注，特别是对于像结构化预测这样的经典分布建模方式。 相比之下，在考虑最近提出的高维分布建模技术，特别是生成对抗网（GANs）时，对这项任务几乎一无所知。因此，在本文中，我们研究了GANs联合生成的挑战。为了解决这些挑战，我们开发了一种基于退火重要性采样（AIS）的汉密尔顿蒙特卡罗（HMC）联合生成算法。
隐式概率模型是以抽样程序自然定义的模型，通常诱导出一个无法明确表达的似然函数。我们开发了一个简单的方法来估计隐式模型中的参数，不需要知道似然函数的形式或任何派生量，但可以证明在某些条件下等同于最大化似然。我们的结果在非渐进参数设置中成立，其中模型的容量和数据例子的数量都是有限的。我们还展示了令人鼓舞的实验结果。
虽然反强化学习（IRL）问题的大多数方法都集中在估计一个最能解释专家代理的政策或在控制任务上表现出来的行为的奖励函数，但通常情况下，这种行为更简洁地由一个简单的奖励和一组硬约束来表示。 在这种情况下，代理人试图在这些给定的行为约束条件下实现累积奖励的最大化。我们在马尔科夫决策过程（MDP）上重新表述IRL的问题，即给定环境的名义模型和名义奖励函数，我们试图估计环境中激励代理人行为的状态、行动和特征约束。 我们的方法基于最大熵IRL框架，该框架允许我们在了解MDP的情况下推断专家代理的演示可能性。我们提出了一种算法，该算法可以迭代推断最大可能性约束，以最好地解释观察到的行为，我们使用模拟行为和人类绕过障碍物的记录数据评估其功效。
强化学习在现实世界中的成功仅限于仪器化的实验室场景，通常需要艰巨的人类监督来实现持续的学习。在这项工作中，我们讨论了一个机器人系统所需的元素，该系统可以通过在现实世界中收集的数据持续和自主地改进，并提出了这样一个系统的具体实例。 随后，我们研究了无仪器学习的一些挑战--包括缺乏偶发重置、状态估计和手工设计的奖励--并提出了应对这些挑战的简单、可扩展的解决方案。我们在模拟和现实世界中证明了我们所提出的系统在灵巧的机器人操纵任务上的功效，并对与这种学习范式相关的挑战进行了深刻的分析和消融研究。
我们研究了在非平行语料库和一次性学习环境下的跨语言语音转换问题。大多数先前的工作需要平行语料库或来自目标说话人的足够数量的训练数据。 为了实现这一目标，我们将问题表述为学习特定于说话人和特定于语境的表征，并遵循[1]的想法，即使用因子化层次变异自动编码器（FHVAE）。在多说话人训练数据上训练FHVAE后，给定任意源说话人和目标说话人的语料，我们估计这些潜在的表征，然后将转换后的声音的理想语料重建为目标说话人的。 我们使用多语言语音语料库来学习一个适用于所有语言的通用模型。我们研究了使用单次语言嵌入来对被查询的语料的语言进行条件化的模型，并展示了该方法的有效性。我们用不同规模的训练语料进行了语音转换实验，即使只有一个训练语料，它也能达到合理的性能。 我们还研究了使用或不使用语言调节的效果。此外，我们还可视化了不同语言和性别的嵌入。最后，在主观测试中，对于一种语言和跨语言的语音转换，与基线相比，我们的方法在语音质量和相似度方面取得了适度的更好或相当的结果。
我们提出了一个用于图像分类的卷积神经网络（CNN）架构的端到端可训练的注意力模块。该模块将二维特征向量图作为输入，这些图在CNN管道的不同阶段形成输入图像的中间表示，并为每个图输出一个二维分数矩阵。 标准的CNN架构通过该模块的加入而被修改，并在中间二维特征向量的凸组合的约束下进行训练，这些中间二维特征向量的参数由分数矩阵决定，必须单独用于分类。 我们的实验观察为这一效果提供了明确的证据：学习到的注意力图在抑制背景杂波的同时巧妙地突出了感兴趣的区域。因此，所提出的函数能够引导标准CNN架构进行图像分类，在6个未见过的基准数据集上显示出卓越的概括性。 当进行二值化处理时，我们的注意力图超过了其他基于CNN的注意力图、传统的显著性图和弱监督分割的顶级物体建议，这在物体发现数据集上得到了证明。
递归神经网络（RNN）是解决非常复杂的有监督和无监督任务的有效神经网络。在RNN领域，如自然语言处理、语音处理、计算机视觉和其他多个领域，已经有了显著的改进。本文涉及RNN在不同用例上的应用，如事件检测、欺诈检测和安卓恶意软件分类。 通过对不同的网络参数和结构进行不同的连锁实验，选择了性能最好的神经网络架构。网络运行到1000个epochs，学习率设置在0.01到0.5之间。
解剖学研究表明，大脑对输入的信息进行重新处理，以产生可靠的反应来进行计算。然而，目前仍不清楚神经回路如何编码复杂的时空模式。 沿着主导混沌投影的输入排列会使混沌轨迹成为稳定的通道（或吸引子），从而提高递归网络的计算能力。我们利用均值场分析，得出输入排列对形成的吸引子的整体稳定性的影响。我们的结果表明，输入排列决定了内在噪声抑制的程度，从而改变了吸引子状态的稳定性，从而控制网络的推理能力。
生成对抗网络是一个学习框架，它依赖于训练一个判别器来估计目标分布和生成分布之间的差异度量。GANs，如通常制定的那样，依赖于生成的样本对生成参数是完全可分的，因此对离散数据不起作用。 我们介绍了一种用离散数据训练GAN的方法，该方法使用来自判别器的估计差异度量来计算生成样本的重要性权重，从而为训练生成器提供政策梯度。重要性权重与判别器的决策边界有很强的联系，我们称我们的方法为寻界GAN（BGAN）。 此外，寻界目标延伸到了连续数据，可以用来提高训练的稳定性，我们在Celeba、大规模场景理解（LSUN）卧室和Imagenet上证明了这一点，没有条件。
政策梯度方法在深度强化学习中取得了巨大的成功，但却受到梯度估计的高方差的影响。高方差问题在具有长视野或高维行动空间的问题上尤其严重。 为了缓解这个问题，我们推导出一个无偏见的行动依赖基线，用于降低方差，它充分地利用了随机策略本身的结构形式，并且不对MDP做任何额外的假设。我们通过理论分析和数值结果来证明和量化行动依赖基线的好处，包括对最佳状态依赖基线的次优性的分析。 结果是一种计算效率高的策略梯度算法，它可以扩展到高维控制问题，正如一个合成的2000维目标匹配任务所证明的那样。我们的实验结果表明，依赖行动的基线允许在标准强化学习基准和高维手部操作和合成任务上更快地学习。最后，我们表明，在基线中包括额外信息以改善方差的一般想法可以扩展到部分观察和多代理人任务。
注释训练数据的成本历来是监督学习方法的一个瓶颈。当监督学习同时应用于一些相关的任务时，这个问题会进一步加剧，因为所需的标签数量会随着任务数量的增加而增加。为了缓解这个问题，我们提出了一种主动的多任务学习算法，实现了任务之间的知识转移。 该方法为每个任务形成了一个所谓的委员会，该委员会共同做出决定并直接分享类似任务的数据。我们的方法减少了训练期间所需的查询次数，同时在测试数据上保持高精确度。
照片篡改的检测依赖于微妙的统计痕迹，这种痕迹被在线采用的积极的有损压缩所清除。我们证明，复杂的照片传播渠道的端到端建模允许以明确的出处为目标进行编码优化。 我们设计了一个轻量级的可训练的有损图像编解码器，它提供了有竞争力的速率-失真性能，与最好的手工设计的替代品相当，但在现代GPU平台上有较低的计算足迹。我们的结果表明，在带宽/存储的零星成本下，操纵检测的准确性有可能得到显著改善。
递归神经网络长期以来一直是序列建模的主要选择。然而，它严重地受制于两个问题：无力捕捉非常长期的依赖性和无法并行化顺序计算程序。因此，最近提出了许多建立在卷积和注意力操作上的非递归序列模型。值得注意的是，具有多头注意力的模型，如Transformer，在捕捉各种序列建模任务的长期依赖性方面表现出极大的有效性。 尽管他们取得了成功，然而，这些模型缺乏必要的组件来模拟序列中的局部结构，并且严重依赖效果有限的位置嵌入，需要相当多的设计工作。在本文中，我们提出了R-Transformer，它同时享有RNN和多头注意机制的优点，同时避免了它们各自的缺点。 本文提出的模型可以有效地捕捉序列中的局部结构和全局性的长期依赖关系，而不需要使用任何位置嵌入。我们通过广泛的实验对R-Transformer进行评估，实验结果表明，R-Transformer在大多数任务中以很大的幅度超过了最先进的方法。
在这项工作中，我们提出了一种结构化的潜变量方法，在标准的自回归神经范式中增加了离散的控制状态。在这种表述下，我们可以包括一系列丰富的后验约束，以执行有效训练到神经模型中的特定任务知识。 这种方法允许我们为内部模型决策提供任意的基础，而不牺牲神经模型的任何表示能力。实验考虑了这种方法在文本生成和语音部分归纳方面的应用。对于自然语言生成，我们发现这种方法比标准基准要好，同时也提供了精细的控制。
假设一个深度分类模型是用样本训练的，而这些样本由于隐私或保密的原因需要保密。在这种情况下，如果把分类模型给了对手，那么对手能不能获得私人样本？我们把这种针对分类模型的反向工程称为分类器到生成器（C2G）攻击。对于C2G攻击，我们引入了一种新的GAN，PreImageGAN。 在PreImageGAN中，生成器被设计成以分类模型$f$的预像为条件估计样本分布，$P(X|f(X)=y)$，其中$X$是样本空间上的随机变量，$y$是代表对手任意指定的目标标签的概率向量。在实验中，我们证明PreImageGAN在手写字符识别和人脸识别中成功工作。 在字符识别中，我们表明，给定一个手写数字的识别模型，PreImageGAN允许对手提取字母图像，而不知道该模型是为字母图像建立的。在人脸识别中，我们表明，当对手获得一个个人集合的人脸识别模型时，PreImageGAN允许对手提取该集合中包含的特定个人的人脸图像，即使对手对这些个人的脸部一无所知。
标准压缩感应的目标是在某些基础的稀疏性假设下，从线性测量中估计一个未知的向量。最近，有研究表明，如果稀疏性假设被未知向量位于一个适当选择的生成模型的范围附近的假设所取代，则需要的测量次数会大大减少。 特别是在（Bora {em / al.}, 2017）中表明，当$k$输入生成模型是有界的和$L$-Lipschitz时，大约$O(k\log L)$随机高斯测量足以实现精确恢复，而对于深度为$d$和宽度为$w$的$k$输入ReLU网络，$O(kd\log w)$测量就足够了。 在本文中，我们利用最小统计分析的工具建立了相应的独立于算法的样本复杂度下限。 根据上述的上界，我们的结果总结如下。(i) 我们构建了一个能够产生群稀疏信号的$L$-Lipschitz生成模型，并表明由此产生的必要测量次数为$Omega(k \log L)$；(ii) 利用类似的想法，我们构建了需要$Omega(k \log w)$测量的高宽度的两层ReLU网络，以及需要$Omega(k d)$测量的低宽度深度ReLU网络。 因此，我们确定，在没有进一步假设的情况下，(Bora {\em et al.}, 2017)中得出的缩放规律是最优或接近最优的。
发现和利用环境中的因果结构是智能代理的一个重要挑战。这里我们探讨了现代深度强化学习是否可以用来训练代理进行因果推理。我们采用元学习方法，代理通过因果干预学习进行实验的政策，以支持后续任务，奖励做出准确的因果推断。我们还发现代理可以做出复杂的反事实预测，以及学习从纯粹的观察数据中得出因果推断。 虽然强大的因果推理形式已经被开发出来，但将它们应用于现实世界的领域可能是困难的，因为适应大量的高维数据往往需要做出理想化的假设。我们的结果表明，复杂环境中的因果推理可能受益于强大的基于学习的方法。
情感分类是一个活跃的研究领域，其应用包括政治观点的分析、评论的分类、电影评论、新闻评论和产品评论。 为了绕过人工开发的时间和成本，我们试图依靠基于语料库的方法来建立阿姆哈拉语情感词典。这种方法的目的是处理阿姆哈拉语语料库中的特定情感术语。 我们提出了基于语料库的方法，该方法依赖于词的共现分布嵌入，包括基于频率的嵌入（即正的点式互信息PPMI）。首先，我们建立词-语料库的频率计数矩阵，并将其转换为点式互信息矩阵。 基于阈值，与种子列表的平均向量最接近的词被添加到词库中。然后，新的情感种子列表的平均向量被更新，这个过程被重复，直到我们在词库中得到足够的词汇。
优化初始化是强化学习（RL）中高效探索的有效策略。在表格的情况下，所有证明有效的无模型算法都依赖于它。然而，无模型的深度RL算法并没有使用乐观的初始化，尽管从这些证明有效的表格算法中得到了启发。 特别是，在只有正奖励的情况下，由于常用的网络初始化方案，Q值被初始化为可能的最低值，这是一种悲观的初始化。仅仅初始化网络以输出乐观的Q值是不够的，因为我们不能确保它们对新的状态-动作对保持乐观，这对探索是至关重要的。 我们提出了一个简单的基于计数的对悲观初始化Q值的增强，将乐观的来源从神经网络中分离出来。我们证明了这个方案在表格设置中是有效的，并将其扩展到深度RL设置中。 我们的算法，乐观悲观地初始化Q-Learning（OPIQ），用计数派生的奖金来增加基于DQN的代理的Q值估计，以确保在行动选择和引导期间的乐观性。我们表明OPIQ优于非乐观的DQN变体，在硬探索任务中利用基于伪计数的内在动机，并且它预测了对新状态行动对的乐观估计。
无模型的深度强化学习（RL）算法已经在一系列具有挑战性的决策和控制任务中得到证明。然而，这些方法通常有两个主要挑战：非常高的样本复杂性和脆弱的收敛特性，这需要细致的超参数调整。 在这个框架中，行为者的目标是最大化预期奖励，同时也最大化熵--也就是说，在任务中取得成功，同时尽可能地随机行动。之前基于这个框架的深度RL方法被表述为非政策性的Q-learning，或政策性的政策梯度方法。 通过将非政策性更新与稳定的随机行为者批评表述相结合，我们的方法在一系列连续控制基准任务上实现了最先进的性能，超过了先前的政策性和非政策性方法。此外，我们证明，与其他非政策性算法相比，我们的方法非常稳定，在不同的随机种子中实现了非常相似的性能。
在许多情况下，通过学习或从专家示范中学习决策和控制政策是可取的。在这个框架下最常见的方法是行为克隆（BC）和逆向强化学习（IRL）。最近IRL的方法已经证明了在获得非常有限的示范集的情况下学习有效政策的能力，这是BC方法经常失败的情况。 不幸的是，直接比较这些方法的算法并不能为理解这种性能差异提供足够的直觉。这是我们工作的动机。我们首先介绍了$f$-MAX，这是AIRL（Fu等人，2018）的概括，是一种最先进的IRL方法。我们证明$f$-MAX，以及继承AIRL，是Ho & Ermon（2016）提出的成本规范化IRL框架的一个子集。
基于价值的方法构成了规划和深度强化学习（RL）的基本方法。在本文中，我们建议利用状态-动作价值函数的基本结构，即。特别是，如果底层系统动力学导致了Q函数的一些全局结构，我们应该能够通过利用这些结构更好地推断出函数。 作为我们的主要贡献，通过利用矩阵估计（ME）技术，我们提出了一个通用框架来利用Q函数中的潜在低秩结构，从而为经典控制带来更有效的规划程序，此外，还有一个简单的方案，可以应用于任何基于价值的RL技术，以在 "低秩 "任务上持续实现更好的性能。
源代码的学习表示使各种软件开发者工具，例如。代码表征的核心往往是源代码中标识符名称的词嵌入，因为标识符占源代码词汇的大部分，并传达重要的语义信息。 我们使用IdBench来评估为自然语言提出的最先进的嵌入技术、专门为源代码设计的嵌入技术和词串距离函数，因为这些经常被用于当前的开发者工具中。因为嵌入将意义相反的标识符视为相似，这可能会导致下游开发者工具的致命错误。IdBench提供了一个黄金标准，以指导开发新的嵌入，解决目前的限制。
生成对抗网（GAN）被广泛用于学习数据采样过程，在计算预算有限的情况下，其性能可能在很大程度上取决于损失函数。本研究重新审视了MMD-GAN，它使用最大平均差异（MMD）作为GAN的损失函数，并做出了两个贡献。 首先，我们认为现有的MMD损失函数可能会阻碍对数据中精细细节的学习，因为它试图收缩真实数据的判别器输出。为了解决这个问题，我们提出了一个排斥性损失函数，通过简单地重新排列MMD中的项来主动学习真实数据之间的差异。 其次，受铰链损失的启发，我们提出了一个有界高斯核，以稳定具有排斥性损失函数的MMD-GAN的训练。所提出的方法被应用于CIFAR-10、STL-10、CelebA和LSUN卧室数据集的无监督图像生成任务。 结果表明，排斥性损失函数在不增加计算成本的情况下明显改善了MMD损失，并优于其他代表性的损失函数。所提出的方法在CIFAR-10数据集上使用单个DCGAN网络和光谱归一化实现了16.21的FID得分。
深度神经网络在各种领域的推理任务中表现出令人难以置信的性能。不幸的是，目前大多数深度网络是基于云的巨大结构，需要大量的存储空间，这限制了深度学习作为一种服务（DLaaS）的扩展和用于设备上的增强智能。 本文发现了直接使用深度前馈网络的无损压缩表示的算法（突触权重来自离散集），在不完全解压的情况下进行推理。允许比天真的方法更少的速率的基本见解是认识到前馈网络的二方图层对节点的标记有一种包络不变性，在推理操作方面，推理操作在本地取决于直接连接到它的边。我们还提供了我们的方法在MNIST数据集上的实验结果。
生成式对抗网络（GANs）形成了一种生成式建模方法，以产生吸引人的样本而闻名，但它们明显难以训练。解决这个问题的一个常见方法是提出GAN目标的新表述。然而，令人惊讶的是，很少有研究关注为这种对抗训练设计的优化方法。 我们利用数学编程文献，反驳了一些关于鞍点优化困难的常见误解，并提议将为变分不等式设计的方法扩展到GAN的训练中。我们将平均法、外推法和一个计算上更便宜的变体（我们称之为从过去外推法）应用于随机梯度法（SGD）和亚当。
为了有效地学习新任务的少量数据，元学习将从以前的任务中学到的知识转移到新的任务中。然而，元学习的一个关键挑战是任务的异质性，传统的全局共享元学习方法不能很好地处理。 此外，目前针对特定任务的元学习方法可能会受到手工结构设计的影响，或者缺乏捕捉任务间复杂关系的能力。在本文中，受知识库中知识组织方式的启发，我们提出了一个自动关系元学习（ARML）框架，它可以自动提取跨任务关系并构建元知识图。 当一个新的任务到来时，它可以快速找到最相关的结构，并将学到的结构知识定制给元学习者。因此，所提出的框架不仅通过学习的元知识图解决了任务异质性的挑战，而且还提高了模型的可解释性。我们在二维玩具回归和少量图像分类上进行了广泛的实验，结果表明ARML比最先进的基线有优势。
本文开发了一种深度提升算法，通过无缝结合一组具有不同能力的基础深度CNN（基础专家）来学习更具辨别力的集合分类器，例如，这些基础深度CNN根据其学习的复杂性，以从易到难的方式连续训练识别一组物体类别。我们的实验结果表明，我们的深度提升算法可以显著提高大规模视觉识别的准确率。
我们提出了一种跨乐器和风格的音乐翻译方法。该方法基于无监督的多领域Wavenet自动编码器的训练，有一个共享的编码器和一个与领域无关的潜在空间，在波形上进行端到端的训练。采用多样化的训练数据集和大网容量，单一的编码器使我们能够翻译在训练期间没有出现的音乐领域。 我们在从专业音乐家那里收集的数据集上评估了我们的方法，并实现了令人信服的翻译。我们还研究了所获得的翻译的特性，并展示了甚至从口哨中的翻译，有可能使未经训练的人类创造器乐。
大多数现有的针对对抗性攻击的防御措施只考虑对L_p约束的失真度的鲁棒性。在现实中，具体的攻击很少事先知道，而且对抗者可以自由地以任何固定的失真模型之外的方式修改图像；例如，对抗性旋转位于L_p约束的失真集之外。在这项工作中，我们主张测量针对更广泛的不可预见的攻击的鲁棒性，这些攻击的确切形式在防御设计期间是未知的。 首先，我们构建了新的对抗性JPEG、Fog、Gabor和Snow失真，以模拟更多不同的对手。然后，我们引入了UAR，这是一个总结性的指标，用来衡量针对特定失真的防御的鲁棒性。 我们使用UAR来评估针对现有和新的攻击的鲁棒性，对对抗性的鲁棒性进行了广泛的研究。我们发现，针对现有L_p攻击的评估产生了多余的信息，不能推广到其他攻击；相反，我们建议针对我们明显更多样化的攻击集进行评估。我们进一步发现，针对一种或多种失真的对抗性训练不能赋予其他失真类型的攻击以鲁棒性。 这些结果强调了评估和研究针对不可预见的扭曲的鲁棒性的必要性。
深度神经网络（DNN）通过解决自然语言处理、语音处理、计算机视觉等方面长期存在的人工智能（AI）监督和非监督任务，在今年见证了一种强大的方法。安卓恶意软件分类、事件检测和欺诈检测。每个用例的数据集都包含真实的已知良性和恶意活动样本，这些用例是2017年网络安全数据挖掘竞赛（CDMC）的一部分。 与经典的机器学习算法相比，DNN的实验在网络安全用例的所有实验中都表现良好。这是由于DNN隐含地提取和建立了更好的特征，识别了数据的特征，从而导致更好的准确性。 DNNs和XGBoost在安卓恶意软件分类上获得的最佳精度为0.940和0.741，事件检测为1.00和0.997，欺诈检测为0.972和0.916.DNNs获得的精度与CDMC 2017任务中得分最高的系统相差-0.05%，+0.02%，-0.01%。
在本文中，我们提出了一种在大规模和不同的操纵演示中学习可重组的运动基元的方法。目前将演示分解为基元的方法通常假设手动定义的基元，并绕过发现这些基元的困难。另一方面，基元发现的方法对基元的复杂性提出了限制性假设，这限制了对狭窄任务的适用性。 通过对基元分解的简约性和给定基元的简单性的约束，我们能够学习一系列不同的运动基元，以及这些基元的连贯的潜在表征。我们从质量和数量上证明，我们学习的基元捕捉了演示的语义方面。这使我们能够在一个分层强化学习设置中组成这些基元，有效地解决机器人的操纵任务，如伸手和推。
使用现代深度学习模型对来自可穿戴传感器的时间序列数据进行预测，通常需要大量的标记数据。然而，标记这些大型数据集可能既麻烦又昂贵。在本文中，我们将弱监督应用于时间序列数据，并以编程方式标记来自帕金森病患者佩戴的传感器的数据集。 我们表明：（1）当我们的模型使用病人特定的数据（先前的传感器会话）进行训练时，我们在使用手标数据训练的模型的9% AUROC之内；（2）当我们假设没有事先观察到受试者时，我们的弱监督模型与手标数据的性能相匹配。这些结果表明，弱监督可能有助于减少费力地手标时间序列训练数据的需要。
学习结构化数据（如槽值对）和相关文本之间的语义对应关系是许多下游NLP应用的一个核心问题，例如。然而，收集到的用于训练的数据-文本对通常是松散对应的，与配对的输入相比，文本包含额外的或矛盾的信息。在本文中，我们提出了一个局部到全局对齐（L2GA）框架，从松散关联的数据-文本对中学习语义对应。 然后，设计了一个建立在记忆引导的条件随机场（CRF）层之上的全局对齐模型，以利用整个训练语料库中的对齐关系，其中记忆被用来整合由局部对齐模型提供的对齐线索。 因此，它能够诱导出不被其不完善的配对输入所支持的文本跨度的缺失对齐。在最近的餐厅数据集上的实验表明，我们提出的方法可以提高对齐的准确性，作为副产品，我们的方法也适用于为神经生成模型诱导出语义上相等的训练数据-文本对。
模仿学习算法为通过标准的监督学习方法训练控制策略提供了一种简单直接的方法。通过最大化专家示范者提供的良好行动的可能性，监督模仿学习可以产生有效的策略，而没有强化学习的算法复杂性和优化挑战，代价是需要一个专家示范者--通常是一个人--提供示范。 在本文中，我们问道：我们能否使用模仿学习来训练有效的策略，而不需要任何专家示范？使这成为可能的关键观察是，在多任务设置中，由次优策略产生的轨迹仍然可以作为其他任务的最优例子。特别是，在任务对应不同目标的设置中，每条轨迹都是它实际达到的状态的成功示范。 根据这一观察，我们提出了一种非常简单的学习行为的算法，不需要任何示范、用户提供的奖励函数或复杂的强化学习方法。我们的方法只是最大限度地提高代理人在自己以前的滚动中实际采取的行动的可能性，条件是目标是它实际达到的状态。 虽然这种方法的相关变体以前在模仿学习环境中被提出来，但我们提出了这种方法的第一个实例，即完全从头开始学习目标达成策略的方法。我们提出了一个理论结果，将自我监督的模仿学习和强化学习联系起来，经验结果表明，在一系列具有挑战性的目标达成问题上，它的表现与更复杂的强化学习方法具有竞争性。
最近，神经网络在许多分类任务上表现出优异的性能。这些网络通常有大量的参数，因此需要大量的数据来训练。然而，当训练数据点的数量较少时，具有高灵活性的网络会很快过度拟合训练数据，导致模型方差大，泛化性能差。为了解决这个问题，我们提出了一种新的集合学习方法，称为InterBoost，用于小样本图像分类。 在训练阶段，InterBoost首先随机生成两个互补的数据集，分别训练两个相同结构的基础网络，然后通过之前训练的两个基础网络之间的交互（或信息共享），生成下两个互补的数据集，用于进一步训练网络。这个交互训练过程反复进行，直到达到停止标准。
在本文中，我们开发了一个新的框架，通过直接解释前馈多层神经网络的权重来检测其捕获的统计交互作用。根据所需的交互作用，我们的方法可以在不搜索可能的交互作用的指数解空间的情况下，实现比最先进的交互作用检测性能更好或相似。 我们通过观察输入特征之间的相互作用是由非线性激活函数的非加性效应产生的，并且相互作用的路径在权重矩阵中被编码，从而获得这种准确性和效率。我们通过在合成数据集和真实世界的应用数据集上的实验结果证明了我们方法的性能和发现的相互作用的重要性。
神经线性模型是一种简单的自适应贝叶斯线性回归方法，最近被用于从贝叶斯优化到强化学习等一系列问题中。尽管它在这些环境中取得了明显的成功，但据我们所知，在简单的回归任务上还没有对其能力进行系统的探索。 在这项工作中，我们在UCI数据集（贝叶斯回归模型的常用基准）以及最近引入的 "差距 "数据集上对其进行了描述，后者是对分布外不确定性的更好测试。
强化学习研究的可重复性已被强调为该领域的一个关键挑战领域。在本文中，我们提出了一个案例研究，再现了一个开创性的算法AlphaZero的结果，这是一个强化学习系统，在只给出游戏规则的情况下学习如何以超人的水平下围棋。 Minigo系统包括中央强化学习回路以及辅助监测和评估基础设施。在800个云TPU上从头开始训练10天后，Minigo可以与LeelaZero和ELF OpenGo这两个最强的公开围棋AI平分秋色。我们讨论了扩展强化学习系统的困难以及了解超参数配置的复杂相互作用所需的监测系统。
生成对抗网络（GANs）通过解决最小化问题来训练隐含的生成模型。这种最小化问题被称为非凸非凹，对于这些问题，一阶方法的动态性并不十分了解。 在本文中，我们考虑了积分概率度量（IPMs）类型的GANs，生成器由超参数化的神经网络表示。当判别器在每次迭代中被解到近似最优时，我们证明了正则化IPM目标上的随机梯度下降以亚线性速率全局收敛到静止点。 此外，我们还证明，当生成器网络的宽度足够大且判别器函数类具有足够的判别能力时，所获得的静止点对应于生成器，其产生的分布在总变化方面接近于观测数据的分布。
我们提出了网络嵌入算法，从节点周围的节点属性的局部分布中捕捉关于节点的信息，这是在随机行走中观察到的类似于Skip-gram的方法。来自不同大小的邻域的观察结果被集中起来（AE）或在多尺度方法中被单独编码（MUSAE）。 我们从理论上证明，节点-特征点相互信息矩阵是由嵌入隐含的因子化的。实验表明，我们的算法是稳健的，计算效率高，在社会、网络和引文网络数据集上的表现优于同类模型。
少量分类的目的是学习分类器在训练期间用有限的标记实例来识别未见过的类别。虽然已经取得了很大的进展，但网络设计、元学习算法的日益复杂和实施细节的差异使得公平比较变得困难。 在本文中，我们提出了：1）对几种有代表性的少量分类算法进行了一致的比较分析，结果表明，更深的骨架大大缩小了包括基线在内的各种方法的差距。 2）一个稍加修改的基线方法，在mini-ImageNet和CUB数据集上与最先进的方法相比，出人意料地取得了有竞争力的性能，以及3）一个新的实验环境，用于评估几率分类算法的跨域概括能力。 我们的结果显示，当特征骨架较浅时，减少类内变异是一个重要的因素，但当使用较深的骨架时就不那么关键了。在一个现实的、跨域的评估环境中，我们表明，一个具有标准微调做法的基线方法与其他最先进的几率学习算法相比是很好的。
时间逻辑对于描述动态系统行为非常有用，并且已经成功地被用作任务规划期间目标定义的语言。在本文中，我们研究了推断描述两组计划轨迹之间时间差异的规范问题。我们正式确定了提供这种对比性解释的概念，然后提出了一个贝叶斯概率模型，用于推断作为线性时间逻辑规范的对比性解释。我们证明了我们的模型在各种基准计划领域和模拟空战任务中推断正确规范的有效性、可扩展性和稳健性。
这项工作解决了描述和理解具有片断线性非线性激活的神经网络的决策边界的问题。我们使用热带几何，一个代数几何领域的新发展，提供了一个简单的神经网络（Affine, ReLU, Affine）形式的决策边界的特征描述。 具体来说，我们表明决策边界是一个热带超曲面的子集，它与两个宗锥的凸壳形成的多角形密切相关。宗锥的生成器是神经网络参数的精确函数。我们利用这一几何特征来阐明三个任务的新视角。 在此过程中，我们为彩票假设提出了一个新的热带视角，我们看到了不同的初始化对决策边界的热带几何表示的影响。同时，我们利用这个特征作为一套新的热带正则器，它直接处理网络的决策边界。 我们研究了这些正则器在神经网络修剪（去除对决策边界的热带几何表示没有贡献的网络参数）和产生对抗性输入攻击（输入扰动明确地扰动决策边界的几何形状以改变网络对输入的预测）中的使用。
一阶方法，如随机梯度下降法（SGD）是目前训练深度神经网络的标准算法。二阶方法，尽管有更好的收敛率，但由于计算二阶信息的计算成本过高，在实践中很少使用。 在本文中，我们提出了一种新颖的Gram-Gauss-Newton（GGN）算法，用于训练深度神经网络的平方损失回归问题。我们的方法从神经网络优化和神经正切核（NTK）的核回归之间的联系中获得了灵感。 与典型的二阶方法不同的是，GGN在每次迭代中都有沉重的计算成本，与SGD等一阶方法相比，GGN的开销很小。我们还给出了理论结果，表明对于足够宽的神经网络，GGN的收敛率是二次的。 此外，我们还提供了迷你批处理GGN算法的收敛保证，据我们所知，这是在超参数化神经网络上二阶方法的迷你批处理版本的第一个收敛结果。在回归任务上的初步实验表明，对于训练标准网络，我们的GGN算法收敛速度更快，并取得了比SGD更好的性能。
最近的预训练句子编码器在语言理解任务上取得了最先进的成果，但这是否意味着它们具有隐含的句法结构知识？我们为语言接受性语料库（CoLA；Warstadt等人，2018）引入了一个语法注释的开发集，我们用它来研究三个预训练编码器的语法知识，包括流行的OpenAI Transformer（Radford等人。2018）和BERT（Devlin等人，2018）。我们对这些编码器进行微调，以便在CoLA上进行可接受性分类，并比较模型在注释的分析集上的表现。有些现象，如附属物的修改，对所有模型来说都很容易学习，而其他现象，如长距离移动，只有整体性能强的模型才能有效学习，还有一些现象，如形态一致，几乎没有模型能学习。
当同时考虑有限数量的任务时，多输出学习使人们能够通过适当的正则器考虑任务的相似性。我们提出了一个经典设置的概括，通过使用矢量值的RKHSs来实现连续的任务。
我们分析了当权重和偏置根据高斯分布随机选择，且输入为二元值时，全连接深层网络不同层中隐藏变量的向量长度的联合概率分布。我们表明，如果激活函数满足一组最小的假设，我们知道所有在实践中使用的激活函数都满足这些假设，那么，随着网络的宽度变大，"长度过程 "在概率上收敛到一个长度图，该长度图被确定为随机权重和偏置的方差以及激活函数的简单函数。我们还表明，对于违反我们假设的激活函数，这种收敛可能失败。
数据增强是提高现代机器学习模型准确性的最有效方法之一，也是训练元学习的深度模型所不可缺少的。然而，目前应用于元学习的大多数数据增强实现与传统的图像分类所使用的数据增强相同。在本文中，我们为元学习引入了一种新的数据增强方法，它被命名为 "任务级数据增强"（简称Task Aug）。 Task Aug的基本思想是增加图像类的数量，而不是每个类中的图像数量。相反，有了更多的类，我们可以在训练过程中采样更多不同的任务实例。
在本文中，我们提出了一个通用框架，用于提炼与深度神经网络的贝叶斯后验分布有关的期望值，极大地扩展了先前关于一种被称为 "贝叶斯黑暗知识 "的方法的工作。 我们的通用框架适用于分类模型的情况，并将 "教师 "网络的架构、感兴趣的一般后验期望和 "学生 "网络的架构作为输入。提炼方法使用教师模型的参数后验迭代生成的蒙特卡洛样本对选定的后验期望进行在线压缩。 我们提出了研究多个数据集、蒸馏目标、教师模型架构和搜索学生模型架构的方法的实验结果。我们建立了一个关键的结果，即蒸馏到一个具有与教师相匹配的架构的学生模型，就像在贝叶斯黑暗知识中所做的那样，会导致次优的性能。最后，我们表明，学生架构搜索方法可以识别具有明显改进性能的学生模型。
变量自动编码器（VAE）已被证明是强大的潜在变量模型。然而，近似后验的形式会限制模型的表现力。分类分布是灵活和有用的构建模块，例如在神经记忆层中。 Concrete/Gumbel-Softmax松弛允许通过随机梯度上升来最大化证据下限的替代物。我们表明，当使用有限数量的潜变量时，HD-VAE在建模多个二进制图像数据集时优于高斯基线。由于使用替代物目标所引起的松弛偏差，训练非常深的HD-VAE仍然是一个挑战。
在本文中，我们提出了一种改进随机梯度下降（SGD）方法来训练深度网络的新技术，我们称之为PowerSGD（emph）。 我们提出的PowerSGD方法只是在迭代过程中把随机梯度提高到一定的功率$\gamma\in[0,1]$，并且只引入了一个额外的参数，即功率指数$\gamma$（当$\gamma=1$时，PowerSGD简化为SGD）。我们进一步提出PowerSGD的动力，我们称之为 emph{PowerSGDM}，并对PowerSGD和PowerSGDM方法提供收敛率分析。 实验在流行的深度学习模型和基准数据集上进行。实证结果表明，所提出的PowerSGD和PowerSGDM比自适应梯度方法获得更快的初始训练速度，与SGD的泛化能力相当，并且对超参数选择和梯度消失有更好的鲁棒性。PowerSGD本质上是一个通过非线性变换的梯度修改器。因此，它与其他加速基于梯度优化的技术是正交和互补的。
我们的目标是建立集成感知、运动控制和记忆的复杂人形代理。在这项工作中，我们把这个问题部分地归结为来自本体感知的低级运动控制和由视觉告知的低级技能的高级协调。我们开发了一个架构，能够对一个相对高DoF的人形身体进行令人惊讶的灵活的、以任务为导向的运动控制，把低级运动控制器的预训练与一个在低级子政策之间切换的、以任务为中心的高级控制器相结合。 由此产生的系统能够控制一个物理模拟的仿人身体，以解决在环境中运动时需要从一个不稳定的自我中心RGB相机中耦合视觉感知的任务。补充视频链接：https://youtu.be/fBoir7PNxPk
通过观察ReLU神经元是线性函数与门的乘积（后者决定了神经元是否活跃），其中两者共享一个共同训练的权重向量，我们建议将两者解耦。我们引入GaLU网络--其中每个神经元是线性单元的乘积，由一个正在训练的权重向量定义，与一个门定义，由一个没有被训练的不同权重向量定义。 一般来说，给定一个基础模型和一个较简单的版本，决定较简单版本质量的两个参数是它的实际性能是否足够接近基础模型，以及是否更容易从理论上分析它。我们表明GaLU网络在标准数据集上的表现与ReLU网络相似，我们开始研究其理论属性，证明它们确实更容易分析。
机器学习系统在处理来自与训练所用分布不同的测试数据时，经常会遇到分布外（OoD）错误。随着它们在关键应用中的使用越来越多，开发能够准确量化其预测不确定性并筛除这些异常输入的系统变得非常重要。然而，与标准学习任务不同，目前还没有完善的指导原则来设计能够准确量化不确定性的架构。此外，常用的OoD检测方法很容易出现错误，甚至有时对OoD样本赋予更高的可能性。 为了解决这些问题，我们首先通过提出神经架构分布搜索（NADS）来确定设计不确定性感知架构的指导原则。与寻求单一最佳性能架构的标准神经架构搜索方法不同，NADS搜索在给定任务上表现良好的架构分布，使我们能够识别所有不确定性感知架构中共同的构建块。 有了这种表述，我们能够优化随机离群检测目标，并构建一个模型集合来进行OD检测。我们进行了多个OD检测实验，并观察到我们的NADS与最先进的OD检测方法相比表现良好。
从自动驾驶汽车到视频监控的现代应用产生了大量的图像数据。在这项工作中，我们提出了一种新的图像离群点检测方法（简称IOD），利用尖端的图像分类器来发现离群点，而不使用任何标记的离群点。我们观察到，尽管从直觉上讲，卷积神经网络（CNN）认为一个图像属于一个特定类别的信心可以作为每个图像的离群点测量，直接应用这种信心来检测离群点效果并不好。 这是因为CNN通常对不属于任何目标类别的离群图像有很高的信心，这是因为它的泛化能力保证了分类的高精确度。为了解决这个问题，我们提出了一种基于深度神经森林的方法，它协调了准确分类图像和正确检测离群图像这两个矛盾的要求。 我们使用包括MNIST、CIFAR-10、CIFAR-100和SVHN在内的几个基准图像数据集进行了实验，证明了我们的IOD方法在离群点检测方面的有效性，捕获了90%以上的由一个图像数据集注入另一个数据集产生的离群点，同时仍然保留了多类分类问题的分类精度。
本文介绍了CloudLSTM，这是一个新的循环神经模型分支，专门用于预测由地理空间点云源产生的数据流。我们设计了一个动态点云卷积（D-Conv）算子作为CloudLSTM的核心组件，它直接在点云上执行卷积，并从围绕输入的不同元素的相邻点集合中提取局部空间特征。 这个算子保持了序列到序列学习框架的不变性，同时代表了每个时间步长的相邻关系--这是时空预测学习的一个重要方面。D-Conv算子解决了现有时空预测模型的网格结构数据要求，可以很容易地插入到具有序列到序列学习和注意机制的传统LSTM架构中。   我们将我们提出的架构应用于两个有代表性的、涉及点云流的实际用例，即移动服务流量预测和空气质量指标预测。我们的结果是在每个用例的不同场景中收集的真实世界数据集，显示CloudLSTM提供了准确的长期预测，超过了各种神经网络模型的表现。
知识图谱（KG）由实体和关系组成，提供了知识的结构化表示。为了方便地获取关系数据的统计方法，引入了多种方法来嵌入KG作为R^d的组成部分。我们提出了TransINT，一种新颖的、可解释的KG嵌入方法，同构地保留了嵌入空间中关系的隐含排序。 我们在FB122数据集的链接预测和三重分类中实现了新的最先进的性能，并有很大的余地，甚至在无法通过逻辑规则推断的测试实例上也有很大的性能提升。
无监督领域自适应对象检测的目的是在领域转移的情况下学习一个稳健的检测器，其中训练（源）领域是具有边界框注释的标签丰富的领域，而测试（目标）领域是标签无关的，训练和测试领域之间的特征分布是不同的，甚至完全不同。 在本文中，我们提出了一种基于梯度分离的叠加互补损失（SCL）方法，该方法以检测目标（交叉熵和平滑l1回归）为主要目标，并在不同的网络阶段切入几个辅助损失，以利用补充数据（目标图像）的信息，从而有效地适应源域和目标域的模型参数。 我们认为，传统的主要目标训练主要是利用源域的信息来实现可能性的最大化，而忽略了网络浅层的补充数据，这导致了不同领域内的整合不足。 例如，从Cityscapes到FoggyCityscapes，我们实现了37.9%的mAP，比之前的Strong-Weak方法好了3.6%。
卷积神经网络（CNN）已经成为许多视觉相关领域中最成功和最流行的方法。虽然CNN特别适合于从现实世界的图像中捕捉适当的概念层次，但它们仅限于数据丰富的领域。最近的尝试是通过将其原始的单任务问题转化为新的多任务学习（MTL）问题来缓解这一数据匮乏问题。 这种归纳转移机制的主要目标是利用相关任务的特定领域信息，以提高对主要任务的概括性。虽然深度学习（DL）社区的最新成果显示了在软参数共享框架中训练特定任务的CNN的巨大潜力，但整合最近的DL进展以改善知识共享仍然是一个开放的问题。在本文中，我们提出了深度协作网络（DCNet），一种在MTL框架中连接特定任务CNN的新颖方法。 我们用两个不同的非线性转换块来定义连接性。一个将特定任务的特征聚集到全局特征中，而另一个则将全局特征与每个特定任务的网络合并。基于任务相关性取决于深度的观察，我们的转换块使用了残差网络方法所建议的跳过连接，以更容易地停用不相关的任务特征。 为了验证我们的方法，我们采用了面部地标检测（FLD）数据集，因为考虑到它们包括的任务数量，它们很容易适应MTL。实验结果表明，与其他最先进的MTL方法相比，我们可以实现高达24.31%的地标失败率的相对改善。我们最后进行了一项消减研究，表明我们的方法通过利用来自我们知道相关任务的特定深度的领域特征，有效地允许知识共享。
零点学习（ZSL）是一项分类任务，其中一些被称为未见过的类没有标记的训练图像。相反，我们只有关于已见过和未见过的类的侧面信息（或描述），通常是以语义或描述性属性的形式。 缺乏一组类的训练图像限制了标准分类技术和损失的使用，包括流行的交叉熵损失。解决ZSL问题的关键步骤是通过学习非线性嵌入将视觉与语义空间连接起来。 在本文中，我们提出了一个新的架构，将ZSL作为一个具有交叉熵损失的全连接神经网络，将视觉空间嵌入到语义空间。在训练过程中，为了将未见过的视觉信息引入到网络中，我们利用了基于已见和未见类之间语义相似性的软标签。 据我们所知，这种基于相似性的软标签并没有在跨模式转移和ZSL方面进行探索。我们在五个基准数据集（AwA1、AwA2、aPY、SUN和CUB数据集）上评估了所提出的模型，并表明，尽管很简单，我们的方法在所有这些数据集上都达到了广义ZSL设置的最先进性能，并在一些数据集上超越了最先进的性能。
在复杂的任务中，如那些具有大的组合行动空间的任务，随机探索可能太低效，无法实现有意义的学习进展。在这项工作中，我们使用逐步增长的行动空间的课程来加速学习。我们假设环境不受我们控制，但代理可以通过最初限制其行动空间来设置内部课程。 我们的方法使用非政策性强化学习来同时估计多个行动空间的最佳价值函数，并有效地将数据、价值估计和状态表示从受限的行动空间转移到完整的任务中。我们在概念证明的控制任务和具有大型、多代理行动空间的大规模StarCraft微管理任务中显示了我们方法的功效。
最近，研究人员发现，最先进的物体分类器很容易被人眼无法察觉的输入中的微小扰动所欺骗。众所周知，如果攻击者知道分类器的参数，她可以产生强大的对抗性例子。相反，如果防御者拥有对抗性例子，她可以通过重新训练来增强分类器的稳定性。攻击和防御的猫鼠游戏性质提出了动态中是否存在均衡的问题。在本文中，我们提出了一个基于神经网络的攻击类，以近似更大但难以解决的攻击类，并将攻击者-防御者的互动表述为零和领导-追随者游戏。 我们提出了敏感性惩罚的优化算法，以找到最小的解决方案，这是对白盒攻击的最好的最坏情况下的防御。与基于梯度的攻击和防御相比，基于学习的攻击和防御的优势在MNIST和CIFAR-10上得到了证明。
由于处理不规则时间间隔的障碍，对不规则采样时间序列的监督学习一直是机器学习方法的挑战。最近一些论文介绍了处理不规则的递归神经网络模型，但它们大多依靠复杂的机制来实现更好的性能。 这项工作提出了一种新的方法，使用正弦函数将时间戳（小时或日期）表示为密集的向量，称为时间嵌入，作为一种数据输入方法，它可以应用于大多数机器学习模型。
图中的社区检测可以通过频谱方法或某些概率图模型下的后验推断来解决。关注随机图族，如随机块模型，最近的研究已经统一了这两种方法，并在信噪比方面确定了统计和计算检测阈值。通过将社区检测重塑为图上的节点分类问题，我们也可以从学习角度来研究它。 我们提出了一个新的图神经网络（GNNs）系列，用于解决监督学习环境下的社区检测问题。我们表明，以数据驱动的方式，在不接触底层生成模型的情况下，它们可以匹配甚至超越二元和多类随机块模型上的信念传播算法的性能，在这些情况下，相信会达到计算的阈值。 特别是，我们提出用定义在边缘相邻关系线图上的非反向追踪算子来增强GNNs。GNNs在真实世界的数据集上取得了良好的性能。 此外，我们首次分析了使用（线性）GNNs解决社区检测问题的优化情况，证明了在某些简化和假设下，任何局部最小值的损失值都接近于全局最小/最小值的损失值。
残差网络（Resnets）已经成为深度学习中一个突出的架构。然而，对Resnets的全面理解仍然是一个正在进行的研究课题。最近的一个观点认为，Resnets对特征进行迭代细化，我们试图进一步揭示这方面的属性。 此外，我们的经验分析表明，Resnets能够同时进行表征学习和迭代细化。一般来说，Resnet块倾向于将表征学习行为集中在前几层，而更高层则进行特征的迭代细化。最后，我们观察到共享残差层会天真地导致表征爆炸并损害泛化性能，并表明简单的现有策略可以帮助缓解这一问题。
我们为基于无光罩的相机开发了端到端的学习重建，包括一个实验系统，用于捕捉对齐的无光和有光图像进行训练。 我们探索了各种重建方法，其规模从经典的迭代方法（基于物理成像模型）到具有许多学习参数的深度学习方法。 在中间地带，我们提出了几种具有不同数量学习参数的非滚动交替乘法（ADMM）的变体。网络结构结合了物理成像模型的知识和从数据中更新的学习参数，这些参数补偿了由物理近似引起的伪影。我们的非滚动方法比经典方法快20倍，在我们的实验系统上产生的重建质量比经典和深度方法都好。 
深度学习是深度神经网络研究工作的重塑，近年来取得了令人瞩目的成就。通过多个隐藏层，深度学习模型旨在计算观察数据的分层特征表示。同时，由于其在数据消耗、计算资源、参数调整成本和结果可解释性方面的严重缺点，深度学习也遭受了很多批评。 在本文中，我们将介绍一种新的表征学习模型，即 "抽样集合遗传进化网络"（SEGEN），它可以作为深度学习模型的替代方法。SEGEN不是基于一组抽样的子实例建立一个单一的深度模型，而是采用遗传进化学习策略，逐代建立一组单元模型。 SEGEN中的单元模型既可以是传统的机器学习模型，也可以是最近出现的具有更 "窄 "和更 "浅 "架构的深度学习模型。 从计算的角度来看，SEGEN需要的数据、计算资源和参数调整工作要少得多，但对学习过程和结果具有良好的理论可解释性。在几个不同的真实世界基准数据集上做了大量的实验，SEGEN获得的实验结果证明了它比最先进的表征学习模型的优势。
我们如何教人工代理灵活地使用人类语言来解决现实世界环境中的问题？我们在自然界中有一个代理能够解决这个问题的例子：人类婴儿最终学会了使用人类语言来解决问题，他们是由一个成年的人类在循环中教导的。不幸的是，目前的机器学习方法（如深度强化学习）数据效率太低，无法以这种方式学习语言（3）。一个突出的目标是找到一种具有合适的 "语言学习先验 "的算法，使其能够学习人类语言，同时使所需的人类互动数量最小化。在本文中，我们建议在模拟中学习这样一个先验，利用机器学习实验越来越多的可用计算量（1）。 具体来说，在L2C中，我们在模拟中训练元学习代理与预先训练好的代理群体进行互动，每个代理都有自己独特的通信协议。一旦元学习代理能够快速适应每个代理群体，它就可以被部署到训练中未看到的新群体中，包括人类群体。 为了展示L2C框架的前景，我们在刘易斯信号游戏（4）中进行了一些初步的实验，我们表明用L2C约束的代理能够在比随机初始化的代理更少的迭代中学习一种简单的人类语言形式（由手工编码的组成语言表示）。
我们从超参数优化的角度来研究适合特定任务的学习率时间表的问题。 我们描述了验证误差的梯度结构，即学习率的超梯度，并在此基础上介绍了一种新的在线算法。我们的方法在最近提出的两种技术（Franceschi等人，2017；Baydin等人，2018）之间进行了自适应插值，具有更高的稳定性和更快的收敛性。
近年来，从基于场景的文本描述中生成图像的领域出现了一些令人兴奋的发展。这些方法主要集中在从静态文本描述中生成图像，并且仅限于在一个单一的过程中生成图像，他们无法根据增量的文本描述（更直观和类似于我们描述图像的方式）互动地生成图像。我们提出了一种基于场景描述的图形序列（场景-图形）增量生成图像的方法。我们提出了一种循环网络架构，它保留了以前步骤中生成的图像内容，并根据新提供的场景信息修改累积的图像。 我们的模型利用图卷积网络（GCN）来满足可变大小的场景图，同时利用生成对抗性图像翻译网络来生成现实的多物体图像，在训练过程中不需要任何中间监督。我们用Coco-Stuff数据集进行实验，该数据集有多物体图像和描述视觉场景的注释，结果表明我们的模型在同一数据集上生成视觉上一致的图像方面明显优于其他方法，而这些场景图是递增的。
在一些重要的计算机视觉领域，如医学或高光谱成像，我们关心的是大图像中微小物体的分类。然而，大多数用于图像分类的卷积神经网络（CNN）是使用有偏见的数据集开发的，这些数据集包含大型物体，主要在图像中心位置。 为了评估经典的CNN架构是否能很好地用于微小物体的分类，我们建立了一个包含两个数据集的综合测试平台：一个来自MNIST数字，一个来自组织病理学图像。这个测试平台允许控制实验，以广泛的信噪比对CNN架构进行压力测试。(1)存在一个信噪比的极限，低于这个极限，CNN就不能泛化，这个极限受数据集大小的影响--更多的数据导致更好的性能；然而，模型泛化所需的训练数据量随着物像比的倒数而迅速增加。(3)当知道物体的近似尺寸时，适应感受野是有益的；(4)对于非常小的信噪比，全局汇集操作的选择会影响优化，而对于相对较大的信噪比值，所有测试的全局汇集操作都表现出类似的性能。
最近在视觉中纳入注意力机制的趋势使研究人员重新考虑卷积层作为主要构建模块的至高无上的地位。除了帮助CNN处理长距离的依赖性，Ramachandran等人（2019）表明，注意力可以完全取代卷积，并在视觉任务上实现最先进的性能。这提出了一个问题：学习的注意力层是否与卷积层类似地运作？ 这项工作提供了证据，证明注意力层可以执行卷积，而且，它们在实践中经常学习这样做。具体而言，我们证明了具有足够数量的头的多头自我注意力层至少与任何卷积层一样具有表现力。我们的数字实验表明，自我注意力层对像素网格模式的关注与CNN层类似，证实了我们的分析。
我们为低秩分解问题引入了一种 "基于学习 "的算法：给定一个$n\times d$矩阵$A$和一个参数$k$，计算一个秩-$k$矩阵$A'$，使近似损失$||A- A'||_F$最小。该算法使用输入矩阵的训练集，以优化其性能。 具体来说，一些计算低秩近似的最有效的近似算法是通过计算投影$SA$进行的，其中$S$是一个稀疏的随机$m/times n$"草图矩阵"，然后进行$SA$的奇异值分解。 我们展示了如何用相同稀疏度的 "学习 "矩阵取代随机矩阵$S$，以减少误差。我们的实验表明，对于多种类型的数据集，与随机矩阵$S$相比，学习的草图矩阵可以大大减少近似损失，有时可以减少一个数量级。我们还研究了混合矩阵，其中只有一些行是训练过的，其余的是随机的，并表明矩阵仍然提供改进的性能，同时保留最坏情况下的保证。
神经对话模型在个人助理和聊天机器人等应用中被广泛使用。这些模型在词的层面上操作时似乎有更好的性能。然而，对于像法语、俄语和波兰语这样的融合语言，词汇量有时变得不可行，因为大多数词都有很多词的形式。我们提出了一个神经网络架构，将规范化的文本转化为语法正确的文本。 我们的模型有效地采用了规范化和目标词之间的对应关系，并且明显优于字符级模型，同时在训练时快2倍，在评估时快20％。我们还提出了一个新的管道来建立对话模型：首先生成一个规范化的答案，然后用我们的网络将其转化为一个语法正确的答案。
本文提出使用谱元方法\citep{canuto_spectral_1988}来快速准确地训练神经常微分方程（ODE-Nets; `citealp{Chen2018NeuralOD}）进行系统识别.这是通过将其动态表达为Legendre多项式的截断序列来实现的。 序列系数以及网络权重是通过最小化损失函数和ODE-网络动态的违反的加权和来计算的。这个问题是通过坐标下降来解决的，在系数和权重方面，使用标准的反向传播和梯度方法交替最小化两个无约束的子问题。 由此产生的优化方案是完全时间并行的，并导致低内存占用。与标准方法，如通过显式求解器的反向传播和邻接技术{Chen2018NeuralOD}进行的实验比较表明，它在达到损失函数的可比值时至少快一个数量级。相应的测试MSE也小一个数量级，表明泛化能力增加。
稀疏奖励强化学习中的探索仍然是一个公开的挑战。许多最先进的方法使用内在动机来补充稀疏的外在奖励信号，使代理在探索过程中获得更多的反馈机会。通常这些信号被添加为奖励，这导致了一种混合政策，既不坚定地进行探索也不坚定地完成任务。 此外，我们引入了一种新的内在奖励类型，表示为后继特征控制（SFC），它是通用的，不针对特定的任务。它考虑到了完整轨迹的统计数据，因此不同于以前只使用局部信息来评估内在动机的方法。我们使用三种不同的纯视觉输入环境来评估我们提出的预定内在驱动（SID）代理。VizDoom、DeepMind Lab和DeepMind Control Suite。结果显示，使用SFC和分层使用内在驱动的探索效率有了大幅提高。我们的实验结果视频可以在https://gofile.io/?c=HpEwTd。
元强化学习方法旨在开发学习程序，使其能够在少数例子的帮助下快速适应任务的分布。开发能够找到最有用的样本的高效探索策略在这种环境下变得至关重要。现有的寻找高效探索策略的方法增加了辅助目标，以促进预更新策略的探索，然而，这使得使用几个梯度步骤的适应变得困难，因为预更新（探索）和后更新（利用）策略是相当不同的。 相反，我们建议为任务分布明确地建立一个单独的探索政策模型。有两种不同的政策，在训练探索政策方面有更大的灵活性，也使适应任何特定的任务变得更容易。我们表明，使用自我监督或监督学习目标来适应，可以稳定训练过程，也证明了我们的模型与该领域的先前工作相比有更高的性能。
深度学习中的 "超对称人工神经网络"（表示为（x；θ，bar{θ}）Tw），支持考虑生物约束的重要性，目的是进一步泛化反向传播。纵观 "解几何 "的进展；从SO(n)表示（如类似Perceptron的模型）到SU(n)表示（如UnitaryRNNs），保证了人工神经网络的权重空间中越来越丰富的表示，因此可以生成越来越好的假设。"超对称人工神经网络 "探索了一个自然的进步，即SU（m|n）表示。 这些超对称的生物脑表征（Perez等人）可以用超电荷兼容的特殊单元符号SU(m|n)来表示，或者（x；θ，bar{θ}）Tw参数化的θ，bar{θ}是超对称的方向，与典型的非超对称深度学习模型中的θ不同。值得注意的是，超对称值可以编码或表示比典型深度学习模型更多的信息，例如在 "伙伴潜能 "信号上。
基于正则化的持续学习方法通常通过用辅助目标增加训练损失来防止灾难性遗忘。然而在大多数具有噪声数据和/或梯度的实际优化场景中，随机梯度下降有可能无意中改变关键参数。在本文中，我们论证了直接正则化优化轨迹的重要性。 我们为持续学习推导了一个新的共自然梯度更新规则，新的任务梯度以先前学习的任务的经验Fisher信息为前提。我们表明，使用共自然梯度可以系统地减少持续学习中的遗忘。
我们研究了在一种强类型的、类似于Java的编程语言中生成源代码的问题，给定一个标签（例如一组API调用或类型），携带少量关于所需代码的信息。生成的程序被期望尊重程序和标签之间的 "现实 "关系，正如训练期间可用的标签程序语料库所体现的那样。为了解决这些问题，我们不是在代码上而是在*程序草图*上训练神经生成器，或者说是在程序语法上抽象出名称和操作的模型，这些名称和操作在程序中并不通用。在生成过程中，我们推断出草图上的osterior分布，然后使用组合技术将这个分布中的样本具体化为类型安全的程序。我们在一个生成API密集型Java代码的系统中实现了我们的想法，并表明它通常可以预测一个方法的全部内容，只需给出方法中出现的几个API调用或数据类型。
我们提出了一种基于自回归归一化流量的序列建模方法。每个自回归变换，跨时间作用，作为一个移动的参考框架，用于建模更高层次的动态。这个技术提供了一个简单的、通用的方法来改善序列建模，与现有的和经典的技术有联系。
众所周知，许多机器学习模型容易受到对抗性攻击，在这种情况下，攻击者通过对输入进行小的扰动来逃避分类器。本文讨论了在网络上起核心作用的工业版权检测工具是如何容易受到对抗性攻击的。 这些弱点对于基于神经网络的系统来说尤其明显。 作为概念证明，我们描述了一个著名的音乐识别方法，并以神经网络的形式实现了这个系统。然后我们使用简单的梯度方法攻击这个系统。以这种方式创建的对抗性音乐成功地骗过了工业系统，包括AudioTag版权检测器和YouTube的内容ID系统。我们的目标是提高对这个领域的对抗性例子所带来的威胁的认识，并强调加强版权检测系统对攻击的重要性。
平衡传播（EP）是一种连接机器学习和神经科学的学习算法，通过计算梯度与通过时间的反向传播（BPTT）密切匹配，但学习规则在空间中是局部的。给定一个输入x和相关目标y，EP分两个阶段进行：在第一阶段，神经元向第一个稳定状态自由进化；在第二阶段，输出神经元被推向y，直到它们达到第二个稳定状态。 然而，在现有的EP实现中，学习规则在时间上不是局部的：权重更新是在第二阶段的动力学收敛后进行的，需要第一阶段的信息，而这些信息在物理上已不再可用。 在这项工作中，我们提出了一个名为连续平衡传播（C-EP）的EP版本，其中神经元和突触的动态在整个第二阶段同时发生，因此权重更新在时间上成为局部的。我们从理论上证明，只要学习率足够小，在第二阶段的每个时间步骤，神经元和突触的动态遵循BPTT给出的损失梯度（理论1）。 我们在MNIST上演示了C-EP的训练，并将C-EP推广到神经元由不对称连接的神经网络。我们通过实验表明，网络更新越是遵循BPTT的梯度，它在训练方面的表现就越好。这些结果使EP更接近生物学，同时保持其与反向传播的密切联系。
关于视觉推理的研究有两条主线：通过手工制作的神经模块进行明确的多跳推理的神经模块网络（NMN），以及在潜在特征空间进行隐性推理的单体网络。前者在可解释性和组成性方面表现出色，而后者由于模型的灵活性和参数效率，通常能取得更好的性能。 为了弥补两者之间的差距，我们提出了Meta Module Network（MMN），这是一种新颖的混合方法，可以有效地利用Meta Module来执行各种功能，同时通过模块化设计来保持组合性和可解释性。所提出的模型首先通过程序生成器将输入问题解析为一个功能程序，而不是像传统的NMN那样手工制作一个特定任务的网络来表示每个功能，我们使用Recipe Encoder将功能转化为相应的配方（规格），这些配方用于动态地将Meta Module实例化为实例模块。 为了赋予不同的实例模块以指定的功能，我们提出了一个师生框架，其中一个符号化的教师针对场景图预先执行，为实例模块（学生）提供指导。总之，MMN采用元模块来提高其参数化效率，并使用配方编码来提高其比NMN更强的泛化能力。在GQA基准上进行的实验表明。(1) MMN比NMN和单片机网络基线都取得了明显的改进；(2) MMN能够泛化到未见过的但相关的功能。
我们对针对深度强化学习代理的对抗性攻击提出了一个新的观点。我们的主要贡献是CopyCAT，这是一个有针对性的攻击，能够持续地引诱代理遵循外人的政策。它是预先计算的，因此可以快速推断，因此可以在实时情况下使用。我们在新的只读设置中展示了它在Atari 2600游戏上的有效性。 在后者中，对手不能直接修改代理的状态--它对环境的表示--而只能攻击代理的观察--它对环境的感知。直接修改代理的状态将需要对代理的内部工作进行写入访问，我们认为这一假设在现实环境中过于强大。
Top-k推荐的冷启动和效率问题对大规模推荐系统至关重要。以前的混合推荐方法通过从侧面信息中提取冷启动项目（用户）的真实潜伏因素，有效地处理了冷启动问题，但由于真实潜伏空间中昂贵的相似性搜索，它们的在线推荐效率仍然很低。本文提出了一种协作生成散列（CGH），通过将用户和项目表示为二进制代码来提高效率，它适用于各种设置：冷启动用户、冷启动项目和暖启动项目。 具体来说，CGH旨在通过最小描述长度（MDL）原则学习用户和项目的散列函数；因此，它可以处理各种推荐设置。此外，CGH通过生成步骤挖掘潜在用户，启动了新的营销策略。为了重建有效的用户，MDL原则被用来从内容数据中学习紧凑和信息丰富的二进制代码。在两个公共数据集上进行的广泛实验表明，在各种设置中的推荐比竞争的基线有优势，并分析了在营销中应用的可行性。
最近将表征学习与形式方法相结合的努力，通常被称为神经符号方法，引起了应用丰富的神经架构来解决经典组合优化问题的新趋势。在本文中，我们提出了一个可以学习解决电路满足性问题的神经框架。 我们的框架建立在两个基本的贡献上：一个丰富的嵌入架构，编码问题结构和一个端到端的可区分训练程序，模仿强化学习，直接训练模型解决SAT问题。
序列生成模型，如递归网络，可以用不同的学习算法进行训练。例如，最大似然学习简单高效，但却存在暴露偏差问题。像策略梯度这样的强化学习可以解决这个问题，但探索效率却很低。 我们提出了一个广义的熵正则化策略优化的表述，并表明这些明显不同的算法都可以被重新表述为该框架的特殊实例，唯一的区别是奖励函数和几个超参数的配置。此外，基于该框架，我们提出了一个新的算法，在现有的算法中动态插值，以提高学习效果。
我们正在报告SHINRA项目，这是一个用协作构建方案构建维基百科的项目。该项目旨在创建一个巨大的、结构良好的知识库，用于NLP应用，如QA、对话系统和可解释NLP系统。 我们进行了一项结构化维基百科的共享任务，同时，提交的结果被用来构建一个知识库。有一些机器可读的知识库，如CYC、DBpedia、YAGO、Freebase Wikidata等，但每一个都有问题需要解决。CYC有一个覆盖问题，而其他的有一个一致性问题，因为这些是基于维基百科和/或由许多但本质上不连贯的人群工作者创建。 为了解决后面的问题，我们开始了一个使用自动知识库构建共享任务来构建维基百科的项目。自动知识库构建共享任务已经流行了几十年，并得到了很好的研究。然而，这些任务的设计只是为了比较不同系统的性能，并找到哪个系统在有限的测试数据上排名最好。2.将所有系统的输出向公众开放，这样我们就可以运行集合学习，创造出比最好的系统更好的结果3.重复任务，这样我们就可以用前一个任务输出的更大、更好的训练数据来运行任务（引导和主动学习）我们用上述方案进行了 "SHINRA2018"，在本文中我们报告了项目的结果和未来方向。 我们已经将日本维基百科中的大部分实体（即73万个实体）归类到200个ENE类别中。基于这些数据，共享任务是从维基百科页面中提取属性值。 我们给出了600个训练数据，并要求参与者提交同一类别的所有剩余实体的属性值。然后，每个类别的100个数据被用来评估共享任务中的系统输出。我们对输出结果进行了初步的集合学习，发现在一个类别上有15个F1得分改进，在我们测试的所有5个类别上平均有8个F1得分改进。 基于这个有希望的结果，我们决定在2019年进行三个任务；多语言分类任务（ML），用更大的训练数据提取相同的5个类别的日语（JP-5）和提取34个新类别的日语（JP-34）。
最近的图像超级分辨率(SR)研究利用了非常深入的卷积神经网络和它们提供的丰富的层次特征，这导致了比传统方法更好的重建性能。然而，这些模型的上采样和重建过程中的小感受野使它们无法充分利用全局上下文信息，这为进一步提高性能带来了问题。 具体来说，我们提出了全局推理上采样模块（GRUM）和全局推理重建块（GRRB）。它们构建了一个图模型，对低分辨率（LR）图像的区域进行关系推理。它们旨在推理上采样和重建过程中不同区域之间的相互作用，从而利用更多的背景信息来产生准确的细节。 我们提出的SRGRN更加稳健，可以处理被多种退化类型破坏的低分辨率图像。在不同的基准数据集上进行的广泛实验表明，我们的模型优于其他最先进的方法。同时，我们的模型是轻量级的，消耗的计算能力较少，这使得它非常适用于现实生活的部署。
图形神经网络（GNNs）最近受到了极大的关注，因为它们在处理不同应用领域的下游任务的图形数据方面具有强大的能力。 然而，在以下方面仍然缺乏深入的分析：（1）是否存在一个能在所有图形数据上表现最好的过滤器；（2）哪些图形属性会影响图形过滤器的最佳选择；（3）如何设计适应图形数据的适当的过滤器。 基于这些发现，我们开发了自适应滤波图神经网络（AFGNN），一个简单但强大的模型，可以自适应地学习特定任务的滤波器。 对于一个给定的图，它利用图滤波器评估作为正则化，并从一组基础滤波器中学习组合。在合成和现实世界的基准数据集上的实验表明，我们提出的模型确实可以学习一个合适的滤波器，并在图任务上表现良好。
图形神经网络（GNN）中节点池化操作的进展落后于新的消息传递技术的狂热设计，池化仍然是深度架构设计的一个重要和具有挑战性的工作。在本文中，我们提出了一种GNN的池化操作，利用了基于minCut优化目标的可区分的无监督损失。我们的方法为每个节点学习了一个软集群分配向量，该向量取决于节点特征、目标推理任务（例如，图分类损失），并且由于minCut目标，也取决于图的连接结构。
知识图谱是现实世界事实的结构化表示。然而，它们通常只包含所有可能事实的一小部分。链接预测是基于现有事实推断缺失事实的任务。我们提出了TuckER，一个相对简单但强大的线性模型，基于知识图谱三要素的二进制张量表示的Tucker分解。
随着架构设计的创新，更深更广的神经网络模型在各种不同的任务上提供了更好的性能。但这些模型增加的内存占用在训练期间带来了挑战，因为所有中间层的激活需要存储用于反向传播。 有限的GPU内存迫使从业者做出次优选择：要么用较小批次的例子进行低效训练；要么限制架构，使其具有较低的深度和宽度，并在较高的空间分辨率下减少层数。这项工作引入了一种近似策略，在训练期间大大减少网络的内存占用，但对训练性能和计算费用的影响可以忽略不计。 在前向过程中，当激活被后续层使用后，我们立即用较低精度的近似值替换激活，从而释放内存。这种方法限制了前向和后向过程中的错误积累--因为整个网络的前向计算仍然以全精度进行，而近似值在计算层的输入梯度时影响有限。 在CIFAR和ImageNet上的实验表明，使用我们的方法对32位浮点激活进行8位甚至4位定点近似，对训练和验证性能只有很小的影响，同时可以大大节省内存用量。
估算数据流中元素的频率是数据分析和机器学习中的一项基本任务。这个问题通常用流式算法来解决，这些算法可以用有限的存储空间来处理非常大的数据。然而，今天的流式算法不能利用其输入的模式来提高性能。  所提出的算法结合了机器学习的好处和通过算法理论获得的形式保证。 我们证明了我们的基于学习的算法比他们的非学习的算法具有更低的估计误差。 我们还在两个真实世界的数据集上评估了我们的算法，并根据经验证明了它们的性能提升。
简单图中的链接预测是一个基本问题，其中节点之间的新链接是根据观察到的图的结构来预测的。然而，在许多现实世界的应用中，需要对节点之间的关系进行建模，这些关系超越了成对的关联。例如，在化学反应中，反应物和产物之间的关系本来就是高阶的。 此外，还需要表示从反应物到产物的方向。超图提供了一种自然的方式来表示这种复杂的高阶关系。尽管图卷积网络（GCN）最近作为一种强大的基于深度学习的方法出现，用于简单图的链接预测，但它们对超图中的链接预测的适用性还没有被开发出来--我们在本文中填补了这个空白并提出了神经超链接预测器（NHP）。NHP使GCN适用于超图中的链接预测。我们提出了NHP的两个变体--NHP-U和NHP-D--分别用于无向和有向超图的链接预测。据我们所知，NHP-D是第一个用于有向超图的链接预测的方法。
在本文中，我们提出了一种文本表征的对抗性分解方法。这种方法可以用来将一个输入句子的表征分解成几个独立的向量，其中每个向量负责输入句子的一个特定方面。我们在两个案例研究中评估了所提出的方法：不同社会语系之间的转换和非同步语言的变化。 例如，我们的模型能够学习句子风格的连续（而不是分类）表示，与语言使用的现实相一致。该模型使用对抗性动机训练，包括一个特殊的动机损失，其作用与判别器相反，鼓励更好的分解。最后，我们在一个下游的短语检测任务上评估所获得的意义嵌入，并表明它们明显优于常规自动编码器的嵌入。
一组参与慢性病人护理的医疗服务提供者与我们联系，寻找潜在的技术来促进临床访问期间审查病人生成的数据的过程。 为了了解医疗服务提供者对审查病人生成的数据的态度，我们（1）与一组混合的医疗服务提供者进行了一个焦点小组。接下来，为了获得病人的观点，我们（2）采访了八个慢性病人，收集了他们的数据样本，并设计了一系列代表我们收集的病人数据的可视化。最后，我们（3）从要求进行这种探索的医疗服务提供者那里寻求对可视化设计的反馈。 我们发现有四个因素影响着病人生成的数据：数据和背景，病人的动机，病人的时间承诺，以及病人的支持圈。根据我们的研究结果，我们讨论了为个人设计病人生成的可视化的重要性，通过考虑病人和医疗服务提供者，而不是以普遍化为目的进行设计，并为未来设计病人生成的数据可视化提供指导。
最近，人们提出了基于神经网络的前向动力学模型，试图以确定的方式学习物理系统的动力学。虽然近期的运动可以被准确预测，但长期的预测会受到输入和预测误差的积累，这可能导致可信但与地面真相不同的轨迹。 因此，一个基于其不确定性预测未来物理状态分布的系统是一个很有前途的解决方案。 在这项工作中，我们引入了一种新的基于图卷积的稳健的蒙特卡洛抽样方法，该方法允许我们在给定一个基于神经网络的前向动力学预测器的情况下，对初始状态进行多个合理的轨迹抽样。 通过引入新的形状保持损失和经常性地训练我们的动力学模型，我们稳定了长期预测。我们表明，我们的模型对各种形状的刚性和可变形物体的复杂物理相互作用的长期前向动力学预测误差明显低于现有的强基线。最后，我们证明了用我们的蒙特卡洛剔除方法产生的多个轨迹可以用来更快地训练无模型的强化学习代理，并在简单操纵任务中获得更好的解决方案。
过去，特别是最近，人们对不同系列的通用函数近似器的相对优势有很大的兴趣，例如神经网络、多项式、有理函数等。然而，目前的研究几乎完全集中在对这个问题的最坏情况的理解上：例如，在一个盒子中描述最佳的L1或L_{infty}近似（有时，甚至在对抗性构造的数据分布下）。在这种情况下，近似理论的许多经典工具可以有效使用。 然而，在典型的应用中，我们希望数据是高维的，但又是结构化的--因此，只需要在其领域的相关部分很好地近似所需的函数，例如，实际输入数据所在的小流形。此外，即使在这个领域内，所需的近似质量也可能不一致；例如在分类问题中，近似需要在决策边界附近更精确。	考虑到这一点，我们分析了神经网络和多项式核在自然回归环境中的性能，其中数据享有稀疏的潜在结构，并且标签以简单的方式依赖于潜在变量。我们对这个问题的神经网络和多项式的性能给出了几乎严密的理论分析，并通过模拟验证我们的理论。
最近的深度生成模型可以提供照片般逼真的图像以及视觉或文本内容的嵌入，有助于解决计算机视觉和自然语言处理的各种任务。然而，它们的用处往往受到对生成过程缺乏控制或对所学表征理解不足的限制。 在本文中，我们建议通过引入一种新的方法，在任何生成模型的潜空间中寻找有意义的方向，我们可以沿着这些方向精确控制生成图像的特定属性，如图像中物体的位置或比例。我们的方法是弱监督的，特别适合搜索编码生成图像的简单变换的方向，如翻译、缩放或颜色变化。
现代神经网络架构使用结构化的线性变换，如低秩矩阵、稀疏矩阵、排列组合和傅里叶变换，以提高推理速度，并减少与一般线性图相比的内存使用。然而，从无数结构化变换中选择使用哪一种（及其相关参数化）是一项费力的工作，需要在速度、空间和准确性之间进行交易。 我们考虑了一种不同的方法：我们引入了一个被称为万花筒矩阵（K-matrices）的矩阵系列，该系列可以证明以接近最优的空间（参数）和时间（算术运算）复杂性捕获任何结构化矩阵。我们通过经验验证，K-matrices可以在端到端管道中自动学习，以取代手工制作的程序，从而提高模型质量。 例如，取代ShuffleNet中的通道洗牌，可以将ImageNet的分类精度提高5%。可学习的K-矩阵也可以简化手工设计的管道--我们用万花筒层取代语音数据预处理中的滤波器组特征计算，结果在TIMIT语音识别任务中只损失0.4%的精度。 K矩阵还可以捕捉模型中的潜在结构：对于一个具有挑战性的包络图像分类任务，在标准卷积架构中加入K矩阵可以学习潜在的包络，并将准确率提高8个百分点以上。我们为我们的方法提供了一个实际有效的实现，并在Transformer网络中使用K矩阵，使语言翻译任务的端到端推理速度提高36%。
我们提出了一种被称为标签嵌入网络的方法，它可以在深度网络的训练过程中学习标签表示（标签嵌入）。用所提出的方法，标签嵌入是通过反向传播自适应地自动学习的。原来的一热表示的损失函数被转换为具有软分布的新损失函数，这样，原来不相关的标签在训练过程中就会有连续的相互作用。 基于竞争性任务的实验结果证明了所提方法的有效性，所学的标签嵌入是合理和可解释的。所提方法取得了与最先进的系统相当甚至更好的结果。
深度学习模型的分散训练是实现数据隐私和网络上的设备学习以及有效扩展到大型计算集群的关键因素。由于目前的方法受到网络带宽的限制，我们建议在分散训练的背景下使用通信压缩。我们在两个关键场景中展示了该算法的实际性能：深度学习模型的训练（i）通过点对点网络连接的分散用户设备和（ii）在数据中心。
我们表明，Entropy-SGD（Chaudhari等人，2017），当被视为一种学习算法时，优化了对吉布斯（后验）分类器风险的PAC-Bayes约束，即。Entropy-SGD通过优化约束的先验，违反了PAC-Bayes定理的假设，即先验的选择是独立于数据的。 事实上，Entropy-SGD的现有实现在随机标签上迅速获得了零训练误差，Gibbs后验也是如此。为了获得有效的泛化约束，我们表明ε-差分隐私先验产生了有效的PAC-Bayes约束，这是连接泛化与差分隐私的结果的直接结果。 使用随机梯度Langevin动力学（SGLD）来近似众所周知的指数释放机制，我们观察到MNIST上的泛化误差（在保留的数据上测量）落在SGLD产生完美样本的假设下计算的（经验上不模糊的）界限内。特别是，Entropy-SGLD可以被配置成产生相对严格的泛化界限，并且仍然适合真实的标签，尽管这些相同的设置没有获得最先进的性能。
在本文中，我们研究了用于工业制造中的自动光学检测的深度神经网络的学习。我们的初步结果显示，通过从完全不同的源域转移学习，性能得到了惊人的改善。实验结果显示，通过转移学习学习的网络在压缩到1/128的卷积器数量之前，准确率的下降可以忽略不计。
半监督学习的关键挑战是如何有效地利用未标记的数据来提高学习性能。经典的标签传播方法尽管很受欢迎，但其建模能力有限，因为它只利用图信息进行预测。本文中，我们从图信号处理的角度考虑标签传播，并将其分解为三个部分：信号、过滤器和分类器。通过扩展这三个部分，我们为半监督学习提出一个简单的通用标签传播（GLP）框架。 GLP自然地整合了图和数据特征信息，并为不同的应用提供了选择适当的过滤器和特定领域分类器的灵活性。有趣的是，GLP也为流行的图卷积网络提供了新的见解，并阐明了其工作机制。在三个引文网络、一个知识图和一个图像数据集上进行的广泛实验证明了GLP的效率和效果。
由于优化器的选择和调整会影响深度学习的速度，并最终影响其性能，因此在这一领域有大量的过去和最近的研究。然而，也许令人惊讶的是，对于深度学习的优化策略的定量和可重复的评估，还没有普遍认同的协议。 我们提出了随机优化的例程和基准，特别关注深度学习的独特方面，如随机性、可调性和泛化。作为主要贡献，我们提出了DeepOBS，一个深度学习优化基准的Python包。 该库包括一套广泛和可扩展的现成的现实优化问题，如在ImageNet上训练图像分类的残差网络或字符级语言预测模型，以及流行的经典问题，如MNIST和CIFAR-10.该包还提供了最流行的优化器在这些测试问题上的现实基线结果，确保在对新的优化器进行基准测试时与竞争对手进行公平比较，而无需运行昂贵的实验。它带有输出后端，直接产生LaTeX代码，以纳入学术出版物。它支持TensorFlow，并可开放源代码。
预训练的词嵌入是一些自然语言处理（NLP）任务中转移学习的主要方法。最近的工作主要是使用无监督技术，如语言建模来获得这些嵌入。 在跨任务、跨领域和跨语言环境下进行的实验表明，这种监督嵌入是有帮助的，尤其是在低资源环境下，但收益的程度取决于任务和领域的性质。
我们建立了一个理解实用元学习方法的理论框架，使任务相似性的复杂形式化与在线凸优化和顺序预测算法的大量文献相结合，以提供任务内的性能保证。 我们的方法改进了最近对参数转移的分析，使任务相似性能够被自适应地学习，并在统计学习到学习的设置中改进了转移风险界限。它还导致了在任务环境动态变化或任务共享某种几何结构的情况下，高效算法的平均案例遗憾界限的直接推导。
在这项工作中，我们提出了一种自我监督的方法，通过注入语言学知识来学习句子表征。多种语言学框架提出了不同的句子结构，从这些结构中可以表达出语义的组合词操作。我们旨在利用这种语言学家的多样性，通过对比这些不同的观点来学习表征句子。 从形式上看，同一个句子的多种观点被映射到接近的表征中。相反，来自其他句子的观点被进一步映射。通过对比不同的语言学观点，我们旨在建立能更好地捕捉语义、对句子外在形式不太敏感的嵌入。
外围神经系统代表了大脑的输入/输出系统。植入外围神经系统的袖带电极可以观察和控制这个系统，然而，这些电极产生的数据具有低信噪比和复杂的信号内容。在本文中，我们考虑分析动物模型中迷走神经记录的神经数据，并开发了一个基于卷积神经网络的无监督学习器，能够同时根据信号内容对数据区域进行去噪和聚类。
对卷积神经网络（CNN）的对抗性攻击得到了极大的关注，在防御机制方面也有积极的研究工作。有人提出了随机输入变换方法，其思路是通过随机变换来恢复图像免受对抗性攻击，并在随机样本中取多数票作为共识。 然而，这种转变是以牺牲干净图像的准确性为代价来提高对抗性图像的准确性的。虽然凭直觉，干净图像的准确性会恶化，但这种情况发生的确切机制还不清楚。在本文中，我们研究了随机转变引起的softmax分布。 我们观察到，在清洁图像上进行随机变换时，虽然softmax分布的质量可能会转移到错误的类别，但所产生的softmax分布可以用来纠正预测。此外，在对抗性的对应图像上，随着图像的变换，所产生的softmax分布的形状与清洁图像的分布相似。有了这些观察，我们提出了一种改进现有的基于变换的防御方法。 我们训练了一个单独的轻量级分布分类器，以识别转换后图像的softmax输出分布中的不同特征。我们的实证研究表明，我们的分布分类器，通过仅对从干净图像中获得的分布进行训练，在干净图像和对抗性图像中都优于多数投票。我们的方法是通用的，可以与现有的基于转换的防御系统集成。
多Agent场景下的强化学习对现实世界的应用非常重要，但它所带来的挑战超出了单Agent环境下的挑战。我们提出了一种行为者批评算法，在多Agent环境下训练分散的策略，使用集中计算的批评，共享一个注意力机制，在每个时间点为每个代理选择相关信息。 与最近的方法相比，这种注意力机制能够在复杂的多代理环境中实现更有效和可扩展的学习。我们的方法不仅适用于具有共享奖励的合作环境，也适用于个性化的奖励环境，包括对抗性环境，而且它对代理的行动空间不做假设。
最近，机器学习在分子生物学中的应用扩展，证明对我们理解生物系统，特别是基因组功能有重大贡献。技术进步使我们能够收集大量的表观遗传数据集，包括各种DNA结合因子（ChIP-Seq）和DNA空间结构（Hi-C）的信息。一些研究证实了DNA结合因子和DNA结构中拓扑关联域（TAD）之间的相关性。 在这项研究中，我们专注于预测经典模式生物黑腹果蝇中DNA折叠模式的机器学习方法。本文考虑了具有四种正则化的线性模型、梯度提升和循环神经网络，用于预测来自表观遗传标记的染色质折叠模式。 双向LSTM RNN模型优于所有的模型，获得了最好的预测分数。这表明了复杂模型的利用和顺序DNA状态的记忆对染色质折叠的重要性。
以前的工作表明，大型神经网络可以在保持其准确性的同时大幅缩小尺寸。模型压缩成为一个核心研究课题，因为它对于在计算和内存资源有限的设备上部署神经网络至关重要。 大多数的压缩方法都是基于启发式的，对于任意新样本的压缩率和近似误差之间的权衡没有提供最坏的保证。我们提出了第一个高效的、与数据无关的神经修剪算法，在其压缩率和任何未来测试样本的近似误差之间有一个可证明的权衡。 我们的方法是基于coreset框架，它找到了一个小的加权子集，可以证明接近原始输入。具体来说，我们通过前一层的神经元核心集来接近一层神经元的输出，并抛弃其余的神经元。我们从上到下逐层应用这个框架。 与以前的工作不同，我们的核心集是独立于数据的，这意味着它可以证明保证任何输入$x/in \mathbb{R}^d$的函数的准确性，包括一个对抗性的。
部分可观察环境中的规划问题不能直接用卷积网络解决，需要某种形式的记忆。但是，由于同时学习访问记忆和规划的复杂性，即使是具有复杂寻址方案的记忆网络也无法令人满意地学习智能推理。 在较低的层次上，它学习在本地观察的空间中进行规划。在较高的层次上，它使用在本地观察的空间中计算出的策略集合来学习它所处的全局环境中的最佳规划。该网络的性能在存在简单和复杂障碍的环境中的路径规划任务上得到了评估，此外，还测试了它对训练集中未见的新环境的概括能力。
诸如ELMo和BERT这样的语境化词汇表征已经成为将预训练表征纳入下游NLP任务的事实上的起点。在这些环境中，语境化表征在很大程度上使其静态嵌入的前身如Word2Vec和GloVe过时。 在这项工作中，我们介绍了从现有的预训练的上下文表征中生成静态查找表嵌入的简单方法，并证明它们在各种单词相似性和单词关联性任务中的表现优于Word2Vec和GloVe嵌入。 在这样做的时候，我们的结果也揭示了一些见解，这些见解可能对随后使用我们的嵌入或原始上下文模型的下游任务有用。此外，我们通过应用现有的估计词嵌入中的社会偏见的方法，证明了分析的增加潜力。 我们的分析构成了对上下文词表征中社会偏见的最全面的研究（通过我们提炼的嵌入的代理），并揭示了当前量化词嵌入中社会偏见的技术中的一些不一致之处。我们公开发布我们的代码和提炼的词嵌入，以支持可重复的研究和更广泛的NLP社区。
大脑进行无监督学习和（也许）同时进行监督学习。这就提出了一个问题，即监督和无监督方法的混合是否会产生更好的学习效果。受Hebbian学习规则的丰富空间的启发，我们着手在本地信息上直接学习无监督学习规则，以最佳方式增强监督信号。 我们提出了Hebbian-augmented训练算法（HAT），用于将基于梯度的学习与突触前活动、突触后活动和当前权重的无监督规则结合起来。我们在一个简单的问题（Fashion-MNIST）上测试了HAT的效果，发现其性能始终高于单独的监督学习。这一发现提供了经验证据，证明突触活动的无监督学习提供了一个强大的信号，可以用来增强基于梯度的方法。       我们进一步发现，元学习的更新规则是一个随时间变化的函数；因此，很难确定一个可解释的Hebbian更新规则来帮助训练。 我们确实发现，元学习器最终会退化成一个非赫比安规则，保留了重要的权重，以避免干扰学习器的收敛。
深度卷积网络通常会在其卷积操作中加入加法常数（"偏置"）项，以实现更丰富的功能映射。偏置也被用来促进训练，通过减去成批训练图像的平均响应（"批归一化 "的一个组成部分）。 最近最先进的盲去噪方法似乎需要这些条款才能成功。然而，我们在这里表明，大多数CNN中使用的偏置条款（加法常数，包括那些用于批量归一化的条款）干扰了这些网络的可解释性，对性能没有帮助，而且事实上阻止了对训练数据中不包括的噪声水平的性能概括。 特别是，无偏置CNN（BF-CNN）是局部线性的，因此可以用线性代数工具进行直接分析。这些分析以投影到低维子空间联盟的方式提供了网络功能的解释，将基于学习的方法与更传统的去噪方法联系起来。此外，BF-CNN的泛化能力很强，在远远超出其训练范围的噪声水平上实现了接近最新的性能。
基于梯度优化的深度神经网络的初始参数值的选择是深度学习系统中最有影响的超参数选择之一，影响收敛时间和模型性能。然而，尽管有大量的经验和理论分析，关于不同初始化方案的具体效果，相对来说证明得很少。在这项工作中，我们分析了深度线性网络中初始化的效果，并首次提供了一个严格的证明，即从正交组抽取初始权重，相对于标准高斯初始化的iid权重，加快了收敛。 我们表明，对于深层网络，正交初始化有效收敛到全局最小值所需的宽度与深度无关，而高斯初始化有效收敛所需的宽度在深度上呈线性扩展。我们的结果表明，良好初始化的好处可以在整个学习过程中持续存在，这说明最近根据动态等值原理初始化非常深的非线性网络所取得的经验成功。
生存函数估计用于许多学科，但它在医学分析中以Kaplan-Meier估计器的形式最为常见。敏感数据（病人记录）被用于估计，而没有明确控制信息泄漏，这是一个重要的隐私问题。 我们提出了第一个生存函数的差异化隐私估计器，并表明它可以很容易地扩展到提供差异化隐私的置信区间和测试统计，而无需花费任何额外的隐私预算。我们进一步提供了竞争风险累积发病率函数的差异化隐私估计的扩展。使用九个真实的临床数据集，我们提供经验证据，我们提出的方法提供了良好的效用，同时提供强大的隐私保证。
神经网络可以学习从数据中提取统计属性，但它们很少利用标签空间的结构化信息来帮助表征学习。虽然在对大量数据进行训练时可以隐含地获得一些标签结构，但在数据很少的情况下，明确利用标签结构可以告知模型重塑表征空间以反映全局意义上的类依赖关系。 我们提出了一个元学习框架，即条件类感知元学习（CAML），该框架基于一个公制空间有条件地转换特征表示，该公制空间被训练用来捕捉类间依赖关系。这使得基础学习者的特征表示有条件地被调制，以施加标签空间告知的规律性。
我们研究了多集预测的问题。多集预测的目标是训练一个预测器，将输入映射到由多个项目组成的多集上。与现有的监督学习问题不同，如分类、排名和序列生成，目标多集中的项目之间没有已知的顺序，而且多集中的每个项目可能出现不止一次，使得这个问题极具挑战性。 在本文中，我们从顺序决策的角度来看待这个问题，提出了一个新的多集损失函数。在两个系列的数据集上对所提出的多集损失函数进行了实证评估，一个是合成的，另一个是真实的，难度不一，与各种基线损失函数包括强化学习、序列和聚合分布匹配损失函数相比，提出的损失函数是有效的。
了解深度和局部连接的非线性网络的理论特性，如深度卷积神经网络（DCNN），尽管它在经验上取得了成功，但仍然是一个困难的问题。在本文中，我们为这种具有ReLU非线性的网络提出了一个新的理论框架。 该框架将数据分布与梯度下降规则联系在一起，倾向于分解表征，并与常见的正则化技术兼容，如Batch Norm，在其投影性质的新发现之后。该框架建立在师生设置之上，通过将学生的前向/后向传递投影到教师的计算图上。我们的框架可以帮助促进对许多实际问题的理论分析，例如深层网络中的分解表示。
多主体合作是自然界的一个重要特征。许多任务涉及到与共同利益不一致的个体激励，但从细菌到昆虫和人类的各种生物都能够克服它们的差异并进行合作。因此，自利个体之间合作行为的出现是多主体强化学习（MARL）和进化理论领域的一个重要问题。 在这里，我们研究了一类特殊的多代理问题，称为时际社会困境（ISDs），其中个人和群体之间的冲突特别尖锐。通过将MARL与适当的结构化自然选择相结合，我们证明个人对合作的归纳偏见可以以无模型的方式学习。
在对机器学习分类器的对抗性攻击中，小的扰动被添加到正确分类的输入中。扰动产生的对抗性例子，与未扰动的输入几乎没有区别，但却被错误分类。在用于深度学习的标准神经网络中，攻击者可以从大多数输入中制作对抗性例子，导致他们选择的错误分类。我们引入了一种新型的网络单元，称为RBFI单元，其非线性结构使它们对对抗性攻击具有内在的抵抗力。在不存在对抗性攻击的MNIST上，使用RBFI单元的网络与使用sigmoid单元的网络性能相当，并略低于使用ReLU单元的网络的准确性。 当受到基于预测梯度下降或快速梯度符号方法的对抗性攻击时，使用RBFI单元的网络保持了75%以上的准确率，而ReLU或Sigmoid的准确率则降低到1%以下。 此外，在常规输入上训练的RBFI网络超过或接近于在对抗性例子帮助下训练的sigmoid和ReLU网络的准确率。我们表明，RBFI单元的RBFI网络可以通过使用伪梯度有效地训练到高准确率，伪梯度的计算是通过特别制作的函数来促进学习而不是其真实导数。
我们表明，在对抗性鲁棒性的目标和标准概括性的目标之间存在着固有的紧张关系。具体来说，训练稳健的模型不仅会消耗更多的资源，而且会导致标准精度的降低。我们证明了模型的标准精度和它对对抗性扰动的稳健性之间的这种权衡，甚至在一个相当简单和自然的环境中也存在。这些发现也证实了在实践中观察到的类似现象。 此外，我们认为这一现象是鲁棒分类器学习与标准分类器根本不同的特征表征的结果。这些差异，特别是，似乎导致了意想不到的好处：鲁棒模型学习的特征往往与突出的数据特征和人类感知更一致。
大型深度神经网络很强大，但也表现出一些不理想的行为，如记忆和对对抗性例子的敏感性。在这项工作中，我们提出了mixup，一个简单的学习原则来缓解这些问题。 通过这样做，mixup使神经网络正规化，以利于在训练实例之间的简单线性行为。 我们在ImageNet-2012、CIFAR-10、CIFAR-100、谷歌指令和UCI数据集上的实验表明，mixup提高了最先进的神经网络架构的泛化能力。 我们还发现，混编减少了对腐败标签的记忆，提高了对对抗性例子的鲁棒性，并稳定了生成性对抗网络的训练。
我们提出了一种新的方法，使用神经聚类过程（NCP）对高密度多电极探针进行尖峰分类，这是一种最近推出的神经架构，为高效的概率聚类执行可扩展的摊销近似贝叶斯推理。为了对尖峰波形进行最佳编码以进行聚类，我们通过添加卷积尖峰编码器来扩展NCP，该编码器与NCP网络进行端到端学习。 纯粹在一个简单的生成模型的标记的合成尖峰上进行训练，NCP尖峰分类模型在多通道尖峰波形的聚类上显示出有希望的性能。 该模型比另一种贝叶斯算法提供了更高的聚类质量，在真实数据上找到了更多具有明确感受野的尖峰模板，与最近的尖峰排序算法相比，在混合测试数据上恢复了更多的地面真相神经元。此外，NCP能够通过GPU并行化后验采样处理模糊的小尖峰的聚类不确定性。源代码是公开的。
本文的目标是提出一种算法，用于从给定的训练数据中学习最通用的解决方案。事实表明，贝叶斯方法导致的解决方案依赖于训练数据的统计，而不是特定的样本。该解决方案在训练数据的扰动下是稳定的，因为它是由可能性的多个最大值的积分贡献而不是单一的全局最大值定义的。 具体来说，由神经网络给出的概率模型的参数（权重）的贝叶斯概率分布是通过递归变异近似来估计的。衍生的递归更新规则对应于SGD型规则，用于寻找有效损失的最小值，有效损失是权重的高斯分布的原始负对数似然的平均值，这使得它成为平均值和变异的函数。 在更新规则的静止解中，在原始损失的局部最小值上有零方差的微不足道的解，还有一个有限方差的非微不足道的解，这是平均方差空间中有效损失的凸性末端的一个临界点。 在临界点上，有效损失的一阶和二阶梯度（相对于均值）都是零。实证研究证实，临界点代表了最通用的解决方案。
人类不断地依赖偶发记忆，记住10分钟前遇到的人的名字，记住电影展开时的情节，或者记住他们把车停在哪里。赋予强化学习代理以偶发记忆是在复制类似人类一般智能的道路上的关键一步。 我们设计了一种新的外部记忆形式，称为掩蔽经验记忆（MEM），它以人类表象记忆的关键特征为模型。为了评估表象记忆，我们定义了一个基于常见的儿童游戏--集中力的RL任务。我们发现MEM RL代理有效地利用表象记忆来掌握集中力，与我们测试的基准代理不同。
参数是机器学习模型最关键的组成部分之一。随着数据集和学习领域的变化，重新学习整个模型往往是必要的，也是耗时的。我们提出了一个框架，建立在emph{最优运输}理论的基础上，通过发现模型和数据之间的对应关系来调整模型参数，大大摊薄了训练成本。我们在为自主机器人创建概率空间表示的挑战性问题上证明我们的想法。 虽然最近的映射技术促进了稳健的占用映射，但在这种近似的贝叶斯模型中学习所有空间上的不同参数需要相当多的计算时间，使它们不适合用于现实世界的机器人映射。考虑到机器人用其传感器观察到的几何特征在不同的环境中是相似的，在本文中，我们展示了如何重新使用在不同领域学到的参数和超参数。 这种适应性在计算上比变分推理和蒙特卡洛技术更有效。在现实环境中进行的一系列实验验证了以可忽略的时间和内存成本转移数千个这样的参数的可能性，使城市环境中的大规模绘图成为可能。
引入了一种算法，用非政策性的时间差（TD）学习来学习预测性的状态表示，然后用强化学习来学习引导车辆。 有三个部分被同时学习。 (1）作为状态的紧凑表示的非政策性预测，（2）用于估计非政策性预测的行为政策分布，以及（3）用于学习行动的确定性政策梯度。 一个行为政策判别器被学习并用于估计用一般价值函数（GVF）学习非政策预测表示所需的重要采样比率。 一个线性确定的政策梯度方法被用来训练代理，只用预测表征，而预测正在被学习。 所有这三个部分被结合起来，在TORCS赛车模拟器环境中从图像中转向车辆的问题上进行了演示和评估。仅从图像中转向是一个具有挑战性的问题，为了衡量预测和控制器的通用性，评估是在训练中从未见过的一组轨道上完成的。 实验表明，所提出的方法能够顺利地转向并导航TORCS中许多但不是所有的轨道，其性能超过了仅使用图像作为输入的DDPG，并接近于理想的基于非视觉的运动学模型的性能。
对于许多领域，包括地球观测、医学成像、天体物理学......，可用的图像和信号数据集往往是不规则的时空采样模式和大量的数据丢失率。在本文中，我们讨论了从不规则采样数据中对信号、图像和图像序列的表征进行端到端学习的问题，{即}当训练数据涉及到缺失数据时，我们考虑基于能量的表征。 这些基于能量的表征（或先验）的学习阶段涉及到一个联合插值问题，这就需要解决观察约束下的能量最小化问题。使用所考虑的能量形式的基于神经网络的实现，我们可以陈述一个来自不规则采样数据的端到端学习方案。我们证明了所提出的表征与不同案例研究的相关性：即多变量时间序列、2{sc }图像和图像序列。
对元强化学习（Meta-RL）中的学分分配仍然知之甚少。现有的方法要么忽略了预适应行为的学分分配，要么是天真地实施了它。这导致了元训练期间的低样本效率以及无效的任务识别策略。 本文对基于梯度的Meta-RL中的学分分配进行了理论分析，在此基础上，我们开发了一种新的元学习算法，该算法克服了学分分配不佳的问题和以前估计元政策梯度的困难。通过控制元政策搜索期间预适应和适应政策的统计距离，所提出的算法赋予了高效和稳定的元学习。
当为一个期望的任务训练一个神经网络时，人们可能更倾向于适应一个预训练的网络，而不是从一个随机初始化的网络开始--由于缺乏足够的训练数据，执行终身学习，系统必须学习一个新的任务，而之前已经为其他任务训练过，或者希望通过预设权重在网络中编码先验因素。在本文中，我们提出了一个直接的替代方案。侧调法通过训练一个轻量级的 "侧 "网络来适应预训练的网络，该网络通过一个简单的加法过程与（未改变的）预训练网络相融合。这个简单的方法与现有的解决方案一样好，甚至更好，同时它解决了一些与微调、固定特征和其他一些常见基线有关的基本问题。 特别是，当训练数据很少时，侧调不容易出现过拟合，比使用固定的特征提取器产生更好的结果，并且在终身学习中不会出现灾难性的遗忘。 我们展示了侧向调整在不同场景下的表现，包括终身学习（iCIFAR，Taskonomy）、强化学习、模仿学习（Habitat中的视觉导航）、NLP问题解答（SQuAD v2）和单任务转移学习（Taskonomy），并取得了一致的良好结果。
在本文中，我们提出了一种生成人工数据集的技术，它保留了真实数据的统计特性，同时对这些数据提供了不同的隐私保证。我们在生成式对抗网络的鉴别器中加入了高斯噪声层，使输出和梯度对训练数据具有不同的隐私，然后使用生成器组件来合成隐私保护的人工数据集。我们的实验表明，在合理的小隐私预算下，我们能够生成高质量的数据，并在这些人工数据上成功训练机器学习模型。
本文提出了两种方法来分解和解释在预训练的深度神经网络中编码的上下文效应。与卷积研究不同的是，卷积研究从全局的角度可视化与网络输出或神经激活相对应的图像外观，我们的研究旨在阐明某个输入单元（维度）如何与其他单元（维度）协作，构成神经网络的推理模式，从而促进网络输出。 对某些输入单元的局部环境影响的分析在实际应用中具有特殊的价值。特别是，我们在实验中用我们的方法来解释alphaGo Zero模型的游戏策略，我们的方法成功地分解了游戏中每一步棋的原理。
网络修剪的主要目标是通过增加零值参数的数量来对神经网络施加稀疏性，以减少架构的大小和计算速度。
为学习神经网络提供可证明的保证是机器学习理论的核心挑战。大多数先前的工作为一个隐藏层网络提供了参数恢复保证，然而，实际使用的网络有多个非线性层。在这项工作中，我们展示了如何将这种结果加强到更深的网络中--我们在假设最低层在应用激活前使用高阈值、上层网络可以被建模为良好的多项式和输入分布是高斯的情况下解决了发掘深度神经网络最低层的问题。
联合学习涉及到从边缘设备上的分布式数据分区（即任务）中训练和有效结合机器学习模型，并被自然地视为一个多任务学习问题。在这项工作中，我们引入了一个框架，称为FedProx，以解决统计异质性的问题。
参考游戏为神经代理提供了一个接地气的学习环境，它说明了语言在功能上是用来交流的。然而，它们没有考虑到第二个被认为是人类语言形态的基本约束：它必须可以被新的语言学习者学习，因此必须克服传输瓶颈。在这项工作中，我们在参考游戏中插入了这样一个瓶颈，通过引入一个不断变化的代理群体，其中新代理通过与更有经验的代理游戏来学习。 我们表明，单纯的文化传播导致了语言效率和交际成功率的大幅提高，其衡量标准是收敛速度、出现的语言的结构程度和语言的种群内一致性。然而，作为我们的核心贡献，我们表明，最佳情况是语言和代理共同进化。
对具有明确特征的大量训练图像数据的需求是将生成式对抗网络（GAN）应用于训练数据有限但多样化的图像生成的主要障碍，因为在已经很稀少的数据中没有足够的潜在特征表示往往导致GAN训练期间的不稳定和模式崩溃。 为了克服在有限的数据集上应用GAN时数据有限的障碍，我们在本文中提出了textit{平行递归数据增强}的策略，即GAN模型通过在连续的训练历时中平行训练的GAN所构建的样本图像来逐步丰富其训练集。在各种小而多样的数据集上的实验表明，我们的方法在几乎不考虑模型的情况下，与没有这种策略产生的图像相比，产生的图像质量更高。
我们开发了一个随机的线虫蛔虫（Caenorhabditis elegans）的全脑和身体模拟器，并表明它有足够的正则化，可以从部分钙荧光成像观察中归纳出潜在的膜电位。 这是我们所知道的 "完成循环 "的第一次尝试，即利用解剖学上的全连接组模拟器，从实际可测量的协变量中，以单细胞的保真度推算时间变化的 "脑 "的状态。 使用最先进的贝叶斯机器学习方法，以容易获得的数据为条件，我们的方法为神经科学家恢复可解释的全连接体状态表示铺平了道路，从数据中自动估计生理上相关的参数值，并进行模拟，研究硅的智能生命体。
从图神经网络（GNNs）中学习到的高质量的节点嵌入已经被应用于广泛的基于节点的应用，其中一些已经取得了最先进的（SOTA）性能。然而，当应用从GNNs中学习到的节点嵌入来生成图嵌入时，标量节点表示可能不足以有效地保留节点/图的属性，导致次优的图嵌入。 受胶囊神经网络（CapsNet）的启发，我们提出了胶囊图神经网络（CapsGNN），它采用了胶囊的概念来解决现有的基于GNN的图嵌入算法的缺陷。 通过提取胶囊形式的节点特征，可以利用路由机制来捕获图层面的重要信息。因此，我们的模型为每个图生成多个嵌入，从不同方面捕获图的属性。CapsGNN中的注意力模块被用来处理不同大小的图，这也使模型能够专注于图的关键部分。 我们对10个图形结构的数据集进行了广泛的评估，证明CapsGNN有一个强大的机制，通过数据驱动来捕捉整个图形的宏观属性。凭借新的工具，它在一些图形分类任务上的表现优于其他SOTA技术。
我们介绍了一个基于受限核机器（RKMs）的生成模型的新框架，该框架具有多视图生成和不相关的特征学习能力，称为Gen-RKM.为了纳入多视图生成，该机制使用来自不同视图的数据的共享表示。该机制很灵活，可以在同一环境中纳入基于核、（深度）神经网络和卷积的模型。
关于语境化词汇表征问题的工作--为句子理解开发可重复使用的神经网络组件--最近出现了一股进展，其核心是用ELMo（Peters等人）等方法进行语言建模的无监督预训练任务。2018）。本文贡献了第一个大规模的系统研究，在此背景下比较不同的预训练任务，既作为语言建模的补充，也作为潜在的替代方案。研究的主要结果支持使用语言建模作为预训练任务，并在使用语言模型的多任务学习的可比模型中设定了新的技术状态。然而，仔细观察这些结果，发现令人担忧的强大基线和引人注目的不同目标任务的结果，表明广泛使用的预训练和冻结句子编码器的范式可能不是进一步工作的理想平台。
在本文中，我们从傅里叶分析的角度研究了深度学习中的对抗性攻防问题。我们首先明确计算了深度ReLU神经网络的傅里叶变换，并表明在神经网络的傅里叶谱中存在衰减但非零的高频成分。 然后我们证明，神经网络对对抗性样本的脆弱性可归因于这些不重要但非零的高频成分。基于这一分析，我们建议使用一个简单的后平均技术来平滑这些高频成分，以提高神经网络对对抗性攻击的稳健性。 在ImageNet和CIFAR-10数据集上的实验结果表明，我们提出的方法普遍有效地防御了文献中提出的许多现有的对抗性攻击方法，包括FGSM、PGD、DeepFool和C&W攻击。我们的后平均化方法很简单，因为它不需要任何重新训练，同时它可以成功地防御这些方法产生的超过80-96%的对抗性样本，而不会对原始的干净图像引入明显的性能下降（小于2%）。
强化学习方法通过博弈树搜索的情节生成不断学习神经网络，在两人完全信息确定的游戏，如国际象棋、将棋和围棋中取得了成功。然而，只有实际案例的报告，几乎没有证据可以保证学习过程的稳定性和最终性能。 在这项研究中，重点关注了剧情生成的协调性。通过将整个系统视为游戏树搜索，新方法可以处理剧情生成过程中利用和探索之间的权衡问题。
消息传递神经网络（MPNNs）已经成功地应用于现实世界中的各种应用。然而，MPNNs的聚合器的两个基本弱点限制了它们表示图结构数据的能力：失去邻域中节点的结构信息和缺乏捕捉不对称图中长程依赖关系的能力。 很少有研究从不同的角度注意到这些弱点。从对经典神经网络和网络几何的观察中，我们提出了一种新的图神经网络的几何聚合方案来克服这两个弱点。 背后的基本想法是，图上的聚合可以从图底层的连续空间中受益。所提出的聚合方案是互换不变的，由三个模块组成，节点嵌入、结构邻域和双级聚合。我们还提出了该方案在图卷积网络中的实现，称为Geom-GCN，以在图上进行归纳学习。
我们考虑了在程序输出上存在奖励函数的情况下进行程序合成的任务，目标是找到具有最大奖励的程序。我们引入了一种新的迭代优化方案，在一个由迄今为止生成的程序的优先级队列中的K个最佳程序组成的数据集上训练一个RNN。然后，我们合成新的程序，并通过从RNN中取样将它们添加到优先级队列中。 我们将我们的算法称为优先队列训练（PQT），与遗传算法和强化学习基线在一个简单但具有表达能力的图灵完整编程语言BF上进行比较。
我们提出了图小波神经网络（GWN），一种新型的图卷积神经网络（CNN），利用图小波变换来解决以前依赖图傅里叶变换的谱图CNN方法的缺点。与图傅里叶变换不同，图小波变换可以通过快速算法获得，不需要高计算成本的矩阵eigendecomposition。 此外，图小波是稀疏的，并且在顶点域中定位，为图卷积提供了高效率和良好的可解释性。提出的GWNN在三个基准数据集上基于图的半监督分类任务中明显优于以前的谱图CNN。Cora、Citeseer和Pubmed。
学习率衰减（lrDecay）是一种训练现代神经网络的技术，它从一个大的学习率开始，然后多次衰减。 关于lrDecay如何工作的常见信念来自于（随机）梯度下降的优化分析：1）最初的大学习率会加速训练或帮助网络摆脱虚假的局部最小值；2）学习率的衰减有助于网络收敛到局部最小值并避免振荡。尽管这些常见信念很受欢迎，但实验表明，它们不足以解释lrDecay在训练深度、宽度和非凸的现代神经网络中的普遍有效性。 我们提供了另一种新的解释：最初的大学习率抑制了网络对噪声数据的记忆，而学习率的衰减则提高了对复杂模式的学习。它的含义是，在lrDecay的后期阶段学到的额外模式更复杂，因此可转移性更差，这在现实世界的数据集中是合理的。
我们展示了如何利用Q^*$函数的集合在深度强化学习中进行更有效的探索。我们建立在匪徒设置的成熟算法上，并使其适应Q$学习设置。我们提出了一个基于置信度上限（UCB）的探索策略。
我们专注于学习一个单一的运动模块的问题，该模块可以灵活地表达一系列的行为，用于控制高维物理模拟的人形物体。为此，我们提出了一个运动架构，该架构具有一个具有潜在变量瓶颈的逆模型的一般结构。 我们表明有可能完全离线训练这个模型，以压缩数以千计的专家策略，并学习一个运动基元嵌入空间。训练好的神经概率运动基元系统可以进行全身人形行为的一次性模仿，稳健地模仿未见过的运动轨迹。 此外，我们证明，训练控制器重复使用所学的运动基元空间来解决任务也是很简单的，而且所产生的运动是相对自然的。为了支持我们模型的训练，我们比较了两种离线策略克隆的方法，包括一种有效的经验方法，我们称之为线性反馈策略克隆。我们鼓励读者观看总结我们结果的补充视频（https://youtu.be/CaDEf-QcKwA）。
数据增强是一种有用的技术，可以扩大训练集的规模，并在训练数据匮乏时防止不同机器学习任务的过度拟合。然而，目前的数据增强技术在很大程度上依赖于人类的设计和领域知识，而现有的自动方法还没有完全利用训练数据集的潜在特征。 在本文中，我们提出了{平行自适应GAN数据增强}(PAGANDA)，其中训练集自适应地丰富了由平行训练的生成对抗网络（GAN）自动构建的样本图像。 我们通过实验证明，我们的数据增强策略，几乎没有特定的模型考虑，可以很容易地适应跨领域的深度学习/机器学习任务，如图像分类和图像内画，同时在这两个任务中显著提高模型性能。我们的源代码和实验细节可在 url{https://github.com/miaojiang1987/k-folder-data-augmentation-gan/}。
具有数百万个参数的深度神经网络可能会因为过度拟合而导致泛化效果不佳。为了缓解这个问题，我们提出了一种新的正则化方法，对类似样本之间的预测分布进行惩罚。特别是，我们在训练期间对同一标签的不同样本和同一来源的增强样本之间的预测分布进行提炼。换句话说，我们对单个网络的暗知识（即关于错误预测的知识）进行正则化，即一种自我知识提炼技术，以迫使它输出更有意义的预测。 我们通过对各种图像分类任务的实验证明了所提方法的有效性：它不仅提高了现代神经网络的泛化能力，还提高了其校准精度。
在本文中，我们考虑了词级语言建模的具体问题，并研究了对基于LSTM的模型进行正则化和优化的策略。我们提出了权重下降的LSTM，它在隐藏到隐藏的权重上使用DropConnect，作为一种循环正则化的形式。 此外，我们引入了NT-ASGD，这是平均随机梯度法（ASGD）的非单调触发（NT）变体，其中平均触发是通过NT条件确定的，而不是由用户调整的。 在探索神经缓存与我们提出的模型的有效性时，我们在Penn Treebank和WikiText-2上分别实现了52.8和52.0的更低的最先进的plexity。我们还在准循环神经网络（QRNN）的背景下探索了拟议的正则化和优化策略的可行性，并展示了与AWD-LSTM对应的性能。
为了更好地理解神经网络的工作原理，人们开发了各种测量单元选择性的方法。 但不同的测量方法提供了不同的选择性估计，这导致了关于选择性对象表征被学习的条件和这些表征的功能相关性的不同结论。为了更好地描述对象的选择性，我们对AlexNet中大量单元的各种选择性测量进行了比较，包括局部主义选择性、精确性、类条件平均活动选择性（CCMAS）、网络解剖、人类对激活最大化（AM）图像的解释以及标准信号检测措施。 我们发现，不同的测量方法提供了对物体选择性的不同估计，其中精度和CCMAS测量方法提供了误导性的高估计。事实上，最具选择性的单元在物体分类中具有较差的命中率或较高的误报率（或两者），使它们成为差的物体检测者。 为了推广这些结果，我们比较了在ImageNet或Places-365数据集上训练的VGG-16和GoogLeNet中的一些单元的选择性措施，这些单元被描述为 "物体检测器"，同样，我们发现物体分类的命中率很低，误报率很高。
DNA分子的折叠结构与辅助分子相结合，也被称为染色质，与DNA的功能特性高度相关。染色质结构在很大程度上是由基本的主DNA序列决定的，尽管这种相互作用还没有被完全理解。 在本文中，我们开发了一个卷积神经网络，以初级DNA序列的图像代表作为其输入，并预测染色质结构的关键决定因素。我们的实验表明，该方法在预测准确性和训练时间方面都优于现有的几种方法。
本文介绍了一种新的方法来进行跨领域和跨任务的转移学习，将其表述为学习聚类的问题。关键的见解是，除了特征，我们还可以转移相似性信息，这足以学习一个相似性函数和聚类网络来进行领域适应和跨任务转移学习。我们首先将分类信息减少到成对约束，只考虑两个实例是否属于同一类别（成对语义相似性）。 这种相似性是与类别无关的，可以通过使用相似性网络从源域的数据中学习到。第二，对于跨任务学习（即对未见过的类别进行无监督聚类），我们提出了一个框架，以重建和估计语义聚类的数量，同样使用聚类网络。由于相似性网络是有噪声的，关键是要使用一个稳健的聚类算法，我们表明我们的表述比其他有约束和无约束的聚类方法更稳健。 使用这种方法，我们首先展示了对具有挑战性的跨任务问题的最先进的结果，应用在Omniglot和ImageNet上。我们的结果显示，我们可以以高精确度重建语义集群。然后，我们使用来自Office-31和SVHN-MNIST任务的图像评估了跨域转移的性能，并在这两个数据集上展示了最高的精确度。 我们的方法没有明确地处理领域差异。如果我们与领域适应性损失相结合，它显示出进一步的改进。
最近在跨语言词嵌入方面的进展主要依赖于基于映射的方法，这些方法通过线性转换将不同语言的预训练词嵌入投射到一个共享空间。这促使我们研究能够克服这一障碍的联合学习方法，通过训练目标中的跨语言术语同时学习不同语言的嵌入。考虑到大量可用的平行数据（Tiedemann，2012），我们提出了CBOW方法的双语扩展，利用句子对齐的语料来获得强大的跨语言单词和句子表示。 我们的方法大大改善了跨语言句子的检索性能，并令人信服地超过了映射方法，同时与联合训练的方法在单词翻译上保持了同等地位。它还在零次跨语言文档分类任务上实现了与深度RNN方法的同等地位，训练和推理所需的计算资源少得多。 我们公开了我们的代码和模型。
为了在复杂的环境中行动和计划，我们认为代理人应该有一个具有三个特征的世界心理模拟器：(a)它应该建立一个代表世界状况的抽象状态；(b)它应该形成一个代表世界不确定性的信念；(c)它应该超越简单的逐步模拟，并表现出时间上的抽象性。 由于缺乏满足所有这些要求的模型，我们提出了TD-VAE，这是一个生成序列模型，它学习包含关于未来几步状态的明确信念的表征，并且可以直接推出，不需要单步过渡。
本文介绍了一个用于无监督学习的信息论联合训练目标。我们考虑的是预测未来的问题。我们不是预测未来的感觉（图像像素或声波），而是预测将被未来感觉证实的 "假设"。更正式地说，我们假设一个关于$(x,y)$对的群体分布，我们可以把$x$看作是过去的感觉，而$y$是未来的感觉。我们训练一个预测模型$P_Phi(z|x)$和一个确认模型$P_Psi(z|y)$，我们将$z$视为假设（当预测时）或事实（当确认时）。对于成对的$(x,y)$的群体分布，我们着重于测量$x$和$y$之间的相互信息问题。根据数据处理的不等式，在确认模型$P_Psi(z|y)$定义的三联体$(x,z,y)$分布下，该互信息至少与$x$和$z$之间的互信息一样大。 对于$P_\Phi(z|x)$和$P_Psi(z|y)$的信息论训练目标可以看作是一种共同训练的形式，我们希望来自$x$的预测与来自$y$的确认相匹配。我们给出了在TIMIT数据集上学习语音的应用实验。
歌声的生成模型主要关注 "歌声合成 "的任务，也就是说。在这项工作中，我们探索了一个新颖但具有挑战性的替代方案：在训练和推理时间内，在没有预先分配的分数和歌词的情况下生成歌声。特别是，我们试验了三种不同的方案：1）自由歌手，模型在没有任何条件下生成歌声；2）伴奏歌手，模型在器乐的波形上生成歌声；以及3）独唱歌手，模型首先即兴创作一个和弦序列，然后用它来生成歌声。 我们概述了相关的挑战，并提出了一个处理这些新任务的管道。这涉及到开发用于数据准备的源分离和转录模型，用于音频生成的对抗网络，以及用于评估的定制指标。
近年来，自然语言处理（NLP）研究的碳足迹一直在增加，因为它依赖于大型和低效的神经网络实现。蒸馏是一种网络压缩技术，它试图将知识从大型模型传授给较小的模型。我们使用师生蒸馏来提高Biaffine依赖性分析器的效率，它在准确性和分析速度方面获得最先进的性能（Dozat & Manning, 2016）。 当蒸馏到原始模型可训练参数的20%时，我们只观察到UAS和LAS在一些不同的通用依赖树库中平均下降了1点，而在推理时比CPU（GPU）上的基线模型快2.26倍（1.21倍）。我们还观察到在某些树库中压缩到80%时性能有小幅提高。 最后，通过提炼，我们获得了一个解析器，它不仅比宾夕法尼亚树库上最快的现代解析器更快，而且更准确。
虽然自动编码器是连续结构（如图像或波形）表示学习的关键技术，但为离散结构（如文本序列或离散图像）开发通用的自动编码器已被证明是更具挑战性的。 ARAE联合训练一个丰富的离散空间编码器，如RNN，和一个更简单的连续空间生成器函数，同时使用生成式对抗网络（GAN）训练来约束分布的相似性。这个方法产生一个更平滑的收缩代码空间，将相似的输入映射到附近的代码，同时也产生一个隐含的潜在变量GAN模型。 在文本和离散图像上的实验表明，GAN模型产生了干净的插值，并捕捉到了原始空间的多模态，自动编码器在半监督学习中产生了改进，并在仅使用共享连续空间表示的未对齐文本风格转移任务中产生了最先进的结果。
当数据来自于多个潜在的子群体时，机器学习框架通常为每个子群体独立估计参数值。
在这项工作中，我们首先对三种RNN单元的记忆进行了数学分析，即简单的递归神经网络（SRN）、长短期记忆（LSTM）和门控递归单元（GRU），其定义为将序列中的一个元素映射到当前输出的函数。 接下来，我们提出了一个对以前的错误预测具有鲁棒性的多任务RNN模型，称为依赖性双向递归神经网络（DBRNN），用于序列中的序列输出（SISO）问题。最后，通过实验结果证明了DBRNN模型与ELSTM单元的性能。
许多最近训练的神经网络采用大量的参数来实现良好的性能。人们可以直观地使用所需的参数数量作为一个问题的难度的粗略衡量标准，但这种概念的准确性如何？ 我们慢慢增加这个子空间的维度，注意在哪个维度上首先出现解决方案，并将其定义为目标景观的内在维度。许多问题的内在维度比我们猜想的要小，而且对于一个给定的数据集，其内在维度在具有巨大差异的模型家族中变化不大。 这后一个结果具有深刻的含义，即一旦参数空间大到足以解决一个问题，额外的参数就会直接增加解决方案流形的维度。内在维度允许对问题的难度进行一些定量比较，包括监督、强化和其他类型的学习，例如，我们得出结论，解决倒立钟摆问题比从MNIST分类数字容易100倍，从像素玩Atari Pong和分类CIFAR-10一样难。 除了提供参数化模型所游荡的客观景观的新制图，该方法是一种简单的技术，可以建设性地获得解决方案的最小描述长度的上界。这种构造的副产品是一种压缩网络的简单方法，在某些情况下可以压缩100倍以上。
图谱神经网络作为图谱信号处理和深度卷积网络的组合，在非欧几里得领域的模式识别中显示出巨大的力量。在本文中，我们提出了一种新的方法，在图的二重性基础上部署两个管道，以提高准确性。通过探索原始图和它的二重图，其中节点和边缘可以被视为彼此，我们已经利用了顶点特征和边缘特征的好处。因此，我们已经到达了一个框架，在半监督和无监督学习中都有很大潜力。
我们描述了两个端到端的自动编码模型，用于基于半监督的图的依赖性解析。第一个模型是本地自动编码解析器（LAP），以连续的潜变量方式对输入进行编码；第二个模型是全局自动编码解析器（GAP），将输入编码为依赖树的潜变量，并进行精确推理。 这两个模型都由两部分组成：一个由深度神经网络（DNN）增强的编码器，可以利用上下文信息将输入编码为潜变量；一个解码器，是一个能够重建输入的生成模型。LAP和GAP都承认一个统一的结构，对有标记和无标记的数据有不同的损失函数，并有共享参数。
我们用快速权重记忆改进了以前的端到端可微分神经网络（NN）。一个门机制通过网络的慢速部分产生的两个独立的基于外积的矩阵，在序列的每个时间步骤上更新快速权重。该系统在关联检索问题的复杂序列到序列的变化上进行了训练，时间记忆（即时变变量）大约是70倍。 在准确性和参数数量方面，我们的架构优于各种RNN，包括长短时记忆、超网络和相关的快速权重架构。
深度学习领域一直在渴望一种优化方法，这种方法在优化和泛化方面都表现出突出的特性。 我们提出了一种基于测地线流动的数学优化方法，即两点之间的最短路径，相对于非线性函数诱导的黎曼尼公制而言。在我们的方法中，流动指的是指数衰减流动（EDF），因为它们可以被设计为以指数方式收敛于局部解决方案。本文进行了实验，以显示其在优化基准上的高性能（即收敛特性），以及其产生良好机器学习基准的潜力（即泛化特性）。
我们介绍了量子图神经网络（QGNN），这是一类新的量子神经网络分析方法，专门用来表示具有图结构的量子过程，并且特别适合在量子网络上的分布式量子系统上执行。 我们提供了四个QGNN的应用实例：学习量子系统的哈密尔顿动力学，学习如何在量子网络中创建多比特纠缠，光谱聚类的无监督学习，以及图形同构性分类的监督学习。
数据泄露涉及信息被未授权方访问。我们的研究涉及用户对数据泄露的看法，特别是与责任有关的问题。一项初步研究表明，许多人对这些问题的理解很薄弱，并认为他们自己有某种程度的责任。我们推测，这种印象可能源于组织的沟通策略。因此，我们将组织的文本与外部来源（如新闻媒体）进行了比较。这表明，组织使用众所周知的危机沟通方法来减少其声誉损失，并且这些策略与故事中涉及的叙述元素的重新定位相一致。 然后，我们进行了一项定量研究，要求参与者对组织文本或关于漏洞的新闻文本进行评分。这项研究的结果与我们的文件分析相一致，表明组织沟通会影响用户对受害的看法，在数据保护方面的态度，以及问责制。我们的研究提出了一些软件设计和法律意义，支持用户保护自己和发展更好的安全漏洞的心理模型。
目标识别是指在给定一组目标假设、领域模型和正在执行的计划的（可能是嘈杂的）样本的情况下，推断出代理人执行计划的正确目标的问题。  这是合作性和竞争性代理互动中的一个关键问题，最近的方法已经产生了快速和准确的目标识别算法。  在本文中，我们利用运算符计数启发法的进展，使用从经典计划问题中得到的约束条件的线性程序来解决目标识别问题。  我们的方法使用从观察中得到的额外的操作者计数约束来有效地推断出正确的目标，并作为一些具有额外约束的进一步方法的基础。
为了帮助解决这个问题，我们建议使用知识蒸馏法，其中单任务模型教导多任务模型。我们用教师退火法加强这种训练，这是一种新颖的方法，逐渐将模型从蒸馏法过渡到监督学习，帮助多任务模型超过其单任务教师。我们通过在GLUE基准上进行多任务微调BERT评估我们的方法。
对用于图像分类的失配样本的检测已被广泛研究。安全关键应用，如自动驾驶，将受益于定位导致图像失配的异常物体的能力。 它进一步在两个新的数据集上实验比较了适应的方法，这些数据集来自于使用PSPNet和DeeplabV3+架构的现有语义分割数据集，并为该任务提出了一个新的指标。评估显示，比较方法的性能排名并没有转移到新的任务上，每个方法的性能都明显比它们的图像级对应方法差。
我们提出了一种将神经表征从标签丰富的源域转移到无标签的目标域的域适应方法。最近为这项任务提出的对抗性方法通过 "愚弄 "一个特殊的域分类器网络来学习跨域的特征。然而，这种方法的一个缺点是，域分类器只是将生成的特征标记为域内或域外，而不考虑类之间的边界。这意味着模糊的目标特征可能在类边界附近生成，降低目标分类精度。 我们提出了一种新的方法，即对抗性辍学正则化（ADR），它鼓励生成器为目标领域输出更多的鉴别性特征。我们的关键想法是用一个批评家来取代传统的领域批评家，该批评家通过使用分类器网络上的辍学来检测非鉴别性特征，然后生成器学会避免这些特征空间的区域，从而创建更好的特征。
将深度学习用于广泛的数据问题，增加了对理解和诊断这些模型的需求，深度学习解释技术已成为数据分析师的基本工具。虽然近年来提出了许多模型解释方法，但这些程序大多基于启发式方法，几乎没有理论保证。在这项工作中，我们提出了一个用于黑盒计算机视觉模型的突出性估计的统计框架。我们建立了一个模型诊断估计程序，在统计上是一致的，并通过Adebayo等人的突出性检查。(我们的方法需要解决一个线性程序，其解决方案可以在多项式时间内有效计算。通过我们的理论分析，我们建立了一个高概率恢复重要区域所需的模型评估数量的上限，并建立了一个新的扰动方案来估计局部梯度，这被证明比常用的随机扰动方案更有效。
我们探讨了组合式集合嵌入的想法，它不仅可以用来推断单一的类，而且可以推断与输入数据（如图像、视频、音频信号）相关的类集合。例如，这在图像中的多物体检测或音频中的多说话者日记（单次学习）中很有用。 特别是，我们设计并实现了两个新的模型，包括(1)一个与 "复合 "函数g共同训练的嵌入函数f，该函数计算两个嵌入向量中编码的类之间的集合联合操作；以及(2)与 "查询 "函数h共同训练的嵌入f，该函数计算一个嵌入中编码的类是否包含另一个嵌入中编码的类。 与之前的工作相反，这些模型必须既能感知与输入实例相关的类，又能编码不同类标签集之间的关系。在模拟数据、OmniGlot和COCO数据集上进行的实验中，所提出的复合嵌入模型优于基于传统嵌入方法的基线。
在这项工作中，我们提出了一个目标驱动的协作任务，其中包含语言、视觉和虚拟环境中的行动作为其核心组成部分。具体来说，我们开发了一个两个代理之间的协作图像绘制游戏，称为CoDraw。我们的游戏基于一个包含可移动剪贴画对象的虚拟世界。 游戏涉及到两个玩家：一个泄密者和一个绘图者。泄密者看到一个抽象的场景，其中包含多个剪贴画作品的语义配置，而绘图者则试图用可用的剪贴画作品在空画布上重建这个场景。两个玩家通过使用自然语言进行双向交流。 我们收集了CoDraw数据集，其中包括人类代理之间交换的约13.8万条信息。我们定义了协议和指标，以评估在这个测试平台上学习的代理的有效性，强调需要一个新的 "串扰 "条件，将在训练数据的不相干子集上独立训练的代理配对进行评估。 我们提出了用于我们任务的模型，包括简单但有效的基线和使用模仿学习和目标驱动训练相结合的神经网络方法。
偏见和混杂效应的存在无疑是机器学习应用中最关键的挑战之一，在最近几年里已经引起了关键的争论。这种挑战包括从医学研究中混杂变量的虚假关联到性别或人脸识别系统中的种族偏见。 一种解决方案是加强数据集并组织它们，使其不反映偏见，这是一项繁琐而密集的任务。另一种方法是利用现有数据并建立考虑这些偏见的模型。 然而，这些技术一般不适用于端到端的深度学习方法。在本文中，我们提出了一种基于对抗性训练策略的方法，以学习无偏见和不受混杂变量影响的鉴别性特征。这是通过纳入一个新的对抗性损失函数，鼓励偏见和学习的特征之间的相关性消失而实现的。 我们将我们的方法应用于合成、医疗诊断和性别分类（Gender Shades）数据集。我们的结果表明，通过我们的方法学到的特征不仅能带来卓越的预测性能，而且与偏见或干扰变量不相关。代码可在http://blinded_for_review/。
现有的神经问题回答（QA）模型需要对大多数大规模QA数据集进行推理，并从长的上下文中得出复杂的推论。然而，如果我们把QA看作是一个综合的检索和推理任务，我们可以假设存在一个最小的上下文，它对回答一个给定的问题是必要和充分的。 最近的工作表明，一个选择较短上下文的句子选择器模块，将其反馈给下游的QA模型，实现了与在完整上下文上训练的QA模型相媲美的性能，同时也更容易解释。 虽然人类对这些分散注意力的句子有免疫力，但QA模型很容易被误导，从这些句子中选择答案。我们假设句子选择器模块可以过滤掉不相干的上下文，从而使下游的QA模型能够关注和推理与问题相关的上下文部分。 在本文中，我们表明句子选择器本身很容易受到对抗性输入的影响。然而，我们证明了由句子选择器模块和QA模型组成的管道与根据完整上下文训练的QA模型相比，可以对对抗性攻击更加稳健。因此，我们提供了证据，证明了问题回答的模块化方法更加稳健和可解释。
虽然知识库数据通常包含树状或循环结构，但现有的方法都不能将这些数据嵌入到与结构一致的兼容空间。 Riemannian TransE将每个关系建模为向一个点的移动，并为每个关系定义了特定的新的距离异同，因此所有的关系都被自然地嵌入到与数据结构的对应关系中。在几个知识库完成任务的实验表明，基于对流形的适当选择，Riemannian TransE即使在参数明显减少的情况下也能达到良好的性能。
神经网络中的灾难性遗忘是持续学习中最著名的问题之一。以前解决这个问题的尝试主要是防止重要的权重发生变化。这种方法往往需要任务边界来有效地学习，并且不支持向后转移学习。在本文中，我们提出了一种元学习算法，它学习重建旧任务的梯度w. 我们提出了一种元学习算法，该算法可以学习重建旧任务的梯度，并将这些重建的梯度与当前的梯度结合起来，以实现持续学习和从当前任务到先前任务的后向转移学习。在标准持续学习基准上的实验表明，我们的算法可以有效地防止灾难性遗忘，并支持后向转移学习。
 我们给出了一个正式的程序，用于计算卷积网络输出的预成像，该程序使用从与网络层相关的超平面集合中定义的双重基础。我们指出了与卷积网络的超平面排列相关的特殊对称性，这些超平面采取了规则的多维多面体圆锥体形式。 我们讨论了大量嵌套锥体层的效率，这些锥体是由递增的小尺寸卷积产生的，以便在有效收缩数据到低维和塑造预像流形之间做出良好的折衷。我们展示了一个特定的网络如何将非线性输入流形平铺到仿生输出流形，并讨论了它与理解深度网络的分类特性的关系。
元学习在快速模型适应方面取得了令人印象深刻的进展。然而，在学习贝叶斯建模的快速不确定性适应方面所做的工作有限。在本文中，我们建议通过将元学习放在概率度量的空间上，诱导快速不确定性适应的元采样的概念来实现这一目标。 具体来说，我们提出了一个由两个主要部分组成的贝叶斯元抽样框架：元抽样器和样本适配器。元抽样器是通过采用神经-反自回归流（NIAF）结构，即最近提出的神经自回归流的变体，来有效地产生要适应的元样本。 样本适配器根据新提出的通用贝叶斯抽样技术（称为最优传输贝叶斯抽样），将元样本移动到特定任务的样本中。这两个部分的结合允许为元采样器开发一个简单的学习程序，它可以通过标准的反向传播进行有效优化。
我们提出了一个用于端到端优化图像压缩的上下文自适应熵模型。我们的模型利用了两种类型的上下文，即比特消耗型上下文和无比特型上下文，根据是否需要额外的比特分配进行区分。基于这些上下文，我们允许模型以一种更通用的近似模型形式更准确地估计每个潜在的表示的分布，这相应地导致了压缩性能的增强。 根据实验结果，所提出的方法在峰值信噪比（PSNR）和多尺度结构相似性（MS-SSIM）指数方面优于传统的图像编解码器，如BPG和JPEG2000，以及其他以前基于人工神经网络（ANN）的方法。测试代码可在https://github.com/JooyoungLeeETRI/CA_Entropy_Model 公开。
深度网络在其训练的数据分布上往往表现良好，但在对训练分布以外的点进行评估时，却给出了不正确的（而且往往是非常自信的）答案。这一点以对抗性例子现象为例，但也可以在模型泛化和领域转移方面看到。 理想情况下，一个模型会对不同于训练分布的点赋予较低的信心。 我们提出了一个正则器，通过用插值的隐藏状态进行训练来解决这个问题，并鼓励分类器对这些点的信心降低。 由于隐藏状态是学习出来的，这就产生了一个重要的效果，即鼓励一个类别的隐藏状态以这样的方式集中起来，使同一类别内或两个不同类别之间的插值不会与其他类别的真实数据点相交。 这有一个很大的好处，那就是它避免了在输入空间内插值可能导致的欠拟合。 我们证明，通过Manifold Mixup避免这种欠拟合问题的确切条件是，隐藏状态的维度超过了类的数量，这在实践中经常发生。 此外，这种集中可以被看作是使早期层的特征更有辨别力。 我们表明，尽管不需要大量的额外计算，但在监督学习、对单步对抗性攻击的鲁棒性、半监督学习以及对保留样本的负对数可能性方面，Mifold Mixup比强大的基线取得了很大的改进。
有时SRS（立体定向无线电手术）需要在感兴趣的区域（ROI）上使用球体包装，如癌症，以确定治疗方案。 我们已经开发了一种球体包装算法，将不相交的球体包装在ROI内。 在我们的案例中，感兴趣的区域是那些被确定为癌症组织的体素。 Epsilon-Rotation invariant是指任意旋转三维ROI的能力，同时保持体积属性在一定的epsilon限度内保持（几乎）不变。所应用的旋转产生的球形包装仍然是高度相关的，因为我们分析了ROI的体积数据旋转前后球形包装的几何特性。 我们的新型球体包装算法在+/-ε的范围内具有高度的旋转不变性。我们的方法使用从基于距离的球体包装算法的不相交球体集的值中得到的形状描述符，从ROI中提取不变的描述符。我们通过使用Slicer3D平台实现这些想法来证明我们的研究。 这些数据是基于核磁共振立体定向图像。我们在Slicer3D平台上对30多个病人的不同基准数据提出了几个性能结果。
我们展示了卷积神经网络（CNN）中隐含的滤波器水平的稀疏性表现，这些网络采用了批量归一化和ReLU激活，并使用具有L2正则化或权重衰减的自适应梯度下降技术进行训练。通过广泛的经验研究（匿名，2019年），我们假设了稀疏化过程背后的机制。我们发现，各种现象的相互作用影响了L2和权重衰减正则器的强度，导致所谓的非稀疏性诱导正则器引起滤波器的稀疏性。 在这篇研讨会文章中，我们总结了我们的一些关键发现和实验，并介绍了在现代网络架构（如ResNet-50）上的额外结果。
人类可以在一生中逐步学习各种概念和技能，同时表现出一系列理想的特性，如不遗忘、概念排练、知识的前向转移和后向转移、少量学习和选择性遗忘。以前的终身机器学习方法只能表现出这些特性的子集，通常是通过结合多种复杂的机制。 在这个观点中，我们提出了一个强大的统一框架，可以通过利用深度神经网络中少量的权重巩固参数来展示所有的特性。此外，我们能够在我们提出的框架的行为和机制与围绕人类学习的行为和机制之间得出许多相似之处，如记忆丧失或睡眠剥夺。这个观点作为一个双向的灵感渠道，以进一步了解机器和人类的终身学习。
有限角度CT重建是一个欠确定的线性逆向问题，需要适当的正则化技术来解决。在这项工作中，我们研究了如何通过有效地投射到推断的图像流形上，用预先训练好的生成对抗网络（GANs）来清除传统技术中的噪声、高假象含量的重建。 特别是，我们使用了流行的GAN先验的稳健版本来处理逆向问题，它基于一种叫做腐败模仿的最新技术，显著提高了重建质量。所提出的方法直接在图像空间中操作，因此它不需要训练或需要访问测量模型，与扫描仪无关，并且可以在广泛的传感场景中工作。
我们提出了一个有效的多任务学习设置，通过利用句子层面的监督来减少远处的监督噪音。我们展示了句子层面的监督如何被用来改善单个句子的编码，并学习哪些输入句子更有可能表达一对实体之间的关系。我们还引入了一个新的神经架构来收集来自多个输入句子的信号，它结合了注意力和maxpooling的好处。所提出的方法将AUC提高了10%（从0.261到0.284），并超越了最近在FB-NYT数据集中公布的结果。
神经网络模型中的注意力层提供了对模型预测背后推理的洞察力，这通常被批评为不透明。最近，关于注意力权重的可解释性出现了看似矛盾的观点（Jain & Wallace, 2019; Vig & Belinkov, 2019）。通过在不同的NLP任务上进行的一系列实验，我们验证了我们的观察结果，并通过人工评估加强了我们对注意力可解释性的主张。
源代码的生成模型是一个有趣的结构化预测问题，需要对硬的句法和语义约束以及自然的、可能的程序进行推理。我们为这个问题提出了一个新的模型，用图来表示生成输出的中间状态。我们的模型通过将语法驱动的扩展步骤与图增强和神经信息传递步骤交错生成代码。
模型的推断、未来符号的预测以及离散时间、离散事件过程的熵率估计是老生常谈的话题。然而，许多时间序列最好被概念化为连续时间、离散事件过程。 这些方法依赖于贝叶斯结构推理的扩展，利用了神经网络的普遍近似能力。根据简单合成数据的实验，只要推断出正确的模型，这些新方法似乎可以与最先进的预测和熵率估计方法竞争。
最近的研究表明，卷积神经网络（CNN）在各种环境下都很脆弱，包括对抗性例子、后门攻击和分布转移。 人类视觉系统更关注全局结构（例如：形状）的识别，而卷积神经网络则更关注全局结构。形状）进行识别，而CNN则偏重于图像中的局部纹理特征，因此我们提出了一个基于鲁棒边缘特征的统一框架EdgeGANRob，以提高CNN的总体鲁棒性，它首先明确地从给定的图像中提取形状/结构特征，然后通过用训练好的生成式对抗网络（GAN）重新填充纹理信息来重建一个新的图像。 此外，为了降低边缘检测算法对对抗性扰动的敏感性，我们提出了一种基于vanilla Canny算法的鲁棒性边缘检测方法Robust Canny。 为了获得更多的见解，我们还将EdgeGANRob与其简化的骨干程序EdgeNetRob进行了比较，后者直接在提取的鲁棒性边缘特征上执行学习任务。我们发现，EdgeNetRob可以帮助大大提升模型的鲁棒性，但代价是清洁模型的准确性。另一方面，与EdgeNetRob相比，EdgeGANRob能够提高清洁模型的准确性，而且不会失去EdgeNetRob引入的鲁棒性优势。大量的实验表明，EdgeGANRob在不同的设置下，对不同的学习任务具有一定的弹性。
从数据中学习丰富的表征是深度生成模型的一项重要任务，如变异自动编码器（VAE）。然而，通过在自下而上的推理过程中提取高层次的抽象，为自上而下的生成保留所有变化因素的目标受到了影响。在 "从小开始 "的概念激励下，我们提出了一种策略，从高到低的抽象层次逐步学习独立的层次表征。 我们在两个基准数据集上使用三个拆分指标，包括我们提出的补充以前提出的互信息差距指标的新指标，定量地证明了所提出的模型与现有工作相比改善拆分的能力。 我们进一步提出了定性和定量的证据，说明学习的渐进性是如何改善分层表征的解缠的。通过借鉴分层表征学习和渐进学习的各自优势，据我们所知，这是第一次尝试通过逐步提高VAE学习分层表征的能力来改善解缠。
在本文中，我们采用了深度信息瓶颈模型，确定了它的缺点，并提出了一个规避这些缺点的模型。为此，我们应用了一个共轭变换，通过恢复信息瓶颈方法的不变性，导致潜空间中特征的解缠。在此基础上，我们展示了这种变换如何转化为新模型中潜空间的稀疏性。 我们在人工和真实数据上评估了我们的方法。
最先进的机器学习方法表现出有限的复合泛化能力。同时，缺乏全面衡量这种能力的现实基准，这使得寻找和评估改进具有挑战性。我们介绍了一种新的方法，通过最大化复合发散，同时保证训练集和测试集之间的小原子发散，系统地构建这种基准，我们将这种方法与其他创建复合泛化基准的方法进行定量比较。 我们提出了一个大型的、现实的自然语言问题回答数据集，该数据集是根据该方法构建的，我们用它来分析三种机器学习架构的组成泛化能力。我们发现它们不能进行组成泛化，并且在复合发散和准确性之间存在令人惊讶的强负相关。我们还展示了我们的方法如何被用来在现有的SCAN数据集之上创建新的组成泛化基准，这证实了这些发现。
为了了解物体视觉在婴儿期和儿童期是如何发展的，有必要开发可测试的计算模型。深度神经网络（DNNs）已被证明作为成人视觉的模型很有价值，但目前还不清楚它们是否有作为发展模型的价值。作为第一个模型，我们测量了旨在模仿视觉系统的结构和代表几何的DNN（CORnet）的学习。 我们通过冻结卷积层和训练一个额外的线性解码层来量化该网络每个层次的明确物体表征的发展。我们在整个ImageNet验证集上评估解码的准确性，也对个别视觉类别进行评估。 然而，CORnet使用监督训练，由于婴儿只能获得极其贫乏的标签，他们必须以无监督的方式学习。因此，我们也测量了最先进的无监督网络（DeepCluster）的学习情况。 我们对婴儿各脑区的学习应该如何发展进行了预测。在所有三个网络中，我们还测试了婴儿和机器获得视觉类的顺序关系，只发现了反直觉关系的证据。我们讨论了其中的潜在原因。
我们表明，在各种大规模的深度学习场景中，梯度在短时间的训练后会动态地收敛到一个非常小的子空间。这个子空间被Hessian的几个顶级特征向量（相当于数据集中的类的数量）所覆盖，并且在长时间的训练中大部分被保留下来。一个简单的论证表明，梯度下降可能主要发生在这个子空间。
回答需要在网络规模上进行多跳推理的问题需要检索多个证据文件，其中一个往往与问题没有什么词汇或语义关系。本文介绍了一种新的基于图的递归检索方法，该方法学习检索维基百科图上的推理路径以回答多跳开放域问题。我们的读者模型对推理路径进行排序，并提取最佳推理路径中包含的答案跨度。实验结果显示，在三个开放领域的QA数据集中取得了最先进的结果，展示了我们方法的有效性和稳健性。值得注意的是，我们的方法在HotpotQA中取得了明显的改善，比之前的最佳模型超出了14分以上。
立体匹配是计算机视觉领域重要的基础任务之一。近年来，基于深度学习的立体匹配算法取得了优异的成绩，成为主流研究方向。现有的算法一般使用深度卷积神经网络（DCNN）来提取更抽象的语义信息，但我们认为空间结构的细节信息对立体匹配任务更为重要。 基于这一观点，本文提出了一个具有大感受野的浅层特征提取网络。该网络由三部分组成：一个初级特征提取模块，一个无腹肌空间金字塔集合（ASPP）模块和一个特征融合模块。初级特征提取网络只包含三个卷积层。 该网络利用浅层网络的基本特征提取能力来提取和保留空间结构的详细信息。本文引入了扩张卷积和阿特拉斯空间金字塔池（ASPP）模块来增加感受野的大小。此外，还设计了一个特征融合模块，该模块将特征图与多尺度感受野结合起来，使不同尺度的特征信息相互补充。 我们用浅层特征提取网络取代了现有立体匹配算法的特征提取部分，并在KITTI 2015数据集上取得了最先进的性能。与参考网络相比，参数数量减少了42%，匹配精度提高了1.9%。
我们为深度神经网络提出了一个新的输出层，它允许使用记录的上下文匪徒反馈进行训练。为此，我们提出了一种反事实风险最小化（Counterfactual Risk Minimization，CRM）的方法，使用带有方差正则化的等变经验风险估计器BanditNet来训练深度网络，并展示了如何以一种允许随机梯度下降（SGD）训练的方式分解所产生的目标。 我们通过展示深度网络--特别是ResNets--如何在没有传统标签图像的情况下进行物体识别训练，来实证证明该方法的有效性。
蛋白质分类是对生物序列负责，我们提出了一个想法，用深度学习算法来处理蛋白质组学的分类问题。这个算法主要是对蛋白质向量的序列进行分类，而蛋白质向量是用来表示蛋白质组学的。 这里使用的蛋白质表示法是n-gram，即3-gram和Kerasembedding，用于蛋白质等生物序列。
在这项工作中，我们试图回答一个关键问题：是否存在一些输入序列会导致训练有素的离散空间神经网络序列到序列（seq2seq）模型产生恶劣的输出（攻击性、恶意的、攻击性等）。 我们采用了一种经验方法，首先创建了恶劣输出序列的列表，然后设计了一种离散的优化算法来寻找会导致模型产生这些输出的输入序列。此外，优化算法在大词汇量搜索方面得到了加强，并被限制为搜索可能被现实世界用户输入的输入序列。我们证明，鉴于我们的算法发现的触发输入，大量的恶意句子被模型赋予了很大的概率，这揭示了标准seq2seq训练的一个不理想的后果。
在无监督学习分解表征的问题上，有希望的方法之一是惩罚取样的潜在变量的总相关性。 不幸的是，由于采样的潜在表征和其相应的平均表征之间存在问题，这种动机良好的策略往往不能实现解缠。 我们提供了一个理论解释，即样本分布的低总相关不能保证平均代表的低总相关。我们证明，对于任意高总相关的平均代表，存在大量总相关的潜变量分布。 然而，我们仍然认为总相关可能是无监督代表学习的一个关键，我们提出了一个补救措施，RTC-VAE，它纠正了总相关的惩罚。  实验表明，与基线模型（如β-TCVAE和FactorVAE）相比，我们的模型具有更合理的平均代表分布。
在本文中，为了更好地将图像结构与生成的文本联系起来，我们用两种可供选择的促进稀疏性的转换来取代传统的softmax注意机制：sparsemax和总变异稀疏注意（TVmax）。 为了促进稀疏性并鼓励融合相关的相邻空间位置，我们提出了TVmax。 通过选择相关的特征组，TVmax转换提高了可解释性。我们在微软COCO和Flickr30k数据集上展示了结果，与softmax相比获得了收益。 TVmax在人类评价的标题质量和注意力的相关性方面优于其他比较的注意力机制。
虽然世界上有超过65000种语言，但许多音素的发音在不同的语言中听起来是相似的。当人们学习一门外语时，他们的发音往往反映了他们的母语特征。 在另一个实验中，我们用有限的英语数据集和大量的韩语数据集来训练网络，并分析了在资源丰富的语言的帮助下训练资源贫乏的语言所需的数据集数量。
生成对抗网络（GANs）最近在许多现实世界的应用中取得了令人印象深刻的结果，随着样本质量和训练稳定性的提高，出现了许多GAN变体。然而，对GANs的可视化和理解在很大程度上是缺失的。GAN如何在内部表现我们的视觉世界？是什么导致GAN结果中出现伪影？ 在这项工作中，我们提出了一个分析框架，在单元、对象和场景层面上对GAN进行可视化和理解。我们首先用一种基于分割的网络剖析方法来识别一组与对象概念密切相关的可解释单元。 然后，我们通过测量干预措施控制输出中的物体的能力来量化可解释单元的因果效应。最后，我们通过将发现的物体概念插入新的图像中来检查这些单元和它们周围的上下文关系。我们展示了由我们的框架促成的几个实际应用，从比较不同层、模型和数据集的内部表示，到通过定位和删除造成伪影的单元来改进GAN，再到互动地操纵场景中的物体。我们提供开源解释工具来帮助同行研究人员和从业人员更好地理解他们的GAN模型。
从历史上看，对高效推理的追求一直是新的深度学习架构和构建模块研究背后的驱动力之一。最近的一些例子包括：（Hu等人，2018）的挤压和激发模块，Xception（Chollet，2017）的深度可分离卷积，以及MobileNet v2（Sandler等人，2018）的倒瓶颈。 值得注意的是，在所有这些案例中，所产生的构建块不仅实现了更高的效率，而且实现了更高的精度，并在该领域得到了广泛的采用。在这项工作中，我们进一步扩大了神经网络架构的高效构建块的武器库；但我们并没有结合标准原语（如卷积），而是主张用稀疏的对应物取代这些密集的原语。 虽然使用稀疏性来减少参数数量的想法并不新鲜（Mozer & Smolensky, 1989），但传统的看法是，理论上FLOPs的减少并不能转化为现实世界的效率提升。 我们的目标是纠正这种误解，为几个硬件平台引入高效的稀疏内核系列，我们计划为社区的利益开放源代码。  在Snapdragon 835上，我们的稀疏网络比密集网络的性能高出1.1-2.2倍，相当于大约整整一代的改进。 我们希望，我们的发现将促进更广泛地采用稀疏性作为创建高效和准确的深度学习架构的工具。
在这项工作中，我们解决了图数据的半监督分类问题，其中那些未标记的节点的类别是由标记的节点以及图结构推断出来的。最近的工作通常以传统的监督方式用先进的图卷积来解决这个问题，但当标记的数据稀少时，性能可能会受到严重影响。这里我们提出一个图推理学习（GIL）框架，通过学习图拓扑上的节点标签推理来提高节点分类的性能。 为了连接两个节点，我们正式定义了一个结构关系，将节点属性、节点间路径和局部拓扑结构封装在一起，这样可以方便地从一个节点推断到另一个节点。 为了学习推理过程，我们进一步引入了从训练节点到验证节点的结构关系的元优化，从而使学习到的图推理能力可以更好地自我适应测试节点。在四个基准数据集（包括Cora、Citeseer、Pubmed和NELL）上的综合评估表明，在半监督节点分类任务中，我们的GIL与其他最先进的方法相比具有优势。
本文提出了一种有效训练连续状态马尔科夫决策过程（MDP）的Q函数的方法，这样得到的策略的轨迹满足线性时态逻辑（LTL）属性。LTL是一种模态逻辑，可以表达广泛的时间相关的逻辑属性，包括安全性和有效性。 然后通过强化学习算法合成控制策略，假设MDP中没有任何先验知识。在一项数值研究中对所提出的方法进行了评估，以测试生成的控制策略的质量，并与传统的策略合成方法进行比较，如MDP抽象（Voronoi量化器）和近似动态编程（拟合值迭代）。
我们介绍了两种方法，用于在包含嵌套随机子程序的随机模拟器中进行有效的贝叶斯推理，即。由此产生的一类模拟器在整个科学领域被广泛使用，并可被解释为概率生成模型。然而，由于无法评估甚至是其未归一化的密度，从它们中得出的推论构成了巨大的挑战，阻止了许多标准推论程序如马尔科夫链蒙特卡洛（MCMC）的使用。 为了解决这个问题，我们引入了基于两步法的推理算法，首先对各个子程序的条件密度进行近似，然后再使用这些近似值在整个程序上运行MCMC方法。由于子程序可以单独处理，并且比整个问题的维度低，这个两步法允许它们被隔离，从而被有效地处理，而不会对问题的整体维度造成限制。
虽然对抗性训练可以提高鲁棒性精度（针对对抗者），但有时会损害标准精度（当没有对抗者时）。以前的工作已经研究了标准和鲁棒性精度之间的这种权衡，但只是在没有预测器在无限数据限制下对两个目标表现良好的情况下。 在本文中，我们表明，即使在无限数据下的最佳预测器在两个目标上都表现良好，在有限数据下仍然可以表现出权衡。此外，由于我们的结构是基于凸学习问题，我们排除了优化问题，从而暴露了鲁棒性和通用性之间的基本矛盾。最后，我们表明，鲁棒性自我训练通过利用未标记的数据大部分消除了这种权衡。
跳过连接越来越多地被深度神经网络所利用，以提高准确性和成本效益。特别是，最近的DenseNet在计算和参数方面是高效的，并通过直接连接每个特征层和所有以前的特征层来实现最先进的预测。然而，DenseNet的极端连接模式可能会阻碍其向高深度的扩展，在像完全卷积网络的应用中，全DenseNet连接是非常昂贵的。这项工作首先通过实验表明，跳过连接的一个关键优势是在反向传播过程中，特征层之间的距离很短。具体来说，使用固定数量的跳过连接，层间反向传播距离较短的连接模式有更准确的预测结果。 根据这一观点，我们提出了一种连接模板--Log-DenseNet，与DenseNet相比，它只将层间的反向传播距离从1略微增加到（1+log_2 L$），但只使用$L/log_2 L$的总连接，而不是$O（L^2）$。 因此，\logdenses比DenseNets更容易扩展，而且不再需要仔细的GPU内存管理。我们通过展示在tabula rasa语义分割上比DenseNets更好的性能和在视觉识别上有竞争力的结果来证明我们设计原则的有效性。
构建与网络互动的代理将使知识理解和表征学习得到显著改善。然而，网络导航任务对于目前的深度强化学习（RL）模型来说是困难的，因为离散的行动空间很大，而且各状态之间的行动数量不一。在这项工作中，我们介绍了DOM-Q-NET，一个基于RL的网络导航的新颖架构，以解决这两个问题。它将Q函数参数化，为不同的行动类别提供独立网络：点击DOM元素和输入字符串输入。 我们的模型利用图形神经网络来表示标准网页的树状结构的HTML。 我们在MiniWoB环境中展示了我们模型的能力，在那里我们可以匹配或超越现有的工作，而不需要使用专家的示范。此外，我们展示了在多任务环境中训练时样本效率的2倍改善，使我们的模型能够在不同的任务中转移学习的行为。
学习与现实世界数据中的变化因素相对应的分离表征，对于可解释和人类可控制的机器学习至关重要。最近，人们对以纯粹的无监督方式学习分离表征的可行性的担忧，促使人们转向纳入弱监督。然而，目前还没有一种形式主义来确定弱监督何时以及如何保证分离。 为了解决这个问题，我们提供了一个理论框架--包括解除纠缠的微积分--来帮助分析弱监督在与基于分布匹配的学习算法结合时赋予的解除纠缠的保证（或缺乏这种保证）。
尽管人们对持续学习的兴趣日益浓厚，但其当代的大多数作品都是在一个相当有限的环境中研究的，在这个环境中，任务是可以明确区分的，而且在训练期间任务的边界是已知的。然而，如果我们的目标是开发一个像人类一样学习的算法，这种环境是远远不现实的，必须开发一个以无任务方式工作的方法。同时，在持续学习的几个分支中，基于扩展的方法具有通过分配新资源学习新数据来消除灾难性的遗忘的优势。 在这项工作中，我们提出了一种基于扩展的无任务持续学习方法。我们的模型被命名为连续神经狄里奇过程混合物（CN-DPM），由一组负责数据子集的神经网络专家组成。CN-DPM在贝叶斯非参数框架下以一种原则性的方式扩展了专家的数量。通过大量的实验，我们表明我们的模型成功地对图像分类和图像生成等鉴别性和生成性任务进行了无任务持续学习。
带有Nesterov动量的随机梯度下降（SGD）是深度学习中广泛使用的优化器，据观察，它具有出色的泛化性能。在这项工作中，我们提出了Amortized Nesterov's Momentum，这是Nesterov's Momentum的一个特殊变体，它具有更强的迭代能力，在早期阶段收敛更快，效率更高。我们的实验结果表明，这种新的势头在几乎没有调整的情况下实现了类似（有时更好）的泛化性能。
我们提出了一个最先进的生成模型，即 "因子化动作变异自动编码器（FAVAE）"，用于通过无监督的信息瓶颈从序列数据中学习分解和可解释的表征。分解表征学习的目的是从数据中获得可解释和可转移的表征。 我们专注于顺序数据的解缠表征，因为如果解缠表征扩展到顺序数据，如视频、语音和股票价格数据，会有广泛的潜在应用。顺序数据的特点是动态因素和静态因素：动态因素与时间有关，而静态因素与时间无关。 以前的工作通过明确建立潜变量的先验模型来区分静态和动态因素，成功地将静态因素和动态因素区分开来。然而，这种模型不能区分动态因素之间的表征，例如区分机器人任务中的 "摘 "和 "扔"。 在本文中，我们提出了可以分解多个动态因素的新模型。由于我们的方法不需要建模先验，它能够分解 "不同 "的动态因素。在实验中，我们表明FAVAE可以提取分解的动态因素。
生成网络是指定视觉转换的有前途的模型。不幸的是，生成模型的认证是具有挑战性的，因为我们需要捕捉足够的非凸性，以便对输出产生精确的约束。现有的验证方法要么不能扩展到生成网络，要么不能捕捉足够的非凸性。 在这项工作中，我们提出了一个新的验证器，称为AxitLine，它可以证明生成式网络的非琐碎属性。AxitLine同时执行确定性和概率性的抽象解释，并捕获生成式网络的无限输出集。我们表明，AxitLine可以验证网络潜在空间中的有趣插值。
多视角视频总结（MVS）由于其主要挑战是视角间的相关性和摄像机的重叠而缺乏研究人员的关注。大多数先前的MVS工作是离线的，只依赖于总结，需要额外的通信带宽和传输时间，没有关注不确定环境。 与现有的方法不同，我们提出了基于边缘智能的MVS和基于时空特征的物联网环境下的活动识别。我们使用轻量级的CNN物体检测模型将每个从属设备上的多视角视频分割成镜头，并计算它们之间的相互信息以生成摘要。 我们的系统并不只依赖摘要，而是将其编码并传输给带有神经计算棒（NCS）的主设备，以智能地计算视图间的相关性并有效地识别活动，从而节省计算资源、通信带宽和传输时间。实验报告显示，在MVS Office数据集上，F-measure得分增加了0.4，活动识别准确率增加了0. 实验报告显示，与UCF-50和YouTube 11数据集相比，活动识别精度分别提高了0.2%和2%，与最先进的技术相比，存储和传输时间更短。
研究灵长类动物视觉皮层和物体识别的分层模型的一个核心目标是了解单个单元如何以及为什么要交换对图像变换的不变性和敏感性。例如，在深度网络和视觉皮层中，层与层之间以及单元与单元之间的翻译不变性程度存在很大的差异。 我们的关键见解来自于这样一个事实：整流同时降低了对变换刺激的反应方差和相关性，自然地诱发了不变性和动态范围之间的正向关系。然后，不变的输入单元往往比那些对小图像变换敏感的单元更能驱动网络。我们讨论了这种关系对人工智能的影响：深度网络自然地将不变的单元置于敏感单元之上，这可以通过训练得到加强，也许有助于泛化性能。
我们研究了使用Wave-U-Net架构进行语音增强，这是Stoller等人介绍的用于分离音乐人声和伴奏的模型。 这种用于音源分离的端到端学习方法直接在时域中操作，允许对相位信息进行综合建模，并能将大的时间背景考虑在内。 我们的实验表明，在语音库（VCTK）数据集的语音增强任务方面，所提出的方法比最先进的方法提高了几个指标，即PESQ、CSIG、CBAK、COVL和SSNR。 我们发现，与为音乐中的声音分离而设计的原始系统相比，减少隐藏层的数量足以实现语音增强。我们认为这一初步结果是一个令人鼓舞的信号，可以进一步探索时域中的语音增强，其本身既是一个目的，也是语音识别系统的一个预处理步骤。
强化学习（RL）是一个强大的框架，通过探索和从错误中学习来解决问题。然而，在自主车辆（AV）控制的背景下，要求代理犯错，甚至允许犯错，在现实世界中可能是相当危险和昂贵的。由于这个原因，AV RL通常只在模拟中是可行的。 因为这些模拟有不完美的表示，特别是在图形、物理和人类互动方面，我们找到了类似于RL的框架的动机，适合于现实世界。为此，我们制定了一个学习框架，通过让人类示范者进行探索，从有限的探索中学习。现有的关于从示范中学习的工作通常要么假设收集的数据是由一个最佳专家执行的，要么需要潜在的危险探索来找到最佳策略。 我们提出了一个替代框架，只从安全行为中学习连续控制。我们的一个关键见解是，如果评价示范的反馈分数适用于原子行动，而不是整个行动序列，那么这个问题就变得棘手了。我们使用人类专家来收集驾驶数据，并通过我们称之为 "后座司机 "的框架来标记驾驶数据，给我们提供与代表行动分数的标量值相匹配的状态行动对。 我们把更通用的学习框架称为ReNeg，因为它可以在给定的负面和正面例子中学习从状态到行动的回归。我们在ReNeg框架中实证了几个模型，在有限的数据下测试了车道跟踪。
我们探索了使用矢量变异自动编码器（VQ-VAE）模型进行大规模图像生成。为此，我们扩大并加强了VQ-VAE中使用的自回归先验，以生成比以前更高的一致性和保真度的合成样本。 我们使用简单的前馈编码器和解码器网络，因此我们的模型对于编码和解码速度至关重要的应用来说是一个有吸引力的候选方案。此外，这使得我们只需在压缩的潜空间中进行自回归采样，这比在像素空间中采样要快一个数量级，特别是对于大型图像。 我们证明了VQ-VAE的多尺度分层组织，加上对潜伏代码的强大先验，能够在ImageNet等多方面的数据集上生成质量可与最先进的生成对抗网络相媲美的样本，同时不会受到GAN已知缺点的影响，如模式崩溃和缺乏多样性。
我们为经典的旅行推销员问题（TSP）提出了一种图形神经网络辅助的蒙特卡洛树搜索方法。我们采用贪婪算法框架，通过连续添加节点来构建TSP的最优解。一个图形神经网络（GNN）被训练来捕捉局部和全局图形结构，并给出每一步选择每个顶点的优先概率。 先验概率为MCTS提供了启发式方法，MCTS的输出是选择连续顶点的改进概率，因为它是融合了先验和侦察程序的反馈信息。在多达100个节点的TSP上的实验结果表明，所提出的方法比其他基于学习的方法获得更短的行程。
图神经网络最近在预测分子的量子力学特性方面取得了巨大的成功。这些模型将分子表示为一个图，只使用原子（节点）之间的距离，而不是从一个原子到另一个原子的空间方向。然而，方向信息在分子的经验势中起着核心作用，例如在角势中。为了缓解这一限制，我们提出了方向性信息传递，其中我们嵌入原子之间传递的信息，而不是原子本身。 这些方向性的信息嵌入是旋转等价的，因为相关的方向会随着分子的旋转而旋转。我们提出了一个类似于信念传播的信息传递方案，该方案通过基于信息之间的角度转换信息来使用方向性信息。 此外，我们使用球面贝塞尔函数构建了一个理论上基础良好的正交径向基，它比目前流行的高斯径向基函数实现了更好的性能，同时使用的参数减少了4倍多。我们利用这些创新来构建方向性信息传递神经网络（DimeNet）。DimeNet在MD17上平均比以前的GNN高出77%，在QM9上高出41%。
事实证明，用无模型强化学习（RL）训练一个代理人直接从高维图像中解决控制任务是很困难的。代理人需要学习一个潜在的表征和一个控制策略来执行任务。 我们剖析了学习好的潜在特征的各种方法，并得出结论，图像重建损失是实现基于图像的RL中高效和稳定的表示学习的基本要素。根据这些发现，我们设计了一个带有辅助解码器的非政策性行为者批评算法，该算法进行端到端训练，在许多具有挑战性的控制任务上与无模型和基于模型的算法的最先进性能相匹配。
 大型深度神经网络的运行需要巨大的内存，而且它们的运行速度对于实际应用来说有时太慢。因此，在保持精度的前提下缩小网络规模对于实际应用来说是至关重要的。
生成对抗网络（GANs）已被证明可以非常成功地生成外观逼真的合成图像，但当训练集高度多样化时，其性能似乎不那么令人印象深刻。为了在数据集包括许多不同类别时更好地适应目标数据分布，我们提出了基本GAN模型的变体，即多模式高斯混合GAN（GM-GAN），其中潜空间的概率分布是高斯的混合物。 为了评估模型的性能，我们提出了一种新的评分方法，它分别考虑了两种（通常是冲突的）措施--多样性与生成数据的质量。 通过一系列的实验，使用合成和真实世界的数据集，我们定量地表明GM-GANs的表现优于基线，无论是使用常用的Inception Score评估，还是使用我们自己的替代评分方法评估。此外，我们定性地证明了GM-GAN的无监督变体如何倾向于将潜空间中从不同高斯采样的潜向量映射到数据空间中的不同类别样本。 我们展示了如何利用这一现象来完成无监督聚类的任务，并提供了定量评估，显示了我们的方法在图像数据集的无监督聚类方面的优越性。最后，我们展示了一个使我们的模型与其他GAN模型进一步区别开来的特征：通过改变训练后的潜空间的概率分布来控制质量-多样性的权衡，这允许人们根据自己的需要来采样更高质量和低多样性的样本，或者相反。
分布式优化对于在大型数据集上训练大型模型至关重要。已经提出了多种方法来减少分布式训练中的通信开销，如仅在执行多个本地SGD步骤后进行同步，以及分散的方法（例如。虽然这些方法比基于AllReduce的方法运行得更快，后者在每次更新前都使用阻塞通信，但在相同数量的更新后，所产生的模型可能不太准确。受Chen & Huo（2016）的BMUF方法的启发，我们提出了一个缓慢动量（SloMo）框架，在基础优化算法的多次迭代后，工人定期同步并执行动量更新。 在图像分类和机器翻译任务上的实验表明，相对于基础优化器，SloMo在优化和泛化性能上一直有改进，即使额外的开销在许多更新中摊销，使SloMo的运行时间与基础优化器的运行时间相当。我们提供了理论收敛保证，表明SloMo收敛到平滑非凸损失的固定点。
结构规划对产生长句很重要，这是目前语言生成模型中缺少的部分。在这项工作中，我们在神经机器翻译中增加了一个规划阶段，以控制输出句子的粗略结构。该模型首先生成一些规划器代码，然后以这些代码为条件预测真正的输出词。这些代码被学习来捕捉目标句子的粗略结构。 为了学习这些代码，我们设计了一个具有离散化瓶颈的端到端神经网络，它可以预测目标句子的简化语篇标签。实验表明，通过提前规划，翻译性能普遍提高。
深度神经网络中的数据插值已经成为一个具有重大研究意义的课题。 我们证明过度参数化的单层全连接自动编码器不仅仅是插值，而是记忆训练数据：它们在训练实例的跨度（非线性版本）中产生输出。与全连接自动编码器相反，我们证明深度对于卷积自动编码器的记忆是必要的。 此外，我们观察到，在深度卷积自动编码器中加入非线性会导致更强的记忆形式：深度卷积自动编码器倾向于输出单个训练图像，而不是输出训练图像的跨度中的点。 由于卷积自动编码器组件是深度卷积网络的构件，我们设想我们的发现将阐明过度参数化的深度网络中的归纳偏见这一重要问题。
深度神经网络的巨大成功促使人们需要更好地理解这些网络的基本属性，但提出的许多理论结果只针对浅层网络。本文中，我们研究了理解深度网络有意义的输入空间的一个重要基元：跨度恢复。对于$k<n$，让$\mathbf{A} \in \mathbb{R}^{k \times n}$ 是任意前馈神经网络$M的最内层权重矩阵。\mathbb{R}^n到$mathbb{R}，所以$M(x)$可以写成$M(x) = \sigma(\mathbf{A} x)$，对于某些网络$sigma。\目标是恢复$mathbf{A}$的行跨度，只需给与$M(x)$的神谕访问。我们表明，如果$M$是一个具有ReLU激活函数的多层网络，那么部分恢复是可能的：即我们可以通过对$M(x)$的poly$(n)$非自适应查询，证明恢复$mathbf{A}$行跨度中的$k/2$线性独立向量。 此外，如果$M$具有可微调的激活函数，我们证明，即使输出首先通过符号或$0/1$阈值函数，也可以实现/textit{full}跨度恢复；在这种情况下，我们的算法是自适应的。从经验上看，我们证实，全跨度恢复并不总是可能的，但只适用于不现实的薄层。 对于合理宽度的网络，我们在随机网络和在MNIST数据上训练的网络上都获得了完全的跨度恢复。此外，我们通过诱导神经网络将被控制的随机噪声混淆的数据错误地归类为敏感输入，证明了跨度恢复作为一种攻击的效用。
在本文中，我们研究了同质神经网络中梯度下降算法的隐性正则化，包括具有ReLU或LeakyReLU激活的全连接和卷积神经网络。我们特别研究了优化任何同质模型（可能是非光滑的）的logistic损失或交叉熵损失的梯度下降或梯度流（即步长为无穷小的梯度下降），并表明如果训练损失下降到某个阈值以下，那么我们可以定义一个随时间增长的平滑版本的归一化边际。我们还制定了一个与边际最大化相关的自然约束优化问题，并证明归一化边际及其平滑版本在优化问题的KKT点都收敛到目标值。 我们的结果概括了以前的单层或多层线性网络的逻辑回归结果，并提供了比以前的同质平滑神经网络结果更弱的定量收敛结果。我们在MNIST和CIFAR-10数据集上进行了几个实验来证明我们的理论发现。最后，由于边际与鲁棒性密切相关，我们讨论了训练时间长对提高模型的鲁棒性的潜在好处。
长期的视频预测是非常具有挑战性的，因为它需要同时捕捉长范围图像帧的空间和时间信息。标准的递归模型是无效的，因为它们容易发生错误传播，并且不能有效地捕捉高阶的相关性。一个潜在的解决方案是扩展到高阶的空间-时间递归模型。然而，这样的模型需要大量的参数和操作，使得它在实践中难以学习，并且容易过度拟合。 在这项工作中，我们提出了卷积张量训练LSTM（Conv-TT-LSTM），它使用卷积张量训练分解（CTTD）有效地学习高阶卷积LSTM（ConvLSTM）。我们提出的模型通过使用高效的低秩张量表示，以较小的内存和计算成本自然地纳入了高阶时空信息。 我们在Moving-MNIST和KTH数据集上评估了我们的模型，显示出比标准ConvLSTM的改进，以及比其他基于ConvLSTM的方法更好/可比较的结果，但参数要少得多。
尽管深度神经网络在广泛的应用中表现出了突出的成果，但从数量非常有限的例子中学习仍然是一项具有挑战性的任务。尽管很少的学习有困难，但度量学习技术显示了神经网络在这项任务中的潜力。 此外，本文提出了一个用于训练自适应核SVM的端到端学习框架，它消除了为SVM选择一个正确的核和良好的特征的问题。 然后，该模型在视觉任务（使用Omniglot数据集）和语音任务（使用TIMIT数据集）上也进行了测试。实际上，使用Omniglot数据集的算法在一次分类任务上的准确性从98.1%提高到98.5%，在几次分类任务上的准确性从98.9%提高到99.3%。
在本文中，我们提出了视频级4D卷积神经网络，即V4D，用4D卷积对长距离时空表征的演变进行建模，同时用残余连接保留三维时空表征。 我们进一步介绍了所提出的V4D的训练和推理方法。在三个视频识别基准上进行了广泛的实验，V4D取得了优异的成绩，以很大的优势超过了最近的3D CNN。
我们研究了通过可微分编程的物理模拟进行学习和优化的问题。我们提出了DiffSim，一种为构建高性能可微分物理模拟而定制的新的可微分编程语言。我们在10个不同的物理模拟器上展示了我们语言在基于梯度的学习和优化任务中的性能和生产力。 6倍于手工设计的CUDA版本，但运行速度同样快，比TensorFlow快188倍。使用我们的可微分程序，神经网络控制器通常只需几十次迭代就能优化。最后，我们分享了从开发这些模拟器的经验中获得的教训，即区分物理模拟器并不总是产生被模拟的物理系统的有用梯度，我们系统地研究了根本原因，并提出解决方案以提高梯度质量。
本地解释框架旨在使黑盒预测模型做出的特定决定合理化。现有的技术往往局限于特定类型的预测器或基于输入的突出性，这可能对与模型决策过程无关的因素不适当地敏感。相反，我们提出充分输入子集，确定最小的特征子集，即使所有其他输入特征值都缺失，仅其观察值就足以达成相同的决定。 我们的方法在概念上是直截了当的，完全与模型无关，只需使用实例逆向选择即可实现，并且能够产生比现有技术更简洁的理由。我们在文本和图像数据上训练的神经网络模型上展示了我们解释方法的效用。
在本文中，我们解决了检测非来自训练分布的样本的问题，即。以前的许多研究试图通过使用深度神经网络（DNN）将分类置信度低的样本视为OOD样本来解决这个问题。然而，在困难的数据集或分类能力低的模型上，这些方法会错误地将靠近决策边界的分布内样本视为OOD样本。 因此，我们提出了一种方法，利用重新参数化的技巧提取DNN每一层的特征的不确定性，并将它们结合起来。在实验中，我们的方法以很大的幅度优于现有的方法，在几个数据集和分类模型上实现了最先进的检测性能。例如，我们的方法在CIFAR-100和Tiny-ImageNet数据集上将先前工作的AUROC得分（83.8%）提高到DenseNet的99.8%。
在本文中，我们探索了新的方法来结合自动编码器的学习表征中所编码的信息。我们探索了能够结合多个输入的属性的模型，这样一个重新合成的输出被训练来欺骗真实与合成数据的对抗性判别器。 此外，我们还探讨了在半监督学习的背景下使用这样的架构，在这种情况下，我们学习一个混合函数，其目标是产生隐藏状态的插值，或者与条件类标签一致的潜在表征的掩蔽组合。我们展示了定量和定性的证据，表明这种表述是一个有趣的研究途径。
我们概述了时间序列数据的概念漂移问题。在这项工作中，我们分析了在无设备的被动室内定位背景下流媒体无线信号的时间不一致性。我们表明，从WiFi信道状态信息（CSI）获得的数据可以用来训练一个能够进行房间级定位的稳健系统。 在这项工作中，我们提出了一个相位和幅度增强的特征空间，以及一个受漂移影响很小的标准化技术。我们表明，这种稳健的数据表示方法产生了更好的学习精度，并且需要较少的重新训练。
联合学习改善了在分布式设备网络上进行的机器学习的数据隐私和效率，如移动电话、物联网和可穿戴设备等。然而，由于领域转移的问题，用联合学习训练的模型仍然无法推广到新的设备。 在这项工作中，我们提出了一种解决联合领域适应问题的原则性方法，其目的是使不同节点之间学到的表征与目标节点的数据分布相一致。此外，我们还设计了一种动态关注机制，并利用特征分解来增强知识转移。
一个胶囊是一组神经元，其输出代表同一实体的不同属性。我们描述了一个胶囊的版本，其中每个胶囊有一个逻辑单元来代表一个实体的存在和一个4x4矩阵，它可以学习代表该实体和观察者之间的关系（姿势）。 一层的胶囊通过将自己的姿势矩阵与可训练的视点不变的变换矩阵相乘，对上一层的许多不同的胶囊的姿势矩阵进行投票，这些矩阵可以学习代表部分-整体的关系。 这些系数使用期望最大化算法对每张图像进行迭代更新，从而使每个胶囊的输出被传送到上面一层中收到类似投票的胶囊。 通过在每对相邻的胶囊层之间反向传播EM的未滚动迭代，对转换矩阵进行鉴别性训练。在小型NORB基准上，与最先进的技术相比，胶囊将测试错误的数量减少了45%。胶囊对白盒对抗性攻击的抵抗力也远远高于我们的基线卷积神经网络。
我们研究了一个程序合成的一般表述，称为语法指导合成（SyGuS），它涉及到合成一个遵循给定语法并满足给定逻辑规范的程序。逻辑规范和语法都有复杂的结构，并且可以因任务而异，这给不同任务的学习带来了重大挑战。 我们的框架由三个部分组成：1）一个编码器，使用图神经网络同时嵌入逻辑规范和语法；2）一个语法自适应策略网络，能够学习可转移的策略；3）一个强化学习算法，联合训练嵌入和自适应策略。我们在214个加密电路合成任务上评估了该框架。 在开箱即用的求解器设置中，它解决了其中的141个任务，大大超过了类似的基于搜索但没有学习的方法，后者只解决了31个。
同步机器翻译模型在编码或读取源序列之前就开始生成目标序列。最近针对这一任务的方法要么在变压器上应用固定的策略，要么在一个较弱的基于递归神经网络的结构上应用可学习的单调注意。在本文中，我们提出了一种新的注意机制，单调多头注意（MMA），它将单调注意机制引入多头注意。 我们还引入了两种专门针对多头注意的延迟控制的新型可解释方法。我们将MMA应用于同步机器翻译任务，与之前最先进的方法MILk相比，证明了更好的延迟-质量权衡。代码将在发表后发布。
本文针对从一个分布到另一个分布的最优地图的学习这一基本问题提出了一种新颖的两步法。首先，我们学习一个最优运输（OT）计划，它可以被认为是两个分布之间的一对多的地图。为此，我们提出了一个随机的正则化OT的对偶方法，并通过经验表明，当样本量非常大时，它比最近的一个相关方法扩展的更好。 第二，我们将Monge图估计为一个深度神经网络，通过近似先前获得的OT计划的arycentric投影来学习。这个参数化允许在输入度量的支持范围之外泛化映射。我们证明了正则化OT的两个理论稳定性结果，表明我们的估计收敛于基础连续度量之间的OT和Monge图。
学习有效的文本表征是众多机器学习和NLP应用的关键基础。虽然著名的Word2Vec技术产生了语义丰富的单词表征，但句子或文档表征是应该建立在单词表征上还是从头开始，却不太清楚。(WMD)，将语义相似的词对齐，产生前所未有的KNN分类精度。然而，WMD的计算成本很高，而且比特征嵌入更难应用于简单的KNN。(WME)，一种从预训练的词嵌入中建立无监督文档（句子）嵌入的新方法。我们的技术扩展了 emph{随机特征}的理论，表明WME之间的内积收敛到一个正无限的核，可以解释为软版的（逆）WMD.提出的嵌入在许多情况下比WMD更有效和灵活。 作为一个例子，带有简单线性分类器的WME降低了基于WMD的KNN的计算成本，在文档长度上从立方到线性}，在样本数量上从二次到线性}，同时提高了准确性。在9个基准文本分类数据集和22个文本相似性任务的实验中，提出的技术始终与最先进的技术相匹配或优于它们，在短长度问题上的准确性明显更高。
我们提出了一种新的方法来评估神经网络的鲁棒性，该方法基于对属性被违反的输入比例的估计。具体来说，我们估计在输入模型下属性被违反的事件的概率。我们的方法与形式验证框架的关键区别在于，当属性可能被违反时，它提供了一个关于网络的鲁棒性的信息概念，而不仅仅是网络不可验证的传统论断。 虽然该框架在成功发现一个或多个违规行为时仍然提供可满足性的形式保证，但这些优势确实是以在没有发现违规行为时只提供不可满足性的统计估计为代价的。我们的方法实际成功的关键是对多级分裂的调整，这是一种用于估计罕见事件概率的蒙特卡洛方法，适用于我们的统计稳健性框架。
最近，基于预训练转化器的语言模型在各种NLP数据集上创造了最先进的性能。然而，尽管他们取得了巨大的进步，但他们受到各种结构和句法偏见的影响。在这项工作中，我们研究了词汇重叠偏见，例如。为了提高鲁棒性，我们用自动检测到的谓语-参数结构来丰富训练数据中的输入句子。这种增强的表示方法允许基于转化器的模型通过关注和识别句子的主要语义和句法上的重要部分来学习不同的注意模式。 我们使用BERT、RoBERTa和XLNET模型对我们的解决方案进行了自然语言推理和基础常识推理任务的评估。我们评估了模型在词汇重叠情况下对句法变化、反义词关系和命名实体的理解。我们的结果表明，在微调期间纳入谓语-参数结构可以大大改善鲁棒性，例如，在区分不同的命名实体方面大约有20pp，同时它在测试时不产生额外成本，不需要改变模型或训练程序。
我们提出了一种量化神经网络回归模型中的不确定性的方法，当目标是$d$维简约上的实值时，如概率。我们表明，每个目标可以被建模为Dirichlet分布的样本，其中Dirichlet的参数由神经网络的输出提供，组合模型可以使用数据似然的梯度来训练。 这种方法以多维分布的形式提供可解释的预测，而不是点估计，人们可以从中获得置信区间或量化决策中的风险。此外，我们表明，同样的方法可用于以经验计数的形式为目标建模，作为Dirichlet-Multinomial复合分布的样本。在实验中，我们验证了我们的方法在两个不同的应用上提供这些好处而不损害点估计预测的性能。(1）提炼在CIFAR-100上训练的深度卷积网络，以及（2）预测XENON1T暗物质探测器中粒子碰撞的位置。
深度神经网络在许多现实世界的应用中取得了出色的表现，但要付出巨大的计算资源。DenseNet是最近提出的神经网络架构之一，在许多视觉任务中取得了最先进的性能。然而，由于内部结构的密集连接，它有很大的冗余度，这导致训练这种密集网络的高计算成本。 为了解决这个问题，我们设计了一个强化学习框架，为不同的任务寻找高效的DenseNet架构，同时保留了DenseNet原有的优势，如特征重用、短路径等。 在这个框架中，代理评估任何两个块层之间每个连接的重要性，并修剪多余的连接。此外，还引入了一个新的奖励整形技巧，使DenseNet在准确性和浮点运算（FLOPs）之间达到更好的平衡。  
我们考虑了一个具有固定身体的代理人与一个未知和不确定的外部世界互动的情况。我们表明，为预测代理人身体的本体信息而训练的模型来代表外部世界的物体。尽管只用内部可用的信号进行训练，这些动态身体模型通过预测它们对代理人自己身体的影响的必要性来代表外部物体。也就是说，尽管唯一的训练信号是身体信号，但该模型学习了世界上物体的整体持久的代表。 我们的动力学模型能够成功地预测未来100多步的132个传感器读数的分布，并且我们证明，即使身体不再与一个物体接触，动力学模型的潜在变量也会继续代表其形状。 我们表明，通过最大限度地收集有关身体预测的熵--触摸传感器、本体感觉和前庭信息--导致学习动态模型，这些模型在用于控制时显示出卓越的性能。我们还从一个真正的机器人手收集数据，并表明同样的模型可以用来回答关于现实世界中物体属性的问题。带有我们模型的定性结果的视频可在https://goo.gl/mZuqAV。
受生成对抗网络（GANs）在图像领域的成功启发，我们介绍了一种新颖的分层结构，用于通过GANs从单一的任意输入图中学习特征拓扑特征。由多个GANs组成的分层结构保留了局部和全局拓扑特征，并自动将输入图划分为特征学习的代表性阶段。 这些阶段有利于重建，并可作为相关拓扑结构重要性的指标。实验表明，我们的方法产生的子图保留了广泛的拓扑特征，甚至在早期重建阶段也是如此。本文包含关于结合使用GANs和图拓扑分析的原创性研究。
在中国社会中，迷信是最重要的，带有理想数字的车牌可以在拍卖会上获得非常高的价格。与其他有价值的物品不同，车牌在拍卖前没有被分配一个估计价格。我提出，预测车牌价格的任务可以被看作是一项自然语言处理（NLP）任务，因为价值取决于车牌上每个字符的含义及其语义。我构建了一个深度递归神经网络（RNN），根据车牌上的字符来预测香港的车辆牌照价格。 我证明了拥有一个深度网络和重新训练的重要性。对13年的历史拍卖价格进行评估，深度RNN的预测可以解释80%以上的价格变化，比以前的模型要好得多。 我还展示了该模型如何被扩展成为一个板块的搜索引擎，并提供预期价格分布的估计。
我们提出了一个可以在大规模数据集上学习的自然图像的新潜伏模型。学习过程为训练数据集中的每张图像提供了一个潜伏嵌入，以及一个将潜伏空间映射到图像空间的深度卷积网络。 训练结束后，新的模型为各种图像修复任务提供了强大而通用的图像先验，如大孔内画、超分辨率和着色。为了给高分辨率的自然图像建模，我们的方法使用了维度非常高的潜空间（比以前的潜图像模型高一到两个数量级）。 为了解决这个高维问题，我们使用了具有特殊流形结构（卷积流形）的潜空间，其参数由某个架构的ConvNet设定。在实验中，我们将学到的潜模型与自动编码器学到的潜模型、生成式对抗网络的高级变体以及使用更简单的潜空间参数化的强大基线系统进行比较。我们的模型在一系列修复任务中优于其他竞争方法。
自动作文评分（AES）一直是一个活跃的研究领域，因为它可以大大减少教师的工作量，并防止主观偏见。最近的AES解决方案应用基于深度神经网络（DNN）的模型与回归，其中基于神经神经的编码器学习一个有助于区分作文的表示，并通过回归器推断出相应的作文分数。 这样的DNN方法通常需要大量的专家评分的文章作为训练数据，以学习一个好的文章表征，从而获得准确的评分。然而，这样的数据通常是昂贵的，因此是稀疏的。 受到人类通常通过与一些参考文献进行比较来给论文打分的启发，我们提出了一个名为参考文献网络（RefNet）的连体框架，它允许模型通过捕捉能够区分论文对的相对特征来比较两篇论文的质量。 所提出的框架可以作为回归模型的扩展应用，因为它可以在内部信息的基础上捕捉额外的相对特征。此外，它通过配对内在地增强了数据，因此是处理数据稀缺性的理想选择。实验表明，我们的框架可以显著改善现有的回归模型，即使在训练数据大大减少的情况下也能达到可接受的性能。
了解基因组学的基本任务之一是预测转录因子结合点（TFBSs）的问题。有超过数百个转录因子（TFs）作为标签，基于基因组序列的TFBS预测是一个具有挑战性的多标签分类任务。TF结合有两个主要的生物学机制。(1) 基因组上被称为 "motifs "的序列特异性结合模式；(2) TFs之间的相互作用被称为共同结合效应。在本文中，我们提出了一个新的深度架构，即原型匹配网络（PMN）来模拟TF结合机制。 我们的PMN模型通过一个新的原型匹配损失为每个TF自动提取原型（"motif"-like features）。借用少数几个匹配模型的想法，我们使用原型支持集的概念和LSTM来学习TFs如何与基因组序列相互作用和结合。 在具有210万个基因组序列的参考TFBS数据集上，PMN的表现明显优于基线，并在经验上验证了我们的设计选择。据我们所知，这是第一个引入原型学习并考虑TF-TF相互作用的大规模TFBS预测的深度学习架构，不仅提出的架构是准确的，而且它也是基础生物学模型。
以前的工作表明，对抗性强的泛化需要更大的样本复杂性，同样的数据集，例如。由于收集新的训练数据的成本很高，我们专注于通过诱导特征空间中具有高样本密度的区域来更好地利用给定的数据，这可能会导致局部足够的样本用于鲁棒学习。我们首先正式表明，softmax交叉熵（SCE）损失及其变体传达了不适当的监督信号，它鼓励学习的特征点在训练中稀疏地分布在空间中。 这促使我们提出了Max-Mahalanobis中心（MMC）损失，以明确诱导密集的特征区域，从而提高鲁棒性。也就是说，MMC损失鼓励模型专注于学习有序和紧凑的表征，这些表征聚集在不同类别的预设最佳中心周围。我们通过经验证明，即使在强大的自适应攻击下，应用MMC损失也能显著提高鲁棒性，同时与SCE损失相比，在清洁输入上保持最先进的准确性，几乎没有额外计算。
我们提出了一个分析框架，在单元、对象和场景层面上对GAN进行可视化和理解。我们首先用基于分割的网络剖析方法确定了一组与对象概念密切相关的可解释单元。然后，我们通过测量干预措施在输出中控制对象的能力来研究可解释单元的因果效应。 最后，我们通过将发现的物体概念插入到新的图像中来检查这些单元和它们周围的上下文关系。我们展示了由我们的框架促成的几个实际应用，从比较不同层和模型的内部表征，到通过定位和删除造成伪影的单元来改进GANs，再到交互地操纵场景中的物体。
我们提出了一个简单的近邻（NN）方法，从一个 "不完整 "的信号，如低分辨率图像、表面法线图或边缘，合成高频逼真的图像。目前最先进的深度生成模型是为这种条件性图像合成设计的，但缺乏两个重要因素。(1)由于模式崩溃问题，它们无法产生大量不同的输出。(2)它们是不可解释的，因此很难控制合成的输出。我们证明，NN方法有可能解决这些限制，但在小数据集上的准确性受到影响。 我们设计了一个简单的管道，结合了两者的优点：第一阶段使用卷积神经网络（CNN）将输入映射到（过度平滑的）图像，第二阶段使用像素最近邻方法将平滑的输出以可控的方式映射到多个高质量的高频输出。重要的是，像素最近邻匹配允许我们的方法通过切割和粘贴不同训练范例的像素来组成新的高频内容。 我们对各种输入模式和各种领域（包括人脸、宠物、鞋子和手袋）展示了我们的方法。
神经网络很容易受到对抗性例子的影响，研究人员已经提出了许多启发式的攻击和防御机制。我们通过分布式稳健优化的原则性视角来解决这个问题，它保证了对抗性输入扰动下的性能。 通过考虑在Wasserstein球中扰动基础数据分布的拉格朗日惩罚表述，我们提供了一个训练程序，用训练数据的最坏情况扰动来增强模型参数更新。对于平滑损失，我们的程序可以证明实现了适度的稳健性，相对于经验风险最小化来说，计算或统计成本很小。此外，我们的统计保证使我们能够有效证明群体损失的稳健性。
后验推理的质量主要由两个因素决定：a）变异分布对真实后验的建模能力；b）识别网络对所有数据点进行泛化推理的能力。我们从这些因素来分析变异自动编码器的近似推理。 我们发现，次优推理往往是由于摊销推理，而不是由于近似分布的有限复杂性。我们表明，这部分是由于发生器学习适应近似的选择。此外，我们表明，用于增加近似的表现力的参数在泛化推理中起作用，而不是简单地提高近似的复杂性。
在本文中，我们提出了一个框架，利用半监督模型来提高无监督聚类的性能。为了利用半监督模型，我们首先需要自动生成标签，称为伪标签。我们发现，先前生成伪标签的方法由于其低精确度而损害了聚类性能。相反，我们使用深度网络的集合来构建一个相似性图，从中提取高精确度伪标签。 使用集合体寻找高质量的伪标签的方法和训练半监督模型的方法被反复使用，产生了持续的改进。我们表明，我们的方法在多个图像和文本数据集的聚类结果上优于最先进的方法。例如，我们对CIFAR-10的准确率达到54.6%，对20news达到43.9%，在绝对值上优于最先进的方法8到12%。
本文涉及字典学习，即。我们表明，在对数据进行温和的统计假设下，一个具有随机初始化的梯度下降算法可以在一个自然的非光滑、非凸的L1最小化问题的表述上恢复正交的字典。 这与之前的可证明方法形成鲜明对比，后者需要昂贵的计算或微妙的初始化方案。我们的分析开发了几个工具来描述非光滑函数的景观，这可能是对具有非光滑激活的深度网络的可证明训练的独立兴趣（例如。初步的合成和真实实验证实了我们的分析，并表明我们的算法在恢复正交字典方面的经验效果良好。
我们研究了数据分类的模型恢复，其中训练标签是由一个具有sigmoid激活的单层全连接神经网络产生的，目标是恢复神经网络的权重向量。我们证明，在高斯输入下，只要样本复杂度足够大，使用交叉熵的经验风险函数就会在基本事实的局部附近均匀地表现出强凸性和光滑性。 这意味着，如果在这个邻域初始化（可以通过张量方法实现），梯度下降会线性收敛到一个临界点，该临界点可以证明是接近于基本事实的，而不需要在每次迭代时提供一组新的样本。据我们所知，这是第一个为使用交叉熵的经验风险最小化通过梯度下降学习单隐层神经网络，在接近最佳样本和网络输入维度的计算复杂性方面建立的全球收敛保证。
随着神经网络在移动设备上的部署以及在有限或昂贵的信道上传输神经网络的必要性，训练过的模型的文件大小被确定为瓶颈。我们提出了一种用于压缩神经网络的编解码器，该编解码器基于卷积层和密集层的变换编码以及偏置和规范化的聚类。
随着机器学习（ML）被应用于安全关键或敏感领域，对外包ML计算的完整性和隐私性的需求越来越大。一个实用的解决方案来自于可信执行环境（TEEs），它使用硬件和软件保护将敏感计算与不信任的软件栈隔离开来。 本文通过在可信设备和不可信设备之间有效划分DNN计算，开始研究在TEE中高性能执行深度神经网络（DNNs）。我们通过在英特尔SGX飞地中运行DNN来评估Sralom，该飞地有选择地将工作委托给一个不受信任的GPU。对于典型的DNN（VGG16、MobileNet和ResNet变体），我们获得了可验证推理6倍到20倍的吞吐量，可验证和私有推理4倍到11倍。
虽然深度神经网络是一个非常成功的模型类别，但其巨大的内存占用对能源消耗、通信带宽和存储要求造成了相当大的压力。因此，缩小模型尺寸已成为深度学习的一个最重要目标。 一个典型的方法是训练一组确定性的权重，同时应用某些技术，如剪枝和量化，以便经验性的权重分布能够适应香农式的编码方案。然而，正如本文所示，放宽权重的确定性并使用权重的完全变异分布可以实现更有效的编码方案，从而提高压缩率。 特别是，根据经典的比特-背论，我们使用随机抽样对网络权重进行编码，只需要与抽样的变异分布和编码分布之间的Kullback-Leibler分歧相对应的比特数。通过对Kullback-Leibler分歧施加约束，我们能够明确地控制压缩率，同时优化训练集的预期损失。 所采用的编码方案可以被证明是接近最佳信息理论下限的，就所采用的变异族而言。我们的方法在神经网络压缩方面树立了新的先进性，因为它在帕累托意义上严格控制了以前的方法。在LeNet-5/MNIST和VGG-16/CIFAR-10的基准上，我们的方法在固定的内存预算下产生了最好的测试性能，反之亦然，它在固定的测试性能下实现了最高的压缩率。
大多数现有的用于学习图形的神经网络通过将网络设想为一种消息传递方案来处理包络不变性的问题，其中每个节点对来自其邻居的特征向量进行求和。我们认为这对其表示能力造成了限制，相反，我们提出了一种新的通用架构来表示由部分层次组成的对象，我们称之为共变组合网络（CCNs）。 这里的协变意味着每个神经元的激活必须在包络下以特定的方式进行转换，类似于CNN中的转向性。我们通过使每个激活根据包络组的张量表示进行转换来实现协变，并推导出每个神经元必须实现的相应的张量聚合规则。实验表明，CCN在一些标准图学习基准上的表现可以超过竞争方法。
 近年来，三维卷积神经网络（3D CNN）被大量应用于视频分析和动作识别，并获得了良好的性能。然而，三维CNN导致了大量的计算和存储消耗，这阻碍了它在移动和嵌入式设备上的部署。本文中，我们提出了一种基于三维正则化的修剪方法，根据不同权重组对网络的重要性，为其分配不同的正则化参数。
在本文中，我们提议将数据声明作为自然语言处理技术专家在研究和开发方面的设计方案和专业实践--通过采用和广泛使用数据声明，该领域可以开始解决在为其他人群开发技术时使用某些人群的数据所导致的关键科学和伦理问题。我们提出了数据声明可以采取的形式，并探讨了将其作为常规做法的一部分的意义。 我们认为，数据声明将有助于缓解与语言技术中的排斥和偏见有关的问题；导致关于NLP研究如何泛化的说法更加精确，从而获得更好的工程结果；保护公司免受公众的尴尬；并最终导致语言技术以其用户自己喜欢的语言风格满足他们，而且不会向其他人错误地表达他们。
我们介绍了CGNN，这是一个将功能性因果模型作为生成性神经网络来学习的框架。这些网络使用反向传播来训练，以最小化与观察数据的最大平均差异。与之前的方法不同，CGNN利用条件独立性和分布不对称性来无缝发现双变量和多变量的因果结构，无论是否有隐藏变量。 通过大量的实验，我们说明了CGNN在模拟和真实数据的观察性因果发现中，在因果推断、V型结构识别和多变量因果发现等任务中，与最先进的替代方案相比，具有很强的竞争力。
网络修剪被广泛用于降低深度模型的沉重计算成本。一个典型的修剪算法是一个三阶段的管道，即。在这项工作中，我们提出了一个相当令人惊讶的观点：对一个修剪过的模型进行微调，其性能与用随机初始化权重训练该模型相当，甚至更差。 我们的结果有几个含义：1）训练一个大型的、过度参数化的模型并不是获得高效的最终模型的必要条件，2）在大型模型中学到的 "重要 "权重不一定对小型修剪模型有用，3）修剪的结构本身，而不是一组继承的权重，是导致最终模型效率提高的原因，这表明一些修剪算法可以被视为执行网络结构搜索。
聚类是无监督学习和数据挖掘的核心任务。K-means是最广泛使用的聚类算法之一。不幸的是，一般来说，将K-means扩展到高斯分布以外的数据点，特别是具有非凸形的聚类是不难的（Beliakov & King, 2006）。为此，我们首次引入极值理论（EVT）来提高K-means的聚类能力。 特别是，欧氏空间被EVT转化为一个新的概率空间，被称为极值空间。我们因此提出了一个新的算法，称为极值k-means（EV k-means），包括GEV k-means和GPD k-means。实验结果表明，我们的算法在大多数情况下明显优于竞争对手。
深度强化学习算法在各种领域都被证明是成功的。然而，当状态空间很大时，具有稀疏奖励的任务仍然具有挑战性。面向目标的任务是这个领域最典型的问题之一，只有在完成最终目标时才能获得奖励。在这项工作中，我们提出了一个潜在的解决方案，即引入基于经验的倾向性奖励机制，在自动反向课程中，基于对过去经验的判别性学习为代理人提供额外提示。 这种机制不仅提供了关于什么状态会导致成功的密集的额外学习信号，而且还允许代理人在多阶段课程学习中只保留这种倾向性奖励而不是整个经验史。 我们广泛研究了我们的方法在标准稀疏奖励领域的优势，如迷宫和超级马里奥兄弟，并表明我们的方法在具有长时间跨度和大状态空间的任务中比以前的方法更有效、更稳健。此外，我们还证明，使用一个可选择的关键帧方案，用非常小数量的关键状态，我们的方法可以直接从感知和稀疏奖励中解决困难的机器人操纵挑战。
人类的场景感知不仅仅是识别物体的集合和它们的成对关系。我们理解场景中更高层次的、抽象的规律性，如对称性和重复性。目前的视觉识别模块和场景表示在这个维度上是不足的。在本文中，我们提出了场景程序，通过其物体、属性及其关系的符号程序表示一个场景。 我们还提出了一个模型，通过利用分层的、基于对象的场景表征来推断这种场景程序。实验证明，我们的模型在合成数据上运行良好，并且可以转移到具有这种组成结构的真实图像上。使用场景程序已经实现了许多应用，例如复杂的视觉类比和场景推断。
现代神经网络通常需要高维非线性函数的深度组合（宽结构）来实现高的测试精度，因此会有压倒性的参数数量。在测试时间的预测中重复的高成本使神经网络不适合于内存或计算能力有限的设备。 我们引入了一种有效的机制，即重塑张量分解，通过利用三种类型的不变结构来压缩神经网络：周期性、调制和低等级。我们的重塑张量分解方法利用这种不变结构，使用一种称为张量化的技术（将层重塑为高阶张量），结合高阶张量分解在张量化的层之上。 我们的压缩方法改进了低秩近似方法，并可以与大多数现有的神经网络压缩方法相结合（是互补的），以实现更好的压缩。在LeNet-5（MNIST）、ResNet-32（CI- FAR10）和ResNet-50（ImageNet）上的实验表明，我们的重塑张量分解在相同的压缩率下优于（在CIFAR10上普遍提高5%的测试精度）最先进的低秩近似技术，此外还实现了数量级的快速收敛率。
虽然在视觉模式方面已经探索了许多不同的可解释性方向，但时间序列数据由于其可理解性差而被忽视，只有少数几种方法被测试。 我们通过提出TSInsight，以一种新的方式来解决可解释性的问题，我们将一个在其输出上具有疏导规范的自动编码器附加到分类器上，并根据分类器的梯度和重建惩罚对其进行微调。自动编码器学习保留对分类器预测重要的特征，并抑制不相关的特征，即。 换句话说，我们要求网络只重建对分类器有用的部分，即与预测相关或有因果关系的部分。与其他大多数归因框架相比，TSInsight能够生成基于实例和基于模型的解释。 我们在一系列不同的时间序列数据集上对TSInsight和其他常用的归因方法进行了评估，以验证其功效。此外，我们分析了TSInsight开箱即用的一系列特性，包括对抗性稳健性和输出空间收缩。
使用大批量和小批量梯度混合的方差减少方法，如SVRG（Johnson & Zhang，2013）和SpiderBoost（Wang等人，2018），每次更新需要的计算资源明显多于SGD（Robbins & Monro，1951）。我们通过引入混合top-K算子的稀疏梯度算子（Stich et al, 2018; Aji & Heafield, 2017）和随机坐标下降算子。虽然计算模型参数导数的计算成本是恒定的，但我们观察到，方差减少的收益与导数的大小成正比。在本文中，我们表明，基于过去梯度大小的稀疏梯度降低了模型更新的计算成本，而在方差减少方面没有显著损失。 理论上，我们的算法在适当的参数设置下至少与现有的最佳算法（如SpiderBoost）一样好，如果我们的算法成功地捕捉到梯度的稀疏性，那么效率会更高。实证上，我们的算法在使用各种模型解决各种图像分类任务时一直优于SpiderBoost。
尽管在单模态数据估算方面取得了可喜的进展（如图像绘画），但多模态数据估算的模型还远远不能令人满意。在这项工作中，我们为这项任务提出了变异选择性自动编码器（VSAE）。 VSAE仅从部分观测数据中学习，可以对观测/未观测的模式和归因掩码的联合分布进行建模，从而为包括数据生成和归因在内的各种下游任务提供一个统一的模型。对合成的高维和具有挑战性的低维多模态数据集的评估表明，与最先进的归因模型相比，有明显的改进。
在许多领域，特别是企业文本分析，有大量的数据可用于开发新的人工智能驱动的智能体验，以提高人们的生产力。然而，有强大的隐私保障，阻止了对个人文本数据的广泛采样和标记，以学习或评估感兴趣的模型。 在本文中，我们研究了将信息从一个电子邮件数据集转移到另一个数据集以预测用户意图的挑战。特别是，我们提出了从内在和外在的角度来描述文本体的转移差距的方法，并评估了文献中提出的几种弥合这一差距的方法。
分层强化学习是解决具有稀疏奖励的长跨度决策问题的一种有前途的方法。不幸的是，大多数方法仍然将低层次的技能获取过程和控制新任务中技能的高层次的训练脱钩。 我们的主要贡献有两个方面。首先，我们推导出一个新的分层策略梯度，以及一个无偏的潜在依赖基线。我们引入了分层近端策略优化（HiPPO），这是一种同时有效训练所有层次的策略方法。其次，我们提出了一种训练时间抽象的方法，提高了所获得的技能对环境变化的鲁棒性。代码和结果可在sites.google.com/view/hippo-rl获得。
由于输入和输出之间的分布是未知的，所以真正的相互信息也是未知的。为了量化学习一项任务的难度，我们通过用估计的相互信息除以输入的熵来计算观察到的相互信息分数。我们通过分析证实了这个分数，表明估计的相互信息有一个随着数据的熵增加的误差。 有趣的是，根据数据的表示方式，观察到的熵和互信息可以有很大的变化。我们通过实验分析了基于图像的输入数据表示，并证明广泛的网络结构搜索的性能结果与计算的分数很一致。
败血症是一种威胁生命的感染并发症，也是医院中死亡的主要原因之一。 虽然早期发现败血症可以改善病人的预后，但对确切的治疗指南几乎没有共识，治疗败血症病人仍然是一个开放的问题。 在这项工作中，我们提出了一种新的深度强化学习方法，用来学习脓毒症患者的最佳个性化治疗政策。我们使用多输出高斯过程对患者的连续值生理时间序列进行建模，这种概率模型可以轻松处理缺失值和不规则间隔的观察时间，同时保持对不确定性的估计。高斯过程直接与深度递归Q网络联系在一起，学习临床上可解释的治疗政策，两种模型都是端到端的学习。 我们在大学卫生系统的一个跨越15个月的脓毒症异质数据集上评估了我们的方法，发现我们学到的政策可以从13.3％的总体基线死亡率中减少多达8.2％的病人死亡率。 我们的算法可用于向医生提出治疗建议，作为决策支持工具的一部分，而且该框架很容易适用于其他依赖稀疏采样和经常缺失的多变量时间序列数据的强化学习问题。
无监督和半监督学习是重要的问题，对于像自然图像这样的复杂数据尤其具有挑战性。如果我们能够获得适当的生成模型来提出相关的推理任务，这些问题的进展将会加快。 受卷积神经网络（CNN）在图像监督预测方面的成功启发，我们设计了神经渲染模型（NRM），一个新的分层概率生成模型，其推理计算与CNN中的计算相对应。 NRM中的概率估计产生了新的Max-Min交叉熵训练损失，它提出了一个新的深度网络架构--Max-Min网络，它超过或匹配了SVHN、CIFAR10和CIFAR100上的半监督和监督学习的技术状态。
深度强化学习（RL）代理往往不能泛化到未见过的环境中（但在语义上与受训代理相似），特别是当它们在高维状态空间（如图像）上受训时。在本文中，我们提出了一种简单的技术，通过引入随机（卷积）神经网络来改善深度RL代理的泛化能力，随机扰动输入观察。 此外，我们还考虑了一种基于蒙特卡洛近似的推理方法，以减少这种随机化引起的变异。我们证明了我们的方法在二维CoinRun、三维DeepMind实验室探索和三维机器人控制任务中的优越性：它明显优于相同目的的各种正则化和数据增强方法。
目前的移动设备的触摸交互表现力有限。用额外的自由度来增强设备可以增加交互的力量，并且已经提出并测试了几种增强方法。然而，对于学习多套映射到不同应用程序的增强交互的效果仍然知之甚少。为了更好地了解多个命令映射是否会相互干扰，或者影响转移和保留，我们开发了一个原型，在智能手机外壳上有三个按钮，可以用来向系统提供增强的输入。 这些按钮可以被串联起来，以提供七个可能的快捷方式或瞬时模式开关。我们将这些按钮映射到三组不同的动作上，并进行了一项研究，看多种映射是否会影响学习和性能、转移和保留。我们的结果显示，所有的映射都很快被学会了，而且多种映射的性能也没有降低。 转移到一个更现实的任务是成功的，尽管准确率略有下降。一周后的保留率最初很差，但专家的表现很快就恢复了。我们的工作提供了关于移动交互中增强输入的设计和使用的新信息。
神经网络在自然语言处理中被广泛使用，然而，尽管它们在经验上取得了成功，但它们的行为却很脆弱：它们对小的输入变化过于敏感，而对大段输入文本的删除则不够敏感。 本文旨在解决自然语言推理中的不敏感问题，确保模型不会因为输入文本中的任意词子集被删除而对其预测变得更有信心。我们开发了一种新技术，通过采用高效的区间约束传播（IBP）方法，对基于流行的可分解注意力机制的模型进行正式验证。 在我们对SNLI和MNLI数据集的实验中，我们观察到IBP训练导致了验证准确性的显著提高。在SNLI测试集上，我们可以验证18.4%的样本，比使用标准训练的2.8%有了大幅提高。
在保持快速迭代的同时，用更多的参数进行训练是开发性能更好的深度神经网络（DNN）模型的一个越来越多的策略和趋势，这就需要增加训练的内存占用和计算要求。 我们将这种方法命名为移位和挤压FP8（S2FP8）。我们发现，与以前的8位精度训练方法不同，所提出的方法对有代表性的模型是开箱即用。我们介绍了DNN张量的两个可学习的统计数据--移位和挤压因子，用于优化调整8位张量的范围，从而使量化造成的信息损失最小。
变异自动编码器（VAE）能够生成真实的图像、声音和视频序列。从实践者的角度来看，我们通常对解决任务的顺序学习感兴趣，以避免在每个阶段重新访问所有以前的数据。我们通过引入概念上简单和可扩展的端到端方法来解决这个问题，通过直接从数据中学习先验，纳入过去的知识。 我们考虑了可扩展的类似于boosting的近似方法，以解决难以解决的理论上的最佳先验。我们在两个常用的基准上进行了实证研究，即MNIST和Fashion MNIST的不连续图像生成任务。对于每个数据集，所提出的方法在可比的方法中提供了最好的结果，以固定的模型结构，以全自动的方式避免了灾难性的遗忘。
少量学习领域最近有了实质性的进展。这些进展大多来自于将少量学习作为一个元学习问题。模型不可知元学习或MAML是目前通过元学习进行少量学习的最佳方法之一。 MAML简单、优雅且非常强大，然而，它有各种问题，如对神经网络架构非常敏感，经常导致训练期间的不稳定，需要艰苦的超参数搜索来稳定训练并实现高泛化，并且在训练和推理时都非常昂贵。在本文中，我们提出了对MAML的各种修改，不仅稳定了系统，还大大改善了MAML的泛化性能、收敛速度和计算开销，我们称之为MAML+。
图中的社群检测在图挖掘、机器学习和网络科学中具有核心重要性。检测重叠的社区尤其具有挑战性，并且仍然是一个开放的问题。 在基于图的深度学习在其他与图相关的任务中取得成功的激励下，我们研究了这个框架在重叠社区检测中的适用性。我们提出了一个基于图神经网络架构的重叠社区检测的概率模型。 尽管我们的模型很简单，但在社区恢复任务中，我们的模型在很大程度上超过了现有的方法。 此外，由于采用了归纳法，所提出的模型能够对训练时不存在的节点进行样本外的社区检测。
神经架构搜索（NAS）旨在促进新任务的深度网络设计。现有的技术依赖于两个阶段：在架构空间上搜索和验证最佳架构。NAS算法目前仅根据其在下游任务上的结果进行比较。虽然很直观，但这未能明确评估其搜索策略的有效性。 为此，我们将NAS搜索策略获得的解决方案的质量与随机架构选择的质量进行了比较。我们发现：（i）平均而言，最先进的NAS算法的表现与随机策略相似；（ii）广泛使用的权重共享策略降低了NAS候选者的排名，以至于不能反映它们的真实性能，从而降低了搜索过程的有效性。我们相信，我们的评估框架将是设计NAS策略的关键，这些策略能够持续发现优于随机的架构。
在本文中，我们利用最优传输（OT）问题的几何特性和Wasserstein距离来定义自动编码器潜伏空间的先验分布。我们引入了Sliced-Wasserstein自动编码器（SWAE），使人们能够将潜伏空间的分布塑造成任何可抽样的概率分布，而不需要训练对抗性网络或指定似然函数。 简而言之，我们用编码的训练样本分布和可抽样的先验分布之间的切片Wasserstein距离来规范自动编码器的损失。我们表明，所提出的公式有一个有效的数值解决方案，提供了与Wasserstein自动编码器（WAE）和Variational自动编码器（VAE）类似的能力，同时受益于一个令人尴尬的简单实现。我们为我们的算法提供了广泛的误差分析，并在三个基准数据集上显示了其优点。
哈密尔顿形式主义在经典和量子物理学中起着核心作用。哈密尔顿是模拟具有守恒量的系统的连续时间演化的主要工具，它们具有许多有用的特性，如时间可逆性和平滑的时间插值。 这些特性对许多机器学习问题--从序列预测到强化学习和密度建模--都很重要，但标准工具如递归神经网络通常不提供这些特性。在本文中，我们介绍了哈密尔顿生成网络（HGN），这是第一个能够从高维观察（如图像）中持续学习哈密尔顿动力学的方法，没有限制性领域假设。 一旦经过训练，我们可以使用HGN对新的轨迹进行采样，在时间上向前和向后执行滚动，甚至加快或减慢所学的动力学。我们演示了对网络结构的简单修改如何将HGN变成一个强大的归一化流模型，称为神经哈密尔顿流（NHF），它使用哈密尔顿动力学来模拟表达式密度。因此，我们希望我们的工作可以作为哈密尔顿形式主义可以为机器学习带来价值的第一个实际演示。更多结果和视频评估可参见： http://tiny.cc/hgn
云迁移将客户的数据、应用和服务从原来的IT平台转移到一个或多个云环境中，目的是提高IT系统的性能，同时降低IT管理成本。企业级的云迁移项目通常是复杂的，涉及动态规划和重新规划多达10k个终端的各种类型的转变。 目前，云迁移中的规划和再规划通常是手动或半手动完成的，严重依赖迁移专家的领域知识，每一轮规划或再规划都需要几天甚至几周的时间。因此，能够在短时间内生成高质量迁移计划的自动化规划引擎对迁移行业来说是特别可取的。
无监督领域适应的目的是将在源域中训练的假设泛化到未标记的目标域。对这个问题的一个流行的方法是为两个领域学习一个领域不变的表示。在这项工作中，我们从理论和经验上研究了嵌入对泛化到目标域的明确影响。特别是，嵌入类的复杂性影响到目标域的风险的上限。这也反映在我们的实验中。
时间序列预测领域已被广泛研究，因为它在许多现实生活的应用中具有根本的重要性。天气预测、交通流量预测或销售是引人注目的连续现象的例子。预测模型通常利用过去和未来的价值之间的关系。然而，在静止的时间序列的情况下，观察到的价值也极大地取决于一些外生的特征，可以用来提高预测质量。 在这项工作中，我们提出了一种范式的改变，包括在递归神经网络中的嵌入向量中学习这种特征。我们将我们的框架应用于预测巴黎地铁网络中的智能卡自取记录。
代理人{发现}自己的学习目标的能力一直被认为是人工通用智能的一个关键因素。自主决策和强化学习的突破主要是在代理人的目标被概述和明确的领域：如玩游戏赢，或安全驾驶。 一些研究表明，学习外部子任务和辅助预测可以改善（1）单一的人类指定的任务学习，（2）学习的转移，（3）和代理对世界的学习表示。在所有这些例子中，代理被指示要学习什么。 我们研究了一个发现的框架：策划一个大的预测集合，用来构建代理人对世界的表征。具体来说，我们的系统维护一个大的预测集合，不断修剪和替换预测。我们强调考虑稳定而不是收敛对这样一个系统的重要性，并为这个目标开发了一个自适应的正则化算法。我们在计算微观世界中提供了几个实验，证明这个简单的方法可以有效地自主发现有用的预测。
仅仅根据图像的内容来估计图像的拍摄地点是一项具有挑战性的任务，即使对人类来说也是如此，因为以这种方式对图像进行正确的标记在很大程度上依赖于上下文信息，而不是像识别图像中的单个物体那样简单。因此，任何试图这样做的方法都必须以某种方式考虑到这些复杂性，而且迄今为止没有一个模型能够完全解决所有挑战。 这项工作通过引入一种新的全局网格策略，概述了各种训练程序，以克服训练这些模型时的相当大的数据限制，并展示了如何通过纳入额外的信息来提高地理位置推理模型的整体性能，从而对图像地理位置推理的研究状况做出了贡献。在这项工作中，与使用四叉树和多数量级训练数据的先进模型的结果相比，Delaunay三角形是一种在相对低容量场景下用于地理位置的有效网格类型。 此外，发布时间、学习过的用户专辑和其他元数据很容易被纳入，以提高地理定位的精度，国家层面（750公里）的定位精度可达11%，城市层面（25公里）的定位精度为3%。
分层贝叶斯方法有可能将许多相关的任务（如K-shot分类、条件和无条件生成）统一起来，将每项任务作为单一生成模型内的推理。我们表明，现有的学习这种模型的方法可能会在PixelCNN等表达式生成网络上失败，因为它描述了全局分布，而很少依赖潜在的变量。 为了解决这个问题，我们开发了一个变异自动编码器的修改版，其中编码的观察值被解码为同一类别的新元素；其结果，我们称之为变异同源编码器（VHE），可以理解为训练一个层次化的潜变量模型，在这些情况下更好地利用潜变量。 使用这个框架使我们能够为Omniglot数据集训练一个层次化的PixelCNN，在测试集的可能性上优于所有现有的模型。VHE的目标自然延伸到更丰富的数据集结构，如因子或层次类别，正如我们通过训练模型将字符内容与绘画风格的简单变化分开，并将字母表的风格推广到新的字符中。
理解自然语言需要常识或背景知识，但在大多数神经自然语言理解（NLU）系统中，必要的背景知识是间接从静态语料库中获得的。我们开发了一个新的阅读架构，用于在NLU模型中动态整合明确的背景知识。 一个新的任务无关的阅读模块通过处理自由文本语句形式的背景知识和特定任务的输入，为特定任务的NLU架构提供了精炼的单词表示。
混合精度算术在同一操作中结合了单精度和半精度操作数，已经成功地应用于训练深度神经网络。尽管混合精度算术在减少对内存带宽或寄存器文件大小等关键资源的需求方面具有优势，但它在降低计算成本方面的能力有限，需要32位来表示其输出操作数。本文提出了两种方法，在训练的大部分时间里用半精度算术取代混合精度。 第一种方法通过在超过99%的训练中使用半精度算术，实现了比最先进的精确度。第二种方法通过在训练中动态切换半精度和混合精度算术，达到与最先进的精确度相同。
我们引入了 "反平方根线性单元"（ISRLU）来加速深度神经网络的学习。ISRLU比ELU有更好的性能，但有许多相同的好处。ISRLU和ELU有相似的曲线和特征。 两者都有负值，允许它们将平均单元激活推向零，并使正常梯度更接近单元自然梯度，确保噪声稳健的停用状态，减少过拟合风险。ISRLU在传统CPU上的显著性能优势也延续到CNNs/RNNs的HW/SW代码设计上更有效的HW实现。 在TensorFlow的实验中，ISRLU比ReLU在CNN上的学习速度更快，泛化效果更好。这项工作还提出了一种计算效率高的变体，称为 "反平方根单元"（ISRU），可用于RNNs。许多RNNs使用长短期记忆（LSTM）和门控递归单元（GRU），它们用tanh和sigmoid激活函数实现。
一般来说，一个智能学习者应该能概括到比以前遇到的更复杂的任务，但机器学习中的两种常见范式--要么为每个任务训练一个单独的学习者，要么为所有任务训练一个单一的学习者--都难以实现这种概括，因为它们没有利用任务分布的组成结构。本文介绍了组成问题图，作为一种广泛适用的形式主义，将不同复杂性的任务与具有共享子问题的问题联系起来。 作为解决构成性泛化问题的第一步，我们引入了构成性递归学习者，这是一个领域通用的框架，用于学习构成表征转换的算法程序，产生一个学习者，通过对以前看到的问题进行类比来推理执行什么计算。我们在一个符号和高维领域中显示，我们的构成性方法可以泛化到比学习者以前遇到的更复杂的问题，而没有明确构成性的基线则不能。
信息瓶颈法为表征学习提供了一种信息理论方法，通过训练编码器来保留所有与预测标签相关的信息，同时尽量减少表征中其他多余的信息量。然而，最初的表述需要标记的数据，以确定哪些信息是多余的。 在这项工作中，我们将这种能力扩展到多视图无监督的环境中，在这种环境中，同一实体的两个视图被提供，但标签是未知的。理论分析导致了一个新的多视图模型的定义，在Sketchy数据集和MIR-Flickr数据集的标签限制版本上产生了最先进的结果。 我们还通过利用标准的数据增强技术将我们的理论扩展到了单视图环境中，与传统的无监督的表示学习方法相比，经验上显示了更好的概括能力。
长期以来，神经科学家一直怀疑反向传播算法在生物学上的合理性。两个主要原因是，神经元需要在前向和后向阶段发送两种不同类型的信号，而且成对的神经元需要通过对称的双向连接进行交流。我们提出了一个简单的两阶段学习程序，用于解决固定点递归网络的问题。 我们的学习方法将平衡传播的框架扩展到一般的动力学，放宽了对能量函数的要求。由于这种泛化的结果，该算法不计算目标函数的真正梯度，而是以一个精度近似它，该精度被证明与前馈和反馈权重的对称程度直接相关。我们通过实验表明，系统的内在属性导致前馈和反馈权重的对齐，我们的算法优化了目标函数。
在本文中，据我们所知，我们提出了第一个以完全无监督的方式从自然图像中学习3D形状生成模型的方法。例如，我们在训练过程中不使用任何地面真实的3D或2D注释、立体视频和自我运动。 我们的方法遵循生成对抗网络的一般策略，其中一个图像生成器网络学习创建足够逼真的图像样本，以欺骗鉴别器网络相信它们是自然图像。相反，在我们的方法中，图像生成分为两个阶段。在第一阶段，生成器网络输出三维目标。 关键的观察是，一个现实的三维物体应该从任何合理的视角产生一个现实的渲染。因此，通过随机选择视角，我们建议的训练迫使生成器网络学习一个与视角无关的可解释的三维表示。 在这项工作中，三维表征由一个三角形网格和一个纹理图组成，该纹理图通过使用UV-映射技术为三角形表面着色。
我们专注于黑箱对抗性攻击的问题，其目的是使用仅限于输入-输出对的损失函数评估的信息来生成对抗性的例子。我们使用贝叶斯优化（BO）来专门应对涉及低查询预算的场景，以开发查询有效的对抗性攻击。 我们通过有效的升维技术缓解了围绕BO优化高维深度学习模型的问题。我们提出的方法实现了与最先进的黑盒对抗性攻击相媲美的性能，尽管其平均查询次数要低得多。特别是在低查询预算制度下，我们提出的方法比最先进的方法减少了高达80%的查询次数。
最先进的深度神经网络（DNNs）通常有数千万个参数，这些参数可能不适合内存层次结构的上层，因此大大增加了推理时间和能源消耗，并禁止它们在手机等边缘设备上使用。 因此，DNN模型的压缩最近成为了一个活跃的研究领域，其中{连接修剪}是最成功的策略之一。一个非常自然的方法是通过$ell_1$正则化来修剪DNN的连接，但最近的经验调查表明，这在DNN压缩的情况下效果并不理想。 在这项工作中，我们重新审视了这一简单的策略，并对其进行了严格的分析，以表明：(a) 无论正则化的强度如何，$ell_1$正则化的层级修剪目标的任何非零元素的数量都受惩罚性预测对数的约束；(b) 成功的修剪高度依赖于精确的优化解算器，并且在压缩速度和预测精度的扭曲之间存在着权衡，由正则化的强度控制。 因此，我们的理论结果表明，只要我们使用准确的优化求解器，$ell_1$的修剪就能成功。我们在实验中证实了这一点，我们表明，用Adamax-L1（累积）求解器进行简单的$ell_1$正则化，其修剪率就能达到最先进的水平。
机器的自主性和适应性要求它们能够测量自己的误差。我们考虑当机器必须测量回归任务中的误差时，这种方法的优点和局限性。当机器没有正确预测的地面真相时，它如何测量回归子组件的误差？ 它允许一些回归器强相关，只要没有太多的相关。然而，它的解决方案不是唯一的--这是地面真相推理解决方案的一个属性。在有可能进行误差修正的情况下，添加$ell_1$--最小化作为条件可以恢复正确的解决方案。我们简要讨论了回归器的地面真相推理的数学与分类器的数学的相似性。
我们提出了一种新的表示方法，即单像素签名，可用于揭示卷积神经网络（CNN）的特征。在这里，每个CNN分类器都与一个签名相关联，该签名是通过逐像素生成一个对抗性的值而产生的，是对类预测的最大变化的结果。 一个像素的签名与CNN架构的设计选择无关，如类型、深度、激活函数以及它们的训练方式。它可以在不访问网络参数的情况下为黑盒分类器有效计算。 经典网络如LetNet、VGG、AlexNet和ResNet在它们的签名图像中表现出不同的特征。在应用方面，我们专注于分类器后门检测问题，其中CNN分类器被恶意插入了一个未知的木马。我们展示了一像素签名在检测后门CNN方面的有效性。我们提出的一像素签名表示是通用的，它可以应用于需要描述鉴别性分类器的问题，特别是基于神经网络。
我们扩展了从演示中学习的范式，提供了一种学习跨任务共享的未知约束的方法，使用任务的演示、它们的成本函数以及系统动力学和控制约束的知识。鉴于安全的演示，我们的方法使用命中和运行采样来获得较低的成本，因此是不安全的轨迹。 安全和不安全的轨迹都被用来通过解决混合整数程序获得不安全集的一致表示。此外，通过利用约束条件的已知参数化，我们修改了我们的方法来学习高维度的参数化约束。我们表明，我们的方法可以学习一个7-DOF机器人手臂的六维姿势约束。
我们提出了一个公制学习框架，用于计算保留距离的地图，为某类流形生成低维嵌入。我们采用连体网络来解决最小二乘法多维缩放的问题，以生成流形上保留测地距离的映射。与以前的参数流形学习方法相比，我们显示了通过计算最远点采样策略中的测地距离，可以大大减少训练工作量。 此外，使用网络对保距图进行建模，降低了多维缩放问题的复杂性，与类似的非参数化对应方法相比，导致了流形的非局部泛化。我们在点云数据和图像流形上展示了我们的主张，并对我们的技术进行了数字分析，以促进对神经网络在流形数据建模中的表现力的进一步理解。
探索是强化学习的一个基本方面，通常使用随机行动选择来实现。然而，如果探索的方向是获得新的世界知识，那么探索的效率会更高。访问计数器在实践和理论上都被证明对定向探索是有用的。 我们提出了$E$值，一种计数器的泛化，可用于评估在状态-动作轨迹上传播的探索值。我们将我们的方法与常用的RL技术进行了比较，并表明使用$E$值比传统的计数器提高了学习和性能。我们还表明我们的方法可以通过函数近似来实现，以有效地学习连续的MDP。我们通过表明我们的方法在Freeway Atari 2600游戏中超过了最新的性能来证明。
深度神经进化和深度强化学习（深度RL）算法是两种流行的策略搜索方法。前者适用范围广，相当稳定，但受制于低样本效率。相比之下，后者的样本效率更高，但样本效率最高的变体也相当不稳定，对超参数设置高度敏感。 在本文中，我们提出了一个不同的组合方案，使用简单的交叉熵法（CEM）和Twin Delayed Deep Deterministic Policy Gradient（TD3），这是另一种非政策性的深度RL算法，比DDPG有所改进。 我们在一组经典的深度强化学习基准上对所产生的方法CEM-RL进行了评估。我们表明，CEM-RL比其竞争对手有几个优势，并在性能和样本效率之间提供了令人满意的折衷。
深度强化学习的最新进展在围棋和Atari游戏等应用的性能方面取得了重大进展。然而，开发实用的方法来平衡复杂领域的探索和利用，在很大程度上仍未解决。Thompson采样及其对强化学习的扩展提供了一种优雅的探索方法，只需要访问模型的后验样本。 因此，在Thompson Sampling框架中考虑近似贝叶斯神经网络是很有吸引力的。为了了解使用近似后验对Thompson Sampling的影响，我们在一系列情境匪徒问题上对已建立的和最近开发的近似后验采样与Thompson Sampling相结合的方法进行了比较。我们发现，许多在监督学习环境中成功的方法在顺序决策场景中表现不佳。
在本文中，我们从数学角度研究了目前利用类标签信息的GANs变体的属性。通过类意识梯度和交叉熵分解，我们揭示了类标签和相关损失如何影响GAN的训练。 我们进行了全面的实验来验证我们的分析和评估我们解决方案的有效性，其中AM-GAN优于其他强大的基线，并在CIFAR-10上取得了最先进的Inception Score（8.91）。 此外，我们证明，在Inception ImageNet分类器中，Inception Score主要跟踪生成器的多样性，然而，没有可靠的证据表明它可以反映真实的样本质量。我们因此提出了一个新的指标，称为AM Score，以提供对样本质量更准确的估计。
现代神经网络是过度参数化的。特别是，每个整流的线性隐藏单元可以通过调整输入和输出的权重，在不改变网络其他部分的情况下，以一个倍数的因素进行修改。受Sinkhorn-Knopp算法的启发，我们引入了一个快速迭代方法，以最小化权重的l2准则，相当于权重衰减正则。 它可以证明收敛到一个唯一的解决方案。在训练期间，我们的算法与SGD交织在一起，提高了测试精度。对于小批量，我们的方法在CIFAR-10和ImageNet的ResNet-18上提供了一个替代批量和群组规范化的方法。
在这项工作中，我们考虑了代理人在环境设置上的最坏情况分析，以检测代理人是否在某些方向上未能泛化。具体而言，我们考虑了一个三维第一人称任务，其中代理人必须浏览程序生成的迷宫，并且强化学习代理人最近已经实现了人类水平的平均性能。 通过对迷宫的结构进行优化，我们发现代理可以遭受灾难性的失败，甚至在令人惊讶的简单迷宫中也无法找到目标，尽管他们有令人印象深刻的平均表现。此外，我们发现这些失败在不同的代理之间，甚至在明显不同的架构之间转移。 我们认为，我们的发现突出了最坏情况分析在识别代理是否有泛化失败的方向上的重要作用。我们的希望是，自动识别泛化失败的能力将促进开发更普遍和更强大的代理。
为了解决这个问题，有人建议用一个伪似然来代替似然，也就是享有适当鲁棒性的损失函数的指数。在本文中，我们建立了一个基于最大平均差异的伪似然，通过将概率分布嵌入再现核希尔伯特空间来定义。 我们表明，这个MMD-Bayes后验是一致的，并且对模型的错误描述是稳健的。由于以这种方式获得的后验可能是难以实现的，我们也证明了这个后验的合理变异近似值享有同样的特性。我们提供了计算这些变异近似值的随机梯度算法的细节。数字模拟确实表明，我们的估计器比基于似然的估计器对错误描述更加稳健。
用于训练潜变量模型的标准变异下限对大多数感兴趣的数量产生了有偏见的估计。我们为潜变量模型引入了基于无限序列随机截断的对数边际似然及其梯度的无偏见估计。如果通过编码器-解码器架构来设定参数，编码器的参数可以被优化以最小化其对该估计的方差。 我们表明，在相同的平均计算成本下，使用我们的估计器训练的模型比基于重要性抽样的标准方法有更好的测试集似然性。这个估计器也允许将潜变量模型用于无偏估计器而不是边际似然下限的任务，如最小化反向KL发散和估计分数函数。
本文对理解随机梯度下降的超参数如何影响神经网络的最终训练损失和测试精度做出了两项贡献。首先，我们认为随机梯度下降表现出两种具有不同行为的制度；一种是噪声主导的制度，通常出现在小批量或中等规模的情况下，另一种是曲率主导的制度，通常出现在大批量的情况下。 在噪声主导的体系中，最佳学习率随着批处理规模的增加而增加，在恒定的历时预算下，训练损失和测试精度与批处理规模无关。在曲率主导的体系中，最佳学习率与批处理规模无关，训练损失和测试精度随着批处理规模的增加而降低。 我们通过对一系列架构的实验来支持这些说法，包括ResNets、LSTM和自动编码器。我们总是对所有批次大小的学习率进行网格搜索。其次，我们证明小的或中等大小的批次在测试集上继续优于非常大的批次，即使两个模型的训练步骤相同，达到类似的训练损失。 此外，当在CIFAR-10上以恒定的64个批次规模训练Wide-ResNets时，当历时预算增加128倍时，使测试精度最大化的最佳学习率只下降了2倍，而使训练损失最小化的最佳学习率下降了16倍。这些结果证实，随机梯度的噪声可以引入有益的隐性正则化。
反事实后悔最小化（CFR）是在不完全信息博弈中寻找近似纳什均衡的最成功的算法。然而，CFR对完整博弈树遍历的依赖限制了其可扩展性和通用性。 因此，游戏的状态和行动空间通常被抽象化（即简化），然后将得到的策略映射回完整的游戏。这需要大量的专业知识，在扑克以外的许多游戏中并不实用，而且往往收敛于高度可利用的政策。 最近提出的一种方法，即深度CFR，将深度学习直接应用于CFR，允许代理从样本中对状态空间进行内在的抽象和概括，而不需要专家知识。在本文中，我们介绍了单一深度CFR（SD-CFR），这是深度CFR的一个变种，通过避免平均策略网络的训练，具有较低的整体近似误差。我们表明，从理论角度来看，SD-CFR更具吸引力，在扑克的可利用性和一对一游戏方面，经验上优于深度CFR。
虽然生成模型在生成以低维描述符为条件的高维样本方面显示出巨大的成功（例如在MNIST中学习笔画厚度，在CelebA中学习头发颜色，或在Wavenet中学习说话者身份），但它们的样本外生成带来了基本问题。 条件变异自动编码器（CVAE）作为一个简单的条件生成模型，在训练过程中没有明确地将条件联系起来，因此，没有动力去学习一个跨条件的紧凑的联合分布。 这为重建同一条件下的样本和转换不同条件下的样本引入了强大的正则化，从而大大改善了泛化效果。 特别是，我们显示了基于高维单细胞基因表达数据的细胞扰动对治疗和疾病反应的预测有了质的提高，解决了以前有问题的少数类别和多种条件。对于一般的任务，我们将高维估计的平均值和方差与它们的基础事实的皮尔逊相关度分别从0.89提高到0.97和0.75提高到0.87。
我们分析了梯度下降训练深度线性神经网络的全局最优的收敛速度，通过最小化白化数据的L2损失。 当以下条件成立时，可以保证以线性速度收敛。(i) 隐层的尺寸至少是输入和输出尺寸的最小值；(ii) 初始化时的权重矩阵是近似平衡的；(iii) 初始损失小于任何等级缺陷的解决方案的损失。 对初始化的假设（条件(ii)和(iii)）是必要的，在这个意义上，违反其中任何一个条件都可能导致收敛失败。 此外，在输出维度为1的重要情况下，即标量回归，它们都得到满足，因此在随机初始化方案下，以恒定的概率收敛到全局最优。 我们的结果大大扩展了之前的分析，例如深度线性残差网络（Bartlett等人，2018）。
监督分类和一般机器学习的基本问题之一是对数据中存在的非参数不变性进行建模。大多数现有技术都侧重于以对预计存在于数据中的参数滋扰变换的不变性的形式强制执行先验。 该层可以学习非参数变换的不变性，有趣的是，它激励并结合了永久随机连接体，被称为永久随机连接体非参数变换网络（PRC-NPTN）。PRC-NPTN网络是用随机连接（不仅仅是权重）初始化的，它是全连接卷积层中连接的一个小子集。重要的是，PRC-NPTN的这些连接一旦初始化，在整个训练和测试中保持永久。 我们认为随机初始化的连接是一种简单的方法，可以从数据本身学习不变性，同时对多个滋扰变换进行不变性的学习。我们发现这些随机初始化的永久连接对泛化有积极的影响，在强制从数据本身学习不变性的基准上，超过了更大的ConvNet基线和最近提出的非参数变换网络（NPTN）。
我们提出了一个完全卷积的条件生成模型--潜变神经网络（LTNN），能够使用适合实时应用的轻量级神经网络进行视图合成。与现有的通过连接纳入条件信息的条件生成模型相比，我们引入了一个专门的网络组件--条件转换单元（CTU），旨在学习对应于指定目标视图的潜变空间。 此外，我们还定义了一个一致性损失项来指导网络学习所需的潜空间映射，构建了一个任务划分的解码器来完善生成视图的质量，并引入了一个自适应判别器来改善对抗性训练过程。 所提出的方法的通用性在三个不同任务的集合上得到了证明：真实手部深度图像的多视图重建、真实和合成脸部的视图合成以及刚性物体的旋转。所提出的模型在每个类别中都超过了最先进的结果，同时实现了推理所需的计算需求平均减少30%。
我们描述了三种方法，使一个计算量极其有限的嵌入式调度器能够根据资源可用性考虑少量的替代活动。 最后一种方法是通过多次调用调度器的替代活动来模拟回溯。我们在美国宇航局下一个行星探测器的任务场景（称为溶胶类型）上对这些技术进行了评估，这些技术正在被评估为纳入机载调度器中。
受生物神经元的模块化和生命周期的启发，我们介绍了通过神经修剪持续学习（CLNP），这是一种新的方法，旨在基于修剪低活性的神经元来实现固定容量模型的终身学习。 随后的任务在重新初始化后使用这些修剪过的神经元进行训练，并对以前的任务性能造成零的恶化。我们通过经验表明，这种受生物启发的方法导致最先进的结果，击败或匹配目前计算复杂性更高的方法。
在本文中，我们实证研究了提高卷积神经网络（CNN）在频谱音频特征上的性能的不同方法。更具体地说，我们探索了CNN设计的三个方面：网络的深度、使用残余块以及使用分组卷积，以及随时间变化的全局聚合。 应用背景是歌手分类和歌唱表演嵌入，我们相信结论可以扩展到使用卷积神经网络的其他类型的音乐分析。结果显示，全局时间聚集最有助于提高CNN的性能。本文的另一个贡献是发布了一个可用于训练和评估的歌唱录音数据集。
Softmax函数被用于几乎所有现有的用于语言生成的序列到序列模型的最后一层。然而，它通常是计算速度最慢的一层，它将词汇量限制在最频繁类型的子集上；并且它的内存占用很大。 我们提出了一种用连续嵌入层替代softmax层的通用技术。我们的主要创新是一种新型的概率损失，以及一种训练和推理程序，在这种程序中，我们对预先训练的词嵌入产生一种概率分布，而不是对通过softmax获得的词汇产生一种多叉型分布。 我们在神经机器翻译任务上评估了这一类新的具有连续输出的序列到序列模型。我们表明，我们的模型在训练时间上获得了高达2.5倍的速度，同时在翻译质量上与最先进的模型表现相当。这些模型能够处理非常大的词汇而不影响翻译质量。它们还产生了比基于softmax的模型更有意义的错误，因为这些错误通常位于参考翻译的向量空间的一个子空间。
我们提出了一个框架来理解使用场理论的深度神经网络前所未有的性能和鲁棒性。同一层内的权重之间的相关性可以通过该层的对称性来描述，如果这种对称性被打破以减少权重的冗余，那么网络的概括性会更好。 使用双参数场理论，我们发现网络可以在训练结束时自己打破这种对称性，这个过程在物理学中通常被称为自发对称性打破。这相当于网络在没有任何用户输入层打破对称性的情况下，通过与相邻层的交流来概括自己。在适用于残余网络的层解耦极限中（He et al, 2015），我们表明在非线性层生存的残余对称性会根据经验结果自发地被打破。非线性层和权重层的拉格朗日与标量的量子场理论中的拉格朗日有惊人的相似之处。利用量子场理论的结果，我们表明我们的框架能够解释许多实验上观察到的现象，如在随机标签上训练的零误差（Zhang et al, 2017），信息瓶颈和走出瓶颈的相变（Shwartz-Ziv & Tishby, 2017），破碎的梯度（Balduzzi等人，2017）等等。
在过去的几年里，人工智能领域取得了显著的突破，这要归功于人工深层神经网络，它在计算机视觉、自然语言处理、语音识别、恶意软件检测等许多机器学习任务中取得了巨大的成功，然而，它们对容易制作的对抗性例子非常脆弱。 许多调查已经指出了这一事实，并提出了不同的方法来产生攻击，同时对原始数据进行有限的扰动。到目前为止，最强大的已知方法是所谓的C&W攻击[1]。 在本文中，我们提出了一种新的方法，我们称之为中心初始攻击（CIA），其优点是双重的：首先，它通过构造确保最大扰动小于事先固定的阈值，而没有降低攻击质量的剪切过程。 虽然它的应用不限于图像，但我们用目前最好的分类ImageNet数据集中的五个来说明这一点，其中两个是为了稳健地抵御攻击而进行的对抗性重新训练。
回答大规模不完整知识图谱（KGs）上的复杂逻辑查询是一项基本但具有挑战性的任务。最近，解决这个问题的一个有希望的方法是将KG实体以及查询嵌入到一个矢量空间中，这样回答查询的实体就被嵌入到查询附近。然而，先前的工作将查询建模为矢量空间中的单点，这是有问题的，因为一个复杂的查询代表了其答案实体的潜在大集合，但不清楚这样一个集合如何能被表示为一个单点。 此外，以前的工作只能处理使用连词（$/wedge$）和存在量词（$/exists$）的查询，处理带有逻辑非连词（$/vee$）的查询仍然是一个开放的问题。我们的主要观点是，查询可以被嵌入为盒子（即超矩形），盒子内的一组点对应于查询的一组答案实体。我们表明，连词可以自然地表示为盒子的交叉点，并且还证明了一个否定的结果，即处理非连词需要嵌入与KG实体的数量成比例的维度。 然而，我们表明，通过将查询转化为Disjunctive Normal Form，query2box能够以可扩展的方式处理带有$\wedge$、$\vee$、$\exists$的任意逻辑查询。我们在两个大型KG上展示了query2box的有效性，并表明query2box比现有技术水平实现了高达25%的相对改进。
在对可并行的超参数优化方法的需求驱动下，本文研究了开环搜索方法：在评估单一配置之前，预先确定并可生成序列。例子包括网格搜索、均匀随机搜索、低差异序列和其他采样分布。 我们描述了一种将超参数搜索空间转换为有效使用k-DPP的方法。此外，我们还介绍了一种新的Metropolis-Hastings算法，该算法可以从定义在任何可以提取均匀样本的空间上的k-DPP中取样，包括具有离散和连续维度或树状结构的混合空间。我们的实验表明，在现实场景中，无论是串行还是并行，在训练监督学习者的有限预算下都有显著的好处。
图嵌入技术已经越来越多地被部署在涉及非欧几里得数据学习的众多不同应用中。然而，现有的图嵌入模型要么在训练过程中未能纳入节点属性信息，要么受到节点属性噪声的影响，从而影响了准确性。 此外，由于其高计算复杂性和内存使用量，很少有模型能扩展到大型图形。在本文中，我们提出了GraphZoom，一个多层次的框架，用于提高无监督图形嵌入算法的准确性和可扩展性。GraphZoom首先进行图形融合，生成一个新的图形，有效地编码原始图形的拓扑结构和节点属性信息。 然后，通过合并具有高光谱相似性的节点，将这个融合图反复粗化为一个更小的图。GraphZoom允许任何现有的嵌入方法应用于粗化的图，然后它逐步将在最粗级别获得的嵌入细化为越来越细的图。我们在一些流行的图数据集上评估了我们的方法，用于归纳和感应任务。
深度生成模型在为连续数据建模方面取得了成功。然而，用正式的语法和语义来捕捉离散结构的表示仍然具有挑战性，例如。在编译器理论的启发下，语法和语义的检查是通过语法定向翻译（SDT）完成的，我们通过引入随机的懒惰属性，提出了一种新颖的语法定向变异自动编码器（SD-VAE）。这种方法将离线的SDT检查转化为即时生成的指导，用于约束解码器。 与最先进的方法相比，我们的方法对输出空间进行了约束，使输出不仅在语法上有效，而且在语义上也是合理的。我们用编程语言和分子中的应用来评估所提出的模型，包括重建和程序/分子优化。
在自然语言中进行非正式推理建模是非常具有挑战性的。随着最近大量注释数据的出现，训练复杂的模型如神经网络来进行自然语言推理（NLI）变得可行，这些模型已经取得了最先进的性能。虽然存在相对大量的注释数据，但机器能从数据中学习所有需要进行NLI的知识吗？ 如果不能，NLI模型如何从外部知识中获益，以及如何建立NLI模型来利用它？在本文中，我们旨在通过用外部知识来丰富最先进的神经自然语言推理模型来回答这些问题。我们证明，在斯坦福自然语言推理（SNLI）数据集上，拟议的带有外部知识的模型进一步改善了技术水平。
高类内多样性和类间相似性是遥感场景图像数据集的一个特点，目前给深度学习算法的分类任务带来了很大的困难。为了提高准确性，已经提出了分类后方法来平滑模型预测的结果。然而，这些方法需要一个额外的神经网络来执行平滑操作，这增加了任务的开销。 我们的方法利用一个连体网络来提高卷积神经网络对一组相邻场景图像的判别能力，然后利用这对图像之间的语义一致性来丰富我们想要预测标签的图像的特征向量。实证结果表明，这种方法为现有方法提供了一个可行的替代方案。 例如，在一项疾病密度估计任务中，我们的模型比基线提高了1个百分点的预测精度，并将平均平方误差值降低了0.02。
稀疏的数据点会导致有限差分的数值误差，从而阻碍物理系统的动态建模。当稀疏的数据是不规则分布的，从而使数据定义在非结构化的网格上时，离散化误差变得更大，从而难以建立深度学习模型来处理非结构化网格上的物理治理观测。 在本文中，我们提出了一个名为物理感知差分图网络（PA-DGN）的新架构，利用相邻信息来学习受物理方程启发的有限差分。PA-DGN进一步利用数据驱动的端到端学习来发现给定观测中的空间和时间差异之间的潜在动态关系。我们证明了PA-DGN在合成数据和来自气象站的真实世界气候观测上的方向性导数近似和图形信号预测的优越性。
解决具有稀疏奖励的环境中的长跨度连续决策任务是强化学习（RL）研究中的一个长期问题。分级强化学习（HRL）有希望通过在不同的时间抽象层次上的操作来提高RL代理的能力。尽管最近的工作在处理固有的非平稳性和样本复杂性方面取得了成功，但它仍然难以推广到未见过的环境，并将政策的不同层转移到其他代理。 在本文中，我们提出了一种新型的HRL架构，即分层分解强化学习（HiDe），它允许将分层分解为独立的子任务，但允许以端到端的方式对所有层进行联合训练。主要的见解是将低层的控制策略与高层的基于图像的规划策略相结合。我们在各种复杂的连续控制导航任务中评估了我们的方法，证明可以实现跨环境的泛化和高层策略的转移。见视频 https://sites.google.com/view/hide-rl
声音对应模式在语言学重建中起着至关重要的作用。语言学家用它们来证明语言关系，重建原形，以及基于共享创新的经典系统发育重建。同源词如果不能符合预期的模式，就会进一步指出声音变化中的各种例外，如频繁的单词的类比或同化。 其核心思想是将所有的同义词集的列表示为网络中的节点，其边表示节点之间的兼容程度。然后，推断所有兼容的对应集的任务可以作为图论中著名的最小悬崖覆盖问题来处理，该问题主要是寻求将图分成最小数量的悬崖，其中每个节点正好由一个悬崖代表。 通过排除那些只出现在少数认知集中的模式，可以推断出定期重复出现的声音对应关系的核心。基于这种想法，本文提出了一种自动对应模式识别的方法，它作为补充论文的Python库的一部分来实现。 为了说明该方法的实用性，提出了各种测试，并提供了该方法输出的具体例子。除了源代码，该研究还补充了一个简短的互动教程，说明如何使用新方法和如何检查其结果。
我们描述了Kernel RNN Learning (KeRNL)，这是一种基于时序资格跟踪的减秩近似方法，用于训练循环神经网络(RNN)，在长时间依赖性任务上具有与BPTT竞争的性能。该近似方法用灵敏度权重和时序资格跟踪的简单减秩乘积取代了描述过去隐藏单元激活如何影响当前状态的等级4梯度学习张量。 在这个以节点扰动为动机的结构化近似中，敏感性权重和资格内核时间尺度本身是通过应用扰动来学习的。该规则代表了向生物上可信的或受神经启发的ML迈出的另一步，在放松的架构要求（没有对称的返回权重）、较小的内存需求（没有随时间展开和存储的状态）和较短的反馈时间方面具有较低的复杂性。
我们提出了Deep Graph Infomax（DGI），这是一种以无监督的方式在图结构数据中学习节点表征的通用方法。DGI依赖于最大化补丁表征和相应的图的高层总结之间的相互信息--两者都是使用既定的图卷积网络架构得出的。所学的补丁表征总结了以感兴趣的节点为中心的子图，因此可以重新用于下游的节点学习任务。 与之前大多数用GCN进行无监督学习的方法相比，DGI不依赖于随机行走目标，并且很容易适用于过渡性和归纳性学习设置。我们在各种节点分类基准上展示了有竞争力的性能，有时甚至超过了监督学习的性能。
在许多环境中，所有状态中只有一个极小的子集产生高回报。 在这些情况下，很少有与环境的相互作用能提供相关的学习信号。因此，我们可能想优先训练那些高回报的状态和导致它们的可能轨迹。为此，我们主张使用一个{回溯模型}来预测终止于某一高回报状态的前面状态。 我们可以训练一个模型，从一个高价值的状态（或一个估计有高价值的状态）开始，预测和采样哪些（状态，行动）图元可能导致该高价值的状态。这些（状态，行动）对的痕迹，我们称之为回忆痕迹，从高价值的状态开始，从这个回溯模型中采样，是有信息的，因为它们终止于好的状态，因此我们可以使用这些痕迹来改进政策。 我们为这一想法提供了一个变异解释，并提供了一个实用的算法，其中回溯模型从导致大额奖励的轨迹的近似后验分布中取样。我们的方法在几种环境和任务中改善了政策内和政策外RL算法的样本效率。 
介词是最常见的词汇之一。良好的介词表示法在计算语言学中具有极大的句法和语义意义。现有的介词表示法要么将介词视为内容词（例如。word2vec和GloVe），或者严重依赖外部语言资源，包括句法分析、训练任务和数据集的特定表示。在本文中，我们使用词三数（其中一个词是介词）来捕捉介词与其头部和子女的互动。 我们揭示了一种涉及Hadamard产品的新的几何学，并通过经验证明了其在短语动词转述中的效用。此外，我们的介词嵌入被用作两个具有挑战性的下游任务的简单特征：介词选择和介词附件歧义。我们在多个标准化数据集上取得了与现有技术水平相当或更好的结果。 
一类有前途的生成模型通过一个可逆的神经网络将点从简单分布映射到复杂分布。  这些模型的基于似然的训练需要限制它们的结构，以允许廉价计算雅各布决定因素。 另外，如果转换是由常微分方程指定的，则可以使用雅各布跟踪。在本文中，我们使用Hutchinson的跟踪估计器来给出对数密度的可扩展无偏估计。 其结果是一个具有无偏密度估计和单通道采样的连续时间可逆生成模型，同时允许不受限制的神经网络架构。我们在高维密度估计、图像生成和变异推理上证明了我们的方法，在具有高效采样的精确似然法中达到了最先进的水平。
我们提出了一种基于特征匹配的非对抗性方法来训练生成模型。我们的方法，生成特征匹配网络（GFMN），利用预训练的神经网络，如自动编码器和ConvNet分类器来进行特征提取。我们对不同的挑战性数据集，包括ImageNet，进行了大量的实验。
我们为Omniglot数据集上的K-shot分类提出了一个新的架构。在原型网络的基础上，我们将其架构扩展到我们所说的高斯原型网络。原型网络在图像和嵌入向量之间学习一个地图，并使用其聚类进行分类。 在我们的模型中，编码器输出的一部分被解释为关于嵌入点的置信区估计，并表示为高斯协方差矩阵。然后，我们的网络在嵌入空间上构建一个方向和类别相关的距离度量，使用单个数据点的不确定性作为权重。 我们表明，高斯原型网络是一个优于具有同等数量参数的香草原型网络的架构。我们报告了在Omniglot数据集上的5路和20路制度中，在1-shot和5-shot分类中与最先进的性能一致的结果。我们探索在训练集中人为地对一部分图像进行下采样，这提高了我们的性能。
我们表明，在无限多的卷积滤波器的限制下，具有适当的权重和偏置先验的（残差）CNN的输出是一个GP，扩展了密集网络的类似结果。对于CNN来说，可以精确地计算出等效的核，而且与 "深度核 "不同，它的参数非常少：只有原始CNN的超参数。 此外，我们表明这个内核有两个特性，允许它被有效地计算；对一对图像评估内核的成本类似于通过原始CNN的一次正向传递，每层只有一个过滤器。相当于32层ResNet的内核在MNIST上获得了0.84%的分类误差，这是一个具有可比参数数量的GP的新记录。
我们提出了一种端到端的设计方法，用于高效的深度学习部署。与之前单独优化神经网络架构、修剪策略和量化策略的方法不同，我们以端到端的方式联合优化它们。为了处理它带来的更大的设计空间，我们训练了一个量化感知的准确性预测器，反馈到进化搜索中以选择最佳匹配。我们首先生成一个<NN架构，ImageNet准确性>对的大型数据集，没有训练每个架构，而是通过采样一个统一的超网。 然后，我们使用这些数据来训练一个没有量化的准确性预测器，进一步使用预测器转移技术来获得量化感知的预测器，这减少了量化后的微调时间。在ImageNet上进行的大量实验显示了端到端方法的好处：它保持了与ResNet34浮动的准确性（75. 1%），同时与8位模型相比节省了2.2倍的BitOps；我们获得了与MobileNetV2+HAQ相同的精度，同时实现了2倍/1.3倍的延迟/能源节省；端到端优化比使用ProxylessNAS+AMC+HAQ的单独优化的精度高2.3%，同时减少了数量级的GPU时间和CO2排放。
分布式优化在解决大规模机器学习问题中至关重要。分布式优化技术的一个广泛共享的特征是要求所有节点在每个计算时代完成其分配的任务，然后系统才能进入下一个时代。在这种情况下，缓慢的节点，称为散兵游勇，会大大减缓进展。 结果是每个节点的minibatch大小是可变的。工作者然后得到固定的通信时间，通过几轮共识来平均他们的minibatch梯度，然后通过双重平均来更新原始变量。Anytime Minibatch防止散兵游勇阻碍系统，而不浪费散兵游勇可以完成的工作。 我们提出了收敛分析，并分析了墙体时间性能。我们的数值结果显示，我们的方法在亚马逊EC2中的速度最多可以达到1.5倍，当计算节点性能存在较大变化时，它的速度可以达到5倍。
许多机器学习图像分类器很容易受到对抗性攻击，即带有旨在故意触发错误分类的扰动的输入。目前的对抗性方法直接改变像素颜色，并针对像素规范球进行评估：根据测量规范，像素扰动小于特定的幅度。 然而，这种评估的实用性有限，因为像素空间的扰动并不对应于导致其形成的现实世界的基本现象，并且没有附加的安全动机。自然图像中的像素是与物理场景的几何形状相互作用的光的测量。 因此，我们提出了一种新的评估措施，即参数规范球，通过直接扰动影响图像形成的物理参数。我们提出的一个有利的贡献是基于物理的可微分渲染器，使我们能够将像素梯度传播到照明和几何的参数空间。我们的方法能够实现基于物理的对抗性攻击，我们的可微分渲染器利用互动渲染文献的模型来平衡性能和准确性的权衡，这是一个内存高效和可扩展的对抗性数据增强工作流程所必需的。
递归神经网络的替代品，特别是基于注意力或卷积的架构，在处理输入序列方面的势头越来越好。尽管它们具有相关性，但这些替代品的计算特性尚未得到充分探索。我们显示这两种模型都是完全基于其计算和访问数据的内部密集表示的能力而成为图灵完备的。特别是，Transformer和Neural GPU都不需要访问外部存储器来成为图灵完备的。
一个学习系统通常很难对罕见的事件进行正确的预测，对于分割算法也不例外。因此，我们希望建立一个报警系统，在分割结果可能不令人满意时发出警报。 一个合理的解决方案是将分割结果投射到一个低维的特征空间中，然后在特征空间中学习分类器/回归器来预测分割结果的质量。在本文中，我们使用形状特征来形成特征空间，形状特征是不同数据之间共享的强先验信息，因此它能够预测不同数据集上不同分割算法的分割结果质量。 当使用变异自动编码器（VAE）对分割结果进行测试时，分割结果的形状特征是通过损失函数的值来获取的。VAE只使用地面真实掩码进行训练，因此形状不好的分割结果成为VAE的罕见事件，并将导致大的损失值。利用这一事实，VAE能够检测出地面真实（GT）中正常形状分布之外的各种形状。 最后，我们学习了一维特征空间中的表征，以预测分割结果的质量。我们在最近几种用于医学分割任务的分割算法上评估了我们的报警系统。这些分割算法在不同的数据集上表现不同，但我们的系统始终对分割结果的质量提供可靠的预测。
收敛的深度网络中的线性变换显示出快速的特征值衰减。特征值的分布看起来像一个重尾分布，其中绝大多数的特征值是小的，但实际上不是零，只有少数大特征值的尖峰存在。我们使用随机近似器来生成特征值的直方图。这使我们能够研究具有数十万维的层。我们显示了分布在图像网训练过程中的变化，在所有中间层中收敛为类似的重尾谱。
捕捉长距离的特征关系一直是卷积神经网络（CNN）的核心问题。为了解决这个问题，在CNN上集成端到端可训练注意力模块的尝试非常普遍。这些工作的主要目标是考虑卷积层内的空间通道相关性来调整特征图。 在本文中，我们专注于层间关系的建模，并提出了一种新的结构，即 "递归层注意力网络"，它将特征的层次结构存储到与CNN同时传播的递归神经网络（RNNs）中，并自适应地扩展所有层的特征量。我们进一步介绍了几个结构衍生物，以证明与最近的注意力模块的兼容性和拟议网络的扩展性。递归层注意网络在使用CIFAR和ImageNet-1K 2012数据集的图像分类任务和使用Microsoft COCO 2014数据集的物体检测任务中，只需略微增加参数就能实现显著的性能提升。
我们试图为面临有限训练数据的ML方法自动生成更强大的输入特征。生物神经网络（BNNs）擅长快速学习，这意味着它们能提取高信息量的特征。特别是，昆虫嗅觉网络通过三个关键因素，非常迅速地学习新气味。在这项工作中，我们部署了MothNet，一个飞蛾嗅觉网络的计算模型，作为一个自动特征发生器。 这些 "昆虫机器人"（部分BNN和部分ML方法）在向量MNIST和Omniglot数据集上的性能明显优于单独的基线ML方法，将测试集的误差平均降低了20%到55%.MothNet特征发生器也大大优于其他特征生成方法，包括PCA、PLS和NN。
我们提出了一种在深度神经网络（DNNs）中进行随时预测的方法。对于每个测试样本，随时预测器会迅速产生一个粗略的结果，然后继续完善它，直到测试时间的计算预算耗尽。这种预测器可以通过自动调整不同的测试时间预算来解决DNNs日益严重的计算问题。 在这项工作中，我们研究了一种对前馈网络的增强，通过辅助预测和损失形成随时神经网络（ANNs）。具体来说，我们指出了最近在这种ANNs研究中的一个盲点：最终高精确度的重要性。 事实上，我们在多个识别数据集和架构上表明，通过在小的随时模型中拥有近乎最佳的最终预测，我们可以有效地将大模型的速度提高一倍，以达到相应的准确度水平。我们还组装了一个指数级加深的ANN序列，以在任何预算下实现理论上和实践上近乎最佳的随时结果，代价是额外消耗的预算的一个恒定部分。
计算成像系统联合设计计算和硬件，以检索传统的标准成像系统无法获得的信息。最近，通过经典的基于物理学的重建（称为基于物理学的网络）的解卷迭代形成的深度神经网络，对实验设计和图像先验等关键方面进行了优化。 然而，对于现实世界的大规模系统，由于图形处理单元的内存限制，通过反向传播计算梯度限制了学习。在这项工作中，我们提出了一个内存高效的学习程序，利用网络层的可逆性，实现大规模计算成像的数据驱动设计。我们在两个大规模系统上证明了我们方法的实用性：超分辨率光学显微镜和多通道磁共振成像。
现有的多代理强化学习（MARL）通信方法依赖于可信的第三方（TTP）来分配奖励给代理，使其无法适用于对等环境。(本文提出了在MARL中使用{神经元作为代理}的奖励分配，没有TTP，有两个关键的想法：（i）代理间的奖励分配和（ii）拍卖理论。 NaaA实现了对等环境中的表示交易，最终将神经网络中的单元视为代理。最后，数值实验（OpenAI Gym的单代理环境和ViZDoom的多代理环境）证实，NaaA框架优化导致强化学习的更好性能。
在对自然语言表征进行预训练时，增加模型的大小往往能提高下游任务的性能。然而，在某些时候，由于GPU/TPU内存的限制、较长的训练时间和意外的模型退化，进一步增加模型变得更加困难。为了解决这些问题，我们提出了两种参数减少技术来降低内存消耗，提高BERT的训练速度。 综合经验证据表明，我们提出的方法导致的模型与原始的BERT相比要好得多。我们还使用了一种自我监督的损失，重点是对句子间的一致性进行建模，并表明它始终有助于具有多句子输入的下游任务。因此，我们的最佳模型在GLUE、RACE和SQuAD基准上建立了新的最先进的结果，同时与BERT-大相比具有更少的参数。
根据Kaggle ML和DS调查，结构化表格数据是工业界最常用的数据形式。梯度提升树、支持向量机、随机森林和逻辑回归通常用于表格数据的分类任务。最近，使用二维词嵌入的超级字符方法在文本分类任务中取得了最先进的结果，展示了这种新方法的前景。 在本文中，我们提出了SuperTML方法，它借用了超级字符方法和二维嵌入的思想来解决表格数据的分类问题。对于每一个表格数据的输入，特征首先被投射到像图像一样的二维嵌入中，然后这个图像被送入微调的ImageNet CNN模型进行分类。实验结果表明，提出的SuperTML方法在大型和小型数据集上都取得了最先进的结果。
理论神经科学中的预测编码和机器学习中的变异自动编码器都涉及潜在的高斯模型和变异推理。虽然这些领域有一个共同的起源，但它们在很大程度上是独立发展的。我们概述了这些领域之间的联系和对比，利用它们的关系来确定机器学习和神经科学之间新的相似之处。
自适应优化方法，如AdaGrad、RMSprop和Adam，已经被提出来，以实现快速的训练过程，并在学习率上有一个元素的缩放项。虽然很普遍，但它们被观察到与SGD相比，概括性很差，甚至由于不稳定和极端的学习率而无法收敛。最近的工作提出了一些算法，如AMSGrad来解决这个问题，但它们未能取得比现有方法更多的改进。在我们的论文中，我们证明极端学习率会导致性能不佳。 我们提供了Adam和AMSGrad的新变体，分别称为AdaBound和AMSBound，它们采用了对学习率的动态约束，以实现从自适应方法到SGD的逐步平稳过渡，并给出了收敛性的理论证明。我们进一步在各种流行的任务和模型上进行了实验，这在以前的工作中往往是不够的。 实验结果表明，新的变体可以消除自适应方法和SGD之间的泛化差距，同时在训练早期保持较高的学习速度。此外，它们可以比其原型带来明显的改进，特别是在复杂的深度网络上。算法的实现可以在https://github.com/Luolc/AdaBound。
强化学习和蒙特卡洛方法中出现的一个重要问题是估计马尔科夫链的静止分布所定义的数量。在许多现实世界的应用中，对基础过渡算子的访问仅限于已经收集的一组固定的数据，而没有与环境的额外互动。我们表明，在这种情况下，一致的估计仍然是可能的，并且在重要的应用中仍然可以实现有效的估计。 我们的方法是基于估计一个比率，该比率修正了静止分布和经验分布之间的差异，从静止分布的基本属性中得出，并利用基于变分最小化的约束重构。
神经网络的力量在于它们对未见过的数据进行泛化的能力，然而这一现象的根本原因仍然难以捉摸。许多严格的尝试被用来解释泛化，但可用的界限仍然相当宽松，而且分析并不总是导致真正的理解。
我们认为对称性是解决系统性问题的一个重要考虑因素，并研究了与符号过程相关的两种形式的对称性。我们在卷积方面实现了这种方法，并表明它可以用来在三个玩具问题中实现有效的泛化：规则学习、组合和语法学习。
关键的赤道气候现象，如QBO和ENSO，从来没有被充分地解释为确定性的过程，尽管最近的研究显示有越来越多的证据表明可预测的行为。
在持续学习领域，目标是在不获取以前任务的数据的情况下，一个接一个地学习几个任务。已经提出了几个解决方案来解决这个问题，但它们通常假设用户知道在测试时对某个特定的样本执行哪个任务，或者依赖于以前数据的小样本，而且当每次只更新一批类别时，它们中的大多数都会出现准确性的大幅下降。在这篇文章中，我们提出一个新方法，OvA-INN，它能够一次学习一个类别，并且不存储任何以前的数据。 为了实现这一点，对于每个类别，我们训练一个特定的可逆神经网络来输出其类别的零向量。在测试时，我们可以通过识别哪个网络输出最小规范的向量来预测样本的类别。 这样一来，我们就能在MNIST和CIFAR-100数据集的持续学习中超越依靠特征学习的最先进的方法。在我们的实验中，在一次训练我们的模型后，我们在CIFAR-100上的准确率达到72%。
人机对话系统在自然语言处理中引起了广泛的关注。对话系统可以大致分为两类：基于检索的系统和基于生成的系统。检索系统在大型对话库中搜索用户发出的语料（即查询），并返回与查询最匹配的回复，生成方法合成新的回复。 我们提出了一种新颖的基于检索和基于生成的对话系统的组合。除了原始查询外，检索到的候选者通过神经网络被送入回复生成器，这样模型就能知道更多的信息。然后，生成的回复与检索到的回复一起参与一个重新排名的过程，以找到最终的回复输出。
在广泛的数据集上训练的深度神经网络表现出令人印象深刻的可转移性。深度特征显得很普遍，因为它们适用于许多数据集和任务。这种特性在现实世界的应用中普遍使用。一个在大型数据集上预训练的神经网络，如ImageNet，如果对一个较小的目标数据集进行微调，可以大大提升泛化能力并加速训练。 尽管它很普遍，但很少有人致力于揭示深度特征表征中可转移性的原因。本文试图从改善泛化、优化和可转移性的可行性等角度来理解可转移性。 我们证明：1）转移的模型倾向于找到更平坦的最小值，因为当转移到类似的目标数据集时，它们的权重矩阵保持在预训练参数的原始平坦区域附近；2）转移的表征通过改进Lipschitzness使损失景观更有利，这大大加快并稳定了训练。 3）可转移性的可行性与输入和标签的相似性有关。一个令人惊讶的发现是，可行性也受到训练阶段的影响，即可转移性在训练期间首先增加，然后下降。我们进一步提供理论分析来验证我们的观察。
我们解决了以下问题。具体来说，我们考虑权重空间的转换，使网络实现的功能不受影响。对于前馈结构来说，有两个这样的转换是已知的：一个层内神经元的互换，以及一个神经元的所有传入权重的正比例，加上其传出权重的反比例。在这项工作中，我们表明对于宽度不增加的结构，互换和比例实际上是唯一保留功能的重量转换。 对于任何符合条件的架构，我们给出了一个神经网络的明确构造，使得任何其他实现相同功能的网络都可以通过应用置换和重新缩放从原始网络中获得。该证明依赖于对ReLU网络的线性区域之间的边界的几何理解，我们希望开发的数学工具具有独立的意义。
最近受到广泛关注的一个普遍问题是如何在同一网络中执行多个任务，最大限度地提高效率和预测精度。一个流行的方法包括在共享主干上的多分支结构，联合训练损失的加权和。 我们的架构没有使用特定的任务分支，也没有特定的任务模块。相反，它使用了一个自上而下的调制网络，在所有的任务之间共享。代码将被发布。
聚类算法有着广泛的应用，在包括时间序列数据分析在内的数据分析领域发挥着重要作用。聚类算法的性能取决于从数据中提取的特征。然而，在时间序列分析中，一直存在着一个问题，即基于信号形状的传统方法对于相移、振幅和信号长度变化是不稳定的。 在本文中，我们提出了一种新的聚类算法，该算法侧重于使用递归神经网络和变异贝叶斯方法对信号的动态系统方面进行聚类。我们的实验表明，我们提出的算法对上述变化具有鲁棒性，并提高了分类性能。
深度学习的两个重要课题都涉及将人类纳入建模过程。以前的工作已经采取了重要措施，通过各种形式的梯度正则化将这些主题联系起来。然而，我们发现，现有的使用归因来调整模型行为与人类直觉的方法是无效的。 我们开发了一种高效的、有理论基础的特征归属方法--预期梯度，以及一个新颖的框架--归属先验，以便在训练过程中强制执行关于模型行为的先验预期。我们通过在三种不同类型的数据上进行实例化来证明归属先验是广泛适用的：图像数据、基因表达数据和健康护理数据。
递归神经网络（RNN）在处理序列数据方面表现出优异的性能。然而，由于它们的递归性质，它们既复杂又耗费内存。这些限制使得RNN难以嵌入到需要实时处理且硬件资源有限的移动设备中。 为了解决上述问题，我们引入了一种方法，可以在训练阶段学习二元和三元权重，以促进RNN的硬件实现。因此，使用这种方法可以用简单的累积取代所有的乘法累积操作，在硅面积和功耗方面给定制硬件带来显著的好处。 在软件方面，我们评估了我们的方法在各种序列模型（包括序列分类和语言建模）上使用长短期存储器（LSTM）和门控递归单元（GRU）的性能（准确性）。我们证明了我们的方法在上述任务上取得了有竞争力的结果，同时在运行期间使用二元/三元加权。 在硬件方面，我们提出了用于加速具有二元/三元权重的LSTM的循环计算的定制硬件。最终，我们表明，与全精度硬件实现设计相比，具有二元/三元权重的LSTM可以实现12倍的内存节省和10倍的推理速度。
本文讨论了无监督的少量物体识别，其中所有的训练图像都是无标签的，并且在测试中不与有标签的支持图像共享类别。我们使用一个新的类似GAN的深度架构，旨在无监督地学习一个图像表示，这将编码潜在的物体部分，从而在我们的少量识别任务中对未见过的类别有良好的概括。 我们的无监督训练整合了对抗性、自我监督和深度度量学习。我们有两个贡献。首先，我们用重建损失扩展了vanilla GAN，以强制判别器捕获从随机抽样代码产生的 "假 "图像的最相关特征。 第二，我们编制了一个三联体图像实例的训练集，用于估计公因子学习中的三联体损失，使用的图像遮蔽程序被适当地设计用来识别潜在的物体部分。因此，公因子学习确保了显示共享某些部分的类似物体类别的图像的深度表示比没有共同部分的图像表示更接近。 我们的结果表明，我们的表现明显优于现有技术水平，并且在Mini-Imagenet和Tiered-Imagenet数据集上获得了与普通偶发训练相似的完全监督的几率学习性能。
迄今为止，现有的对深度神经网络（DNN）的黑盒攻击主要集中在可转移性上，即为本地训练的模型产生的对抗性实例可以 "转移 "到攻击其他学习模型上。在本文中，我们提出了针对对抗者的新型梯度估计黑盒攻击，可以查询到目标模型的类概率，这不依赖于可转移性。 我们还提出了将生成每个对抗性样本所需的查询次数与输入的维度脱钩的策略。我们的攻击的迭代变体在对DNN的目标和非目标攻击中都实现了接近100%的对抗性成功率。 我们进行了广泛的实验，对黑盒攻击进行了全面的比较评估，并表明所提出的梯度估计攻击优于我们在MNIST和CIFAR-10数据集上测试的所有基于转移性的黑盒攻击，实现了与众所周知的、最先进的白盒攻击类似的对抗成功率。 我们还将梯度估计攻击成功地应用于Clarifai主持的真实世界的内容节制分类器。此外，我们评估了针对最先进的防御措施的黑盒攻击。我们表明，梯度估计攻击甚至对这些防御措施都非常有效。
我们提出了一种新的子图图像表示法，用于网络片段的分类，目标是它们的父网络。该图图像表示法是基于邻接矩阵的二维图像嵌入。 我们从多个数据集得出的结论是：1.与图核和基于经典特征的方法相比，使用结构化图像特征的深度学习表现最好；2.纯转移学习在用户干扰最小的情况下有效工作，对小数据具有鲁棒性。
本文探讨了视觉领域适应问题的自我组合。我们的技术来自于时空组合（Laine等人，2017）的平均教师变体（Tarvainen等人，2017），该技术在半监督学习领域取得了最先进的成果。我们为他们的方法引入了一些修改，以应对挑战性的领域适应场景，并评估其有效性。 我们的方法在各种基准中取得了最先进的结果，包括我们在VISDA-2017视觉领域适应性挑战中的获胜作品。在小型图像基准中，我们的算法不仅优于现有技术，而且还可以达到接近以监督方式训练的分类器的准确性。
人们很容易想象一个粉红色头发的人是什么样子的，即使他们以前从未见过这样的人。我们把创造新的语义概念的图像的能力称为视觉基础想象力。在本文中，我们展示了我们如何修改变异自动编码器来执行这项任务。我们的方法使用了一个新的训练目标，以及一个新的专家产品推理网络，它可以以一种原则性的和有效的方式处理部分指定（抽象）概念。 我们还提出了一套易于计算的评价指标，这些指标反映了我们对良好的视觉想象力的直观概念，即正确性、覆盖率和构成性（3个C）。最后，我们对我们的方法与现有的两种图像-属性联合VAE方法（Suzuki等人的JMVAE方法和BiVCCA方法）进行了详细比较。最后，我们将我们的方法与现有的两种图像-属性联合VAE方法（Suzuki等人的JMVAE方法和Wang等人的BiVCCA方法，2016年）进行了详细比较，将它们应用于两个数据集：MNIST-with-attributes数据集（我们在此介绍）和CelebA数据集（Liu等人，2015年）。
在SAVE中，一个关于状态动作值的先验学习被用来指导MCTS，MCTS估计出一套改进的状态动作值。 SAVE可以在任何可以访问模型的Q-learning代理上实现，我们通过将其纳入执行具有挑战性的物理推理任务和Atari的代理来证明这一点。SAVE始终以较少的训练步骤获得较高的奖励，并且--与典型的基于模型的搜索方法相反--在非常小的搜索预算下产生强大的性能。
两个主要的强化学习算法系列，Q-learning和策略梯度，最近被证明在一部分使用softmax松弛，另一部分使用entropic正则化时是等价的。我们将这一结果与著名的Shannon entropy和softmax函数的凸对偶性联系起来。
计算机模拟为训练机器人控制策略以实现复杂的任务（如运动）提供了一种自动和安全的方法。然而，由于两种环境之间的差异，在模拟中训练的策略通常不能直接转移到真实的硬件上。使用领域随机化的转移学习是一种有前途的方法，但它通常假设目标环境接近训练环境的分布，因此严重依赖准确的系统识别。 其关键思想是，我们不是在模拟中学习单一的政策，而是同时学习表现出不同行为的政策家族。当在目标环境中测试时，我们直接根据任务表现在家族中寻找最佳政策，而不需要识别动态参数。我们在训练和测试环境中具有不同差异的五个模拟机器人控制问题上评估了我们的方法，证明我们的方法与训练稳健政策或自适应政策相比可以克服较大的建模误差。
我们提出了vq-wav2vec，通过wav2vec式的自监督语境预测任务来学习音频片段的离散表征。该算法使用gumbel softmax或在线k-means聚类来量化密集表征，离散化使得NLP社区的算法可以直接应用，这些算法需要离散输入。
深度强化学习算法带来的代理可以解决复杂环境中的困难决策问题。然而，许多困难的多代理竞争游戏，特别是实时战略游戏仍然被认为超出了当前深度强化学习算法的能力，尽管最近有一种努力来改变这种情况\citep{openai_2017_dota, vinyals_2017_starcraft}。 此外，当竞争性游戏中的对手是次优的时候，目前的/textit{Nash Equilibrium}寻求、自我发挥的算法往往无法将其策略泛化到与自己的策略大相径庭的对手身上。这表明，一种超越传统自我发挥的学习算法是必要的。 我们开发了HASP（Hierarchical Agent with Self-play），这是一种获得分层结构策略的学习方法，通过使用我们从Counter Self-Play（CSP）中获得的多样化的子策略池，在竞争性游戏中可以获得比传统的自我游戏更高的性能。我们证明，在面对使用次优策略的未知对手时，HASP生成的集合策略可以获得更好的性能。 在一个激励性的迭代石头剪刀布游戏和一个部分可观察的实时战略游戏（http://generals.io/）中，我们得出结论，HASP可以比传统的自我游戏表现得更好，并且在与FloBot（一个在线排行榜上排名第二的开源代理）的比赛中取得77%的胜利率。
我们为神经语言建模引入了自适应输入表征，它将Grave等人（2017）的自适应softmax扩展到可变容量的输入表征。在如何对输入和输出层进行因子化，以及是否对单词、字符或子词单元进行建模方面有几种选择。我们对自注意力架构的流行选择进行了系统比较。 我们的实验表明，配备自适应嵌入的模型比流行的字符输入CNN的训练速度快两倍以上，同时参数数量更少。在WikiText-103基准测试中，我们取得了18.7的困惑度，与之前公布的最佳结果相比，提高了10.5的困惑度；在Billion Word基准测试中，我们取得了23.02的困惑度。
在本文中，我们考虑了在遮挡下检测物体的问题。大多数物体检测器将边界盒回归作为一个单模态任务（即。然而，我们观察到，被遮挡的物体的边界可以有多种合理的配置，而且，被遮挡的边界与可见的边界有相关性。在这两个观察结果的激励下，我们提出了一个用于遮挡下边界盒回归的深度多变量高斯混合模型。 从数量上看，我们的模型在CrowdHuman和MS-COCO上的AP分别提高了3.9%和1.2%，而且几乎没有计算或内存开销。从质量上看，我们的模型享有可解释性，因为我们可以通过协方差矩阵和混合成分来解释所得到的边界盒。
抗辩例子是机器学习模型的一个普遍现象，在这种情况下，对输入的看似难以察觉的扰动会导致对其他统计上准确的模型的错误分类。 对抗性训练是对对抗性例子最成功的经验防御之一，指的是在几何约束集内产生的对抗性例子上进行训练。最常用的几何约束是一个半径为$L_p$的球，在某种规范下为$epsilon$。 我们引入了Voronoi约束的对抗性训练，它用训练集中每一个点的Voronoi单元代替了$L_p$-球的约束。我们表明，Voronoi约束的对抗性训练产生了稳健的模型，这些模型在MNIST上的表现大大超过了最先进的水平，在CIFAR-10上也有竞争力。
最近的研究工作使人们能够研究在照片现实的环境中以自然语言为基础的导航，例如。然而，现有的方法往往在已见的环境中过度拟合训练数据，而在以前未见的环境中却不能很好地概括。 为了缩小已见和未见环境之间的差距，我们旨在从两个新的角度学习一个可推广的导航模型：（1）我们引入了一个多任务导航模型，可以在视觉语言导航（VLN）和对话历史导航（NDH）任务中无缝训练，这得益于更丰富的自然语言指导，并有效地在不同的任务中转移知识；（2）我们建议学习环境无关的导航策略表示，这些表示在环境中是不变的，因此在未见环境中推广得更好。 广泛的实验表明，我们的环境无关的多任务导航模型大大减少了已见和未见环境之间的性能差距，在未见环境中的表现比VLN的基线高出16%（相对于成功率的衡量），在NDH的基线高出120%（目标进度），为NDH任务建立了新的技术水平。
在本文中，我们建议在多类或多标签学习环境中使用Wasserstein（W.）边界中心进行模型集合。我们展示了Wasserstein合集在基于属性的分类、多标签学习和图像标题生成中的应用。这些结果表明，W.合集是基本的几何或算术平均合集的一个可行替代方案。
虽然深度学习在具有大量精心策划的标签数据集的建模任务中取得了令人难以置信的成功，但它在具有有限标签数据的问题上的应用仍然是一个挑战。本工作的目的是通过结合多任务学习和无标签数据的自我监督学习，提高在音频数据上运行的大型神经网络的标签效率。我们训练了一个基于WaveNet的端到端音频特征提取器，该提取器被输入简单但多功能的特定任务神经网络。 我们描述了几个容易实现的自监督学习任务，这些任务可以在任何大型的、未标记的音频语料库上运行。我们证明，在标记训练数据有限的情况下，通过与这些额外的自监督任务同时训练，可以显著提高三个不同的监督分类任务的单独性能，最高可达6%。我们还表明，将数据增强纳入我们的多任务设置，可以进一步提高性能。
最近的神经网络和语言模型已经开始依赖于具有极大数量类别的softmax分布。在这种情况下，计算softmax归一化常数的成本过高，这促使越来越多的关于可有效计算但有偏见的softmax估计的文献。 在本文中，我们提出了第一种用于最大化softmax可能性的无偏算法，其每次迭代的工作量与类别和数据点的数量无关（并且不需要在每个历时结束时进行额外的工作）。我们将我们的无偏方法的经验性能与七个真实世界数据集上的最先进性能进行了比较，它们全面超越了所有竞争对手。
在离散输入（如文本序列）上制作对抗性例子与在连续输入（如图像）上生成这样的例子有着本质的区别。本文试图回答这样一个问题：在黑箱设置下，我们能否自动创建对抗性例子，通过做出不易察觉的改变来有效愚弄文本上的深度学习分类器？我们的答案是肯定的。 以前的努力主要是利用梯度证据，由于自动寻找最近的邻接词（就意义而言）是很困难的，或者严重依赖手工制作的语言规则，所以效果不大。相反，我们使用蒙特卡洛树搜索（MCTS）来寻找最重要的几个词来扰乱，并通过用相同形状的符号替换每个选定的词中的一个字符来进行同形攻击。 我们的新算法，我们称之为MCTSBug，是黑箱的，同时也是非常有效的。我们的实验结果表明，MCTSBug可以在七个大规模基准数据集上愚弄深度学习分类器，成功率为95%，只需扰乱几个字符。 令人惊讶的是，MCTSBug在完全不依赖梯度信息的情况下，比基于梯度的白盒基线更有效。由于同音字攻击的性质，产生的对抗性扰动对人眼来说几乎是不可察觉的。
  我们介绍了一个模型，它可以学习将简单的手绘转换为用LaTeX子集编写的图形程序~该模型结合了深度学习和程序合成的技术。 我们学习了一个卷积神经网络，它提出了解释图像的合理的绘画基元。这些绘画基元就像一个图形程序发出的基元命令集的痕迹。我们学习了一个模型，它使用程序合成技术从该痕迹中恢复一个图形程序。这些程序具有像变量绑定、迭代循环或简单的条件类型的结构。 总的来说，这些结果是朝着从感知输入中诱导出有用的、人类可读的程序的代理迈出了一步。
对当代神经网络来说，对抗性例子仍然是一个问题。本文借鉴了模型校准中的背景检查（Perello-Nieto等人，2016）技术，协助两类神经网络检测对抗性例子，使用logit值之间的一维差异作为基础措施。这个方法有趣地倾向于在用大扰动向量生成的图像集上实现最高的平均召回率，这与现有文献中的对抗性攻击不同（Cubuk等人。2017）。所提出的方法在训练时不需要知道攻击参数或方法，与大量使用基于深度学习的方法来检测对抗性例子的文献不同，如Metzen等人（2017），为所提出的方法注入了额外的灵活性。
我们提出了一种新颖的多任务训练方法来学习文本的多语言分布式表征。我们的系统通过训练一个多语言跳格模型和一个跨语言的句子相似性模型来共同学习单词和句子嵌入。我们通过用LSTM处理单词嵌入并对输出进行平均来构建句子嵌入。 我们的架构可以透明地使用单语和句子对齐的双语语料来学习多语嵌入，从而涵盖了比单独的双语语料的词汇量大得多的词汇量。
生成对抗网络（GANs）是一个非常强大的生成建模框架。然而，它们通常很难训练，而且GANs的学习经常变得不稳定。Wasserstein GAN（WGAN）是一个有希望处理不稳定问题的框架，因为它具有良好的收敛特性。WGAN的一个缺点是，它在双域中评估Wasserstein距离，这需要一些近似，所以它可能无法优化真正的Wasserstein距离。 在本文中，我们提出在原始域中有效地评估精确的经验最优传输成本，并对其导数进行梯度下降，以训练生成器网络。在MNIST数据集上的实验表明，我们的方法明显稳定地收敛，并在生成图像的一些清晰度的代价下实现了WGAN变体中最低的Wasserstein距离。在8-高斯玩具数据集上的实验表明，在我们的方法中可以获得更好的生成器梯度。此外，所提方法能够比WGAN更灵活地生成模型。
神经结构搜索（NAS）是一个令人兴奋的新领域，有望像2012年卷积神经网络一样改变游戏规则。尽管许多伟大的工作导致了各种任务的实质性改进，但不同方法之间的比较仍然是一个非常开放的问题。虽然大多数算法在相同的数据集上进行测试，但没有共同的实验协议，因此，由于消融研究的使用不足，对为什么某些方法比其他方法更有效缺乏明确的认识。 我们的第一个贡献是在5个数据集上的8个NAS方法的基准。为了克服比较具有不同搜索空间的方法的障碍，我们建议使用一个方法对随机抽样的平均架构的相对改进，这有效地消除了专家设计的搜索空间或训练协议产生的优势。 我们用常用的DARTS搜索空间进行了进一步的实验，以了解NAS管道中每个组件的贡献。 这些实验强调：(i)在评估协议中使用技巧对报告的架构性能有主要影响；(ii)基于单元的搜索空间有一个非常狭窄的精度范围，因此种子对架构排名有相当大的影响；(iii)手工设计的宏观结构（单元）比搜索的微观结构（操作）更重要；(iv)深度差距是一个真实的现象，8到20个单元架构之间的排名变化就是证明。 最后，我们提出了最佳做法，我们希望这些做法对社区有用，并有助于减轻目前NAS的缺陷，例如在可重复性和搜索方法的比较方面的困难。所用代码可在https://github.com/antoyang/NAS-Benchmark。
在没有监督的情况下识别跨域的类比是人工智能的一项关键任务。最近在跨域图像映射方面的进展集中在跨域图像的翻译上。虽然取得的进展令人印象深刻，但视觉保真度很多时候并不足以识别来自另一域的匹配样本。在本文中，我们解决的正是寻找数据集之间精确类比的任务，即为A域的每张图像找到B域的类似图像。我们进一步表明，跨域映射任务可以分成两部分：域对齐和学习映射函数。这些任务可以迭代解决，随着对齐的改进，无监督翻译函数的质量可以与完全监督相媲美。
有效地推断短文的辨别性和连贯性的潜在话题是许多现实世界应用的关键任务。然而，由于短文的特点所引起的数据稀少问题，这项任务被证明是对传统话题模型的巨大挑战。此外，复杂的推理算法也成为这些传统模型快速探索变化的瓶颈。 在本文中，我们提出了一个新的模型，称为神经变异稀疏主题模型（NVSTM），该模型基于一个名为稀疏主题编码（STC）的稀疏性增强的主题模型。在该模型中，辅助词嵌入被用来改善表征的生成。 变异自动编码器（VAE）方法被应用于有效地推理模型，这使得该模型易于探索其黑盒推理过程的扩展。在Web Snippets、20Newsgroups、BBC和生物医学数据集上的实验结果表明了该模型的有效性和效率。
Neural Tangents是一个用于处理无限宽神经网络的库。它提供了一个高水平的API，用于指定复杂和分层的神经网络架构，然后这些网络可以像往常一样在有限宽度下进行训练和评估，或者在其无限宽限制下进行训练和评估。 对于无限宽的网络，Neural Tangents通过贝叶斯规则或梯度下降进行精确推理，并生成相应的神经网络高斯过程和神经切线核。此外，Neural Tangents提供了研究宽但有限网络的梯度下降训练动态的工具。整个库在CPU、GPU或TPU上开箱即用。所有的计算都可以自动分布在多个加速器上，设备的数量可以近乎线性地扩展。除了下面的资源库，我们还提供了一个配套的交互式Colab笔记本：https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb
符号逻辑允许从业者建立系统，进行基于规则的推理，这种推理是可解释的，并且可以很容易地用先前的知识来增加。然而，由于语言的巨大语言变异性，这种系统传统上很难应用于涉及自然语言的问题。目前，自然语言处理的大多数工作集中在神经网络上，它学习单词的分布式表示及其组成，从而在存在巨大的语言变异性时表现良好。 我们建议将神经网络和逻辑编程结合起来应用于自然语言问题的回答，从而获得这两种方法的好处。我们建议采用一个外部的、无差异的Prolog验证器，它利用相似性函数对预训练的句子编码器进行验证。我们通过进化策略对这些表示进行微调，目的是对自然语言进行多跳推理。 这使我们能够创建一个系统，可以将基于规则的推理应用于自然语言，并从训练数据中诱导出特定领域的自然语言规则。我们在两个不同的问题回答任务上评估了拟议的系统，表明它补充了两个非常强大的基线--BIDAF（Seo等人，2016a）和FASTQA（Weissenborn等人，2017）--并且在组合使用时优于两者。
训练标签的获取成本很高，而且质量不一，因为有些标签可能来自值得信赖的专家标签，而其他标签可能来自启发式方法或其他弱监督的来源，如众包。 我们是要从少量的高质量数据中学习，还是从潜在的大量弱标签数据中学习？我们认为，如果学习者能够以某种方式知道并考虑到标签的质量，我们就可以得到两个世界的最佳结果。  为此，我们引入了 "保真度加权学习"（fidelity-weighted learning），这是一种半监督的学生-教师方法，用于使用弱标签数据训练深度神经网络。FWL根据教师估计的标签质量的后验可信度来调节学生网络的参数更新，而教师可以获得高质量标签的有限样本，在每个样本基础上训练学生网络。
本文的重点是研究和解读过度参数化神经网络训练中一个有趣的鲁棒性现象。特别是我们提供了经验和理论证据，证明一阶方法，如梯度下降，尽管在丰富的数据集模型下过度参数化，但对恒定部分标签的噪声/腐败具有鲁棒性。特别是：i）首先，我们表明，在前几个迭代中，更新仍然在初始化附近，这些算法只适合正确的标签，基本上忽略了噪声标签。 其次，我们证明，要开始过度拟合噪声标签，这些算法必须偏离初始模型相当远，而这只能在更多的迭代之后发生。这些都表明，带有早期停止的梯度下降对标签噪声具有可证明的鲁棒性，并阐明了深度网络的经验鲁棒性，以及普遍采用的早期停止启发式方法。
权重修剪作为一种有效的模型压缩技术被引入。即使修剪删除了网络中大量的权重，但由于传统的稀疏矩阵格式需要大量的内存来存储索引相关的信息，所以内存需求的减少是有限的。 此外，与这种稀疏矩阵格式相关的计算是缓慢的，因为顺序的稀疏矩阵解码过程不能有效地利用高度并行的计算系统。作为压缩索引信息同时保持解码过程可并行化的尝试，建议采用基于维特比的剪枝法。 在本文中，我们提出了一种新的稀疏矩阵格式，以实现整个稀疏矩阵的高度并行解码过程。 压缩后的权重和指数可以使用维特比编码器快速重建为密集矩阵。仿真结果显示，与密集矩阵值直接来自DRAM的情况相比，提议的方案可以将参数送入处理元件的速度提高20％至106％。
使用深度学习模型作为压缩式传感任务的先验，为廉价的地震数据采集提供了新的潜力。基于对几个历史勘测进行训练的生成式对抗网络架构，设计了一个适当的Wasserstein生成式对抗网络，能够学习地震小波的统计特性。验证和性能测试压缩式传感的用法有三个步骤。首先，研究不同压缩率的地震勘测稀疏表示的存在。 最后，根据对重建地震图像和指标的评估，提出了对非均匀地震勘探网格的建议。所提出的深度学习模型的主要目标是为地震采集的优化设计提供基础，以减少成像质量的损失。 沿着这些思路，建议利用所提出的方法，在墨西哥湾的一个资产上进行非均匀网格的压缩传感设计，与传统的地震勘探网格相比，后者每隔几英尺就均匀地收集数据。
近年来，我们看到在人工智能的一些基准问题上取得了快速进展，现代方法在围棋、扑克和Dota中实现了接近或超过人类的表现。与这些环境相比，现实世界的成功通常需要人类在至少部分合作的环境中与他人合作和交流。 去年，纸牌游戏Hanabi被确立为人工智能填补这一空白的新基准环境。特别是，Hanabi对人类来说很有趣，因为它完全专注于心智理论，即在观察其他代理的行动时有效推理其意图、信念和观点的能力。学习在被他人观察时提供信息是强化学习（RL）的一个有趣挑战。从根本上说，RL需要代理探索以发现好的政策。然而，如果天真地做，这种随机性会在训练期间内在地使他们的行动对他人的信息量减少。我们提出了一种新的深度多代理RL方法，简化行动解码器（SAD），它利用集中训练阶段来解决这一矛盾。 在训练过程中，SAD允许其他代理不仅观察所选择的（探索性）行动，而且代理也观察其队友的贪婪行动。通过将这种简单的直觉与状态预测的辅助任务和多代理学习的最佳实践相结合，SAD为2-5名玩家在Hanabi挑战的自我游戏部分建立了新的技术状态。
我们提出了一种端到端可训练的方法，用于印刷文件的光学字符识别（OCR）。它基于预测文件图像的二维字符网格（'chargrid'）表示，作为一项语义分割任务。为了从chargrid中识别单个字符实例，我们将字符视为对象，并使用计算机视觉中的对象检测技术。我们通过实验证明，我们的方法在准确性上优于之前的最先进方法，同时在GPU上容易并行（因此明显更快），并且更容易训练。
我们提出了NovoGrad，一种具有层级梯度归一化和解耦权重衰减的自适应随机梯度下降方法。在我们对图像分类、语音识别、机器翻译和语言建模的神经网络进行的实验中，它的表现与带动量的SGD和Adam/AdamW相当或更好。此外，NovoGrad（1）对学习率和权重初始化的选择具有鲁棒性，（2）在大批量设置中运行良好，（3）内存占用比Adam小2倍。
大多数音频生成模型直接在两个领域之一生成样本：时间或频率。虽然足以表达任何信号，但这些表示方法是低效的，因为它们没有利用现有的关于声音如何产生和感知的知识。 第三种方法（声码器/合成器）成功地结合了信号处理和感知的强大领域知识，但由于表达能力有限，而且难以与现代基于自动分化的机器学习方法整合，因此研究得不太积极。在本文中，我们介绍了可分化数字信号处理（DDSP）库，它能够直接整合经典信号处理元素和深度学习方法。 以音频合成为重点，我们在不需要大型自回归模型或对抗性损失的情况下实现了高保真度的生成，证明DDSP能够利用强大的归纳偏见而不失去神经网络的表现力。 此外，我们表明，结合可解释的模块允许操纵每个单独的模型组件，其应用包括独立控制音高和响度、真实地推断出训练期间未见的音高、房间声学的盲除杂、将提取的房间声学转移到新的环境，以及不同音源之间的音色转换。 简而言之，DDSP为生成式建模提供了一种可解释的、模块化的方法，而不牺牲深度学习的好处。该库可在https://github.com/magenta/ddsp，我们鼓励社区和领域专家做出进一步贡献。
谱图卷积网络（GCNs）是卷积网络在图结构数据上学习的一个泛化。谱图GCNs的应用已经成功，但仅限于图是固定的少数问题，如形状对应和节点分类。 在这项工作中，我们通过重新审视谱系图网络的一个特定系列，Chebyshev GCNs来解决这一局限性，显示了它在解决具有可变图结构和大小的图分类任务中的功效。目前的GCNs还限制图在任何一对节点之间最多只有一条边。 为此，我们提出了一种新的多图网络，从多关系图中学习。我们明确地对不同类型的边缘进行建模：注释的边缘、具有抽象意义的学习边缘和层次化的边缘。我们还试验了不同的方法来融合从不同边缘类型中提取的表征。这种限制有时是来自数据集的暗示，然而，我们对所有类型的数据集放松了这种限制。我们在各种化学、社会和视觉图分类基准上取得最先进的结果。
深度神经网络最近被证明容易受到后门攻击。具体来说，通过改变一小部分训练实例，对手能够安装一个后门，可以在推理过程中使用，以完全控制模型的行为。虽然这种攻击非常强大，但它关键是依赖于对手能够向训练集引入任意的，通常是明显错误标记的输入，因此甚至可以通过相当粗略的数据过滤来检测。 在本文中，我们介绍了一种执行后门攻击的新方法，利用对抗者的例子和GAN生成的数据。关键特征是，所产生的中毒输入似乎与他们的标签一致，因此即使在人类检查时也显得很良性。
尽管深度神经网络具有记忆大型数据集的能力，但它们往往能实现良好的泛化性能。然而，泛化和不泛化的网络的学习解决方案之间的差异仍不清楚。此外，单一方向的调谐特性（定义为单个单元或一些单元的线性组合对某些输入的响应的激活）已被强调，但其重要性尚未得到评估。 在这里，我们将这些调查路线联系起来，证明一个网络对单一方向的依赖是其泛化性能的一个很好的预测因素，在具有不同比例的损坏标签的数据集上训练的网络，在具有未修改标签的数据集上训练的网络集合，在不同的超参数上，以及在训练过程中。 最后，我们发现，类的选择性对任务重要性的预测很差，这不仅表明泛化良好的网络通过降低其选择性来减少对单个单元的依赖，而且表明单个选择性单元对强大的网络性能来说可能不是必需的。
在变异自动编码器中，典型的摊销推理是专门针对单一的概率查询的。这里我们提出了一个推理网络架构，可以泛化到未见过的概率查询。代替编码器-解码器对，我们可以直接从数据中训练一个单一的推理网络，使用一个成本函数，不仅对样本是随机的，对查询也是随机的。 我们可以用这个网络来执行与隐藏变量的无向图形模型相同的推理任务，而不必处理难以解决的分区函数。结果可以映射到实际无向模型的学习，这是一个众所周知的困难问题。 我们表明，我们的方法适用于未见过的测试数据上的未见过的概率查询，提供快速和灵活的推理。实验表明，这种方法在9个基准数据集上优于或匹配PCD和AdVIL。
大量的计算机视觉任务，如光流和图像对齐，可以被表述为非线性优化问题。在深度学习重新兴起之前，解决这类优化问题的主流系列是数值优化，如Gauss-Newton（GN）。最近，人们做了一些尝试，将可学习的GN步骤表述为级联回归架构。在本文中，我们从上述角度研究了最近的机器学习架构，如具有剩余连接的深度神经网络。 为此，我们首先证明了如何将残差块（当被视为ODE的离散化时）视为GN步骤。然后，我们更进一步，提出了一个新的残差块，它让人想起数值优化中的牛顿方法，并表现出更快的收敛性。我们通过对图像和语音分类以及图像生成进行实验，使用4个数据集来彻底评估拟议的Newton-ResNet。所有实验表明，Newton-ResNet需要更少的参数来实现与原始ResNet同样的性能。
在竞争情况下，代理可能会采取一些行动来实现他们的目标，而这些行动在不知不觉中促进了对手的目标。我们考虑一个有三个代理的领域。(当用户和攻击者所拥有的领域知识存在差异时，攻击者可能会利用用户对该领域的不熟悉来实现自己的目标。在这种情况下，以支持用户为目标的观察者可能需要进行干预，而且这种干预需要在线、及时、准确地进行。 我们对在线计划干预问题进行了形式化，并提出了一个解决方案，即使用决策树分类器来识别代理人在不知不觉中为对手的目标提供便利的情况下的干预点。我们使用从观察者的决策空间中提取的与领域无关的特征来评估当前状态的 "关键性 "来训练分类器，然后在IPC基准的在线设置中使用该训练模型来识别需要干预的观察。
深度生成模型可以模拟复杂图像数据集的感知特性，提供数据的潜在表征。然而，如果没有某种形式的监督，操纵这种表征在数据空间中进行有意义的和可控的转换仍然具有挑战性。虽然以前的工作主要是利用统计独立性来确定textit{disentangle}潜在因素，但我们认为这种要求可以有利地放松，而是提出一个非统计框架，依靠识别网络的模块化组织，基于反事实的操作。 我们的实验支持通道组之间的模块化在各种生成模型上达到一定程度。这允许在复杂的图像数据集上设计有针对性的干预措施，为计算上高效的风格转移和自动评估模式识别系统中对环境变化的鲁棒性等应用开辟道路。
灾难性遗忘给持续学习系统带来了巨大的挑战，它使神经网络在连续学习新任务的同时无法保护旧知识。我们提出了一个可微分的Hebbian可塑性（DHP）Softmax层，它在Softmax输出层的慢速权重上增加了一个快速学习的可塑性成分。DHP Softmax表现为一个压缩的事件记忆，在创造新的记忆痕迹的同时重新激活现有记忆。 我们将我们的模型与现有的知名巩固方法相结合，以防止灾难性遗忘，从而证明了我们模型的灵活性。我们在Permuted MNIST和Split MNIST基准上评估了我们的方法，并引入了Imbalanced Permuted MNIST--一个结合了类不平衡和概念漂移挑战的数据集。
虽然真实的大脑网络表现出功能模块化，但我们研究了通过反向传播训练的深度神经网络（DNN）是否也存在功能模块化。在DNN也被组织成特定任务模块的假设下，在本文中，我们试图借助于相对研究良好的神经元归属方法，将隐藏层分解成不相干的特定任务隐藏神经元组。 所谓任务特定，我们指的是同一组中的隐藏神经元在功能上与预测一组类似的数据样本，即具有类似特征模式的样本相关联。我们认为这种神经元组，我们称之为功能模块，可以作为DNN的基本功能单元。我们发现，首先，功能神经元是高度稀疏的，即只有一小部分神经元对预测一小部分数据样本很重要，而且，虽然我们不使用任何标签监督，但对应于同一组（双簇）的样本显示出惊人的一致性特征模式。我们还表明，这些功能模块在通过消融实验区分数据样本方面发挥了关键作用。
高效的CNN领域特定加速器（CNN-DSA）芯片目前可广泛用于移动设备。这些芯片主要用于计算机视觉应用。然而，最近使用二维CNN模型进行文本分类和情感分析任务的Super Characters方法，通过从视觉到文本的转移学习方法，也取得了最先进的成果。本文中，我们使用CNN-DSA芯片实现了移动设备上的文本分类和情感分析应用。 在CNN-DSA芯片中使用了系数精度为1位和3位、激活精度为5位的紧凑网络表示，其功耗低于300mW。对于内存和计算量受限的边缘设备，通过在CNN-DSA芯片中接近外部全连接（FC）层来进一步压缩网络。在研讨会上，我们有两个针对NLP任务的系统演示。第一个演示将输入的英文维基百科句子分类为14类之一。
比较不同候选模型的推论是模型检查和摆脱局部优化的重要部分。为了实现有效的比较，我们引入了一个摊销的变异推理框架，可以在相同结构的模型之间进行快速可靠的后验估计。我们的任意参数编码器（APE）扩展了摊销推理中常见的编码器神经网络，将数据特征向量和模型参数向量作为输入。 在比较合成数据和产品评论的候选主题模型的实验中，我们的任意参数编码器以更少的时间产生了与更昂贵的方法相当的后验，特别是当编码器架构是以模型感知的方式设计的。
将知识转移到新的环境和任务中的能力是一般学习代理的合理要求。尽管有明显的承诺，RL中的转移仍然是一个开放的、很少被利用的研究领域。在本文中，我们对转移采取了一个全新的观点：我们认为分配信用的能力揭示了任务中的结构不变性，可以通过转移来使RL更有效率。 我们的主要贡献是Secret，这是一种用于RL的转移学习的新方法，它使用了一种基于自我注意结构的后视信用分配机制。两个方面是其通用性的关键：它作为一个单独的离线监督过程来学习分配信用，并专门修改奖励函数。
神经变异推理的最新进展使得潜伏变量模型在涉及高维数据的各种领域得到了复兴。在本文中，我们为知识图谱的生成模型引入了两个通用的变异推理框架；潜伏事实模型和潜伏信息模型。 传统的变异方法为潜在变量上的难以处理的分布推导出一个分析性的近似值，而在这里，我们根据知识图谱中实体和关系类型的符号表示构建一个推理网络，以提供变异的分布。 新的框架可以创建模型，通过利用可参数化的分布来发现符号表示的潜在概率语义，这些分布允许在神经变异推理的背景下通过反向传播进行训练，从而形成一种高度可扩展的方法。在伯努利采样框架下，我们为大规模随机变异推理中的常用技术提供了另一种理由，这大大减少了训练时间，代价是对变异下限的额外近似。 生成框架足够灵活，允许在任何允许重新参数化技巧的先验分布下进行训练，也允许在任何允许参数最大似然估计的评分函数下进行训练。实验结果显示了这个框架的潜力和效率，它改善了高斯先验表示的多个基准。
我们提出了一个深度生成模型，名为Monge-Amp\`ere流，它建立在最优运输理论中的Monge-Amp\`ere方程所产生的连续时间梯度流上。从潜在空间到数据空间的生成图遵循一个动态系统，其中一个可学习的势函数引导可压缩流体向目标密度分布流动。 我们将该方法应用于MNIST数据集的无监督密度估计和临界点的二维Ising模型的变异计算。该方法将来自Monge-Amp`ere方程、最优运输和流体动力学的见解和技术引入基于可逆流的生成模型。
图形神经网络（GNNs）是一类深度模型，在具有任意拓扑结构和以图表示的秩序不变结构的数据上运行。我们为GNNs引入了一个高效的记忆层，可以学习联合执行图表示学习和图池。实验结果表明，所提出的模型在七个图形分类和回归基准中的六个中取得了最先进的结果。
深度神经网络需要大量的计算资源，不能有效地应用于移动电话等嵌入式设备，这严重限制了它们的适用性。为了解决这个问题，我们提出了一种新的编码方案，使用{-1,+1}将量化的神经网络（QNN）分解成多分支的二进制网络，可以通过位运算（xnor和bitcount）来有效实现模型压缩、计算加速和节省资源。 我们的方法与全精度的对应方法相比，最多可以实现~59的速度提升和~32的内存节省。因此，用户可以根据他们的要求和硬件资源，轻松实现不同的编码精度。我们在大规模图像分类（如ImageNet）和物体检测任务上验证了我们的方法的有效性。
我们提出了一种将条件信息纳入生成对抗网络（GAN）的新方法，用于结构化预测任务。该方法基于融合特征空间中的生成信息和条件信息的特征，并允许判别器更好地捕捉数据中的高阶统计数据。该方法还增加了真实或生成数据与条件数据一致时通过网络的信号强度。 所提出的方法在概念上比联合卷积神经网络-条件马尔可夫随机场（CNN-CRF）模型更简单，并强制执行高阶一致性，而不局限于一类非常具体的高阶电位。实验结果表明，这种方法导致了各种不同结构预测任务的改进，包括图像合成、语义分割和深度估计。
在本文中，我们提出了 "多样性是你所需要的"（DIAYN），一种没有奖励函数的学习有用技能的方法。我们提出的方法通过使用最大熵策略使信息理论目标最大化来学习技能。 在一些强化学习的基准环境中，我们的方法能够学习一种技能，尽管从未收到真正的任务奖励，但仍能解决基准任务。我们表明，预训练的技能可以为下游任务提供良好的参数初始化，并可以分层组成，以解决复杂、稀疏的奖励任务。我们的结果表明，无监督的技能发现可以作为一种有效的预训练机制，克服强化学习中探索和数据效率的挑战。
词汇的模糊性，即。即使使用递归神经网络和注意力机制有望解决这个问题，但机器翻译系统并不总是能够正确翻译词汇模糊的句子。 在这项工作中，我试图解决英语-日语神经机器翻译系统中的词义模糊问题，方法是将一个预先训练好的可以产生上下文词嵌入的Bidirectional Encoder Representations from Transformer（BERT）语言模型和一个Transformer翻译模型相结合，后者是机器翻译任务的最先进的架构。 事实证明，这两个拟议的架构在翻译模棱两可的句子时比vanilla Transformer模型和谷歌翻译系统更有效。此外，其中一个拟议的模型，Transformer_BERT-WE，与vanilla Transformer模型相比，在一般翻译方面取得了更高的BLEU分数，这具体证明了使用来自BERT的语境化词嵌入不仅可以解决词义模糊的问题，还可以在总体上提升翻译质量。
本文的重点是城市地区人类流动性数据的合成生成。我们提出了一种新颖的、可扩展的生成对抗网络（GANs）的应用，用于建模和生成人类流动性数据。我们利用美国四个主要城市的共享/叫车服务的实际乘车请求来训练我们的GANs模型。 以前的工作已经简明扼要地描述了人类流动性数据集的空间和时间属性，分别使用分形维度和密集化幂律，我们利用这些数据集来验证我们的GANs生成的合成数据集。这种合成数据集可以避免隐私问题，对城市流动性和智能交通的研究人员和政策制定者非常有用。
虽然深度神经网络是一个非常成功的模型类别，但其巨大的内存足迹对能源消耗、通信带宽和存储要求造成了相当大的压力。因此，减少模型大小已成为深度学习的首要目标。按照经典的比特-回溯论证，我们使用随机样本对网络权重进行编码，只需要对应于采样变异分布和编码分布之间的Kullback-Leibler分歧的比特数。 通过对Kullback-Leibler发散施加约束，我们能够明确地控制压缩率，同时优化训练集的预期损失。所采用的编码方案可以被证明接近最佳的信息理论下限，就所采用的变异族而言。
深度神经网络为图像去噪提供了最先进的性能，其目标是从噪声图像中恢复一个接近无噪声的图像。基本原理是，在大型数据集上训练的神经网络已被经验证明能够从图像的低维潜在表示中很好地生成自然图像。给定这样一个生成器网络或先验，噪声图像可以通过在先验范围内找到最接近的图像来去噪。 在本文中，我们考虑了从加性高斯噪声中去除图像的问题，假设图像被一个具有ReLu激活函数的深度神经网络很好地描述，将k维潜伏空间映射到n维图像。 我们陈述并分析了一个简单的类似梯度白化的迭代算法，该算法最小化了一个非凸的损失函数，并证明可以去除一部分（1-O(k/n)）的噪声能量。我们还在数字实验中证明，这种去噪性能确实是通过从数据中学习的生成性先验来实现的。
深度学习在许多领域产生了巨大的成果，从语音识别、图像分类到翻译。但对于每个问题，要让深度模型工作得好，需要对架构进行研究并进行长时间的调整。我们的模型架构包含了来自多个领域的构建块。它包含卷积层、注意力机制和稀疏门控层。有趣的是，即使某个块对某项任务并不重要，我们也观察到，添加它绝不会损害性能，而且在大多数情况下会提高所有任务的性能。我们还表明，数据较少的任务主要受益于与其他任务的联合训练，而大型任务的性能即使下降也只是轻微下降。
用于控制设备的机器学习算法需要快速学习，只需很少的试验。这样的目标可以通过从大陆哲学中借用的概念来实现，并使用类别的数学理论工具进行形式化。在一个网络物理系统：老虎机游戏和Atari 2600游戏上介绍了这种方法的例子。
音频信号是以高时间分辨率采样的，学习合成音频需要捕捉一系列时间尺度的结构。生成对抗网络（GANs）在生成局部和全局一致的图像方面取得了广泛的成功，但它们在音频生成方面的应用很少。 我们的实验表明，在没有标签的情况下，WaveGAN在小词汇量的语音数据集上训练时，能够学会产生可理解的单词，并且还能合成其他领域的音频，如鼓声、鸟类发声和钢琴声。我们将WaveGAN与一种方法进行了比较，该方法将为图像生成设计的GAN应用于类似图像的音频特征表示，发现两种方法都很有前景。
为监督学习获得足够的标记数据的困难促使了领域适应，其中分类器在一个领域（源域）中被训练，但在另一个领域（目标域）中运行。减少领域差异提高了性能，但它受到嵌入式特征的阻碍，这些特征没有形成明显的可分离和对齐的集群。我们通过使用流形结构传播标签来解决这个问题，并通过强制循环一致性来使每个领域的特征集群更紧密地对齐。 具体来说，我们证明，如果源域是理想的聚类，那么循环一致性会使嵌入的特征远离所有的聚类。我们还利用近似的局部流形的更多信息，并追求局部流形的一致性，以获得更多的改进。
我们提出了一种在强盗环境下在线训练神经网络的新方法。与之前的工作类似，我们只在网络的最后一层对不确定性进行建模，将网络的其余部分视为特征提取器。由于线性模型可用的高效、封闭式的不确定性估计，这使我们能够成功地在探索和利用之间取得平衡。 为了训练网络的其余部分，我们利用我们对最后一层的后验，对最后一层分布中的所有值进行优化，并以概率加权。我们推导出这个目标的闭合形式、微分近似，并通过各种模型和数据集的经验表明，与其他方法相比，以这种方式训练网络的其余部分会带来更好的在线和离线性能。
	尽管关于解释神经网络的文献越来越多，但在如何解释神经网络决策或如何评估解释方面还没有达成共识。	我们在本文中的贡献有两个方面。首先，我们研究了结合解释方法和减少模型不确定性的方案，以获得一个单一的聚合解释。聚合比任何单一的解释方法更稳健，并与神经网络更一致。	第二，我们提出了一种评估解释方法的新方法，它规避了人工评估的需要，并且不依赖于神经网络和人类决策过程的一致性。
我们提出了SOSELETO（SOurce SELEction for Target Optimization），一种利用源数据集来解决目标数据集上的分类问题的新方法。 SOSELETO基于以下简单的直觉：对于目标问题，一些源样本比其他样本更有信息量。 为了抓住这个直觉，源样本都被赋予了权重；这些权重是通过一个二层的优化方案与源和目标分类问题共同解决的。 因此，目标可以选择对自己的分类任务最有参考价值的源样本。 此外，优化的二层性质在目标上起到了一种正则化的作用，缓解了过度拟合。 SOSELETO可以应用于传统的迁移学习，以及在有噪声标签的数据集上的训练问题；我们展示了在这两个问题上的最新结果。
我们提出了一种训练神经抽象架构的新方法，该方法对机器的可解释组件进行了（部分）监督。为了干净地捕捉我们的方法所适用的神经架构的集合，我们引入了差分神经计算机（∂NCM）的概念，并表明现有的几种架构（例如。基于我们的方法，我们对NTM和NRAM架构进行了详细的实验评估，结果显示，与仅使用输入输出实例进行训练相比，该方法导致学习阶段的收敛性和泛化能力明显提高。
神经网络中模型参数的贝叶斯学习在具有良好校准不确定性的估计值的场景中非常重要。在本文中，我们提出了贝叶斯量化网络（BQNs），量化神经网络（QNNs），我们为其学习离散参数的后验分布。 我们为BQNs的学习和预测提供了一套有效的算法，而不需要从它们的参数或激活中取样，这不仅允许量化模型的可微分学习，而且还减少了梯度估计的方差。 我们在MNIST、Fashion-MNIST和KMNIST分类数据集上对BQNs进行了评估，并与QNN的bootstrap集合（E-QNN）进行了比较。我们证明BQNs比E-QNN实现了更低的预测误差和更好的校准不确定性（不到20%的负对数可能性）。
在训练过程中注入对抗性例子，也就是所谓的对抗性训练，可以提高对一步攻击的鲁棒性，但对未知的迭代攻击则不然。为了应对这一挑战，我们首先表明迭代产生的对抗性图像很容易在用相同策略训练的网络之间转移。 受此启发，我们提出了级联式对抗训练，它可以转移对抗训练的最终结果的知识。我们从头开始训练一个网络，除了注入从已经防御的网络中精心制作的迭代生成的对抗图像外，还注入正在训练的网络中的单步对抗图像。 我们还提出利用嵌入空间进行分类和低级（像素级）相似性学习，以忽略未知的像素级扰动。在训练过程中，我们注入对抗性图像，而不替换其相应的干净图像，并惩罚两个嵌入（干净和对抗性）之间的距离。 实验结果表明，级联对抗训练和我们提出的低级相似性学习有效地提高了对迭代攻击的鲁棒性，但代价是对单步攻击的鲁棒性下降。我们表明，结合这两种技术也可以提高最坏情况下黑盒攻击场景的鲁棒性。
虽然人们认为Adam、批量规范化和最近的SeLU非线性等技术 "解决了爆炸梯度问题"，但我们表明情况并非如此，在一系列流行的MLP架构中，爆炸梯度是存在的，而且在理论和实践中，它们限制了网络可以有效训练的深度。我们解释了爆炸梯度发生的原因，并强调了{it collapsing domain problem}，它可能出现在避免爆炸梯度的架构上。ResNets的梯度明显较低，因此可以规避爆炸性梯度问题，能够有效地训练更深的网络，我们表明这是一个令人惊讶的数学特性的结果。通过注意到{任何神经网络都是一个残差网络}，我们设计了{残差技巧}，它揭示了引入跳过连接在数学上简化了网络，这种简单性可能是它们成功的主要原因。
在本文中，我们对两个看似不同的概念感兴趣。\纹理{对抗训练}和纹理{生成对抗网络（GANs）}.特别是，这些技术是如何相互改进的。为此，我们分析了对抗训练作为一种防御方法的局限性，从质疑一个模型的稳健性能如何泛化开始。 然后，我们通过从生成式对抗网络中采样的 "假 "图像进行数据增强，成功地提高了泛化能力。之后，我们惊讶地发现，所产生的鲁棒性分类器导致了更好的生成器，而且是免费的。我们直观地解释了这个有趣的现象，并将理论分析留给未来的工作。 在这些观察的激励下，我们提出了一个将生成器、鉴别器和对抗性攻击者结合在一个网络中的系统。经过端到端训练和微调，我们的方法可以同时提高分类器的鲁棒性（以强对抗性攻击下的准确性来衡量）和生成器的质量（从美学和数量上进行评估）。 在分类器方面，我们实现了比（Madry\textit{et al.}，2017）中提出的最先进的对抗性训练算法更好的鲁棒性，而我们的生成器与SN-GAN（Miyato and Koyama，2018）相比实现了有竞争力的性能。
我们提出了一种训练自我二进制化神经网络的方法，即在训练过程中进化其权重和激活的网络，使其成为二进制网络。为了获得类似的二进制网络，现有的方法依赖于符号激活函数。然而，这个函数对于非零值没有梯度，这使得标准的反向传播不可能。为了规避训练依赖于符号激活函数的网络的困难，这些方法在训练期间交替使用网络的浮点和二进制表示，这是次优和低效率。 我们通过对涉及平滑激活函数的独特表示进行训练来完成二进制化任务，该表示在训练过程中被迭代锐化，直到它成为相当于符号激活函数的二进制表示。此外，我们引入了一种新的技术来执行二进制批量规范化，通过将其转化为简单的比较操作来简化传统的批量规范化。 这与现有的方法不同，这些方法被迫保留了传统的基于浮点的批量归一化。我们的二进制网络，除了显示出与传统的浮点和二进制网络相比更低的内存和计算量的优势外，还在多个基准数据集上显示出比现有最先进的方法更高的分类精度。
  在许多应用中，机器学习任务的训练数据被分割成多个节点，由于存储、通信或隐私限制，汇总这些数据可能不可行。"在基准和医疗数据集的实验中，我们的方法优于其他基线聚合技术，如集合或模型平均，并与理想的非分布式模型表现相当。
我们提出了Wasserstein自动编码器（WAE）--一种建立数据分布生成模型的新算法。WAE最小化了模型分布和目标分布之间的Wasserstein距离的惩罚形式，这导致了一种不同于变异自动编码器（VAE）的正则器。 我们将我们的算法与其他几种技术进行了比较，并表明它是对抗性自动编码器（AAE）的一种概括。我们的实验表明，WAE具有VAE的许多特性（稳定的训练、编码器-解码器结构、漂亮的潜流形结构），同时产生了质量更好的样本。
在本文中，我们旨在开发一种新的机制来保护深度神经网络对抗性学习中的差异隐私（DP），并对对抗性例子具有可证明的鲁棒性。我们利用DP中的顺序组成理论，在DP保护和可证明的鲁棒性之间建立一种新的联系。 为了解决模型效用、隐私损失和鲁棒性之间的权衡问题，我们根据DP中的后处理特性，设计了一个原始的、有差异的、对抗性的目标函数，以加强我们模型的敏感性。一个端到端的理论分析和全面的评估表明，我们的机制明显改善了DP深度神经网络的鲁棒性。
在具有稀疏奖励的高维强化学习环境中，进行有效的探索以获得任何奖励信号是一个公开的挑战。虽然基于模型的方法有希望通过规划进行更好的探索，但在高维（例如，超过10^100个状态）中学习一个足够可靠的马尔科夫决策过程（MDP）是非常困难的。在本文中，我们建议在更少的状态（如10^5）上学习一个抽象的MDP，我们可以通过规划来实现有效的探索。在我们的方法中，管理者在抽象状态的一个子集上维护一个抽象的MDP，该子集通过有针对性的探索（由于抽象的MDP是可能的）单调地增长。同时，我们学习一个工人政策来在抽象状态之间旅行；工人处理具体状态的混乱并向管理者提出一个干净的抽象。在拱廊学习环境中最难的三个游戏（Montezuma's、Pitfall！和Private Eye）中，我们的方法在每个游戏中的表现都超过了以前的最先进水平2倍以上。在Pitfall！中，我们的方法是第一个无需演示就能达到超人的表现。
大多数深度强化学习（RL）系统不能有效地从非政策性数据中学习，特别是如果它们不能在环境中进行在线探索的话。这对于将RL应用于现实世界的问题来说是一个关键的缺陷，在这些问题中，收集数据是昂贵的，而且模型在部署到环境中进行交互之前必须进行离线测试--例如。 因此，我们开发了一类新型的非政策性批量RL算法，该算法使用KL控制来惩罚与预先训练的可能行动模型的分歧。这种KL约束减少了外推误差，使有效的离线学习成为可能，无需探索，来自固定的一批数据。 我们还使用基于dropout的不确定性估计来下限目标Q值，作为双Q学习的一个更有效的替代方案。这个Way Off-Policy（WOP）算法在OpenAI Gym的传统RL任务和开放域对话生成问题上进行了测试；这是一个具有20,000维行动空间的挑战性强化学习问题。 WOP允许从收集的人类互动数据中事后提取多种不同的奖励函数，并能有效地从所有这些函数中学习。我们通过部署对话模型在开放域环境中与人类对话来测试现实世界的通用性，并证明WOP在批量深度RL中实现了比最先进的先前方法的显著改进。
转移和适应新的未知环境动态是强化学习（RL）的一个关键挑战。一个更大的挑战是在测试时间的单次尝试中执行接近最佳状态，可能无法获得密集的奖励，这是目前需要多次经验推出的适应方法所不能解决的。为了在具有相关动态的环境系列中实现单集转移，我们提出了一种通用算法，优化探针和推理模型，以快速估计测试动态的潜在变量，然后立即用作通用控制策略的输入。 这种模块化的方法能够整合最先进的变异推理或RL算法。此外，我们的方法不需要在测试时获得奖励，使其能够在现有的自适应方法无法执行的情况下执行。在具有单一事件测试约束的不同实验领域，我们的方法明显优于现有的自适应方法，并显示出对稳健转移基准的有利性能。
特定领域的目标导向对话系统通常需要对三种类型的输入进行建模，即。 (i)与该领域相关的知识库，(ii)对话的历史，即一连串的话语，(iii)需要生成响应的当前话语。在对这些输入进行建模时，目前最先进的模型，如Mem2Seq，通常忽略了知识图谱中固有的丰富结构和对话背景中的句子。 受结构感知图卷积网络（GCNs）最近在各种NLP任务（如机器翻译、语义角色标签和文档约会）中的成功启发，我们提出了一种用于目标导向对话的记忆增强型GCN。 我们的模型利用(i)知识库中的实体关系图和(ii)与语料相关的依赖图来计算更丰富的词和实体的表示。此外，我们认识到在某些情况下，例如，当对话是在代码混合的语言中，依赖分析器可能无法使用。 我们表明，在这种情况下，我们可以使用全局词共现图，并利用它来丰富语料的表示。我们用修改过的DSTC2数据集和最近发布的四种语言的混合编码版本进行了实验，结果表明，我们的方法在各种评价指标上都优于现有的先进方法。
我们通过(i)首先学习单个图形节点的矢量嵌入，(ii)然后将它们组合起来紧凑地表示节点序列来实现这一目标。具体来说，我们提出了SENSE-S（Semantically Enhanced Node Sequence Embedding - for Single nodes），一种基于跳格的新型嵌入机制，用于单个图形节点，共同学习图形结构及其文本描述。 我们证明了SENSE-S向量在各种情况下使用真实数据集将多标签分类任务的准确性提高了50%，将链接预测任务的准确性提高了78%。基于SENSE-S，我们接下来提出了通用的SENSE来计算代表节点序列的复合向量，其中保留节点顺序很重要。我们证明了这种方法在嵌入节点序列中是有效的，我们在真实数据上的实验证实了它在节点顺序解码中的高准确性。
最近的证据表明，卷积神经网络（CNN）偏向于纹理，因此CNN对纹理上的对抗性扰动没有鲁棒性，而传统的鲁棒性视觉特征如SIFT（标度不变特征变换）被设计为在相当大的范围内对仿生扭曲、添加噪声等具有鲁棒性，模仿人类的感知性质。 本文旨在利用SIFT的良好特性来改造CNN架构，使其具有更好的准确性和鲁棒性。我们从SIFT中借用了标度空间极值思想，并提出了EVPNet（极值保护网络），它包含三个新的组件来模拟极值。(1)高斯参数差异(DoG)来提取极值，(2)截断的ReLU来抑制非稳定的极值，(3)投影归一化层(PNL)来模仿PCA-SIFT的特征归一化。实验证明，EVPNets可以达到与传统CNN相似或更好的精度，同时在一组对抗性攻击（FGSM、PGD等）上实现更好的鲁棒性，即使没有对抗性训练。
压缩表征的概括性更好（Shamir等人。信息瓶颈（IB）方法（Tishby等人（2000））提供了一个有洞察力的原则性方法，用于平衡表示学习中的压缩和预测。IB目标I（X；Z）-βI（Y；Z）采用拉格朗日乘数β来调整这种权衡。 然而，对于如何选择β几乎没有理论指导。对于β、数据集、模型容量和可学习性之间的关系也缺乏理论认识。在这项工作中，我们表明，如果β选择不当，学习就无法发生：琐碎的表示P(Z|X)=P(Z)成为IB目标的全局最小值。 我们展示了如何避免这种情况的发生，方法是确定随着β的变化，在不可学习和可学习之间出现一个尖锐的相变。这个相变定义了IB-可学习的概念。 我们进一步证明了IB-可学习性是由训练实例中最大的自信、典型和不平衡的子集决定的。我们给出了一个实用的算法来估计给定数据集的最小β。我们在合成数据集、MNIST和具有噪声标签的CIFAR10上测试了我们的理论结果，并提出了一个令人惊讶的观点：准确性可能与β非单调。
 我们考虑了一类新的针对神经网络的 "数据中毒 "攻击，其中攻击者通过对其训练数据的一个子集进行小的扰动来控制一个模型。 我们将寻找毒药的任务制定为一个双层次的优化问题，可以使用从元学习社区借用的方法来解决。 与以前的中毒策略不同，元中毒可以使用攻击者未知的初始化和跨超参数转移来毒害从头开始训练的网络。此外，我们表明我们的攻击是更通用的：它们可以导致目标图像被错误地分类到一个任意选择的类别。
我们给出了一种在非常普遍的输入分布下学习两层神经网络的新算法。假设有一个真实的两层网络y = A\sigma(Wx) + \xi，其中A、W是权重矩阵，\xi代表噪声，并且隐藏层的神经元数量不大于输入或输出，我们的算法可以保证恢复真实网络的参数A、W。 对输入x的唯一要求是它是对称的，这仍然允许高度复杂和结构化的输入。我们的算法基于矩量法框架，并扩展了张量分解中的一些结果。我们使用频谱算法来避免学习神经网络中复杂的非凸性优化。实验表明，我们的算法可以用少量的样本对许多对称的输入分布稳健地学习基础真实的神经网络。
教师有意挑选信息量最大的例子给学生看。然而，如果教师和学生都是神经网络，教师网络学习到的例子虽然能有效地教导学生，但通常是不可解释的。 我们通过以下方式评估可解释性：（1）测量教师出现的策略与每个领域的直观策略的相似性；（2）进行人类实验，评估教师的策略在教人类方面的有效性。我们表明，教师网络学会选择或产生可解释的、教学性的例子来教基于规则、概率、布尔和层次的概念。
随机梯度下降（SGD）是解决大规模机器学习问题的事实上的优化算法，它以嘈杂的梯度更新来换取计算效率。SGD可以通过使用子采样训练数据进行更新来取得快速的学习进展，但嘈杂的更新也导致缓慢的渐进收敛。  一些降低方差的算法，如SVRG，引入控制变量以获得更低的方差梯度估计和更快的收敛。 尽管它们有吸引人的渐进保证，但类似SVRG的算法并没有在深度学习中被广泛采用。传统的随机优化中的渐进分析对在固定的历时数下训练深度学习模型提供了有限的洞察力。在本文中，我们提出了一个在噪声最小二乘回归问题下的SVRG的非渐进分析。 我们的分析和实验结果表明，在MNIST和CIFAR-10上，我们的回归模型的学习动态与神经网络的学习动态密切相关，无论是低参数化还是高参数化的模型都是如此。
非自回归机器翻译（NAT）系统并行地预测一连串的输出标记，与自回归模型相比，在生成速度上取得了实质性的改善。现有的NAT模型通常依赖于知识蒸馏技术，它从预训练的自回归模型中创建训练数据以获得更好的性能。 我们发现，知识提炼可以降低数据集的复杂性，并帮助NAT对输出数据的变化进行建模。此外，在NAT模型的能力和最佳翻译质量的提炼数据的最佳复杂性之间观察到了强烈的相关性。基于这些发现，我们进一步提出了几种可以改变数据集的复杂性以提高NAT模型性能的方法。
由于它们能够在长时间范围内有效地整合信息并扩展到海量数据，自我关注架构最近在自然语言处理（NLP）方面显示出突破性的成功，在语言建模和机器翻译等领域取得了最先进的成果。 利用转化器处理长时间范围信息的能力可以在部分可观察的强化学习（RL）领域提供类似的性能提升，但在NLP中使用的大规模转化器尚未成功应用于RL环境。在这项工作中，我们证明了标准转化器架构难以优化，这一点以前在监督学习环境中观察到，但在RL目标下变得特别明显。 我们提出了架构上的修改，大大改善了原始Transformer和XL变体的稳定性和学习速度。所提出的架构，Gated Transformer-XL（GTrXL），在具有挑战性的内存环境上超过了LSTM，并在多任务DMLab-30基准套件上取得了最先进的结果，超过了外部内存架构的性能。 我们表明，使用相同的损失进行训练的GTrXL，其稳定性和性能始终符合或超过有竞争力的LSTM基线，包括在内存不太关键的更多反应性任务上。GTrXL提供了一个易于训练、简单实现但表现力更强的架构，以替代普遍用于部分可观察环境中RL代理的标准多层LSTM。 
知识提炼是一种有效的模型压缩技术，其中一个较小的模型被训练来模仿一个较大的预训练模型。然而，为了使这些紧凑的模型适用于现实世界的部署，我们不仅需要减少性能差距，而且还需要使它们对通常发生的和敌对的扰动更加强大。 因此，我们认为噪声可以成为改善神经网络训练的关键因素，并解决改善模型的泛化和鲁棒性这两个明显矛盾的目标。受大脑中可能由多个噪声源导致的试验-试验变化的启发，我们通过输入水平或监督信号的噪声引入变化。 我们的结果表明，噪声可以改善模型的泛化和鲁棒性。"善变的教师 "将教师模型中的辍学作为响应变化的来源，导致泛化的显著改善。"软随机化"，将学生模型在高斯噪声下的输出分布与教师在原始图像上的输出相匹配，与用高斯噪声训练的学生模型相比，提高了对抗性的鲁棒性指数。 我们进一步展示了随机标签损坏对模型的对抗性鲁棒性的惊人影响。该研究强调了在知识提炼框架中添加建设性噪声的好处，并希望能激发该领域的进一步工作。
我们引入了一种新的端到端方法，用于学习在没有标签的情况下进行聚类。我们的聚类目标是基于优化归一化切割，这是一个既能衡量聚类内相似性又能衡量聚类间不相似性的标准。 与无监督深度学习中的许多工作不同，我们训练的模型直接输出最终的集群分配，而不是需要进一步处理才能使用的嵌入。具体来说，我们在流行的无监督聚类基准（如MNIST、Reuters、CIFAR-10和CIFAR-100）上取得了最先进的结果，比最强的基线高出了10.9%。
我们介绍了最大的（公开的）西里尔手写文本识别数据集和第一个西里尔野外文本识别数据集，并提出了一种识别西里尔手写文本和野外文本的方法。基于这种方法，我们开发了一个系统，可以为乌克兰最大的数学竞赛之一减少12天的文件处理时间，减少0.5吨的纸张用量。
大多数用于NLP的深度学习用语义空间中的单点或单模式区域表示每个单词，而现有的多模式单词嵌入不能表示较长的单词序列，如短语或句子。我们引入了一个短语表示法（也适用于句子），每个短语都有一组不同的多模式编码簿嵌入，以捕捉短语意义的不同语义面。 编码簿嵌入可以被看作是聚类中心，它概括了预训练的词嵌入空间中可能共同出现的词的分布。我们发现每个短语/句子的编码簿嵌入不仅提供了一个更可解释的语义表示，而且在无监督的短语相似性、句子相似性、超短语检测和抽取式总结的基准数据集上的表现也超过了强大的基线（在某些任务中差距很大）。
在训练过程中，我们使用一个共享的条件WaveNet核心和每个说话人的独立学习嵌入来学习一个多说话人模型。训练的目的不是产生一个具有固定权重的神经网络，然后作为一个TTS系统部署。 我们介绍了三种策略并对其进行了测试：(i)在保持WaveNet核心固定的同时学习说话人嵌入，(ii)用随机梯度下降法对整个架构进行微调，以及(iii)用经过训练的神经网络编码器预测说话人嵌入。实验表明，这些方法在使多说话人神经网络适应新说话人方面是成功的，只需几分钟的新说话人的音频数据就能在样本自然度和语音相似度方面获得最先进的结果。
本文介绍了一个用于K-shot图像分类的概率框架。目标是从最初的大规模分类任务推广到由新的类别和少量例子组成的单独任务。新方法不仅利用了神经网络从最初任务中学到的基于特征的表示（表示转移），还利用了关于类别的信息（概念转移）。 概念信息被封装在神经网络最后一层权重的概率模型中，作为概率K-shot学习的先验。我们表明，即使是一个简单的概率模型，在标准的K-shot学习数据集上也能以很大的幅度达到最先进的水平。此外，它能够准确地模拟不确定性，导致校准好的分类器，并且容易扩展和灵活，与许多最近的K-shot学习方法不同。
在最近RL代理的分布式训练成功的基础上，本文研究了基于RNN的RL代理从分布式优先经验回放中的训练。我们研究了参数滞后导致的表征漂移和循环状态僵化的影响，并根据经验得出了一个改进的训练策略。 使用单一的网络结构和固定的超参数集，所产生的代理，循环重放分布式DQN，在Atari-57上比以前的技术水平翻了两番，在DMLab-30上与技术水平相当。它是第一个在57个Atari游戏中的52个中超过人类水平的代理。
目前最先进的端到端语义角色标签（SRL）模型是一个深度神经网络架构，没有明确的语言特征。然而，先前的工作表明，金句法树可以极大地改善SRL，这表明神经网络模型可以从明确的句法建模中看到巨大的改善。在这项工作中，我们提出了语言学上的自我注意（LISA）：一个新的神经网络模型，它将多头自我注意与跨依赖性解析、语篇、谓词检测和SRL的多任务学习相结合。例如，句法是通过训练其中一个注意头来关注每个标记的句法父母而加入的。 在CoNLL-2005 SRL数据集的实验中，LISA实现了比以前最先进的预测新闻线的2.5个F1绝对值，并在域外数据上实现了超过2.0个F1。在ConLL-2012英语SRL上，我们也显示了超过3.0个F1的改进，错误减少了13%。
具有身份（如残差）连接的瓶颈结构是目前设计深度卷积神经网络（CNN）的流行范式，用于有效处理大规模特征。特别是，我们提出了选择性卷积单元（Selective Convolutional Unit，SCU），这是一个广泛适用的架构单元，可以提高各种具有瓶颈的现代CNN的参数效率。在训练过程中，SCU通过以下的替代用法逐渐学习通道选择性：（a）修剪不重要的通道，以及（b）将修剪后的参数重新连接到重要通道。 我们的实验结果表明，与基线相比，基于SCU的模型在没有任何后处理的情况下通常能实现模型的压缩和精度的提高，这在所有测试的架构中都是一致的。
一个例子是对观察数据的治疗效果的估计，其中一个子任务是预测治疗对受试者的影响，这些受试者与数据中接受治疗的人有系统的不同。一种相关的分布变化出现在无监督的领域适应中，我们的任务是对与我们观察标签的分布不同的输入进行泛化。 流行的克服分布变化的方法往往是启发式的，或者依赖于在实践中很少是真实的假设，比如有一个明确的模型或者知道产生观察到的数据的政策。其他方法由于需要一个预先指定的指标来比较观察，或者由于较差的渐进特性而受到阻碍。 在这项工作中，我们设计了一个基于积分概率指标和样本再加权的设计转变下的泛化误差约束。我们将这一想法与表示学习结合起来，概括并收紧这一领域的现有结果。
深度集合已经被经验证明是一种很有前途的方法，可以提高深度学习模型的准确性、不确定性和分布外的鲁棒性。虽然深度集合在理论上是由自举法激发的，但只用随机初始化训练的非自举法集合在实践中也表现良好，这表明深度集合之所以效果好，可能还有其他解释。 贝叶斯神经网络学习网络参数的分布，在理论上受到贝叶斯原则的良好激励，但在实践中表现不如深度合集，特别是在数据集转移的情况下。理论与实践之间的这种差距的一个可能的解释是，流行的可扩展近似贝叶斯方法倾向于关注单一模式，而深度合集倾向于探索函数空间的不同模式。 我们通过在最近了解神经网络损失景观的工作基础上研究这一假设，并加入我们自己的探索，以衡量预测空间中的函数的相似性。我们的结果表明，随机初始化探索完全不同的模式，而沿着优化轨迹或从其子空间采样的函数在预测方面聚集在单一模式中，而在权重空间中往往有很大的偏差。 我们证明，虽然模式之间存在低损耗的连接器，但它们在预测空间中没有联系。我们发展了多样性--准确性平面的概念，表明随机初始化的装饰能力是流行的子空间采样方法所无法比拟的。
现有的学习视觉特征的深度学习方法倾向于提取比手头任务所需的更多信息。从隐私保护的角度来看，输入的视觉信息没有得到保护，使模型变得比它被训练的更聪明。现有的抑制额外任务学习的方法假设在训练期间存在要抑制的任务的地面真实标签。 在这项研究中，我们提出了一个三方面的新贡献：(i)一个新的指标来衡量训练过的深度学习模型的信任分数，(ii)一个模型无关的解决方案框架，通过抑制所有不需要的任务来提高信任分数，(iii)一个模拟的基准数据集，PreserveTask，有五个不同的基本图像分类任务来研究模型的泛化性质。在第一组实验中，我们衡量和提高五个流行的深度学习模型的信任分数。VGG16、VGG19、Inception-v1、MobileNet和DenseNet，并证明Inception-v1的信任度最低。此外，我们展示了我们的框架在彩色-MNIST数据集上的结果，以及在Diversity in Faces（DiF）和IMDB-Wiki数据集上的人脸属性保存的实际应用。
  在这项工作中，我们建议在训练策略的同时，明确惩罚这两个分布在固定时间范围内的不匹配。我们通过使用学习到的环境动态模型来实现这一点，该模型被展开为多个时间步骤，并训练一个策略网络来最小化这个展开轨迹上的可区分成本。 这个成本包含两个条件：一个是政策成本，代表政策寻求优化的目标；另一个是不确定性成本，代表它与被训练的状态的偏差。我们建议通过使用动力学模型对其自身预测的不确定性来衡量第二个成本，使用最近的深度网络不确定性估计的想法。
动态系统模型（包括RNN）通常缺乏使序列生成或预测适应特定环境的能力，从而限制了它们在现实世界中的应用。在本文中，我们展示了分层多任务动态系统（MTDS）通过使用指定定制个人数据序列的潜在代码z，提供了用户对序列生成的直接控制。 我们表明MTDS可以通过潜伏代码插值改善预测，并避免标准RNN方法的长期性能下降。
通过在训练数据中注入对抗性例子，对抗性训练对提高深度学习模型的鲁棒性很有希望。然而，大多数现有的对抗性训练方法都是基于特定类型的对抗性攻击。它可能没有从对抗性领域提供足够的代表性样本，导致对来自其他攻击的对抗性例子的概括能力很弱。 此外，在对抗训练过程中，对输入的对抗扰动通常是由快速的单步对抗者制作的，以便扩展到大型数据集。这项工作主要是针对对抗训练却高效的FGSM对抗者。在这种情况下，由于缺乏代表性的对抗样本，又称样本无法准确反映对抗领域，因此很难训练出具有很大泛化能力的模型。 为了缓解这个问题，我们提出了一种新颖的领域适应性对抗训练（ATDA）方法。我们的直觉是把对FGSM对抗的对抗训练看作是一项领域适应性任务，目标领域样本的数量有限。 在Fashion-MNIST、SVHN、CIFAR-10和CIFAR-100上的实证评估表明，ATDA可以极大地提高对抗训练的泛化和所学模型的平稳性，并在标准基准数据集上优于最先进的方法。为了显示我们方法的转移能力，我们还将ATDA扩展到迭代攻击的对抗训练，如PGD-对抗训练（PAT），防御性能得到很大的改善。
字符级的语言建模是自然语言处理中一项重要但具有挑战性的任务。之前的工作集中在识别字符之间的长期依赖关系，并建立了更深更广的网络以获得更好的性能。然而，他们的模型需要大量的计算资源，这阻碍了字符级语言模型在资源有限的应用中的可用性。在本文中，我们提出了一个轻量级的模型，称为Group-Transformer，它减少了Transformer的资源需求，这是一个有希望的具有长期依赖性的序列建模方法。 具体来说，所提出的方法对线性操作进行分区，以减少参数数量和计算成本。结果，与性能最好的基于LSTM的模型相比，Group-Transformer只使用了18.2\%的参数，同时在两个基准任务enwik8和text8上提供了更好的性能。当与参数数量和时间复杂性相当的Transformer相比，提出的模型显示出更好的性能。将提供实施代码。
  领域适应性解决了将知识从标签丰富的源领域转移到无标签或标签稀缺的目标领域的问题。最近，领域对抗训练（DAT）显示出有希望通过逆转领域分类器的梯度传播来学习领域不变的特征空间的能力。 然而，DAT在几个方面仍然是脆弱的，包括（1）由于对抗性训练中领域分类器压倒性的判别能力而导致的训练不稳定，（2）限制性的特征级对齐，以及（3）缺乏可解释性或对所学特征空间的系统解释。在本文中，我们通过设计对抗性重建网络（ARN）提出了一种新的最大边缘领域对抗性训练（MDAT）。 本文提出的MDAT通过用重建网络取代领域分类器来稳定ARN中的梯度反转，以这种方式，ARN同时进行特征级和像素级的领域对齐，而不涉及额外的网络结构。此外，ARN对广泛的超参数设置显示出强大的鲁棒性，大大减轻了模型选择的任务。
我们提出了一种用同义词或别名扩展分类法的方法。我们的目标是有数千个节点的大型购物分类法。一套全面的实体别名是识别非结构化文本（如产品评论或搜索查询）中实体的重要组成部分。我们的方法包括两个阶段：我们从WordNet和购物搜索查询中生成同义词候选，然后使用二进制分类器来筛选候选。 我们对有数千个同义词的分类法进行了处理，以产生超过90,000个同义词。我们表明，使用分类法得出的上下文特征比单独使用目标节点的特征更能提高分类性能。我们表明，我们的方法有可能在不同的分类法领域之间进行迁移学习，从而减少收集新分类法训练数据的需要。
关系推理，即对物体之间的互动和关系进行建模的能力，对稳健的多物体跟踪很有价值，对轨迹预测也很关键。在本文中，我们提出了MOHART，一种与类别无关的、端到端的多物体跟踪和轨迹预测算法，它在其关系推理中明确考虑了包络不变性。 我们在三个真实世界的跟踪数据集上显示，以这种方式增加关系推理能力提高了跟踪和轨迹预测性能，特别是在存在自我运动、遮挡、拥挤的场景和错误的传感器输入的情况下。据我们所知，MOHART是文献中报道的第一个应用于真实世界数据的完全端到端多物体跟踪的视觉方法。
我们研究了行为批评强化学习算法与统一的大规模经验重放的结合，并为两个挑战提出了解决方案：（a）高效的行为批评学习与经验重放（b）非常偏离政策的学习的稳定性。我们采用这些见解来加速超参数扫描，其中所有参与的代理同时运行并通过一个共同的重放模块分享他们的经验。 基于我们的分析，我们主张将重放的经验与政策上的经验混合起来，并提出一种新的信任区域方案，该方案可以有效地扩展到V-trace变得不稳定的数据分布中。
随机梯度下降（SGD）是现代机器学习的主力军。有时，有许多不同的潜在梯度估计器可以使用，在这种情况下，选择一个成本和方差之间的最佳权衡是非常重要的。 这一选择对于SGD的不同变体和关于目标的不同假设（如凸性或平滑性）都是一样的。受这一原则的启发，我们提出了一种技术，在给出有限的估计器池时自动选择一个估计器。然后，我们扩展到无限的估计器池，其中每个估计器都由控制变量权重来索引。
我们研究了通过新的架构设计来缓解GAN训练过程中的不稳定问题。最小值和最大值目标值之间的差异可以作为交替梯度下降在GAN优化中遇到的困难的代表。在这项工作中，我们给出了关于GAN的多生成器架构的好处的新结果。我们表明，随着生成器数量以O（1/\epsilon）的速度增加，最小值差距缩小到\epsilon。 我们技术的核心是Shapley-Folkman法则在一般最小化问题上的新应用，在文献中，该技术只在目标函数被限制为约束优化问题的拉格朗日函数时有效。 我们提出的Stackelberg GAN在合成数据集和真实世界数据集上都表现良好，在基准数据集上比以前的多生成器GANs提高了14.61%的Frechet Inception Distance。
Nesterov SGD被广泛用于训练现代神经网络和其他机器学习模型。然而，它与SGD相比的优势还没有在理论上得到澄清。事实上，正如我们在本文中所展示的，无论是理论上还是经验上，Nesterov SGD的任何参数选择一般都不会比普通SGD提供加速。此外，Nesterov SGD可能在确保普通SGD收敛的步骤大小上出现分歧。 这与确定性设置中的经典结果相反，同样的步长可以确保Nesterov方法比最优梯度下降法加速收敛。为了解决不加速问题，我们为Nesterov SGD引入了一个补偿项。 对于全批，MaSS的收敛率与著名的Nesterov方法的加速率相匹配。我们还分析了收敛率和最佳超参数对小批量大小的依赖性这一重要的实际问题，证明了三种不同的制度：线性扩展、收益递减和饱和。对于几个标准的深度网络架构，包括ResNet和卷积网络，MaSS的实验评估显示了比SGD、Nesterov SGD和Adam更好的性能。
我们提出了固定分组层（FGL）；一个新颖的前馈层，旨在将结构化平滑度的归纳偏见纳入深度学习模型。FGL通过基于空间相似性的跨层连接节点来实现这一目标。FGL实现的结构化平滑度的使用是由结构化空间数据的应用所激发的，而这又是由领域知识所激发的。所提出的模型架构在各种具有结构化平滑度的模拟和真实数据集上的表现优于传统神经网络架构。
卷积网络不知道物体的几何变化，这导致了模型和数据容量的低效利用。为了克服这个问题，最近关于变形建模的工作寻求在空间上将数据重新配置为一个共同的安排，从而使语义识别受变形的影响更小。 这通常是通过在图像空间中用学习到的自由形式的采样网格来增强静态操作者，动态地调整数据和任务以适应感受野。然而，适应感受野并没有完全达到实际的目标--对网络真正重要的是*有效的感受野（ERF），它反映了每个像素的贡献程度。 在这项工作中，我们将一个可能的解决方案实例化为可变形内核（DKs），这是一个新颖的通用卷积算子系列，通过直接适应ERF而不触及感受野来处理物体的变形。我们方法的核心是对原始内核空间进行重采样以恢复物体的变形。 我们将DK作为刚性核的通用替代物来实现，并进行了一系列的实证研究，其结果与我们的理论相一致。在几个任务和标准的基础模型上，我们的方法与之前在运行期间适应的工作相比更有优势。此外，进一步的实验表明了一种与之前的工作正交和互补的工作机制。
从一个蛋白质的氨基酸序列推断其结构特性是生物学中一个具有挑战性的重要问题。绝大多数蛋白质序列的结构是未知的，但结构对于理解功能至关重要。 现有的从序列中检测蛋白质结构相似性的方法无法识别和利用结构模式，当序列分歧过大时，限制了我们在结构相关的蛋白质之间转移知识的能力。我们通过表征学习的视角来处理这个问题。我们引入一个框架，将任何蛋白质序列映射到矢量嵌入序列--每个氨基酸位置一个--编码结构信息。 我们在蛋白质序列上训练双向长短期记忆（LSTM）模型，该模型具有两部分反馈机制，包括来自（i）蛋白质之间的全球结构相似性和（ii）单个蛋白质的成对残基接触图的信息。 我们的方法能够学习有用的特定位置的嵌入，尽管缺乏对序列之间位置级对应关系的直接观察。我们通过经验表明，我们的多任务框架在预测结构相似性时优于其他基于序列的方法，甚至优于表现最好的基于结构的比对方法，这是我们的目标。最后，我们证明，我们学习的嵌入可以转移到其他蛋白质序列问题，改善跨膜域预测的最先进水平。
受生物神经元发射的适应现象的启发，我们提出了规律性归一化：考虑到隐含空间的统计规律性，对神经网络中的激活进行重新参数化。通过将神经网络优化过程视为一个模型选择问题，隐含空间受到归一化因素的约束，即最佳通用代码的最小描述长度。 我们引入了一个增量版本的计算该通用代码作为归一化最大似然，并证明了它的灵活性，包括数据先验，如自上而下的注意力和其他神谕信息，以及它的兼容性，以纳入批量归一化和层归一化。 初步结果显示，所提出的方法在处理来自非平稳分布的有限和不平衡的数据方面优于现有的归一化方法，以计算机视觉任务为基准。作为一种给定输入数据的无监督注意力机制，这种生物学上合理的归一化有可能处理其他复杂的现实世界场景，以及奖励稀少和不均匀的强化学习环境。
在本文中，我们通过自动编码器研究生成模型，同时利用最优传输（OT）问题和Wasserstein距离的优雅几何特性。我们引入了Sliced-Wasserstein自动编码器（SWAE），它是生成模型，使人们能够将潜在空间的分布塑造成任何可采样的概率分布，而不需要训练对抗网络或定义分布的封闭形式。 简而言之，我们用编码的训练样本分布和预定义的可抽样分布之间的切片-瓦瑟斯坦距离来规范自动编码器的损失。我们表明，所提出的公式有一个有效的数字解决方案，提供与瓦瑟斯坦自动编码器（WAE）和变异自动编码器（VAE）类似的能力，同时受益于一个令人尴尬的简单实现。
模仿学习（IL）的目标是从高质量的示范中学习一个好的策略。然而，现实中示范的质量可能是多样的，因为从专家和业余爱好者的混合中收集示范更容易和更便宜。在这种情况下，IL可能具有挑战性，特别是当示范者的专业知识水平未知时。我们提出了一个新的IL范式，称为多样化质量示范的可变模仿学习（VILD），其中我们用一个概率图形模型明确地模拟示范者的专业知识水平，并与奖励函数一起估计它。 我们表明，天真的估计方法不适合大的状态和行动空间，并通过使用变异方法来解决这个问题，该方法可以很容易地使用现有的强化学习方法来实现。在连续控制基准上的实验表明，VILD优于最先进的方法。我们的工作使可扩展和数据有效的IL在比以前更现实的设置下。
随着可用服务的数量不断增加，每个服务的参数、前提条件和效果都略有不同，一般语义服务的自动化规划变得非常重要。然而，大多数现有的规划者只考虑PDDL，或者如果他们声称使用OWL-S，他们通常会将其翻译成PDDL，在此过程中失去许多语义。 在本文中，我们提出了一种基于语义距离的、独立于领域的新启发式，它可以被A*等通用规划算法用于自动规划用OWL-S描述的语义服务。为了让启发式包含更多相关信息，我们在运行时计算启发式。使用这种启发式，我们能够在比既有技术更短的时间内产生更好的结果（更少的扩展状态）。
在彩色图中，节点类通常与它们的邻居类或与每个节点相关的图中未纳入的信息相关联。我们在此提出，节点类也与节点的拓扑特征相关联。我们利用这种关联来改进一般的图机器学习，特别是图卷积网络（GCN）。首先，我们表明，即使在没有任何关于节点的外部信息的情况下，使用拓扑特征或使用邻居类别作为GCN的输入，也可以获得良好的节点类别预测精度。 其次，我们表明，当与节点的外部信息相结合时，明确地增加拓扑结构作为GCN的输入并不能提高准确性。然而，增加一个额外的邻接矩阵，在具有类似拓扑结构的远处节点之间的边缘与GCN相接，确实能显著提高其准确性，导致在多个数据集中的结果优于所有的先进方法。
苍蝇和小鼠是相隔6亿年的进化物种，但它们进化出的嗅觉系统在解剖学和功能组织上有许多相似之处。这些共同的解剖学和功能特征有哪些功能，它们对气味的感知是否最优化？在这项研究中，我们通过研究为感知气味而训练的人工神经网络来解决嗅觉电路中进化设计的优化问题。 我们发现，人工神经网络定量地再现了嗅觉系统固有的结构，包括在压缩层上形成肾小球，在扩张层上形成稀疏的随机连接。最后，我们为每个结果提供了理论依据。我们的工作提供了一个框架来解释嗅觉电路的进化收敛，并对嗅觉系统的解剖学和功能结构给予洞察和逻辑。
最近的非配对图像到图像的翻译方向一方面是非常令人兴奋的，因为它减轻了在获得标签密集的像素到像素的监督方面的巨大负担，但另一方面由于存在伪影和退化的变换，它并不完全令人满意。 在本文中，我们从流形的角度来看待这个问题，在样本图上引入平滑项，以达到谐波函数，在翻译过程中强制执行一致的映射。我们开发了HarmonicGAN来学习源域和目标域之间的双向翻译。 在与CycleGAN相同的问题设置下，不需要额外的人工输入，只需要很小的训练时间成本，HarmonicGAN展示了比现有技术水平更显著的定性和定量改进，以及更好的可解释性。 我们在一些应用中展示了实验结果，包括医学成像、物体变形和语义标签。我们在所有的任务中都优于竞争方法，特别是在医学成像任务中，我们的方法将CycleGAN从失败变成了成功，将平均平方误差减半，并在95%的情况下产生了放射科医生比竞争方法更喜欢的图像。
最近的方法通过深度学习来解决这个问题，它可以利用语义线索来处理无纹理和反射区域等挑战。在本文中，我们提出了一个称为DPSNet（深度平面扫描网络）的卷积神经网络，其设计灵感来自传统的基于几何的方法的最佳实践。 DPSNet没有像以前的许多深度学习方法那样直接从图像对中估计深度和/或光流的对应关系，而是采取了一种平面扫描的方法，包括使用平面扫描算法从深度特征中建立一个成本体积，通过上下文感知的成本聚合来规范成本体积，并从成本体积中回归深度图。 成本体积是通过一个可区分的翘曲过程构建的，该过程允许对网络进行端到端的训练。通过在深度学习框架内有效结合传统的多视角立体概念，DPSNet在各种具有挑战性的数据集上实现了最先进的重建结果。
设计生物结构（如DNA或蛋白质）的能力将具有相当大的医疗和工业影响。这样做提出了一个具有挑战性的黑箱优化问题，其特点是由于需要劳动密集型的湿实验室评估，所以是大批量、低回合的设置。作为回应，我们提出使用基于近似政策优化（PPO）的强化学习（RL）来进行生物序列设计。 RL为优化生成性序列模型提供了一个灵活的框架，以实现特定的标准，如所发现的高质量序列的多样性。我们提出了一个基于模型的PPO变体，DyNA-PPO，以提高样本效率，其中新一轮的策略是使用适合前几轮功能测量的模拟器离线训练的。为了适应跨轮观察数量的增长，模拟器模型在每轮都会从不同能力的不同模型池中自动选择。 在设计DNA转录因子结合位点、设计抗菌蛋白和优化基于蛋白质结构的Ising模型的能量等任务上，我们发现在建模可行的情况下，DyNA-PPO的表现明显优于现有方法，而在无法学习到可靠模型的情况下，表现仍不差。
实现机器智能需要感知和推理的顺利整合，但迄今为止开发的模型往往只专注于其中之一；对从丰富的感知空间获得的符号进行复杂的操作，迄今已被证明是难以实现的。考虑一个视觉算术任务，其目标是对在自然条件下呈现的数字进行简单的算术算法（如手写的、放置的）。 低层由信息处理模块的异质集合组成，其中可以包括用于从图像中定位和提取字符的预训练深度神经网络，以及对感知提取的表征进行符号转换的模块。 高层由一个控制器组成，它通过强化学习进行训练，协调各模块以解决高层任务。例如，控制器可以学习在什么情况下执行感知网络以及对其输出进行什么符号转换。
动物不仅通过与环境的相互作用来发展新的技能，而且也从他人的影响中发展新的技能。在这项工作中，我们将社会影响建模到强化学习的方案中，使代理既能从环境中学习，也能从他们的同伴中学习。具体而言，我们首先定义了一个衡量政策之间距离的指标，然后定量地推导出唯一性的定义。 与以前不稳定的联合优化方法不同，在我们的工作中，社会唯一性的动机被强加为一种约束，以鼓励代理学习与现有代理不同的政策，同时仍然解决原始任务。所产生的算法，即内部政策差异化（IPD），带来了性能的提高，以及解决特定任务的具有独特行为的政策集合
生成对抗网络（GANs）在逼近高维、输入数据样本的复杂分布方面非常有效，在理解和提高GAN的性能方面已经在理论和应用方面取得了实质性进展。然而，我们目前缺乏评估模型的定量方法。正因为如此，虽然有许多GAN变体被提出，但我们对它们的相对能力了解得相对较少。在本文中，我们使用通常只用于训练的发散和距离函数评估各种类型的GAN的性能。
知识库（KB）通常表示为（HEAD, PREDICATE, TAIL）形式的事实集合，其中HEAD和TAIL是实体，而PREDICATE是连接两者的二元关系。众所周知，知识库远非完整，因此关于知识库完成方法的研究非常多，特别是关于链接预测。然而，尽管经常被忽视，这些存储库也包含数字事实。(PARIS, LATITUDE, 48.8).同样，数字事实也存在不完全性问题。为了解决这个问题，我们引入了数字属性预测问题。这个问题涉及一种新型的查询，其中关系是一个数字谓词。因此，与链接预测相反，这个查询的答案是一个数字值。 我们认为，与实体相关的数值在某种程度上解释了知识库的关系结构。因此，我们利用知识库嵌入方法来学习对数字属性有用的预测表征。在FREEBASE和YAGO的基准版本上进行的大量实验表明，我们的方法在很大程度上超过了合理的基准。
我们提出了评估和加强上下文嵌入对齐的程序，并表明它们在分析和改进多语言BERT方面是有用的。特别是，在我们提出的对齐程序之后，BERT在XNLI上表现出比基础模型明显改善的零点性能，与保加利亚语和希腊语的伪完全监督翻译训练模型明显匹配。 此外，为了衡量对齐程度，我们引入了一个上下文版本的单词检索，并表明它与下游的零点转移有很好的相关性。使用这个单词检索任务，我们也分析了BERT，并发现它表现出系统性的缺陷，例如，对于开放类的语音部分和用不同文字写的单词对，对齐程度较差，这些都被对齐程序纠正了。这些结果支持上下文对齐作为理解大型多语言预训练模型的有用概念。
神经网络在解决影像学中的各种不理想的逆向问题方面达到了出色的表现。然而，与经典的变异方法相比，端到端的学习方法的缺点是，即使是稍微不同的问题陈述，也需要昂贵的再训练，而且在推理过程中缺乏可证明的错误界限。 最近的工作通过使用为高斯图像去噪而训练的网络作为能量最小化算法中的通用即插即用正则器来解决第一个问题。尽管这在许多任务上获得了最先进的结果，但如果基础固定点迭代的可证明收敛性是一个要求，就必须对网络结构进行严格限制。 在本文中，我们考虑将这两种方法结合起来，将即插即用的去噪网络的输出投射到给定能量的下降方向的锥体上，这样，一个预先训练好的网络就可以用于各种重建任务。
知识图谱的嵌入研究忽略了概率校准的问题。我们表明流行的嵌入模型确实是未经校准的。这意味着与预测的三元组相关的概率估计是不可靠的。我们提出了一种新的方法来校准一个模型，当地面真实的负数不可用时，这就是知识图谱的通常情况。 在三个有地面真实底片的数据集上的实验表明，与使用底片的黄金标准相比，我们的贡献导致了良好的校准模型。我们从所有的校准方法中得到了明显优于未校准模型的结果。
作为人脸识别领域的一个新兴课题，设计基于边际的损失函数可以增加不同类别之间的特征边际，以增强可辨别性。最近，吸收基于挖掘的策略的想法被采用，以强调错误分类的样本并取得有希望的结果。然而，在整个训练过程中，先前的方法要么没有明确强调基于其重要性的样本，使硬样本没有得到充分的利用，要么明确强调半硬/硬样本的影响，甚至在早期训练阶段可能导致收敛问题。 在这项工作中，我们提出了一种新的自适应课程学习损失（CurricularFace），将课程学习的思想嵌入到损失函数中，实现了一种新的深度人脸识别的训练策略，它主要解决早期训练阶段的容易样本和后期阶段的困难样本。 具体来说，我们的CurricularFace在不同的训练阶段自适应地调整容易和困难样本的相对重要性。在每个阶段，不同的样本根据其相应的难易程度被赋予不同的重要性。
对抗性例子是旨在愚弄机器学习模型的扰动输入。对抗性训练将这种例子注入训练数据以提高鲁棒性。为了将这种技术扩展到大数据集，扰动是使用快速的单步方法制作的，它使模型的损失的线性近似最大化。 我们表明，这种形式的对抗性训练会收敛到一个退化的全局最小值，其中数据点附近的小曲率假象混淆了损失的线性近似值，因此该模型学会了生成弱扰动，而不是防御强扰动。 因此，我们发现对抗性训练仍然容易受到黑箱攻击，在黑箱攻击中，我们转移了在未防御模型上计算的扰动，以及一个强大的新的单步攻击，通过一个小的随机步骤逃脱了输入数据的非光滑附近的。 我们进一步介绍了Ensemble Adversarial Training，这是一种用从其他模型转移过来的扰动来增强训练数据的技术。在ImageNet上，Ensemble Adversarial Training产生的模型对黑箱攻击具有很强的鲁棒性。特别是，我们最鲁棒的模型赢得了NIPS 2017关于防御对抗攻击的第一轮比赛。
对抗性特征学习（AFL）是一个强大的框架，用于学习对滋生属性不变的表征，它在特征提取器和分类属性分类器之间使用对抗性游戏。理论上听起来，它在属性和表征之间最大化条件熵。 然而，正如本文所示，AFL经常导致不稳定的行为，从而减慢了收敛速度。我们提出了一种{em属性感知匹配}作为替代方法，基于条件熵最大化的重新表述，即{em对等分布匹配}。虽然实现成对分布匹配的天真方法需要大量的参数，但我们提出的方法需要与AFL相同的参数数量，但有更好的收敛特性。在玩具和真实世界的数据集上的实验证明，我们提出的方法比AFL更快地收敛到更好的不变性表示。 
我们通过Hessian矩阵来研究普通损失面的特性。特别是在深度学习的背景下，我们根据经验表明Hessian的频谱由两部分组成。(我们为Sagun等人（2016）提出的以下猜想提供了数字证据和数学依据。固定数据，增加参数的数量只是扩展了频谱的大部分；固定维度和改变数据（例如增加更多的集群或使数据的可分离性降低）只影响离群值。我们认为我们的观察对高维度的非凸优化有惊人的影响。 首先，这种景观的*平坦性（可以通过Hessian的奇异性来衡量）意味着吸引力盆地的经典概念可能会有很大的误导性。而且关于宽/窄盆地的讨论可能需要围绕过度参数化和能够在景观底部创造*大*连接成分的冗余来进行新的视角。 其次，少数大特征值对数据分布的依赖性可以与模型输出梯度的协方差矩阵的频谱相联系。考虑到这一点，我们可以重新评估模型的数据-结构-算法框架内的联系，希望它能阐明现代应用中高维和非凸空间的几何学。 特别是，我们提出了一个将两个观察结果联系起来的案例：小批量和大批量梯度下降似乎收敛于不同的吸引盆地，但我们表明它们实际上是通过其平坦区域连接的，因此属于同一个盆地。
骨干计算资源的分配是物体检测中的一个关键问题。然而，分类分配模式通常是直接采用物体检测器，这被证明是次优的。为了以更有效的方式重新分配所从事的计算资源，我们提出了CR-NAS（计算重新分配神经结构搜索），可以在目标检测数据集上精确地学习不同特征分辨率和空间位置的计算重新分配策略。提出了一个两级重新分配空间，用于阶段和空间重新分配。 我们将CR-NAS应用于多个骨干网，并取得了一致的改进。我们的CR-ResNet50和CR-MobileNetV2分别比基线高出1.9%和1.7%的COCO AP，而没有任何额外的计算预算。 CR-NAS发现的模型可以装备到其他强大的检测颈部/头部，并可以很容易地转移到其他数据集，如PASCAL VOC，以及其他视觉任务，如实例分割。我们的CR-NAS可以作为一个插件来提高各种网络的性能，这一点要求很高。
不确定性是智能的一个非常重要的特征，有助于大脑成为一个灵活的、创造性的和强大的智能系统。基于crossbar的神经形态计算芯片，其中的计算主要由模拟电路完成，具有不确定性，可以用来模仿大脑。然而，目前大多数的深度神经网络没有考虑到神经形态计算芯片的不确定性。 因此，它们在神经形态计算芯片上的表现不如在原始平台（CPU/GPU）上的表现好。在这项工作中，我们提出了不确定性适应训练方案（UATS），在训练过程中把不确定性告诉神经网络。实验结果表明，与原始平台上的结果相比，神经网络在不确定的神经形态计算芯片上可以达到相当的推理表现，而且比没有这种训练方案的表现好得多。
现实世界中图像分类系统的一个重要特性是，它们既能准确地将物体从目标类（"已知"）中分类出来，又能安全地拒绝属于训练数据中不存在的类的未知物体（"未知"）。不幸的是，尽管现有CNN的强大泛化能力保证了它们在对已知物体进行分类时的准确性，但这也导致它们经常以高置信度将未知物体分配到目标类。 因此，简单地使用低置信度检测作为检测未知物的方式效果并不好。在这项工作中，我们提出了一个未知的深度神经网络（简称UDN）来解决这个具有挑战性的问题。UDN的关键思想是增强现有的CNN，以支持产品操作，该操作对卷积层产生的特征之间的产品关系进行建模。 这样一来，缺少一个目标类别的单一关键特征将大大降低将一个物体分配到这个类别的概率。UDN使用这些产品操作的学习集合，这使得它能够平衡准确分类已知物体和正确拒绝未知物体的矛盾要求。为了进一步提高UDN检测未知物体的性能，我们提出了一个信息理论的正则化策略，将拒绝未知物体的目标纳入UDN的学习过程。 我们在包括MNIST、CIFAR-10、CIFAR-100和SVHN在内的基准图像数据集上进行了实验，通过将一个数据集注入另一个数据集来增加未知数。我们的结果表明，UDN在拒绝未知数方面明显优于最先进的方法，准确率提高了25个百分点，同时仍然保留了分类准确率。
具有缺失值的多变量时间序列在医疗和金融等领域很常见，而且多年来数量和复杂性都在增加。这就提出了一个问题，即深度学习方法在这一领域能否胜过经典的数据归置方法。然而，深度学习的天真应用在给出可靠的置信度估计方面存在缺陷，而且缺乏可解释性。 我们的建模假设很简单，而且可以解释：高维时间序列有一个低维表示，根据高斯过程在时间上平滑演化。在存在缺失数据的情况下，非线性降维是通过一个具有新型结构化变异近似的VAE方法实现的。我们证明，在计算机视觉和医疗领域的高维数据上，我们的方法优于几个经典的和基于深度学习的数据归因方法，同时还提高了归因的平滑度并提供可解释的不确定性估计。
在实践中，人们经常发现，大型超参数神经网络的泛化能力比小型的同类网络要好，这种观察似乎与函数复杂性的经典概念相冲突，后者通常有利于小型模型。在这项工作中，我们通过对与输入扰动的敏感性有关的两个自然的复杂性指标进行广泛的经验探索，来研究复杂性和泛化之间的矛盾。 我们发现，经过训练的神经网络对训练数据流形附近的输入扰动更为稳健，这可以通过网络的输入-输出雅各布系数来衡量，而且这与泛化有很好的关联。 我们进一步确定，与不良泛化相关的因素--如全批训练或使用随机标签--对应于更高的灵敏度，而与良好的泛化相关的因素--如数据增强和ReLU非线性--产生了更多的鲁棒性功能。最后，我们证明了输入-输出雅各布常数如何能够预测单个测试点水平上的泛化。
深度强化学习（RL）方法通常通过在行动空间注入噪声来进行探索行为。另一种方法是直接向代理的参数添加噪声，这可以导致更一致的探索和更丰富的行为集。进化策略等方法使用参数扰动，但在这个过程中抛弃了所有的时间结构，并需要明显更多的样本。 将参数噪声与传统的RL方法相结合，可以将两者的优点结合起来。我们通过对DQN、DDPG和TRPO在高维离散行动环境以及连续控制任务上的实验比较，证明非政策性方法和政策性方法都能从这种方法中受益。
预训练的深度神经网络语言模型，如ELMo、GPT、BERT和XLNet最近在各种语言理解任务上取得了最先进的性能。然而，它们的大小使它们在一些场景中不切实际，特别是在移动和边缘设备上。 特别是，由于输入词汇和嵌入维度较大，输入词嵌入矩阵占模型内存足迹的很大一部分。知识提炼技术在压缩大型神经网络模型方面取得了成功，但它们在产生具有与原始教师模型不同词汇的学生模型方面效果不佳。 我们引入了一种新的知识蒸馏技术，用于训练具有明显较小词汇量以及较低嵌入和隐藏状态维度的学生模型。具体而言，我们采用了一种双训练机制，同时训练教师和学生模型，以获得学生词汇的最佳词嵌入。 我们将这种方法与学习共享投影矩阵结合起来，将层级知识从教师模型转移到学生模型。我们的方法能够将BERT-BASE模型压缩60倍以上，而下游任务指标仅有小幅下降，从而使语言模型的占用空间低于7MB。
人类的推理包括通过利用变量来识别许多例子中共同的基本原则。这种推理的副产品是不变量，它可以捕捉到整个例子中的模式，例如 "如果有人去了某个地方，那么他们就在那里"，而不需要提到具体的人或地方。人类在年轻时就学会了什么是变量以及如何使用它们，本文要解决的问题是机器是否也可以完全从例子中学习和使用变量而不需要人类预先设计。 我们提出了统一网络，它将软统一纳入神经网络来学习变量，并通过这样做将例子提升为不变量，然后可以用来解决一个给定的任务。我们在四个数据集上评估了我们的方法，以证明学习不变量可以捕捉到数据中的模式，并可以提高比基线的性能。
我们提出了一种新的基于学习的方法来解决成像中的问题。我们解决的情况是，地面真实训练样本很少，问题是严重的问题--既因为基础物理学，也因为我们只能得到很少的测量数据。这种情况在地球物理成像和遥感中很常见。我们表明，在这种情况下，直接学习从测量数据到重建的映射的常见方法变得不稳定。 相反，我们建议首先学习一个从数据到未知图像的投影的较简单的映射集合，然后通过解决一个类似解卷积的问题将投影结合起来形成最终的重建。
为深度神经网络设计架构需要专家知识和大量的计算时间。我们提出了一种技术，通过学习一个辅助的超网络来加速架构的选择，该网络可以根据模型的架构生成主模型的权重。通过比较具有超网络生成权重的网络的相对验证性能，我们可以以单次训练的代价有效地搜索广泛的架构。 为了促进这种搜索，我们开发了一种基于内存读写的灵活机制，允许我们定义广泛的网络连接模式，其中ResNet、DenseNet和FractalNet块是特殊情况。我们在CIFAR-10和CIFAR-100、STL-10、ModelNet10和Imagenet32x32上验证了我们的方法（SMASH），取得了与类似规模的手工设计的网络相竞争的性能。
事实证明，在需要语言理解的任务中，文本嵌套（或NLI）数据作为预训练数据是非常有用的，即使是在像RoBERTa这样已经预训练过的模型上。 使用这些替代方案和一个简单的基于MNLI的基线，我们收集并比较了五个新的9k例子训练集。我们的主要结果基本上是负面的，这些新方法中没有一个在迁移学习中显示出重大的改进。然而，我们提出了几个意见，应该告知未来关于NLI数据的工作，例如，使用自动提供的种子句子来激发灵感，在大多数措施上提高了所产生的数据的质量，而且我们调查的所有干预措施都大大减少以前观察到的注释人工制品的问题。
在现实世界的环境中预测未来，尤其是从图像等原始感官观察中预测，是非常具有挑战性的。现实世界的事件可能是随机的和不可预测的，自然图像的高维度和复杂性要求预测模型建立对自然世界的复杂理解。许多现有的方法通过对环境的简化假设来解决这个问题。一个常见的假设是结果是确定的，只有一个看似合理的未来。 在本文中，我们开发了一种随机变异视频预测（SV2P）方法，对其潜变量的每个样本预测不同的可能未来。据我们所知，我们的模型是第一个为现实世界视频提供有效的随机多帧预测的模型。 我们展示了所提出的方法在多个真实世界的数据集上预测视频的详细未来帧的能力，包括无动作和有动作的数据集。我们发现，与没有随机性的相同模型以及其他随机视频预测方法相比，我们提出的方法产生了大幅改善的视频预测。我们的SV2P实现将在发表后开放源代码。
在多代理系统中，由于代理之间的高度关联性，会产生复杂的交互行为。然而，以前从演示中对多代理交互进行建模的工作主要受限于假设政策之间的独立性及其奖励结构。在本文中，我们将多代理互动建模问题投射到多代理模仿学习框架中，通过近似对手的政策对相关政策进行明确建模，可以恢复能够重新生成类似互动的代理政策。 因此，我们开发了一种具有相关政策的分散对抗模仿学习算法（CoDAIL），它可以进行分散的训练和执行。各种实验证明，CoDAIL可以更好地再生出接近示范者的复杂互动，并优于最先进的多Agent模仿学习方法。我们的代码可在url{https://github.com/apexrl/CoDAIL}上找到。
随着互联网的发展，抄袭和文本重用变得越来越多。因此，检查科学论文的作弊事实是很重要的，特别是在学术界。现有的抄袭检测系统显示出良好的性能，并且拥有庞大的源数据库。因此，现在仅仅从源文件中复制文本来获得原创作品是不够的。因此，另一种类型的抄袭变得很流行--跨语言的抄袭。
数据并行已经成为在多个节点上扩展深度神经网络（DNN）训练的主流方法。 由于局部模型或梯度的同步化可能是大规模分布式训练的瓶颈，压缩通信流量最近得到了广泛的关注。 在最近提出的几种压缩算法中，剩余梯度压缩（RGC）是最成功的方法之一---它可以显著压缩每个节点的传输信息大小（梯度大小的0.1%），并且仍然保持精度。然而，关于压缩深度网络的文献几乎只关注于实现良好的压缩率，而对RGC在实际执行中的效率研究较少。在本文中，我们开发了一种RGC方法，在现实世界的多GPU系统中实现了训练时间的显著改善。 我们提出的RGC系统设计称为RedSync，引入了一系列优化，以减少通信带宽，同时引入有限的开销。我们在两个不同的多GPU平台上检查了RedSync的性能，包括一台超级计算机和一台多卡服务器。我们的测试案例包括Cifar10和ImageNet的图像分类，以及Penn Treebank和Wiki2数据集的语言建模任务。
Backpropagation是当今人工神经网络的驱动力。然而，尽管进行了广泛的研究，但仍不清楚大脑是否实现了这种算法。在神经科学家中，强化学习（RL）算法通常被视为一种现实的替代方案。然而，这种学习的收敛率随着参与的神经元数量的增加而变化。 在这里，我们提出了一种混合学习方法，其中每个神经元使用RL类型的策略来学习如何近似反向传播将提供的梯度。我们表明，我们的方法可以学习到近似的梯度，并且可以匹配全连接和卷积网络上基于梯度的学习性能。
大多数深度神经网络（DNNs）需要复杂的模型来实现高性能。参数量化被广泛用于减少执行的复杂性。以前关于量化的研究大多是基于使用训练数据的广泛模拟。我们选择了一种不同的方法，试图测量DNN模型的每个参数能力，并解释结果以获得关于参数最佳量化的见解。这项研究使用人工生成的数据和全连接DNNs、卷积神经网络和循环神经网络的通用形式。 我们进行了记忆和分类测试，以研究参数的数量和精度对性能的影响。通过测量输入和分类输出之间的相互信息来评估模型和每个参数的能力。我们还将记忆能力测量结果扩展到图像分类和语言建模任务。
受视觉皮层中前馈和迭代计算的启发，并利用去噪自动编码器估计联合分布得分的能力，我们提出了一种新的迭代推理方法，用于捕捉和利用以某些输入变量为条件的输出变量的复杂联合分布。 这种方法被应用于图像像素分割，估计的条件得分被用来对估计的条件分布的模式进行梯度上升。这将以前通过去噪自动编码器进行得分估计的工作扩展到条件分布的情况下，新的方法是使用一个被破坏的前馈预测器来代替高斯破坏。 与对结构化输出进行迭代推理的更经典的方法相比，这种方法的一个优点是，它不再需要定义连接输出变量的明确的能量函数。为了保持计算的可操作性，这种能量函数参数通常是相当受限制的，只涉及每个悬崖中的每个输出变量的几个邻居。 我们在实验中发现，所提出的由条件去噪自动编码器进行的条件分数估计的迭代推理比基于CRF的可比模型或那些不使用输出的条件联合分布的明确模型表现得更好。
受自我注意机制和Transformer架构在序列转导和图像生成应用中的成功启发，我们提出了新的基于自我注意的架构，以改善文本生成中基于对抗性潜伏代码的方案的性能。 在我们的实验中，利用谷歌句子压缩数据集，用各种客观和主观的措施将我们的方法与这些方法进行比较。实验表明，所提出的（自我）基于注意力的模型在基于对抗性代码的文本生成中的表现优于最先进的。
水印已被用于各种目的。最近，研究人员开始研究将水印用于深度神经网络。一些作品试图在攻击神经网络时在其对抗性样本上隐藏攻击触发器，另一些作品则希望通过水印神经网络来证明其所有权以防止剽窃。 在神经网络中植入后门水印模块正得到社区的更多关注。在本文中，我们提出了一种通用的编码器-解码器联合训练方法，其灵感来自生成对抗网络（GANs）。然而，与GANs不同，我们的编码器和解码器神经网络合作寻找给定数据样本的最佳水印方案。 换句话说，我们不设计任何新的水印策略，但我们提出的两个神经网络会自己找到最适合的方法。经过训练后，解码器可以被植入其他神经网络，以攻击或保护它们（关于它们的使用案例和实际实现，见附录）。 为此，解码器应该非常小，以便在连接到其他神经网络时不产生任何开销，但同时提供非常高的解码成功率，这是非常具有挑战性的。我们的联合训练方法成功地解决了这个问题，在我们的实验中，在对数据样本进行很少的修改以隐藏水印的情况下，多个数据集的编码-解码成功率几乎保持在100%。我们还在附录中介绍了几个真实世界的用例。
我们推导出一个基于无替换抽样的离散随机变量期望值的无偏估计器，该估计器由于避免了重复抽样而减少了方差。我们表明，我们的估计器可以作为三个不同估计器的Rao-Blackwellization来推导。将我们的估计器与REINFORCE相结合，我们得到了一个政策梯度估计器，我们使用一个内置控制变量来减少其方差，该变量无需额外模型评估即可获得。 由此产生的估计器与其他梯度估计器密切相关。用一个玩具问题、一个分类变异自动编码器和一个结构化预测问题进行的实验表明，我们的估计器是唯一一个在高熵和低熵情况下都能持续属于最佳估计器的估计器。
我们引入了一个参数共享方案，其中卷积神经网络（CNN）的不同层是由来自全球模板库的参数张量的线性组合来定义的。 限制模板的数量会产生传统CNN和递归网络的灵活混合。 与传统的CNN相比，我们在标准的图像分类任务上证明了大量的参数节省，同时保持了准确性。我们简单的参数共享方案，虽然是通过软权重定义的，但在实践中经常产生训练有素的网络，具有近乎严格的递归结构；在可忽略的副作用下，它们转化为具有实际循环的网络。 因此，训练这些网络隐含着对合适的递归结构的发现。虽然只考虑了递归链接的方面，我们训练的网络达到了与那些使用最先进的神经结构搜索（NAS）程序构建的网络相竞争的精度。我们对递归和卷积网络的混合也可能代表了一种有益的结构偏差。 具体来说，在具有算法性质的合成任务上，我们的混合网络既能更快地训练，又能更好地推断出训练集范围之外的测试实例。
梯度剪裁是深度网络训练中广泛使用的技术，一般是从优化的角度出发的：非正式地，它控制迭代的动态，从而提高收敛到局部最小值的速度。 这一直觉在最近的一系列工作中得到了精确的体现，这些工作表明，适当的剪裁可以产生比虚无的梯度下降更快的收敛速度。在本文中，我们提出了一个研究梯度剪裁的新视角，即鲁棒性：非正式地，人们期望剪裁能够提供对噪声的鲁棒性，因为我们不会过分相信任何单一的样本。 令人惊讶的是，我们证明，对于分类中常见的标签噪声问题，标准梯度剪裁一般不提供鲁棒性。另一方面，我们表明，梯度剪裁的一个简单变体是可证明的鲁棒性，并对应于适当地修改基础损失函数。这产生了一个简单的、噪声鲁棒的标准交叉熵损失的替代品，在经验上表现良好。
 在深度生成模型中，基于流的模型，在本文中简称为 "流"，与其他模型不同的是，它们提供了可操作的似然。 然而，据观察，在FashionMNIST上训练的流量对MNIST的OoD样本分配了更高的可能性。这个反直觉的观察引起了人们对流量可能性的鲁棒性的关注。我们选择了两个典型的流量作为目标模型：基于耦合变换的Glow和基于自回归变换的pixelCNN。我们的实验揭示了流量的可能性和图像语义之间惊人的弱相关性：流量的预测可能性会受到保持图像语义不变的微不足道的变换的严重影响，我们称之为语义不变的变换~（SIT）。 我们探讨了三种SITs~（都是小的像素级修改）：图像像素平移、随机噪声扰动、潜在因素归零~（仅限于使用多尺度架构的流量，例如Glow）。 这些发现，虽然是反直觉的，但却与一个事实产生了共鸣，即一个流的预测可能性是所有图像像素的联合概率。所以流的可能性，在像素级强度上建模，并不能表明高级图像语义的存在可能性。我们呼吁注意，如果我们使用流的预测可能性进行OoD样本检测，这可能是emph{abuse}。
本文研究了通过在将输入物送入系统之前对输入物进行转换来防御对图像分类系统的对抗性样本攻击的策略。具体而言，我们研究了在将图像送入卷积网络分类器之前应用图像转换，如比特深度减少、JPEG压缩、总方差最小化和图像绗缝。 我们在ImageNet上的实验表明，总方差最小化和图像绗缝在实践中是非常有效的防御措施，特别是当网络在转换后的图像上训练时。这些防御措施的优势在于它们的无差异性和内在的随机性，这使得对手很难规避防御措施。我们的最佳防御措施消除了各种主要攻击方法的60%强灰盒和90%强黑盒攻击。
	在本文中，我们提出了异步加速非均匀随机块坐标下降算法（A2BCD）。我们证明A2BCD以与NU_ACDM相同的速度线性收敛到凸最小化问题的解，只要最大延迟不是太大。这是第一个达到任何可证明速度的异步Nesterov加速算法。此外，我们随后证明这些算法都具有最佳复杂度。 因此，我们在实验中观察到，A2BCD是性能最好的坐标下降算法，在一些数据集上的收敛速度比NU_ACDM快4-5倍，就壁时钟时间而言。为了激励我们的理论和证明技术，我们还推导和分析了我们算法的连续时间类似物，并证明它以同样的速度收敛。
通过将采样器嵌入到变异后验近似中，介绍了概率程序中高效贝叶斯推断的框架。它的优势在于易于实现和自动调整采样器参数以加快混合时间。介绍了近似证据下限（ELBO）计算的几种策略，包括ELBO目标的重写。 通过对密度估计任务的无条件VAE进行实验，展示了实验证据；用条件变异自动编码器（cVAE）作为深贝叶斯分类器解决高维空间中的影响图；以及时间序列数据的状态空间模型。
ReLU分类网络的点估计，可以说是最广泛使用的神经网络架构，最近被证明在远离训练数据的地方具有任意高的置信度。经验表明，权重空间上的近似贝叶斯后验可以改善深度学习中的预测不确定性，这种贝叶斯近似的理论分析是有限的，包括ReLU分类网络。 我们表明，即使是一个简单的（因此是便宜的）、非贝叶斯的高斯分布也能解决渐进的过度自信问题。此外，当采用贝叶斯方法，即使是一个简单的方法，来获得高斯时，置信度会变得更好。这个理论结果促使一系列拉普拉斯近似沿着保真度成本权衡。我们通过使用常见的深度ReLU网络进行实验来验证这些发现。
词汇对齐对于统计和神经机器翻译（NMT）以及注释预测等任务非常有用。统计词汇对齐器表现良好，在NMT中与翻译共同提取对齐的方法也是如此。然而，大多数方法需要平行训练数据，并且质量会随着训练数据的减少而下降。 我们的多语言嵌入仅从单语数据中创建，而不依赖任何平行数据或词典。我们发现，传统的统计对准器被上下文嵌入所超越--即使在有大量平行数据的情况下。例如，对于一组100k的平行句子，上下文嵌入实现的单词对准F1比eflomal高5%以上（绝对值）。
最近的研究表明，强化学习（RL）模型在噪声环境中的脆弱性。噪声的来源在不同的情况下有所不同。在本文中，我们考虑了嘈杂的RL问题，其中RL代理观察到的奖励是通过奖励混淆矩阵产生的，我们把这种观察到的奖励称为扰动的奖励。 我们的框架借鉴了有噪声数据的监督学习方法。我们解决方案的核心思想包括估计奖励混淆矩阵和定义一组无偏代用奖励。我们证明了我们方法的收敛性和样本复杂性。在不同的DRL平台上进行的广泛实验表明，基于我们估计的代用奖励的政策可以获得更高的预期奖励，并且比现有的基线收敛得更快。例如，最先进的PPO算法能够在五个Atari游戏中获得67.5%和46.7%的平均改进，此时错误率分别为10%和30%。
使用通过时间的反向传播（BPTT）在长序列上训练循环神经网络（RNN）仍然是一个基本挑战。有研究表明，在优化目标中加入局部无监督损失项可以使长序列上的RNNs训练更加有效。虽然无监督任务的重要性原则上可以由目标函数中的一个系数来控制，但关于无监督损失项的梯度仍然影响着所有的隐藏状态维度，这可能导致关于监督任务的重要信息被降低或抹去。与现有的半监督序列学习方法相比，本文专注于一个传统上被忽视的机制--一个具有明确设计的私有和共享隐藏单元的架构，旨在减轻辅助性无监督损失对主要监督任务的不利影响。 我们通过将RNN隐藏空间划分为监督任务的私有空间和监督与非监督任务的共享空间来实现这一目标。我们在几个长序列建模基准数据集上对所提出的框架进行了广泛的实验。结果表明，所提出的框架可以在RNN模型中产生性能的提升，而长期的依赖性是众所周知的处理挑战。
我们研究了对所有任务使用共享特征表示的多任务学习方法。为了更好地理解任务信息的转移，我们研究了对所有任务使用共享模块和对每个任务使用单独的输出模块的架构。我们研究了对线性和ReLU激活模型的这种设置的理论。我们的关键观察是，任务的数据是否良好对齐可以大大影响多任务学习的性能。 在理论见解的启发下，我们表明，对齐任务的嵌入层会导致GLUE基准和情感分析任务的多任务训练和转移学习的性能提高；例如，我们使用我们的对齐方法在5个GLUE任务上获得了比BERT LARGE平均2.35%的GLUE得分提高。我们还设计了一个基于SVD的任务重权方案，并表明它提高了多标签图像数据集上的多任务训练的鲁棒性。
我们回顾了BLEU和ROUGE的三个局限性--这是用来评估参考摘要与假设摘要的最流行的指标，提出了一个好的指标应该表现为什么的标准，并提出了详细评估指标性能的具体方法，展示了基于变形金刚的语言模型评估参考摘要与假设摘要的潜力。
在本文中，我们探讨了元学习在少数文本分类中的应用。元学习在计算机视觉中表现出了强大的性能，在这种情况下，低层次的模式是可以跨学习任务转移的。然而，直接将这种方法应用于文本是具有挑战性的--对于一个任务来说信息量很大的词汇特征对于另一个任务来说可能并不重要。 因此，我们的模型不是仅仅从单词中学习，而是利用它们的分布特征，这些特征编码了相关的单词出现模式。我们的模型在元学习框架内进行训练，将这些特征映射为注意力分数，然后用于加权单词的词汇表征。我们证明，我们的模型始终优于根据词汇知识学习的原型网络（Snell et al, 2017）在六个基准数据集中，在少量文本分类和关系分类中都有明显的优势（在单次分类中平均为19.96%）。
神经科学领域对神经计算的描述依赖于两种相互竞争的观点：（i）经典的单细胞观点，将单个神经元的活动与感觉或行为变量联系起来，并关注不同细胞类别如何映射到计算上；（ii）最近的群体观点，以集体神经轨迹来描述计算，并关注动物执行任务时这些轨迹的维度。 然而，细胞类别和低维轨迹这两个关键概念是如何相互作用以形成神经计算的，目前还不清楚。在这里，我们通过将训练RNN的机器学习工具与网络动态的逆向工程和理论分析相结合来解决这个问题。我们介绍了一类新的理论上可行的循环网络：低等级、高斯混合RNN。 在这些网络中，连接的等级控制着动态的维度，而高斯混合物中的成分数量与细胞类别的数量相对应。使用反向传播，我们确定了实现日益复杂的神经科学任务所需的最小等级和细胞类别数量。 我们表明，等级决定了可用于实现输入-输出映射的动力学的相位空间，而拥有多个细胞类别可以使网络在可用的相位空间中灵活地切换不同类型的动力学。我们的结果对神经科学实验的分析和可解释人工智能的发展有意义。
我们提出了融合判别器，这是一个将条件信息纳入生成对抗网络（GAN）的单一统一框架，用于各种不同的结构预测任务，包括图像合成、语义分割和深度估计。 与常用的卷积神经网络--条件马尔科夫随机场（CNN-CRF）模型非常相似，所提出的方法能够在模型中强制执行高阶一致性，但不局限于非常具体的潜力类别。该方法在概念上简单而灵活，我们的实验结果表明在几个不同的结构化预测任务上有所改进。
基于模型的强化学习（MBRL）已被证明是一个强大的框架，可以有效地学习控制连续任务。最近MBRL的工作主要集中在使用更先进的函数近似器和规划方案，使一般框架自其概念以来几乎没有变化。在本文中，我们确定了标准MBRL框架的一个基本问题--我们称之为目标不匹配问题。 当一个目标被优化时，希望第二个通常不相关的指标也能被优化，就会出现目标不匹配的情况。在MBRL的背景下，我们描述了训练前向动力学模型（即提前一步预测的可能性）与提高下游控制任务性能的总体目标之间的目标不匹配。 例如，这个问题会随着以下认识而出现：对特定任务有效的动力学模型不一定需要全局精确，反之，全局精确的模型在局部可能不够精确，无法在特定任务上获得良好的控制性能。在我们的实验中，我们研究了这个目标不匹配问题，并证明一步预测的可能性并不总是与下游控制性能相关。 这一观察结果突出了当前MBRL框架中的一个关键缺陷，需要进一步的研究来充分理解和解决。我们提出了一个初步的方法，通过重新加权动力学模型训练来缓解不匹配问题。在此基础上，我们最后讨论了未来解决这一问题的其他潜在研究方向。
最近，关于使用信息论的技术来测量深度神经网络中的信息流，有一场激烈的辩论（例如Schwartz-Ziv & Tishby（2017），Saxe等人（2018），Noshad等人（2018），Goldfeld等人（2018））。 据称，一般来说，深度神经网络具有良好的泛化能力，因为它们不仅学会了如何从输入映射到输出，还学会了如何压缩训练数据输入的信息（Schwartz-Ziv & Tishby, 2017）.也就是说，它们对输入信息进行抽象，并剥离任何不必要的或过于特殊的信息。 如果是这样，信息压缩方法，即信息瓶颈（IB），可以作为网络性能的自然比较器，因为这种方法给出了一个最佳的信息压缩边界。这种说法后来被谴责以及重申（如Saxe等人（2018），Achille等人（2017），Noshad等人。(2018），因为采用的互信息测量方法实际上不是测量信息，而是测量内部层表征的聚类（Goldfeld等人（2018））。在本文中，我们将详细解释信息平原（IP）中的发展，这是一种比较互信息判断压缩的情节类型（Schwartz-Ziv & Tishby（2017）），当噪声被追溯添加（使用分选估计）。 我们还解释了为什么不同的激活函数在IP上显示出不同的轨迹。此外，我们还研究了聚类对网络损失的影响，通过使用信息平面的早期和完美停止，以及如何使用聚类来帮助网络修剪。
对深度卷积网络背后的归纳偏见的正式理解是有限的，即网络的结构特征和它能够建模的功能之间的关系。在这项工作中，我们在量子物理学和深度学习领域之间建立了一个基本的联系，并利用它来获得关于卷积网络的归纳偏见的新理论观察。 具体来说，我们展示了卷积算术电路（ConvAC）实现的功能与量子多体波函数之间的结构等价性，这有利于使用量子纠缠度量作为深度网络对相关关系建模的表达能力的量化器。 此外，在量子张量网络方面构建深度ConvAC是可行的。这使我们能够对卷积网络进行图论分析，将其表现力与底层图中的最小切口联系起来。我们以通过每层的通道数量（宽度）直接控制归纳偏差的形式展示了一个实际的结果。 我们在涉及ReLU激活和最大池的标准卷积网络上实证验证了我们的发现。用定义明确的图论工具描述深度卷积网络以及与量子纠缠的结构联系，是这项工作带来的两个跨学科的桥梁。
深度学习算法越来越多地被用于化学过程建模。然而，没有理由的黑箱预测在实际应用中的应用有限，如药物设计。我们将这一问题表述为分子图上的强化学习问题，由两个卷积网络对应于理由选择和基于理由的预测，其中后者引起奖励函数。 我们在两个基准毒性数据集上对该方法进行了评估。我们证明，在预测严格遵循理由的额外约束下，我们的模型保持了较高的性能。此外，我们通过与化学文献中描述的理由进行比较，并通过合成实验验证了提取的理由。
近年来，图卷积网络（GCN）取得了实质性的进展。在本文中，我们首次从理论上分析了GCN和矩阵分解（MF）之间的联系，并将GCN统一为具有协同训练和单元化的矩阵分解。此外，在这个理论分析的指导下，我们提出了一个GCN的替代模型，名为协同训练和单元化矩阵分解（CUMF）。 实验结果表明，与GCN相比，CUMF实现了类似或更高的性能。此外，CUMF继承了基于MF的方法的优点，自然地支持构建小批量，与GCN相比，对分布式计算更加友好。分布式CUMF在半监督节点分类上的表现明显优于分布式GCN方法。
分子图生成是药物发现的一个基本问题，已经引起了越来越多的关注。这个问题具有挑战性，因为它不仅需要生成化学上有效的分子结构，同时还要优化它们的化学性质。(实验结果表明，GraphAF即使在没有化学知识规则的情况下也能生成68%的化学有效分子，在有化学规则的情况下也能生成100%的有效分子。 GraphAF的训练过程比现有的最先进的方法GCPN快2倍。在用强化学习对目标导向的属性优化模型进行微调后，GraphAF在化学属性优化和受限属性优化上都达到了最先进的性能。
我们在一个统一的框架中研究了两种类型的前置条件和前置条件随机梯度下降（SGD）方法。由于第一种类型与牛顿方法关系密切，我们称其为牛顿类型，第二种类型为Fisher类型，因为其前置条件与Fisher信息矩阵的逆值密切相关。这两种前置条件可以从一个框架中导出，并在用户指定的任何矩阵列组上使用自然或相对梯度下降最小化某些前置条件估计标准进行高效估计。RMSProp、Adam、KFAC、平衡SGD、批量归一化等，都是牛顿型或Fisher型的特例或与之密切相关。
我们提出了EDA：用于提高文本分类任务性能的简易数据增强技术。EDA包括四个简单而强大的操作：同义词替换、随机插入、随机交换和随机删除。在五个文本分类任务中，我们表明EDA提高了卷积和递归神经网络的性能。 EDA对较小的数据集表现出特别强的效果；平均而言，在五个数据集中，用EDA训练，同时只使用50%的可用训练集，取得了与所有可用数据的正常训练相同的准确率。
我们提出了一个能够对视频中的系统进行物理参数估计的模型，其中支配场景动力学的微分方程是已知的，但标记的状态或物体是不可用的。现有的物理场景理解方法要么需要物体状态监督，要么不与可微分物理学结合以学习可解释的系统参数和状态。我们通过一个textit{物理学即逆向图形}方法来解决这个问题，它将视觉即逆向图形和可微分物理学引擎结合起来，其中物体和明确的状态和速度表示是由模型发现。 这个框架使我们能够进行长期的外推式视频预测，以及基于视觉的模型预测控制。我们的方法在具有相互作用的物体的系统（如球-弹簧或3体引力系统）的长期未来帧预测中明显优于相关的无监督方法，因为它能够将动力学作为一个归纳的偏向建立在模型中。 我们进一步展示了这种视觉与物理学的紧密结合的价值，展示了基于视觉驱动模型的摆锤系统的数据高效学习。我们还展示了控制器的可解释性，在目标驱动控制和物理推理的零数据适应方面提供了独特的能力。
本文提出了ASAL，一种新的基于池的主动学习方法，可以生成高熵的样本。ASAL不是直接对合成样本进行注释，而是从池中搜索类似的样本，并将其纳入训练。因此，新样本的质量很高，注释也很可靠。 ASAL特别适用于大数据集，因为它比传统的不确定性抽样（线性）实现了更好的样本选择的运行时间复杂性（亚线性）。我们在两个数据集上提出了一套全面的实验，并表明ASAL优于类似的方法，明显超过了既定的基线（随机抽样）。 在讨论部分，我们分析了ASAL在哪些情况下表现最好，以及为什么有时很难超过随机抽样。据我们所知，这是第一个使用深度卷积分类器的多类问题的对抗性主动学习技术，并且表现出比随机抽样更优越的性能。
我们指出了使用最佳单一模型性能来比较深度学习架构的常见做法的重要问题，并提出了一种纠正这些缺陷的方法。每次训练一个模型时，由于训练过程中的随机因素，包括随机参数初始化和随机数据洗牌，都会得到不同的结果，报告最佳单一模型性能不能适当地解决这种随机性。
无监督领域适应的目的是将在源域中训练的假设推广到未标记的目标域。对这个问题的一个流行的方法是学习两个领域的领域变量嵌入。 特别是，这种复杂性会影响目标风险的上限；这也反映在实验中。接下来，我们将理论框架具体化到多层神经网络上。因此，我们开发了一种策略，缓解了对嵌入复杂性的敏感性，并根据经验实现了与最佳层依赖复杂性权衡相当或更好的性能。
我们提出了一个新的架构，称为双对抗转移网络（DATNet），用于解决低资源的命名实体识别（NER）。DATNet-F和DATNet-P，被提出来探索高资源和低资源之间的有效特征融合。为了解决嘈杂和不平衡的训练数据，我们提出了一个新的通用资源对抗性判别器（GRAD）。此外，对抗性训练被采用来提高模型的通用性。 我们研究了DATNet中不同组件对不同领域和语言的影响，并表明特别是对低资源数据可以获得明显的改善。在不增加任何额外的手工制作的特征的情况下，我们在CoNLL和Twitter NER上实现了新的最先进的性能--西班牙语的88.16% F1，WNUT-2016的53.43% F1，以及WNUT-2017的42.83% F1。
生成式对抗网络（GANs）已经发展成为生成真实图像的最成功的无监督技术之一。尽管最近表明GAN训练会收敛，但GAN模型经常在局部纳什平衡中结束，这与模式崩溃有关，或以其他方式未能对目标分布进行建模。我们引入库伦GANs，它将GAN学习问题作为一个势场，其中生成的样本被训练集样本吸引，但相互排斥。 鉴别器学习势场，而生成器通过沿势场梯度决定的矢量（力）场移动其样本来减少能量。通过减少能量，GAN模型学会根据整个目标分布生成样本，而不是只覆盖其部分模式。 我们证明库仑GANs只拥有一个纳什均衡，在模型分布等于目标分布的意义上是最优的。我们展示了库仑GANs在LSUN卧室、CelebA面孔、CIFAR-10和Google Billion Word文本生成中的功效。
预测图中节点的属性是一个重要的问题，在各种领域都有应用。基于图的半监督学习（SSL）方法旨在通过标记一小部分节点作为种子，然后利用图结构来预测图中其他节点的标签分数来解决这个问题。 最近，图卷积网络（GCN）在基于图的SSL任务上取得了令人印象深刻的性能。除了标签分数，还希望有一个与之相关的信心分数。 我们在本文中填补了这一重要空白，并提出了ConfGCN，它在基于GCN的设置中联合估计标签分数和它们的置信度。ConfGCN使用这些估计的置信度来确定一个节点在邻域聚合期间对另一个节点的影响，从而获得各向异性的能力。
在本文中，我们为图像分类器的鲁棒性建立了严格的基准。我们的第一个基准，ImageNet-C，标准化并扩展了腐败的鲁棒性主题，同时显示了哪些分类器在安全关键应用中是可取的。然后我们提出了一个新的数据集，称为ImageNet-P，使研究人员能够对分类器对常见扰动的鲁棒性进行基准测试。 与最近的鲁棒性研究不同，这个基准评估的是常见的腐败和扰动的性能，而不是最坏情况下的对抗性扰动。我们发现，从AlexNet分类器到ResNet分类器的相对腐败鲁棒性的变化可以忽略不计。之后，我们发现了增强腐败和扰动鲁棒性的方法。我们甚至发现，绕过对抗性防御提供了大量常见扰动鲁棒性。
本文通过在密码文本生成和密钥生成中应用基于文本的递归神经网络（RNN）编码器-解码器模型，探讨了源代码混淆中的一种新方法。 与现有混淆方法的定量基准比较表明，所提出的解决方案在隐蔽性和执行成本方面有明显的改善，关于模型属性的实验在其特征变化、与原始代码库的不相似性以及混淆代码的一致长度方面产生了积极的结果。
我们提出了一种用GAN进行图像和每像素注释合成的方法。我们证明GAN对目标数据有很好的高层表示，可以很容易地投射到语义分割掩码上。这个方法可以用来创建一个训练数据集，用于教导单独的语义分割网络。
视觉语言导航（VLN）是一项任务，代理人被命令用自然语言指令在照片般逼真的未知环境中进行导航。以前关于VLN的研究主要是在只有英语指令的房间到房间（R2R）数据集中进行的。 然而，VLN的最终目标是为讲任意语言的人提供服务。为了实现多种语言的VLN，我们收集了一个跨语言的R2R数据集，它用相应的中文指令扩展了原始基准。 该跨语言代理配备了一个元学习器来聚合跨语言表征，以及一个以视觉为基础的跨语言对齐模块来对齐不同语言的文本表征。在零次学习的情况下，我们的模型甚至与用所有目标语言指令训练的模型相比都显示出有竞争力的结果。此外，当给定一定数量的目标语言数据时，我们引入了对抗性领域适应损失来提高我们模型的转移能力。我们的方法和数据集证明了建立一个跨语言代理来为不同语言的人服务的潜力。
深度生成模型已经推动了半监督分类的发展，但是它们以完全无监督的方式推导出有用的鉴别性特征的能力还没有得到充分的探索，在现实世界的数据集中，需要充分的流形分离。 我们提出了一个深度分层生成模型，它使用离散和连续分布的混合物来学习有效地分离不同的数据流形，并且是可训练的端到端。我们表明，通过指定离散变量分布的形式，我们对模型的潜在表征施加了一个特定的结构。
在图结构数据上的表征学习由于其无处不在的适用性最近受到了极大的关注。然而，大多数进展都是在静态图环境下取得的，而对图的动态和图上的动态进行联合学习的努力仍处于初级阶段。在动态图上的学习有两个基本问题：（i）如何在图上优雅地建立动态过程模型？ (我们提出了DyRep--一个新的动态图的建模框架，它将表示学习作为一个潜在的中介过程，连接两个观察到的过程，即--网络的动态（实现为拓扑演化）和网络上的动态（实现为节点间的活动）。 具体来说，我们提出了一个双时间尺度的深度时间点过程模型，该模型捕捉了观察到的过程的交错动态。该模型由一个时间注意的表示网络进一步参数化，该网络将时间演化的结构信息编码到节点表示中，反过来驱动观察到的图动态的非线性演变。 我们的统一框架使用高效的无监督程序进行训练，并有能力对未见过的节点进行泛化。我们证明DyRep在动态链接预测和时间预测任务方面优于最先进的基线，并对我们的框架提出广泛的定性见解。
随着现代机器学习的成功，理解和控制学习算法的交互方式变得越来越重要。不幸的是，博弈论的负面结果显示，理解或控制一般的n人游戏希望渺茫。
虽然自20世纪60年代以来，计数器机器在理论计算机科学中很少受到关注，但它们最近在自然语言处理（NLP）领域取得了新的相关性。最近的工作表明，一些性能强大的递归神经网络利用它们的内存作为计数器。因此，理解这些网络成功的一个潜在方法是重新审视计数器计算的理论。因此，我们选择研究实时计数器机器作为形式语法的能力。我们首先表明，计数器机器的几个变体收敛以表达同一类形式语言。 我们还证明了计数器语言在补、并、交和许多其他常见的集合操作下是封闭的。接下来，我们表明计数器机器不能评估布尔表达式，即使它们可以弱化验证其语法。这对神经网络系统的可解释性和评估有影响：成功匹配语法模式并不能保证类似计数器的模型准确地代表潜在的语义结构。最后，我们考虑计数器语言是否是半线性的问题。这项工作对形式语言理论做出了一般贡献，对循环神经网络的可解释性特别感兴趣。
基于注意力的RNN编码器-解码器模型在短的输入和输出序列上取得了良好的性能。然而，对于较长的文件和摘要，这些模型往往包括重复和不连贯的短语。我们引入了一个具有新颖的内部注意力的神经网络模型，它分别关注输入和连续产生的输出，以及一个新的训练方法，结合标准的监督词预测和强化学习（RL）。我们在CNN/Daily Mail和《纽约时报》数据集上评估了这个模型。我们的模型在CNN/Daily Mail数据集上获得了41.16分的ROUGE-1分，比以前的最先进的模型有所提高。
知识提炼（KD）是将一个机器学习模型（教师）学到的 "知识 "转移到另一个模型（学生）的常用方法。据我们所知，现有的方法忽略了这样一个事实：虽然学生从老师那里吸收了额外的知识，但两个模型共享相同的输入数据--而这些数据是可以证明老师知识的唯一媒介。由于模型能力的不同，学生可能无法从老师训练的相同数据点中充分受益。 另一方面，人类教师可能会用适合于特定学生的个性化例子来展示一段知识，例如，在她的文化背景和兴趣方面。受这种行为的启发，我们设计了具有不同角色的数据增强代理来促进知识提炼。 我们的数据增强代理分别为教师和学生生成不同的训练数据。我们特别关注当教师网络比学生网络具有更大的精度（位宽）时的KD。我们根据经验发现，特别定制的数据点能够使教师的知识更有效地展示给学生。我们将我们的方法与现有的训练流行神经架构的KD方法进行比较，并证明角色明智的数据增强提高了KD的有效性，而不是先前的强大方法。
当应用于抽象概括时，神经网络模型表现出了出色的流畅性和性能。许多神经抽象概括的方法涉及引入显著的归纳性偏见，如指针式生成器架构、覆盖率和部分提取程序，旨在模仿人类的总结。 我们表明，通过直接将总结视为语言建模，有可能达到有竞争力的性能。我们介绍了一个建立在预训练的解码器-转换器上的简单程序，以获得有竞争力的ROUGE分数，仅使用语言建模损失，没有波束搜索或其他解码时间优化，而是依靠有效的核采样和贪婪解码。
本文讨论了增量领域适应（IDA）的问题。我们假设每个领域都是按顺序来的，而且我们只能访问当前领域的数据。IDA的目标是建立一个统一的模型，在所有遇到的领域中表现良好。 我们建议用一个直接参数化的记忆库来增强循环神经网络（RNN），该记忆库在RNN转换的每一步都由注意力机制来检索。记忆库提供了一种自然的IDA方式：当适应我们的模型到一个新的领域时，我们逐步向记忆库添加新的插槽，这增加了模型容量。实验表明，我们的方法明显优于天真的微调和以前关于IDA的工作，包括弹性权重巩固和渐进式神经网络。 与扩大隐藏状态相比，我们的方法对旧领域更加稳健，经验和理论结果都表明了这一点。
在压缩传感中，要解决的一个主要问题是从少量的观测数据中重建高维稀疏信号。在这项工作中，我们利用强化学习（RL）和蒙特卡洛树搜索（MCTS）开发了一种新的稀疏信号恢复算法。与正交匹配追求（OMP）类似，我们的RL+MCTS算法按顺序选择信号的支持。 关键的创新之处在于，拟议的算法学习如何选择下一个支持，而不是像OMP那样遵循预先设计好的规则。
许多现实世界中的顺序决策问题可以被表述为具有高维观测和未知动态的最优控制。一个有前途的方法是将高维观测嵌入到低维的潜在表示空间，估计潜在的动态模型，然后利用这个模型在潜在空间进行控制。 通过从最优控制的角度制定和分析表征学习问题，我们建立了所学表征应包括的三个基本原则：1）观察空间的准确预测，2）潜伏空间和观察空间动态的一致性，以及3）潜伏空间转换的低曲率。 这些原则自然对应于由三个项组成的损失函数：预测、一致性和曲率（PCC）。至关重要的是，为了使PCC具有可操作性，我们推导出PCC损失函数的摊销变异约束。在基准领域的广泛实验表明，新的变异PCC学习算法得益于明显更稳定和可重复的训练，并导致卓越的控制性能。 进一步的消融研究支持了所有三个PCC组件对于学习一个好的潜伏空间以进行控制的重要性。
连接组学研究人员和网络科学家对神经元间网络拓扑结构和认知之间的相互作用进行了深入研究，这对理解生物神经网络的显著功效至关重要。奇怪的是，复兴神经网络的深度学习革命并没有对拓扑结构方面给予过多关注。 深度神经网络（DNNs）的架构在拓扑学意义上并不类似于它们的生物对应物。我们通过展示深度连接学网络（DCNs）的初步结果来弥补这一差距，DCNs的拓扑结构受到了现实世界神经元网络的启发。我们展示了DCNs获得的高分类精度，其架构受到了C. Elegans和老鼠视觉皮层的生物神经元网络的启发。
卷积神经网络和递归神经网络的设计，其网络结构分别很适合空间和顺序数据的性质。然而，标准前馈神经网络（FNNs）的结构仅仅是全连接层的堆叠，而不考虑数据中的特征相关性。此外，层数和神经元的数量是在验证数据上手动调整的，这很耗时，并可能导致次优网络。 在本文中，我们提出了一种无监督的结构学习方法来学习准深层FNNs。我们的方法从数据中自动确定层数、每层的神经元数量以及相邻层之间的稀疏连接。
贝叶斯推理被广泛用于推断和量化相关领域的不确定性，当两者被一个数学模型联系起来时。尽管贝叶斯推理有很多应用，但当推断具有大维度离散表示的领域时，和/或具有难以用数学方法描述的先验分布时，贝叶斯推理面临挑战。在这项工作中，我们展示了如何将生成式对抗网络（GAN）学到的近似分布作为贝叶斯更新的先验来解决这两个挑战。 我们通过推断和量化基于物理学的逆向问题和计算机视觉中出现的逆向问题来证明这种方法的功效。在后一个例子中，我们还证明了不确定性的空间变化的知识如何被用来选择放置传感器（即进行测量）的最佳策略，其中关于图像的信息每次都是一个子区域被揭示出来。
我们认为，广泛使用的Omniglot和miniImageNet基准太简单了，因为它们的类语义在不同的情节中没有变化，这违背了它们评估少数照片分类方法的预期目的。 Omniglot的类语义总是 "人物"，而miniImageNet的类语义是 "物体类别"。由于类语义如此相似，我们提出了一种名为Centroid网络的新方法，它可以在Omniglot和miniImageNet上达到令人惊讶的高精确度，而在元评价时不使用任何标签。 我们的结果表明，这些基准不适合于有监督的几次分类，因为监督本身在元评价期间是不必要的。Meta-Dataset是10个数据集的集合，最近被提议作为一个更难的几次分类基准。 使用我们的方法，我们得出了一个新的指标，即类语义一致性标准，并使用它来量化Meta-Dataset的难度。最后，在一些限制性假设下，我们表明Centroid Networks比最先进的学习-集群方法更快、更准确（Hsu et al, 2018).
我们学习识别决策状态，即决策有意义地影响代理人在环境中可以达到的未来状态的准状态集。我们利用VIC框架，该框架最大化了代理人的 "权力"，即可靠地达到不同状态集的能力，并制定了一个允许识别决策状态的权力目标的三明治约束。与以前的工作不同，我们的决策状态是在没有外在奖励的情况下发现的--仅仅是通过与世界的互动。我们的结果表明，我们的决策状态：1）通常是可解释的，2）导致在部分可观察环境中对下游目标驱动任务的更好探索。
逆向问题在自然科学中无处不在，指的是在给定一组观测值的情况下推断复杂的、可能是多模态的隐性参数后验分布的挑战性任务。通常情况下，有一个微分方程形式的物理过程模型，但会导致对其参数的难以推断。虽然参数通过模型的前向传播模拟了系统的演变，但给定状态序列寻找参数的逆向问题并不唯一。 在这项工作中，我们提出了对贝叶斯优化框架的概括，以进行近似推理。由此产生的方法通过在高斯过程模型的估计值之上应用斯坦因变异梯度下降来学习后验分布的近似值。
我们提出了一种数据驱动的方法来构建一个非整体性车辆的反馈运动基元库，保证在遵循任意长的轨迹时有一定的误差。这保证了只要对车辆的干扰保持在一定范围内，以及当障碍物在一定范围内被移动时，就可以避免运动重新规划。 该库是沿着动力学的局部抽象构建的，可以通过抽象细化来增加新的运动基元。我们为一大类非线性动力学，包括常用的模型，如标准的Reeds-Shepp模型，提供了构建这种稳健运动基元的充分条件。该算法被应用于运动规划和控制一个有滑移的漫游车，而无需事先建模。
深度卷积网络（DCN）已被证明对通用对抗性扰动（UAPs）很敏感：输入不可知的扰动，在数据集的很大一部分上愚弄模型。这些UAPs表现出有趣的视觉模式，但这种现象至今还没有被很好地理解。 我们的工作表明，视觉上类似的程序性噪声模式也可以作为UAP。特别是，我们证明不同的DCN架构对Gabor噪声模式很敏感。这种行为、其原因和影响值得进一步深入研究。
深度神经网络（DNNs）在各种任务上表现良好，尽管在实践中使用的大多数是巨大的过度参数化，甚至能够完全拟合随机标记的数据。最近的证据表明，开发 "可压缩 "表示是调整过度参数化网络的复杂性以适应手头的任务并避免过度拟合的关键（Arora等人，2018；周等人。2018）。在本文中，我们提供了支持这一假设的新的经验证据，确定了网络宽度增加时出现的两个独立机制：鲁棒性（拥有可以被移除而不影响准确性的单元）和冗余性（拥有类似活动的单元）。 在CIFAR-10和ImageNet数据集中的AlexNet、ResNet和Inception网络的一系列实验中，以及使用合成数据的浅层网络，我们表明DNN在更大的宽度下，对于一组全面的超参数，它们的鲁棒性、冗余度或两者都会持续增加。这些结果表明，深度学习体系中的网络通过发展鲁棒性或冗余度来调整其有效能力。
深度网络实现了复杂的映射，这些映射通常由它们在感兴趣的点或周围的局部线性行为来理解。例如，我们使用映射相对于其输入的导数来进行敏感性分析，或者解释（获得坐标相关性）预测。一个关键的挑战是，这种导数本身是固有的不稳定的。在本文中，我们提出了一个新的学习问题，鼓励深度网络在更大的区域拥有稳定的导数。 我们的算法由一个推理步骤和一个优化步骤组成，前者确定了线性近似可证明是稳定的点周围的区域，后者扩大了这些区域。
近年来，深度卷积神经网络（CNN）出现了两种看似相反的发展。一方面，通过增加跨层连接来增加CNN的密度，实现更高的精度。另一方面，通过正则化和剪枝方法创建稀疏结构，享受更低的计算成本。在本文中，我们通过提出一种具有局部密集但外部稀疏连接的新网络结构来连接这两者。 实验结果表明，局部密集但外部稀疏的结构可以在基准任务（CIFAR10、CIFAR100和ImageNet）中获得有竞争力的性能，同时保持网络结构的纤细。
统计推理方法在机器学习中具有根本的重要性。大多数最先进的推理算法是马尔科夫链蒙特卡洛（MCMC）或变异推理（VI）的变种。然而，这两种方法在实践中都有局限性。MCMC方法在计算上要求很高；VI方法可能有很大的偏差。在这项工作中，我们旨在通过一种新的混合方法来改进MCMC和VI，该方法基于使用基于梯度的优化来减少有限长度MCMC链的模拟偏差的想法。
为了证明这一效果，我们训练了一个新的 "元 "网络，以便从底层 "基础 "网络的最终输出或基础网络的一个中间层的输出中预测基础网络对特定输入的正确与否。我们发现，在广泛的任务和基础网络中，元网络在做出这一判断时可以达到65%-85%的准确率。
我们开发了一种新的算法，用于从单一的专家示范中进行模仿学习。与许多以前的一次性模仿学习方法相比，我们的算法并不假设在训练阶段获得一个以上的专家示范。相反，我们利用探索策略来获得无监督的轨迹，然后用来训练编码器和上下文感知的模仿策略。 编码器、模仿学习者和探索策略的优化程序都是紧密相连的。这种联系创造了一个反馈回路，其中探索策略收集了挑战模仿学习者的新演示，而编码器试图尽其所能帮助模仿策略。
我们在任务调度的背景下研究了这个问题。我们介绍了代理资源受限项目调度问题（ARCPSP），这是资源受限项目调度问题的延伸，包括一个平行执行任务的代理概念。 我们概述了一个通用的框架，基于有效地列举最小不可满足集（MUS）和最大可满足集（MSS），以产生不可行性来源的小描述。这些描述被补充了潜在的放松，这些放松将修复问题实例中发现的不可行性。
我们研究了两种方法：一种是经过训练的CNN，使用我们学习的显著性模型产生的显著性掩码来区分自然图像和敌对图像；另一种是经过训练的CNN，将显著性像素本身作为其输入。 在MNIST、CIFAR-10和ASSIRA上，我们的防御措施能够检测到各种对抗性攻击，包括C&W和DeepFool等强攻击，这与基于梯度的显著性和依赖输入图像的检测器相反。当扰动的L_2和L_infinity-规范太小时，后者无法检测到对抗性图像。最后，我们发现，基于显著性像素的检测器比基于显著性图的检测器更有优势，因为它对白盒攻击更加稳健。
Disentangled编码是朝着更好的表征学习迈出的重要一步。然而，尽管有许多努力，但仍然没有明显的赢家，以无监督的方式捕获数据的独立特征。在这项工作中，我们在为NeurIPS 2019年Disentanglement挑战赛策划和发布的mpi3d玩具数据集上实证评估了六种无监督disentanglement方法的性能。"这项工作中研究的方法是Beta-VAE、Factor-VAE、DIP-I-VAE、DIP-II-VAE、Info-VAE和Beta-TCVAE。 在整个训练过程中，所有模型的能力都是逐步提高的，而且超参数在不同的实验中保持不变。这些方法是根据五个解缠指标进行评估的，即DCI、Factor-VAE、IRS、MIG和SAP-分数。 在本研究的限制下，发现Beta-TCVAE方法在归一化指标总和方面优于其他方法。然而，对编码潜质的定性研究显示，报告的指标与模型的解缠潜力之间没有一致的关联性。
大多数关于多代理强化学习（MARL）的前期工作都是通过直接学习每个代理的策略来实现最佳协作，以使共同的奖励最大化。在本文中，我们旨在从不同的角度解决这个问题。为了实现这些代理之间的最佳协调，我们训练一个超级代理（即经理）来管理他们，首先根据当前和过去的观察来推断他们的想法，然后启动合同，将合适的任务分配给工人，并承诺给他们相应的奖金，这样他们就会同意一起工作。 为了训练管理者，我们提出了Mind-aware Multi-agent Management Reinforcement Learning（M^3RL），它由代理建模和策略学习组成。我们在资源收集和手工制作这两种环境中评估了我们的方法，以模拟具有各种任务设置和工人代理的多种设计的多代理管理问题。实验结果验证了我们的方法在在线建模工人代理的思想，以及在实现具有良好概括性和快速适应的最佳临时团队方面的有效性。
推断时间上连贯的数据特征对于大量的学习任务来说是至关重要的。我们提出了一个网络架构，为广泛使用的剩余块的内部状态引入了时间上的递归连接。我们证明，有了这些连接，卷积神经网络可以更稳健地学习稳定的时间状态，在评估之间持续存在。我们证明了它们在从实时渲染器产生的低分辨率图像中推断高质量超分辨率图像的潜力。 这种数据出现在广泛的应用中，而且特别具有挑战性，因为它包含强烈的混叠信号。因此，这种数据与自然视频中遇到的平滑输入有很大的不同，现有的技术不能成功地产生可接受的图像质量。我们还提出了一系列针对视频超分辨率的典型生成式对抗架构的仔细调整，以达到第一个模型，可以从实时渲染器的混叠输入流中产生详细而又时间上一致的图像。
逆传播算法是目前最流行的训练神经网络的算法。然而，它存在前向锁定、后向锁定和更新锁定的问题，特别是当一个神经网络非常大，其层分布在多个设备上时。 现有的解决方案要么只能处理一个锁定问题，要么导致严重的精度损失或内存效率低下。此外，它们都没有考虑设备之间的散兵游勇问题。(我们还分析了DSP与两种流行的基于梯度的方法的收敛性，并证明它们都能保证在非凸问题上收敛到临界点。最后，在训练深度卷积神经网络上的大量实验结果表明，我们提出的DSP算法可以实现明显的训练速度提升，并具有比其他方法更强的鲁棒性和更好的概括性。
如果人工智能能够通过使用语言来理解语言的含义，它也可以灵活地将其转移到其他情况下，这被视为实现通用人工智能的重要一步。然而，到目前为止，新兴通信的范围仍然有限。 我们从人类的语言习得和移情联系在这一过程中的重要性举了一个例子。我们提出了一种方法，将移情的概念引入到多Agent深度强化学习中。我们用一个辅助任务来扩展现有的关于参考游戏的方法，让说话者预测听者的思想变化，从而提高学习时间。我们的实验表明，通过将测试装置的学习速度提高一倍，这一架构元素具有很大的潜力。
图像段落标题是自动生成多个句子的任务，用于描述图像的颗粒状和连贯的文本。现有的典型的基于深度学习的图像标题模型包括一个提取视觉特征的图像编码器和一个语言模型解码器，这在单一的高级别句子生成中显示出有前途的结果。 然而，当图像编码器被优化为提取视觉特征时，只有字级的标量引导信号。当生成的文本长度较长（超过50个字）时，视觉特征的平行提取和顺序文本监督之间的不一致限制了它的成功。在本文中，我们提出了一个新的模块，称为文本嵌入库（TEB）模块，以解决图像段落说明的问题。 这个模块使用段落矢量模型从可变长度的段落中学习固定长度的特征表示。我们把固定长度的特征称为TEB。 其次，它作为一个分布式存储器，向语言模型提供整个段落的特征，从而缓解了长期依赖问题。在现有的两种最先进的方法中加入这个模块，在段落标题的视觉基因组数据集上以很大的幅度取得了新的最先进的结果。
学习Mahalanobis度量空间是一个重要的问题，已经发现了许多应用。已经为这个问题设计了几种算法，包括信息理论度量学习（ITML）[Davis等人，2007]和大边际最近的邻居（LMNN）分类[Weinberger和Saul，2009]。 我们考虑将Mahalanobis度量学习表述为一个优化问题，目标是最小化违反的相似性/非相似性约束的数量。 我们表明，对于任何固定的环境维度，都存在一个完全多项式时间的近似方案（FPTAS），其运行时间接近线性。
标准的图像标题任务，如COCO和Flickr30k，是事实性的，语气是中性的，（对人类来说）是显而易见的（例如，"一个人在弹吉他"）。虽然这样的任务对于验证机器是否理解图像的内容是有用的，但它们作为标题对人类来说并不吸引。  考虑到这一点，我们定义了一个新的任务--人格化字幕，其目标是通过纳入可控制的风格和人格特征来尽可能地吸引人类。我们收集并发布了一个大型数据集，其中有201,858个这样的字幕，其条件是215个可能的特征。 我们建立的模型结合了现有的工作：（i）句子表示（Mazaré等人，2018年）与在17亿个对话例子上训练的变形金刚；以及（ii）图像表示（Mahajan等人，2018年）与在35亿张社交媒体图像上训练的ResNets。 我们在Flickr30k和COCO上获得了最先进的性能，并在我们的新任务上获得了强大的性能。最后，在线评估验证了我们的任务和模型对人类的吸引力，我们最好的模型接近人类的表现。
通过差分隐私随机梯度下降法（DP-SGD）训练的机器学习（ML）模型比非隐私模型的效用要低得多。为了缓解这种退化，我们提出了一种DP拉普拉斯平滑SGD（DP-LSSGD），以训练具有差分隐私（DP）保证的ML模型。DP-LSSGD的核心是拉普拉斯平滑，它平滑了高斯机制中使用的高斯噪声。 在高斯机制中使用相同数量的噪声的情况下，DP-LSSGD获得了相同的DP保证，但特别是在具有强DP保证的情况下有更好的效用。实际上，DP-LSSGD使凸和非凸的ML模型的训练更加稳定，使训练的模型能够更好地泛化。
我们研究了稳健的一比特压缩传感问题，其目标是设计一种算法，从$m$量化的噪声测量中忠实地恢复任何稀疏的目标向量$theta_0\in\mathbb{R}^d$\emph{uniformly}。 在测量是亚高斯的假设下，为了高概率地恢复任何$k$稀疏$theta_0$($k/ll d$)/emph{uniformly}的误差$varepsilon$，最好的已知可计算的算法需要/footnote{这里，如果一个算法有可证明的收敛保证，它就是 "可计算的 "算法。 符号$\tilde{mathcal{O}}(\cdot)$省略了$varepsilon^{-1}$的对数因子。｝ $m\geq\tilde{mathcal{O}}(k\log d/\varepsilon^4)$.在本文中，我们考虑了一个新的单比特传感问题的框架，该框架通过已知的$n$层ReLU生成网络$G:\mathbb{R}^k\rightarrow\mathbb{R}^d$映射低维表示$x_0$来隐含强制实现稀疏性。 这样的框架对$G(x_0)提出了没有已知基础的低维先验。我们建议在一个更弱的/emph{次指数测量假设}下，通过无约束的经验风险最小化（ERM）问题来恢复目标$G(x_0)$。对于这样一个问题，我们建立了一个联合的统计和计算分析。特别是，我们证明了在这个新框架下的ERM估计器实现了$m=\tilde{mathcal{O}}的改进统计率。(kn/log d /\epsilon^2)$均匀地恢复任何$G(x_0)$的误差$varepsilon$。此外，从计算的角度，我们证明在ReLU权重的适当条件下，我们提出的经验风险，尽管是非凸性的，在真实表示$x_0$及其负倍数周围的小邻居之外没有固定点。 此外，我们表明经验风险的全局最小化器停留在$x_0$周围的邻域内，而不是其负倍数。我们的分析揭示了在部分和量化测量下反演深度生成模型的可能性，补充了最近使用深度生成模型反演问题的成功。
我们为深度前馈神经网络引入了一种无监督的结构学习算法。我们为深度和层间连接提出了一种新的解释，即输入分布中的独立层次在网络结构中被编码。这导致结构允许神经元连接到任何更深层的神经元，跳过中间层。 此外，较深层的神经元编码低阶（小条件集）的独立性，并具有广泛的输入范围，而第一层的神经元编码高阶（大条件集）的独立性，并具有较窄的范围。因此，网络的深度是自动确定的--等于输入分布中最大的独立性顺序，这就是算法的递归深度。 所提出的算法构建了两个主要的图形模型：1）从数据中学习的生成潜势图（深度信念网络）；2）从生成潜势图中构建的深度判别图。我们证明，在学习的生成潜势图中节点之间的条件依赖性在类别条件的判别图中得到保留。最后，基于判别图构建了一个深度神经网络结构。 我们在图像分类基准上证明，该算法取代了普通卷积网络的最深层（卷积层和密集层），实现了较高的分类精度，同时构建了明显较小的结构。所提出的结构学习算法需要较小的计算成本，并在标准台式CPU上有效运行。
L1和L2正则化器是机器学习中的关键工具，因为它们能够简化解决方案。然而，用梯度下降法强加L1或L2正则化很容易失败，这限制了底层神经网络的泛化能力。为了理解这一现象，我们研究了强正则化的训练如何以及为何失败。具体而言，我们研究了不同正则化强度的梯度如何随时间变化，并分析了为什么梯度减小得如此快。 我们发现存在一个正则化强度的容忍度，如果正则化强度超过这个容忍度，学习就会完全失败。我们提出了一个简单而新颖的方法，即延迟强正则化，以缓和容忍度。实验结果表明，我们提出的方法确实实现了L1和L2正则化的强正则化，并提高了公共数据集的准确性和稀疏性。
尽管神经网络的性能很高，但缺乏可解释性一直是其在实践中安全使用的主要瓶颈。在具有高风险的领域（如医疗诊断），获得对网络的洞察力对于获得信任和被采用至关重要。这项工作的目的是通过概念激活向量（CAV）对感兴趣的概念的相对重要性提供定量的答案。特别是，这个框架使非机器学习专家能够表达感兴趣的概念，并使用例子（例如。例如，用CAV测试可以回答一个特定的概念（如性别）在预测一个特定的类别（如医生）时是否比其他概念集更重要。我们展示了许多层次的有意义的概念（例如，颜色、纹理、物体、一个人的职业），我们提出了CAV的/textit{empirical deepdream}--在这里我们使用一组例子图片来最大化激活。
我们提出了一个新的目标函数系列，我们称之为条件熵瓶颈（CEB）.这些目标是由最小必要信息（MNI）标准激发的.我们展示了CEB在分类任务中的应用。 我们表明，CEB给出了：良好的校准预测；对具有挑战性的分布外例子和强大的白盒对抗性例子的强大检测；以及对这些对抗者的实质性鲁棒性。最后，我们报告说，CEB未能从无信息数据集中学习，为Zhang等人（2016）观察到的泛化问题提供了可能的解决方案。
深度表征学习已经成为视觉搜索、推荐和识别中最广泛采用的方法之一。然而，从大型数据库中检索这种表征在计算上具有挑战性。基于学习紧凑表征的近似方法，已经被广泛探索用于这个问题，如位置敏感散列、产品量化和PCA。 在这项工作中，与学习紧凑表征相反，我们建议学习高维和稀疏表征，这些表征具有与密集嵌入类似的表征能力，同时由于稀疏矩阵乘法操作比密集乘法快得多，因此效率更高。 我们提出了一种新的方法，通过使用精心构造的正则化函数来学习这种分布式稀疏嵌入，该函数直接最小化了检索过程中产生的浮点运算（FLOPs）数量的连续松弛，我们的实验表明，我们的方法与其他基线相比具有竞争力，在实际数据集上产生了类似或更好的速度-vs-精度权衡。
模型的可解释性和系统的、有针对性的模型适应性是深度学习的核心挑战。在直观的物理学领域，我们研究了视觉预测积木塔的稳定性的任务，目的是理解和影响模型的推理。 首先，我们引入了神经听诊器作为一个框架，用于量化深度网络中特定影响因素的重要程度，以及酌情积极促进和抑制信息。其次，我们部署听诊器框架，对最先进的深度神经网络的稳定性预测进行深入分析，特别是检查其物理推理。 我们表明，基线模型很容易受到不正确的视觉线索的误导。当在视觉线索与稳定性成反比的情况下进行训练时，这导致性能崩溃，达到随机猜测的水平。使用听诊器促进有意义的特征提取，将性能从51%提高到90%的预测精度。 相反，在视觉线索与稳定性正相关的简单数据集上训练时，基线模型学会了一种偏见，导致在更难的数据集上表现不佳。使用对抗性听诊器，网络成功去除了偏见，导致性能从66%提高到88%。
基于流的模型，如Real NVP，是一种极其强大的密度估计方法。然而，现有的基于流的模型仅限于将连续输入空间上的连续密度转化为连续潜变量上的类似连续分布，这使得它们不适合建模和表示数据分布中的离散结构，例如类成员或离散对称性。为了解决这个困难，我们提出了一个归一化流架构，它依赖于使用局部可逆函数进行域划分，并拥有真实和离散值潜变量。 这种真实和离散（RAD）方法保留了精确采样、精确推理和可分析计算概率等理想的归一化流特性，同时允许同时对数据分布中的连续和离散结构进行建模。
我们研究了神经网络的损失面。我们证明，即使是具有 "最轻微 "非线性的单隐层网络，经验风险在大多数情况下也有虚假的局部最小值。我们的结果因此表明，一般来说，"没有虚假的局部最小值 "是仅限于深度线性网络的属性，从线性网络获得的见解可能并不稳健。具体来说，对于ReLU（-like）网络，我们建设性地证明，对于几乎所有实际数据集，存在无限多个局部最小值。 我们还提出了一个更普遍的激活（sigmoid、tanh、arctan、ReLU等）的反例，对于这些激活，存在一个坏的局部最小值。我们的结果相对于现有的关于神经网络中假性局部最优的结果，做出了最少的限制性假设。
代理人需要解决的任务在训练期间往往是不知道的。然而，如果代理人知道环境的哪些属性我们认为是重要的，那么在学习了它的行动如何影响这些属性之后，代理人可能能够使用这些知识来解决复杂的任务，而不需要专门为它们进行训练。为了这个目的，我们考虑一个设置，其中环境被增加了一组用户定义的属性，这些属性将感兴趣的特征参数化。 我们提出了一个模型，该模型学习在 "附近 "的属性集之间转换的策略，并维护一个可能的转换图。给定测试时间的任务可以用目标属性集和当前状态来表示，我们的模型推断出当前状态的属性，并通过属性空间搜索路径来获得一个高水平的计划，然后使用其低水平的策略来执行该计划。 我们在网格世界游戏和三维积木堆叠中显示，我们的模型能够在测试时概括出更长、更复杂的任务，即使它在训练时只看到短的、简单的任务。
用少量数据学习新概念的能力是智能的一个关键方面，这对深度学习方法来说已被证明具有挑战性。元学习已经成为一种有前途的技术，可以利用以前的任务数据来实现对新任务的有效学习。然而，大多数元学习算法隐含地要求元训练任务是相互排斥的，这样没有一个模型可以同时解决所有的任务。 例如，在为少数照片的图像分类创建任务时，先前的工作采用了按任务随机分配图像类别到N路分类标签的方法。如果不这样做，元学习者可以忽略任务训练数据，学习一个单一的模型，该模型可以零散地执行所有的元训练任务，但不能有效地适应新的图像类别。 这一要求意味着用户在设计任务时必须非常小心，例如通过洗标签或从输入中删除任务识别信息。在某些领域，这使得元学习完全不适用。在本文中，我们通过使用信息理论设计一个元正则化目标，将数据驱动的适应性放在首位来解决这一挑战。这使得元学习者决定什么必须从任务训练数据中学习，什么应该从任务测试输入中推断出来。 通过这样做，我们的算法可以成功地使用来自非互斥任务的数据来有效地适应新的任务。我们证明了它对上下文和基于梯度的元学习算法的适用性，并将其应用于应用标准元学习已经很困难的实际环境中。我们的方法在这些环境中大大超过了标准元学习算法的表现。 
从数据中学习有效的更新规则，以促进从相同的分布中快速学习新的任务，仍然是元学习中的一个开放性问题。通常，以前的工作已经接近这个问题，要么试图训练一个直接产生更新的神经网络，要么试图学习更好的初始化或基于梯度的更新规则的缩放因子。 另一方面，试图控制基于梯度的更新规则的方法通常会通过学习过程来计算梯度，以获得其元梯度，从而导致方法的规模不能超过几次任务的适应。 在这项工作中，我们提出了扭曲梯度下降（WarpGrad），这是一种与这些方法交叉的方法，以减轻它们的局限性。WarpGrad元学习了一个有效的参数化预设矩阵，促进了整个任务分布的梯度下降。预设是通过在任务学习者的层之间交错的非线性层产生的，被称为扭曲层。 Warp-layer是元学习，不需要通过任务训练过程进行反向传播，其方式类似于直接产生更新的学习方法。WarpGrad计算效率高，易于实现，可以扩展到任意大的元学习问题。我们提供了该方法的几何解释，并评估了其在各种设置中的有效性，包括少数几个镜头、标准监督、持续和强化学习。
近年来，深度神经网络方法已被广泛用于机器学习任务，包括分类。然而，它们被证明容易受到对抗性扰动的影响：精心设计的小扰动会导致合法图像的错误分类。我们提出了Defense-GAN，一个新的框架，利用生成模型的表达能力来保护深度神经网络免受这种攻击。 在推理时，它找到一个与给定图像接近的输出，该输出不包含对抗性变化。我们提出的方法可以与任何分类模型一起使用，并且不修改分类器结构或训练程序。
我们研究了使用神经网络嵌入模型在非常大的语料库上学习相似性函数的问题。这些模型通常使用SGD对未观察到的对进行随机抽样训练，其样本量随着语料库的大小呈二次增长，使得它的扩展成本很高。我们提出了新的有效方法来训练这些模型，而不必对未观察到的对进行抽样。 受矩阵分解的启发，我们的方法依赖于增加一个全局的二次惩罚，并将这个项表达为两个广义格拉米的内积。我们表明，这个项的梯度可以通过保持格拉米的估计值来有效地计算，并开发方差减少方案来提高估计的质量。
为了使自然语言理解（NLU）技术发挥最大的作用，它必须能够以一种不局限于单一任务、体裁或数据集的方式来处理语言。为了实现这一目标，我们引入了通用语言理解评估（GLUE）基准，这是一套用于评估模型在不同的现有NLU任务中表现的工具。 GLUE还包括一个手工制作的诊断测试套件，可以对模型进行详细的语言学分析。我们根据目前的转移和表示学习方法对基线进行了评估，发现在所有任务上的多任务训练比每个任务训练一个单独的模型表现得更好。然而，我们最好的模型的低绝对性能表明需要改进通用NLU系统。
各种合作的多代理控制问题要求代理在实现个人目标的同时为集体的成功做出贡献。这种多目标的多代理设置给最近的算法带来了困难，这些算法主要针对具有单一全局奖励的设置，由于两个新的挑战：高效探索学习个人目标的实现和为他人的成功合作，以及为不同代理的行动和目标之间的相互作用分配信用。 为了解决这两个挑战，我们将问题重组为一个新的两阶段课程，其中在学习多代理人合作之前，先学习单代理人目标的实现，我们推导出一个新的多目标多代理人政策梯度，其中有一个信用函数用于本地化的信用分配。我们使用一个函数增强方案来连接整个课程的价值和政策函数。 完整的架构被称为CM3，在三个具有挑战性的多目标多代理问题上的学习速度明显快于现有算法的直接调整：困难阵型中的合作导航，SUMO交通模拟器中的多车道变化谈判，以及跳棋环境中的战略合作。
我们发现基于对抗性模仿学习框架的算法系列有两个问题。第一个问题是这些算法中使用的奖励函数中存在隐性偏见。虽然这些偏见在某些环境中可能很有效，但在其他环境中也可能导致次优行为。其次，即使这些算法可以从少数专家的示范中学习，他们需要与环境进行大量的互动，以便在许多现实世界的应用中模仿专家。 为了解决这些问题，我们提出了一种名为Discriminator-Actor-Critic的新算法，该算法使用非政策强化学习，将政策与环境互动的样本复杂度平均降低了10倍。此外，由于我们的奖励函数被设计成无偏差的，我们可以将我们的算法应用于许多问题，而无需进行任何特定任务的调整。
胶囊网络在诸如MNIST、CIFAR和smallNORB等事实上的基准计算机视觉数据集上显示出令人鼓舞的结果。尽管如此，它们仍有待于在以下任务中进行测试：（1）检测到的实体本身具有更复杂的内部表征；（2）每类有非常少的实例可以学习；（3）不适合进行点式分类。 我们发现连体胶囊网络在两个成对学习数据集上都有很好的表现，在测试集中的图像对包含未见过的对象的情况下，连体胶囊网络在少数镜头的学习环境中产生了最佳结果。
动物擅长根据环境调整它们的意图、注意力和行动，使它们在与丰富的、不可预测的和不断变化的外部世界进行互动时具有明显的效率，这是智能机器目前所缺乏的特性。这种适应特性强烈依赖于细胞神经调节，这种生物机制动态地控制神经元的内在属性和对外部刺激的反应，其方式与背景有关。 在本文中，我们从细胞神经调节中获得灵感，构建了一个新的深度神经网络架构，专门用于学习适应性行为。结果表明，神经调节能够使代理人适应不同的任务，基于神经调节的方法为改善人工系统的适应性提供了一个很好的方法。
卷积算子是卷积神经网络（CNN）的核心，占据了最多的计算成本。为了使CNN更有效率，人们提出了许多方法来设计轻量级网络或压缩模型。尽管已经提出了一些有效的网络结构，如MobileNet或ShuffleNet，我们发现卷积核之间仍然存在冗余的信息。 为了解决这个问题，我们在本文中提出了一种新的动态卷积方法，名为DyNet，它可以根据图像内容自适应地生成卷积核。为了证明其有效性，我们将DyNet应用于多个最先进的CNN。 实验结果表明，DyNet可以显著降低计算成本，同时保持性能几乎不变。具体来说，对于ShuffleNetV2（1.0）、MobileNetV2（1.0）、ResNet18和ResNet50，DyNet分别减少了40.0%、56.7%、68.2%和72.4%的FLOPs，而ImageNet的Top-1精度仅变化+1.0%、-0.27%、-0. 同时，DyNet进一步加快了MobileNetV2（1.0）、ResNet18和ResNet50在CPU平台上的推理速度，分别为1.87倍、1.32倍和1.48倍。为了验证可扩展性，我们还将DyNet应用于分割任务，结果显示，DyNet可以减少69.3%的FLOPs，同时保持分割任务的平均IoU。
在资源有限的设备上运行深度神经网络需要减少它们的内存足迹和计算要求。在本文中，我们介绍了一种训练方法，称为查找表量化（LUT-Q），它学习一个字典并将每个权重分配给字典的一个值。 例如，我们可以限制用LUT-Q训练的字典，以产生具有修剪权重矩阵的网络，或者限制字典为2的幂，以避免乘法的需要。为了获得完全无乘法的网络，我们还引入了一个无乘法的批量归一化版本。在图像识别和物体检测任务上的大量实验表明，在相同的量化位宽下，LUT-Q一直比其他方法取得更好的性能。
特征共享的一个意外后果是模型拟合到数据集内的相关任务，称为负转移。 在本文中，我们重新审视了多任务设置中的负迁移问题，发现其腐蚀作用适用于广泛的线性和非线性模型，包括神经网络。我们首先以一种原则性的方式研究了负迁移的影响，并表明以前提出的对策是不够的，特别是对于可训练的特征。 我们提出了一种对抗性训练方法，通过在领域适应性设置中看待问题来减轻负迁移的影响。最后，在AWA和CUB数据集上的属性预测多任务的经验结果进一步验证了以端到端方式纠正负共享的必要性。
最近的理论和实验结果表明，有可能在具有挑战性的采样任务中使用当前和未来的量子硬件。在本文中，我们介绍了基于自由能的强化学习（FERL）作为量子硬件的应用。我们提出了一种方法，用于处理量子退火器的测量量子比特自旋配置，以接近量子波兹曼机（QBM）的自由能。 然后，我们应用这种方法，使用D-Wave 2000Q量子退火器对网格世界问题进行强化学习。实验结果表明，我们的技术是一种很有希望的方法，可以在强化学习任务中利用量子采样的力量。
深度学习模型很容易受到通过在良性输入上应用人类难以察觉的扰动而精心制作的对抗性例子的影响。然而，在黑箱设置下，大多数现有的对抗者往往在攻击其他防御模型方面有很差的可转移性。 在这项工作中，我们从将对抗性例子的生成视为一个优化过程的角度出发，提出了两种新的方法来提高对抗性例子的可转移性，即Nesterov迭代快速梯度符号法（NI-FGSM）和尺度不变攻击法（SIM）。NI-FGSM旨在将Nesterov加速梯度调整为迭代攻击，从而有效地展望和提高对抗性例子的可转移性。 而SIM是基于我们发现的深度学习模型的标度不变属性，为此我们利用输入图像的标度副本来优化对抗性扰动，以避免对被攻击的白盒模型的 "过度拟合"，并产生更多可转移的对抗性例子。 NI-FGSM和SIM可以自然地集成到一起，建立一个强大的基于梯度的攻击，针对防御模型产生更多可转移的对抗例子。在ImageNet数据集上的实证结果表明，我们的攻击方法表现出更高的可转移性，比最先进的基于梯度的攻击取得了更高的攻击成功率。
低位宽的权重和激活是对抗深度神经网络对内存和计算能力日益增长的需求的有效方法。在这项工作中，我们提出了一种具有二进制权重和激活的神经网络的概率训练方法，称为PBNet。 通过在训练过程中拥抱随机性，我们规避了对导数几乎总是为零的函数梯度的近似需要，例如$/textrm{sign}(\cdot)$，同时在测试时仍然获得一个完全的二元神经网络。 此外，它允许随时进行集合预测，通过从权重分布中抽样来提高性能和不确定性估计。由于PBNet的一个层中的所有操作都是对随机变量进行操作，我们引入了随机版本的Batch Normalization和max pooling，它们在测试时可以很好地转移到确定性网络中。 我们评估了两种相关的PBNet训练方法：一种是在整个网络中传播激活分布，另一种是在每一层中对二进制激活进行采样。
生成模型的目标是对基于样本的数据集的基本数据分布进行建模。我们的直觉是，一个准确的模型原则上也应该包括基于样本的数据集，作为其诱导概率分布的一部分。为了研究这一点，我们使用生成对抗网络（GAN）框架研究了完全训练的生成模型，并分析了生成器记忆数据集的能力。 这使我们与压缩理论有了联系，自动编码器（AE）被用来降低我们生成模型的重建能力。 从感知的角度来看，给定一个小的潜空间，AE产生低质量的输出，而GAN产生高质量的输出。相反，AE的失真误差较小。通过增加潜空间的维度，两个模型的失真都在减少，但感知质量只在AE中增加。
我们解决了开放集作者身份验证的问题，这一分类任务包括当测试集中的未知文档被排除在训练集之外时，将未知作者身份的文本归属于某个特定的作者。我们提出了一个端到端的模型建立过程，普遍适用于各种语料库，几乎不需要修改或微调。 语言模型将已知和未知作者的文件编码到一个领域不变的空间中，将文件对作为分类器的输入对准，同时保持它们的分离。 整个管道是双向的；前向和后向的结果是平均的。我们在四个传统的作者身份验证数据集、一个从网络上挖掘的机器学习论文集和一个大型的Amazon-Reviews数据集上进行了实验。实验结果超过了基线和当前最先进的技术，验证了所提出的方法。
我们考虑通过将问题分配给n个学习者来解决单代理RL问题。这些学习者被称为顾问，他们努力从不同的焦点来解决问题。他们的建议，采取行动值的形式，然后被传达给控制系统的聚合器。 我们表明，顾问的局部规划方法是至关重要的，在文献中发现的方法没有一个是完美无缺的：在其他顾问不同意的情况下，textit{egocentric}规划高估了状态值，而textit{agnostic}规划在危险区周围是低效的。我们介绍了一种称为textit{empathic}的新方法，并讨论了其理论方面。
我们提出了一个混合框架，利用音频表征中的时间和频率精度之间的权衡来提高语音增强任务的性能。我们首先表明，使用特定表征的传统方法，如原始音频和频谱图，各自有效地针对不同类型的噪声。通过整合这两种方法，我们的模型可以学习多尺度和多领域的特征，以互补的方式有效去除存在于时频空间不同区域的噪声。
在介绍了统计测试的概念后，我们回顾了相关的统计测试，并在假阳性率和统计能力方面对它们进行了经验性的比较，作为样本量（种子数）和效应大小的函数。 我们进一步研究了这些测试对违反最常见假设（正态分布、相同分布、等方差）的鲁棒性。除了模拟，我们比较了在Half-Cheetah上运行Soft-Actor Critic和Twin-Delayed Deep Deterministic Policy Gradient得到的经验分布。
在信息瓶颈（IB）中，当调整压缩和预测项之间的相对强度时，这两个项是如何表现的，它们与数据集和学习的表示有什么关系？在本文中，我们通过研究IB目标中的多个相变来回答这些问题。IB_β[p(z|x)] = I(X; Z) - βI(Y; Z) 定义在输入X、目标Y和表示Z的编码分布p(z|x)上，其中dI(Y; Z)/dβ的突然跳跃和预测精度随着β的增加而观察到。 我们引入了一个IB相位转换的定义，作为IB损失景观的质变，并表明该转换对应于学习新类别的开始。使用二阶变化微积分，我们推导出一个公式，为IB相位转换提供了一个实际条件，并得出它与参数化模型的费雪信息矩阵的联系。 我们提供了两个角度来理解这个公式，揭示了每个IB相变都是在寻找X和Y之间的最大（非线性）相关成分，与线性设置中的典型相关分析（CCA）密切类似。最后，我们验证了我们的理论和算法能够准确预测分类数据集的相变，预测MNIST中学习新类的开始和类的难度，并预测CIFAR10中突出的相变。
我们提出了两种局部自适应激活函数的方法，即层-智和神经元-智的局部自适应激活函数，它们可以提高深度和物理信息神经网络的性能。激活函数的局部自适应是通过在每个层（层-智）和每个神经元（神经元-智）分别引入可扩展的超参数，然后用随机梯度下降算法对其进行优化来实现。 相对于传统的由固定的、全局的和逐层的激活所给出的标量激活函数，引入神经元导向的激活函数就像一个矢量激活函数。为了进一步提高训练速度，在损失函数中加入了一个基于激活斜率的斜率恢复项，这进一步加速了收敛，从而降低了训练成本。 在数值实验中，我们使用深度神经网络对一个非线性不连续函数进行了逼近，该网络具有层级和神经元级的局部自适应激活函数，并与全局对应函数进行了比较。此外，使用所提出的方法还得到了非线性Burgers方程的解决方案，该方程表现出陡峭的梯度。 在理论方面，我们证明了在所提出的方法中，梯度下降算法在初始化和学习率的实际条件下不会被吸引到次优临界点或局部最小值。此外，在使用CIFAR-10、CIFAR-100、SVHN、MNIST、KMNIST、Fashion-MNIST和Semeion数据集的标准深度学习基准中，所提出的带有斜率恢复的自适应激活函数被证明可以加速训练过程，无论是否有数据增强。
高斯过程模型的学习是通过对均值和协方差函数的超参数进行调整而进行的。经典的方法需要对产生固定点估计的边际似然进行最大化（这种方法称为第二类最大似然或ML-II）。另一种学习程序是推断GPs分层规范中超参数的后验，我们称之为全贝叶斯高斯过程回归（GPR）。这项工作考虑了对难以解决的超参数后验的两种近似方法：1）汉密尔顿蒙特卡罗（HMC），产生一个基于抽样的近似方法；2）变异推理（VI），其中超参数的后验被因子化高斯（均值场）或全等级高斯近似，考虑了超参数之间的相关性。
我们考虑了使用变异潜变量模型进行数据压缩的问题。对于这种模型产生压缩的二进制序列，也就是数字世界中的通用数据表示，潜变量表示需要进行熵编码。范围编码作为一种熵编码技术是最佳的，但如果发送方和接收方的先验计算有哪怕是微小的差异，它就会发生灾难性的失败。 不幸的是，当使用浮点数学，并且发送方和接收方在不同的硬件或软件平台上操作时，这是一个常见的情况，因为数字的舍入往往取决于平台。我们建议使用整数网络作为这个问题的通用解决方案，并证明它们能够使用变量模型对图像进行可靠的跨平台编码和解码。
用外部存储器供电的神经网络模拟计算机行为。这些模型使用存储器为神经控制器存储数据，可以学习算法和其他复杂的任务。在本文中，我们引入了一个新的存储器来存储控制器的权重，类似于现代计算机架构中的存储程序存储器。 本文提出的模型被称为神经存储程序存储器，它增强了目前的存储器增强的神经网络，创造了可区分的机器，可以通过时间切换程序，适应可变的环境，从而完全类似于通用图灵机。广泛的实验表明，所产生的机器不仅在经典的算法问题上表现出色，而且在构成性的、持续的、少量的学习和回答问题的任务中有潜力。
衰减学习率是常见的做法。这里我们表明，通常可以通过在训练期间增加批处理量，在训练和测试集上获得相同的学习曲线。这个程序对随机梯度下降（SGD）、带动量的SGD、Nesterov动量和Adam都很成功。它在相同数量的训练历时后达到同等的测试精度，但参数更新较少，导致更大的并行性和更短的训练时间。 我们可以通过增加学习率$epsilon$和将批次大小$B\prop扩展到$epsilon$来进一步减少参数更新的数量。最后，我们可以增加动量系数$m$并将$B\prop扩展到1/(1-m)$，尽管这往往会稍微降低测试精度。
传统的问题回答模型使用交叉熵损失进行优化，它鼓励精确的答案，代价是惩罚有时同样准确的附近或重叠的答案。我们提出了一个混合目标，将交叉熵损失与自我批评的策略学习相结合，使用来自单词重叠的奖励来解决评价指标和优化目标之间的不一致。 除了混合目标外，我们还引入了一个深度残差外套编码器，该编码器受到了最近在深度自我注意和残差网络方面工作的启发。我们的建议提高了不同问题类型和输入长度的模型性能，特别是对于需要捕捉长期依赖关系的长问题。在斯坦福大学问题回答数据集上，我们的模型以75.1%的精确匹配精度和83.1%的F1取得了最先进的结果，而合集则取得78.9%的精确匹配精度和86.0%的F1。
在过去的几年里，神经结构搜索（NAS）在大量的应用中取得了突破性的成功。现在可能是时候退一步分析NAS领域的好坏了。在这项工作中，我们提出了对NAS-Bench-101的扩展：NAS-Bench-201，它具有不同的搜索空间、多个数据集的结果和更多的诊断信息。NAS-Bench-201有一个固定的搜索空间，为几乎所有最新的NAS算法提供一个统一的基准。 我们的搜索空间的设计灵感来自于最流行的基于细胞的搜索算法，其中细胞被表示为一个有向无环图。这里的每条边都与从预定义的操作集中选择的操作有关。为了适用于所有的NAS算法，NAS-Bench-201中定义的搜索空间包括由4个节点和5个相关操作选项产生的所有可能的架构，这导致总共有15,625个神经细胞候选人。 使用相同设置的训练日志和每个架构候选者的性能提供了三个数据集。这使得研究人员可以避免对选定的架构进行不必要的重复训练，而只关注搜索算法本身。为每个架构节省的训练时间也在很大程度上提高了大多数NAS算法的效率，并为更多的研究人员提供了一个计算成本更低的NAS社区。 我们提供了额外的诊断信息，如细粒度的损失和准确性，这可以给NAS算法的新设计带来灵感。为了进一步支持所提出的NAS-Bench-102，我们从许多方面对其进行了分析，并对10个最近的NAS算法进行了基准测试，这验证了其适用性。
生成对抗网络（GANs）是学习复杂高维分布的最流行的工具之一。然而，GANs的泛化特性还没有被很好地理解。本文中，我们分析了GANs在实际环境中的泛化问题。我们表明，用原始GAN损失在离散数据集上训练的判别器的泛化能力很差，不能接近理论上的最佳判别器。 我们提出了一个以零为中心的梯度惩罚，通过将其推向最优判别器来提高判别器的泛化能力。该惩罚保证了GANs的泛化和收敛。在合成和大规模数据集上的实验验证了我们的理论分析。
生成对抗网络（GANs）是一类深度生成模型，旨在以无监督的方式学习目标分布。虽然它们被成功地应用于许多问题，但训练GAN是一项众所周知的挑战性任务，需要大量的超参数调整、神经结构工程以及非微不足道的 "技巧"。 在许多实际应用中的成功，加上缺乏量化GANs失败模式的措施，导致了大量提议的损失、正则化和规范化方案以及神经架构。在这项工作中，我们从实用的角度冷静地看待GANs的现状。我们再现了当前的技术状态，并超越了公平探索GAN的景观。我们讨论了常见的陷阱和可重复性问题，在Github上开源了我们的代码，并在TensorFlow Hub上提供预训练模型。
随着深度强化学习（RL）被应用到更多的任务中，有必要对所学的代理的行为进行可视化和理解。Saliency地图通过突出输入状态中与代理采取行动最相关的特征来解释代理行为。 现有的基于扰动的计算显著性的方法经常突出与代理人采取的行动无关的输入区域。我们的方法通过平衡两个方面（特异性和相关性）来生成更有针对性的显著性地图，这些方面反映了显著性的不同要求。 第二种是对改变待解释行动之外的其他行动的相对预期报酬的不相关特征进行减权。 我们将我们的方法与现有的关于代理训练的方法进行了比较，以玩棋类游戏（国际象棋和围棋）和Atari游戏（Breakout、Pong和Space Invaders）。 我们通过说明性的例子（国际象棋、雅达利游戏、围棋）、人类研究（国际象棋）和自动评估方法（国际象棋）表明，我们的方法产生的显著性地图对人类来说比现有的方法更容易解释。
为了理解深度神经网络的内部工作并提供可能的理论解释，我们通过未经训练的随机权重CNN-DCN架构来研究深度表征。作为卷积自动编码器，CNN表示卷积神经网络中从输入到中间卷积层的部分，DCN表示相应的解卷积部分。与预训练的CNN的DCN训练相比，随机权重CNN的DCN训练收敛得更快，并产生更高质量的图像重建。 然后，对于整个随机CNN-DCN会发生什么？我们获得了耐人寻味的结果，即图像可以以良好的质量进行重建。为了获得对中间随机表示的更多了解，我们研究了网络宽度与深度、随机通道的数量以及随机核的大小对重建质量的影响，并对经验观察提供了理论依据。我们进一步提供了一个使用随机权重CNN-DCN架构的快速风格转换应用，以展示我们观察的潜力。
 目前深度和计算成本之间的权衡使得许多工业应用很难采用深度神经网络，特别是在计算能力有限的情况下。在这里，我们受到启发，虽然需要更深的嵌入来区分困难的样本，但大量的样本可以通过更浅的嵌入得到很好的区分。 在这项研究中，我们引入了决策门（d-gate）的概念，这些模块被训练来决定一个样本是否需要被投射到更深的嵌入中，或者是否可以在d-gate进行早期预测，从而实现在不同深度的动态表征的计算。 实验结果表明，利用所提出的d-gate模块，ResNet-101的速度提高了38%，FLOPS减少了39%；在CIFAR10数据集上训练的DenseNet-201的速度提高了46%，FLOPS减少了36%，而精度只下降了2%。
这项工作为神经网络的理论理解提供了一个额外的步骤。我们考虑了具有一个隐藏层的神经网络，并表明当学习对称函数时，可以选择初始条件，从而使标准的SGD训练有效地产生泛化保证。我们通过经验验证，并表明当初始条件被随机选择时，这并不成立。
架构搜索的目的是自动寻找与人类专家设计的架构相竞争的神经架构。虽然最近的方法在图像识别方面取得了最先进的预测性能，但在资源限制下它们是有问题的，原因有两个。(我们通过提出LEMONADE来解决第一个缺陷，LEMONADE是一种用于多目标架构搜索的进化算法，它可以在方法的一次运行中接近多个目标下的架构的Pareto-front，例如预测性能和参数数量。 我们通过为LEMONADE提出一个拉马克式的继承机制来解决第二个缺陷，该机制产生的子代网络是以其训练有素的父代的预测性能为基础的，这是通过使用（近似的）网络变形算子来完成的。 这两个贡献的结合使我们能够在8个GPU上找到与不同大小的NASNets、MobileNets、MobileNets V2和Wide Residual Networks相当甚至优于CIFAR-10和ImageNet64x64的模型，这比之前产生最先进性能的架构搜索方法的计算能力要低20-40倍。
我们解决了概率主题模型的两个挑战，以便更好地估计一个词在给定语境中的概率，即P（wordjcontext）：（1）语境中没有语言结构。在这项工作中，我们通过将神经自回归主题模型（TM）与基于LSTM的语言模型（LSTM-LM）结合在一个单一的概率框架中，纳入语言结构。 LSTM-LM通过考虑本地搭配模式中的词序来学习每个词的矢量空间表示，而TM同时从整个文档中学习一个潜在的表示。此外，LSTM-LM对语言的复杂特征（如语法和语义）进行建模，而TM则在文档集合中发现潜在的主题结构。我们通过在一个统一的概率框架中结合主题模型和语言模型，将学习单词出现的意义的两种互补模式结合起来，命名为ctx-DocNADE。我们通过语言建模方法将外部知识纳入神经自回归主题模型来解决这个问题：我们使用词嵌入作为LSTM-LM的输入，目的是在较小的和/或短文语料库中改进词主题映射。 我们提出的DocNADE扩展被命名为ctx-DocNADEe。我们提出了新的神经自回归主题模型变体，加上神经语言模型和嵌入预设，在来自不同领域的6个长文本和8个短文本数据集的泛化（perplexity）、可解释性（topic coherence）和适用性（检索和分类）方面一直优于现有的生成性主题模型。
深度神经网络的大内存需求使许多设备的能力受到限制，从而限制了它们的部署和采用。模型压缩方法有效地减少了这些模型的内存需求，通常是通过应用权重修剪或量化等转换来实现的。在本文中，我们提出了一种新的有损权重编码方案，它是对传统压缩技术的补充。 编码是基于布卢米尔过滤器，这是一种概率数据结构，可以以引入随机错误为代价节省空间。利用神经网络容忍这些不完美的能力，并通过围绕错误进行重新训练，所提出的技术，即Weightless，可以将DNN的权重压缩高达496倍；在模型精度相同的情况下，这导致比最先进的技术改进高达1.51倍。
最近的工作表明，从神经机器翻译（NMT）中得到的语境化的单词表征是一种可行的替代方法，可以替代这种来自简单的单词预测任务。这是因为为了能够从一种语言翻译到另一种语言，需要建立的内部理解要全面得多。 不幸的是，到目前为止，计算和内存的限制使NMT模型无法使用大型词汇表，因此，诸如子词单元（BPE和形态学分割）和字符等替代物已经被使用。在这里，我们研究了使用不同种类的单元对用于建立语法、语义和形态学模型时产生的表示质量的影响。 我们发现，虽然由子词产生的表征在建模句法方面略胜一筹，但基于字符的表征在建模形态方面更胜一筹，而且对嘈杂的输入也更稳健。
少数次学习是指只用少数几个例子学习新类的过程，它仍然是机器学习中的一项挑战性任务。许多复杂的少数次学习算法已经被提出，其依据是如果只用少数几个例子对网络进行简单的微调，就很容易过度适应新的例子。 在这项研究中，我们表明在常用的低分辨率mini-ImageNet数据集中，微调方法在1次拍摄任务中取得了比普通的几次拍摄学习算法更高的精度，在5次拍摄任务中几乎与最先进算法的精度相同。 然后，我们用更多的实际任务来评估我们的方法，即高分辨率单域和跨域任务。我们进一步分析了实验结果，并表明：1）通过采用低学习率可以稳定再训练过程，2）在微调过程中使用自适应梯度优化器可以提高测试精度，3）当基础类和新类之间存在较大的领域转换时，通过更新整个网络可以提高测试精度。
我们提出了一种基于生成式对抗网络生成现实的高保真股票市场数据的方法。我们将订单流建模为具有有限历史依赖性的随机过程，并采用条件Wasserstein GAN来捕捉股票市场中订单的历史依赖性。我们用实际的市场数据和一些不同的统计数字的合成数据来测试我们的方法，发现生成的数据与真实数据很接近。
我们提出了一种新型的黑箱对抗攻击算法，在$ell_infty$和$ell_2$指标下的查询效率具有最先进的模型规避率。它利用了一种基于textit{sign}而不是基于量级的梯度估计方法，将梯度估计从连续的黑箱优化转移到二进制黑箱优化。 它自适应地构建查询来估计梯度，一个查询依赖于前一个查询，而不是每一步随机查询的重新估计梯度。它对符号位的依赖产生了较小的内存占用，它既不需要超参数调整也不需要降维。 此外，它的理论性能是有保证的，它可以比白盒梯度对齐的子空间更好地表征对抗性子空间。在两个公开的黑盒攻击挑战和一个针对转移攻击的强大训练的模型上，该算法的规避率超过了所有提交的攻击。 对于一套已公布的模型，该算法的故障率降低了3.8倍，而花费的查询次数比最先进的算法组合少了2.5倍。例如，它平均只用12美元的查询就能躲过一个标准的MNIST模型。
递归神经网络（RNN）是广泛使用的序列数据模型。就像前馈网络一样，建立 "深度 "RNN已经变得很普遍，即堆叠多个递归层以获得数据的更高层次的抽象。我们研究了多层RNN的训练，并检查了梯度的大小，因为它们在网络中传播。我们表明，根据基本递归单元的结构，梯度被系统地削弱或放大，所以随着深度的增加，它们倾向于消失或爆炸。 基于我们的分析，我们设计了一种新型的门控单元，它能更好地保留梯度大小，因此有可能训练更深的RNN。
尽管在经验上取得了成功，但批量归一化（BN）的稳定性、收敛性和加速特性的理论基础仍然难以捉摸。在本文中，我们从建模的方法来攻击这个问题，我们对应用于简化模型的BN进行了彻底的理论分析：普通最小二乘法（OLS）。 我们发现，带有BN的OLS的梯度下降具有有趣的特性，包括缩放规律、任意学习率下的权重收敛、渐进加速效应，以及对学习率的选择不敏感等。
解决强化学习中的任务并非易事。由于代理的目标是使累积的奖励最大化，它经常学会利用奖励信号中的漏洞和错误的规格，从而导致不需要的行为。虽然约束条件可以解决这个问题，但对于一般的约束条件没有封闭式的解决方案。 在这项工作中，我们提出了一种新的多时间尺度的约束政策优化方法，称为 "奖励约束政策优化"（RCPO），它使用替代的惩罚信号来引导政策走向满足约束的政策。我们证明了我们的方法的收敛性，并提供了它能够训练满足约束的政策的经验证据。
手持虚拟面板（HVP）是虚拟现实（VR）中连接到非主导手控制器的虚拟面板。HVP是在VR设备中启用菜单和工具箱的首选技术。 在本文中，我们研究了HVP的目标获取性能作为四个因素的函数：目标宽度、目标距离、相对于重力的接近方向和接近角度。我们的结果表明，所有四个因素都对用户的性能有重大影响。
深度神经网络在各种知识管理应用中表现出了前所未有的成功。然而，创建的网络往往非常复杂，有大量的可训练边缘，需要大量的计算资源。我们注意到，许多成功的网络往往包含大量的冗余边缘。 iSparse利用一个新的边缘重要性分数E来确定一个边缘对最终网络输出的重要性。此外，iSparse既可以在训练模型时应用，也可以在预训练模型的基础上应用，这使得它成为一个无需再训练的方法--导致最小的计算开销。在基准数据集上对iSparse与PFEC、NISP、DropConnect和Retraining-Free的比较表明，iSparse导致有效的网络稀疏化。
在语言建模任务中，根据局部语境预测单词，对于学习单词嵌入和短语的语境依赖性表示非常有效。由于观察到将世界知识编入机器可读知识库的努力倾向于以实体为中心，我们研究了使用填空任务来从提到这些实体的语境中学习实体的语境独立表示。 我们表明，神经模型的大规模训练使我们能够学习到极高保真度的实体类型信息，这一点我们用维基百科类别的几张照片来证明。我们的学习方法强大到足以编码专门的主题，如环意自行车手。
变异自动编码器（VAEs）在从具有复杂依赖关系的高维数据中学习低维流形方面取得了成功。在其核心中，它们包括一个强大的贝叶斯概率推理模型，以捕获数据的突出特征。 通过利用传统下限ELBO和IWAE中的q变形对数，以及上限CUBO，我们对这一研究方向做出了贡献。 在这项概念验证研究中，我们探索了创建这些比经典界线更紧的q变形界线的不同方法，并且我们显示了这种VAE在二值化的MNIST数据集上的性能改进。
在机器学习中长期存在一种信念，即在训练数据上扩大边际，可以通过增加鲁棒性来抵抗模型的过拟合。然而，Breiman显示了一个困境（Breiman, 1999），即对边际分布的统一改进\emph{不一定能减少泛化误差。在本文中，我们通过最近提出的使用谱规范积的Lipschitz常数约束的规范化边际，在深度神经网络中重新审视Breiman的困境。 通过简化的理论和广泛的实验，Breiman的困境被证明是依赖于归一化边际分布的动态性，这反映了模型表达能力和数据复杂性之间的权衡。 当数据的复杂性与模型表达能力相当时，即训练和测试数据在归一化边际动态中具有相似的相变，通过经典的基于边际的泛化界线得出两种有效的方法来成功预测泛化误差的趋势。另一方面，过度表达的模型在训练归一化边际上表现出均匀的改进，可能会失去这种预测能力，无法防止过拟合。
开发一个端到端的多领域任务导向的对话系统一直是一个公开的研究挑战，在这个系统中，人可以与对话代理进行对话，完成一个以上领域的任务。首先，跟踪多领域对话的信念状态是很困难的，因为对话代理必须从所有相关领域获得完整的信念状态，每个领域都可以有各领域共同的共享槽，以及只针对该领域的独特槽。 其次，对话代理还必须处理各种类型的信息，包括来自对话环境的上下文信息，当前对话回合的解码对话状态，以及来自知识库的查询结果，以便从语义上形成对人类的语境感知和特定任务的反应。 为了应对这些挑战，我们提出了一个端到端的神经架构，用于多个领域的任务导向型对话。我们提出了一个新颖的多级神经信仰追踪器，它通过独立学习槽和领域层面的信号来追踪对话信仰状态。这些表征在后期融合方法中被结合起来，形成（领域，槽）对的联合特征向量。 根据最近在端到端对话系统方面的工作，我们将信念跟踪器与生成组件结合起来，以解决端到端对话任务。我们在MultiWOZ2.1基准上取得了最先进的性能，联合目标准确率为50.91%，在任务完成和响应生成方面具有竞争力。
分数匹配为学习灵活的非正则化模型提供了有效的方法，但由于需要评估二阶导数，其可扩展性受到限制。 在本文中，我们将包括分数匹配在内的一般学习目标系列与Wasserstein梯度流联系起来。这种联系使我们能够设计出这些目标的可扩展近似值，其形式类似于单步对比发散。
本文开发了变异持续学习（VCL），这是一个简单而通用的持续学习框架，它融合了在线变异推理（VI）和神经网络蒙特卡洛VI的最新进展。该框架可以在复杂的持续学习环境中成功地训练深度判别模型和深度生成模型，其中现有任务随时间演变，并出现了全新的任务。实验结果表明，VCL在各种任务上优于最先进的持续学习方法，以全自动的方式避免了灾难性的遗忘。
在部分可观察（PO）环境中，深度强化学习（RL）代理经常遭受不满意的表现，因为有两个问题需要一起解决：如何从原始观察中提取信息来解决任务，以及如何改进策略。在这项研究中，我们提出了一种解决PO任务的RL算法。我们的方法包括两部分：一个用于环境建模的变异递归模型（VRM），以及一个可以访问环境和VRM的RL控制器。 我们在两种类型的PO机器人控制任务中测试了所提出的算法，一种是坐标或速度不可观察的任务，另一种是需要长期记忆的任务。我们的实验表明，与其他替代方法相比，所提出的算法在未观察到的状态不能以简单的方式从原始观察中推断出来的任务中取得了更好的数据效率和/或学到了更多的最优策略。
本文在强化学习（RL）的背景下正式提出了在线算法选择的问题。设置如下：给定一个偶发任务和有限数量的非政策RL算法，一个元算法必须决定在下一个偶发事件中控制哪个RL算法，以使预期收益最大化。 文章提出了一种新的元算法，称为 "纪元随机强盗算法选择"（ESBAS）。其原理是在每个纪元冻结策略更新，并让重启的随机强盗负责算法选择。 在一些假设条件下，考虑到结构性采样预算的限制，彻底的理论分析证明了它的近乎最优性。ESBAS首先在一个对话任务上进行了经验评估，结果表明它在大多数配置中都优于每个单独的算法。然后，ESBAS被调整为一个真正的在线设置，其中算法在每次转换后更新其政策，我们称之为SSBAS。SSBAS在一个水果收集任务上进行了评估，结果表明它比经典的双曲衰减更有效地适应步长参数，并在一个Atari游戏中，它以很大的幅度提高性能。
在本文中，我们提出了一个新的学习潜伏嵌入的生成模型。与经典的生成过程相比，每个观察到的数据点都是由一个单独的潜伏变量生成的，我们的方法假设了一个全局潜伏变量来生成整个观察到的数据点。然后，我们提出了一个学习目标，它被推导为数据对数似然的近似下限，导致我们的算法WiSE-ALE。 与标准的ELBO目标（鼓励每个数据点的变异后验与先验分布相匹配）相比，WiSE-ALE目标将所有样本的平均后验与先验相匹配，允许样本的后验分布具有更广泛的可接受的嵌入平均值和方差，并导致自动编码过程中更好的重建质量。通过各种例子和与其他最先进的VAE模型的比较，我们证明WiSE-ALE具有出色的信息嵌入特性，同时仍然保留学习平滑、紧凑表示的能力。
我们通过使用插值函数作为输出激活，提高了深度神经网络对对抗性攻击的鲁棒性。  这种依赖于数据的激活函数明显提高了分类精度和对对抗性扰动的稳定性。与对抗性图像的总变异最小化和增强训练一起，在最强的攻击下，我们分别在快速梯度符号法、迭代快速梯度符号法和Carlini-WagnerL2攻击下实现了高达20.6%、50.7%和68.7%的精度提高。 我们的防御策略是对许多现有方法的补充。 我们通过分析特征空间的几何形状，对我们的防御策略给出了直观的解释。为了重现性，代码将在GitHub上提供。
这项工作提出了一个可扩展的连续视觉语音识别解决方案。为了实现这一目标，我们构建了现有的最大的视觉语音识别数据集，包括成对的文本和人脸说话的视频剪辑（3,886小时的视频）。同时，我们设计并训练了一个综合唇读系统，包括一个视频处理管道，将原始视频映射到嘴唇和音素序列的稳定视频，一个可扩展的深度神经网络，将嘴唇视频映射到音素分布的序列，以及一个生产级语音解码器，输出单词的序列。 我们的方法大大改进了以前的唇读方法，包括LipNet和Watch, Attend, and Spell (WAS)的变体，它们分别只能达到89.8%和76.8%的WER。
以前的工作（Bowman等人，2015；Yang等人。2017）发现了开发基于变异自动编码器（VAE）的文本生成模型的困难。为了解决解码器忽略来自编码器的信息（后置崩溃）的问题，这些以前的模型削弱了解码器的能力，迫使模型使用来自潜在变量的信息。然而，这种策略并不理想，因为它降低了生成文本的质量并增加了超参数。 在本文中，我们提出了一种新的文本VAE，利用多模态先验分布、修正的编码器和多任务学习。我们表明我们的模型可以生成条件良好的句子，而不会削弱解码器的能力。
模型修剪旨在诱导深度神经网络的各种连接矩阵的稀疏性，从而减少模型中非零值参数的数量。最近的报告（Han等人，2015；Narang等人。最近的报告（Han等人，2015；Narang等人，2017）对深度网络进行了修剪，其代价是只损失了少量的准确性，并实现了模型大小的大幅减少。这暗示了一种可能性，即这些实验中的基线模型在一开始就可能严重地过度参数化，模型压缩的一个可行的选择可能是简单地减少隐藏单元的数量，同时保持模型的密集连接结构，暴露了模型大小和准确性的类似权衡。 我们研究了在资源受限的环境中进行节能推理的这两条不同的模型压缩路径，并提出了一种新的渐进式修剪技术，该技术简单明了，可以在各种模型/数据集上应用，只需进行最小的调整，并可以无缝地纳入训练过程中。 我们比较了大型但经过修剪的模型（large-sparse）和较小但密集（small-dense）的对应模型的准确性，它们的内存占用相同。在广泛的神经网络架构（深度CNN、堆叠LSTM和seq2seq LSTM模型）中，我们发现大型稀疏模型的表现一直优于小型密集模型，在非零参数的数量上实现了10倍的减少，而准确性损失最小。
我们提出了一个简单的替代方案：用于可控语言生成的即插即用语言模型（PPLM），它将预训练的语言模型与一个或多个简单的属性分类器结合起来，指导文本生成，而无需对语言模型进行任何进一步训练。 在我们提出的典型方案中，属性模型是简单的分类器，由用户指定的词包或单一的学习层组成，其参数比LM少100,000倍。采样需要向前和向后传递，其中属性模型的梯度推动LM的隐藏激活，从而指导生成。 模型样本展示了对一系列主题和情感风格的控制，广泛的自动和人工注释的评估显示了属性的一致性和流畅性。PPLMs是灵活的，因为任何可区分的属性模型的组合都可以用来指导文本生成，这将使本文给出的例子之外的多样化和创造性应用成为可能。
现有的深度多任务学习（MTL）方法将任务间的共享层以平行排序的方式排列。这种组织方式大大限制了可以学习的共享结构的类型。 结果表明，灵活的排序可以实现更有效的共享，从而推动了软排序方法的发展，该方法可以学习共享层如何以不同的方式应用于不同的任务。这些结果表明，深度MTL的力量来自于学习高度通用的构建模块，这些模块可以被组装起来以满足每个任务的需求。
我们提出了一个通用框架，通过深度神经网络中的随机推断来校准预测的准确性和可信度（分数）。我们首先分析了单个实例推断的多个模型参数的变化与随机正则化的贝叶斯模型中相应预测分数的方差之间的关系。我们的经验观察表明，预测的准确性和分数与随机深度或辍学给出的多个随机推断的方差高度相关。 在这些事实的激励下，我们设计了一个新的方差加权的置信度整合损失函数，该函数由两个相对于ground-truth和均匀分布的交叉熵损失项组成，它们被随机预测分数的方差所平衡。 我们的算法呈现出出色的置信度校准性能，并在多种模型和数据集中使用两种流行的随机正则化技术--随机深度和辍学--来提高分类精度；它通过训练网络来实现与预测置信度成比例的预测精度，大大缓解了深度神经网络的过度置信问题。
现实生活中的控制任务涉及各种物质的事项---刚性或软体、液体、气体---每个都有不同的物理行为。这给传统的刚体物理引擎带来了挑战。基于粒子的模拟器已经被开发出来，以模拟这些复杂场景的动态；然而，依靠近似技术，他们的模拟往往偏离现实世界的物理，特别是在长期。在本文中，我们提出学习一个基于粒子的模拟器，用于复杂控制任务。 将学习与基于粒子的系统相结合带来了两个主要的好处：首先，学习的模拟器，就像其他基于粒子的系统一样，广泛地作用于不同材料的物体；其次，基于粒子的表示法为学习带来了强烈的归纳偏见：同一类型的粒子内部具有相同的动力学。 这使模型能够在几次观察中迅速适应未知动态的新环境。我们通过模拟和现实世界的实验，展示了机器人利用学习的模拟器实现复杂的操纵任务，如操纵液体和可变形的泡沫。我们的研究有助于为机器人学习基于粒子的动态场景打下基础。
生成对抗网络（GANs）在对具有不同模式的大型数据集进行训练时，已知会产生不明显属于任何模式的混杂图像。(1)对于种类繁多的数据集，模式很可能位于不同的流形上。(2)生成器(G)被表述为一个连续函数，而输入的噪声来自一个连接集，因此G的输出是一个连接集。 如果G涵盖了所有的模式，那么G的输出中一定有一些部分将它们连接起来。这相当于不理想的、混淆的图像。我们提出了支持这些直觉的理论论据。我们提出了一种新的方法，通过潜伏噪声空间的可学习不连续性来打破第二个假设。 我们还用分类器C增强了GAN的表述，该分类器预测哪个噪声分区/生成器产生了输出图像，鼓励每个分区/生成器之间的多样性。我们在MNIST、celebA、STL-10和一个具有明显不同模式的困难数据集上进行了实验，结果表明，噪声分区对应于数据分布的不同模式，并产生质量更好的图像。
为了利用众包数据来训练能够为所有说话人合成干净语音的多说话人文本-语音（TTS）模型，必须学习能够独立控制说话人身份和生成信号中的背景噪声的分离表征。然而，学习这样的表征可能具有挑战性，因为缺乏描述每个训练例子的录音条件的标签，而且说话人和录音条件往往是相关的，例如，由于用户经常使用相同的设备进行许多录音。(本文提出了三个部分来解决这个问题：（1）制定一个具有因子化潜变量的条件生成模型，（2）使用数据增强来添加与说话人身份不相关的噪声，并且其标签在训练期间是已知的，以及（3）使用对抗性因子化来改善分离。实验结果表明，即使在训练数据中说话人和噪声属性是相关的，拟议的方法也能分离它们，并且可以用来为所有说话人持续合成干净的语音。
基于LSTM的语言模型在其表征中表现出构成性，但这种行为是如何在训练过程中出现的还没有被探讨过。通过分析上下文分解的合成数据实验，我们发现LSTM在训练过程中通过从较短的成分中建立长距离的依赖关系来学习构成性。
学习深度神经网络需要解决一个具有挑战性的优化问题：它是一个具有大量项的高维、非凸和非平滑的最小化问题。目前神经网络优化的做法是依靠随机梯度下降（SGD）算法或其自适应变体。 然而，SGD需要手工设计学习率的时间表。此外，其自适应变体往往产生的解决方案在未见数据上的概括性不如手工设计时间表的SGD。我们提出了一种优化方法，根据经验提供了两个世界的最佳方案：我们的算法产生了良好的概括性能，同时只需要一个超参数。 我们的方法是基于一个复合近似框架，它利用了深度神经网络的组成性质，并可以通过设计利用强大的凸优化算法。具体来说，我们采用了SVM的Frank-Wolfe（FW）算法，它在每个时间步长中以闭合形式计算出最佳步长。我们进一步表明，下降方向是由网络中一个简单的后向传递给出的，产生与SGD相同的每次迭代计算成本。 我们在CIFAR和SNLI数据集上进行了实验，证明了我们的方法比Adam、Adagrad以及最近提出的BPGrad和AMSGrad有明显的优势。此外，我们将我们的算法与SGD与手工设计的学习率计划进行了比较，结果表明它提供了类似的泛化，同时往往收敛得更快。代码可在https://github.com/oval-group/dfw 公开。
在本文中，我们展示了如何使用逼真的AI2THOR模拟器将新颖的转移强化学习技术应用于目标驱动的导航这一复杂任务。 我们引入了新的架构1贡献的继任特征依赖策略（SFDP），并采用了变异信息瓶颈的概念来实现最先进的性能。VUSFA是我们最终的架构，是一种直接的方法，可以使用我们的开源资源库来实现。
学习图像表征的一个主要挑战是分解图像形成的基础变化因素。 这通常是通过自动编码器结构实现的，其中潜变量的一个子集被限制为对应于特定的因素，其余的被认为是滋扰变量。这种方法有一个重要的缺点：随着滋扰变量的维度增加，图像重建得到改善，但解码器可以灵活地忽略指定的因素，从而失去了对它们进行输出的能力。 在这项工作中，我们建议通过逐步增加潜伏代码的维度来克服这种权衡，同时约束输出图像相对于分解变量的雅各布系数保持不变。 因此，所获得的模型在解缠和重建方面都是有效的。 我们证明了这种方法在无监督和有监督的情况下学习分解表征的适用性。在一个面部属性操作任务中，我们获得了高质量的图像生成，同时用一个模型平滑地控制了几十个属性。这比最先进的方法多了一个数量级的分解因素，同时获得了视觉上相似或优越的结果，避免了对抗性训练。
我们提出了一个网络层相对于输入批次的 "非线性 "的新概念，该概念基于其与线性系统的接近程度，反映在激活矩阵的非负等级上。我们通过对激活矩阵应用非负因子化来测量这种非线性。考虑到类似样本的批次，我们发现深层的高非线性表明了记忆。 此外，通过逐层应用我们的方法，我们发现记忆的机制包括不同的阶段。我们在几个图像和音频数据集上训练的全连接和卷积神经网络上进行了实验。我们的结果表明，作为记忆的指标，我们的技术可以用来进行早期停止。
深度神经网络在各个领域都取得了最先进的性能，但它们必须缩小规模才能用于现实世界的应用。作为减少神经网络规模同时保持其性能的一种手段，知识转移引起了很多人的注意。一种流行的知识转移方法是知识蒸馏（KD），其中预训练的教师网络的软化输出帮助训练学生网络。自KD以来，其他转移方法已经被提出，它们主要关注损失函数、隐藏层的激活或附加模块，以便将知识从教师网络很好地转移到学生网络。 在这项工作中，我们专注于教师网络的结构，以获得多个教师网络的效果，而不需要额外的资源。我们建议改变教师网络的结构，使其具有随机的块和跳过的连接。 在训练阶段，每个子网络都是通过随机丢弃随机块产生的，并被用作教师网络。这允许用多个教师网络训练学生网络，并在单个教师网络的相同资源上进一步增强学生网络。我们验证了提议的结构在基准数据集上给学生网络带来了进一步的改善。
基于输入文本的表情符号建议系统已经被提出来，以鼓励表情符号的使用和丰富文本信息；然而，这种系统对聊天体验的实际影响仍然是未知的。我们建立了一个具有词汇（基于单词）和语义（基于意义）的表情符号建议功能的安卓键盘，并在两个不同的研究中进行了比较。为了研究表情符号建议在在线对话中的效果，我们对24名参与者进行了实验室文本信息研究，还对18名参与者进行了15天的纵向实地部署。 我们发现，词汇性的表情符号建议比没有建议的键盘增加了31.5%的表情符号使用量，而语义建议则增加了125.1%的表情符号使用量。
用于文本生成的传统生成对抗网络（GANs）往往存在奖励稀少和模式崩溃的问题，影响了生成样本的质量和多样性。 在训练过程中，当发现当前生成的句子比之前生成的样本更好时，SAL就会对生成器进行奖励。这种自我改进的奖励机制使模型更容易获得积分，并避免向有限的真实样本崩溃，这不仅有助于缓解奖励的稀疏性问题，还能减少模式崩溃的风险。在文本生成基准数据集上的实验表明，与之前用于文本生成的GAN相比，我们提出的方法大大改善了质量和多样性，并产生更稳定的性能。
确定潜伏维数是机器学习中一个普遍存在的问题。在这项研究中，我们引入了一种新的方法，依靠SVD来发现潜伏维数。该方法的一般原理是比较数据集的SVD分解的奇异值曲线与随机数据集曲线。 为了评估我们的方法，我们将其与其他竞争方法进行了比较，如Kaisers特征值大于零（K1），平行分析（PA），Velicers MAP测试（最小平均部分）。我们还将我们的方法与Silhouette Width（SW）技术进行了比较，该技术被用于不同的聚类方法以确定最佳的集群数量。
深度学习模型通常对对抗性攻击很敏感，精心设计的输入样本会导致系统产生不正确的决策。我们在这里关注检测攻击的问题，而不是稳健的分类，因为检测攻击的发生可能比避免错误分类更重要。我们提出了EXAID，一种新颖的攻击检测方法，它使用模型的可解释性来识别那些解释与预测的类别不一致的图像。具体来说，我们使用SHAP，它在输入图像的空间中使用Shapley值，来识别哪些输入特征对类别决定有贡献。有趣的是，这种方法不需要修改被攻击的模型，它可以在不建立特定攻击模型的情况下应用。因此，它可以成功地应用于检测不熟悉的攻击，这些攻击在设计检测模型时是未知的。我们在两个基准数据集CIFAR-10和SVHN上对EXAID进行了评估，并与三种领先的攻击技术FGSM、PGD和C&W进行了对比。我们发现EXAID在很大的噪声水平范围内比SoTA检测方法有很大的改进，对于小的扰动，检测率从70%提高到90%以上。
我们描述了一种简单而通用的神经网络权重压缩方法，其中网络参数（权重和偏差）在一个 "潜在 "空间中表示，相当于重新参数化。这个空间配备了一个学习的概率模型，在训练期间用于对参数表示施加熵惩罚，并在训练后使用一个简单的算术编码器来压缩表示。 我们在MNIST、CIFAR-10和ImageNet分类基准上使用六种不同的模型架构对该方法进行了评估。我们的结果表明，最先进的模型压缩可以以可扩展和通用的方式实现，不需要多阶段训练等复杂程序。
在这方面，我们提出了Ada-Boundary，一种新型的自适应批量选择算法，它根据模型的学习进度构建有效的小批量。我们的关键想法是提出混乱的样本，即真正的标签是什么。 利用我们的设计，Ada-Boundary在不同的训练难度下都能保持其优势。我们通过使用两个卷积神经网络对三个基准数据集进行广泛的实验来证明Ada-Boundary的优势。实验结果表明，与最先进的策略相比，Ada-Boundary的训练时间最多可提高31.7%，与基线策略相比最多可提高33.5%。
最先进的声音事件分类依赖于神经网络来学习数据集中的类标签和音频记录之间的关联。这些数据集通常定义一个本体来创建一个结构，将这些声音类与更抽象的超级类联系起来。因此，本体作为声音的领域知识表示的来源。 我们提出了两个基于本体的神经网络架构，用于声音事件分类。我们定义了一个框架来设计简单的网络架构，保留本体结构。
与全连接网络相反，卷积神经网络（CNN）通过学习与具有有限空间范围的局部过滤器相关的权重来实现效率。这意味着一个过滤器可能知道它在看什么，但不知道它在图像中的位置。 在本文中，我们测试了这一假设，揭示了在常用的神经网络中编码的绝对位置信息的惊人程度。一组全面的实验显示了这一假设的有效性，并阐明了这一信息是如何和在哪里表示的，同时提供了关于位置信息在深度CNN中的来源的线索。
语义分析将自然语言句子映射为其意义的正式机器可读表示，受到有限的注释训练数据的高度限制。受从粗到细的启发，我们提出了一个从一般到详细的神经网络（GDNN），在语料及其逻辑形式中纳入跨领域草图（CDS）。 我们的实验表明，与直接的多任务学习相比，CDS提高了语义解析任务的性能，该任务将用户的请求转化为意义表示语言（MRL）。我们还通过实验说明CDS通过在目标解码过程中添加一些约束条件而发挥作用，这进一步证明了CDS的有效性和合理性。
不同神经架构的可学习性可以通过数据复杂性的可计算措施来直接表征。在本文中，我们将架构选择的问题重构为理解数据如何决定适合该数据的最具表现力和可概括性的架构，而不是归纳偏见。在提出代数拓扑结构作为数据复杂性的衡量标准后，我们表明网络在其决策边界中表达数据集的拓扑复杂性的能力是其概括能力的严格限制因素。 然后，我们提供了神经网络拓扑能力的第一个实证特征。我们的实证分析表明，在数据集复杂性的每一个层次上，神经网络都表现出拓扑相变和分层。这一观察使我们能够将现有理论与经验驱动的关于单隐层神经网络的架构选择的猜想联系起来。
自然科学中的挑战往往可以被表述为优化问题。机器学习技术最近被应用于解决此类问题。化学中的一个例子是设计定制的有机材料和分子，这需要有效的方法来探索化学空间。 我们提出了一种遗传算法（GA），该算法通过基于神经网络（DNN）的判别器模型来提高生成分子的多样性，同时引导GA.我们表明我们的算法在优化任务中优于其他生成模型。
来自变形器的双向编码器表示法（BERT）在各种自然语言处理任务中达到了最先进的结果。然而，对其内部功能的理解仍然是不充分和不满意的。为了更好地理解BERT和其他基于变形器的模型，我们提出了对BERT的隐藏状态进行分层分析。 与以前的研究不同，以前的研究主要是通过Transformer模型的权重来解释它们，我们认为隐藏状态包含同样有价值的信息。具体来说，我们的分析集中在对问题回答（QA）任务进行微调的模型上，作为复杂下游任务的一个例子。 我们检查了QA模型是如何转换令牌向量以找到正确答案的。为此，我们应用了一套一般的和针对QA的探测任务，揭示了存储在每个表示层中的信息。我们对隐藏状态可视化的定性分析提供了对BERT推理过程的额外见解。 我们的结果表明，BERT内部的转换经历了与传统管道任务相关的阶段。因此，该系统可以隐含地将特定的任务信息纳入其标记表示中。此外，我们的分析显示，微调对模型的语义能力影响不大，甚至在早期层的矢量表示中可以识别预测错误。
我们提出了一种通用的深度强化学习方法，并将其应用于机器人操纵任务。我们的方法利用演示数据来协助强化学习代理学习解决广泛的任务，主要是以前未解决的任务。我们对视觉运动策略进行端对端训练，学习从RGB相机输入到关节速度的直接映射。 我们的实验表明，我们的强化和模仿方法可以解决富于接触的机器人操纵任务，而这些任务是最先进的强化和模仿学习方法都无法单独解决的。我们还说明，这些策略通过在大的视觉和动力学变化下的训练，实现了零次的模拟真实转移。
集合体，即对多个神经网络进行单独训练，并对它们的预测进行平均，已被证明在提高单个神经网络的准确性和预测的不确定性方面取得了广泛成功。 在本文中，我们提出了BatchEnsemble，这是一种计算和存储成本明显低于典型合集的合集方法。BatchEnsemble通过将每个权重矩阵定义为所有合集成员之间的共享权重和每个成员的等级一矩阵的Hadamard乘积来实现这一目的。 与合集不同，BatchEnsemble不仅可以跨设备并行，即一个设备训练一个成员，而且在一个设备内也可以并行，即对于一个给定的小批量，多个合集成员同时更新。在CIFAR-10、CIFAR-100、WMT14 EN-DE/EN-FR翻译和上下文匪徒任务中，BatchEnsemble产生的准确性和不确定性与典型合集具有竞争力；在测试时间的速度提高3倍，合集规模为4的内存减少3倍。 我们还将BatchEnsemble应用于终身学习，在Split-CIFAR-100上，BatchEnsemble产生了与渐进式神经网络相当的性能，而计算和内存成本却低得多。我们进一步表明，BatchEnsemble可以很容易地扩展到Split-ImageNet上的终身学习，其中涉及100个连续的学习任务。
强化学习通常需要精心设计的奖励函数，以学习所需的行为。我们提出了一种新的奖励估计方法，该方法基于专家示范的最优状态轨迹的有限样本，可用于指导代理模仿专家的行为。最优状态轨迹被用来学习 "好 "状态分布的生成或预测模型。 有了这个推断的奖励函数，我们在内循环中执行标准的强化学习，以引导代理学习给定的任务。在一系列任务中的实验评估表明，与标准的强化学习相比，所提出的方法在完整或稀疏的手工设计的奖励方面产生了优越的性能。此外，我们表明，我们的方法成功地使代理直接从超级马里奥兄弟和Flappy Bird等游戏的专家玩家视频中学习良好的行动。
深度神经网络在处理自然语言的复杂语义方面取得了令人印象深刻的表现，而大部分被视为黑盒子。为了解释模型如何处理单词和短语的组成语义，我们研究了分层解释问题。我们强调的关键挑战是计算单个单词和短语的非加和与上下文无关的重要性。我们表明一些先前关于分层解释的努力，例如上下文分解，在数学上不满足期望的属性，导致不同模型中解释质量不一致。 在本文中，我们提出了一种正式的方法来量化每个单词或短语的重要性，以生成分层解释。我们根据我们的表述修改了上下文分解算法，并提出了一种具有竞争力的模型无关的解释算法。在多个数据集上对LSTM模型和微调的BERT Transformer模型进行的人类评估和自动指标评估表明，我们的算法稳健地优于先前的分层解释工作。我们表明我们的算法有助于解释语义的组成，提取分类规则，并提高人类对模型的信任。
带有随机动量的随机梯度下降（SGD）在非凸随机优化中很受欢迎，特别是在深度神经网络的训练中。在标准的SGD中，参数的更新是沿着当前迭代的梯度路径对一批例子进行改进，其中添加的 "动量 "项使更新的方向偏向于参数的先前变化。 在非随机凸优化中，人们可以证明动量调整在许多情况下可以减少收敛时间，但在随机和非凸设置中，这样的结果是难以捉摸的。同时，一个被广泛观察到的经验现象是，在训练深度网络时，随机动量似乎可以显著改善收敛时间，它的变体在其他流行的更新方法的发展中得到了蓬勃发展，例如。 然而，使用随机动量的理论依据仍然是一个重要的开放性问题。在本文中，我们提出了一个答案：随机动量改善了深度网络的训练，因为它修改了SGD以更快地逃离鞍点，因此，更快地找到二阶静止点。 我们的理论结果也阐明了如何选择理想的动量参数这一相关问题--我们的分析表明，在[0,1]中的$β\应该很大（接近1），这与经验发现相吻合。我们还提供了进一步验证这些结论的实验结果。
GANs为训练模仿数据分布的生成模型提供了一个框架。然而，在许多情况下，我们希望训练生成模型来优化它所生成的数据中的一些辅助目标函数，例如制作更美观的图像。在某些情况下，这些目标函数很难评估，例如，它们可能需要人类互动。 在这里，我们开发了一个系统，用于有效地训练GAN，以提高用户积极互动的一般比率，例如审美评价。要做到这一点，我们从相对较小的互动集合中建立一个目标领域的人类行为模型，然后使用这个行为模型作为辅助损失函数来改进生成模型。
半监督学习（SSL）是一种有效利用大量未标记数据的研究，以提高在有限标记数据条件下的性能。大多数传统的SSL方法假设未标记数据的类别包括在标记数据的类别集合中。 此外，这些方法没有对无用的无标签样本进行分类，而是使用所有的无标签数据进行学习，这并不适合现实情况。在本文中，我们提出了一种叫做选择性自我训练（SST）的SSL方法，它可以选择性地决定是否将每个无标签的样本纳入训练过程中。 对于传统的SSL问题，即处理有标签和无标签样本共享相同类别的数据，所提出的方法不仅表现得与其他传统SSL算法相当，而且可以与其他SSL算法相结合。 传统方法不能应用于分离的数据不共享类别的新SSL问题，而我们的方法即使在未标注数据的类别与标注数据的类别不同的情况下也不会出现任何性能下降。
深度生成神经网络已被证明在复杂数据分布的条件和无条件建模方面都很有效。条件生成可以实现交互式控制，但创建新的控制往往需要昂贵的再训练。本文中，我们开发了一种无需再训练模型的条件生成方法。通过事后学习潜伏约束，价值函数识别潜伏空间中产生具有所需属性的输出的区域，我们可以用基于梯度的优化或摊销的演员函数从这些区域有条件地采样。 将属性约束与普遍的 "现实性 "约束结合起来，强制执行与数据分布的相似性，我们从无条件变异自动编码器中生成现实的条件图像。最后，利用基于梯度的优化，我们展示了在潜空间中进行最小调整以修改图像属性的保全身份转换。
最近在训练神经网络的硬件和方法上取得的进展迎来了新一代在丰富数据上训练的大型网络。这些模型在许多NLP任务中获得了显著的准确性提高。然而，这些准确性的提高取决于特别大的计算资源的可用性，需要类似的大量能量消耗。 因此，这些模型的训练和开发成本很高，在经济上，由于硬件和电力或云计算时间的成本，在环境上，由于现代张量处理硬件所需的碳足迹。在本文中，我们通过量化训练各种最近成功的NLP神经网络模型的近似财务和环境成本，提请NLP研究人员注意这个问题。基于这些发现，我们提出了可操作的建议，以降低成本，提高NLP研究和实践的公平性。
许多基于变异自动编码器的模型被提出来以实现推理中的潜变量分解。然而，目前的工作大多集中在设计强大的分解正则器上，而初始化时潜变量表示的给定维数可能会严重影响分解。 因此，我们引入了一种修剪机制，旨在自动寻求数据的内在维度，同时促进解缠的表示。所提出的方法在MPI3D和MNIST上得到了验证，在解缠、重建和鲁棒性方面都领先于最先进的方法。代码提供在https://github.com/WeyShi/FYP-of-Disentanglement。
我们探索了字节级递归语言模型的特性。当给定足够的容量、训练数据和计算时间时，这些模型学到的表征包括与高级概念相对应的拆分特征。 它们也是非常有效的数据。当只使用少数标记的例子时，我们的方法与在完整数据集上训练的强大基线的性能相匹配。我们还证明了情感单元对模型的生成过程有直接的影响。简单地将其值固定为积极或消极，就能生成具有相应积极或消极情感的样本。
 离散潜变量模型虽然适用于各种环境，但往往难以学习。对离散潜变量进行采样会导致高变异梯度估计，主要原因有两个：1）模型内样本的分支；2）样本缺乏路径导数。 虽然目前最先进的方法对前者采用了控制变量方案，对后者采用了连续松弛方法，但由于实施和训练有效的控制变量方案的复杂性以及评估模型中许多分支路径（可能是指数级的）的必要性，它们的效用受到限制。 在这里，我们重新审视了Reweighted Wake Sleep（RWS；Bornschein和Bengio，2015）算法，并通过广泛的评估，表明它规避了这两个问题，在学习离散的潜变量模型方面优于目前最先进的方法。 此外，我们观察到，与重要性加权自动编码器不同，RWS可以随着粒子数量的增加学习到更好的模型和推理网络，而且它的优势也延伸到了连续潜变量模型。
深度极端多标签学习的目标是共同学习特征表示和分类器，以便从一个极其庞大的标签集中用最相关的标签子集自动标记数据点。不幸的是，最先进的深度极端分类器对于短文本文件来说要么不可扩展，要么不准确。本文开发了DeepXML算法，该算法通过引入一个新的架构，将头部和尾部标签的训练分开来解决这两个限制。 DeepXML通过以下方式提高准确性：(a)在头部标签上学习词嵌入，并通过新的剩余连接将其转移到数据贫乏的尾部标签上；(b)通过扩展最先进的负面子采样技术增加可用的负面训练数据量；以及(c)对预测标签集重新排序以消除原始分类器的最难负面。 因此，DeepXML可以有效地扩展到涉及数百万个标签的问题，而这些问题是最先进的深度极端分类器所无法解决的，因为它的训练速度比XML-CNN和AttentionXML快10倍以上。同时，DeepXML也被经验确定为比领先的技术在匹配搜索引擎查询和广告商投标短语方面的准确性高达19%。
在Huber的$epsilon$污染模型下的稳健估计已经成为统计学和理论计算机科学的一个重要课题。速率最优程序，如Tukey的中位数和其他基于统计深度函数的估计器，由于其计算上的不可控性，是不实际的。 在本文中，我们通过f-Learning的视角在f-GAN和各种深度函数之间建立了一种有趣的联系。与f-GAN的推导类似，我们表明这些导致速率最优稳健估计的深度函数都可以被看作是f-Learning框架中总变异距离的变异下限。 这种联系打开了使用为训练GANs开发的工具计算稳健估计器的大门。特别是，我们表明，在Huber的$epsilon$-污染模型下，使用至少有一个隐藏层的神经网络判别器的JS-GAN能够实现稳健平均估计的最小速率。有趣的是，判别器类的神经网络结构的隐藏层被证明是稳健估计的必要条件。
长短期记忆（LSTM）是最强大的序列模型之一.尽管性能很强，然而，它缺乏像状态空间模型那样漂亮的可解释性.在本文中，我们通过引入状态空间LSTM（SSL）提出了一种结合两个世界的最佳方法，它概括了早期工作 `cite{zaheer2017latent}的主题模型与LSTM的结合。 然而，与 `cite{zaheer2017latent}不同的是，我们在推理算法中不做任何因子化假设。我们提出了一种基于顺序蒙特卡洛（SMC）方法的高效采样器，直接从联合后验中提取。实验结果证实了这种SMC推理算法在各种领域的优越性和稳定性。
在这项工作中，我们旨在通过分层贝叶斯程序归纳来解决这个问题。我们提出了一种新的学习算法，可以将概念推断为短的、生成的、随机的程序，同时学习程序的全局先验以提高泛化能力，并学习识别网络以提高推理效率。 我们的算法，Wake-Sleep-Remember（WSR），结合了连续参数的梯度学习和程序上的神经引导的搜索。我们表明，WSR在两个艰难的符号领域学习引人注目的潜在程序：蜂窝自动机和高斯过程核。我们还收集并评估了一个新的数据集，Text-Concepts，用于发现自然文本数据的结构模式。
有关蛋白质功能的知识是必要的，因为它给出了生物过程的清晰图像。然而，有许多蛋白质序列被发现并被添加到数据库中，但缺乏功能注释。 在我们的工作中，我们从Swiss-Prot收集了包含40433个蛋白质的数据，这些蛋白质被分为30个家族。我们将其传递给循环神经网络（RNN）、长短期记忆（LSTM）和门控循环单元（GRU）模型，并通过在同一数据集上应用Trigram与深度神经网络和浅度神经网络进行比较。
口语术语检测（STD）是确定一个给定的单词或短语是否出现在一个给定的语段中以及出现在哪里的任务。STD的算法通常旨在使正面和负面例子的分数差距最大化。因此，它们侧重于确保出现术语的语段比没有出现术语的语段排名更高。 在本文中，我们提出了一种新的方法，通过引入一个新的校准损失函数来设置所有术语的绝对检测阈值。在训练过程中最小化这个损失函数的好处是，它不仅旨在最大化相对排名分数，而且还调整系统使用固定的阈值，从而增强系统的稳健性，使检测准确率最大化。 我们在结构化预测中使用了新的损失函数，并扩展了用于学习口语术语检测器的判别性关键词发现算法，该算法对所有术语都采用了单一的阈值。我们进一步证明了新的损失函数的有效性，在弱监督环境下将其应用于基于模板的口语术语检测的深度神经暹罗网络，同样采用了单一的固定阈值。在TIMIT、WSJ和Switchboard语料库的实验表明，当使用固定阈值时，我们的方法不仅提高了准确率，而且还获得了更高的曲线下面积（AUC）。
联合学习涉及到在远程设备上产生的大规模分布式数据分区上的联合学习。在这样的网络中，直截了当地最小化一个总的损失函数可能会对一些设备造成不成比例的优势或劣势。 在这项工作中，我们提出了q-Fair Federated Learning（q-FFL），这是一个受无线网络资源分配策略启发的新型优化目标，它鼓励在联合网络中的设备间进行更公平的精度分配。
我们提出了一种新的自动编码模型，称为配对增强的GANs。我们以对抗的方式联合训练生成器和编码器。生成器网络学习对现实对象进行采样，反过来，编码器网络也同时被训练为将真实数据分布映射到潜空间的先验。 在这里，我们训练了一个判别器来区分两种类型的对：有增强的对象和有重建的对象。我们表明，这种对抗性损失是基于内容而不是精确匹配来比较对象的。我们通过实验证明，我们的模型在MNIST、CIFAR10、CelebA等数据集上产生的样本和重建的质量与最先进的数据集有竞争力，并在CIFAR10上取得了良好的定量结果。
我们提出了一种稳健的贝叶斯深度学习算法来推断具有潜在变量的复杂后验。受Dropout（一种用于正则化和模型集合的流行工具）的启发，我们为深度神经网络（DNN）中的权重分配了稀疏的先验，以实现自动 "放弃 "并避免过度拟合。 通过随机梯度马尔科夫链蒙特卡洛（SG-MCMC）从后验分布中取样，并通过随机近似（SA）优化潜变量，目标权重的轨迹被证明收敛于以最佳潜变量为条件的真实后验分布。 这确保了对过度拟合的参数空间进行更强的正则化，并对决定性变量进行更准确的不确定性量化。来自大-小-N回归的模拟显示了这种方法在应用于具有潜变量的模型时的稳健性。此外，它在卷积神经网络（CNN）上的应用导致了在MNIST和时尚MNIST数据集上的最先进的性能，以及对对抗性攻击的改进。
感觉神经元群体的活动在时间和空间维度上都带有刺激信息。这就提出了一个问题，即如何紧凑地表示群体代码在所有这些维度上携带的所有信息。在这里，我们开发了一种分析方法，将大量视网膜神经节细胞的尖峰序列分解成一个强大的低维表示，有效地捕获它们的空间和时间信息。 特别是，我们扩展了以前使用的基于非负矩阵因式分解的单试验空间-时间张量分解，以有效地折算刺激前基线活动。在从具有强烈刺激前基线的视网膜神经节细胞记录的数据上，我们表明，在刺激引起发射率强烈变化的情况下，我们的扩展产生了刺激解码性能的提升。
我们提出了一个基于生成对抗网络（GANs）的极端学习型图像压缩框架，以比以前的方法低得多的比特率获得视觉上令人愉悦的图像。这是通过我们的学习型压缩的GAN表述与生成器/解码器相结合而实现的，后者在全分辨率图像上运行，并与一个多尺度判别器一起训练。 此外，如果原始图像的语义标签图是可用的，我们的方法可以从标签图中完全合成解码图像中不重要的区域，如街道和树木，因此只需要存储保留的区域和语义标签图。一个用户研究证实，对于低比特率，我们的方法优于最先进的方法，甚至当它们使用超过两倍的比特。
在学习排名中，人们感兴趣的是根据项目对用户的效用来优化项目列表的全局排序。流行的方法学习一个评分函数，通过优化点状、对状或列表状损失对项目进行单独评分（即不考虑列表中其他项目的背景）。 在本文中，我们提出了一个上下文感知的神经网络模型，该模型通过应用自我关注机制来学习项目得分。最后，我们通过经验证明了基于自我关注的神经架构比多层感知器基线有明显的性能提升。
我们提出了一个主动学习的算法架构，能够组织其学习过程，以便通过学习原始运动政策的序列来实现复杂任务的领域。在本文中，我们考虑的是分层组织一组相互关联的复杂结果的学习。我们引入了一个名为 "程序 "的新框架，它能够自主地发现如何结合以前学到的技能，以便学习越来越复杂的运动策略（原始运动策略的组合）。 我们的架构可以主动决定关注哪种结果和应用哪种探索策略。这些策略可以是自主探索，也可以是主动的社会指导，在这种情况下，它依赖于人类教师的专业知识，根据学习者的要求提供示范。我们在一个模拟环境中显示，我们的新架构能够解决复杂的运动策略的学习，使其策略的复杂性适应手头的任务。我们还显示，我们的 "程序 "提高了代理学习复杂任务的能力。
蒙特卡洛树搜索（MCTS）在一系列离散环境中取得了令人印象深刻的结果，如围棋、马里奥和街机游戏，但它在连续领域还没有发挥其真正的潜力。在这项工作中，我们介绍了TPO，一种基于树搜索的连续环境的策略优化方法。TPO采取混合方法进行策略优化。 在连续的行动空间中建立MCTS树，并使用非政策MCTS轨迹更新政策梯度，这都是不容易的。为了克服这些挑战，我们提出通过只从政策分布中抽取少数行动样本来限制树搜索的分支因子，并根据轨迹的平均值和标准偏差定义一个新的损失函数。 我们的方法导致了一些非直觉性的发现。 MCTS训练通常需要大量的样本和模拟。然而，我们观察到，用预先训练的策略引导树搜索，使我们能够以较低的MCTS分支因子和少量的模拟实现高质量的结果。 例如，在复杂的环境中，如Humanoid，我们实现了比基线算法2.5倍的改进。
变异自动编码器（VAE）在某些数据集上对自然图像的流形建模方面取得了成功，允许在潜伏代码空间内插值或外推时生成有意义的图像，但考虑到文本的离散性，目前还不清楚类似的能力对文本是否可行。在这项工作中，我们研究了可控表示的无监督学习对文本失败的原因。 我们发现，传统的序列VAE可以在一定程度上通过潜伏代码学习离散的表征，但是当潜伏因子被操纵时，它们往往不能正确解码，因为被操纵的代码往往落在聚集的后置潜伏空间中的洞或空缺区域，而解码网络没有被训练来处理。 作为对该解释的验证和对该问题的修正，我们建议将后验平均数限制在一个学习的概率单线上，并在这个单线上进行操作。我们提出的方法缓解了潜伏空缺问题，并在文本的可控表征的无监督学习中取得了首次成功。此外，当在一个长句子的生成过程中切换潜在因素（如主题）时，我们提出的框架往往能以一种看似自然的方式完成句子--这是一种以前的方法从未尝试过的能力。
在本文中，我们开发了一个分层网络模型，称为分层预测网络（HPNet），以了解时空记忆是如何被学习和编码的，用于预测未来的视频帧。该模型受到哺乳动物分层视觉系统中前馈、反馈和横向递归电路的启发。 该模型包含一个计算和编码连续复杂度的时空特征的前馈路径和一个将解释从高层次投射到低层次的反馈路径。在每个层次内，前馈路径和反馈路径相交于一个循环门控电路，该电路整合它们的信号以及电路的内部记忆状态以产生对传入信号的预测。 该网络通过比较传入的信号和它的预测来学习，通过最小化每个层次的预测误差来更新其内部的世界模型，以达到{预测性自我监督学习}的风格。该网络以视频帧块为单位处理数据，而不是以帧对帧为基础。 这使得它能够学习运动模式之间的关系，在基准数据集的长距离视频序列预测中产生最先进的性能。我们观察到，网络中的分层互动引入了对全球运动模式记忆的敏感性，甚至在最早级别的单元的群体表示中也是如此。最后，我们提供了神经生理学证据，显示清醒猴子的早期视觉皮层中的神经元表现出非常相似的敏感性和行为。这些发现表明，预测性自我监督学习可能是视觉皮层中表示学习的一个重要原则。 
我们引入了一种基于反事实推理的实证方法来测试由盐度图产生的假设，并表明由盐度图提出的解释往往没有得到实验的支持。我们的实验表明，盐度图最好被视为一种探索工具，而不是解释工具。
深度学习中尚未解决的问题之一是正在发现的解决方案的性质。我们研究了由相同的网络架构达成的解决方案的集合，其中有不同的权重随机初始化和随机小批次。这些解决方案被证明是相当相似的--更多的时候，每个训练和测试实例要么被所有的网络正确分类，要么根本就没有。 令人惊讶的是，所有的网络实例似乎都有相同的学习动力，即最初相同的训练和测试实例被学习模型正确识别，然后是以大致相同的顺序学习其他实例。 当把调查扩展到神经网络架构的异质集合时，人们再次看到，无论何种架构，例子都是以相同的顺序学习的，尽管更强大的架构可能会继续学习，从而获得更高的准确性。即使测试集的类别组成与训练集无关，例如，当使用样本外的自然图像或甚至人工图像时，这种结果模式仍然真实。 为了显示这些现象的稳健性，我们对我们的实证研究进行了广泛的总结，其中包括数百个描述数万个具有不同NN架构、超参数和领域的网络的图。我们还讨论了这种相似性模式被打破的情况，这表明报告的相似性不是梯度下降优化的伪命题。相反，观察到的相似性模式是用大网络学习复杂问题的特征。最后，我们表明，这种相似性模式似乎与有效概括密切相关。
我们提出了一个统一的框架，用于建立单个对象或实体（以及它们的组合）的无监督表征，通过与每个对象相关联的分布式以及点估计（矢量嵌入）来实现。 作为一个指导性的例子，我们制定了文本的无监督表征，特别是句子表征和含意检测。实证结果表明，通过所提出的框架获得了强大的优势。这种方法可以用于任何具有共同发生结构的无监督或监督问题（关于文本或其他模式），如任何序列数据。
在本文中，我们提出了两种方法，即跟踪规范回归（TNR）和稳定跟踪规范分析（StaTNA），以提高带有侧面信息的推荐系统的性能。我们的跟踪规范回归方法提取了侧面信息下的低等级潜在因素，这些因素在不同的背景下驱动用户的偏好。 此外，我们的新型推荐框架StaTNA不仅可以捕捉到用户偏好的潜在低等级共同驱动因素，而且还考虑了个别用户的特异性品味。我们在MovieLens数据集上与最先进的模型比较了TNR和StaTNA的性能，并证明StaTNA和TNR总体上优于这些方法。
这项工作提出了一个基于探索和模仿学习的代理，能够在玩基于文本的计算机游戏中获得最先进的性能。基于文本的计算机游戏通过自然语言向玩家描述他们的世界，并希望玩家使用文本与游戏互动。这些游戏是有意义的，因为它们可以被视为人工代理的语言理解、问题解决和语言生成的测试平台。使得这些游戏对学习代理特别具有挑战性的一个方面是组合性的大行动空间。现有的解决基于文本的游戏的方法仅限于非常简单的游戏，或者是行动空间被限制在一个预定的可接受的行动集合中。更具体地说，在最初的探索阶段，我们首先提取具有高奖励的轨迹，然后我们训练一个策略，通过模仿这些轨迹来解决游戏。我们的实验表明，这种方法在解决基于文本的游戏方面优于现有的解决方案，而且在与环境的互动数量方面，它的样本效率更高。此外，我们表明，学到的策略可以比现有的解决方案更好地概括到未见过的游戏，而不使用任何行动空间的限制。
Frankle和Carbin最近发表的 "彩票假说 "论文表明，创建稀疏网络的简单方法（保留大权重）导致模型可以从头开始训练，但只有从相同的初始权重开始时才可以。这些网络的性能往往超过非稀疏基础模型的性能，但原因并不十分清楚。在本文中，我们研究了彩票（LT）算法的三个关键组成部分，表明每个组成部分可以显著变化而不影响整体结果。 最后，我们发现了超级掩码的存在，或者说掩码可以应用于未经训练的、随机初始化的网络，从而产生一个性能远远优于机会的模型（在MNIST上为86%，在CIFAR-10上为41%）。
用预训练的模型进行微调，在许多语言任务中取得了优异的成绩。在这项研究中，我们专注于这样一个自我注意网络模型，即BERT，它在不同的语言理解基准中的堆叠层方面表现良好。然而，在许多下游任务中，BERT在微调时忽略了层之间的信息。此外，尽管自我注意网络以其捕捉全局依赖的能力而闻名，但在强调本地语境的重要性方面仍有改进空间。 鉴于这些优点和缺点，本文提出了SesameBERT，一种通用的微调方法：（1）通过Squeeze和Excitation在所有层中提取全局信息；（2）通过高斯模糊捕捉邻近的语境来丰富局部信息。 此外，我们在HANS数据集中证明了我们的方法的有效性，该数据集用于确定模型是否采用了浅层启发式方法，而不是学习底层概括。实验显示，SesameBERT在GLUE基准和HANS评估集方面的表现优于BERT。
越来越多的学习方法实际上是可分化的游戏，其玩家并行优化多个相互依赖的目标--从GAN和内在的好奇心到多代理RL.对手塑造是一个强大的方法，以改善这些游戏中的学习动态，考虑到玩家对他人更新的影响。 有对手学习意识的学习（LOLA）是最近的一种算法，它利用了这种反应，导致了在迭代囚徒困境等环境中的合作。虽然实验上很成功，但我们表明LOLA代理可以表现出与收敛直接相悖的 "傲慢 "行为。 事实上，很少有算法能在理论上保证适用于所有（n-player，非凸）游戏。在本文中，我们提出了稳定的对手塑造（SOS），这是一种在LOLA和一个名为LookAhead的稳定变体之间插值的新方法。我们证明LookAhead在所有可微分游戏中局部收敛到均衡点并避免严格的马鞍。
医学问题的在线提问率大大超过了合格人员回答问题的能力，导致许多问题没有得到回答或回答不充分。这些问题中有许多不是唯一的，对类似问题的可靠识别将使问题回答模式更加高效和有效。虽然许多研究工作集中在一般问题的相似性问题上，但这些方法并不能很好地推广到医学领域，在医学领域通常需要确定语义相似性。 在本文中，我们展示了在医学问题-答案对上预训练神经网络的半监督方法对于确定医学问题相似性的最终目标来说是一个特别有用的中间任务。虽然其他预训练任务在这个任务上的准确率低于78.7%，但我们的模型在相同数量的训练例子下达到了82.6%的准确率，在一个小得多的训练集上达到了80.0%的准确率，而在使用医学问题-答案数据的完整语料库时达到了84.5%。
我们研究了任务间共享表征的好处，以便在多任务强化学习中有效地使用深度神经网络。我们利用了这样一个假设：从不同的任务中学习，共享共同的属性，有助于概括它们的知识，与学习单一任务相比，导致更有效的特征提取。 我们通过提供理论上的保证来证明这一点，这些保证强调了在任务间共享表征的便利条件，将众所周知的近似值迭代的有限时间界限扩展到多任务设置中。此外，我们通过提出三种强化学习算法的多任务扩展来补充我们的分析，我们在广泛使用的强化学习基准上进行了经验评估，在采样效率和性能方面比单任务对应的算法有了明显的改善。
我们提出了一个用于处理点云的三维胶囊架构，该架构对于无序输入集的SO(3)旋转组、平移和置换是等价的。该网络在一个稀疏的局部参考框架集上运行，该框架由输入点云计算而成，并通过一个新颖的三维四元组胶囊层建立了端到端的等价性，包括一个等价的动态路由程序。胶囊层使我们能够将几何与姿势分开，为更丰富的描述和结构化的潜在空间铺路。 在这个过程中，我们从理论上将胶囊之间的动态路由过程与著名的Weiszfeld算法联系起来，这是一种解决迭代重加权最小二乘法（IRLS）问题的方案，具有可证明的收敛特性，能够在胶囊层之间进行稳健的姿势估计。由于稀疏的等变四元数胶囊，我们的架构允许联合物体分类和方向估计，我们在常见基准数据集上进行了经验验证。
矢量语义，特别是句子矢量，最近在自然语言处理的许多领域得到了成功的应用。然而，相对较少的工作探索了句子矢量空间的内部结构和属性。在本文中，我们将通过研究一个特殊的现实世界的应用来探索句子矢量的属性。特别是，我们表明，句子向量和文档向量之间的余弦相似度与句子的重要性密切相关，而且向量语义学可以识别并纠正迄今为止所选择的句子和文档之间的差距。此外，我们还确定了与有效总结相关的特定维度。据我们所知，这是第一次将句子嵌入的特定维度与句子属性联系起来。
我们提出了价值传播（VProp），这是一个建立在价值迭代基础上的参数有效的可微分规划模块，它可以成功地以强化学习的方式训练来解决未见过的任务，有能力泛化到更大的地图尺寸，并且可以学习在动态环境中导航。 我们在MazeBase网格世界的配置上进行了评估，随机生成了几种不同大小的环境。此外，我们表明，当环境中还包括随机元素时，该模块能够学习规划，为各种交互式导航问题提供了一个具有成本效益的学习系统，以建立低水平的大小不变的规划器。
推荐是机器学习的一个普遍应用，影响到许多用户；因此，推荐模型的准确性和可解释性是至关重要的。在这项工作中，我们提出了一种方法来解释和增强黑盒推荐系统的预测。特别是，我们提出从源推荐模型中提取特征交互解释，并在目标推荐模型中明确编码这些交互，其中源和目标模型都是黑盒的。通过不假设推荐系统的结构，我们的方法可以用于一般环境。 在我们的实验中，我们专注于机器学习推荐的一个突出用途：广告点击预测。我们发现，我们的互动解释既是信息性的，也是预测性的，也就是说，明显优于现有的推荐模型。更重要的是，解释互动的相同方法甚至可以为推荐以外的领域提供新的洞察力。
在本文中，我们考虑了在存在非线性（由ReLU函数建模）的情况下学习生成模型的问题。给定一组信号向量$\mathbf{y}^i\in \mathbb{R}^d, i =1, 2, \dots , n$，我们旨在学习网络参数，即。在$mathbf{y}^i = mathrm{ReLU}(A\mathbf{c}^i +\mathbf{b})$的模型下，我们旨在学习网络参数，即$d\times k$矩阵$A$。其中$mathbf{b}\in \mathbb{R}^d$是一个随机偏置向量，{$mathbf{c}^i \in \mathbb{R}^k$是任意的未知潜在向量}。 我们表明，在$$mathbf{b}$分布的某些条件下，有可能在$O(d)$的误差范围内恢复$A$的列空间（在Frobenius规范中）。
我们研究了在有明确标签的监督学习和没有标签的无监督学习中，当数据集中存在多种特征类型时，如何估计密集的表示。 在有监督的情况下，我们表明我们的方法比最近提出的方法更有优势；例如能够实现更高的预测精度，并提供一种避免冷启动问题的方法。在无监督的情况下，我们的实验表明Feat2Vec明显优于现有的不利用数据结构的算法。
我们研究了一个循环神经网络（RNN）在学习识别常规形式语言时使用的内部表征。具体来说，我们在常规语言的正反例子上训练RNN，并询问是否存在一个简单的解码函数，将这个RNN的状态映射到语言的最小确定性有限自动机（MDFA）的状态。 我们的实验表明，这样的解码函数确实存在，而且它不是将RNN的状态映射到MDFA的状态，而是映射到一个{em abstraction}的状态，这个{em abstraction}是通过将MDFA状态的小集聚成 "超级状态 "而获得的。
为大量的硬件设计准确有效的卷积神经架构是具有挑战性的，因为硬件设计是复杂多样的。本文解决了神经架构搜索（NAS）中的硬件多样性挑战。与以前的方法不同的是，我们提出了HURRICANE，在一个更大的搜索空间和坐标上升框架中的多步骤搜索方案上探索自动的硬件感知搜索，为不同类型的硬件生成定制模型。 在ImageNet上的大量实验表明，我们的算法在三种类型的硬件上持续实现了更低的推理延迟，并具有与最先进的NAS方法相似或更好的准确性。值得注意的是，HURRICANE在ImageNet上实现了76.63%的顶级准确率，而DSP的推理延迟仅为16.5毫秒，这比FB的准确率高3.4%，推理速度快6. 对于VPU，HURRICANE实现了比Proxyless-mobile高0.53%的最高1级准确率，速度提高了1.49倍。
在本文中，我们提出了基于神经短语的机器翻译（NPMT）。我们的方法使用最近提出的基于分割的序列建模方法Sleep-WAke网络（SWAN）对输出序列中的短语结构进行了明确建模。为了减轻SWAN的单调对齐要求，我们引入了一个新的层来执行输入序列的（软）局部重新排序。 我们的实验表明，与强大的NMT基线相比，NPMT在IWSLT 2014德语-英语/英语-德语和IWSLT 2015英语-越南语机器翻译任务中取得了卓越的性能。我们还观察到，我们的方法在输出语言中产生了有意义的短语。
生成对抗网络（GANs）在复杂流形上的分布建模方面显示出令人印象深刻的结果，例如自然图像的分布。然而，GANs经常受到模式崩溃的影响，这意味着它们很容易只描述数据分布的单一或少数模式。 我们首先引入了潜伏分布匹配（LDM）约束，通过将生成的样本分布与潜伏空间中的真实样本分布相一致来规范生成器。为了利用这种潜伏空间，我们提出了一个规范化的自动编码器（AE），将数据分布映射到编码空间的先验分布。
实现更快的执行和更短的编译时间可以促进神经网络的进一步多样性和创新。然而，目前执行神经网络的范式要么依赖于手工优化的库、传统的编译启发式方法，要么是最近的遗传算法和其他随机方法。这些方法受到频繁的昂贵的硬件测量的影响，使得它们不仅过于耗时，而且是次优的。因此，我们设计了一个解决方案，可以学习快速适应先前未见的设计空间的代码优化，既加速搜索又提高输出性能。 这个被称为CHAMELEON的解决方案利用了强化学习，其解决方案需要更少的步骤来收敛，并开发了一种自适应采样算法，不仅关注代表性点上的昂贵样本（真实的硬件测量），而且还使用领域知识启发的逻辑来改善样本本身。用真实硬件进行的实验表明，CHAMELEON比AutoTVM提供了4.45倍的优化时间，同时还将现代深度网络的推理时间提高了5.6%。
在本文中，我们提出了一个用于未来预测的可微分对抗语法模型。目的是用可微分函数和潜伏表征来模拟形式语法，这样就可以通过标准的反向传播来学习它们。 学习用潜在终端、非终端和生产规则表示的形式化语法，可以从数据中捕捉到具有多种可能性的序列结构。 我们在两个不同的任务上证实了对抗性语法的好处：未来的三维人体姿势预测和未来的活动预测。对于所有的设置，所提出的对抗性语法优于最先进的方法，能够比以前的工作更准确地预测未来，而且预测得更远。
我们考虑对序列的互变不变量函数（或集合函数）进行简单的总体表示。我们的方法，我们称之为Janossy pooling，将互变不变量函数表示为应用于输入序列的所有重新排序的互变敏感函数的平均值。这使我们能够利用关于互变敏感函数的丰富和成熟的文献来构建新颖和灵活的互变不变量函数。 我们的框架统一了文献中的各种现有工作，并提出了可能的建模和算法扩展。我们在实验中探索了一些，这些实验证明了比目前最先进的方法更好的性能。
虽然在现实环境中，任务的实例和类别的数量可能会有所不同，但现有的用于少量分类的元学习方法假设每个任务和类别的实例数量是固定的。由于这种限制，他们学习在所有的任务中平等地利用元知识，即使每个任务和类别的实例数量大不相同。 此外，他们没有考虑未见过的任务的分布差异，在这些任务中，元知识的作用可能较小，这取决于任务的关联性。为了克服这些限制，我们提出了一个新的元学习模型，它可以自适应地平衡每个任务中元学习和特定任务学习的效果。 通过对平衡变量的学习，我们可以决定是依靠元知识还是特定任务的学习来获得解决方案。我们将这一目标制定为贝叶斯推理框架，并使用变异推理来解决这一问题。我们在两个现实的任务和类别不平衡的数据集上验证了我们的贝叶斯任务自适应元学习（Bayesian TAML），在这些数据集上它明显优于现有的元学习方法。进一步消融研究证实了每个平衡组件和贝叶斯学习框架的有效性。
人工智能中的许多任务需要多个代理的协作。我们研究了多代理领域的深度强化学习。最近的研究工作经常采取两种看似冲突的观点，一种是分散的观点，每个代理都应该有自己的控制器；另一种是集中的观点，人们假设有一个更大的模型控制所有的代理。在这方面，我们重新审视了主从结构的想法，在一个框架内纳入了这两种观点。 这种分层结构自然地利用了彼此的优势。结合两种观点的想法是直观的，可以从许多现实世界的系统中得到很好的激励，然而，在各种可能的实现中，我们强调了三个关键因素，即组成行动表示、可学习的通信和独立推理。通过网络设计来明确促进这些，我们的建议在合成实验和应用于挑战性的《星际争霸》微管理任务时都一直优于最新的竞争方法。
我们研究了梯度下降方法在解决线性可分离数据集上的二元分类问题时的隐性偏差。分类器由非线性ReLU模型描述，目标函数采用指数损失函数。我们首先描述了损失函数的景观，并表明除了渐进全局最小值外，还可能存在虚假的渐进局部最小值。 然后，我们表明梯度下降（GD）可以收敛到全局或局部最大边际方向，或者在一般情况下可能偏离期望的最大边际方向。对于随机梯度下降（SGD），我们表明，如果SGD收敛，它在期望中收敛到全局或局部最大边际方向。 我们进一步探讨了这些算法在某些静止条件下学习多神经元网络的隐性偏向，并表明在ReLU激活下，学习的分类器能使每个样本模式分区的边际最大化。
模型修剪已经成为一种有用的技术，它提高了深度学习的计算效率，使得在资源有限的情况下部署解决方案成为可能。相关工作中广泛使用的一种做法是假设一个较小规范的参数或特征在推理时发挥的信息量较小。 在本文中，我们提出了一种加速深度卷积神经网络（CNN）计算的通道修剪技术，该技术并不严重依赖这一假设。相反，它侧重于直接简化CNN的通道到通道的计算图，而不需要执行计算上的困难和并不总是有用的任务，使CNN的高维向量结构稀疏。 我们的方法分为两个阶段：首先采用端到端的随机训练方法，最终迫使一些通道的输出为常数，然后通过调整其影响层的偏置，从原始神经网络中修剪这些常数通道，从而使产生的紧凑模型可以快速微调。
随机梯度下降法（SGD）由于其许多理想的特性，一直是训练深度神经网络的主要优化方法。SGD的一个更显著和最不被理解的品质是，即使神经网络有数百万个参数，它也能在未见过的数据上有相对好的概括。 我们假设，在某些情况下，最好放宽其固有的泛化特性，并引入SGD的扩展，称为深度梯度提升（DGB）。DGB的关键思想是，使用链式规则推断出的反向传播梯度可以被视为梯度提升问题的伪残余目标。 因此，在神经网络的每一层，权重更新是通过使用线性基础学习器解决相应的提升问题来计算的。当作为一个单独的输入归一化层（INN）来实现时，与没有归一化层的相同架构相比，新架构在图像识别任务上显示出更好的性能。
我们建议扩展现有的深度强化学习（Deep RL）算法，允许它们额外选择行动序列作为其策略的一部分。 这种修改迫使网络预测行动序列的奖励，正如我们所显示的，这改善了探索，导致更好的收敛。我们的建议简单、灵活，可以很容易地纳入任何深度RL框架。
要优化一个神经网络，人们通常会想到优化其参数，但最终是优化将输入映射到输出的函数。由于参数的变化可能作为函数变化的一个糟糕的代理，因此，人们对参数的首要地位给予了关注，但对应关系并没有得到测试。 在这里，我们表明在$L^2$希尔伯特空间中计算函数之间的距离是简单和可行的。我们研究了典型的网络在这个空间中的表现，并比较了参数$ell^2$距离与优化轨迹各点之间的函数$L^2$距离的比较。 特别是，在整个优化过程中，$L^2/ell^2$的比率会下降，在测试误差达到峰值时达到一个稳定值。我们随后研究了如何将$L^2$距离直接应用于优化。我们首先提出，在多任务学习中，可以通过直接限制任务间输入/输出函数的变化程度来避免灾难性的遗忘。 其次，我们提出了一个新的学习规则，该规则限制了网络在任何一次更新中可以通过$L^2$空间的距离。这使得新的例子可以以最小的方式被学习，并对之前已经学习过的东西产生干扰。这些应用证明了人们如何能够直接测量和规范化函数距离，而不依赖参数或像损失曲率这样的局部近似。
信息瓶颈（IB）问题解决的是为预测Y的任务获得一些随机变量X的相关压缩表示T的问题。它被定义为一个受限的优化问题，该问题使表示关于任务的信息最大化，I(T;Y)，同时确保达到最小的压缩水平r（即I(X;T)<=r）。由于实际原因，该问题通常通过最大化IB拉格朗日乘数的许多值来解决，因此绘制IB曲线（即。对于给定的I(X;Y)，最大I(T;Y)的曲线），并选择所需的可预测性和压缩性的表示。已知当Y是X的确定性函数时，IB曲线不能被探索，其他拉格朗日被提出来解决这个问题（例如。在本文中，我们(i)提出了一个通用的拉格朗日系列，允许在所有情况下探索IB曲线；(ii)证明如果使用这些拉格朗日，对于已知的IB曲线形状，在拉格朗日乘数和期望的压缩率r之间存在一个一对一的映射，因此，摆脱了为许多拉格朗日乘数值解决优化问题的负担。
我们提出了神经逻辑机（NLM），一个用于归纳学习和逻辑推理的神经符号架构。NLM利用了神经网络的力量--作为函数近似器，以及逻辑编程--作为具有属性、关系、逻辑连接词和量词的对象的符号处理器。 在对小规模任务（如短阵列的排序）进行训练后，NLMs可以恢复被解除的规则，并泛化到大规模任务（如较长阵列的排序）。在我们的实验中，NLMs在一些任务中实现了完美的泛化，从家族树和一般图的关系推理任务，到决策任务，包括阵列排序，寻找最短路径，以及玩积木世界。
序列到序列（seq2seq）的神经模型已被积极研究用于抽象化总结。然而，现有的神经抽象化系统经常产生事实上不正确的总结，并容易受到对抗性信息的影响，这表明关键是缺乏语义理解。在本文中，我们提出了一种新型的语义感知的神经抽象化总结模型，它通过对突出内容的语义解释来学习产生高质量的总结。 我们引入了一种带有对抗性样本的新型评估方案来衡量一个模型识别离题信息的程度，其中我们的模型产生的性能明显优于流行的指针式生成器总结器。人类的评估也证实了我们的系统总结一致地比seq2seq模型更有信息量和忠实度，也更少冗余。
最近，使用二维词汇嵌入的超级字符方法在文本分类任务中取得了最先进的成果，展示了这种新方法的前景。本文借用超级字符方法和二维嵌入的理念，提出了一种为开放领域对话生成会话响应的方法。在公共数据集上的实验结果表明，所提出的SuperChat方法生成了高质量的响应。一个互动演示已经准备好在研讨会上展示。
卷积神经网络（CNN）被普遍认为是计算机视觉进步的驱动力之一。尽管CNN在许多任务上有很好的表现，但在实现理想的机器智能的道路上仍然面临着重大障碍。一个是CNN很复杂，很难解释。 另一个是标准的CNN需要大量的注释数据，而这些数据有时是很难获得的，最好是能够从少数例子中学习它们。在这项工作中，我们通过开发新的、简单的和可解释的模型来解决CNN的这些限制，以实现少数的学习。我们的模型是基于用视觉概念对物体进行编码的想法，这些概念是由CNN中的特征向量所代表的可解释的视觉线索。我们首先将视觉概念的学习适应于少数照片的设置，然后发现了使用视觉概念进行特征编码的两个关键属性，我们称之为类别敏感性和空间模式。 实验表明，我们的模型取得了有竞争力的性能，同时比其他最先进的几率学习方法更加灵活和可解释。我们得出结论，使用视觉概念有助于暴露CNN在几率学习方面的自然能力。
最近，各种神经网络被提出来用于不规则结构的数据，如图和流形。据我们所知，所有现有的图网络都是离散深度的。受用于欧几里得领域数据的神经常微分方程（NODE）的启发，我们将连续深度模型的想法扩展到图数据，并提出图常微分方程（GODE）。(1)用邻接法进行间接反向传播；(2)通过ODE求解器进行直接反向传播，准确计算梯度。 我们证明了GODE可以很容易地适应不同的现有图神经网络并提高准确性。我们验证了GODE在半监督节点分类任务和图分类任务中的性能。我们的GODE模型在时间、内存效率、精确梯度估计和不同图网络的通用性方面实现了连续模型。
无监督的图像到图像的翻译是最近提出的一项任务，即在训练时只给定未配对的图像例子，将图像翻译成不同的风格或领域。在本文中，我们提出了一个新的任务，即无监督的视频到视频的翻译，这带来了它自己独特的挑战。 翻译视频意味着不仅要学习物体和场景的外观，还要学习现实的运动和连续帧之间的转换。我们研究了使用现有的图像到图像翻译网络进行每帧视频到视频翻译的性能，并提出了一个空间-时间的三维翻译器作为这个问题的替代解决方案。 我们在多个合成数据集上评估了我们的三维方法，如移动的彩色数字，以及现实的分割到视频的GTA数据集和新的CT到MRI的体积图像翻译数据集。我们的结果表明，按帧翻译在单帧水平上产生了现实的结果，但与我们的三维翻译方法相比，在整个视频的规模上表现明显不足，后者能够更好地学习视频的复杂结构和运动以及物体外观的连续性。
胶囊网络受制于其层的参数昂贵的性质，以及普遍缺乏可证明的等值保证。我们提出了一种胶囊网络的变体，旨在补救这一问题。我们发现，学习连续层的胶囊之间的所有成对的部分-整体关系是低效的。 此外，我们还意识到，预测网络和路由机制的选择都是等值的关键。基于这些，我们提出了一个胶囊网络的替代框架，学习对每一层的每个胶囊类型的姿势变化流形进行投影编码，称为变化空间（SOV）。 因此，路由的预测阶段包括使用相应的函数投射到更深的胶囊的SOV中。作为这个想法的一个具体实例，同时也是为了获得更多的参数共享的好处，我们在这个阶段使用较浅的胶囊的类型均一的组等价卷积。我们还引入了一个基于度中心的等价路由机制。 我们表明，我们一般模型的这个特定实例是等价的，因此在变换下保留了输入的组成表示。我们在标准物体分类数据集上进行了几个实验，展示了我们的模型对几个胶囊基线的变换鲁棒性以及一般性能的提高。
最近，深度神经网络已经显示出它们记忆训练数据的能力，即使是有噪声的标签，也会损害泛化性能。为了缓解这个问题，我们提出了一个简单而有效的方法，对噪声标签具有鲁棒性，即使有严重的噪声。 我们的目标涉及到一个方差正则化项，它隐含地惩罚了整个训练集（包括有噪声标签的数据）上的神经网络的雅各布准则，这鼓励了泛化并防止了对损坏的标签的过度拟合。在有噪声的基准上的实验表明，我们的方法实现了最先进的性能，对严重的噪声有很高的容忍度。
最近的研究表明，在WMT汉英新闻翻译任务中，神经机器翻译达到了与专业人工翻译相当的水平。我们用其他评估协议对这一说法进行了实证测试，对单句和整个文档的评估进行了对比。 在一个成对的排名实验中，与孤立的句子相比，评估适当性和流畅性的人类评分者在评估文件时显示出对人类翻译的更大偏好。我们的发现强调，随着机器翻译的改进，在句子层面难以或不可能发现的错误在区分不同翻译产出的质量方面成为决定性因素，需要转向文件层面的评价。
模仿学习的目的是从专家的示范中反过来学习一种策略，这在文献中已被广泛研究，包括用马尔科夫决策过程（MDP）模型的单代理设置和用马尔科夫游戏（MG）模型的多代理设置。然而，现有的一般多代理马尔科夫游戏的方法并不适用于多代理广泛马尔科夫游戏，其中代理按照一定顺序做出异步决策，而不是同时决策。 我们提出了一个新的框架，用于一般广泛马尔可夫博弈背景下的异步多代理生成性对抗性模仿学习（AMAGAIL），学习到的专家政策被证明可以保证亚博弈完全均衡（SPE），这是一个比纳什均衡（NE）更普遍、更强大的均衡。实验结果证明，与最先进的基线相比，我们的AMAGAIL模型可以利用从异步决策场景（即。广泛的马尔科夫游戏）。
自我训练是最早和最简单的半监督方法之一。关键的想法是用与模型预测配对的未标记数据来增加原始标记的数据集。自我训练在分类问题上已经得到了充分研究。 然而，在复杂的序列生成任务中，如机器翻译，由于目标空间的构成性，自我训练如何发挥作用仍不清楚。在这项工作中，我们首先表明，在序列生成中应用自我训练不仅是可能的，而且是值得推荐的。通过对性能增益的仔细检查，我们发现，在隐藏状态上添加的噪声（如dropout）是关键。 为了进一步鼓励这一机制，我们建议向输入空间注入噪声，从而形成一个 "噪声 "版本的自我训练。在不同的资源设置下，对机器翻译和文本总结任务的标准基准进行的经验研究表明，噪声自我训练能够有效地利用未标记的数据，并以较大的幅度提高基线性能。
我们提出了一个端到端的训练记忆系统，它能快速适应新的数据并生成类似的样本。受Kanerva的稀疏分布式记忆的启发，它有一个强大的分布式读写机制。该记忆是可分析的，它能通过贝叶斯更新规则实现最佳的在线压缩。 因此，自上而下的记忆和自下而上的感知相结合，产生了代表观察的代码。从经验上看，我们证明了自适应记忆明显改善了在Omniglot和CIFAR数据集上训练的生成模型。与可分化神经计算机（DNC）及其变体相比，我们的记忆模型具有更大的容量，并且明显更容易训练。
在现有的方法中，修剪是在迭代优化过程中进行的，要么是启发式设计的修剪时间表，要么是额外的超参数，这削弱了它们的效用。在这项工作中，我们提出了一种新的方法，在训练前的初始化阶段对给定的网络进行一次修剪。 我们的方法获得了极其稀疏的网络，在MNIST、CIFAR-10和Tiny-ImageNet分类任务上与参考网络的准确性几乎相同，并且广泛适用于各种架构，包括卷积、残差和递归网络。与现有的方法不同，我们的方法使我们能够证明保留的连接确实与特定任务有关。
转移学习使用来自源模型的训练权重作为训练目标数据集的初始权重。 一个精心选择的具有大量标记数据的源，会使准确性得到显著提高。 我们展示了一种技术，该技术可以自动标记大量未标记的数据集，以便它们可以训练转移学习的源模型。 我们表明，这些自动训练的模型的性能平均在基线的17%以内。
在本文中，我们提出了一个简单的和高度参数化的高效模块，名为树状结构注意模块（TAM），它递归地鼓励相邻的通道进行合作，以产生一个空间注意图作为输出。 我们在CIFAR-10/100和SVHN数据集上进行了大量的实验，实证验证了我们模块的有效性。在采用我们提出的注意力模块后，ResNet50和ResNet101模型分别获得了2.3%和1.2%的精度提升，而参数超标率不到1.5%。
强化学习在实际应用中面临的一个重大挑战是需要指定一个能正确定义任务的神谕奖励函数。 虽然很有吸引力，但要收集涵盖现实世界中常见的变化（例如，打开任何类型的门）的演示数据集是不切实际的。因此，在实践中，IRL通常必须在只有有限的演示集的情况下进行，在那里明确地恢复一个奖励函数是非常困难的。 在这项工作中，我们利用了这样一种观点，即来自其他任务的演示可以用来限制可能的奖励函数集，即学习一种 "先验"，这种先验是专门为从有限数量的演示中推断富有表现力的奖励函数的能力而优化。 我们证明了我们的方法可以有效地从图像中恢复新任务的奖励，并提供了关于我们的方法如何类似于学习先验的直觉。
最近的工作集中在结合核方法和深度学习上。考虑到这一点，我们介绍了Deepström网络--一种新的神经网络架构，我们用Nyström近似来取代标准卷积架构的顶部密集层，并对核函数进行近似。我们的方法很容易高度灵活。它与任何核函数兼容，并允许利用多个核。我们表明Deepström网络在标准数据集如SVHN和CIFAR100上达到了最先进的性能。该方法的一个好处是它的可学习参数数量有限，这使得它特别适合于小规模的训练集，例如每类5到20个样本。最后我们说明了使用多核的两种方法，包括多Deepström设置，利用模型卷积部分输出的每个特征图上的核。   
这篇短文的主要目的是告知广大神经艺术界，使用在imagenet数据集上训练的模型，或使用来自445-n02892767-['比基尼，两件式']和459-n02837789-['胸罩，胸罩，绷带']类的种子图像的伦理后果。 我们发现，许多属于这些类别的图片都是可证实的色情图片，是在非自愿的情况下拍摄的，有偷窥癖，而且还涉及到未成年的裸体。与 "文本"（ivory carving-illegal poaching）和 "文本"（diamond jewelry art-blood diamond）的关系一样，我们认为这里也有类似的道德难题，并希望在社区的神经艺术家中发起一场对话。
归纳和无监督的图形学习是预测或信息检索任务的关键技术，在这些任务中，标签信息很难获得。同时使图形学习归纳和无监督也是一种挑战，因为由基于重建误差的损失函数指导的学习过程不可避免地要求图形相似性评估，这通常是难以计算的。在本文中，我们提出一个通用框架SEED（采样、编码和嵌入分布），用于图形结构对象的归纳和无监督的表示学习。 SEED框架不是直接处理图的相似性评估所带来的计算挑战，而是给定一个输入图，对一些子图进行采样，这些子图的重建误差可以被有效评估，将子图样本编码为子图向量的集合，并采用子图向量分布的嵌入作为输入图的输出向量表示。 通过理论分析，我们证明了SEED和图的同构性之间的密切联系。使用公共基准数据集，我们的实证研究表明，与有竞争力的基线方法相比，拟议的SEED框架能够实现高达10%的改进。
神经群体对感官刺激的反应可以表现出非线性的刺激依赖性和丰富的结构共享变异性。这里，我们展示了对抗性训练如何被用来优化神经编码模型，以捕捉神经群体数据的确定性和随机性成分。 为了说明神经尖峰列车的离散性，我们使用REBAR方法来估计无偏梯度，以便对神经编码模型进行对抗性优化。我们在初级视觉皮层的群体记录上说明了我们的方法。我们表明，在卷积神经网络中添加潜在的噪声源，可以产生一个模型，同时捕获群体活动的刺激相关性和噪声相关性。
本文提出了一个基于弱监督学习的聚类框架。作为这个框架的核心，我们引入了一个新的多实例学习任务，该任务基于一个称为独特类计数（ucc）的包级标签，即包内所有实例中独特类的数量。在这个任务中，在模型的训练中不需要对包内的单个实例进行注释。 我们构建了一个基于神经网络的ucc分类器，并通过实验证明，我们的框架与我们的弱监督ucc分类器的聚类性能与所有实例的标签都已知的完全监督学习模型相当。此外，我们已经测试了我们的框架在现实世界中对组织学淋巴结切片中的乳腺癌转移的语义分割任务的适用性，并表明我们的弱监督框架的性能与完全监督的Unet模型的性能相当。
在深度学习的最新进展的帮助下，无模型强化学习（RL）方法在越来越多的任务中获得了成功。 然而，它们往往受到高样本复杂性的影响，这阻碍了它们在现实世界领域的应用。 另外，基于模型的强化学习有望降低样本复杂度，但往往需要仔细调整，迄今为止主要在限制性领域取得成功，在这些领域中，简单的模型就足以进行学习。在本文中，我们分析了当深度神经网络被用来学习模型和策略时，基于模型的强化学习方法的行为，并表明学习的策略往往会利用模型学习数据不足的区域，造成训练的不稳定。 为了克服这个问题，我们建议使用模型集合来保持模型的不确定性，并使学习过程正规化。我们进一步表明，使用似然比导数产生的学习比通过时间的反向传播要稳定得多。总之，我们的方法模型-集合信任-区域政策优化（ME-TRPO）与无模型的深度RL方法相比，在具有挑战性的连续控制基准任务上大大降低了样本复杂性。
神经网络的高计算和参数复杂性使得它们的训练非常缓慢，难以部署在能源和存储限制的计算系统上。我们描述了一种神经网络训练的精度分配方法，其中所有的网络参数，即前馈路径中的激活和权重，反馈路径中的梯度和权重累加器，都被分配到接近最小的精度。精度分配是通过分析得出的，并且能够跟踪全精度训练的收敛行为，已知是先验收敛的。 因此，我们的工作导致了一种系统的方法来确定适合定点训练的精度。在CIFAR-10、CIFAR-100和SVHN数据集上的四个网络中，所产生的精度分配的近似最优性（最小）得到了经验验证。
机器学习（ML）研究已经调查了原型：代表要学习的行为的例子。我们系统地评估了五种识别原型的方法，包括以前介绍的方法和我们提出的新方法，发现所有这些方法都提供了有意义但不同的解释。通过人类研究，我们确认所有五个指标都与人类直觉很匹配。 检查指标不一致的情况，为学习中使用的数据和算法的属性提供了一个信息视角，对数据库的构建、效率、对抗性稳健性、可解释性和其他ML方面都有影响。特别是，我们确认 "在困难上训练 "的课程方法可以提高许多数据集和任务的准确性，但当有许多错误标记或模糊的例子时，它严格来说更糟糕。
在这项工作中，我们提出了稀疏深度散射克罗伊塞网络（SDSN），这是一个基于深度散射网络（DSN）的新架构。DSN是通过级联小波变换卷积与复杂模数和时间不变的算子来实现的。我们首先通过跨越多个小波族变换来增加特征多样性，同时避免任何学习，从而扩展这项工作。 因此，我们提供了一个信息量更大的潜在表征，并受益于过去几十年来高度专业化的小波滤波器的发展。此外，通过结合所有不同的小波表征，我们减少了对手头信号的先验信息量。 其次，我们为过度完整的滤波器组开发了一个最佳阈值策略，使网络正规化并控制不稳定性，如信号中固有的非平稳噪声。我们系统化和原则性的解决方案通过作为区分活动和噪声的局部掩码，使网络的潜在表示变得稀疏。
我们提出了一个神经聚类模型，该模型可以共同学习潜在特征和它们的聚类方式。与类似的方法不同，我们的模型不需要预定的聚类数量。使用监督方法，我们将潜在特征聚集到同一空间内随机抽样的目标，同时逐步去除目标，直到我们只剩下代表聚类中心的目标。 为了显示我们的模型在不同模式下的行为，我们将我们的模型应用于文本和图像数据，并在MNIST上取得了非常有竞争力的结果。最后，我们还提供了针对时尚-MNIST、20个新闻组数据集和我们自己创建的Twitter数据集的基线模型的结果。
最近关于决策问题的解释生成的工作将解释过程视为模型调和的过程，其中人工智能代理将人类的心理模型（其能力、信仰和目标）带到关于手头任务的同一页面。这种表述简洁地捕捉了许多可能的解释类型，并明确地解决了社会科学中人与人之间互动的解释的各种属性--例如社会方面、对比性和选择性。 然而，事实证明，同一过程可以被劫持为产生 "替代性解释"--即不真实但仍然满足适当解释的所有属性的解释。在以前的工作中，我们已经研究了这种解释如何被循环中的人类所感知，并暗示了产生这些解释的一种可能方式。
我们考虑了优化算法的新变体。我们的算法是基于这样的观察：连续迭代中的随机梯度的小批量不会发生剧烈变化，因此可能是可预测的。受在线学习文献中类似设置的启发，我们通过利用梯度的可预测性，为AMSGrad和Adam分别提出了两种新的乐观算法。 新算法结合了动量法、自适应梯度法和优化在线学习中的算法的思想，从而在实践中加快了训练深度神经网络的速度。
在序列建模任务中使用循环神经网络（RNNs）有希望提供高质量的结果，但由于RNNs的内存约束执行模式，要满足严格的延迟要求是很有挑战性的。我们提出了一个大-小双模块推理，动态地跳过不必要的内存访问和计算来加速RNN推理。 利用RNN中使用的非线性激活函数的抗错特性，我们提出使用一个轻量级的小模块，近似于原始的RNN层，这被称为大模块，来计算抗错性更强的不敏感区域的激活值。 大模块昂贵的内存访问和计算可以减少，因为其结果只用于敏感区域。我们的方法可以将整个内存访问平均减少40%，在基于CPU的服务器平台上实现1.54倍到1.75倍的速度提升，对模型质量的影响可以忽略不计。
细粒度实体识别（FgER）是将实体提及的内容检测和分类到一个大的类型集上的任务，这个类型集横跨不同的领域，如生物医学、金融和体育。  我们观察到，当类型集横跨多个领域时，实体提及的检测成为监督学习模型的一个限制。 主要原因是缺乏对实体边界进行适当注释的数据集，而这些数据集涵盖了大量的实体类型。 我们的工作直接解决了这个问题。 我们提出了启发式远程监督（HAnDS）框架来自动构建适合FgER任务的高质量数据集。 HAnDS框架以流水线的方式利用了维基百科和Freebase之间的高度相互联系，减少了因天真地使用远距离监督方法而引入的注释错误。 使用HAnDS框架，我们创建了两个数据集，一个适用于建立FgER系统，根据FIGER类型层次结构识别多达118种实体类型，另一个适用于基于TypeNet层次结构的多达1115种实体类型。 我们广泛的经验性实验证明了生成的数据集的质量。 与此同时，我们还提供了一个手动注释的数据集，用于对FgER系统进行基准测试。
实现正确的方法调用是软件开发者的一项重要任务。然而，这是一项具有挑战性的工作，因为方法调用的结构可能很复杂。在本文中，我们提出了InvocMap，一个代码完成工具，允许开发者从代码上下文的方法名称列表中获得多个方法调用的实现。 为了实现这一目标，我们通过四个抽象层次来分析方法调用。我们建立了一个机器翻译引擎来学习多个方法调用的第一层次到第三层次的抽象映射，这只需要开发人员从生成的表达式中手动添加局部变量来获得最终的代码。我们在六个流行的库上评估了我们提出的方法。通过从1000个Java Github项目中提取的286万个方法调用的训练语料和从120个在线论坛代码片段中提取的测试语料，InvocMap在F1-评分中取得了高达84的准确率，这取决于与方法名称一起提供的上下文信息的多少，这表明其在自动代码完成方面的潜力。
神经网络中的对抗者自其首次亮相以来就引起了广泛关注。虽然大多数现有的方法旨在欺骗图像分类模型，使其陷入错误分类，或在物体检测任务中为特定的物体实例设计攻击，但我们专注于创建通用的对手来欺骗物体检测器，并从检测器中隐藏物体。我们研究的敌手在三个方面是通用的。(1) 它们不针对特定的物体实例；(2) 它们与图像无关；(3) 它们可以进一步转移到不同的未知模型。为了实现这一点，我们提出了两种新的技术来提高对抗者的可转移性。\纹理{piling-up}和纹理{monochromatization}。事实证明，这两种技术都简化了生成的对抗者的模式，并最终导致了更高的可转移性。
这项工作提出了Poincaré Wasserstein自动编码器，这是最近提出的Wasserstein自动编码器框架在非欧几里得模子上的重新表述，即双曲空间H n的Poincaré球模型。通过假设latent空间是双曲的，我们可以利用其固有的层次结构来对学习的latent空间表征施加结构。 我们表明，对于具有latenthierarchies的数据集，我们可以在一个低维的潜空间中恢复结构。我们还在视觉领域展示了这个模型，以分析它的一些特性，并在一个图形链接预测任务中展示了有竞争力的结果。
在本文中，我们介绍了一种压缩深度神经网络（DNN）中间特征图的方法，以减少推理过程中的内存存储和带宽需求。与以前的工作不同，所提出的方法是基于将定点激活转换为最小的GF（2）有限域的向量，然后将非线性降维（NDR）层嵌入到DNN中。 这种端到端的学习表示法通过利用沿通道或空间维度的定点激活的量化冗余来找到更紧凑的特征图。我们将所提出的网络结构应用于ImageNet分类和PASCAL VOC物体检测任务。与先前的方法相比，所进行的实验表明，在只增加位计算的情况下，内存需求减少了2倍，而准确度却略有下降。
对抗性训练是对对抗性攻击最有力的防御措施之一，但它需要在优化过程中为每个小批次生成对抗性例子。 在训练过程中产生这些例子的费用往往使对抗性训练无法用于复杂的图像数据集。在这项研究中，我们探索了对抗性训练提高分类器鲁棒性的机制，并表明这些机制可以通过简单的正则化方法有效地模仿，包括标签平滑和对数挤压。 值得注意的是，使用这些简单的正则化方法与高斯噪声注入相结合，我们能够实现强大的对抗性鲁棒性--通常超过对抗性训练的鲁棒性--不使用对抗性例子。
无监督学习的一个主要目标是发现对后续任务有用的数据表征，在训练过程中不需要访问监督标签。通常，这包括最小化一个替代目标，如生成模型的负对数可能性，希望对后续任务有用的表征会作为一个副作用出现。 具体来说，我们以半监督分类性能为目标，元学习一种算法--无监督权重更新规则--产生对该任务有用的表征。此外，我们将我们的无监督更新规则限制为生物驱动的神经元局部函数，这使得它能够推广到不同的神经网络架构、数据集和数据模式。 我们表明，元学习的更新规则产生了有用的特征，有时优于现有的无监督学习技术。我们进一步表明，元学习的无监督更新规则可以推广到训练具有不同宽度、深度和非线性的网络。它还可以推广到训练具有随机排列的输入尺寸的数据，甚至可以从图像数据集推广到文本任务。
在监督学习中，最近有大量证据表明，在过度参数化的设置中，更宽的网络可以获得更好的测试误差。换句话说，当任意增加网络宽度时，偏差-变异的权衡是无法直接观察到的。 我们在四个OpenAI Gym环境中进行了实验，增加了价值和政策网络的宽度，超过了它们的规定值。然而，单独调整每个网络宽度的超参数仍然是未来的重要工作，在这些环境/算法中，最佳的超参数在不同的宽度上有明显的不同，当所有宽度都使用相同的超参数时，就会混淆结果。
学习数据的分离表征是无监督学习的核心主题之一，特别是生成式建模。 在这项工作中，我们处理了一个稍微复杂的情况，即观测值是由一些已知的控制变量和一些潜在的噪声变量的条件分布产生的。 为此，我们提出了一个分层模型和训练方法（CZ-GEM），它利用了基于似然和无似然生成模型的一些最新发展。 我们表明，通过表述，CZ-GEM引入了正确的归纳偏差，确保了控制变量与噪声变量的脱钩，同时也保持了控制变量的成分脱钩。
深度学习在许多自然语言处理任务上产生了最先进的性能，包括命名实体识别（NER）。然而，这通常需要大量的标记数据。在这项工作中，我们证明，当深度学习与主动学习相结合时，标记训练数据的数量可以大大减少。虽然主动学习是有效的样本，但它可能是计算昂贵的，因为它需要迭代再训练。该模型在该任务的标准数据集上实现了近乎最先进的性能，同时在计算上比性能最好的模型要高效得多。我们在训练过程中进行增量主动学习，仅用25％的原始训练数据就能达到近乎最先进的性能。
网络量化是一种模型压缩和加速技术，对神经网络的部署至关重要。大多数量化方法在预训练的网络上进行微调，但这有时会导致与原始网络相比在准确性上的巨大损失。 与其他在4比特下运行的完全量化网络相比，我们显示出准确率的大幅提高，例如，使用ResNet-18在ImageNet上的top-1准确率为66.68%，而之前最先进的准确率为61.52% Louizos等人（2019）和全精度参考准确率为69.76%。我们进行了一套彻底的实验来测试我们方法的功效，还对方法和技术的不同方面进行了消融研究，以提高训练稳定性和准确性。我们的代码库和训练的模型在GitHub上提供。
在过去的五年中，卷积网络设计的大部分工作都是围绕深度、过滤器大小和特征通道数量的重要性进行的经验调查，而最近的研究表明，分支，即。为了应对多分支架构中设计选择的复杂性，之前的工作采用了简单的策略，如固定的分支因子，将相同的输入输入到所有的平行分支，并将所有分支在聚集点产生的输出进行加法组合。在这项工作中，我们取消了这些预定义的选择，并提出了一种算法来学习网络中各分支之间的连接，而不是由人类设计者预先选择，多分支连接与网络的权重同时学习，通过优化针对最终任务定义的单一损失函数。
尽管深度卷积网络在许多自然语言任务中取得了更好的性能，但由于它们难以解释，所以被当作黑盒子。特别是，人们对它们如何在中间层表示语言知之甚少。为了理解在语言任务中训练的深度卷积网络的表示，我们表明个别单元对特定的语素、单词和短语有选择性的反应，而不是对任意的和不可解释的模式作出反应。 为了定量分析这种耐人寻味的现象，我们提出了一种基于单元如何响应复制文本的概念对齐方法。我们在分类和翻译任务的多个数据集上用不同的架构进行了分析，并对深度模型如何理解自然语言提供了新的见解。
将强化学习（RL）应用于现实世界的问题，将需要在很长的时间范围内对行动-回报的相关性进行推理。层次强化学习（HRL）方法通过将任务划分为层次来处理，通常具有手工调整的网络结构或预先定义的子目标。 我们提出了一个新的HRL框架TAIC，它从过去的经验或专家示范中学习时间抽象，而不需要特定的任务知识。我们将时间抽象问题表述为学习动作序列的潜表征，并提出了一种通过添加信息理论约束来规范潜空间的新颖方法。 具体来说，我们使潜变量和状态变化之间的相互信息最大化。潜空间的可视化表明，我们的算法学习了一个长动作序列的有效抽象，学到的抽象允许我们更有效地学习更高层次的新任务。
循环神经网络（RNN）在许多不同的任务上取得了最先进的性能，从机器翻译到手术活动识别，然而训练RNN来捕捉长期的依赖性仍然很困难。 我们采取了一种正交的方法，引入了MIST RNNs，一种允许从非常遥远的过去直接连接的NARX RNN架构。 我们表明，MIST RNNs1）与LSTM和以前提出的NARX RNNs相比，表现出卓越的消失梯度特性；2）比以前提出的NARX RNN架构高效得多，需要的计算量甚至比LSTM还少；3）在需要非常长期依赖的任务上，比LSTM和Clockwork RNNs大幅提高性能。
一个训练有素的模型应该对每个类别的物体进行一致的分类。这就要求高层次的语义特征在样本之间应该是相同的，尽管在分辨率、纹理、变形等方面有很大的跨度。以前的工作主要是重新设计损失函数或对损失提出新的正则化约束。在本文中，我们通过一个新的视角来解决这个问题。对于每个类别，我们假设在特征空间中有两个集合：一个有更可靠的信息，另一个有不可靠的来源。 我们认为，在训练过程中，可靠的集合可以指导不太可靠的集合的特征学习--本着学生模仿老师的行为，从而推动在高维空间中形成更紧凑的类中心点。这样的方案也有利于可靠的集合，因为样本在同一类别中变得更加接近--意味着分类器更容易识别。 我们将这种相互学习的过程称为特征交织器，并将这种精神嵌入到物体检测中。众所周知，由于在网络前向传递过程中失去了详细信息，低分辨率的物体更难检测。因此，我们将高分辨率的物体视为可靠的集合，低分辨率的物体视为不太可靠的集合。具体来说，交织器是通过最小化两个集合之间的分布分歧来实现的。 我们设计了一个历史缓冲区来表示可靠集合中所有以前的样本，并利用它们来指导不可靠集合的特征学习。我们进一步研究了为可靠集合获得有效特征表示的设计，其中我们将最优传输（OT）算法引入框架。
持续学习的方法旨在成功地学习一组以在线方式到达的相关任务。最近，已经开发了几个框架，使深度学习能够部署在这种学习场景中。一个关键的建模决定是在多大程度上应该在各个任务之间共享架构。 一方面，对每个任务单独建模可以避免灾难性遗忘，但它不支持转移学习，并导致大型模型。另一方面，严格规定一个共享组件和一个特定任务的部分可以实现任务转移并限制模型的大小，但它很容易受到灾难性遗忘的影响，并限制了可能发生的任务转移的形式。 理想情况下，网络应该以数据驱动的方式自适应地识别网络的哪些部分可以共享。这里我们引入了这样一种方法，称为自适应权重的持续学习（CLAW），它基于概率建模和变异推理。实验表明，CLAW在六个基准上的整体持续学习性能（以分类精度衡量）和解决灾难性遗忘方面实现了最先进的性能。
高维数据往往位于或接近于低维子空间。由L0-norm诱导的稀疏子空间聚类方法，如L0-Sparse Subspace Clustering（L0-SSC），被证明比其L1对应的方法如Sparse Subspace Clustering（SSC）更有效。 然而，这些基于L0-norm的子空间聚类方法只限于完全位于子空间的干净数据。真实的数据经常受到噪声的影响，它们可能位于子空间附近。 我们表明，在确定性和随机化模型下，噪声L0-SSC的优化问题的最优解实现了子空间检测特性（SDP），这是分离不同子空间的数据的关键因素。 我们进一步提出了Noisy-DR-L0-SSC，它可以证明在降维数据上恢复子空间。Noisy-DR-L0-SSC首先通过线性变换将数据投射到低维空间，然后在降维数据上执行Noisy L0-SSC，从而提高效率。
模式连接为分析损失景观提供了新的几何见解，并能在训练有素的神经网络之间建立高精确度的路径。在这项工作中，我们提出在损失景观中采用模式连接来研究深度神经网络的对抗性鲁棒性，并提供新的方法来改善这种鲁棒性。 我们的实验涵盖了应用于不同网络架构和数据集的各种类型的对抗性攻击。当网络模型被后门或错误注入攻击篡改时，我们的结果表明，使用有限数量的真实数据学习的路径连接可以有效地缓解对抗性影响，同时保持清洁数据的原始准确性。因此，模式连接为用户提供了修复被后门或错误注入的模型的能力。 我们还使用模式连接来研究常规模型和稳健模型对逃避攻击的损失景观。实验表明，在连接常规模型和对抗性训练模型的路径上，存在着对抗性稳健性损失的障碍。 在对抗性鲁棒性损失和输入Hessian矩阵的最大特征值之间观察到了高度的相关性，为此提供了理论上的理由。 我们的结果表明，模式连接为评估和提高对抗性鲁棒性提供了一个整体工具和实用手段。
生成式对抗网络（GANs）学习将样本从噪声分布映射到选定的数据分布。最近的工作表明，GANs因此对噪声分布的形状很敏感，而且受到限制。 例如，单一生成器难以将连续噪声（如均匀分布）映射到不连续的输出（如独立的高斯）或复杂的输出（如相交的抛物线）。我们通过学习从多个模型生成来解决这个问题，这样生成器的输出实际上是几个不同网络的组合。 我们为多生成器模型提供了一种新的表述，在这里我们学习了一个以噪声为条件的生成器的先验，由一个神经网络进行参数化。因此，这个网络不仅学习了从每个生成器采样的最佳速率，而且还优化了每个生成器所接收的噪声。
生成对抗网络（GANs）的最新进展--在架构设计、训练策略和经验技巧方面--已经在ImageNet等大规模数据集上带来了几乎是逼真的样本。 事实上，对于一个特别的模型--BigGAN，诸如Inception Score或Frechet Inception Distance等指标几乎与数据集的指标一致，这表明这些模型接近于与训练集的分布相匹配。  鉴于这些模型的质量，值得了解的是这些样本在多大程度上可以用于数据扩充，这也是GAN研究项目的一个长期目标。 为此，我们使用纯粹的BigGAN图像或ImageNet和BigGAN图像的混合物来训练ResNet-50分类器，并在ImageNet验证集上进行测试。我们的初步结果既表明了对最先进的GAN质量的衡量观点，也突出了当前指标的局限性。 只使用BigGAN图像，我们发现Top-1和Top-5的错误分别增加了120%和384%，此外，在ImageNet训练集上增加更多的BigGAN数据，最多只能稍微提高分类器的性能。最后，我们发现Inception Score、FID或其组合都不能预测分类准确性。  这些结果表明，随着GANs开始被部署到下游任务中，我们应该建立能更好地衡量下游任务性能的指标。 我们建议将分类性能作为这样一个指标，除了评估每类样本的质量外，它还更适合于这种下游任务。
现代联合网络，如由可穿戴设备、移动电话或自动驾驶汽车组成的网络，每天都会产生大量的数据。这些丰富的数据可以帮助学习模型，改善每个设备上的用户体验。然而，联合数据的规模和异质性给联合学习、元学习和多任务学习等研究领域带来了新的挑战。 随着机器学习界开始应对这些挑战，我们正处于一个关键时刻，以确保这些领域的发展以现实的基准为基础。Leaf包括一套开源的联合数据集、一个严格的评估框架和一套参考实现，所有这些都是为了捕捉实际联合环境中的障碍和复杂性。
理解物体运动是计算机视觉的核心问题之一。它需要随着时间的推移对物体进行分割和跟踪。在实例分割方面已经取得了重大进展，但这种模型不能跟踪物体，更关键的是，它们不能在三维空间和时间上进行推理。我们提出了一种新的视频上的空间-时间嵌入损失，产生时间上一致的视频实例分割。 我们的模型包括一个时间网络，它可以学习模拟时间背景和运动，这对于随着时间的推移产生平滑的嵌入是至关重要的。此外，我们的模型还估计单眼深度，具有自我监督的损失，因为与物体的相对距离有效地限制了它的下一个位置，确保了时间一致的嵌入。最后，我们表明，我们的模型可以准确地跟踪和分割实例，即使有闭塞和漏检，在KITTI多物体和跟踪数据集上推进了最先进的水平。
随着强化学习继续推动机器智能超越其传统边界，稀疏奖励环境中的大量实践严重限制了在更广泛的先进领域的进一步应用。出于对适应稀疏奖励环境的有效深度强化学习算法的需求，本文提出了后见之明信任区域策略优化（HTRPO），一种有效利用稀疏奖励条件下的相互作用来优化信任区域内的策略，同时保持学习稳定性的方法。 首先，我们从理论上使TRPO目标函数（政策的预期收益形式）适应替代目标产生的事后数据的分布。然后，我们应用Monte Carlo的重要性抽样来估计两个政策之间的KL-分歧，将事后数据作为输入。在分布足够接近的条件下，KL-分歧被另一个f-分歧所近似。这种近似导致方差的减少，缓解了政策更新期间的不稳定性。 在离散和连续基准任务上的实验结果表明，HTRPO的收敛速度明显快于以前的政策梯度方法。它在稀疏奖励环境中的训练政策方面实现了有效的性能和高数据效率。
开放域的问题回答（QA）是人工智能和NLP中的一个重要问题，它正在成为人工智能方法和技术通用性进展的风向标。开放域QA系统的大部分进展是通过信息检索方法和语料库建设的进展实现的。 在本文中，我们专注于最近推出的ARC挑战赛数据集，其中包含了2,590道为小学科学考试编写的选择题。这些问题被选为对当前质量保证系统最具挑战性的问题，而目前的技术水平只比随机机会略好。 我们提出了一个系统，该系统将给定的问题重新表述为查询，用于从大型科学相关文本语料库中检索支持性文本。我们的改写器能够结合概念网的背景知识，并与一个在SciTail上训练的通用文本尾随系统一起识别检索结果中的支持性内容，在端到端的质量保证任务中优于几个强大的基线，尽管只被训练为识别原始源问题中的基本术语。 我们对检索到的证据和答案候选者使用了一种可推广的决策方法，以选择最佳答案。通过结合查询重构、背景知识和文本必然性，我们的系统能够在ARC数据集上超越几个强大的基线。
近年来，深度CNN在众多机器学习和计算机视觉任务中取得了最先进的性能，但随着它们变得越来越深，它们使用的参数数量也在增加，使得它们很难在内存受限的环境中部署，也很难解释。机器学习理论暗示，这种网络是高度过度参数化的，应该有可能在不牺牲准确性的情况下减少其规模，事实上，最近的许多研究已经开始强调可以利用的具体冗余来实现这一目标。 在本文中，我们在这个方向上又迈出了一步，提出了一种压缩深度CNN的滤波器共享方法，通过重复应用单一的卷积映射学习的滤波器来模拟CNN管道，从而减少其内存占用。 我们通过在CIFAR-10、CIFAR-100、Tiny ImageNet和ImageNet上的实验表明，这使得我们可以将基于常见设计（如VGGNet和ResNet）的网络的参数数量减少到与它们的深度成正比的系数，同时使它们的准确性基本不受影响。在更广泛的层面上，我们的方法也表明了如何利用视觉信号中的尺度空间规则性来构建更加简明和可解释的神经架构。
我们通过对核和神经嵌入对应的表征进行频谱分析，扩展了（Arora等人，2019）的最新成果。他们表明，在一个简单的单层网络中，标签与相应的格拉姆矩阵的特征向量的对齐决定了训练期间的优化收敛以及泛化特性。我们将他们的结果推广到核和神经表征，并表明这些扩展改善了（Arora等人，2019）中研究的基本设置的优化和泛化。
获得高质量的不确定性估计对于深度神经网络的许多应用是至关重要的。在本文中，我们从理论上证明了一种估计不确定性的方案，该方案基于从先验分布中抽样。关键是，不确定性估计被证明是保守的，即它们从不低估由假设的贝叶斯算法获得的后验不确定性。 我们还显示了集中性，这意味着随着我们获得更多的数据，不确定性估计会收敛到零。从随机先验获得的不确定性估计可以适应任何深度网络架构，并使用标准的监督学习管道进行训练。我们提供了随机先验在典型计算机视觉任务上的校准和分布外检测的实验评估，证明它们在实践中优于深度组合。
在本文中，我们提出了一个建立在变异自动编码器和归一化流基础上的任意条件的数据归置框架。所提出的模型能够将任何部分数据映射到一个多模态的潜在变异分布。
本文研究的是{模型反转攻击}，即滥用对模型的访问来推断训练数据的信息。自从它被~\citet{fredrikson2014privacy}首次引入以来，鉴于训练数据通常包含敏感信息，这种攻击已经引起了严重的关注。 迄今为止，成功的模型反转攻击只在简单的模型上得到了证明，如线性回归和逻辑回归。以前试图反转神经网络，甚至是那些具有简单架构的神经网络，都未能产生令人信服的结果。我们提出了一种新的攻击方法，被称为emph{生成模型反转攻击}，它可以以高成功率反转深度神经网络。 我们不是从头开始重建私人训练数据，而是利用部分公共信息（可以是非常通用的），通过生成对抗网络（GANs）学习分布式先验，并使用它来指导反转过程。此外，我们从理论上证明，一个模型的预测能力和它对反转攻击的脆弱性确实是一个硬币的两面--高预测性的模型能够在特征和标签之间建立一个强关联，这与对手利用来发动攻击的情况完全一致。 我们的实验表明，在从最先进的人脸识别分类器中重建人脸图像时，所提出的攻击比现有的工作提高了大约75%/$的识别精度。我们还表明，差分隐私，在其典型形式下，对保护我们的攻击没有什么作用。
基于潜空间的GAN方法和基于注意力的编码器-解码器架构分别在文本生成和无监督NMT方面取得了令人印象深刻的结果。利用这两个领域，我们提出了一个基于对抗性潜空间的架构，能够同时生成两种语言的平行句子并进行双向翻译。 首先，通过逆向翻译和对抗性设置来训练NMT模型，以强制执行两种语言之间的潜伏状态。接下来，训练GAN来生成模仿语言共享潜伏空间的 "合成 "代码，然后将该代码输入解码器，生成任何一种语言的文本。
随着人工智能（AI）成为我们生活中不可或缺的一部分，开发可解释的AI，体现在AI或机器人代理的决策过程中，变得势在必行。 对于机器人队友来说，产生解释以说明其行为的能力是可解释机构的关键要求之一。以前关于解释生成的工作主要是支持机器人行为背后的推理。然而，这些方法没有考虑到理解收到的解释所需的心理工作量。 换句话说，人类队友被期望理解所提供的任何解释，往往是在任务执行之前，无论解释中提出了多少信息。在这项工作中，我们认为解释，特别是复杂的解释，应该在执行过程中以在线方式进行，这有助于分散需要解释的信息，从而减少人类的心理工作量。 然而，这里的一个挑战是，解释的不同部分是相互依赖的，在生成在线解释时必须考虑到这一点。为此，我们提出了在线解释生成的一般表述，以及满足不同在线属性的三种不同的实现方式。我们的解释生成方法基于我们先前工作中引入的模型调和设置。
深度神经网络使用更深更广的结构来实现更好的性能，因此也使用越来越多的GPU内存。然而，有限的GPU内存限制了许多潜在的神经网络设计。 重新计算可以通过在前向传播过程中删除一些特征图来换取时间和空间。然而，如何自动决定哪些变量需要被交换或重新计算仍然是一个具有挑战性的问题。
在vanilla backpropagation（VBP）中，激活函数在非线性和可分性方面非常重要。消失的梯度一直是与深度学习（DL）中激活函数选择不当有关的一个重要问题。激活函数的导数可以通过使用固定的随机反馈权重排列（FBA）的迭代时间差分（ITD）来代替。使用FBA与ITD，我们可以将VBP转化为一种更符合生物学原理的方法来学习深度神经网络架构。我们并不声称ITD与我们大脑中的尖峰时间依赖可塑性（STDP）完全相同，但这项工作可以成为向深度学习中基于STDP的误差反向传播整合迈出的第一步。
受WaveNet（van den Oord等人，2016）和Tacotron（Wang等人，2017）等用于文本-语音（TTS）的深度生成模型最近的成功启发，本文提出使用为自动语音识别（ASR）定制的深度生成模型作为具有独立语言模型（LM）的整体识别系统的主要声学模型（AM）.考虑了深度的两个层面。(1)使用混合密度网络，包括自回归和非自回归，以产生能够对声学输入序列进行建模的密度函数，其条件比用于ASR的第一代生成模型--高斯混合模型/隐马尔科夫模型（GMM/HMMs）强大得多，以及(2)使用标准LSTM，本着原始串联方法的精神，产生用于生成模型的判别特征向量。 将混合密度网络和深度判别特征结合起来，导致了一种与RNN Transducer（Graves，2012）直接相关的新型双堆LSTM架构，但具有明确的密度函数形式，并与单独的语言模型自然结合，使用贝叶斯规则。这里讨论的生成模型在对数可能性和框架精度方面进行了实验比较。
最近的研究表明，广泛使用的深度神经网络（DNNs）容易受到精心制作的对抗性例子的影响。许多先进的算法已经被提出来，通过利用L_p距离来惩罚扰动来生成对抗性例子，不同的防御方法也被探索来防御这种对抗性攻击。虽然L_p距离作为感知质量指标的有效性仍然是一个活跃的研究领域，但在本文中，我们将专注于不同类型的扰动，即空间变换，而不是像以前的工作那样直接操纵像素值。通过空间变换产生的扰动可能导致大的L_p距离测量，但我们广泛的实验表明，这种空间变换的对抗性例子在感知上是现实的，而且更难以用现有的防御系统进行防御。 这有可能为对抗性例子的生成和相应防御系统的设计提供一个新的方向。我们对不同例子的基于空间变换的扰动进行了可视化，并表明我们的技术可以产生具有平滑图像变形的现实对抗性例子。最后，我们对不同类型的对抗性例子的深度网络的注意力进行了可视化，以更好地了解这些例子是如何被解释的。
大多数现有的深度强化学习（DRL）框架考虑的行动空间要么是离散空间，要么是连续空间。在为全球最受欢迎的手机游戏之一《王者荣耀》（KOG）设计游戏人工智能项目的激励下，我们考虑了离散-连续混合行动空间的情况。为了直接应用现有的DLR框架，现有的方法要么用离散集近似混合空间，要么将其放松为连续集，这通常是效率较低且不稳定的。 在本文中，我们为混合行动空间提出了一个没有近似或放松的参数化深度Q网络（P-DQN）。我们的算法结合了DQN和DDPG，可以被视为DQN对混合行动的扩展。
在过去的十年中，知识图谱成为捕捉结构化领域知识的热门。关系学习模型能够预测知识图谱内部缺失的链接。更具体地说，潜伏距离方法通过潜伏表征之间的距离来模拟实体之间的关系。翻译嵌入模型（如TransE）是最流行的潜伏距离方法之一，它使用一个距离函数来学习多种关系模式。然而，它们在捕捉对称关系方面大多是低效的，因为所有对称关系的表征向量规范变得等于零。它们在学习具有反身模式的关系时也会失去信息，因为它们变得对称和反身。我们提出了多距离嵌入模型（MDE），它解决了这些限制，并提出了一个框架，使基于潜在距离的术语（MDE）的协作组合成为可能。 我们的解决方案基于两个原则：1）使用基于极限的损失而不是边际排名损失；2）通过为每个术语学习独立的嵌入向量，我们可以使用矛盾的距离术语进行集体训练和预测。我们进一步证明，MDE允许对具有（反）对称性、反转和组成模式的关系进行建模。 我们提出MDE是一个神经网络模型，它允许我们在嵌入向量和分数函数的预期输出之间映射非线性关系。我们的经验结果显示，MDE在几个基准数据集上的表现优于最先进的嵌入模型。
改进型生成对抗网络（Improved GAN）是使用生成对抗模型解决半监督学习问题的成功方法。然而，它存在训练不稳定的问题。在本文中，我们发现不稳定的原因主要是生成器上的梯度消失。 为了弥补这个问题，我们提出了一种新的方法，利用协作训练来提高半监督GAN与Wasserstein GAN的结合的稳定性。实验表明，我们提出的方法比原来的改进型GAN更稳定，在不同的数据集上取得了相当的分类精度。
众所周知，在网络宽度无限大的情况下，一个单层全连接的神经网络，其参数上有一个i.i.d.先验，等同于高斯过程（GP）。 最近，模仿多层随机神经网络的核函数已经被开发出来，但只是在贝叶斯框架之外。因此，以前的工作没有发现这些核函数可以作为GP的协方差函数，并允许用深度神经网络进行完全贝叶斯预测。 在这项工作中，我们推导出无限宽的深度网络和具有特定协方差函数的GP之间的确切等价关系。我们进一步开发了一个计算效率高的管道来计算这个协方差函数。然后，我们使用所得的GP对MNIST和CIFAR-10的深度神经网络进行贝叶斯推断。 我们观察到，随着层宽的增加，训练后的神经网络精度接近相应的GP，而且GP的不确定性与训练后的网络预测误差密切相关。我们进一步发现，当有限宽度的训练后的网络变得更宽、更类似于GP时，测试性能会增加，而且基于GP的预测通常优于有限宽度网络的预测。最后我们将我们的GP表述中的权重和变异的先验分布与随机神经网络中信号传播的最新发展联系起来。
搜索空间是神经架构搜索的一个关键考虑因素。最近，Xie等人（2019a）发现，来自相同分布的随机生成的网络表现相似，这表明我们应该搜索随机图分布而不是图。我们提出graphon作为一个新的搜索空间。 graphon是图的Cauchy序列和无标度概率分布的极限，可以从中抽取不同顶点数量的图。这一特性使我们能够使用快速、低容量的模型进行NAS，并在必要时扩大所发现的模型。我们在graphon空间中开发了一种NAS算法，并通过经验证明，它可以找到阶段性的图，在ImageNet上表现优于DenseNet和其他基线。
尽管作为一种防御机制，对抗性训练是迄今为止最成功的策略，以提高神经网络对对抗性攻击的鲁棒性。我们假设，这种糟糕的泛化是对抗性训练在每个训练样本周围的均匀扰动半径的结果。接近决策边界的样本可以在小的扰动预算下变形为不同的类别，而在这些样本周围强制执行大的边际产生糟糕的决策边界，泛化很差。 在这一假设的激励下，我们提出了实例适应性对抗训练--一种在每个训练样本周围强制执行特定样本扰动边际的技术。我们表明，使用我们的方法，在未扰动样本上的测试精度得到了提高，而鲁棒性却略有下降。在CIFAR-10、CIFAR-100和Imagenet数据集上的广泛实验证明了我们提出的方法的有效性。
包括传统模型和神经网络在内的机器学习模型很容易被对抗性例子所愚弄，这些例子是由自然例子中的小幅扰动生成的。 这给机器学习的安全性带来了严峻的挑战，并阻碍了机器学习在许多重要领域的广泛应用，如计算机视觉和恶意软件检测。 不幸的是，即使是最先进的防御方法，如对抗性训练和防御性提炼，仍然存在很大的局限性，并且可以被规避。 从一个独特的角度，我们提议在本文中研究两个重要的研究问题。对抗性例子是否可以与自然例子区分开来？ 由不同方法产生的对抗性例子是否可以相互区分？ 这两个问题涉及到对抗性例子的可区分性。 回答这两个问题将有可能导致一种简单而有效的方法，在本文中被称为多标签分类下的防御性区分，以防止对抗性例子。 我们使用MNIST数据集设计并进行了实验来研究这两个问题，并获得了非常积极的结果，证明了对抗性例子的强可区分性。 我们建议，应该认真考虑这种独特的防御性区分方法，以补充其他防御方法。
长期以来，设计表现出高性能的神经架构被认为是一门黑暗的艺术，需要专家手工调整。架构设计的少数著名准则之一是避免爆炸性或消失的梯度。 然而，即使是这一准则也仍然是相对模糊的和间接的，因为没有一个定义明确的、基于梯度的指标，可以在训练开始前计算出来，并能稳健地预测网络的性能{在训练完成后}。我们引入了，就我们所知的，第一个这样的指标：非线性系数（NLC）。 通过广泛的实证研究，我们表明，在网络随机初始化状态下计算的NLC是一个强大的测试误差预测器，而且至少在全连接前馈网络中，获得一个合适大小的NLC对获得最佳测试误差至关重要。 NLC在概念上也很简单，计算成本很低，而且对一系列混杂因素和架构设计选择都很稳健，而可比的指标不一定稳健。因此，我们认为NLC是架构搜索和设计的一个重要工具，因为它可以在训练开始前稳健地预测不良的训练结果。
本文对Fan（2017）提出的训练过的广义汉明网络（GHN）进行了严格的分析，并披露了关于GHN的一个有趣的发现，即GHN中的堆叠卷积层等同于一个单一而宽的卷积层。在理论方面，所揭示的等同性可以被视为普遍近似定理Cybenko（1989）；Hornik（1991）的建设性表现。 在实践中，它具有深远和多方面的意义。对于网络可视化，每一层构建的深层表征提供了一个不依赖于输入数据的网络内部表征的可视化。此外，深层表征允许在一个步骤中直接提取特征，而不需要借助于现有可视化工具中的正则优化。
有两种主要的白盒对抗性攻击范式，试图施加输入扰动。 第一种范式，称为固定扰动攻击，在一个给定的扰动水平内制作对抗性样本。 第二种范式，称为零置信度攻击，找到导致误分类所需的最小扰动，也称为输入特征的边际。 虽然前一个范式得到了很好的解决，但后一个范式却没有。 现有的零置信度攻击要么引入明显的近似误差，要么过于耗时。 因此，我们提出了MarginAttack，一个能够以更高的精度和效率计算边际的零信心攻击框架。 我们的实验表明，MarginAttack能够计算出比最先进的零置信度攻击更小的余量，并与最先进的固定扰动攻击相匹配。 此外，它的运行速度明显快于Carlini-Wagner攻击，这是目前最准确的零置信度攻击算法。
鉴于视觉世界的多样性，不存在一个真正的识别尺度：物体在整个视野中可能以截然不同的尺寸出现。动态模型不是列举不同滤波通道或金字塔级别的变化，而是局部预测尺度并相应地调整感受野。 我们将动态尺度推理从前馈预测扩展到迭代优化，以实现进一步的适应性。我们为推理提出了一个新的熵最小化目标，并对任务和结构参数进行优化，以根据每个输入调整模型。推理过程中的优化提高了语义分割的准确性，并对导致前馈动态推理失败的极端尺度变化有更好的概括。
深度图像先验（DIP，Ulyanov等人。首先，我们通过对DIP的早期输出进行分类，并使用一种新的显著性地图方法，显示出这些早期迭代对对抗性扰动的不变性。 最后，我们研究了早期DIP输出的抗干扰性，并假设这些输出可能会去除非稳健的图像特征。通过比较分类置信度值，我们显示了一些证据来证实这一假设。
在本文中，我们解决了基于机器学习的社交媒体谣言检测的有限标记数据和类不平衡问题的挑战。我们提出了一种基于语义关联性的谣言检测的离线数据增强方法。为此，未标记的社交媒体数据被利用来增强有限的标记数据。 一个用大型特定领域语料库微调的语言模型在训练数据增强方面比预训练的语言模型有显著的改善。我们在六个不同的真实世界事件上进行了基于五个公开可用的数据集和一个增强的数据集的实验。我们的实验表明，所提出的方法允许我们通过弱监督产生具有合理质量的较大的训练数据。
自我注意是建立语言和图像生成模型的一个有用的机制。它通过比较每个元素和当前的时间步骤来确定上下文元素的重要性。在本文中，我们表明一个非常轻量级的卷积可以与报告的最佳自我注意结果竞争。 我们仅根据当前的时间步长来预测单独的卷积核，以确定上下文元素的重要性。这种方法所需的操作数量在输入长度上呈线性扩展，而自注意力是二次的。在大规模机器翻译、语言建模和抽象总结上的实验表明，动态卷积比强大的自注意力模型更有优势。
由于其与生成对抗网络（GAN）的联系，鞍点问题最近在机器学习和其他领域引起了相当大的兴趣。出于需要，大多数理论保证都围绕着凸凹（甚至是线性）问题；然而，要在理论上实现高效的GAN训练，关键是要超越这一经典框架。 为了沿着这些路线取得零星进展，我们分析了镜像下降（MD）在一类非单调问题中的行为，这些问题的解决方案与自然相关的变异不等式的解决方案重合--我们称之为一致性的属性。 我们首先表明，普通的、"vanilla "MD在这个条件的严格版本下收敛，但在其他情况下并不收敛；特别是，即使在具有唯一解的双线性模型中，它也可能无法收敛。我们随后表明，这一缺陷可以通过乐观主义来缓解：通过采取 "额外梯度 "步骤，乐观主义镜像下降法（OMD）在所有一致性问题中都收敛。 我们的分析概括并扩展了Daskalakis等人[2018]关于双线性问题中乐观梯度下降（OGD）的结果，并在证明凸凹游戏之外的可收敛性方面取得了具体进展。我们还提供了这些结果的随机类似物，并通过在广泛的GAN模型（包括高斯混合模型以及CelebA和CIFAR-10数据集）的数值实验验证了我们的分析。
批量归一化（BN）经常被用来稳定和加速深度神经网络的训练。在许多情况下，它确实减少了实现低训练误差所需的参数更新数量。然而，它也降低了对小的对抗性输入扰动和常见腐败的鲁棒性，其百分比为两位数，正如我们在五个标准数据集上显示的那样。 最近的一次均值场分析发现，当BN用于多层时，会诱发梯度爆炸，但这不能完全解释我们观察到的脆弱性，因为它已经发生在单一的BN层。我们认为，实际原因是决策边界沿着低方差的输入维度向最近的中心分类器倾斜。 因此，在BN步骤中为数值稳定性引入的常数是一个重要的超参数，可以通过调整来恢复一些鲁棒性，但要以标准测试精度为代价。我们在一个线性 "玩具 "模型上明确解释了这一机制，并在实验中表明它仍然适用于非线性 "现实世界的模型。
学习优化是最近提出的一个使用强化学习学习优化算法的框架。在本文中，我们探讨了学习优化算法来训练浅层神经网络。这种高维随机优化问题对现有的强化学习算法提出了有趣的挑战。 我们开发了一种适合在这种环境下学习优化算法的扩展，并证明所学的优化算法即使在未见过的任务上也能持续胜过其他已知的优化算法，并且对梯度的随机性和神经网结构的变化具有鲁棒性。更具体地说，我们表明，在MNIST上训练神经网的问题上用所提出的方法训练的优化算法可以推广到在多伦多面孔数据集、CIFAR-10和CIFAR-100上训练神经网的问题。
神经网络的泛化误差对模型和数据集大小的依赖性在实践中和对神经网络理论的理解中都是至关重要的，然而，这种依赖性的函数形式仍然难以捉摸。我们利用模型比例的成功概念（如宽度、深度），能够同时构建这样的形式，并指定能够在不同的模型/数据尺度上实现它的确切模型。我们的构建遵循了从视觉和语言任务中的各种模型/数据尺度、各种模型类型和数据集的观察中获得的见解。我们表明，该形式既适合不同尺度的观察，又能提供从小型到大型模型和数据的精确预测。
我们提出了一种无监督学习算法，以训练代理实现感知上指定的目标，只使用观察和行动流。我们的代理同时学习目标条件政策和目标实现奖励函数，衡量状态与目标状态的相似程度。 这种双重优化导致了一个合作游戏，产生了一个学习的奖励函数，它反映了环境可控方面的相似性，而不是观察空间中的距离。我们证明了我们的代理以无监督的方式学习的功效，以达到三个领域的不同目标--Atari、DeepMind控制套件和DeepMind实验室。
用于大规模任务的最先进的序列到序列模型对每个输入序列进行固定数量的计算，而不管它是容易还是难以处理的。在本文中，我们训练变形器模型，它可以在网络的不同阶段进行输出预测，我们研究了预测特定序列需要多少计算的不同方法。 与通用变形器中的动态计算不同的是，我们在每一步都应用不同的层，以调整计算量和模型容量。在IWSLT德英翻译中，我们的方法与调整好的基线变形器的准确性相匹配，而使用的解码器层不到四分之一。
变异自动编码器（VAE）是由两部分组成的深度生成潜变量模型：一个生成模型，通过在潜空间上转换分布p(z)来捕捉数据分布p(x)；一个推理模型，为每个数据点推断可能的潜代码（Kingma和Welling，2013）。最近的工作表明，传统的训练方法往往产生违反建模要求的解决方案。(1)学习到的生成模型捕捉到了观察到的数据分布，但这样做的同时忽略了潜伏代码，导致代码不能代表数据（例如van den Oord等人（2017）；Kim等人（2018））；(2)学习到的潜伏代码的总量与先验p（z）不匹配。 这种不匹配意味着所学的生成模型将无法用p(z)的样本生成现实的数据（例如Makhzani等人（2015）；Tomczak和Welling（2017））。在本文中，我们证明这两个问题都源于VAE训练目标的全局最优值往往对应于不理想的解决方案。我们的分析建立在两个观察点上。(1)生成模型是不可识别的--存在许多同样能解释数据的生成模型，每个模型都有不同的（可能是不需要的）属性；(2)VAE目标中的偏差--VAE目标可能倾向于解释数据较差的生成模型，但其后验值容易近似。 我们提出了一种新的推理方法，LiBI，缓解了我们分析中发现的问题。在合成数据集上，我们表明LiBI可以学习捕捉数据分布的生成模型和推理模型，在传统方法难以做到的情况下，更好地满足建模假设。
对超义词进行建模，如poodle is-a dog，是许多NLP任务的重要泛化辅助手段，如entailment、关系提取和问题回答。从有标签的超义词来源（如WordNet）进行监督学习，限制了这些模型的覆盖范围，这可以通过从无标签文本中学习超义词来解决。 现有的无监督方法要么不能扩展到大的词汇表，要么产生不可接受的低准确性。 本文介绍了{{分布式包容向量嵌入（DIVE）}，这是一种简单易行的无监督的超词发现方法，通过每个词的非负向量嵌入，保留了单词上下文的包容属性。 在实验评估中，我们发现我们的方法比以前的任何文献都要全面---在11个数据集上使用多个现有的以及新提出的评分函数进行评估---我们发现我们的方法提供了比以前的无监督方法高一倍的精度，以及最高的平均性能，使用更紧凑的单词表示，并产生许多新的最先进的结果。此外，DIVE中每个维度的含义是可解释的，这导致了一个新的词义歧义的方法，作为DIVE的另一个有希望的应用。
持续学习的目的是学习新的任务而不忘记以前学过的任务。当我们不能从以前的任务中获取数据并且模型有固定的容量时，这尤其具有挑战性。 不确定性是一种自然的方式，可以在我们不断学习的过程中识别出/textit{要记住什么}和/textit{要改变什么}，从而减轻灾难性的遗忘。我们还展示了我们模型的一个变体，它使用不确定性进行权重修剪，通过保存每个任务的二进制掩码来保留修剪后的任务表现。 我们在具有长短任务序列的不同对象分类数据集上对我们的UCB方法进行了广泛的评估，并报告了与现有方法相比的优越性能或相同的性能。此外，我们表明，我们的模型在测试时不一定需要任务信息，即~它不假定知道一个样本属于哪个任务。
人类有一种天然的好奇心，可以想象作为别人或其他东西存在的感觉。这种好奇心对于我们照顾的宠物来说变得更加强烈。人类无法真正了解作为宠物的感觉，但我们可以加深对像它们一样感知和探索世界的理解。 为了评估可穿戴设备在动物透视方面的潜力，我们开发了一种感觉增强的可穿戴设备，为佩戴者提供类似猫的胡须。
泛化误差（也被称为样本外误差）衡量了从训练数据中学到的假设对以前未见过的数据的泛化程度。证明严格的泛化误差界限是统计学习理论的核心问题。  在本文中，我们获得了学习一般非凸目标的泛化误差界线，这在最近几年引起了极大的关注。  我们开发了一个新的框架，称为Bayes-Stability，用于证明依赖于算法的泛化误差界线。 这个新框架结合了PAC-Bayesian理论和算法稳定性概念的思想。 应用Bayes-Stability方法，我们获得了随机梯度Langevin动力学（SGLD）和其他几种噪声梯度方法（例如，与动量、小批量和加速、Entropy-SGD）的新的数据依赖的泛化边界。我们的结果恢复了（并且通常比）Mou等人（2018）的最新结果更严格，并改进了Pensia等人（2018）的结果。 我们的实验表明，我们的数据依赖界线可以区分随机标记的数据和正常数据，这为Zhang等人（2017a）观察到的有趣现象提供了解释。我们还研究了总损失是有界损失和额外的l`2正则化项之和的设置。 我们通过开发一个新的参数分布的Log-Sobolev不等式，获得了这种设置下的连续Langevin动态的新的泛化界线。当过程的噪声水平不是非常小的时候，我们的新界线是比较理想的，即使当T趋于无穷大时也不会变得虚无。
本文提出了一种新的基于动力学模型集合的稀疏奖励强化学习的内在奖励生成方法。在所提出的方法中，多个动力学模型的混合物被用来近似真实的未知过渡概率，内在奖励被设计为从每个动力学模型到动力学模型的混合物中看到的惊喜的最小值。 为了显示所提出的内在奖励生成方法的有效性，通过将所提出的内在奖励生成方法与近似策略优化（PPO）算法相结合，构建了一个工作算法。数值结果显示，对于代表性的运动任务，所提出的基于模型集合的内在奖励生成方法优于以前基于单一动力学模型的方法。
鉴于NLP和语音处理系统的分析技术的快速发展，很少有系统的研究来比较每种方法的优点和缺点。我们使用两种常用的分析技术，即诊断分类器和表征相似性分析，来量化神经激活模式对音素和音素序列的编码程度。我们操纵了两个可能影响分析结果的因素。 其次，我们通过探查对应于语音信号几毫秒的局部激活和整个语篇的全局激活来研究激活的时间范围。我们的结论是，用随机初始化的模型报告分析结果是至关重要的，全局范围的方法往往能产生更一致和可解释的结果，我们建议将其作为局部范围诊断方法的补充。
超级分辨率（SR）是一项基本而重要的低级计算机视觉（CV）任务。与传统的SR模型不同，本研究集中于一个具体而现实的SR问题。我们如何从互联网上广泛存在的压缩JPG（C-JPG）图像中获得满意的SR结果。一般来说，C-JPG可以释放存储空间，同时保持相当的视觉质量。为了解决这个问题，我们提出了一个新颖的SR结构，其中有两个专门设计的组件，以及一个循环损耗。总之，本文主要有三个贡献。第一，我们的研究可以为普遍存在的C-JPG图像生成高质量的SR图像。 第二，我们提出了一个功能子模型来恢复C-JPG图像的信息，而不是传统SR方法中的消除噪声的观点。第三，我们进一步将循环损失整合到SR求解器中，建立一个混合损失函数，以更好地生成SR。实验表明，我们的方法在最先进的方法中取得了突出的性能。
在这项工作中，我们提出了DONUT，一种基于CTC的在线逐例查询关键词发现算法，能够实现自定义唤醒词的检测。 该算法通过记录用户的少量训练实例，从这些训练实例中生成一组标签序列假设，并通过汇总所有假设的分数来检测新的音频记录的唤醒词。我们的方法将基于CTC的关键词发现的通用性和可解释性与传统的逐例查询系统的用户适应性和便利性相结合。
为了灵活有效地推理时间序列，需要紧凑地表示序列中的重要信息的抽象表征。构建这种表征的一种方法是关注序列中的重要事件。在本文中，我们提出了一个模型，它既能学习发现这种关键事件（或关键帧），又能用它们表示序列。 我们使用一个层次化的关键帧-绘者（KeyIn）模型来做到这一点，该模型首先生成关键帧和它们的时间位置，然后在关键帧之间绘出序列。我们提出了一个完全可分的公式来有效学习关键帧的位置。
这项工作通过最大化输入和深度神经网络编码器的输出之间的相互信息来研究无监督的表征学习。重要的是，我们表明结构很重要：将关于输入中的位置性的知识纳入目标可以显著提高表征对下游任务的适用性。 我们的方法，我们称之为Deep InfoMax（DIM），优于一些流行的无监督学习方法，在一些标准架构的分类任务上与完全监督学习相比更胜一筹。DIM为无监督的表征学习开辟了新的途径，是朝着为特定的最终目标灵活制定表征学习目标迈出的重要一步。
估算分子中每个原子的重要性是化学、物理和材料工程中最吸引人和最具挑战性的问题之一。估算原子重要性最常见的方法是使用密度函数理论（DFT）计算电子结构，然后使用人类专家的领域知识进行解释。然而，这种传统的方法对于大型分子数据库来说是不切实际的，因为DFT计算需要大量的计算，具体来说，O（n^4）的时间复杂性与分子中电子数有关。 此外，计算结果应该由人类专家来解释，以估计目标分子属性的原子重要性。为了解决这个问题，我们首先利用基于机器学习的方法来估计原子重要性。
谱系聚类是无监督数据分析中的一种领先和流行技术。 它的两个主要限制是可扩展性和光谱嵌入的通用性（即。我们的网络，我们称之为SpectralNet，学习一种将输入数据点嵌入其相关图拉普拉斯矩阵的特征空间的地图，并随后对它们进行聚类。我们使用一种涉及受限随机优化的程序来训练SpectralNet。 随机优化使其能够扩展到大型数据集，而使用特殊目的输出层实现的约束使我们能够保持网络输出的正交性。此外，由SpectralNet学习的地图自然地将光谱嵌入到未见过的数据点上。 我们的端到端学习过程是完全无监督的。此外，我们应用VC维度理论来推导SpectralNet的大小下限。 我们报告了MNIST和路透社数据集的最先进的聚类结果。
元学习方法，特别是模型无知元学习（Finn等人，2017年）或MAML，在适应新任务方面取得了巨大的成功，在类似的任务上进行训练后，他们的成功背后的机制却不为人知。我们以MAML的实验分析开始这项工作，发现深度模型对其成功至关重要，即使给定简单的任务集，线性模型在任何单个任务上都足够。 此外，在图像识别任务中，我们发现MAML训练的模型的早期层学习任务变量特征，而后期层则用于适应，这进一步证明这些模型需要比其个别任务严格所需的更大能力。 根据我们的发现，我们提出了一种方法，通过将元学习的适应方面分离到只用于适应而不属于前向模型的参数中，从而在推理时更好地利用模型的容量。我们发现，我们的方法能够在较小的模型中进行更有效的元学习，这些模型的大小适合于各个任务。
卷积神经网络不断推进二维和三维图像和物体分类的进展。这种算法的坚定使用需要不断评估和升级基础概念以保持进展。网络正则化技术通常集中在卷积层操作上，而使池层操作没有合适的选择。我们引入小波池作为传统邻域池的另一种选择。 该方法将特征分解为第二级分解，并丢弃第一级子带以减少特征维度。该方法解决了最大池化所遇到的过拟合问题，同时以比通过邻域池化更紧凑的结构方式减少特征。在四个基准分类数据集上的实验结果表明，我们提出的方法优于最大、平均、混合和随机池化等方法，或与之相比表现出色。
动态乘车服务（DRS）在提高城市交通效率方面发挥着重要作用。动态乘车服务的用户满意度由多个因素决定，如旅行时间、成本和与共同乘客的社会兼容性。 现有的DRS通过最大化服务提供商的运营价值或最小化用户的旅行时间来优化利润，但它们忽略了乘客的社会体验，而这一体验在很大程度上影响了服务对用户的总价值。我们提出了DROPS，一个动态共享出行框架，在匹配过程中考虑了乘客的社会偏好，从而提高了出行的质量。 为用户安排行程是一个多目标优化，目的是使服务提供商的运营价值最大化，同时使用户的行程价值最大化。用户价值是根据共同乘客之间的兼容性和乘坐时间来估计的。
深度神经网络（DNNs）最近被证明容易受到对抗性例子的影响，这些例子是精心制作的，可以误导DNNs在预测过程中出现错误。为了更好地理解这种攻击，需要对对抗性例子所在的区域（所谓的 "对抗性子空间"）的特性进行描述。 我们通过使用局部本征维度（LID）来解决这一挑战。LID根据例子与邻居的距离分布来评估参考例子周围区域的空间填充能力。 我们首先解释了对抗性扰动如何影响对抗性区域的LID特征，然后通过经验表明LID特征可以促进对使用最先进的攻击方式产生的对抗性例子的区分。 作为概念验证，我们表明LID的一个潜在应用是区分对抗性例子，初步结果显示，对于本文所考虑的五种攻击策略，在三个基准数据集上，它可以以较大的幅度超过几种最先进的检测措施。我们对对抗性区域的LID特征的分析不仅激励了有效的对抗性防御的新方向，而且为开发新的攻击提供了更多挑战，以更好地了解DNN的脆弱性。
在本文中，我们设计并分析了一种新的零阶（ZO）随机优化算法--ZO-signSGD，它享有无梯度运算和signSGD的双重优势。后者只需要梯度估计的符号信息，却能达到与SGD型算法相当甚至更好的收敛速度。 我们的研究表明，ZO signSGD需要的迭代次数是signSGD的$sqrt{d}$倍，在温和条件下，导致收敛速度为$O(\sqrt{d}/sqrt{T})$，其中$d$是优化变量的数量，$T$是迭代次数。 此外，我们分析了不同类型的梯度估计器对ZO-signSGD收敛的影响，并提出了ZO-signSGD的两个变体，它们至少可以达到$O(\sqrt{d}/\sqrt{T})$的收敛率。在应用方面，我们探讨了ZO-signSGD和黑盒对抗性攻击在鲁棒深度学习中的联系。 我们在图像分类数据集MNIST和CIFAR-10上的实证评估表明，ZO-signSGD在生成黑盒神经网络的对抗性例子上表现出色。
太阳能的非稳态特性使得传统的点预测方法由于预测误差大而变得不那么有用。这导致电网运行的不确定性增加，从而对可靠性产生负面影响，导致运行成本增加。本研究论文提出了一个统一的架构，用于使用循环神经网络（RNN）进行短期和长期预测的多时间跨度太阳能预测。 结果表明，基于统一架构的拟议方法对多时间跨度的太阳能预测是有效的，并且与之前对每个时间跨度使用一个模型的最佳表现方法相比，实现了较低的均方根预测误差。
ResNet和batch-normalization（BN）即使在只有少数标记数据的情况下也能取得很高的性能。然而，其高性能的原因尚不清楚。为了弄清原因，我们分析了ResNet和BN中的跳过连接对数据分离能力的影响，这是分类问题的一项重要能力。 我们的结果表明，在随机初始化权重的多层感知器中，两个输入向量之间的角度以其深度的指数级收敛为零，跳过连接使这种指数级的下降变成了亚指数级的下降，而BN则将这种亚指数级的下降放松为倒数级的下降。 此外，我们的分析表明，初始化时角度的保留鼓励了训练有素的神经网络分离不同类别的点。这些意味着，即使只有少数标记的数据，跳过连接和BN也能提高数据分离的能力并实现高性能。
学习数据表征是机器学习中的一个重要问题。虽然GAN在数据表征方面有了很大的改进，但它仍然存在一些问题，如不稳定的训练、隐藏的数据流形和巨大的计算开销。GAN倾向于在没有任何数据流形信息的情况下简单地生成数据，这阻碍了控制所需特征的生成。 在本文中，我们提出了一种新型的GAN来控制潜在语义表征，称为LSC-GAN，它允许我们生成所需的数据，并有效地学习数据的表征。与传统的GAN模型的潜在空间的隐藏分布不同，我们事先明确定义分布，通过输入遵循分布的潜在变量，根据相应的特征生成数据进行训练。 由于在一个潜空间中部署不同的分布导致潜空间的规模较大，在保持潜空间维度的同时，训练不稳定，我们需要将明确定义分布的过程和生成的操作分开。我们证明VAE适合前者，并修改VAE的损失函数，将数据映射到预先定义的潜空间中，以便根据其特征将重建的数据定位为接近输入数据。 我们在CelebA数据集上进行了一些实验，以验证所提出的方法在稳定和高效地生成所需数据方面的有效性，实现了较高的压缩率，在潜伏空间的每个维度上可容纳约24个像素的信息。此外，我们的模型只通过普通和微笑的面部表情数据来学习不笑（而是皱眉）等反向特征。
随着需求的不断增加和随之而来的服务质量的下降，重点已经转移到缓解网络拥堵，以使交通、供应链和电网等系统更有效地流动。这个方向的一个步骤是重新想象传统的基于启发式的系统训练，因为这种方法无法模拟所涉及的动态。 虽然人们可以通过将网络中的每个顶点视为一个代理，将多代理强化学习（MARL）应用于此类问题，但大多数基于MARL的模型都假定代理是独立的。在许多现实世界的任务中，代理需要作为一个群体，而不是作为一个个体的集合。 在本文中，我们提出了一个框架，通过底层网络连接的代理之间的合作和协调，在基于MARL的设置中使用突发通信。我们在一般的网络设置中提出问题，并在交通系统案例研究的帮助下证明了网络中通信的效用。此外，我们研究了突发通信协议，并展示了具有基础词汇的不同社区的形成。
我们介绍了卷积条件神经过程（ConvCNP），这是神经过程家族的一个新成员，对数据中的翻译等值进行建模。翻译等值是许多学习问题的一个重要归纳偏见，包括时间序列建模、空间数据和图像。该模型将数据集嵌入到一个无穷大的函数空间，而不是有限大的矢量空间。 为了正式确定这一概念，我们将集合的神经表征理论扩展到包括函数表征，并证明任何翻译等价嵌入都可以用卷积深度集来表示。我们在几个环境中评估了ConvCNPs，证明它们与现有的NPs相比达到了最先进的性能。我们证明，在翻译等价中建立能够对具有挑战性的域外任务进行零次概括。
经典模型将初级视觉皮层（V1）描述为方向选择的线性-非线性（LN）或能量模型的滤波器库，但这些模型未能准确预测对自然刺激的神经反应。最近的工作表明，卷积神经网络（CNN）可以被训练来更准确地预测V1活动，但除了方向选择和相位不变性之外，V1神经元提取的哪些特征仍然不清楚。 我们提出了一个框架，通过使用旋转等价卷积神经网络来识别独立于单个神经元方向选择性的共同特征，该网络能自动提取多个不同方向的每个特征。我们将这个旋转等价CNN适用于6000个神经元群体对小鼠初级视觉皮层使用双光子成像记录的自然图像的反应。 我们表明，我们的旋转等价网络优于具有相同数量特征图的普通CNN，并揭示了一些共同的特征，这些特征为许多V1神经元所共享，并被稀疏地汇集起来以预测神经活动。我们的发现是向研究视觉皮层非线性功能组织的强大新工具迈出的第一步。
在经典论文中，Zellner（1988年，2002年）证明了贝叶斯推断可以作为信息论函数的解决方案。 下面我们推导出这个函数的广义形式，作为预测性信息瓶颈目标的变异下限。 这个广义的函数包含了大多数现代推理程序，并提出了一些新的推理程序。
在许多应用中，最好只从复杂的输入数据中提取相关的信息，这涉及到决定哪些输入特征是相关的。信息瓶颈方法通过保持压缩（丢弃不相关的输入信息）和预测目标之间的最佳权衡，将其形式化为一个信息理论优化问题。 在许多问题环境中，包括我们在这项工作中考虑的强化学习问题，我们可能更愿意只压缩部分输入。这通常是当我们有一个标准的条件输入，如状态观察，和一个 "特权 "输入，这可能对应于一个任务的目标，一个昂贵的规划算法的输出，或与另一个代理人的沟通。在这种情况下，我们可能倾向于压缩特权输入，要么是为了实现更好的泛化（例如，关于目标），要么是为了尽量减少对昂贵信息的访问（例如，在通信的情况下）。基于变量推理的信息瓶颈的实际实现需要访问特权输入以计算瓶颈变量，因此尽管它们进行压缩，这种压缩操作本身需要不受限制的、无损的访问。我们为这个框架制定了一个可行的近似值，并在一系列强化学习实验中证明它可以提高泛化能力并减少对计算成本高的信息的访问。
呼吸练习是管理压力和许多精神疾病症状的一种方便的方法。传统上，学习呼吸练习需要亲自指导或录音。向移动设备的转变导致了一种学习和参与呼吸练习的新方式，从具有不同呼吸表征的多种移动应用程序的兴起可以看出。 我们利用一项主体内研究来评估四种常见的呼吸视觉，以了解哪种视觉在提供呼吸运动指导方面最有效。通过控制实验室研究和访谈，我们确定了两种具有明显优势的表现形式。此外，我们发现并非所有用户都喜欢听觉指导。我们确定了这些表现形式的潜在可用性问题，并为未来开发应用支持的呼吸训练提出了设计指南。
目前的神经代码合成工作包括在高度简化的特定领域语言上训练越来越复杂的架构，在这些语言的程序空间中使用统一的采样进行训练。相比之下，类似C语言的程序空间是巨大的，而且在 "有用 "的功能方面极其稀少；这需要一种更智能的方法来生成语料库以进行有效训练。 我们使用遗传编程的方法，使用反复重新训练的鉴别器来产生适合作为神经代码合成结构的标记训练数据的群体。我们证明，使用基于鉴别器的训练语料库生成器，仅使用经典的 "编程实例 "格式的未标记问题规格进行训练，与目前的统一采样技术相比，大大改善了网络性能。
神经阅读理解模型最近取得了令人印象深刻的生成结果，但在给定对抗性选择的输入时仍然表现不佳。大多数先前的工作研究了语义不变的文本扰动，导致模型的预测在它不应该改变时发生变化。在这项工作中，我们专注于互补的问题：过度的预测不敏感，输入文本被有意义地改变，而模型的预测在它应该改变时没有改变。 我们提出了一种噪音对抗性攻击，在理解性问题的语义变化中进行搜索，对于这些变化，模型仍然错误地得出与原始问题相同的答案--而且是以更高的可能性。 0和NewsQA模型很容易受到这种攻击，并且在对抗性生成的问题上犯了相当大的错误。这表明目前的模型--即使它们能够正确地预测答案--依赖于虚假的表面模式，并且不一定知道特定理解问题中提供的所有信息。 最后，我们证明，在训练/评价分布不匹配的情况下，对抗性强的模型在有偏见的数据环境中表现得更好；它们不容易过度依赖只存在于训练集的预测线索，在有偏见的数据环境中比传统模型的表现高出11% F1。
本文提出了一种新的文本张量表示法，它依靠信息压缩技术为最常用的字符分配较短的代码。这种表示法是独立于语言的，不需要预训练，产生的编码没有信息损失。它对文本的形态进行了充分的描述，因为它能够用类似的向量表示前缀、后缀和转折，甚至能够表示训练数据集上未见过的词。 同样，由于它是紧凑而稀疏的，是使用张量处理库加快训练时间的理想选择。作为本文的一部分，我们表明这种技术在与卷积神经网络（CNN）结合用于字符级的文本分类时特别有效。实验结果表明，它极大地减少了需要优化的参数数量，仅用一热编码表示的一小部分时间就获得了有竞争力的分类精度值，从而能够在商品硬件中进行训练。
序列预测模型可以通过各种训练算法从实例序列中学习。最大似然学习简单而高效，但在测试时可能会出现复合误差。强化学习，如策略梯度，解决了这个问题，但可能有令人望而却步的探索效率。其他丰富的算法，如数据噪声、RAML和softmax策略梯度，也从不同角度被开发出来。在本文中，我们提出了一个熵正则化策略优化的形式主义，并表明包括MLE在内的明显不同的算法可以被重新表述为该形式的特殊实例。它们之间的差异由奖励函数和两个权重超参数来表征。 统一的解释使我们能够系统地并排比较这些算法，并对算法设计的权衡有了新的认识。新的观点也导致了一种改进的方法，即在算法家族中动态插值，并以预定的方式学习模型。在机器翻译、文本总结和游戏模仿学习方面的实验证明了所提出的方法的优越性。
起点-终点（OD）流量数据是交通研究中的一个重要工具。鉴于一系列先前的快照，精确预测从每个原始地点到目的地的客户需求有助于共享汽车平台更好地了解他们的市场机制。 在本文中，我们提出了一个潜在的空间-时间原点-目的地（LSTOD）模型，用一个新颖的卷积神经网络（CNN）滤波器从图的角度学习OD对的空间特征，用注意力结构来捕捉它们的长期周期性。在一个真实的客户请求数据集上的实验表明，LSTOD的优势在于比第二好的模型在预测准确性上至少提高了6.5%。
对抗性训练已被证明是训练稳健模型以抵御对抗性例子的最有效方法之一。然而，对抗性训练的模型在未见过的测试数据上往往缺乏对抗性的稳健概括。最近的工作表明，对抗性训练的模型更偏向于全局结构特征。 相反，在这项工作中，我们想研究对抗性训练的泛化和稳健的局部特征之间的关系，因为稳健的局部特征对未见的形状变化有很好的泛化作用。为了学习稳健的局部特征，我们开发了一个随机块洗牌（RBS）变换，在正常对抗性例子上打破全局结构特征。 我们继续提出了一种新的方法，称为Robust Local Features for Adversarial Training (RLFAT)，它首先通过对RBS变换的对抗性例子进行对抗性训练来学习鲁棒的局部特征，然后将鲁棒的局部特征转移到正常对抗性例子的训练中。为了证明我们论点的普遍性，我们在目前最先进的对抗性训练框架中实施RLFAT。 在STL-10、CIFAR-10和CIFAR-100上的广泛实验表明，RLFAT极大地改善了对抗训练的对抗性鲁棒概括和标准概括。此外，我们证明了我们的模型捕捉了图像上物体的更多局部特征，与人类的感知更好地保持一致。
规划领域模型的验证对于确保基于规划的自动化系统的安全性、完整性和正确性至关重要。这项任务通常使用模型检查技术进行。 然而，直接应用模型检查器来验证规划域模型可能会导致假阳性，即在规划任务中使用被验证的域时，健全的规划者无法达到的反例。在本文中，我们讨论了无约束的规划域模型验证的缺点。然后，我们提出了一种设计规划域模型的故障安全做法，可以在域模型中未检测到错误的情况下内在地保证产生的计划的安全性。 此外，我们展示了模型检查器以及状态轨迹约束规划技术应如何用于验证规划域模型，以便不返回不可达的反例。
这些年我们看到了图像生成模型的巨大成功。通过神经网络生成图像通常是基于像素的，这与人类使用画笔创作艺术品的方式有根本的不同。为了模仿人类的绘画，需要环境和代理之间的互动来允许试验。然而，环境通常是不可分的，导致缓慢的收敛和大量的计算。在本文中，我们试图用一个中间的、可分的模拟解决软件环境的离散性。 我们提出了StrokeNet，这是一个新颖的模型，代理在一个精心设计的绘画环境的神经近似上进行训练。通过这种方法，我们的代理能够以无监督的方式学习书写字符，如MNIST数字，比强化学习方法更快。我们的主要贡献是对真实世界环境的神经模拟。此外，用模拟环境训练的代理能够直接将其技能转移到真实世界的软件。
我们展示了机器学习是如何对量子物理学的实验进行建模的。量子纠缠是即将到来的量子技术的基石，如量子计算和量子密码学。特别感兴趣的是具有两个以上粒子和大量纠缠量子水平的复杂量子态。 在这项工作中，我们表明机器学习模型可以提供比随机搜索更重要的改进。我们证明了长短期记忆（LSTM）神经网络可以成功地学习量子实验模型，正确预测给定设置的输出状态特征，而不需要计算状态本身。
在本文中，我们提出了一个非线性无监督度量学习框架，以提高聚类算法的性能。在我们的框架下，非线性距离度量学习和流形嵌入被整合并同时进行，以增加数据样本之间的自然分离。度量学习部分是通过特征空间转换实现的，由一个称为相干点漂移（CPD）的非线性可变形模型调节。 在CPD的驱动下，数据点可以达到更高的线性可分离度，随后被流形嵌入组件拾取，以生成可分离的样本投影用于聚类。在合成和基准数据集上的实验结果表明，我们提出的方法比无监督度量学习的最先进解决方案更有效。
随着机器学习变得无处不在，部署的系统需要尽可能的准确。因此，机器学习服务提供商对有用的、有利于训练的额外训练数据的需求激增，而不需要放弃关于训练程序的所有细节。同时，数据所有者希望以其数据的价值进行交易，而不必在获得补偿之前首先放弃数据本身。 目前，数据所有者和模型所有者没有一个公平的定价系统，不需要信任第三方，也不需要在数据上训练模型，因为1）需要很长的时间来完成，2）不能保证有用的数据得到有价值的报酬，也不能保证无用的数据不得到报酬，如果不信任拥有模型和数据的第三方的话。 现有的保证交易安全的改进措施主要集中在加密或近似数据上，如在加密数据上进行训练，以及联合学习的变种。尽管这些方法看起来很强大，但我们发现它们在我们的用例中是不切实际的，因为在面对黑盒模型时，现实世界的假设是为数据所有者保留隐私。 因此，在数据交换之前，需要一个不依赖安全数据加密和混淆的公平定价方案。本论文提出了一种新的公平定价方法，使用数据模型功效技术，如影响函数、模型提取和模型压缩方法，从而实现安全数据交易。 我们成功地表明，在不通过模型运行数据的情况下，人们可以近似地评估数据的价值；也就是说，如果数据被证明是多余的，那么定价是最小的，如果数据导致了适当的改进，那么它的价值就会得到适当的评估，而不需要对模型的性质做强有力的假设。未来的工作将集中在建立一个具有更强交易安全性的系统，以对抗将向对方透露模型或数据细节的对抗性攻击。
我们提出了Line-Storm，一个用于创造性表演的交互式计算机系统。我们调查的背景是使用Line-Storm在纸上写作。我们使用自我报告问卷作为涉及人类参与者的研究的一部分，来评估Line-Storm。Line-Storm包括一个书写笔和写字板，用电子装置来增强。 我们试图发现Line-Storm是否增强了自我报告的在写作任务中的存在感和参与感，并且我们将Line-Storm与非交互式控制条件进行了比较。在SPSS中进行统计分析后，我们无法支持我们的研究假设，即Line-Storm增强了存在感和参与感。 由于创造力是微妙的，并且随着人、时间、环境、空间和其他许多因素的变化而变化，这个结果在某种程度上是我们所期望的。我们研究的一个具有统计学意义的结果是，一些参与者对Line-Storm的反应比其他人更积极。这些Line-Storm的保存者是一个群体，与其他参与者不同，他们报告了更多的存在和参与，他们在Line-Storm和控制条件下写了更多的字。 我们讨论了我们的研究结果，并将Line-Storm置于艺术-技术背景中，在考虑Line-Storm的性质时，借鉴了马丁-海德格尔的著作。未来的工作包括修改互动组件，改善美学和使用更小型化的电子产品，尝试用绘画任务代替写作任务，以及与电子音乐作曲家合作，为写作或绘画表演制作更有趣、沉浸式和吸引人的互动声景。
语言风格转移是将源句子的内容迁移到目标风格的问题。在许多应用中，没有平行的训练数据，要转移的源句子可能有任意的和未知的风格。在本文中，我们提出了这种问题设置下的编码器-解码器框架。每个句子被编码为其内容和风格的潜在表示。通过将内容与目标风格重新组合，我们可以解码一个在目标领域对齐的句子。 第一个是风格差异损失，强制要求风格表示准确地编码由句子风格和目标风格之间的差异引导的风格信息。第二个是循环一致性损失，确保转移的句子应该保留与风格分离的原句的内容。我们在两个任务上验证了我们提出的模型的有效性：餐厅评论的情感修改和具有浪漫风格的对话回应修改。
在本文中，我们提出了一个新的损失函数，用于使用线性自动编码器（LAE）进行主成分分析（PCA）。优化标准的L2损失导致解码器矩阵跨越数据样本协方差的主子空间，但无法识别准确的特征向量。这个缺点源于全局图中抵消的不变性。 对于这个新的损失，我们确定所有的局部最小值都是全局最优值，并且还表明计算新的损失（以及它的梯度）具有与经典损失相同的复杂性。我们报告了合成模拟和MNIST（即60,000 x784矩阵）的真实数据PCA实验的数值结果，证明了我们的方法是实际适用的，并纠正了以前LAE的缺点。
用户在帮助构建和维护知识库（KBs）方面具有巨大的潜力，他们通过反馈来识别不正确和缺失的实体属性和关系。然而，随着新数据被添加到知识库中，通过运行实体解析（ER）构建的知识库实体可能会发生变化，使用户反馈的预期目标变得未知--我们称之为身份不确定性。 在这项工作中，我们提出了一个在身份不确定的情况下将用户反馈整合到KB中的框架。我们的方法是基于让用户反馈与ER中的提及一起参与。我们提出了一种具体的用户反馈表示方法，即反馈提及，并引入了一种新的在线算法，将这些提及整合到现有的KB中。在实验中，我们证明我们提出的方法在70%的实验条件下优于基线。
机器学习算法很容易受到中毒攻击。对手可以在训练数据集中注入恶意点，以影响学习过程并降低算法的性能。最佳中毒攻击已经被提出来评估最坏的情况，将攻击建模为一个双级优化问题。我们提出了一个生成式对抗网，它有三个组成部分：生成器、判别器和目标分类器。这种方法使我们能够自然地模拟现实攻击中可预期的可检测性约束，并确定基础数据分布中更容易受到数据中毒的区域。
期望最大化（EM）算法是无监督机器学习的一个基本工具。它经常被用作解决最大似然（ML）和最大后验估计问题的有效方法，特别是对于有潜伏变量的模型。它也是适合混合模型的首选算法：代表来自$k$不同过程的无标签点的生成模型，作为$k$多元分布的样本。在这项工作中，我们定义并使用EM的量子版本来适合高斯混合模型。 考虑到量子访问尺寸为$d$的$n$向量数据集，我们的算法具有与经典算法类似的收敛性和精度保证，但运行时间仅是训练集中元素数量的多项式，并且是其他参数的多项式，如特征空间的尺寸和混合物中的成分数量。 我们通过拟合指数族中任何基数分布的混合模型来进一步推广该算法。我们讨论了该算法在那些有望被这些算法成功分类的数据集上的表现，认为在这些情况下我们可以对运行时间给予强有力的保证。
我们提出了一种新的方法，称为迭代正则化双重平均法（iRDA），以提高卷积神经网络（CNN）的效率，大大减少模型的冗余而不降低其准确性。 该方法已经针对各种数据集进行了测试，并被证明比深度学习文献中大多数现有的压缩技术明显更有效率。 对于许多流行的数据集，如MNIST和CIFAR-10，超过95%的权重可以被清零而不损失精度。特别是，我们能够使具有95%稀疏度的ResNet18的精度与文献中报道的具有最佳60%稀疏度的更大模型ResNet50的精度相当。
基于行为克隆（BC）的监督学习方法受到分布偏移的影响：由于代理人贪婪地模仿示范动作，它可能会因为错误积累而偏离示范状态。 最近基于强化学习（RL）的方法，如逆向RL和生成对抗性模仿学习（GAIL），通过训练RL代理来克服这个问题，使其在较长的范围内与演示相匹配。由于任务的真正奖励函数是未知的，这些方法从演示中学习奖励函数，通常使用复杂和脆弱的近似技术，涉及对抗性训练。 关键的想法是通过鼓励代理人在遇到新的、非分布状态时返回到已演示的状态，为代理人提供激励，使其在较长的时间内与演示相匹配。我们通过给代理人一个恒定的奖励r=+1来实现在已演示状态下与已演示的行动相匹配，给所有其他行为一个恒定的奖励r=0。 理论上，我们表明SQIL可以被解释为BC的正则化变体，它使用稀疏性先验来鼓励长视距模仿。实证上，我们表明SQIL优于BC，与GAIL相比，在Box2D、Atari和MuJoCo的各种基于图像和低维的任务上取得了有竞争力的结果。本文是一个概念证明，说明了基于RL的简单模仿方法与使用学习奖励的更复杂方法一样有效。
从高维数据中产生可视化和解释是许多领域的一个常见问题。解决这个问题的两个关键方法是聚类和表示学习。一方面有非常高性能的深度聚类模型，另一方面有可解释的表示学习技术，通常依赖于潜在的拓扑结构，如自组织地图。 我们提出了一个用于概率聚类的新的深度架构VarPSOM，以及它对时间序列数据的扩展VarTPSOM，由LSTM单元连接的VarPSOM模块组成。我们表明，与目前的深度聚类方法相比，它们在静态MNIST/Fashion-MNIST数据以及医学时间序列上实现了卓越的聚类性能，同时引起了可解释的表示。
许多计算机视觉应用需要实时解决多个任务。一个神经网络可以通过 "多任务学习 "被训练来同时解决多个任务，这可以节省推理时间的计算，因为只需要评估一个网络。 我们系统地研究了任务的合作和竞争，并提出了一个将任务分配给几个神经网络的框架，使合作的任务由同一个神经网络计算，而竞争的任务由不同的网络计算。我们的框架提供了一个时间-准确性的权衡，不仅比单一的大型多任务神经网络，而且比许多单任务网络都能用更少的推理时间产生更好的准确性。
搜索引擎已经成为各种网络和移动应用的基本组成部分。从海量数据集中检索相关文件对搜索引擎系统来说是一个挑战，特别是在面对冗长或尾部查询时。在本文中，我们探索了一个用于文件检索的向量空间搜索框架。 具体来说，我们训练了一个深度语义匹配模型，这样每个查询和文档都可以被编码为低维嵌入。我们的模型是基于BERT架构训练的。我们部署了一个快速的k-近邻索引服务用于在线服务。
半监督学习（SSL）方法一直是一个有影响力的框架，当训练过程中没有足够数量的标记数据时，可以使用未标记的数据。基于卷积神经网络（CNN）的SSL方法最近在标准基准任务（如图像分类）上提供了成功的结果。 我们提出了一种新的方法，采用最优传输（OT）技术作为离散经验概率度量之间的相似性指标，为未标注数据提供伪标签，然后可以与初始标注数据一起使用，以SSL的方式训练CNN模型。我们在标准数据集上对我们提出的方法与最先进的SSL算法进行了评估和比较，以证明我们SSL算法的优越性和有效性。
批量规范化（batch norm）经常被用来稳定和加速深度神经网络的训练。在许多情况下，它确实减少了实现低训练误差所需的参数更新数量。然而，它也降低了对小的对抗性输入扰动和噪声的鲁棒性，达到两位数的百分比，正如我们在五个标准数据集上所显示的。 此外，用权重衰减代替批量规范足以使对抗性弱点和输入维度之间的关系失效。我们的工作与均值场分析一致，该分析发现批量规范会导致爆炸性梯度。
本文介绍了我们在非特异性领域中对分层目标网络进行自动学习的初步想法。我们目前正在实施本文所表达的想法。
深度神经网络在有大量数据的情况下表现出色，但当数据稀少或需要快速适应任务变化时，往往会陷入困境。作为回应，最近的元学习工作提出在类似任务的分布上训练元学习者，希望通过学习捕捉问题本质的高级策略，将其推广到新的但相关的任务。 然而，最近的许多元学习方法都是大量手工设计的，要么使用专门针对某一特定应用的架构，要么硬编码算法组件来限制元学习器如何解决任务。我们提出了一类简单和通用的元学习器架构，使用时间卷积和软注意力的新组合；前者从过去的经验中聚集信息，后者则精确地确定特定的信息。 在迄今为止最广泛的元学习实验中，我们对所产生的简单神经注意力学习器（或称SNAIL）在几个严重的基准任务中进行评估。 在所有的任务中，无论是监督学习还是强化学习，SNAIL都以显著的优势达到了最先进的性能。
近年来，知识图谱嵌入（KGE）吸引了更多的关注。大多数KGE模型都是从无时间意识的三元组中学习的。然而，在三元组旁边加入时间信息将进一步提高KGE模型的性能。 在这方面，我们提出了LiTSE，一个时间性的KGE模型，它通过使用线性时间序列分解将时间信息纳入实体/关系表示。此外，考虑到实体/关系表示随时间演变过程中的时间不确定性，我们将时间性KGs的表示映射到多维高斯分布空间。 实验表明，LiTSE不仅在时态KG的链接预测方面达到了先进水平，而且有能力预测缺少时间注释的事实的发生时间，以及未来事件的存在。
我们研究了强化学习代理中合作行为的出现，引入了一个具有挑战性的多代理足球竞争环境，并连续模拟了物理学。我们证明了分散的、基于群体的训练与共同游戏可以导致代理行为的进步：从随机到简单的追球，并最终显示出合作的证据。 我们的研究强调了在连续控制的大规模多代理训练中遇到的几个挑战。特别是，我们证明了简单的塑造奖励的自动优化，本身并不利于合作行为，可以导致长期的团队行为。我们进一步应用一个评估方案，以博弈论原理为基础，可以在没有预先定义的评估任务或人类基准的情况下评估代理的表现。
程序合成是自动生成符合规范的程序的任务。近年来，人们提出了许多程序合成的神经方法，其中许多采用了类似于神经机器翻译的序列生成范式，其中序列到序列模型被训练为最大化已知参考程序的可能性。 为了解决第一个限制，我们在一个有监督的模型上进行强化学习，其目标是明确地使生成语义正确的程序的可能性最大化。 为了解决第二个限制，我们引入了一个训练程序，直接最大化生成符合规范的语法正确程序的可能性。
时间序列预测在市场营销、金融和许多其他定量领域发挥着重要作用。在这个问题上已经开发了大量的方法，包括ARIMA、Holt-Winters等。 为了推断所有的隐藏变量，我们开发了一个贝叶斯框架，它能够获得时间序列预测的分布和预测区间，并具有可证明的理论属性。为了实施，我们提出了一个带有马尔科夫链蒙特卡洛（MCMC）、卡尔曼滤波和卡尔曼平滑的迭代算法。在合成数据和真实数据应用中，与现有方法相比，我们的方法在时间序列预测中产生了更好的性能，同时也有更准确的变化点检测和异常检测。
我们周围的复杂世界本质上是多模态和顺序的（连续的）。信息分散在不同的模态中，需要多个连续的传感器来捕获。随着机器学习向更好地概括现实世界的方向跃进，多模态顺序学习成为一个基本的研究领域。 在本文中，我们提出了一个新的变换器模型，称为因子化多模态变换器（FMT），用于多模态顺序学习。FMT在其多模态输入中以因子化的方式对模内和模间（涉及两个或多个模态）动态进行固有的建模。 所提出的因子化允许增加自我注意的数量，以更好地模拟手头的多模态现象；即使在相对较低的资源设置上，也不会在训练中遇到困难（如过度拟合）。FMT中的所有注意机制都有一个完整的时域接受场，这使它们能够异步地捕捉长距离的多模态动态。 在我们的实验中，我们专注于包含语言、视觉和声学这三种常见研究模式的数据集。我们进行了广泛的实验，跨越了3个研究良好的数据集和21个不同的标签。FMT显示出比以前提出的模型更优越的性能，在研究的数据集中设置了新的技术状态。
我们开发了一种新的高效的神经网络优化算法，其灵感来自于最近提出的测地线优化算法。我们的算法，我们称之为随机测地线优化（SGeO），在Polyak的重球法基础上利用了一个自适应系数，有效地控制了基于优化路径方向变化的参数的先前更新的权重。 对具有Lipschitz梯度的强凸函数和深层自动编码器基准的实验结果表明，SGeO达到了比既定的一阶方法更低的误差，并且与最近的二阶方法K-FAC（Kronecker-Factored Approximate Curvature）竞争，误差更低或相似。我们还将Nesterov风格的lookahead梯度纳入我们的算法（SGeO-N）并观察到明显的改进。
我们引入了掩码翻译模型（MTM），它将序列的编码和解码结合在同一个模型组件中。MTM基于掩码语言建模的思想，通过简单地改变掩码的顺序，支持自回归和非自回归的解码策略。在WMT 2016罗马尼亚语-英语任务的实验中，MTM显示出强大的恒定时间翻译性能，击败了所有具有可比复杂性的相关方法。
我们提出了局部合集，这是一种在测试时检测预训练模型的外推的方法。我们专注于作为外推的一个关键组成部分的欠确定：我们的目标是在许多可能的预测与训练数据和模型类别一致时进行检测。我们的方法使用局部二阶信息来近似同一类别的模型合集中预测的方差。 我们通过估计与Hessian的低曲率方向一致的测试点梯度分量的规范来计算这个近似值，并提供一个估计这个数量的可行方法。实验表明，我们的方法能够检测出预训练模型在测试数据上的推断，并应用于分布外检测、检测虚假的相关因素和主动学习。
联盟行动对于应对越来越多的需要大规模人道主义援助的世界性事件至关重要。许多国家和非政府组织经常协调解决这些问题，但他们的合作往往受到他们能够分享的信息的限制。在本文中，我们考虑使用一种先进的加密技术，称为安全多方计算，使联盟成员能够实现联合目标，同时仍然满足隐私要求。我们特别关注的是一个多国援助交付调度任务，该任务涉及协调各种援助提供者国家在自然灾害发生后何时何地交付救援物资。即使使用了安全的多方计算技术，私人数据的信息也会泄露。 我们描述了如何利用新兴的定量信息流领域来帮助数据所有者了解私人数据在多大程度上可能因可能的或实际的调度操作而变得脆弱，并使调度过程的自动调整能够确保隐私要求。
学习离散表征的挑战最近引起了很多人的注意，归结为一场竞争。已经提出了各种基于变异自动编码器的方法来解决这个问题，通过强制表征之间的独立性和修改变异下限中的正则化项，然而最近Locatello等人（2018）的工作表明，提出的方法受随机性和超参数的选择影响很大，这项工作是建立在第一阶段（Li et al, 2019年）中的相同框架，但设置不同；为了使其自成一体，我们提供了这份手稿，它不可避免地与第一阶段的报告非常相似。详细地说，在这项工作中，我们没有设计一个新的正则化项，而是采用了FactorVAE，但改进了重建性能，增加了网络和训练步骤的容量。"事实证明，该策略在实现解缠方面非常有效。
我们为澄清问题的生成提出了一种生成式对抗训练方法。我们的方法生成澄清问题的目的是激发新的信息，使给定的语境更加完整。我们开发了一个生成式对抗网络（GAN），其中生成器是一个序列到序列的模型，判别器是一个效用函数，用来模拟用澄清问题的答案更新语境的价值。 我们在两个数据集上进行了评估，使用了自动指标和人类对有用性、特异性和相关性的判断，表明我们的方法优于基于检索的模型和排除了效用模型和对抗性训练的消融。
模式数据库是一些最强的可接受启发式最优经典规划的基础。实验表明，结合多个模式数据库信息的最有信息量的方式是使用饱和成本分区。以前的工作在两个独立的步骤中选择模式和计算所产生的模式数据库启发式的饱和成本分区。
关于神经机器翻译的最先进的结果通常使用具有某种形式的卷积或递归的注意力序列到序列模型。Vaswani等人（2017）提出了一个新的架构，完全避免了递归和卷积，相反，它只使用自我注意力和前馈层。 我们提出了Weighted Transformer，一个带有修改过的注意力层的Transformer，它不仅在BLEU分数上优于基线网络，而且收敛速度也快了15-40%。具体来说，我们用模型在训练过程中学习组合的多个自注意力分支取代了多头注意力。我们的模型在WMT 2014英德翻译任务上的最先进性能提高了0.5个BLEU点，在英法翻译任务上提高了0.4。
模仿学习为自主控制提供了一个有吸引力的框架：在许多任务中，可以很容易地从人类专家那里获得首选行为的示范，从而消除了在现实世界中进行昂贵的、有潜在危险的在线数据收集的需要。然而，用模仿学习学到的策略在测试时适应不同目标的灵活性有限。 基于模型的强化学习（MBRL）提供了相当大的灵活性，因为从数据中学习的预测模型可以用来在测试时实现各种目标。然而，MBRL有两个缺点。首先，模型不能帮助选择期望的或安全的结果--它的动力学只估计可能的结果，而不是首选的结果。 第二，MBRL通常需要额外的在线数据收集，以确保模型在试图实现测试时间目标时实际遇到的那些情况下是准确的。在本文中，我们旨在结合模仿学习和MBRL的优点，并提出模仿模型：能够计划专家般的轨迹以实现任意目标的概率预测模型。 我们发现这种方法在模拟的自动驾驶任务中大大优于直接模仿和MBRL，并且可以从固定的专家示范中有效地学习，而不需要额外的在线数据收集。我们还表明我们的模型可以在测试时间灵活地纳入用户提供的成本，可以计划目标序列，甚至可以在不精确的目标下表现良好，包括在道路错误一侧的目标。
不确定性估计和合集方法是相辅相成的。不确定性估计是评估合集性能的主要基准之一。同时，深度学习合集在不确定性估计方面提供了最先进的结果。在这项工作中，我们专注于图像分类的域内不确定性。 为了避免这些陷阱，我们对不同的合集技术进行了广泛的研究。为了在广泛的比较中提供更多的洞察力，我们引入了深度合集等价物（DEE），并表明许多复杂的合集技术在测试对数似然方面等同于极少数独立训练的网络的合集。
尽管迄今为止开发的大多数技术都需要对机器学习模型的架构有所了解，并且仍然难以扩展到复杂的预测管道，但随机平滑的方法已经被证明可以克服许多这些障碍。由于只需要对底层模型进行黑盒访问，随机平滑可以扩展到大型架构，并且对网络的内部结构不了解。 然而，过去关于随机平滑的工作主要集中在平滑措施或扰动的有限类别（如高斯或离散），并且只能证明与简单规范界限有关的鲁棒性。在本文中，我们引入了一个通用框架，用于证明黑盒设置中平滑机器学习模型的鲁棒性属性。 我们的方法在MNIST、CIFAR-10和ImageNet以及音频分类任务Librispeech上，针对几类对抗性扰动，实现了最先进的}认证的鲁棒性。
将复杂的多物体场景分解成有意义的抽象概念（如物体）的能力是实现更高层次认知的基础。以前的无监督的面向物体的场景表示学习方法要么是基于空间注意力或场景混合方法，可扩展性有限，这是实现真实世界场景建模的主要障碍。在本文中，我们提出一个生成的潜在变量模型，称为SPACE，它提供了一个统一的概率建模框架，结合了空间注意力和场景混合方法的优点。 SPACE可以明确地为前景物体提供因子化的物体表征，同时也可以分解复杂形态的背景片段。以前的模型擅长其中之一，但不擅长两者。SPACE还通过纳入平行空间注意力解决了以前方法的可扩展性问题，因此适用于具有大量物体的场景而不会出现性能下降。我们通过在Atari和3D-Rooms的实验表明，与SPAIR、IODINE和GENESIS相比，SPACE实现了上述特性的一致性。实验结果可以在我们的项目网站找到： https://sites.google.com/view/space-project-page
我们提出了一个基于变异自动编码器的单一神经概率模型，该模型可以以观察到的特征的任意子集为条件，然后对剩余的特征进行 "一次性 "采样。特征既可以是实值的，也可以是分类的。
我们探索了高效的神经架构搜索方法，并表明一个简单而强大的进化算法可以发现具有优异性能的新架构。我们的方法结合了一个新的分层遗传表示方案，模仿人类专家通常采用的模块化设计模式，以及一个支持复杂拓扑结构的表达式搜索空间。 我们的算法有效地发现了优于大量手工设计的图像分类模型的架构，在CIFAR-10上获得了3.6%的最高误差，在转移到ImageNet上时获得了20.3%的最高误差，这与现有的最好的神经架构搜索方法相比具有竞争力。我们还展示了使用随机搜索的结果，在CIFAR-10上获得的最高误差减少0.3%，在ImageNet上减少0.1%，同时搜索时间从36小时减少到1小时。
在视觉规划（VP）中，代理人通过离线获得的对动态系统的观察来学习规划目标导向的行为，例如。VP算法本质上结合了数据驱动的感知和规划，对机器人操纵和导航等领域非常重要。最近一种有前途的VP方法是半参数拓扑记忆（SPTM）方法，其中图像样本被视为图中的节点，并使用深度图像分类学习图中的连接。 因此，学到的图代表了数据的拓扑连接性，并且可以使用传统的图搜索方法进行规划。然而，训练SPTM需要为连接性分类器提供一个合适的损失函数，这需要进行非繁琐的人工调整。 更重要的是，SPTM对领域变化的概括能力受到限制，因为它的图是由直接观察构建的，因此需要收集新的样本进行规划。在本文中，我们提出了幻觉拓扑记忆（HTM），它克服了这些缺点。 此外，我们学习了一个有条件的VAE模型，该模型在给定领域的背景图像的情况下生成样本，并使用这些幻觉样本来构建连接图，允许对领域的变化进行零次泛化。在模拟领域中，HTM在计划质量和长距离规划的成功方面都优于传统的SPTM和视觉预见方法。
深度神经网络（DNN）近年来蓬勃发展，其中批量归一化（BN）发挥了不可或缺的作用。然而，据观察，由于还原操作，BN的成本很高。在本文中，我们建议通过在每次迭代中只使用一小部分数据进行均值和方差估计来减轻BN的成本。 实现这一目标的关键挑战是如何在规范化的有效性和执行效率之间取得令人满意的平衡。我们发现，有效性期望减少数据的相关性，而效率则期望有规律的执行模式。为此，我们提出了两类方法：抽样或创建少数不相关的数据用于统计估计，并有一定的策略约束。 前者包括从每批样本中随机选择少数样本的 "批量采样（BS）"和从所有样本的每个特征图中随机选择一小块的 "特征采样（FS）"，后者是生成少数合成随机样本的 "虚拟数据集归一化（VDN）"。相应地，多途径的策略被设计用来减少数据的相关性，以实现准确的估计，同时优化执行模式以实现运行加速。 所有提出的方法都在各种DNN模型上进行了综合评估，在现代GPU上，无需任何专业库的支持，就可以实际实现高达21.7%的整体训练速度，而且模型精度和收敛率的损失可以忽略不计。此外，我们的方法在解决著名的 "微批归一化 "问题时，在小批尺寸的情况下表现出强大的性能。
联合学习允许边缘设备协作学习一个共享模型，同时将训练数据保留在设备上，将模型训练的能力与在云端存储数据的需求解耦。我们提出了联合匹配平均（FedMA）算法，该算法设计用于现代神经网络架构的联合学习，例如卷积神经网络（CNN）和LSTMs。 FedMA通过对具有类似特征提取特征的隐藏元素（即卷积层的通道；LSTM的隐藏状态；全连接层的神经元）进行匹配和平均，以逐层的方式构建共享的全局模型。我们的实验表明，FedMA在现实世界数据集上训练的深度CNN和LSTM架构上优于流行的最先进的联合学习算法，同时提高通信效率。
我们提出了SOSELETO（SOurce SELEction for Target Optimization），一种利用源数据集来解决目标数据集上的分类问题的新方法。 SOSELETO基于以下简单的直觉：对于目标问题，一些源样本比其他样本更有信息量。 为了抓住这个直觉，源样本都被赋予了权重；这些权重是通过一个二层的优化方案与源和目标分类问题共同解决的。 因此，目标可以选择对自己的分类任务最有参考价值的源样本。 此外，优化的二层性质在目标上起到了一种正则化的作用，缓解了过度拟合。 SOSELETO可以应用于传统的迁移学习，以及在有噪声标签的数据集上的训练问题；我们展示了在这两个问题上的最新结果。
使用信任区方法的无派生优化（DFO）经常被用于机器学习应用，例如在不知道目标函数导数的情况下进行（超）参数优化。 受最近连续时间最小化器工作的启发，我们的工作将常见的信任区方法与探索-开发结合起来，使用一对动态过程的动态系统。当第一个探索过程通过最小化时间演化的代用函数来搜索黑箱函数的最小值时，另一个开发过程使用探索过程所穿越的点来逐时更新代用函数。 在本文中，我们提出了一个新的动力系统，即ThePrev---\underline{S}tochastic\underline{H}amiltonian\underline{E}xploration和\underline{E}xploitation，它使用一个时间演化的二次函数来代理黑箱函数的子区域，然后使用一个快速转换的汉密尔顿系统探索并跟踪二次函数的最小值。 为了进一步加速优化，我们提出了并行化多个线程的TheName\，用于并发探索和利用。基于广泛的机器学习应用的实验结果表明，在相同的设置下，TheName\以更快的收敛速度优于一系列无导数优化算法。
 二值化神经网络（BNN）已被证明可以在网络训练后的推理阶段有效地提高网络效率。然而，BNN只是在传播过程中对模型参数和激活进行二值化。因此，BNN在训练过程中并没有提供明显的效率改进，因为梯度仍然被传播并以高精度使用。  我们表明，使用 "二值化反向传播"（BBP）来训练BNN并不存在固有的困难，其中我们也对梯度进行了二值化处理。为了避免测试精度的显著下降，我们只需增加每个卷积层的滤波图数量。 在专用硬件上使用BBP有可能显著提高执行效率（例如，减少动态内存占用、内存带宽和计算能量），并在适当的硬件支持下加快训练过程，即使在网络规模增加后也是如此。此外，我们的方法是分布式学习的理想选择，因为它大大降低了通信成本（例如。使用这种方法，我们证明了在几个数据集和拓扑结构上分类精度的最小损失。
深度神经网络的权重初始化和激活函数对训练程序的性能有至关重要的影响。不恰当的选择会导致前向传播过程中输入信息的丢失，以及反向传播过程中梯度的指数消失/爆炸。 了解未经训练的随机网络的理论属性是确定哪些深度网络可能被成功训练的关键，最近Schoenholz等人（2017）证明了这一点，他们表明对于深度前馈神经网络，只有被称为 "混乱边缘 "的超参数的特定选择可以导致良好的性能。 我们通过提供定量结果来完成这一分析，表明对于一类类似ReLU的激活函数，信息传播确实比在混沌边缘的初始化要深。通过进一步扩展这一分析，我们确定了一类激活函数，它们比类似ReLU的函数改善了信息传播。 这类函数包括Hendrycks & Gimpel（2016）、Elfwing等（2017）和Ramachandran等（2017）中使用的Swish激活，$phi_{swish}(x) = x\cdot text{sigmoid}(x)$，这为这些贡献中观察到的$phi_{swish}$的优秀经验性能提供了理论基础。
最近LSTM成功背后的驱动力是它们学习复杂和非线性关系的能力。因此，我们无法描述这些关系，导致LSTM被定性为黑盒子。为此，我们引入了上下文分解（CD），这是一种分析标准LSTM做出的单个预测的解释算法，不需要对基础模型进行任何改变。 通过分解LSTM的输出，CD捕捉到了单词或变量的组合对LSTM最终预测的贡献。在用Yelp和SST数据集进行情感分析的任务中，我们表明CD能够可靠地识别情感对比的单词和短语，以及它们是如何组合以产生LSTM的最终预测的。使用SST中的短语级标签，我们还表明CD能够成功地从LSTM中提取正面和负面的否定，这在以前是没有的事。
大多数基于深度学习的语音增强模型主要集中在估计频谱的幅度，同时重新使用噪声语音的相位进行重建。这是因为估计干净语音的相位很困难。 首先，我们提出了Deep Complex U-Net，这是一种先进的U-Net结构模型，包含了定义明确的复值构件，以处理复值谱图。第二，我们提出了一种极坐标式复值掩码方法，以反映复值理想比掩码的分布。 第三，我们定义了一个新的损失函数，加权源失真比（wSDR）损失，其目的是与定量评价指标直接相关。我们的模型在语音银行语料库和DEMAND数据库的混合数据上进行了评估，该数据库已被许多深度学习模型广泛用于语音增强。在混合数据集上进行了消融实验，显示所有三个提出的方法在经验上是有效的。实验结果显示，提出的方法在所有指标上取得了最先进的性能，以较大的幅度超过了以前的方法。
所有的生物体都在与自然界的力量作斗争，以开辟出它们可以保持相对稳定的壁龛。我们提出，这种在混乱中寻找秩序的做法可能为人工代理中有用行为的出现提供一个统一的原则。我们将这一想法正式纳入一种无监督的强化学习方法，称为惊喜最小化RL（SMiRL）。 SMiRL训练一个代理，其目标是在一个对所有先前看到的状态进行训练的模型下使观察到的状态的概率最大化。由此产生的代理获得了一些主动行为，以寻求和维持稳定的状态，如平衡和避免损害，这些行为与环境的承受力及其普遍的熵源密切相关，如风、地震和其他代理。我们证明，我们的惊喜最小化代理可以成功地玩俄罗斯方块、末日游戏，并控制一个人形物体避免跌倒，而不需要任何特定的任务奖励监督。 我们进一步表明，SMiRL可以作为一个无监督的预训练目标，大大加速了后续的奖励驱动学习。
一个流行的元学习方法是训练一个循环模型来读取训练数据集作为输入，并输出所学模型的参数，或输出对新测试输入的预测。另外，一个更近的元学习方法旨在获得深度表征，这些表征可以通过标准梯度下降有效地微调到新的任务。 在本文中，我们从普遍性的角度来考虑元学习问题，将学习算法近似的概念形式化，并将上述递归模型的表达能力与最近将梯度下降嵌入元学习器的方法进行比较。 特别是，我们试图回答以下问题：深度表征与标准梯度下降相结合，是否有足够的能力来逼近任何学习算法？我们发现这确实是真的，并进一步发现，在我们的实验中，基于梯度的元学习始终导致学习策略，与那些由递归模型表示的学习策略相比，泛化得更广泛。
脑机接口（BCI）可帮助因神经退行性疾病而交流能力减弱的患者通过直接的神经处理产生文本或语音。然而，由于现有接口的速度、准确性和通用性的限制，其实际实现已被证明是困难的。 这些频带形成了一个特征集，输入到一个LSTM中，该LSTM在每个时间点识别受试者说出的所有音素的概率分布。最后，一个粒子过滤算法在时间上平滑这些概率，结合英语语言的先验知识，输出与解码词相应的文本。 此外，与以前的研究不同，在产生输出时，我们避免将重建的单词限制在一个给定的词包中。我们提出的方法的经验性成功，为病人在不受约束的自然环境中使用这种界面提供了希望。
特征提取的迁移学习可用于在训练数据极少、计算资源有限的情况下利用深度表征，或者当调整训练所需的超参数不是一种选择。 为了做到这一点，该嵌入方法在问题的背景下将特征归一化，并将它们的值离散化，以减少噪音和规范化嵌入空间。重要的是，这也减少了处理结果表示的计算成本。 所提出的方法在几个图像分类任务上的表现优于单层嵌入，同时对用于获得初始特征的预训练模型的选择更加稳健。彻底调整的解决方案和全网络嵌入之间的分类精度的性能差距也减少了，这使得所提出的方法成为大量应用中具有竞争力的解决方案。
我们提出了一种新的网络修剪算法，称为动态稀疏训练，它可以在一个统一的优化过程中，通过可训练的修剪阈值共同找到最佳的网络参数和稀疏的网络结构。 动态稀疏训练与其他稀疏训练算法相比，在各种网络架构上都达到了先有的性能。此外，我们有几个令人惊讶的观察，为我们算法的有效性和高效性提供了有力的证据。这些观察揭示了传统的三阶段修剪算法的潜在问题，并提出了我们的算法为设计更紧凑的网络架构提供的潜在指导。
近年来，一种流行的研究途径是直接将这种算法映射到现实的电路实现中。我们在这里关注递归网络中的学习，并研究一系列的学习算法。我们的方法是将它们分解成计算的构件，并讨论它们作为生物操作的抽象潜力。
近年来，人们提出了一些对抗性的攻击和防御措施。当更复杂的攻击被使用时，看似稳健的模型往往变成了非稳健的。 我们为ReLU网络提出了一种新的正则化方案，即MMR-Universal，它能在l_1$-\textit{和}$l_\infty$-扰动下实现鲁棒性，并展示了如何在$p\geq 1$的任何$l_p$-规范下实现第一个可证明的鲁棒模型。
在本文中，我们提出了一个新的控制框架，称为移动端点控制，以恢复在一个模型中被不同退化水平破坏的图像。所提出的控制问题包含一个由RNN建模的恢复动态，移动端点，基本上是相关动态的终端时间，由策略网络决定。 数值实验表明，DURR能够在盲目的图像去噪和JPEG图像解锁上取得最先进的性能。此外，DURR能够很好地推广到不包括在训练阶段的具有更高退化水平的图像。
Wasserstein距离最近在机器学习界受到了很多关注，特别是它比较分布的原则性方式。它已经在一些硬问题中找到了许多应用，如领域适应、降维或生成模型。 我们表明，这样的嵌入可以通过一个与解码器网络相关的连体结构找到，该网络允许从嵌入空间移动到原始输入空间。一旦找到这个嵌入，就可以极快地计算Wasserstein空间中的优化问题（如中心点、主要方向甚至原型）。在图像数据集上进行了支持这一想法的数字实验，并显示了我们的方法的广泛潜在优势。
连续词袋（CBOW）是一种强大的文本嵌入方法。由于其强大的编码单词内容的能力，CBOW嵌入在广泛的下游任务中表现良好，同时计算效率高。然而，CBOW不能够捕捉到单词的顺序。原因是CBOW的单词嵌入的计算是交换的，即。为了解决这个缺点，我们提出了一个连续矩阵空间模型的学习算法，我们称之为连续词乘法（CMOW）。我们的算法是对word2vec的改编，因此它可以在大量的未标记文本上进行训练。我们的经验表明，CMOW能更好地捕捉语言特性，但在记忆单词内容方面不如CBOW。 在这些发现的激励下，我们提出了一个混合模型，结合了CBOW和CMOW的优势。我们的结果表明，混合CBOW-CMOW-模型保留了CBOW记忆单词内容的强大能力，同时将其编码其他语言信息的能力大幅提高了8%。因此，混合模型在11个有监督的下游任务中的8个表现更好，平均提高1.2%。
本文提出了Metagross（Meta Gated Recursive Controller），一个新的神经序列建模单元。我们提出的单元的特点是其门控功能的递归参数化，即。Metagross的门控机制由其自身的实例控制，这些实例以递归的方式被反复调用。这可以解释为一种元门控和递归参数化的递归模型。为此，我们在递归逻辑任务（排序、树形遍历、逻辑推理）、逐个像素的序列分类、语义解析、代码生成、机器翻译和复调音乐建模等方面进行了广泛的实验，证明了所提方法的广泛效用，即在所有任务上都达到了最先进（或接近）的性能。
哪种生成模型最适合持续学习？本文旨在评估和比较生成模型在不相干的连续图像生成任务上的表现。我们研究了几种模型如何学习和遗忘，考虑了各种策略：排练、正则化、生成重放和微调。我们用两个定量指标来估计生成质量和记忆能力。 我们在三个常用的持续学习基准（MNIST、Fashion MNIST和CIFAR10）上进行了顺序任务的实验。我们发现，在所有模型中，原始GAN表现最好，在持续学习策略中，生成式重放优于所有其他方法。即使我们在MNIST和Fashion MNIST上找到了满意的组合，在CIFAR10上顺序训练生成式模型特别不稳定，仍然是一个挑战。
我们提出了一种新的样本效率方法，称为监督策略更新（SPU），用于深度强化学习。从当前策略产生的数据开始，SPU在非参数化的近似策略空间中制定并解决一个受限的优化问题。使用监督回归，然后将最佳非参数化策略转换为参数化策略，从中提取新样本。 该方法是通用的，因为它适用于离散和连续的行动空间，并且可以处理各种非参数化优化问题的近似约束。我们展示了自然政策梯度和信任区域政策优化（NPG/TRPO）问题，以及近似政策优化（PPO）问题如何通过该方法来解决。 SPU的实现比TRPO简单得多。在采样效率方面，我们大量的实验表明SPU在Mujoco模拟机器人任务中优于TRPO，在Atari视频游戏任务中优于PPO。
最近关于灵长类视觉系统中神经反应的建模工作得益于在大规模物体识别上训练的深度神经网络，并发现人工神经网络的层与沿腹侧视觉流的大脑区域之间存在层次性的对应关系。 然而，我们既不知道这种任务优化的网络是否能够为啮齿类动物的视觉系统提供同样好的模型，也不知道是否存在类似的层次对应关系。在这里，我们通过提取在ImageNet上训练的卷积神经网络（CNN）的几层特征来预测四个视觉区域（V1、LM、AL、RL）中成千上万的神经元对自然图像的反应，来解决小鼠视觉系统中的这些问题。 我们发现CNN的特征优于经典的子单元能量模型，但没有发现我们通过与CNN层的层次对应而记录的区域的顺序的证据。此外，同样的CNN但有随机权重，为预测神经反应提供了一个同等有用的特征空间。 我们的结果表明，物体识别作为一个高层次的任务，并没有比随机网络提供更多的鉴别特征来描述小鼠的视觉系统。与灵长类动物不同，可能需要对与伦理学相关的视觉引导行为进行训练--超越静态的物体识别--来揭示小鼠视觉皮层的功能组织。
重参数化技巧已经成为变异推理领域最有用的工具之一。然而，重参数化技巧是基于标准化变换的，它将这种方法的应用范围限制在具有可操作的反累积分布函数的分布上，或者可表达为这种分布的确定性变换。在本文中，我们通过允许一般的变换来概括重参数化技巧。 我们发现，所提出的模型是控制变量的一个特例，表明所提出的模型可以结合CV和广义重参数化的优点。基于所提出的梯度模型，我们提出了一个新的基于多项式的梯度估计器，在某些条件下比重参数化技巧有更好的理论性能，并且可以应用于更大类的变量分布。在对合成和真实数据的研究中，我们表明我们提出的梯度估计器比其他先进的方法有一个明显低的梯度方差，从而使推理过程更快。
为了从文本语料库中同时捕获语法和语义，我们提出了一个新的大背景语言模型，通过动态的深度话题模型来提取经常性的层次语义结构，以指导自然语言的生成。超越了传统的语言模型，该模型忽略了长距离的单词依赖和句子顺序，提出的模型不仅捕获了句子内的单词依赖，而且还捕获了句子间的时间转换和句子间的话题依赖。 对于推理，我们开发了一种随机梯度MCMC和递归自动编码变异贝叶斯的混合方法。在各种真实世界的文本语料库上的实验结果表明，所提出的模型不仅优于最先进的大语境语言模型，而且还能学习可解释的递归多层主题，并生成句法正确和语义连贯的各种句子和段落。
我们提出了一种新的方法，利用强化学习来训练自然媒体绘画。给定一个参考图像，我们的表述是基于笔画的渲染，模仿人类的绘画，可以在没有监督的情况下从头开始学习。我们的绘画代理计算一连串的行动，代表原始的绘画笔触。 为了确保生成的策略是可预测和可控制的，我们使用约束学习方法，使用环境模型训练绘画代理，并遵循观察中编码的命令。我们在许多基准上应用了我们的方法，结果表明我们的约束代理可以处理不同的绘画媒体和行动空间中的不同约束，与人类或其他代理协作。
妄想偏差是近似Q-learning的一个基本错误来源。到目前为止，唯一明确解决妄想的技术需要使用表格价值估计进行全面搜索。在本文中，我们开发了有效的方法，通过训练Q-近似器的标签与潜在的贪婪政策类 "一致 "来缓解妄想偏差。 我们引入了一个简单的惩罚方案，鼓励跨训练批次使用的Q-标签与可表达的策略类保持（联合）一致。我们还提出了一个搜索框架，允许生成和跟踪多个Q-近似器，从而减轻了过早（隐含）策略承诺的影响。实验结果表明，这些方法可以改善各种Atari游戏中的Q-学习性能，有时甚至是显著的。
本文提出并演示了一个深度卷积神经网络（DCNN）架构，以识别试图进行欺诈性ATM交易的伪装脸部的用户。最近推出的伪装脸部识别（DFI）框架证明了深度神经网络在这个问题上的适用性。现在所有的ATM都有一个隐藏的摄像头，并捕捉用户的录像。 所提出的深度卷积神经网络经过训练，可以实时识别所捕获的图像中的用户是否试图掩盖其身份，然后将DCNN的输出报告给ATM，以采取适当措施，防止骗子完成交易。
自动编码器通常用于无监督的表征学习和预训练更深层次的神经网络。当它的激活函数是线性的，并且编码维度（隐藏层的宽度）小于输入维度时，众所周知，自动编码器被优化为学习数据分布的主成分（Oja 1982）。然而，当激活是非线性的，并且宽度大于输入维度时（过完整），自动编码器的行为与PCA不同，事实上，根据经验，它在稀疏编码问题上表现良好。我们为这种经验观察到的现象提供了理论上的解释，当采用整流线性单元（ReLu）作为激活函数，并且隐层宽度被设定为大的时候。在这种情况下，我们表明，以显著的概率，通过从球形高斯分布中采样来初始化自动编码器的权重矩阵，然后进行随机梯度下降（SGD）训练，对于一类稀疏字典学习模型来说，向基础事实的表示收敛。 此外，我们可以证明，以收敛为条件，预期收敛率为O(1/t)，其中t为更新次数。我们的分析量化了当使用随机初始化时，增加隐层宽度如何帮助训练性能，以及网络权重的规范如何影响SGD收敛的速度。
分层代理有可能以比非分层代理更高的样本效率解决顺序决策任务，因为分层代理可以将任务分解为只需要短序列决策的子任务集。 为了实现这种快速学习的潜力，分层代理需要能够平行地学习其多层次的政策，以便这些较简单的子问题能够同时得到解决。 然而，平行学习多层次的政策是很难的，因为它本身是不稳定的：层次结构中某一层次的政策的变化可能会引起层次结构中更高层次的过渡和奖励函数的变化，这使得联合学习多层次的政策很困难。 在本文中，我们介绍了一个新的层次强化学习（HRL）框架--层次演员批评（HAC），它可以克服代理人试图联合学习多层次政策时出现的不稳定问题。 HAC的主要思想是通过训练层次结构中的每一层独立于低层，就像低层的政策已经是最优的一样。 我们在网格世界和模拟机器人领域的实验中证明，相对于其他非层次性和层次性的方法，我们的方法可以显著加速学习。 事实上，我们的框架是第一个成功地在具有连续状态和行动空间的任务中平行学习3级层次的方法。
正面无标签（PU）学习解决了从正面（P）和无标签（U）数据中学习二元分类器的问题。它经常被应用于负面（N）数据难以被完全标记的情况。然而，收集一个非代表性的N集，只包含所有可能的N数据的一小部分，在许多实际情况下会更容易。 本文研究了一个新的分类框架，该框架在PU学习中纳入了这种有偏见的N（bN）数据。训练的N数据是有偏见的这一事实也使我们的工作与那些标准的半监督学习非常不同。 我们提供了一种基于经验风险最小化的方法来解决这个PUbN分类问题。我们的方法可以被视为传统例子加权算法的变种，每个例子的权重通过一个初步的步骤计算，从PU学习中获得灵感。
强化学习（RL）经常被用来提高文本生成任务的性能，包括机器翻译（MT），特别是通过使用最小风险训练（MRT）和生成对抗网络（GAN）。然而，人们对这些方法在MT背景下的学习内容和方式知之甚少。我们证明，最常见的MT的RL方法之一并没有优化预期的回报，同时也表明其他的方法需要花费非常长的时间来收敛。事实上，我们的结果表明，MT中的RL实践可能只在预训练的参数已经接近产生正确的翻译时才会改善性能。
自2018年初以来，自然语言处理（NLP）建模取得了重大进展。新的方法允许准确的结果，即使是在没有标记数据的情况下，因为这些NLP模型可以从任务诊断和特定任务的未标记数据的训练中受益。然而，这些优势伴随着巨大的规模和计算成本。 这篇研讨会论文概述了我们提出的卷积学生架构是如何通过对大规模模型的提炼过程进行训练的，可以实现300倍的推理速度和39倍的参数数量。在某些情况下，学生模型的性能在研究任务上超过了它的老师。
在机器学习模型中结合多个函数近似器，与单一函数相比，通常会有更好的性能和鲁棒性。在强化学习中，诸如平均法和多数投票法等集合算法并不总是最佳的，因为每个函数可以从探索中学习到根本上不同的最佳轨迹。 在本文中，我们提出了时差加权（TDW）算法，这是一种基于累积的时差误差来调整每个贡献的权重的集合方法。这种算法的优点是，它通过减少不熟悉当前轨迹的Q函数的权重来提高集合性能。我们提供了Gridworld任务和Atari任务的实验结果，与基线算法相比，显示出明显的性能改善。
本文介绍了强化学习行为套件（Behaviour Suite for Reinforcement Learning），简称bsuite。bsuite是一组精心设计的实验，研究强化学习（RL）代理的核心能力，有两个目标。首先，收集清晰的、信息丰富的、可扩展的问题，这些问题抓住了设计一般和高效学习算法的关键问题。其次，通过代理在这些共享基准上的表现研究其行为。为了补充这一努力，我们开源了这个http URL，它可以自动评估和分析bsuite上任何代理。 我们的代码是Python的，很容易在现有的项目中使用。我们包括OpenAI Baselines、Dopamine以及新的参考实现的例子。在未来，我们希望从研究界纳入更多优秀的实验，并承诺由知名研究人员组成的委员会定期审查bsuite。
基于序列的注意力模型是一种很有前途的端到端语音识别方法。模型能力的提高使得训练过程更加困难，而且由于端到端的性质，分析这些模型的失败模式变得更加困难。在这项工作中，我们提出了各种分析，以更好地了解训练和模型的特性。 我们对预训练的变体进行了调查，如深度和宽度的增长，以及它们对最终性能的影响，这导致单词错误率的相对改善超过8%.为了更好地理解注意力过程是如何工作的，我们研究了编码器输出和注意力能量和权重.我们的实验是在Switchboard、LibriSpeech和Wall Street Journal上进行的。
验证是寻找安全自主性的一个关键挑战。模拟往往要么太简单，无法提供稳健的验证，要么太复杂，无法进行切实的计算。因此，需要近似的验证方法来切实发现故障，而不进行不安全的简化。
多步贪婪策略已被广泛用于基于模型的强化学习（RL）和环境模型可用的情况下（如围棋）。在这项工作中，我们探索了多步贪婪策略在无模型RL中的好处，当采用多步动态编程（DP）的框架时：多步政策和价值迭代。这些算法迭代解决短跨度决策问题并收敛到原始问题的最优解。 通过使用无模型算法作为短跨度问题的求解器，我们推导出完全无模型的算法，这些算法是多步DP框架的实例。由于无模型算法在决策问题的跨度上容易出现不稳定性，这种简单的方法可以帮助缓解这些不稳定性，并导致改进的无模型算法。我们测试了这种方法，并在离散和连续控制问题上显示了结果。
Madry等人（2018）提出的对抗性训练程序是防御深度神经网络（DNNs）中对抗性例子的最有效方法之一。 在我们的论文中，我们揭示了对抗性训练的实用性和硬度，表明对抗性训练的有效性（测试集上的鲁棒性）与测试点和网络嵌入的训练数据流形之间的距离有很大的关联。 因此，基于对抗性训练的防御很容易受到一类新的攻击，即 "盲点攻击"，其中输入图像位于训练数据经验分布的 "盲点"（低密度区域），但仍在地面真实数据流形上。对于MNIST，我们发现这些盲点可以通过简单的缩放和移动图像像素值轻松找到。 最重要的是，对于具有高维和复杂数据流形的大型数据集（CIFAR、ImageNet等），由于维度的诅咒和训练数据的稀缺性，对抗性训练中盲点的存在使得在任何有效的测试实例上进行防御变得困难。此外，我们发现盲点也存在于可证明的防御中，包括（Kolter & Wong, 2018）和（Sinha等人。2018），因为这些可训练的鲁棒性证书只能在有限的训练数据集上进行实际优化。
边缘智能尤其是二进制神经网络（BNN）最近引起了人工智能界的极大关注。BNN大大降低了计算成本、模型大小和内存占用。 然而，在成功的ReLU激活的全精度神经网络和BNNs之间仍然存在性能差距。我们认为BNNs的精度下降是由于它们的几何结构。我们分析了具有ReLU激活的全精度神经网络的行为，并与它的二进制对应物进行了比较。这种比较表明，随机偏置初始化是全精度网络中激活饱和的补救措施，并引导我们走向改进的BNN训练。我们的数字实验证实了我们的几何直觉。
了解人们如何表示类别是认知科学的核心问题，人类学习的灵活性仍然是现代人工智能和机器学习所向往的黄金标准。数十年的心理学研究已经产生了各种正式的类别理论，然而用自然的刺激来验证这些理论仍然是一个挑战。问题是人类的类别表示不能被直接观察到，用自然的刺激（如图像）运行信息实验需要有这些刺激的可行表示。 在本文中，我们介绍了一种估计人类类别结构的方法，该方法借鉴了认知科学和机器学习的理念，将基于人类的算法与最先进的深度表征学习者相融合。我们提供了定性和定量的结果作为该方法可行性的概念证明。
深度递归网络在音频转录中的应用使自动语音识别（ASR）系统取得了令人印象深刻的成果。许多人已经证明，小的对抗性扰动可以愚弄深度神经网络，使其错误地预测出一个具有高置信度的指定目标。 目前关于愚弄ASR系统的工作主要集中在白盒攻击上，其中模型架构和参数是已知的。在本文中，我们采用黑盒方法来生成对抗性，结合遗传算法和梯度估计的方法来解决这个任务。我们在3000代后实现了89.25%的目标攻击相似度，同时保持94.6%的音频文件相似度。
最近在基于物理的角色动画方面的进展显示，通过深度强化学习模仿运动捕捉数据，在人类运动合成方面取得了令人印象深刻的突破。然而，结果大多是在模仿单一的独特运动模式上得到证明，并没有推广到需要灵活运动模式的互动任务中，因为人与物体的空间配置不同。为了弥补这一差距，我们专注于一类互动任务--坐在椅子上。 我们提出了一个分层强化学习框架，它依赖于一个子任务控制器的集合，这些子任务控制器经过训练可以模仿简单的、可重复使用的mocap动作，而元控制器经过训练可以正确执行子任务以完成主要任务。我们通过实验证明了我们的方法在不同的单层和分层基线上的优势。我们还表明，我们的方法可以应用于给定图像输入的运动预测。视频亮点可以在https://youtu.be/XWU3wzz1ip8/。
我们建议将GAN训练动态研究为遗憾最小化，这与流行的观点相反，即真实分布和生成分布之间的分歧一致最小化。我们从这个新的角度分析GAN训练的收敛性，以了解模式崩溃发生的原因。我们假设在这个非凸博弈中存在不理想的局部均衡，这就是模式崩溃的原因。 我们观察到，这些局部平衡往往在一些真实的数据点周围表现出判别器函数的尖锐梯度。我们证明，这些退化的局部平衡可以用一种叫做DRAGAN的梯度惩罚方案来避免。我们表明，DRAGAN可以实现更快的训练，实现更好的稳定性，减少模式崩溃，并导致发电机网络在各种架构和目标函数中具有更好的建模性能。
深度神经网络（DNNs）由于具有自动特征学习和自由表达的优势，在过去十年中取得了令人惊讶的成就。然而，由于DNNs是线性和非线性变换的复杂组合，其可解释性仍然很神秘。尽管已经提出了许多模型来探索DNNs的可解释性，但仍有几个挑战没有解决：1）缺乏DNNs的可解释性数量测量，2）缺乏DNNs的稳定性理论，以及3）难以解决具有可解释性约束的非凸DNN问题。 为了同时解决这些挑战，本文提出了一个新的DNN内在可解释性评价框架。具体来说，在现有工作的基础上，我们定义了可解释性的四个独立属性。此外，我们研究了DNN的稳定性理论，这是可解释性的一个重要方面，并证明在不同的激活函数下，DNN一般是稳定的。 最后，我们提出了一个扩展版的深度学习乘数交替方向法（dlADMM），以高效准确地解决具有可解释性约束的DNN问题。在几个基准数据集上进行的广泛实验，通过我们提出的可解释性框架验证了几个DNN。
强化学习（RL）通常定义一个折扣因子作为马尔可夫决策过程的一部分。 贴现因子通过一个指数方案对未来的奖励进行估值，导致贝尔曼方程的理论收敛保证。然而，来自心理学、经济学和神经科学的证据表明，人类和动物反而有双曲线时间偏好。 在这里，我们扩展了Kurth-Nelson和Redish的早期工作，并提出了一个高效的深度强化学习代理，通过双曲折现和其他非指数折现机制行事。我们证明了一个简单的方法可以接近双曲折现函数，同时仍然使用RL中熟悉的时间差学习技术。 此外，独立于双曲折现，我们有一个令人惊讶的发现，在多个时间范围内同时学习价值函数是一个有效的辅助任务，往往比最先进的方法更有优势。
情感在我们的日常生活中发挥着巨大的作用。自动情感识别系统的必要性和重要性越来越大。传统的情感识别方法是基于面部图像、心率、血压、温度、语音/语调等的测量。然而，这些特征有可能被改变为虚假的特征。因此，为了检测不受人控制的隐藏和真实的特征，从大脑信号中测量数据。本研究的主要目的是根据大脑对视觉刺激的反应而记录的脑电信号分析来检测情绪。所使用的方法是将选定的视觉刺激呈现给11名健康的目标受试者，并在受控情况下记录脑电信号，以减少伪影（肌肉或/和眼动）。 最后，对所提方法的性能进行了测试。机器学习算法（即J48、Bayes Net、Adaboost和Random Forest）的平均精确度分别为78.86、74.76、77.82和82.46。 在这项研究中，我们还在神经营销的背景下应用了脑电图的应用。结果实证证明了对客户最喜欢的颜色偏好的检测，以回应一个组织或服务的标志颜色。
  我们提出了一个基于会话的推荐的概率框架。 用户状态的潜变量随着用户浏览更多的项目和我们对其兴趣的了解而更新。 我们提供了使用重新参数化技巧和使用Softmax函数的Bouchard约束的计算解决方案，我们进一步探索采用变异自动编码器和变异期望最大化算法来收紧变异约束。 最后，我们表明，Bouchard约束导致softmax的分母分解为一个和，使约束的快速噪声梯度得到了一个完全的概率算法，让人联想到word2vec和一个快速在线EM算法。
任务转移学习中的一个重要问题是确定任务的可转移性，即给定一个共同的输入域，估计从源任务学到的表征在多大程度上可以帮助学习目标任务。通常情况下，可转移性要么通过实验测量，要么通过任务相关度推断，而任务相关度的定义往往没有明确的操作意义。 受原则性的信息论方法的启发，H-score与基于转移特征的决策函数的渐进误差概率有直接联系。这种可转移性的表述可以进一步用于在任务转移学习问题中选择合适的源任务集或设计有效的转移学习策略。使用合成和真实图像数据的实验表明，不仅我们的可转移性表述在实践中是有意义的，而且它可以推广到分类以外的推理问题，如三维室内场景理解的识别任务。
本文提出了一个通用的框架来解决多类分布的无监督领域适应（UDA）中关键的类不匹配问题。 以前的对抗性学习方法只以伪标签为条件进行领域对齐，但嘈杂和不准确的伪标签可能会扰乱嵌入概率预测中的多类分布，因此对潜在的不匹配问题的缓解不够。 与伪标签相比，类原型更准确、更可靠，因为它们概括了所有的实例，能够代表各领域共享的固有语义分布。因此，我们提出了一种新颖的原型辅助对抗性学习（PAAL）方案，它将实例的概率预测和类原型结合起来，为对抗性领域对齐提供可靠指标。  通过PAAL方案，我们将实例特征表征和类原型表征统一起来，以缓解语义不同的类之间的不匹配问题。  同时，我们利用类原型作为代理，使目标域中的类内差异最小化，以减轻语义相似类之间的不匹配。 我们在两个UDA任务上证明了PAAL方案和PACDA框架的良好性能和通用能力，即物体识别（Office-Home,ImageCLEF-DA,和Office）和合成到真实的语义分割（GTA5→Cityscapes和Synthia→Cityscapes）。
我们提出了一种新的方法，从任何给定的结构化输入的异同度测量中构建一个系列的\emph{positive definite kernels}，其元素是实值时间序列或离散结构，如字符串、直方图和图形。我们的方法，我们称之为D2KE（从距离到核和嵌入），借鉴了随机特征的文献。然而，我们不是从用户定义的核中推导出随机特征图来近似核机，而是从随机特征图中建立一个核，我们指定了距离度量。我们进一步提出使用有限数量的随机对象来产生每个实例的随机特征嵌入。我们提供了一个理论分析，表明D2KE比普遍的近邻估计享有更好的泛化能力。一方面，D2KE包含了广泛使用的代表集方法（representative-set method），作为一个特例，并在一个极限情况下与著名的距离替换核（distance substitution kernel）相关联。另一方面，D2KE将现有的只适用于矢量输入表示的{随机特征方法}推广到大小可变的复杂结构输入。我们在时间序列、字符串和直方图（文本和图像）等不同的领域进行了分类实验，对于这些领域，我们提出的框架在测试精度和计算时间方面都优于现有的基于距离的学习方法。
深度卷积神经网络（CNN），由Hinton1986,LeCun1985,Alex2012}的先驱工作扎根，并在LeCunBengioHinton2015}中总结，已被证明在各种领域非常有用。 最先进的CNN机器，如image rest net \cite{He_2016_CVPR}是由实值输入和内核卷积描述的，然后是局部和非线性整流的线性输出。 了解这些层的作用，它们的准确性和局限性，以及使它们更有效率（更少的参数）都是正在进行的研究问题。  在量子理论的启发下，我们提出使用复值核函数，然后是局部非线性绝对（模数）算子平方。我们认为，量子启发的复值核的一个优势是对现实中不可预测的情况（如杂波噪声、数据变形）的鲁棒性。 我们研究了一个具体的形状检测问题，并表明当多个重叠的形状被变形和/或加入杂波噪声时，具有量子启发的复合核的卷积层优于统计/经典核的对应物和 "贝叶斯形状估计器"。卓越的性能是由于量子干扰现象，在经典CNN中不存在。 
我们提出了一个人工智能研究平台，其灵感来自于人类游戏类型的MMORPGs（大型多人在线角色扮演游戏，又称MMOs）。我们展示了这个平台如何用于研究大型神经代理群体的行为和学习。与目前流行的游戏环境不同，我们的平台支持持久的环境，代理数量可变，任务描述不限。 地球上复杂生命的出现通常归因于军备竞赛，而军备竞赛是由大量的生物体竞争有限的资源而产生的。我们的平台旨在模拟这种微观环境：我们进行了一系列的实验，以测试大规模的多代理竞争如何能够激励熟练行为的发展。我们发现，人口规模放大了出现的行为的复杂性，导致代理超过在较小人口中训练的代理。
在本文中，我们为生成领域特定图像的生成对抗网络（GANs）提出了一个改进的定量评价框架，我们在两个层面上改进了传统的评价方法：特征表示和评价指标。与大多数现有的评价框架不同的是，我们的框架将ImageNet概念模型的表示转移到图像映射到特征空间，我们的框架使用一个专门的编码器来获得细粒度的领域特定表示。 此外，对于有多个类别的数据集，我们提出了 "感知类别的Frechet Distance"（CAFD），它在特征空间上采用了高斯混合模型，以更好地适应多领域的特征分布。我们在特征层面和图像层面进行了实验和分析，以证明我们提出的框架比最近提出的最先进的FID方法有所改进。 据我们所知，我们是第一个提供FID与人类判断结果不一致的反例。实验表明，我们的框架能够克服FID的短板，提高鲁棒性。将提供代码。
最近在训练轻量级二元神经网络方面的努力提供了很好的执行/内存效率。本文介绍了ResBinNet，它是由两种相互联系的方法组成的，旨在解决二元卷积神经网络的缓慢收敛速度和有限的准确性。第一种方法，称为残差二值化，为某一神经网络层内的特征学习多级二值表示。 第二种方法，称为温度调整，逐渐对特定层的权重进行二进制化。这两种方法共同学习了一组软二进制化的参数，提高了二元神经网络的收敛率和准确性。我们通过实现一个原型硬件加速器来证实ResBinNet的适用性和可扩展性。该加速器在二进制化特征的数字精度方面是可重新配置的，在运行时间和推理准确性之间提供了一个权衡。
在现实世界的机器学习应用中，大的离群值和普遍存在的噪声是很常见的，而且不可能像标准的深度自动编码器那样获得干净的训练数据。在一组给定的图像中可靠地检测异常是一项对视觉质量检查、监视或医学图像分析具有高度实际意义的任务。 自动编码器神经网络学习重建正常图像，因此，如果重建误差超过某个阈值，可以将这些图像归类为异常图像。在本文中，我们提出了一种基于自动编码器激活的子集扫描的无监督方法。 我们的工作有三个方面的贡献。首先，我们提出了一种新颖的方法，将检测与重建误差和子集扫描得分相结合，以提高当前自动编码器的异常得分，而不需要任何再训练。第二，我们提供了检查和可视化重建误差空间中的异常节点集的能力，这些节点使样本有噪声。第三，我们表明子集扫描可用于自动编码器内部层的异常检测。我们提供了标准数据集下几个非目标对抗性噪声模型的检测能力结果。
学习知识图谱嵌入（KGEs）是完成知识图谱的一种有效方法。传统的KGEs经常受到有限的知识表示的影响，特别是在稀疏的知识图谱上进行训练时，会导致较低的准确性。为了补救这个问题，我们提出了Pretrain-KGEs，一个学习更好的知识实体和关系嵌入的训练框架，利用预训练语言模型的丰富语言学知识。 具体来说，我们提出了一种统一的方法，即首先通过预训练的语言模型学习实体和关系表征，并使用这些表征来初始化实体和关系嵌入，以训练KGE模型。实验结果表明，我们的方法可以持续改善结果，并在四个基准KG数据集的链接预测和三重分类任务中使用不同的KGE模型，如TransE和QuatE，实现最先进的性能。
我们描述了一种表示符号知识库（KB）的新方法，称为稀疏矩阵重化KB。 这种表示方法使神经模块完全可分，忠实于知识库的原始语义，具有足够的表现力来模拟多跳推理，并具有足够的可扩展性来使用现实中的大型知识库。稀疏矩阵重化知识库可以分布在多个GPU上，可以扩展到数千万个实体和事实，并且比天真的稀疏矩阵实现快几个数量级。 统一的KB使非常简单的端到端架构能够在代表两个任务系列的几个基准上获得有竞争力的性能。KB完成，以及从语义解析器中学习语义解析器。
我们研究了用于连续和参数化目标函数的非凸优化的交叉熵法（CEM），并引入了一个可微分的变体（DCEM），使我们能够将CEM的输出相对于目标函数的参数进行微分。在机器学习中，这使得CEM进入了端到端的学习管道，否则这是不可能的。 我们展示了在基于合成能量的结构化预测任务和非凸连续控制中的应用。在控制方面，我们在模拟猎豹和步行者的任务中展示了我们可以用DCEM嵌入它们的最佳行动序列，然后使用策略优化来微调控制器的组件，作为结合基于模型和无模型RL的一个步骤。
我们提出了一个基于生成对抗模仿学习的实体和事件提取的新框架--一种使用生成对抗网络（GAN）的反强化学习方法。我们假设实例和标签会产生不同程度的困难，收益和惩罚（奖励）预计也会不同。 实验还表明，所提出的框架优于最先进的方法。
尽管最近在生成性图像建模方面取得了进展，但从复杂的数据集（如ImageNet）中成功生成高分辨率、多样化的样本仍然是一个难以实现的目标。我们发现，对生成器应用正交正则化使其适用于一个简单的 "截断技巧"，允许通过减少生成器输入的方差来精细控制样本保真度和多样性之间的权衡。 当在128x128分辨率的ImageNet上训练时，我们的模型（BigGANs）达到了166.3的Inception Score（IS）和9.6的Frechet Inception Distance（FID），比之前的最佳IS 52.52和FID 18.65有所提高。
在这项工作中，我们提出了一个新的目标误差上界，以解决无监督领域适应的问题。最近的研究显示，深度神经网络可以学习可转移的特征，这些特征可以很好地泛化到新的任务中。此外，Ben-David等人（2010）提供了一个转移知识时的目标误差上界，可以概括为同时最小化源误差和边际分布之间的距离。然而，基于该理论的普通方法通常忽略了联合误差，如在匹配边际分布时，不同类别的样本可能混在一起。 在这种情况下，无论我们如何最小化边际差异，由于联合误差的增加，目标误差是没有界限的。为了解决这个问题，我们提出了一个考虑到联合误差的一般上界，这样不理想的情况可以得到适当的惩罚。 此外，我们利用受限的假设空间来进一步正式确定一个更严格的界限，以及一个新的交叉边际差异来衡量假设之间的不相似性，这缓解了对抗性学习中的不稳定性。广泛的经验证据表明，我们的建议在标准领域适应性基准上的图像分类错误率方面优于相关方法。
如今，几乎任何给定的任务都有大量的深度网，在处理一个新的任务时，越来越不清楚应该从哪个网开始，或者用哪个网作为微调新模型的初始化。为了解决这个问题，在本文中，我们开发了知识流，将 "知识 "从多个深度网（被称为教师）转移到一个新的深度网模型，称为学生。 教师和学生的结构可以任意不同，他们可以在完全不同的任务上进行训练，也有不同的输出空间。
尽管深度神经网络（DNNs）在众多学习任务中表现出令人印象深刻的性能，但它们仍然表现出不礼貌的行为。一个令人费解的行为是DNNs对各种噪声攻击的微妙敏感反应。这种困扰加强了围绕开发和训练抗噪声网络的研究路线。 在这项工作中，我们提出了一种新的训练正则，旨在最小化受一般高斯输入影响的DNN的概率预期训练损失。我们提供了一种有效而简单的方法来近似这种任意深度网络的正则。 我们在LeNet和AlexNet的各种数据集上进行了广泛的实验，包括MNIST、CIFAR10和CIFAR100，以证明我们提出的正则器的有效性。特别是，我们表明，用提出的正则器训练的网络在对抗高斯噪声的鲁棒性方面受益，相当于执行3-21倍的噪声数据增强。 此外，我们在几个架构和数据集上的经验表明，通过使用新的正则器，提高对高斯噪声的鲁棒性，可以将对其他6种类型攻击的整体鲁棒性提高两个数量级。
阅读理解是一项具有挑战性的任务，特别是当在较长或多个证据文件中执行时，答案很可能会重复出现。现有的神经架构通常不能扩展到整个证据，因此，诉诸于选择文件中的一个段落（通过截断或其他手段），并在该段落中仔细搜索答案。然而，在某些情况下，这种策略可能是次优的，因为通过关注特定段落，很难利用整个文件中多次提到的相同答案。 在这项工作中，我们采取了一种不同的方法，即构建轻量级的模型，通过级联的方式来寻找答案。每个子模型仅由配备注意机制的前馈网络组成，这使得它可以简单地并行化。我们表明，我们的方法可以扩展到大约一个数量级的大型证据文件，并且可以从整个文件中的每个答案候选人的多次提及中收集信息。
深度神经网络的压缩形式对于在资源有限的设备上部署大规模计算模型至关重要。与类似领域相反，大规模系统是作为小规模单元的分层重复而建立的，目前机器学习的实践主要依赖于具有非重复组件的模型。 经验表明，与微调的全连接神经网络相比，ACNs实现了高达3个数量级的压缩率（减少88倍到1116倍），而分类准确率仅有部分下降（0.15%到5.33%）。此外，我们的方法可以产生亚线性模型的复杂性，并允许用比逻辑回归更少的参数学习深度ACNs，而分类准确率没有下降。
虽然最近的一些工作研究了能够代表不确定未来的概率模型，但这些模型要么在计算上极其昂贵，如像素级自回归模型，要么不能直接优化数据的可能性。 据我们所知，我们的工作是第一个提出带有归一化流的多帧视频预测，它允许直接优化数据的似然性，并产生高质量的随机预测。我们描述了一种对潜在空间动态建模的方法，并证明基于流的生成模型为视频的生成建模提供了一种可行的和有竞争力的方法。
了解随机梯度下降（SGD）在深度神经网络背景下的行为最近引起了很多人的关注。沿着这一思路，我们从理论上研究了一种基于梯度的优化动力学的一般形式，它将SGD和标准Langevin动力学统一起来。通过研究这种一般的优化动力学，我们分析了SGD逃离最小值的行为及其正规化效应。 基于这一指标，建立了两个条件，以显示哪种类型的噪声结构在逃逸效率方面优于各向同性的噪声。我们进一步表明，SGD中的各向异性噪声满足这两个条件，因此有助于有效地逃逸尖锐和不良的最小值，走向更稳定和平坦的最小值，通常可以很好地泛化。我们通过将这种各向异性扩散与完全梯度下降和各向同性扩散（即Langevin动力学）以及其他类型的位置相关噪声相比较，验证我们的理解。
目前基于模型的强化学习方法仅仅将模型作为一个学习的黑箱模拟器来使用，以增加政策优化或价值函数学习的数据。在本文中，我们展示了如何通过利用其可分性来更有效地利用模型。 此外，我们以模型和价值函数的梯度误差来推导我们目标的单调改进。我们表明，我们的方法(i)始终比现有的最先进的基于模型的算法的样本效率更高，(ii)与无模型算法的渐进性能相匹配，(iii)可扩展到长周期，这是过去基于模型的方法通常难以做到的地方。
元学习算法可以从过去的经验中更快地获得新的任务。在强化学习的背景下，元学习算法可以获得强化学习程序，通过利用先前任务的经验更有效地解决新问题。 元学习算法的性能取决于可用于元训练的任务：与监督学习对来自与训练点相同分布的测试点的归纳效果最好一样，元学习方法对来自与元训练任务相同分布的任务的归纳效果最好。实际上，元强化学习将设计负担从算法设计转移到任务设计上。如果我们能将任务设计过程也自动化，我们就能设计出真正自动化的元学习算法。 在这项工作中，我们朝着这个方向迈出了一步，提出了一系列用于强化学习的无监督元学习算法。我们激励并描述了无监督元强化学习的一般配方，并提出了这种方法的一个实例。 我们在概念上和理论上的贡献包括提出无监督元强化学习问题，并描述了基于相互信息的任务建议原则上可以用来训练最佳元学习者。我们的实验结果表明，无监督元强化学习可以有效地获得加速的强化学习程序，而不需要手工设计任务，并大大超过了从头学习的性能。
我们引入了一种新的方法，使深度神经网络的参数高效转移和多任务学习成为可能。基本方法是学习一个模型补丁--一小套参数--专门用于每个任务，而不是对最后一层或整个网络进行微调。例如，我们表明，学习一组尺度和偏置足以使预训练的网络在质量不同的问题上表现良好（例如。例如，我们表明，学习一组尺度和偏置足以将预训练的网络转换为在不同问题上表现良好（例如，将单次多盒检测（SSD）模型转换为1000级图像分类模型，同时重复使用SSD特征提取器的98%的参数）。同样，我们表明，重新学习现有的低参数层（如深度卷积），同时保持网络的其他部分冻结，也能显著提高转移学习的准确性。
对抗性例子在一定程度上破坏了机器学习（ML）的巨大成功，并引起了对其可信度的担忧。在本文中，我们推导出一个信息理论模型，该模型将对抗性攻击普遍地解释为ML算法中特征冗余的滥用。 我们证明了特征冗余是对抗性例子存在的必要条件。我们的模型有助于解释许多关于对抗性例子的轶事研究中提出的主要问题。我们的理论得到了图像和文本数据集上良性和对抗性例子的信息含量的经验测量的支持。 我们的测量结果表明，典型的对抗性例子引入了足够的冗余，足以溢出在相应的良性例子上训练的机器学习者的决策。我们最后提出了可操作的建议，以提高机器学习者对对抗性例子的鲁棒性。
我们提出了一个新的无监督生成模型--Elastic-InfoGAN，它可以学习在类不平衡的数据集中将物体的身份与其他低层次的方面区分开来。我们首先研究了InfoGAN所做的关于统一性的假设的问题，并证明了它在不平衡的数据中正确区分物体身份的无效性。 我们的关键想法是使离散的潜在变异因子的发现不受真实图像中的身份保护变换的影响，并以此作为信号来学习潜在分布的参数。在人工（MNIST）和真实世界（YouTube-Faces）数据集上的实验证明了我们的方法在不平衡数据中的有效性：（i）更好地将物体身份作为潜在变异因子进行分离；以及（ii）更好地接近数据中的类不平衡，反映在学习的潜在分布的参数上。
许多实际应用显示出对从不同数据源/模式中学习多个任务的极大兴趣，这些任务的样本和维度是不平衡的。不幸的是，现有的前沿深度多任务学习（MTL）方法不能直接应用于这些环境，原因是输入维度的异质性或不同任务的最佳网络结构的异质性。 因此，需要开发知识共享机制来处理不同任务的网络架构之间的内在差异。为此，我们提出了一个灵活的知识共享框架，用于从不同的数据源/模式中联合学习多个任务。 我们提出的框架允许每个任务通过利用紧凑的张量表示拥有其特定的任务（数据）网络设计，而共享是通过部分共享的潜核实现的。通过提供更详细的潜核共享控制，我们的框架在传输任务变量知识方面是有效的，但在学习特定任务特征方面也是高效的。在单一和多个数据源/模式设置上的实验显示了所提方法的良好效果，特别是在数据不足的情况下是有利的。
棋盘游戏通常依赖于视觉信息，如棋子的位置和卡片上的文字信息。由于这种对视觉反馈的依赖，盲人玩家处于不利地位，因为他们无法阅读卡片或看到棋子的位置，在没有视力帮助的情况下可能无法玩游戏。我们提出了Game Changer，一个增强的工作空间，提供音频描述和触觉补充，使盲人和视力障碍的玩家能够了解棋盘游戏的状态。 在本文中，我们描述了Game Changer的设计，并介绍了一项用户研究的结果，其中7位盲人参与者使用Game Changer与一位视力正常的伙伴进行游戏。大多数玩家表示，有了Game Changer的补充，游戏更容易进入，并认为Game Changer可以用于增强其他游戏。
在许多应用中，标记的数据不是现成的，需要通过艰苦的人类监督来收集。我们提出了一个收集人类监督的规则-范例模型，以结合规则的可扩展性和实例标签的质量。 我们提出了一种训练算法，通过潜在的覆盖率变量对规则进行联合去噪，并通过覆盖率和标签变量的软暗示损失训练模型。 对五个不同任务的实证评估表明：（1）我们的算法比现有的几种混合清洁和噪声监督的学习方法更准确；（2）耦合的规则-范例监督在去噪规则方面是有效的。
我们考虑了在未知动态下从专家实例中学习奖励和政策的问题。我们提出的方法建立在生成对抗网络的框架上，并引入了赋权-正则化的最大熵逆向强化学习，以学习接近最优的奖励和政策。基于赋权的正则化防止了政策对专家示范的过度拟合，这有利地导致了更普遍的行为，导致学习接近最优奖励。 我们的方法通过变异信息最大化与对抗性学习表述下的奖励和政策同时学习赋权。我们在各种高维复杂控制任务上评估了我们的方法。我们还在具有挑战性的转移学习问题中测试了我们学习到的奖励，其中训练和测试环境在动态或结构方面是不同的。结果表明，我们提出的方法不仅学习了与专家行为相匹配的近优奖励和政策，而且比最先进的逆强化学习算法表现得更好。
递归神经网络（RNN）可以学习符号结构的连续矢量表示，如序列和句子；这些表示往往表现出线性规律性（类比）。这种规律性促使我们假设，显示这种规律性的RNN隐含地将符号结构编译成张量乘积表征（TPRs; Smolensky, 1990），它将代表角色（如序列位置）的向量和代表填充物（如特定单词）的向量乘积加在一起。为了检验这一假设，我们引入了张量积分解网络（TPDNs），它使用TPRs来近似于现有的矢量表示。我们用合成数据证明，TPDNs可以成功地近似于线性和基于树的RNN自动编码器表征，表明这些表征表现出可解释的组成结构；我们探讨了导致RNNs诱发这种结构敏感表征的设置。 相比之下，进一步的TPDN实验表明，为编码自然发生的句子而训练的四个模型的表征在很大程度上可以用一个词包来近似，只有更复杂的结构才会有微小的改进。我们的结论是，TPDNs为解释矢量表征提供了一种强大的方法，标准的RNNs可以诱导出由TPRs明显近似的组合序列表征；同时，现有的句子表征学习的训练任务可能不足以诱导出稳健的结构表征
我们的方法是将编程语言理论中的参数化思想概括为一种语义属性，将常见的算法与任意的非算法函数区分开来。这种特征化自然导致了一种学习数据增强方案，该方案鼓励RNN学习算法行为，并在各种列表处理任务中实现小样本学习。
多标签学习（MLL）的目标是将一个给定的实例与一组概念中的相关标签联系起来。以前的MLL工作主要集中在概念集被假定为固定的情况下，而许多现实世界的应用需要将新的概念引入概念集以满足新的需求。一个常见的需求是细化原始的粗略概念并将其分成更细的概念，其中细化过程通常从更细的概念的有限标签数据开始。 为了解决这个问题，我们提出了一个特殊的弱监督MLL问题，它不仅关注有限的细粒度监督的情况，而且还利用了粗略概念和细粒度概念之间的层次关系。这个问题可以被简化为使用层次关系的多标签版本的负数无标签学习问题。 实验结果表明，我们提出的方法能够分配准确的伪标签，并且与其他现有的方法相比，取得了优异的分类性能。
我们开发了一个基于强化学习的搜索助手，它可以通过一系列的行动和互动序列来帮助用户实现他们的意图。我们的方法迎合了主观的搜索，其中用户正在寻找数字资产，如图像，这与具有客观和有限的搜索模式的任务是根本不同的。在这样的搜索任务中，标记的对话数据通常是不可用的，通过人类的互动来训练代理可能是耗时的。 我们提出了一个随机的虚拟用户，它冒充真实的用户，可以用来有效地对用户的行为进行采样，以训练代理，从而加速代理的引导。我们开发了基于A3C算法的上下文保护架构，使代理能够为用户提供上下文帮助。我们将A3C代理与Q-learning进行比较，并评估其在验证情节中与虚拟用户获得的平均奖励和状态值的性能。
我们提出了一种基于像素级近邻的简单方法来理解和解释最先进的神经网络在像素级任务中的功能。我们旨在理解和揭示最先进的卷积神经网络的合成/预测机制。为此，我们主要分析生成模型的合成过程和判别模型的预测机制。 这项工作的主要假设是，用于像素级任务的卷积神经网络学会了一个快速合成的近邻合成/预测功能。我们在语义分割和图像-图像翻译方面的实验显示了支持这一假设的定性和定量证据。
考虑到一个发生了涉及各种实体的事件的世界。当我们考虑更多类型的事件时，学习如何从过去的事件模式中预测未来的事件变得更加困难。普通的LSTM在数据集中检测到的许多模式将是虚假的，因为潜在的成对关联的数量，例如，随着事件的数量呈二次增长。 我们提出了一种因子LSTM架构，其中不同的LSTM单元块负责捕捉世界状态的不同方面。我们使用Datalog规则来指定如何从关于世界实体的事实数据库中导出LSTM结构。 这类似于概率关系模型（Getoor & Taskar, 2007）指定了从数据库中导出图形模型结构的配方。在这两种情况下，目标是通过将知情的独立性假设编码到模型中来获得有用的归纳偏差。我们特别考虑了神经霍克斯过程，它使用LSTM来调节连续时间内瞬时事件的速率。
在这项工作中，我们的目标是解决数据驱动的优化问题，其中的目标是找到一个输入，在获得输入、分数对的数据集的情况下使未知的分数函数最大化。输入可能位于高维空间的极薄流形上，使得优化容易从流形上掉下来。此外，评估未知函数可能很昂贵，所以算法应该能够利用静态、离线数据。 与之前的工作不同，MINs可以扩展到极高维的输入空间，并且可以有效地利用离线记录的数据集，在上下文和非上下文环境中进行优化。我们表明，MINs也可以扩展到之前工作中通常研究的主动环境，通过简单、新颖和有效的主动数据收集方案。
从自然语言输入生成由关系图元表示的形式语言，如Lisp程序或数学表达式，是一项极具挑战性的任务，因为它需要明确地从输入中捕获离散的符号结构信息来生成输出。大多数最先进的神经序列模型没有明确地捕获这种结构信息，因此在这些任务中表现不佳。 TP-N2F的编码器采用TPR "绑定 "来编码向量空间中的自然语言符号结构，而解码器则采用TPR "解绑 "来生成符号空间中的一连串关系图元，每个图元由一个关系（或操作）和若干参数组成。 TP-N2F大大超过了基于LSTM的Seq2Seq模型，在两个基准上创造了新的技术成果：用于数学问题解决的MathQA数据集和用于程序合成的AlgoList数据集。消解研究表明，改进主要归功于在编码器和解码器中使用TPR来明确捕获关系结构信息进行符号推理。
本文我们提出了一个消解器，一个从部分观察中学习预测未来隐藏信息的模型。我们在正向建模的背景下制定这个模型，并分别通过卷积神经网络和长短期记忆网络来利用空间和顺序约束和关联。我们在人类游戏《星际争霸》的大型数据集上评估我们的方法。我们在《星际争霸：兄弟战争》这一实时战略视频游戏的人类游戏大型数据集上对我们的方法进行了评估。我们的模型持续击败了基于规则的强大基线，并在质量上产生了合理的未来游戏状态。
在这项工作中，我们通过分析目标景观的各种属性来研究神经网络在基于梯度的元学习中的泛化。我们通过实验证明，随着元训练的进展，通过几步基于梯度的微调使模型的元训练解适应新的任务，得到的元测试解变得更平坦，损失更小，并且离元训练解更远。 我们还表明，即使泛化能力开始下降，这些元测试方案也会变得更平坦，从而提供了一个实验证据，反对基于梯度的元精调范式中泛化和平坦的最小值之间的相关性。 此外，我们还提供了经验证据，证明对新任务的泛化与它们在参数空间中的适应轨迹之间的一致性有关，这种一致性由特定任务的轨迹方向之间的平均余弦相似度衡量，从同一个元训练方案开始。我们还表明，元测试梯度的一致性，由元训练方案中评估的特定任务梯度向量之间的平均内积衡量，也与泛化相关。
人们已经多次尝试用变异自动编码器（VAE）来学习复杂数据的强大全局表征，使用潜在随机变量和数据维度上的自回归模型的组合。然而，对于最具挑战性的自然图像任务，带有随机变量的纯自回归模型仍然优于随机自回归模型的组合。 在本文中，我们提出了对VAE框架的简单补充，通过在随机层中嵌入空间信息来概括自然图像。当随机变量的特征图参数化与自回归PixelCNN方法相结合时，我们在MNIST、OMNIGLOT、CIFAR10和ImageNet上明显改善了最先进的结果。有趣的是，我们也观察到在没有自回归部分时接近最先进的结果。
普通的递归网络深受梯度消失问题的困扰，而门控神经网络（GNN），如长短时记忆（LSTM）和门控递归单元（GRU）通过复杂的网络设计在许多序列学习任务中提供了有希望的结果。本文展示了我们如何通过分析GNN的门控机制来解决普通递归网络的这个问题。 我们提出了一种新的网络，称为递归身份网络（RIN），它允许普通的递归网络克服梯度消失问题，同时在不使用门的情况下训练非常深的模型。我们在多个序列建模基准上将该模型与IRNN和LSTM进行了比较。
最近，人们对强化学习（RL）中的安全和稳健技术兴趣大增。目前RL中的风险概念未能捕捉到系统故障的可能性，如系统故障导致的突然停止或超过安全阈值，以及在这种情况下的适当响应控制。我们提出了一种新的RL中的容错方法，其中控制器学习的策略可以应对对抗性攻击和导致系统子组件故障的随机停止。 本文的结果还包括容错（FT）控制，使控制器学会避免带有系统故障风险的状态。通过证明该类问题是由SG的变体表示的，我们证明了一个解决方案的存在，这是游戏的唯一固定点均衡，并描述了最佳控制器行为。
我们提出了一个新的框架，从单一运动模糊的图像中生成干净的视频帧。虽然广泛的文献集中在从模糊的图像中恢复单一的图像，在这项工作中，我们处理一个更具挑战性的任务，即从模糊的图像中修复视频。 我们的框架是基于带有空间变换器网络模块的编码器-解码器结构，以端到端的方式恢复视频序列及其底层运动。我们设计了一个损失函数和具有互补特性的正则器，以稳定训练和分析拟议网络的变体模型。通过在两种不同类型的数据集上进行大量的实验，突出了我们网络的有效性和可转移性：从全景场景产生的相机旋转模糊和高速视频的动态运动模糊。
深度学习模型的高性能通常以相当大的模型尺寸和计算时间为代价。这些因素限制了在手机或嵌入式系统等内存和电池有限的设备上部署的适用性。在这项工作中，我们提出了一种新的修剪技术，根据与网络其他部分相比的相对L1-norm消除整个过滤器和神经元，产生更多的压缩和减少参数中的冗余度。 我们通过对LeNet-5、ResNet-56和ResNet-110分别实现97.4%、47.8%和53%的压缩，证明了我们方法的可行性，超过了ResNet上报告的最先进的压缩结果，而与基线相比，没有损失任何性能。我们的方法不仅表现出良好的性能，而且容易在许多架构上实现。
最近，神经网络在解决困难的决策任务方面的成功激励了 "边缘 "智能决策的融入。然而，由于内存和计算的限制，特别是在新兴的非易失性内存系统中，写入能量成本高且会减少寿命，这项工作传统上侧重于神经网络推理，而不是训练。然而，在边缘训练的能力正变得越来越重要，因为它可以实现实时适应设备漂移和环境变化、用户定制和跨设备联合学习等应用。 在这项工作中，我们解决了在具有非易失性存储器的边缘设备上进行训练的四个关键挑战：低权重更新密度、权重量化、低辅助存储器和在线学习。我们提出了一个低等级训练方案，在保持计算效率的同时解决了这四个挑战。然后，我们在几个适应问题的代表性卷积神经网络上演示了该技术，它在准确性和权重更新数量上都优于标准SGD。
知识提取技术被用来将神经网络转换为符号描述，目的是产生更易理解的学习模型。核心挑战是找到一个比原始模型更易理解的解释，同时仍然忠实地表达该模型。 深度网络的分布式性质使得许多人认为神经网络的隐藏特征不能用简单到可以被人类理解的逻辑描述来解释，因此应该放弃分解式知识提取，而采用其他方法。在本文中，我们通过提出一种使用textit{M-of-N}规则的知识提取方法来系统地研究这个问题，该方法允许我们映射描述卷积神经网络（CNN）中隐藏特征的规则的复杂性/准确度景观。 本文报告的实验表明，这种景观的形状揭示了可理解性和准确性之间的最佳权衡，表明每个潜在变量都有一个最佳的textit{M-of-N}规则来描述其行为。我们发现，在第一层和最后一层具有最佳权衡的规则具有高度的可解释性，而在第二层和第三层具有最佳权衡的规则的可解释性较低。结果阐明了从深度网络提取规则的可行性，并指出了分解性知识提取作为一种可解释方法的价值。
最近的研究结果表明，深度生成模型可以判断出分布外的样本比来自与训练数据相同分布的样本更有可能。在这项工作中，我们专注于变异自动编码器（VAE），并解决图像数据上似然估计错误的问题。 我们开发了一个新的似然函数，它不仅基于VAE返回的参数，还基于以自我监督的方式学习的数据特征。通过这种方式，该模型额外地捕捉了通常的VAE似然函数所忽略的语义信息。
强化学习和连续控制问题的直接策略梯度方法是一种流行的方法，原因有很多。1）它们很容易实现，不需要明确了解基础模型；2）它们是一种 "端到端 "的方法，直接优化感兴趣的性能指标；3）它们本质上允许丰富的参数化策略。 一个明显的缺点是，即使在最基本的连续控制问题（线性二次调节器）中，这些方法也必须解决一个非凸的优化问题，从计算和统计的角度对其效率了解甚少。 这项工作填补了这一空白，表明（无模型的）政策梯度方法在全球范围内收敛到最优解，并且在其样本和计算复杂性方面是有效的（在相关的问题依赖量中是多项式的）。
使用LSTM或更简单的技术计算的低维向量嵌入，是捕捉文本 "意义 "的流行方法，也是对下游任务有用的无监督学习形式，然而，它们的力量在理论上没有得到理解。 这导致了一个关于LSTM的新的理论结果：从低内存LSTM中得到的低维嵌入被证明在分类任务上至少和BonG向量上的线性分类器一样强大，但误差很小，这是迄今为止大量经验性工作无法证明的结果。 我们的实验支持这些理论发现，并在标准基准上建立了强大的、简单的、无监督的基线，在某些情况下，这些基线在词级方法中是最先进的。我们还展示了GloVe和word2vec等嵌入的一个令人惊讶的新属性：它们为文本形成了一个良好的感应矩阵，比随机矩阵、标准稀疏恢复工具更有效，这可能解释了为什么它们在实践中导致了更好的表示。
    一些传统的变换，如离散Walsh-Hadamard变换（DWHT）和离散余弦变换（DCT）已被广泛用作图像处理中的特征提取器，但很少应用于神经网络。然而，我们发现这些传统的变换有能力在DNN中捕获跨通道的相关性，而没有任何可学习的参数。本文首先提出将传统的变换应用于点状卷积，表明这种变换大大降低了神经网络的计算复杂性，而精度性能却下降。 特别是对于DWHT，它不需要浮点乘法，而只需要加法和减法，这可以大大减少计算开销。此外，它的快速算法进一步降低了浮点加法的复杂性，从O(n^2)到O(nlog n)。 这些非参数化和低计算的特性在数量参数和操作方面构建了极其有效的网络，享受准确性的提升。我们提出的基于DWHT的模型在CIFAR 100数据集上与它的基线模型（MoblieNet-V1）相比，获得了1.49%的准确性提升，参数减少79.4%，FLOPs减少48.4%。
我们引入了 `emph `lattice representation learning `的概念，其中一些感兴趣的对象（如句子或图像）的表示是欧几里得空间中的一个格子点。 我们的主要贡献是将采用格子量化的目标函数替换为不存在量化的目标函数，从而使基于梯度下降的优化技术得以应用；我们将由此产生的算法称为emph{dithered stochastic gradient descent}算法，因为它们被明确设计为允许只采用局部信息的优化程序。 我们还认为，变异自动编码器中常用的技术（高斯先验和高斯近似后验）与格子表示的思想紧密相连，因为良好的高维格子的量化误差可以被建模为高斯分布。 我们使用一个传统的编码器/解码器架构来探索格子表征的思想，并通过修改\texttt{OpenNMT-py}通用的\texttt{seq2seq}架构，使其不仅可以实现高斯抖动的表征，还可以实现众所周知的直通估计器及其对矢量量化的应用，从而提供了使用格子表征潜力的实验证据。
有很多人试图解释准确性和对抗性鲁棒性之间的权衡。然而，对具有类似人类鲁棒性的鲁棒性分类器的行为没有明确的理解。 我们认为（1）为什么我们需要考虑针对不同扰动程度的对抗性鲁棒性，而不仅仅是关注固定的扰动阈值；（2）为什么我们需要使用不同的方法来生成对抗性扰动的样本，以用于训练鲁棒分类器和测量分类器的鲁棒性；（3）为什么我们需要优先考虑不同程度的对抗性精度。 我们还提出了一个候选的甲骨文分类器，称为 "最佳词汇学真实鲁棒性分类器（OLGRC）"，它优先考虑由较小幅度的扰动产生的有意义的对抗性扰动例子的准确性。  与现有的对抗性训练方法不同，估计OLGRC的训练算法需要词法优化。为了将词法优化应用于神经网络，我们利用梯度外显记忆（GEM），它最初是为防止灾难性遗忘而开发的持续学习。
生成复杂的离散分布仍然是机器学习中具有挑战性的问题之一。现有的生成具有高自由度的复杂分布的技术依赖于标准的生成模型，如生成对抗网络（GAN）、Wasserstein GAN和相关的变化。这些模型是基于涉及两个连续分布之间距离的优化。 我们引入了离散Wasserstein GAN（DWGAN）模型，该模型基于两个离散分布之间的Wasserstein距离的双重表述。我们根据该表述推导出一种新的训练算法和相应的网络结构。对合成的离散数据和MNIST手写数字的真实离散数据都提供了实验结果。
我们介绍了开放的、模块化的、自我改进的Omega人工智能统一架构，这是从第一原理考虑的Solomonoff的Alpha架构的改进。该架构体现了通用智能的几个关键原则，包括表示的多样性、数据类型的多样性、集成记忆、模块化和高阶认知。我们保留了一个基本算法基底的基本设计，称为 "AI内核"，用于解决问题和基本认知功能，如记忆，以及一个更大的模块化架构，以多种方式重新使用该内核。 Omega包括八种表示语言和六类神经网络，这些都是简单介绍的。该架构最初是为了解决数据科学自动化，因此它包括许多统计任务的问题解决方法。我们回顾了广泛的软件架构、高阶认知、自我改进、模块化神经架构、智能代理、进程和记忆层次、硬件抽象、对等计算和数据抽象设施。
对话式机器理解需要对对话历史有深刻的理解。为了使传统的、单一回合的模型能够全面地编码历史，我们引入了Flow，一种能够通过交替的并行处理结构，将回答以前问题过程中产生的中间表征纳入其中的机制。与将以前的问题/回答连接起来作为输入的浅层方法相比，Flow更深刻地整合了对话历史的潜在语义。 我们的模型，FlowQA，在最近提出的两个对话挑战中显示出卓越的性能（在CoQA上+7.2%的F1，在QuAC上+4.0%）。Flow的有效性也显示在其他任务中。通过将顺序指令理解减少到对话式机器理解，FlowQA在SCONE的所有三个领域都超过了最佳模型，准确性提高了+1.8%-+4.4%。
我们考虑了强化学习和匪徒结构的预测问题，这些问题的损失反馈非常稀少：只有在一集结束时才有。我们介绍了一种新的算法，RESLOPE（RESLOPE），它通过自动学习更密集的奖励函数的内部表示来解决此类问题。 RESLOPE作为上下文匪徒的一个缩减操作，使用其学习的损失表示来解决信用分配问题，并使用上下文匪徒神谕来权衡探索和利用。RESLOPE享有无悔缩减式的理论保证，在MDP环境和匪徒结构化预测设置中都优于最先进的强化学习算法。
对抗性神经网络解决了数据科学中的许多重要问题，但在训练上是出了名的困难。这些困难来自于对抗性网络的最佳权重对应于损失函数的鞍点，而不是最小化。通常用于此类问题的交替随机梯度方法不能可靠地收敛到鞍点，当收敛发生时，往往对学习率非常敏感。 我们提出了一种稳定对抗性网络的随机梯度下降法的简单修改。我们在理论和实践中都表明，所提出的方法可靠地收敛到鞍点。这使得对抗性网络不太可能 "崩溃"，并能以更大的学习率进行快速训练。
在可解释规划中出现的一种重要问题是对比性问题，其形式为 "为什么是行动A而不是行动B？"。这类问题可以用对比性解释来回答，即比较包含A的原始计划和包含B的对比性计划的属性。这种类型的有效解释有助于突出规划者所做的决定与用户所期望的不同，并提供对模型和规划过程的进一步了解。 本文介绍了与领域无关的、将用户问题汇编成约束条件的方法。这些约束条件被添加到规划模型中，因此，新模型的解决方案代表了对比性计划。
学习图中节点表征的方法在网络分析中起着关键作用，因为它们能够实现许多下游学习任务。我们提出Graph2Gauss--一种能够在大规模（归属）图上有效学习多功能节点嵌入的方法，在链接预测和节点分类等任务上表现出强大的性能。 与大多数将节点表示为低维连续空间中的点向量的方法不同，我们将每个节点嵌入为高斯分布，使我们能够捕捉到表示的不确定性。此外，我们提出了一种无监督的方法，可以处理归纳学习场景，并适用于不同类型的图：普通/归属，有向/无向。 通过利用网络结构和相关的节点属性，我们能够对未见过的节点进行归纳，而不需要额外的训练。为了学习嵌入，我们采用了一种个性化的节点距离排名方法，利用网络结构对节点进行自然排序。 在真实世界的网络上的实验证明了我们方法的高性能，在几个不同的任务上超过了最先进的网络嵌入方法。此外，我们证明了建模不确定性的好处--通过分析它，我们可以估计邻域多样性和检测图的内在潜在维度。
虽然在使神经网络在广泛的任务中有效方面已经取得了很大的进展，但许多神经网络对其输入的小的、精心选择的扰动（被称为对抗性例子）的脆弱性令人惊讶。在本文中，我们提倡并实验研究使用logit正则化技术作为对抗性防御，它可以与其他方法一起使用，以很少或没有成本来创造对抗性鲁棒性。 我们证明了最近一种对抗性防御机制的大部分有效性可以归功于对数正则化，并展示了如何提高其对白盒和黑盒攻击的防御能力，在这个过程中，对基于PGD的模型产生了更强的黑盒攻击。
在深度学习中，性能受到架构和超参数选择的强烈影响。虽然在简单空间的自动超参数优化方面有大量工作，但复杂空间，如深度架构空间，在很大程度上仍未被探索。 在本文中，我们描述了一个自动设计和训练深度模型的框架。我们提出了一个可扩展和模块化的语言，允许人类专家紧凑地表示复杂的搜索空间，包括架构及其超参数。 我们可以利用搜索空间的结构来引入不同的模型搜索算法，如随机搜索、蒙特卡洛树状搜索（MCTS）和基于模型的顺序优化（SMBO）。我们提出了在CIFAR-10上比较不同算法的实验，并表明MCTS和SMBO优于随机搜索。 我们还介绍了在MNIST上的实验，显示相同的搜索空间在少数样本的情况下达到了接近最先进的性能。这些实验表明，我们的框架可以有效地用于模型辨别，因为它可以描述富有表现力的搜索空间并发现有竞争力的模型，而不需要人类专家做很多努力。
深度学习网络在图像分类和物体检测等计算机视觉工作负载上取得了最先进的精度。然而，高性能的系统通常涉及具有许多参数的大模型。 在本文中，我们研究了这两种技术的结合，并表明低精度网络的性能可以通过使用知识蒸馏技术得到显著改善。我们称我们的方法为学徒，并展示了在ImageNet数据集上使用三元精度和4位精度的ResNet架构的许多变种的最先进的精度。我们研究了三种方案，其中可以将知识蒸馏技术应用于训练和部署管道的各个阶段。
深度神经网络（DNN）被广泛用于许多应用中。然而，它们在边缘设备上的部署一直很困难，因为它们是资源饥渴型的。二进制神经网络（BNN）有助于缓解DNN令人望而却步的资源需求，在这种情况下，激活和权重都被限制在1位。我们提出了一种改进的二进制训练方法（BNN+），通过引入一个鼓励围绕二进制值训练权重的规范化函数。 此外，我们在后向计算中使用了符号激活函数的改进的近似值。这些添加是基于线性操作，很容易在二进制训练框架中实现。我们在CIFAR-10上展示了实验结果，在AlexNet上获得了86.5%的准确性，在VGG网络上获得了91.3%的准确性。在ImageNet上，我们的方法也优于传统的BNN方法和XNOR-net，使用AlexNet的最高1级准确性分别为4%和2%。
聚类是一种基本的机器学习方法。其结果的质量取决于数据的分布。由于这个原因，深度神经网络可以用来学习更好的数据表征。在本文中，除了对该领域的方法进行回顾外，我们还提出了一个系统的深度学习聚类的分类法。 基于我们的分类法，创建新的方法是比较直接的。我们还提出了一种新的方法，它建立在分类法的基础上，超越了以前一些工作的局限性。我们在图像数据集上的实验评估表明，该方法接近最先进的聚类质量，在某些情况下表现得更好。
生成模型经常使用人类评价来确定和证明进展。不幸的是，现有的人类评价方法是临时性的：目前还没有标准化的、经过验证的评价。(1)测量知觉保真度，(2)可靠，(3)将模型分成明确的等级顺序，(4)确保高质量的测量而没有难以解决的成本。 作为回应，我们构建了Human-eYe Perceptual Evaluation (HYPE)，这是一个人类衡量标准，(1)基于感知的心理物理学研究，(2)在模型的不同随机抽样输出集合中是可靠的，(3)导致可分离的模型性能，以及(4)在成本和时间上是高效的。我们使用两个数据集，即流行的CelebA和较新的高分辨率FFHQ，以及两种模型输出的采样技术，在四个最先进的生成式对抗网络（GAN）中测试了HYPE在无条件图像生成方面的表现。 通过多次模拟HYPE的评估，我们展示了不同模型的一致排名，确定了在FFHQ上采用截断技巧采样的StyleGAN（HYPE-Infinity欺骗率为27.6%，大约四分之一的图像被人类错误分类）优于没有截断的StyleGAN（19.0%）。
由于深度学习的最新进展和大规模平行语料库的可用性，机器翻译最近取得了令人印象深刻的性能。已经有许多尝试将这些成功扩展到低资源语言对，但需要数以万计的平行句子。在这项工作中，我们将这个研究方向推向极端，研究是否有可能在没有任何平行数据的情况下学习翻译。 我们提出了一个模型，它从两种不同语言的单语语料库中获取句子，并将它们映射到相同的潜在空间中。通过学习从这个共享的特征空间中重建两种语言，该模型在不使用任何标记数据的情况下有效地学习翻译。我们在两个广泛使用的数据集和两种语言对上展示了我们的模型，在Multi30k和WMT英语-法语数据集上的BLEU分数为32.8和15.1，训练时甚至没有使用一个平行句。
我们为多代理强化学习（MARL）推导出一种新的内在社会动机，其中代理因对另一代理的行动有因果影响而得到奖励，其中因果影响是用反事实推理来评估的。该奖励不依赖于观察另一代理的奖励函数，因此是一种比以往工作更现实的MARL方法。 我们表明，因果影响的奖励与代理人行动之间的相互信息最大化有关。我们在具有挑战性的社会困境环境中测试了这种方法，它始终导致代理人之间的合作增强，集体奖励提高。 此外，我们发现奖励影响可以导致代理开发出新兴的通信协议。因此，我们也采用影响来训练代理使用明确的通信渠道，并发现它导致了更有效的通信和更高的集体奖励。最后，我们表明，影响可以通过给每个代理配备一个预测其他代理行动的内部模型来计算。
这项工作采用了关于强化学习的非常成功的分布式观点，并将其适应于连续控制设置。我们将其结合在一个分布式的非政策学习框架内，以开发我们称之为分布式分布式深度确定政策梯度算法，D4PG。我们还将这种技术与一些额外的、简单的改进相结合，如使用N步返回和优先经验回放。 实验中，我们研究了这些单个组件的贡献，并展示了它们之间的相互作用，以及它们的综合贡献。我们的结果显示，在各种简单的控制任务、困难的操纵任务和一组基于障碍物的艰难运动任务中，D4PG算法都达到了最先进的性能。
状态-动作值函数（即。Q值）在强化学习（RL）中无处不在，产生了SARSA和Q-learning等流行算法。我们提出了一个新的行动值概念，该概念由SARSA中使用的预期Q值的高斯平滑版本定义。 我们表明，这种平滑的Q值仍然满足贝尔曼方程，使它们可以自然地从环境采样的经验中学习。此外，预期奖励相对于参数化高斯策略的平均值和协方差的梯度可以从平滑的Q值函数的梯度和Hessian中恢复出来。 基于这些关系，我们开发了新的算法，用于直接从学到的Q值近似值中训练高斯策略。该方法也适用于近似优化技术，通过对KL-偏离先前策略的惩罚来增加目标。
交互式小说游戏是基于文本的模拟，其中一个代理纯粹通过自然语言与世界互动。它们是研究如何扩展强化学习代理的理想环境，以应对自然语言理解、部分可观察性和基于组合大文本的行动空间中的行动生成等挑战。 我们提出了KG-A2C，一个在探索时建立动态知识图谱的代理，并使用基于模板的行动空间生成行动。我们认为，知识图谱在推理游戏状态和约束自然语言生成方面的双重用途是可扩展的探索组合大自然语言行动的关键。在各种IF游戏中的结果表明，尽管行动空间的大小呈指数级增长，KG-A2C仍优于当前的IF代理。
众所周知，神经网络是通用的逼近器，但在实践中，更深的网络往往比更浅的网络更强大。我们通过证明：对于深层神经网络来说，逼近n个变量的多变量多项式的自然类所需的神经元总数m只随n线性增长，但当只允许有一个隐藏层时，会呈指数级增长。 我们还提供证据表明，当隐藏层的数量从1增加到k时，神经元的需求不是随n而是随n^{1/k}呈指数增长，这表明实际可表达性所需的最小层数只随n呈对数增长。
近年来，卷积神经网络（CNN）在科学、技术和工业领域产生了巨大的影响，然而CNN架构设计的理论机制仍然令人惊讶地模糊。CNN的神经元，包括其独特的元素--卷积滤波器，已知是可学习的特征，然而它们在产生输出中的单独作用相当不明确。 这项工作的论点是，并非所有的神经元都同样重要，其中一些神经元含有更多有用的信息来执行特定的任务。因此，我们建议对神经元的重要性进行量化和排序，并直接将神经元的重要性纳入两个公式的目标函数中。(1)基于Shapley值的博弈论方法，计算每个过滤器的边际贡献；(2)基于我们所说的概率方法，使用变异推理的重要性开关。使用这两种方法，我们证实了一些神经元本质上比其他神经元更重要的一般理论。各种实验说明，学到的等级可以随时用于结构化网络压缩和学习特征的可解释性。
这项工作提出了一种模块化和分层的方法来学习探索三维环境的策略。我们的方法利用了经典方法和基于学习的方法的优势，通过使用带有学习映射器的分析路径规划器，以及全局和局部策略，使用学习提供了与输入模式有关的灵活性（在映射器中），利用世界的结构规律性（在全局策略中），并提供对状态估计错误的鲁棒性（在局部策略中）。 这种在每个模块内的学习的使用保留了它的好处，同时，分层分解和模块化训练使我们能够避开与训练端到端策略相关的高样本复杂性。我们在视觉和物理现实的模拟三维环境中的实验证明了我们提出的方法比过去的学习和基于几何的方法有效。
计算机视觉的深度学习主要取决于监督的来源。照片逼真的模拟器可以产生大规模的自动标记的合成数据，但引入了对性能有负面影响的领域差距。我们提出了一种新的无监督的领域适应算法，称为SPIGAN，依赖于模拟器特权信息（PI）和生成对抗网络（GAN）。 我们使用模拟器的内部数据作为目标任务网络训练期间的PI。我们在语义分割上对我们的方法进行了实验评估。我们在真实世界的Cityscapes和Vistas数据集上训练网络，只使用无标签的真实世界图像和来自SYNTHIA数据集的具有z-buffer（深度）PI的合成标签数据。我们的方法比没有适应和最先进的无监督领域适应技术有所改进。
对抗性训练是对抗性攻击的主要防御手段之一。在本文中，我们提供了第一个关于诊断ImageNet上大规模对抗性训练元素的严格研究，它揭示了两个有趣的特性。首先，我们研究了规范化的作用。批量规范化（BN）是在许多视觉任务上实现最先进性能的关键因素，但我们表明它可能会阻止网络在对抗性训练中获得强大的鲁棒性。一个意想不到的观察是，对于用BN训练的模型，只需从训练数据中删除干净的图像，就能大大提升对抗性鲁棒性，即。18.3%。我们将这一现象与清洁图像和对抗性图像来自两个不同领域的假设联系起来。这个双域假设可以解释用清洁和对抗性图像的混合物进行训练时的BN问题，因为估计这种混合物分布的归一化统计是具有挑战性的。第二，我们研究了网络容量的作用。我们发现我们所谓的 "深度 "网络对于对抗性学习的任务来说仍然很浅。ResNet-152），对抗性训练对更深的网络表现出更强烈的需求，以实现更高的对抗性鲁棒性。即使将网络容量推到前所未有的规模，即ResNet-638，也可以观察到这种鲁棒性的改善。 
深度神经网络（DNN）相对于输入的梯度提供了可用于解释输入特征的输出预测的信息，并被广泛研究以帮助解释DNN。在一个线性模型（即$g(x)=wx+b$）中，梯度只对应于权重$w$.这样的模型可以合理地局部线性地接近一个平滑的非线性DNN，因此，这个局部模型的权重就是梯度.然而，局部线性模型的另一部分，即。然而，局部线性模型的另一部分，即偏见$b$，通常在归因方法中被忽视，因为它不是梯度的一部分。在本文中，我们观察到，由于DNN中的偏见对预测的正确性也有不可忽视的贡献，它也可以在理解DNN行为中发挥重要作用。 特别是，我们研究了如何将DNN的偏差归于其输入特征。我们提出了一种反向传播型算法 "偏差反向传播（BBp）"，该算法从输出层开始，迭代地将每一层的偏差归于其输入节点，并结合上一层的偏差项。 这个过程在输入层停止，在那里将所有输入特征的归属相加起来，正好恢复了$b$。与产生$w$的梯度的反向传播一起，我们可以完全恢复局部线性模型$g(x)=wx+b$。 因此，DNN输出对其输入的归因被分解成两部分，即梯度$w$和偏置归因，提供独立的互补解释。我们研究了几种可能的归因方法，应用于BBp中每一层的偏置。在实验中，我们表明BBp除了基于梯度的归因外，还能对DNN产生互补的、高度可解释的解释。
本文提出了一种自主寻找信号中的周期性的方法。它基于Vlachos等人在2005年提出的使用傅里叶变换和自相关函数的相同想法。虽然显示了有趣的结果，但这种方法在噪声信号或具有多个周期性的信号上表现不佳。因此，我们的方法增加了几个新的额外步骤（提示聚类、过滤和去趋势）来解决这些问题。实验结果显示，提出的方法优于目前的算法。
我们提出了一种对抗性探索策略，这是一种简单而有效的模仿学习方案，激励人们在没有任何外在奖励或人类示范的情况下对环境进行探索。我们的框架由一个深度强化学习（DRL）代理和一个反向动力学模型相互竞争组成。 在这样的竞争环境中，DRL代理学习生成逆向动力学模型未能正确预测的样本，而逆向动力学模型则学习适应具有挑战性的样本。我们进一步提出一种奖励结构，确保DRL代理只收集适度困难的样本，而不是过于困难的样本，以免逆向模型无法有效模仿。 我们评估了我们的方法在几个OpenAI体育馆机器人手臂和手部操纵任务上的有效性，并与一些基线模型进行了对比。实验结果表明，我们的方法与直接用专家示范训练的方法相当，甚至在没有任何人类先验的情况下也优于其他基线。
本文提出了一个双变异自动编码器（DualVAE），这是一个生成与多类标签相对应的图像的框架。最近对条件生成模型的研究，如条件VAE，通过改变标签表现出图像转移。然而，当多类标签的维度很大时，这些模型不能改变与标签相对应的图像，因为学习相应类别的多个分布对于转移图像是必要的，这导致缺乏训练数据。 因此，DualVAE可以通过将潜伏向量向决策边界移动来轻松转移图像，并且对多类标签的缺失值具有鲁棒性。为了评估我们提出的方法，我们引入了条件接纳分数（CIS）来衡量图像对目标类的改变程度。我们使用CelebA数据集中的CIS来评估DualVAE转移的图像，并在多类环境中展示了最先进的性能。
深度学习最显著的贡献之一是将卷积神经网络（ConvNets）应用于结构化信号分类，特别是图像分类。除了它们在监督学习中令人印象深刻的表现，这种网络的结构激发了被称为散射变换的深度滤波器组的发展。这些变换应用小波变换和复杂模数算子的级联来提取对分组操作不变的特征，并对变形稳定。 此外，ConvNets启发了几何深度学习的最新进展，其目的是通过应用图信号处理的概念来学习深度图滤波器级联，从而将这些网络推广到图数据。 我们展示了用这种设计的深度滤波器组提取的特征在生物化学和社会网络数据的图分类中的效用（包括后者的最新结果），以及在数据探索中的效用，在那里它们能够推断出酶进化中的EC交换偏好。
OCR不可避免地与NLP联系在一起，因为它的最终输出是文本。文档智能的进步促使人们需要一种统一的技术，将OCR与各种NLP任务结合起来，特别是语义解析。由于到目前为止，OCR和语义解析被作为独立的任务来研究，每个任务本身的数据集很丰富，而那些整合后的OCR解析任务则相对不足。 在这项研究中，我们发布了一个用于收据解析的综合数据集，作为实现OCR后解析任务的第一步。该数据集由数千张印尼收据组成，其中包含用于OCR的图像和盒子/文本注释，以及用于解析的多级语义标签，建议的数据集可用于解决各种OCR和解析任务。
卷积神经网络（CNN）复杂性的增长使人们对在训练期间将一个网络划分到多个加速器上并在加速器上进行流水线式的反向传播计算越来越感兴趣。现有的方法通过微调度或权重存储等技术避免或限制陈旧权重的使用。 我们使用了4个CNN（LeNet-5、AlexNet、VGG和ResNet），并表明当管道化被限制在网络的早期层时，使用陈旧权重的训练会收敛，并导致模型的推理精度与MNIST和CIFAR-10数据集上的非管道化训练的精度相当；4个网络的精度分别下降0.4%、4%、0.83%和1.45%。 我们在PyTorch中使用ResNet演示了我们的流水线反向传播在2个GPU上的实现和性能，与1个GPU的基线相比，实现了高达1.8倍的速度提升，但推理精度却有小幅下降。
尽管它们的性能令人印象深刻，但深度神经网络在分布之外的输入上表现出惊人的失败。对抗性例子研究的一个核心思想是揭示这种分布变化下的神经网络错误。我们将这些错误分解为两个互补的来源：敏感性和不变性。 我们表明，深度网络不仅对其输入的任务无关的变化过于敏感，正如从ε-adversarial例子中所知道的那样，而且对广泛的任务有关的变化也过于不变，从而使输入空间的广大区域容易受到对抗性攻击。 我们表明这种过度不变性发生在各种任务和结构类型中。在MNIST和ImageNet上，人们可以操纵几乎任何图像的特定类别内容而不改变隐藏的激活。我们发现标准交叉熵损失的不足是这些失败的原因之一。
基于流量的生成模型是强大的精确似然模型，具有高效的采样和推理功能。尽管它们的计算效率很高，但与最先进的自回归模型相比，基于流的模型的密度建模性能通常要差得多。在本文中，我们调查并改进了基于流的模型在先前工作中采用的三种限制性设计选择：使用均匀噪声进行去量化，使用不表达的仿射流，以及在耦合层中使用纯粹的卷积调节网络。 基于我们的发现，我们提出了Flow++，一个新的基于流的模型，它现在是标准图像基准上无条件密度估计的最先进的非自回归模型。我们的工作已经开始缩小迄今为止自回归模型和基于流的模型之间存在的显著性能差距。
现代深层人工神经网络通过具有比训练实例多出几个数量级的参数的模型，在正则化的帮助下控制过拟合，取得了令人印象深刻的结果。正则化可以是隐性的，如随机梯度下降和卷积层中的参数共享，也可以是显性的。显性正则化技术，最常见的形式是权重衰减和放弃，在改善泛化方面被证明是成功的，但它们盲目地降低了模型的有效容量，引入了敏感的超参数，需要更深和更宽的架构来补偿容量的减少。 相比之下，数据增强技术利用领域知识来增加训练实例的数量并提高泛化能力，而不会降低有效容量，也不会引入模型依赖的参数，因为它是在训练数据上应用的。在本文中，我们在三个流行的架构和三个数据集上系统地对比了数据增强和显式正则化。我们的结果表明，单独的数据增强可以实现与正则化模型相同或更高的性能，并对架构和训练数据量的变化表现出更高的适应性。
对抗性特征学习（AFL）是明确约束神经网络学习所需表征的有前途的方法之一；例如，AFL可以帮助学习匿名表征以避免隐私问题。AFL通过训练网络来欺骗预测网络敏感信息的对手，因此，AFL的成功很大程度上依赖于对手的选择。(本文提出了一种新的对手设计，即{随机子空间上的多个对手}，该设计体现了{随机性}的概念。相比之下，所提出的方法被设计成不那么脆弱，通过利用独立分类器的集合，每个分类器都试图从不同的{em子集}的表征中预测敏感变量。对三个用户匿名化任务的经验验证表明，我们提出的方法在所有三个数据集中都实现了最先进的性能，而没有明显损害数据的效用。这很重要，因为它给出了关于设计对手的新含义，这对提高AFL的性能很重要。
这方面的一个例子是体内钙成像数据，其中数据是由管理神经元中钙通量的低阶动态系统产生的，而该系统本身是由神经计算的高阶动态系统驱动的。 最近一种使用顺序变异自动编码器的方法表明，有可能通过使用被建模为泊松过程的加压数据来学习大脑中伸手行为期间计算的单一动态系统的潜在动态。 在这种方法中，尖峰事件驱动低阶的钙动力学，并且本身由高阶的潜在动力学系统控制。我们通过产生点火率、对尖峰序列进行采样以及将尖峰序列转换为荧光瞬态来产生合成数据，这两个动力学系统在最近的文献中被用作关键基准：洛伦兹吸引子和混沌的递归神经网络。 然而，尽管我们的模型能够很好地从混沌神经网络中重建基本的尖峰率和钙瞬态，但它在重建发射率方面的表现不如从钙数据中推断尖峰的基本技术。这些结果表明，VLAEs是生命科学中对层次动态系统数据进行建模的一种有前途的方法，但推断低阶系统的动力学有可能用更简单的方法来实现。
尽管之前有很多关于神经网络权重或激活的量化工作，但在软件量化器和低精度加速器的实现之间仍有很大差距，由于设计阶段缺乏软件和硬件的协调，导致网络或硬件的效率下降。 在本文中，我们提出了一个用于整数神经网络处理器的学习型线性对称量化器，它不仅将神经参数和激活量量化为低位整数，而且还通过使用批量归一化融合和低精度累加器（如。16位）和乘法器（如。我们使用统一的方法对权重和激活进行量化，结果超过了许多以前的方法，适用于各种网络，如AlexNet、ResNet和像MobileNet这样的轻量级模型，同时对加速器架构保持友好。另外，我们还将该方法应用于物体检测模型，并见证了YOLO-v2的高性能和准确性。 最后，我们在我们专门的只用整数算术的DNN加速器上部署了量化模型，以显示所提出的量化器的有效性。我们表明，即使采用线性对称量化，在4位网络中的结果也比非对称或非线性方法好。在评估中，当按照整数处理器的要求对整个网络进行量化时，所提出的量化器在ResNet18、ResNet34和AlexNet中引起的精度下降不到0.4\%。
运行高性能卷积神经网络（CNN）的高昂能量成本一直限制着它们在资源受限的平台上的部署，包括移动和可穿戴设备。我们提出了一个用于能源感知动态路由的CNN，称为EnergyNet，它实现了基于输入的自适应复杂度推理，导致运行时间能量成本的整体降低，而不会明显损失（甚至提高）精度。 这是通过提出捕获计算和数据移动成本的能量损失来实现的。我们将其与面向精度的损失结合起来，并学习一种跳过网络中某些层的动态路由策略，以优化混合损失。 我们的实证结果表明，与基线CNN相比，EnergyNet可以在CIFAR10和Tiny ImageNet测试集的推理过程中分别减少40%和65%的能量成本，同时保持相同的测试精度。 更令人鼓舞的是，能量意识可以作为训练正则化，甚至可以提高预测精度：我们的模型在CIFAR-10上节省27%的能量时，测试精度比基线高0.7%，在Tiny ImageNet上节省50%的能量时，测试精度高1.0%。
对数线性模型模型在机器学习中被广泛使用，特别是在深度学习架构中以softmax的形式无处不在。虽然这些模型的精确推理和学习需要线性时间，但它可以在亚线性时间内近似完成，并有很强的浓度保证。在这项工作中，我们提出了LSH Softmax，一种在深度学习环境中对softmax层进行亚线性学习和推理的方法。 我们的方法依赖于流行的位置敏感哈希，使用最近的邻居和均匀的样本，建立一个很好的集中梯度估计器。我们还提出了一个使用Gumbel分布的LSH Softmax的亚线性时间的推理方案。在语言建模方面，我们表明，用LSH Softmax训练的循环神经网络与计算精确的softmax表现相当，同时需要亚线性计算。
强化学习方法在简单组合优化问题上的成功能否扩展到多机器人顺序分配规划上？除了在大型问题上实现接近最佳性能的挑战外，对未见过的机器人和任务数量的可转移性是现实世界应用的另一个关键挑战。在本文中，我们提出了一种方法，在机器人/机器调度问题上首次实现了这两个挑战。 我们的方法由三部分组成。首先，我们表明任何机器人调度问题都可以表达为随机概率图形模型（PGM），我们为随机PGM开发了一种均值场推理方法，并将其用于Q-函数推理。第二，我们表明通过精心设计问题状态的两步顺序编码可以实现可转移性。第三，我们通过提出一种基于启发式拍卖的Q-迭代拟合方法来解决拟合Q-迭代的计算扩展性问题。 我们将我们的方法应用于离散时间、离散空间问题（多机器人奖励收集（MRRC）），并通过可转移性实现了97%的最优性。通过将我们的方法扩展到连续时间、连续空间的表述，我们声称是第一个在任何类型的多机器调度问题中具有可扩展性能的基于学习的方法；我们的方法在相同平行机器调度（IPMS）问题中可扩展性实现了与流行元启发法相媲美的性能。
课程学习包括学习一个困难的任务，首先训练它的简单版本，然后是越来越难的版本，最后是困难的任务。为了使这种学习有效，给定一个课程和代理的当前学习状态，我们需要找到什么是好的下一个任务来训练代理。教师-学生算法假设好的下一个任务是代理进步最快或偏离的任务。 我们首先简化和改进它们。然而，可能会出现两种有问题的情况，即代理主要在它还不能学习的任务上进行训练，或者它已经学会了。因此，我们引入了一种新的算法，使用最小最大排序的课程，假设好的下一个任务是那些可以学习但还没有学会的任务。它在小课程上的表现优于师生算法，在有许多任务的复杂课程上明显优于它们。
人工智能和神经科学领域有着丰富的双向互动的悠久历史。一方面，开发人工智能系统的重要灵感来自对自然智能系统的研究，特别是哺乳动物的新皮层。 另一方面，大脑模型和理论的重要灵感来自于人工智能研究。这两个领域交叉的一个核心问题是关于新皮层的学习过程，以及它们与深度网络的反向传播训练算法的类似程度。 匹配新皮层学习的数据效率、转移和泛化特性仍然是深度学习领域的一个积极研究领域。最近我们对新皮层的神经元、突触和树突生理学的理解取得了进展，为无监督的表征学习提出了新的方法，也许是通过一类新的目标函数，它可以与反向传播一起作用或代替反向传播。 将它们纳入深度网络的表征学习可以更好地利用无标签的数据集，为下游的监督读出学习提供显著的数据效率改善，并减少对对抗性扰动的敏感性，但代价是适用领域更加有限。
现实世界中的问题回答（QA）任务由成千上万的词组成，这些词通常代表许多事实和实体。现有的基于LSTM的模型需要大量的参数来支持外部记忆，并且对于长序列的输入不能很好地概括。记忆网络试图通过将信息存储到外部记忆模块来解决这些限制，但必须检查记忆中的所有输入。因此，对于长序列的输入，中间的记忆组件按比例扩大，导致推理时间差和计算成本高。 在本文中，我们提出了自适应内存网络（AMN），它处理输入的问题对，动态地构建一个优化的网络架构，以降低推理时间。然而，与以前的方法不同，AMN是一个动态的网络架构，根据问题的相关性加权创建可变数量的内存库。因此，解码器可以选择可变数量的内存库，使用较少的内存库构建一个答案，在准确性和速度之间创造一个运行时间的权衡。在我们的结果中，我们证明了我们的模型学会了根据任务的复杂性构建不同数量的记忆库，并实现了对标准bAbI任务和修改后的bAbI任务更快的推理时间。
当一个双语学生学习解决数学中的文字问题时，我们希望该学生能够用他所精通的两种语言来解决这些问题，即使数学课只用一种语言教授。 我们通过从语言学中获得灵感来学习这些表征，特别是通用语法假说，并学习与语言无关的通用潜在表征（Chomsky, 2014; Montague, 1970）。
具有离散和连续潜变量的生成模型被许多现实世界的数据集的结构所激励。然而，它们在训练中呈现出微妙的特点，往往表现为离散潜变量没有被利用。在本文中，我们展示了为什么这样的模型在使用传统的对数似然最大化进行训练时很困难，它们适合使用Wasserstein自动编码器的最佳传输框架进行训练。 我们发现我们的离散潜伏变量在训练时被模型充分利用，不需要对目标函数进行任何修改或重大微调。我们的模型在使用相对简单的神经网络时产生了与其他方法相当的样本，因为离散潜伏变量承载了大部分的描述性负担。此外，离散潜伏变量提供了对生成的重大控制。
虽然机器学习模型在顺序数据上实现了与人类相媲美的性能，但利用结构化知识仍然是一个具有挑战性的问题。空间-时间图已被证明是抽象互动图的有用工具，以前的工作利用精心设计的前馈架构来保留这种结构。 我们认为，为了将这种网络设计扩展到现实世界的问题，一个模型需要自动学习可能的关系的有意义的表示。学习这种交互结构并不简单：一方面，一个模型必须以无监督的方式发现不同问题因素之间的隐藏关系；另一方面，挖掘的关系必须是可解释的。在本文中，我们提出了一个注意力模块，能够在一个固定大小的嵌入中投射一个图的子结构，保留邻居对给定顶点的影响。在对现实世界和玩具任务的综合评估中，我们发现我们的模型对强大的基线具有竞争力。
我们介绍了NAMSG，一种用于训练神经网络的自适应一阶算法。该方法在计算和内存方面都很高效，并且易于实现。它在可配置的远程观察点计算梯度，以便通过调整随机设置中不同曲率方向的步长来加速收敛。 我们通过将训练过程建模为一个动态系统，分析了凸和非凸问题的收敛特性，并提供了一种无需网格搜索的观察因子选择策略。
生成对抗网络的最新进展得益于对框架的改进和对各种问题的成功应用，导致了对多个领域的扩展。IRGAN试图利用该框架进行信息检索（IR），这项任务可以被描述为在查询（q）的情况下，对文档（d）的正确条件概率分布p（d|q）进行建模。 提出IRGAN的工作声称，优化他们的最小损失函数将产生一个可以学习分布的生成器，但他们的设置和基线术语引导模型远离精确的对抗性表述，这项工作试图指出他们表述中的某些不准确之处。分析他们的损失曲线可以洞察损失函数中可能存在的错误，通过使用我们提出的类似协同训练的设置可以获得更好的性能，其中两个模型是以合作而不是对抗的方式进行训练。
协作式个性化，如通过学习的用户表征（嵌入），可以显著提高基于神经网络的模型的预测精度。我们提出了联合用户表征学习（FURL），这是一种简单、可扩展、保护隐私和资源高效的方式，在联合学习（FL）的设置中利用现有的神经个性化技术。 FURL将模型参数分为联盟参数和私人参数。私人参数，如私人用户嵌入，是在本地训练的，但与联盟参数不同的是，它们不会被转移到服务器上或在服务器上平均化。 我们从理论上表明，这种参数分割不会影响大多数模型个性化方法的训练。将用户嵌入存储在本地，不仅可以保护用户隐私，而且与服务器上的训练相比，还可以提高个性化的内存定位。 我们在两个数据集上评估了FURL，证明了模型质量的显著改善，性能提高了8%和51%，与集中式训练的性能水平大致相同，只降低了0%和4%。此外，我们表明在FL和集中式设置中学习的用户嵌入具有非常相似的结构，表明FURL可以通过共享参数协同学习，同时保留了用户隐私。
序列的递归模型最近在许多任务中取得了成功，特别是在语言建模和机器翻译方面。然而，从这些模型中提取良好的表示仍然具有挑战性。 为了通过这个离散表示传播梯度，我们引入了一个改进的语义散列技术。我们表明这个技术在一个新提出的定量效率措施上表现良好。我们还分析了由模型产生的潜伏代码，表明它们如何对应于单词和短语。
 自动编码和生成模型在图像和信号表示的学习和生成方面取得了巨大的成功。然而，这些模型通常采用完整的欧氏空间或有界子集（如$[0,1]^l$）作为潜空间，其琐碎的几何结构往往过于简单，不能有意义地反映数据的结构。 受微分几何学的启发，我们提出了Chart Auto-Encoder (CAE)}，它用多个图表和它们之间的过渡函数来捕捉数据的流形结构。CAE通过将整个数据集参数化为一个重叠的图表集合来翻译流形的数学定义，创建局部的潜像。 因此，CAE实现了对数据更精确的近似，并产生了现实的新数据。我们用合成和现实生活中的数据进行了实验，以证明所提出的CAE的有效性。
我们解决了连续视觉现象的建模问题。给定一个可以分为离散时间步骤的现象的例子，我们的目标是从任何这样的时间中获取一个输入，并在序列中的所有其他时间步骤中实现这个输入。此外，我们的目标是做到这一点/textit{without} ground-truth aligned sequences--避免了收集aligned数据所需的困难。 我们将周期一致性扩展到了循环一致性，并缓解了与由此产生的长计算链中的学习相关的困难。在对几个不同的数据集（包括地球的季节和人脸的老化）进行建模时，我们展示了与现有图像对图像技术相比具有竞争力的结果。
我们提出了并行随机权重平均法（SWAP），这是一种加速DNN训练的算法。我们的算法使用大型迷你批次来快速计算近似解决方案，然后通过对独立和并行计算的多个模型的权重进行平均来完善它。所产生的模型与使用小型迷你批次训练的模型具有同样的泛化能力，但产生的时间大大缩短。
加快大型卷积网络训练的常见方法是增加计算单元。然后使用数据并行的同步随机梯度下降法（SGD）进行训练，在计算单元之间划分一个小型批次。 我们认为目前大批量训练的配方（带预热的线性学习速率缩放）不够通用，训练可能会出现偏差。为了克服这些优化困难，我们提出了一种基于层级自适应速率缩放（LARS）的新训练算法。使用LARS，我们将AlexNet和ResNet-50缩放到16K的批量大小。
找到非线性动态系统的线性近似的嵌入空间可以实现高效的系统识别和控制综合。Koopman算子理论为用数据驱动的方法识别非线性到线性的坐标转换奠定了基础。最近，研究人员提出使用深度神经网络作为计算Koopman算子的一类更具表现力的基础函数。然而，这些方法假设了一个固定维度的状态空间；因此它们不适用于对象数量可变的情况。 在本文中，我们建议学习组合式Koopman算子，使用图神经网络将状态编码为以物体为中心的嵌入，并使用块状线性过渡矩阵来规范各物体间的共享结构。所学的动力学可以快速适应未知物理参数的新环境，并产生控制信号以实现指定目标。
我们推导出随机微分方程（SDE）解的反向模式（或邻接）自动微分，允许时间效率和恒定内存计算路径梯度，这是再参数化技巧的连续时间类似物。具体而言，我们构建了一个反向SDE，其解决方案是梯度，并提供了数字解决方案收敛的条件。 我们还将我们的随机邻接方法与连续时间SDE模型的随机变异推理方案相结合，使我们能够使用随机梯度下降法学习函数的分布。与现有的时间序列建模方法相比，我们的潜在SDE模型实现了有竞争力的性能。
很明显，用户应该拥有并控制他们的数据和隐私。公用事业提供商也越来越关注保证数据隐私。因此，用户和提供商可以而且应该在保护隐私的挑战中合作，本文讨论了这一新的模式。 我们提出了一个框架，在这个框架中，用户可以控制他们想要分享的数据特征（效用）和他们想要保持的隐私（秘密），而不一定要求公用事业提供者改变其现有的机器学习算法。我们首先分析了隐私保护表示的空间，并在披露数据X的消毒版本时推导出效用-隐私权衡的自然信息理论界限。我们提出了明确的学习架构，以学习以数据驱动的方式接近这一界限的隐私保护表示。我们描述了重要的用例场景，其中效用提供者愿意与消毒过程合作。我们研究了空间保护转换，其中效用提供者可以在原始数据和消毒数据上使用相同的算法，这是一个关键和新颖的属性，以帮助服务提供商用一套单一的效用算法适应不同的隐私要求。 我们通过实现三个用例来说明这个框架；主体内的主体，我们解决的问题是拥有一个只对同意的用户子集工作的人脸身份检测器，这是一个重要的应用，例如，对于由人脸识别激活的移动设备；性别和主体，我们保留面部验证，同时为选择这样做的用户隐藏性别属性；以及情感和性别，我们隐藏独立变量，如隐藏性别同时保留情感检测的情况。
解决具有稀疏奖励的任务是强化学习中最重要的挑战之一。在单代理设置中，这一挑战已经通过引入内在奖励来解决，这些奖励激励代理探索其状态空间中未见过的区域。 在本文中，我们提出了一种学习方法，即如何在不同类型的内在奖励之间进行动态选择，这些奖励不仅考虑单个代理所探索的内容，而且考虑所有的代理，这样代理可以协调他们的探索并使内在回报最大化。 具体来说，我们将该方法表述为一个分层策略，其中高层控制器在根据不同类型的内在奖励训练的策略集中进行选择，而低层控制器则学习所有代理在这些特定奖励下的行动策略。我们在一个具有稀疏奖励的多代理网格世界领域中证明了所提方法的有效性，然后通过在VizDoom平台上的评估表明，我们的方法可以扩展到更复杂的环境。
现实世界中的物体识别需要处理长尾甚至是开放式的数据。一个理想的视觉系统需要可靠地识别流行的视觉概念，同时通过少数训练实例有效地学习新出现的类别。 类平衡的多拍学习和少拍学习解决了这个问题的一个方面，通过为已填充的类别学习强分类器或为尾部类别学习少拍分类器。在本文中，我们研究了广义少拍学习（GFSL）的问题--在部署过程中，一个模型不仅需要用少拍学习 "尾部 "类别，还需要同时对 "头部 "和 "尾部 "类别进行分类。 我们提出了分类器合成学习（CASTLE），这是一个学习框架，可以学习如何在 "头 "类的多类分类器之外，利用一个共享的神经词典来合成经过校准的少数镜头分类器。CASTLE通过优化一个干净而有效的GFSL学习目标，揭示了归纳式GFSL的情况。 它在MiniImageNet和TieredImageNet数据集上表现出比现有的GFSL算法和强大的基线更优越的性能。更有趣的是，在对标准的几率学习进行评估时，它比以前最先进的方法更出色。
机器学习的工作负载往往是昂贵的，需要几周的时间来收敛。目前这一代的框架依靠定制的后端来实现效率，这使得在不太常见的硬件上训练模型是不切实际的，因为那里没有这样的后端。Knossos建立在最近的工作上，避免了对手写库的需求，而是以编译其他类型的软件的方式编译机器学习模型。 然而，与传统的编译器采用手写的优化通道不同，我们采取了由$A^\star$搜索算法和学习值函数驱动的重写方法，评估了对程序采取各种重写行动的未来潜在成本降低。 此外，我们证明Knossos可以在一套机器学习程序（包括基本的线性代数和卷积网络）上实现与手工调整的编译器相比的时间缩短。Knossos编译器的依赖性最小，可以在任何支持\Cpp工具链的架构上使用。由于所提出的算法优化的成本模型可以为特定的硬件架构量身定做，因此所提出的方法有可能适用于各种硬件。
对于传统的机器人控制或手工设计的方法来说，抓取一个物体并将其精确地堆叠在另一个物体上是一项困难的任务。在这里，我们在模拟中研究了这个问题，并提供了旨在通过深度强化学习来解决这个问题的技术。我们对深度确定策略梯度算法（DDPG）进行了两个直接的扩展，这使得它的数据效率和可扩展性大大增加。 我们的结果表明，通过广泛利用非政策数据和重放，有可能找到高性能的控制政策。此外，我们的结果暗示，通过收集真实机器人上的互动，训练成功的堆叠政策可能很快就可行了。
近年来，深度强化学习已被证明善于解决具有高维状态空间的顺序决策过程，如在Atari游戏中。然而，许多强化学习问题涉及高维离散行动空间以及高维状态空间。我们提出了两种创建参数化策略的方法：LSTM参数化和修正的MDP（MMDP）引起的Feed-Forward Network（FFN）参数化。这两种方法都提供了表达式模型，可以应用反向传播进行训练。 在高维行动空间的情况下，计算熵和熵的梯度需要列举行动空间中的所有行动，并为每个行动运行正向和反向传播，这在计算上可能是不可行的。我们为熵奖励及其梯度开发了几个新的无偏估计器。最后，我们在两个环境中测试了我们的算法：一个多猎人多兔网格游戏和一个多代理人多臂强盗问题。
神经网络为一系列问题提供了高精度的解决方案，但在生产系统中运行的计算成本很高。我们提出了一种叫做深度学习近似的技术，通过操纵网络结构和系数，在不需要重新训练或访问训练数据的情况下，采用已经训练好的神经网络模型，建立一个更快的（而且几乎同样精确的）网络。 在PASCAL VOC 2007上使用YOLO网络，我们显示了网络前向传递的端到端速度提高了2倍，而mAP下降了5美元/%，这可以通过微调重新获得，使这个网络（以及其他类似的网络）可以部署在计算受限的系统中。
异步分布式方法是减少大规模优化的通信和同步成本的一种流行方式。然而，尽管它们取得了所有的成功，但在一般非光滑、非凸目标的挑战性情况下，除了有闭式近似算子解决方案的情况外，人们对它们的收敛保证知之甚少，这更令人惊讶，因为这些目标是出现在深度神经网络训练中的。 我们的分析适用于随机子梯度下降方法，包括有和没有块状变量分区，以及有和没有动量。它是在异步调度的一般概率模型的背景下表述的，准确地适应现代硬件特性。我们在训练深度神经网络架构的背景下通过实验验证了我们的分析。
具有离散随机变量的随机神经网络因其表现力和可解释性而成为一类重要的模型。由于直接分化和反向传播是不可能的，蒙特卡洛梯度估计技术已被广泛用于训练此类模型。 高效的随机梯度估计器，如Straight-Through和Gumbel-Softmax，对于有一到两个随机层的浅层模型效果很好，然而，它们的性能随着模型复杂性的增加而受到影响。 我们用它来推导出有偏见的Straight-Through估计器中偏见来源的分析公式。基于分析，我们提出了FouST}，一种简单的梯度估计算法，它依赖于三个简单的偏见减少步骤。 广泛的实验表明，与最先进的有偏估计器相比，FouST的表现很好，同时比无偏估计器快得多。据我们所知，FouST是第一个训练深度随机神经网络的梯度估计器，有多达80个确定层和11个随机层。
我们提出了一种新型的视觉属性操作生成对抗网络（ManiGAN），它能够使用自然语言描述对给定图像的视觉属性进行语义修改。我们的方法的关键是设计一个新型的共同关注模块来结合文本和图像信息，而不是简单地沿通道方向将两个特征连接起来。 此外，我们还提出了一个细节校正模块，以纠正合成图像的不匹配属性，并重建与文本无关的内容。最后，我们提出了一个新的指标来评估操纵结果，在文本相关属性的生成和文本不相关内容的重建方面。在基准数据集上的广泛实验证明了我们提出的方法在图像操纵的有效性和产生高质量结果的能力方面的优势。
为了解决可扩展性问题，我们提出了一个基于隐性生成学习的框架，称为SPOT（可扩展的最佳运输推送）。具体来说，我们通过参考分布的推送来近似最佳运输计划，并将最佳运输问题转化为一个最小问题。 我们还表明，我们可以使用神经常微分方程恢复最优运输计划的密度。在合成和真实数据集上的数值实验表明，SPOT是稳健的，具有良好的收敛行为。
在这项工作中，我们提出了一种新的规划表述，将其视为未来最优轨迹的概率推理问题。这使我们能够使用抽样方法，从而用固定的计算预算解决连续领域的规划问题。  我们设计了一种新的算法--顺序蒙特卡洛规划，通过利用顺序蒙特卡洛和贝叶斯平滑中的经典方法，将控制作为推理。此外，我们表明，顺序蒙特卡洛规划可以捕获多模式的政策，并可以快速学习连续控制任务。
训练生成对抗网络（GANs）是众所周知的挑战。我们提出并研究了一种架构修改，即自我调制，它可以提高GAN在不同数据集、架构、损失、正则器和超参数设置中的性能。直觉上，自我调制允许生成器的中间特征图作为输入噪声向量的函数而变化。 虽然让人联想到其他调节技术，但它不需要标记数据。在一项大规模的经验研究中，我们观察到FID相对下降了5%-35%。此外，在其他条件相同的情况下，在研究的124/144（86%）的设置中，向生成器添加这种修改导致性能的提高。自我调节是一个简单的架构变化，不需要额外的参数调整，这表明它可以很容易地应用于任何GAN。
极端分类方法已经变得极为重要，特别是对于信息检索（IR）问题，由于开发了可扩展到行业挑战的智能算法，其中一类旨在解决极端多标签学习的内存和速度挑战的主要模型是分组测试。 多标签组测试（MLGT）方法通过随机或基于某种相似性对原始标签进行分组来构建标签组，然后训练较小的分类器来首先预测这些组，然后恢复原始标签向量。最近，一种名为MACH（通过哈希的合并平均分类器）的新方法被提出，它将巨大的标签向量投射到一个小的、可管理的计数-最小草图（CMS）矩阵，然后学习预测这个矩阵来恢复原始预测概率。 尽管MACH很简单，但在对MACH的权衡的理论理解上有很大的差距。 我们将该理论扩展到多标签分类的情况下，其中的依赖性使得估计器难以以封闭形式计算。为了缓解这一问题，我们提出了使用 "包容-排斥原则 "的新型二次近似。
神经网络通常被用作各种任务的分类模型。通常，一个学习过的仿生变换被放在这种模型的末端，产生一个用于分类的每类值。这个分类器可以有大量的参数，这些参数随着可能的类的数量线性增长，因此需要越来越多的资源。 在这项工作中，我们认为这个分类器可以被固定，直到一个全局规模的常数，对于大多数任务来说，几乎没有损失准确性，允许内存和计算的好处。此外，我们表明，通过用Hadamard矩阵初始化分类器，我们也可以加快推理的速度。
本文介绍了NEMO，一种无监督物体检测的方法，它使用运动--而不是图像标签--作为学习物体检测的线索。为了区分目标物体的运动和图像中的其他变化，它依赖于显示没有物体的场景的负面例子。 所需的数据可以很容易地通过录制两个简短的视频来收集，一个是显示物体运动的正面视频，一个是显示没有物体的场景的负面视频。尽管有遮挡、干扰、相机运动和不利的光线，这些视频足以学习物体检测器，可以应用于新的视频，甚至可以推广到未见过的场景和相机的角度。 在基线比较中，无监督的物体检测优于现成的模板匹配和跟踪方法，这些方法给定了物体的初始边界框。学习到的物体表征也被证明是准确的，足以捕获来自操纵任务演示的相关信息，这使它们适用于机器人学中的演示学习。从3分钟的视频中学习到的物体检测例子可以在这里找到：http://y2u.be/u_jyz9_ETz4
最近，强大的预训练语言模型在大多数流行的阅读理解数据集上取得了显著的性能。现在是时候引入更具挑战性的数据集，以推动这一领域的发展，使其向更全面的文本推理发展。本文中，我们引入了一个新的阅读理解数据集，需要逻辑推理（ReClor），提取自标准化的研究生入学考试。正如早期研究表明，人类注释的数据集通常包含偏见，模型往往利用这些偏见来实现高准确性，而没有真正理解文本。 为了全面评估模型在ReClor上的逻辑推理能力，我们建议识别有偏见的数据点，并将它们分成EASY集，其余的作为HARD集。实证结果表明，最先进的模型在EASY集上具有出色的捕捉数据集中包含的偏见的能力，并具有较高的准确性。然而，它们在HARD集上表现不佳，接近随机猜测的表现，表明需要更多的研究来从本质上提高当前模型的逻辑推理能力。
本文探讨了在什么情况下攻击者可以声称 "噪音和访问模型的softmax层是你所需要的 "来窃取卷积神经网络的权重，而该网络的架构已经是已知的。 我们认为，权重的易被盗性是数据集复杂性的指示，并提出了一个新的指标来捕捉同样的情况。这次传播的目的不仅仅是展示在模型偷窃方面知道架构可以带你走多远，而且还提请注意这个由i. 我们还传播了一些使用Ising概率分布代替i.i.d.Bernoulli分布的初步结果。
我们提出了一种新形式的自动编码模型，它结合了变异自动编码器（VAE）和生成对抗网络（GAN）的最佳特性。众所周知，GAN可以产生非常真实的样本，而VAE不存在模式崩溃问题。 我们的模型优化了模型分布和真实数据分布之间的λ-Jeffreys发散。我们表明，它采用了VAE和GAN目标的最佳特性。它由两部分组成。其中一部分可以通过使用标准对抗训练来优化，而第二部分正是VAE模型的目标。 然而，如果我们使用高斯或拉普拉斯这样的显式似然，直接替代VAE损失的方法效果并不好，因为这些似然在高维度上的灵活性有限，对像素空间中的图像建模是不自然的。为了解决这个问题，我们提出了一种新的方法，通过对抗性训练的判别器用隐性似然训练VAE模型。 在CIFAR-10和TinyImagent数据集的大量实验中，我们表明我们的模型实现了最先进的生成和重建质量，并证明了我们如何通过调整目标中的权重λ来平衡我们模型的模式搜索和模式覆盖行为。
学习到学习或元学习利用数据驱动的归纳偏见来提高对新任务的学习效率。当转移不是互利的时候，例如，当任务足够不同或随时间变化时，这种方法会遇到困难。这里，我们利用基于梯度的元学习和层次贝叶斯之间的联系，提出了一个关于任意函数近似器（如神经网络）参数的层次贝叶斯模型混合物。 归纳模型诊断元学习（MAML）算法，我们提出了一个随机期望最大化程序，以联合估计梯度下降的参数初始化，以及任务与初始化的潜在分配。这种方法更好地捕捉了训练任务的多样性，而不是将归纳偏见整合到单一的超参数集。 我们的实验表明，在标准的miniImageNet基准上，单次分类有更好的通用性。我们进一步推导出我们的方法的一个新的和可扩展的非参数变体，它可以捕捉到任务分布随时间的演变，这在一组少数次回归任务中得到了证明。
我们为胶囊网络引入了一种新的路由算法，在这种算法中，只根据父方的状态和子方的投票之间的一致性将子胶囊路由到父方。与以前提出的路由算法不同，在更新路由概率时没有明确考虑父方重建子方的能力。这简化了路由程序，提高了基准数据集如CIFAR-10和CIFAR-100的性能。 新机制1）通过倒点积注意设计路由；2）将层归一化作为归一化；3）用并发迭代路由取代顺序迭代路由。除了优于现有的胶囊网络，我们的模型与强大的CNN（ResNet-18）表现相当，使用不到25%的参数。 在从叠加的数字图像中识别数字的不同任务中，在层数和每层神经元数量相同的情况下，所提出的胶囊模型的表现优于CNNs。 我们相信，我们的工作提高了将胶囊网络应用于复杂的真实世界任务的可能性。
 我们介绍了Doc2Dial，一个通过众包产生基于商业文件的对话数据的端到端框架。这些数据可以用来训练为企业或组织执行客户关怀任务的自动对话代理。 对话流程被用来指导人群工作者产生的语料收集。结果包括基于给定文件的对话数据，以及有助于确保数据质量和（重新）合成对话的灵活性的各种类型的注释。
捕捉音频波形中的高层次结构是具有挑战性的，因为一秒钟的音频跨越了数万个时间步长。 虽然长距离的依赖关系很难在时域中直接建模，但我们表明，它们可以在二维时频表示中更容易地建模，如谱图。 通过利用这种表征优势，结合高表现力的概率模型和多尺度生成程序，我们设计了一个能够生成高保真音频样本的模型，该模型在时域模型尚未达到的时间尺度上捕捉结构。 我们证明了我们的模型比时域模型（如WaveNet）在一系列不同的无条件生成任务中捕捉到了更远的依赖性，包括单人语音生成、多人语音生成和音乐生成。
深度卷积网络架构通常被认为可以保证对小的图像平移和变形的泛化。在本文中，我们表明现代CNN（VGG16、ResNet50和InceptionResNetV2）在图像平面内被平移几个像素时，它们的输出会发生巨大的变化，这种泛化失败也发生在其他现实的小图像变换中。此外，我们看到这些泛化失败在更现代的网络中更加频繁。 我们表明，这些失败与现代CNN的结构忽略了经典的抽样定理有关，因此不能保证泛化。我们还表明，常用图像数据集的统计学偏差使得CNN不可能学会对这些变换的不变性。总之，我们的结果表明，CNN在物体识别中的表现远远低于人类的泛化能力。
最近，研究人员试图通过使用自回归技术来合成新的运动，但现有的方法往往在几秒钟后就会冻结或发散，因为错误的积累会反馈到网络中。此外，这种方法只被证明对相对简单的人类运动是可靠的，如走路或跑步。 相比之下，我们的方法可以合成具有高度复杂风格的任意运动，包括舞蹈或武术，以及运动。acRNN能够通过明确地适应训练期间的自回归噪声积累来实现这一点。
 {em Saliency方法}试图通过给输入的每个特征/像素分配一个{em score}来解释深度网的决定，通常通过输出相对于输入的梯度来进行这种信用分配。最近，citet{adebayosan}对这些方法的有效性提出了质疑，因为它们不能通过简单的{em sanity checks}，即测试当训练好的网络层被随机化，或者当网络使用随机标签重新训练输入时，分数是否会发生变化/消失。 令人惊讶的是，测试的方法并没有通过这些检查：解释相对没有变化。我们对现有的显著性方法提出了一个简单的修正，帮助它们通过理智检查，我们称之为{像素的竞争}。这涉及计算分类任务中所有可能的标签的显著性地图，并使用它们之间的简单竞争来识别并从地图中删除不太相关的像素。为它提供了一些理论依据，并在几个流行的方法上实证了其性能。
分类系统通常是孤立行动的，这意味着它们需要隐含地记忆所有候选类的特征以进行分类。这样做的代价是增加了内存的使用和较差的采样效率。我们提出一个模型，在分类过程中使用参考图像进行验证，减少记忆的负担。 然而，我们表明，在图像识别和验证之间找到正确的平衡是推动模型走向理想行为的关键，这表明识别和验证的管道是一种更有前途的方法，可以用更简单的架构设计出更强大的网络。
为了减少内存占用和运行时的延迟，已经分别探索了诸如神经网络工作修剪和二进制化等技术。然而，目前还不清楚如何将这两个领域的优点结合起来，以获得极小而高效的模型。 在本文中，我们首次定义了二元神经网络的滤波级修剪问题，该问题不能通过简单迁移现有的全精度模型的结构修剪方法来解决。 我们提出了一种新的基于学习的方法来修剪主/附属网络框架中的过滤器，其中主网络负责学习代表性特征以优化预测性能，而附属部分则作为主网络的过滤器选择器工作。为了避免在训练附属部分时出现梯度不匹配，我们提出了一种分层和自下而上的方案。 我们还提供了基于学习的方法和基于贪婪规则的方法之间的理论和实验比较。 最后，我们实证了我们的方法在几个二元模型上的应用效果，包括二进制NIN、VGG-11和ResNet-18，以及各种图像分类数据集。 对于ImageNet上的二进制ResNet-18，我们使用了78.6%的过滤器，但可以达到比原始模型略好的测试误差49.87%（50.02%-0.15%）。
为了解决这个问题，我们开发了AntMan，将结构化的稀疏性和低秩分解协同起来，以减少模型的计算、大小和RNN的执行时间，同时达到理想的精度。AntMan扩展了基于知识提炼的训练，以有效地学习压缩模型。 我们的评估显示，AntMan为语言和机器阅读理解模型提供了高达100倍的计算量，而准确率下降不到1个百分点。我们的评估还显示，对于给定的准确率目标，AntMan产生的模型比最先进的模型小5倍。最后，我们显示AntMan提供了与理论速度相比的超线性速度提升，证明了它在商品硬件上的实用价值。
图结构的数据，如社交网络、大脑功能网络、基因调控网络、通信网络，使人们对将深度学习技术推广到图领域产生了兴趣。在本文中，我们有兴趣为具有可变长度的图设计神经网络，以解决学习问题，如顶点分类、图分类、图回归和图生成任务。 大多数现有的工作都集中在递归神经网络（RNN）来学习有意义的图形表示，而最近新的卷积神经网络（ConvNets）已经被引入。在这项工作中，我们希望严格比较这两个基本的架构系列来解决图形学习任务。 我们回顾了现有的图RNN和ConvNet架构，并提出了LSTM和ConvNet对任意大小的图的自然扩展。然后，我们对两个基本的图问题，即子图匹配和图聚类，设计了一组分析控制实验来测试不同的架构。 数值结果显示，所提出的图ConvNets比图RNNs的精度高3-17%，速度快1.5-4倍。图ConvNets的精度也比变异（非学习）技术高36%。最后，最有效的图ConvNet架构使用门控边缘和剩余性。剩余性对学习多层架构起着至关重要的作用，因为它们提供10%的性能增益。
复值神经网络并不是一个新的概念，然而，由于训练的困难和结果的准确性，使用实值神经网络往往比复值神经网络更受青睐。现有的文献忽略了使用的参数数量。我们使用五个激活函数比较了复值和实值神经网络。我们发现，当使用简单分类任务比较实值和复值神经网络时，复值神经网络的表现与实值神经网络相同或略差。 然而，当使用专门的架构时，复值神经网络的表现优于实值神经网络。因此，当输入数据也是复杂的，或者它可以有意义地到复数平面，或者网络架构使用使用复数定义的结构时，应该使用复值神经网络。
图结构数据的兴起，如社交网络、监管网络、引文图和大脑功能网络，结合深度学习在各种应用中的巨大成功，使人们对将深度学习模型推广到非欧几里得领域产生了兴趣。在本文中，我们介绍了一个新的谱域卷积架构，用于图上的深度学习。我们模型的核心成分是一类新的参数化有理复数函数（Cayley多项式），允许有效地计算图上的频谱过滤器，专门用于感兴趣的频段。 我们的模型产生了丰富的光谱过滤器，它在空间上是本地化的，对于稀疏连接的图来说，它与输入数据的大小成线性比例，并且可以处理拉普拉斯算子的不同构造。广泛的实验结果表明，我们的方法在光谱图像分类、群落检测、顶点分类和矩阵完成任务方面有卓越的表现。
我们提出了FasterSeg，一个自动设计的语义分割网络，不仅具有最先进的性能，而且速度比目前的方法更快。利用神经结构搜索（NAS），FasterSeg是从一个新的和更广泛的搜索空间中发现的，整合了多分辨率分支，最近发现这在人工设计的分割模型中是至关重要的。 为了更好地平衡高精度和低延迟的目标，我们提出了一个解耦和细粒度的延迟正则化，有效地克服了我们观察到的现象，即搜索到的网络容易 "崩溃 "为低延迟但精度差的模型。 此外，我们将FasterSeg无缝扩展到一个新的协作搜索（co-searching）框架中，在同一次运行中同时搜索教师和学生网络。在流行的分割基准上的实验证明了FasterSeg的能力。
本文介绍了R2D3，一个能有效利用演示来解决在部分可观察环境中具有高度可变初始条件的困难探索问题的代理。我们还介绍了一套结合了这三个特性的八个任务，并表明R2D3能解决其中几个任务，而其他最先进的方法（包括有演示和无演示）在经过数百亿步的探索后甚至不能看到一个成功的轨迹。
我们研究了一个解决简单任务的递归神经网络的学习动态景观，该任务需要两种记忆机制的相互作用：长期和短期。我们的结果表明，虽然长期记忆是由渐近吸引子实现的，但顺序回忆现在又是由这些稳定稳定状态的吸引盆地的横向子空间的振荡动力学实现的。基于我们的观察，我们提出不同类型的记忆机制如何能在一个神经网络中共存和共同工作，并讨论在人工智能和神经科学领域的可能应用。
强化学习中的探索问题在表格的情况下是很好理解的，许多有效的样本算法也是已知的。然而，通常不清楚如何将表格设置中的算法扩展到需要泛化的大状态空间的任务中。 在本文中，我们介绍了一种简单的探索方法，它允许我们在表格的情况下开发出理论上合理的算法，但也给我们提供了适用于需要函数近似的环境的新算法的直觉。我们的方法及其基础理论是基于subochastic继承者表示，这是我们在这里开发的一个概念。 虽然传统的继任者表示是一种通过继任者状态的相似性来定义状态泛化的表示，但亚ochastic继任者表示也能够隐含地计算每个状态（或特征）被观察到的次数。这种扩展连接了两个直到现在还相互分离的研究领域。 我们在传统的表格领域（RiverSwim和SixArms）展示了我们的算法在经验上的表现不亚于其他有样本效率的算法。然后我们描述了一个受这些想法启发的深度强化学习算法，并展示了它在硬探索Atari 2600游戏中与最近基于伪计数的方法的表现相匹配。
由于具有高效的训练和合成过程的可操作的精确对数似然估计，使用流量的深度生成建模已经获得了普及。然而，流量模型遭受了具有高维潜在空间的挑战，其维度与输入空间相同。(Dinh等人（2016年）提出的上述挑战的有效解决方案是一个多尺度架构，它是基于定期对总维度的一部分进行早期迭代分解。 为了促进同样的工作，我们引入了基于每个维度对总对数似然的贡献的启发式方法，该方法编码了维度的重要性。我们提出的启发式方法很容易作为流训练过程的一部分获得，使我们基于似然贡献的多尺度架构的通用流模型的多功能实现成为可能。我们为Dinh等人（2016）介绍的原始流提出了这样一个实现，并在标准图像基准上展示了对数似然得分和采样质量的改进。
理解深度神经网络（DNN）中的信息流是一个具有挑战性的问题，在过去几年中得到了越来越多的关注。虽然已经提出了几种方法来解释网络预测，但只有少数人试图从理论角度对它们进行比较。更重要的是，过去没有进行过详尽的经验比较。在这项工作中，我们分析了四种基于梯度的归因方法，正式证明了它们之间的等价和近似条件。 最后，我们提出了一个新的评价指标，称为敏感性-n，并在图像和文本分类领域的几个数据集上，使用各种网络结构，测试了基于梯度的归因方法和基于扰动的简单归因方法。
我们研究了SGD和Adam，以估计种植在矩阵或张量噪声中的等级一信号。问题设置的极端简单性使我们能够隔离各种因素的影响：信噪比、临界点密度、随机性和初始化。Adam似乎一出现多项临界点就会陷入局部最小值（矩阵情况），而SGD则摆脱了这些临界点。然而，当临界点的数量退化为指数时（张量情况），两种算法都会被困住。 理论告诉我们，在固定的信噪比下，大$d$的问题变得难以解决，而在我们的实验中，SGD也没有摆脱这种情况。我们展示了在这些情况下暖启动的好处。我们的结论是，在这一类问题中，暖启动不能被梯度的随机性所取代，以找到吸引力的盆地。
本文提出了一种从数量和语义上解释卷积神经网络（CNN）中编码的知识的方法。如何分析CNN做出的每个预测的具体理由是理解神经网络的关键问题之一，但它在某些应用中也具有重要的实用价值。 在这项研究中，我们提出将CNN的知识提炼成一个可解释的加法模型，这样我们就可以用可解释的模型为CNN的预测提供定量解释。我们分析了可解释模型的典型偏差解释问题，并开发了先验损失来指导可解释加法模型的学习。实验结果证明了我们方法的有效性。
我们提出了使用动态评价改进神经序列模型的方法。模型通过基于梯度下降的机制适应最近的历史，使它们为重新出现的序列模式分配更高的概率。动态评价在我们的比较中优于现有的适应方法。动态评价将Penn Treebank和WikiText-2数据集上最先进的词级复杂度分别提高到51.1和44.3，将text8和Hutter Prize数据集上最先进的字符级交叉熵分别提高到1.19比特/字符和1.08比特/字符。
我们提出了一种新颖的、基于投影的方法，将条件信息纳入GANs的判别器，尊重条件信息在底层概率模型中的作用。这种方法与目前应用中的大多数条件型GANs框架不同，后者通过将（嵌入的）条件向量与特征向量相连接来使用条件信息。通过这一修改，我们能够在ILSVRC2012（ImageNet）数据集上显著提高类条件图像生成的质量，与目前最先进的结果相比，我们通过一对判别器和生成器就实现了这一目标。我们还能够将该应用扩展到超分辨率，并成功地生成了高度判别性的超分辨率图像。这种新的结构也使高质量的类别转换成为可能，其基础是生成器中条件批量归一化层的参数化功能转换。
学习理论告诉我们，当最小化分布相同的训练集和测试集的泛化误差时，更多的数据是更好的。然而，当训练和测试分布不同时，这种分布转变会有很大的影响。通过对函数转换学习的新视角，我们能够用嵌入的训练集和测试集分布之间的Wasserstein距离来降低从训练集转移到测试集时的性能变化。 我们发现，在一个函数对训练和测试分布的变化的不变性和这种分布的转变有多大之间，存在着影响性能的权衡。在几个数据领域的经验上，我们证实了这个观点，表明测试性能与训练和测试集之间的数据分布距离密切相关。
我们介绍了一个新的程序性动态系统，它可以生成各种形状，这些形状通常以曲线的形式出现，但从技术上讲，这些数字是由许多点组成的图。 我们证明了一些基本属性，并分析了一些实例，以了解输入的几何或拓扑结构如何决定生成的图形。我们表明，一些螺线图有一个有限的周期，并返回到初始情况，而另一些螺线图将无限地产生新的点。本文附有一个JavaScript应用程序，允许任何人生成螺线图。
无监督的图像到图像的翻译旨在通过使用未配对的训练对来学习几个视觉领域之间的映射。最近的研究表明，在多个领域的图像到图像的翻译方面取得了显著的成功，但它们有两个主要的局限性：它们要么是由几个需要独立学习的双领域映射建立起来的，要么是产生低多样性的结果，这种现象被称为模型崩溃。为了克服这些限制，我们提出一个名为GMM-UNIT的方法，它基于内容-属性分离的表示，其中属性空间是用GMM拟合。 每个GMM组件代表一个域，这个简单的假设有两个突出的优点。首先，属性空间的维度不会随着域的数量而线性增长，就像文献中的情况一样。其次，连续的域编码允许域之间的插值和外推到未见过的域。此外，我们展示了GMM-UNIT如何能够被约束到文献中的不同方法，这意味着GMM-UNIT是一个无监督的图像到图像的统一框架。
我们提出了Compositional Attention Networks，一种新型的完全可分化的神经网络架构，旨在促进明确和表达式的推理。虽然许多类型的神经网络在从大量数据中学习和归纳方面是有效的，但这个模型摆脱了单一的黑箱架构，走向了为迭代推理提供强大先验的设计，使其能够支持可解释和结构化的学习，以及从适量的数据中进行归纳。该模型建立在现有递归单元如LSTMs的巨大成功之上。该模型建立在现有的递归单元（如LSTM）的巨大成功之上：它对单一的递归记忆、注意力和控制（MAC）单元进行排序，并通过精心设计对每个单元的操作和它们之间的相互作用施加结构约束，将明确的控制和软注意力机制纳入其界面。 我们在具有挑战性的视觉推理CLEVR数据集上证明了该模型的强度和鲁棒性，实现了新的最先进的98.9%的准确率，将以前的最佳模型的错误率减半。更重要的是，我们表明新的模型在计算上更有效率，数据效率高，需要的时间和/或数据少了一个数量级，以达到良好效果。
  变异自动编码器（VAEs）被设计用来捕获关于数据集的可压缩信息。 因此，存储在潜伏空间的信息很少足以重建一个特定的图像。 为了帮助理解存储在潜空间中的信息类型，我们训练了一个GAN式的解码器，该解码器被限制为产生VAE编码器将映射到潜空间的同一区域的图像。 我们认为，这对于使VAE成为一个真正的生成模型是必要的。 我们用我们的GAN将标准VAE和$beta$-VAE的潜空间可视化。
通过功能磁共振成像(fMRI)测量的人类大脑功能表现出丰富的多样性。作为回应，理解大脑功能的个体差异性及其与行为的关联已成为现代认知神经科学的主要关注点之一。 因此，这两个模型都是3D条件生成对抗网络（GANs），它应用卷积神经网络（CNNs）来学习大脑图像表征的抽象。除了定性评估，我们利用生成的合成大脑体积作为额外的训练数据来改进下游fMRI分类器（也称为解码，或大脑阅读）。 我们的方法对各种数据集、分类任务和评价分数都有明显的改善。我们的分类结果对生成的图像的质量进行了定量评价，也是本稿的一个额外贡献。
在计算机视觉和自然语言处理（NLP）的机器学习中，将表征从大规模的监督任务转移到下游任务已经显示出了突出的成果。一个特别的例子可以是机器翻译的序列到序列模型（神经机器翻译-NMT）。 这是因为，一旦在多语言设置中进行了训练，NMT系统就可以在多种语言之间进行翻译，并且还能够在测试时在未见过的源-目标对之间进行零点翻译。在本文中，我们首先研究是否可以将多语言NMT系统的零点翻译能力扩展到跨语言NLP任务（除MT之外的任务，如情感分类和自然语言推理）。 我们展示了一个简单的框架，通过重用多语言NMT系统的编码器，一个多语言编码器分类器，在三个下游基准任务--亚马逊评论、斯坦福情感树库（SST）和斯坦福自然语言推理（SNLI）上几乎开箱即用，取得了显著的零点跨语言分类性能。 为了了解导致这一发现的基本因素，我们对共享词汇、NMT模型的训练数据类型、分类器复杂性、编码器表示能力和模型泛化对零点性能的影响进行了一系列分析。我们的结果提供了强有力的证据，表明从多语言NMT系统学到的表示方法广泛适用于各种语言和任务，而且高的、开箱即用的分类性能与这种系统的泛化能力相关联。
本文通过以可区分的方式制定任务来解决架构搜索的可扩展性挑战。与传统的在离散和不可区分的搜索空间上应用进化或强化学习的方法不同，我们的方法是基于架构表示的连续松弛，允许使用梯度下降来有效搜索架构。 在CIFAR-10、ImageNet、Penn Treebank和WikiText-2上的大量实验表明，我们的算法在发现用于图像分类的高性能卷积架构和用于语言建模的递归架构方面表现出色，同时比最先进的无差异技术快了几个数量级。
尽管在神经语言建模方面取得了相当大的进展，但对于从语言模型中生成文本（例如生成一个故事）的最佳解码策略是什么，仍然是一个开放的问题。 反直觉的经验观察是，即使使用可能性作为训练目标，也会为广泛的语言理解任务带来高质量的模型，但基于最大化的解码方法，如波束搜索，会导致退化--输出的文本平淡无奇、不连贯，或陷入重复的循环中。 为了解决这个问题，我们提出了 "核抽样"，这是一种简单而有效的方法，可以从神经语言模型中抽出质量相当高的文本。我们的方法通过截断概率分布的不可靠的尾部，从包含绝大部分概率质量的动态符号核中抽样，从而避免了文本退化。 为了正确考察目前基于最大化的和随机的解码方法，我们将这些方法中的每一个的世代与人类文本的分布沿着几条轴线（如可能性、多样性和重复性）进行比较。 我们的结果表明：（1）最大化对于开放式文本生成来说是一个不合适的解码目标；（2）目前最好的语言模型的概率分布有一个不可靠的尾巴，需要在生成过程中被截断；（3）"核抽样 "是生成长格式文本的最佳解码策略，这种文本既是高质量的（由人类评价来衡量），又是与人类书写的文本一样多样化。
直到最近，受计算机视觉对抗性例子的大量研究的启发，人们对设计自然语言处理（NLP）任务的对抗性攻击的兴趣越来越浓厚，随后对NLP的对抗性防御的工作很少。 据我们所知，目前还没有针对成功的基于同义词替换的攻击的防御方法，这些攻击旨在满足所有的词汇、语法、语义约束，因此很难被人类察觉。我们为填补这一空白做出了贡献，并提出了一种新型的对抗性防御方法，即同义词编码法（SEM），它在模型的输入层之前插入了一个编码器，然后训练模型以消除对抗性扰动。 大量的实验表明，SEM可以有效地防御目前最好的基于同义词替换的对抗性攻击，而对良性例子的准确性几乎没有衰减。为了更好地评估SEM，我们还设计了一种称为改进的遗传算法（IGA）的强攻击方法，它采用遗传元启发法进行基于同义词替换的攻击。与现有的基于遗传的对抗性攻击相比，IGA可以在保持对抗性例子的可转移性的前提下获得更高的攻击成功率。
通过时间反向传播（BPTT）的一个主要缺点是学习长期依赖关系的困难，因为必须通过向前计算的每一步向后传播信用信息。 然而，这通常会导致对梯度的有偏见的估计，其中较长期的依赖性被忽略了。 为了解决这个问题，我们提出了一种替代算法--稀疏注意力回溯，这可能也与大脑学习长期依赖关系的原则有关。稀疏注意力回溯在过去的隐藏状态上学习一种注意力机制，并选择性地通过具有高注意力权重的路径进行回溯。 这使得模型能够学习长期的依赖关系，同时只对少量的时间步骤进行回溯，不仅从最近的过去，而且从被关注的相关过去的状态。  
我们在这项工作中提出了一个新的对抗性学习框架。现有的对抗性学习方法涉及两个独立的网络，即。现有的对抗学习方法在训练中涉及两个独立的网络，即结构化预测模型和鉴别性模型。鉴别性模型所捕获的信息是对结构化预测模型的补充，但现有的研究很少研究在推理阶段利用这些信息来改进结构化预测模型。 与对抗性学习类似，判别性模型被训练来估计衡量预测输出质量的分数，而结构化预测模型被训练来预测具有最大能量分数的对比性输出。通过这种方式，梯度消失的问题得到了改善，因此我们能够通过遵循判别性模型的上升梯度方向来进行推理，从而完善结构化预测模型。 这两项任务的经验结果验证了我们学习方法的有效性。
我们提出了RaPP，一种利用从深度自动编码器获得的隐藏空间激活值进行新奇度检测的新方法。准确地说，RaPP不仅在输入空间，而且在隐藏空间对输入和其自动编码器的重建进行比较。我们表明，如果我们将重建的输入再次送入同一个自动编码器，其在隐藏空间的激活值等同于在该隐藏空间给定的相应重建。 通过使用不同的数据集进行广泛的实验，我们验证了RaPP改善了基于自动编码器的方法的新颖性检测性能。此外，我们表明RaPP优于最近在流行基准上评估的新颖性检测方法。
鉴于大量的特征和我们可以向单个用户提出的有限的查询，学习用户对计划轨迹的偏好是一项具有挑战性的任务。此外，偏好函数本身可能是相当复杂和非线性的。 这个数据被用来训练一个简单的前馈神经网络来学习顺序数据的偏好。我们评估了主动学习对训练一个准确和可解释的模型所需的痕迹数量的影响。这个评估是通过比较上述的前馈网络和一个更复杂的神经网络模型来完成的，该模型使用LSTM，并在没有主动学习的情况下用一个更大的数据集训练。
强化学习是解决控制问题的一个很有前途的框架，但它在实际情况下的使用受到了阻碍，因为奖励函数往往难以工程化。为自主机器（如机器人）指定目标和任务是一个重大挑战：传统上，奖励函数和目标状态被用来传达目标。但人们可以通过描述或演示来相互传达目标。我们如何才能建立学习算法，让我们告诉机器我们希望它们做什么？ 在这项工作中，我们研究了使用反强化学习将语言命令作为奖励函数的基础问题，并认为语言条件下的奖励比语言条件下的政策更容易转移到新的环境中。我们提出了语言条件下的奖励学习（LC-RL），它将语言命令作为深度神经网络代表的奖励函数的基础。我们证明，我们的模型在现实的高维视觉环境中用自然语言命令学习奖励，转移到新的任务和环境，而直接学习语言条件下的政策导致性能不佳。
许多生物学习系统，如蘑菇体、海马体和小脑，都是由稀疏连接的神经元网络构建的。为了对这种网络有一个新的认识，我们研究了由稀疏随机特征引起的函数空间，并描述了哪些函数可以学习，哪些不可以学习。一个每个神经元有d个输入的网络被发现等同于一个d阶的加性模型，而有一个程度分布，网络结合了不同阶的加性项。 我们确定了稀疏性的三个具体优势：加性函数近似是一个强大的归纳偏见，限制了维度的诅咒，稀疏网络对输入中的离群噪声是稳定的，稀疏的随机特征是可扩展的。因此，即使是简单的大脑结构也可以成为强大的函数近似器。最后，我们希望这项工作有助于在计算神经科学家中普及网络的内核理论。
我们提出了嵌入技术在自适应辅导中问题检索的新应用。目标是检索数学概念中类似的问题。有两个挑战。首先，像句子一样，对辅导有帮助的问题在基本概念方面从来都是完全相同的。相反，好的问题以创新的方式混合概念，同时仍然显示出它们之间关系的连续性。其次，人类很难确定一个在足够大的训练集上一致的相似性分数。 我们提出了一种层次化的问题嵌入算法，称为Prob2Vec，由抽象和嵌入步骤组成。Prob2Vec在问题相似性测试中实现了96.88%的准确率，而直接应用最先进的句子嵌入方法则为75%。 令人惊讶的是，Prob2Vec能够区分问题之间非常细微的差异，这种能力是人类需要时间和努力才能获得的。此外，用不平衡的训练数据集进行概念标签的子问题本身就很有趣，这是一个遭受维度爆炸的多标签问题，我们提出了改善的方法。
我们引入了一种新的深度卷积神经网络--CrescendoNet，通过堆叠无残余连接的简单构建块。每个Crescendo块都包含深度增加的独立卷积路径，卷积层的数量和参数只在Crescendo块中线性增加。 在实验中，只有15层的CrescendoNet在基准数据集、CIFAR10、CIFAR100和SVHN上的表现几乎超过了所有没有残余连接的网络。在SVHN数据集中有足够的数据量，15层和4.1M参数的CrescendoNet可以与有250层和15. 此外，通过研究CrescendoNet中子网络的行为和性能，我们注意到CrescendoNet的高性能可能来自于它的隐性集合行为，这与FractalNet不同，后者也是一个没有残余连接的深度卷积神经网络。
高斯过程是随机函数分布的主要类别，但它们存在众所周知的问题，包括难以缩放和对某些形状约束（如非负性）不灵活。这里我们提出了深度随机样条，这是一类灵活的随机函数，通过深度神经网络转换高斯噪声而获得，其输出是样条的参数。 与高斯过程不同，深度随机样条允许我们随时执行形状约束，同时继承了深度生成模型的丰富性和可操作性。我们还提出了一个点过程数据的观察模型，该模型使用深度随机样条对每个点过程的强度函数进行建模，并将其应用于神经科学数据，以获得尖峰活动的低维表示。推理通过一个变异自动编码器进行，该编码器使用一种新型的递归编码器架构，可以处理多个点过程作为输入。
最近，自然语言处理（NLP）的发展已经取得了巨大的成功，使用大型预训练的模型有数亿个参数。然而，这些模型存在着沉重的模型尺寸和高延迟的问题，因此我们不能直接将它们部署到资源有限的移动设备上。 在本文中，我们提出了MobileBERT，用于压缩和加速流行的BERT模型。与BERT一样，MobileBERT是任务无关的；也就是说，它可以通过微调普遍应用于各种下游NLP任务。MobileBERT是BERT-LARGE的瘦身版，增加了瓶颈结构和精心设计的自我关注和前馈网络之间的平衡。 为了训练MobileBERT，我们使用了一个从下到上的渐进方案，将专门设计的倒置瓶颈BERT-LARGE教师的内在知识转移到它身上。实证研究表明，MobileBERT比原来的BERT-BASE小4.3倍，快4.0倍，同时在著名的NLP基准上取得了有竞争力的结果。 在GLUE的自然语言推理任务上，MobileBERT实现了0.6的GLUE得分性能下降，在Pixel 3手机上实现了367毫秒的延迟。在SQuAD v1.1/v2.0问题回答任务上，MobileBERT实现了90.0/79.2 dev F1得分，比BERT-BASE高出1.5/2.1。
重要性加权自动编码器（IWAE）（Burda等人。2016年）是一种流行的变异推理方法，它通过优化多样本目标（即可表示为$K>1$ 蒙特卡洛样本的积分的目标）实现了比标准变异自动编码器更严格的证据约束（因此偏差更低）。不幸的是，IWAE关键是依赖于可利用的重定理，即使这些存在，多样本目标导致推理网络梯度随着$K$增加而分解（Rainforth等人。2018）。这种崩溃只能通过去除高变异的得分函数项来规避，要么启发式地忽略它们（这产生了Roeder等人（2017）的 "坚持-着陆 "的IWAE（IWAE-STL）梯度），要么通过Tucker等人（2019）的身份（这产生了 "双重参数化 "的IWAE（IWAE-DREG）梯度）。 在这项工作中，我们认为直接优化重要性抽样中的提议分布，如Bornschein & Bengio（2015）的重新加权唤醒（RWS）算法，比优化IWAE型多样本目标更好。 为了正式确定这一论点，我们引入了一个自适应重要性抽样框架，称为自适应重要性抽样学习（AISLE），它稍微概括了RWS算法。
随着模型和数据集的规模和复杂性的增长，对通信效率高的随机梯度下降的变体的需求也在增长，这些变体可以部署在集群上并行地进行模型拟合。Alistarh等人（2017）描述了数据并行SGD的两个变体，对梯度进行量化和编码以减少通信成本。 对于第二个变体，我们称之为QSGDinf，他们展示了大型神经网络分布式训练的令人印象深刻的经验收益。在他们的工作基础上，我们提出了一个量化梯度的替代方案，并表明它产生了比QSGD更强的理论保证，同时与QSGDinf的经验表现相匹配。
动物大脑中令人印象深刻的终身学习主要是通过突触连接的可塑性变化实现的。重要的是，这些变化不是被动的，而是由神经调节主动控制的，而神经调节本身是在大脑的控制之下的。由此产生的大脑的自我调节能力在学习和适应中发挥了重要作用，是生物强化学习的主要基础。 在一项任务中，具有数百万个参数的神经调节可塑性LSTM在基准语言建模任务中的表现优于标准LSTM（控制参数数量）。我们的结论是，可区分的神经调节可塑性为训练神经网络提供了一个强大的新框架。
深度学习在许多领域都取得了显著的成就。然而，学习神经网络的参数通常需要大量的标签数据，因此，深度学习的算法在应用于只有少量数据的监督学习时遇到了困难。 在测试过程中，结合测试样本和类中的点，形成一个新的单线，然后测试样本和类之间的相似性可以用新单线的体积与原始类单线的体积之比进行量化。 此外，我们还提出了一种利用卷积神经网络产生的特征图的局部区域来构建简约的方法。在Omniglot和miniImageNet上的实验验证了我们的简约算法在少量学习中的有效性。
储能计算是解释大脑如何学习时间序列（如运动）的有力工具，但现有的学习方案要么在生物学上不靠谱，要么效率太低，无法解释动物的表现。我们表明，如果储能神经元网络与第二个网络相结合，作为动态工作存储器，并向储能神经元提供时空骨干信号，那么网络可以用奖励调制的Hebbian学习规则学习复杂序列。 与工作记忆相结合，读出神经元的奖励调节赫比恩学习的表现与FORCE学习一样好，但其优点是对学习规则和学习范式都有生物学上合理的解释。
卷积结构最近被证明在许多序列建模任务上具有竞争力，与事实标准的递归神经网络（RNN）相比，同时由于固有的并行性而提供了计算和建模优势。 在这项工作中，我们提出了随机时间卷积网络（STCNs），一种结合了时间卷积网络（TCN）的计算优势和随机潜伏空间的表示能力和鲁棒性的新型架构。 特别是，我们提出了一个随机潜变量的层次结构，以捕捉不同时间尺度的时间依赖性。由于确定层和随机层的解耦，该架构是模块化和灵活的。我们表明，所提出的架构在几个任务中实现了最先进的对数可能性。最后，该模型能够在手写文本建模中预测远距离时间范围内的高质量合成样本。
弱收缩映射是一种自映射，其范围总是域的一个子集，它承认有一个唯一的固定点。弱收缩映射的迭代是一个Cauchy序列，可以得到唯一的固定点。提出了一种无梯度的优化方法作为弱收缩映射的应用，以实现全局最小收敛。该优化方法对局部最小值和初始点位置具有鲁棒性。
在过去的十年中，出现了两种相互竞争的控制策略，用于解决具有高效力的复杂控制任务。基于模型的控制算法，如模型预测控制（MPC）和轨迹优化，窥视基础系统动力学的梯度，以解决具有高采样效率的控制任务。 然而，像所有基于梯度的数值优化方法一样，基于模型的控制方法对惯性很敏感，容易陷入局部最小值。另一方面，深度强化学习（DRL）通过采样探索解决方案空间，可以在一定程度上缓解这些问题--代价是计算成本。 在本文中，我们提出了一种混合方法，结合了基于梯度的方法和DRL的最佳方面。我们的算法基于深度确定性策略梯度（DDPG）算法，并提出了一个简单的修改，即使用来自可分化物理模拟器的真实梯度来提高行为者和批评者的收敛率。 我们在七个二维机器人控制任务上演示了我们的算法，其中最复杂的是具有硬接触约束的可微分半猎豹。实证结果表明，我们的方法提高了DDPG的性能，而没有牺牲其对局部最小值的鲁棒性。
我们提出了贝叶斯超网络：神经网络中近似贝叶斯推理的框架。贝叶斯超网络h是一个神经网络，它学习将简单的噪声分布p(e)=N(0,I)转换为另一个神经网络（"主网络"）的参数t上的分布q(t):=q(h(e))。 我们用变异推理来训练q，使用一个可逆的h，以便通过抽样对后验p(t | D)进行有效的变异下限估计。与大多数贝叶斯深度学习的方法不同，贝叶斯超神经网络可以代表一个复杂的多模态近似后验，参数之间具有相关性，同时可以对q(t)进行廉价的iid抽样。 在实践中，贝叶斯超净提供了比dropout更好的防御对抗性例子的能力，并且在评估模型不确定性的一系列任务上表现出有竞争力的性能，包括正规化、主动学习和异常检测。
基于进化的优化方法最近在Atari和机器人运动等领域表现出了很好的效果，但在直接从像素解决3D任务方面却不尽如人意。本文提出了一种叫做深度创新保护（DIP）的方法，可以为这种3D环境端到端训练复杂的世界模型。该方法的主要思想是采用多目标优化，在时间上减少世界模型中特定组件的选择压力，允许其他组件适应。我们研究这些进化网络的新兴表征，它们学习世界的模型而不需要特定的前向预测损失。
对抗性训练是学习鲁棒模型的最流行的方法之一，但通常是依赖于攻击和时间成本的。在本文中，我们提出了MACER算法，它不使用对抗性训练来学习鲁棒模型，但比所有现有的可证明的l2防御性能更好。最近的工作表明，随机平滑可以用来为平滑分类器提供认证的l2半径，我们的算法通过MAximizing the CErtified Radius（MACER）训练了可证明的鲁棒平滑分类器。 无攻击的特性使得MACER的训练速度更快，更容易优化。在我们的实验中，我们表明我们的方法可以在广泛的数据集上应用于现代深度神经网络，包括Cifar-10、ImageNet、MNIST和SVHN。对于所有的任务，MACER比最先进的对抗性训练算法花费更少的训练时间，而且学到的模型可以实现更大的平均认证半径。
Actor-critic方法通过更新被称为actor的参数化策略来解决强化学习问题，该策略的方向是增加被称为critic的预期收益的估计值。 我们的主要理论贡献有两个方面。首先，我们表明GAC通过在行动空间中进行二阶优化来更新指导演员，其中曲率矩阵是基于批评者的黑森斯。其次，我们表明当黑森斯被忽略时，确定性策略梯度方法是GAC的一个特例。
Deep Infomax~(DIM)是一个无监督的表示学习框架，通过最大化编码器的输入和输出之间的相互信息，同时对输出施加概率约束。在本文中，我们提出了监督的Deep InfoMax~(SDIM)，它对编码器输出引入了监督的概率约束。 受监督的概率约束相当于高层次数据表示上的生成分类器，其中样本的类条件对数可能性可以被评估。与其他用条件生成模型构建生成分类器的工作不同，SDIM可以在复杂的数据集上进行扩展，并且可以实现与判别性对应的性能相媲美。 有了SDIM，我们可以进行 拒绝分类（classification with rejection）。SDIM不是总是报告一个类别标签，而是只在测试样本的最大对数超过一些预先选择的阈值时才进行预测，否则它们将被视为不在数据分布范围内，并被拒绝。 我们的实验表明，带有拒绝策略的SDIM可以有效地拒绝非法输入，包括超出分布的样本和对抗性例子。
摘要 在这项工作中，我们描述了一套用于设计和初始化条件良好的神经网络的规则，其指导思想是在训练开始时自然平衡Hessian的对角线块。我们展示了我们对块的调节措施与另一种自然的调节措施的关系，即权重梯度与权重的比率。 我们证明，对于基于ReLU的深度多层感知器，使用扇入和扇出的几何平均值的简单初始化方案满足我们的缩放规则。对于更复杂的架构，我们展示了我们的缩放原则如何被用来指导设计选择以产生条件良好的神经网络，减少猜测工作。
我们研究了自然语言处理中的模型提取问题，其中只有查询权限的对手试图重建该模型的本地副本。假设对手和受害者模型都微调了一个大型预训练语言模型，如BERT（Devlin et al, 事实上，攻击者甚至不需要使用语法或语义上的查询：我们表明，随机的单词序列加上特定任务的启发式方法形成了有效的查询，用于在一组不同的NLP任务（包括自然语言推理和问题回答）上提取模型。 因此，我们的工作强调了一种仅因NLP社区内向转移学习方法的转变而变得可行的利用方式：对于几百美元的查询预算，攻击者可以提取一个性能仅比受害者模型稍差的模型。最后，我们研究了两种针对模型提取的防御策略--成员分类和API水印--虽然对一些对手很成功，但也可以被更聪明的对手规避。
我们提出了SEARNN，这是一种新型的循环神经网络（RNN）训练算法，其灵感来自于结构化预测的 "学习搜索"（L2S）方法。RNN在结构化预测应用中取得了广泛的成功，如机器翻译或解析，并且通常使用最大似然估计（MLE）进行训练。 不幸的是，这种训练损失并不总是测试误差的适当替代物：通过仅最大化地面真相概率，它未能利用结构化损失提供的大量信息。此外，它引入了训练和预测之间的差异（如暴露偏差），可能会损害测试性能。相反，SEARNN利用类似测试的搜索空间探索，引入更接近测试误差的全局-局部损失。然后，我们提出了一个子采样策略，使SEARNN能够扩展到大的词汇量，这使我们能够在机器翻译任务上验证我们的方法的好处。
深度强化学习在连续控制问题上表现出越来越强的能力，包括可以在环境中熟练而敏捷地移动的代理人。在这种情况下，一个开放的问题是开发良好的策略来整合或合并多种技能的政策，其中每个单独的技能是一个特定技能及其相关状态分布的专家。我们将策略提炼方法扩展到连续行动设置中，并利用这一技术来结合专家策略，在不同类别的地形上模拟双足运动领域进行了评估。 最后，我们的方法使用转移学习来帮助有效地获得新的技能。这些方法的结合使得策略可以用新的技能逐步增强。我们将我们的渐进式学习和通过蒸馏整合（PLAID）的方法与三个替代基线进行比较。
深度强化学习（DRL）最近在复杂的控制任务上取得了许多突破，例如在围棋游戏中击败了最好的人类棋手。然而，DRL代理做出的决定是无法解释的，这阻碍了它在安全关键环境中的应用。 Viper是最近提出的一项技术，它通过模仿DRL代理构建了一个决策树策略。决策树是可解释的，因为所做的每个行动都可以追溯到导致它的决策规则路径。然而，一个接近DRL策略的全局决策树在决策边界的几何方面有很大的局限性。 我们提出了MoET，一个更具表现力，但仍可解释的基于混合专家的模型，由一个分割状态空间的门控函数和多个专门处理不同分区的决策树专家组成。我们提出了一个训练程序，以支持无差别的决策树专家，并将其纳入Viper的模仿学习程序。 我们在四个OpenAI体育馆环境中评估了我们的算法，并表明以这种方式构建的政策性能更高，并通过降低错误预测和增加奖励来更好地模仿DRL代理。我们还表明MoET政策可以使用现成的自动定理证明器（如Z3）进行验证。
我们考虑了在$/mathbb{R}^d$中平滑目标函数的无约束最小化问题，在这个环境中只有函数评估是可能的。 特别是，我们提出了SMTP，即随机三点法（STP）Bergou等人（2019）的动量版本。我们展示了非凸、凸和强凸函数的新复杂性结果。(我们在不同难度的MuJoCo Todorov等人的环境中测试了我们的方法，并与STP、其他最先进的无导数优化算法和策略梯度方法进行了比较。我们的第二个贡献是带有重要性采样的SMTP，我们称之为SMTP_IS。
使用类标签来表示类的相似性是训练用于检索的深度散列系统的典型方法；来自相同或不同类的样本采取二进制的1或0的相似性值。这种相似性并不模拟数据点之间可能存在的全部丰富的语义关系知识。在这项工作中，我们建立在使用语义层次结构的想法上，在所有可用的样本标签之间形成距离度量；例如，猫到狗的距离比猫到吉他要小。 我们将这种类型的语义距离结合到损失函数中，以促进深度神经网络嵌入之间的相似距离。我们还引入了一个经验性的Kullback-Leibler发散损失项，以促进嵌入的二进制化和统一性。我们测试了所产生的SHREWD方法，并证明了使用紧凑的二进制哈希代码而不是实值代码的层次检索分数的改进，并表明在弱监督的哈希设置中，我们能够在不明确依赖类标签，而是依赖标签之间相似性的情况下有竞争力地学习。
在本文中，我们提出了一种新的方法，通过局部细化来改善一个给定的表面映射。 该方法接收两个表面之间的既定映射，并遵循四个阶段：(i)检查映射并在不匹配的区域创建一个稀疏的地标集；(ii)用基于扁平化这些分割部分的低失真区域增长过程进行分割；(iii)优化分割部分的变形以对齐平面参数化域中的地标；以及(iv)聚合来自分段的映射以更新表面映射。 此外，我们提出了一种新的方法来使网格变形以满足约束条件（在我们的例子中，是阶段(iii)的地标对齐）。我们逐步调整约束条件的正切权重，并以一种保证变形后的网格没有翻转的面和低保形变形的方式应用变形。 我们的新变形方法，迭代最小二乘保形映射（ILSCM），优于其他低失真的变形方法。该方法是通用的，我们通过改进不同的现有表面映射方法的映射来测试它。我们还通过编辑各种三维物体的映射来测试其有效性。
了解深度神经网络的突破性性能是当今科学界面临的最大挑战之一。在这项工作中，我们对深度网络优化过程的行为及其泛化能力引入了信息论的观点。通过研究信息平面，即每个隐藏层的输入变量和预期标签之间的相互信息平面。 此外，我们明确表明，这两个基本的信息理论量与网络的泛化误差相对应，因为我们引入了一个新的泛化约束，该约束在表示压缩中是指数级的。
我们提出并研究了一种为回归任务学习可解释表征的方法。特征被表示为由神经网络中常见的激活函数和其他基本函数组成的多类型表达树网络。 我们在这个框架内比较了几种随机优化方法。我们在100个开源回归问题上对这些变体与最先进的机器学习方法进行了比较。我们的主要发现是，这种方法在不同的问题上产生了最高的平均测试分数，同时产生的表示比下一个表现最好的方法（梯度提升）小几个数量级。我们还报告了一个负面结果，即试图直接优化表示的离散性导致更高度相关的特征。
大多数分布式机器学习（ML）系统在每台机器上都存储了一份模型参数的副本，以尽量减少网络通信。在实践中，为了减少同步等待时间，这些模型的副本不一定是同步更新的，可能会变得陈旧。尽管在大规模ML方面有很多发展，但陈旧性对学习效率的影响还没有定论，主要是因为在复杂的分布式环境中控制或监测陈旧性是具有挑战性的。 在这项工作中，我们研究了广泛的ML模型和算法在延迟更新下的收敛行为。我们广泛的实验揭示了呆滞性对ML算法收敛影响的丰富多样性，并对文献中看似矛盾的报告提供了见解。实证研究结果也启发了SGD在呆滞性下非凸优化的新收敛分析，与O(1/sqrt{T})的最佳已知收敛率相符。
系统识别是通过对未知系统的输入和输出的测量建立其数学模型的过程，它是基于模型的控制、估计器设计和输出预测的一个关键步骤。 我们在一个耦合洛伦兹吸引子的模拟系统上测试了我们的算法，表明我们的算法有能力识别高维系统，而这些系统对于基于粒子的方法来说是难以实现的。我们还使用SISL来识别特技直升机的动力学。通过用未观察到的流体状态来增强状态，我们学习了一个比最先进的方法更能预测直升机加速度的模型。
人们提出了各种梯度压缩方案，以减轻大规模机器学习模型的分布式训练的通信成本。在本文中，我们对基于符号的非凸优化方法进行了一般性分析，我们的分析建立在成功概率的直观界限上，不依赖于特殊的噪声分布，也不依赖于随机梯度方差的有界性。 在参数服务器框架内将理论扩展到分布式设置，我们保证了相对于节点数量的指数级快速方差减少，在两个方向上保持1位的压缩，并使用小批量大小。
非政策性学习，即利用从记录政策中收集的历史数据来评估和改进政策的任务，是很重要的，因为政策性评估通常是昂贵的，而且有不利的影响。非政策性学习的主要挑战之一是导出反事实估计器，它也有低方差，因此有低概括误差。在这项工作中，受重要性抽样问题的学习界限的启发，我们提出了一个新的反事实学习原则，用于带有匪徒反馈的非政策学习。我们的方法通过最小化记录政策和新政策之间的分布发散来规整泛化误差，并消除了先前工作中通过所有训练样本迭代计算样本方差规整的需要。对于神经网络政策，我们使用变量发散最小化的端到端训练算法比传统基线算法有了显著改善，也与我们的理论结果一致。
我们概述了将深度学习的思想纳入基于波浪的最小二乘法成像的新方法。这项工作的目的和主要贡献是将手工制作的约束与深度卷积神经网络相结合，以此来利用其生成自然图像的显著便利性。我们的方法的数学基础是期望最大化框架，其中数据被分批划分并与额外的 "潜在 "未知数相耦合。 这些未知数是来自原始未知空间的一对元素（但现在与特定的数据批次相耦合）和网络输入。在这种情况下，神经网络控制这些额外参数之间的相似性，作为一个 "中心 "变量。
当把自然语言问题翻译成SQL查询以回答数据库中的问题时，当代语义解析模型很难概括到未见过的数据库模式。 泛化的挑战在于：（a）以语义解析器可访问的方式对数据库关系进行编码，以及（b）对数据库列和它们在特定查询中的提及进行建模。 我们提出了一个统一的框架，基于关系感知的自我关注机制，以解决模式编码、模式链接和文本到SQL编码器中的特征表示。在具有挑战性的Spider数据集上，这个框架将精确匹配的准确率提高到53.7%，而之前未用BERT嵌入增强的最先进的模型的准确率为47.4%。此外，我们观察到模型对模式链接和对齐的理解有了质的提高。
正如我们的经验所示，人类可以学习和部署无数不同的技能，以解决他们每天遇到的情况。相反，神经网络有一个固定的记忆容量，使他们无法学习超过几套技能，然后开始忘记它们。在这项工作中，我们迈出了将神经网络与人类的学习能力联系起来的一步。为此，我们提出了一个具有不断增长和开放约束的记忆容量的模型，可以根据模型的当前需求进行访问。为了测试这个系统，我们引入了一个基于语言建模的持续学习任务，模型依次接触多种语言和领域，而不提供任何关于它当前处理的输入类型的明确信号。所提出的系统表现出改进的适应能力，即在输入语言或领域转换后，它可以比可比的基线更快地恢复。
我们建议通过计算概率密度函数的时间演化来解决时间序列回归问题，以提供概率预测。我们采用了一个基于循环神经网络（RNN）的模型来学习概率密度函数的时间演化的非线性算子。 引入了显性和隐性的正则化策略，对估计的概率分布施加了一个平滑性条件。
在认知系统中，工作记忆的作用对于视觉推理和决策至关重要。在理解人类/动物工作记忆的机制以及制定不同的人工神经网络框架方面已经取得了巨大的进展。 就人类而言，视觉工作记忆（VWM）任务是一个标准的任务，在这个任务中，受试者会看到一连串的图像，每个图像都需要识别它是否已经被看过。我们的工作是研究使用递归神经网络学习工作记忆模型的多种方法，这些神经网络学会跨时间段记忆输入的图像。我们通过在监督和强化学习环境下用图像序列训练这些神经网络来解决工作记忆任务。 强化学习设置的灵感来自于神经科学的流行观点，即前额叶皮层的工作记忆是由多巴胺能机制调节的。 我们对这些模型在标准图像数据集（CIFAR-100）的图像序列上的表现进行了定量评估。此外，我们评估了它们在不断训练的过程中的记忆和回忆能力。基于我们的分析，我们确定了一个具有长短期记忆单元的门控递归神经网络模型，该模型使用强化学习进行训练，在时间上巩固了输入的空间信息，具有很强的效率。这项工作是初步分析，是我们最终目标的一部分，即利用人工神经网络对大脑工作记忆的行为和信息处理进行建模，并利用在VWM认知任务中从人类受试者那里捕获的脑成像数据来理解大脑的各种记忆机制。
非线性对深度（神经）网络（DN）的性能至关重要。迄今为止，对现有非线性的理解进展甚微，但最近在理解片状仿射和凸状非线性（如ReLU和绝对值激活函数和最大池）所发挥的作用方面取得了进展。特别是，由这些操作构建的DN层可以被解释为{em max-affine spline operator}。(MASO)，与矢量量化(VQ)和$K$-means有优雅的联系。虽然这是良好的理论进展，但整个MASO方法的前提是要求非线性是片状仿生和凸的，这就排除了重要的激活函数，如sigmoid、双曲切线和softmax。 {本文通过将确定性的MASO与概率性的高斯混合模型（GMMs）联系起来，将MASO框架扩展到这些和无限大的一类新的非线性因素。 我们表明，在GMM下，像ReLU、绝对值和max-pooling这样的片状仿生、凸状非线性可以被解释为某些自然的 "硬 "VQ推理问题的解决方案，而sigmoid、双曲切线和softmax可以被解释为相应的 "软 "VQ推理问题的解决方案。 我们通过混合硬性和软性的VQ优化来进一步扩展框架，创建一个在硬性、软性和线性VQ推理之间插值的$/beta$-VQ推理。一个$/beta$-VQ DN非线性的主要例子是{em/swish}非线性，它在一系列计算机视觉任务中提供了最先进的性能，但是是通过实验临时开发的。最后，我们通过实验验证了我们理论的一个重要论断，即DN性能可以通过在其线性过滤器中强制执行正交性而得到显著改善。
工程蛋白为解决生物医学、能源和材料科学中的许多问题提供了潜力，但创造成功的设计在实践中是很困难的。这一挑战的一个重要方面是蛋白质序列和三维结构之间的复杂耦合，寻找可行设计的任务通常被称为反蛋白质折叠问题。 我们开发了以设计目标的图结构规范为条件的蛋白质序列生成模型。我们的方法通过关注那些在序列中的长程但在三维空间中的局部，有效地捕捉了蛋白质中的复杂依赖关系。我们的框架大大改进了先前给定结构的蛋白质序列参数模型，并在深度生成模型的帮助下向快速和有针对性的生物分子设计迈出了一步。
我们对深度网络中层块的前向传递提供了一个新的视角。特别是，我们表明，前向传递通过一个标准辍学层，然后是一个线性层和一个非线性激活，相当于用$tau$-nice近似随机梯度方法的一次迭代来优化一个凸目标。我们进一步表明，用加性辍学取代标准伯努利辍学，相当于用一个方差减少的近似方法来优化相同的凸目标。 通过将全连接层和卷积层表达为高阶张量积的特例，我们在张量设置中统一了基本的凸优化问题，并推导出用于确定上述近似方法最佳步长的Lipschitz常数$L$的公式。我们对应用于CIFAR-10和CIFAR-100数据集的标准卷积网络进行了实验，结果表明用相应求解器的多次迭代取代一个层块，通过$L$设置步长，持续提高分类精度。
在推理时间以低精度操作运行的深度网络与高精度的替代方案相比，具有功率和空间上的优势，但需要克服随着精度降低而保持高精度的挑战。这里，我们提出了一种训练这种网络的方法--学习步长量化，当使用来自各种架构的模型时，在ImageNet数据集上实现了迄今为止最高的精度，其权重和激活被量化到2、3或4比特的精度，并且可以训练出达到全精度基线精度的3比特模型。 我们的方法通过改进量化器本身的配置方式，建立在现有的量化网络中学习权重的方法之上。具体来说，我们引入了一种新的方法来估计和扩展每个权重和激活层的量化器步长的任务损失梯度，这样它就可以和其他网络参数一起学习。
最近有几项研究表明，强大的自然语言理解（NLU）模型很容易依赖不需要的数据集偏见，而不学习基本的任务，导致模型不能泛化到域外数据集，并可能在现实世界的场景中表现不佳。我们提出了几种学习策略来训练神经模型，这些模型对这种偏见更加稳健，并能更好地转移到域外的数据集。 我们引入了一个额外的轻量级纯偏见模型，该模型学习数据集的偏见，并使用其预测来调整基础模型的损失，以减少偏见。换句话说，我们的方法降低了有偏见的例子的重要性，并将训练重点放在硬例子上，即只依靠偏见不能正确分类的例子。 我们在大规模的自然语言推理和事实验证数据集及其域外数据集上进行了实验，结果表明我们的去偏模型在所有环境下都能显著提高鲁棒性，包括在FEVER对称评估数据集上获得9.76分，在HANS数据集上获得5.45分，在SNLI硬集上获得4.78分。 这些数据集是专门为评估模型在域外环境中的鲁棒性而设计的，其中训练数据中的典型偏差不存在于评估集中。
少数视角的X射线计算机断层扫描（CT）数据的重建是一个高难度的问题。它经常被用于需要低辐射剂量的临床CT、快速工业扫描或固定门式CT的应用中。现有的分析或迭代算法通常产生很差的重建图像，因伪影和噪声而严重恶化，特别是当X射线投影的数量相当少时。 本文提出了一种深度网络驱动的方法，通过将基于卷积神经网络的推理纳入最先进的迭代重建，来解决极端的少数视角CT。
在开放领域的对话中，智能代理应该表现出对知识的使用，然而到目前为止，很少有令人信服的证明。最流行的序列到序列模型通常是 "产生和希望 "通用语词，当从输入语词映射到输出时，可以记忆在模型的权重中，而不是采用回忆的知识作为背景。 到目前为止，知识的使用被证明是困难的，部分原因是缺乏一个监督下的学习基准任务，该任务展示了具有明确基础的知识性公开对话。为此，我们收集并发布了一个大型数据集，其中的对话直接以从维基百科中检索的知识为基础。 然后，我们设计了能够检索知识、阅读和调节知识并最终生成自然回应的架构。我们表现最好的对话模型能够对开放领域的话题进行有知识的讨论，这是由自动指标和人类评价所评估的，而我们的新基准允许衡量这一重要研究方向上的进一步改进。
我们在半监督学习和情境匪徒的交叉点上制定了一个新的问题，其动机是包括临床试验和对话系统在内的几个应用。我们展示了情境匪徒和图卷积网络如何调整以适应新的问题表述。然后，我们采取两种方法的最佳方式来开发多通用网络嵌入式情境匪徒。我们的算法在几个真实世界的数据集上得到验证。
 一个科学论文集通常伴随着标签：与每篇论文相关的关键词、主题、概念等。 有时这些标签是人为生成的，有时是机器生成的。 我们提出了一个简单的衡量科学论文标签一致性的方法：这些标签对引用图的链接是否有预测性。 由于作者倾向于引用与他们出版物主题相近的论文，一个一致的标签系统可以预测引用。 我们提出了一种计算一致性的算法，并对人类和机器生成的标签进行了实验。 我们表明，增强，即人工标签与机器生成的标签的结合，可以提高标签的一致性。 我们进一步介绍了交叉一致性，即预测由不同标记器（如手工和机器）标记的论文之间的引文联系的能力。 当标签数据量有限时，交叉一致性可以用来评估标签的质量。
最近的研究集中揭示了深度神经网络的脆弱性，特别是卷积神经网络（CNN）在图像识别任务上的脆弱性，通过创建与合法样本 "略有不同 "的对抗性样本。 在这项工作中，我们提出了一种基于量化的方法，使CNN能够有效地过滤掉对抗性扰动。值得注意的是，与之前关于输入量化的工作不同，我们在CNN的中间层应用量化。 我们的方法与CNN学到的粗粒度语义信息的聚类自然一致。此外，为了补偿量化不可避免地造成的信息损失，我们提出了多头量化，我们将数据点投射到不同的子空间，并在每个子空间内进行量化。 在MNIST和Fashion-MNSIT数据集上获得的结果表明，只需在CNN中加入一个Q层，就可以显著提高其对白盒和黑盒攻击的鲁棒性。
不变和等价网络已经成功地用于学习图像、集合、点云和图形。开发这种网络的一个基本挑战是找到不变和等价的最大集合/emph{linear}层。虽然这个问题对于前三个例子（至少对于流行的变换）已经有了答案，但是对于图形的不变和等价线性层的完整特征还不知道。在本文中，我们提供了（超）图数据的所有互换不变和等价线性层的特征，并表明在边值图数据的情况下，其维度分别为2美元和15美元。更一般地说，对于定义在$k$节点元组上的图数据，其维度为$k$-th和$2k$-th贝尔数。 从理论的角度来看，我们的结果概括并统一了最近在等价深度学习方面的进展。特别是，我们表明我们的模型能够近似于任何消息传递的神经网络。在一个简单的深度神经网络框架中应用这些新的线性层被证明能够取得与最先进的结果相媲美的效果，并且比以前的不变和等价基数具有更好的表达能力。
在强化学习中，我们可以学习未来观察和奖励的模型，并使用它来计划代理人的下一步行动。然而，如果观察是高维的（如图像），联合建模未来的观察可能是计算昂贵的，甚至是难以实现的。由于这个原因，以前的工作考虑了部分模型，它只对观察的一部分进行建模。 在本文中，我们表明部分模型可能是因果关系不正确的：它们被它们没有建模的观察所混淆，因此可能导致不正确的规划。为了解决这个问题，我们引入了一个通用的部分模型系列，这些模型被证明是因果关系正确的，但避免了对未来观察进行完全建模的需要。
在终身学习中，学习者被呈现出一连串的任务，逐步建立一个数据驱动的先验，可以利用它来加速新任务的学习。在这项工作中，我们研究了当前终身方法的效率，在样本复杂性、计算和内存成本方面。为此，我们首先引入了一个新的和更现实的评估协议，学习者对每个例子只观察一次，超参数选择是在一个小而不相连的任务集上进行的，这不用于实际学习经验和评估。 第二，我们引入了一个新的指标来衡量学习者获得新技能的速度。第三，我们提出了GEM（Lopez-Paz & Ranzato, 2017）的改进版本，被称为平均GEM（A-GEM），它享有与GEM相同甚至更好的性能，同时在计算和内存方面几乎与EWC（Kirkpatrick et al, 最后，我们表明，如果向所有算法提供指定分类任务的任务描述符，包括A-GEM在内的所有算法都能更快地学习。
低精度计算是解决日益扩大的 "计算差距 "的关键领域之一，由深度学习应用的指数级增长所驱动。近年来，深度神经网络训练已基本迁移到16位精度，并在性能和能源效率方面取得了重大进展。然而，由于反向传播的精度和动态范围要求更高，试图以8位精度训练DNN遇到了重大挑战。  在本文中，我们提出了一种使用8位浮点表示权重、激活、误差和梯度的方法来训练深度神经网络。 我们在多个数据集（imagenet-1K、WMT16）和比以前报告的更广泛的工作负载集（Resnet-18/34/50、GNMT和Transformer）中展示了最先进的准确性。  我们提出了一种增强的损失缩放方法，以增强8位浮点的减少的次正常范围，以改善错误传播。我们还研究了量化噪声对泛化的影响，并提出了一种随机四舍五入技术来解决梯度噪声。作为应用所有这些技术的结果，我们报告的验证精度比全精度基线略高。
损失函数在深度度量学习中起着至关重要的作用，因此人们提出了各种损失函数。一些损失函数通过成对或三倍的相似性约束来监督学习过程，而另一些则利用多个数据点之间的结构化相似性信息。 首先，与分类交叉熵（CCE）类似，ICE有明确的概率解释，并利用结构化的语义相似性信息进行学习监督。其次，ICE可以扩展到无限的训练数据，因为它在迷你批次上迭代学习，并且与训练集大小无关。 第三，在我们的相对权重分析的激励下，加入了无缝样本重权。它重新调整了样本的梯度，以控制训练实例的差异化程度，而不是通过样本挖掘来截断它们。除了它的简单性和直观性，在三个真实世界基准上的广泛实验证明了ICE的优越性。
在基于模型的强化学习中，代理在模型学习和规划之间交错进行。 这两个部分是密不可分的。如果模型不能提供合理的长期预测，被执行的计划者就会利用模型的缺陷，这可能会产生灾难性的失败。本文着重于建立一个可以推理长期未来的模型，并演示如何利用这个模型进行有效的计划和探索。为此，我们利用最近的变异推理思想，建立一个潜伏变量自回归模型。 我们认为，通过一个辅助任务迫使潜变量携带未来信息，可以大大改善长期预测。此外，通过在潜空间中进行规划，可以确保规划者的解决方案在模型有效的区域内。
检测异常对于包括卫星系统在内的各种工业应用和关键任务基础设施来说越来越重要。虽然在基于规则或基于机器学习的方法检测卫星系统的异常方面已经有了一些研究，但基于张量的分解方法还没有被广泛地探索用于异常检测。在这项工作中，我们引入了一个基于张量的综合异常检测（ITAD）框架来检测卫星系统的异常。 由于高风险和高成本，检测卫星系统中的异常是至关重要的。我们用从韩国多用途卫星-2（KOMPSAT-2）收集的遥测数据构建三阶张量，并使用通过应用CANDECOMP/PARAFAC分解得到的一个分量矩阵计算异常得分，以检测异常。
许多现实世界的任务表现出丰富的结构，这些结构在状态空间的不同部分或时间上是重复的。在这项工作中，我们研究了利用这种重复结构来加速和规范化学习的可能性。我们从KL规范化期望奖励目标出发，引入了一个额外的组成部分，即默认策略。 我们正式确定了这一策略，并讨论了与信息瓶颈方法和变异EM算法的联系。我们在离散和连续行动领域提出了经验结果，并证明对于某些任务，与政策一起学习默认政策可以大大加快和改善学习。请观看视频，演示在几个连续控制任务上学习专家和默认政策( https://youtu.be/U2qA3llzus8 )。
当图像分类器进行预测时，图像的哪些部分是相关的，为什么？我们可以把这个问题改写为：如果分类器没有看到图像的哪些部分，将最多地改变它的决定？产生一个答案需要对可能被看到但没有被看到的图像进行边缘化。 我们的方法与临时性的填充方法形成对比，如模糊或注入噪声，这些方法产生的输入远离数据分布，并忽略了图像不同部分之间的信息关系。我们的方法产生了更紧凑和相关的显著性地图，与以前的方法相比，人工痕迹更少。
本文提出了变异网络（VarNet），这是一个生成模型，提供了操作给定输入的高级属性的手段。我们的方法的独创性在于VarNet不仅能够处理预先定义的属性，而且还可以自己学习数据集的相关属性。 这两种设置可以很容易地结合起来，这使得VarNet适用于各种各样的任务。此外，VarNet有一个健全的概率解释，这给我们提供了一个在潜在空间中导航的新方法，以及控制属性如何被学习的手段。我们通过实验证明，这个模型能够进行有趣的输入操作，并且学到的属性是相关和可解释的。
尽管人们对机器学习系统在训练数据中依赖所谓的虚假模式感到震惊，但这个术语在标准的统计框架中缺乏连贯的含义。然而，因果关系的语言提供了清晰度：虚假的关联是那些由于共同的原因（混杂）与直接或间接的影响。在本文中，我们专注于NLP，介绍对虚假模式不敏感的训练模型的方法和资源。 给定文档和它们的初始标签，我们要求人类修改每个文档以符合反事实的目标标签，要求修改后的文档内部一致，同时避免任何无偿的改变。有趣的是，在情感分析和自然语言推理任务中，在原始数据上训练的分类器在反事实的修改后的对应数据上失败，反之亦然。 在组合数据集上训练的分类器表现得非常好，只比那些专门用于任何一个领域的分类器逊色。我们将公开发布这两个数据集。
在解释机器学习模型的多种方式中，测量一组与预测相联系的特征的重要性可能是解释模型的最直观的方式之一。在本文中，我们用一个新的评价标准--鲁棒性分析来建立一组特征与预测之间的联系，鲁棒性分析测量的是对抗性扰动的最小容忍度。 通过测量对抗性攻击的容忍度，我们可以提取一组特征，为当前的预测提供最稳健的支持，也可以提取一组特征，通过设置有针对性的对抗性攻击，将当前的预测与目标类进行对比。通过将这种方法应用于多个领域的各种预测任务，我们观察到所得出的解释确实从质量和数量上捕捉到了重要的特征集。
生成式对抗网络（GANs）已被证明提供了一种有效的方式来模拟复杂的分布，并在各种具有挑战性的任务上获得了令人印象深刻的结果。然而，典型的GANs在训练期间需要完全观察到的数据。在本文中，我们提出了一个基于GAN的框架，用于从复杂、高维的不完整数据中学习。 本文提出了一个基于GAN的框架，用于从复杂的、高维的不完整数据中学习。所提出的框架学习了一个完整的数据生成器，以及一个模拟缺失数据分布的掩码生成器。
在本文中，我们提出了一种新的压缩方法，层间权重预测（ILWP）和量化方法，该方法基于传统视频编码方案中的帧间预测方法，对所有卷积层中的权重的预测残差进行量化。基于SVWH，我们提出了第二种ILWP和量化方法，对相邻卷积层的权重之间的预测残差进行量化。由于预测的权重残差往往遵循方差很低的拉普拉斯分布，权重量化可以更有效地应用，从而产生更多的零权重，提高权重压缩率。 此外，我们提出了一种新的层间损失，用于消除非纹理位，这使我们能够更有效地只存储纹理位。也就是说，所提出的损失使权重正规化，使相邻两层之间的拼合权重具有相同的值。最后，我们提出了一种具有层间损失和量化方法的ILWP。我们的综合实验表明，与以前深度神经网络中基于量化的压缩方法相比，在相同精度水平上，所提出的方法实现了更高的权重压缩率。
在优化过程中，存在大量诱导参数模型的结构化稀疏性的技术，其最终目标是资源效率推断。然而，据我们所知，尽管报告FLOPs是结果的一部分，但没有一个技术是以特定的浮点运算（FLOPs）数量作为单一端到端的优化目标的一部分。 此外，"一刀切 "的方法忽视了现实的系统限制，例如，GPU和手机之间的差异很大--前者的浮点运算比后者的延迟要小；因此，在模型压缩期间，从业者能够指定浮点运算的目标数量是很重要的。在这项工作中，我们扩展了一项最先进的技术，直接将浮点运算作为优化目标的一部分，并表明，在给出所需的浮点运算要求后，可以成功训练不同的神经网络进行图像分类。
在过去的几十年里，类别领域之间的非配对图像翻译已经取得了显著的成功。最近的研究主要集中在两个挑战上。另一方面，现有的多模态方法在处理两个以上的领域时有局限性，即它们必须为每一对领域独立建立一个模型。为了解决这些问题，我们提出了层次化图像-图像翻译（HIT）方法，该方法在语义层次结构中共同制定了多模态和多领域的问题，并能进一步控制多模态的不确定性。 具体来说，我们将特定领域的变化视为领域的多粒度特性的结果，人们可以通过将一个具有较大变化的领域划分为多个子领域来控制多模态翻译的粒度，这些子领域捕捉局部和细粒度的变化。 在高斯先验的假设下，域的变化在一个共同的空间中被建模，这样就可以在一个模型中的多个域之间进一步进行翻译。为了学习这样复杂的空间，我们建议利用域之间的包容关系来约束父子的分布，使其嵌套。
递归神经网络（RNN）在解决有顺序数据的挑战性问题上非常成功。然而，这种观察到的效率还不能完全用理论来解释。众所周知，某类乘法RNN享有深度效率的特性------要实现由这种RNN计算的相同分数函数，需要一个指数级大宽度的浅层网络。 在这项工作中，我们试图通过将理论分析扩展到采用各种非线性的RNN，如Rectified Linear Unit（ReLU），来缩小理论与实践之间的差距，并表明它们也受益于普遍性和深度效率的特性。
虽然深度神经网络已被证明是许多识别和分类任务的有力工具，但它们的稳定性特性仍然没有得到很好的理解。过去，图像分类器已被证明容易受到所谓的对抗性攻击，这些攻击是通过加法扰动正确分类的图像而产生的。 在本文中，我们提出了ADef算法，以构建一种不同的对抗性攻击，这种攻击是通过梯度下降步骤对图像反复施加小的变形而产生的。我们在MNIST与卷积神经网络以及ImageNet与Inception-v3和ResNet-101上展示了我们的结果。
对抗性学习方法已被提出用于广泛的应用，但对抗性模型的训练可能是出了名的不稳定。有效地平衡生成器和鉴别器的性能是至关重要的，因为一个鉴别器如果能达到非常高的精度，就会产生相对无信息的梯度。 在这项工作中，我们提出了一种简单而通用的技术，通过信息瓶颈来约束鉴别器中的信息流。通过对观测值和鉴别器内部表示之间的相互信息进行约束，我们可以有效地调节鉴别器的精度并保持有用和有信息的梯度。 我们证明了我们提出的变异判别器瓶颈（VDB）导致了对抗性学习算法在三个不同应用领域的显著改善。我们的主要评估研究了VDB对动态连续控制技能的模仿学习的适用性，如跑步。 我们表明，我们的方法可以直接从原始视频演示中学习这种技能，大大优于先前的对抗性模仿学习方法。VDB还可以与对抗性逆向强化学习相结合，学习可以在新环境中转移和重新优化的解析性奖励函数。最后，我们表明，VDB可以更有效地训练GANs的图像生成，改善先前的一些稳定化方法。
在现实世界中部署机器学习系统，既需要在干净的数据上有很高的准确性，也需要对自然发生的损坏有很强的鲁棒性。虽然架构上的进步导致了准确性的提高，但建立鲁棒模型仍然具有挑战性，涉及到训练程序和数据集的重大变化。 以前的工作认为，在鲁棒性和准确性之间存在固有的权衡，例如标准的数据增强技术，如Cutout，它提高了清洁的准确性，但没有鲁棒性，而加性高斯噪声，它提高了鲁棒性，但损害了准确性。我们引入Patch Gaussian，一个简单的增强方案，在输入图像中随机选择的斑块上添加噪声。 用Patch Gaussian训练的模型在CIFAR-10和ImageNet Common Corruptions基准上达到了最先进的水平，同时在干净的数据上也保持了准确性。我们发现这种增强导致对高频噪声的敏感性降低（类似于高斯），同时保留了利用图像中相关高频信息的能力（类似于Cutout）。我们表明它可以与其他正则化方法和数据增强策略如AutoAugment一起使用。 最后，我们发现将扰动限制在斑块上的想法在对抗性学习的背景下也是有用的，产生的模型不会像无约束的对抗性训练那样出现精度损失。
偏移回归是许多视觉任务中空间定位的标准方法，包括人类姿势估计、物体检测和实例分割。然而，如果高定位精度对一项任务至关重要，卷积神经网络将偏移回归通常难以实现。 这可以归因于卷积操作的局部性，并因规模、杂波和视角的变化而加剧。一个更根本的问题是真实世界图像的多模式性。 相反，我们建议使用混合密度网络（MDN）进行偏移回归，允许模型有效地管理各种模式，并学习预测给定输入的输出的全条件密度。在野外的二维人类姿势估计中，需要准确地定位身体关键点，我们表明这在定位精度方面产生了显著的改善。特别是，我们的实验显示，视点变化是主要的多模式因素。 此外，通过仔细地初始化MDN参数，我们在训练中没有面临任何不稳定因素，而这是众所周知的MDN广泛部署的一大障碍。我们的研究结果强调了现实世界视觉的多模态性质，以及明确考虑视点变化的意义，至少在涉及空间定位时是如此。
像语言一样，音乐可以被表示为一连串的离散符号，形成一个分层的语法，音符大致上就像字符，而音符的图案就像单词。 然而，与文本不同的是，音乐在很大程度上依赖于多个时间尺度的重复，以建立结构和意义。"音乐转化器 "在生成具有结构的音乐方面显示出引人注目的结果（Huang等人，2018）。 在本文中，我们介绍了一个用交互式钢琴棒对多声部音乐进行可视化的自我关注的工具。 我们将音乐转化器作为一个描述性工具和生成模型。 对于前者，我们用它来分析现有的音乐，看看所产生的自我注意结构是否与音乐理论中已知的音乐结构相吻合。 对于后者，我们在生成过程中检查模型的自我注意，以了解过去的音符如何影响未来的音符。我们还将常规注意的注意结构与相对注意的注意结构进行比较和对比（Shaw等人，2018，Huang等人，2018），并检查其对所生成音乐的影响。 例如，对于JSB合唱团数据集，用相对注意力训练的模型在关注前一时间段的所有声音和之前的和弦，以及与乐句开头的和弦方面更加一致，使其能够创造一个弧线。 我们希望我们的分析能提供更多的证据，证明相对的自我注意是一个强大的归纳偏向，用于模拟音乐。 我们邀请读者探索我们的音乐注意力视频动画，并在https://storage.googleapis.com/nips-workshop-visualization/index.html，与可视化的内容互动。
我们研究了随机梯度下降（SGD）终点的统计特性。我们将SGD近似为随机微分方程（SDE），并在损失梯度各向同性的假设下考虑其玻尔兹曼吉布斯平衡分布。通过这一分析，我们发现三个因素--学习率、批次大小和损失梯度的方差--控制着SGD发现的最小值的深度和宽度之间的权衡，学习率与批次大小的比率越高，越有利于较宽的最小值。在均衡分布中，只有学习率与批量大小的比率出现，这意味着它在同时以相同的数量重新缩放时是不变的。我们通过实验从两个方面展示了学习率和批量大小是如何影响SGD的：SGD的终点和导致它的动态变化。对于端点，实验表明，在同时调整批量和学习率的情况下，SGD的端点是相似的，同时，较高的比率会导致更平坦的最小值，这两个发现与我们的理论分析是一致的。我们在实验中注意到，在学习率和批次大小的同样重构下，动态似乎也是相似的，我们探索表明可以在循环学习率的安排中交换批次大小和学习率。接下来，我们说明了噪声是如何影响记忆的，表明高噪声水平会导致更好的泛化。最后，我们通过实验发现，如果学习率过大或批量大小过小，学习率和批量大小同时重新调整下的相似性会被打破。
尽管词类比问题已经成为评估词向量的标准工具，但人们对词向量为什么能很好地解决这些问题知之甚少。在本文中，我试图通过开发一种简单但高度精确的生成方法来解决问题中涉及的所有术语都是名词的情况下的词类比问题，来进一步了解这个问题。 我的结果证明了与学习词对之间的关系有关的模糊性，以及训练数据集在决定最突出的关系方面的作用。此外，我的结果表明，一个模型准确解决词类比问题的能力可能并不表明一个模型像人类那样学习词对之间关系的能力。
从预训练的ImageNet模型中进行微调已经成为各种计算机视觉任务的事实标准。目前的微调实践通常包括选择一个临时的超参数，并将它们固定在通常用于从头训练的值上。本文重新审查了设置微调超参数的几种常见做法。 我们发现，选择合适的动量值对微调性能至关重要，并与之前的理论研究结果相联系。（2）微调的最佳超参数，特别是有效学习率，不仅取决于数据集，而且对源域和目标域之间的相似度也很敏感。 (3) 基于参考的正则化使模型接近于初始模型，但不一定适用于 "不相似 "的数据集。
通过微调预训练的神经网络与极其庞大的数据集（如ImageNet）进行迁移学习，可以大大加快训练速度，而准确性却经常受到新目标任务的有限数据集规模的瓶颈。为了解决这个问题，人们研究了一些正则化方法，用起点作为参考来约束目标网络的外层权重（SPAR）。 具体来说，除了最小化经验损失外，DELTA还打算通过约束由注意力精确选择的特征图子集，使两个网络的外层输出保持一致，这些特征图子集是以监督学习的方式学习的。我们用最先进的算法评估DELTA，包括L2和L2-SP。
神经模型在许多自然语言处理任务中取得了相当大的改进，但它们提供的透明度很小，而且可解释性是有代价的。在某些领域，没有理由的自动预测的适用性是有限的。 现有的模型不能在一次训练中处理一个以上的方面，并诱导出可能有歧义的二进制掩码。在我们的工作中，我们提出了一个预测评论的多方面情绪的神经模型，并以无监督和多任务学习的方式同时生成一个概率性的多维掩码（每个方面一个）。我们的评估表明，在啤酒和酒店领域的三个数据集上，我们的模型优于强基线，生成的掩码是：强特征预测，有意义，可解释。
神经序列的生成通常采用最大似然（ML）估计或强化学习（RL）的方法。然而，众所周知，它们有各自的缺点；ML会出现训练/测试差异，而RL会受到样本效率低下的影响。 我们指出，由于ML和RL之间的权衡，很难同时解决所有的缺点。为了应对这些问题，我们提出了一个使用α发散的序列生成的目标函数，这导致了一个ML-RL综合方法，利用了ML和RL的更好的部分。 我们证明了所提出的目标函数概括了ML和RL的目标函数，因为它包括了两者的特殊情况（ML对应于α→0，RL对应于α→1）。我们提供了一个命题，说明RL目标函数和所提出的目标函数之间的差异随着α的增加而单调地减少。机器翻译任务的实验结果表明，最小化所提出的目标函数比基于ML的方法实现了更好的序列生成性能。
Capsule网络在MNIST、CIFAR和smallNORB等基准计算机视觉数据集上显示出令人鼓舞的结果。尽管如此，它们还需要在以下任务中进行测试：（1）检测到的实体本身具有更复杂的内部表征；（2）每类有非常少的实例可以学习；（3）不适合点式分类。 因此，本文在受控和非受控环境下进行了人脸验证的实验，共同解决这些问题。在此过程中，我们引入了textit{Siamese Capsule Networks}，这是一个可用于成对学习任务的新变体。我们发现，该模型在少量的学习环境中比基线有所提高，这表明胶囊网络在给定少量样本时能有效学习鉴别性的表征。 我们发现，当使用对比损失和$ell_2$归一化的胶囊编码姿势特征进行训练时，在两个成对学习数据集上，`textit{Siamese Capsule Networks}都有很好的表现，在测试集中的图像对包含未见过的对象的情况下，产生最好的结果。
在这项工作中，我们提出了一个新的代理架构，称为Reactor，它结合了多种算法和架构的贡献，产生了一个比Prioritized Dueling DQN（Wang等人，2016）和Categorical DQN（Bellemare等人，2017）具有更高的样本效率的代理，同时比A3C（Mnih等人。我们的第一个贡献是一种新的政策评估算法，称为分布式回溯，它将多步骤的非政策更新带到分布式强化学习环境中。同样的方法可以用来将几类为期望值评估设计的多步骤政策评估算法转换成分布式的。接下来，我们介绍了β-leave-out政策梯度算法，该算法通过使用行动值作为基线来改善方差和偏差的权衡。 我们最后的算法贡献是一种新的序列优先重放算法，该算法利用相邻观察的时间定位来实现更有效的重放优先级。使用Atari 2600基准，我们表明这些创新都有助于提高采样效率和最终的代理性能。最后，我们证明Reactor在2亿帧和不到一天的训练后达到最先进的性能。
分层计划，特别是分层任务网络，被提出来作为一种方法，通过将任务分解为子任务来描述计划，直到获得原始任务，即行动。计划验证假设一个完整的计划作为输入，目标是找到分解为这个计划的任务。在计划识别中，给定计划的前缀，目标是找到分解为给定前缀的（最短）计划的任务。本文描述了如何使用形式化语法中已知的常见方法，通过解析验证和识别计划。
神经结构搜索（NAS），即自动寻找神经结构的任务，最近作为一种有希望的方法出现，以揭开比人类设计的更好的模型。然而，大多数成功案例都是针对视觉任务的，对于文本来说相当有限，除了一个小的语言建模设置。在本文中，我们通过首先关注语言翻译的任务，后来扩展到阅读理解，来探索文本序列的规模的NAS。 从一个标准的序列到序列的翻译模型，我们在两个翻译任务，IWSLT英语-越南语和WMT德语-英语中对递归单元和注意力相似性函数进行了广泛的搜索。我们报告了执行单元搜索的挑战，并展示了注意力搜索的初步成功，翻译效果比强大的基线好。
自动编码器为学习压缩表征提供了一个强大的框架，通过对潜伏代码中重建数据点所需的所有信息进行编码。在某些情况下，自动编码器可以 "插值"。通过对两个数据点的潜伏代码的凸组合进行解码，自动编码器可以产生一个输出，该输出在语义上混合了数据点的特征。在本文中，我们提出了一个正则化程序，该程序鼓励内插的输出通过愚弄已经被训练为从内插数据中恢复混合系数的批评者网络而显得更加真实。 然后，我们开发了一个简单的基准任务，在这个任务中，我们可以定量地测量各种自动编码器的插值程度，并表明我们的正则程序在这种情况下极大地改善了插值。我们还根据经验证明，我们的正则程序产生的潜伏代码在下游任务中更有效，这表明插值能力和学习有用表征之间可能存在联系。
我们考虑的问题是，当我们只得到一个开始帧和一个结束帧时，生成可信的和多样化的视频序列。这项任务也被称为inbetweening，它属于更广泛的随机视频生成领域，这通常是通过循环神经网络（RNN）来实现的。在本文中，我们提出了一个完全卷积模型，直接在像素域生成视频序列。 我们的模型通过逐步提高时间分辨率来学习产生这种潜在的表示，然后使用三维卷积在时空域进行解码。该模型通过最小化对抗性损失进行端到端的训练。在几个广泛使用的基准数据集上的实验表明，根据定量和定性的评价，它能够产生有意义和多样化的视频序列。
对来自不同来源或语言的知识图谱进行对齐，旨在对齐实体和关系，这对于知识图谱构建和问题回答等多种应用至关重要。我们提出了一个基于对抗性训练的无监督框架，它能够将源知识图谱中的实体和关系映射到目标知识图谱中的实体和关系。这个框架可以进一步与现有的监督方法无缝集成，其中只利用了有限数量的对齐的三联体作为指导。
当同一数据有不同的观点时，多视图学习可以提供自我监督。分布式假设从相邻的句子中提供了另一种形式的有用的自我监督，这些句子在大型无标签语料库中非常多。在人脑两个半球的不对称性以及不同的学习架构倾向于强调句子意义的不同方面的观察的激励下，我们提出了两个多视图框架，以无监督的方式学习句子表征。 在这两个框架中，最终的表征是两个视图的组合，其中一个视图用循环神经网络（RNN）对输入的句子进行编码，另一个视图用简单的线性模型进行编码。
有无数种分割方式，最终一个给定场景的 "正确 "分割在注释者的眼中。标准方法需要大量的标记数据来学习一种特定的分割方式。 作为减轻这种注释负担的第一步，我们提出了引导性分割的问题：给定不同数量的像素级标签，通过在本地（图像内）和非本地（跨图像）传播监督来分割未注释的像素。我们提出了引导性网络，它从不同数量和类别（类别、实例等）的像素监督中提取潜在的任务表示--引导。 我们提出了引导式网络，从可变数量和类别（实例等）的像素监督中提取潜在的任务表示---引导，并通过元学习对我们的架构进行端到端优化，以实现快速、准确和数据高效的分割。为了跨越少数照片和许多照片的学习制度，我们研究了从每个概念只有一个像素到多达1000多个图像的引导，并与两个极端的完全梯度优化进行比较。 为了探索通用性，我们将指导作为不同级别的监督之间的桥梁来分析，将类作为实例的联合体来分割。我们的分割器将不同类型的类的不同数量的监督集中到一个有效的潜在表示中，非局部地在图像中传播这种监督，并在给予更多的监督时可以快速和累积地更新。
训练生成式对抗网络需要平衡微妙的对抗动态。即使仔细调整，训练也可能会发散或最终进入一个糟糕的平衡状态，出现放弃的模式。在这项工作中，我们引入了一种由CS-GAN启发的新形式的潜伏优化，并表明它通过加强判别器和生成器之间的相互作用改善对抗动态。 我们的实验表明，潜在的优化可以大大改善GAN的训练，在ImageNet（128 x 128）数据集上获得最先进的性能。我们的模型实现了148的Inception Score（IS）和3.4的Frechet Inception Distance（FID），与相同架构和参数数量的BigGAN-deep模型相比，IS和FID分别提高了17%和32%。
在本文中，我们研究了最适合训练数据集的两层人工神经网络的优化问题。我们在参数数量大于采样点数量的情况下研究了这个问题。我们表明，对于一类广泛的可微分激活函数（该类涉及大多数非线性函数，不包括片状线性函数），只要隐藏层是非星形的，我们就有任意的一阶最优解满足全局最优性。 我们基本上表明，这些非星形隐藏层矩阵对这些大类激活函数满足 "好 "的属性。证明这一结果所涉及的技术启发我们研究一种新的算法，在隐藏层的两个梯度步骤之间，我们增加一个输出层的随机梯度下降（SGD）步骤。 在这个新的算法框架中，我们扩展了先前的结果，并表明对于所有有限的迭代，隐藏层满足先前提到的 "好 "的属性，因此部分解释了噪声梯度方法的成功，并解决了我们先前结果的数据独立性问题。 即使网络有一个以上的隐藏层，只要所有的内部隐藏层是任意的，满足非单调性，所有的激活都来自于给定的可微函数类，并且只对最外层的隐藏层进行优化，结果也是适用的。我们利用平滑特性保证了$O(1/text{number of iterations})$的渐进收敛性，达到一阶最优解。
我们在ConvNet架构中引入了通道聚合的概念，这是一种新型的CNN特征的紧凑表示，有助于明确地对非线性通道编码进行建模，特别是当新单元被嵌入到用于动作识别的深度架构中时。通道聚合是基于ConvNet的多通道特征，旨在以快速的速度找到光学收敛路径。 我们从理论上提出了通道聚合函数，并实证研究了它们对收敛速度和分类精度的影响。这项工作的另一个贡献是对NCAL的高效实现，使其速度提高了几个数量级。我们在标准基准UCF101和HMDB51上评估了其性能，实验结果表明，这种提法不仅能获得快速收敛，而且在不牺牲性能的情况下获得了更强的泛化能力。
我们提出了一种新的黑盒对抗性攻击方法。与之前通过使用梯度或初始化代用白盒模型来结合基于转移和基于评分的方法不同，这种新方法试图使用预训练的模型来学习低维嵌入，然后在嵌入空间内进行有效搜索，以攻击未知目标网络。 我们表明，这种方法可以极大地提高不同目标网络架构的黑盒对抗性攻击的查询效率。我们在MNIST、ImageNet和Google Cloud Vision API上评估了我们的方法，结果是查询次数大大减少。我们还在CIFAR10和ImageNet上攻击了对抗性防御网络，我们的方法不仅减少了查询次数，还提高了攻击成功率。
深度神经网络（DNNs）的灵感来自于人脑，两者之间的相互联系在文献中被广泛研究。 然而，DNNs是否能够像大脑一样做出决策仍然是一个开放的问题。以前的工作表明，通过匹配猴子大脑下颞叶（IT）皮层的神经反应来训练DNNs，能够在图像物体识别任务上达到人类水平。这表明神经动态可以提供信息知识，帮助DNNs完成特定任务。 在本文中，我们介绍了神经-人工智能界面的概念，其目的是利用人类的神经反应作为监督信息，帮助人工智能系统解决使用传统机器学习策略时难以完成的任务。为了传递神经-人工智能界面的理念，我们专注于将其部署到生成对抗网络（GANs）的基本问题之一：设计一个适当的评估指标来评估GANs生成的图像的质量。  
虽然最近自动驾驶汽车（AV）技术的发展突出了实质性的进展，但我们缺乏严格和可扩展的测试工具。现实世界的测试，事实上的评估环境，将公众置于危险之中，并且，由于事故的罕见性，将需要数十亿英里，以统计学的方式验证性能声明。 我们实现了一个模拟框架，可以测试整个现代自动驾驶系统，特别是包括采用深度学习感知和控制算法的系统。使用自适应抽样方法来加速罕见事件的概率评估，我们估计在管理标准交通行为的基础分布下发生事故的概率。
自然语言理解中的许多任务需要学习两个序列之间的关系，以完成各种任务，如自然语言推理、转述和包含等。上述这些任务在性质上是相似的，但它们往往是单独建模的。知识转移对密切相关的任务来说是有效的，这通常是在神经网络中使用参数转移来进行的。然而，转移所有的参数，其中一些与目标任务无关，可能会导致次优的结果，并可能对性能产生负面影响，被称为/textit{negative}转移。因此，本文通过提出一种基于集合的转移学习方法来关注实例和参数在自然语言理解任务中的可转移性。我们的主要贡献是在使用神经网络时减轻跨任务的负面转移，这涉及到动态地将在源任务的不同子集上训练的小型循环神经网络装袋。 我们提出了一种简单而新颖的方法，通过使用根据训练期间子间隔的平滑样条误差曲线的斜率变化选择的衰减参数，将这些网络纳入到目标任务中进行少量学习。
量化和预测疾病进展的能力是选择适当治疗方法的基础。许多临床指标不能经常获得，因为它们的成本（如MRI，步态分析）或因为它们不方便或对病人有害（如活检，X射线）。在这种情况下，为了估计疾病进展的个人轨迹，利用病人之间的相似性，即轨迹的协方差，并找到进展的潜在代表是有利的。 在这项研究中，我们开发了一个名为Coordinatewise-Soft-Impute（CSI）的机器学习框架，用于在存在混杂事件的情况下从稀疏的观察中分析疾病的进展。
多语言神经机器翻译（NMT）系统能够在一个系统内进行多种源语言和目标语言之间的翻译。这些系统内通用性的一个重要指标是零点翻译的质量--在系统训练期间从未见过的语言对之间进行翻译。然而，直到现在，多语言模型的零点翻译性能已经远远落后于使用两步翻译过程，通过中间语言（通常是英语）来实现的质量。 在这项工作中，我们诊断了为什么多语言模型在零镜头设置中表现不佳。我们提出了明确的语言不变性损失，引导NMT编码器学习语言无关的表征。我们提出的策略大大改善了WMT英法德和IWSLT 2017共享任务中的零镜头翻译性能，并首次在监督方向上保持性能的同时，与透视方法的性能匹配。
我们证明了在有限的深度和宽度下，随机初始化的ReLU网络中神经切线核（NTK）的平均值和方差的精确比例。标准偏差在网络深度和宽度的比率中是指数级的。因此，即使在无限超参数化的限制下，如果深度和宽度同时趋于无穷大，NTK也不是确定性的。 此外，我们证明，对于这样的深广网络，NTK在训练期间有一个非微妙的演变，表明其第一次SGD更新的平均值也是网络深度和宽度的比例的指数。
大多数在关系数据中进行表示学习和链接预测的算法都是为静态数据设计的。然而，它们所应用的数据通常是随时间变化的，比如社交网络中的朋友图或推荐系统中用户与项目的交互。对于知识库来说也是如此，它包含的事实如（美国，有总统奥巴马，[2009-2017]）只在某些时间点有效。对于在时间限制下的链接预测问题，即回答形式为（美国，有总统，？，2012）的查询，我们提出了一个解决方案，其灵感来自于4阶张量的典型分解。
传统的解决推荐问题的方法是贪婪地按预测分数对单个文件候选者进行排名。然而，这种方法未能将板块作为一个整体进行优化，因此，往往难以捕捉到由页面布局和文件相互渗透造成的偏差。板块推荐问题的目的是直接找到最符合用户利益的最佳排序的文件子集（即板块）。由于文件候选者及其在页面上的显示位置的组合爆炸，解决这个问题很难。 因此，我们提出了一个范式的转变，从解决排名问题的传统观点转变为直接生成板块的框架。在本文中，我们引入了列表条件变异自动编码器（ListCVAE），它根据用户的反应学习板块上文档的联合分布，并直接生成完整的板块。在模拟和真实世界的数据上的实验表明，列表CVAE在各种规模的文档体上持续优于贪婪的排名方法。
近年来，对图等结构化数据的神经网络进行了广泛的研究。迄今为止，大部分研究活动主要集中在静态图上。然而，大多数现实世界的网络是动态的，因为它们的拓扑结构往往会随着时间的推移而改变。预测动态图的演变是图挖掘领域的一项重要任务。尽管它具有实际的重要性，但到目前为止，这项任务还没有被深入探讨，主要是因为它具有挑战性。 具体来说，我们使用一个图神经网络和一个递归架构来捕捉动态图的时间演化模式。然后，我们采用一个生成模型来预测下一个时间步骤的图的拓扑结构，并构建一个与该拓扑结构相对应的图实例。我们在几个遵循常见网络演化动态的人工数据集以及真实世界的数据集上评估了所提出的模型。
至于基于知识的问题回答，一个基本的问题是将可回答的问题的假设从简单问题放宽到复合问题。传统的方法首先检测问题中提到的主题实体，然后遍历知识图谱，找到关系作为通往答案的多跳路径，而我们提出了一种新的方法，利用简单问题回答者来回答复合问题。 我们的模型由两部分组成：(i)一个新颖的学习分解代理，学习将复合问题分解成简单问题的策略；(ii)三个独立的简单问题回答者，对每个简单问题的相应关系进行分类。实验证明，我们的模型学习复杂的组成规则作为随机策略，这有利于简单神经网络在WebQuestions和MetaQA上实现最先进的结果。我们分析了可解释的分解过程以及生成的分区。
基于能量的模型输出未规范化的对数概率值，给定数据样本。 这样的估计在各种应用问题中是必不可少的，如样本生成、去噪、样本修复、离群点检测、贝叶斯推理等等。 然而，由于需要对模型分布进行采样，标准的最大似然训练在计算上非常昂贵。分数匹配有可能缓解这一问题，去噪分数匹配（Vincent, 2011）是一个特别方便的版本。 然而，以前的尝试未能产生能够进行高质量样本合成的模型。 我们认为这是因为他们只在单一噪声尺度上进行去噪评分匹配。为了克服这一局限性，我们在这里使用所有的噪声尺度学习能量函数。  当使用Annealed Langevin动力学和单步去噪跳跃进行采样时，我们的模型产生了高质量的样本，可与最先进的技术如GANs相媲美，此外，对测试数据的似然分配也与以前的似然模型相媲美。 我们的模型在基于似然的模型中设定了一个新的样本质量基线。 我们进一步证明，我们的模型能够学习样本分布，并在图像绘制任务中具有良好的概括性。
限制性波尔兹曼机（RBM）在其输入样本上学习一个概率分布，并有许多用途，如降维、分类和生成模型。传统的RBM接受矢量数据，忽略了原始张量（多向）输入中潜在的重要结构信息。矩阵-变量和张量-变量RBM，名为MvRBM和TvRBM，已被提出，但在结构上都有限制。 这项工作提出了矩阵乘积算子RBM（MPORBM），它利用了Mv/TvRBM的张量网络泛化，保留了可见层和隐藏层的输入格式，并带来了更高的表达能力。还开发了一种整合了对比发散和交替优化过程的新型训练算法。
自主驾驶仍然被认为是一个 "未解决的问题"，因为它本身具有重要的可变性，而且与它的发展相关的许多过程，如车辆控制和场景识别仍然是开放的问题。尽管强化学习算法已经在游戏和一些机器人操作中取得了显著的成果，但这种技术还没有被广泛地扩展到像自主驾驶这样更具挑战性的现实世界应用。 在这项工作中，我们提出了一种深度强化学习（RL）算法，该算法嵌入了一个具有多步骤回报的演员评论家架构，以实现代理人在复杂和不稳定环境中行动时学习策略的更好的鲁棒性。实验在卡拉模拟器上进行，该模拟器提供了一个可定制的和现实的城市驾驶条件。
在生成对抗网络（GANs）的背景下，一个基本的、在很大程度上尚未回答的问题是GANs是否真的能够捕获它们所训练的数据集的关键特征。目前研究这个问题的方法需要大量的人类监督，例如对采样图像的视觉检查，并且通常只提供相当有限的可扩展性。 这些技术只需要最低限度的人工监督，并且可以很容易地进行扩展和调整，以评估各种最先进的GAN在大型流行数据集上的情况。他们还表明，GAN在再现训练数据集的更多分布特性方面存在重大问题。特别是，这种合成数据的多样性比原始数据的多样性小几个数量级。
生存聚类的目的是为了映射主体（例如。在本文中，我们放弃了这一假设，并引入了一个损失函数，该函数使用改进的Kuiper统计学来区分集群的经验寿命分布。 我们通过优化这个损失来学习一个深度神经网络，将用户执行软聚类到生存组中。我们将我们的方法应用于一个有超过100万个主题的社交网络数据集，并显示与其他方法相比，C-index有明显的改善。
贝叶斯优化（BO）是一种流行的方法，用于调整昂贵的黑箱函数的超参数。尽管它很成功，但标准的BO一次只关注一个任务，并不是为了利用相关函数的信息，如在多个数据集上调整同一算法的性能指标。 在这项工作中，我们引入了一种新的方法来实现跨不同数据集以及不同指标的转移学习。主要的想法是用一个半参数的高斯Copula分布来回归从超参数到指标量值的映射，这对不同任务中可能出现的不同尺度或异常值提供了稳健性。 我们引入了两种方法来利用这种估计：汤普森抽样策略以及使用这种量化估计作为先验的高斯Copula过程。我们表明，这些策略可以结合多种指标的估计，如运行时间和准确性，引导优化到更便宜的超参数，以达到相同的准确度。
我们提出了没有路由程序的纯胶囊网（P-CapsNets）。具体来说，我们对胶囊网做了三个修改。 首先，基于耦合系数可以隐式学习的观察，我们从CapsNets中删除了路由程序。第二，我们替换了CapsNets中的卷积层以提高效率。第三，我们将胶囊打包成3级张量以进一步提高效率。 实验表明，在MNIST&CIFAR10上，P-CapsNets通过使用明显较少的参数，实现了比具有不同常规程序的CapsNets更好的性能。P-CapsNets的高效率甚至可以与一些深度压缩模型相媲美。例如，我们在MNIST上只使用了3888个参数就达到了99%以上的准确率。 我们将胶囊以及相应的相关矩阵可视化，以显示未来初始化CapsNets的可能方式。我们还探索了与CNN相比，P-CapsNets的对抗性稳健性。
	最近的一项工作是从{平均场理论}的角度对神经网络的统计特性进行了研究，取得了巨大的成功，对神经网络的行为和测试时间的性能进行了非常精确的预测并加以验证。	在本文中，我们在这些工作的基础上，探索了两种驯服随机残差网络行为的方法（只有全连接层，没有批处理规范）。	第一种方法是{/it width variation (WV)}，即改变层的宽度作为深度的一个函数。	我们表明，宽度衰减可以减少梯度爆炸而不影响随机网络的平均前向动态。	第二种方法是{/it variance (VV)}，即改变权重的初始化方差和深度的偏置。	我们表明，适当使用VV，可以将tanh和ReLU网络的梯度爆炸从$exp(\Theta(sqrt L))$和$exp(\Theta(L))$分别减少到常数$Theta(1)$。	对于方差衰减如何影响不同的动力学，例如梯度和激活规范的动力学，我们得出了一个完整的相位图。	特别是，我们显示了许多相变的存在，这些动力学在指数、多项式、对数甚至常数行为之间切换。	利用获得的均值场理论，我们能够出人意料地跟踪初始化时间的VV如何在设定的历时数之后影响MNIST的训练和测试时间性能：测试/训练集准确度的水平集与某些梯度规范或度量表现力（如在 \cite{yang_meanfield_2017}中的定义）的期望水平集相吻合，这是随机神经网络中的扩展措施。	基于过去在深度均值场理论和信息几何学中的见解，我们还提供了一个关于梯度爆炸/消失问题的新视角：它们会导致费雪信息矩阵的条件不良，造成优化麻烦。
我们探索了协作式多代理环境，其中一队深度强化学习代理试图在部分可观察的环境中解决一个共同的任务。在这种情况下，学习一个有效的通信协议是关键。我们提出了一个通信协议，允许有针对性的通信，代理学习发送什么信息以及向谁发送这些信息。此外，我们引入了一个多阶段的通信方法，代理在环境中采取行动之前通过几轮通信进行协调。 我们在多个多Agent合作任务中评估了我们的方法，这些任务的难度不同，Agent的数量也不同，环境范围从2D形状的网格布局和模拟交通路口到复杂的3D室内环境。我们证明了有针对性的以及多阶段的通信的好处。
蚀刻拿铁艺术的初学者很难通过使用两种不同粘度的液体（如发泡牛奶和糖浆）来制作平衡的图案。即使在制作蚀刻拿铁艺术的过程中观看制作视频，也很难保持平衡。在本文中，我们提出了一个系统，通过将蚀刻拿铁艺术的制作过程直接投射到卡布奇诺上，支持初学者制作均衡的蚀刻拿铁艺术。实验结果显示了使用我们的系统所取得的进展。 我们还讨论了通过使用背景减去法，蚀刻拿铁艺术和设计模板的相似性。
我们专注于基于GAN的视频生成任务的时间自我监督。虽然对抗性训练成功地产生了各种领域的生成模型，但对生成数据中的时间关系的探索要少得多。这对于连续的生成任务至关重要，例如视频超分辨率和非配对视频翻译。 对于前者，最先进的方法往往倾向于更简单的规范损失，如L2而不是对抗性训练。然而，它们的平均性很容易导致时间上的平滑结果，而不希望缺乏空间细节。对于非配对视频翻译，现有的方法修改生成器网络以形成空间-时间周期的一致性。 相比之下，我们专注于改善学习目标，并提出了一种时间上的自我监督算法。对于这两项任务，我们表明，时间上的对抗性学习是实现时间上的一致性解决方案的关键，而不会牺牲空间细节。我们还提出了一种新颖的Ping-Pong损失来改善长期的时间一致性。 它有效地防止了递归网络在时间上积累的假象，而不会压制详细的特征。我们还提出了第一套指标来定量评估准确性以及时间演化的感知质量。一系列的用户研究证实了用这些指标计算的排名。
标记的训练数据的稀缺性常常禁止NLP模型的国际化，使其不能用于多种语言。 跨语言理解利用语言通用表示法在这一领域取得了进展。然而，目前的大多数方法都把问题集中在语言的统一上，而没有解决跨语言和文化的自然领域漂移问题。 在本文中，我们在半监督的跨语言文档分类的设置中解决了这个领域的差距，其中标记的数据在源语言中可用，而目标语言中只有未标记的数据可用。 我们将最先进的无监督学习方法--屏蔽式语言建模预训练，与最近的半监督学习方法--无监督数据增强（UDA）相结合，以同时缩小语言和领域的差距。 我们表明，解决跨语言任务中的领域差距是至关重要的。 我们超越了强大的基线，在跨语言文档分类方面取得了新的进展。
HMMs和RNNs之间的一个明显的共同点是，它们都为连续的数据学习隐藏的表征。此外，有人指出，HMMs的Baum-Welch算法的反向计算是用于神经网络的反向传播算法的一个特殊情况（Eisner（2016））。 这些观察结果表明，尽管有许多明显的差异，但HMMs是RNNs的一个特例吗？  在本文中，我们通过理论推导和经验杂交，研究了HMM和RNN之间的一系列架构转换，以回答这个问题。特别是，我们研究了三个关键的设计因素--隐藏状态和观察之间的独立性假设、softmax的放置以及非线性的使用，以确定其经验效果。 我们提出了一个全面的实证研究，以提供关于语言建模和语篇归纳方面的表达性和可解释性之间的相互作用的见解。
我们提出了一个信息理论框架，用于理解使用变异推理的深度潜变量模型的无监督学习中的权衡。这个框架强调需要沿着两个维度考虑潜变量模型：重建输入的能力（失真）和通信成本（速率）。我们推导出生成模型在二维速率-失真平面中的最佳边界，并表明标准的证据下限目标如何不足以在这个边界的点之间进行选择。 然而，通过进行有针对性的优化来学习具有不同速率的生成模型，我们能够学习许多模型，这些模型能够实现类似的生成性能，但在潜变量的使用方面做出了巨大的权衡。通过对MNIST和Omniglot的各种架构的实验，我们展示了我们的框架如何揭示了最近提出的对变异自动编码器系列的扩展。
图形神经网络（GNNs）是一个有效的图形表示学习框架。GNNs遵循邻域聚合方案，其中节点的表示向量是通过递归聚合和转换其邻域节点的表示向量计算的。 在这里，我们提出了一个理论框架，用于分析GNNs捕捉不同图形结构的表达能力。我们的结果描述了流行的GNN变体的辨别能力，如图形卷积网络和GraphSAGE，并表明它们不能学习区分某些简单的图形结构。 然后，我们开发了一个简单的架构，该架构被证明是GNN类中最具表现力的，并且与Weisfeiler-Lehman图同构测试一样强大。我们在一些图分类基准上实证验证了我们的理论发现，并证明我们的模型达到了最先进的性能。
我们介绍了MTLAB，一种具有强大理论保证的学习多个相关任务的新算法。它的关键思想是在所有任务的数据上连续进行学习，在任务边界没有中断或重启。单个任务的预测器是通过一个额外的在线到批量的转换步骤从这个过程中得到的。 在终身学习的设置中，这导致了一个改进的泛化约束，它收敛于所有观察到的任务的总样本数，而不是每个任务的例子数或独立的任务数。同时，它是广泛适用的：它可以处理有限的任务集，如多任务学习中常见的，以及随机的任务序列，如终身学习中研究的。
最近的工作展示了多语言BERT（M-BERT）令人惊讶的跨语言能力--令人惊讶的是，它是在没有任何跨语言目标和没有对齐数据的情况下训练的。在这项工作中，我们全面研究了M-BERT中不同组件对其跨语言能力的贡献。 实验研究是在三种类型不同的语言--西班牙语、印地语和俄语--的背景下进行的，并使用了两个概念不同的NLP任务，即文本连带关系和命名实体识别。我们的主要结论是，语言之间的词汇重叠在跨语言成功中起着可忽略的作用，而网络的深度则是其重要部分。
我们分析了训练深度ReLU网络的动态及其对泛化能力的影响。利用师生设置，我们发现了深度ReLU网络中隐藏的学生节点收到的梯度和教师节点的激活之间的新关系。 有了这种关系和教师节点激活小重叠的假设，我们证明：（1）权重被初始化为接近教师节点的学生节点以更快的速度向教师节点收敛；（2）在过度参数化的制度和2层的情况下，虽然一小部分幸运的节点确实向教师节点收敛，但其他节点的扇出权重收敛为零。 这个框架为深度学习中多种令人困惑的现象提供了见解，如过度参数化、隐性正则化、彩票等。我们通过显示预训练的VGG11/16模型的大多数BatchNorm偏差为负值来验证我们的假设。在（1）高斯输入的随机深度教师网络、（2）在CIFAR-10上预训练的教师网络和（3）广泛的消融研究上的实验验证了我们多种理论预测。
最先进的学习跨语言词嵌入的方法依赖于双语词典或平行语料库。最近的研究表明，平行数据监督的需要可以通过字符级信息来缓解。虽然这些方法显示了令人鼓舞的结果，但它们不能与有监督的同类方法相提并论，而且仅限于共享共同字母的语言对。在这项工作中，我们表明，我们可以在不使用任何平行语料库的情况下，通过以无监督的方式对齐单语词嵌入空间，建立两个语言之间的双语词典。 在不使用任何字符信息的情况下，我们的模型在某些语言对的跨语言任务上甚至优于现有的监督方法。我们的实验表明，我们的方法对于遥远的语言对，如英俄或英汉，也有很好的效果。最后，我们描述了在英语-埃斯佩兰托低资源语言对上的实验，在该语言对上只存在有限的平行数据，以显示我们的方法在完全无监督机器翻译中的潜在影响。
需要计算图像中各种物体的问题仍然是视觉问题回答（VQA）的主要挑战。VQA最常见的方法包括根据图像和问题的固定长度表示对答案进行分类，或者对图像的每个部分估计的分数计数进行求和。 具体来说，该模型从检测到的物体中依次选择，并学习影响后续选择的物体之间的相互作用。我们的方法的一个区别是其直观和可解释的输出，因为离散的计数是在图像中自动建立的。此外，我们的方法在评估计数的多个指标上优于VQA的技术架构现状。
图是对许多重要的现实世界数据进行建模所需的基本数据结构，从知识图、物理和社会互动到分子和蛋白质。在本文中，我们研究了从感兴趣的图的数据集中学习图的生成模型的问题。在学习之后，这些模型可以用来生成与数据集中的样本具有相似的属性。 然而，学习图的生成模型的任务有其独特的挑战。特别是，在生成过程中如何处理图的对称性和其元素的排序是重要的问题。我们提出一个通用的基于图神经网的模型，能够生成任何任意的图。 我们研究了它在一些图形生成任务上的性能，与利用领域知识的基线进行比较。 我们讨论了这种生成模型的潜在问题和未来的开放问题。
我们引入了一个神经网络，它通过根据诱导的二进制解析树对单词进行组合来表示句子。我们使用Tree-LSTM作为我们的组合函数，沿着一个完全可分的自然语言图表解析器发现的树结构应用。我们的模型同时优化了组合函数和解析器，从而消除了对外部提供的解析树的需求，这通常是Tree-LSTM需要的。 由于它是完全可分的，我们的模型很容易用现成的梯度下降法和反向传播法进行训练。我们证明，与各种有监督的Tree-LSTM架构相比，它在文本包含任务和反向字典任务上取得了更好的性能。最后，我们展示了如何用一种注意力机制来提高性能，该机制通过关注句子的所有可能的子跨度，充分利用了分析图。
我们提出了三种技术并进行了实验测试：基于距离的正则化、嵌套等级修剪和逐层双字节匹配。前两种算法分别用于训练和修剪阶段，第三种算法用于排列神经元阶段。 实验表明，无论是否有逐层双字节匹配，基于距离的正则化和基于权重的修剪往往表现最好。这些结果表明，这些技术在创建神经网络以实现广泛部署的专用电路中可能是有用的。
给定一个视频和一个句子，弱监督视频时刻检索的目标是找到句子所描述的视频片段，而在训练过程中无法获得时间注释。 相反，一个模型必须学习如何在只提供视频-句子对的情况下识别正确的片段（即时刻）。 因此，一个固有的挑战是自动推断视觉和语言表征之间的潜在对应关系。为了促进这种对齐，我们提出了弱监督时刻对齐网络（wMAN），它利用多层次的共同关注机制来学习更丰富的多模态表征。上述机制由一个帧-字互动模块以及一个新颖的字-条件视觉图（WCVG）组成。 我们的方法还结合了变形金刚中常用的位置编码的新颖应用，通过迭代信息传递来学习视觉语义表征，这些表征包含了它们在时间序列中的相对位置的上下文信息。在DiDeMo和Charades-STA数据集上的综合实验证明了我们学习表征的有效性：我们的组合wMAN模型不仅以显著的优势超越了最先进的弱监督方法，而且在某些指标上也比强监督的最先进方法做得好。
在机器学习任务中，当目标领域的样本数量不足时，经常会出现过度的情况，因为在这种情况下分类器的概括能力很差。为了解决这个问题，转移学习利用类似领域的知识来提高学习者的鲁棒性。 现有的迁移学习算法的主要思想是通过样本选择或领域适应来减少领域之间的差异。然而，无论我们使用何种迁移学习算法，差异总是存在的，源数据和目标数据的混合训练会导致学习者在目标领域的识别能力下降。 为了解决这个问题，我们提出了一个基于集合学习的两阶段转移学习架构，在第一阶段使用现有的转移学习算法来训练弱学习者，在第二阶段使用目标数据的预测来训练最终的学习者。 在这种架构下，可以同时保证筛选能力和泛化能力。我们在公共数据集上评估了所提出的方法，这证明了我们所提出的方法的有效性和稳健性。
深度学习在许多任务上取得了惊人的成果，有大量的数据和训练数据附近的泛化。对于许多重要的现实世界的应用，这些要求是不可行的，需要额外的任务领域的先验知识来克服由此产生的问题。特别是，为基于模型的控制学习物理模型需要从较少的样本（通常是实时在线收集的）进行鲁棒性推断，模型错误可能导致系统的急剧损坏。 直接纳入物理洞察力使我们能够获得一种新的深度模型学习方法，在需要较少样本的情况下进行良好的推断。作为第一个例子，我们提出了深度拉格朗日网络（DeLaN）作为一种深度网络结构，在其上施加了拉格朗日力学。该方法不仅在学习速度上优于以前的模型学习方法，而且在新的轨迹上表现出大幅改进和更强大的推断能力，并能实时在线学习。
许多具有挑战性的预测问题，从分子优化到程序合成，都涉及到创建复杂的结构化对象作为输出。然而，可用的训练数据可能不足以让生成式模型学习所有可能的复杂转换。通过利用评估比生成更容易的想法，我们展示了一个简单的、广泛适用的、迭代的目标增强方案在指导这种模型的训练和使用方面是如何令人惊讶地有效。 我们的方案将生成模型视为先验分布，并采用单独训练的过滤器作为似然。在每个增强步骤中，我们对模型的输出进行过滤，以获得下一个训练周期的额外预测目标。我们的方法适用于监督和半监督环境。
玻尔兹曼分布是许多系统的自然模型，从大脑到材料和生物大分子，但由于蒙特卡洛算法无法在可用的时间内对其进行模拟，因此对数据的拟合作用往往是有限的。蛋白质折叠问题就体现了基于能量的模型的表达能力和采样实用性之间的差距，因为能量景观是当代蛋白质生物物理学知识的基础，但计算机模拟却难以从第一原理中折叠除最小的蛋白质以外的所有蛋白质。 在这项工作中，我们旨在通过使用未滚动的蒙特卡洛模拟作为数据模型来弥补能量函数的表达能力和其模拟器的实际能力之间的差距。我们将神经能量函数与基于朗格文动力学的新型高效模拟器组成，以建立一个给定氨基酸序列信息的端到端差异化的原子蛋白质结构模型。 我们介绍了在长时间滚动下稳定反向传播的技术，并展示了该模型进行多模式预测的能力，以及在某些情况下，在对大量蛋白质结构进行训练时对未观察到的蛋白质折叠类型的概括。
在了解个体动物如何学习方面取得的进展需要高通量的标准化行为训练方法和适应训练的方法。在数百或数千次试验的训练过程中，动物可能会突然改变其基本策略，而捕捉这些变化需要实时推断动物的潜在决策策略。 为了应对这一挑战，我们开发了一个用于自动化动物训练的综合平台，以及一个迭代决策推理模型，该模型能够推断出瞬间的决策策略，并预测动物在每次试验中的选择，准确率达到80%，即使在动物表现不佳的情况下也是如此。我们还将单次试验分辨率的决策预测与自动姿势估计相结合，评估运动轨迹。
循环神经网络（RNNs）是对连续数据进行建模的强大工具。尽管它们被广泛使用，但对RNNs如何解决复杂问题的理解仍然难以捉摸。 在这里，我们描述了流行的RNN架构如何进行文档级的情感分类。尽管它们在理论上有能力实现复杂的高维计算，但我们发现训练有素的网络会收敛到高度可解释的低维表示。 我们确定了一个简单的机制，即沿近似线吸引器的整合，并发现这一机制存在于RNN架构中（包括LSTMs、GRU和vanilla RNNs）。总之，这些结果表明，在一系列的递归网络中可以出现令人惊讶的通用和人类可解释的计算。
神经科学和深度学习的平行发展导致了相互之间富有成效的交流，推动了我们对感官和认知系统中真实和人工神经网络的理解。在这项工作中，我们开发了一个虚拟啮齿动物，作为对体现控制的人工模型中的运动活动进行基础研究的平台。然后我们利用这个平台，通过训练一个模型来解决四个复杂任务，研究不同背景下的运动活动。 使用神经科学家熟悉的方法，我们描述了网络不同层采用的行为表征和算法，使用神经伦理学的方法来描述相对于啮齿动物行为和目标的运动活动。我们发现，该模型使用两类表征，分别编码特定任务的行为策略和任务不变的行为运动学。这些表征反映在神经亚群的顺序活动和群体动力学上。
我们提出了最优传输GAN（OT-GAN），这是生成式对抗网的一个变种，它最小化了衡量生成器分布和数据分布之间距离的新指标。这个指标，我们称之为小批量能量距离，它将原始形式的最优传输与定义在对抗性学习特征空间的能量距离相结合，产生了一个具有无偏小批量梯度的高度鉴别性的距离函数。实验表明，当用大型小批量训练时，OT-GAN高度稳定，我们在图像生成的几个流行基准问题上提出了最先进的结果。
我们建立了一个在二维迷宫般的世界中学习语言的虚拟代理。该代理看到周围环境的图像，听一个虚拟老师讲课，并通过行动来获得奖励。 它同时学习世界的视觉表征、语言和动作控制。通过将语言基础与其他计算程序分开，并在语言基础和预测之间共享一个概念检测功能，该代理可靠地进行内插和外推，以解释包含新词组合或训练句子中缺少的新词的句子。 新词是从语言预测的答案中转移出来的。这样的语言能力是在超过160万个不同的句子群中训练和评估的，这些句子包括119个物体词、8个颜色词、9个空间关系词和50个语法词。所提出的模型在解释零星句子方面明显优于五种比较方法。此外，我们在附录中展示了该模型的人类可解释中间输出。
本文提出了$text{W}\text{R}^{2}\text{L}$--一种稳健的强化学习算法，在低维和高维控制任务上具有明显的稳健性能。 我们的方法将稳健强化学习形式化为一种新型的最小-最大博弈，并带有一个正确和收敛的求解器的Wasserstein约束。除此之外，我们还提出了一种高效和可扩展的求解器，遵循一种新型的零阶优化方法，我们相信这对一般的数值优化是有用的。我们通过经验证明，在高维的MuJuCo环境中，与标准和稳健的最先进的算法相比，有明显的收益。
部分可观察的马尔科夫决策过程（POMDPs）是一个自然的模型，用于处理不完整的知识和随机事件。然而，POMDPs的许多相关属性要么是不可判定的，要么在运行时间和内存消耗方面计算非常昂贵。 在我们的工作中，我们开发了一种基于游戏的抽象方法，能够为这些属性的重要子类提供安全的界限和严格的近似。我们讨论了理论意义，并展示了我们的结果在广泛的基准上的适用性。
在本文中，我们接近两个相关的深度学习主题：i）处理图结构的输入数据和ii）更好地理解和分析深度网络和相关的学习算法。考虑到这一点，我们专注于平面图（Mazes）的一个特定子集中的可达性的拓扑分类。 我们还得出了这个解决方案周围的成本函数的形状，值得注意的是，在大迷宫的限制下，它不依赖于迷宫的大小。对这种行为负责的是数据集中的罕见事件，这些事件强烈地调节了这个全局最小值附近的成本函数的形状。我们进一步确定了学习的障碍，其形式是表现不佳的局部最小值，其中网络选择忽略一些输入。
虽然神经网络可以被训练成从一个特定的数据集映射到另一个数据集，但它们通常不会学习一种可以在训练空间之外准确推断的通用转换。例如，一个专门训练的生成对抗网络（GAN）将汽车图像从浅色转化为深色，可能对马的图像没有同样的效果。 这是因为神经网络善于在它们被训练的数据流形内生成。然而，在流形之外生成新的样本或推断 "样本外 "是一个更难的问题，而且研究得不多。 为了解决这个问题，我们引入了一种叫做神经元编辑的技术，学习神经元如何编码潜空间中的特定变换的编辑。我们使用自动编码器将数据集内的变化分解为不同神经元的激活，并通过对这些神经元定义编辑变换来生成变换的数据。 通过在潜伏的训练有素的空间中进行转换，我们对数据进行了相当复杂的非线性转换，对神经元的激活进行了简单得多的分布转移。我们在图像领域/风格转移和两个生物应用上展示了我们的技术：去除代表不需要的噪声的批量伪影和对药物治疗效果进行建模以预测药物间的协同作用。
我们提出了一种在复杂的不确定领域中使用关系规则来描述过渡模型的表述。 对于任何行动，规则选择了一组相关的对象，并计算了这些对象在结果状态下的属性分布，并给出了它们在先前状态下的属性。 一个迭代的贪婪算法被用来构建一组决定性的引用，以确定哪些对象在任何给定的状态下是相关的。  前馈神经网络被用来学习相关对象属性的过渡分布。 在一个模拟领域中，机器人在杂乱的桌子上推送成堆的物体，与学习单一的过渡模型相比，这一策略被证明是更通用和更有效的样本。
微分规划网络架构在解决转移规划任务方面显示出强大的功能，同时拥有简单的端到端训练功能。后来在文献中提出的许多伟大的规划架构都是受到这一设计原理的启发，其中应用递归网络架构来模拟值迭代算法的备份操作。然而，现有的框架工程只能在具有晶格结构的领域中有效地学习和规划，即嵌入一定欧几里得空间的规则图。 在本文中，我们提出了一个通用的规划网络，称为基于图形的运动规划网络（GrMPN），它将能够：i）在一般的不规则图形上学习和规划，因此ii）使现有的规划网络架构成为特例。所提出的GrMPN框架对任务图形的变异是不变的，即图形异构。 我们证明了所提出的GrMPN方法与其他基线在三个领域的性能，包括二维迷宫（规则图）、不规则图的路径规划和运动规划（机器人配置的不规则图）。
我们描述了训练高质量图像去噪模型的技术，这些模型只需要将损坏的图像的单个实例作为训练数据。受最近一种技术的启发，我们解决了它的两个缺点：训练效率低和最终去噪性能差。 这是通过一个新颖的盲点卷积网络架构实现的，该架构允许有效的自我监督训练，以及对输出颜色的贝叶斯分布预测的应用。它们一起使自我监督模型在质量和训练速度方面与完全监督的深度学习技术在i.i.d.高斯噪声的情况下看齐。
强化学习（RL）代理通过试错来改进，但当奖励稀少，代理不能发现成功的行动序列时，学习就会停滞不前。这一直是训练深度RL代理来执行基于网络的任务的一个明显问题，如预订航班或回复电子邮件，其中一个错误可能会破坏整个行动序列。 一个常见的补救措施是通过预先训练代理来模仿专家的演示来 "热启动"，但这很容易造成过度拟合。相反，我们建议使用演示来约束探索。从每个演示中，我们诱导出高层次的 "工作流程"，约束每个时间步骤中允许的行动与演示中的行动相似（例如。"然后，我们的探索策略学会识别成功的工作流程，并对满足这些工作流程的行动进行采样。工作流程剔除了不好的探索方向，并加速了代理发现奖励的能力。 我们使用我们的方法来训练一个新的神经策略，旨在处理网站的半结构化性质，并在一套网络任务上进行评估，包括最近的World of Bits基准。我们取得了新的最先进的结果，并表明工作流程引导的探索比行为克隆的样本效率提高了100倍以上。
现在，深度学习几乎是每个领域的主要话题之一。它有助于在大量的任务中获得惊人的结果。主要的问题是，这种学习和随之而来的神经网络，可以被定义为深度，是资源密集型的。它们需要专门的硬件在合理的时间内进行计算。 不幸的是，这不足以使深度学习在现实生活中 "可用"。许多任务是强制性的，尽可能的实时。因此，需要优化许多组件，如代码、算法、数字精度和硬件，以使它们 "高效可用"。所有这些优化可以帮助我们产生令人难以置信的准确和快速的学习模型。
词嵌入是一种有用的方法，可以捕捉大型文本语料库中的共现结构。除了文本数据本身，我们经常有额外的协变量与语料库中的单个文档相关--例如，作者的人口统计学、出版的时间和地点等----。 在本文中，我们提出了一个新的张量分解模型，用于具有协变量的词嵌入。我们的模型为所有的词共同学习一个emph{base}嵌入，以及一个加权对角线变换，以模拟每个协变量如何修改基础嵌入。 为了获得特定作者或地点的具体嵌入，例如，我们可以简单地将基础嵌入乘以与该时间或地点相关的转换矩阵。我们的方法的主要优点是数据效率和协变量转换矩阵的可解释性。 我们的实验表明，与只使用相关的数据子集为每个协变量学习单独的嵌入的标准方法相比，我们的联合模型在每个协变量的条件下学习的嵌入要好得多。此外，我们的模型鼓励嵌入是 "主题一致的"，即维度有特定的独立含义。 这使得我们针对协变量的嵌入可以按主题进行比较，从而实现下游的差异化分析。我们在几个数据集上实证评估了我们算法的好处，并展示了它如何被用来解决关于协变量影响的许多自然问题。
深度学习由于其在许多最先进的学习任务中的令人印象深刻的性能而受到了极大的关注。不幸的是，虽然非常强大，但深度学习在理论上并没有得到很好的理解，特别是最近才获得了关于训练深度神经网络的复杂性的结果。铰链损失、欧几里得损失等）可以用线性编程训练到接近最优的目标精度，其时间在输入数据和参数空间维度上是指数级的，在数据集的大小上是多项式的；假设$P\neq NP$，对输入维度的依赖性的改进是已知的不大可能的，而对参数空间维度的依赖性的改进仍然没有答案。 特别是，我们得到了给定的固定网络架构的训练的多项式时间算法。我们的工作更广泛地适用于经验风险最小化问题，这使我们能够概括以前的各种结果，并在适当的学习设置中为以前未研究的架构获得新的复杂性结果。
扩展卡尔曼滤波器（EKF）是一种经典的信号处理算法，它通过对局部测量函数的线性化，在非共轭模型中进行有效的近似贝叶斯推理，避免了在计算后验时需要计算难解的积分。在某些情况下，EKF优于依靠立方体来解决这种积分的方法，特别是在时间紧迫的现实世界问题中。EKF的缺点是其局部性质，而最先进的方法，如变分推理或期望传播（EP）被视为全局近似。 我们将功率EP表述为非线性卡尔曼滤波，然后表明线性化的结果是一个全局迭代的算法，在第一次通过数据时与EKF完全匹配，并在随后的通过中迭代改进线性化。另一个好处是能够计算EP功率趋于零的极限，这消除了类似EP算法的不稳定性。
本文探讨了不同设置下的学习型神经网络的简单性：在真实与随机数据上学习，不同的规模/架构，使用大批量与小批量规模。虽然可学习性不同于（事实上通常高于）测试精度，但本文的结果表明，小的泛化误差和高的可学习性之间有很强的相关性。这项工作还表明，与流行的深度网络相比，浅层网络存在明显的质量差异。 更广泛地说，本文在一个新的方向上扩展了以前关于理解学习型神经网络特性的工作。我们希望这种理解学习型神经网络的实证研究可能会阐明可以为深度学习的理论研究做出的正确假设。
    随着自然语言处理（NLP）任务的模型激增，理解模型之间的差异和它们的相对优点就更难了。简单地看整体指标之间的差异，如准确性、BLEU或F1，并不能告诉我们\emph{why}或\emph{how}某个特定方法更好，以及数据集的偏差如何影响模型设计的选择。   在本文中，我们提出了一种对NLP系统进行{emph{interpretable}评估的一般方法，并选择命名实体识别（NER）任务作为案例研究，这是一项识别文本中的人、地点或组织的核心任务。 所提出的评估方法使我们能够解释 \textit{模型偏差}、 \textit{数据集偏差}，以及 \emph{数据集的差异}如何影响模型的设计，确定当前方法的优势和劣势。通过提供我们的{分析}工具，我们使未来的研究人员很容易运行类似的分析并推动这一领域的进展。
视觉基础对话的任务包括在自主代理之间学习以目标为导向的合作对话，这些代理通过几轮问答来交换关于一个场景的信息。我们认为，要求代理遵守人类语言的规则，同时最大化信息交换是一个不理想的问题，并观察到人类不会偏离共同语言，因为他们是社会动物，每天必须与许多人交流，即使以一些效率损失为代价，坚持使用共同语言也要容易得多。 以此为灵感，我们提出并评估了一个多代理的对话框架，其中每个代理都与多个代理互动，并向其学习，并表明这导致了更相关和一致的对话（由人类评价者判断），而不牺牲任务性能（由定量指标判断）。
本文通过分析线性VAE及其与概率PCA（pPCA）的直接对应关系，对后验坍塌做出了简单而直观的解释。 我们表明，用变异推理训练线性VAE可以恢复与主成分方向相对应的唯一可识别的全局最大值。我们提供的经验证据表明，局部最大值的存在会导致深度非线性VAE的后验塌陷。我们的发现有助于解释文献中广泛的启发式方法，这些方法试图减少ELBO中KL项的影响以减少后验塌陷。
变形金刚在各种自然语言处理任务中取得了最先进的成果。尽管性能良好，但Transformers在长句建模中仍然很弱，在这种情况下，全局注意力图过于分散，无法捕捉到有价值的信息。在这种情况下，对序列建模同样重要的局部/标记特征在一定程度上被省略了。 为了解决这个问题，我们提出了一个多尺度注意力模型（MUSE），将注意力网络与卷积网络和位置前馈网络连接起来，以明确捕捉局部和标记特征。考虑到参数大小和计算效率，我们重新使用了原始Transformer中的前馈层，并采用轻量级动态卷积作为实现。实验结果表明，与Transformer相比，所提出的模型实现了大幅度的性能改进，特别是在长句子上，并将IWSLT 2014德译英任务的最先进水平从35.6推至36.2，在IWSLT 2015英译越任务中从30.6推至31.3。我们还在WMT 2014英译法数据集上达到了最先进的性能，BLEU分数为43.2。
现有的一些方法利用基于线性松弛的神经网络在扰动下的输出边界，但它们会使训练速度减慢数百倍，这取决于底层网络架构。同时，基于区间边界传播（IBP）的训练是高效的，在许多任务上明显优于基于线性松弛的方法，但它可能遭受稳定性问题，因为边界要宽松得多，特别是在训练的开始。 在本文中，我们提出了一种新的认证对抗训练方法--CROWN-IBP，它将快速的IBP边界与紧密的基于线性松弛的边界，即CROWN，在前向边界传递中结合在一起。CROWN-IBP计算效率高，在训练可验证的稳健神经网络方面一直优于IBP基线。 我们在MNIST和CIFAR数据集上进行了大规模的实验，在L_inf鲁棒性方面优于以前所有基于线性松弛和约束传播的认证防御。值得注意的是，在epsilon=0.3时，我们在MNIST上实现了7.02%的验证测试误差，在CIFAR-10上，epsilon=8/255时实现了66.94%的验证测试误差。
网络剪枝被广泛用于降低低资源环境下深度模型的沉重推理成本。一个典型的剪枝算法是一个三阶段的管道，即。在修剪过程中，根据一定的标准，多余的权重被修剪掉，重要的权重被保留，以最好地保持准确性。 对于我们研究的所有最先进的结构化修剪算法，微调一个修剪过的模型只能得到与随机初始化权重训练该模型相当或更差的性能。对于假定有预定的目标网络结构的修剪算法，人们可以摆脱整个管道，直接从头训练目标网络。1）训练一个大型的、过度参数化的模型往往不是获得高效的最终模型所必需的，2）大型模型中学习到的 "重要 "权重通常对小型剪枝模型没有用处，3）剪枝架构本身，而不是一组继承的 "重要 "权重，对最终模型的效率更为关键，这表明在某些情况下，剪枝可以作为一种架构搜索范式而发挥作用。我们的结果表明，在未来对结构化剪枝方法的研究中需要进行更仔细的基线评估。 我们还与 "彩票假说"（Frankle & Carbin 2019）进行了比较，发现在最佳学习率下，Frankle & Carbin（2019）中使用的 "中奖彩票 "初始化并没有带来比随机初始化更好的效果。
刷图技术有很长的历史，第一个交互式选择工具出现在20世纪90年代。从那时起，为了解决选择的准确性、可扩展性和灵活性问题，开发了许多额外的技术。在大型数据集中，许多视觉项目纠缠在一起并产生重叠，选择尤其困难。本文研究了一种新的刷图技术，它不仅依赖于实际的刷图位置，还依赖于刷图区域的形状。 首先，用户在感兴趣的轨迹可见的区域进行刷屏。其次，刷屏区域的形状被用来选择类似的项目。第三，用户可以调整相似度来过滤掉所要求的轨迹。这种技术包括两种类型的比较指标，即片断皮尔逊相关和基于信息几何的相似度测量。我们将其应用于具体场景，包括来自空中交通管制、眼动跟踪数据和GPS轨迹的数据集。
生成对抗网络（GANs）可以产生令人惊讶的复杂性和真实性的图像，但其结构一般是从单一的潜在源中取样，忽略了场景中可能存在的多个实体之间明确的空间互动。捕捉世界上不同物体之间的这种复杂互动，包括它们的相对比例、空间布局、遮挡或视角转换是一个具有挑战性的问题。 我们的模型以物体图像的边际分布为条件，并能从它们的联合分布中生成一个现实的图像。我们通过定性实验和用户评价来评估我们的模型，在训练过程中给出单个物体图像和联合场景的配对或不配对的例子。我们的结果显示，学到的模型捕捉了作为输入的两个物体域之间的潜在相互作用，在测试时间以合理的方式输出组成场景的新实例。
最近的研究强调了对抗性例子是对不同神经网络模型和许多下游应用的普遍威胁。然而，由于独特的数据属性激发了独特而强大的学习原则，本文旨在探索它们对减轻对抗性输入的潜力。特别是，我们的结果揭示了利用音频数据的时间依赖性来获得对对抗性例子的分辨能力的重要性。 在自动语音识别（ASR）任务和最近的三种音频对抗性攻击的测试中，我们发现（i）从图像对抗性防御中开发的输入转换提供了有限的鲁棒性改进，并且对高级攻击很微妙；（ii）时间依赖性可以被利用来获得对音频对抗性例子的判别能力，并且对我们实验中考虑的适应性攻击有抵抗力。我们的结果不仅显示了改进ASR系统鲁棒性的有希望的手段，而且在利用领域特定数据属性来减轻对抗性例子的消极影响中提供了新见解。
为了缓解生成式对抗网络（GANs）中臭名昭著的模式崩溃现象，我们提出了一种新的GANs训练方法，在训练过程中可以将某些假样本重新视为真实样本。 我们还证明，通过惩罚判别器输出之间的差异，并将相邻的真假样本对中的某些假数据点视为真实，可以缓解梯度爆炸。
我们提出了一个用于模型选择的交互式视觉探索潜空间（IVELS）的工具。 评估来自连续潜空间的离散序列的生成模型是一个具有挑战性的问题，因为它们的优化涉及多个相互竞争的目标项。 我们引入了一个模型选择管道，在更复杂和昂贵的指标的连续阶段对模型进行比较和过滤。我们在一个交互式的视觉工具中展示了该管道，以实现对指标的探索、对所学潜空间的分析，以及为给定任务选择最佳模型。 我们在对肽序列建模的案例研究中特别关注变异自动编码器系列，肽序列是氨基酸的短序列，由于存在我们想要建模的多种属性，这项任务特别有趣。
通过随机梯度下降（SGD）训练的神经网络已经存在了30多年，但它们仍然逃脱不了我们的理解。本文采取了一种实验方法，考虑到分而治之的策略：我们从研究单个神经元中发生的事情开始。虽然是深度神经网络的核心构件，但它们编码输入信息的方式以及这种编码如何出现仍然是未知的。我们报告的实验提供了强有力的证据，证明隐藏的神经元在训练和测试期间的行为像二进制分类器。 在训练过程中，对梯度的分析显示，一个神经元将两类输入分开，这在整个训练过程中是令人印象深刻的。在测试过程中，我们显示，上述的模糊、二进制分区嵌入了网络用于预测的核心信息。这些观察结果揭示了深度神经网络的一些核心内部机制，并有可能指导接下来的理论和实践发展。
对象的自动分类是工程和数据挖掘应用中最重要的任务之一。虽然使用更复杂和先进的分类器可以帮助提高分类系统的准确性，但它可以通过分析数据集及其特征来解决一个特定的问题。 由于数据集的性质和它们的特征对组合和分类系统的有效性有影响，因此使用线性和非线性激活函数（或转移函数）来实现更可靠的系统。对几个UCI数据集的实验和使用最小距离分类器作为简单分类器表明，拟议的基于线性和非线性智能FFN的特征组合可以呈现更可靠和有前途的结果。
最近对生成对抗网络（GANs）的改进使我们有可能根据自然语言描述（如图像说明）生成高分辨率的真实图像。我们引入了一种新的方法，允许我们通过在生成器和判别器上添加一个对象通路来控制图像中任意多的对象的位置。 我们的方法不需要详细的语义布局，只需要边界框和所需物体的各自标签。物体路径只关注单个物体，并在边界框指定的位置反复应用。全局路径关注图像背景和一般图像布局。我们在Multi-MNIST、CLEVR和更复杂的MS-COCO数据集上进行了实验。 我们的实验表明，通过使用物体路径，我们可以控制图像中的物体位置，并且可以对不同位置的多个物体的复杂场景进行建模。我们进一步表明，物体路径关注单个物体并学习与之相关的特征，而全局路径则关注全局图像特征和图像背景。
我们提出了一个基于MultiWOZ的抽象对话总结数据集。如果我们直接在对话中应用以前最先进的文档总结方法，有两个明显的缺点：信息实体如餐厅名称很难保留，不同对话领域的内容有时不匹配。 为了解决这两个缺点，我们提出了脚手架指针网络（SPNet），以利用现有的关于说话人角色、语义槽和对话领域的注释。由于ROUGE不能捕捉到上述两个缺点，我们还提出了一个新的评价指标，考虑文本中的关键信息实体。
自动和人工构建的知识库（KB）往往是不完整的---许多有效的事实可以通过综合现有的信息从KB中推断出来。一个流行的KB完成方法是通过对连接一对实体的其他路径上的信息进行组合推理来推断新的关系。 鉴于知识库的巨大规模和路径的指数级数量，以前的基于路径的模型只考虑了预测给定两个实体的缺失关系的问题，或者评估一个提议的三重关系的真实性。此外，这些方法传统上使用固定实体对之间的随机路径，或者最近学会在它们之间挑选路径。 我们提出了一种新的算法，MINERVA，它解决了回答关系已知但只有一个实体的更困难和实际的任务。由于随机行走在未知目的地和从起始节点开始的组合许多路径的情况下是不切实际的，我们提出了一种神经强化学习方法，它学习如何在输入查询的条件下浏览图来寻找预测路径。在对七个知识库数据集的综合评估中，我们发现MINERVA与许多目前最先进的方法相比具有竞争力。
卷积网络和灵长类动物的腹侧视觉流之间有许多不同之处。例如，标准卷积网络缺乏递归和侧向连接、细胞动力学等。然而，它们的前馈架构与腹侧流有些类似，值得进行更详细的比较。 最近的一项研究发现，视觉皮层的前馈架构可以近似为卷积网络，但所产生的架构在几个方面与广泛使用的深度网络不同。同一研究还发现，有点令人惊讶的是，训练这个网络的腹侧流进行物体识别的结果是性能不佳。本文更详细地研究了这个网络的性能。 特别是，我对基于腹侧流的架构做了一些改变，使其更像DenseNet，并测试了每一步的性能。我选择DenseNet是因为它有很高的BrainScore，而且它有一些类似皮质的架构特征，如大的in-egrees和长的跳过连接。大多数改变（使类似皮质的网络更像DenseNet）提高了性能。 需要进一步的工作来更好地理解这些结果。一种可能性是，腹股沟结构的细节可能不适合前馈计算、简单处理单元和/或反向传播，这可能表明高性能深度网络和大脑处理核心物体识别的方式之间的差异。
在强化学习中，通常让代理与环境互动一段时间，然后重新设置环境，并在一系列事件中重复这一过程。代理必须学习的任务可以是在（i）固定的时间内使其性能最大化，或者（ii）一个不确定的时期，该时间限制只在训练期间使用。 在本文中，我们从理论上研究了如何在这两种情况下有效地处理时间限制。在第一种情况下，我们认为由于时间限制造成的终止实际上是环境的一部分，并建议将剩余时间的概念作为代理的输入的一部分。 在第二种情况下，时间限制不是环境的一部分，只是用来促进学习。我们认为，这种终止不应该被视为环境的终止，并提出一种方法，具体到基于价值的算法，通过在每个部分情节结束时继续引导，纳入这一见解。为了说明我们建议的意义，我们在一系列环境中进行了一些实验，从简单的少数状态过渡图到复杂的控制任务，包括新颖和标准基准领域。
虽然随机梯度下降（SGD）是最近深度学习成功背后的驱动力，但我们对其在高维参数空间中的动态理解是有限的。近年来，一些研究人员使用迷你批次梯度的随机性或信噪比来更好地描述SGD的学习动态。受这些工作的启发，我们在这里通过检查迷你批次梯度的规范和方向的随机性从几何角度分析SGD。 我们通过von Mises-Fisher（VMF）分布提出了一个迷你批梯度的方向集中模型，并表明迷你批梯度的方向均匀性在SGD的过程中会增加。我们用深度卷积网络实证验证了我们的结果，并观察到梯度随机性和提议的方向均匀性之间的相关性高于梯度规范随机性的相关性，表明迷你批梯度的方向统计是SGD背后的一个主要因素。
在机器学习中，这种保证需要防止过度拟合，并确保模型对输入数据损坏的鲁棒性。为了最大限度地提高稳定性，我们分析并开发了一种计算效率高的雅各布正则化实现，以增加神经网络的分类边际。雅各布正则化的稳定效应导致了鲁棒性的明显改善，这是对随机和对抗性输入扰动的测量，而不会严重降低清洁数据的泛化特性。
随着在移动平台上部署卷积神经网络（CNN）的需求不断增加，稀疏核方法被提出，它可以比标准卷积节省更多的参数，同时保持准确性。然而，尽管潜力巨大，但之前的研究没有指出如何制作一个具有这种潜力的稀疏核设计（即。在本文中，我们是该领域中第一个考虑如何通过消除大的设计空间来制作一个有效的稀疏核设计的人。具体来说，我们提出了一个稀疏核方案，说明如何从三个方面来减少空间。 其次，为了去除精度下降较大的设计，我们在各种稀疏核设计背后找到了一个统一的属性，名为~emph{信息场}，它可以直接表明最终的精度。最后，我们在两种情况下去除可以实现更好的参数效率的设计。此外，我们对我们方案中的最终4个设计提供了详细的效率分析。实验结果验证了我们方案的想法，表明我们的方案能够找到在使用参数和计算方面更有效的设计，并具有类似或更高的精度。
在弱监督的时间动作定位中，由于高估了最突出的区域，以前的工作未能为每个完整的动作定位密集和完整的区域。为了缓解这个问题，我们提出了一个边际化平均注意网络（MAAN），以一种原则性的方式抑制最突出区域的主导反应。MAAN采用了一个新颖的边际化平均聚集（MAA）模块，并以端到端的方式学习了一组潜在的判别性概率。MAA根据一组潜在的判别概率从视频片段特征中抽取多个子集，并对所有平均的子集特征进行期望。从理论上讲，我们证明了具有学习过的潜在判别概率的MAA模块成功地减少了最突出的区域和其他区域之间的反应差异。因此，MAAN能够产生更好的类激活序列，并识别视频中密集和完整的动作区域。此外，我们提出了一种快速算法，将构建MAA的复杂性从$O(2^T)$降低到$O(T^2)$。在两个大规模视频数据集上进行的广泛实验表明，我们的MAAN在弱监督的时间动作定位上取得了卓越的性能。
深度图像先验（DIP），利用深度卷积网络（ConvNet）结构本身作为图像先验，在计算机视觉界引起了巨大关注。 经验表明，ConvNet结构在各种图像修复应用中的有效性。 然而，为什么DIP的效果如此好仍然是未知的，为什么卷积操作对于图像重建或增强是必不可少的也不是很清楚。在这项研究中，我们解决了这些问题。建议的方法是将卷积分为 "延迟-嵌入 "和 "转换（\ie编码器-解码器）"，并提出了一个简单但重要的图像/张量建模方法，这与动态系统和自相似性密切相关。 尽管MMES很简单，但在我们广泛的实验中，MMES的图像/张量完成和超分辨率结果与DIP非常相似，甚至具有竞争性，这些结果将有助于我们从 "低维斑块-折线先验 "的角度重新解释/描述DIP。
联合学习是最近在隐私保护方面的一个进展。在这种情况下，一个受信任的策展人以分散的方式聚集由多个客户优化的参数。然后将产生的模型分发回所有客户，最终收敛到一个联合代表模型，而无需明确分享数据。然而，该协议很容易受到差异化攻击，这种攻击可能来自任何一方在联合优化期间的贡献。在这种攻击中，客户在训练期间的贡献和他们的数据集的信息通过分析分布式模型而被发现。我们解决了这个问题，并提出了一种客户端差异化隐私保护的联合优化算法。目的是在训练期间隐藏客户端的贡献，平衡隐私损失和模型性能之间的权衡。经验研究表明，在有足够多的参与客户的情况下，我们提出的程序可以保持客户层面的差异化隐私，而在模型性能上只需付出很小的代价。
采用深度神经网络作为自然图像先验来解决逆向问题，要么需要大量的数据来充分训练表达式生成模型，要么可以通过未训练的神经网络在没有数据的情况下取得成功。然而，很少有作品考虑如何在这些无数据到高数据的情况下进行穿插。特别是，在解决这些逆向问题时，如何利用少量数据（甚至5-25个例子）的可用性来发挥自己的优势，以及系统的性能能否随着数据量的增加而提高？ 在这项工作中，我们考虑了在给定少量图像例子时解决线性逆向问题，这些例子来自与感兴趣的图像相同的分布。与没有使用任何数据的未经训练的神经网络相比，我们展示了如何用一些给定的例子预训练神经网络，以改善压缩传感和语义图像恢复问题的重建结果，如着色。我们的方法导致了随着可用数据量的增加而改善重建，与完全训练的生成模型相当，同时需要不到1%的数据来训练一个生成模型。
我们提出了合作训练（CoT），用于训练生成模型，测量离散数据的可操作密度。CoT协调地训练生成器G和辅助预测调解器M。M的训练目标是估计所学分布G和目标分布P的混合密度，G的目标是最小化通过M估计的Jensen-Shannon分歧。CoT实现了独立的成功，而不需要通过最大似然估计进行预训练，也不需要涉及像REINFORCE这样的高变异算法。这种低变异算法在理论上被证明对样本生成和似然预测都有优势。我们还从理论和经验上证明了CoT在生成质量和多样性、预测泛化能力和计算成本方面优于大多数以前的算法。
强化学习中的内在奖励为代理人提供了强大的算法能力，以学习如何以任务通用的方式与他们的环境互动。 在我们的实验中，一个视频游戏代理使用我们的模型来自主学习如何玩雅达利游戏，使用我们的好奇心奖励与游戏的外在奖励相结合，在具有稀疏外在奖励的游戏中取得更好的性能。
词嵌入是自然语言处理中的一个强有力的工具。在本文中，我们考虑了词嵌入的组成问题：------给定两个词的向量表示，计算整个短语的向量。我们给出了一个生成模型，可以捕获词之间的特定句法关系。在我们的模型下，我们证明三个词之间的相关性（由它们的PMI衡量）形成一个张量，具有一个近似的低等级塔克分解。 塔克分解的结果给出了词嵌入以及一个核心张量，它可以用来产生更好的词嵌入组合。我们还用实验来补充我们的理论结果，验证我们的假设，并证明新的组合方法的有效性。
我们提出了一种基于模型和无模型的混合方法--LEArning and Planning with Semantics（LEAPS），由一个作用于视觉输入的多目标子策略和一个语义结构的贝叶斯模型组成。 当被放置在一个看不见的环境中时，代理利用语义模型进行规划，以做出高级决策，为子政策的执行提出下一个子目标，并根据新的观察结果更新语义模型。我们利用House3D在视觉导航任务中进行了实验，House3D是一个三维环境，包含了各种由人类设计的带有真实世界物体的室内场景。
在本文中，我们专注于两个挑战，它们抵消了稀疏信号表示、传感和恢复的承诺。首先，现实世界的信号很少能被描述为已知基础上的完全稀疏向量，传统上使用的随机测量方案很少是感知它们的最佳方案。其次，现有的信号恢复算法通常不够快，无法使它们适用于实时问题。在本文中，我们通过提出一个基于深度学习的新框架来解决这两个挑战。 对于第一个挑战，我们通过使用最大似然（ML）的表述来寻找信息量的问题，并展示了我们如何使用卷积架构为感知信号建立一个数据驱动的降维协议。对于第二个挑战，我们讨论并分析了一个新的并行化方案，并展示了它大大加快了信号恢复过程。我们通过一系列的实验证明了我们的方法比竞争对手的方法获得的显著改善。
为了在复杂的环境中选择有效的行动，智能代理需要从过去的经验中进行归纳。世界模型可以代表关于环境的知识，以促进这种归纳。虽然通过深度学习从高维感官输入中学习世界模型变得可行，但有许多潜在的方法可以从中衍生出行为。 我们提出了Dreamer，一个纯粹通过潜在想象力解决长期任务的强化学习代理。我们通过在学习世界模型的紧凑状态空间中想象出的轨迹，对学习状态值的分析梯度进行反向传播，从而有效地学习行为。在20个具有挑战性的视觉控制任务中，Dreamer在数据效率、计算时间和最终性能方面超过了现有方法。
转移强化学习（RL）旨在通过利用在相关任务上训练的其他源代理的知识来提高代理的学习效率。在这项工作中，我们探索了转移RL的一个新挑战，即只有在未知的不同动态下收集的一组源策略可用于有效学习目标任务。 我们学习自适应地聚合源策略提供的行动，以最大限度地提高目标任务的性能。同时，我们学习一个辅助网络，预测聚合行动周围的残差，这确保了目标策略的表现力，即使一些源策略表现不佳。我们通过广泛的实验评估，证明了MULTIPOLAR的有效性，在连续和离散行动空间下，有六个模拟环境，从经典控制问题到挑战性的机器人模拟。
强化学习算法依赖于精心设计的来自环境的奖励，这些奖励对代理人来说是外在的。然而，用手工设计的、密集的奖励来注释每个环境是困难的，而且不能扩展，促使人们需要开发对代理人来说是内在的奖励函数。好奇心就是这样的内在奖励函数，它使用预测错误作为奖励信号。在本文中：(a) 我们对纯粹的好奇心驱动的学习进行了第一次大规模的研究，即{没有任何外在奖励}，跨越54美元的标准基准环境，包括Atari游戏套件。 (b) 我们研究了使用不同的特征空间来计算预测误差的效果，并表明随机特征对于许多流行的RL游戏基准来说是足够的，但学习过的特征似乎可以更好地概括（例如超级马里奥兄弟中的新游戏关卡）。(c) 我们证明了在随机设置中基于预测的奖励的局限性。游戏的视频和代码在https://doubleblindsupplementary.github.io/large-curiosity/。
这项工作提供了理论和经验上的证据，即不变性诱导的正则器可以提高最坏情况下的空间变换的预测精度（空间鲁棒性）。 在这些对抗性变换的例子上进行评估，我们证明在标准或对抗性训练的基础上增加正则化，可以使CIFAR10的相对误差减少20%，而不增加计算成本。 此外，我们观察到，对于已知具有固有方向差异的SVHN，稳健训练也提高了测试集的标准精度。
我们提出了顺序学习，以确定类的顺序图，代表等级或优先级，并将对象实例归入其中一个类。为此，我们设计了一个成对比较器，将两个实例之间的关系归入三种情况之一：一个实例 "大于"、"类似于 "或 "小于 "另一个。 然后，通过将输入实例与参考实例进行比较，并使比较结果之间的一致性最大化，就可以可靠地估计出输入的类别。我们应用顺序学习来开发一个面部年龄估计器，它提供了最先进的性能。此外，当使用性别和族群信息或甚至以无监督的方式将顺序图分为不相干的链时，性能得到进一步提高。
我们研究了一个由两部分组成的数据集在二元分类问题中代表两类物体的拓扑结构是如何变化的，因为它通过了一个训练有素的神经网络的层，即：。我们的目标是揭示深度神经网络中两个著名的谜团：(i)像ReLU这样的非平滑激活函数优于像双曲切这样的平滑函数；(ii)成功的神经网络架构依赖于拥有许多层，尽管事实上一个浅层网络能够任意很好地近似任何函数。(1)神经网络通过改变拓扑结构来运作，当它通过各层时，将拓扑结构复杂的数据集转化为拓扑结构简单的数据集。无论我们开始的数据集的拓扑结构有多复杂，当通过一个训练有素的神经网络时，两个组成部分的贝蒂数无一例外地减少到它们的最低值：第2个贝蒂数是1，所有更高的贝蒂数是0。 此外，（2）与双曲切线激活相比，ReLU激活的贝蒂数减少速度明显更快--这与前者定义非同构图（改变拓扑结构）而后者定义同构图（保持拓扑结构）的事实一致。最后，（3）浅层网络和深层网络对同一数据集的处理方式不同---浅层网络主要通过改变几何结构来操作，并且只在其最后几层改变拓扑结构，深层网络则将拓扑结构的变化更均匀地分布在其所有层中。
常见的深度学习模型的收敛率和最终性能大大受益于最近提出的启发式方法，如学习率计划、知识蒸馏、跳过连接和规范化层。在缺乏理论基础的情况下，旨在解释这些策略的功效的控制性实验可以帮助我们理解深度学习的景观和训练的动态性。 现有的经验分析方法依赖于线性插值和降维的可视化工具，每一种都有其局限性。相反，我们通过最近提出的损失面和表征分析方法，即模式连接和典型相关分析（CCA），重新审视启发式的经验分析，并假设启发式成功的原因。特别是，我们利用模式连接和CCA探索知识提炼和学习率启发式的（余弦）重启和热身。我们的实证分析表明。(a)经常引用的余弦退火成功的原因在实践中没有得到证明；(b)学习率预热的效果是防止深层产生训练不稳定性；(c)教师共享的潜在知识主要分散在深层。
嵌入式设备对神经网络（NN）的需求不断增加，导致大量研究调查低精度NN的训练方法。虽然大多数方法涉及量化步骤，但我们提出了一种原则性的贝叶斯方法，我们首先推断出离散权重空间的分布，然后从中推导出硬件友好的低精度NN。 为此，我们引入了一个概率前向通道来近似难以实现的变异目标，使我们能够对具有符号激活函数的NN的离散值权重分布进行优化。在我们的实验中，我们表明我们的模型在几个现实世界的数据集上实现了最先进的性能。此外，所产生的模型表现出大量的稀疏性，可以用来进一步降低推理的计算成本。
许多不规则的领域，如社交网络、金融交易、神经元连接和自然语言结构都被表示为图。近年来，各种图神经网络（GNN）已经成功地应用于此类图的表示学习和预测。然而，在许多应用中，底层图随时间变化，现有的GNN不足以处理这种动态图。 我们的方法扩展了流行的图卷积网络（GCN），使用最近提出的张量M-product技术来学习动态图的表示。
我们的主要动机是提出一种有效的方法来生成可用于现实世界应用的新型多元素稳定化合物。这项任务可以被表述为一个组合问题，需要人类专家花费许多时间来构建，并评估新的数据。无监督学习方法，如生成对抗网络（GANs）可以有效地用于生成新的数据。 据报道，跨域生成对抗网络在图像处理应用中取得了令人振奋的结果。然而，在材料科学领域，需要合成与观察样本相比具有更高阶复杂性的数据，而最先进的跨域GANs不能直接适应。在这一贡献中，我们提出了一种叫做CrystalGAN的新型GAN，它可以生成具有更高领域复杂性的新的化学稳定的晶体结构。我们介绍了一种原始的架构，我们提供了相应的损失函数，并且我们表明CrystalGAN生成了非常合理的数据。我们在一个真实的新型氢化物发现的原始问题上说明了所提方法的效率，该问题可进一步用于储氢材料的开发。
在高维体系中，每个参数都有自己的结构，如稀疏性或组稀疏性。在本文中，我们考虑了数据丰富的一般形式，其中数据来自固定但任意数量的任务$G$，任何凸函数，如规范，可以描述共同和单独参数的结构。 	我们提出了一个高维数据丰富模型的估计器，并研究其统计特性。 我们划定了我们的估计器的样本复杂性，并在一个比最先进的条件下为所有参数的估计误差提供了高概率的非渐进约束。我们提出了一个具有几何收敛率的迭代估计算法。	
自动驾驶车辆在城市交通中越来越普遍。 公司将开始发现需要对这些车辆进行智能城市车队协调教学。 目前，基于模拟的建模以及手工编码的规则决定了这些自主车辆的决策。我们相信，复杂的智能行为可以通过强化学习由这些代理学习。在本文中，我们讨论了我们的工作，通过调整深度Q-学习（DQN）模型来解决这个系统的多代理设置。 我们的方法通过结合卷积神经网络和DQN来应用深度强化学习，教导代理在一个对他们来说部分可观察的环境中满足客户的需求。我们还展示了如何利用转移学习来教导代理平衡多个目标，如在其能量水平低时导航到一个充电站。所提出的两个评估表明，我们的解决方案已经表明帽子我们能够成功教导代理合作政策，同时平衡多个目标。
稳定性是数据分析的一个关键方面。在许多应用中，稳定性的自然概念是几何学的，例如在计算机视觉中就有说明。散射变换构建的深度卷积表征对输入变形是认证稳定的。这种对变形的稳定性可以解释为对域的公制结构变化的稳定性。在这项工作中，我们表明散射变换可以使用扩散小波泛化到非欧几里得域，同时保留一个关于域的度量变化的稳定性概念，用扩散图来测量。
我们提出了一种用于终身学习的新型深度网络架构，我们称之为动态可扩展网络（DEN），它可以在一系列任务的训练中动态地决定其网络容量，以学习任务间紧凑的重叠知识共享结构。DEN通过执行选择性再训练，以在线方式有效地进行训练，在每个任务到来时动态地扩展网络容量，只使用必要数量的单元，并通过分割/复制单元和时间戳有效防止语义漂移。 我们在多个公共数据集的终身学习场景中对DEN进行了验证，在这些数据集上，它不仅明显优于现有的深度网络终身学习方法，而且在参数数量大幅减少的情况下达到了与批量模型相同的性能水平。
本文促进了这样一个想法，即深度学习方法可以用于经典的视觉测绘管道，以提高其准确性，并对其估计产生不确定性模型。我们表明，视觉测绘过程中固有的偏差可以被忠实地学习和补偿，与概率损失函数相关的学习架构可以联合估计剩余误差的全方差矩阵，定义一个异方差误差模型。 在自动驾驶图像序列和微型航空器摄像头采集的实验中，评估了同时改善视觉里程和估计与其输出相关的误差的可能性。
建立强大的在线内容推荐系统需要学习用户偏好和内容特征之间的复杂互动。近年来，该领域已经从传统的多臂匪徒和协作过滤技术迅速发展，新的方法整合了深度学习模型，能够捕捉到非线性特征的互动。尽管取得了进展，但在线推荐的动态性质仍然带来了巨大的挑战，例如找到探索和利用之间的微妙平衡。 在本文中，我们提供了一种新的方法，即深度密度网络（DDN），它可以分解测量和数据的不确定性，并预测CTR的概率密度，使我们能够对特征空间进行更有效的探索。我们展示了在现实世界的内容推荐系统中在线使用DDN的有用性，该系统每天提供数十亿的推荐，并提出在线和离线结果来评估使用DDN的好处。
虽然有资料显示，随着时间的推移，气候变化接受者和否认者在美国变得越来越两极化，但还没有大规模的研究这些人是否容易因外部自然事件而改变他们的意见。在Twitter用户的子群体中，我们研究了气候变化情绪是否会因2018年美国发生的五次独立自然灾害而改变。 我们首先表明，当使用我们的方法来补偿有限的标记数据时，推文可以以超过75%的准确率分类为接受或否认气候变化；结果在几个机器学习模型中是稳健的，并产生与先前研究一致的地理层面的结果。 然后，我们应用RNNs进行队列级分析，显示2018年飓风产生的平均推文情绪在统计学上有明显的增加，肯定了气候变化。然而，这种效果在研究的2018年暴雪和野火中并不成立，这意味着Twitter用户对气候变化的意见在这个自然灾害子集上是相当根深蒂固的。
我们研究了具有未知动态和隐藏状态的对称线性动态系统的控制。使用最近的谱过滤技术在线性基础上简洁地表示这种系统，我们将这种设置中的最佳控制表述为凸程序。 这种方法消除了解决明确识别系统及其潜在状态的非凸问题的需要，并允许对控制信号进行可证明的优化保证。我们给出了第一个有效的算法，用于寻找任意时间范围T的最佳控制信号，其样本复杂度（训练次数）仅为log（T）和其他相关参数的多项式。
生成对抗网络（GANs）已经成为学习高维分布生成模型的黄金标准。自从它们出现以来，文献中已经介绍了许多GANs的变体，主要集中在利用新的损失函数、优化/规则化策略和网络结构。 具体来说，我们提出了PolyGAN，我们通过高阶多项式对数据发生器进行建模，其未知参数自然由高阶张量表示。我们引入了两种张量分解，大大减少了参数的数量，并展示了它们如何被只采用线性/卷积块的层次神经网络有效实现。 我们首次展示了通过使用我们的方法，GAN生成器可以在不使用任何激活函数的情况下对数据分布进行近似。对合成和真实数据（图像和三维点云）的彻底实验评估表明了PolyGAN相对于现有技术的优点。
近年来，在大型监督数据集上训练的深度神经网络取得了令人印象深刻的结果。然而，由于良好的注释数据集的收集成本过高且耗时过长，最近的工作已经探索了使用更容易获得的更大但有噪声的数据集。在本文中，我们研究了深度神经网络在具有大量噪声标签的训练集上的行为。我们在多个数据集上显示，如MINST、CIFAR-10和ImageNet，即使有基本的任意数量的噪声，成功学习是可能的。 例如，在MNIST上，我们发现，即使数据集被稀释为每个干净的例子有100个嘈杂的例子，仍然可以达到90%以上的准确率。这种行为在多种标签噪声模式下都成立，即使嘈杂的标签偏向于混乱的类别。最后，我们提出了在高标签噪声下改善学习的简单可操作技术。
在本文中，我们建议将最近引入的模型诊断元学习算法（MAML，Finn等人，2017年）扩展到低资源神经机器翻译（NMT）。我们将低资源翻译构建为一个元学习问题，我们学习适应基于多语种高资源语言任务的低资源语言。2018b）来克服不同语言之间的输入输出不匹配。我们使用18种欧洲语言（Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv和Ru）作为源任务和5种不同语言（Ro, Lv, Fi, Tr和Ko）作为目标任务来评估所提出的元学习策略。例如，所提出的方法在罗马尼亚-英语WMT'16上只看到16,000个翻译词（约600个平行句子），就能达到高达22.04的BLEU。）
这项工作提出了一种主动异常检测的方法，它可以建立在现有的深度学习解决方案上，用于无监督的异常检测。我们表明，为了在无监督的异常检测中获得性能保证，需要假设一个关于异常是什么的先验。 我们认为，主动异常检测在实践中具有与无监督异常检测相同的成本，但有可能获得更好的结果。为了解决这个问题，我们提出了一个新的层，可以附加到任何为无监督异常检测设计的深度学习模型上，将其转化为主动方法，在合成和真实的异常检测数据集上呈现结果。
在本文中，我们询问了决定分类器决策的主要因素，并通过研究自动编码框架产生的潜伏代码来发现这些因素。为了提供对分类器行为的解释，我们提出了一种方法，提供一系列的例子，突出分类器决策之间的语义差异。 我们引入并正式确定了语义随机路径的概念，作为通过潜伏代码插值在特征空间中定义的合适的随机过程。然后，我们引入了语义拉格朗日的概念，作为纳入所需分类器行为的一种方式，并发现相关变异问题的解决允许突出分类器决策中的差异。非常重要的是，在我们的框架内，分类器被用作黑箱，只需要对其进行评估。
计划的合理性和优化取决于领域模型的正确性。在现实世界的应用中，指定完整的领域模型是很困难的，因为代理和它的环境之间的相互作用可能相当复杂。我们提出了一个框架，在多个规划问题上逐步学习模型的PPDDL表示，只使用当前规划问题的经验，这适合于非稳定环境。 我们引入了可靠性的新概念，作为强化学习的内在动力，以及从失败中学习的手段，以防止类似失败的重复发生。我们的动机是提高学习效率和目标导向性。
深度强化学习（DRL）领域最近出现了一个最大熵强化学习算法的流行趋势。 它们的流行源于对最大熵目标的直观解释，以及它们在标准基准上的卓越样本效率。在本文中，我们试图了解熵项对最大熵算法性能的主要贡献。对于Mujoco基准，我们证明软演员批评（SAC）中的熵项主要解决行动空间的有界性。 有了这样的认识，我们提出了一个简单的规范化方案，使没有熵值最大化的精简算法与SAC的性能相匹配。我们的实验结果表明，有必要重新审视DRL中熵值规范化的好处。 我们进一步表明，采用简单的非均匀抽样方案的精简算法优于SAC，在具有挑战性的连续控制任务上取得了最先进的性能。
最近，通过首先搜索与问题相关的段落，然后应用阅读理解模型来提取答案，成为回答开放领域问题的一种流行方法。现有的工作通常从单个段落中独立提取答案，因此没有充分利用多个搜索的段落，特别是对于一些需要多个证据的问题，这些证据可能出现在不同的段落中，需要回答。 在本文中，我们将这一问题作为答案重排来处理。具体来说，基于现有的最先进的质量保证模型所产生的答案候选者，我们提出了两种不同的重排方法，即基于强度的重排和基于覆盖率的重排，它们利用来自不同段落的聚合证据来帮助得出问题的基本真实的答案。 我们的模型在三个公共开放域QA数据集Quasar-T、SearchQA和TriviaQA的开放域版本上取得了最先进的技术，在前两个数据集上有大约8％的改进。
许多大型文本集表现出图形结构，要么是内容本身所固有的，要么是在单个文档的元数据中编码的。从文档集中提取的图形例子是共同作者网络、引文网络或命名实体-发生率网络。在本文中，我们建议将文本和图形两者结合起来，不仅将文件内容中编码的语义信息可视化，而且还将固有的网络结构所表达的关系可视化。 为此，我们引入了一种基于多目标优化的新算法，在二维景观中共同定位嵌入的文档和图形节点。我们用真实世界的数据集说明了我们的方法的有效性，并表明我们可以比其他基于内容或网络信息的可视化更好地捕捉大型文档集的语义。
机器学习模型表现出偏见，通常是因为用来训练它们的数据集是有偏见的。这给这种技术的部署带来了严重的问题，因为所产生的模型可能在训练集内的少数民族人群中表现不佳，最终给他们带来更高的风险。我们建议使用高保真计算机模拟来审视和诊断ML分类器内的偏见。 我们提出了一个框架，利用贝叶斯参数搜索来有效地描述高维特征空间，并更快地确定性能的弱点。我们将我们的方法应用于一个例子领域--人脸检测，并表明它可以用来帮助识别商业人脸应用编程接口（API）中的人口偏差。
点云是一种灵活的、无处不在的、具有任意分辨率和精度的三维物体表示方式。以前的工作表明，调整编码器网络以匹配其输入点云的语义，可以大大改善其有效性，而不是天真的前馈替代方案。然而，绝大多数关于点云解码器的工作仍然基于全连接的网络，将形状表示映射到固定数量的输出点。 具体来说，我们研究了基于样本的点云解码器，它将形状表示映射到点特征分布，允许任意数量的采样特征转化为单个输出点。我们开发了三种基于样本的解码器架构，并将它们的性能相互比较，显示它们比前馈架构更有效。此外，我们调查了学习的分布，以深入了解输出转化。我们的工作可以作为一个可扩展的软件平台来再现这些结果并作为未来工作的基线。
我们提出了一种深度强化学习的方法来最小化优化编译器中的神经网络计算图的执行成本。与早期基于学习的工作不同，这些工作需要在要优化的同一图上训练优化器，我们提出了一种学习方法，离线训练优化器，然后将其推广到以前未见过的图，而无需进一步训练。 这使得我们的方法能够在几秒钟内而不是几小时内对真实世界的TensorFlow图产生高质量的执行决定。我们考虑了计算图的两个优化任务：最小化运行时间和峰值内存使用。与一组广泛的基线相比，我们的方法在这两个任务上比经典的和其他基于学习的方法取得了明显的改善。
预测编码理论表明，大脑通过预测不同层次的抽象观察来进行学习。最基本的预测任务之一是视图预测：一个给定的场景从另一个角度看会如何？人类擅长这项任务。我们想象和填补缺失的视觉信息的能力与感知紧密相连：我们感觉好像看到了三维的世界，而事实上，只有来自世界正面的信息进入我们的（二维）视网膜。 本文探讨了视图预测表征学习之间的联系及其在三维视觉识别发展中的作用。我们提出了逆向图形网络，该网络将移动摄像机拍摄的2.5D视频流作为输入，并通过将场景内容与摄像机的运动分离，映射到稳定的场景三维特征图。 该模型还可以将其三维特征图投射到新的视点，以预测和匹配目标视图。我们提出了对比性预测损失，可以处理视觉输入的随机性，并可以将视图预测学习扩展到比以往工作中考虑的更逼真的场景。 我们表明，所提出的模型学习的三维视觉表征对于（1）三维物体检测器的半监督学习，以及（2）三维移动物体检测器的无监督学习，通过估计动态场景视频中推断出的三维特征图的运动。就我们所知，这是第一项工作，实证表明视图预测是一项有益于三维物体检测的、可扩展的自我监督的任务。 
一些最先进的方法在成对的文本和音频样本（x_txt, x_aud）上训练一个编码器-解码器网络，鼓励其输出来重建x_aud。 不幸的是，TTS中的风格建模在某种程度上是不确定的，仅用重建损失训练模型不足以将内容和风格与其他变化因素分开。在这项工作中，我们引入了一个端到端的TTS模型，提供了增强的内容风格分离能力和可控性。 我们通过将成对训练程序、对抗性博弈和协作性博弈结合到一个训练方案中来实现这一点。对抗性博弈集中了真实的数据分布，而协作性博弈在原始空间和潜在空间中最小化了真实样本和生成样本之间的距离。 受益于风格和内容的独立建模，我们的模型可以生成满足所需风格条件的人类保真语音。我们的模型在多个任务中取得了最先进的结果，包括风格转移（内容和风格互换）、情感建模和身份转移（适合新发言人的声音）。
经验证据表明，具有ReLU激活的神经网络在超参数化的情况下泛化得更好。然而，目前还没有理论分析来解释这一观察。在这项工作中，我们研究了一个简化的学习任务，超参数化的卷积网络在经验上表现出相同的定性现象。 对于这种情况，我们对梯度下降的优化和泛化性能进行了理论分析。具体来说，我们证明了与数据相关的样本复杂性界限，这表明超参数化改善了梯度下降的泛化性能。
我们引入了一种新的、严格表述的PAC-Bayes几发元学习算法，该算法隐含地学习了感兴趣的模型先验分布。我们提出的方法将PAC-Bayes框架从单一任务设置扩展到几发元学习设置，以对未见过的任务的泛化误差进行上限值。 我们还提出了一种基于生成的方法，与通常的对角线高斯假设相比，对共享先验和特定任务的后验进行了更明确的建模。我们表明，用我们提出的元学习算法训练的模型是很好的校准和准确的，在mini-ImageNet基准上有最先进的校准和分类结果，在多模式的任务分布回归中也有竞争力的结果。
随着可解释人工智能（XAI）和可解释人工智能规划（XAIP）领域的成熟，代理人生成和策划解释的能力也将同样增长。我们提出了一个新的挑战领域，即反叛性和欺骗性的解释。我们讨论了如何生成这些解释，然后简要讨论评价标准。
我们研究了变异自动编码器的一个变体，在这个变体上有一个离散潜变量的上层结构。一般来说，我们的上层结构是一个由多个超潜变量组成的树状结构，它可以从数据中自动学习。当上层结构中只有一个潜变量时，我们的模型简化为一个假设潜特征是由高斯混合模型产生的模型。 我们称我们的模型为潜树变异自动编码器（LTVAE）。以前用于聚类的深度学习方法只产生一个数据分区，而LTVAE产生多个数据分区，每个分区由一个超级潜变量给出。这是可取的，因为高维数据通常有许多不同的自然面，可以以多种方式进行有意义的划分。
许多实用的机器人运动任务要求代理人使用可由目标参数化的控制策略。在这个方向上流行的深度强化学习方法涉及学习目标条件的策略或价值函数，或逆向动力学模型（IDMs）。IDMs将代理人的当前状态和预期目标映射到所需的行动上。 我们设计了一个训练过程，指导学习潜在的表征来编码这种共享信息。使用有限的环境互动，我们的代理能够有效地导航到目标空间中的任意点。我们在高维运动环境中证明了我们的方法的有效性，如Mujoco Ant、PyBullet Humanoid和PyBullet Minitaur。我们提供定量和定性的结果，表明我们的方法明显优于竞争的基线方法。
在本文中，我们首先确定了textit{角度偏差}，这是一个简单而显著的现象，它导致了具有sigmoid激活函数的多层感知器（MLP）中的梯度消失问题。然后我们提出了textit{线性约束权重（LCW）}来减少神经网络中的角度偏差，以便在每个权重向量的元素之和为零的约束下训练网络。 提出了一种重新参数化技术，通过将对权重向量的约束嵌入到网络结构中来有效地训练LCW模型。有趣的是，批量归一化（Ioffe & Szegedy, 2015）可以被看作是一种纠正角度偏差的机制。初步实验表明，LCW比批量归一化更有效地帮助训练一个100层的MLP。
马尔科夫逻辑网络（MLN）优雅地结合了逻辑规则和概率图形模型，可用于解决许多知识图问题。然而，MLN的推理是计算密集型的，使得MLN的工业规模应用非常困难。近年来，图神经网络（GNN）已经成为大规模图问题的高效和有效工具。然而，GNN没有明确地将先验逻辑规则纳入模型，并且可能需要许多标记的例子来完成目标任务。 在本文中，我们探索了MLN和GNN的结合，并在MLN中使用图神经网络进行变异推理。我们提出了一个GNN变体，名为ExpressGNN，它在模型的表示能力和简单性之间取得了很好的平衡。我们在几个基准数据集上进行了广泛的实验，证明ExpressGNN导致了有效和高效的概率逻辑推理。
强化学习（RL）方法在多个任务中取得了超越人类性能的重大进展。然而，大多数RL策略显示出一定程度的弱点，在处理高维和非平稳环境时可能变得难以计算。
信息瓶颈原理是一种优雅而有用的表征学习方法。在本文中，我们利用信息瓶颈框架研究了强化学习中的表征学习问题，旨在提高学习算法的样本效率。我们分析得出表征的最佳条件分布，并提供了一个变异下限。然后，我们用斯坦因变异（SV）梯度方法使这个下限最大化。我们将这一框架纳入优势行为者批评算法（A2C）和近似政策优化算法（PPO）。我们的实验结果表明，我们的框架可以显著提高香草A2C和PPO的采样效率。最后，我们用称为互信息神经估计（MINE）的算法研究深度RL中的信息瓶颈（IB）观点。 我们通过实验验证了信息提取-压缩过程也存在于深度RL中，而我们的框架能够加速这一过程。我们还分析了MINE和我们的方法之间的关系，通过这种关系，我们从理论上推导出一种算法来优化我们的IB框架，而不构建下限。
 人类智能的一个核心方面是快速学习新任务并在它们之间灵活切换的能力。这里，我们描述了一个受这些能力启发的模块化持续强化学习范式。我们首先介绍了一个视觉交互环境，它允许许多类型的任务被统一在一个框架中。然后我们描述了一个奖励图预测方案，在这样一个环境所要求的非常大的状态和行动空间中稳健地学习新任务。 我们研究了模块架构的属性如何影响任务学习的效率，表明一个包含特定设计原则（如早期瓶颈、低阶多项式非线性和对称性）的模块图案明显优于更标准的神经网络图案，需要更少的训练实例和更少的神经元来实现高水平的性能。最后，我们提出了一个基于动态神经投票方案的任务切换的元控制器架构，它允许新模块使用从以前看到的任务学到的信息来大幅提高自己的学习效率。
可解释性和小的标记数据集是深度学习实际应用中的关键问题，特别是在医学等领域。在本文中，我们提出了一种半监督技术，同时解决这两个问题。 我们从大型无标签图像数据集中学习密集表征，然后使用这些表征从小型标签集中学习分类器，并产生解释预测的视觉理由。使用胸部放射诊断作为激励性应用，我们通过学习表征我们的胸部放射数据集，同时在不同机构的独立数据集上训练分类器，显示我们的方法具有良好的泛化能力。 我们的方法可以识别心力衰竭和其他胸腔疾病。对于每个预测，我们通过优化潜像表示，使疾病的概率最小化，同时受到图像空间中相似性度量的限制，为正面分类产生视觉理由。
进化策略（ES）是一个流行的黑盒子零阶优化算法系列，它依靠搜索分布来有效地优化大量的目标函数。本文研究了在ES算法中使用高度灵活的搜索分布的潜在好处，与标准的搜索分布（通常是高斯）相比。 我们用生成神经网络（GNN）对这种分布进行建模，并引入一种新的ES算法，利用其表现力来加速随机搜索。
我们提出并评估了压缩和加速密集矩阵乘法的新技术，这些技术在用于嵌入式大词汇量连续语音识别（LVCSR）的神经网络的全连接层和递归层中发现。对于压缩，我们引入并研究了用于训练矩阵乘法的低等级因子版本的跟踪规范化技术。 与标准的低等级训练相比，我们表明我们的方法导致了良好的准确性与参数数量的权衡，并可用于加速大型模型的训练。在加速方面，我们通过为小批量优化的新开源内核在ARM处理器上实现了更快的推理，与广泛使用的gemmlowp库相比，速度提高了3倍到7倍。
训练激活量化的神经网络需要最小化一个片状常数的训练损失，其梯度几乎到处消失，这对标准的反向传播或链式规则来说是不可取的。由于这个不寻常的 "梯度 "肯定不是损失函数的梯度，下面的问题就出现了：为什么在它的负方向搜索可以使训练损失最小化？ 我们将STE-modifed chain rule给出的不寻常的 "梯度 "称为粗梯度。我们证明，如果STE选择得当，预期的粗梯度与群体梯度（训练中不可用）正相关，其否定是最小化群体损失的下降方向。 此外，我们还表明，STE的选择不当会导致训练算法在某些局部最小值附近不稳定，这一点在CIFAR-10实验中得到了验证。
本文介绍了GumbelClip，这是一套对演员批评算法的修改，用于非政策性强化学习。GumbelClip使用截断重要性抽样的概念和加性噪声来产生一个损失函数，使非政策性样本得以使用。 与政策性算法相比，修改后的算法实现了收敛速度和样本效率的提高，与现有的非政策性政策梯度方法相比具有竞争力，同时实施起来也明显简单。在Atari领域的一个子集上，GumbelClip的有效性与现有的政策性和非政策性行为者批评算法进行了比较。
在过去的几年里，由于生成对抗网络（GANs）的提出，生成模型取得了各种进展。GANs已被证明在与图像生成和风格转移有关的各种任务中表现出色。在自然语言处理领域，单词嵌入，如word2vec和GLoVe是在文本数据上应用神经网络模型的最先进方法。 这项工作提出了一种使用Skip-Thought句子嵌入与基于梯度惩罚函数和f-measures的GANs相结合的文本生成方法。使用句子嵌入与GANs生成以输入信息为条件的文本的结果与使用词嵌入的方法相当。
在这项工作中，我们提出了一个新的解码器架构，它可以按任意顺序生成自然语言序列。除了从给定的词汇中生成标记外，我们的模型还学习为每个生成的标记选择最佳位置。 我们在IWSLT机器翻译任务上展示了我们的新解码器的性能，并通过分析模型如何为每个后续标记选择新的位置来检查和解释学到的解码模式。
在本文中，我们研究了一个新的图学习问题：学习计算子图的同构性。虽然基于学习的方法是不精确的，但我们能够在多项式时间内概括计算大型模式和数据图，而原始NP-complete问题的时间是指数级的。与其他传统的图学习问题如节点分类和链接预测不同，子图同构性计算需要更多的全局推理来监督整个图。 为了解决这个问题，我们提出了一个动态中间注意记忆网络（DIAMNet），它增强了不同的表示学习架构，并迭代地关注模式和目标数据图，为全局计数记忆不同的子图同构。 我们开发了小图（每个图中<=1,024个子图同构）和大图（每个图中<=4,096个子图同构）集，以评估不同的模型。实验结果表明，基于学习的子图同构计数可以帮助降低时间复杂度，并具有可接受的准确性。我们的DIAMNet可以进一步改善现有的表示学习模型，以解决这个更全面的问题。
领域适应是深度强化学习（RL）中的一个开放性问题。通常，代理被要求在难以获得数据的环境中执行。在这种情况下，代理在类似的环境中被训练，如模拟器，然后被转移到原始环境中。源环境和目标环境的视觉观察之间的差距往往导致代理在目标环境中失败。 我们提出了一种新的RL代理，SADALA（Soft Attention DisentAngled Representation Learning Agent）。SADALA首先学习一个压缩的状态表征，然后共同学习忽略分散注意力的特征并解决所提出的任务。SADALA对重要和不重要的视觉特征的分离导致了稳健的领域转移。SADALA在跨RL环境（Visual Cartpole和DeepMind Lab）的表现优于先前基于disentangled表征的RL和领域随机方法。
旨在正式认证神经网络预测行为的鲁棒性验证已成为了解特定模型行为和获得安全保证的重要工具。然而，以前的方法通常仅限于相对简单的神经网络。在本文中，我们考虑了变形金刚的鲁棒性验证问题。 变形金刚有复杂的自我注意层，这给验证带来了许多挑战，包括交叉非线性和交叉位置依赖，这在以前的工作中没有讨论过。我们解决了这些挑战，并开发了第一个变形金刚的验证算法。我们的方法计算出的认证鲁棒性界限明显比天真的区间边界传播法更严格。这些界限也为解释变形金刚提供了启示，因为它们一致反映了情感分析中单词的重要性。
在过去的几年里，深度学习在许多应用中取得了巨大的成功。然而，我们对深度学习的理论理解，以及提供原则性改进的能力，似乎落后了。一个理论上的难题涉及到深度网络的预测能力，尽管他们引人注意的明显缺乏概括性：他们在训练集上的分类准确性并不能代表他们在测试集上的表现。 训练性能怎么可能独立于测试性能呢？深层网络确实需要一个全新的泛化理论吗？或者有基于训练数据的测量方法可以预测网络在未来数据上的性能？这里我们表明，当性能被适当测量时，训练性能实际上可以预测预期性能，与经典的机器学习理论一致。
我们提出了一个 "在线计划和离线学习 "的框架，在这个框架中，一个具有内部模型的代理人需要不断地在世界中行动和学习。我们的工作建立在基于局部模型的控制、全局价值函数学习和探索之间的协同关系上。我们研究局部轨迹优化如何应对价值函数的近似误差，并能稳定和加速价值函数学习。相反，我们也研究近似的价值函数如何帮助减少计划的范围，并允许超越局部解决方案的更好的政策。 最后，我们还展示了轨迹优化如何与估计价值函数近似中的不确定性一起用于执行时间上的协调探索。这种探索对于快速和稳定的价值函数学习至关重要。结合这些组件，能够解决复杂的控制任务，如人形运动和灵巧的手部操纵，相当于现实世界中几分钟的经验。
现代神经网络架构利用越来越深的层，以及其结构的各种进步来实现更好的性能。虽然传统的显式正则化技术，如dropout、权重衰减和数据增强，仍然在这些新模型中使用，但关于这些新结构的正则化和泛化效果的研究很少。除了比它们的前辈更深入之外，像ResNet和DenseNet这样较新的架构是否也能从其结构的隐性正则化特性中获益？在这项工作中，我们研究了跳过连接对网络泛化特征的影响。通过实验，我们表明某些神经网络架构有助于它们的泛化能力。具体来说，我们研究了低级特征对泛化性能的影响，当它们被引入到DenseNet、ResNet以及具有 "跳过连接 "的网络的更深层时，我们表明，当训练数据的质量和数量都下降时，这些低级表征确实有助于在多种环境中的泛化。
训练生成模型如变异自动编码器（VAE）的挑战之一是避免后验塌陷。当生成器的容量过大时，它很容易忽略潜伏代码。这个问题在数据集较小、潜伏维度较高时更加严重。 本文还介绍了一种新的技术，名为潜伏剪裁，用于控制潜伏空间中样本之间的距离。本文设计了一个概率自动编码器模型，名为$mu$-VAE，并在MNIST和MNIST时尚数据集上使用新的目标函数进行训练，结果表明其性能优于用ELBO和$beta$-VAE目标训练的模型。 $mu$-VAE不容易出现后验塌陷，并能以良好的质量生成重建和新的样本。由$mu$-VAE学到的潜在表征被证明是良好的，可用于下游任务，如分类。 
基于似然的生成模型是检测分布外（OOD）输入的一种有前途的资源，这可能会损害机器学习系统的鲁棒性或可靠性。然而，从这种模型得出的似然已被证明在检测与训练数据有显著差异的某些类型的输入时有问题。在本文中，我们提出这一问题是由于输入复杂性对生成模型的似然有过度的影响。 我们报告了一组支持这一假设的实验，并使用输入复杂性的估计值来推导出一个有效的、无参数的OOD分数，它可以被看作是一种似然比，类似于贝叶斯模型比较。我们发现，在广泛的数据集、模型、模型大小和复杂性估计值下，这种分数与现有的OOD检测方法表现相当，甚至更好。
 最近提出的几种随机优化方法已经成功地用于训练深度网络，如RMSProp、Adam、Adadelta、Nadam，这些方法是基于使用过去梯度平方的指数移动平均数的平方根进行梯度更新。在许多应用中，例如在大输出空间的学习中，根据经验观察，这些算法无法收敛到最优解（或者在非凸设置中的临界点）。 我们提供了一个简单的凸优化环境的明确例子，其中Adam没有收敛到最优解，并描述了以前对Adam算法分析的确切问题。我们的分析表明，收敛问题可以通过赋予此类算法过去梯度的 "长期记忆 "来解决，并提出了Adam算法的新变体，这些变体不仅解决了收敛问题，而且往往还导致经验性能的提高。
有针对性的清洁标签中毒是对机器学习系统的一种对抗性攻击，即对手在训练数据中注入一些正确标记的、干扰最小的样本，从而导致部署的模型在推理过程中对特定的测试样本进行错误分类。尽管已经提出了针对一般中毒攻击（那些旨在降低整体测试准确性的攻击）的防御措施，但还没有证明对清洁标签攻击的可靠防御，尽管这些攻击的有效性及其现实的使用案例。我们针对最近发表的两种清洁标签中毒攻击测试了我们提出的方法，这两种攻击都使用了CIFAR-10数据集。在复制了他们的实验后，我们证明了我们的防御措施能够检测到两种攻击中超过99%的中毒例子，并在不影响模型性能的情况下将其删除。我们简单的防御措施表明，目前的清洁标签中毒攻击策略可以被取消，并作为强大但易于实施的基线防御，用于测试未来的清洁标签中毒攻击。
抽象推理，特别是在视觉领域，是一种复杂的人类能力，但它对人工神经学习系统来说仍然是一个具有挑战性的问题。在这项工作中，我们提出了MXGNet，一个用于多面板图解推理任务的多层图神经网络。MXGNet结合了三个强大的概念，即对象级表示、图神经网络和多重图，用于解决视觉推理任务。 MXGNet首先为图表所有面板中的每个元素提取对象级表示，然后形成一个多层多重图，捕捉不同图表面板中对象之间的多重关系。 MXGNet总结了从任务的图中提取的多个图形，并利用这种总结从给定的候选人中选出最可能的答案。我们在两种类型的图解推理任务上测试了MXGNet，即图解逻辑和Raven进步矩阵（RPM）。 对于PGM和RAVEN这两个用于RPM推理的综合数据集，MXGNet以相当大的优势超过了最先进的模型。
电子表格的语义结构提取包括检测表格区域、识别结构组件和分类单元格类型。自动语义结构提取是将各种表格结构的数据自动转化为规范模式的关键，从而实现数据分析和知识发现。然而，它们受到了多样化表格结构和单元格上空间相关语义的挑战。 首先，我们提出了一个多任务框架，联合学习表格区域、结构组件和单元格类型；其次，我们利用最近的语言模型的进展来捕捉每个单元格值的语义；第三，我们建立了一个大型的人类标记的数据集，广泛覆盖表格结构。我们的评估表明，我们提出的多任务框架非常有效，超过了单独训练每个任务的结果。
开放领域的对话生成在自然语言处理中获得了越来越多的关注。比较这些方法需要一个整体的对话评估手段，人类评分被认为是黄金标准。 我们的指标包括：（1）基于GPT-2的对话中句子之间的上下文连贯性，（2）基于GPT-2的措辞流畅性，以及（3）基于$n$-gram的对增强查询的反应的多样性。我们的指标的经验有效性通过与人类判断的强相关性得到了证明。
卷积神经网络（CNN）以其捕捉局部潜在特征的杰出能力在计算机视觉任务中获得了巨大的成功。最近，人们对将CNN扩展到一般的空间领域越来越感兴趣。虽然各种类型的图形卷积和几何卷积方法已经被提出，但它们与传统的二维卷积的联系还没有被充分理解。 在本文中，我们表明深度可分离卷积是将两种卷积方法统一在一个数学视图中的途径，在此基础上，我们推导出一种新型的深度可分离图形卷积，它将现有的图形卷积方法归纳为我们表述的特例。实验表明，所提出的方法在多个领域的基准数据集上始终优于其他图形卷积和几何卷积基线。
用神经网络直接生成音乐音频是众所周知的困难，因为它需要在许多不同的时间尺度上对结构进行连贯建模。幸运的是，大多数音乐也是高度结构化的，可以表示为在乐器上演奏的离散音符事件。在这里，我们表明，通过使用音符作为中间表示，我们可以训练一套模型，能够在跨越六个数量级（〜0。 我们发布了新的MAESTRO（MIDI和音频编辑的同步TRACK和组织）数据集，该数据集由超过172小时的钢琴演奏组成，在音符标签和音频波形之间有很好的一致性（约3毫秒）。
基于chi-square分歧最小化（CHIVI）的变异推理提供了一种方法来近似模型的后验，同时获得边际似然的上限。然而，在实践中，CHIVI依赖于蒙特卡洛（MC）估计的上限目标，在适度的样本量中，不保证是边际似然的真实界限。 本文对CHIVI在一系列合成推理任务上的表现进行了实证研究。我们表明，CHIVI对初始化远比基于KL最小化的经典VI敏感，往往需要非常多的样本（超过一百万），而且可能不是一个可靠的上界。我们还提出了检测和缓解其中一些病态的可能方法，包括诊断性的边界和初始化策略。
人们已经广泛认识到，对抗性例子可以很容易地被制作来愚弄深度网络，这主要源于输入例子附近的局部非线性行为。在训练中应用混杂提供了一个有效的机制来提高泛化性能和模型对对抗性扰动的鲁棒性，这引入了训练例子之间的全局线性行为。然而，在以前的工作中，混杂训练的模型只是在推理中通过直接分类输入来被动地防御对抗性攻击，其中诱导的全局线性没有被很好地利用。 也就是说，由于对抗性扰动的局域性，通过模型预测的全局性来主动打破局域性会更有效。受简单的几何直觉启发，我们为混合训练的模型开发了一个推理原则，名为混合推理（MI）。 我们在CIFAR-10和CIFAR-100上的实验表明，MI可以进一步提高由mixup及其变体训练的模型的抗干扰性。
在特定领域的语料库上微调语言模型，如BERT，已被证明在科学论文和生物医学文本等领域很有价值。在本文中，我们表明，在法律文件上微调BERT同样为法律领域的NLP任务提供了有价值的改进。证明这一结果对分析商业协议很重要，因为由于其保密性质，获得大型法律语料库很有挑战性。
我们提出了一个用于无监督文本风格转换的深度生成模型，它统一了以前提出的非生成技术。我们的概率方法将来自两个领域的非平行数据建模为一个部分观察到的平行语料库。通过假设一个生成每个观察到的序列的平行潜在序列，我们的模型学习以完全无监督的方式将序列从一个领域转换到另一个。 与传统的生成序列模型（如HMM）相比，我们的模型对其生成的数据几乎不做任何假设：它使用一个循环语言模型作为先验，使用一个编码器-解码器作为转换分布。 此外，通过在我们的变异目标和其他最近的无监督风格转移和机器翻译技术之间建立联系，我们表明我们的概率观点如何能够统一一些已知的非生成目标，如反译和对抗性损失。最后，我们在广泛的无监督风格转移任务上证明了我们的方法的有效性，包括情感转移、形式转移、单词破译、作者模仿和相关语言翻译。 在所有的风格转换任务中，我们的方法比最先进的非生成基线产生了巨大的收益，包括我们的方法所概括的最先进的无监督机器翻译技术。此外，我们在一个标准的无监督机器翻译任务上进行了实验，发现我们的统一方法与当前的最先进技术相匹配。
目前机器学习的做法是在过度参数化的限制下采用深度网，参数的名义数量通常超过测量的数量。这类似于压缩传感或带有$l_1$惩罚项的稀疏回归中的情况，并为理解深度网背景下出现的现象提供了理论途径。 其中一个现象是深度网在训练误差为零的插值制度中成功地提供了良好的泛化。传统的统计实践要求进行正则化或平滑，以防止 "过拟合"（泛化性能差）。然而，最近的工作表明，存在着数据插值程序，它们在统计上是一致的，并提供良好的泛化性能。(在这种情况下，有人认为机器学习的 "经典 "和 "现代 "制度被泛化误差（"风险"）曲线中的一个峰值所区分，这种现象被称为 "双重下降"。虽然这种过拟合峰值确实存在，并且来自于条件不良的设计矩阵，但在这里我们挑战将过拟合峰值解释为划分在过度参数化下发生良好泛化的制度。我们提出了一个Misparamatrized Sparse Regression（MiSpaR）模型，并分析计算了$l_2$和$l_1$惩罚的GE曲线。 在所谓的 "热力学 "极限中得到了分析表达式。我们还发现了一个有趣的现象：在拟合模型中增加过度参数化会增加稀疏度，这在直觉上应该会改善$l_1$惩罚回归的性能。 然而，与此同时，与拟合参数的数量相比，测量的相对数量减少了，最终过度参数化确实导致了较差的泛化。这些结果为使用深度网等过度参数化的拟合函数研究插值制度中的反问题提供了理论途径。
基于散列的协同过滤学习用户和项目的二进制矢量表示（散列代码），这样就可以使用汉明距离非常有效地计算推荐，汉明距离只是两个散列代码之间不同位的总和。使用汉明距离的基于散列的协同过滤的一个问题是，每个位在距离计算中的权重相同，但实际上一些位可能比其他位编码更重要的属性，其中的重要性取决于用户的。为此，我们提出了一种端到端可训练的基于变异散列的协同过滤方法，该方法使用了自我掩码的新概念：用户散列代码作为项目的掩码（使用布尔和操作），这样它就学会了编码哪些比特对用户来说是重要的，而不是用户对比特所代表的基本项目属性的偏好。 我们在4个数据集上对我们的方法与最先进的基线进行了实验评估，并在NDCG中获得了高达12%的显著收益。我们还提供了自我掩码的有效实现，与标准汉明距离相比，实验中产生的运行时间开销<4%。
本文考虑了一种基于多臂匪徒的可调整大小的小批量梯度下降（RMGD）算法，该算法实现了与最佳固定批量大小相当的性能。在每个历时中，RMGD根据与批量成功降低损失函数成比例的某种概率分布来采样批量大小。 实验结果表明，RMGD实现的性能优于性能最好的单一批次大小，令人惊讶的是，RMGD实现了比网格搜索更好的性能。
近年来，知识图谱因其在众多任务中的成功应用而获得了越来越多的关注。尽管知识构建发展迅速，但知识图谱仍然存在着严重的不完整性，并不可避免地涉及各种错误。 在本文中，我们提出用一个统一的生成对抗网络（GAN）框架来联合这两项任务，以学习噪声感知的知识图谱嵌入。广泛的实验表明，我们的方法在知识图谱的完成和错误检测方面都优于现有的最先进算法。
基于能量的模型（EBM），又称未归一化模型，最近在连续空间中取得了成功，然而，它们还没有成功地应用于文本序列的建模。 虽然减少训练样本的能量是直接的，但挖掘应该增加能量的（负）样本是困难的。  部分原因是，当输入是高维和离散的时候，基于梯度的标准方法并不容易适用。 在这里，我们通过使用预先训练好的自动回归语言模型来生成负数，从而避开了这个问题。 然后，EBM在语言模型的{em residual}中工作；并被训练成从自动回归模型生成的文本中分辨出真实文本。我们研究了剩余EBM的泛化能力，这是将其用于其他应用的前提。 我们广泛地分析了分类输入是机器还是人类产生的任务的泛化能力，考虑到训练损失和我们如何挖掘底片，这是一项自然的任务。总的来说，我们观察到EBM可以很好地泛化产生底片的生成器的结构变化。然而，EBM对这些生成器使用的训练集表现出更大的敏感性。
半监督学习，即从有标签和无标签的样本中共同学习，是一个活跃的研究课题，因为它在放宽人类注释限制方面起着关键作用。在图像分类的背景下，最近从无标签样本中学习的进展主要集中在一致性正则化方法上，鼓励对无标签样本的不同扰动作出不变的预测。 我们表明，由于所谓的确认偏差，天真的伪标签会过度适应不正确的伪标签，并证明混杂增强和设置每个迷你批次的最小标签样本数是减少它的有效正则化技术。所提出的方法在CIFAR-10/100和Mini-ImageNet中取得了最先进的结果，尽管比其他最先进的方法简单得多。
无模型强化学习（RL）已被证明是学习复杂行为的一个强大的通用工具。然而，它的样本效率对于解决具有挑战性的现实世界问题来说往往是不切实际的，即使是Q-learning这样的非政策性算法。 经典的无模型RL的一个限制因素是，学习信号仅由标量奖励组成，忽略了状态转换图元中包含的许多丰富信息。基于模型的RL通过训练一个预测模型来使用这些信息，但由于模型的偏差，往往不能达到与无模型RL相同的渐进性能。 我们引入了时间差异模型（TDMs），这是一个目标条件值函数系列，可以用无模型学习来训练，并用于基于模型的控制。TDMs结合了无模型和基于模型的RL的优点：它们利用状态转换中的丰富信息来非常有效地学习，同时仍然获得超过直接基于模型的RL方法的渐进性能。我们的实验结果表明，在一系列的连续控制任务中，TDMs与最先进的基于模型和无模型方法相比，效率有了很大提高。
我们引入了一个神经架构，对两组物体的潜在随机排列进行摊销的近似贝叶斯推理。该方法涉及到使用最近对集合定义的函数的想法来近似配对概率矩阵的永久值。
机器学习的大规模检索系统需要大量代表查询项目相关性的训练数据。然而，收集用户的明确反馈是昂贵的。在本文中，我们建议利用用户日志和隐性反馈作为辅助目标来改善检索系统的相关性建模。 具体来说，我们采用了一个双塔神经网架构，在给定协作信息和内容信息的情况下，对查询-项目的相关性进行建模。通过引入用更丰富的隐性用户反馈数据训练的辅助任务，我们提高了查询和项目的学习表示的质量和分辨率。将这些学习表示应用于工业检索系统，带来了显著的改进。
自主探索和导航物理空间的能力是几乎所有移动自主代理的基本要求，从家用机器人吸尘器到自主车辆。传统的基于SLAM的探索和导航方法主要集中在利用场景的几何形状，但未能对动态物体（如其他代理）或语义约束（如潮湿的地板或门口）进行建模。基于学习的RL代理是一个有吸引力的选择，因为它们可以纳入语义和几何信息，但众所周知，样本效率低，难以推广到新环境，并且难以解释。 在本文中，我们用一种模块化的方法结合了这两个世界的优点，{自动学习}场景的空间表征，当与传统的几何规划器结合时，它被训练成有效的。具体来说，我们设计了一个代理，它可以学习预测空间承受力图，通过主动的自我监督的经验收集，阐明场景的哪些部分是可以浏览的。 与大多数假设静态世界的模拟环境相反，我们在VizDoom模拟器中评估了我们的方法，使用包含各种动态演员和危险的大规模随机生成的地图。我们表明，学习的承受力地图可以用来增强探索和导航的传统方法，在性能上有明显的改善。
