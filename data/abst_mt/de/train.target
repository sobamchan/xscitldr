Aufgrund des Erfolges von Deep Learning bei der Lösung einer Vielzahl von anspruchsvollen Aufgaben des maschinellen Lernens besteht ein steigendes Interesse daran, Verlustfunktionen für das Training neuronaler Netze aus theoretischer Sicht zu verstehen, insbesondere die Eigenschaften kritischer Punkte und die Landschaft um sie herum sind von Bedeutung, um die Konvergenzleistung von Optimierungsalgorithmen zu bestimmen. Wir zeigen, dass die analytischen Formen der kritischen Punkte die Werte der entsprechenden Verlustfunktionen sowie die notwendigen und hinreichenden Bedingungen für das Erreichen eines globalen Minimums charakterisieren und nutzen die analytischen Formen der kritischen Punkte, um die Landschaftseigenschaften für die Verlustfunktionen von linearen neuronalen Netzen und flachen ReLU-Netzen zu charakterisieren: Während die Verlustfunktion von linearen Netzen kein ungewolltes lokales Minimum hat, hat die Verlustfunktion von einschichtigen nichtlinearen Netzen mit ReLU-Aktivierungsfunktion ein lokales Minimum, das kein globales Minimum ist.
Einer der Hauptgründe dafür ist, dass BP symmetrische Gewichtsmatrizen in den Feedforward- und Feedback-Pfaden erfordert. Um dieses "Gewichtstransportproblem" (Grossberg, 1987) zu lösen, wurden zwei biologisch plausible Algorithmen von Liao et al. (2016) und Lillicrap et al. (Eine aktuelle Studie von Bartunov et al. (2018) zeigt jedoch, dass Feedback Alignment (FA) und einige Varianten von Target-Propagation (TP), obwohl sie bei MNIST und CIFAR gut abschneiden, bei ImageNet deutlich schlechter abschneiden als BP.Hier evaluieren wir zusätzlich den Sign-Symmetry (SS) Algorithmus (Liao et al, 2016), der sich von BP und FA dadurch unterscheidet, dass die Feedback- und Feedforward-Gewichte nicht die Größen, sondern die Vorzeichen teilen. Wir haben die Leistung der Vorzeichensymmetrie und des Feedback-Abgleichs auf ImageNet- und MS COCO-Datensätzen unter Verwendung verschiedener Netzwerkarchitekturen (ResNet-18 und AlexNet für ImageNet; RetinaNet für MS COCO) untersucht. Überraschenderweise können Netzwerke, die mit Vorzeichensymmetrie trainiert wurden, eine Klassifikationsleistung erreichen, die sich der von BP-trainierten Netzwerken annähert.Diese Ergebnisse ergänzen die Studie von Bartunov et al. (2018) und etablieren einen neuen Maßstab für zukünftige biologisch plausible Lernalgorithmen auf schwierigeren Datensätzen und komplexeren Architekturen.
Wir stellen den 2-simplicial Transformer vor, eine Erweiterung des Transformers, die eine Form der höherdimensionalen Aufmerksamkeit beinhaltet, die die Punktprodukt-Attention verallgemeinert, und diese Aufmerksamkeit verwendet, um Entitätsrepräsentationen mit Tensorprodukten von Wertvektoren zu aktualisieren.Wir zeigen, dass diese Architektur ein nützlicher induktiver Bias für logisches Denken im Kontext von Deep Reinforcement Learning ist.
Wir stellen Tensor-Train RNN (TT-RNN) vor, eine neuartige Familie von neuronalen Sequenzarchitekturen für multivariate Prognosen in Umgebungen mit nichtlinearer Dynamik.Langzeitprognosen in solchen Systemen sind sehr anspruchsvoll, da es langfristige zeitliche Abhängigkeiten, Korrelationen höherer Ordnung und Empfindlichkeit gegenüber Fehlerfortpflanzung gibt.Unsere vorgeschlagene tensor-rekurrente Architektur geht diese Probleme an, indem sie die nichtlineare Dynamik direkt mit Momenten höherer Ordnung und Zustandsübergangsfunktionen höherer Ordnung lernt. Darüber hinaus zerlegen wir die Struktur höherer Ordnung mit Hilfe der Tensor-Train (TT)-Zerlegung, um die Anzahl der Parameter zu reduzieren und gleichzeitig die Leistung des Modells zu erhalten. Wir etablieren theoretisch die Approximationseigenschaften von Tensor-Train RNNs für allgemeine Sequenzeingaben, und solche Garantien sind für gewöhnliche RNNs nicht verfügbar. Wir demonstrieren auch signifikante langfristige Vorhersageverbesserungen gegenüber allgemeinen RNN- und LSTM-Architekturen auf einer Reihe von simulierten Umgebungen mit nichtlinearer Dynamik, sowie auf realen Klima- und Verkehrsdaten.
Jüngste Bemühungen um die Kombination von tiefen Modellen mit probabilistischen grafischen Modellen sind vielversprechend, da sie flexible Modelle liefern, die auch leicht zu interpretieren sind.Wir schlagen einen Variational Message-Passing-Algorithmus für die Variationsinferenz in solchen Modellen vor.Wir leisten drei Beiträge.Erstens schlagen wir strukturierte Inferenznetzwerke vor, die die Struktur des grafischen Modells in das Inferenznetzwerk von Variational Auto-Encodern (VAE) einbeziehen. Schließlich leiten wir einen Variationsnachrichtenübergabealgorithmus ab, um eine effiziente Inferenz mit natürlichem Gradienten durchzuführen und gleichzeitig die Effizienz der amortisierten Inferenz beizubehalten.Durch die gleichzeitige Ermöglichung von strukturierter, amortisierter und natürlich-gradienter Inferenz für tiefe strukturierte Modelle vereinfacht und verallgemeinert unsere Methode bestehende Methoden.
Ein gängiger Ansatz zur Verringerung der Modellgröße und der Rechenkosten ist die Verwendung von Low-Rank-Faktorisierung zur Annäherung an eine Gewichtsmatrix. Allerdings kann die Durchführung einer standardmäßigen Low-Rank-Faktorisierung mit einem kleinen Rang die Ausdruckskraft des Modells beeinträchtigen und die Leistung erheblich verringern. In dieser Arbeit schlagen wir vor, eine Mischung aus mehreren Low-Rank-Faktorisierungen zu verwenden, um eine große Gewichtsmatrix zu modellieren, und die Mischungskoeffizienten werden dynamisch in Abhängigkeit von der Eingabe berechnet.Wir demonstrieren die Wirksamkeit des vorgeschlagenen Ansatzes sowohl bei der Sprachmodellierung als auch bei Bildklassifizierungsaufgaben.Experimente zeigen, dass unsere Methode nicht nur die Berechnungseffizienz verbessert, sondern auch die Genauigkeit im Vergleich zu den Full-Rank-Pendants beibehält (manchmal übertrifft).
Wir stellen eine Möglichkeit vor, den Overhead beim Abrufen und Transportieren von Trainingsdaten mit einer Methode dynamisch zu reduzieren, die wir als Progressive Compressed Records (PCRs) bezeichnen. PCRs weichen von früheren Formaten ab, indem sie eine progressive Kompression nutzen, um jedes Trainingsbeispiel in mehrere Beispiele mit zunehmend höherer Genauigkeit aufzuteilen, ohne die Gesamtdatengröße zu erhöhen. Wir zeigen, dass Modelle auf aggressiv komprimierten Repräsentationen der Trainingsdaten trainiert werden können und dennoch eine hohe Genauigkeit beibehalten, und dass PCRs im Durchschnitt eine zweifache Beschleunigung im Vergleich zu Basisformaten mit JPEG-Kompression ermöglichen.Unsere Ergebnisse gelten für alle Deep-Learning-Architekturen und eine Vielzahl von Datensätzen: ImageNet, HAM10000, Stanford Cars und CelebA-HQ.
Es ist eine fundamentale und herausfordernde Aufgabe, robuste und genaue Deep Neural Networks (DNNs) zu trainieren, wenn semantisch abnormale Beispiele existieren.Obwohl große Fortschritte gemacht wurden, gibt es immer noch eine entscheidende Forschungsfrage, die noch nicht gründlich erforscht ist: In dieser Arbeit untersuchen wir diese Frage und schlagen Gradienten-Reskalierung (GR) vor, um sie zu lösen. GR verändert die Größe des Gradienten des Logit-Vektors, um relativ einfache Trainingsdatenpunkte zu betonen, wenn das Rauschen stärker wird, was als explizite Regularisierung der Betonung funktioniert, um die Generalisierungsleistung von DNNs zu verbessern. Abgesehen von der Regularisierung verbinden wir GR mit der Gewichtung von Beispielen und dem Entwurf robuster Verlustfunktionen, wobei wir empirisch zeigen, dass GR sehr anomalienrobust ist und den Stand der Technik deutlich übertrifft, z.B, Wir zeigen empirisch, dass GR hochgradig anomalienrobust ist und den Stand der Technik um 7% übertrifft, z.B. bei CIFAR100 mit 40% verrauschten Labels, und dass es Standardregularisierern sowohl in reinen als auch in anomalen Situationen signifikant überlegen ist.Darüber hinaus präsentieren wir umfassende Ablationsstudien, um das Verhalten von GR in verschiedenen Fällen zu untersuchen, was für die Anwendung von GR in realen Szenarien aufschlussreich ist.
Generative Adversarial Networks (GANs) haben bemerkenswerte Ergebnisse bei der Aufgabe erzielt, realistische natürliche Bilder zu generieren.GAN-Modelle haben in den meisten Anwendungen zwei Aspekte gemeinsam.Einerseits beinhaltet das Training von GANs die Lösung eines anspruchsvollen Sattelpunkt-Optimierungsproblems, das als adversariales Spiel zwischen einer Generator- und einer Diskriminatorfunktion interpretiert wird.Andererseits werden der Generator und der Diskriminator in Form von tiefen Faltungsneuronalen Netzen parametrisiert. Das Ziel dieses Papiers ist es, den Beitrag dieser beiden Faktoren zum Erfolg von GANs zu entflechten. Insbesondere stellen wir die Generative Latent Optimization (GLO) vor, einen Rahmen, um tiefe Faltungsgeneratoren ohne Diskriminatoren zu trainieren und so die Instabilität von kontradiktorischen Optimierungsproblemen zu vermeiden. In einer Vielzahl von Experimenten zeigen wir, dass GLO viele der wünschenswerten Eigenschaften von GANs besitzt: Lernen aus großen Datenmengen, Synthetisierung visuell ansprechender Muster, sinnvolle Interpolation zwischen Mustern und lineare Arithmetik mit Rauschvektoren.
In diesem Papier schlagen wir eine neue Art von Kernel vor, den Random-Forest-Kernel, um die empirische Leistung von MMD-GANs zu verbessern. Im Gegensatz zu gewöhnlichen Wäldern mit deterministischen Routings wird in unserem innovativen Random-Forest-Kernel eine probabilistische Routing-Variante verwendet, die mit den CNN-Frameworks zusammengeführt werden kann.Unser vorgeschlagener Random-Forest-Kernel hat die folgenden Vorteile: Aus der Perspektive des Random-Forest kann die Ausgabe des GAN-Diskriminators als Feature-Inputs für den Forest betrachtet werden, wobei jeder Baum nur Zugang zu einem Bruchteil der Features erhält und somit der gesamte Forest vom Ensemble-Lernen profitiert.Im Hinblick auf die Kernel-Methode erweist sich der Random-Forest-Kernel als charakteristisch und daher für die MMD-Struktur geeignet. Da es sich um einen asymmetrischen Kernel handelt, ist unser Random-Forest-Kernel viel flexibler, was die Erfassung der Unterschiede zwischen den Verteilungen angeht. Durch die gemeinsame Nutzung der Vorteile von CNN, Kernel-Methode und Ensemble-Lernen erzielt unser Random-Forest-Kernel-basiertes MMD-GAN wünschenswerte empirische Leistungen auf CIFAR-10-, CelebA- und LSUN-Schlafzimmer-Datensätzen.
Die Kombination von Funktionsannäherung, temporalem Differenzlernen (TD) und Off-Policy-Training kann jedoch zu einer Überschätzung der Wertfunktion führen. Eine Lösung ist die Verwendung von Clipped Double Q-learning (CDQ), das im TD3-Algorithmus verwendet wird und das Minimum von zwei Kritiken im TD-Target berechnet. Wir zeigen, dass CDQ zu einer Unterschätzung führt und schlagen einen neuen Algorithmus vor, der dies berücksichtigt, indem er einen gewichteten Durchschnitt des Ziels von CDQ und des Ziels, das von einem einzelnen Kritiker stammt, verwendet.Der Gewichtungsparameter wird während des Trainings so angepasst, dass die Wertschätzungen mit der tatsächlichen diskontierten Rendite der letzten Episoden übereinstimmen und dadurch Über- und Unterschätzung ausgleichen.Empirisch gesehen erhalten wir genauere Wertschätzungen und demonstrieren Ergebnisse auf dem neuesten Stand der Technik bei mehreren OpenAI-Gym-Aufgaben.
Als Teil dieses Rahmens konstruieren wir ImageNet-Vid-Robust, einen von menschlichen Experten begutachteten Datensatz von 22.668 Bildern, die in 1.145 Sätze von wahrnehmungsmäßig ähnlichen Bildern gruppiert sind, die aus Frames im ImageNet Video Object Detection-Datensatz abgeleitet sind. Wir evaluieren eine Reihe von Klassifizierern, die auf ImageNet trainiert wurden, einschließlich Modellen, die für Robustheit trainiert wurden, und zeigen einen mittleren Rückgang der Klassifizierungsgenauigkeit von 16%. Darüber hinaus evaluieren wir die Faster R-CNN- und R-FCN-Modelle für die Erkennung und zeigen, dass natürliche Störungen sowohl Klassifizierungs- als auch Lokalisierungsfehler verursachen, was im Median zu einem Rückgang der Erkennungsgenauigkeit um 14 Punkte führt.Unsere Analyse zeigt, dass natürliche Störungen in der realen Welt für aktuelle CNNs äußerst problematisch sind und eine erhebliche Herausforderung für ihren Einsatz in sicherheitskritischen Umgebungen darstellen, die zuverlässige Vorhersagen mit geringer Latenz erfordern.
Gradient Boosting Trees, Support Vector Machine, Random Forest und Logistic Regression werden typischerweise für Klassifizierungsaufgaben auf tabellarischen Daten verwendet. Die jüngste Arbeit der Super Characters-Methode, die zweidimensionale Worteinbettung verwendet, erzielte Spitzenergebnisse bei Textklassifizierungsaufgaben und zeigt das Potenzial dieses neuen Ansatzes. In diesem Papier schlagen wir die SuperTML-Methode vor, die die Idee der Super Characters-Methode und der zweidimensionalen Einbettung aufgreift, um das Problem der Klassifizierung von Tabellendaten anzugehen. Für jede Eingabe von Tabellendaten werden die Merkmale zunächst in eine zweidimensionale Einbettung wie ein Bild projiziert, und dann wird dieses Bild in fein abgestimmte ImageNet CNN-Modelle für die Klassifizierung eingespeist.Experimentelle Ergebnisse haben gezeigt, dass die vorgeschlagene SuperTML-Methode State-of-the-Art-Ergebnisse sowohl auf großen als auch auf kleinen Datensätzen erzielt hat.
Generative Pre-Training war bisher nicht so erfolgreich wie kontrastive Methoden bei der Modellierung von Repräsentationen von Rohbildern.In diesem Papier schlagen wir eine neuronale Architektur für selbstüberwachtes Repräsentationslernen auf Rohbildern mit dem Namen PatchFormer vor, die lernt, räumliche Abhängigkeiten über Patches in einem Rohbild zu modellieren.Unsere Methode lernt, die bedingte Wahrscheinlichkeitsverteilung von fehlenden Patches angesichts des Kontexts der umgebenden Patches zu modellieren. Wir evaluieren den Nutzen der erlernten Repräsentationen durch Feinabstimmung des vortrainierten Modells auf Klassifizierungsaufgaben mit geringem Datenumfang, insbesondere auf der halbüberwachten ImageNet-Klassifizierung, die in letzter Zeit zu einem beliebten Benchmark für halbüberwachte und selbstüberwachte Lernmethoden geworden ist. 30,3 % und 65,5 % Top-1-Genauigkeit erreicht unser Modell, wenn es mit nur 1 % bzw. 10 % der ImageNet-Etiketten trainiert wird, was ein vielversprechendes Beispiel für generative Vortrainingsmethoden ist.
Adaptive Regularisierungsmethoden multiplizieren eine Abstiegsrichtung mit einer Vorkonditionierungsmatrix. Aufgrund der großen Anzahl von Parametern bei Problemen des maschinellen Lernens sind Vollmatrix-Vorkonditionierungsmethoden unerschwinglich teuer. Wir zeigen, wie die adaptive Vollmatrix-Regularisierung modifiziert werden kann, um sie praktisch und effektiv zu machen. Der Kern unseres Algorithmus, genannt GGT, besteht aus einer effizienten inversen Berechnung von Quadratwurzeln von Matrizen mit niedrigem Rang. Unsere vorläufigen Experimente unterstreichen die verbesserte Konvergenzrate von GGT bei einer Vielzahl von synthetischen Aufgaben und Standard-Benchmarks für Deep Learning.
Dialogsysteme erfordern eine Vielzahl von unterschiedlichen, aber komplementären Fachkenntnissen, um Menschen zu unterstützen, zu informieren und zu unterhalten, z.B. in verschiedenen Bereichen (z.B., In diesem Papier schlagen wir vor, ein Dialogsystem zu lernen, das unabhängig verschiedene Dialogfähigkeiten parametrisiert und lernt, jede von ihnen durch Attention over Parameters (AoP) auszuwählen und zu kombinieren.Die experimentellen Ergebnisse zeigen, dass dieser Ansatz eine konkurrenzfähige Leistung auf einem kombinierten Datensatz von MultiWOZ (Budzianowski et al., 2018), In-Car Assistant (Eric et al., 2017), und Persona-Chat (Zhang et al., 2018).Schließlich zeigen wir, dass jede Dialogfähigkeit effektiv erlernt wird und mit anderen Fähigkeiten kombiniert werden kann, um selektive Antworten zu erzeugen.
Die Modelldestillation zielt darauf ab, das Wissen eines komplexen Modells in ein einfacheres Modell zu destillieren. In dieser Arbeit betrachten wir eine alternative Formulierung, die wir als Datensatzdestillation bezeichnen: Wir halten das Modell fest und versuchen stattdessen, das Wissen aus einem großen Trainingsdatensatz in einen kleinen zu destillieren. Die Idee besteht darin, eine kleine Anzahl von Datenpunkten zu synthetisieren, die nicht aus der korrekten Datenverteilung stammen müssen, sondern, wenn sie dem Lernalgorithmus als Trainingsdaten gegeben werden, das auf den Originaldaten trainierte Modell annähern. Wir zeigen zum Beispiel, dass es möglich ist, 60.000 MNIST-Trainingsbilder in nur 10 synthetische destillierte Bilder (eines pro Klasse) zu komprimieren und bei einer festen Netzwerkinitialisierung annähernd die ursprüngliche Leistung zu erreichen.Wir bewerten unsere Methode in verschiedenen Initialisierungseinstellungen.  Experimente mit mehreren Datensätzen, MNIST, CIFAR10, PASCAL-VOC und CUB-200, zeigen den Vorteil unseres Ansatzes im Vergleich zu alternativen Methoden.  Wir zeigen, dass die Speicherung von destillierten Bildern als episodische Erinnerung an frühere Aufgaben das Vergessen effektiver verhindern kann als echte Bilder.
Wir beziehen das Minimax-Spiel von generativen adversen Netzen (GANs) auf die Suche nach den Sattelpunkten der Lagrangeschen Funktion für ein konvexes Optimierungsproblem, wobei die Diskriminatorausgänge und die Verteilung der Generatorausgänge die Rolle von primären bzw. dualen Variablen spielen.Diese Formulierung zeigt die Verbindung zwischen dem Standard-GAN-Trainingsprozess und den primär-dualen Subgradientenmethoden für konvexe Optimierung.Die inhärente Verbindung liefert nicht nur einen theoretischen Konvergenzbeweis für das Training von GANs im Funktionsraum, sondern inspiriert auch eine neuartige Zielfunktion für das Training. Die modifizierte Zielfunktion erzwingt die Aktualisierung der Verteilung der Generatorausgaben entlang der Richtung gemäß den primär-dualen Subgradientenmethoden.Ein Spielzeugbeispiel zeigt, dass die vorgeschlagene Methode in der Lage ist, den Mode-Kollaps aufzulösen, der in diesem Fall nicht durch das Standard-GAN oder das Wasserstein-GAN vermieden werden kann.Experimente sowohl mit synthetischen Gauß'schen Mischdaten als auch mit realen Bilddatensätzen zeigen die Leistung der vorgeschlagenen Methode bei der Generierung verschiedener Proben.
Die Festlegung von Belohnungsfunktionen ist schwierig, was den Bereich der Belohnungsinferenz motiviert: das Lernen von Belohnungen aus menschlichem Verhalten. Die Ausgangsannahme in diesem Bereich ist, dass menschliches Verhalten angesichts der gewünschten Belohnungsfunktion optimal ist, aber in der Realität haben Menschen viele verschiedene Formen von Irrationalität, von Rauschen über Kurzsichtigkeit bis hin zu Risikoaversion und mehr. Diese Tatsache scheint für die Ableitung von Belohnungen äußerst schädlich zu sein: Es ist bereits schwierig, aus rationalem Verhalten auf die Belohnung zu schließen, und Rauschen und systematische Verzerrungen führen dazu, dass Handlungen weniger direkt mit der Belohnung zusammenhängen. Bei einigen Arten und Mengen von Irrationalität produziert der Experte nun im Vergleich zu rationalem Verhalten vielfältigere Strategien, die dabei helfen, zwischen verschiedenen Belohnungsparametern zu unterscheiden, die ansonsten demselben rationalen Verhalten entsprechen. Wir beginnen damit, den Raum der Irrationalitäten als Abweichungen von der Bellman-Aktualisierung zu erfassen, simulieren das Verhalten von Experten und messen die Genauigkeit der Schlussfolgerungen, um die verschiedenen Typen zu kontrastieren und die Gewinne und Verluste zu untersuchen.wir liefern eine auf gegenseitiger Information basierende Analyse unserer Ergebnisse und schließen mit einer Diskussion über die Notwendigkeit, Irrationalität genau zu modellieren, sowie über die Frage, inwieweit wir erwarten können (oder in der Lage sind, sie zu trainieren), dass echte Menschen hilfreiche Irrationalitäten zeigen, wenn sie Lernenden Belohnungen beibringen.
In diesem Beitrag stellen wir WildNLP vor - ein Framework zum Testen der Modellstabilität in einer natürlichen Umgebung, in der Textstörungen wie Tastaturfehler oder Rechtschreibfehler auftreten. Wir vergleichen die Robustheit von Modellen aus 4 beliebten NLP-Aufgaben: Wir vergleichen die Robustheit von Modellen aus vier populären NLP-Aufgaben: Q&A, NLI, NER und Sentiment Analysis, indem wir ihre Leistung anhand von Aspekten testen, die im Rahmen des Frameworks eingeführt wurden, wobei wir uns insbesondere auf einen Vergleich zwischen aktuellen Textrepräsentationen und nicht kontextualisierten Worteinbettungen konzentrieren. Um die Robustheit zu verbessern, führen wir adversariales Training auf ausgewählten Aspekten durch und prüfen die Übertragbarkeit auf die Verbesserung von Modellen mit verschiedenen Korruptionsarten. Wir stellen fest, dass die hohe Leistung von Modellen keine ausreichende Robustheit gewährleistet, obwohl moderne Einbettungstechniken helfen, diese zu verbessern.
Das Training von generativen Modellen wie Generative Adversarial Network (GAN) ist eine Herausforderung für verrauschte Daten.Ein neuartiger Curriculum-Lernalgorithmus, der sich auf Clustering bezieht, wird vorgeschlagen, um dieses Problem in diesem Papier anzugehen.Der Curriculum-Aufbau basiert auf der Zentralität der zugrunde liegenden Cluster in den Datenpunkten.  Um unseren Algorithmus für große Datenmengen skalierbar zu machen, wird die aktive Menge in dem Sinne entwickelt, dass jede Trainingsrunde nur auf einer aktiven Teilmenge stattfindet, die einen kleinen Teil der bereits trainierten Daten und die inkrementellen Daten mit niedrigerer Zentralität enthält. Die Experimente mit Katzen- und Menschengesichtsdaten zeigen, dass unser Algorithmus in der Lage ist, optimale generative Modelle (z.B. ProGAN) in Bezug auf bestimmte Qualitätsmetriken für verrauschte Daten zu erlernen, und dass der optimale Cluster-Lehrplan eng mit dem kritischen Punkt des geometrischen Perkolationsprozesses zusammenhängt, der in dem Papier formuliert wurde.
Backdoor-Angriffe zielen darauf ab, eine Teilmenge von Trainingsdaten zu manipulieren, indem gegnerische Auslöser eingeschleust werden, so dass maschinelle Lernmodelle, die auf dem manipulierten Datensatz trainiert wurden, willkürlich (gezielt) falsche Vorhersagen auf dem Testset mit demselben eingebetteten Auslöser machen.Während föderiertes Lernen (FL) in der Lage ist, Informationen, die von verschiedenen Parteien bereitgestellt werden, zu aggregieren, um ein besseres Modell zu trainieren, können die verteilte Lernmethodik und die inhärent heterogene Datenverteilung zwischen den Parteien neue Schwachstellen mit sich bringen. Zusätzlich zu den jüngsten zentralisierten Backdoor-Angriffen auf FL, bei denen jede Partei denselben globalen Auslöser während des Trainings einbettet, schlagen wir den verteilten Backdoor-Angriff (DBA) vor - ein neuartiges Rahmenwerk zur Bedrohungsanalyse, das unter vollständiger Ausnutzung der verteilten Natur von FL entwickelt wurde.DBA zerlegt ein globales Auslösemuster in separate lokale Muster und bettet sie jeweils in die Trainingsmenge verschiedener gegnerischer Parteien ein. Wir zeigen, dass DBA im Vergleich zu zentralisierten Standard-Backdoors wesentlich hartnäckiger und unauffälliger gegen FL in verschiedenen Datensätzen wie Finanz- und Bilddaten ist, und führen umfangreiche Experimente durch, um zu zeigen, dass die Erfolgsrate von DBA unter verschiedenen Bedingungen deutlich höher ist als die von zentralisierten Backdoors, und dass verteilte Angriffe in der Tat heimtückischer sind, da DBA zwei moderne robuste FL-Algorithmen gegen zentralisierte Backdoors umgehen kann. Um die Eigenschaften von DBA weiter zu erforschen, testen wir die Angriffsleistung, indem wir verschiedene Auslösefaktoren variieren, einschließlich lokaler Auslösevariationen (Größe, Lücke und Ort), Skalierungsfaktor in FL, Datenverteilung sowie Giftverhältnis und -intervall.Unsere vorgeschlagene DBA und die gründlichen Bewertungsergebnisse werfen ein Licht auf die Charakterisierung der Robustheit von FL.
Diese Methoden arbeiten in der Regel durch die Erzeugung von Knoten Repräsentationen, die in einem bestimmten gewichteten Graphen propagiert werden.Hier argumentieren wir, dass für semi-supervised Lernen, ist es natürlicher, um die Ausbreitung Etiketten in der graph.Towards diesem Zweck schlagen wir eine differenzierbare neuronale Version des klassischen Label Propagation (LP) Algorithmus. Ausgehend von einer Schicht, die eine einzige Iteration des LP-Algorithmus implementiert, fügen wir mehrere wichtige nicht-lineare Schritte hinzu, die den Mechanismus der Label-Propagation erheblich verbessern.Experimente in zwei verschiedenen Umgebungen zeigen den Nutzen unseres Ansatzes.
Die Suche nach neuronalen Architekturen (NAS) hat im Bereich der Computervision rasante Fortschritte gemacht, wobei in einer Vielzahl von Aufgaben mit automatisch gesuchten neuronalen Netzwerkarchitekturen (NN) neue State-of-the-Art-Ergebnisse erzielt wurden. Entsprechend der Encoder-Aggregator-Meta-Architektur typischer neuronaler Netzmodelle für NLU-Aufgaben (Gong et al. 2018) re-deﬁnieren wir den Suchraum, indem wir ihn in zwei Teile aufteilen: den Encoder-Suchraum und den Aggregator-Suchraum.Der Encoder-Suchraum enthält grundlegende Operationen wie Faltungen, RNNs, Multi-Head-Attention und ihre spärlichen Varianten, Sterntransformatoren. Unser Suchalgorithmus wird dann über DARTS, ein differenzierbares neuronales Architektur-Such-Framework, ausgefüllt.Wir reduzieren den Suchraum schrittweise alle paar Epochen, was die Suchzeit und die Ressourcenkosten weiter reduziert.Experimente an fünf Benchmark-Datensätzen zeigen, dass die neuen neuronalen Netze, die wir generieren, eine Leistung erreichen können, die mit den modernsten Modellen vergleichbar ist, die kein Sprachmodell-Vortraining beinhalten.
Netzwerkeinbettungsmethoden (NE) zielen darauf ab, niedrigdimensionale Repräsentationen von Netzwerkknoten als Vektoren zu erlernen, typischerweise im euklidischen Raum, und diese Repräsentationen dann für eine Vielzahl von nachgelagerten Vorhersageaufgaben zu verwenden. Die Komplexität der Linkvorhersage erfordert jedoch eine sorgfältig konzipierte Evaluierungspipeline, um konsistente, reproduzierbare und vergleichbare Ergebnisse zu liefern.Wir argumentieren, dass dies in den jüngsten Arbeiten nicht ausreichend berücksichtigt wurde.Das Hauptziel dieses Beitrags ist es, die mit Evaluierungspipelines und der Reproduzierbarkeit der Ergebnisse verbundenen Schwierigkeiten zu überwinden. Wir stellen EvalNE vor, ein Evaluations-Framework zur transparenten Bewertung und zum Vergleich der Leistung von NE-Methoden bei der Link-Vorhersage.EvalNE bietet Automatisierung und Abstraktion für Aufgaben wie die Abstimmung von Hyperparametern, Modellvalidierung, Edge Sampling, Berechnung von Edge Embeddings und Modellvalidierung. Das Framework integriert effiziente Verfahren für Edge- und Non-Edge-Sampling und kann dazu verwendet werden, jede beliebige Standard-Embedding-Methode zu evaluieren.EvalNE ist als Python-Toolbox frei verfügbar.Um die Nützlichkeit von EvalNE in der Praxis zu demonstrieren, führen wir eine empirische Studie durch, in der wir versuchen, experimentelle Abschnitte mehrerer einflussreicher Arbeiten zu replizieren und zu analysieren.
Deep-Learning-Modelle können effizient über stochastische Gradientenabstieg optimiert werden, aber es gibt wenig theoretische Beweise, um dies zu unterstützen.Eine wichtige Frage in der Optimierung ist zu verstehen, wenn die Optimierung Landschaft eines neuronalen Netzes ist zugänglich für Gradienten-basierte Optimierung.Wir konzentrieren uns auf ein einfaches neuronales Netz zweischichtigen ReLU Netzwerk mit zwei versteckten Einheiten, und zeigen, dass alle lokalen Minimierer sind global.Dies kombiniert mit jüngsten Arbeiten von Lee et al. (2017); Lee et al. (2016) zeigen, dass Gradientenabstieg konvergiert zum globalen Minimierer.
Dropout ist eine einfache, aber effektive Technik zur Verbesserung der Generalisierungsleistung und zur Vermeidung von Überanpassung in tiefen neuronalen Netzen (DNNs).In diesem Papier diskutieren wir drei neue Beobachtungen über Dropout, um die Generalisierung von DNNs mit gleichgerichteten linearen Aktivierungen (ReLU) besser zu verstehen: 1) Dropout ist eine Glättungsmethode, die jedes lokale lineare Modell eines DNN dazu ermutigt, auf Datenpunkten aus nahegelegenen Regionen trainiert zu werden; 2) eine konstante Dropout-Rate kann zu effektiven neuronalen Deaktivierungsraten führen, die für Schichten mit unterschiedlichen Anteilen aktivierter Neuronen signifikant unterschiedlich sind; und 3) der Reskalierungsfaktor von Dropout verursacht eine Inkonsistenz zwischen der Normalisierung während der Trainings- und Testbedingungen, wenn auch eine Batch-Normalisierung verwendet wird.  Die obigen Ausführungen führen zu drei einfachen, aber nicht trivialen Verbesserungen der Dropout-Methode, die in unserer vorgeschlagenen Methode "Jumpout" resultieren: "Jumpout" tastet die Dropout-Rate anhand einer monoton abnehmenden Verteilung ab (z. B. dem rechten Teil eines abgeschnittenen Gauß), so dass das lokale lineare Modell an jedem Datenpunkt mit hoher Wahrscheinlichkeit so trainiert wird, dass es für Datenpunkte aus nahe gelegenen Regionen besser funktioniert als für solche aus weiter entfernten Regionen. Anstatt eine Dropout-Rate für jede Schicht einzustellen und sie auf alle Proben anzuwenden, normalisiert Jumpout außerdem adaptiv die Dropout-Rate auf jeder Schicht und jeder Trainingsprobe/jedem Trainingsstapel, so dass die effektive Dropout-Rate, die auf die aktivierten Neuronen angewendet wird, gleich bleibt. Darüber hinaus skalieren wir die Ausgaben von jumpout neu, um einen besseren Kompromiss zu erreichen, der sowohl die Varianz als auch den Mittelwert der Neuronen zwischen Trainings- und Testphasen konsistent hält, was die Inkompatibilität zwischen Dropout und Batch-Normalisierung abschwächt. jumpout zeigt im Vergleich zum ursprünglichen Dropout eine deutlich verbesserte Leistung bei CIFAR10, CIFAR100, Fashion-MNIST, STL10, SVHN, ImageNet-1k, etc, bei gleichzeitig vernachlässigbaren zusätzlichen Speicher- und Rechenkosten.
Bedenken hinsichtlich der Interpretierbarkeit, der Rechenressourcen und der prinzipiellen induktiven Prioritäten haben zu Bemühungen geführt, spärliche neuronale Modelle für NLP-Aufgaben zu entwickeln. Unter Verwendung der Taxi-Euklidischen Norm zur Messung der Sparsamkeit stellen wir fest, dass häufige Eingabewörter mit konzentrierten oder spärlichen Aktivierungen verbunden sind, während häufige Zielwörter mit zerstreuten Aktivierungen, aber konzentrierten Gradienten verbunden sind.Wir stellen fest, dass Gradienten, die mit Funktionswörtern verbunden sind, konzentrierter sind als die Gradienten von Inhaltswörtern, selbst wenn man die Worthäufigkeit kontrolliert.
Die Integration einer Wissensdatenbank (Knowledge Base, KB) in einen neuronalen Dialogagenten ist eine der wichtigsten Herausforderungen in der Konversations-KI. Gedächtnisnetzwerke haben sich als effektiv erwiesen, um KB-Informationen in einem externen Speicher zu kodieren und so flüssigere und informiertere Antworten zu generieren. Leider wird ein solcher Speicher während des Trainings mit latenten Repräsentationen gefüllt, so dass die gängigste Strategie darin besteht, alte Speichereinträge zufällig zu überschreiben. In dieser Arbeit stellen wir diesen Ansatz in Frage und liefern experimentelle Beweise, die zeigen, dass konventionelle Speichernetzwerke viele redundante latente Vektoren erzeugen, was zu einer Überanpassung und der Notwendigkeit größerer Speicher führt.Wir führen Memory Dropout als eine automatische Technik ein, die die Vielfalt im latenten Raum fördert, indem1) redundante Speicher gealtert werden, um ihre Wahrscheinlichkeit zu erhöhen, während des Trainings überschrieben zu werden2) neue Speicher abgetastet werden, die das durch redundante Speicher erworbene Wissen zusammenfassen. Diese Technik ermöglicht es uns, Wissensdatenbanken einzubinden, um eine hochmoderne Dialoggenerierung im Stanford Multi-Turn-Dialogdatensatz zu erreichen. Bei gleicher Architektur führt ihre Verwendung zu einer Verbesserung von +2,2 BLEU-Punkten bei der automatischen Generierung von Antworten und zu einer Steigerung von +8,1% bei der Erkennung von benannten Entitäten.
Su-Boyd-Candes (2014) stellte eine Verbindung zwischen der Nesterov-Methode und einer gewöhnlichen Differentialgleichung (ODE) her.  Wir zeigen, dass, wenn ein hessischer Dämpfungsterm zu der ODE von Su-Boyd-Candes (2014) hinzugefügt wird, die Nesterov-Methode als eine einfache Diskretisierung der modifizierten ODE entsteht.   Trotz des hessischen Terms können beide ODEs zweiter Ordnung als Systeme erster Ordnung dargestellt werden. Die etablierte Liapunov-Analyse wird verwendet, um die beschleunigten Konvergenzraten sowohl in kontinuierlicher als auch in diskreter Zeit zu ermitteln.  Darüber hinaus kann die Liapunov-Analyse auf den Fall stochastischer Gradienten ausgedehnt werden, so dass der Fall der vollständigen Gradienten als Spezialfall des stochastischen Falls betrachtet werden kann.  Das Ergebnis ist ein einheitlicher Ansatz zur konvexen Beschleunigung sowohl in kontinuierlicher als auch in diskreter Zeit und sowohl im stochastischen als auch im Vollgradientenfall. 
Wir schlagen das Transfer-Lernen (L2TL) vor, um das Transfer-Lernen auf einem Ziel-Datensatz durch die gezielte Extraktion von Informationen aus einem Quell-Datensatz zu verbessern. L2TL berücksichtigt die gemeinsame Optimierung von stark geteilten Gewichten zwischen Modellen für Quell- und Zielaufgaben und verwendet adaptive Gewichte für die Skalierung der konstituierenden Verluste. Die Anpassung der Gewichte basiert auf Reinforcement Learning, das mit einer Leistungsmetrik auf dem Zielvalidierungsset geführt wird. L2TL zeigt die modernste Leistung von L2TL bei festen Modellen und übertrifft die Feinabstimmung der Basislinien auf verschiedenen Datensätzen. Bei kleinen Zieldatensätzen und signifikantem Label-Mismatch zwischen Quell- und Zieldatensätzen übertrifft L2TL frühere Arbeiten sogar noch um einiges.
In vielen teilweise beobachtbaren Szenarien müssen sich Agenten des Reinforcement Learning (RL) auf das Langzeitgedächtnis verlassen, um eine optimale Strategie zu erlernen. wir zeigen, dass die Verwendung von Techniken aus dem NLP und dem überwachten Lernen bei RL-Aufgaben aufgrund der Stochastizität der Umgebung und der Exploration versagt. wir nutzen unsere Erkenntnisse über die Grenzen traditioneller Speichermethoden im RL und schlagen AMRL vor, eine Klasse von Modellen, die bessere Strategien mit größerer Stichprobeneffizienz erlernen können und resistent gegenüber verrauschten Eingaben sind. Wir zeigen, dass dies sowohl in Bezug auf den Gradientenabfall als auch auf das Signal-Rausch-Verhältnis im Laufe der Zeit Vorteile bietet. Bei der Evaluierung in Minecraft- und Labyrinth-Umgebungen, die das Langzeitgedächtnis testen, stellen wir fest, dass unser Modell die durchschnittliche Rendite um 19 % gegenüber einer Basislinie mit der gleichen Anzahl von Parametern und um 9 % gegenüber einer stärkeren Basislinie mit weitaus mehr Parametern verbessert.
Die meisten früheren Arbeiten konzentrieren sich auf den Fall mit einer einzigen Mannigfaltigkeit, aber in der Praxis ist es ziemlich häufig, dass das Optimierungsproblem mehr als eine Einschränkung beinhaltet (jede Einschränkung entspricht einer Mannigfaltigkeit). Es ist im Allgemeinen nicht klar, wie man auf mehreren Mannigfaltigkeiten effektiv und beweisbar optimieren kann, besonders wenn der Schnittpunkt mehrerer Mannigfaltigkeiten keine Mannigfaltigkeit ist oder nicht leicht berechnet werden kann. Wir schlagen einen einheitlichen Algorithmus vor, um die Optimierung auf mehreren Mannigfaltigkeiten zu handhaben, insbesondere integrieren wir Informationen von mehreren Mannigfaltigkeiten und bewegen uns entlang einer Ensemble-Richtung, indem wir die Informationen von jeder Mannigfaltigkeit als Drift betrachten und sie zusammen addieren, und wir beweisen die Konvergenzeigenschaften der vorgeschlagenen Algorithmen.
Es wurde lange Zeit angenommen, dass hochdimensionale kontinuierliche Steuerungsprobleme nicht effektiv durch Diskretisierung einzelner Dimensionen des Aktionsraums gelöst werden können, da die Anzahl der Bins, über die Strategien gelernt werden müssten, exponentiell groß ist.In diesem Beitrag lassen wir uns von dem jüngsten Erfolg von Sequenz-zu-Sequenz-Modellen für strukturierte Vorhersageprobleme inspirieren, um Strategien über diskretisierte Räume zu entwickeln.Im Mittelpunkt dieser Methode steht die Erkenntnis, dass komplexe Funktionen über hochdimensionale Räume durch neuronale Netze modelliert werden können, die eine Dimension nach der anderen vorhersagen. Konkret zeigen wir, wie Q-Werte und Richtlinien über kontinuierlichen Räumen mit Hilfe eines Next-Step-Vorhersagemodells über diskretisierten Dimensionen modelliert werden können. Mit dieser Parametrisierung ist es möglich, sowohl die kompositorische Struktur von Aktionsräumen während des Lernens zu nutzen als auch Maxima über Aktionsräume (näherungsweise) zu berechnen. Anhand einer einfachen Beispielaufgabe zeigen wir empirisch, dass unsere Methode eine globale Suche durchführen kann, wodurch die lokalen Optimierungsprobleme, die DDPG plagen, effektiv umgangen werden.
Modellbasiertes Verstärkungslernen (MBRL) zielt darauf ab, ein dynamisches Modell zu erlernen, um die Anzahl der Interaktionen mit realen Umgebungen zu reduzieren. Aufgrund von Schätzungsfehlern stimmen die Rollouts im erlernten Modell, insbesondere diejenigen mit langem Horizont, nicht mit denen in realen Umgebungen überein. Diese Fehlanpassung hat die Komplexität der MBRL-Beispiele ernsthaft beeinträchtigt. Basierend auf dieser Behauptung schlagen wir vor, das synthetische Modell zu erlernen, indem wir die Verteilungen der mehrstufigen Rollouts, die aus dem synthetischen Modell und den realen Rollouts über WGAN abgetastet wurden, abgleichen. Wir zeigen theoretisch, dass der Abgleich der beiden die Differenz der kumulativen Belohnungen zwischen dem realen Übergang und dem erlernten minimieren kann.
Die Batch-Normalisierung (BN) und ihre Varianten sind in der Deep-Learning-Gemeinschaft weit verbreitet, weil sie das Training von tiefen neuronalen Netzen verbessern, aber die Frage, warum diese Normalisierung so gut funktioniert, ist noch ungeklärt.  Wir stellen die Beziehung zwischen gewöhnlichen kleinsten Quadraten und partiellen Ableitungen, die beim Backpropagieren durch BN berechnet werden, explizit dar. Wir stellen die Backpropagation von BN als eine Anpassung mit kleinsten Quadraten dar, die partielle Ableitungen von normalisierten Aktivierungen nullzentriert und dekorreliert. Diese Sichtweise, die wir als {\em gradient-least-squares} bezeichnen, ist eine erweiterbare und arithmetisch genaue Beschreibung von BN. Um diese Perspektive weiter zu erkunden, motivieren, interpretieren und bewerten wir zwei Anpassungen von BN.
Batch Normalization (BN) hat sich zu einem Eckpfeiler des Deep Learning in verschiedenen Architekturen entwickelt, da sie sowohl bei der Optimierung als auch bei der Generalisierung zu helfen scheint.Während die Idee intuitiv Sinn macht, fehlte bisher eine theoretische Analyse ihrer Effektivität.Hier wird theoretische Unterstützung für eine ihrer vermuteten Eigenschaften geliefert, nämlich die Fähigkeit, den Gradientenabstieg mit weniger Tuning der Lernraten erfolgreich zu machen.Es wird gezeigt, dass selbst wenn wir die Lernrate von skaleninvarianten Parametern (z.B. Gewichte jeder Schicht mit BN) fixieren, Es wird gezeigt, dass selbst wenn wir die Lernrate skaleninvariabler Parameter (z.B. Gewichte jeder Schicht mit BN) auf eine Konstante (z.B. 0,3) festlegen, sich der Gradientenabstieg immer noch einem stationären Punkt (d.h. einer Lösung, bei der der Gradient gleich Null ist) mit einer Rate von T^{-1/2} in T Iterationen nähert, was asymptotisch der besten Grenze für den Gradientenabstieg mit gut abgestimmten Lernraten entspricht.Ein ähnliches Ergebnis mit einer Konvergenzrate von T^{-1/4} wird auch für den stochastischen Gradientenabstieg gezeigt.
Wir versuchen, diesen Erfolg auf den Bereich der Videomodellierung zu übertragen, indem wir zeigen, dass große Generative Adversarial Networks, die auf dem komplexen Kinetics-600-Datensatz trainiert wurden, in der Lage sind, Videomuster mit wesentlich höherer Komplexität und Genauigkeit zu erzeugen als frühere Arbeiten.  Unser vorgeschlagenes Modell, Dual Video Discriminator GAN (DVD-GAN), skaliert auf längere und höher aufgelöste Videos, indem es eine rechnerisch effiziente Dekomposition seines Diskriminators nutzt. Wir evaluieren die verwandten Aufgaben der Videosynthese und der Videovorhersage und erreichen eine neue, hochmoderne Fréchet Inception Distance für die Vorhersage für Kinetics-600 sowie eine hochmoderne Inception Score für die Synthese auf dem UCF-101-Datensatz, neben der Etablierung einer starken Basislinie für die Synthese auf Kinetics-600.
Um prozedurale Sprache zu verstehen, ist es notwendig, die kausalen Effekte von Handlungen zu antizipieren, auch wenn diese nicht explizit angegeben werden. In dieser Arbeit führen wir neuronale Prozessnetzwerke ein, um prozeduralen Text durch (neuronale) Simulation von Handlungsdynamik zu verstehen.   Empirische Ergebnisse zeigen, dass das von uns vorgeschlagene Modell in der Lage ist, die nicht explizit genannten kausalen Effekte von Handlungen zu verstehen und damit genauere Kontextinformationen für das Verständnis und die Generierung von prozeduralem Text zu liefern, während es gleichzeitig besser interpretierbare interne Repräsentationen als bestehende Alternativen bietet.
In letzter Zeit gibt es einen Trend, neuronale Netze zu trainieren, um Datenstrukturen zu ersetzen, die von Hand erstellt wurden, um eine schnellere Ausführung, eine höhere Genauigkeit oder eine größere Komprimierung zu erreichen.  In vielen Anwendungen ist diese teure Initialisierung nicht praktikabel, zum Beispiel bei Streaming-Algorithmen, bei denen die Eingaben flüchtig sind und nur eine kleine Anzahl von Malen überprüft werden kann.  Wir schlagen eine neuartige Speicherarchitektur vor, den neuronalen Bloom-Filter, von dem wir zeigen, dass er komprimierter ist als Bloom-Filter und verschiedene bestehende speichererweiterte neuronale Netze in Szenarien mit schiefen Daten oder strukturierten Mengen.
Wir nutzen die jüngsten Erkenntnisse aus der Optimierung zweiter Ordnung für neuronale Netze, um eine Kronecker-faktorisierte Laplace-Approximation für das Posterior über die Gewichte eines trainierten Netzes zu konstruieren. Unsere Approximation erfordert keine Änderung des Trainingsverfahrens, so dass Praktiker die Unsicherheit ihrer derzeit in der Produktion verwendeten Modelle schätzen können, ohne sie neu trainieren zu müssen. Unser Ansatz erfordert nur die Berechnung von zwei quadratischen Krümmungsfaktormatrizen für jede Schicht, deren Größe gleich dem jeweiligen Quadrat der Eingangs- und Ausgangsgröße der Schicht ist, was die Methode sowohl rechnerisch als auch in Bezug auf die Speichernutzung effizient macht.
Verschiedene Regularisierungstechniken wurden vorgeschlagen, um die Qualität der Einbettung im Hinblick auf nachgelagerte Aufgaben wie Clustering zu verbessern. In diesem Papier erklären wir an einem einfachen Blockmodell die Auswirkungen der vollständigen Graphenregularisierung, bei der eine Konstante zu allen Einträgen der Adjazenzmatrix hinzugefügt wird. Insbesondere zeigen wir, dass die Regularisierung die spektrale Einbettung dazu zwingt, sich auf die größten Blöcke zu konzentrieren, was die Darstellung weniger empfindlich gegenüber Rauschen oder Ausreißern macht.Wir illustrieren diese Ergebnisse sowohl an synthetischen als auch an realen Daten und zeigen, wie die Regularisierung die Standard-Clustering-Ergebnisse verbessert.
Das Exposure-Bias-Problem bezieht sich auf die Trainings-Inferenz-Diskrepanz, die durch das Lehrer-Forcing in der Maximum-Likelihood-Schätzung (MLE) Training für auto-regressive neuronale Netzwerk-Sprachmodelle (LM) verursacht wird.Es wurde als ein zentrales Problem für die natürliche Sprache Generation (NLG) Modell training.Although eine Menge von Algorithmen vorgeschlagen worden, um Lehrer-Forcing zu vermeiden und daher zu lindern Exposure-Bias, gibt es nur wenig Arbeit zeigt, wie ernst die Exposition Bias Problem ist. In dieser Arbeit identifizieren wir zunächst die Fähigkeit zur automatischen Wiederherstellung von MLE-trainierten LM, was Zweifel an der Ernsthaftigkeit von Exposure Bias aufkommen lässt, und entwickeln dann eine präzise, quantifizierbare Definition für Exposure Bias. Nach unseren Messungen in kontrollierten Experimenten gibt es jedoch nur einen Leistungsgewinn von etwa 3 %, wenn die Diskrepanz zwischen Training und Inferenz vollständig beseitigt wird.Unsere Ergebnisse deuten darauf hin, dass das Exposure Bias-Problem viel weniger schwerwiegend sein könnte als derzeit angenommen.
Die Fähigkeit von Algorithmen, (kompositorische) Kommunikationsprotokolle zu entwickeln oder zu erlernen, wurde in der Literatur zur Sprachevolution traditionell durch die Verwendung von emergenten Kommunikationsaufgaben untersucht. Wir erweitern frühere Arbeiten, in denen Agenten in symbolischen Umgebungen trainiert wurden, durch die Entwicklung von Agenten, die in der Lage sind, aus rohen Pixeldaten zu lernen, einer anspruchsvolleren und realistischeren Eingabedarstellung, und stellen fest, dass der Grad der Struktur, der in den Eingabedaten gefunden wird, die Art der entstehenden Protokolle beeinflusst.  
Unser neuartiges BERTgrid, das auf Chargrid von Katti et al. (2018) basiert, stellt ein Dokument als ein Gitter kontextualisierter Einbettungsvektoren für Wortteile dar und macht so seine räumliche Struktur und Semantik für das verarbeitende neuronale Netzwerk zugänglich. Die kontextualisierten Einbettungsvektoren werden von einem BERT-Sprachmodell abgerufen.Wir verwenden BERTgrid in Kombination mit einem vollständig konvolutionalen Netzwerk für eine semantische Instanzsegmentierungsaufgabe zur Extraktion von Feldern aus Rechnungen.Wir demonstrieren seine Leistung bei der Extraktion von tabellarischen Einzelposten und Dokumentenkopffeldern.
Deep Reinforcement Learning (RL)-Politiken sind bekannt dafür, dass sie anfällig für negative Störungen ihrer Beobachtungen sind, ähnlich wie negative Beispiele für Klassifikatoren, aber ein Angreifer ist normalerweise nicht in der Lage, die Beobachtungen eines anderen Agenten direkt zu verändern. Wir demonstrieren die Existenz gegnerischer Strategien in Nullsummenspielen zwischen simulierten humanoiden Robotern mit propriozeptiven Beobachtungen gegen hochmoderne Opfer, die durch Selbstspiel darauf trainiert wurden, robust gegenüber Gegnern zu sein.die gegnerischen Strategien gewinnen zuverlässig gegen die Opfer, erzeugen aber scheinbar zufälliges und unkoordiniertes Verhalten.wir stellen fest, dass diese Strategien in hochdimensionalen Umgebungen erfolgreicher sind und wesentlich andere Aktivierungen im Strategiennetzwerk des Opfers hervorrufen, als wenn das Opfer gegen einen normalen Gegner spielt.Videos sind verfügbar unter https://attackingrl.github.io.
GloVe- und Skip-gram-Worteinbettungsmethoden lernen Wortvektoren, indem sie eine entrauschte Matrix von Wortkookkurrenzen in ein Produkt von Matrizen mit niedrigem Rang zerlegen.In dieser Arbeit schlagen wir einen iterativen Algorithmus zur Berechnung von Wortvektoren vor, der auf der Modellierung von Wortkookkurrenzen mit Generalized Low Rank Models basiert. Unser Algorithmus verallgemeinert sowohl Skip-gram und GloVe als auch andere Einbettungsmethoden, die auf der spezifizierten Kookkurrenzmatrix, der Verteilung der Kookkurrenzen und der Anzahl der Iterationen im iterativen Algorithmus basieren. Beispielsweise führt die Verwendung einer Tweedie-Verteilung mit einer Iteration zu GloVe und die Verwendung einer Multinomial-Verteilung mit voller Konvergenz zu Skip-gram. Experimentelle Ergebnisse zeigen, dass mehrere Iterationen unseres Algorithmus die Ergebnisse gegenüber der GloVe-Methode bei der Google-Wortanalogie-Aufgabe verbessern.
Deterministische Modelle sind Annäherungen an die Realität, die oft einfacher zu erstellen und zu interpretieren sind als stochastische Alternativen.  Da die Natur launisch ist, können Beobachtungsdaten in der Praxis leider nie vollständig durch deterministische Modelle erklärt werden.  Um deterministische Modelle so anzupassen, dass sie sich stochastisch verhalten und in der Lage sind, verrauschte Daten zu erklären und zu extrapolieren, müssen Beobachtungs- und Prozessrauschen hinzugefügt werden. Das Hinzufügen von Prozessrauschen zu deterministischen Simulatoren kann zu einem Ausfall des Simulators führen, was dazu führt, dass für bestimmte Eingaben kein Rückgabewert ermittelt werden kann - eine Eigenschaft, die wir als ``spröde'' bezeichnen. Wir zeigen, dass die Durchführung von Schlussfolgerungen in diesem Raum als Ablehnungsstichprobe betrachtet werden kann, und trainieren einen bedingten Normalisierungsfluss als Vorschlag über Rauschwerte, so dass die Wahrscheinlichkeit, dass der Simulator abstürzt, gering ist, was die Recheneffizienz und die Schlussfolgerungstreue bei einem festen Stichprobenbudget erhöht, wenn es als Vorschlag in einem approximativen Schlussfolgerungsalgorithmus verwendet wird.
Bei der Beantwortung von Fragen mit mehreren Schritten müssen Modelle Informationen aus verschiedenen Teilen eines Textes sammeln, um eine Frage zu beantworten. Die meisten aktuellen Ansätze lernen, diese Aufgabe mit neuronalen Netzen durchgängig zu bewältigen, ohne eine explizite Darstellung des Schlussfolgerungsprozesses beizubehalten.Wir schlagen eine Methode zur Extraktion einer diskreten Schlussfolgerungskette über den Text vor, die aus einer Reihe von Sätzen besteht, die zur Antwort führen. Die extrahierten Ketten werden dann in ein BERT-basiertes QA-Modell eingespeist, um eine endgültige Antwortvorhersage zu treffen. Entscheidend ist, dass wir uns nicht auf Gold-kommentierte Ketten oder "unterstützende Fakten" verlassen: Zur Trainingszeit leiten wir Pseudo-Gold-Argumentationsketten ab, indem wir Heuristiken verwenden, die auf der Erkennung von benannten Entitäten und der Auflösung von Koreferenzen basieren, und wir verlassen uns auch nicht auf diese Annotationen zur Testzeit, da unser Modell lernt, Ketten allein aus dem Rohtext zu extrahieren.  Wir testen unseren Ansatz an zwei kürzlich vorgeschlagenen großen Multi-Hop-Fragebeantwortungsdatensätzen: Unsere Analyse zeigt die Eigenschaften von Ketten, die für eine hohe Leistung ausschlaggebend sind: insbesondere ist es wichtig, die Extraktion sequentiell zu modellieren und jeden Kandidatensatz kontextbewusst zu behandeln; außerdem zeigt die menschliche Bewertung, dass unsere extrahierten Ketten es Menschen ermöglichen, Antworten mit hohem Vertrauen zu geben, was darauf hindeutet, dass sie eine starke Zwischenabstraktion für diese Aufgabe sind.
Die Normalisierung der Konstante (auch Partitionsfunktion, Bayes'sche Evidenz oder marginale Wahrscheinlichkeit genannt) ist eines der zentralen Ziele der Bayes'schen Inferenz, doch die meisten der bestehenden Methoden sind sowohl teuer als auch ungenau.Hier entwickeln wir einen neuen Ansatz, ausgehend von posterioren Stichproben, die mit einem Standard Markov Chain Monte Carlo (MCMC) erhalten wurden. Wir vergleichen unsere Methode, die wir Gaussianized Bridge Sampling (GBS) nennen, mit bestehenden Methoden wie Nested Sampling (NS) und Annealed Importance Sampling (AIS) an mehreren Beispielen und zeigen, dass unsere Methode sowohl deutlich schneller als auch wesentlich genauer als diese Methoden ist und eine zuverlässige Fehlerschätzung bietet.
Wir präsentieren eine groß angelegte empirische Studie zum katastrophalen Vergessen (CF) in modernen Deep Neural Network (DNN)-Modellen, die sequentielles (oder: inkrementelles) Lernen durchführen, und schlagen ein neues experimentelles Protokoll vor, das typische Einschränkungen in Anwendungsszenarien berücksichtigt. Da es sich um eine empirische Untersuchung handelt, evaluieren wir das CF-Verhalten auf der bisher größten Anzahl von visuellen Klassifizierungsdatensätzen, aus denen wir jeweils eine repräsentative Anzahl von sequentiellen Lernaufgaben (SLTs) in enger Anlehnung an frühere Arbeiten zum Thema CF konstruieren.Unsere Ergebnisse zeigen deutlich, dass es kein Modell gibt, das CF für alle untersuchten Datensätze und SLTs unter Anwendungsbedingungen vermeidet.Wir schließen mit einer Diskussion möglicher Lösungen und Umgehungsmöglichkeiten für CF, insbesondere für die Modelle EWC und IMM.
Föderiertes Lernen (FL) bezieht sich auf das Erlernen eines qualitativ hochwertigen globalen Modells auf der Grundlage einer dezentralen Datenspeicherung, ohne die Rohdaten zu kopieren. In dieser Arbeit zeigen wir auf, dass das Umfeld des Model Agnostic Meta Learning (MAML), bei dem eine schnelle, gradientenbasierte Anpassung an eine heterogene Verteilung von Aufgaben in wenigen Schritten optimiert wird, eine Reihe von Ähnlichkeiten mit dem Ziel der Personalisierung von FL aufweist. ) Der populäre FL-Algorithmus, Federated Averaging, kann als Meta-Lernalgorithmus interpretiert werden.2) Eine sorgfältige Feinabstimmung kann zu einem globalen Modell mit höherer Genauigkeit führen, das gleichzeitig leichter zu personalisieren ist.3) Ein Modell, das mit einer Standard-Rechenzentrums-Optimierungsmethode trainiert wurde, ist im Vergleich zu einem Modell, das mit Federated Averaging trainiert wurde, viel schwerer zu personalisieren, was die erste Behauptung unterstützt.4) Diese Ergebnisse werfen neue Fragen für FL, MAML und die breitere ML-Forschung auf.
Die Speicherung von Daten in tiefen neuronalen Netzen ist zu einem Thema von großem Forschungsinteresse geworden. In diesem Papier verbinden wir die Speicherung von Bildern in tiefen Faltungs-Autoencodern mit Downsampling durch gestufte Faltung.  Um diesen Mechanismus in einer einfacheren Umgebung zu analysieren, trainieren wir lineare Faltungs-Autoencoder und zeigen, dass lineare Kombinationen von Trainingsdaten als Eigenvektoren in dem linearen Operator gespeichert werden, der dem Netzwerk entspricht, wenn Downsampling verwendet wird.  Netze ohne Downsampling merken sich dagegen keine Trainingsdaten.  Wir liefern weitere Beweise dafür, dass derselbe Effekt in nichtlinearen Netzen auftritt.  Darüber hinaus führt das Downsampling in nichtlinearen Netzen dazu, dass sich das Modell nicht nur lineare Bildkombinationen, sondern auch einzelne Trainingsbilder merkt.  Da Faltungs-Autoencoder-Komponenten Bausteine von tiefen Faltungsnetzwerken sind, stellen wir uns vor, dass unsere Ergebnisse Licht auf das wichtige Phänomen der Speicherung in überparametrisierten tiefen Netzwerken werfen werden.  
Deep Reinforcement Learning-Methoden können die Notwendigkeit einer expliziten Entwicklung von Politik- oder Wertmerkmalen beseitigen, erfordern aber immer noch eine manuell spezifizierte Belohnungsfunktion. Inverse Reinforcement Learning verspricht eine automatische Belohnungserfassung, hat sich aber als außerordentlich schwierig erwiesen, wenn es um große, hochdimensionale Probleme mit unbekannter Dynamik geht. In dieser Arbeit schlagen wir AIRL vor, einen praktischen und skalierbaren Algorithmus für inverses Verstärkungslernen, der auf einer gegnerischen Belohnungslernformulierung basiert, die mit Algorithmen für direktes Nachahmungslernen konkurrieren kann. Darüber hinaus zeigen wir, dass AIRL in der Lage ist, tragbare Belohnungsfunktionen wiederherzustellen, die gegenüber Änderungen in der Dynamik robust sind, wodurch wir in der Lage sind, Strategien auch unter erheblichen Variationen in der Umgebung während des Trainings zu lernen.
Wir befassen uns mit zwei zentralen Fragen des maschinellen Lernens: Wie können wir vorhersagen, ob sich ein Minimum auf die Testmenge verallgemeinern lässt, und warum findet der stochastische Gradientenabstieg Minima, die sich gut verallgemeinern lassen? Unsere Arbeit ist eine Antwort auf \citet{zhang2016understanding}, der gezeigt hat, dass sich tiefe neuronale Netze leicht an zufällig beschriftete Trainingsdaten erinnern können, obwohl sie auf realen Beschriftungen derselben Eingaben gut verallgemeinern.Wir zeigen, dass dasselbe Phänomen in kleinen linearen Modellen auftritt.Diese Beobachtungen werden durch die Bayes'sche Evidenz erklärt, die scharfe Minima bestraft, aber invariant zur Modellparametrisierung ist. Wir zeigen auch, dass es eine optimale Stapelgröße gibt, die die Genauigkeit des Testsatzes maximiert, wenn man die Lernrate konstant hält, und schlagen vor, dass das Rauschen, das durch kleine Ministapel eingeführt wird, die Parameter zu Minima treibt, deren Evidenz groß ist. Indem wir den stochastischen Gradientenabstieg als stochastische Differentialgleichung interpretieren, identifizieren wir die "Rauschskala" $g = \epsilon (\frac{N}{B} - 1) \approx \epsilon N/B$, wobei $\epsilon$ die Lernrate, $N$ die Größe des Trainingssatzes und $B$ die Chargengröße ist, so dass die optimale Chargengröße sowohl zur Lernrate als auch zur Größe des Trainingssatzes proportional ist, $B_{opt} \propto \epsilon N$.Wir verifizieren diese Vorhersagen empirisch.
Aufgrund der schlechten Bildqualität, die durch die Streuung der Gammastrahlenphotonen, die Abschwächung und die kurze Abtastzeit im Positronenprozess verursacht wird, schlagen wir die Kombination von Deep Learning vor, um Positronenbilder mit guter Qualität und klaren Details durch adversarische Netze zu erzeugen. Die Struktur des Papiers ist wie folgt: Erstens kodieren wir, um die versteckten Vektoren von medizinischen CT-Bildern auf der Grundlage von Transfer Learning zu erhalten, und verwenden PCA, um Positronenbildmerkmale zu extrahieren.Zweitens konstruieren wir einen Positronenbildspeicher auf der Grundlage von Aufmerksamkeitsmechanismen als Gesamteingabe für die adversen Netze, die medizinische versteckte Variablen als Abfrage verwenden.Schließlich trainieren wir das gesamte Modell gemeinsam und aktualisieren die Eingabeparameter bis zur Konvergenz.Experimente haben die Möglichkeit bewiesen, seltene Positronenbilder für die industrielle zerstörungsfreie Prüfung unter Verwendung von adversen Netzen zu erzeugen, und es wurden gute Bildergebnisse erzielt.
Wir überarbeiten das Recurrent Attention Model (RAM, Mnih et al. (2014)), ein rekurrentes neuronales Netz für visuelle Aufmerksamkeit, aus der Perspektive des aktiven Informationssamplings. Wir greifen Ideen aus der neurowissenschaftlichen Forschung zur Rolle des aktiven Informationssamplings im Kontext der visuellen Aufmerksamkeit und des Blicks auf (Gottlieb, 2018), wo der Autor drei Arten von Motiven für aktive Informationssampling-Strategien vorschlägt.Wir finden, dass das ursprüngliche RAM-Modell nur eines davon implementiert. Wir identifizieren drei Hauptschwächen des ursprünglichen RAM und bieten eine einfache Lösung, indem wir zwei zusätzliche Terme zur Zielfunktion hinzufügen.Das modifizierte RAM1) erreicht eine schnellere Konvergenz,2) ermöglicht eine dynamische Entscheidungsfindung pro Probe ohne Genauigkeitsverlust, und3) generalisiert viel besser auf längere Sequenzen von Blicken, für die nicht trainiert wird, verglichen mit dem ursprünglichen RAM. 
Graph Neural Networks (GNNs) für die Vorhersage Aufgaben wie Knoten Klassifizierung oder Kante Vorhersage haben zunehmende Aufmerksamkeit in den letzten maschinellen Lernens von grafisch strukturierten data.However, eine große Menge von beschrifteten Graphen ist schwierig zu erhalten, die erheblich begrenzen den wahren Erfolg der GNNs.Although aktives Lernen wurde weitgehend untersucht für die Bewältigung Label-sparse Probleme mit anderen Daten wie Text, Bilder, etc.  In diesem Papier stellen wir die Untersuchung des aktiven Lernens mit GNNs für Knotenklassifizierungsaufgaben vor.  Insbesondere schlagen wir eine neue Methode vor, die Knotenmerkmalspropagierung gefolgt von K-Medoids-Clustering der Knoten für die Instanzauswahl beim aktiven Lernen verwendet. Mit einer theoretischen Grenzanalyse rechtfertigen wir die Designwahl unseres Ansatzes.
Continuous Normalizing Flows (CNFs) haben sich dank ihrer Invertierbarkeit und exakten Likelihood-Schätzung als vielversprechende tiefe generative Modelle für eine Vielzahl von Aufgaben herauskristallisiert. Allerdings ist die Konditionierung von CNFs auf Signale von Interesse für die bedingte Bilderzeugung und nachgelagerte Vorhersageaufgaben aufgrund des hochdimensionalen latenten Codes, der vom Modell erzeugt wird und die gleiche Größe wie die Eingabedaten haben muss, ineffizient. In diesem Beitrag schlagen wir InfoCNF vor, eine effiziente bedingte CNF, die den latenten Raum in einen klassenspezifischen überwachten Code und einen unbeaufsichtigten Code unterteilt, der von allen Klassen gemeinsam genutzt wird, um die markierten Informationen effizient zu nutzen. Da die Partitionierungsstrategie die Anzahl der Funktionsauswertungen (NFEs) (geringfügig) erhöht, verwendet InfoCNF auch Gating-Netzwerke, um die Fehlertoleranzen seiner ODE-Löser zu erlernen, um die Geschwindigkeit und Leistung zu verbessern. Wir zeigen empirisch, dass InfoCNF die Testgenauigkeit gegenüber der Basislinie verbessert, während es vergleichbare Likelihood-Scores liefert und die NFEs auf CIFAR10 reduziert.Darüber hinaus trägt die Anwendung der gleichen Partitionierungsstrategie in InfoCNF auf Zeitreihendaten zur Verbesserung der Extrapolationsleistung bei.
Ein zentrales Ziel des unüberwachten Lernens ist es, Repräsentationen aus unmarkierten Daten oder Erfahrungen zu gewinnen, die für ein effektiveres Lernen von nachgelagerten Aufgaben aus bescheidenen Mengen von markierten Daten verwendet werden können. Stattdessen entwickeln wir eine unüberwachte Meta-Learning-Methode, die explizit für die Fähigkeit optimiert, eine Vielzahl von Aufgaben aus kleinen Datenmengen zu lernen, indem wir automatisch Aufgaben aus unmarkierten Daten konstruieren und Meta-Learning über die konstruierten Aufgaben ausführen. Unsere Experimente mit vier Bilddatensätzen zeigen, dass unser unbeaufsichtigter Meta-Learning-Ansatz einen Lernalgorithmus ohne gelabelte Daten entwickelt, der für eine Vielzahl von nachgelagerten Klassifizierungsaufgaben anwendbar ist und die von vier früheren unbeaufsichtigten Lernmethoden gelernte Einbettung verbessert.
Der Domaintransfer ist ein spannender und anspruchsvoller Zweig des maschinellen Lernens, da Modelle lernen müssen, reibungslos zwischen Domänen zu wechseln, wobei lokale Variationen erhalten bleiben und viele Aspekte der Variation ohne Kennzeichnung erfasst werden. Die meisten bisherigen erfolgreichen Anwendungen setzen jedoch voraus, dass die beiden Domänen eng miteinander verwandt sind (z. B. Bild-zu-Bild, Video-zu-Video), wobei ähnliche oder gemeinsame Netzwerke verwendet werden, um domänenspezifische Eigenschaften wie Textur, Farbgebung und Linienformen zu transformieren. Hier zeigen wir, dass es möglich ist, zwischen den Modalitäten zu wechseln (z. B. Bild zu Audio), indem wir die Daten zunächst mit latenten generativen Modellen abstrahieren und dann Transformationen zwischen latenten Räumen lernen. Wir stellen fest, dass ein einfacher Variations-Auto-Codierer in der Lage ist, einen gemeinsamen latenten Raum zu erlernen, um eine Brücke zwischen zwei generativen Modellen auf unbeaufsichtigte Weise zu schlagen, und sogar zwischen verschiedenen Arten von Modellen (z. B. Variations-Auto-Codierer und ein generatives adversariales Netzwerk). Darüber hinaus können wir die gewünschte semantische Ausrichtung von Attributen mit einem linearen Klassifikator im gemeinsamen latenten Raum erzwingen. Schließlich entkoppelt die hierarchische Struktur die Kosten für das Training der generativen Basismodelle und der semantischen Ausrichtungen und ermöglicht so ein rechnerisch effizientes und dateneffizientes Retraining der personalisierten Mapping-Funktionen.
Wir schlagen Adversarial Inductive Transfer Learning (AITL) vor, eine Methode zur Behandlung von Diskrepanzen in Eingabe- und Ausgaberäumen zwischen Quell- und Zieldomänen, die adversarische Domänenanpassung und Multitasking-Lernen nutzt, um diese Diskrepanzen zu beseitigen. Unsere motivierende Anwendung ist die Pharmakogenomik, bei der das Ziel darin besteht, das Ansprechen von Patienten auf Medikamente anhand ihrer genomischen Informationen vorherzusagen. Die Herausforderung besteht darin, dass klinische Daten (d. h. Patienten) mit Ergebnissen zum Ansprechen auf Medikamente sehr begrenzt sind. Es bestehen Diskrepanzen zwischen1) den genomischen Daten der präklinischen und klinischen Datensätze (dem Input-Raum) und2) den unterschiedlichen Maßen der Medikamentenreaktion (dem Output-Raum).AITL ist unseres Wissens die erste adversarische induktive Transfer-Learning-Methode, die sowohl Input- als auch Output-Diskrepanzen adressiert.Experimentelle Ergebnisse deuten darauf hin, dass AITL den Stand der Technik in der Pharmakogenomik und beim Transfer-Learning übertrifft und die Präzisionsonkologie genauer steuern kann.
Neuere Arbeiten haben gezeigt, dass es vorteilhaft ist, diese Aufgaben gemeinsam zu erlernen, um die Fehlerfortpflanzung zu vermeiden, die bei Pipeline-basierten Systemen auftritt, und um die Leistung zu verbessern. Modernste gemeinsame Modelle stützen sich jedoch in der Regel auf externe NLP-Tools (Natural Language Processing), wie z. B. Dependency Parser, und beschränken ihren Nutzen auf Bereiche (z. B. Nachrichten), in denen diese Tools gut funktionieren. In dieser Arbeit schlagen wir ein neuronales End-to-End-Modell für die gemeinsame Extraktion von Entitäten und ihren Beziehungen vor, das nicht auf externe NLP-Tools angewiesen ist und ein umfangreiches, vortrainiertes Sprachmodell integriert. Da der Großteil der Parameter unseres Modells vortrainiert ist und wir die Rekursion zugunsten der Selbstaufmerksamkeit vermeiden, ist unser Modell schnell zu trainieren.
In dieser Arbeit erforschen wir ein einfaches Variations-Bayes-Schema für rekurrente neuronale Netze: Erstens zeigen wir, dass eine einfache Anpassung der abgeschnittenen Backpropagation über die Zeit qualitativ hochwertige Unsicherheitsschätzungen und eine überlegene Regularisierung bei nur geringen zusätzlichen Rechenkosten während des Trainings liefern kann, wobei auch die Anzahl der Parameter um 80\% reduziert wird.Zweitens demonstrieren wir, wie eine neuartige Art der Posterior-Approximation weitere Verbesserungen der Leistung von Bayes'schen RNNs liefert. Wir zeigen, dass diese Technik nicht nur für rekurrente neuronale Netze geeignet ist, sondern auch für das Training von Bayes'schen neuronalen Netzen.Wir demonstrieren auch empirisch, wie Bayes'sche RNNs den traditionellen RNNs bei einem Sprachmodellierungs-Benchmark und einer Bildbeschriftungsaufgabe überlegen sind, und wir zeigen, wie jede dieser Methoden unser Modell gegenüber einer Vielzahl anderer Trainingsverfahren verbessert.Wir führen auch einen neuen Benchmark für die Untersuchung der Unsicherheit von Sprachmodellen ein, so dass zukünftige Methoden leicht verglichen werden können.
Im Laufe der Zeit haben unbemannte autonome Fahrzeuge (UAVs), insbesondere autonome Flugdrohnen, in der künstlichen Intelligenz viel Aufmerksamkeit erregt. Da die elektronische Technologie immer kleiner, billiger und effizienter wird, wurden in letzter Zeit enorme Fortschritte bei der Untersuchung von UAVs beobachtet. Von der Überwachung von Überschwemmungen über die Erkennung der Ausbreitung von Algen in Gewässern bis hin zum Aufspüren von Waldwegen - die Einsatzmöglichkeiten sind vielfältig.Unsere Arbeit konzentriert sich hauptsächlich auf autonome Flugdrohnen, wobei wir eine Fallstudie zur Effizienz, Robustheit und Genauigkeit von UAVs erstellen und unsere Ergebnisse durch Experimente untermauern. Wir stellen Details der Software- und Hardware-Architektur, die in der Studie verwendet wird, zur Verfügung, diskutieren über unsere Implementierungsalgorithmen und präsentieren Experimente, die einen Vergleich zwischen drei verschiedenen State-of-the-Art-Algorithmen, nämlich TrailNet, InceptionResnet und MobileNet, in Bezug auf Genauigkeit, Robustheit, Stromverbrauch und Inferenzzeit ermöglichen.
Musik beruht in hohem Maße auf Wiederholungen, um Struktur und Bedeutung aufzubauen.  Selbstreferenz findet auf verschiedenen Zeitebenen statt, von Motiven über Phrasen bis hin zur Wiederverwendung ganzer Musikabschnitte, wie z. B. in Stücken mit ABA-Struktur.  Der Transformer (Vaswani et al., 2017), ein auf Selbstaufmerksamkeit basierendes Sequenzmodell, hat in vielen Generierungsaufgaben, die die Aufrechterhaltung einer weitreichenden Kohärenz erfordern, überzeugende Ergebnisse erzielt, was darauf hindeutet, dass Selbstaufmerksamkeit auch für die Modellierung von Musik gut geeignet sein könnte.Bei der musikalischen Komposition und Aufführung ist jedoch das relative Timing von entscheidender Bedeutung.  Bestehende Ansätze zur Darstellung relativer Positionsinformationen im Transformer modulieren die Aufmerksamkeit auf der Grundlage des paarweisen Abstands (Shaw et al., 2018).  Dies ist für lange Sequenzen wie musikalische Kompositionen unpraktisch, da ihre Speicherkomplexität quadratisch zur Sequenzlänge ist.  Wir schlagen einen Algorithmus vor, der die Anforderungen an den Zwischenspeicher auf einen Wert reduziert, der linear zur Sequenzlänge ist, und können so zeigen, dass ein Transformer mit unserem modifizierten Mechanismus der relativen Aufmerksamkeit minutenlange (Tausende von Schritten) Kompositionen mit überzeugender Struktur erzeugen kann, Fortsetzungen generieren kann, die ein gegebenes Motiv kohärent ausarbeiten, und in einem seq2seq-Setup Begleitungen erzeugen kann, die auf Melodien basieren.   Wir evaluieren den Transformer mit unserem Mechanismus der relativen Aufmerksamkeit an zwei Datensätzen, JSB Chorales und Piano-e-competition, und erhalten bei letzterem die besten Ergebnisse.
Sequentielle Entscheidungsprobleme für reale Anwendungen müssen oft in Echtzeit gelöst werden und erfordern Algorithmen, die mit einem begrenzten Rechenbudget gute Leistungen erbringen.Breitenbasierte Lookaheads haben bei klassischen Planungsproblemen sowie bei Atari-Spielen mit knappen Budgets Spitzenleistungen gezeigt.In dieser Arbeit untersuchen wir breitenbasierte Lookaheads über Stochastic Shortest Paths (SSP). Wir analysieren, warum breitenbasierte Algorithmen bei SSP-Problemen schlecht abschneiden, und überwinden diese Fallstricke, indem wir eine Methode zur Schätzung der Cost-to-Go vorschlagen. Wir formalisieren breitenbasierte Lookaheads als eine Instanz des Rollout-Algorithmus, geben eine Definition der Breite für SSP-Probleme und erklären seine Beispielkomplexität. Unsere experimentellen Ergebnisse über eine Vielzahl von SSP-Benchmarks zeigen, dass der Algorithmus andere moderne Rollout-Algorithmen wie UCT und RTDP übertrifft.
Deep Neural Networks (DNNs) sind für ihre exzellente Leistung bei überwachten Aufgaben wie der Klassifizierung bekannt. Insbesondere Convolutional Neural Networks (CNNs) können effektive Merkmale erlernen und High-Level-Darstellungen aufbauen, die für die Klassifizierung, aber auch für Abfragen und die Suche nach dem nächsten Nachbarn verwendet werden können.Es hat sich jedoch auch gezeigt, dass CNNs unter einem Leistungsabfall leiden, wenn sich die Verteilung der Daten von Trainings- zu Testdaten ändert. In diesem Beitrag analysieren wir die internen Repräsentationen von CNNs und stellen fest, dass die Repräsentationen der ungesehenen Daten in jeder Klasse im Vergleich zu den Repräsentationen der Trainingsdaten eine größere Streuung (mit höherer Varianz) im Einbettungsraum des CNN aufweisen, wobei dieser Unterschied noch extremer ist, wenn die ungesehenen Daten aus einer verschobenen Verteilung stammen. Wir wenden die Nearest Neighbour-Klassifikation auf die Repräsentationen an und zeigen empirisch, dass die Einbettungen mit der hohen Varianz tatsächlich eine signifikant schlechtere KNN-Klassifikationsleistung aufweisen, obwohl dies aus ihren End-to-End-Klassifikationsergebnissen nicht vorhersehbar war. Um dieses Problem zu lösen, schlagen wir die Deep Within-Class Covariance Analysis (DWCCA) vor, eine tiefe neuronale Netzwerkschicht, die die klasseninterne Kovarianz der DNN-Darstellung signifikant reduziert und so die Leistung bei ungesehenen Testdaten aus einer verschobenen Verteilung verbessert. Wir zeigen, dass DWCCA nicht nur die interne Repräsentation des Netzwerks signifikant verbessert, sondern auch die End-to-End-Klassifizierungsgenauigkeit erhöht, insbesondere wenn der Testdatensatz eine leichte Verteilungsverschiebung aufweist. Durch das Hinzufügen von DWCCA zu einem neuronalen VGG-Netzwerk erreichen wir eine Verbesserung von etwa 6 Prozentpunkten im Falle einer Verteilungsfehlanpassung.
Generative Modelle haben sich als hervorragendes Werkzeug zur Darstellung hochdimensionaler Wahrscheinlichkeitsverteilungen und zur Erzeugung realistisch aussehender Bilder erwiesen. Eine grundlegende Eigenschaft generativer Modelle ist ihre Fähigkeit, multimodale Ausgaben zu erzeugen. In diesem Beitrag lassen wir uns vom Determinantal Point Process (DPP) inspirieren, um ein generatives Modell zu entwickeln, das den Mode-Kollaps mildert und gleichzeitig qualitativ hochwertigere Stichproben erzeugt.DPP ist ein elegantes probabilistisches Maß zur Modellierung negativer Korrelationen innerhalb einer Teilmenge und somit zur Quantifizierung ihrer Vielfalt. Wir verwenden den DPP-Kernel, um die Diversität in realen und synthetischen Daten zu modellieren, und entwickeln dann einen Straftermin für die Generierung, der den Generator dazu anregt, Daten mit einer ähnlichen Diversität wie die realen Daten zu synthetisieren.Im Gegensatz zu bisherigen generativen Modellen, die dazu neigen, zusätzliche trainierbare Parameter oder komplexe Trainingsparadigmen zu verwenden, ändert unsere Methode das ursprüngliche Trainingsschema nicht. Eingebettet in einen adversen Trainings- und Variations-Autoencoder, zeigt unser generativer DPP-Ansatz eine konsistente Resistenz gegen Mode-Collapse auf einer Vielzahl von synthetischen Daten und natürlichen Bilddatensätzen, einschließlich MNIST, CIFAR10 und CelebA, und übertrifft dabei State-of-the-Art-Methoden in Bezug auf Daten-Effizienz, Konvergenzzeit und Generierungsqualität.
Trotz bestehender Arbeiten zur Sicherstellung der Generalisierung neuronaler Netze in Bezug auf skalensensitive Komplexitätsmaße, wie Normen, Marge und Schärfe, bieten diese Komplexitätsmaße keine Erklärung dafür, warum neuronale Netze bei Überparametrisierung besser generalisieren.In dieser Arbeit schlagen wir ein neuartiges Komplexitätsmaß vor, das auf einheitsweisen Kapazitäten basiert und zu einer engeren Generalisierungsgrenze für zweischichtige ReLU-Netze führt. Unsere Kapazitätsschranke korreliert mit dem Verhalten des Testfehlers bei zunehmender Netzwerkgröße (innerhalb der in den Experimenten berichteten Spanne) und könnte teilweise die Verbesserung der Generalisierung mit Überparametrisierung erklären.Wir stellen außerdem eine passende untere Schranke für die Rademacher-Komplexität vor, die frühere untere Kapazitätsschranken für neuronale Netzwerke übertrifft.
Die neuartigen Verarbeitungsblöcke, die einen effizienten Informationsfluss ermöglichen, sind ein faltungsartiger Operationsblock für Punktmengen, der Nachbarschaftsinformationen auf speichereffiziente Weise zusammenführt; ein Verarbeitungsblock für Punktwolken mit mehreren Auflösungen; und ein Crosslink-Block, der Informationen effizient über niedrig- und hochauflösende Verarbeitungszweige hinweg austauscht. Wir evaluieren die vorgeschlagenen Architekturen ausgiebig an mehreren Punktsegmentierungs-Benchmarks (ShapeNetPart, ScanNet, PartNet) und berichten über systematische Verbesserungen sowohl in Bezug auf die Genauigkeit als auch auf den Speicherverbrauch durch die Verwendung unserer generischen Module in Verbindung mit mehreren neueren Architekturen (PointNet++, DGCNN, SpiderCNN, PointCNN). 9,7 % mehr IoU erreichen wir mit dem PartNet-Datensatz, der der komplexeste ist, während der Speicherbedarf um 57 % sinkt.
End-to-End-Akustik-zu-Wort-Spracherkennungsmodelle haben in letzter Zeit an Popularität gewonnen, da sie einfach zu trainieren sind, gut auf große Mengen von Trainingsdaten skalieren und kein Lexikon benötigen.Darüber hinaus können Wortmodelle auch einfacher in nachgelagerte Aufgaben wie das Verstehen gesprochener Sprache integriert werden, da die Inferenz (Suche) im Vergleich zu Phonemen, Zeichen oder jeder anderen Art von Unterworteinheiten sehr viel einfacher ist.In diesem Papier beschreiben wir Methoden, um kontextuelle akustische Worteinbettungen direkt aus einem überwachten Sequenz-zu-Sequenz-Akustik-zu-Wort-Spracherkennungsmodell unter Verwendung der gelernten Aufmerksamkeitsverteilung zu konstruieren. In einer Reihe von 16 Standard-Satz-Evaluierungsaufgaben zeigen unsere Einbettungen eine konkurrenzfähige Leistung gegenüber einem word2vec-Modell, das auf den Sprachtranskriptionen trainiert wurde. Zusätzlich evaluieren wir diese Einbettungen in einer Aufgabe zum Verstehen gesprochener Sprache und stellen fest, dass unsere Einbettungen mit der Leistung textbasierter Einbettungen in einer Pipeline übereinstimmen, in der zuerst die Spracherkennung durchgeführt und dann die Worteinbettungen aus den Transkriptionen konstruiert werden.
Das Training mit binokularen Stereobildern wird als gute Option angesehen, da die Daten leicht zu beschaffen sind. Die Ergebnisse der Tiefen- oder Disparitätsvorhersage zeigen jedoch eine schlechte Leistung für die Objektgrenzen, was hauptsächlich auf die Behandlung von Verdeckungsbereichen während des Trainings zurückzuführen ist. In diesem Papier, schlagen wir eine neuartige Methode, um dieses Problem zu überwinden.Exploiting Disparität mapsproperty, generieren wir eine Okklusion Maske, um die Back-Propagation der occlusionareas während Bild warping.We auch Design neue Netzwerke mit flippedstereo Bilder, um die Netze zu lernen okkludiert boundaries.It zeigt, dass unsere Methode erreicht klarer Grenzen und bessere Ergebnisse auf KITTIdriving Datensatz und Virtual KITTI Datensatz zu bewerten.
Die Klassifizierung von Graphen wird derzeit von Graphenkernen dominiert, die zwar leistungsfähig sind, aber einige erhebliche Einschränkungen aufweisen.Convolutional Neural Networks (CNNs) bieten eine sehr ansprechende Alternative.Die Verarbeitung von Graphen mit CNNs ist jedoch nicht trivial.Um diese Herausforderung zu bewältigen, wurden kürzlich viele anspruchsvolle Erweiterungen von CNNs vorgeschlagen. In dieser Arbeit kehren wir das Problem um: Anstatt ein weiteres Graph-CNN-Modell vorzuschlagen, stellen wir eine neuartige Methode vor, um Graphen als mehrkanalige, bildähnliche Strukturen darzustellen, die es ermöglicht, sie mit einfachen 2D-CNNs zu verarbeiten.Trotz ihrer Einfachheit erweist sich unsere Methode als sehr konkurrenzfähig gegenüber den modernsten Graph-Kerneln und Graph-CNNs und übertrifft sie bei einigen Datensätzen um ein Vielfaches.
Die Schlüsseleigenschaft, die den beispiellosen Erfolg moderner rekurrenter neuronaler Netze (RNNs) bei Lernaufgaben mit sequentiellen Daten ausmacht, ist ihre immer besser werdende Fähigkeit, komplizierte langfristige zeitliche Abhängigkeiten zu modellieren.Allerdings fehlt ein etabliertes Maß für die Langzeitspeicherkapazität von RNNs, so dass das formale Verständnis ihrer Fähigkeit, Daten über die Zeit zu korrelieren, begrenzt ist. Obwohl die Tiefeneffizienz in Faltungsnetzwerken mittlerweile gut etabliert ist, reicht sie nicht aus, um den Erfolg von tiefen RNNs bei Eingaben unterschiedlicher Länge zu erklären, und es entsteht die Notwendigkeit, ihre "zeitserielle Ausdruckskraft" zu adressieren.In diesem Papier analysieren wir die Auswirkung der Tiefe auf die Fähigkeit von rekurrenten Netzwerken, Korrelationen über lange Zeitskalen auszudrücken.Um die oben genannte Notwendigkeit zu erfüllen, führen wir ein Maß für den Informationsfluss über die Zeit ein, der durch das Netzwerk unterstützt werden kann, das als Start-End-Trennungsrang bezeichnet wird. Wir beweisen, dass tiefe rekurrente Netze Start-End-Separationsränge unterstützen, die exponentiell höher sind als diejenigen, die von ihren flachen Gegenstücken unterstützt werden, und wir zeigen, dass die Fähigkeit tiefer rekurrenter Netze, verschiedene Teile der Eingabesequenz zu korrelieren, exponentiell zunimmt, wenn sich die Eingabesequenz verlängert, während sich die Fähigkeit flacher rekurrenter Netze überhaupt nicht an die Sequenzlänge anpasst. Wir stellen also fest, dass die Tiefe einen überwältigenden Vorteil für die Fähigkeit rekurrenter Netze zur Modellierung langfristiger Abhängigkeiten mit sich bringt, und liefern ein Beispiel für die Quantifizierung dieser Schlüsseleigenschaft, das leicht auf andere RNN-Architekturen von Interesse ausgedehnt werden kann, z. B. auf Varianten von LSTM-Netzen.Wir erhalten unsere Ergebnisse, indem wir eine Klasse rekurrenter Netze betrachten, die als rekurrente arithmetische Schaltungen (RACs) bezeichnet werden und die den verborgenen Zustand mit der Eingabe über die Operation der multiplikativen Integration zusammenführen.
Die ganzheitliche Erforschung der Wahrnehmungs- und neuronalen Repräsentationen, die der tierischen Kommunikation zugrunde liegen, war bisher aufgrund der Komplexität des zugrunde liegenden Signals sehr schwierig. Wir stellen hier eine neuartige Reihe von Techniken vor, mit denen ganze kommunikative Repertoires in niedrigdimensionale Räume projiziert werden können, die systematisch abgetastet werden können, um die Beziehung zwischen Wahrnehmungsrepräsentationen, neuronalen Repräsentationen und den latenten Repräsentationsräumen, die von Algorithmen des maschinellen Lernens erlernt werden, zu untersuchen. Wir stellen diese Methode in einem laufenden Experiment vor, in dem wir die sequenzielle und zeitliche Aufrechterhaltung des Kontexts in neuronalen und wahrnehmungsbezogenen Silbenrepräsentationen von Singvögeln untersuchen, und erörtern, wie die Untersuchung der neuronalen Mechanismen, die der Aufrechterhaltung des weitreichenden Informationsgehalts im Vogelgesang zugrunde liegen, in die maschinelle Sequenzmodellierung einfließen kann.
Das Prinzip des Informationsengpasses (Shwartz-Ziv & Tishby, 2017) legt nahe, dass das SGD-basierte Training von tiefen neuronalen Netzen aus informationstheoretischer Sicht zu optimal komprimierten versteckten Schichten führt, allerdings wurde diese Behauptung an Spielzeugdaten aufgestellt.Das Ziel der hier vorgestellten Arbeit ist es, diese Behauptungen in einer realistischen Umgebung mit einer größeren und tieferen Faltungsarchitektur, einem ResNet-Modell, zu testen. Wir haben PixelCNN++ Modelle als inverse Repräsentationsdecoder trainiert, um die wechselseitige Information zwischen den versteckten Schichten eines ResNets und den eingegebenen Bilddaten zu messen, wenn sie für (1) Klassifizierung und (2) Autocodierung trainiert wurden. Wir finden, dass zwei Lernphasen für beide Trainingsregime stattfinden und dass Kompression auftritt, sogar für einen Autocodierer.
Wir untersuchen das Problem der sicheren Anpassung: Kann ein Modell, das auf der Grundlage einer Vielzahl von Erfahrungen aus der Vergangenheit für eine bestimmte Aufgabe trainiert wurde, lernen, diese Aufgabe in einer neuen Situation auszuführen, ohne dass es zu katastrophalen Fehlern kommt?Diese Problemstellung tritt häufig in realen Verstärkungslernszenarien auf, z. B. bei der Anpassung eines Fahrzeugs an das Fahren in einer neuen Stadt oder bei der Anpassung einer Roboterdrohne an eine nur in der Simulation trainierte Strategie. Während das Lernen ohne katastrophale Ausfälle außerordentlich schwierig ist, können wir aufgrund früherer Erfahrungen Modelle erlernen, die dies sehr viel einfacher machen. Diese Modelle lassen sich zwar nicht direkt auf neue Umgebungen übertragen, können aber eine vorsichtige Anpassung ermöglichen, die wesentlich sicherer ist als eine naive Anpassung oder ein Lernen von Grund auf. Aufbauend auf dieser Intuition schlagen wir eine risikoaverse Domänenanpassung (RADA) vor, die in zwei Schritten funktioniert: Zunächst werden probabilistische, modellbasierte RL-Agenten in einer Population von Quelldomänen trainiert, um Erfahrungen zu sammeln und die epistemische Unsicherheit über die Dynamik der Umgebung zu erfassen. Wir zeigen, dass diese einfache Maximin-Politik die Domänenanpassung in einer sicherheitskritischen Fahrumgebung mit variierenden Fahrzeuggrößen beschleunigt und vergleichen unseren Ansatz mit anderen Ansätzen zur Anpassung an neue Umgebungen, einschließlich Meta-Reinforcement Learning.
Wir schlagen den Neuro-Symbolic Concept Learner (NS-CL) vor, ein Modell, das visuelle Konzepte, Wörter und das semantische Parsing von Sätzen ohne explizite Überwachung erlernt; stattdessen lernt unser Modell durch einfaches Betrachten von Bildern und Lesen von gepaarten Fragen und Antworten.Unser Modell baut eine objektbasierte Szenendarstellung auf und übersetzt Sätze in ausführbare, symbolische Programme. Um das Lernen von zwei Modulen zu überbrücken, verwenden wir ein Modul für neurosymbolische Schlussfolgerungen, das diese Programme auf der latenten Szenenrepräsentation ausführt.Analog zum menschlichen Konzeptlernen lernt das Wahrnehmungsmodul visuelle Konzepte auf der Grundlage der sprachlichen Beschreibung des Objekts, auf das Bezug genommen wird. Ausführliche Experimente zeigen die Genauigkeit und Effizienz unseres Modells beim Erlernen von visuellen Konzepten, Wortrepräsentationen und semantischem Parsing von Sätzen.Darüber hinaus ermöglicht unsere Methode eine einfache Verallgemeinerung auf neue Objektattribute, Kompositionen, Sprachkonzepte, Szenen und Fragen und sogar neue Programmdomänen.Sie ermöglicht auch Anwendungen wie visuelle Fragebeantwortung und bidirektionales Bild-Text-Retrieval.
Die Bayes'sche Inferenz bietet eine theoretisch fundierte und allgemeine Möglichkeit, neuronale Netze zu trainieren, und kann potenziell eine kalibrierte Unsicherheit liefern. Es ist jedoch eine Herausforderung, einen sinnvollen und nachvollziehbaren Prior für die Netzparameter zu spezifizieren und mit den Gewichtskorrelationen im Posterior umzugehen. Zu diesem Zweck führt dieses Papier zwei Innovationen ein:(i) ein auf Gaußschen Prozessen basierendes hierarchisches Modell für die Netzwerkparameter, das auf kürzlich eingeführten Einheitseinbettungen basiert, die flexibel Gewichtsstrukturen kodieren können, und(ii) eingabeabhängige Kontextvariablen für den Gewichtsprior, die bequeme Möglichkeiten zur Regularisierung des vom Netzwerk modellierten Funktionsraums durch den Einsatz von Kerneln bieten. Wir zeigen, dass diese Modelle wünschenswerte Unsicherheitsschätzungen für die Testzeit liefern, demonstrieren Fälle der Modellierung induktiver Verzerrungen für neuronale Netze mit Kerneln und zeigen eine konkurrenzfähige Vorhersageleistung bei einem aktiven Lernvergleich.
Wir testen das Standard-Transformatormodell sowie eine neuartige Variante, bei der der Encoder-Block Informationen aus nahegelegenen Zeichen mittels Faltung kombiniert. Wir führen umfangreiche Experimente mit WMT- und UN-Datensätzen durch und testen sowohl zweisprachige als auch mehrsprachige Übersetzungen ins Englische mit bis zu drei Eingabesprachen (Französisch, Spanisch und Chinesisch). Unsere Transformator-Variante ist dem Standard-Transformator auf Zeichenebene durchweg überlegen und konvergiert schneller, während sie robustere Alignments auf Zeichenebene lernt.
Viele Aufgaben in der Radiologie, zum Beispiel, sind weitgehend Probleme der Multi-Label-Klassifikation, in denen medizinische Bilder interpretiert werden, um mehrere vorhandene oder vermutete Pathologien anzuzeigen.Klinische Einstellungen treiben die Notwendigkeit für hohe Genauigkeit gleichzeitig über eine Vielzahl von pathologischen Ergebnissen und schränken den Nutzen von Werkzeugen, die nur eine Teilmenge berücksichtigen, stark ein. Dieses Problem wird durch eine allgemeine Knappheit an Trainingsdaten verschärft und maximiert die Notwendigkeit, klinisch relevante Merkmale aus verfügbaren Proben zu extrahieren - idealerweise ohne die Verwendung von vortrainierten Modellen, die unerwünschte Verzerrungen aus tangential verwandten Aufgaben mit sich bringen können. Wir präsentieren und evaluieren eine Teillösung für diese Einschränkungen, indem wir LSTMs verwenden, um Interdependenzen zwischen Zielmarkierungen bei der Vorhersage von 14 pathologischen Mustern aus Röntgenbildern des Brustkorbs zu nutzen, und stellen Ergebnisse auf dem neuesten Stand der Technik auf dem größten öffentlich zugänglichen Röntgendatensatz des NIH ohne Vortraining fest.
Semmelhack et al. (2014) haben eine hohe Klassifizierungsgenauigkeit bei der Unterscheidung von Schwimmzügen von Zebrafischen mit einer Support Vector Machine (SVM) erreicht.Convolutional Neural Networks (CNNs) haben bei verschiedenen Bilderkennungsaufgaben eine bessere Leistung als SVMs erreicht, aber diese leistungsstarken Netzwerke bleiben eine Blackbox.Das Erreichen einer besseren Transparenz hilft, Vertrauen in ihre Klassifizierungen aufzubauen und macht gelernte Merkmale für Experten interpretierbar.Mit einer kürzlich entwickelten Technik namens Deep Taylor Decomposition haben wir Heatmaps erzeugt, um Eingabebereiche mit hoher Relevanz für Vorhersagen hervorzuheben. Wir fanden heraus, dass unser CNN Vorhersagen macht, indem es die Stetigkeit des Schwanzes analysiert, was sich deutlich von den manuell extrahierten Merkmalen unterscheidet, die von Semmelhack et al. (2014) verwendet wurden.Wir deckten außerdem auf, dass das Netzwerk auf experimentelle Artefakte achtete.Das Entfernen dieser Artefakte gewährleistete die Gültigkeit der Vorhersagen.Nach der Korrektur schlägt unser bestes CNN die SVM um 6,12% und erreicht eine Klassifizierungsgenauigkeit von 96,32%.Unsere Arbeit demonstriert somit den Nutzen der KI-Erklärbarkeit für CNNs.
Bei der Kommunikation verlassen sich Menschen auf intern konsistente Sprachrepräsentationen, d.h. als Sprecher erwarten wir, dass die Zuhörer sich genauso verhalten wie wir, wenn wir zuhören. Wir betrachten zwei Hypothesen über die Wirkung von internen Konsistenzbeschränkungen:1) dass sie die Fähigkeit der Agenten verbessern, sich auf ungesehene Referenten zu beziehen, und2) dass sie die Fähigkeit der Agenten verbessern, sich über kommunikative Rollen hinweg zu verallgemeinern (z.B. als Sprecher aufzutreten, obwohl sie nur als Zuhörer trainiert wurden).Während wir keine Beweise für die erste Hypothese finden, zeigen unsere Ergebnisse signifikante Unterstützung für die zweite.
Neuronale Netze (NNs) sind in der Lage, Aufgaben auszuführen, die auf einer kompositorischen Struktur beruhen, auch wenn ihnen offensichtliche Mechanismen zur Darstellung dieser Struktur fehlen. Um die internen Repräsentationen zu analysieren, die einen solchen Erfolg ermöglichen, schlagen wir ROLE vor, eine Technik, die erkennt, ob diese Repräsentationen implizit eine symbolische Struktur kodieren. ROLE lernt, die Repräsentationen eines Zielcodierers E zu approximieren, indem es eine symbolische Konstituentenstruktur und eine Einbettung dieser Struktur in den Repräsentationsvektorraum von E lernt.Die Konstituenten der approximierenden Symbolstruktur sind durch strukturelle Positionen - Rollen - definiert, die durch Symbole ausgefüllt werden können.Wir zeigen, dass, wenn E so konstruiert ist, dass es explizit einen bestimmten Strukturtyp einbettet (z.B, Wir analysieren dann ein seq2seq-Netzwerk, das für eine komplexere kompositorische Aufgabe (SCAN) trainiert wurde, für die kein Rollenschema verfügbar ist. Für dieses Modell entdeckt ROLE erfolgreich eine interpretierbare symbolische Struktur, die das Modell implizit verwendet, um die SCAN-Aufgabe auszuführen, und liefert damit einen umfassenden Bericht über die Verbindung zwischen den Darstellungen und dem Verhalten eines notorisch schwer zu interpretierenden Modells. Wir verifizieren die kausale Bedeutung der entdeckten symbolischen Struktur, indem wir zeigen, dass, wenn wir versteckte Einbettungen, die auf dieser symbolischen Struktur basieren, systematisch manipulieren, sich auch die Ausgabe des Modells in der von unserer Analyse vorhergesagten Weise ändert. Schließlich verwenden wir ROLE, um zu untersuchen, ob populäre Modelle zur Satzeinbettung kompositionelle Strukturen erfassen, und finden Hinweise darauf, dass dies nicht der Fall ist; wir schließen mit einer Diskussion darüber, wie die Erkenntnisse aus ROLE genutzt werden können, um neue induktive Vorurteile zu vermitteln, die die kompositionellen Fähigkeiten solcher Modelle verbessern.
Das visuelle System der Wirbeltiere ist hierarchisch organisiert, um visuelle Informationen in aufeinanderfolgenden Stufen zu verarbeiten. Die neuronalen Repräsentationen variieren drastisch in den ersten Stufen der visuellen Verarbeitung: Am Ausgang der Retina weisen die rezeptiven Felder (RF) der Ganglienzellen eine klare antagonistische Zentrum-Umgebung-Struktur auf, während im primären visuellen Kortex (V1) die typischen RF scharf auf eine präzise Orientierung abgestimmt sind. Mit Hilfe eines tiefen neuronalen Faltungsnetzwerks, das auf Bilderkennung als Modell des visuellen Systems trainiert wurde, zeigen wir, dass solche Unterschiede in der Repräsentation als direkte Folge unterschiedlicher neuronaler Ressourcenbeschränkungen für retinale und kortikale Netzwerke entstehen können. Zweitens stellen wir fest, dass bei einfachen, nachgeschalteten kortikalen Netzwerken die visuellen Repräsentationen am Netzhautausgang als nichtlineare und verlustbehaftete Merkmalsdetektoren auftreten, während sie bei komplexeren kortikalen Netzwerken als lineare und treue Kodierer der visuellen Szene auftreten.Dieses Ergebnis sagt voraus, dass die Netzhäute von kleinen Wirbeltieren (z.B. Salamander, Frosch) in der Lage sein sollten, die visuelle Szene zu erfassen. Dieses Ergebnis sagt voraus, dass die Netzhäute von kleinen Wirbeltieren (z.B. Salamander, Frosch) hochentwickelte nichtlineare Berechnungen durchführen und Merkmale extrahieren, die direkt für das Verhalten relevant sind, während die Netzhäute von großen Tieren wie Primaten die visuelle Szene hauptsächlich linear kodieren und auf ein viel breiteres Spektrum von Reizen reagieren sollten.
Es ist zwar noch nicht bewiesen, aber empirische Belege deuten darauf hin, dass die Modellgeneralisierung mit den lokalen Eigenschaften der Optima zusammenhängt, die über die Hessian beschrieben werden können.Wir verbinden die Modellgeneralisierung mit der lokalen Eigenschaft einer Lösung unter dem PAC-Bayes-Paradigma. Insbesondere beweisen wir, dass die Fähigkeit zur Modellverallgemeinerung mit der Hessian, den "Glättungs"-Termen höherer Ordnung, die durch die Lipschitz-Konstante der Hessian charakterisiert sind, und den Skalen der Parameter zusammenhängt.
Beim unüberwachten Lernen geht es um die Erfassung von Abhängigkeiten zwischen Variablen und um den Kontrast zwischen wahrscheinlichen und unwahrscheinlichen Konfigurationen dieser Variablen, oft entweder über ein generatives Modell, das nur wahrscheinliche Konfigurationen abfragt, oder mit einer Energiefunktion (unnormalisierte Log-Dichte), die niedrig für wahrscheinliche und hoch für unwahrscheinliche Konfigurationen ist.Hier betrachten wir sowohl das Lernen einer Energiefunktion als auch einen effizienten Näherungsmechanismus für die entsprechende Verteilung. Während der Kritiker (oder Diskriminator) in generativen adversen Netzen (GANs) lernt, Daten- und Generatorproben zu trennen, kann die Einführung eines Regularisierers zur Entropiemaximierung auf dem Generator die Interpretation des Kritikers in eine Energiefunktion umwandeln, die die Trainingsverteilung von allem anderen trennt und somit für Aufgaben wie die Erkennung von Anomalien oder Neuartigkeiten verwendet werden kann. Dieses Papier ist durch die ältere Idee motiviert, Stichproben im latenten Raum statt im Datenraum zu nehmen, weil sich die Ausführung einer Monte-Carlo-Markov-Kette (MCMC) im latenten Raum als einfacher und effizienter erwiesen hat und weil ein GAN-ähnlicher Generator Stichproben aus dem latenten Raum in Stichproben aus dem Datenraum umwandeln kann.Zu diesem Zweck zeigen wir, wie eine Markov-Kette im latenten Raum ausgeführt werden kann, deren Stichproben auf den Datenraum abgebildet werden können, wodurch bessere Stichproben entstehen. Um die Entropie am Ausgang des Generators zu maximieren, nutzen wir die kürzlich eingeführten neuronalen Schätzer der gegenseitigen Information und stellen fest, dass der resultierende Ansatz nicht nur eine nützliche Scoring-Funktion für die Erkennung von Anomalien erzeugt, sondern auch scharfe Samples (wie GANs), die die Modi gut abdecken, was zu hohen Inception- und Fréchet-Scores führt.
Neural Style Transfer hat sich zu einem beliebten Verfahren für die Generierung von Bildern verschiedener künstlerischer Stile mit Hilfe von neuronalen Faltungsnetzwerken entwickelt. Dieser jüngste Erfolg bei der Übertragung von Bildstilen hat die Frage aufgeworfen, ob ähnliche Methoden genutzt werden können, um den "Stil" von musikalischen Audiodaten zu verändern.In dieser Arbeit versuchen wir einen qualitativ hochwertigen Audiotransfer und eine Textursynthese im Zeitbereich auf langer Zeitskala, die harmonische, rhythmische und klangliche Elemente im Zusammenhang mit dem musikalischen Stil erfasst. Wir demonstrieren die Fähigkeit, zufällig initialisierte neuronale Faltungsnetze zu verwenden, um diese Aspekte des Musikstils von einem Stück auf ein anderes zu übertragen, indem wir drei verschiedene Repräsentationen von Audio verwenden: die log-Magnitude der Kurzzeit-Fourier-Transformation (STFT), das Mel-Spektrogramm und das Constant-Q-Transformations-Spektrogramm.Wir schlagen vor, diese Repräsentationen zu verwenden, um wahrnehmungsrelevante Merkmale von musikalischen Audioinhalten zu erzeugen und zu verändern. Schließlich zeigen wir, dass die überzeugendsten "Stil"-Transferbeispiele ein Ensemble dieser Repräsentationen verwenden, um die verschiedenen gewünschten Eigenschaften von Audiosignalen zu erfassen.
Aktuelle Sprachmodelle, die mit tiefen neuronalen Netzen trainiert werden, sind in der Lage, die bestehenden Konventionen in ihren Trainingsdaten zu verstehen und zu produzieren, aber sie sind nicht in der Lage, diese Konventionen flexibel und interaktiv zu adaptieren, wie es Menschen tun. Wir führen eine wiederholte Referenzaufgabe als Maßstab für Modelle der Anpassung in der Kommunikation ein und schlagen ein reguliertes kontinuierliches Lernsystem vor, das es einem künstlichen Agenten, der mit einem generischen Sprachmodell initialisiert wurde, ermöglicht, seinen Partner im Laufe der Zeit genauer und effizienter zu verstehen.
Wir stellen eine Pooling-Methode für Sätze von Merkmalsvektoren vor, die auf der Sortierung von Merkmalen über die Elemente des Satzes hinweg basiert und zur Konstruktion eines permutationsäquivarianten Autoencoders verwendet werden kann, der dieses Verantwortungsproblem vermeidet. Anhand eines Spielzeug-Datensatzes von Polygonen und einer Set-Version von MNIST zeigen wir, dass ein solcher Auto-Encoder wesentlich bessere Rekonstruktionen und Darstellungen erzeugt. Das Ersetzen der Pooling-Funktion in bestehenden Set-Encodern durch FSPool verbessert die Genauigkeit und Konvergenzgeschwindigkeit bei einer Vielzahl von Datensätzen.
Wir verwenden einen hierarchischen Ansatz, bei dem zwei Agenten darauf trainiert werden, zusammenzuarbeiten, um eine komplexe Navigationsaufgabe zu erfüllen.Ein Planer-Agent arbeitet auf einer höheren Ebene und schlägt einem Ausführungs-Agenten Unterziele vor.Der Ausführungs-Agent meldet dem Planer am Ende seiner Reihe von Operationen eine Einbettungszusammenfassung als zusätzliche Nebeninformation für den nächsten Unterzielvorschlag des Planers zurück. Wir zeigen, dass dieses Planner-Executor-Setup die Sample-Effizienz unserer Methode im Vergleich zu traditionellen Single-Agent-Ansätzen drastisch erhöht, indem es die Schwierigkeiten, die mit langen Serien von Aktionen mit einem spärlichen Belohnungssignal einhergehen, effektiv abmildert.In der anspruchsvollen Habitat-Umgebung, die das Navigieren in verschiedenen realistischen Innenräumen erfordert, zeigen wir, dass unser Ansatz eine signifikante Verbesserung gegenüber früheren Arbeiten zur Navigation bietet.
Diese Methoden sind unzuverlässig, wenn die Erklärung empfindlich auf Faktoren reagiert, die nicht zur Vorhersage des Modells beitragen. Wir verwenden einen einfachen und üblichen Vorverarbeitungsschritt - das Hinzufügen einer Mittelwertverschiebung zu den Eingabedaten -, um zu zeigen, dass eine Transformation ohne Auswirkung auf das Modell dazu führen kann, dass zahlreiche Methoden eine falsche Zuordnung vornehmen. Wir definieren Input-Invarianz als die Anforderung, dass eine Saliency-Methode die Empfindlichkeit des Modells in Bezug auf Transformationen des Inputs widerspiegelt.
Wir führen zwei Techniken ein, um die Effizienz von Transformers zu verbessern: Zum einen ersetzen wir die Punktprodukt-Attention durch eine Methode, die ortsabhängiges Hashing verwendet, wodurch sich ihre Komplexität von O(L^2) auf O(L) ändert, wobei L die Länge der Sequenz ist. Darüber hinaus verwenden wir reversible Residualschichten anstelle der Standardresiduale, wodurch die Aktivierungen nur einmal im Trainingsprozess gespeichert werden müssen, anstatt N-mal, wobei N die Anzahl der Schichten ist.
In dieser Arbeit zeigen wir, dass das Verstehen von Sprache mit Hilfe eines Leselerners ein vielversprechendes Vehikel für die Generalisierung auf neue Umgebungen ist. Wir schlagen ein fundiertes Lernproblem vor, Read to Fight Monsters (RTFM), bei dem der Agent gemeinsam über ein Sprachziel, relevante Dynamiken, die in einem Dokument beschrieben sind, und Umweltbeobachtungen nachdenken muss. Wir generieren prozedural Umgebungsdynamiken und entsprechende sprachliche Beschreibungen der Dynamiken, so dass Agenten lesen müssen, um neue Umgebungsdynamiken zu verstehen, anstatt sich bestimmte Informationen zu merken. txt2π ist ein Modell, das Drei-Wege-Interaktionen zwischen dem Ziel, dem Dokument und den Beobachtungen erfasst. Bei RTFM verallgemeinert txt2π auf neue Umgebungen mit Dynamiken, die während des Trainings durch Lesen nicht gesehen wurden. Darüber hinaus übertrifft unser Modell Basislösungen wie FiLM und sprachkonditionierte CNNs bei RTFM. Durch Curriculum-Lernen erzeugt txt2π Richtlinien, die sich bei komplexen RTFM-Aufgaben auszeichnen, die mehrere Argumentations- und Koreferenzschritte erfordern.
Eine offene Frage in der Deep Learning-Gemeinschaft ist, warum neuronale Netze, die mit Gradient Descent trainiert wurden, auf realen Datensätzen gut verallgemeinern, obwohl sie in der Lage sind, zufällige Daten anzupassen.Wir schlagen einen Ansatz zur Beantwortung dieser Frage vor, der auf einer Hypothese über die Dynamik des Gradientenabstiegs basiert, die wir kohärente Gradienten nennen: Wir unterstützen diese Hypothese mit heuristischen Argumenten und perturbativen Experimenten und skizzieren, wie dies mehrere allgemeine empirische Beobachtungen über Deep Learning erklären kann.Darüber hinaus ist unsere Analyse nicht nur deskriptiv, sondern präskriptiv.Es schlägt eine natürliche Modifikation des Gradientenabstiegs vor, die Overfitting stark reduzieren kann.
 Jüngste Fortschritte im Deep Learning haben vielversprechende Ergebnisse in vielen Low-Level-Vision-Aufgaben gezeigt.Allerdings ist die Lösung der Einzelbild-basierte Ansicht Synthese noch ein offenes Problem.Insbesondere die Erzeugung neuer Bilder auf parallele Kamera Ansichten gegeben ein einzelnes Eingabebild ist von großem Interesse, da es 3D-Visualisierung der 2D-Eingabe scenery.We vorschlagen, eine neuartige Netzwerk-Architektur, um stereoskopische Ansicht Synthese an beliebigen Kamerapositionen entlang der X-Achse, oder Deep 3D Pan, mit "t-förmigen" adaptive Kernel mit global und lokal adaptive Dilatationen ausgestattet durchzuführen. Die von uns vorgeschlagene Netzwerkarchitektur, das Monsternetz, wurde mit einem neuartigen t-förmigen adaptiven Kernel mit global und lokal adaptiver Dilatation entwickelt, der die globale Kameraverschiebung effizient in die lokalen 3D-Geometrien der Pixel des Zielbildes einbeziehen und diese verarbeiten kann, um natürlich aussehende 3D-Schwenkansichten zu synthetisieren, wenn ein 2D-Eingangsbild gegeben ist. In umfangreichen Experimenten mit dem KITTI-, CityScapes- und unserem VXXLXX_STEREO-Datensatz für Innenräume haben wir die Wirksamkeit unserer Methode unter Beweis gestellt: Unser Monsternetz übertrifft die State-of-the-Art-Methode SOTA in allen Metriken wie RMSE, PSNR und SSIM deutlich. Darüber hinaus ist die Disparitätsinformation, die aus dem "t-förmigen" Kernel extrahiert werden kann, viel zuverlässiger als die des SOTA für die unbeaufsichtigte monokulare Tiefenschätzung, was die Wirksamkeit unserer Methode bestätigt.
Leider ist der GPU-Speicher als Hardware-Ressource immer endlich, was die Bildauflösung, die Stapelgröße und die Lernrate einschränkt, die für eine bessere DNN-Leistung verwendet werden könnten.In diesem Papier schlagen wir einen neuartigen Trainingsansatz vor, der Re-Forwarding genannt wird und den Speicherverbrauch beim Training erheblich reduziert.Unser Ansatz findet automatisch eine Teilmenge von Knoten in einem DNN-Berechnungsgraphen und speichert Tensoren nur an diesen Knoten während des ersten Vorwärtslaufs. Die gesamten Speicherkosten ergeben sich aus der Summe (1) der Speicherkosten an der Untergruppe der Knoten und (2) der maximalen Speicherkosten unter den lokalen Weiterleitungen (Re-Forwarding-Prozess). Wir schlagen Theorien und Algorithmen vor, die optimale Speicherlösungen für DNNs mit entweder linearen oder beliebigen Berechnungsgraphen erreichen. Experimente zeigen, dass Re-forwarding bis zu 80% des Trainingsspeichers bei populären DNNs wie Alexnet, VGG, ResNet, Densenet und Inception net einspart.
Komprimierung ist ein wichtiger Schritt, um große neuronale Netze auf ressourcenbeschränkten Plattformen einzusetzen.als eine beliebte Komprimierungstechnik schränkt die Quantisierung die Anzahl der unterschiedlichen Gewichtungswerte ein und reduziert so die Anzahl der Bits, die für die Darstellung und Speicherung jedes Gewichts erforderlich sind.in diesem Papier untersuchen wir die Darstellungsleistung von quantisierten neuronalen Netzen. Zunächst beweisen wir die universelle Approximierbarkeit von quantisierten ReLU-Netzen für eine große Klasse von Funktionen und geben dann obere Schranken für die Anzahl der Gewichte und die Speichergröße für eine gegebene Approximationsfehlergrenze und die Bitbreite der Gewichte für funktionsunabhängige und funktionsabhängige Strukturen an. Unsere Ergebnisse zeigen, dass die Anzahl der Gewichte, die ein quantisiertes Netzwerk benötigt, um eine Approximationsfehlergrenze von $\epsilon$ zu erreichen, nicht mehr als das $\mathcal{O}\left(\log^5(1/\epsilon)\right)$-fache der Anzahl der Gewichte eines unquantisierten Netzwerks beträgt. Dieser Overhead ist von viel geringerer Größenordnung als die untere Schranke der Anzahl der Gewichte, die für die Fehlerschranke benötigt werden, was den empirischen Erfolg verschiedener Quantisierungsverfahren unterstützt.
Verstärkungslernen (Reinforcement Learning, RL) mit wertbasierten Methoden (z.B. Q-learning) hat sich in einer Vielzahl von Domänen wie Spielen und Empfehlungssystemen (RS) als erfolgreich erwiesen. Während es üblich ist, die Parametrisierung der Q-Funktion zu beschränken, um konkav in Aktionen zu sein, um das max-Q-Problem zu vereinfachen, kann eine solche Beschränkung zu einer Leistungsverschlechterung führen. Alternativ kann das max-Q-Problem NP-schwer sein, wenn die Q-Funktion mit einem generischen neuronalen Feed-Forward-Netzwerk (NN) parametrisiert ist. In dieser Arbeit schlagen wir die CAQL-Methode vor, die das Bellman-Residuum unter Verwendung von Q-Learning mit einem von mehreren Plug-and-Play-Aktionsoptimierern minimiert. Insbesondere zeigen wir unter Ausnutzung der Fortschritte der Optimierungstheorien in tiefen NN, dass das max-Q-Problem optimal mit gemischt-ganzzahliger Programmierung (MIP) gelöst werden kann - wenn die Q-Funktion über eine ausreichende Darstellungsstärke verfügt, führt diese MIP-basierte Optimierung zu besseren Richtlinien und ist robuster als Gegenstücke, z. B., Um das Training von CAQL zu beschleunigen, entwickeln wir drei Techniken, nämlich(i) dynamische Toleranz,(ii) duale Filterung und(iii) Clustering.Um die Inferenz von CAQL zu beschleunigen, führen wir die Aktionsfunktion ein, die gleichzeitig die optimale Politik lernt.Um die Effizienz von CAQL zu demonstrieren, vergleichen wir es mit modernsten RL-Algorithmen auf Benchmark-Problemen der kontinuierlichen Kontrolle, die verschiedene Grade von Aktionsbeschränkungen haben, und zeigen, dass CAQL die politikbasierten Methoden in stark eingeschränkten Umgebungen deutlich übertrifft.
Generative Adversarial Networks (GANs) haben sich als leistungsfähiger Rahmen für das Lernen von Stichproben aus komplexen Verteilungen erwiesen, aber GANs sind auch notorisch schwer zu trainieren, wobei Mode-Kollaps und Oszillationen ein häufiges Problem sind. Wir stellen die Hypothese auf, dass dies zumindest teilweise auf die Entwicklung der Generator-Verteilung und die katastrophale Vergessensneigung von neuronalen Netzen zurückzuführen ist, die dazu führt, dass der Diskriminator die Fähigkeit verliert, sich an synthetisierte Stichproben aus früheren Instanzen des Generators zu erinnern. Erstens zeigen wir, dass das GAN-Training einen interessanteren und realistischeren Maßstab für die Evaluierung von Methoden des kontinuierlichen Lernens darstellt als einige der kanonischeren Datensätze, und zweitens schlagen wir vor, Techniken des kontinuierlichen Lernens zu nutzen, um den Diskriminator zu erweitern und seine Fähigkeit zu erhalten, frühere Generatormuster zu erkennen.
Unseen Class Categorization (UCC) mit minimalen Informationen über die Zielklassen ist jedoch die am häufigsten anzutreffende Einstellung in der Industrie, die eine herausfordernde Forschungsproblem im maschinellen Lernen bleibt.Bisherige Ansätze zur UCC entweder nicht auf eine leistungsfähige diskriminierende Feature-Extraktor zu erzeugen oder nicht zu lernen, eine flexible Klassifikator, der leicht auf ungesehene Klassen angepasst werden kann.In diesem Papier schlagen wir vor, diese Probleme durch Netzwerk-Reparametrisierung, \textit{i. Dadurch entkoppeln wir den Teil der Merkmalsextraktion und den Klassifizierungsteil eines tiefen Klassifizierungsmodells, um es an die speziellen Gegebenheiten von UCC anzupassen und sowohl eine starke Unterscheidungsfähigkeit als auch eine hervorragende Anpassungsfähigkeit zu gewährleisten. Ausführliche Experimente für UCC auf mehreren weit verbreiteten Benchmark-Datensätzen in den Einstellungen des Zero-Shot- und Little-Shot-Lernens zeigen, dass unsere Methode mit Netzwerk-Reparametrisierung die beste Leistung erzielt.
Proteine sind allgegenwärtige Moleküle, deren Funktion in biologischen Prozessen durch ihre 3D-Struktur bestimmt wird. Die experimentelle Identifizierung der Struktur eines Proteins kann zeitaufwändig, prohibitiv teuer und nicht immer möglich sein. GraphQA ist eine graphenbasierte Methode zur Abschätzung der Qualität von Proteinmodellen, die vorteilhafte Eigenschaften wie Repräsentationslernen, explizite Modellierung sowohl der sequentiellen als auch der 3D-Struktur, geometrische Invarianz und Berechnungseffizienz aufweist. In dieser Arbeit demonstrieren wir signifikante Verbesserungen des Stands der Technik sowohl für handgefertigte als auch für Repräsentationslernansätze und evaluieren sorgfältig die einzelnen Beiträge von GraphQA.
Wir untersuchen das Problem des inkrementellen Trainings von Modellen des maschinellen Lernens durch aktives Lernen mit Zugang zu unvollkommenen oder verrauschten Orakeln und betrachten dabei speziell die Situation des aktiven Batch-Lernens, bei dem mehrere Stichproben ausgewählt werden, im Gegensatz zu einer einzigen Stichprobe wie in klassischen Einstellungen, um den Trainingsaufwand zu reduzieren. Unser Ansatz schlägt eine Brücke zwischen gleichmäßiger Zufälligkeit und punktebasiertem Wichtigkeitssampling von Clustern bei der Auswahl eines Stapels neuer Proben.Experimente mit Benchmark-Bildklassifizierungsdatensätzen (MNIST, SVHN und CIFAR10) zeigen Verbesserungen gegenüber bestehenden aktiven Lernstrategien.Wir führen eine zusätzliche Denoising-Schicht in tiefe Netzwerke ein, um aktives Lernen robust gegenüber Etikettengeräuschen zu machen und zeigen signifikante Verbesserungen.
Obwohl neuere neuronale Feed-Forward-Netzwerke in der Lage sind, stilisierte Bilder in Echtzeit zu erzeugen, produzieren diese Modelle eine einzige Stilisierung für ein Paar von Stil-/Inhaltsbildern, und der Benutzer hat keine Kontrolle über die synthetisierte Ausgabe. Darüber hinaus hängt die Stilübertragung von den Hyperparametern des Modells ab, die für verschiedene Eingabebilder unterschiedlich optimal sind, so dass der Benutzer, wenn ihm die stilisierte Ausgabe nicht zusagt, mehrere Modelle ausprobieren oder ein Modell mit anderen Hyperparametern neu trainieren muss, um eine bevorzugte Stilisierung zu erhalten. Diese Parameter ermöglichen es dem Benutzer, die synthetisierten Ergebnisse aus demselben Paar von Stil-/Inhaltsbildern zu modifizieren, um ein bevorzugtes stilisiertes Bild zu finden. Unsere quantitativen und qualitativen Experimente zeigen, dass das Anpassen dieser Parameter mit dem erneuten Trainieren des Modells mit anderen Hyperparametern vergleichbar ist.Wir zeigen auch, wie diese Parameter randomisiert werden können, um Ergebnisse zu generieren, die zwar unterschiedlich, aber dennoch sehr ähnlich in Stil und Inhalt sind.
Um diese Einschränkung zu überwinden, stellen wir einen Rahmen vor, in dem anweisungsabhängige RL-Agenten mit Hilfe von Belohnungen trainiert werden, die nicht aus der Umgebung stammen, sondern aus Belohnungsmodellen, die gemeinsam mit Expertenbeispielen trainiert werden.  Dieser Rahmen trennt effektiv die Darstellung dessen, was Anweisungen erfordern, von der Art und Weise, wie sie ausgeführt werden können, und ermöglicht es einem Agenten, in einer einfachen Gitterwelt eine Reihe von Befehlen zu erlernen, die eine Interaktion mit Blöcken und ein Verständnis für räumliche Beziehungen und unterspezifizierte abstrakte Anordnungen erfordern, und wir zeigen außerdem, dass die Methode es unserem Agenten ermöglicht, sich an Veränderungen in der Umgebung anzupassen, ohne dass neue Expertenbeispiele erforderlich sind.
Wir stellen Multitask Soft Option Learning (MSOL) vor, ein hierarchisches Multi-Task-Framework, das auf Planning-as-Inference basiert. MSOL erweitert das Konzept der Optionen, indem es separate Variations-Posterioren für jede Aufgabe verwendet, die durch einen gemeinsamen Prior reguliert werden. Die gelernten Soft-Optionen werden zeitlich erweitert, so dass eine übergeordnete Master-Policy schneller auf neue Aufgaben trainieren kann, indem sie Entscheidungen mit geringerer Häufigkeit trifft. Darüber hinaus ermöglicht MSOL eine Feinabstimmung der Soft-Optionen für neue Aufgaben, ohne vorheriges nützliches Verhalten zu verlernen, und vermeidet Probleme mit lokalen Minima im Multitask-Training.Wir zeigen empirisch, dass MSOL sowohl hierarchische als auch flache Transfer-Learning-Baselines in anspruchsvollen Multitask-Umgebungen deutlich übertrifft.
Wir schlagen einen Algorithmus vor, den Guided Variational Autoencoder (Guided-VAE), der in der Lage ist, ein kontrollierbares generatives Modell zu erlernen, indem er die latente Repräsentation entwirrend lernt. Das Lernziel wird erreicht, indem ein Signal für die latente Kodierung/Einbettung in VAE bereitgestellt wird, ohne die Hauptarchitektur des Backbones zu verändern, wodurch die wünschenswerten Eigenschaften des VAE beibehalten werden. In der unbeaufsichtigten Strategie leiten wir das Lernen der VAE durch die Einführung eines leichtgewichtigen Decoders, der latente geometrische Transformationen und Hauptkomponenten lernt; in der überwachten Strategie verwenden wir einen gegnerischen Anregungs- und Hemmungsmechanismus, um die Entflechtung der latenten Variablen zu fördern.Guided-VAE genießt seine Transparenz und Einfachheit für die allgemeine Repräsentationslernaufgabe sowie das Entflechtungslernen.In einer Reihe von Experimenten für das Repräsentationslernen wurden eine verbesserte Synthese/Abtastung, eine bessere Entflechtung für die Klassifizierung und reduzierte Klassifizierungsfehler beim Metalernen beobachtet.
Neuronale Sprachmodelle (NLMs) sind generativ und modellieren die Verteilung grammatikalischer Sätze. Trainiert auf einem großen Korpus, stoßen NLMs an die Grenzen der Modellierungsgenauigkeit, Durch eine erneute Bewertung der n-besten Liste kann NLM grammatikalisch korrektere Kandidaten aus der Liste auswählen und die Wort-/Zeichen-Fehlerrate erheblich reduzieren. Die generative Natur von NLM kann jedoch keine Unterscheidung zwischen "guten" und "schlechten" (in einem aufgabenspezifischen Sinne) Sätzen garantieren, was zu einer suboptimalen Leistung führt. Anders als das üblicherweise verwendete Maximum-Likelihood-Ziel zielt die vorgeschlagene Methode darauf ab, die Spanne zwischen "guten" und "schlechten" Sätzen zu vergrößern. Sie wird Ende-zu-Ende trainiert und kann in großem Umfang auf Aufgaben angewandt werden, die eine erneute Bewertung des dekodierten Textes beinhalten.Signifikante Gewinne werden sowohl bei ASR- als auch bei statistischen Maschinenübersetzungsaufgaben (SMT) beobachtet.
In diesem Papier schlagen wir vor, sample-spezifische Filter für Faltungsschichten im Vorwärtspass zu generieren, da die Filter on-the-fly generiert werden, wird das Modell flexibler und kann sich im Vergleich zu traditionellen CNNs besser an die Trainingsdaten anpassen.Um sample-spezifische Merkmale zu erhalten, extrahieren wir die intermediären Merkmalskarten aus einem Autoencoder. Da Filter in der Regel hochdimensional sind, schlagen wir vor, eine Reihe von Koeffizienten anstelle einer Reihe von Filtern zu erlernen. Diese Koeffizienten werden verwendet, um die Basisfilter aus einem Filter-Repository linear zu kombinieren, um die endgültigen Filter für ein CNN zu generieren. Die vorgeschlagene Methode wird auf MNIST-, MTFL- und CIFAR10-Datensätzen bewertet.
Im Vergleich zu konventionellen neuronalen Netzwerken, die nur in der Tiefe steuerbar sind, führt die erhöhte architektonische Vielfalt zu einer höheren Ressourcenauslastung und folglich zu einer Leistungsverbesserung unter verschiedenen und dynamischen Ressourcenbudgets.Wir heben architektonische Merkmale hervor, die unser Schema praktikabel und effizient machen, und zeigen seine Effektivität bei Bildklassifizierungsaufgaben.
Wir schlagen ein neues Modell vor, um verallgemeinerbare und vielfältige retrosynthetische Reaktionsvorhersagen zu machen. Bei einer Zielverbindung besteht die Aufgabe darin, die wahrscheinlichen chemischen Reaktanten vorherzusagen, um das Ziel zu produzieren. Diese generative Aufgabe kann als ein Sequenz-zu-Sequenz-Problem gerahmt werden, indem die SMILES-Darstellungen der Moleküle verwendet werden. Darüber hinaus integrieren wir ein diskretes latentes Variablenmodell in die Architektur, um das Modell zu ermutigen, einen vielfältigen Satz alternativer Vorhersagen zu produzieren. Auf der 50k Untermenge von Reaktionsbeispielen aus der US-Patentliteratur (USPTO-50k) Benchmark-Datensatz, verbessert unser Modell die Leistung gegenüber der Baseline erheblich, während es auch Vorhersagen erzeugt, die vielfältiger sind.
Die Disentanglement-PyTorch-Bibliothek wurde entwickelt, um die Forschung, die Implementierung und das Testen neuer Variationsalgorithmen zu erleichtern. In dieser modularen Bibliothek sind die neuronalen Architekturen, die Dimensionalität des latenten Raums und die Trainingsalgorithmen vollständig entkoppelt, was unabhängige und konsistente Experimente mit verschiedenen Variationsmethoden ermöglicht. Die Bibliothek umfasst bisher Implementierungen der folgenden unüberwachten Algorithmen: VAE, Beta-VAE, Factor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE und Beta-TCVAE sowie bedingte Ansätze wie CVAE und IFCVAE. Die Bibliothek ist kompatibel mit der Disentanglement Challenge von NeurIPS 2019, die auf AICrowd gehostet wird, und wurde verwendet, um in der ersten und zweiten Phase der Challenge zu konkurrieren, wo sie zu den besten wenigen Teilnehmern gehörte.
Während aktuelle Trust-Region-Strategien für kontinuierliche Kontrolle effektiv sind, erfordern sie typischerweise eine große Menge an On-Policy-Interaktion mit der Umgebung. Um dieses Problem zu lösen, schlagen wir eine Off-Policy-Trust-Region-Methode, Trust-PCL, vor, die eine Beobachtung ausnutzt, dass die optimalen Policy- und State-Werte eines Maximum-Reward-Ziels mit einem Relative-Entropie-Regularisierer eine Reihe von mehrstufigen pfadweisen Konsistenzen entlang eines beliebigen Pfades erfüllen. Die Einführung der relativen Entropie-Regularisierung ermöglicht es Trust-PCL, die Stabilität der Optimierung aufrechtzuerhalten und gleichzeitig Off-Policy-Daten zu nutzen, um die Stichprobeneffizienz zu verbessern. Bei der Auswertung einer Reihe von kontinuierlichen Steuerungsaufgaben verbessert Trust-PCL die Lösungsqualität und die Stichprobeneffizienz von TRPO erheblich.
Deep Reinforcement Learning hat in letzter Zeit viele Erfolge erzielt, aber unser Verständnis seiner Stärken und Grenzen wird durch den Mangel an reichhaltigen Umgebungen behindert, in denen wir optimales Verhalten vollständig charakterisieren und dementsprechend individuelle Aktionen anhand einer solchen Charakterisierung diagnostizieren können. Hier betrachten wir eine Familie von kombinatorischen Spielen, die aus der Arbeit von Erdos, Selfridge und Spencer hervorgegangen sind, und wir schlagen ihre Verwendung als Umgebungen für die Bewertung und den Vergleich verschiedener Ansätze zum Verstärkungslernen vor. Wir verwenden diese Erdos-Selfridge-Spencer-Spiele nicht nur zum Vergleich verschiedener Algorithmen, sondern auch zum Vergleich von Ansätzen, die auf überwachtem und verstärktem Lernen basieren, zur Analyse der Leistungsfähigkeit von Multi-Agenten-Ansätzen zur Verbesserung der Leistung und zur Bewertung der Verallgemeinerung auf Umgebungen außerhalb des Trainingssets.
Mehrere Methoden zur Schätzung der Modellunsicherheit wurden vorgeschlagen, aber diese Methoden schränken entweder die Art und Weise ein, wie das neuronale Netzwerk trainiert oder konstruiert wird. Wir stellen die Ausreißererkennung in neuronalen Netzwerken (ODIN) vor, eine annahmefreie Methode zur Erkennung von Ausreißerbeobachtungen während der Vorhersage, die auf Prinzipien basiert, die in der Überwachung von Fertigungsprozessen weit verbreitet sind. Durch die Verwendung einer linearen Approximation der versteckten Schicht fügen wir den Modellen nach dem Training eine Ausreißererkennung hinzu, ohne die Architektur oder das Training zu verändern. Wir zeigen, dass ODIN effizient Ausreißer während der Vorhersage bei Fashion-MNIST, ImageNet-Syndromen und Spracherkennung erkennt.
Diese Arbeit stellt ein einfaches Netzwerk zur Erzeugung von zeichenbewussten Worteinbettungen vor, wobei positionsunabhängige und positionsbewusste Zeicheneinbettungen kombiniert werden, um einen Einbettungsvektor für jedes Wort zu erzeugen.Die gelernten Wortrepräsentationen sind nachweislich sehr spärlich und ermöglichen verbesserte Ergebnisse bei Sprachmodellierungsaufgaben, obwohl deutlich weniger Parameter verwendet werden und ohne die Notwendigkeit, Dropout anzuwenden.Ein abschließendes Experiment legt nahe, dass Gewichtsteilung zur Spärlichkeit beiträgt, die Leistung erhöht und Überanpassung verhindert.
Neuronale Netze mit niedrigpräzisen Gewichten und Aktivierungen bieten überzeugende Effizienzvorteile gegenüber ihren Äquivalenten mit voller Präzision.Die beiden am häufigsten diskutierten Vorteile der Quantisierung sind ein geringerer Speicherverbrauch und ein schnellerer Vorwärtsdurchlauf, wenn sie mit effizienten bitweisen Operationen implementiert werden.Wir schlagen einen dritten Vorteil sehr niedrigpräziser neuronaler Netze vor: verbesserte Robustheit gegenüber einigen gegnerischen Angriffen und im schlimmsten Fall eine Leistung, die mit Modellen mit voller Präzision vergleichbar ist. Wir konzentrieren uns auf den Fall mit sehr geringer Genauigkeit, bei dem sowohl die Gewichte als auch die Aktivierungen auf $\pm$1 quantisiert sind, und stellen fest, dass die stochastische Quantisierung der Gewichte in nur einer Schicht die Auswirkungen iterativer Angriffe stark reduzieren kann. Wir stellen fest, dass nicht skalierte binäre neuronale Netze einen ähnlichen Effekt aufweisen wie das ursprüngliche \emph{defensive distillation}-Verfahren, das zu \emph{gradient masking} geführt hat, und eine falsche Vorstellung von Sicherheit vermitteln. Wir gehen dieses Problem an, indem wir sowohl Black-Box- als auch White-Box-Experimente mit binären Modellen durchführen, die Gradienten nicht künstlich maskieren.
Unsupervised bilingual dictionary induction (UBDI) ist nützlich für die unüberwachte maschinelle Übersetzung und für den sprachübergreifenden Transfer von Modellen in Sprachen mit geringen Ressourcen.Ein Ansatz für UBDI ist die Angleichung von Wortvektorräumen in verschiedenen Sprachen unter Verwendung von Generative adversarial networks (GANs) mit linearen Generatoren, die für mehrere Sprachpaare eine Spitzenleistung erzielen.Für einige Paare ist die GAN-basierte Induktion jedoch instabil oder versagt bei der Angleichung der Vektorräume vollständig. Wir zeigen, dass die Instabilität von der Form und der Dichte der Vektorsätze abhängt, aber nicht vom Rauschen; sie ist das Ergebnis lokaler Optima, aber weder eine Überparametrisierung noch eine Änderung der Batch-Größe oder der Lernrate reduziert die Instabilität konsequent.
Aufgrund der Allgegenwart von Computersoftware ist die Erkennung von Software-Schwachstellen (SVD) zu einem wichtigen Problem in der Softwareindustrie und im Bereich der Computersicherheit geworden. Ein möglicher Lösungsansatz ist der Einsatz von Deep Domain Adaptation, die in jüngster Zeit enorme Erfolge bei der Übertragung des Lernens von strukturell gekennzeichneten auf nicht gekennzeichnete Datenquellen verzeichnen konnte. Die allgemeine Idee besteht darin, sowohl Quell- als auch Zieldaten in einen gemeinsamen Merkmalsraum abzubilden und die Diskrepanz zwischen diesen Daten in diesem gemeinsamen Merkmalsraum zu schließen. Generative adversarial network (GAN) ist eine Technik, die versucht, die Diskrepanzlücke zu überbrücken und auch als ein Baustein für die Entwicklung von tiefen Domänenanpassungsansätzen mit State-of-the-Art-Leistung auftaucht. Unser Ziel in diesem Papier ist es, Dual-GD-DDAN (Dual Generator-Discriminator Deep Code Domain Adaptation Network) vorzuschlagen, um das Problem des Transfer-Lernens von gelabelten zu nicht gelabelten Software-Projekten im Kontext von SVD zu lösen, um das Problem des Modus-Kollabierens zu lösen, das bei früheren Ansätzen auftrat.
Representation Lernen ist eine der Grundlagen des Deep Learning und ermöglicht wichtige Verbesserungen auf mehrere Machine Learning Aufgaben, wie Neuronale Maschinelle Übersetzung, Fragebeantwortung und Spracherkennung.Jüngste Arbeiten haben neue Methoden für das Lernen Darstellungen für Knoten und Kanten in Graphen vorgeschlagen.Mehrere dieser Methoden basieren auf dem SkipGram Algorithmus, und sie in der Regel verarbeiten eine große Anzahl von Multi-Hop-Nachbarn, um den Kontext zu produzieren, aus denen Knoten Darstellungen gelernt werden. In diesem Papier schlagen wir eine effektive und auch effiziente Methode zur Erzeugung von Knoteneinbettungen in Graphen vor, die eine begrenzte Anzahl von Permutationen über die unmittelbare Nachbarschaft eines Knotens als Kontext verwendet, um seine Darstellung zu erzeugen, also egozentrische Darstellungen.Wir präsentieren eine gründliche Evaluierung, die zeigt, dass unsere Methode die State-of-the-Art-Methoden in sechs verschiedenen Datensätzen übertrifft, die mit den Problemen der Link-Vorhersage und der Knotenklassifizierung zusammenhängen, wobei sie eine bis drei Größenordnungen schneller ist als die Grundlinien, wenn sie Knoteneinbettungen für sehr große Graphen erzeugt.
Orthogonale rekurrente neuronale Netze lösen das Problem des verschwindenden Gradienten, indem sie die rekurrenten Verbindungen mit einer orthogonalen Matrix parametrisieren. diese Klasse von Modellen ist besonders effektiv, um Aufgaben zu lösen, die die Speicherung langer Sequenzen erfordern. wir schlagen eine alternative Lösung vor, die auf expliziter Speicherung mit linearen Autocodierern für Sequenzen basiert. Wir zeigen, wie eine kürzlich vorgeschlagene rekurrente Architektur, das Linear Memory Network, bestehend aus einer nichtlinearen Feedforward-Schicht und einer separaten linearen Rekursion, verwendet werden kann, um schwierige Gedächtnisaufgaben zu lösen.Wir schlagen ein Initialisierungsschema vor, das die Gewichte einer rekurrenten Architektur so einstellt, dass sie einen linearen Autoencoder der Eingabesequenzen annähert, der mit einer geschlossenen Formlösung gefunden werden kann.Das Initialisierungsschema kann leicht an jede rekurrente Architektur angepasst werden.    Wir argumentieren, dass dieser Ansatz einer zufälligen orthogonalen Initialisierung aufgrund des Autoencoders, der die Speicherung langer Sequenzen noch vor dem Training erlaubt, überlegen ist. Die empirische Analyse zeigt, dass unser Ansatz konkurrenzfähige Ergebnisse gegenüber alternativen orthogonalen Modellen und dem LSTM auf sequenziellen MNIST, permutierten MNIST und TIMIT erzielt.
Wir verwenden so genannte Black-Box-Langzeitgedächtnis-Encoder (LSTM), um State-of-the-Art-Ergebnisse zu erzielen, und bieten gleichzeitig ein aufschlussreiches Verständnis dessen, was das autoregressive Modell mit einem parallelen Mechanismus der Selbstaufmerksamkeit lernt.Insbesondere entkoppeln wir das Sequenzetikettierungsproblem der NER in ein Entitäts-Chunking, z.B, Barack_B Obama_E was_O elected_O, und Entity Typing, z.B. Barack_PERSON Obama_PERSON was_NONE elected_NONE, und analysieren, wie das Modell lernt oder Schwierigkeiten hat, Textmuster für jede der Teilaufgaben zu erfassen.Die Erkenntnisse, die wir gewinnen, führen uns dann dazu, einen anspruchsvolleren tiefen Cross-Bi-LSTM-Encoder zu erforschen, der sich als besser bei der Erfassung globaler Interaktionen erweist, sowohl angesichts empirischer Ergebnisse als auch einer theoretischen Rechtfertigung.
Wissensgrapheneinbettung (Knowledge Graph Embedding, KGE) ist die Aufgabe des gemeinsamen Lernens von Entitäts- und Beziehungseinbettungen für einen gegebenen Wissensgraphen. Bestehende Methoden zum Lernen von KGEs können als zweistufiger Prozess betrachtet werden, bei dem(a) Entitäten und Beziehungen im Wissensgraphen durch einige lineare algebraische Strukturen (Einbettungen) dargestellt werden und(b) eine Bewertungsfunktion definiert wird, die die Stärke einer Beziehung zwischen zwei Entitäten unter Verwendung der entsprechenden Beziehungs- und Entitätseinbettungen bewertet. Um dieses Problem zu beheben, schlagen wir eine generative Darstellung der KGE-Lernaufgabe vor: Bei einem Wissensgraphen, der durch eine Menge relationaler Tripel (h, R, t) repräsentiert wird, bei denen die semantische Beziehung R zwischen den beiden Entitäten h (Kopf) und t (Schwanz) besteht, erweitern wir das Random-Walk-Modell (Arora et al, Wir leiten eine theoretische Beziehung zwischen der gemeinsamen Wahrscheinlichkeit p(h, R, t) und den Einbettungen von h, R und t ab. Darüber hinaus zeigen wir, dass die Minimierung des marginalen Verlusts, ein beliebtes Ziel, das in vielen früheren Arbeiten zu KGE verwendet wurde, auf natürliche Weise aus der Maximierung des Log-Likelihood-Verhältnisses unter den aus den KGEs geschätzten Wahrscheinlichkeiten gemäß unserer theoretischen Beziehung folgt. Wir schlagen ein Lernziel vor, das durch die theoretische Analyse motiviert ist, um KGEs aus einem gegebenen Wissensgraphen zu erlernen. Die KGEs, die durch unsere vorgeschlagene Methode erlernt werden, erzielen eine Spitzenleistung auf den FB15K237- und WN18RR-Benchmark-Datensätzen und liefern empirische Beweise zur Unterstützung der Theorie.
Derzeit sind die einzigen Techniken für die gemeinsame Verwaltung eines Deep-Learning-Modells homomorphe Verschlüsselung und sichere Mehrparteienberechnung.Leider ist keine dieser Techniken für das Training großer neuronaler Netze aufgrund ihrer großen Rechen- und Kommunikationsoverheads anwendbar.Als skalierbare Technik für die gemeinsame Verwaltung eines Modells schlagen wir die Aufteilung eines Deep-Learning-Modells zwischen mehreren Parteien vor.In diesem Papier wird die Sicherheitsgarantie dieser Technik empirisch untersucht, die als Problem der Modellvervollständigung eingeführt wird:  Wie viel Training ist erforderlich, um die ursprüngliche Leistung des Modells wiederherzustellen, wenn der gesamte Trainingsdatensatz oder ein Umgebungssimulator und eine Teilmenge der Parameter eines trainierten Deep-Learning-Modells gegeben sind?  Unsere Experimente zeigen, dass (1) das Problem der Modellvervollständigung beim verstärkenden Lernen schwieriger ist als beim überwachten Lernen, da die Trajektorien des trainierten Agenten nicht verfügbar sind, und (2) die Schwierigkeit nicht in erster Linie von der Anzahl der Parameter des fehlenden Teils abhängt, sondern vielmehr von deren Art und Position.  Unsere Ergebnisse deuten darauf hin, dass das Modellsplitting in einigen Situationen, in denen das Training sehr teuer ist, eine praktikable Technik für die gemeinsame Modellführung sein könnte.
Im Gegensatz zu den bestehenden Methoden zur Übertragung von Domänen durch tiefe generative Modelle, wie StarGAN (Choi et al., 2017) und UFDN (Liu et al., 2018), ist die variable Domänenanpassung ein einheitlicher, skalierbarer und einfacher Rahmen für das Lernen mehrerer Verteilungen durch Variationsinferenz, 2018) hat die variationale Domänenanpassung drei Vorteile: Erstens sind die Proben des Ziels nicht erforderlich; stattdessen benötigt das Framework eine bekannte Quelle als Prior $p(x)$ und binäre Diskriminatoren $p(\mathcal{D}_i|x)$, die die Zieldomäne $\mathcal{D}_i$ von anderen diskriminieren. Folglich betrachtet der Rahmen ein Ziel als ein Posterior, das explizit durch die Bayes'sche Inferenz $p(x|\mathcal{D}_i) \propto p(\mathcal{D}_i|x)p(x)$ formuliert werden kann, wie ein weiteres vorgeschlagenes Modell des dualen Variations-Auto-Encoders (DualVAE) zeigt. Ebenso wie VAE eine Stichprobe $x$ als Modus auf einem latenten Raum kodiert: $\mu(x) \in \mathcal{Z}$, kodiert DualVAE eine Domäne $\mathcal{D}_i$ als Modus auf dem dualen latenten Raum $\mu^*(\mathcal{D}_i) \in \mathcal{Z}^*$, genannt Domäneneinbettung. Sie formuliert das Posterior mit einer natürlichen Parierung $\langle, \rangle um: \mathcal{Z} \times \mathcal{Z}^* \rightarrow \Real$, das auf nicht abzählbare, unendliche Domänen, wie z.B. kontinuierliche Domänen, sowie Interpolation ausgedehnt werden kann. Drittens konvergiert DualVAE im Vergleich zu GANs schnell ohne aufwendige automatische/manuelle Hyperparametersuche, da es nur einen zusätzlichen Parameter zu VAE benötigt. Durch das numerische Experiment demonstrieren wir die drei Vorteile mit einer Multidomänen-Bilderzeugungsaufgabe auf CelebA mit bis zu 60 Domänen, und es zeigt sich, dass DualVAE die beste Leistung im Vergleich zu StarGAN und UFDN aufweist.
Wir schlagen eine neue Methode zum Trainieren neuronaler Netze vor, die auf einer neuartigen Kombination aus gegnerischem Training und beweisbaren Verteidigungsmaßnahmen basiert. Wir zeigen experimentell, dass diese Trainingsmethode vielversprechend ist und das Beste aus beiden Welten erreicht - sie erzeugt ein Modell mit der modernsten Genauigkeit (74,8%) und zertifizierter Robustheit (55,9%) auf dem anspruchsvollen CIFAR-10-Datensatz mit einer 2/255 L-Unendlichkeits-Störung. Dies ist eine erhebliche Verbesserung gegenüber den derzeit bekannten besten Ergebnissen von 68,3% Genauigkeit und 53,9% zertifizierter Robustheit, die mit einem fünfmal größeren Netzwerk als unserer Arbeit erzielt wurden.
Lernaufgaben auf Quellcode (d.h., Die meisten Arbeiten haben jedoch versucht, Methoden der natürlichen Sprache zu übertragen und nutzen nicht die einzigartigen Möglichkeiten, die die bekannte Syntax des Codes bietet. So werden beispielsweise weitreichende Abhängigkeiten, die durch die Verwendung derselben Variablen oder Funktion an verschiedenen Stellen entstehen, oft nicht berücksichtigt. Wir schlagen vor, Graphen zu verwenden, um sowohl die syntaktische als auch die semantische Struktur von Code darzustellen, und Graph-basierte Deep-Learning-Methoden zu verwenden, um zu lernen, über Programmstrukturen nachzudenken.In dieser Arbeit stellen wir vor, wie Graphen aus Quellcode konstruiert werden können und wie das Training von Gated Graph Neural Networks auf solch große Graphen skaliert werden kann.Wir evaluieren unsere Methode an zwei Aufgaben: Unser Vergleich mit Methoden, die weniger strukturierte Programmrepräsentationen verwenden, zeigt die Vorteile der Modellierung bekannter Strukturen und deutet darauf hin, dass unsere Modelle lernen, sinnvolle Namen abzuleiten und die VarMisuse-Aufgabe in vielen Fällen zu lösen.Darüber hinaus zeigten unsere Tests, dass VarMisuse eine Reihe von Fehlern in ausgereiften Open-Source-Projekten identifiziert.
Hier stellen wir diese Logik in Frage und untersuchen Kriterien für Overfitting, ohne einen Holdout-Datensatz zu verwenden. Konkret trainieren wir ein Modell für eine feste Anzahl von Epochen mehrmals mit unterschiedlichen Anteilen an randomisierten Labels und für eine Reihe von Regularisierungsstärken. Ein korrekt trainiertes Modell sollte nicht in der Lage sein, eine Genauigkeit zu erreichen, die größer ist als der Anteil der korrekt beschrifteten Datenpunkte, andernfalls wird das Modell übererfüllt. Wir führen zwei Kriterien zur Erkennung von Überanpassung und eines zur Erkennung von Unteranpassung ein, wobei wir das frühe Anhalten, den Regularisierungsfaktor und die Netzwerktiefe analysieren.Bei sicherheitskritischen Anwendungen sind wir an Modellen und Parametereinstellungen interessiert, die eine gute Leistung erbringen und bei denen eine Überanpassung unwahrscheinlich ist.Die Methoden dieser Arbeit ermöglichen die Charakterisierung und Identifizierung solcher Modelle.
Partiell beobachtbare Markov-Entscheidungsprozesse (POMDPs) sind ein weit verbreiteter Rahmen für die Modellierung von Entscheidungsprozessen mit unsicherer Umwelt und stochastischem Ausgang.In konventionellen POMDP-Modellen stammen die Beobachtungen, die der Agent erhält, aus einer festen, bekannten Verteilung.In einer Vielzahl von realen Szenarien spielt der Agent jedoch eine aktive Rolle in seiner Wahrnehmung, indem er auswählt, welche Beobachtungen er erhält. Um eine solche Erweiterung des Aktionsraums zu verhindern, schlagen wir eine gierige Strategie für die Beobachtungsauswahl vor, die darauf abzielt, die Unsicherheit im Zustand zu minimieren. Wir entwickeln einen neuartigen punktbasierten Wert-Iterationsalgorithmus, der die Gierstrategie einbezieht, um eine nahezu optimale Unsicherheitsreduzierung für abgetastete Glaubenspunkte zu erreichen, was wiederum dem Löser ermöglicht, den erreichbaren Unterraum des Glaubenssimplex effizient zu approximieren, indem er im Wesentlichen Berechnungen im Zusammenhang mit der Wahrnehmung von der Planung trennt.
Tiefe neuronale Netze, insbesondere Faltungsneuronale Netze, haben sich zu hocheffektiven Werkzeugen für die Komprimierung von Bildern und die Lösung von inversen Problemen entwickelt, wie z.B. die Rauschunterdrückung, das Inpainting und die Rekonstruktion von wenigen und verrauschten Messungen.Dieser Erfolg kann zum Teil auf ihre Fähigkeit zurückgeführt werden, natürliche Bilder gut darzustellen und zu erzeugen.Im Gegensatz zu klassischen Werkzeugen, wie z.B. Wavelets, haben bildgenerierende tiefe neuronale Netze eine große Anzahl von Parametern - typischerweise ein Vielfaches ihrer Ausgangsdimension - und müssen auf großen Datensätzen trainiert werden. In diesem Papier schlagen wir ein untrainiertes, einfaches Bildmodell vor, das als Deep Decoder bezeichnet wird und ein tiefes neuronales Netzwerk ist, das natürliche Bilder aus sehr wenigen Gewichtsparametern erzeugen kann. Der tiefe Dekoder ist einfach in dem Sinne, dass jede Schicht eine identische Struktur hat, die nur aus einer Upsampling-Einheit, einer pixelweisen linearen Kombination von Kanälen, einer ReLU-Aktivierung und einer kanalweisen Normalisierung besteht.
In diesem Papier untersuchen wir die Familie der Funktionen, die durch tiefe neuronale Netze (DNN) mit gleichgerichteten linearen Einheiten (ReLU) darstellbar sind, und geben einen Algorithmus an, um ein ReLU-DNN mit einer versteckten Schicht zu globaler Optimalität zu trainieren, wobei die Laufzeit polynomial zur Datengröße ist, obwohl sie exponentiell zur Eingangsdimension ist. Darüber hinaus verbessern wir die bekannten unteren Schranken für die Größe (von exponentiell bis superexponentiell) für die Approximation einer ReLU-Deep-Net-Funktion durch ein flacheres ReLU-Netz.Unsere Lückentheoreme gelten für glatt parametrisierte Familien von ``harten'' Funktionen, im Gegensatz zu den in der Literatur bekannten abzählbaren, diskreten Familien.  Eine beispielhafte Konsequenz unserer Lückentheoreme ist die folgende: Für jede natürliche Zahl $k$ existiert eine Funktion, die durch ein ReLU-DNN mit $k^2$ versteckten Schichten und einer Gesamtgröße von $k^3$ dargestellt werden kann, so dass jedes ReLU-DNN mit höchstens $k$ versteckten Schichten mindestens $\frac12k^{k+1}-1$ Gesamtknoten benötigt. Schließlich zeigen wir für die Familie der $\R^n\- bis \R$-DNNs mit ReLU-Aktivierungen eine neue Untergrenze für die Anzahl der affinen Teile, die in bestimmten Regimen der Netzwerkarchitektur größer ist als frühere Konstruktionen, und vor allem wird unsere Untergrenze durch eine explizite Konstruktion einer \emph{glatt parametrisierten} Familie von Funktionen demonstriert, die diese Skalierung erreichen.
Der jüngste Erfolg von tiefen Netzwerken im Bereich des maschinellen Lernens und der künstlichen Intelligenz hat jedoch zu einer Reihe von Vorschlägen inspiriert, wie das Gehirn über mehrere Schichten hinweg lernen und somit BP implementieren oder approximieren könnte. Bislang wurde keiner dieser Vorschläge rigoros an Aufgaben evaluiert, bei denen sich BP-gesteuertes tiefes Lernen als kritisch erwiesen hat, oder in Architekturen, die stärker strukturiert sind als einfache, vollständig verbundene Netzwerke. Hier präsentieren wir die ersten Ergebnisse zur Skalierung eines biologisch motivierten Modells des tiefen Lernens auf Datensätze, die tiefe Netzwerke mit geeigneten Architekturen benötigen, um eine gute Leistung zu erzielen.  Für CIFAR-10 zeigen wir, dass unser Algorithmus, eine einfache, gewichtstransportfreie Variante der DTP (Difference Target Propagation), die modifiziert wurde, um die Backpropagation aus der vorletzten Schicht zu entfernen, mit BP konkurrieren kann, wenn es darum geht, tiefe Netzwerke mit lokal definierten rezeptiven Feldern zu trainieren, die ungebundene Gewichte haben.  Für ImageNet stellen wir fest, dass sowohl DTP als auch unser Algorithmus signifikant schlechter als BP abschneiden, was die Frage aufwirft, ob unterschiedliche Architekturen oder Algorithmen erforderlich sind, um diese Ansätze zu skalieren Unsere Ergebnisse und Implementierungsdetails helfen dabei, Grundlinien für biologisch motivierte Deep-Learning-Schemata zu etablieren.
Tiefe neuronale Netze (DNNs) enthalten in der Regel Millionen, vielleicht sogar Milliarden von Parametern/Gewichten, was sowohl die Speicherung als auch die Berechnung sehr teuer macht, was eine Vielzahl von Arbeiten motiviert hat, die Komplexität des neuronalen Netzes durch die Verwendung von Regularisierern zu reduzieren, die Sparsamkeit induzieren.  Ein weiterer bekannter Ansatz zur Kontrolle der Komplexität von DNNs ist die gemeinsame Nutzung von Parametern, bei der bestimmte Gruppen von Gewichten gezwungen werden, einen gemeinsamen Wert zu teilen.Einige Formen der gemeinsamen Nutzung von Gewichten sind fest verdrahtet, um bestimmte Invarianten auszudrücken, wobei ein bemerkenswertes Beispiel die Shift-Invarianz von Faltungsschichten ist. In dieser Arbeit verwenden wir einen kürzlich vorgeschlagenen sparsamkeitsinduzierenden Regularisierer namens GrOWL (group ordered weighted l1), der Sparsamkeit fördert und gleichzeitig lernt, welche Gruppen von Parametern einen gemeinsamen Wert haben sollten.GrOWL hat sich in der linearen Regression als effektiv erwiesen, da es in der Lage ist, stark korrelierte Kovariaten zu identifizieren und zu bewältigen.Im Gegensatz zu Standard sparsamkeitsinduzierenden Regularisierern (z.B., l1 a.k.a.Lasso), eliminiert GrOWL nicht nur unwichtige Neuronen, indem es alle entsprechenden Gewichte auf Null setzt, sondern identifiziert auch explizit stark korrelierte Neuronen, indem es die entsprechenden Gewichte an einen gemeinsamen Wert bindet.Diese Fähigkeit von GrOWL motiviert das folgende zweistufige Verfahren: (i) Verwendung der GrOWL-Regularisierung im Trainingsprozess, um gleichzeitig signifikante Neuronen und Gruppen von Parametern zu identifizieren, die miteinander verbunden werden sollten; (ii) erneutes Trainieren des Netzwerks, wobei die in der vorherigen Phase aufgedeckte Struktur durchgesetzt wird, d.h. nur die signifikanten Neuronen werden beibehalten und die gelernte Verbindungsstruktur wird durchgesetzt.
Weight-Sharing - die gleichzeitige Optimierung mehrerer neuronaler Netze mit denselben Parametern - hat sich als Schlüsselkomponente der modernen neuronalen Architektursuche herauskristallisiert, deren Erfolg jedoch nur unzureichend verstanden und oft als überraschend empfunden wird. Algorithmisch zeigen wir, wie die Geometrie des ERM für die Gewichtsteilung eine größere Sorgfalt bei der Entwicklung von gradientenbasierten Minimierungsmethoden erfordert, und wenden Werkzeuge aus der nicht-konvexen nicht-euklidischen Optimierung an, um Allzweckalgorithmen zu entwickeln, die sich an die zugrunde liegende Struktur anpassen. Anhand von Fallstudien zur Kernelkonfiguration und zur Auswahl von NLP-Merkmalen demonstrieren wir, wie die Gewichtsteilung auf die Verallgemeinerung der Architektursuche von NAS angewendet werden kann und das resultierende bilevel-Ziel effektiv optimiert. Schließlich verwenden wir unsere Optimierungsanalyse, um eine einfache exponentielle Gradientenmethode für NAS zu entwickeln, die sich an die zugrunde liegende Optimierungsgeometrie anpasst und den modernsten Ansätzen auf CIFAR-10 entspricht.
Die verlustfreie Komprimierung ist eine Anwendung dieser Modelle, die zwar das Potenzial hat, sehr nützlich zu sein, aber noch nicht praktisch umgesetzt wurde. Wir stellen "Bits Back with ANS" (BB-ANS) vor, ein Verfahren zur verlustfreien Komprimierung mit latenten Variablenmodellen mit einer nahezu optimalen Rate. Wir demonstrieren dieses Schema, indem wir es zur Komprimierung des MNIST-Datensatzes mit einem Variations-Auto-Encoder-Modell (VAE) verwenden und dabei Kompressionsraten erzielen, die denen von Standardmethoden mit nur einem einfachen VAE überlegen sind. Da das Schema in hohem Maße parallelisierbar ist, kommen wir zu dem Schluss, dass dieses Schema mit einem generativen Modell von ausreichend hoher Qualität verwendet werden könnte, um erhebliche Verbesserungen der Kompressionsrate bei akzeptabler Laufzeit zu erzielen. Wir stellen unsere Implementierung als Open Source unter https://github.com/bits-back/bits-back zur Verfügung.
Die Abstimmung von Hyperparametern ist wohl der wichtigste Bestandteil, um bei tiefen Netzen die bestmögliche Leistung zu erzielen.  Wir konzentrieren uns auf Hyperparameter, die mit dem Optimierungsalgorithmus zusammenhängen, z.B. Lernraten, die einen großen Einfluss auf die Trainingsgeschwindigkeit und die daraus resultierende Genauigkeit haben.Typischerweise werden während des Trainings feste Lernratenschemata verwendet.Wir schlagen Hyperdyn vor, eine dynamische Hyperparameter-Optimierungsmethode, die am Ende jeder Epoche neue Lernraten auswählt.Unser Explore-Exploit-Framework kombiniert Bayes'sche Optimierung (BO) mit einer Ablehnungsstrategie, die auf einem einfachen probabilistischen Abwartetest basiert.   Wir erzielen auf den CIFAR- und Imagenet-Datensätzen die besten Genauigkeitsresultate, aber mit deutlich schnellerem Training, verglichen mit den besten manuell abgestimmten Netzwerken.
Die Beantwortung von textbasierten Fragen mit mehreren Schritten ist eine aktuelle Herausforderung im Bereich des maschinellen Verstehens. In diesem Papier schlagen wir eine neuartige Architektur vor, das sogenannte Latent Question Reformulation Network (LQR-net), ein Multi-Hop- und paralleles, aufmerksames Netzwerk, das für die Beantwortung von Fragen entwickelt wurde, die logische Fähigkeiten erfordern.LQR-net besteht aus einer Assoziation von \textbf{Lesemodulen} und \textbf{Reformulierungsmodulen}.Der Zweck des Lesemoduls ist es, eine fragebewusste Darstellung des Dokuments zu erzeugen. Wir evaluieren unsere Architektur anhand des \hotpotqa-Datensatzes zur Beantwortung von Fragen, der zur Bewertung von Multi-Hop-Reasoning-Fähigkeiten entwickelt wurde. Unser Modell erzielt konkurrenzfähige Ergebnisse in der öffentlichen Rangliste und übertrifft die besten aktuellen \textit{published}-Modelle in Bezug auf Exact Match (EM) und $F_1$-Score. Schließlich zeigen wir, dass eine Analyse der sequenziellen Reformulierungen interpretierbare Argumentationspfade liefern kann.
Wir schlagen eine Methode zur automatischen Berechnung der Wichtigkeit von Merkmalen bei jeder Beobachtung in Zeitreihen vor, indem wir kontrafaktische Trajektorien für frühere Beobachtungen simulieren.Wir definieren die Wichtigkeit jeder Beobachtung als die Änderung der Modellausgabe, die durch das Ersetzen der Beobachtung durch eine generierte verursacht wird.Unsere Methode kann auf beliebig komplexe Zeitreihenmodelle angewandt werden.Wir vergleichen die generierte Merkmalswichtigkeit mit bestehenden Methoden wie Sensitivitätsanalysen, Merkmalsverdeckung und anderen Erklärungsgrundlagen, um zu zeigen, dass unser Ansatz präzisere Erklärungen erzeugt und weniger empfindlich auf Rauschen in den Eingangssignalen reagiert.
Dieses Papier befasst sich mit der unbeaufsichtigten Domänenanpassung, d.h. mit der Situation, in der beschriftete Trainingsdaten in einer Quelldomäne verfügbar sind, das Ziel aber darin besteht, eine gute Leistung in einer Zieldomäne mit nur unbeschrifteten Daten zu erzielen.  Jede selbstüberwachte Aufgabe bringt die beiden Domänen entlang der für diese Aufgabe relevanten Richtung näher zusammen. Das Training dieser Aufgabe zusammen mit dem Klassifikator der Hauptaufgabe auf der Quelldomäne zeigt, dass sie erfolgreich auf die unbeschriftete Zieldomäne verallgemeinert werden kann.  Das vorgestellte Ziel ist einfach zu implementieren und leicht zu optimieren; wir erzielen Spitzenergebnisse bei vier von sieben Standard-Benchmarks und konkurrenzfähige Ergebnisse bei der Segmentierungsanpassung; wir zeigen auch, dass sich unsere Methode gut mit einer anderen beliebten Anpassungsmethode auf Pixelebene kombinieren lässt.
Wir stellen einfache, effiziente Algorithmen zur Berechnung eines MinHash einer Wahrscheinlichkeitsverteilung vor, die sowohl für spärliche als auch für dichte Daten geeignet sind und in beiden Fällen die gleichen Laufzeiten haben wie der Stand der Technik.Die Kollisionswahrscheinlichkeit dieser Algorithmen ist ein neues Maß für die Ähnlichkeit positiver Vektoren, das wir im Detail untersuchen. Die Kollisionswahrscheinlichkeit ist ein neues Maß für die Ähnlichkeit positiver Vektoren, das wir im Detail untersuchen. Wir beschreiben, inwiefern diese Kollisionswahrscheinlichkeit für jedes Locality Sensitive Hash, das auf Sampling basiert, optimal ist, und argumentieren, dass dieses Ähnlichkeitsmaß für Wahrscheinlichkeitsverteilungen nützlicher ist als die Ähnlichkeit, die von anderen Algorithmen für gewichtete MinHash verfolgt wird, und die natürliche Verallgemeinerung des Jaccard-Indexes darstellt.
Unter den bestehenden Modellen ist Graph Neuronale Netze (GNNs) einer der effektivsten Ansätze für Multi-Hop Relational Reasoning.In der Tat, Multi-Hop Relational Reasoning ist unverzichtbar in vielen Aufgaben der Verarbeitung natürlicher Sprache wie Relation Extraktion.In diesem Papier, schlagen wir vor, um die Parameter von Graph Neuronale Netze (GP-GNNs) nach natürlichen Sprache Sätze, die GNNs zu verarbeiten Relational Reasoning auf unstrukturierte Texteingaben ermöglicht generieren. Wir verifizieren GP-GNNs in der Beziehungsextraktion aus Text.Experimentelle Ergebnisse auf einem von Menschen kommentierten Datensatz und zwei fernüberwachten Datensätzen zeigen, dass unser Modell signifikante Verbesserungen im Vergleich zu den Baselines erzielt.Wir führen auch eine qualitative Analyse durch, um zu zeigen, dass unser Modell genauere Beziehungen durch Multi-Hop-Relational Reasoning entdecken könnte.
Off-Policy Actor-Critic (Off-PAC) Methoden haben sich in einer Vielzahl von kontinuierlichen Steuerungsaufgaben als erfolgreich erwiesen.Normalerweise wird die Action-Value-Funktion des Kritikers mit Hilfe der temporalen Differenz aktualisiert, und der Kritiker wiederum liefert einen Verlust für den Akteur, der ihn trainiert, Aktionen mit höherem erwarteten Ertrag auszuführen.In diesem Beitrag führen wir eine neuartige und flexible Meta-Kritik ein, die den Lernprozess beobachtet und einen zusätzlichen Verlust für den Akteur ermittelt, der das Lernen der Akteurskritik beschleunigt und verbessert. Im Vergleich zur Vanilla-Kritik wird das metakritische Netzwerk explizit trainiert, um den Lernprozess zu beschleunigen; und im Vergleich zu bestehenden Meta-Lernalgorithmen wird die Meta-Kritik schnell online für eine einzelne Aufgabe gelernt, anstatt langsam über eine Familie von Aufgaben.Entscheidend ist, dass unser metakritisches Framework für Off-Picy-basierte Lerner entwickelt wurde, die derzeit die modernste Effizienz von Verstärkungslernbeispielen bieten.Wir zeigen, dass das Online-Meta-Kritik-Lernen zu Verbesserungen in einer Vielzahl von kontinuierlichen Kontrollumgebungen führt, wenn es mit den zeitgenössischen Off-PAC-Methoden DDPG, TD3 und dem modernsten SAC kombiniert wird.
Moderne neuronale Netze sind in hohem Maße überparametrisiert, mit der Fähigkeit, wesentlich über die Trainingsdaten zu passen.Dennoch sind diese Netze oft gut in der Praxis zu verallgemeinern.Es wurde auch beobachtet, dass trainierte Netze können oft ``komprimiert werden, um viel kleinere Darstellungen.Der Zweck dieses Papiers ist es, diese beiden empirischen Beobachtungen zu verbinden.Unsere wichtigsten technischen Ergebnis ist eine Verallgemeinerung Schranke für komprimierte Netze auf der Grundlage der komprimierten Größe, die, kombiniert mit off-the-shelf Kompressionsalgorithmen, führt zu State-of-the-Art-Generalisierung garantiert. Darüber hinaus zeigen wir, dass die Komprimierbarkeit von Modellen, die zur Überanpassung neigen, begrenzt ist. Empirische Ergebnisse zeigen, dass eine Zunahme der Überanpassung die Anzahl der Bits erhöht, die zur Beschreibung eines trainierten Netzwerks erforderlich sind.
Ursprüngliche Modelle von adversen Angriffen werden hauptsächlich im Kontext von Klassifizierungs- und Computer-Vision-Aufgaben untersucht. Während mehrere Angriffe im Bereich der natürlichen Sprachverarbeitung (NLP) vorgeschlagen wurden, unterscheiden sie sich oft in der Definition der Parameter eines Angriffs und wie ein erfolgreicher Angriff aussehen würde.Das Ziel dieser Arbeit ist es, ein vereinheitlichendes Modell von adversen Beispielen vorzuschlagen, das für NLP-Aufgaben sowohl im generativen als auch im Klassifizierungsbereich geeignet ist. Wir definieren den Begriff "adversarial gain": Er basiert auf der Kontrolltheorie und ist ein Maß für die Veränderung des Outputs eines Systems im Verhältnis zur Störung des Inputs (verursacht durch den sogenannten "adversary"), der dem Lernenden präsentiert wird. Dieser Begriff des gegnerischen Gewinns bietet nicht nur eine nützliche Methode zur Bewertung von Gegnern und Verteidigungsmaßnahmen, sondern kann aufgrund seiner Verwurzelung in der Stabilitäts- und Mannigfaltigkeitstheorie als Baustein für zukünftige Arbeiten zur Robustheit unter Gegnern dienen.
Wir schlagen ein Warped Residual Network (WarpNet) vor, das einen parallelisierbaren Warp-Operator für die Vorwärts- und Rückwärtspropagation zu entfernten Schichten verwendet, der schneller trainiert als das ursprüngliche residuale neuronale Netzwerk.Wir wenden eine Störungstheorie auf residuale Netzwerke an und entkoppeln die Interaktionen zwischen residualen Einheiten.Der resultierende Warp-Operator ist eine Annäherung erster Ordnung an die Ausgabe über mehrere Schichten.Die Störungstheorie erster Ordnung weist Eigenschaften wie binomiale Pfadlängen und exponentielle Gradientenskalierung auf, die von Veit et al. (2016) experimentell gefunden wurden. Wir zeigen durch eine umfangreiche Leistungsstudie, dass das vorgeschlagene Netzwerk eine vergleichbare Vorhersageleistung wie das ursprüngliche Residualnetzwerk mit der gleichen Anzahl von Parametern erreicht und gleichzeitig eine erhebliche Beschleunigung der Gesamttrainingszeit erzielt.Da WarpNet eine Modellparallelität bei der Ausbildung des Residualnetzwerks durchführt, bei der die Gewichte auf verschiedene GPUs verteilt werden, bietet es eine Beschleunigung und die Möglichkeit, größere Netzwerke im Vergleich zu den ursprünglichen Residualnetzwerken zu trainieren.
Eine Vielzahl von Methoden, die versuchen, Vorhersagen von Black-Box-Modellen zu erklären, wurden von der Explainable Artificial Intelligence (XAI)-Gemeinschaft vorgeschlagen. Dennoch ist die Messung der Qualität der generierten Erklärungen weitgehend unerforscht, was quantitative Vergleiche nicht trivial macht.In dieser Arbeit schlagen wir eine Reihe von vielschichtigen Metriken vor, die es uns ermöglichen, Erklärer objektiv zu vergleichen, basierend auf der Korrektheit, Konsistenz und dem Vertrauen der generierten Erklärungen. Diese Metriken sind rechnerisch kostengünstig, erfordern kein Modelltraining und können für verschiedene Datenmodalitäten verwendet werden. wir evaluieren sie an gängigen Erklärern wie Grad-CAM, SmoothGrad, LIME und Integrated Gradients. unsere Experimente zeigen, dass die vorgeschlagenen Metriken qualitative Beobachtungen widerspiegeln, die in früheren Arbeiten berichtet wurden.
Neuronale Netze sind dafür bekannt, dass sie unerwartete Ergebnisse bei Eingaben liefern, die weit von der Trainingsverteilung entfernt sind. Ein Ansatz zur Lösung dieses Problems besteht darin, die Proben zu erkennen, auf die das trainierte Netz keine zuverlässige Antwort geben kann. ODIN ist eine kürzlich vorgeschlagene Methode zur Erkennung von Verteilungsfehlern, die das trainierte Netz nicht verändert und eine gute Leistung für verschiedene Bildklassifizierungsaufgaben erzielt. in diesem Papier passen wir ODIN für Satzklassifizierungs- und Wortmarkierungsaufgaben an. wir zeigen, dass die von ODIN erzeugten Punktzahlen als Vertrauensmaß für die Vorhersagen sowohl in Verteilungs- als auch in Verteilungsdatensätzen verwendet werden können.
Einige neuere Arbeiten haben gezeigt, dass es eine Trennung zwischen der Ausdruckskraft von neuronalen Netzen der Tiefe 2 und der Tiefe 3 gibt, indem Funktionen und Eingabeverteilungen konstruiert werden, so dass die Funktion durch ein neuronales Netz der Tiefe 3 mit polynomialer Größe gut approximiert werden kann, aber nicht durch ein neuronales Netz der Tiefe 2 mit polynomialer Größe unter der gewählten Eingabeverteilung gut approximiert werden kann. Wir zeigen eine ähnliche Trennung zwischen der Ausdruckskraft von sigmoidalen neuronalen Netzen der Tiefe 2 und der Tiefe 3 über eine große Klasse von Eingangsverteilungen, solange die Gewichte polynomial begrenzt sind, und wir zeigen, dass sigmoidale neuronale Netze der Tiefe 2 mit kleiner Breite und kleinen Gewichten durch multivariate Polynome niedrigen Grades gut approximiert werden können.
Beim Verstärkungslernen (Reinforcement Learning, RL), bei dem der gewichtete Graph als Zustandsübergangsprozess interpretiert werden kann, der durch eine auf die Umgebung einwirkende Verhaltenspolitik induziert wird, bietet die Approximation der Eigenvektoren des Laplacian einen vielversprechenden Ansatz für das Lernen der Zustandsrepräsentation, wobei die bestehenden Methoden zur Durchführung dieser Approximation für allgemeine RL-Umgebungen aus zwei Gründen ungeeignet sind:  Erstens sind sie rechenintensiv, da sie oft Operationen auf großen Matrizen erfordern, und zweitens fehlt es diesen Methoden an einer angemessenen Rechtfertigung jenseits einfacher, tabellarischer, endlicher Zustandseinstellungen.In diesem Papier stellen wir eine völlig allgemeine und skalierbare Methode zur Annäherung der Eigenvektoren des Laplacian in einem modellfreien RL-Kontext vor.Wir evaluieren unseren Ansatz systematisch und zeigen empirisch, dass er über die tabellarische, endliche Zustandseinstellung hinaus verallgemeinert. Schließlich zeigen wir die potenziellen Vorteile der Verwendung einer Laplacian-Darstellung, die mit unserer Methode in zielführenden RL-Aufgaben gelernt wurde, und liefern Beweise dafür, dass unsere Technik verwendet werden kann, um die Leistung eines RL-Agenten erheblich zu verbessern.
Unsere Arbeit bietet eine neue Methode für die Domänenübersetzung von semantischen Etikettenkarten und Computergrafik (CG)-Simulationskantenkartenbildern in fotorealistische Bilder. Wir trainieren ein Generatives Adversariales Netzwerk (GAN) in einer bedingten Weise, um eine fotorealistische Version einer gegebenen CG-Szene zu generieren. Bestehende Architekturen von GANs haben immer noch nicht die fotorealistischen Fähigkeiten, die benötigt werden, um DNNs für Computervision-Aufgaben zu trainieren. Wir gehen dieses Problem an, indem wir Kantenkarten einbetten und es in einem adversarischen Modus trainieren.Wir bieten auch eine Erweiterung unseres Modells an, die unsere GAN-Architektur verwendet, um visuell ansprechende und zeitlich kohärente Videos zu erstellen.
Tiefe neuronale Netze sind in verschiedenen Bereichen weit verbreitet, aber die unerschwingliche Rechenkomplexität verhindert ihren Einsatz auf mobilen Geräten.Zahlreiche Modellkompressionsalgorithmen wurden vorgeschlagen, aber es ist oft schwierig und zeitaufwendig, die richtigen Hyper-Parameter zu wählen, um ein effizientes komprimiertes Modell zu erhalten.In diesem Papier schlagen wir einen automatisierten Rahmen für die Modellkompression und Beschleunigung, nämlich PocketFlow. PocketFlow ist ein einfach zu bedienendes Toolkit, das eine Reihe von Algorithmen zur Modellkomprimierung integriert und ein Modul zur Optimierung von Hyperparametern enthält, um automatisch nach der optimalen Kombination von Hyperparametern zu suchen. Darüber hinaus kann das komprimierte Modell in das TensorFlow Lite Format konvertiert und leicht auf mobilen Geräten eingesetzt werden, um die Inferenz zu beschleunigen. PocketFlow ist jetzt quelloffen und öffentlich verfügbar unter https://github.com/Tencent/PocketFlow.
Generative Modelle bieten eine Möglichkeit, die Struktur komplexer Verteilungen zu modellieren, und haben sich für viele Aufgaben von praktischem Interesse als nützlich erwiesen. Aktuelle Techniken für das Training generativer Modelle erfordern jedoch den Zugang zu vollständig beobachteten Stichproben. In vielen Situationen ist es teuer oder sogar unmöglich, vollständig beobachtete Stichproben zu erhalten, aber wirtschaftlich, partielle, verrauschte Beobachtungen zu erhalten. Wir zeigen, dass die wahre zugrundeliegende Verteilung kann nachweislich wiederhergestellt werden, auch in Anwesenheit von pro-Probe-Informationsverlust für eine Klasse von Mess-Modelle.Basierend auf dieser, schlagen wir eine neue Methode der Ausbildung Generative Adversarial Networks (GANs), die wir nennen AmbientGAN.Auf drei Benchmark-Datensätze, und für verschiedene Mess-Modelle, zeigen wir erhebliche qualitative und quantitative Verbesserungen.Generative Modelle mit unserer Methode trainiert kann $2$-$4$x höhere Inception Scores als die Baselines zu erhalten.
Random Matrix Theory (RMT) wird angewendet, um die Gewichtsmatrizen von Deep Neural Networks (DNNs) zu analysieren, einschließlich der Produktionsqualität, vor-trainierte Modelle wie AlexNet und Inception, und kleinere Modelle von Grund auf trainiert, wie LeNet5 und eine Miniatur-AlexNet.  Empirische und theoretische Ergebnisse zeigen deutlich, dass die empirische Spektraldichte (ESD) von DNN-Schichtenmatrizen Merkmale von traditionell regulierten statistischen Modellen aufweist, selbst wenn keine exogenen Regularisierungsformen wie Dropout- oder Gewichtsnorm-Beschränkungen spezifiziert werden.  Aufbauend auf den jüngsten Ergebnissen der RMT, insbesondere ihrer Erweiterung auf Universalitätsklassen von Heavy-Tailed-Matrizen, entwickeln wir eine Theorie zur Identifizierung von 5+1 Trainingsphasen, die einer zunehmenden Menge an impliziter Selbstregularisierung entsprechen.  Für kleinere und/oder ältere DNNs ist diese Implizite Selbstregularisierung wie die traditionelle Tikhonov-Regularisierung, da es eine "Größenskala" gibt, die das Signal vom Rauschen trennt.  Für hochmoderne DNNs identifizieren wir jedoch eine neue Form der Heavy-Tailed-Selbstregularisierung, die der Selbstorganisation in der statistischen Physik ungeordneter Systeme ähnelt.  Diese implizite Selbstregularisierung kann stark von den vielen Knöpfen des Trainingsprozesses abhängen.  Unter Ausnutzung des Phänomens der Generalisierungslücke zeigen wir, dass wir ein kleines Modell dazu bringen können, alle 5+1 Phasen des Trainings zu zeigen, indem wir einfach die Stapelgröße ändern.
Wir führen einen Aufmerksamkeitsmechanismus ein, um die Merkmalsextraktion für tiefes aktives Lernen (AL) in der halbüberwachten Einstellung zu verbessern. Der vorgeschlagene Aufmerksamkeitsmechanismus basiert auf aktuellen Methoden zur visuellen Erklärung von Vorhersagen, die von DNNs gemacht werden. Wir wenden die vorgeschlagene erklärungsbasierte Aufmerksamkeit auf MNIST- und SVHN-Klassifikation an.
Wir wenden kanonische Formen von Gradientenkomplexen (Barcodes) an, um die Verlustoberflächen neuronaler Netze zu untersuchen, und stellen einen Algorithmus zur Berechnung der Barcodes von Minima der Zielfunktion vor.  Unsere Experimente bestätigen zwei wesentliche Beobachtungen: (1) die Barcodes der Minima befinden sich in einem kleinen unteren Teil des Wertebereichs der Zielfunktion und (2) eine Erhöhung der Tiefe des neuronalen Netzes führt zu einer Verringerung der Barcodes der Minima, was natürliche Implikationen für das Lernen des neuronalen Netzes und die Fähigkeit zur Generalisierung hat.
Bestehende Software-Frameworks und Trainingsalgorithmen für Deep Learning müssen jedoch noch weiterentwickelt werden, um die Fähigkeiten der neuen Welle von Silizium vollständig nutzen zu können. Insbesondere Modelle, die strukturierte Eingaben über komplexe und instanzabhängige Kontrollflüsse nutzen, sind mit bestehenden Algorithmen und Hardware, die typischerweise auf Minibatching basieren, schwer zu beschleunigen. Wir stellen einen asynchronen modellparallelen (AMP) Trainingsalgorithmus vor, der speziell durch das Training auf Netzwerken von miteinander verbundenen Geräten motiviert ist.Durch eine Implementierung auf Multi-Core-CPUs zeigen wir, dass das AMP-Training in einer ähnlichen Anzahl von Epochen zur gleichen Genauigkeit konvergiert wie herkömmliche synchrone Trainingsalgorithmen, aber die verfügbare Hardware effizienter nutzt, sogar für kleine Minibatch-Größen, was zu kürzeren Gesamttrainingszeiten führt.Unser Framework öffnet die Tür für die Skalierung einer neuen Klasse von Deep-Learning-Modellen, die heute nicht effizient trainiert werden können.
Beim kooperativen Multi-Agenten-Verstärkungslernen (MARL) ist es ein kritisches Problem, ein geeignetes Belohnungssignal zu entwerfen, um das Lernen zu beschleunigen und die Konvergenz zu stabilisieren.Das globale Belohnungssignal weist allen Agenten dieselbe globale Belohnung zu, ohne zwischen ihren Beiträgen zu unterscheiden, während das lokale Belohnungssignal jedem Agenten unterschiedliche lokale Belohnungen zuweist, die ausschließlich auf dem individuellen Verhalten basieren.Beide Belohnungszuweisungsansätze haben einige Mängel: Ersterer könnte faule Agenten ermutigen, während letzterer egoistische Agenten hervorbringen könnte.In diesem Papier untersuchen wir das Belohnungsdesignproblem beim kooperativen MARL auf der Grundlage von Paketrouting-Umgebungen. Zunächst zeigen wir, dass die beiden oben genannten Reward-Signale dazu neigen, suboptimale Policies zu erzeugen.Dann, inspiriert durch einige Beobachtungen und Überlegungen, entwerfen wir einige gemischte Reward-Signale, die von der Stange sind, um bessere Policies zu lernen.Schließlich verwandeln wir die gemischten Reward-Signale in die adaptiven Gegenstücke, die in unseren Experimenten die besten Ergebnisse erzielen.Andere Reward-Signale werden ebenfalls in diesem Papier diskutiert.Da Reward-Design ein sehr grundlegendes Problem in RL und insbesondere in MARL ist, hoffen wir, dass MARL-Forscher die in ihren Systemen verwendeten Rewards überdenken können.
Jüngste Fortschritte haben gezeigt, dass es oft möglich ist, zu lernen, um lineare inverse Probleme in der Bildgebung mit Hilfe von Trainingsdaten zu lösen, die mehr als herkömmliche regulierte Kleinste-Quadrate-Lösungen übertreffen können.In diesem Sinne stellen wir einige Erweiterungen des Neumann-Netzwerks vor, eine kürzlich eingeführte End-to-End-gelernte Architektur, die von einer verkürzten Neumann-Serienerweiterung der Lösungskarte für ein reguliertes Kleinste-Quadrate-Problem inspiriert ist. Hier fassen wir den Ansatz des Neumann-Netzwerks zusammen und zeigen, dass er eine Form hat, die mit der optimalen Rekonstruktionsfunktion für ein gegebenes inverses Problem kompatibel ist.
In unserem Modell werden ein globaler Speicher-Encoder und ein lokaler Speicher-Decoder vorgeschlagen, um externes Wissen zu teilen. Der Encoder kodiert die Dialoghistorie, modifiziert die globale kontextuelle Repräsentation und generiert einen globalen Speicherpointer. Der Decoder generiert zunächst eine Skizzenantwort mit unbesetzten Slots. Wir zeigen empirisch, dass unser Modell die Kopiergenauigkeit verbessern und das übliche Out-of-Vocabulary-Problem entschärfen kann. GLMP ist daher in der Lage, die bisherigen State-of-the-Art-Modelle sowohl im simulierten bAbI-Dialog-Datensatz als auch im Mensch-Mensch-Datensatz des Stanford Multi-domain Dialogue bei der automatischen und menschlichen Bewertung zu übertreffen.
Das Schachbrett-Phänomen ist eines der bekanntesten visuellen Artefakte in der Computer Vision field.The Ursprünge und Lösungen der Schachbrett-Artefakte in der Pixel-Raum wurden für eine lange Zeit untersucht, aber ihre Auswirkungen auf die Gradient Raum wurden selten untersucht.In diesem Papier, wir wieder die Schachbrett-Artefakte in der Gradient Raum, der sich als der Schwachpunkt eines Netzwerk-Architektur.We erkunden Bild-agnostische Eigenschaft der Gradient Schachbrett-Artefakte und schlagen eine einfache, aber effektive Verteidigung Methode durch die Nutzung der Artefakte. Wir stellen unser Verteidigungsmodul, genannt Artificial Checkerboard Enhancer (ACE), vor, das gegnerische Angriffe auf bestimmte Pixel auslöst und es dem Modell ermöglicht, Angriffe abzuwehren, indem es nur ein einziges Pixel im Bild mit einer bemerkenswerten Verteidigungsrate verschiebt.Wir bieten umfangreiche Experimente, um die Effektivität unserer Arbeit für verschiedene Angriffsszenarien unter Verwendung modernster Angriffsmethoden zu unterstützen.Darüber hinaus zeigen wir, dass ACE sogar auf große Datensätze, einschließlich des ImageNet-Datensatzes, anwendbar ist und leicht auf verschiedene vortrainierte Netzwerke übertragen werden kann.
Min-Max-Formulierungen haben in der ML-Gemeinschaft aufgrund des Aufstiegs von tiefen generativen Modellen und adversen Methoden große Aufmerksamkeit erregt, und das Verständnis der Dynamik von (stochastischen) Gradientenalgorithmen zur Lösung solcher Formulierungen war eine große Herausforderung. In einem ersten Schritt beschränken wir uns auf bilineare Nullsummenspiele und führen eine systematische Analyse populärer Gradienten-Updates durch, sowohl für simultane als auch für alternierende Versionen, wobei wir exakte Bedingungen für deren Konvergenz aufstellen und die optimalen Parametereinstellungen und Konvergenzraten finden.
Die meisten Ansätze im Bereich des verallgemeinerten Zero-Shot-Lernens beruhen auf einem cross-modalen Mapping zwischen einem Bildmerkmalsraum und einem Klasseneinbettungsraum oder auf der Generierung künstlicher Bildmerkmale, wobei sich das Erlernen einer gemeinsamen cross-modalen Einbettung durch Angleichung der latenten Räume modalitätsspezifischer Auto-Coder als vielversprechend für das (verallgemeinerte) Zero-Shot-Lernen erwiesen hat. Wir gehen einen Schritt weiter und schlagen ein Modell vor, bei dem ein gemeinsamer latenter Raum von Bildmerkmalen und Klasseneinbettungen durch ausgerichtete Variations-Autocoder erlernt wird, um latente Merkmale für das Training eines Softmax-Klassifikators zu generieren. wir evaluieren unsere erlernten latenten Merkmale auf konventionellen Benchmark-Datensätzen und etablieren einen neuen Stand der Technik für verallgemeinertes Zero-Shot- sowie Little-Shot-Lernen.
Intuitiv sollte die Bildklassifikation von der Verwendung räumlicher Informationen profitieren. Jüngste Arbeiten deuten jedoch darauf hin, dass dies in Standard-CNNs überbewertet werden könnte. In diesem Papier gehen wir an die Grenzen und zielen darauf ab, die Abhängigkeit von und die Notwendigkeit von räumlichen Informationen weiter zu untersuchen. Wir schlagen vor und analysieren drei Methoden, nämlich Shuffle Conv, GAP+FC und 1x1 Conv, die räumliche Informationen sowohl während der Trainings- als auch der Testphase zerstören. Wir evaluieren diese Methoden ausgiebig auf mehreren Objekterkennungsdatensätzen (CIFAR100, Small-ImageNet, ImageNet) mit einer breiten Palette von CNN-Architekturen (VGG16, ResNet50, ResNet152, MobileNet, SqueezeNet).Interessanterweise stellen wir immer wieder fest, dass räumliche Informationen aus einer beträchtlichen Anzahl von Schichten ohne oder nur mit geringen Leistungseinbußen vollständig gelöscht werden können.
Unsere erste Methode, Unlabeled Disentangling GAN (UD-GAN, unbeaufsichtigt), zerlegt das latente Rauschen, indem sie ähnliche/unähnliche Bildpaare erzeugt und eine Distanzmetrik auf diesen Paaren mit siamesischen Netzwerken und einem kontrastiven Verlust lernt. Unsere zweite Methode (UD-GAN-G, schwach überwacht) modifiziert das UD-GAN mit benutzerdefinierten Führungsfunktionen, die die Informationen einschränken, die in die siamesischen Netze einfließen. Diese Einschränkung hilft UD-GAN-G, sich auf die gewünschten semantischen Variationen in den Daten zu konzentrieren.
Es gibt eine wachsende Kluft zwischen den Ansätzen zur Erstellung von Systemen: einerseits die Verwendung von menschlichen Experten (z.B. Programmierung) und andererseits die Verwendung von aus Daten gelerntem Verhalten (z.B. ML).PVars zielt darauf ab, die Verwendung von ML in der Programmierung zu vereinfachen, indem die beiden Ansätze miteinander kombiniert werden.Wir nutzen das bestehende Konzept von Variablen und erstellen einen neuen Typ, eine vorhergesagte Variable.PVars sind ähnlich wie native Variablen mit einem wichtigen Unterschied: Wir beschreiben PVars und ihre Schnittstelle, wie sie in der Programmierung verwendet werden können, und demonstrieren die Machbarkeit unseres Ansatzes an drei algorithmischen Problemen: binäre Suche, QuickSort und Caches.Wir zeigen experimentell, dass PVars in der Lage sind, die üblicherweise verwendeten Heuristiken zu verbessern und zu einer besseren Leistung als die ursprünglichen Algorithmen zu führen.Im Gegensatz zu früheren Arbeiten, die ML auf algorithmische Probleme anwenden, haben PVars den Vorteil, dass sie innerhalb der bestehenden Frameworks verwendet werden können und das bestehende Domänenwissen nicht ersetzen müssen. Unsere PVars-Implementierung stützt sich derzeit auf Standard-RL-Methoden (Reinforcement Learning).Um schneller zu lernen, verwenden PVars die heuristische Funktion, die sie ersetzen, als Ausgangsfunktion.Wir zeigen, dass PVars das Verhalten der Ausgangsfunktion schnell übernehmen und dann die Leistung darüber hinaus verbessern, ohne jemals wesentlich schlechter abzuschneiden - was einen sicheren Einsatz in kritischen Anwendungen ermöglicht.
Die hierarchische Videovorhersagemethode von Villegas et al. (2017) ist ein Beispiel für ein modernes Verfahren zur langfristigen Videovorhersage.  Ihre Methode ist jedoch in der Praxis nur begrenzt anwendbar, da sie eine Grundwahrheitspose (z. B. die Posen der Gelenke eines Menschen) zur Trainingszeit erfordert.   Wir zeigen, dass das Netzwerk seine eigene übergeordnete Struktur erlernt (z.B. Pose-äquivalente versteckte Variablen), die in Fällen besser funktioniert, in denen die Grundwahrheits-Pose nicht alle für die Vorhersage des nächsten Bildes benötigten Informationen vollständig erfasst.   Diese Methode liefert schärfere Ergebnisse als andere Videovorhersagemethoden, die keine Grundwahrheitspose benötigen, und ihre Effizienz wird anhand der Datensätze Humans 3.6M und Robot Pushing gezeigt.
Die Kombination von Informationen aus verschiedenen sensorischen Modalitäten zur Ausführung zielgerichteter Aktionen ist ein Schlüsselaspekt menschlicher Intelligenz. menschliche Agenten sind sehr leicht in der Lage, die in einer sensorischen Domäne (z.B. Sehen) vermittelte Aufgabe in eine Repräsentation zu übersetzen, die es ihnen ermöglicht, diese Aufgabe zu erfüllen, wenn sie ihre Umgebung nur mit einer anderen sensorischen Modalität (z.B. Berührung) wahrnehmen können.um Agenten mit ähnlichen Fähigkeiten zu entwickeln, betrachten wir in dieser Arbeit das Problem, ein Zielobjekt aus einer Schublade zu holen. Der Agent erhält ein Bild eines zuvor ungesehenen Objekts und erkundet die Objekte in der Schublade, indem er nur den Tastsinn nutzt, um das Objekt zu finden, das auf dem Bild zu sehen war, ohne visuelles Feedback zu erhalten.Der Erfolg bei dieser Aufgabe erfordert eine enge Integration von visuellem und taktilem Empfinden.Wir stellen eine Methode vor, um diese Aufgabe in einer simulierten Umgebung mit einer anthropomorphen Hand auszuführen.Wir hoffen, dass zukünftige Forschungen in Richtung der Kombination von sensorischen Signalen für das Handeln das Finden von Objekten aus einer Schublade als nützliches Benchmark-Problem sehen werden.
Lokalitätssensitive Hash-Verfahren wie \simhash bieten kompakte Darstellungen von Multisets, aus denen die Ähnlichkeit geschätzt werden kann; in bestimmten Anwendungen müssen wir jedoch die Ähnlichkeit von sich dynamisch ändernden Sets schätzen.  In diesem Fall muss die Darstellung ein Homomorphismus sein, so dass der Hash von Vereinigungen und Differenzen von Mengen direkt aus den Hashes der Operanden berechnet werden kann.  Wir schlagen zwei Repräsentationen vor, die diese Eigenschaft für die Kosinus-Ähnlichkeit besitzen (eine Erweiterung von \simhash und winkelerhaltende Zufallsprojektionen), und machen wesentliche Fortschritte bei einer dritten Repräsentation für die Jaccard-Ähnlichkeit (eine Erweiterung von \minhash). Wir setzen diese Hashes ein, um die hinreichenden Statistiken eines bedingten Zufallsfeld-Coreferenzmodells (CRF) zu komprimieren, und untersuchen, wie sich diese Kompression auf unsere Fähigkeit auswirkt, Ähnlichkeiten zu berechnen, wenn Entitäten während der Inferenz aufgeteilt und zusammengeführt werden. \Wir untersuchen diese Hashes in einem hierarchischen Conditional Random Field (CRF)-Coreferenzmodell, um die Ähnlichkeit von Entitäten zu berechnen, wenn sie während der Inferenz zusammengeführt und aufgeteilt werden. Wir bieten auch eine neuartige statistische Analyse von \simhash, um sie als Schätzer innerhalb eines CRF zu rechtfertigen, und zeigen, dass die Verzerrung und Varianz schnell mit der Anzahl der Bits abnimmt. Anhand eines Problems der Autorenkoreferenz stellen wir fest, dass unser \simhash-Schema die Skalierung des hierarchischen Koreferenz-Algorithmus um eine Größenordnung ermöglicht, ohne dass sich seine statistische Leistung oder die Genauigkeit der Koreferenz des Modells verschlechtert, solange wir mindestens 128 oder 256 Bits verwenden.  Durch winkeltreue Zufallsprojektionen wird die Qualität der Koreferenz weiter verbessert, so dass möglicherweise sogar weniger Dimensionen verwendet werden können.
Kürzlich durchgeführte Analysen haben gezeigt, dass naive kNN-Schätzer der gegenseitigen Information ernsthafte statistische Beschränkungen aufweisen, die raffiniertere Methoden erforderlich machen.In diesem Papier beweisen wir, dass ernsthafte statistische Beschränkungen jeder Messmethode innewohnen.Genauer gesagt zeigen wir, dass jede verteilungsfreie untere Schranke mit hohem Vertrauen in die gegenseitige Information nicht größer als $O(\ln N)$ sein kann, wobei $N$ die Größe der Datenstichprobe ist. Wir analysieren auch die untere Schranke von Donsker-Varadhan für die KL-Divergenz und zeigen, dass diese Schranke, wenn einfache statistische Überlegungen berücksichtigt werden, niemals zu einem Wert mit hohem Vertrauen führen kann, der größer ist als $\ln N$.Während große untere Schranken mit hohem Vertrauen unmöglich sind, kann man in der Praxis Schätzer ohne formale Garantien verwenden.Wir schlagen vor, die gegenseitige Information als eine Differenz von Entropien auszudrücken und die Kreuzentropie als Entropieschätzer zu verwenden. Wir stellen fest, dass, obwohl die Kreuzentropie nur eine obere Schranke für die Entropie ist, die Kreuzentropieschätzungen mit einer Rate von $1/\sqrt{N}$ gegen die wahre Kreuzentropie konvergieren.
In dieser Arbeit schlagen wir ein neuronales Netzwerk vor, das als neuronales hierarchisches Netzwerk (NHN) bezeichnet wird, das sich über die Hierarchie der Schichten hinaus entwickelt und sich auf die Hierarchie der Neuronen konzentriert.Wir beobachten eine massive Redundanz in den Gewichten sowohl von handgefertigten als auch von zufällig gesuchten Architekturen.Inspiriert von der Entwicklung des menschlichen Gehirns beschneiden wir Neuronen mit geringer Empfindlichkeit im Modell und fügen neue Neuronen zum Graphen hinzu, wobei die Beziehung zwischen einzelnen Neuronen betont und die Existenz von Schichten abgeschwächt wird. Wir schlagen einen Prozess vor, um das beste Basismodell durch eine zufällige Architektursuche zu finden und die besten Positionen und Verbindungen der hinzugefügten Neuronen durch eine evolutionäre Suche zu ermitteln. Experimentelle Ergebnisse zeigen, dass das NHN eine höhere Testgenauigkeit auf Cifar-10 erreicht als die modernsten handgefertigten und zufällig gesuchten Architekturen, während es viel weniger Parameter und weniger Suchzeit benötigt.
In dieser Arbeit schlagen wir eine auf Verstärkungslernen basierende Methode vor, um die Parameter eines beliebigen (nicht-differenzierbaren) Simulators automatisch anzupassen und so die Verteilung der synthetisierten Daten zu kontrollieren, um die Genauigkeit eines auf diesen Daten trainierten Modells zu maximieren. Im Gegensatz zum Stand der Technik, der diese Simulationsparameter von Hand erstellt oder nur Teile der verfügbaren Parameter anpasst, steuert unser Ansatz den Simulator vollständig mit dem eigentlichen Ziel der Maximierung der Genauigkeit, anstatt die reale Datenverteilung zu imitieren oder zufällig eine große Datenmenge zu erzeugen.
Die Modellierung statistischer Beziehungen, die über den bedingten Mittelwert hinausgehen, ist in vielen Bereichen von entscheidender Bedeutung. Die bedingte Dichteschätzung (Conditional Density Estimation, CDE) zielt darauf ab, die vollständige bedingte Wahrscheinlichkeitsdichte aus den Daten zu lernen.Obwohl CDE-Modelle, die auf neuronalen Netzen basieren, sehr aussagekräftig sind, können sie unter einer starken Überanpassung leiden, wenn sie mit dem Maximum-Likelihood-Ziel trainiert werden.Aufgrund der inhärenten Struktur solcher Modelle werden klassische Regularisierungsansätze im Parameterraum unwirksam. Wir zeigen, dass der vorgeschlagene Ansatz einer Glättungsregularisierung entspricht und beweisen seine asymptotische Konsistenz.In unseren Experimenten übertrifft die Rauschregularisierung andere Regularisierungsmethoden in sieben Datensätzen und drei CDE-Modellen signifikant und konsistent.Die Effektivität der Rauschregularisierung macht die auf neuronalen Netzwerken basierende CDE zur bevorzugten Methode gegenüber früheren nicht- und semiparametrischen Ansätzen, selbst wenn die Trainingsdaten knapp sind.
Eine vielversprechende Technik für unüberwachtes Lernen ist das System der Variationalen Autokodierer (VAEs). Allerdings sind unüberwachte Repräsentationen, die durch VAEs gelernt werden, denen, die durch Überwachung für die Erkennung gelernt werden, deutlich unterlegen.Unsere Hypothese ist, dass, um nützliche Repräsentationen für die Erkennung zu lernen, das Modell ermutigt werden muss, über wiederkehrende und konsistente Muster in den Daten zu lernen. Unser Hauptbeitrag ist eine Engpassformulierung in einem VAE-Rahmen, der Repräsentationen auf mittlerer Ebene fördert. Unsere Experimente zeigen, dass Repräsentationen, die durch unsere Methode gelernt wurden, bei den Erkennungsaufgaben viel besser abschneiden als solche, die durch Vanilla VAEs gelernt wurden.
Verschwindende und explodierende Gradienten sind zwei der Haupthindernisse bei der Ausbildung von tiefen neuronalen Netzen, insbesondere bei der Erfassung von langreichweitigen Abhängigkeiten in rekurrenten neuronalen Netzen (RNNs).In diesem Papier präsentieren wir eine effiziente Parametrisierung der Übergangsmatrix eines RNN, die es uns ermöglicht, die Gradienten zu stabilisieren, die bei der Ausbildung entstehen. Insbesondere parametrisieren wir die Übergangsmatrix durch ihre Singulärwertzerlegung (SVD), die es uns erlaubt, ihre Singulärwerte explizit zu verfolgen und zu kontrollieren. Wir erreichen Effizienz durch die Verwendung von Werkzeugen, die in der numerischen linearen Algebra üblich sind, nämlich Householder-Reflektoren zur Darstellung der orthogonalen Matrizen, die in der SVD entstehen. Durch die explizite Kontrolle der singulären Werte ermöglicht die von uns vorgeschlagene svdRNN-Methode eine einfache Lösung des Problems des explodierenden Gradienten, und wir stellen fest, dass sie das Problem des verschwindenden Gradienten empirisch weitgehend löst.Wir stellen fest, dass die SVD-Parametrisierung für jede beliebige rechteckige Gewichtsmatrix verwendet werden kann, so dass sie leicht auf jedes tiefe neuronale Netz, wie z. B. ein mehrschichtiges Perzeptron, erweitert werden kann. Theoretisch demonstrieren wir, dass unsere Parametrisierung nicht an Ausdruckskraft verliert und zeigen, wie sie den Optimierungsprozess potenziell vereinfacht. 
Um den Stil eines beliebigen Bildes auf ein Inhaltsbild zu übertragen, verwendeten diese Methoden ein Feed-Forward-Netzwerk mit einem Feature-Transformator mit der niedrigsten Skala oder eine Kaskade von Netzwerken mit einem Feature-Transformator der entsprechenden Skala, aber ihre Ansätze berücksichtigten weder den mehrskaligen Stil in ihrem einskaligen Feature-Transformator noch die Abhängigkeit zwischen den transformierten Feature-Statistiken in den Kaskaden-Netzwerken. Um diese Einschränkung der partiellen Stilübertragung zu überwinden, schlagen wir eine Methode zur Gesamtstilübertragung vor, die mehrskalige Merkmalsstatistiken durch einen einzigen Vorwärtsprozess überträgt: Erstens transformiert unsere Methode mehrskalige Merkmalskarten eines Inhaltsbildes in die eines Zielstilbildes, indem sie sowohl Interkanal-Korrelationen in jeder einzelnen skalierten Merkmalskarte als auch interskalige Korrelationen zwischen mehrskaligen Merkmalskarten berücksichtigt. Anschließend wird jede umgewandelte Merkmalskarte mit Hilfe von Skip-Connection in die Decoderschicht der entsprechenden Skala eingefügt, und schließlich werden die über Skip-Connection verbundenen mehrskaligen Merkmalskarten durch unser trainiertes Decodernetzwerk in ein stilisiertes Bild decodiert.
Eine Herausforderung für Dialogagenten ist es, die Gefühle des Gesprächspartners zu erkennen und entsprechend zu antworten - eine wichtige kommunikative Fähigkeit, die für Menschen trivial ist.Die Forschung in diesem Bereich wird durch den Mangel an geeigneten öffentlich verfügbaren Datensätzen sowohl für Emotionen als auch für Dialoge erschwert.Diese Arbeit schlägt eine neue Aufgabe für die empathische Dialoggenerierung und EmpatheticDialogues vor, einen Datensatz von 25k Gesprächen, die auf emotionalen Situationen basieren, um das Training und die Evaluierung von Dialogsystemen zu erleichtern. Unsere Experimente zeigen, dass Dialogmodelle, die unseren Datensatz verwenden, von menschlichen Bewertern als einfühlsamer wahrgenommen werden, während sie auch bei anderen Metriken besser abschneiden (z.B. wahrgenommene Relevanz der Antworten, BLEU-Scores), verglichen mit Modellen, die lediglich auf groß angelegten Internet-Konversationsdaten trainiert wurden.Wir stellen auch empirische Vergleiche verschiedener Möglichkeiten vor, die Leistung eines bestimmten Modells zu verbessern, indem bestehende Modelle oder Datensätze genutzt werden, ohne dass ein langwieriges Neutraining des gesamten Modells erforderlich ist.
Da die meisten physikalischen Interaktionen von Natur aus nichtlinear sind, betrachten wir das Problem der Ableitung der paarweisen Granger-Kausalität zwischen nichtlinear interagierenden stochastischen Prozessen aus deren Zeitreihenmessungen. Der von uns vorgeschlagene Ansatz beruht auf der Modellierung der eingebetteten Nichtlinearitäten in den Messungen unter Verwendung eines komponentenweisen Zeitreihenvorhersagemodells auf der Grundlage von Statistical Recurrent Units (SRUs), wobei die Netzwerktopologie der Granger-Kausalbeziehungen direkt aus einer strukturierten, spärlichen Schätzung der internen Parameter der SRU-Netzwerke abgeleitet werden kann, die zur Vorhersage der Zeitreihenmessungen der Prozesse trainiert wurden. Wir schlagen eine Variante der SRU vor, die als Economy-SRU bezeichnet wird und die aufgrund ihrer Konstruktion wesentlich weniger trainierbare Parameter hat und daher weniger anfällig für eine Überanpassung ist: Die Economy-SRU berechnet eine niedrigdimensionale Skizze ihres hochdimensionalen verborgenen Zustands in Form von Zufallsprojektionen, um das Feedback für ihre rekurrente Verarbeitung zu erzeugen. Zusätzlich werden die internen Gewichtsparameter des Economy-SRU strategisch gruppenweise reguliert, um das vorgeschlagene Netzwerk bei der Extraktion von aussagekräftigen Vorhersagemerkmalen zu unterstützen, die in hohem Maße zeitlich lokalisiert sind, um reale kausale Ereignisse zu imitieren. Es werden umfangreiche Experimente durchgeführt, um zu zeigen, dass das vorgeschlagene Economy-SRU-basierte Zeitreihenvorhersagemodell die MLP-, LSTM- und aufmerksamkeitsgesteuerten CNN-basierten Zeitreihenmodelle übertrifft, die zuvor für die Ableitung der Granger-Kausalität in Betracht gezogen wurden.
Graph Convolutional Networks (GCNs) sind leistungsstarke tiefe neuronale Netze für graphenstrukturierte Daten. GCNs berechnen jedoch die Repräsentation der Knoten rekursiv aus ihren Nachbarn, wodurch die Größe des rezeptiven Feldes exponentiell mit der Anzahl der Schichten wächst.  Bisherige Versuche, die Größe des rezeptiven Feldes durch Subsampling von Nachbarn zu reduzieren, haben keine Konvergenzgarantie, und die Größe des rezeptiven Feldes pro Knoten liegt immer noch in der Größenordnung von Hunderten. Empirische Ergebnisse zeigen, dass unsere Algorithmen eine ähnliche Konvergenzgeschwindigkeit pro Epoche haben wie der exakte Algorithmus, selbst wenn nur zwei Nachbarn pro Knoten verwendet werden. Der Zeitaufwand unseres Algorithmus für den Reddit-Datensatz beträgt nur ein Fünftel der bisherigen Algorithmen für das Nachbarschafts-Sampling.
Wir schlagen vor, Monte-Carlo-Methoden und Wichtigkeitssampling anzuwenden, um vortrainierte neuronale Netze zu sparsifizieren und zu quantisieren, ohne sie neu zu trainieren.Wir erhalten spärliche, ganzzahlige Darstellungen mit geringer Bitbreite, die den Gewichten und Aktivierungen mit voller Präzision nahe kommen. Unser Ansatz, Monte-Carlo-Quantisierung (MCQ) genannt, ist sowohl zeitlich als auch räumlich linear, während die resultierenden quantisierten spärlichen Netzwerke nur minimale Genauigkeitsverluste im Vergleich zu den ursprünglichen Netzwerken mit voller Genauigkeit aufweisen.
Im Gegensatz zum Variations-Autoencoder beginnt IMAE mit einem stochastischen Encoder, der versucht, alle Eingabedaten auf eine hybride diskrete und kontinuierliche Repräsentation abzubilden, mit dem Ziel, die gegenseitige Information zwischen den Daten und ihren Repräsentationen zu maximieren.Ein Decoder ist enthalten, um die posteriore Verteilung der Daten angesichts ihrer Repräsentationen zu approximieren, wobei eine Annäherung mit hoher Genauigkeit erreicht werden kann, indem die informativen Repräsentationen genutzt werden.     Wir zeigen, dass das vorgeschlagene Ziel theoretisch gültig ist und einen prinzipiellen Rahmen für das Verständnis der Kompromisse in Bezug auf die Informativität jedes Repräsentationsfaktors, die Entflechtung der Repräsentationen und die Dekodierungsqualität bietet.
Die meisten Regularisierungstechniken werden im Raum der Parameter konzipiert und implementiert, aber es ist auch möglich, im Raum der Funktionen zu regularisieren. Hier schlagen wir vor, Netzwerke in einem $L^2$-Hilbert-Raum zu messen und eine Lernregel zu testen, die die Entfernung regularisiert, die ein Netzwerk bei jeder Aktualisierung durch den $L^2$-Raum zurücklegen kann.  Die daraus resultierende Lernregel, die wir Hilbert-constrained gradient descent (HCGD) nennen, ist also eng mit dem natürlichen Gradienten verwandt, reguliert aber eine andere und besser berechenbare Metrik über den Funktionsraum.Experimente zeigen, dass der HCGD effizient ist und zu einer deutlich besseren Generalisierung führt.
Der stochastische Gradientenabstieg (SGD), der auf die 1950er Jahre zurückgeht, ist einer der populärsten und effektivsten Ansätze für die Durchführung stochastischer Optimierung.Die SGD-Forschung wurde vor kurzem im Bereich des maschinellen Lernens wiederbelebt, um konvexe Verlustfunktionen zu optimieren und nichtkonvexe tiefe neuronale Netze zu trainieren.Die Theorie geht davon aus, dass man leicht einen unvoreingenommenen Gradientenschätzer berechnen kann, was aufgrund der Stichproben-Durchschnittsnatur der empirischen Risikominimierung in der Regel der Fall ist.Es gibt jedoch viele Szenarien (z. B. Graphen), in denen ein unvoreingenommener Gradient nicht berechnet werden kann, Kürzlich schlugen Chen et al. (2018) vor, einen konsistenten Gradientenschätzer als ökonomische Alternative zu verwenden.Ermutigt durch den empirischen Erfolg zeigen wir in einer allgemeinen Umgebung, dass konsistente Schätzer zum gleichen Konvergenzverhalten führen wie unverzerrte Schätzer. Unsere Analyse deckt stark konvexe, konvexe und nicht-konvexe Ziele ab und wir verifizieren die Ergebnisse mit anschaulichen Experimenten auf synthetischen und realen Daten.Diese Arbeit eröffnet mehrere neue Forschungsrichtungen, einschließlich der Entwicklung effizienter SGD-Updates mit konsistenten Schätzern und dem Entwurf effizienter Trainingsalgorithmen für große Graphen.
In diesem Zusammenhang basieren alle bekannten Methoden auf der Extraktion von Unsicherheitssignalen aus einem trainierten Netzwerk, das für die Lösung des vorliegenden Klassifizierungsproblems optimiert ist. Wir zeigen, dass solche Techniken dazu neigen, verzerrte Schätzungen für Instanzen einzuführen, deren Vorhersagen eigentlich sehr sicher sein sollten.Wir argumentieren, dass dieser Mangel ein Artefakt der Dynamik des Trainings mit SGD-ähnlichen Optimierern ist und einige ähnliche Eigenschaften wie Overfitting hat. Basierend auf dieser Beobachtung entwickeln wir einen Algorithmus zur Unsicherheitsschätzung, der selektiv die Unsicherheit hochzuverlässiger Punkte schätzt, indem er frühere Schnappschüsse des trainierten Modells verwendet, bevor ihre Schätzungen verwackelt sind (und lange bevor sie für die tatsächliche Klassifizierung bereit sind).
Die Modelle, die auf der Grundlage solcher Datensätze trainiert werden, leiden unter einer uneinheitlichen Klassifizierungsgenauigkeit, was die Anwendbarkeit von Gesichtsanalysesystemen auf nicht-weiße Rassengruppen einschränkt. Um das Problem der Rassenverzerrung in diesen Datensätzen zu entschärfen, haben wir einen neuartigen Gesichtsdatensatz mit 108.501 Bildern erstellt, der hinsichtlich der Rasse ausgeglichen ist. 7 Rassengruppen werden definiert: Weiße, Schwarze, Indische, Ostasiatische, Südostasiatische, Nahöstliche und Latino-Bilder wurden aus dem YFCC-100M Flickr-Datensatz entnommen und mit Rasse, Geschlecht und Altersgruppen beschriftet.Evaluierungen wurden sowohl mit bestehenden Gesichtsattribut-Datensätzen als auch mit neuen Bilddatensätzen durchgeführt, um die Generalisierungsleistung zu messen. Wir stellen fest, dass das Modell, das auf der Grundlage unseres Datensatzes trainiert wurde, bei neuartigen Datensätzen wesentlich genauer ist und die Genauigkeit über Rassen- und Geschlechtergruppen hinweg konsistent ist.
Dramatische Fortschritte bei generativen Modellen haben dazu geführt, dass künstlich gerenderte Gesichter, Tiere und andere Objekte in der natürlichen Welt eine nahezu fotografische Qualität aufweisen. Trotz solcher Fortschritte ergibt sich ein besseres Verständnis des Sehens und der Bildgebung nicht aus der vollständigen Modellierung eines Objekts, sondern aus der Identifizierung von übergeordneten Attributen, die die Aspekte eines Objekts am besten zusammenfassen.   In dieser Arbeit versuchen wir, den Zeichenprozess von Schriftarten zu modellieren, indem wir sequenzielle generative Modelle von Vektorgrafiken erstellen.  Wir demonstrieren diese Ergebnisse anhand eines großen Datensatzes von Schriftarten und zeigen, wie ein solches Modell die statistischen Abhängigkeiten und den Reichtum dieses Datensatzes einfängt, und wir stellen uns vor, dass unser Modell als Werkzeug für Designer verwendet werden kann, um die Gestaltung von Schriftarten zu erleichtern.
Was können wir über die funktionelle Organisation von kortikalen Mikroschaltkreisen aus groß angelegten Aufzeichnungen neuronaler Aktivität lernen?  Um ein explizites und interpretierbares Modell der zeitabhängigen funktionalen Verbindungen zwischen Neuronen zu erhalten und die Dynamik des kortikalen Informationsflusses zu ermitteln, entwickeln wir die "dynamische neuronale relationale Inferenz" (dNRI). wir untersuchen sowohl synthetische als auch reale neuronale Spike-Daten und zeigen, dass die entwickelte Methode in der Lage ist, die dynamischen Beziehungen zwischen Neuronen zuverlässiger aufzudecken als die bestehenden Basisverfahren.
DeePa ist ein Deep-Learning-Framework, das Parallelität in allen parallelisierbaren Dimensionen erforscht, um den Trainingsprozess von faltigen neuronalen Netzen zu beschleunigen.DeePa optimiert die Parallelität auf der Granularität jeder einzelnen Schicht im Netz.Wir stellen einen auf Eliminierung basierenden Algorithmus vor, der eine optimale Parallelitätskonfiguration für jede Schicht findet.Unsere Evaluierung zeigt, dass DeePa im Vergleich zu State-of-the-Art Deep-Learning-Frameworks eine bis zu 6,5-fache Beschleunigung erreicht und den Datentransfer um das 23-fache reduziert.
Das neue Netzwerk erbt die Ausdruckskraft und die Architektur des Originals, arbeitet aber auf intuitivere Weise, da jeder Knoten die einfache Interpretation als Hyperebene (in einem reproduzierenden Kernel-Hilbert-Raum) genießt. Im Gegensatz zur Backpropagation, die Modelle in Black Boxes verwandelt, kann die optimale versteckte Repräsentation intuitiv geometrisch interpretiert werden, so dass die Dynamik des Lernens in einem tiefen Kernel-Netzwerk einfach zu verstehen ist.Empirische Ergebnisse werden bereitgestellt, um unsere Theorie zu bestätigen.
Viele Begriffe der Fairness können als lineare Einschränkungen ausgedrückt werden, und das resultierende eingeschränkte Ziel wird oft durch die Umwandlung des Problems in sein Lagrange-Dual mit additiven linearen Strafen optimiert. In nicht-konvexen Umgebungen kann das resultierende Problem schwierig zu lösen sein, da das Lagrange-Gleichgewicht nicht garantiert ist.  In diesem Papier schlagen wir vor, die linearen Strafen in Strafen zweiter Ordnung zu ändern, und wir argumentieren, dass dies zu einem praktikableren Trainingsverfahren in nicht-konvexen Umgebungen mit großen Datenmengen führt: Zum einen ermöglicht die Verwendung von Strafen zweiter Ordnung das Training des bestraften Ziels mit einem festen Wert des Strafkoeffizienten, wodurch die Instabilität und der potenzielle Mangel an Konvergenz vermieden werden, die mit Min-Max-Spielen mit zwei Spielern verbunden sind. Zweitens leiten wir eine Methode zur effizienten Berechnung der Gradienten ab, die mit den Strafen zweiter Ordnung in stochastischen Mini-Batch-Einstellungen verbunden sind, und unser daraus resultierender Algorithmus zeigt gute empirische Ergebnisse, indem er einen angemessen fairen Klassifikator auf einer Reihe von Standard-Benchmarks lernt.
Trainingsmethoden für tiefe Netzwerke sind hauptsächlich Varianten des stochastischen Gradientenabstiegs.  Techniken, die (ungefähre) Informationen zweiter Ordnung verwenden, werden aufgrund der Rechenkosten und des Rauschens, die mit diesen Ansätzen im Zusammenhang mit Deep Learning verbunden sind, nur selten eingesetzt.  In diesem Papier zeigen wir jedoch, wie tiefe Feedforward-Netzwerke eine Derivatstruktur mit niedrigem Rang aufweisen.  Diese Struktur mit niedrigem Rang ermöglicht die Verwendung von Informationen zweiter Ordnung, ohne dass Näherungen erforderlich sind und ohne dass die Rechenkosten wesentlich höher sind als beim Gradientenabstieg.  Um diese Fähigkeit zu demonstrieren, implementieren wir die kubische Regularisierung (CR) auf einem tiefen Feedforward-Netzwerk mit stochastischem Gradientenabstieg und zwei seiner Varianten.  Dabei verwenden wir CR zur Berechnung der Lernraten pro Iteration beim Training auf den Datensätzen MNIST und CIFAR-10.  CR erwies sich als besonders erfolgreich, wenn es darum ging, Plateauregionen der Zielfunktion zu vermeiden.  Wir haben auch festgestellt, dass dieser Ansatz weniger problemspezifische Informationen (z. B. eine optimale anfängliche Lernrate) benötigt als andere Methoden erster Ordnung, um gut zu funktionieren.
Das Prinzip des Worst-Case-Trainings, das den maximalen Verlust des Gegners minimiert, auch bekannt als adversariales Training (AT), hat sich als State-of-the-Art-Ansatz zur Verbesserung der Robustheit des Gegners gegenüber normkugelgebundenen Eingabestörungen erwiesen. Beispiele für diese allgemeine Formulierung sind der Angriff auf Modell-Ensembles, die Entwicklung einer universellen Störung unter mehreren Eingaben oder Datentransformationen und die verallgemeinerte AT über verschiedene Arten von Angriffsmodellen. Wir zeigen, dass diese Probleme unter einem einheitlichen und theoretisch prinzipiellen Min-Max-Optimierungsrahmen gelöst werden können.  Wir zeigen auch, dass die selbstangepasste Domänengewichte aus unserer Methode gelernt bietet ein Mittel, um den Schwierigkeitsgrad der Angriff und Verteidigung über mehrere domains.Extensive Experimente zeigen, dass unser Ansatz führt zu erheblichen Leistungsverbesserung gegenüber der konventionellen Mittelwertbildung Strategie zu erklären.
Die meisten Deep-Learning-Modelle stützen sich auf aussagekräftige hochdimensionale Darstellungen, um eine gute Leistung bei Aufgaben wie der Klassifizierung zu erzielen, aber die hohe Dimensionalität dieser Darstellungen macht sie schwer zu interpretieren und anfällig für Überanpassung. Bei der Anwendung unseres Frameworks auf die Visualisierung spiegeln unsere Darstellungen die Abstände zwischen den Klassen genauer wider als Standard-Visualisierungstechniken wie t-SNE. Wir zeigen experimentell, dass unser Framework die Generalisierungsleistung auf ungesehene Kategorien beim Zero-Shot-Lernen verbessert.
Intrinsisch motivierte Zielexplorationsalgorithmen ermöglichen es Maschinen, Repertoires von Strategien zu entdecken, die eine Vielfalt von Effekten in komplexen Umgebungen erzeugen.Diese Explorationsalgorithmen haben gezeigt, dass Roboter in der realen Welt Fähigkeiten wie den Gebrauch von Werkzeugen in hochdimensionalen kontinuierlichen Zustands- und Aktionsräumen erwerben können.Allerdings haben sie bisher angenommen, dass selbst generierte Ziele in einem speziell entwickelten Merkmalsraum abgetastet werden, was ihre Autonomie einschränkt.In dieser Arbeit schlagen wir einen Ansatz vor, der tiefe Repräsentationslernalgorithmen verwendet, um einen angemessenen Zielraum zu lernen. Es handelt sich dabei um einen zweistufigen Entwicklungsansatz: In einer ersten Phase des Wahrnehmungslernens verwenden Deep-Learning-Algorithmen passive Rohsensorbeobachtungen von Weltveränderungen, um einen entsprechenden latenten Raum zu erlernen; in einer zweiten Phase erfolgt dann die Zielexploration, indem Ziele in diesem latenten Raum abgetastet werden.
Eine der größten Herausforderungen von Deep-Learning-Methoden ist die Wahl einer geeigneten Trainingsstrategie, insbesondere zusätzliche Schritte, wie unbeaufsichtigtes Pre-Training, können die Leistung von Deep-Strukturen erheblich verbessern.In diesem Artikel schlagen wir einen zusätzlichen Trainingsschritt vor, der Post-Training genannt wird und nur die letzte Schicht des Netzwerks optimiert. Wir zeigen, dass dieses Verfahren im Kontext der Kernel-Theorie analysiert werden kann, wobei die ersten Schichten eine Einbettung der Daten berechnen und die letzte Schicht ein statistisches Modell, um die Aufgabe auf der Grundlage dieser Einbettung zu lösen.Dieser Schritt stellt sicher, dass die Einbettung oder Repräsentation der Daten in der bestmöglichen Weise für die betrachtete Aufgabe verwendet wird.Diese Idee wird dann auf mehreren Architekturen mit verschiedenen Datensätzen getestet und es zeigt sich, dass sie durchweg eine Leistungssteigerung bietet.
Der Einsatz von neuronalen NLP-Modellen auf mobilen Geräten erfordert eine Komprimierung der Worteinbettungen ohne signifikante Leistungseinbußen.Zu diesem Zweck schlagen wir vor, die Einbettungen mit wenigen Basisvektoren zu konstruieren.Für jedes Wort wird die Zusammensetzung der Basisvektoren durch einen Hash-Code bestimmt.Um die Kompressionsrate zu maximieren, verwenden wir anstelle eines binären Kodierungsschemas den Multi-Codebook-Quantisierungsansatz. Wir schlagen vor, die diskreten Codes direkt in einem neuronalen End-to-End-Netzwerk zu lernen, indem wir den Gumbel-Softmax-Trick anwenden. Experimente zeigen, dass die Kompressionsrate 98% in einer Sentiment-Analyse-Aufgabe und 94% bis 99% in maschinellen Übersetzungsaufgaben ohne Leistungsverlust erreicht. Im Vergleich zu anderen Ansätzen wie der Segmentierung auf Zeichenebene ist die vorgeschlagene Methode sprachunabhängig und erfordert keine Änderungen an der Netzwerkarchitektur.
Es ist wichtig, anomale Eingaben zu erkennen, wenn maschinelle Lernsysteme eingesetzt werden.Die Verwendung von größeren und komplexeren Eingaben in Deep Learning vergrößert die Schwierigkeit der Unterscheidung zwischen anomalen und in der Verteilung befindlichen Beispielen.Gleichzeitig sind vielfältige Bild- und Textdaten in enormen Mengen verfügbar.Wir schlagen vor, diese Daten zu nutzen, um die Erkennung von Anomalien in Deep Learning zu verbessern, indem Anomaliedetektoren gegen einen Hilfsdatensatz von Ausreißern trainiert werden, ein Ansatz, den wir Outlier Exposure (OE) nennen. In umfangreichen Experimenten zur Verarbeitung natürlicher Sprache und zu kleinen und großen Bildverarbeitungsaufgaben stellen wir fest, dass Outlier Exposure die Erkennungsleistung signifikant verbessert. Wir beobachten auch, dass hochmoderne generative Modelle, die auf CIFAR-10 trainiert wurden, SVHN-Bildern höhere Wahrscheinlichkeiten zuweisen als CIFAR-10-Bildern; wir verwenden OE, um dieses Problem zu entschärfen.Wir analysieren auch die Flexibilität und Robustheit von Outlier Exposure und identifizieren Merkmale des Hilfsdatensatzes, die die Leistung verbessern.
Generative neuronale Netze können zwar lernen, einen bestimmten Eingabedatensatz in einen bestimmten Zieldatensatz umzuwandeln, benötigen aber genau einen solchen gepaarten Satz von Eingabe-/Ausgabedatensätzen. Um beispielsweise den Diskriminator zu täuschen, müsste ein generatives adverses Netz (GAN), das ausschließlich darauf trainiert ist, Bilder von schwarzhaarigen *Männern* in blondhaarige *Männer* umzuwandeln, geschlechtsbezogene Merkmale sowie die Haarfarbe ändern, wenn es Bilder von schwarzhaarigen *Frauen* als Eingabe erhält. Die rechnerische Herausforderung besteht darin, dass generative Modelle gut in der Lage sind, innerhalb der Mannigfaltigkeit der Daten, auf die sie trainiert wurden, neue Stichproben außerhalb der Mannigfaltigkeit zu generieren oder "außerhalb der Stichprobe" zu extrapolieren, was jedoch ein viel schwierigeres Problem darstellt, das weniger gut untersucht wurde. Um dieses Problem anzugehen, führen wir eine Technik ein, die *Neuronenbearbeitung* genannt wird und die lernt, wie Neuronen eine Bearbeitung für eine bestimmte Transformation in einem latenten Raum kodieren.Wir verwenden einen Autoencoder, um die Variation innerhalb des Datensatzes in Aktivierungen verschiedener Neuronen zu zerlegen und transformierte Daten zu erzeugen, indem wir eine Bearbeitungstransformation auf diesen Neuronen definieren. Durch die Durchführung der Transformation in einem latenten trainierten Raum kodieren wir ziemlich komplexe und nicht-lineare Transformationen der Daten mit viel einfacheren Verteilungsverschiebungen der Neuronenaktivierungen. Unsere Technik ist allgemein und funktioniert auf einer Vielzahl von Datendomänen und Anwendungen.
In diesem Papier schlagen wir vor, Nachahmung und Verstärkungslernen durch die Idee der Belohnungsgestaltung mit Hilfe eines Orakels zu kombinieren. Wir untersuchen die Wirksamkeit des nahezu optimalen Cost-to-Go-Orakels auf den Planungshorizont und zeigen, dass das Cost-to-Go-Orakel den Planungshorizont des Lernenden in Abhängigkeit von seiner Genauigkeit verkürzt: Ein global optimales Orakel kann den Planungshorizont auf eins verkürzen, was zu einem einstufigen gierigen Markov-Entscheidungsprozess führt, der viel einfacher zu optimieren ist, während ein Orakel, das weit von der Optimalität entfernt ist, eine Planung über einen längeren Horizont erfordert, um eine nahezu optimale Leistung zu erzielen. Motiviert durch die oben erwähnten Erkenntnisse schlagen wir Truncated HORizon Policy Search (THOR) vor, eine Methode, die sich auf die Suche nach Strategien konzentriert, die die gesamte umgestaltete Belohnung über einen endlichen Planungshorizont maximieren, wenn das Orakel suboptimal ist.Wir zeigen experimentell, dass eine gradientenbasierte Implementierung von THOR im Vergleich zu RL-Basislinien und IL-Basislinien eine überlegene Leistung erzielen kann, selbst wenn das Orakel suboptimal ist.
Kürzlich haben sich Generative Adversarial Networks (GANs) als eine beliebte Alternative für die Modellierung komplexer hochdimensionaler Verteilungen herauskristallisiert.Die meisten der bestehenden Arbeiten gehen implizit davon aus, dass die sauberen Stichproben aus der Zielverteilung leicht verfügbar sind. In diesem Papier betrachten wir die Beobachtung Einstellung, in der die Proben aus einer Zielverteilung durch die Überlagerung von zwei strukturierten Komponenten gegeben sind, und Hebelwirkung GANs für das Lernen der Struktur der Komponenten.Wir schlagen eine neuartige Rahmen, Demixing-GAN, die die Verteilung von zwei Komponenten zur gleichen Zeit lernt.Durch umfangreiche numerische Experimente, zeigen wir, dass der vorgeschlagene Rahmen saubere Proben aus unbekannten Verteilungen, die weiter in Demixing der ungesehenen Testbilder verwendet werden können generieren kann.
Normalisierende Flüsse (NFs) sind eine Klasse von Likelihood-basierten generativen Modellen, die in letzter Zeit an Popularität gewonnen haben.Sie basieren auf der Idee, eine einfache Dichte in die der Daten zu transformieren.Wir versuchen, diese Klasse von Modellen besser zu verstehen, und wie sie im Vergleich zu früher vorgeschlagenen Techniken für generative Modellierung und unbeaufsichtigtes Repräsentationslernen.Zu diesem Zweck interpretieren wir NFs im Rahmen von Variational Autoencodern (VAEs) neu und präsentieren eine neue Form von VAE, die normalisierende Flüsse verallgemeinert. Unter Verwendung unseres vereinheitlichten Modells untersuchen wir systematisch den Modellraum zwischen Flows, Variations-Autoencodern und Denoising-Autoencodern in einer Reihe von vorläufigen Experimenten mit handgeschriebenen MNIST-Ziffern.
Wir untersuchen Methoden zum effizienten Erlernen verschiedener Strategien beim Verstärkungslernen für ein generatives strukturiertes Vorhersageproblem: die Umformulierung von Anfragen. Unsere Methode macht das Lernen schneller, weil sie hochgradig parallelisierbar ist, und hat eine bessere Generalisierungsleistung als starke Grundlinien, wie z.B. ein Ensemble von Agenten, die auf den vollständigen Daten trainiert werden. Dies deutet darauf hin, dass hierarchische Multi-Agenten-Ansätze eine wichtige Rolle bei strukturierten Vorhersageaufgaben dieser Art spielen könnten.Wir stellen jedoch auch fest, dass es nicht offensichtlich ist, wie die Vielfalt in diesem Kontext zu charakterisieren ist, und ein erster Versuch, der auf Clustering basiert, keine guten Ergebnisse lieferte.Darüber hinaus ist das Verstärkungslernen für die Reformulierungsaufgabe in Hochleistungsregimen schwierig.Bestenfalls verbessert es sich nur geringfügig gegenüber dem Stand der Technik, was die Komplexität des Trainings von Modellen in diesem Rahmen für End-to-End-Sprachverstehensprobleme hervorhebt.
Wir untersuchen Probleme des kontinuierlichen Aktionsverstärkungslernens, bei denen es entscheidend ist, dass der Agent mit der Umwelt nur durch sichere Strategien interagiert, d.h. Strategien, die den Agenten sowohl während des Trainings als auch bei der Konvergenz in wünschenswerten Situationen halten. Wir formulieren diese Probleme als Markov-Entscheidungsprozesse (CMDPs) und stellen Algorithmen zur sicheren Politikoptimierung vor, die auf einem Lyapunov-Ansatz basieren, um sie zu lösen. Unsere Algorithmen können jede Standard-Politikgradienten-Methode (PG) verwenden, wie z. B. Deep Deterministic Policy Gradient (DDPG) oder Proximal Policy Optimization (PPO), um eine Politik für ein neuronales Netzwerk zu trainieren, während sie für jede Politikaktualisierung eine annähernde Erfüllung der Einschränkungen garantieren, indem sie entweder den Politikparameter oder die ausgewählte Aktion auf die Menge der machbaren Lösungen projizieren, die durch die zustandsabhängigen linearisierten Lyapunov-Einschränkungen induziert werden. Im Vergleich zu den bestehenden beschränkten PG-Algorithmen sind unsere Algorithmen dateneffizienter, da sie in der Lage sind, sowohl On-Policy- als auch Off-Policy-Daten zu nutzen. Darüber hinaus führt unser Action-Projection-Algorithmus oft zu weniger konservativen Policy-Updates und ermöglicht eine natürliche Integration in eine End-to-End-PG-Trainings-Pipeline. Wir evaluieren unsere Algorithmen und vergleichen sie mit den State-of-the-Art-Baselines auf mehreren simulierten (MuJoCo) Aufgaben sowie einem realen Roboter-Hindernisvermeidungsproblem und demonstrieren ihre Effektivität in Bezug auf die Ausgewogenheit von Leistung und Constraint-Erfüllung.
Neuere Arbeiten zu generativen Modellen, insbesondere zu generativen adversen Netzen, erzeugen schöne Muster verschiedener Bildkategorien, aber die Validierung ihrer Qualität hängt stark von der verwendeten Methode ab: Ein guter Generator sollte Daten erzeugen, die aussagekräftige und vielfältige Informationen enthalten und der Verteilung eines Datensatzes entsprechen. Unser Ansatz basiert auf dem Training eines Klassifizierers mit einer Mischung aus realen und generierten Stichproben. Wir trainieren ein generatives Modell über einen markierten Trainingssatz und verwenden dieses generative Modell dann, um neue Datenpunkte zu sammeln, die wir mit den ursprünglichen Trainingsdaten mischen. Durch die Berechnung der Genauigkeit des Klassifizierers mit verschiedenen Verhältnissen von Proben aus beiden Verteilungen (real und generiert) können wir abschätzen, ob der Generator erfolgreich die Verteilung des Datensatzes anpasst und verallgemeinern kann. Unsere Experimente vergleichen die Ergebnisse verschiedener Generatoren aus dem VAE- und GAN-Framework auf MNIST- und Mode-MNIST-Datensatz.
Wir schlagen vor, Automating Science Journalism (ASJ), der Prozess der Herstellung einer Pressemitteilung von einem wissenschaftlichen Papier, als eine neue Aufgabe, die als eine neue Benchmark für neuronale abstrakte summarization.ASJ dienen kann, ist eine anspruchsvolle Aufgabe, wie es lange Quelltexte erfordert, um lange Zieltexte zusammengefasst werden, während auch komplexe wissenschaftliche Konzepte zu paraphrasieren, um von der allgemeinen audience.for diesen Zweck zu verstehen, führen wir eine spezialisierte Datensatz für ASJ, die wissenschaftliche Papiere und ihre Pressemitteilungen von Science Daily enthält. Während State-of-the-Art Sequenz-zu-Sequenz-Modelle (seq2seq) leicht überzeugende Pressemitteilungen für ASJ generieren könnten, sind diese in der Regel nicht faktisch und weichen von der Quelle ab. Um dieses Problem zu lösen, verbessern wir die seq2seq-Generierung durch Transfer-Lernen, indem wir mit neuen Zielen co-trainieren:(i) wissenschaftliche Zusammenfassungen von Quellen und(ii) partitionierte Pressemitteilungen. Unsere quantitative und qualitative Bewertung zeigt erhebliche Verbesserungen gegenüber einer starken Basislinie, was darauf hindeutet, dass der vorgeschlagene Rahmen die seq2seq-Zusammenfassung über ASJ hinaus verbessern könnte.
Die Interpretierbarkeit des Verhaltens eines KI-Agenten ist von größter Bedeutung für eine effektive Mensch-KI-Interaktion.Zu diesem Zweck gibt es ein zunehmendes Interesse an der Charakterisierung und Generierung von interpretierbarem Verhalten des Agenten.Ein alternativer Ansatz, um zu garantieren, dass der Agent interpretierbares Verhalten generiert, wäre es, die Umgebung des Agenten so zu gestalten, dass nicht-interpretierbares Verhalten entweder prohibitiv teuer oder für den Agenten nicht verfügbar ist.Bis heute gibt es Arbeiten unter dem Dach der Ziel-oder Plan-Erkennung Design erforscht diesen Begriff der Umwelt Neugestaltung in einigen spezifischen Fällen von interpretierbaren Verhalten. Wir konzentrieren uns auf drei spezifische Arten von interpretierbarem Verhalten - Erklärbarkeit, Lesbarkeit und Vorhersagbarkeit - und stellen einen allgemeinen Rahmen für das Problem des Umgebungsdesigns vor, der instanziiert werden kann, um jedes der drei interpretierbaren Verhaltensweisen zu erreichen, und diskutieren, wie spezifische Instanziierungen dieses Rahmens mit früheren Arbeiten zum Umgebungsdesign korrespondieren, und zeigen spannende Möglichkeiten für zukünftige Arbeiten auf.
Inferenzmodelle, die eine optimierungsbasierte Inferenzprozedur durch ein gelerntes Modell ersetzen, sind grundlegend für die Weiterentwicklung des Bayes'schen Deep Learning, wobei das bemerkenswerteste Beispiel die Variations-Autokodierer (VAEs) sind.In diesem Papier schlagen wir iterative Inferenzmodelle vor, die lernen, wie man eine Variationsuntergrenze durch wiederholtes Kodieren von Gradienten optimiert. Unser Ansatz verallgemeinert VAEs unter bestimmten Bedingungen, und durch die Betrachtung von VAEs im Kontext der iterativen Inferenz bieten wir weitere Einblicke in mehrere aktuelle empirische Erkenntnisse.Wir demonstrieren die Inferenzoptimierungsfähigkeiten iterativer Inferenzmodelle, untersuchen einzigartige Aspekte dieser Modelle und zeigen, dass sie Standard-Inferenzmodelle auf typischen Benchmark-Datensätzen übertreffen.
In künstlichen neuronalen Netzen, die mit Gradientenabstieg trainiert werden, werden die Gewichte, die für die Verarbeitung von Reizen verwendet werden, auch bei Rückwärtsdurchläufen verwendet, um Gradienten zu berechnen.Damit das reale Gehirn Gradienten annähern kann, müsste die Gradienteninformation separat propagiert werden, so dass ein Satz synaptischer Gewichte für die Verarbeitung und ein anderer Satz für Rückwärtsdurchläufe verwendet wird. Daraus ergibt sich für biologische Lernmodelle das so genannte "Gewichtstransportproblem", bei dem die zur Berechnung von Gradienten verwendeten Rückwärtsgewichte die zur Verarbeitung von Reizen verwendeten Vorwärtsgewichte widerspiegeln müssen. Dieses Gewichtstransportproblem wurde als so schwierig angesehen, dass gängige Vorschläge für biologisches Lernen davon ausgehen, dass die Rückwärtsgewichte einfach zufällig sind, wie beim Feedback-Algorithmus. Der resultierende Algorithmus ist ein Spezialfall eines Schätzers, der für kausale Schlussfolgerungen in der Ökonometrie verwendet wird, dem Regressions-Diskontinuitäts-Design. Wir zeigen empirisch, dass dieser Algorithmus die Rückwärtsgewichte schnell an die Vorwärtsgewichte annähert. Wir zeigen empirisch, dass dieser Algorithmus die rückwärts gerichteten Gewichte schnell an die vorwärts gerichteten Gewichte annähert, was die Lernleistung gegenüber dem Feedback-Abgleich bei Aufgaben wie Fashion-MNIST und CIFAR-10 verbessert.
Variational Inference (VI)-Methoden und insbesondere Variational Autoencoders (VAEs) spezifizieren skalierbare generative Modelle, die eine intuitive Verbindung zum Lernen von Mannigfaltigkeiten haben --- bei vielen Standardprioritäten kann das Posterior/Likelihood-Paar $q(z|x)$/$p(x|z)$ als ein approximativer Homöomorphismus (und dessen Inverse) zwischen der Datenmannigfaltigkeit und einem latenten euklidischen Raum betrachtet werden. Es ist jedoch gut dokumentiert, dass diese Näherungen beim Training degenerieren. Wenn der subjektive Prior nicht sorgfältig gewählt wird, stimmen die Topologien der Prior- und Datenverteilungen oft nicht überein. Umgekehrt \textit{infer} Diffusionskarten (DM) automatisch die Datentopologie und genießen eine strenge Verbindung zum Lernen von Mannigfaltigkeiten, aber skalieren nicht leicht oder liefern den inversen Homöomorphismus.In diesem Papier schlagen wir \textbf{a)} ein prinzipielles Maß für die Erkennung der Nichtübereinstimmung zwischen Daten und latenten Verteilungen und \textbf{b)} eine Methode vor, die die Vorteile der Variationsinferenz und Diffusionskarten kombiniert, um ein homöomorphes generatives Modell zu lernen. Das Maß, die \textit{lokale Bi-Lipschitz-Eigenschaft}, ist eine hinreichende Bedingung für einen Homöomorphismus und einfach zu berechnen und zu interpretieren.Die Methode, der \textit{variational diffusion autoencoder} Die Methode, der \textit{variational diffusion autoencoder} (VDAE), ist ein neuartiger generativer Algorithmus, der zunächst die Topologie der Datenverteilung ableitet und dann einen Diffusions-Zufallsspaziergang über die Daten modelliert.Um eine effiziente Berechnung in VDAEs zu erreichen, verwenden wir stochastische Versionen sowohl der Variationsinferenz als auch der Manifold Learning Optimierung. Wir beweisen approximationstheoretische Ergebnisse für die Dimensionsabhängigkeit von VDAEs und zeigen, dass lokal isotropes Sampling im latenten Raum zu einem Random Walk über die rekonstruierte Mannigfaltigkeit führt, und demonstrieren den Nutzen unserer Methode an verschiedenen realen und synthetischen Datensätzen.
Während Deep Learning und Deep Reinforcement Learning-Systeme beeindruckende Ergebnisse in Bereichen wie Bildklassifikation, Spielen und Robotersteuerung gezeigt haben, bleibt die Dateneffizienz eine große Herausforderung, insbesondere da diese Algorithmen einzelne Aufgaben von Grund auf neu lernen.Multi-Task-Lernen hat sich als vielversprechender Ansatz für die gemeinsame Nutzung von Strukturen über mehrere Aufgaben, um effizienteres Lernen zu ermöglichen.Allerdings stellt die Multi-Task-Einstellung eine Reihe von Optimierungsherausforderungen, so dass es schwierig ist, große Effizienzgewinne im Vergleich zum Lernen von Aufgaben unabhängig zu realisieren.Die Gründe, warum Multi-Task-Lernen ist so anspruchsvoll im Vergleich zu Einzelaufgabe Lernen sind nicht vollständig verstanden. Motiviert durch die Einsicht, dass Gradienteninterferenzen zu Optimierungsproblemen führen, entwickeln wir einen einfachen und allgemeinen Ansatz zur Vermeidung von Interferenzen zwischen den Gradienten verschiedener Aufgaben, indem wir die Gradienten durch eine Technik verändern, die wir als "Gradientenchirurgie" bezeichnen.Wir schlagen eine Form der Gradientenchirurgie vor, die den Gradienten einer Aufgabe auf die Normalebene des Gradienten jeder anderen Aufgabe projiziert, die einen konfligierenden Gradienten hat.Bei einer Reihe von anspruchsvollen überwachten Multitask- und Multitask-Verstärkungslernproblemen stellen wir fest, dass dieser Ansatz zu erheblichen Effizienz- und Leistungsgewinnen führt.  Außerdem kann er effektiv mit zuvor vorgeschlagenen Multi-Task-Architekturen kombiniert werden, um die Leistung auf modellunabhängige Weise zu verbessern.
In dieser Arbeit schlagen wir vor, die Akzeptanzrate des Metropolis-Hastings-Algorithmus als ein universelles Ziel für das Lernen von Stichproben aus der Zielverteilung zu betrachten, die entweder als eine Menge von Stichproben oder in Form einer unnormalisierten Dichte gegeben ist, was die Ziele von Ansätzen wie Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs) und Variationsinferenz vereint. Um diese Verbindung aufzuzeigen, leiten wir die untere Schranke für die Akzeptanzrate ab und behandeln sie als Ziel für das Lernen von expliziten und impliziten Samplern.Die Form der unteren Schranke erlaubt eine doppelt stochastische Gradientenoptimierung für den Fall, dass die Zielverteilung faktorisiert ist (d.h. über Datenpunkte).Wir validieren unseren Ansatz empirisch an Bayes'scher Inferenz für neuronale Netze und generative Modelle für Bilder.
Unsere Methode erweitert das BERT-Modell für Textsequenzen auf den Fall von Sequenzen reellwertiger Merkmalsvektoren, indem wir den Softmax-Verlust durch eine rauschkontrastive Schätzung (Noise Contrastive Estimation, NCE) ersetzen. Wir zeigen auch, wie man Repräsentationen aus Sequenzen visueller Merkmale und aus Wortsequenzen lernt, die aus der automatischen Spracherkennung (ASR) abgeleitet wurden, und wir zeigen, dass ein solches cross-modales Training (wenn möglich) noch mehr hilft.
Wir stellen eine generische dynamische Architektur vor, die einen problem-spezifischen differenzierbaren Forking-Mechanismus einsetzt, um diskrete logische Informationen über die Datenstruktur des Problems zu nutzen. wir passen unser Modell an CLEVR Visual Question Answering an und wenden es an, wodurch die DDRprog-Architektur entsteht; im Vergleich zu früheren Ansätzen erreicht unser Modell eine höhere Genauigkeit in halb so vielen Epochen mit fünfmal weniger lernbaren Parametern. Unser Modell modelliert direkt die zugrundeliegende Fragelogik mit Hilfe eines rekurrenten Controllers, der funktionale neuronale Module gemeinsam vorhersagt und ausführt; er gabelt explizit Unterprozesse auf, um logische Verzweigungen zu handhaben. während FiLM und andere konkurrierende Modelle statische Architekturen mit weniger Überwachung sind, argumentieren wir, dass die Einbeziehung von Programmbeschriftungen das Lernen von logischen Operationen auf höherer Ebene ermöglicht - unsere Architektur erreicht besonders hohe Leistungen bei Fragen, die Zählen und Ganzzahlvergleiche erfordern. Darüber hinaus demonstrieren wir die Allgemeingültigkeit unseres Ansatzes durch DDRstack - eine Anwendung unserer Methode auf die Bewertung von Ausdrücken in umgekehrter polnischer Notation, bei der die Einbeziehung einer Stack-Annahme es unserem Ansatz ermöglicht, sich auf lange Ausdrücke zu verallgemeinern und ein LSTM mit zehnmal so vielen lernbaren Parametern deutlich zu übertreffen.
Wir schlagen Support-guided Adversarial Imitation Learning (SAIL) vor, einen generischen Rahmen für das Imitationslernen, der die Support-Schätzung der Expertenpolitik mit der Familie der Adversarial Imitation Learning (AIL)-Algorithmen vereint. SAIL adressiert zwei wichtige Herausforderungen von AIL, einschließlich der impliziten Belohnungsverzerrung und der möglichen Trainingsinstabilität. Wir zeigen auch, dass SAIL mindestens so effizient ist wie Standard-AIL. In einer umfangreichen Evaluierung zeigen wir, dass die vorgeschlagene Methode effektiv mit der Belohnungsverzerrung umgeht und eine bessere Leistung und Trainingsstabilität als andere Basismethoden bei einer breiten Palette von Benchmark-Kontrollaufgaben erreicht.
Wir betrachten die Aufgabe der few shot link prediction, wo das Ziel ist, fehlende Kanten über mehrere Graphen mit nur einer kleinen Stichprobe von bekannten edges.we zeigen, dass aktuelle Link-Vorhersage-Methoden sind in der Regel schlecht ausgestattet, um diese Aufgabe zu behandeln - da sie nicht effektiv übertragen Wissen zwischen Graphen in einem Multi-Graph Einstellung und sind nicht in der Lage, effektiv lernen aus sehr spärlichen Daten. Um diese Herausforderung zu bewältigen, stellen wir ein neues gradientenbasiertes Meta-Lernverfahren vor, Meta-Graph, das Gradienten höherer Ordnung zusammen mit einer erlernten Graphsignaturfunktion nutzt, die bedingt eine Initialisierung eines neuronalen Graphennetzwerks erzeugt.
Generative neuronale Netze bilden eine mögliche Standardverteilung auf eine komplexe hochdimensionale Verteilung ab, die den realen Datensatz repräsentiert. Eine bestimmte Eingangsverteilung sowie eine spezifische Architektur neuronaler Netze können jedoch zu Einschränkungen bei der Erfassung der Vielfalt im hochdimensionalen Zielraum führen. Wir zeigen theoretisch und empirisch, dass unser Trainingsalgorithmus zur theoretisch optimalen Verteilung konvergiert, der Projektion der realen Verteilung auf die konvexe Hülle des Verteilungsraums des Netzwerks.
Generative Prioren haben sich bei der Lösung inverser Probleme wie Rauschunterdrückung, Inpainting und Rekonstruktion aus wenigen und verrauschten Messungen als sehr effektiv erwiesen.Mit einem generativen Modell können wir ein Bild mit einem viel niedriger dimensionalen latenten Code darstellen.Wenn das unbekannte Bild zum Bereich eines vortrainierten generativen Netzwerks gehört, können wir das Bild wiederherstellen, indem wir den zugrunde liegenden kompakten latenten Code aus den verfügbaren Messungen schätzen.Jüngste Studien haben jedoch gezeigt, dass auch untrainierte tiefe neuronale Netze als Prior für die Wiederherstellung natürlicher Bilder funktionieren können. Diese Ansätze aktualisieren die Netzwerkgewichte und halten die latenten Codes fest, um das Zielbild aus den gegebenen Messungen zu rekonstruieren. In diesem Papier optimieren wir die Netzwerkgewichte und die latenten Codes, um ein untrainiertes generatives Netzwerk als Prior für das Video Compression Sensing Problem zu verwenden. Wir zeigen, dass durch die Optimierung über latente Codes, können wir zusätzlich erhalten prägnante Darstellung der Frames, die die strukturelle Ähnlichkeit der Video-Frames.We auch Low-Rank-Beschränkung auf die latenten Codes, um die Video-Sequenzen in noch niedriger dimensionalen latenten space.We empirisch zeigen, dass unsere vorgeschlagenen Methoden bieten bessere oder vergleichbare Genauigkeit und geringe rechnerische Komplexität im Vergleich zu den bestehenden Methoden.
Magnitudenbasiertes Pruning ist eine der einfachsten Methoden zum Pruning neuronaler Netze, die trotz ihrer Einfachheit bemerkenswerte Leistungen beim Pruning moderner Architekturen zeigt. Basierend auf der Beobachtung, dass das magnitudenbasierte Pruning in der Tat die Frobenius-Verzerrung eines linearen Operators, der einer einzelnen Schicht entspricht, minimiert, entwickeln wir eine einfache Pruning-Methode, das sogenannte Lookahead Pruning, indem wir die einschichtige Optimierung zu einer mehrschichtigen Optimierung erweitern.
Die jüngste Literatur hat vielversprechende Ergebnisse bei der Ausbildung von Generative Adversarial Networks durch den Einsatz einer Reihe von Diskriminatoren, im Gegensatz zu den traditionellen Spiel mit einem Generator gegen einen einzigen adversary.Those Methoden führen Single-Objektiv-Optimierung auf einige einfache Konsolidierung der Verluste, z. B. ein Durchschnitt.In dieser Arbeit, die wir wieder die Multi-Diskriminator-Ansatz durch Framing die gleichzeitige Minimierung der Verluste, die von verschiedenen Modellen als Multi-Objektiv-Optimierung Problem. Darüber hinaus argumentieren wir, dass die zuvor vorgeschlagenen Methoden und die Hypervolumen-Maximierung alle als Variationen des multiplen Gradientenabstiegs angesehen werden können, bei denen die Berechnung der Aktualisierungsrichtung effizient durchgeführt werden kann.Unsere Ergebnisse deuten darauf hin, dass die Hypervolumen-Maximierung einen besseren Kompromiss zwischen Probenqualität und -vielfalt und den Rechenkosten darstellt als frühere Methoden.
Wir schlagen einen feinkörnigen Suchraum vor, der aus atomaren Blöcken besteht, eine minimale Sucheinheit, die viel kleiner ist als die, die in den aktuellen NAS-Algorithmen verwendet wird. Darüber hinaus schlagen wir einen ressourcenschonenden Architektur-Suchalgorithmus vor, der während des Trainings dynamisch atomare Blöcke auswählt und durch eine dynamische Netzwerkschrumpfungstechnik weiter beschleunigt wird. Unsere Methode erreicht die beste Leistung unter verschiedenen FLOPS-Konfigurationen auf ImageNet mit vernachlässigbaren Suchkosten. Wir öffnen unsere gesamte Codebasis unter: https://github.com/meijieru/AtomNAS.
Wir stellen Lyceum vor, ein hochleistungsfähiges rechnerisches Ökosystem für das Roboterlernen.   Lyceum basiert auf der Programmiersprache Julia und demMuJoCo-Physiksimulator und kombiniert die Benutzerfreundlichkeit einer Hochsprache mit der Leistung von C. Lyceum ist bis zu 10-20 Mal schneller als andere populäre Abstraktionen wie OpenAI'sGym und Deep-Mind'sdm-control.  Dadurch wird die Trainingszeit für verschiedene Re-Inforcement-Learning-Algorithmen erheblich verkürzt; außerdem ist Lyceum schnell genug, um prädiktive Modellsteuerung in Echtzeit mit Physiksimulatoren zu unterstützen.   Lyceum verfügt über eine unkomplizierte API und unterstützt parallele Berechnungen auf mehreren Kernen oder Maschinen. https://sites.google.com/view/lyceum-anon bietet die Codebasis, Tutorials und Demonstrationsvideos.
Es besteht ein starker Anreiz, vielseitige Lerntechniken zu entwickeln, die das Wissen über die Klassentrennbarkeit von einer beschrifteten Quelldomäne auf eine unbeschriftete Zieldomäne übertragen können, wenn eine Domänenverschiebung vorliegt.Bestehende Domänenanpassungsansätze (DA) sind für praktische DA-Szenarien nicht geeignet, da sie auf das Wissen über die Beziehung zwischen Quelle und Zielbeschriftung angewiesen sind (z. B. Closed-set, Open-set oder Partial DA). Darüber hinaus erfordern fast alle bisherigen unüberwachten DA-Arbeiten die Koexistenz von Quell- und Zielmustern auch während des Einsatzes, was sie für eine inkrementelle Echtzeitanpassung ungeeignet macht.Ohne solche höchst unpraktischen Annahmen schlagen wir einen neuartigen zweistufigen Lernprozess vor.In der Beschaffungsphase besteht das Ziel zunächst darin, das Modell für einen zukünftigen quellenfreien Einsatz auszurüsten, wobei kein Vorwissen über die bevorstehende Kategorielücke und Domänenverschiebung vorausgesetzt wird. Um dies zu erreichen, verbessern wir die Fähigkeit des Modells, Muster, die nicht aus der Quelle stammen, abzulehnen, indem wir die verfügbaren Quelldaten in einem neuartigen generativen Klassifikatorrahmen nutzen.Anschließend, in der Einsatzphase, besteht das Ziel darin, einen einheitlichen Anpassungsalgorithmus zu entwerfen, der in der Lage ist, über einen breiten Bereich von Kategorielücken zu operieren, ohne Zugang zu den zuvor gesehenen Quellmustern. Um dies zu erreichen, definieren wir im Gegensatz zur Verwendung komplexer adversarialer Trainingsverfahren ein einfaches, aber effektives quellenfreies Anpassungsziel, indem wir einen neuartigen Abwägungsmechanismus auf Instanzebene verwenden, der als Source Similarity Metric (SSM) bezeichnet wird.Eine gründliche Evaluierung zeigt die praktische Anwendbarkeit des vorgeschlagenen Lernrahmens mit einer überlegenen DA-Leistung sogar im Vergleich zu modernen quellenabhängigen Ansätzen.
Eine der seit langem bestehenden Herausforderungen in der Künstlichen Intelligenz beim Erlernen von zielgerichtetem Verhalten besteht darin, einen einzigen Agenten zu entwickeln, der mehrere Aufgaben lösen kann. Jüngste Fortschritte im Bereich des Multitasking-Lernens für zielgerichtete sequentielle Probleme wurden in Form von auf Destillation basierendem Lernen erzielt, bei dem ein Schülernetzwerk von mehreren aufgabenspezifischen Expertennetzwerken lernt, indem es die aufgabenspezifischen Strategien der Expertennetzwerke nachahmt. Während solche Ansätze eine vielversprechende Lösung für das Multi-Task-Learning-Problem bieten, erfordern sie die Überwachung von großen Expertennetzwerken, die umfangreiche Daten und Rechenzeit für das Training benötigen.In dieser Arbeit schlagen wir einen effizienten Multi-Task-Learning-Rahmen vor, der mehrere zielgerichtete Aufgaben in einem Online-Setup löst, ohne die Notwendigkeit einer Expertenüberwachung. Wir schlagen drei verschiedene Modelle im Rahmen unseres aktiven Sampling-Rahmens vor: eine adaptive Methode mit extrem wettbewerbsfähiger Multitasking-Performance, einen UCB-basierten Meta-Lerner, der das Problem der Auswahl der nächsten Aufgabe als ein mehrarmiges Bandit-Problem darstellt. Eine Meta-Learning-Methode, die das Problem der Auswahl der nächsten Aufgabe als ein vollständiges Reinforcement-Learning-Problem darstellt und akteurskritische Methoden zur direkten Optimierung der Multitasking-Performance verwendet. Wir demonstrieren Ergebnisse in der Atari 2600-Domäne auf sieben Multitasking-Instanzen: drei 6-Task-Instanzen, eine 8-Task-Instanz, zwei 12-Task-Instanzen und eine 21-Task-Instanz.
Zahlreiche Datensätze zum maschinellen Leseverstehen (MRC) werden oft manuell annotiert, was einen enormen menschlichen Aufwand erfordert, und daher ist die Größe des Datensatzes deutlich geringer als die Größe der Daten, die für das unüberwachte Lernen zur Verfügung stehen.Kürzlich haben Forscher ein Modell zur Erzeugung synthetischer Frage-Antwort-Daten aus großen Korpora wie Wikipedia vorgeschlagen. Dieses Modell wird verwendet, um synthetische Daten für das Training eines MRC-Modells zu generieren, bevor es mit Hilfe des ursprünglichen MRC-Datensatzes feinabgestimmt wird. Diese Technik zeigt eine bessere Leistung als andere allgemeine Pre-Trainingstechniken, wie z. B. Sprachmodellierung, da die Eigenschaften der generierten Daten denen der nachgelagerten MRC-Daten ähneln.Allerdings ist es schwierig, qualitativ hochwertige synthetische Daten zu haben, die mit von Menschen kommentierten MRC-Datensätzen vergleichbar sind. Um dieses Problem zu lösen, schlagen wir die Answer-containing Sentence Generation (ASGen) vor, eine neuartige Pre-Training-Methode zur Generierung synthetischer Daten, die zwei fortschrittliche Techniken beinhaltet: (1) die dynamische Bestimmung von K-Antworten und (2) das Pre-Training des Fragengenerators für die Aufgabe der Generierung von Sätzen mit Antworten. Die experimentellen Ergebnisse zeigen, dass unser Ansatz die bestehenden Generierungsmethoden übertrifft und die Leistung der modernsten MRC-Modelle in einer Reihe von MRC-Datensätzen wie SQuAD-v1.1, SQuAD-v2.0, KorQuAD und QUASAR-T ohne architektonische Änderungen am ursprünglichen MRC-Modell erhöht.
Aufgrund der massiven GPU-Parallelisierung ist das Training von DNNs kein Engpass mehr, und große Modelle mit vielen Parametern und hohem Rechenaufwand führen gängige Benchmark-Tabellen an.Im Gegensatz dazu haben eingebettete Geräte eine sehr begrenzte Kapazität.Daher müssen sowohl die Modellgröße als auch die Inferenzzeit deutlich reduziert werden, wenn DNNs eine angemessene Leistung auf eingebetteten Geräten erreichen sollen.Wir schlagen einen Soft-Quantisierungsansatz vor, um DNNs zu trainieren, die mit reiner Festkomma-Arithmetik ausgewertet werden können. Durch Ausnutzung des Bit-Shift-Mechanismus leiten wir Festkomma-Quantisierungs-Constraints für alle wichtigen Komponenten ab, einschließlich Batch-Normalisierung und ReLU.Im Vergleich zur Fließkomma-Arithmetik reduzieren Festkomma-Berechnungen den Rechenaufwand erheblich, während Low-Bit-Darstellungen die Speicherkosten sofort senken.Wir evaluieren unseren Ansatz mit verschiedenen Architekturen auf gängigen Benchmark-Datensätzen und vergleichen ihn mit aktuellen Quantisierungsansätzen.Wir erreichen eine neue State-of-the-Art-Leistung mit 4-Bit-Festkomma-Modellen mit einer Fehlerrate von 4,98% auf CIFAR-10.
	Wir präsentieren einen neuen Ansatz und eine neuartige Architektur, genannt WSNet, für das Lernen von kompakten und effizienten tiefen neuronalen Netzen.Bestehende Ansätze lernen konventionell vollständige Modellparameter unabhängig und komprimieren sie dann über \emph{ad hoc}-Verarbeitung wie Modellbeschneidung oder Filterfaktorisierung.Alternativ schlägt WSNet vor, Modellparameter durch Sampling aus einem kompakten Satz von lernbaren Parametern zu lernen, was natürlich die gemeinsame Nutzung von Parametern während des gesamten Lernprozesses erzwingt.Wir zeigen, dass ein solcher neuartiger Ansatz des Gewichtssamplings (und induziert durch WSNet) sowohl Gewichte als auch die gemeinsame Nutzung von Berechnungen vorteilhaft fördert. Durch den Einsatz dieser Methode können wir effizienter lernen viel kleinere Netzwerke mit konkurrenzfähiger Leistung im Vergleich zu Basis-Netzwerken mit der gleichen Anzahl von Faltungsfiltern.Speziell betrachten wir das Lernen kompakte und effiziente 1D Faltungs neuronale Netze für Audio-Klassifizierung. Ausführliche Experimente mit mehreren Audioklassifizierungsdatensätzen bestätigen die Effektivität von WSNet: Kombiniert mit Gewichtsquantisierung sind die resultierenden Modelle bis zu 180 Mal kleiner und theoretisch bis zu 16 Mal schneller als die etablierten Basislösungen, ohne dass die Leistung merklich abnimmt.
Jüngste Arbeiten auf dem Gebiet des adversen maschinellen Lernens konzentrieren sich auf die visuelle Wahrnehmung beim autonomen Fahren und untersuchen Adversarial Examples (AEs) für Objekterkennungsmodelle. In solchen visuellen Wahrnehmungspipelines müssen die erkannten Objekte jedoch auch verfolgt werden, und zwar in einem Prozess, der als Multiple Object Tracking (MOT) bezeichnet wird, um die sich bewegenden Trajektorien der umliegenden Hindernisse zu erstellen. Da MOT so konzipiert ist, dass es robust gegenüber Fehlern bei der Objekterkennung ist, stellt es eine generelle Herausforderung für bestehende Angriffstechniken dar, die blind auf die Erkennung von Objekten abzielen: Wir stellen fest, dass eine Erfolgsrate von über 98 % erforderlich ist, damit sie die Verfolgungsergebnisse tatsächlich beeinflussen, eine Anforderung, die keine bestehende Angriffstechnik erfüllen kann. In dieser Arbeit untersuchen wir als erste die Angriffe auf die gesamte visuelle Wahrnehmungspipeline beim autonomen Fahren und entdecken eine neuartige Angriffstechnik, das Tracker-Hijacking, mit der MOT durch AEs bei der Objekterkennung effektiv getäuscht werden kann. Mit unserer Technik können erfolgreiche AEs auf nur einem einzigen Frame ein vorhandenes Objekt in oder aus dem Fahrbereich eines autonomen Fahrzeugs verschieben, um potenzielle Sicherheitsrisiken zu verursachen. Wir führen eine Evaluierung mit dem Berkeley Deep Drive-Datensatz durch und stellen fest, dass unser Angriff bei durchschnittlich drei Frames eine Erfolgsquote von fast 100 % hat, während Angriffe, die blind auf die Objekterkennung abzielen, nur bis zu 25 % erreichen.
Selbstüberwachtes Lernen (SlfSL), das darauf abzielt, Merkmalsrepräsentationen durch ausgeklügelte Vorwandaufgaben ohne menschliche Annotation zu erlernen, hat in den letzten Jahren überzeugende Fortschritte erzielt.In jüngster Zeit wurde SlfSL auch als vielversprechende Lösung für halbüberwachtes Lernen (SemSL) identifiziert, da es ein neues Paradigma zur Nutzung unbeschrifteter Daten bietet. Unsere Einsicht ist, dass das Vorhersageziel in SemSL als latenter Faktor im Prädiktor für das SlfSL-Ziel modelliert werden kann. Die Marginalisierung über den latenten Faktor führt natürlich zu einer neuen Formulierung, die die Vorhersageziele dieser beiden Lernprozesse verbindet. Durch die Implementierung dieses Rahmens durch einen einfachen, aber effektiven SlfSL-Ansatz - die Vorhersage des Drehwinkels - schaffen wir einen neuen SemSL-Ansatz namens Conditional Rotation Angle Prediction (CRAP), der durch die Annahme eines Moduls zur Vorhersage des Bilddrehwinkels \textbf (abhängig von der Bildklasse) gekennzeichnet ist. Durch die experimentelle Auswertung zeigen wir, dass CRAP eine bessere Leistung als die anderen existierenden Möglichkeiten der Kombination von SlfSL und SemSL erreicht, und dass der vorgeschlagene SemSL-Rahmen sehr erweiterbar ist.
Ranking ist eine zentrale Aufgabe im maschinellen Lernen und Information Retrieval.in dieser Aufgabe ist es besonders wichtig, den Benutzer mit einem Schiefer von Elementen, die als Ganzes ansprechend ist.dies wiederum erfordert die Berücksichtigung von Wechselwirkungen zwischen den Elementen, da intuitiv, die Platzierung eines Elements auf dem Schiefer wirkt sich auf die Entscheidung, welche anderen Elemente sollten neben ihm gewählt werden.in dieser Arbeit schlagen wir eine Sequenz-zu-Sequenz-Modell für das Ranking genannt seq2slate. Die rekurrente Natur des Modells ermöglicht es, komplexe Abhängigkeiten zwischen den Elementen direkt in einer flexiblen und skalierbaren Weise zu erfassen.Wir zeigen, wie das Modell Ende-zu-Ende von schwacher Überwachung in Form von leicht zu erhaltenden Click-Through-Daten zu lernen.Wir zeigen auch die Nützlichkeit unseres Ansatzes in Experimenten auf Standard-Ranking-Benchmarks sowie in einem realen Empfehlungssystem.
Während Momentum-basierte Methoden in Verbindung mit dem stochastischen Gradientenabstieg beim Training von Machine-Learning-Modellen weit verbreitet sind, gibt es nur wenige theoretische Erkenntnisse über den Generalisierungsfehler solcher Methoden.In der Praxis wird der Momentum-Parameter oft auf heuristische Weise und mit wenig theoretischer Anleitung gewählt. In dieser Arbeit verwenden wir den Rahmen der algorithmischen Stabilität, um eine Obergrenze des Generalisierungsfehlers für die Klasse der stark konvexen Verlustfunktionen unter milden technischen Annahmen zu bestimmen, die umgekehrt mit der Größe der Trainingsmenge gegen Null abfällt und mit der Erhöhung des Impulsparameters zunimmt.
Wir stellen ein modellbasiertes Nachahmungslernverfahren vor, das umweltspezifische optimale Handlungen nur aus den Zustandstrajektorien der Experten erlernen kann. Unser vorgeschlagenes Verfahren beginnt mit einem modellfreien Verstärkungslernalgorithmus mit einem heuristischen Belohnungssignal, um die Dynamik der Umgebung zu erfassen, die dann zum Trainieren der Zustandsübergangswahrscheinlichkeit verwendet wird. Experimentelle Auswertungen zeigen, dass die von uns vorgeschlagene Methode eine ähnliche Leistung wie traditionelle, auf Trajektorien (Zustand, Aktion) basierende Nachahmungslernmethoden erzielt, selbst wenn keine Aktionsinformationen vorliegen, und zwar mit viel weniger Iterationen im Vergleich zu konventionellen modellfreien Verstärkungslernmethoden.Wir zeigen auch, dass unsere Methode lernen kann, nur anhand von Videodemonstrationen des Expertenagenten für einfache Spiele zu handeln, und dass sie die gewünschte Leistung in weniger Iterationen erreichen kann.
Jüngste Forschungen haben die Lotterielos-Hypothese vorgeschlagen, die besagt, dass es für ein tiefes neuronales Netzwerk trainierbare Subnetze gibt, die bei angemessenen Trainingsschritten die gleiche oder eine bessere Leistung erbringen als das ursprüngliche Modell.Während diese Entdeckung aufschlussreich ist, erfordert das Finden geeigneter Subnetze iteratives Training und Pruning. Wir zeigen, dass es eine Untermenge der oben erwähnten Subnetze gibt, die während des Trainingsprozesses deutlich schneller konvergieren und somit das Kostenproblem entschärfen können.Wir führen umfangreiche Experimente durch, um zu zeigen, dass solche Subnetze durchgängig in verschiedenen Modellstrukturen für eine restriktive Einstellung von Hyperparametern existieren (z.B., sorgfältig ausgewählte Lernrate, Beschneidungsrate und Modellkapazität).  Als praktische Anwendung unserer Erkenntnisse zeigen wir, dass solche Subnetze dazu beitragen können, die Gesamtzeit für adversariales Training, ein Standardansatz zur Verbesserung der Robustheit, um bis zu 49% auf CIFAR-10 zu reduzieren, um die Robustheit auf dem neuesten Stand der Technik zu erreichen.
Entwirrte Repräsentationen, bei denen die generativen Faktoren auf höherer Ebene in disjunkten latenten Dimensionen widergespiegelt werden, bieten mehrere Vorteile, wie z. B. die einfache Ableitung invarianter Repräsentationen, die Übertragbarkeit auf andere Aufgaben, die Interpretierbarkeit usw. Wir betrachten das Problem des unüberwachten Lernens entwirrter Repräsentationen aus einem großen Pool unbeschrifteter Beobachtungen und schlagen einen auf Variationsinferenz basierenden Ansatz zur Ableitung entwirrter latenter Faktoren vor. Wir führen einen Regularisierer für die Erwartung des approximativen Posteriors über beobachtete Daten ein, der die Entflechtung fördert, und schlagen eine neue Entflechtungsmetrik vor, die besser mit der qualitativen Entflechtung übereinstimmt, die in der Ausgabe des Decoders zu beobachten ist.Wir beobachten empirisch eine signifikante Verbesserung gegenüber bestehenden Methoden, sowohl hinsichtlich der Entflechtung als auch der Datenwahrscheinlichkeit (Rekonstruktionsqualität). 
In Multiagentensystemen (MASs), macht jeder Agent individuelle Entscheidungen, aber alle von ihnen tragen global auf die Systementwicklung.Lernen in MASs ist schwierig, da jeder Agent die Auswahl von Aktionen in der Gegenwart von anderen Co-Learning-Agenten stattfinden muss.Darüber hinaus erhöhen die Umwelt Stochastizität und Unsicherheiten exponentiell mit der Zunahme der Anzahl der Agenten.Frühere Arbeiten leihen verschiedene Multiagentenkoordinationsmechanismen in Deep-Learning-Architektur, um Multiagentenkoordination zu erleichtern.Jedoch keiner von ihnen explizit betrachten Aktion Semantik zwischen Agenten, dass verschiedene Aktionen haben unterschiedliche Einflüsse auf andere Agenten. In diesem Papier schlagen wir eine neuartige Netzwerkarchitektur vor, genannt Action Semantics Network (ASN), die explizit eine solche Handlungssemantik zwischen Agenten darstellt.ASN charakterisiert den Einfluss verschiedener Aktionen auf andere Agenten mit Hilfe von neuronalen Netzen, die auf der Handlungssemantik zwischen ihnen basieren.ASN kann leicht mit bestehenden Deep Reinforcement Learning (DRL) Algorithmen kombiniert werden, um deren Leistung zu steigern.Experimentelle Ergebnisse auf StarCraft II Mikromanagement und Neural MMO zeigen, dass ASN die Leistung von State-of-the-Art DRL-Ansätzen im Vergleich zu verschiedenen Netzwerkarchitekturen deutlich verbessert.
Spiking neuronale Netze werden sowohl als biologisch plausible Modelle für neuronale Berechnungen als auch als potenziell effizientere Art von neuronalen Netzen untersucht. Während Faltungs-Spiking neuronale Netze nachweislich eine Leistung nahe dem Stand der Technik erreichen, wurde bisher nur eine Lösung zur Umwandlung von gated recurrent neuronalen Netzen vorgeschlagen. Hier entwerfen wir eine analoge gated LSTM-Zelle, deren Neuronen durch effiziente stochastische Spiking-Neuronen ersetzt werden können. Diese adaptiven Spiking-Neuronen implementieren eine adaptive Form der Sigma-Delta-Codierung, um intern berechnete analoge Aktivierungswerte in Spike-Trains umzuwandeln. Wir zeigen, wie analoge Neuronen mit solchen Aktivierungsfunktionen verwendet werden können, um eine analoge LSTM-Zelle zu erstellen; Netzwerke dieser Zellen können dann mit Standard-Backpropagation trainiert werden. Wir trainieren diese LSTM-Netzwerke auf einer verrauschten und geräuschlosen Version der ursprünglichen Sequenzvorhersageaufgabe von Hochreiter & Schmidhuber (1997) und auch auf einer verrauschten und geräuschlosen Version einer klassischen Arbeitsgedächtnis-Verstärkungslernaufgabe, dem T-Maze.Wir ersetzen die analogen Neuronen durch entsprechende adaptive Spiking-Neuronen und zeigen dann, dass fast alle resultierenden Äquivalente von Spiking-Neuronen die ursprünglichen Aufgaben korrekt berechnen.
CNNs sind sehr erfolgreich bei der Erkennung menschlicher Handlungen in Videos, wenn auch mit einem hohen Rechenaufwand, der im Falle von weitreichenden Handlungen, bei denen sich ein Video im Durchschnitt über einige Minuten erstrecken kann, noch deutlich höher ist.Das Ziel dieser Arbeit ist es, den Rechenaufwand dieser CNNs zu reduzieren, ohne ihre Leistung zu beeinträchtigen. Wir schlagen VideoEpitoma vor, eine neuronale Netzwerkarchitektur, die aus zwei Modulen besteht: einem Zeitstempelselektor und einem Videoklassifikator. Bei einem Langzeitvideo mit Tausenden von Zeitschritten lernt der Selektor, nur einige wenige, aber repräsentative Zeitschritte für das Video auszuwählen. Dieser Selektor befindet sich auf einem leichtgewichtigen CNN wie MobileNet und verwendet ein neuartiges Gating-Modul, um eine binäre Entscheidung zu treffen: einen Videozeitschritt berücksichtigen oder verwerfen. Ein schwergewichtiges CNN-Modell wie I3D nimmt die ausgewählten Einzelbilder als Eingabe und führt eine Videoklassifizierung durch. Mit handelsüblichen Videoklassifizierern reduziert VideoEpitoma den Rechenaufwand um bis zu 50\%, ohne die Genauigkeit zu beeinträchtigen. Darüber hinaus zeigen wir, dass der Selektor, wenn er Ende-zu-Ende trainiert wird, lernt, bessere Entscheidungen zum Vorteil des Klassifikators zu treffen, obwohl der Selektor und der Klassifikator auf zwei verschiedenen CNNs basieren: Insbesondere erreichen wir die Genauigkeit von I3D, indem wir weniger als die Hälfte der Rechenzeit benötigen.
Eine Frage, die wir uns stellen, ist, ob man eine Fülle von unbeschrifteten Texten nutzen kann, um syntaktische Parser zu verbessern, und zwar über die bloße Verwendung der Texte hinaus, um verallgemeinerbare lexikalische Merkmale zu erhalten (d.h. über Worteinbettungen hinaus).Zu diesem Zweck schlagen wir ein neuartiges latent-variables generatives Modell für semi-supervised syntactic dependency parsing vor. Da eine exakte Inferenz nicht möglich ist, führen wir eine differenzierbare Entspannung ein, um ungefähre Stichproben zu erhalten und Gradienten in Bezug auf die Parser-Parameter zu berechnen. Unsere Methode (Differentiable Perturb-and-Parse) basiert auf differenzierbarer dynamischer Programmierung über stochastisch gestörte Kantenwerte. Wir demonstrieren die Wirksamkeit unseres Ansatzes mit Experimenten auf Englisch, Französisch und Schwedisch.
DeConvNet, Guided BackProp und LRP wurden erfunden, um tiefe neuronale Netze besser zu verstehen, und wir zeigen, dass diese Methoden nicht die theoretisch korrekte Erklärung für ein lineares Modell liefern, obwohl sie für mehrschichtige Netze mit Millionen von Parametern verwendet werden, was Anlass zur Sorge gibt, da lineare Modelle einfache neuronale Netze sind. Auf der Grundlage unserer Analyse linearer Modelle schlagen wir eine Verallgemeinerung vor, die zu zwei Erklärungstechniken (PatternNet und PatternAttribution) führt, die für lineare Modelle theoretisch fundiert sind und verbesserte Erklärungen für tiefe Netze liefern.
Graphische neuronale Netze haben vielversprechende Ergebnisse bei der Darstellung und Analyse verschiedener graphenstrukturierter Daten wie sozialer Netzwerke, Zitationsnetzwerke und Proteininteraktionsnetzwerke erbracht, wobei die bestehenden Ansätze in der Regel unter dem Problem der Überglättung leiden, unabhängig davon, ob die Strategien für die Aggregation von Nachbarschaften kanten- oder knotenbasiert sind. Die meisten Methoden konzentrieren sich auch auf transduktive Szenarien für feste Graphen, was zu einer schlechten Verallgemeinerungsleistung für ungesehene Graphen führt. Um diese Probleme anzugehen, schlagen wir ein neues neuronales Graphennetzwerkmodell vor, das sowohl kantenbasierte Nachbarschaftsbeziehungen als auch knotenbasierte Entitätsmerkmale berücksichtigt, d.h. Graph Entities with Step Mixture via random walk (GESM). GESM verwendet eine Mischung aus verschiedenen Schritten durch Random Walk, um das Oversmoothing-Problem zu mildern und die Knoteninformationen explizit zu verwenden.Diese beiden Mechanismen ermöglichen eine gewichtete Nachbarschaftsaggregation, die die Eigenschaften von Entitäten und Beziehungen berücksichtigt.Mit intensiven Experimenten zeigen wir, dass das vorgeschlagene GESM State-of-the-Art oder vergleichbare Leistungen auf vier Benchmark-Graph-Datensätzen erreicht, die transduktive und induktive Lernaufgaben umfassen.Darüber hinaus zeigen wir empirisch die Bedeutung der Berücksichtigung globaler Informationen.Der Quellcode wird in naher Zukunft öffentlich zugänglich sein.
Basis Pursuit ist eine Compressed-Sensing-Optimierung, bei der die l1-Norm vorbehaltlich von Modellfehlerbeschränkungen minimiert wird.Hier verwenden wir einen tiefen neuronalen Netzwerkprior anstelle der l1-Regularisierung.Unter Verwendung bekannter Rauschstatistiken lernen wir gemeinsam den Prior und rekonstruieren Bilder ohne Zugang zu Grundwahrheitsdaten.Während des Trainings verwenden wir alternierende Minimierung über ein unrolliertes iteratives Netzwerk und lösen gemeinsam die Gewichte des neuronalen Netzwerks und die Bildrekonstruktionen der Trainingsmenge. Wir vergleichen die Rekonstruktionsleistung zwischen unbeaufsichtigten und überwachten (d.h. mit Ground-Truth) Methoden und stellen die Hypothese auf, dass diese Technik zum Erlernen der Rekonstruktion verwendet werden kann, wenn keine Ground-Truth-Daten verfügbar sind, wie z.B. bei der hochauflösenden dynamischen MRT.
Deep-Learning-Ansätze erfordern in der Regel eine große Menge an markierten Daten zu verallgemeinern.Allerdings kann der Mensch ein neues Konzept nur durch ein paar Proben zu lernen.Eine der hohen cogntition menschlichen Fähigkeiten ist es, mehrere Konzepte zur gleichen Zeit zu lernen.In diesem Papier, adressieren wir die Aufgabe der Klassifizierung von mehreren Objekten, indem Sie nur ein paar Proben aus jeder Kategorie. Wir entwerfen eine Aufgabe zur Klassifizierung von mehreren Objekten mit wenigen Klassen und eine Umgebung zur einfachen Erstellung von kontrollierbaren Datensätzen für diese Aufgabe.Wir zeigen, dass der vorgeschlagene Datensatz solide ist, indem wir eine Methode verwenden, die eine Erweiterung von prototypischen Netzwerken ist.
Wir stellen einen neuen Ansatz zur Definition einer Sequenzverlustfunktion vor, um einen Summarizer zu trainieren, indem wir einen sekundären Kodierer-Dekodierer als Verlustfunktion verwenden und so ein Manko des Trainings auf Wortebene für Sequenzausgaben ausgleichen. Wir präsentieren experimentelle Ergebnisse, bei denen wir diese zusätzliche Verlustfunktion auf einen allgemeinen abstrakten Zusammenfassungsdatensatz für Nachrichten anwenden. Das Ergebnis ist eine Verbesserung der ROUGE-Metrik und eine besonders große Verbesserung bei menschlichen Bewertungen, was auf eine verbesserte Leistung hindeutet, die mit spezialisierten State-of-the-Art-Modellen konkurrenzfähig ist.
In dieser Arbeit schlagen wir ein neuartiges unbeaufsichtigtes Video-zu-Video-Übersetzungsmodell vor, das den Stil und den Inhalt zerlegt, eine spezielle Encoder-Decoder-Struktur verwendet und die Informationen zwischen den Frames durch bidirektionale rekurrente neuronale Netzwerke (RNN) weitergibt. Der Stil-Inhalts-Zerlegungsmechanismus ermöglicht es uns, langfristig stilkonsistente Videoübersetzungsergebnisse zu erzielen, und bietet uns eine gute Schnittstelle für eine modalitätsflexible Übersetzung. Darüber hinaus schlagen wir einen Video-Interpolationsverlust vor, der zeitliche Informationen innerhalb der Sequenz erfasst, um unsere Bausteine in einer selbstüberwachten Weise zu trainieren, indem wir die Eingabeframes und Stilcodes ändern, die in unsere Übersetzung integriert sind.Unser Modell kann fotorealistische, räumlich-zeitlich konsistente übersetzte Videos auf multimodale Weise produzieren.Subjektive und objektive experimentelle Ergebnisse bestätigen die Überlegenheit unseres Modells gegenüber den bestehenden Methoden.
Um ein transparentes System zu schaffen, muss ein Benutzer in der Lage sein, Erklärungen zu seinen Ergebnissen abzufragen.Wir argumentieren, dass ein Schlüsselprinzip hierfür die Verwendung von Kausalität innerhalb eines Planungsmodells ist, und dass Argumentationsrahmen eine intuitive Darstellung solcher Kausalität bieten.In diesem Beitrag diskutieren wir, wie Argumentation bei der Extraktion von Kausalitäten in Plänen und Modellen helfen kann, und wie sie Erklärungen aus ihnen erstellen können.
Wir schlagen eine neuartige Deep-Learning-Methode vor, um stabile und zeitlich kohärente Merkmalsräume für Punktwolken zu lernen, die sich im Laufe der Zeit verändern. Wir identifizieren eine Reihe inhärenter Probleme mit diesen Ansätzen: Ohne Kenntnis der zeitlichen Dimension können die abgeleiteten Lösungen starkes Flackern aufweisen, und einfache Lösungen zur Unterdrückung dieses Flackerns können zu unerwünschten lokalen Minima führen, die sich als Halo-Strukturen manifestieren.Wir schlagen eine neuartige zeitliche Verlustfunktion vor, die höhere zeitliche Ableitungen der Punktpositionen berücksichtigt und die Vermischung fördert, d.h., Wir kombinieren diese Techniken in einer Super-Resolution-Methode mit einem Trunkierungsansatz, um die Größe der generierten Positionen flexibel anzupassen, und zeigen, dass unsere Methode für große, deformierende Punktmengen aus verschiedenen Quellen funktioniert, um die Flexibilität unseres Ansatzes zu demonstrieren.
Wir untersuchen das Problem der Generierung von Gegenbeispielen in einer Blackbox-Umgebung, in der nur ein verlustbehafteter Zugang zu einem Modell verfügbar ist, und stellen einen Rahmen vor, der einen Großteil der bestehenden Arbeiten zu Blackbox-Angriffen konzeptionell vereinheitlicht, und zeigen, dass die aktuellen State-of-the-Art-Methoden in einem natürlichen Sinne optimal sind.Trotz dieser Optimalität zeigen wir, wie man Blackbox-Angriffe verbessern kann, indem wir ein neues Element in das Problem einbringen: Gradientenprioritäten. Wir geben einen auf Bandit-Optimierung basierenden Algorithmus an, der es uns ermöglicht, solche Prioren nahtlos zu integrieren, und wir identifizieren und integrieren explizit zwei Beispiele.die resultierenden Methoden verwenden zwei- bis viermal weniger Abfragen und scheitern zwei- bis fünfmal weniger als der aktuelle Stand der Technik.der Code zur Reproduktion unserer Arbeit ist unter https://git.io/fAjOJ verfügbar.
Das Ziel der vorliegenden Arbeit ist es, die Effizienz großer neuronaler Netze, die auf Audiodaten arbeiten, durch Multitasking-Lernen mit selbstüberwachten Aufgaben auf unmarkierten Daten zu verbessern.Zu diesem Zweck haben wir einen End-to-End-Audio-Feature-Extraktor auf der Grundlage von WaveNet trainiert, der in einfache, aber vielseitige aufgabenspezifische neuronale Netze einfließt.Wir beschreiben drei selbstüberwachte Lernaufgaben, die auf jedem großen, unmarkierten Audiokorpus arbeiten können. Wir zeigen, dass man in einem Szenario mit begrenzten Trainingsdaten die Leistung einer überwachten Klassifizierungsaufgabe erheblich verbessern kann, indem man sie gleichzeitig mit diesen zusätzlichen selbstüberwachten Aufgaben trainiert.Wir zeigen, dass man die Leistung bei einer Klassifizierungsaufgabe für verschiedene Schallereignisse um fast 6\% verbessern kann, wenn man sie gemeinsam mit bis zu drei verschiedenen selbstüberwachten Aufgaben trainiert.Diese Verbesserung skaliert mit der Anzahl der zusätzlichen Hilfsaufgaben sowie der Menge der unbeaufsichtigten Daten.Wir zeigen auch, dass die Einbeziehung der Datenerweiterung in unsere Multitask-Einstellung zu noch weiteren Leistungssteigerungen führt.
Der Datensatz besteht aus 16.441 Kochrezepten mit 160.479 Fotos, die mit verschiedenen Schritten assoziiert sind.Wir stellen eine Baseline auf, die durch das leistungsstärkste Modell in Bezug auf die menschliche Bewertung für die Aufgabe Visual Story Telling (ViST) motiviert ist.Darüber hinaus führen wir zwei Modelle ein, um die von einer Finite State Machine (FSM) gelernte Struktur auf hoher Ebene in den neuronalen sequenziellen Generierungsprozess einzubinden: (1) Scaffolding Structure in Decoder (SSiD) (2) Scaffolding Structure in Loss (SSiL).Diese Modelle zeigen eine Verbesserung sowohl in der empirischen als auch in der menschlichen Bewertung.Unser leistungsstärkstes Modell (SSiL) erreicht einen METEOR-Score von 0,31, was eine Verbesserung von 0,6 gegenüber dem Basismodell darstellt. Wir haben auch eine menschliche Bewertung der generierten geerdeten Rezepte durchgeführt, die zeigt, dass 61% fanden, dass unser vorgeschlagenes (SSiL) Modell besser ist als das Basismodell in Bezug auf die Gesamtrezepte, und 72,5% bevorzugten unser Modell in Bezug auf Kohärenz und Struktur.Wir diskutieren auch die Analyse der Ausgabe, die wichtige NLP-Themen für zukünftige Richtungen hervorhebt.
Implizite Modelle, die zwar die Generierung von Stichproben, nicht aber die punktuelle Bewertung von Wahrscheinlichkeiten erlauben, sind in realen Problemen, die durch maschinelles Lernen gelöst werden, allgegenwärtig und ein aktuelles Forschungsthema. Einige Beispiele sind Datensimulatoren, die in der technischen und wissenschaftlichen Forschung weit verbreitet sind, generative adversarische Netzwerke (GANs) für die Bildsynthese und brandaktuelle approximative Inferenztechniken, die auf impliziten Verteilungen beruhen. Die meisten bestehenden Ansätze zum Erlernen impliziter Modelle beruhen auf der Annäherung der schwer zu fassenden Verteilung oder des Optimierungsziels für die gradientenbasierte Optimierung, was zu ungenauen Aktualisierungen und damit zu schlechten Modellen führen kann. In diesem Papier wird die Notwendigkeit solcher Annäherungen durch den Vorschlag des \emph{Stein-Gradientenschätzers} verringert, der die Bewertungsfunktion der implizit definierten Verteilung direkt schätzt.
In dieser Studie werden die Auswirkungen des Hinzufügens oder der Anwendung verschiedener Rauschmodelle unterschiedlicher Größe auf Architekturen von Convolutional Neural Networks (CNN) analysiert. Die grundlegenden Ergebnisse stimmen mit den meisten gängigen Begriffen im Bereich des maschinellen Lernens überein, und es werden auch einige neuartige Heuristiken und Empfehlungen zur Rauschinjektion vorgestellt.
State-of-the-Art Unsupervised Domain Adaptation (UDA)-Methoden lernen übertragbare Merkmale, indem sie die Diskrepanz in der Merkmalsverteilung zwischen der Quell- und der Zieldomäne minimieren.Anders als diese Methoden, die die Merkmalsverteilungen nicht explizit modellieren, untersuchen wir in diesem Papier die explizite Modellierung der Merkmalsverteilung für UDA. Insbesondere schlagen wir das Distribution Matching Prototypical Network (DMPN) vor, um die tiefen Merkmale aus jeder Domäne als Gaußsche Mischverteilungen zu modellieren. Mit der expliziten Modellierung der Merkmalsverteilung können wir die Diskrepanz zwischen den beiden Domänen leicht messen. Die erste minimiert die Abstände zwischen den entsprechenden Gauß-Komponenten-Mittelwerten der Quell- und Zieldaten, die zweite minimiert die pseudo-negative logische Wahrscheinlichkeit der Erzeugung der Zielmerkmale aus der Quell-Merkmalsverteilung.Um sowohl diskriminative als auch domäneninvariante Merkmale zu erlernen, wird DMPN trainiert, indem der Klassifikationsverlust auf den gelabelten Quelldaten und die Domänendiskrepanzverluste zusammen minimiert werden. Umfangreiche Experimente werden über zwei UDA tasks.Our Ansatz liefert eine große Marge in der Digits Image Transfer Aufgabe über state-of-the-art approaches.More bemerkenswert, DMPN erreicht eine mittlere Genauigkeit von 81,4% auf VisDA 2017 dataset.The Hyper-Parameter Sensitivitätsanalyse zeigt, dass unser Ansatz ist robust w.r.t Hyper-Parameter Änderungen.
Effizientes Lernen, um Aufgaben in komplexen Umgebungen zu lösen, ist eine zentrale Herausforderung für Reinforcement Learning (RL) Agenten. Wir schlagen vor, eine komplexe Umgebung mit Hilfe eines aufgabenagnostischen Weltgraphen zu zerlegen, eine Abstraktion, die das Lernen beschleunigt, indem sie es den Agenten ermöglicht, sich auf die Erkundung eines Unterraums der Umgebung zu konzentrieren.  Unser Rahmenwerk hat zwei Lernphasen: 1) Identifizierung von Knoten und Kanten des Weltgraphen durch Training eines binären rekurrenten Variations-Autocodierers (VAE) auf Trajektoriendaten und2) ein hierarchisches RL-Framework, das Struktur- und Konnektivitätswissen aus dem gelernten Weltgraphen nutzt, um die Erkundung auf aufgabenrelevante Wegpunkte und Regionen zu fokussieren. Wir zeigen, dass unser Ansatz RL bei einer Reihe von anspruchsvollen 2D-Grid-World-Aufgaben signifikant beschleunigt: Im Vergleich zu Baselines verdoppelt die Integration des Weltgraphen die erzielten Belohnungen bei einfacheren Aufgaben, z.B. MultiGoal, und schafft es, anspruchsvollere Aufgaben, z.B. Door-Key, zu lösen, bei denen Baselines versagen.
Wir führen den Begriff der Eigenschaftssignaturen ein, eine Repräsentation für Programme und Programmspezifikationen, die für den Gebrauch durch Algorithmen des maschinellen Lernens gedacht ist.Eine Funktion mit dem Eingabetyp τ_in und dem Ausgabetyp τ_out gegeben, ist eine Eigenschaft eine Funktion vom Typ: (τ_in, τ_out) → Bool, die (informell) eine einfache Eigenschaft der betrachteten Funktion beschreibt.Wenn τ_in und τ_out beispielsweise beide Listen desselben Typs sind, könnte eine Eigenschaft die Frage stellen: "Ist die Eingabeliste genauso lang wie die Ausgabeliste?".Wenn wir eine Liste solcher Eigenschaften haben, können wir sie alle für unsere Funktion auswerten, um eine Liste von Ausgaben zu erhalten, die wir die Eigenschaftssignatur nennen. Wir erörtern mehrere mögliche Anwendungen von Eigenschaftssignaturen und zeigen experimentell, dass sie verwendet werden können, um einen Basis-Synthesizer so zu verbessern, dass er doppelt so viele Programme in weniger als einem Zehntel der Zeit erzeugt.
Die Entwicklung künstlich intelligenter Agenten, die in solchen Situationen gute Ergebnisse erzielen, ist wichtig, da viele Interaktionen in der realen Welt ein Spannungsverhältnis zwischen egoistischen Interessen und dem Wohlergehen anderer beinhalten. Wir zeigen, wie moderne Methoden des Verstärkungslernens modifiziert werden können, um Agenten zu konstruieren, die einfach zu verstehen, nett (sie beginnen mit der Kooperation), provozierbar (sie versuchen zu vermeiden, dass sie ausgenutzt werden) und vergebend (sie versuchen, zur gegenseitigen Kooperation zurückzukehren) sind. Wir zeigen sowohl theoretisch als auch experimentell, dass solche Agenten die Kooperation in sozialen Markov-Dilemmas aufrechterhalten können. Unsere Konstruktion erfordert keine Trainingsmethoden, die über eine Modifikation des Selbstspiels hinausgehen; wenn also eine Umgebung so beschaffen ist, dass gute Strategien im Nullsummenfall konstruiert werden können (z.B. Atari), dann können wir Agenten konstruieren, die soziale Dilemmas in dieser Umgebung lösen.
Der Reparametrisierungstrick ist zu einem der nützlichsten Werkzeuge im Bereich der Variationsinferenz geworden, basiert jedoch auf der Standardisierungstransformation, die den Anwendungsbereich dieser Methode auf Verteilungen beschränkt, die verfolgbare inverse kumulative Verteilungsfunktionen haben oder als deterministische Transformationen solcher Verteilungen ausgedrückt werden können.In diesem Papier haben wir den Reparametrisierungstrick verallgemeinert, indem wir eine allgemeine Transformation zulassen.Im Gegensatz zu anderen ähnlichen Arbeiten entwickeln wir das verallgemeinerte transformationsbasierte Gradientenmodell formal und rigoros. Im Gegensatz zu anderen ähnlichen Arbeiten entwickeln wir das verallgemeinerte transformationsbasierte Gradientenmodell formal und rigoros. Wir stellen fest, dass das vorgeschlagene Modell ein Spezialfall der Kontrollvariante ist, was darauf hindeutet, dass das vorgeschlagene Modell die Vorteile der CV und der verallgemeinerten Reparametrisierung kombinieren kann.Auf der Grundlage des vorgeschlagenen Gradientenmodells schlagen wir einen neuen polynomialbasierten Gradientenschätzer vor, der unter bestimmten Bedingungen eine bessere theoretische Leistung als der Reparametrisierungstrick aufweist und auf eine größere Klasse von Variationsverteilungen angewendet werden kann.In Studien mit synthetischen und realen Daten zeigen wir, dass der von uns vorgeschlagene Gradientenschätzer eine signifikant niedrigere Gradientenvarianz als andere State-of-the-Art-Methoden aufweist und somit ein schnelleres Inferenzverfahren ermöglicht.
Die Fehlerdiagnose in einem modernen Kommunikationssystem gilt traditionell als schwierig oder sogar unpraktisch für einen rein datengesteuerten Ansatz des maschinellen Lernens, da es sich um ein von Menschenhand geschaffenes System mit intensivem Wissen handelt.Einige wenige beschriftete rohe Paketströme, die aus einem Fehlerarchiv extrahiert wurden, können kaum ausreichen, um die komplizierte Logik der zugrunde liegenden Protokolle abzuleiten.In diesem Papier ergänzen wir diese begrenzten Stichproben mit zwei unerschöpflichen Datenquellen: die unbeschrifteten Datensätze, die von einem in Betrieb befindlichen System abgetastet wurden, und die beschrifteten Daten, die in einer Emulationsumgebung simuliert wurden. Um ihr inhärentes Wissen auf die Zieldomäne zu übertragen, konstruieren wir einen gerichteten Informationsflussgraphen, dessen Knoten neuronale Netzkomponenten sind, die aus zwei Generatoren, drei Diskriminatoren und einem Klassifikator bestehen, und dessen jeder Vorwärtspfad ein Paar von gegnerischen Optimierungszielen darstellt, in Übereinstimmung mit den Anforderungen des semi-supervised und transfer learning. Das mehrköpfige Netzwerk kann in einem alternativen Ansatz trainiert werden, bei dem wir bei jeder Iteration ein Ziel auswählen, um die Gewichte entlang des vorgelagerten Pfades zu aktualisieren, und das Residuum schichtweise auf alle nachgelagerten Ausgänge auffrischen.Die tatsächlichen Ergebnisse zeigen, dass es eine vergleichbare Genauigkeit bei der Klassifizierung von Transmission Control Protocol (TCP)-Streams ohne absichtliche Expertenmerkmale erreichen kann.Die Lösung hat Betriebsingenieure von massiven Arbeiten zum Verstehen und Pflegen von Regeln entlastet und eine schnelle Lösung unabhängig von spezifischen Protokollen bereitgestellt.
Unsere Arbeit befasst sich mit zwei wichtigen Problemen bei rekurrenten neuronalen Netzen: (1) sie sind überparametrisiert, und (2) die rekurrente Gewichtsmatrix ist schlecht konditioniert.Ersteres erhöht die Komplexität des Lernens und die Trainingszeit.Letzteres verursacht das Problem des verschwindenden und explodierenden Gradienten.Wir stellen ein flexibles rekurrentes neuronales Netzmodell vor, das Kronecker Recurrent Units (KRU) genannt wird.KRU erreicht Parametereffizienz in RNNs durch eine Kronecker-faktorisierte rekurrente Matrix.Es überwindet die schlechte Konditionierung der rekurrenten Matrix, indem es weiche unitäre Beschränkungen für die Faktoren erzwingt. Unsere experimentellen Ergebnisse an sieben Standarddatensätzen zeigen, dass KRU die Anzahl der Parameter in der rekurrenten Gewichtsmatrix im Vergleich zu den bestehenden rekurrenten Modellen um drei Größenordnungen reduzieren kann, ohne dass die statistische Leistung darunter leidet, und dass es zwar Vorteile hat, einen hochdimensionalen rekurrenten Raum zu haben, die Kapazität des rekurrenten Teils des Modells aber drastisch reduziert werden kann.
Dieses Papier untersucht das unerwünschte Phänomen der Überempfindlichkeit von Repräsentationen, die von tiefen Netzen auf semantisch irrelevante Änderungen in Daten gelernt werden, und identifiziert eine Ursache für diesen Mangel im klassischen Variational Auto-Encoder (VAE) Ziel, die Evidenzuntergrenze (ELBO). Wir zeigen, dass die ELBO nicht in der Lage ist, das Verhalten des Encoders außerhalb der Unterstützung der empirischen Datenverteilung zu kontrollieren, und dass dieses Verhalten der VAE zu extremen Fehlern in der gelernten Repräsentation führen kann - ein zentrales Hindernis für die effektive Nutzung von Repräsentationen für dateneffizientes Lernen und Transfer. Um diese Spezifikationen einzubeziehen, schlagen wir eine Regularisierungsmethode vor, die auf einem Selektionsmechanismus basiert, der einen fiktiven Datenpunkt durch explizite Störung eines beobachteten wahren Datenpunktes erzeugt.Für bestimmte Entscheidungen von Parametern führt unsere Formulierung natürlich zur Minimierung der entropie regularisierten Wasserstein-Distanz zwischen Repräsentationen. Wir illustrieren unseren Ansatz an Standarddatensätzen und zeigen experimentell, dass signifikante Verbesserungen in der nachgelagerten adversen Genauigkeit durch das Erlernen von robusten Repräsentationen in einer unbeaufsichtigten Art und Weise erreicht werden können, ohne einen Bezug zu einer bestimmten nachgelagerten Aufgabe und ohne ein kostspieliges überwachtes adverses Trainingsverfahren. 
Wir schlagen einen neuartigen Score-basierten Ansatz zum Erlernen eines gerichteten azyklischen Graphen (DAG) aus Beobachtungsdaten vor und passen eine kürzlich vorgeschlagene kontinuierliche eingeschränkte Optimierungsformulierung an, um nichtlineare Beziehungen zwischen Variablen unter Verwendung neuronaler Netze zu berücksichtigen Diese Erweiterung ermöglicht die Modellierung komplexer Interaktionen und ist gleichzeitig globaler in der Suche im Vergleich zu anderen Greedy-Ansätzen. Zusätzlich zum Vergleich unserer Methode mit bestehenden kontinuierlichen Optimierungsmethoden bieten wir fehlende empirische Vergleiche mit nichtlinearen Greedy-Suchmethoden: Sowohl auf synthetischen als auch auf realen Datensätzen übertrifft diese neue Methode aktuelle kontinuierliche Methoden bei den meisten Aufgaben und ist gleichzeitig konkurrenzfähig mit bestehenden Greedy-Suchmethoden bei wichtigen Metriken für kausale Inferenz.
Wir untersuchen das Problem des Entwurfs nachweislich optimaler Algorithmen für Störgeräusche, die eine Fehlklassifizierung in Situationen hervorrufen, in denen ein Lerner Entscheidungen von mehreren Klassifikatoren aggregiert.Angesichts der nachgewiesenen Anfälligkeit von State-of-the-Art-Modellen für Störbeispiele haben sich die jüngsten Bemühungen im Bereich des robusten maschinellen Lernens auf die Verwendung von Ensemble-Klassifikatoren konzentriert, um die Robustheit einzelner Modelle zu erhöhen. Wir zeigen, wie dieses Problem als Gleichgewichtsstrategie in einem Nullsummenspiel mit zwei Spielern zwischen einem Lernenden und einem Gegner formuliert werden kann, und veranschaulichen folglich die Notwendigkeit der Randomisierung bei gegnerischen Angriffen. Die wichtigste technische Herausforderung, die wir betrachten, ist der Entwurf von Best-Response-Orakeln, die in einem Multiplicative-Weight-Updates-Rahmen implementiert werden können, um Gleichgewichtsstrategien im Nullsummenspiel zu finden.Wir entwickeln eine Reihe skalierbarer Algorithmen zur Rauscherzeugung für tiefe neuronale Netze und zeigen, dass sie die modernsten Angriffe auf verschiedene Bildklassifikationsaufgaben übertreffen. Obwohl es im Allgemeinen keine Garantien für tiefes Lernen gibt, zeigen wir, dass es sich um einen prinzipientreuen Ansatz handelt, der nachweislich optimal für lineare Klassifizierer ist. Die wichtigste Erkenntnis ist eine geometrische Charakterisierung des Entscheidungsraums, die das Problem des Entwurfs der besten Antwortorakel auf die Minimierung einer quadratischen Funktion über einen Satz konvexer Polytope reduziert.
Multiagentensysteme, bei denen die Agenten untereinander und mit einer stochastischen Umgebung interagieren, können als stochastische Spiele formalisiert werden. Wir untersuchen eine Unterklasse dieser Spiele, die sogenannten Markov-Potential-Spiele (MPGs), die häufig in wirtschaftlichen und technischen Anwendungen vorkommen, wenn die Agenten eine gemeinsame Ressource teilen.Wir betrachten MPGs mit kontinuierlichen Zustands-Aktions-Variablen, gekoppelten Einschränkungen und nicht-konvexen Belohnungen. Frühere Analysen folgten einem Variationsansatz, der nur für sehr einfache Fälle gilt (konvexe Belohnungen, invertierbare Dynamik und keine gekoppelten Beschränkungen); oder sie betrachteten eine deterministische Dynamik und lieferten eine Open-Loop (OL)-Analyse, die Strategien untersuchte, die aus vordefinierten Handlungssequenzen bestehen, die für stochastische Umgebungen nicht optimal sind. Wir stellen eine Closed-Loop (CL) Analyse für MPGs vor und betrachten parametrische Strategien, die vom aktuellen Zustand abhängen und bei denen sich Agenten an stochastische Übergänge anpassen, Wir zeigen, dass ein geschlossenes Nash-Gleichgewicht (NE) durch die Lösung eines zugehörigen optimalen Kontrollproblems (OCP) gefunden (oder zumindest angenähert) werden kann. Dies ist nützlich, da die Lösung eines OCP - das ein Ein-Ziel-Problem ist - in der Regel viel einfacher ist als die Lösung des ursprünglichen Satzes gekoppelter OCPs, die das Spiel bilden - das ein Mehr-Ziel-Kontrollproblem ist. Dies ist eine beträchtliche Verbesserung gegenüber dem bisherigen Standardansatz für die CL-Analyse von MPGs, der keine ungefähre Lösung liefert, wenn keine NE zu der gewählten parametrischen Familie gehört, und der nur für einfache parametrische Formen praktikabel ist.Wir veranschaulichen die theoretischen Beiträge mit einem Beispiel, indem wir unseren Ansatz auf ein nicht-kooperatives Spiel der Kommunikationstechnik anwenden.Wir lösen das Spiel dann mit einem Deep Reinforcement Learning-Algorithmus, der Strategien lernt, die sich einer exakten Variations-NE des Spiels stark annähern.
Wir machen die folgende bemerkenswerte Beobachtung: Vollständig gefaltete VAE-Modelle, die auf 32x32 ImageNet trainiert wurden, lassen sich nicht nur auf 64x64, sondern auch auf weitaus größere Fotos verallgemeinern, ohne dass das Modell verändert werden muss. Wir nutzen diese Eigenschaft, indem wir voll gefaltete Modelle auf verlustfreie Kompression anwenden, eine Methode zur Skalierung des VAE-basierten "Bits-Back mit ANS"-Algorithmus für verlustfreie Kompression auf große Farbfotografien demonstrieren und den Stand der Technik für die Kompression von ImageNet-Bildern in voller Größe erreichen.Wir veröffentlichen Craystack, eine Open-Source-Bibliothek für das bequeme Prototyping von verlustfreier Kompression unter Verwendung probabilistischer Modelle, zusammen mit vollständigen Implementierungen aller unserer Kompressionsergebnisse.
    Mit anderen Worten, die meisten Bilder in der Datenverteilung werden sowohl vom Modell korrekt klassifiziert als auch liegen sehr nahe an einem visuell ähnlichen, falsch klassifizierten Bild.Trotz erheblichem Forschungsinteresse ist die Ursache dieses Phänomens immer noch schlecht verstanden und bleibt ungelöst. Wir stellen die Hypothese auf, dass dieses kontraintuitive Verhalten ein natürliches Ergebnis der hochdimensionalen Geometrie der Datenvielfalt ist, und untersuchen in einem ersten Schritt einen einfachen synthetischen Datensatz zur Klassifizierung zwischen zwei konzentrischen hochdimensionalen Kugeln. Für diesen Datensatz zeigen wir einen grundlegenden Kompromiss zwischen der Höhe des Testfehlers und dem durchschnittlichen Abstand zum nächsten Fehler, insbesondere beweisen wir, dass jedes Modell, das einen kleinen konstanten Bruchteil einer Kugel falsch klassifiziert, anfällig für Störungen der Größe $O(1/\sqrt{d})$ ist.Überraschenderweise nähern sich alle Fehlersätze dieser theoretischen Grenze, wenn wir verschiedene Architekturen auf diesem Datensatz trainieren. Wir hoffen, dass unsere theoretische Analyse dieses sehr einfachen Falles den Weg weist, um zu erforschen, wie die Geometrie komplexer realer Datensätze zu nachteiligen Beispielen führt.
Es wurde festgestellt, dass verschiedene Verhaltensweisen, die den kontrollierbaren Unterraum eines Markov-Entscheidungsprozesses überspannen, trainiert werden können, indem eine Politik dafür belohnt wird, dass sie sich von anderen Politiken unterscheidet. Eine Einschränkung dieser Formulierung ist jedoch die Schwierigkeit, über die endliche Menge der explizit gelernten Verhaltensweisen hinaus zu verallgemeinern, wie es in nachfolgenden Aufgaben erforderlich sein kann. Um dies zu tun, stellen wir Variational Intrinsic Successor FeatuRes (VISR) vor, einen neuartigen Algorithmus, der kontrollierbare Merkmale erlernt, die genutzt werden können, um eine verbesserte Generalisierung und schnelle Aufgabeninferenz durch das Nachfolgemerkmal-Framework zu ermöglichen.Wir validieren VISR empirisch auf der gesamten Atari-Suite in einem neuartigen Setup, in dem die Belohnungen nur kurz nach einer langen unbeaufsichtigten Phase offengelegt werden.VISR erreicht Leistung auf menschlichem Niveau bei 12 Spielen und übertrifft alle Baselines, und wir glauben, dass es einen Schritt in Richtung Agenten darstellt, die schnell aus begrenztem Feedback lernen.
Die Vorhersage strukturierter Ergebnisse, wie z.B. semantische Segmentierung, stützt sich auf teure Pixel-Annotationen, um starke überwachte Modelle wie Faltungsneuronale Netze zu lernen. Um den arbeitsintensiven Prozess der Annotation zu vermeiden, entwickeln wir eine Domänenanpassungsmethode, um die Quelldaten an die nicht beschriftete Zieldomäne anzupassen. zu diesem Zweck schlagen wir vor, diskriminative Merkmalsrepräsentationen von Patches auf der Grundlage von Beschriftungshistogrammen in der Quelldomäne durch die Konstruktion eines entflochtenen Raums zu erlernen. Darüber hinaus zeigen wir, dass unser Rahmenwerk einen globalen Ausrichtungsprozess mit der vorgeschlagenen Ausrichtung auf Patch-Ebene integrieren kann und eine State-of-the-Art-Leistung bei der semantischen Segmentierung erreicht.Umfangreiche Ablationsstudien und -experimente werden an zahlreichen Benchmark-Datensätzen mit verschiedenen Einstellungen durchgeführt, wie z. B. synthetisch-reale und städteübergreifende Szenarien.
Bisher war unklar, wie groß das Risiko für die Sicherheit von Anwendungen des maschinellen Lernens ist, da die meisten Methoden zur Erzeugung solcher Störungen entweder auf detaillierten Modellinformationen (gradientenbasierte Angriffe) oder auf Konfidenzwerten wie Klassenwahrscheinlichkeiten (scorebasierte Angriffe) beruhen, die in den meisten realen Szenarien nicht verfügbar sind. Hier betonen wir die Bedeutung von Angriffen, die sich ausschließlich auf die endgültige Modellentscheidung stützen: Solche entscheidungsbasierten Angriffe sind (1) auf reale Black-Box-Modelle wie autonome Autos anwendbar, (2) benötigen weniger Wissen und sind einfacher anzuwenden als transferbasierte Angriffe und (3) sind robuster gegenüber einfachen Verteidigungsmaßnahmen als gradienten- oder scorebasierte Angriffe. Bisherige Angriffe in dieser Kategorie waren auf einfache Modelle oder einfache Datensätze beschränkt: Hier stellen wir den Boundary-Angriff vor, einen entscheidungsbasierten Angriff, der von einer großen gegnerischen Störung ausgeht und dann versucht, die Störung zu reduzieren, ohne die gegnerische Seite zu verändern. Wir wenden den Angriff auf zwei Black-Box-Algorithmen von Clarifai.com an. Der Boundary-Angriff im Besonderen und die Klasse der entscheidungsbasierten Angriffe im Allgemeinen eröffnen neue Wege zur Untersuchung der Robustheit von Machine-Learning-Modellen und werfen neue Fragen zur Sicherheit von eingesetzten Machine-Learning-Systemen auf. Eine Implementierung des Angriffs ist als Teil von Foolbox (https://github.com/bethgelab/foolbox) verfügbar.
Wir zeigen, dass es möglich ist, große rekurrente Sprachmodelle mit differenziellen Datenschutzgarantien auf Benutzerebene zu trainieren, wobei die Kosten für die Vorhersagegenauigkeit vernachlässigbar sind.  Unsere Arbeit baut auf den jüngsten Fortschritten beim Training von tiefen Netzwerken auf benutzerpartitionierten Daten und der Berücksichtigung der Privatsphäre beim stochastischen Gradientenabstieg auf. Insbesondere fügen wir den Schutz der Privatsphäre auf Benutzerebene zum Algorithmus der föderierten Mittelwertbildung hinzu, der große Schrittaktualisierungen von Daten auf Benutzerebene vornimmt. Unsere Arbeit zeigt, dass bei einem Datensatz mit einer ausreichend großen Anzahl von Nutzern (eine Anforderung, die selbst von kleinen Internet-Datensätzen leicht erfüllt werden kann), das Erreichen eines differenzierten Datenschutzes mit einem erhöhten Rechenaufwand verbunden ist und nicht mit einem verringerten Nutzen, wie in den meisten früheren Arbeiten.Wir stellen fest, dass unsere privaten LSTM-Sprachmodelle quantitativ und qualitativ ähnlich sind wie Modelle ohne Rauschen, wenn sie auf einem großen Datensatz trainiert werden.
Convolutional Neural Networks (CNNs) werden üblicherweise mit einer festen räumlichen Bildgröße trainiert, die für ein bestimmtes Modell vorgegeben ist. Obwohl sie auf Bildern einer bestimmten Größe trainiert werden, ist es gut etabliert, dass CNNs verwendet werden können, um eine breite Palette von Bildgrößen zur Testzeit auszuwerten, indem die Größe der dazwischenliegenden Feature Maps angepasst wird. Wir zeigen, dass Modelle, die mit unserer Methode trainiert werden, widerstandsfähiger gegen Änderungen der Bildgröße sind und auch auf kleinen Bildern gut generalisieren, was eine schnellere Inferenz durch die Verwendung kleinerer Bilder zur Testzeit ermöglicht. 76. Darüber hinaus zeigen wir, dass diese Methode für eine gegebene Bildgröße, die zur Testzeit verwendet wird, genutzt werden kann, um entweder das Training oder die endgültige Testgenauigkeit zu beschleunigen. z.B. sind wir in der Lage, eine Genauigkeit von 79,27% mit einem Modell zu erreichen, das bei einer räumlichen Größe von 288 ausgewertet wurde, was einer relativen Verbesserung von 14% gegenüber dem Basismodell entspricht.
Wir schlagen eine einfache Technik vor, um generative RNNs zu ermutigen, vorauszuplanen: Wir trainieren ein "rückwärts" rekurrentes Netzwerk, um eine gegebene Sequenz in umgekehrter Reihenfolge zu generieren, und wir ermutigen die Zustände des vorwärts gerichteten Modells, zeitgleiche Zustände des rückwärts gerichteten Modells vorherzusagen. Wir stellen die Hypothese auf, dass unser Ansatz die Modellierung langfristiger Abhängigkeiten erleichtert, indem er die Vorwärtszustände implizit dazu zwingt, Informationen über die längerfristige Zukunft zu enthalten (wie sie in den Rückwärtszuständen enthalten sind). wir zeigen empirisch, dass unser Ansatz eine relative Verbesserung von 9% bei einer Spracherkennungsaufgabe und eine signifikante Verbesserung bei einer COCO-Titelgenerierungsaufgabe erreicht.
Tiefe generative Modelle versuchen, den Prozess, mit dem die beobachteten Daten erzeugt wurden, wiederherzustellen. Sie können verwendet werden, um neue Proben zu synthetisieren oder nachträglich Repräsentationen zu extrahieren.Erfolgreiche Ansätze in der Domäne der Bilder werden durch mehrere Kern induktiven Verzerrungen angetrieben.Allerdings wurde eine Verzerrung für die kompositorische Art und Weise, in der Menschen eine visuelle Szene in Bezug auf Objekte strukturieren, häufig übersehen.In dieser Arbeit schlagen wir vor, den Generator eines GAN zu strukturieren, um Objekte und ihre Beziehungen explizit zu berücksichtigen und Bilder durch Komposition zu erzeugen. Wir evaluieren unseren Ansatz an mehreren Multi-Objekt-Bilddatensätzen und stellen fest, dass der Generator lernt, Informationen, die verschiedenen Objekten entsprechen, auf einer Repräsentationsebene zu identifizieren und zu entflechten.Eine Humanstudie zeigt, dass das resultierende generative Modell besser in der Lage ist, Bilder zu erzeugen, die der Referenzverteilung treuer sind.
Wir führen ein neues Sender-Empfänger-Spiel ein, um emergente Kommunikation für dieses Spektrum von teilweise konkurrierenden Szenarien zu untersuchen, und widmen der Bewertung besondere Aufmerksamkeit: Wir stellen fest, dass Kommunikation in teilweise konkurrierenden Szenarien tatsächlich entstehen kann, und wir entdecken drei Dinge, die zu ihrer Verbesserung beitragen. Erstens, dass egoistische Kommunikation proportional zur Kooperation ist und natürlich in Situationen auftritt, die eher kooperativ als wettbewerbsorientiert sind.Zweitens, dass Stabilität und Leistung durch die Verwendung von LOLA (Foerster et al, 2018) verbessert werden, insbesondere in wettbewerbsorientierten Szenarien.Und drittens, dass sich diskrete Protokolle besser zum Erlernen kooperativer Kommunikation eignen als kontinuierliche.
Tiefe neuronale Netze (DNNs) haben in der Regel genug Kapazität, um zufällige Daten mit roher Gewalt anzupassen, auch wenn konventionelle datenabhängige Regularisierungen, die sich auf die Geometrie der Merkmale konzentrieren, auferlegt werden.Wir finden heraus, dass der Grund dafür die Inkonsistenz zwischen der erzwungenen Geometrie und dem Standard-Softmax-Kreuzentropieverlust ist. Um dieses Problem zu lösen, schlagen wir einen neuen Rahmen für die datenabhängige DNN-Regularisierung vor, das Geometrisch-Regularisierte-Selbst-Validierende Neuronale Netz (GRSVNet), bei dem während des Trainings die für einen Stapel von Merkmalen erzwungene Geometrie gleichzeitig für einen separaten Stapel unter Verwendung eines Validierungsverlustes validiert wird, der mit der Geometrie übereinstimmt. Wir untersuchen einen besonderen Fall von GRSVNet, das Orthogonal-Low-rank Embedding (OLE)-GRSVNet, das in der Lage ist, hochdiskriminierende Merkmale zu erzeugen, die in orthogonalen Low-Rank-Unterräumen liegen.Numerische Experimente zeigen, dass OLE-GRSVNet DNNs mit konventioneller Regularisierung übertrifft, wenn es auf realen Daten trainiert wird. Noch wichtiger ist, dass OLE-GRSVNet im Gegensatz zu konventionellen DNNs keine Zufallsdaten oder Zufallsbezeichnungen speichert, was darauf hindeutet, dass es nur intrinsische Muster lernt, indem es die Speicherkapazität des Basis-DNNs reduziert.
Die automatische End-to-End-Spracherkennung (ASR) transkribiert üblicherweise Audiosignale in Zeichensequenzen, während ihre Leistung durch die Messung der Wortfehlerrate (WER) bewertet wird, was darauf hindeutet, dass die direkte Vorhersage von Wortsequenzen stattdessen hilfreich sein könnte.Allerdings kann das Training mit Überwachung auf Wortebene aufgrund der geringen Anzahl von Beispielen pro Labelklasse schwieriger sein.In diesem Beitrag analysieren wir ein End-to-End-ASR-Modell, das eine Wort- und Zeichendarstellung in einem Multi-Task-Learning (MTL)-Rahmen kombiniert. Wir zeigen, dass es die WER verbessert und untersuchen, wie das Modell auf Wortebene von der Überwachung auf Zeichenebene profitieren kann, indem wir die erlernten induktiven Präferenzen jeder Modellkomponente empirisch analysieren.Wir stellen fest, dass das MTL-Modell durch Hinzufügen von Überwachung auf Zeichenebene zwischen der Erkennung häufigerer Wörter (bevorzugt durch das Modell auf Wortebene) und kürzerer Wörter (bevorzugt durch das Modell auf Zeichenebene) interpoliert.
Die Diskretisierung von Fließkomma-Vektoren ist ein grundlegender Schritt moderner Indizierungsverfahren, bei denen die Parameter der Quantisierer anhand von Trainingsdaten erlernt werden, um eine optimale Leistung zu erzielen und die Quantisierer an die Daten anzupassen. In dieser Arbeit schlagen wir vor, dieses Paradigma umzukehren und die Daten an den Quantisierer anzupassen: Wir trainieren ein neuronales Netz, dessen letzte Schichten einen festen, parameterfreien Quantisierer bilden, wie z.B. vordefinierte Punkte einer Sphäre.Als stellvertretendes Ziel entwerfen und trainieren wir ein neuronales Netz, das Einheitlichkeit im sphärischen latenten Raum begünstigt, während die Nachbarschaftsstruktur nach der Abbildung erhalten bleibt.  Zu diesem Zweck schlagen wir einen neuen Regularizer vor, der vom Kozachenko-Leonenko-Differentialentropie-Schätzer abgeleitet ist, und kombinieren ihn mit einem ortsabhängigen Triplett-Verlust. Experimente zeigen, dass unser End-to-End-Ansatz die meisten erlernten Quantisierungsmethoden übertrifft und bei weit verbreiteten Benchmarks mit dem Stand der Technik konkurrieren kann. Darüber hinaus zeigen wir, dass das Training ohne den Quantisierungsschritt fast keinen Unterschied in der Genauigkeit ergibt, sondern einen generischen Katalysator liefert, der mit jeder nachfolgenden Quantisierungstechnik angewendet werden kann.
Ein varianzreduzierter TD-Algorithmus (VRTD) wurde von Korda und La (2015) vorgeschlagen, der die Varianzreduzierungstechnik direkt auf das Online-TD-Lernen mit markovianischen Stichproben anwendet. In dieser Arbeit weisen wir zunächst auf die technischen Fehler in der Analyse von VRTD in Korda und La (2015) hin und liefern dann eine mathematisch solide Analyse der nicht asymptotischen Konvergenz von VRTD und seiner Varianzreduktionsleistung. Wir zeigen, dass VRTD garantiert zu einer Nachbarschaft der Fixpunktlösung von TD mit einer linearen Konvergenzrate konvergiert und dass der Varianzfehler (sowohl für i.i.d. als auch für markovianisches Sampling) und der Verzerrungsfehler (für markovianisches Sampling) von VRTD durch die Losgröße der Varianzreduktion im Vergleich zu denen von Vanilla TD signifikant reduziert werden.
Wir befassen uns mit der unbeaufsichtigten Domänenanpassung, indem wir die Tatsache berücksichtigen, dass verschiedene Domänen unterschiedlich verarbeitet werden müssen, um zu einer gemeinsamen Merkmalsrepräsentation zu gelangen, die für die Erkennung wirksam ist, und führen zu diesem Zweck einen Deep-Learning-Rahmen ein, bei dem jede Domäne eine andere Abfolge von Operationen durchläuft, so dass einige, möglicherweise komplexere Domänen mehr Berechnungen durchlaufen als andere. Dies steht im Gegensatz zu modernen Methoden der Domänenanpassung, bei denen alle Domänen mit der gleichen Abfolge von Operationen bearbeitet werden müssen, selbst wenn sie Multi-Stream-Architekturen verwenden, deren Parameter nicht gemeinsam genutzt werden.
Um das Training zu beschleunigen, wenden sich Praktiker oft an verteilte Architekturen für das verstärkte Lernen, um den Trainingsprozess zu parallelisieren und zu beschleunigen. Moderne Methoden für skalierbares Reinforcement Learning (RL) stellen jedoch häufig einen Kompromiss zwischen dem Durchsatz an Proben, aus denen ein RL-Agent lernen kann (Probendurchsatz), und der Qualität des Lernens aus jeder Probe (Sammeleffizienz) dar.In diesen skalierbaren RL-Architekturen sinkt die Sammeleffizienz deutlich, wenn man den Probendurchsatz erhöht (d. h. die Parallelisierung in IMPALA (Espeholt et al., Um dies zu adressieren, schlagen wir einen neuen verteilten Reinforcement-Learning-Algorithmus, IMPACT, vor.IMPACT erweitert PPO um drei Änderungen: ein Zielnetzwerk zur Stabilisierung des Surrogat-Ziels, einen zirkulären Puffer und abgeschnittenes Wichtigkeits-Sampling.In diskreten Aktionsraum-Umgebungen zeigen wir, dass IMPACT eine höhere Belohnung erreicht und gleichzeitig die Trainingszeit um bis zu 30 % geringer ist als die von IMPALA.Für kontinuierliche Kontrollumgebungen trainiert IMPACT schneller als bestehende skalierbare Agenten, während die Sammeleffizienz von synchronem PPO erhalten bleibt.
In diesem Papier zeigen wir, dass ein einfaches Färbungsschema sowohl theoretisch als auch empirisch die Ausdruckskraft von Message Passing Neural Networks (MPNNs) verbessern kann. Genauer gesagt stellen wir ein neuronales Graphen-Netzwerk namens Colored Local Iterative Procedure (CLIP) vor, das Farben verwendet, um identische Knotenattribute zu disambiguieren, und zeigen, dass diese Darstellung ein universeller Approximator von kontinuierlichen Funktionen auf Graphen mit Knotenattributen ist. Schließlich zeigen wir experimentell, dass CLIP in der Lage ist, strukturelle Charakteristika zu erfassen, die traditionelle MPNNs nicht unterscheiden können, während sie in Benchmark-Graphenklassifizierungsdatensätzen auf dem neuesten Stand sind.
In diesem Beitrag stellen wir eine Methode zur algorithmischen Melodieerzeugung vor, die ein generatives adversariales Netzwerk ohne rekurrente Komponenten verwendet.  Hier verwenden wir die DCGAN-Architektur mit erweiterten Faltungen und Türmen, um sequenzielle Informationen als räumliche Bildinformationen zu erfassen und weitreichende Abhängigkeiten in Melodieformen mit fester Länge, wie z. B. dem irischen Traditional Reel, zu lernen.
Neuronale maschinelle Übersetzungssysteme (NMT) haben den Stand der Technik bei der Übersetzung von Text erreicht und sind weit verbreitet.  Dennoch ist nur wenig darüber bekannt, wie diese Systeme funktionieren oder brechen.  Hier zeigen wir, dass NMT-Systeme anfällig dafür sind, hochgradig pathologische Übersetzungen zu produzieren, die völlig losgelöst vom Ausgangsmaterial sind, was wir als Halluzinationen bezeichnen.  Solche pathologischen Übersetzungen sind problematisch, weil sie das Vertrauen der Benutzer zutiefst erschüttern und leicht zu finden sind.  Wir beschreiben eine Methode zur Erzeugung von Halluzinationen und zeigen, dass viele gängige Varianten der NMT-Architektur dafür anfällig sind. Wir untersuchen eine Reihe von Ansätzen zur Verringerung der Häufigkeit von Halluzinationen, darunter Datenerweiterung, dynamische Systeme und Regularisierungstechniken, und zeigen, dass die Datenerweiterung die Häufigkeit von Halluzinationen deutlich verringert.
Damit KI-Systeme eine breite öffentliche Akzeptanz finden, müssen wir Methoden entwickeln, die in der Lage sind, die Entscheidungen von Black-Box-Modellen wie neuronalen Netzen zu erklären.In dieser Arbeit identifizieren wir zwei Probleme aktueller Erklärungsmethoden.Erstens zeigen wir, dass zwei vorherrschende Perspektiven auf Erklärungen - Feature-Additivität und Feature-Auswahl - zu grundlegend unterschiedlichen instanziellen Erklärungen führen. Das zweite Problem besteht darin, dass die derzeitigen Post-hoc-Erklärer nur für einfache Modelle, wie z. B. lineare Regression, gründlich validiert wurden, und dass bei der Anwendung auf reale neuronale Netze die Erklärer in der Regel unter der Annahme bewertet werden, dass sich die gelernten Modelle vernünftig verhalten. Wir stellen einen Verifizierungsrahmen für Erklärungsmethoden unter der Perspektive der Merkmalsauswahl vor, der auf einer nicht-trivialen Architektur eines neuronalen Netzes basiert, das für eine reale Aufgabe trainiert wurde und für das wir Garantien über seine innere Funktionsweise geben können. wir validieren die Wirksamkeit unserer Bewertung, indem wir die Fehlermodi aktueller Erklärungsmethoden aufzeigen. wir streben an, dass dieser Rahmen eine öffentlich zugängliche Standardbewertung bietet, wenn die Perspektive der Merkmalsauswahl für Erklärungen benötigt wird.
Das Planen im hochdimensionalen Raum bleibt ein schwieriges Problem, selbst mit den jüngsten Fortschritten bei Algorithmen und Rechenleistung.1 Wir lassen uns von der Efferenzkopie und der sensorischen Reafferenztheorie aus den Neurowissenschaften inspirieren.  Unser Ziel ist es, Agenten zu ermöglichen, mentale Modelle ihrer Umgebung für die Planung zu bilden.  Das Kleinhirn wird mit einem vollständig verbundenen Zweistrom-Prädiktornetz emuliert, das als Eingaben sowohl die Efferenz als auch die Merkmale des aktuellen Zustands erhält.  Die Repräsentation wird so gewählt, dass sie eine schnelle Suche mit klassischen Graphen-Suchalgorithmen ermöglicht, und wir zeigen die Effektivität unseres Ansatzes anhand einer Aufgabe zur Anpassung von Blickpunkten mit einem modifizierten Best-First-Suchalgorithmus.
Durch die Einführung des Konzepts eines optimalen Repräsentationsraums bieten wir eine einfache theoretische Lösung für dieses scheinbare Paradoxon und stellen ein unkompliziertes Verfahren vor, mit dem tiefe rekurrente Modelle im Vergleich zu flachen Modellen ohne Umschulung oder architektonische Änderungen gleich gut (und manchmal besser) abschneiden. Um unsere Analyse zu validieren, führen wir eine Reihe konsistenter empirischer Auswertungen durch und stellen dabei mehrere neue Satzeinbettungsmodelle vor. Obwohl diese Arbeit im Kontext der Verarbeitung natürlicher Sprache präsentiert wird, sind die Erkenntnisse ohne weiteres auf andere Bereiche übertragbar, die auf verteilte Repräsentationen für Transferaufgaben angewiesen sind.
In diesem Papier schlagen wir den ersten großen, von Menschen entworfenen Lückentest-Datensatz CLOTH vor, dessen Fragen in Sprachprüfungen der Mittel- und Oberstufe verwendet wurden. Mit den fehlenden Leerzeichen, die sorgfältig von Lehrern erstellt wurden, und den absichtlich verwirrend gestalteten Auswahlmöglichkeiten der Kandidaten erfordert CLOTH ein tieferes Sprachverständnis und eine größere Aufmerksamkeitsspanne als frühere automatisch generierte Lückendatensätze. Wir untersuchen die Ursache für diesen Leistungsunterschied, führen die Schwächen des Modells auf einige besondere Eigenschaften von CLOTH zurück und identifizieren die begrenzte Fähigkeit, einen langfristigen Kontext zu verstehen, als den entscheidenden Engpass.
Während die Reaktionseigenschaften von Neuronen in künstlichen neuronalen Netzen denen des Gehirns ähneln, sind die Netzwerkarchitekturen oft unterschiedlich. Hier stellen wir die Frage, ob ein neuronales Netzwerk sowohl neuronale Repräsentationen als auch, wenn die Architektur nicht eingeschränkt und optimiert ist, die anatomischen Eigenschaften der neuronalen Schaltkreise wiederherstellen kann. Wir demonstrieren dies an einem System, bei dem die Konnektivität und die funktionelle Organisation charakterisiert wurden, nämlich dem Kopfrichtungs-Schaltkreis des Nagers und der Fruchtfliege.Wir trainierten rekurrente neuronale Netze (RNNs), um die Kopfrichtung durch Integration der Winkelgeschwindigkeit zu schätzen. Wir fanden heraus, dass die beiden unterschiedlichen Klassen von Neuronen, die im Kopfrichtungs-System beobachtet werden, die Ring-Neuronen und die Shifter-Neuronen, in künstlichen neuronalen Netzen als Ergebnis des Trainings auf natürliche Weise entstanden sind, und dass die Konnektivitätsanalyse und die In-silico-Neurophysiologie strukturelle und mechanistische Ähnlichkeiten zwischen künstlichen Netzen und dem Kopfrichtungs-System aufzeigen. Insgesamt zeigen unsere Ergebnisse, dass die Optimierung von RNNs in einer zielgerichteten Aufgabe die Struktur und Funktion biologischer Schaltkreise rekapitulieren kann, was darauf hindeutet, dass künstliche neuronale Netze zur Untersuchung des Gehirns sowohl auf der Ebene der neuronalen Aktivität als auch der anatomischen Organisation verwendet werden können.
Faltungsneuronale Netze (Convolutional Neural Networks, CNN) sind rechenintensiv, was ihre Anwendung auf mobilen Geräten einschränkt: Ihr Energieverbrauch wird von der Anzahl der Multiplikationen dominiert, die zur Durchführung der Faltungen erforderlich sind, Wir schlagen zwei Modifikationen für Winograd-basierte CNNs vor, um diese Methoden in die Lage zu versetzen, die Sparsamkeit zu nutzen: Erstens verschieben wir die ReLU-Operation in den Winograd-Bereich, um die Sparsamkeit der transformierten Aktivierungen zu erhöhen. Für Modelle auf CIFAR-10-, CIFAR-100- und ImageNet-Datensätzen reduziert unsere Methode die Anzahl der Multiplikationen um das 10,4-fache, 6,8-fache bzw. 10,8-fache bei einem Genauigkeitsverlust von weniger als 0,1 % und übertrifft damit frühere Basisverfahren um das 2,0-fache bis 3,0-fache.Wir zeigen auch, dass die Verlagerung von ReLU in den Winograd-Bereich ein aggressiveres Pruning ermöglicht.
Das Ziel dieses Algorithmus ist es, tiefe neuronale Netze zu trainieren und bei Bedarf als Alternative zu Stochastic Gradient Descent (SGD) und seinen Varianten zu fungieren. Wir haben unseren Algorithmus auf dem MNIST-Datensatz sowie auf mehreren globalen Optimierungsproblemen wie der Ackley-Funktion evaluiert und festgestellt, dass der Algorithmus in beiden Fällen relativ gut abschneidet und andere globale Optimierungsalgorithmen wie Particle Swarm Optimization (PSO) und Evolution Strategies (ES) überholt.
Stochastische neuronale Netzgewichte werden in einer Vielzahl von Kontexten verwendet, einschließlich Regularisierung, Bayes'sche neuronale Netze, Exploration beim Verstärkungslernen und Evolutionsstrategien. Leider teilen sich aufgrund der großen Anzahl von Gewichten alle Beispiele in einem Mini-Batch typischerweise dieselbe Gewichtsstörung, wodurch der Varianzreduzierungseffekt von großen Mini-Batches eingeschränkt wird. Empirisch gesehen erreicht Flipout die ideale lineare Varianzreduktion für vollständig verbundene Netze, Faltungsnetze und RNNs. Wir finden signifikante Beschleunigungen beim Training neuronaler Netze mit multiplikativen Gauß-Störungen. Flipout ermöglicht es uns auch, Evolutionsstrategien zu vektorisieren: In unseren Experimenten kann ein einzelner Grafikprozessor mit Flipout den gleichen Durchsatz wie mindestens 40 CPU-Kerne mit bestehenden Methoden bewältigen, was einer Kostenreduktion um den Faktor 4 bei Amazon Web Services entspricht.
Tiefe generative Modelle wie Variational AutoEncoder (VAE) und Generative Adversarial Network (GAN) spielen eine immer wichtigere Rolle im maschinellen Lernen und in der Computer Vision. Allerdings gibt es zwei grundlegende Probleme, die ihre realen Anwendungen behindern: die Schwierigkeit der Durchführung von Variationsinferenz in VAE und die funktionale Abwesenheit der Codierung von realen Proben in GAN. In diesem Papier schlagen wir einen neuartigen Algorithmus namens Latently Invertible Autoencoder (LIA) vor, um die beiden oben genannten Probleme in einem Rahmenwerk anzugehen: Ein invertierbares Netzwerk und seine inverse Abbildung sind symmetrisch in den latenten Raum der VAE eingebettet. Der partielle Kodierer transformiert also zunächst die Eingabe in Merkmalsvektoren und formt dann die Verteilung dieser Merkmalsvektoren so um, dass sie durch das invertierbare Netzwerk an einen Prior angepasst wird.Der Dekodierer geht in der umgekehrten Reihenfolge der zusammengesetzten Abbildungen des Kodierers vor. Ein zweistufiges, stochastizitätsfreies Trainingsschema wurde entwickelt, um LIA über adversariales Lernen zu trainieren, in dem Sinne, dass der Decoder von LIA zunächst als Standard-GAN mit dem invertierbaren Netzwerk trainiert wird und dann der partielle Encoder aus einem Autoencoder gelernt wird, indem das invertierbare Netzwerk von LIA getrennt wird.  Experimente, die mit dem FFHQ-Gesichtsdatensatz und drei LSUN-Datensätzen durchgeführt wurden, bestätigen die Wirksamkeit von LIA für Inferenz und Generierung.
Trotz des Potenzials, die Interpretierbarkeit und die Kompositionalität des Verhaltens künstlicher Agenten zu erhöhen, ist es nach wie vor schwierig, aus Demonstrationen neuronaler Netze zu lernen, die Computerprogramme repräsentieren. Die wichtigsten Herausforderungen, die algorithmische Domänen von anderen Bereichen des Imitationslernens unterscheiden, sind die Notwendigkeit einer hohen Genauigkeit, die Einbeziehung spezifischer Datenstrukturen und die extrem eingeschränkte Beobachtbarkeit.Um diese Herausforderungen anzugehen, schlagen wir vor, Programme als parametrisierte hierarchische Prozeduren (PHPs) zu modellieren. Eine PHP ist eine Sequenz von bedingten Operationen, die einen Programmzähler zusammen mit der Beobachtung verwendet, um zwischen der Ausführung einer elementaren Aktion, dem Aufruf einer anderen PHP als Unterprozedur und der Rückkehr zum Aufrufer zu wählen. Wir entwickeln einen Algorithmus für das Training von PHPs aus einer Menge von Supervisor-Demonstrationen, von denen nur einige mit der internen Aufrufstruktur annotiert sind, und wenden ihn für ein effizientes stufenweises Training von mehrstufigen PHPs an. Wir zeigen in zwei Benchmarks, NanoCraft und Long-Hand-Addition, dass PHPs neuronale Programme aus kleineren Mengen von sowohl annotierten als auch unannotierten Demonstrationen genauer lernen können.
Deep Reinforcement Learning hat es geschafft, modernste Ergebnisse beim Erlernen von Kontrollstrategien direkt aus Rohpixeln zu erzielen, aber trotz seines bemerkenswerten Erfolgs versagt es bei der Generalisierung, einer grundlegenden Komponente, die für ein stabiles System der künstlichen Intelligenz erforderlich ist. Wir zeigen, dass verschiedene Formen der Feinabstimmung, eine gängige Methode des Transferlernens, für die Anpassung an solch kleine visuelle Veränderungen nicht effektiv sind. Wir schlagen vor, dass in einigen Fällen das Transferlernen durch Hinzufügen einer speziellen Komponente verbessert werden kann, deren Ziel es ist, die visuelle Abbildung zwischen der bekannten und der neuen Domäne zu erlernen.Konkret verwenden wir Unaligned Generative Adversarial Networks (GANs), um eine Abbildungsfunktion zu erstellen, die Bilder in der Zielaufgabe in entsprechende Bilder in der Ausgangsaufgabe übersetzt. Wir zeigen, dass das Erlernen dieses Mappings wesentlich effizienter ist als ein erneutes Training. \url{https://streamable.com/msgtm} und \url{https://streamable.com/5e2ka} zeigen eine Visualisierung eines trainierten Agenten, der Breakout und Road Fighter mit und ohne den GAN-Transfer spielt.
Wir überprüfen diese Annahme sowohl theoretisch als auch experimentell im Lichte der Erkenntnisse und Trends der letzten Jahre. Wir gehen auf einige frühere, oft zitierte Experimente und theoretische Darstellungen noch einmal genauer ein und stellen eine neue Reihe von Experimenten in größeren, hochmodernen Umgebungen vor. Wir kommen zu dem Schluss, dass die verbesserte Trainingsleistung von adaptiven Optimierern bei richtiger Abstimmung im Allgemeinen nicht mit einer Überanpassung einhergeht, vor allem nicht bei modernem Deep Learning.Schließlich fassen wir einen ``Benutzerleitfaden'' für adaptive Optimierer zusammen, einschließlich einiger vorgeschlagener Änderungen an AdaGrad, um einige seiner empirischen Mängel zu mildern.
In diesem Beitrag stellen wir APL vor, einen Algorithmus, der Wahrscheinlichkeitsverteilungen annähert, indem er sich an die überraschendsten Beobachtungen erinnert, die ihm in der Vergangenheit begegnet sind. Diese vergangenen Beobachtungen werden von einem externen Speichermodul abgerufen und von einem Decoder-Netzwerk verarbeitet, das Informationen aus verschiedenen Speicherplätzen kombinieren kann, um über den direkten Abruf hinaus zu verallgemeinern.Wir zeigen, dass dieser Algorithmus bei Klassifizierungs-Benchmarks mit wenigen Aufnahmen mit einem geringeren Speicherbedarf genauso gut abschneidet wie moderne Basislösungen.  Darüber hinaus ermöglicht die Speicherkompression eine Skalierung auf Tausende von unbekannten Bezeichnungen.  Schließlich stellen wir eine Meta-Learning-Aufgabe vor, die anspruchsvoller ist als die direkte Klassifizierung: In dieser Umgebung ist APL in der Lage, mit weniger als einem Beispiel pro Klasse durch deduktives Schließen zu generalisieren.
Für die meisten dieser Aufgaben, wie z.B. die Stimmungsanalyse von Kundenrezensionen, analysiert ein rekurrentes neuronales Netzmodell die gesamte Rezension, bevor es eine Entscheidung trifft.Wir argumentieren, dass das Lesen der gesamten Eingabe in der Praxis nicht immer notwendig ist, da viele Rezensionen oft einfach zu klassifizieren sind, d.h., Inspiriert von verschiedenen bekannten menschlichen Lesetechniken, implementiert unser Ansatz einen intelligenten rekurrenten Agenten, der die Wichtigkeit des aktuellen Textausschnitts bewertet, um zu entscheiden, ob er eine Vorhersage trifft, einige Texte überspringt oder einen Teil des Satzes erneut liest. Mit einem End-to-End-Trainingsalgorithmus, der auf einem Policy-Gradienten basiert, trainieren und testen wir unseren Agenten auf verschiedenen Textklassifizierungsdatensätzen und erreichen sowohl eine höhere Effizienz als auch eine bessere Genauigkeit im Vergleich zu früheren Ansätzen. 
Die Bayes-optimale Lösung für die Erkundung ist für komplexe Umgebungen unlösbar, und obwohl mehrere Erkundungsmethoden als Annäherungen vorgeschlagen wurden, bleibt es unklar, welches zugrunde liegende Ziel durch die bestehenden Erkundungsmethoden optimiert wird, oder wie sie verändert werden können, um Vorwissen über die Aufgabe einzubeziehen. Darüber hinaus ist unklar, wie man eine einzelne Explorationsstrategie erlernen kann, die für die Lösung mehrerer nachgelagerter Aufgaben nützlich ist. Wir beheben diese Mängel, indem wir eine einzelne Explorationsstrategie erlernen, die schnell eine Reihe von nachgelagerten Aufgaben in einer Multi-Task-Umgebung lösen kann und die Kosten für das Erlernen der Exploration amortisiert. Wir fassen die Exploration als ein Problem des State Marginal Matching (SMM) um, bei dem es darum geht, eine Strategie zu erlernen, bei der die Zustandsrandverteilung mit einer gegebenen Zielzustandsverteilung übereinstimmt, die das Vorwissen über die Aufgabe einbeziehen kann.Wir optimieren das Ziel, indem wir es auf ein Nullsummenspiel mit zwei Spielern zwischen einem Zustandsdichtemodell und einer parametrischen Strategie reduzieren. Unsere theoretische Analyse dieses Ansatzes deutet darauf hin, dass frühere Explorationsmethoden keine Politik erlernen, die eine Verteilungsanpassung vornimmt, sondern einen Wiederholungspuffer erwerben, der eine Verteilungsanpassung vornimmt, eine Beobachtung, die möglicherweise den Erfolg dieser früheren Methoden in Einzelaufgaben erklärt.Sowohl bei simulierten als auch bei realen Aufgaben zeigen wir, dass unser Algorithmus schneller exploriert und sich schneller anpasst als frühere Methoden.
Die Effektivität von Convolutional Neural Networks (CNNs) beruht zu einem großen Teil auf ihrer Fähigkeit, die Translationsinvarianz auszunutzen, die vielen Lernproblemen innewohnt. Für Bilder mit quadratischen Pixeln sind daher nur ganzzahlige Translationen, Rotationen um Vielfache von 90 Grad und Spiegelungen zulässig, während die quadratische Kachelung eine 4-fache Rotationssymmetrie bietet, während eine hexagonale Kachelung der Ebene eine 6-fache Rotationssymmetrie aufweist. In diesem Beitrag zeigen wir, wie man effizient planare Faltung und Gruppenfaltung über hexagonalen Gittern implementieren kann, indem man bestehende, hoch optimierte Faltungsroutinen wiederverwendet.Wir stellen fest, dass planare HexaConv aufgrund der geringeren Anisotropie hexagonaler Filter eine bessere Genauigkeit bietet als planare Faltung mit quadratischen Filtern, wenn man ein festes Parameterbudget zugrunde legt. Darüber hinaus stellen wir fest, dass der erhöhte Symmetriegrad des hexagonalen Gitters die Effektivität der Gruppenfaltung erhöht, da mehr Parameter gemeinsam genutzt werden können. wir zeigen, dass unsere Methode konventionelle CNNs auf dem AID-Luftbildklassifizierungsdatensatz deutlich übertrifft und sogar die vortrainierten Modelle von ImageNet übertrifft.
Deep Convolutional Neural Networks (CNNs) haben wiederholt gezeigt, dass sie bei Bildklassifizierungsaufgaben gut abschneiden und erfolgreich eine breite Palette von Objekten erkennen, wenn sie genügend Trainingsdaten erhalten.Methoden für die Objektlokalisierung müssen jedoch noch erheblich verbessert werden.Gängige Ansätze für dieses Problem beinhalten die Verwendung eines gleitenden Fensters, manchmal in mehreren Maßstäben, das Input für ein Deep CNN liefert, das darauf trainiert ist, den Inhalt des Fensters zu klassifizieren. In diesem Papier bieten wir einen grundlegend anderen Ansatz für die Lokalisierung von erkannten Objekten in Bildern an. Unsere Methode basiert auf der Idee, dass ein tiefes CNN, das in der Lage ist, ein Objekt zu erkennen, implizit Wissen über die Objektposition in seinen Verbindungsgewichten enthalten muss. Diese Methode beinhaltet die Berechnung der Ableitung von netzwerkgenerierten Aktivierungsmustern, wie z.B. die Aktivierung von Output-Klassenlabel-Einheiten, in Bezug auf jedes eingefügte Pixel, wobei eine Sensitivitätsanalyse durchgeführt wird, die die Pixel identifiziert, die auf lokaler Ebene den größten Einfluss auf die internen Repräsentationen und die Objekterkennung haben. Wir zeigen, dass eine einfache lineare Zuordnung von Sensitivitätskarten zu Bounding-Box-Koordinaten erlernt werden kann, um das erkannte Objekt zu lokalisieren. Unsere experimentellen Ergebnisse unter Verwendung von realen Datensätzen, für die die wahre Lokalisierungsinformation bekannt ist, zeigen eine konkurrenzfähige Genauigkeit unserer schnellen Technik.
Einerseits ist ein Trellis-Netzwerk ein temporales Faltungsnetzwerk mit einer speziellen Struktur, die durch Gewichtsverknüpfung über die Tiefe und direkte Injektion des Inputs in tiefe Schichten gekennzeichnet ist, andererseits zeigen wir, dass abgeschnittene rekurrente Netzwerke äquivalent zu Trellis-Netzwerken mit einer speziellen Sparsity-Struktur in ihren Gewichtsmatrizen sind. Wir nutzen diese Verbindungen, um leistungsstarke Trellis-Netze zu entwerfen, die strukturelle und algorithmische Elemente sowohl von rekurrenten als auch von konvolutionalen Modellen übernehmen.Experimente zeigen, dass Trellis-Netze den aktuellen Stand der Technik bei einer Reihe von anspruchsvollen Benchmarks übertreffen, darunter Aufgaben zur Sprachmodellierung auf Wortebene und auf Zeichenebene sowie Stresstests zur Bewertung der Langzeitspeicherfähigkeit.Der Code ist unter https://github.com/locuslab/trellisnet verfügbar.
Wir schlagen ein End-to-End-Framework für das Training von domänenspezifischen Modellen (DSMs) vor, um sowohl eine hohe Genauigkeit als auch eine hohe Recheneffizienz für Objekterkennungsaufgaben zu erreichen.DSMs werden mit Destillation trainiert und konzentrieren sich auf das Erreichen einer hohen Genauigkeit in einer begrenzten Domäne (z. B. feste Ansicht einer Kreuzung).Wir argumentieren, dass DSMs wesentliche Merkmale auch mit einer kleinen Modellgröße gut erfassen können, was eine höhere Genauigkeit und Effizienz als bei traditionellen Techniken ermöglicht.  Für den begrenzten Bereich konnten wir beobachten, dass kompakte DSMs die Genauigkeit von mit COCO trainierten Modellen gleicher Größe deutlich übertreffen. Durch das Training auf einem kompakten Datensatz zeigen wir, dass bei einem Genauigkeitsverlust von nur 3,6% die Trainingszeit um 93% reduziert werden kann.
Wir vergleichen das modellfreie Reinforcement Learning mit den modellbasierten Ansätzen durch die Linse der Ausdruckskraft von neuronalen Netzen für Policies, $Q$-Funktionen und Dynamik.  Wir zeigen theoretisch und empirisch, dass es selbst für einen eindimensionalen kontinuierlichen Zustandsraum viele MDPs gibt, deren optimale $Q$-Funktionen und Policies viel komplexer sind als die Dynamik.Wir vermuten, dass viele reale MDPs eine ähnliche Eigenschaft haben.Für diese MDPs ist die modellbasierte Planung ein vorteilhafter Algorithmus, da die resultierenden Policies die optimale Policy wesentlich besser approximieren können als eine Parametrisierung durch ein neuronales Netzwerk, und die modellfreie oder modellbasierte Policy-Optimierung auf der Policy-Parametrisierung beruht. Motiviert durch die Theorie wenden wir einen einfachen, mehrstufigen, modellbasierten Bootstrapping-Planer (BOOTS) an, um eine schwache $Q$-Funktion in eine stärkere Politik zu booten. empirische Ergebnisse zeigen, dass die Anwendung von BOOTS auf modellbasierte oder modellfreie Politikoptimierungsalgorithmen zur Testzeit die Leistung bei MuJoCo-Benchmark-Aufgaben verbessert.
Physikalisches Denken mit gesundem Menschenverstand ist ein wesentlicher Bestandteil für jeden intelligenten Agenten, der in der realen Welt operiert, z.B. um die Umgebung zu simulieren oder um den Zustand von Teilen der Welt abzuleiten, die derzeit unbeobachtet sind.Um den Bedingungen der realen Welt zu entsprechen, muss dieses kausale Wissen ohne Zugang zu überwachten Daten erlernt werden.Um dieses Problem anzugehen, stellen wir eine neuartige Methode vor, die lernt, Objekte zu entdecken und ihre physikalischen Interaktionen aus visuellen Rohbildern auf eine rein unbeaufsichtigte Weise zu modellieren. Anhand von Videos von hüpfenden Bällen zeigen wir die überlegenen Modellierungsfähigkeiten unserer Methode im Vergleich zu anderen unbeaufsichtigten neuronalen Ansätzen, die kein solches Vorwissen einbeziehen, und demonstrieren die Fähigkeit, mit Verdeckungen umzugehen und das gelernte Wissen auf Szenen mit einer unterschiedlichen Anzahl von Objekten zu extrapolieren.
Die Idee, dass neuronale Netze eine Vorliebe für Einfachheit haben, hat eine lange Geschichte und die Vorliebe für Einfachheit bietet eine Möglichkeit, diese Intuition zu quantifizieren.  Sie sagt für eine breite Klasse von Input-Output-Maps, die viele Systeme in Wissenschaft und Technik beschreiben können, voraus, dass einfache Outputs bei einer gleichmäßigen Zufallsauswahl von Inputs exponentiell wahrscheinlicher sind als komplexe Outputs.  Dieses auf Einfachheit basierende Verhalten wurde bei Systemen beobachtet, die von der RNA-Sequenz bis zur Sekundärstrukturkarte, von Systemen gekoppelter Differentialgleichungen bis zu Modellen des Pflanzenwachstums reichen.   Tiefe neuronale Netze können als eine Abbildung vom Raum der Parameter (der Gewichte) auf den Raum der Funktionen (wie Eingaben durch das Netz in Ausgaben umgewandelt werden) betrachtet werden.  Wir zeigen, dass diese Parameter-Funktions-Abbildung den notwendigen Bedingungen für eine Einfachheitsverzerrung gehorcht, und zeigen numerisch, dass sie stark auf Funktionen mit geringer Beschreibungskomplexität ausgerichtet ist.  Wir zeigen auch eine Zipf-ähnliche Powerlaw-Wahrscheinlichkeits-Rang-Beziehung.   Eine Tendenz zur Einfachheit kann erklären, warum neuronale Netze so gut verallgemeinern.
Imitationslernen (IL) ist ein attraktiver Ansatz, um erwünschtes autonomes Verhalten zu erlernen, aber es ist schwierig, IL so zu steuern, dass beliebige Ziele erreicht werden.Im Gegensatz dazu verwenden planungsbasierte Algorithmen dynamische Modelle und Belohnungsfunktionen, um Ziele zu erreichen.Belohnungsfunktionen, die erwünschtes Verhalten hervorrufen, sind jedoch oft schwer zu spezifizieren.In diesem Beitrag schlagen wir "Imitative Modelle" vor, um die Vorteile von IL und zielgerichteter Planung zu kombinieren. Imitative Modelle sind probabilistische Vorhersagemodelle für wünschenswertes Verhalten, die in der Lage sind, interpretierbare, expertenähnliche Trajektorien zu planen, um spezifizierte Ziele zu erreichen.Wir leiten Familien von flexiblen Zielvorgaben ab, einschließlich eingeschränkter Zielregionen, uneingeschränkter Zielsätze und energiebasierter Ziele.Wir zeigen, dass unsere Methode diese Zielvorgaben verwenden kann, um das Verhalten erfolgreich zu steuern.Unsere Methode übertrifft sechs IL-Ansätze und einen planungsbasierten Ansatz in einer dynamischen simulierten autonomen Fahraufgabe erheblich und wird effizient von Expertendemonstrationen ohne Online-Datenerfassung gelernt.  Wir zeigen auch, dass unser Ansatz robust gegenüber schlecht spezifizierten Zielen ist, wie z.B. Ziele auf der falschen Straßenseite.
Um dieses Problem anzugehen, stellen wir einen vollständig differenzierbaren Rahmen für Kommunikation und Argumentation vor, der es den Agenten ermöglicht, kooperative Aufgaben in teilweise beobachtbaren Umgebungen zu lösen. Der Rahmen wurde entwickelt, um explizite Argumentation zwischen Agenten durch ein neuartiges gedächtnisbasiertes Aufmerksamkeitsnetzwerk zu ermöglichen, das selektiv aus seinen vergangenen Erinnerungen lernen kann. Das Modell kommuniziert durch eine Reihe von Argumentationsschritten, die die Absichten jedes Agenten in gelernte Repräsentationen zerlegen, die erstens dazu verwendet werden, die Relevanz der kommunizierten Informationen zu berechnen und zweitens, um Informationen aus den Erinnerungen zu extrahieren, wenn neu empfangene Informationen vorliegen.Durch die selektive Interaktion mit neuen Informationen lernt das Modell effektiv ein Kommunikationsprotokoll direkt, in einer Ende-zu-Ende-Weise.Wir demonstrieren empirisch die Stärke unseres Modells in kooperativen Multi-Agenten-Aufgaben, bei denen die Inter-Agenten-Kommunikation und die Argumentation über frühere Informationen die Leistung im Vergleich zu den Basissystemen erheblich verbessert.
Um eine bessere Verallgemeinerung mit weniger Trainingsproben zu erreichen, beinhaltet SymODEN eine angemessene induktive Verzerrung, indem es den zugehörigen Berechnungsgraphen in einer physikalisch informierten Art und Weise entwirft. Insbesondere erzwingen wir eine Hamilton-Dynamik mit Kontrolle, um die zugrundeliegende Dynamik in einer transparenten Art und Weise zu erlernen, die dann genutzt werden kann, um Erkenntnisse über relevante physikalische Aspekte des Systems, wie Masse und potentielle Energie, zu gewinnen. Darüber hinaus schlagen wir eine Parametrisierung vor, die diesen Hamilton'schen Formalismus auch dann durchsetzen kann, wenn die verallgemeinerten Koordinatendaten in einen hochdimensionalen Raum eingebettet sind oder wir nur auf Geschwindigkeitsdaten anstelle des verallgemeinerten Impulses zugreifen können.
Föderiertes Lernen, bei dem ein globales Modell durch iterative Parameter-Mittelung von lokal berechneten Updates trainiert wird, ist ein vielversprechender Ansatz für das verteilte Training von tiefen Netzwerken; es bietet eine hohe Kommunikationseffizienz und Datenschutz, was es ermöglicht, sich gut in dezentralisierte Datenumgebungen, z.B. Mobile-Cloud-Ökosysteme, einzufügen.Trotz der Vorteile haben die auf föderiertem Lernen basierenden Methoden jedoch immer noch eine Herausforderung im Umgang mit Nicht-IID-Trainingsdaten von lokalen Geräten (d.h. Lernern), In diesem Zusammenhang untersuchen wir die Auswirkungen verschiedener hyperparametrischer Bedingungen in Nicht-IID-Umgebungen, um wichtige Fragen in praktischen Implementierungen zu beantworten: (i) Wir untersuchen zunächst die Parameterdivergenz lokaler Aktualisierungen, um die Leistungsverschlechterung durch Nicht-IID-Daten zu erklären, und finden den Ursprung der Parameterdivergenz sowohl empirisch als auch theoretisch. (ii) Anschließend untersuchen wir die Auswirkungen von Optimierern, Netzwerktiefe/-breite und Regularisierungstechniken; unsere Beobachtungen zeigen, dass die bekannten Vorteile der Hyperparameter-Optimierungsstrategien bei Nicht-IID-Daten eher zu abnehmenden Erträgen führen könnten.(iii) Schließlich liefern wir die Gründe für die Fehlerfälle auf kategorisierte Weise, hauptsächlich basierend auf Metriken der Parameterdivergenz.
Ein praktischer Angriff sollte so wenig wie möglich Wissen über die angegriffenen Modelle T erfordern. Aktuelle Ersatzangriffe benötigen vortrainierte Modelle, um Gegenbeispiele zu generieren, und die Erfolgsraten ihrer Angriffe hängen stark von der Übertragbarkeit der Gegenbeispiele ab. In dieser Studie schlagen wir einen neuartigen Nachahmungsangriff vor, der zunächst eine Kopie der T durch ein Zwei-Spieler-Spiel wie die generativen adversen Netzwerke (GANs) erzeugt. Das Ziel des generativen Modells G ist es, Beispiele zu generieren, die dazu führen, dass D andere Ergebnisse als T liefert. Das Ziel des diskriminativen Modells D ist es, bei gleichen Eingaben die gleichen Etiketten wie T auszugeben. Im Vergleich zu den derzeitigen Ersatzangriffen kann der Nachahmungsangriff weniger Trainingsdaten verwenden, um eine Kopie von T zu erzeugen, und die Übertragbarkeit der gegnerischen Beispiele verbessern.Experimente zeigen, dass unser Nachahmungsangriff weniger Trainingsdaten benötigt als die Black-Box-Ersatzangriffe, aber eine Angriffserfolgsrate erreicht, die der des White-Box-Angriffs auf ungesehene Daten ohne Abfrage nahe kommt.
Stochastic Gradient Descent (SGD)-Methoden mit zufällig ausgewählten Stapeln sind weit verbreitet, um neuronale Netzwerkmodelle (NN) zu trainieren.Design Exploration durchzuführen, um das beste NN für eine bestimmte Aufgabe zu finden, erfordert oft ein umfangreiches Training mit verschiedenen Modellen auf einem großen Datensatz, was sehr rechenintensiv ist.Die einfachste Methode, um diese Berechnung zu beschleunigen, ist die Verteilung des SGD-Stapels auf mehrere Prozessoren.Allerdings führt das Training mit großen Stapeln oft zu einer Verschlechterung der Genauigkeit, schlechter Generalisierung und sogar schlechter Robustheit gegenüber gegnerischen Angriffen.  Um dieses Problem anzugehen, schlagen wir eine neuartige Methode für das Training von großen Stapeln vor, die die neuesten Ergebnisse im Bereich des gegnerischen Trainings (Regularisierung gegen "scharfe Minima") und der Optimierung zweiter Ordnung (Verwendung von Krümmungsinformationen zur adaptiven Änderung der Stapelgröße während des Trainings) kombiniert.  Unser neuer Ansatz übertrifft die Leistung der bestehenden Lösungen sowohl in Bezug auf die Genauigkeit als auch auf die Anzahl der SGD-Iterationen (bis zu 1\% bzw. $3\mal$), und wir betonen, dass dies ohne zusätzliches Hyper-Parameter-Tuning erreicht wird, um unsere Methode an jedes dieser Experimente anzupassen.
Inspiriert durch neurophysiologische Entdeckungen von Navigationszellen im Säugetiergehirn, stellen wir die erste tiefe neuronale Netzwerkarchitektur zur Modellierung des egozentrischen räumlichen Gedächtnisses (ESM) vor, die lernt, die Pose des Agenten abzuschätzen und progressiv Top-Down-2D-Globalkarten aus egozentrischen Ansichten in einer räumlich erweiterten Umgebung zu konstruieren. Während der Erkundung aktualisiert das von uns vorgeschlagene ESM-Netzwerkmodell den Glauben an die globale Karte auf der Grundlage lokaler Beobachtungen mit Hilfe eines rekurrenten neuronalen Netzwerks. Es erweitert außerdem die lokale Abbildung mit einem neuartigen externen Speicher, um latente Repräsentationen der besuchten Orte auf der Grundlage ihrer entsprechenden Positionen in den egozentrischen Koordinaten zu kodieren und zu speichern. In den Experimenten demonstrieren wir die Funktionalitäten des ESM-Netzwerks bei zufälligen Spaziergängen in komplizierten 3D-Labyrinthen, indem wir es mit mehreren konkurrierenden Basislinien und hochmodernen SLAM-Algorithmen (Simultaneous Localization and Mapping) vergleichen. Eine umfassende Analyse unserer Modelle zeigt die wesentliche Rolle der einzelnen Module in der von uns vorgeschlagenen Architektur und demonstriert die Effizienz der Kommunikation zwischen diesen Modulen. Wir hoffen, dass diese Arbeit die Forschung im Bereich der Zusammenarbeit und Kommunikation in beiden Bereichen der Informatik und der Computational Neuroscience vorantreibt.
Wir zeigen, dass die Netzwerke verallgemeinert werden können, wenn der übliche Trainingsverlust durch einen Lipschitz-Regularisierungsterm ergänzt wird.  Wir beweisen die Verallgemeinerung, indem wir zunächst ein stärkeres Konvergenzergebnis sowie eine Konvergenzrate aufstellen.   Ein zweites Ergebnis löst eine Frage, die in Zhang et al. (2016) gestellt wurde: Wie kann ein Modell zwischen dem Fall sauberer Etiketten und randomisierter Etiketten unterscheiden?  Unsere Antwort ist, dass die Lipschitz-Regularisierung unter Verwendung der Lipschitz-Konstante der sauberen Daten diese Unterscheidung ermöglicht.  In diesem Fall lernt das Modell eine andere Funktion, von der wir annehmen, dass es die schmutzigen Etiketten nicht richtig lernt.  
Für einen schnellen und energieeffizienten Einsatz von trainierten tiefen neuronalen Netzen auf ressourcenbeschränkter eingebetteter Hardware sollte jeder gelernte Gewichtungsparameter idealerweise mit einem einzigen Bit dargestellt und gespeichert werden.  Hier berichten wir über große Verbesserungen der Fehlerraten bei mehreren Datensätzen für tiefe neuronale Faltungsnetze, die mit 1-Bit-pro-Gewicht eingesetzt werden. Unter Verwendung von Wide-Residual-Netzen als Hauptgrundlage vereinfacht unser Ansatz bestehende Methoden, die Gewichte durch Anwendung der Vorzeichenfunktion beim Training binarisieren; wir wenden Skalierungsfaktoren für jede Schicht mit konstanten ungelernten Werten an, die den schichtspezifischen Standardabweichungen entsprechen, die zur Initialisierung verwendet werden. Für CIFAR-10, CIFAR-100 und ImageNet sowie für Modelle mit 1-Bit-pro-Gewicht, die weniger als 10 MB Parameterspeicher benötigen, erreichen wir Fehlerraten von 3,9%, 18,5% bzw. 26,0% / 8,5% (Top-1 / Top-5).Wir betrachteten auch MNIST, SVHN und ImageNet32 und erzielten Testergebnisse mit 1-Bit-pro-Gewicht von 0. Für CIFAR halbieren sich unsere Fehlerraten auf die Hälfte der zuvor berichteten Werte und liegen innerhalb von etwa 1 % unserer Fehlerraten für dasselbe Netzwerk mit Gewichten voller Präzision. für Netzwerke, die überanpassen, zeigen wir auch signifikante Verbesserungen der Fehlerrate, indem wir keine Batch-Normalisierungsskala und Offset-Parameter lernen. dies gilt sowohl für Netzwerke mit voller Präzision als auch für solche mit 1-Bit-pro-Gewicht. Bei Verwendung eines Warmstart-Lernzeitplans haben wir festgestellt, dass das Training für 1-Bit-pro-Gewicht-Netzwerke genauso schnell ist wie für Netzwerke mit voller Präzision, wobei die Genauigkeit besser ist als bei Standardzeitplänen, und wir erreichten etwa 98%-99% der Spitzenleistung in nur 62 Trainingsepochen für CIFAR-10/100. Für den vollständigen Trainingscode und die trainierten Modelle in MATLAB, Keras und PyTorch siehe https://github.com/McDonnell-Lab/1-bit-per-weight/.
Es nutzt die Fähigkeiten der neuen Generation von Grafikprozessoren, die auf der Turing-Architektur von NVIDIA basieren, um neue Methoden für die intuitive Erkundung von Landschaften mit nicht-trivialer Geometrie und Topologie in der virtuellen Realität zu entwickeln.
Es ist eng verwandt mit Sequenz-zu-Sequenz-Modellen, die latente Repräsentationen fester Größe für Sequenzen erlernen und für eine Reihe anspruchsvoller überwachter Sequenzaufgaben, wie z. B. maschinelle Übersetzung, sowie für das unüberwachte Lernen von Repräsentationen für Sequenzen eingesetzt wurden. Im Gegensatz zu Sequenzen sind Mengen permutationsinvariant, und der vorgeschlagene Mengen-Auto-Codierer berücksichtigt diese Tatsache sowohl in Bezug auf die Eingabe als auch auf die Ausgabe des Modells.Auf der Eingabeseite adaptieren wir eine kürzlich eingeführte rekurrente neuronale Architektur unter Verwendung eines inhaltsbasierten Aufmerksamkeitsmechanismus. Wir trainieren das Modell auf synthetischen Datensätzen von Punktwolken und zeigen, dass sich die gelernten Repräsentationen reibungslos mit Übersetzungen in den Eingaben ändern, Distanzen in den Eingaben erhalten bleiben und dass die Mengengröße direkt dargestellt wird. Für eine Reihe schwieriger Klassifizierungsprobleme sind die Ergebnisse besser als die eines Modells, das die Permutationsinvarianz nicht berücksichtigt, insbesondere bei kleinen Trainingssätzen profitiert das Set-Aware-Modell vom unbeaufsichtigten Vortraining.
Deep Reinforcement Learning Algorithmen können komplexe Verhaltensfähigkeiten erlernen, aber die reale Anwendung dieser Methoden erfordert eine beträchtliche Menge an Erfahrung, die vom Agenten gesammelt werden muss.In praktischen Situationen, wie z.B. in der Robotik, beinhaltet dies, dass eine Aufgabe wiederholt versucht wird und die Umgebung zwischen jedem Versuch neu eingestellt wird.Allerdings sind nicht alle Aufgaben leicht oder automatisch umkehrbar.In der Praxis erfordert dieser Lernprozess beträchtliche menschliche Intervention. In dieser Arbeit schlagen wir eine autonome Methode für sicheres und effizientes Verstärkungslernen vor, die gleichzeitig eine Vorwärts- und eine Rückwärtsstrategie erlernt, wobei die Rückwärtsstrategie die Umgebung für einen nachfolgenden Versuch zurücksetzt.Durch das Erlernen einer Wertfunktion für die Rückwärtsstrategie können wir automatisch bestimmen, wann die Vorwärtsstrategie in einen nicht umkehrbaren Zustand eintritt, und so für unsichere Sicherheitsabbrüche sorgen.Unsere Experimente zeigen, dass die richtige Verwendung der Rückwärtsstrategie die Anzahl der manuellen Rücksetzungen, die zum Erlernen einer Aufgabe erforderlich sind, stark reduzieren kann und die Anzahl der unsicheren Aktionen, die zu nicht umkehrbaren Zuständen führen, verringern kann.
Es wurde argumentiert, dass aktuelle maschinelle Lernmodelle keinen gesunden Menschenverstand haben und daher mit Vorwissen hart kodiert werden müssen (Marcus, 2018).Hier zeigen wir überraschende Beweise, dass Sprachmodelle bereits lernen können, bestimmtes Wissen über den gesunden Menschenverstand zu erfassen.Unsere Schlüsselbeobachtung ist, dass ein Sprachmodell die Wahrscheinlichkeit einer beliebigen Aussage berechnen kann, und diese Wahrscheinlichkeit kann verwendet werden, um die Wahrhaftigkeit dieser Aussage zu bewerten.  Bei der Winograd Schema Challenge (Levesque et al., 2011) haben Sprachmodelle eine um 11 % höhere Genauigkeit als frühere überwachte Methoden auf dem neuesten Stand der Technik.Sprachmodelle können auch für die Aufgabe des Mining von Commonsense-Wissen auf ConceptNet fein abgestimmt werden, um einen F1-Score von 0,912 und 0,824 zu erreichen, was die bisher besten Ergebnisse übertrifft (Jastrzebskiet al., 2018).  Weitere Analysen zeigen, dass Sprachmodelle einzigartige Merkmale von Winograd-Schema-Kontexten entdecken können, die ohne explizite Überwachung über die richtigen Antworten entscheiden.
In vielen realen Lernszenarien sind Merkmale nur zu Kosten, die durch ein Budget beschränkt sind, erwerbbar.In diesem Papier schlagen wir einen neuartigen Ansatz für die kostensensitive Merkmalserfassung zur Vorhersagezeit vor.Die vorgeschlagene Methode erwirbt Merkmale inkrementell auf der Grundlage einer kontextbewussten Merkmalswertfunktion.Wir formulieren das Problem im Paradigma des Verstärkungslernens und führen eine Belohnungsfunktion ein, die auf dem Nutzen jedes Merkmals basiert. Der vorgeschlagene Ansatz ist vollständig online und lässt sich ohne weiteres auf Stream-Learning-Setups anwenden. Die Lösung wird an drei verschiedenen Datensätzen evaluiert, darunter der bekannte MNIST-Datensatz als Benchmark sowie zwei kostensensitive Datensätze: Die Ergebnisse zeigen, dass die vorgeschlagene Methode in der Lage ist, Merkmale effizient zu erfassen und genaue Vorhersagen zu machen.
In diesem Papier wird das Problem der Sequenzmodellierung mit Hilfe von Faltungsarchitekturen erneut aufgegriffen. Obwohl sowohl Faltungsarchitekturen als auch rekurrente Architekturen eine lange Geschichte in der Sequenzvorhersage haben, ist die derzeitige "Standard"-Denkweise in einem Großteil der Deep-Learning-Gemeinschaft, dass die allgemeine Sequenzmodellierung am besten mit rekurrenten Netzwerken gehandhabt wird. Das Ziel dieses Papiers ist es, diese Annahme zu hinterfragen. Konkret betrachten wir ein einfaches generisches temporales Faltungsnetzwerk (TCN), das Merkmale moderner ConvNet-Architekturen wie Dilatationen und Restverbindungen übernimmt. Wir zeigen, dass das TCN bei einer Vielzahl von Sequenzmodellierungsaufgaben, darunter viele, die häufig als Benchmarks für die Bewertung von rekurrenten Netzwerken verwendet werden, die grundlegenden RNN-Methoden (LSTMs, GRUs und Vanilla RNNs) und manchmal sogar hoch spezialisierte Ansätze übertrifft. Wir zeigen außerdem, dass der potenzielle Vorteil des "unendlichen Speichers", den RNNs gegenüber TCNs haben, in der Praxis weitgehend fehlt: TCNs weisen in der Tat längere effektive Historiengrößen auf als ihre rekurrenten Gegenstücke.  Insgesamt argumentieren wir, dass es an der Zeit sein könnte, ConvNets als Standardarchitektur für die Sequenzmodellierung (wieder) in Betracht zu ziehen.
Tiefe neuronale Netze eignen sich gut für die Annäherung komplizierter Funktionen, wenn sie mit Daten versorgt und mit Methoden des Gradientenabstiegs trainiert werden; gleichzeitig gibt es eine große Anzahl bestehender Funktionen, die programmgesteuert verschiedene Aufgaben präzise lösen, ohne dass ein Training erforderlich ist. In vielen Fällen ist es möglich, eine Aufgabe in eine Reihe von Funktionen zu zerlegen, von denen wir für einige ein neuronales Netz zum Erlernen der Funktionalität bevorzugen, während für andere die bevorzugte Methode darin besteht, bestehende Black-Box-Funktionen zu verwenden.Wir schlagen eine Methode für das End-to-End-Training eines neuronalen Basisnetzes vor, die Aufrufe bestehender Black-Box-Funktionen integriert. Wir tun dies, indem wir die Black-Box-Funktionalität durch ein differenzierbares neuronales Netz so approximieren, dass das Basisnetz während des End-to-End-Optimierungsprozesses mit der Black-Box-Funktionsschnittstelle übereinstimmt. Zur Inferenzzeit ersetzen wir den differenzierbaren Schätzer durch sein externes, nicht differenzierbares Black-Box-Gegenstück, so dass die Ausgabe des Basisnetzes mit den Eingangsargumenten der Black-Box-Funktion übereinstimmt. Mit diesem Paradigma ``Schätzen und Ersetzen'' trainieren wir ein neuronales Netz von Ende zu Ende, um die Eingabe der Black-Box-Funktion zu berechnen, während wir die Notwendigkeit von Zwischenetiketten eliminieren. Wir zeigen, dass durch die Nutzung der bestehenden präzisen Black-Box-Funktion während der Inferenz das integrierte Modell besser generalisiert als ein vollständig differenzierbares Modell und effizienter lernt als RL-basierte Methoden.
In diesem Papier wird ein neuer Algorithmus im Stil der Akteurskritik vorgeschlagen, der Dual Actor-Critic oder Dual-AC genannt wird.  Er wird auf prinzipielle Weise aus der dualen Lagrangeschen Form der Bellman-Optimalitätsgleichung abgeleitet, die als ein Zwei-Personen-Spiel zwischen dem Akteur und einer kritikähnlichen Funktion betrachtet werden kann, die als duale Kritik bezeichnet wird.  Im Vergleich zu seinen Akteur-Kritiker-Verwandten hat Dual-AC die gewünschte Eigenschaft, dass der Akteur und der duale Kritiker kooperativ aktualisiert werden, um dieselbe Zielfunktion zu optimieren, was einen transparenteren Weg für das Lernen des Kritikers bietet, der direkt mit der Zielfunktion des Akteurs verbunden ist.Wir bieten dann einen konkreten Algorithmus, der das Minimax-Optimierungsproblem effektiv lösen kann, indem wir Techniken des mehrstufigen Bootstrapping, der Pfadregularisierung und des stochastischen dualen Aufstiegsalgorithmus verwenden.Wir zeigen, dass der vorgeschlagene Algorithmus die modernsten Leistungen in mehreren Benchmarks erreicht.
Das Training eines Modells zur Durchführung einer Aufgabe erfordert in der Regel eine große Menge an Daten aus den Domänen, in denen die Aufgabe angewendet werden soll. Allerdings ist es oft der Fall, dass Daten in einigen Domänen reichlich vorhanden sind, in anderen jedoch nur spärlich. CycleGAN ist ein leistungsfähiges Framework, das effizient lernt, Eingaben von einer Domäne auf eine andere abzubilden, indem es adversariales Training und eine Zykluskonsistenzbeschränkung verwendet. Der konventionelle Ansatz, Zykluskonsistenz durch Rekonstruktion zu erzwingen, kann jedoch in Fällen, in denen eine oder mehrere Domänen begrenzte Trainingsdaten haben, zu restriktiv sein. Dieses aufgabenspezifische Modell lockert sowohl die Zyklus-Konsistenz-Beschränkung als auch die Rolle des Diskriminators während des Trainings und dient als erweiterte Informationsquelle für das Erlernen des Mappings.Wir erforschen die Anpassung in Sprach- und visuellen Domänen in einem ressourcenarmen, überwachten Umfeld.In Sprachdomänen übernehmen wir ein Spracherkennungsmodell aus jeder Domäne als aufgabenspezifisches Modell. Unser Ansatz verbessert die absolute Leistung der Spracherkennung um 2 % für weibliche Sprecher im TIMIT-Datensatz, wo die Mehrheit der Trainingsproben von männlichen Stimmen stammt. In der ressourcenarmen visuellen Domänenanpassung zeigen die Ergebnisse, dass unser Ansatz die absolute Leistung um 14 % und 4 % verbessert, wenn er SVHN an MNIST bzw. umgekehrt anpasst, was die unbeaufsichtigten Domänenanpassungsmethoden übertrifft, die eine ressourcenreiche, nicht beschriftete Zieldomäne erfordern. 
Der Erfolg populärer Algorithmen für Deep Reinforcement Learning, wie Policy-Gradients und Q-Learning, hängt stark von der Verfügbarkeit eines informativen Belohnungssignals zu jedem Zeitschritt des sequentiellen Entscheidungsprozesses ab. Wenn Belohnungen während einer Episode nur spärlich verfügbar sind oder ein belohnendes Feedback erst nach Beendigung der Episode bereitgestellt wird, schneiden diese Algorithmen aufgrund der schwierigen Zuordnung von Credits suboptimal ab. Alternativ dazu erfordern trajektorienbasierte Optimierungsmethoden, wie die Cross-Entropie-Methode und Evolutionsstrategien, keine Belohnungen pro Zeitschritt, aber es hat sich gezeigt, dass sie unter der hohen Komplexität von Stichproben leiden, da sie die zeitliche Natur des Problems komplett außer Acht lassen.Die Verbesserung der Effizienz von RL-Algorithmen bei realen Problemen mit spärlichen oder episodischen Belohnungen ist daher ein dringender Bedarf. In dieser Arbeit stellen wir einen Selbstimitations-Lernalgorithmus vor, der die spärlichen und episodischen Belohnungen gut ausnutzt und erforscht. Wir betrachten jede Politik als eine Zustands-Aktions-Besuchs-Verteilung und formulieren die Politik-Optimierung als ein Divergenz-Minimierungsproblem. Wir zeigen, dass dieses Divergenz-Minimierungsproblem mit Jensen-Shannon-Divergenz in einen Politik-Gradienten-Algorithmus mit geformten Belohnungen, die aus Erfahrungswiederholungen gelernt wurden, reduziert werden kann. Experimentelle Ergebnisse zeigen, dass unser Algorithmus funktioniert vergleichbar mit bestehenden Algorithmen in Umgebungen mit dichten Belohnungen, und deutlich besser in Umgebungen mit spärlichen und episodischen Belohnungen.Wir diskutieren dann Grenzen der Selbst-Imitation Lernen, und schlagen vor, sie zu lösen, indem sie Stein variational Politik Gradient Descent mit dem Jensen-Shannon-Kernel zu lernen, mehrere verschiedene policies.We demonstrieren ihre Wirksamkeit auf eine anspruchsvolle Variante der kontinuierlichen Kontrolle MuJoCo Fortbewegung Aufgaben.
Wir untersuchen die genauen Mechanismen, die es dem Autoencoder ermöglichen, eine einfache geometrische Form, die Scheibe, zu kodieren und zu dekodieren.In dieser sorgfältig kontrollierten Umgebung sind wir in der Lage, die spezifische Form der optimalen Lösung des Minimierungsproblems des Trainingsschritts zu beschreiben.Wir zeigen, dass der Autoencoder sich dieser Lösung während des Trainings tatsächlich annähert.Zweitens identifizieren wir ein klares Versagen in der Generalisierungskapazität des Autoencoders, nämlich seine Unfähigkeit, Daten zu interpolieren. Angesichts der großen Aufmerksamkeit, die in letzter Zeit der generativen Kapazität neuronaler Netze gewidmet wurde, sind wir der Meinung, dass die eingehende Untersuchung einfacher geometrischer Fälle ein gewisses Licht auf den Generierungsprozess wirft und einen experimentellen Aufbau mit minimalen Anforderungen für komplexere Architekturen bieten kann. 
Wir stellen eine einfache Idee vor, die es ermöglicht, einen Sprecher in einer bestimmten Sprache aufzunehmen und seine Stimme in anderen Sprachen zu synthetisieren, die er vielleicht nicht einmal kennt. Diese Techniken eröffnen eine breite Palette von potenziellen Anwendungen wie sprachübergreifende Kommunikation, Sprachenlernen oder automatische Video-Synchronisation. Der Hauptunterschied besteht darin, dass wir nicht auf Zeichen oder Phoneme konditionieren, die für eine bestimmte Sprache spezifisch sind, sondern auf eine gemeinsame phonetische Repräsentation, die für alle Sprachen universell ist.Diese sprachübergreifende phonetische Repräsentation von Text ermöglicht es, Sprache in jeder Sprache zu synthetisieren, während die stimmlichen Eigenschaften des ursprünglichen Sprechers erhalten bleiben.Darüber hinaus zeigen wir, dass die Feinabstimmung der Gewichte unseres Modells es uns ermöglicht, unsere Ergebnisse auf Sprecher außerhalb des Trainingsdatensatzes zu erweitern.
Das Ziel des Nachahmungslernens (IL) ist es, den Lernenden in die Lage zu versetzen, das Verhalten von Experten zu imitieren, wenn diese es vormachen. In letzter Zeit hat das generative adversarische Nachahmungslernen (GAIL) bedeutende Fortschritte beim IL für komplexe, kontinuierliche Aufgaben gezeigt, aber GAIL und seine Erweiterungen erfordern eine große Anzahl von Umgebungsinteraktionen während des Trainings. Je mehr eine IL-Methode vom Lernenden verlangt, mit der Umgebung zu interagieren, um sie besser zu imitieren, desto mehr Trainingszeit wird benötigt und desto mehr Schaden wird der Umgebung und dem Lernenden selbst zugefügt.Wir glauben, dass IL-Algorithmen besser auf reale Probleme anwendbar wären, wenn die Anzahl der Interaktionen reduziert werden könnte. Unser Algorithmus besteht im Wesentlichen aus drei Änderungen an den bestehenden Methoden des adversen Nachahmungslernens (AIL): (a) Übernahme des Off-Policy-Actor-Critic-Algorithmus (Off-PAC) zur Optimierung der Lernerpolitik, (b) Schätzung des Zustands-Aktionswerts unter Verwendung von Off-Policy-Stichproben ohne Lernen von Belohnungsfunktionen und (c) Darstellung der stochastischen Politikfunktion, so dass ihre Ausgaben begrenzt sind. Experimentelle Ergebnisse zeigen, dass unser Algorithmus konkurrenzfähige Ergebnisse mit GAIL erzielt und gleichzeitig die Interaktionen mit der Umgebung erheblich reduziert.
Der vorherrschende Ansatz zur unbeaufsichtigten "Stilübertragung" in Texten basiert auf der Idee, eine latente Repräsentation zu erlernen, die unabhängig von den Attributen ist, die ihren "Stil" spezifizieren. In diesem Papier zeigen wir, dass diese Bedingung nicht notwendig ist und in der Praxis nicht immer erfüllt wird, selbst bei einem gegnerischen Domänen-Training, das explizit darauf abzielt, solche entkoppelten Repräsentationen zu erlernen. Wir schlagen daher ein neues Modell vor, das mehrere Variationsfaktoren in Textdaten kontrolliert, wobei diese Bedingung für die Entflechtung durch einen einfacheren Mechanismus auf der Grundlage der Rückübersetzung ersetzt wird, Unsere Experimente zeigen, dass das vollständig verschränkte Modell bessere Generationen erzeugt, selbst wenn es an neuen und anspruchsvolleren Benchmarks getestet wird, die Bewertungen mit mehreren Sätzen und mehreren Attributen umfassen.
Vanilla RNN mit ReLU-Aktivierung haben eine einfache Struktur, die einer systematischen Analyse und Interpretation dynamischer Systeme zugänglich ist, aber sie leiden unter dem Problem der explodierenden bzw. verschwindenden Gradienten. Jüngste Versuche, diese Einfachheit beizubehalten und gleichzeitig das Gradientenproblem zu lindern, basieren auf geeigneten Initialisierungsschemata oder orthogonalen/unitären Beschränkungen der RNN-Rekursivitätsmatrix, was jedoch mit Einschränkungen der Ausdruckskraft in Bezug auf dynamische Systemphänomene wie Chaos oder Multistabilität einhergeht. Hier schlagen wir stattdessen ein Regularisierungsschema vor, das einen Teil des latenten Unterraums des RNN in Richtung einer Line-Attraktor-Konfiguration verschiebt, die ein langes Kurzzeitgedächtnis und beliebig langsame Zeitskalen ermöglicht. Wir zeigen, dass unser Ansatz bei einer Reihe von Benchmarks wie den sequentiellen MNIST- oder Multiplikationsproblemen hervorragend abschneidet und die Rekonstruktion dynamischer Systeme mit sehr unterschiedlichen Zeitskalen ermöglicht.
Im Bereich der Generative Adversarial Networks (GANs) bleibt die Frage, wie man eine stabile Trainingsstrategie entwickelt, ein offenes Problem.Wasserstein-GANs haben die Stabilität gegenüber den ursprünglichen GANs durch die Einführung der Wasserstein-Distanz erheblich verbessert, sind aber immer noch instabil und anfällig für eine Vielzahl von Fehlermöglichkeiten. In diesem Beitrag wird ein allgemeiner Rahmen namens Wasserstein-Bounded GAN (WBGAN) vorgestellt, der eine große Familie von WGAN-basierten Ansätzen verbessert, indem er einfach eine Obergrenzenbeschränkung zum Wasserstein-Term hinzufügt.
Generative Modelle wie Variational Auto Encoders (VAEs) und Generative Adversarial Networks (GANs) werden typischerweise für eine feste Prioritätsverteilung im latenten Raum trainiert, wie z.B. Uniform oder Gaussian.Nachdem ein trainiertes Modell erhalten wurde, kann man den Generator in verschiedenen Formen zur Erforschung und zum Verständnis abtasten, wie z.B. Interpolation zwischen zwei Proben, Abtasten in der Nähe einer Probe oder Erforschung von Unterschieden zwischen einem Paar von Proben, die auf eine dritte Probe angewendet werden. In diesem Papier zeigen wir, dass die bisher in der Literatur verwendeten Latent-Space-Operationen zu einer Verteilungsfehlanpassung zwischen den resultierenden Ausgaben und der Prior-Verteilung, auf der das Modell trainiert wurde, führen und schlagen daher die Verwendung von Distribution-Matching-Transport-Maps vor, um sicherzustellen, dass solche Latent-Space-Operationen die Prior-Verteilung bewahren, während die ursprüngliche Operation nur minimal verändert wird. Unsere experimentellen Ergebnisse bestätigen, dass die vorgeschlagenen Operationen im Vergleich zu den ursprünglichen Operationen eine höhere Qualität der Proben liefern.
Neuronale Programmeinbettungen haben sich in letzter Zeit als vielversprechend für eine Vielzahl von Programmanalyseaufgaben erwiesen, darunter Programmsynthese, Programmreparatur, Codevervollständigung und Fehlerlokalisierung. Die meisten bestehenden Programmeinbettungen basieren jedoch auf syntaktischen Merkmalen von Programmen, wie Token-Sequenzen oder abstrakten Syntaxbäumen. Im Gegensatz zu Bildern und Texten verfügt ein Programm über eine gut definierte Semantik, die nur durch die Betrachtung der Syntax schwer zu erfassen ist (d.h. syntaktisch ähnliche Programme können ein sehr unterschiedliches Laufzeitverhalten aufweisen), wodurch syntaxbasierte Programmeinbettungen grundlegend eingeschränkt sind.Wir schlagen eine neuartige semantische Programmeinbettung vor, die aus Programmausführungsspuren gelernt wird. Unsere wichtigste Erkenntnis ist, dass Programmzustände, die als sequenzielle Tupel von Live-Variablenwerten ausgedrückt werden, nicht nur die Programmsemantik genauer erfassen, sondern auch eine natürlichere Modellierung für rekurrente neuronale Netze bieten.Wir evaluieren verschiedene syntaktische und semantische Programmeinbettungen anhand der Aufgabe, die Arten von Fehlern zu klassifizieren, die Studenten in ihren Beiträgen zu einem einführenden Programmierkurs und auf der CodeHunt-Bildungsplattform machen. Unsere Evaluierungsergebnisse zeigen, dass die semantischen Programmeinbettungen die syntaktischen Programmeinbettungen, die auf Token-Sequenzen und abstrakten Syntaxbäumen basieren, deutlich übertreffen. Darüber hinaus erweitern wir ein suchbasiertes Programmreparatursystem mit Vorhersagen aus unserer semantischen Einbettung und zeigen eine deutlich verbesserte Sucheffizienz.
In diesem Beitrag schlagen wir eine Verallgemeinerung des BN-Algorithmus, die abnehmende Batch-Normalisierung (DBN), vor, bei der wir die BN-Parameter in Form eines abnehmenden gleitenden Durchschnitts aktualisieren.Batch-Normalisierung (BN) ist sehr effektiv bei der Beschleunigung der Konvergenz der Trainingsphase eines neuronalen Netzes, so dass sie zu einer gängigen Praxis geworden ist. Der von uns vorgeschlagene DBN-Algorithmus behält die allgemeine Struktur des ursprünglichen BN-Algorithmus bei, führt aber eine gewichtete Durchschnittsaktualisierung für einige trainierbare Parameter ein. Wir bieten eine Analyse der Konvergenz des DBN-Algorithmus, die zu einem stationären Punkt in Bezug auf die trainierbaren Parameter konvergiert.Unsere Analyse kann leicht für die ursprüngliche BN-Algorithmus verallgemeinert werden, indem einige Parameter konstant.Nach bestem Wissen der Autoren, ist diese Analyse die erste ihrer Art für die Konvergenz mit Batch-Normalisierung eingeführt.Wir analysieren ein Zwei-Schicht-Modell mit beliebigen Aktivierungsfunktion. Die primäre Herausforderung der Analyse ist die Tatsache, dass einige Parameter durch den Gradienten aktualisiert werden, während andere nicht aktualisiert werden. Die Konvergenzanalyse gilt für jede Aktivierungsfunktion, die unsere allgemeinen Annahmen erfüllt, und wir zeigen die notwendigen und ausreichenden Bedingungen für die Schrittweiten und die abnehmenden Gewichte, um die Konvergenz zu gewährleisten. In den numerischen Experimenten verwenden wir komplexere Modelle mit mehr Schichten und ReLU-Aktivierung und stellen fest, dass DBN den ursprünglichen BN-Algorithmus in den Datensätzen Imagenet, MNIST, NI und CIFAR-10 mit angemessen komplexen FNN- und CNN-Modellen übertrifft.
Generative Modelle wie Variational Auto Encoders (VAEs) und Generative Adversarial Networks (GANs) werden typischerweise für eine feste Prioritätsverteilung im latenten Raum trainiert, wie z.B. Uniform oder Gaussian. Nachdem man ein trainiertes Modell erhalten hat, kann man den Generator in verschiedenen Formen abtasten, um ihn zu erforschen und zu verstehen, z. B. durch Interpolation zwischen zwei Abtastwerten, Abtasten in der Nähe eines Abtastwerts oder Erforschen von Unterschieden zwischen einem Paar von Abtastwerten, die auf einen dritten Abtastwert angewandt werden.2 Die bisher in der Literatur häufig verwendeten Operationen im latenten Raum führen jedoch zu einer Verteilungsfehlanpassung zwischen den resultierenden Ergebnissen und der priorisierten Verteilung, auf der das Modell trainiert wurde. In diesem Papier schlagen wir einen Rahmen für die Modifizierung der latenten Raum Operationen, so dass die Verteilung mismatch ist vollständig eliminiert.Unser Ansatz basiert auf optimale Transport-Karten, die die latenten Raum Operationen, so dass sie vollständig mit der vorherigen Verteilung, während minimal modifizieren die ursprüngliche operation.Our angepasst Operationen sind leicht für die häufig verwendeten Operationen und Verteilungen erhalten und erfordern keine Anpassung an die Ausbildung procedure.
Das Problem des Aufbaus eines kohärenten und nicht monotonen Konversationsagenten mit angemessenem Diskurs und Abdeckung ist immer noch ein offenes Forschungsgebiet. Die derzeitigen Architekturen kümmern sich nur um semantische und kontextuelle Informationen für eine gegebene Anfrage und berücksichtigen nicht vollständig syntaktisches und externes Wissen, das für die Generierung von Antworten in einem Chit-Chat-System entscheidend ist. Um dieses Problem zu überwinden, schlagen wir eine End-to-End-Multi-Stream Deep-Learning-Architektur vor, die einheitliche Einbettungen für Anfrage-Antwort-Paare erlernt, indem sie kontextuelle Informationen aus Speichernetzwerken und syntaktische Informationen durch die Einbeziehung von Graph Convolution Networks (GCN) über deren Dependency Parse nutzt. Ein Teil dieses Netzwerks nutzt auch Transfer-Lernen durch Vortraining eines bidirektionalen Transformators, um semantische Repräsentationen für jeden Eingabesatz zu extrahieren, und bezieht externes Wissen durch die Nachbarschaft der Entitäten aus einer Wissensdatenbank (KB) ein.
Das Action Schema Network (ASNet) ist ein neuerer Beitrag zur Planung, der Deep Learning und neuronale Netze verwendet, um verallgemeinerte Strategien für probabilistische Planungsprobleme zu erlernen. ASNets eignen sich gut für Probleme, bei denen lokales Wissen über die Umgebung genutzt werden kann, um die Leistung zu verbessern, können aber nicht auf Probleme verallgemeinert werden, für die sie nicht trainiert wurden. Monte-Carlo Tree Search (MCTS) ist ein vorwärtsgerichteter Suchalgorithmus für die optimale Entscheidungsfindung, der Simulationen durchführt, um einen Suchbaum zu erstellen und die Werte jedes Zustands zu schätzen. Durch die Kombination von ASNets mit MCTS sind wir in der Lage, die Fähigkeit eines ASNet zu verbessern, über die Verteilung der Probleme hinaus zu verallgemeinern, für die es trainiert wurde, und die Navigation im Suchraum durch MCTS zu verbessern.
Moderne tiefe neuronale Netze können eine hohe Genauigkeit erreichen, wenn die Trainingsverteilung und die Testverteilung identisch verteilt sind, aber diese Annahme wird in der Praxis häufig verletzt.Wenn die Trainings- und Testverteilungen nicht übereinstimmen, kann die Genauigkeit einbrechen.Derzeit gibt es nur wenige Techniken, die die Robustheit gegenüber unvorhergesehenen Datenverschiebungen verbessern, die während des Einsatzes auftreten.In dieser Arbeit schlagen wir eine Technik vor, um die Robustheit und die Unsicherheitsschätzungen von Bildklassifikatoren zu verbessern. Wir schlagen AugMix vor, eine Datenverarbeitungstechnik, die einfach zu implementieren ist, nur einen begrenzten Rechenaufwand verursacht und den Modellen hilft, unvorhergesehenen Störungen zu widerstehen.AugMix verbessert die Robustheit und die Unsicherheitsmaße bei anspruchsvollen Bildklassifizierungs-Benchmarks erheblich und schließt die Lücke zwischen früheren Methoden und der bestmöglichen Leistung in einigen Fällen um mehr als die Hälfte.
Da das Sammeln von Daten schwierig und teuer ist, schlagen wir vor, diesen Prozess zu automatisieren, indem wir automatisch Fingersätze aus öffentlich zugänglichen Videos und MIDI-Dateien extrahieren, indem wir Computer-Vision-Techniken verwenden. Wenn wir diesen Prozess auf 90 Videos laufen lassen, erhalten wir den größten Datensatz für Klaviergriffe mit mehr als 150.000 Noten.Wir zeigen, dass wir die besten Ergebnisse erzielen, wenn wir ein zuvor vorgeschlagenes Modell für automatische Klaviergriffe auf unserem Datensatz laufen lassen und es dann auf manuell beschriftete Klaviergriffe abstimmen. Zusätzlich zu der Methode der Fingerextraktion stellen wir auch eine neuartige Methode vor, um Deep-Learning-Computer-Vision-Modelle auf Daten außerhalb der Domäne zu übertragen, indem wir eine Feinabstimmung auf Daten außerhalb der Domäne vornehmen, die von einem Generative Adversarial Network (GAN) vorgeschlagen wurden. zur Demonstration veröffentlichen wir anonym eine Visualisierung der Ergebnisse unseres Prozesses für ein einzelnes Video auf https://youtu.be/Gfs1UWQhr5Q.
Die Domänenanpassung bezieht sich auf das Problem, markierte Daten in einer Quelldomäne zu nutzen, um ein genaues Modell in einer Zieldomäne zu erlernen, in der nur wenige oder gar keine Markierungen vorhanden sind. Ein neuerer Ansatz, um eine gemeinsame Darstellung der beiden Domänen zu finden, ist das domänenadversarische Training (Ganin & Lempitsky, 2015), bei dem versucht wird, einen Merkmalsextraktor zu induzieren, der die Verteilungen von Quell- und Zielmerkmalen in einem Merkmalsraum anpasst. Allerdings stößt das adversarische Domänen-Training auf zwei kritische Einschränkungen:1) wenn die Merkmalsextraktionsfunktion eine hohe Kapazität hat, ist die Anpassung der Merkmalsverteilung eine schwache Einschränkung,2) bei nicht-konservativer Domänenanpassung (bei der kein einzelner Klassifikator sowohl in der Quell- als auch in der Zieldomäne gut abschneiden kann) schadet das Training des Modells, um in der Quelldomäne gut abzuschneiden, der Leistung in der Zieldomäne.In diesem Papier gehen wir diese Probleme durch die Linse der Cluster-Annahme an, d.h., Wir schlagen zwei neuartige und verwandte Modelle vor:1) das Virtual Adversarial Domain Adaptation (VADA)-Modell, das adversariales Domänen-Training mit einem Straf-Term kombiniert, der die Verletzung der Cluster-Annahme bestraft;2) das Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T)-Modell, das das VADA-Modell als Initialisierung verwendet und natürliche Gradientenschritte einsetzt, um die Verletzung der Cluster-Annahme weiter zu minimieren. Umfassende empirische Ergebnisse zeigen, dass die Kombination dieser beiden Modelle die State-of-the-Art-Leistung bei den Anpassungsbenchmarks für Ziffern, Verkehrszeichen und Wi-Fi-Erkennung deutlich verbessert.
In diesem Papier schlagen wir Continuous Graph Flow vor, eine generative, auf kontinuierlichem Fluss basierende Methode, die darauf abzielt, komplexe Verteilungen von graph-strukturierten Daten zu modellieren.  Einmal gelernt, kann das Modell auf einen beliebigen Graphen angewendet werden und definiert eine Wahrscheinlichkeitsdichte über die Zufallsvariablen, die durch den Graphen repräsentiert werden, und wird als gewöhnliches Differentialgleichungssystem mit gemeinsamen und wiederverwendbaren Funktionen formuliert, die über die Graphen arbeiten.  Diese Klasse von Modellen bietet mehrere Vorteile: eine flexible Darstellung, die auf variable Datendimensionen verallgemeinert werden kann; die Fähigkeit, Abhängigkeiten in komplexen Datenverteilungen zu modellieren; reversibel und speichereffizient; und eine exakte und effiziente Berechnung der Wahrscheinlichkeit der Daten.Wir demonstrieren die Effektivität unseres Modells bei einer Reihe von Generierungsaufgaben in verschiedenen Domänen: Graphengenerierung, Bildpuzzlegenerierung und Layoutgenerierung aus Szenegraphen.Unser vorgeschlagenes Modell erreicht eine deutlich bessere Leistung im Vergleich zu State-of-the-Art-Modellen.
In dieser Arbeit untersuchen wir die Informationsengpass-Theorie (IB) des tiefen Lernens, die drei spezifische Behauptungen aufstellt: Erstens, dass tiefe Netzwerke zwei unterschiedliche Phasen durchlaufen, die aus einer anfänglichen Anpassungsphase und einer anschließenden Kompressionsphase bestehen; zweitens, dass die Kompressionsphase in kausalem Zusammenhang mit der hervorragenden Generalisierungsleistung tiefer Netzwerke steht; und drittens, dass die Kompressionsphase aufgrund des diffusionsähnlichen Verhaltens des stochastischen Gradientenabstiegs auftritt. Durch eine Kombination von analytischen Ergebnissen und Simulationen zeigen wir, dass die Trajektorie der Informationsebene in erster Linie eine Funktion der verwendeten neuronalen Nichtlinearität ist: Doppelseitige sättigende Nichtlinearitäten wie tanh führen zu einer Kompressionsphase, wenn die neuronalen Aktivierungen in den Sättigungsbereich eintreten, während lineare Aktivierungsfunktionen und einseitige sättigende Nichtlinearitäten wie die weit verbreitete ReLU dies nicht tun. Darüber hinaus stellen wir fest, dass es keinen offensichtlichen kausalen Zusammenhang zwischen Kompression und Generalisierung gibt: Netze, die nicht komprimieren, sind immer noch zur Generalisierung fähig und umgekehrt. Als Nächstes zeigen wir, dass die Kompressionsphase, wenn sie existiert, nicht durch Stochastik im Training entsteht, indem wir demonstrieren, dass wir die IB-Befunde replizieren können, indem wir vollständigen Batch-Gradientenabstieg statt stochastischen Gradientenabstieg verwenden. Schließlich zeigen wir, dass, wenn eine Eingabedomäne aus einer Teilmenge von aufgabenrelevanten und aufgabenirrelevanten Informationen besteht, die verborgenen Repräsentationen die aufgabenirrelevanten Informationen komprimieren, obwohl die Gesamtinformationen über die Eingabe mit der Trainingszeit monoton zunehmen können, und dass diese Kompression gleichzeitig mit dem Anpassungsprozess und nicht während einer nachfolgenden Kompressionsperiode stattfindet.
In den letzten vier Jahren hat sich gezeigt, dass neuronale Netze anfällig für unerwünschte Bilder sind: Gezielte, aber nicht wahrnehmbare Bildstörungen führen zu drastisch unterschiedlichen Vorhersagen.Wir zeigen, dass die Anfälligkeit für unerwünschte Bilder mit den Gradienten des Trainingsziels zunimmt, wenn man sie als Funktion der Eingaben betrachtet. Für die meisten aktuellen Netzarchitekturen zeigen wir, dass die L1-Norm dieser Gradienten mit der Quadratwurzel aus der Größe der Eingaben wächst, so dass diese Netze mit zunehmender Bildgröße immer anfälliger werden.Unsere Beweise beruhen auf der Gewichtsverteilung des Netzes bei der Initialisierung, aber umfangreiche Experimente bestätigen, dass unsere Schlussfolgerungen auch nach dem üblichen Training gelten.
Wir stellen die Amortized Proximal Optimization (APO) vor, die davon ausgeht, dass jeder Optimierungsschritt ein proximales Ziel annähernd minimieren sollte (ähnlich wie bei der natürlichen Gradienten- und Trust Region Policy-Optimierung). Wir zeigen, dass eine idealisierte Version von APO (bei der ein Orakel das Proximalziel exakt minimiert) globale Konvergenz zum stationären Punkt und lokale Konvergenz zweiter Ordnung zum globalen Optimum für neuronale Netze erreicht.APO verursacht minimalen Rechenaufwand. Wir experimentieren mit der Verwendung von APO zur Online-Anpassung einer Vielzahl von Optimierungshyperparametern während des Trainings, einschließlich (möglicherweise schichtspezifischer) Lernraten, Dämpfungskoeffizienten und Gradienten-Varianz-Exponenten. Für eine Vielzahl von Netzarchitekturen und Optimierungsalgorithmen (einschließlich SGD, RMSprop und K-FAC) zeigen wir, dass APO bei minimaler Abstimmung mit sorgfältig abgestimmten Optimierern konkurrieren kann.
Dichte Wortvektoren haben sich in den letzten Jahren in vielen nachgelagerten NLP-Aufgaben bewährt, allerdings sind die Dimensionen solcher Einbettungen nicht leicht zu interpretieren: Von den d-Dimensionen eines Wortvektors können wir nicht verstehen, was hohe oder niedrige Werte bedeuten.Bisherige Ansätze, die sich mit diesem Problem befassen, haben sich hauptsächlich darauf konzentriert, entweder spärliche/nicht-negative eingeschränkte Worteinbettungen zu trainieren oder vortrainierte Standard-Worteinbettungen nachzubearbeiten. Wir verwenden eine neuartige Eigenvektor-Analysemethode, die von der Zufallsmatrixtheorie inspiriert ist, und zeigen, dass sich semantisch kohärente Gruppen nicht nur im Zeilenraum, sondern auch im Spaltenraum bilden, was es uns ermöglicht, einzelne Wortvektordimensionen als vom Menschen interpretierbare semantische Merkmale zu betrachten.
Neuronale Netze im Gehirn und in neuromorphen Chips verleihen Systemen die Fähigkeit, mehrere kognitive Aufgaben auszuführen. Beide Arten von Netzen sind jedoch einer Vielzahl physikalischer Störungen ausgesetzt, die von der Beschädigung von Kanten des Netzes bis hin zur vollständigen Löschung von Knoten reichen und schließlich zum Ausfall des Netzes führen können. Eine entscheidende Frage ist, wie sich die rechnerischen Eigenschaften neuronaler Netze als Reaktion auf die Beschädigung von Knoten verändern und ob es Strategien gibt, diese Netze zu reparieren, um den Leistungsabfall zu kompensieren. Hier untersuchen wir die Schadensreaktionscharakteristika von zwei Klassen neuronaler Netze, nämlich mehrschichtige Perzeptronen (MLPs) und faltige neuronale Netze (CNNs), die für die Klassifizierung von Bildern aus MNIST- bzw. CIFAR-10-Datensätzen trainiert wurden. Der Rahmen beinhaltet die Definition von Schadens- und Reparaturoperatoren, um die Verlustlandschaft des neuronalen Netzes dynamisch zu durchqueren, mit dem Ziel, die hervorstechenden geometrischen Merkmale abzubilden.Mit dieser Strategie entdecken wir Merkmale, die pfadverbundenen Attraktoren in der Verlustlandschaft ähneln. Wir stellen außerdem fest, dass ein dynamisches Wiederherstellungsschema, bei dem Netze ständig beschädigt und repariert werden, eine Gruppe von Netzen hervorbringt, die gegen Schäden resistent sind, da sie schnell gerettet werden können.Unsere Arbeit zeigt im Großen und Ganzen, dass wir fehlertolerante Netze entwerfen können, indem wir während der Beschädigung konsequent Online-Umschulung für Echtzeitanwendungen in der Biologie und im maschinellen Lernen anwenden.
Automatische Frage Generation von Absätzen ist ein wichtiges und anspruchsvolles Problem, vor allem wegen der langen Kontext von paragraphs.In diesem Papier, wir vorschlagen und untersuchen zwei hierarchische Modelle für die Aufgabe der Frage Generation von paragraphs.Specifically, schlagen wir(a) eine neuartige hierarchische BiLSTM Modell mit selektiver Aufmerksamkeit und(b) eine neuartige hierarchische Transformer Architektur, die beide lernen hierarchische Repräsentationen von Absätzen. Während die Einführung des Aufmerksamkeitsmechanismus dem hierarchischen BiLSTM-Modell zugute kommt, schneidet der hierarchische Transformer mit seinen inhärenten Aufmerksamkeits- und Positionskodierungsmechanismen ebenfalls besser ab als das flache Transformer-Modell.Wir haben eine empirische Evaluation auf den weit verbreiteten SQuAD- und MS MARCO-Datensätzen unter Verwendung von Standardmetriken durchgeführt. Die Ergebnisse zeigen die allgemeine Effektivität der hierarchischen Modelle gegenüber ihren flachen Gegenstücken. Qualitativ sind unsere hierarchischen Modelle in der Lage, flüssige und relevante Fragen zu generieren.
Interne Informationen über das Modell, wie z. B. die Architektur, das Optimierungsverfahren oder die Trainingsdaten, werden nicht explizit offengelegt, da sie geschützte Informationen enthalten oder das System angreifbarer machen könnten. Wir zeigen, dass die aufgedeckten internen Informationen helfen, effektivere gegnerische Beispiele gegen das Black-Box-Modell zu generieren, und dass diese Technik für einen besseren Schutz privater Inhalte vor automatischen Erkennungsmodellen mit Hilfe von gegnerischen Beispielen verwendet werden kann.Unsere Arbeit zeigt, dass es tatsächlich schwierig ist, eine Grenze zwischen White-Box- und Black-Box-Modellen zu ziehen.
Ein neuartiger Ansatz, der Variationsinferenz für das Lernen der Belohnungsfunktion verwendet, wird in dieser Forschungsarbeit vorgeschlagen. Mit dieser Technik wird die schwer zu fassende posteriore Verteilung der kontinuierlichen latenten Variablen (in diesem Fall die Belohnungsfunktion) analytisch angenähert, um so nah wie möglich an der vorherigen Überzeugung zu sein, während versucht wird, den zukünftigen Zustand zu rekonstruieren, der durch den aktuellen Zustand und die aktuelle Aktion bedingt ist. Die Belohnungsfunktion wird mit Hilfe eines bekannten tiefen generativen Modells abgeleitet, das als Conditional Variational Auto-encoder (CVAE) mit Wasserstein-Verlustfunktion bekannt ist und daher als Conditional Wasserstein Auto-encoder-IRL (CWAE-IRL) bezeichnet wird, der als eine Kombination aus Rückwärts- und Vorwärtsinferenz analysiert werden kann. Experimentelle Ergebnisse auf Standard-Benchmarks wie Objectworld und Pendulum zeigen, dass der vorgeschlagene Algorithmus die latente Belohnungsfunktion in komplexen, hochdimensionalen Umgebungen effektiv erlernen kann.
Das Travelling Salesman Problem (TSP) ist ein bekanntes kombinatorisches Optimierungsproblem mit einer Vielzahl von realen Anwendungen, das wir durch die Einbeziehung von Methoden des maschinellen Lernens und die Nutzung der variablen Nachbarschafts-Suchstrategie angehen, wobei der Suchprozess als Markov-Entscheidungsprozess (MDP) betrachtet wird, bei dem eine lokale Suche mit zwei Optionen verwendet wird, um innerhalb einer kleinen Nachbarschaft zu suchen, während eine Monte-Carlo-Baum-Suchmethode (MCTS) (die durch Simulations-, Auswahl- und Backpropagation-Schritte iteriert) verwendet wird, um eine Reihe von gezielten Aktionen innerhalb einer erweiterten Nachbarschaft zu testen. Dieses neue Paradigma unterscheidet sich deutlich von den bestehenden Paradigmen des maschinellen Lernens (ML) zur Lösung des TSP, die entweder ein durchgängiges ML-Modell verwenden oder einfach traditionelle Techniken nach ML für die Nachoptimierung anwenden.Experimente auf der Grundlage von zwei öffentlichen Datensätzen zeigen, dass unser Ansatz alle bestehenden lernbasierten TSP-Algorithmen in Bezug auf die Leistung klar dominiert, was sein hohes Potenzial für das TSP demonstriert.
Trotz dieses Fortschritts sind modernste Ansätze noch immer nicht in der Lage, komplexe globale Strukturen in Daten zu erfassen. So enthalten Bilder von Gebäuden typischerweise räumliche Muster wie sich in regelmäßigen Abständen wiederholende Fenster; modernste generative Methoden können diese Strukturen nicht ohne weiteres reproduzieren.Wir schlagen vor, dieses Problem zu lösen, indem wir Programme, die globale Strukturen repräsentieren, in das generative Modell einbeziehen, Darüber hinaus schlagen wir einen Rahmen für das Erlernen dieser Modelle vor, indem wir die Programmsynthese nutzen, um Trainingsdaten zu generieren. Sowohl bei synthetischen als auch bei realen Daten zeigen wir, dass unser Ansatz wesentlich besser ist als der Stand der Technik, sowohl bei der Erzeugung als auch bei der Vervollständigung von Bildern, die globale Strukturen enthalten.
Das Erlernen von Steuerungsrichtlinien bei Roboteraufgaben erfordert eine große Anzahl von Interaktionen aufgrund von kleinen Lernraten, Beschränkungen der Aktualisierungen oder unbekannten Einschränkungen. Im Gegensatz dazu kann der Mensch nach einem einzigen Fehler oder einer unerwarteten Beobachtung schützende und sichere Lösungen ableiten. Um eine ähnliche Leistung zu erreichen, haben wir einen hierarchischen Bayes'schen Optimierungsalgorithmus entwickelt, der den kognitiven Inferenz- und Erinnerungsprozess zur Vermeidung von Fehlern bei motorischen Steuerungsaufgaben nachbildet.Ein Gauß'scher Prozess implementiert die Modellierung und das Sampling der Erfassungsfunktion.Dies ermöglicht schnelles Lernen mit großen Lernraten, während eine mentale Wiederholungsphase sicherstellt, dass Politikbereiche, die zu Fehlern geführt haben, während des Samplingprozesses gehemmt werden.    Die Eigenschaften der hierarchischen Bayes'schen Optimierungsmethode werden in einer simulierten und physiologischen humanoiden Gleichgewichtsaufgabe evaluiert, wobei wir die menschliche Lernleistung mit unserem Lernansatz quantitativ vergleichen, indem wir die Abweichungen des Massenschwerpunkts während des Trainings auswerten. Unsere Ergebnisse zeigen, dass wir das effiziente Lernen menschlicher Probanden in Aufgaben der Haltungskontrolle reproduzieren können, was ein testbares Modell für zukünftige physiologische motorische Steuerungsaufgaben darstellt. In diesen Haltungskontrollaufgaben übertrifft unsere Methode die standardmäßige Bayes'sche Optimierung in der Anzahl der Interaktionen zur Lösung der Aufgabe, in den Rechenanforderungen und in der Häufigkeit der beobachteten Fehler.
Deep Reinforcement Learning hat in vielen bisher schwierigen Reinforcement Learning-Aufgaben große Erfolge erzielt, jedoch zeigen neuere Studien, dass Deep RL-Agenten auch unvermeidlich anfällig für adversarische Störungen sind, ähnlich wie tiefe neuronale Netze in Klassifizierungsaufgaben.frühere Arbeiten konzentrieren sich meist auf modellfreie adversarische Angriffe und Agenten mit diskreten Aktionen. In dieser Arbeit untersuchen wir das Problem der kontinuierlichen Kontrolle Agenten in tiefen RL mit adversen Angriffen und schlagen die erste zweistufige Algorithmus auf der Grundlage von gelernten Modell dynamics.Extensive Experimente auf verschiedenen MuJoCo Domänen (Cartpole, Fisch, Walker, Humanoid) zeigen, dass unsere vorgeschlagenen Rahmen ist viel effektiver und effizienter als modellfreie basierte Angriffe Grundlinien bei der Verschlechterung der Agenten Leistung sowie Fahren Agenten in unsichere Zustände.
Eine führende Hypothese für die überraschende Verallgemeinerung neuronaler Netze ist, dass die Dynamik des Gradientenabstiegs das Modell in Richtung einfacher Lösungen neigt, indem es den Lösungsraum in einer inkrementellen Reihenfolge der Komplexität durchsucht. Unser wichtigster theoretischer Beitrag ist ein Ergebnis der dynamischen Tiefenseparation, das beweist, dass flache Modelle zwar eine inkrementelle Lerndynamik aufweisen können, dass aber die Initialisierung exponentiell klein sein muss, damit diese Dynamik auftritt.Sobald das Modell jedoch tiefer wird, wird die Abhängigkeit polynomial und inkrementelles Lernen kann in natürlicheren Umgebungen auftreten.Wir ergänzen unsere theoretischen Erkenntnisse durch Experimente mit tiefem Matrix-Sensing, quadratischen neuronalen Netzen und mit binärer Klassifikation unter Verwendung von diagonalen und faltbaren linearen Netzen und zeigen, dass alle diese Modelle inkrementelles Lernen aufweisen.
Generative Modellierung von hochdimensionalen Daten wie Bildern ist ein notorisch schwieriges und schlecht definiertes Problem, insbesondere ist unklar, wie ein gelerntes generatives Modell zu bewerten ist. In diesem Beitrag argumentieren wir, dass *adversariales Lernen*, das mit generativen adversen Netzen (GANs) Pionierarbeit geleistet hat, einen interessanten Rahmen bietet, um implizit aussagekräftigere Aufgabenverluste für nicht überwachte Aufgaben zu definieren, wie z. B. für die Erzeugung "visuell realistischer" Bilder. Indem wir GANs und strukturierte Vorhersage im Rahmen der statistischen Entscheidungstheorie in Beziehung setzen, stellen wir Verbindungen zwischen den jüngsten Fortschritten in der Theorie der strukturierten Vorhersage und der Wahl der Divergenz in GANs her und argumentieren, dass die Erkenntnisse über die Begriffe "schwer" und "leicht" zu erlernende Verluste analog auf adversarische Divergenzen ausgedehnt werden können.
Experimentelle Reproduzierbarkeit und Replizierbarkeit sind kritische Themen im Bereich des maschinellen Lernens, deren Fehlen in wissenschaftlichen Publikationen oft bemängelt wird, um die Qualität des Fachgebiets zu verbessern.In jüngster Zeit hat der Bereich des Lernens von Graphenrepräsentationen die Aufmerksamkeit einer breiten Forschungsgemeinschaft auf sich gezogen, was zu einem großen Strom von Arbeiten geführt hat.So wurden mehrere Modelle für neuronale Graphennetzwerke entwickelt, um die Klassifizierung von Graphen effektiv anzugehen.Allerdings mangelt es den experimentellen Verfahren oft an Strenge und sie sind kaum reproduzierbar. Um diesem beunruhigenden Trend entgegenzuwirken, haben wir mehr als 47000 Experimente in einem kontrollierten und einheitlichen Rahmen durchgeführt, um fünf beliebte Modelle in neun gängigen Benchmarks neu zu bewerten. Darüber hinaus haben wir durch den Vergleich von GNNs mit strukturunabhängigen Grundmodellen überzeugende Beweise dafür erbracht, dass bei einigen Datensätzen strukturelle Informationen noch nicht genutzt wurden, und wir glauben, dass diese Arbeit zur Entwicklung des Bereichs des Graphenlernens beitragen kann, indem sie eine dringend benötigte Grundlage für strenge Bewertungen von Graphenklassifizierungsmodellen liefert.
Data Augmentation (DA) ist von grundlegender Bedeutung, um eine Überanpassung in großen neuronalen Faltungsnetzen zu verhindern, insbesondere bei einem begrenzten Trainingsdatensatz. Bei Bildern basiert DA in der Regel auf heuristischen Transformationen, wie z. B. geometrischen oder Farbtransformationen. Unsere Experimente zeigen, dass unser Ansatz besser ist als frühere generative Datenerweiterungsmethoden und vergleichbar mit vordefinierten Transformationsmethoden beim Training eines Bildklassifizierers.
Wir betrachten das Problem der Informationskompression von hochdimensionalen Daten, wobei viele Studien das Problem der Kompression durch nicht invertierbare Trans- formationen betrachten, wir betonen die Bedeutung der invertierbaren Kompression.Wir führen eine neue Klasse von Likelihood-basierten Auto-Encodern mit pseudo-bijektiver Architektur ein, die wir Pseudo-Invertible Encoder nennen.Wir liefern die theoretische Erklärung ihrer Prinzipien.Wir evaluieren Gaussian Pseudo Invertible Encoder auf MNIST, wo unser Modell WAE und VAE in der Schärfe der erzeugten Bilder übertrifft.
Wir nutzen ein kürzlich abgeleitetes Inversionsschema für beliebige tiefe neuronale Netze, um einen neuen halbüberwachten Lernrahmen zu entwickeln, der auf eine Vielzahl von Systemen und Problemen anwendbar ist.  Der Ansatz erreicht die aktuellen State-of-the-Art-Methoden auf MNIST und bietet vernünftige Leistungen auf SVHN und CIFAR10.Durch die eingeführte Methode, Restwert Netzwerke sind zum ersten Mal auf semi-supervised tasks.Experimente mit eindimensionalen Signalen unterstreichen die Allgemeingültigkeit der Methode.Important, unser Ansatz ist einfach, effizient, und erfordert keine Änderung in der tiefen Netzwerk-Architektur.
Deep Learning ist zu einem weit verbreiteten Werkzeug bei vielen Berechnungs- und Klassifizierungsproblemen geworden. Die Beschaffung und Kennzeichnung von Daten, die für aussagekräftige Ergebnisse erforderlich sind, ist jedoch oft teuer oder gar nicht möglich. In diesem Beitrag werden drei verschiedene algorithmische Ansätze für den Umgang mit begrenztem Datenzugang bewertet und miteinander verglichen. Wir zeigen die Vor- und Nachteile der einzelnen Methoden auf. Ein erfolgreicher Ansatz, insbesondere bei ein- oder mehrstufigen Lernaufgaben, ist die Verwendung von externen Daten während der Klassifizierung. Ein weiterer erfolgreicher Ansatz, der in Semi-Supervised Learning (SSL)-Benchmarks den Stand der Technik erreicht, ist die Konsistenzregulierung, insbesondere das Virtual Adversarial Training (VAT), das in dieser Arbeit untersucht wird. Das Ziel der Konsistenzregulierung ist es, das Netzwerk zu zwingen, die Ausgabe nicht zu ändern, wenn die Eingabe oder das Netzwerk selbst gestört wird.Generative adversarial networks (GANs) haben ebenfalls starke empirische Ergebnisse gezeigt. In vielen Ansätzen wird die GAN-Architektur genutzt, um zusätzliche Daten zu erzeugen und damit die Generalisierungsfähigkeit des Klassifizierungsnetzes zu erhöhen.Darüber hinaus ziehen wir die Verwendung von unbeschrifteten Daten zur weiteren Leistungssteigerung in Betracht. Die Verwendung von unmarkierten Daten wird sowohl für GANs als auch für VAT untersucht. 
Dieses Papier stellt eine neue neuronale Struktur namens FusionNet vor, die bestehende Aufmerksamkeitsansätze aus drei Perspektiven erweitert: Erstens wird ein neuartiges Konzept der "History of Word" vorgestellt, um Aufmerksamkeitsinformationen von der niedrigsten Einbettung auf Wortebene bis zur höchsten Repräsentation auf semantischer Ebene zu charakterisieren; zweitens wird eine Aufmerksamkeitsbewertungsfunktion identifiziert, die das Konzept der "History of Word" besser nutzt; und drittens wird ein vollständig bewusster Multi-Level-Attention-Mechanismus vorgeschlagen, um die gesamte Information in einem Text (z. B. einer Frage) zu erfassen und sie in seinem Gegenstück (z. B. Kontext oder Passage) Schicht für Schicht zu nutzen. Wir wenden FusionNet auf den Stanford Question Answering Dataset (SQuAD) an und es erreicht sowohl für das Einzel- als auch für das Ensemble-Modell den ersten Platz in der offiziellen SQuAD-Rangliste zum Zeitpunkt der Erstellung dieses Artikels (4. Oktober 2017). In der Zwischenzeit verifizieren wir die Generalisierung von FusionNet mit zwei gegnerischen SQuAD-Datensätzen und es stellt den neuen Stand der Technik auf beiden Datensätzen dar: auf AddSent erhöht FusionNet die beste F1-Metrik von 46,6 % auf 51,4 %; auf AddOneSent erhöht FusionNet die beste F1-Metrik von 56,0 % auf 60,7 %.
Wir zeigen, dass die hierarchische Struktur das Lernen von zunehmend abstrakteren Repräsentationen unterstützt und semantisch sinnvolle Rekonstruktionen mit unterschiedlichen Genauigkeitsgraden liefert. Außerdem zeigen wir, dass die Minimierung der Jensen-Shanon-Divergenz zwischen dem generativen und dem Inferenznetzwerk ausreicht, um den Rekonstruktionsfehler zu minimieren. Die daraus resultierende semantisch sinnvolle hierarchische latente Strukturentdeckung wird anhand des CelebA-Datensatzes veranschaulicht.  Dort zeigen wir, dass die von unserem Modell auf unbeaufsichtigte Weise gelernten Merkmale die besten handgefertigten Merkmale übertreffen. Darüber hinaus bleiben die extrahierten Merkmale konkurrenzfähig, wenn sie mit mehreren neueren Deep Supervised-Ansätzen bei einer Attributvorhersageaufgabe auf CelebA verglichen werden. Schließlich nutzen wir das Inferenznetzwerk des Modells, um bei einer semi-supervised-Variante der MNIST-Ziffernklassifizierungsaufgabe die beste Leistung zu erzielen.
Die Lösung der Differentialgleichungen, die mit den Erhaltungsgesetzen verbunden sind, ist ein wichtiger Zweig der Computermathematik. Der jüngste Erfolg des maschinellen Lernens, insbesondere des Deep Learning, in Bereichen wie dem Computersehen und der Verarbeitung natürlicher Sprache, hat in der Gemeinschaft der Computermathematik viel Aufmerksamkeit erregt und viele interessante Arbeiten inspiriert, die maschinelles Lernen mit traditionellen Methoden kombinieren. In diesem Papier sind wir die ersten, die die Möglichkeit und den Nutzen der Lösung nichtlinearer Erhaltungssätze mit tiefen Verstärkungslernen zu erforschen.als Beweis des Konzepts, konzentrieren wir uns auf 1-dimensionale skalare Erhaltungssätze.wir die Maschinerie der tiefen Verstärkungslernen einsetzen, um eine Politik Netzwerk, das auf, wie die numerischen Lösungen sollten in einer sequentiellen und räumlich-zeitliche adaptive Art und Weise angenähert werden entscheiden kann trainieren.wir werden zeigen, dass das Problem der Lösung von Erhaltungssätze kann natürlich als eine sequentielle Entscheidungsfindung und die numerische Schemata in einer solchen Art und Weise gelernt kann leicht erzwingen langfristige Genauigkeit betrachtet werden. Darüber hinaus ist das erlernte Policy-Netzwerk sorgfältig entworfen, um eine gute lokale diskrete Annäherung auf der Grundlage des aktuellen Zustands der Lösung zu bestimmen, was die vorgeschlagene Methode im Wesentlichen zu einem Meta-Lernansatz macht, d.h. die vorgeschlagene Methode ist in der Lage zu lernen, wie man für eine gegebene Situation diskretisiert, indem sie menschliche Experten imitiert.Schließlich werden wir Details darüber liefern, wie das Policy-Netzwerk trainiert wird, wie gut es im Vergleich zu einigen hochmodernen numerischen Lösern, wie z.B. WENO-Schemata, abschneidet und wie gut es verallgemeinert wird.Unser Code wird anomyn unter \url{https://github.com/qwerlanksdf/L2D} veröffentlicht.
Wir stellen eine neuronale Rendering-Architektur vor, die Variations-Autoencodern (VAEs) hilft, entmischte Darstellungen zu erlernen. Anstelle des dekonvolutionellen Netzwerks, das typischerweise im Decoder von VAEs verwendet wird, kacheln (übertragen) wir den latenten Vektor über den Raum, verketten feste X- und Y-"Koordinaten"-Kanäle und wenden ein vollständig konvolutionelles Netzwerk mit 1x1-Stride an. Wir zeigen, dass diese Architektur, die wir als Spatial Broadcast Decoder bezeichnen, die Entflechtung, die Rekonstruktionsgenauigkeit und die Verallgemeinerung auf ausgeklammerte Regionen im Datenraum verbessert.  Wir zeigen, dass der Spatial Broadcast Decoder komplementär zu State-of-the-Art (SOTA) Entwirrungstechniken ist und deren Leistung verbessert, wenn er integriert wird.
Ähnlich wie bei Menschen und Tieren gibt es bei tiefen künstlichen neuronalen Netzen kritische Perioden, in denen ein vorübergehendes Reizdefizit die Entwicklung einer Fähigkeit beeinträchtigen kann, wobei das Ausmaß der Beeinträchtigung wie bei Tiermodellen vom Beginn und der Länge des Defizitfensters und von der Größe des neuronalen Netzes abhängt.  Um dieses Phänomen besser zu verstehen, verwenden wir die Fisher-Information der Gewichte, um die effektive Konnektivität zwischen den Schichten eines Netzwerks während des Trainings zu messen.  Kontraintuitiv steigt die Information in den frühen Phasen des Trainings schnell an und nimmt dann ab, wodurch eine Umverteilung der Informationsressourcen verhindert wird, ein Phänomen, das wir als Verlust der "Informationsplastizität" bezeichnen.  Unsere Analyse deutet darauf hin, dass die ersten Epochen entscheidend für die Bildung starker Verbindungen sind, die im Verhältnis zur Verteilung der Eingabedaten optimal sind, und dass sich diese starken Verbindungen im weiteren Verlauf des Trainings nicht mehr ändern, was darauf hindeutet, dass die anfängliche Lernphase, die im Vergleich zum asymptotischen Verhalten unterbewertet ist, eine Schlüsselrolle bei der Bestimmung des Trainingsergebnisses spielt. Schließlich sind kritische Perioden nicht auf biologische Systeme beschränkt, sondern können aufgrund grundlegender Einschränkungen, die sich aus der Lerndynamik und der Informationsverarbeitung ergeben, auf natürliche Weise in lernenden Systemen, ob biologisch oder künstlich, auftreten.
Wir schlagen eine Methode vor, um inkrementell einen Einbettungsraum über die Domäne der Netzwerkarchitekturen zu lernen, um die sorgfältige Auswahl von Architekturen für die Bewertung während der komprimierten Architektursuche zu ermöglichen.Angesichts eines Lehrernetzwerks suchen wir nach einer komprimierten Netzwerkarchitektur, indem wir Bayes'sche Optimierung (BO) mit einer Kernel-Funktion verwenden, die über unseren vorgeschlagenen Einbettungsraum definiert ist, um Architekturen für die Bewertung auszuwählen.Wir zeigen, dass unser Suchalgorithmus verschiedene Basismethoden, wie zufällige Suche und Verstärkungslernen, deutlich übertreffen kann (Ashok et al, 2018).Die komprimierten Architekturen, die durch unsere Methode gefunden werden, sind auch besser als die State-of-the-Art manuell entworfene kompakte Architektur ShuffleNet (Zhang et al., 2018).Wir zeigen auch, dass der gelernte Einbettungsraum auf neue Einstellungen für die Architektursuche, wie ein größeres Lehrernetzwerk oder ein Lehrernetzwerk in einer anderen Architekturfamilie, ohne Training übertragen werden kann.
Reinforcement Learning (RL) Problem kann auf zwei verschiedene Arten gelöst werden - die Value-Funktion-basierten Ansatz und die Politik-Optimierung-basierten Ansatz - um schließlich zu einer optimalen Politik für die gegebenen environment.One der jüngsten Durchbrüche in Reinforcement Learning ist die Verwendung von tiefen neuronalen Netzen als Funktion Approximatoren zur Annäherung der Wert-Funktion oder q-Funktion in einem Reinforcement Learning Schema.This hat dazu geführt, dass Ergebnisse mit Agenten automatisch lernen, wie man Spiele wie Alpha-Go zeigen besser als die menschliche Leistung. Deep Q-learning networks (DQN) und Deep Deterministic Policy Gradient (DDPG) sind zwei solcher Methoden, die in letzter Zeit hochmoderne Ergebnisse gezeigt haben. Unter den vielen Varianten von RL ist eine wichtige Klasse von Problemen, bei denen die Zustands- und Aktionsräume kontinuierlich sind - autonome Roboter, autonome Fahrzeuge, optimale Kontrolle sind alles Beispiele für solche Probleme, die sich natürlich für verstärkungsbasierte Algorithmen eignen und kontinuierliche Zustands- und Aktionsräume haben. In dieser Arbeit adaptieren und kombinieren wir Ansätze wie DQN und DDPG auf neuartige Weise, um frühere Ergebnisse für kontinuierliche Zustands- und Aktionsraumprobleme zu übertreffen.  Wir glauben, dass diese Ergebnisse eine wertvolle Ergänzung zu den schnell wachsenden Ergebnissen des Reinforcement Learning sind, insbesondere für Probleme mit kontinuierlichen Zuständen und Aktionsräumen.
Wir stellen Interactive Inference Network (IIN) vor, eine neue Klasse neuronaler Netzwerkarchitekturen, die in der Lage ist, ein hochrangiges Verständnis des Satzpaares zu erreichen, indem sie semantische Merkmale aus dem Interaktionsraum hierarchisch extrahiert. Wir zeigen, dass ein Interaktionstensor (Aufmerksamkeitsgewicht) semantische Informationen enthält, um die Inferenz natürlicher Sprache zu lösen, und dass ein dichterer Interaktionstensor reichere semantische Informationen enthält. Eine Instanz einer solchen Architektur, Densely Interactive Inference Network (DIIN), demonstriert die State-of-the-Art-Leistung auf großen NLI-Kopora und groß angelegten NLI-Korpus gleichermaßen. Es ist bemerkenswert, dass DIIN eine mehr als 20%ige Fehlerreduktion auf dem anspruchsvollen Multi-Genre NLI (MultiNLI) Datensatz in Bezug auf das stärkste veröffentlichte System erreicht.
Determinantal Point Processes (DPPs) bieten eine elegante und vielseitige Möglichkeit zur Auswahl von Punktmengen, die ein Gleichgewicht zwischen der punktuellen Qualität und der Vielfalt der ausgewählten Elemente herstellen. Das Sampling aus einem DPP über eine Grundmenge der Größe N ist jedoch ein kostspieliger Vorgang, der im Allgemeinen O(N^3) Vorverarbeitungskosten und O(Nk^3) Samplingkosten für Teilmengen der Größe k erfordert. Wir nähern uns diesem Problem durch die Einführung von DppNets: generative tiefe Modelle, die DPP-ähnliche Samples für beliebige Grundmengen erzeugen.  Wir entwickeln einen hemmenden Aufmerksamkeitsmechanismus, der auf Transformator-Netzwerken basiert und einen Begriff der Unähnlichkeit zwischen Merkmalsvektoren erfasst.  Wir zeigen theoretisch, dass eine solche Annäherung sinnvoll ist, da sie die Garantien der Inhibition oder Unähnlichkeit beibehält, die DPP so leistungsfähig und einzigartig machen.  Empirisch zeigen wir, dass Proben aus unserem Modell eine hohe Wahrscheinlichkeit unter der teureren DPP-Alternative erhalten.
In diesem Beitrag wird eine Netzwerkarchitektur zur Lösung des Structure-from-Motion (SfM)-Problems über die Feature-Metric-Bundle-Anpassung (BA) vorgestellt, die explizit Multi-View-Geometriebeschränkungen in Form von Feature-Metric-Fehlern erzwingt.Die gesamte Pipeline ist differenzierbar, so dass das Netzwerk geeignete Merkmale erlernen kann, die das BA-Problem überschaubarer machen.Darüber hinaus wird in dieser Arbeit eine neuartige Tiefenparametrisierung zur Wiederherstellung der dichten Tiefe pro Pixel eingeführt. Das Netzwerk generiert zunächst mehrere Basis-Tiefenkarten entsprechend dem Eingangsbild und optimiert die endgültige Tiefe als Linearkombination dieser Basis-Tiefenkarten über feature-metric BA.The Basis-Tiefenkarten-Generator ist auch über End-to-End-Training gelernt.Das gesamte System kombiniert schön Domänenwissen (dh hart kodierte Multi-View-Geometrie Einschränkungen) und Deep Learning (dh Feature-Lernen und Basis-Tiefenkarten Lernen), um die anspruchsvolle dichte SfM Problem.Experimente auf großen realen Daten beweisen den Erfolg der vorgeschlagenen Methode.
Frühere Arbeiten wie \citet{sutton2009fast} und \citet{sutton2009convergent} haben alternative Ziele vorgestellt, die stabil zu minimieren sind. In der Praxis erfordert das TD-Lernen mit neuronalen Netzen jedoch verschiedene Tricks wie die Verwendung eines Zielnetzes, das sich langsam aktualisiert \citep{mnih2015human}. In dieser Arbeit schlagen wir eine Einschränkung für die TD-Aktualisierung vor, die die Änderung der Zielwerte minimiert. Diese Einschränkung kann auf die Gradienten jedes TD-Ziels angewandt werden und lässt sich leicht auf die Approximation nichtlinearer Funktionen anwenden. Wir validieren diese Aktualisierung, indem wir unsere Technik auf tiefes Q-Lernen und Training ohne Zielnetz anwenden.
Wir schlagen DuoRC vor, einen neuartigen Datensatz für Leseverständnis (RC), der mehrere neue Herausforderungen für neuronale Ansätze im Sprachverständnis motiviert, die über die bestehenden RC-Datensätze hinausgehen.DuoRC enthält 186.089 einzigartige Frage-Antwort-Paare, die aus einer Sammlung von 7680 Paaren von Filmplots erstellt wurden, wobei jedes Paar in der Sammlung zwei Versionen desselben Films widerspiegelt - eine aus Wikipedia und die andere aus IMDb - geschrieben von zwei verschiedenen Autoren. Diese einzigartige Eigenschaft von DuoRC, bei der Fragen und Antworten aus verschiedenen Versionen eines Dokuments erstellt werden, das dieselbe Geschichte erzählt, stellt sicher, dass es nur sehr wenige lexikalische Überschneidungen zwischen den Fragen, die aus einer Version erstellt wurden, und den Segmenten, die die Antwort in der anderen Version enthalten, gibt, da die beiden Versionen unterschiedliche Details der Handlung, des Erzählstils, des Vokabulars usw. aufweisen, Darüber hinaus erfordert die Beantwortung von Fragen aus der zweiten Version ein tieferes Sprachverständnis und die Einbeziehung von Hintergrundwissen, das in dem gegebenen Text nicht vorhanden ist, da der narrative Stil von Passagen aus Filmplots (im Gegensatz zu typischen beschreibenden Passagen in bestehenden Datensätzen) komplexe Schlussfolgerungen über Ereignisse in mehreren Sätzen erfordert. In der Tat stellen wir fest, dass neuronale RC-Modelle auf dem neuesten Stand der Technik, die auf dem SQuAD-Datensatz annähernd menschliche Leistungen erzielt haben, selbst dann sehr schlechte Leistungen zeigen, wenn sie mit traditionellen NLP-Techniken gekoppelt werden, um die Herausforderungen von DuoRC zu bewältigen (F1-Ergebnis von 37,42% auf DuoRC gegenüber 86% auf dem SQuAD-Datensatz), was mehrere interessante Forschungsmöglichkeiten eröffnet, bei denen DuoRC andere Datensätze im Stil des Leseverstehens ergänzen könnte, um neue neuronale Ansätze zur Untersuchung des Sprachverständnisses zu erforschen.
Wir betrachten das Problem der schwach überwachten strukturierten Vorhersage (SP) mit Verstärkungslernen (RL) - z.B. bei einer Datenbanktabelle und einer Frage eine Sequenz von Berechnungsaktionen auf der Tabelle durchführen, die eine Antwort erzeugt und eine binäre Erfolg-Misserfolg-Belohnung erhält.   Diese Forschungsrichtung war erfolgreich, indem RL genutzt wurde, um die gewünschten Metriken der SP-Aufgaben direkt zu optimieren - zum Beispiel die Genauigkeit bei der Beantwortung von Fragen oder den BLEU-Score bei der maschinellen Übersetzung.  Da SP-Modelle in der Regel vollen Zugriff auf die Dynamik der Umgebung haben, schlagen wir vor, modellbasierte RL-Methoden anzuwenden, die sich auf die Planung als primäre Modellkomponente stützen. Wir demonstrieren die Effektivität von planungsbasierten SP mit einem neuronalen Programmplaner (NPP), der angesichts einer Reihe von Programmkandidaten aus einer vortrainierten Suchpolitik entscheidet, welches Programm das vielversprechendste ist, wenn man alle Informationen berücksichtigt, die bei der Ausführung dieser Programme generiert werden.Wir evaluieren NPP auf schwach überwachter Programmsynthese aus natürlicher Sprache (semantisches Parsing) durch gestapeltes Lernen eines Planungsmoduls, das auf vortrainierten Suchpolitiken basiert.Auf dem WIKITABLEQUESTIONS-Benchmark erreicht NPP einen neuen Spitzenwert von 47,2% Genauigkeit.
Deep-Learning-Algorithmen erreichen eine hohe Klassifizierungsgenauigkeit auf Kosten erheblicher Rechenkosten. Um diese Kosten anzugehen, wurde eine Reihe von Quantisierungsschemata vorgeschlagen - aber die meisten dieser Techniken konzentrierten sich auf die Quantisierung der Gewichte, die im Vergleich zu den Aktivierungen relativ klein sind.Dieses Papier schlägt ein neuartiges Quantisierungsschema für Aktivierungen während des Trainings vor, das es neuronalen Netzen ermöglicht, mit Gewichten und Aktivierungen von extrem geringer Präzision zu arbeiten, ohne dass die Genauigkeit signifikant abnimmt.  Diese Technik, PArameterized Clipping acTi-vation (PACT), verwendet einen Aktivierungs-Clipping-Parameter α, der während des Trainings optimiert wird, um die richtige Quantisierungsskala zu finden.PACT ermöglicht die Quantisierung von Aktivierungen auf beliebige Bitpräzisionen und erreicht dabei eine viel bessere Genauigkeit im Vergleich zu veröffentlichten State-of-the-Art-Quantisierungsverfahren. Wir zeigen zum ersten Mal, dass sowohl Gewichte als auch Aktivierungen mit einer Genauigkeit von 4 Bit quantisiert werden können und dabei eine Genauigkeit erreichen, die mit der von Netzwerken mit voller Genauigkeit vergleichbar ist, und zwar für eine Reihe von beliebten Modellen und Datensätzen.Wir zeigen auch, dass die Nutzung dieser Recheneinheiten mit reduzierter Genauigkeit in der Hardware eine super-lineare Verbesserung der Inferenzleistung ermöglichen kann, und zwar durch eine signifikante Verringerung der Fläche von Beschleuniger-Recheneinheiten in Verbindung mit der Möglichkeit, die quantisierten Modell- und Aktivierungsdaten in On-Chip-Speichern zu speichern.
Das Pruning von Parametern neuronaler Netze wird häufig als Mittel zur Komprimierung von Modellen betrachtet, aber auch der Wunsch, eine Überanpassung zu verhindern, ist von besonderer Bedeutung angesichts der vielleicht überraschenden Beobachtung, dass eine Vielzahl von Pruning-Ansätzen die Testgenauigkeit trotz einer manchmal massiven Verringerung der Parameteranzahl erhöht. Um dieses Phänomen besser zu verstehen, analysieren wir das Verhalten des Prunings im Verlauf des Trainings und stellen fest, dass der Effekt des Prunings auf die Generalisierung mehr von der Instabilität abhängt, die es erzeugt (definiert als Rückgang der Testgenauigkeit unmittelbar nach dem Pruning) als von der endgültigen Größe des beschnittenen Modells.
Neuronale Netze, die mit Backpropagation, dem Standardalgorithmus des Deep Learning, der Gewichtstransport verwendet, trainiert werden, sind leicht durch bestehende gradientenbasierte Angriffe zu täuschen. Diese Klasse von Angriffen basiert auf bestimmten kleinen Störungen der Eingaben, um die Netze dazu zu bringen, sie falsch zu klassifizieren.Wir zeigen, dass weniger biologisch unplausible tiefe neuronale Netze, die mit Feedback-Anpassung trainiert werden und keinen Gewichtstransport verwenden, schwerer zu täuschen sind und tatsächliche Robustheit bieten. Im MNIST-Test haben tiefe neuronale Netze, die ohne Gewichtstransport trainiert wurden, (1) eine Fehlergenauigkeit von 98 % im Vergleich zu 0,03 % für neuronale Netze, die mit Backpropagation trainiert wurden, und (2) erzeugen nicht übertragbare Fehlerbeispiele, wobei sich dieser Unterschied bei CIFAR-10 verringert, aber immer noch signifikant ist, insbesondere bei kleinen Störungsgrößen von weniger als 1 ⁄ 2.
Chemische Reaktionen können als schrittweise Umverteilung von Elektronen in Molekülen beschrieben werden und werden daher oft mit Pfeildiagrammen dargestellt, die diese Bewegung als eine Abfolge von Pfeilen zeigen.Wir schlagen ein Elektronenpfad-Vorhersagemodell (ELECTRO) vor, um diese Sequenzen direkt aus rohen Reaktionsdaten zu lernen. Anstatt Produktmoleküle direkt aus Reaktantenmolekülen vorherzusagen, hat das Erlernen eines Modells der Elektronenbewegung den Vorteil, dass es(a) für Chemiker leicht zu interpretieren ist,(b) die Einschränkungen der Chemie, wie z.B. ausgeglichene Atomzahlen vor und nach der Reaktion, einbezieht und(c) auf natürliche Weise die Spärlichkeit chemischer Reaktionen kodiert, die normalerweise nur Änderungen in einer kleinen Anzahl von Atomen in den Reaktanten beinhalten. Wir entwickeln eine Methode, um ungefähre Reaktionspfade aus einem beliebigen Datensatz von SMILES-Zeichenfolgen zu extrahieren, und zeigen, dass unser Modell bei einer wichtigen Teilmenge des USPTO-Reaktionsdatensatzes eine hervorragende Leistung erzielt, die mit den stärksten Baselines vergleichbar ist, und dass unser Modell ein grundlegendes Wissen über Chemie wiederherstellt, ohne explizit dafür trainiert zu werden.
Um diese drei Anforderungen zu erfüllen, muss ein Modell in der Lage sein, eine Ablehnungsoption auszugeben (d.h. "Ich weiß nicht" zu sagen), wenn es nicht qualifiziert ist, eine Vorhersage zu treffen. in dieser Arbeit schlagen wir das Aufschiebungslernen vor, eine Methode, mit der ein Modell die Entscheidung an einen nachgeschalteten Entscheidungsträger, z.B. einen menschlichen Benutzer, delegieren kann. Wir zeigen, dass das Aufschiebungslernen den Rahmen des Ablehnungslernens in zweierlei Hinsicht verallgemeinert: durch die Berücksichtigung der Auswirkungen anderer Agenten im Entscheidungsprozess und durch die Möglichkeit der Optimierung komplexer Ziele.Wir schlagen einen Lernalgorithmus vor, der potenzielle Voreingenommenheiten von Entscheidungsträgern berücksichtigt, die sich später in einer Pipeline befinden.Experimente mit realen Datensätzen zeigen, dass das Aufschiebungslernen ein Modell nicht nur genauer, sondern auch weniger voreingenommen machen kann.Selbst wenn es von stark voreingenommenen Nutzern betrieben wird, zeigen wir, dass Aufschiebungsmodelle die Fairness der gesamten Pipeline erheblich verbessern können.
Während viele HTN-Planer während des Zerlegungsprozesses Aufrufe an externe Prozesse (z. B. an eine Simulatorschnittstelle) tätigen können, ist dies ein rechenintensiver Prozess, so dass die Planerimplementierungen solche Aufrufe oft ad-hoc verwenden und sehr spezialisiertes Domänenwissen einsetzen, um die Anzahl der Aufrufe zu begrenzen. Umgekehrt verwenden die wenigen klassischen Planer, die in der Lage sind, externe Aufrufe (oft als semantische Anhänge bezeichnet) während der Planung zu verwenden, dies auf sehr viel eingeschränktere Art und Weise, indem sie eine feste Anzahl von Grundoperatoren zum Zeitpunkt der Problemerdung erzeugen. Der resultierende Planer kann dann solche Co-Routinen als Teil seines Backtracking-Mechanismus verwenden, um durch parallele Dimensionen des Zustandsraums zu suchen (z.B. durch numerische Variablen). Wir zeigen empirisch, dass unser Planer die modernsten numerischen Planer in einer Reihe von Domänen mit minimalem zusätzlichen Domänenwissen übertrifft.
Die jüngste Literatur legt nahe, dass gemittelte Wortvektoren, gefolgt von einer einfachen Nachbearbeitung, viele Deep-Learning-Methoden bei semantischen Textähnlichkeitsaufgaben übertreffen und dass sie, wenn gemittelte Wortvektoren unter Aufsicht auf großen Korpora von Paraphrasen trainiert werden, bei Standard-STS-Benchmarks Spitzenergebnisse erzielen. Wir schlagen eine neuartige Fuzzy Bag-of-Words (FBoW) Repräsentation für Text vor, die alle Wörter des Vokabulars gleichzeitig enthält, aber mit unterschiedlichen Zugehörigkeitsgraden, die aus Ähnlichkeiten zwischen Wortvektoren abgeleitet werden. Schließlich schlagen wir DynaMax vor, ein völlig unbeaufsichtigtes und nicht-parametrisches Ähnlichkeitsmaß, das dynamisch gute Merkmale in Abhängigkeit vom Satzpaar extrahiert und max-poolt. Diese Methode ist sowohl effizient als auch einfach zu implementieren und übertrifft dennoch die aktuellen Basisverfahren bei STS-Aufgaben um ein Vielfaches und ist sogar konkurrenzfähig mit überwachten Wortvektoren, die zur direkten Optimierung der Kosinusähnlichkeit trainiert wurden.
State-of-the-Art-Ergebnisse beim Imitationslernen werden derzeit von adversen Methoden erzielt, die iterativ die Divergenz zwischen Schüler- und Expertenstrategien abschätzen und dann diese Divergenz minimieren, um die Imitationsstrategie näher an das Expertenverhalten heranzuführen.Analoge Techniken für das Imitationslernen allein aus Beobachtungen (ohne Experten-Handlungskennzeichnungen) haben jedoch nicht den gleichen allgegenwärtigen Erfolg. Jüngste Arbeiten im Bereich kontradiktorischer Methoden für generative Modelle haben gezeigt, dass das Maß, das zur Beurteilung der Diskrepanz zwischen realen und synthetischen Stichproben verwendet wird, eine algorithmische Designentscheidung ist und dass unterschiedliche Entscheidungen zu erheblichen Unterschieden in der Modellleistung führen können. Die Wasserstein-Distanz und verschiedene $f$-Divergenzen wurden bereits in der Literatur zu adversen Netzwerken untersucht, während die letztgenannte Klasse in jüngerer Zeit für das Nachahmungslernen untersucht wurde. Leider haben wir festgestellt, dass in der Praxis dieser bestehende Nachahmungslernrahmen für die Verwendung von $f$-Divergenzen unter numerischen Instabilitäten leidet, die aus der Kombination von Funktionsapproximation und Policy-Gradient-Verstärkungslernen resultieren. In dieser Arbeit lindern wir diese Probleme und bieten eine Neuparametrisierung des gegnerischen Nachahmungslernens als $f$-Divergenzminimierung an, bevor wir den Rahmen weiter ausbauen, um das Problem der Nachahmung nur aus Beobachtungen zu behandeln. Darüber hinaus stellen wir fest, dass wir mit der richtigen Wahl der $f$-Divergenz Algorithmen zur Nachahmung aus Beobachtungen erhalten können, die die Basisansätze übertreffen und der Leistung von Experten in kontinuierlichen Steuerungsaufgaben mit niedrigdimensionalen Beobachtungsräumen näher kommen.Bei hochdimensionalen Beobachtungen beobachten wir immer noch eine signifikante Lücke mit und ohne Aktionskennzeichnungen, was einen interessanten Weg für zukünftige Arbeit bietet.
Momentane Fluktuationen in der Aufmerksamkeit (Wahrnehmungsgenauigkeit) korrelieren mit Fluktuationen der neuronalen Aktivität in visuellen Arealen von Primaten, aber die Verbindung zwischen solchen momentanen neuronalen Fluktuationen und dem Aufmerksamkeitszustand muss noch im menschlichen Gehirn nachgewiesen werden.Wir untersuchen diese Verbindung mit Hilfe einer kognitiven Echtzeit-Gehirn-Maschine-Schnittstelle (cBMI), die auf visuell evozierten Potentialen (SSVEPs) basiert: okzipitale EEG-Potentiale, die durch rhythmisch blinkende Reize hervorgerufen werden. Wir beobachteten einen signifikanten Anstieg der Unterscheidungsgenauigkeit (d'), wenn die Stimuli während Epochen mit hoher (im Vergleich zu niedriger) SSVEP-Leistung an der Stelle ausgelöst wurden, auf die die Aufmerksamkeit gerichtet war.
Inspiriert von der Beobachtung, dass die intrinsische Dimension von Bilddaten viel kleiner ist als die Dimension des Pixelraums und die Anfälligkeit neuronaler Netze mit der Eingangsdimension wächst, schlagen wir vor, hochdimensionale Eingangsbilder in einen niedrigdimensionalen Raum einzubetten, um eine Klassifizierung durchzuführen. Wir schlagen einen neuen Rahmen vor, den Embedding Regularized Classifier (ER-Classifier), der die Robustheit des Klassifizierers durch Einbettung der Regularisierung verbessert. Experimentelle Ergebnisse auf verschiedenen Benchmark-Datensätzen zeigen, dass der von uns vorgeschlagene Rahmen die beste Leistung gegen starke Angriffsmethoden bietet.
Wir untersuchen das Aufgabenclustering für Deep Learning-basiertes Multi-Task- und few-shot-Lernen in Umgebungen mit einer großen Anzahl verschiedener Aufgaben. Unsere Methode misst Aufgabenähnlichkeiten mithilfe einer Cross-Task-Transferleistungsmatrix, Um diese Einschränkungen zu überwinden, schlagen wir einen neuartigen Algorithmus zur Schätzung der Ähnlichkeitsmatrix vor, der auf der Theorie der Matrixvervollständigung basiert. der vorgeschlagene Algorithmus kann auf teilweise beobachteten Ähnlichkeitsmatrizen arbeiten, die nur auf Stichproben von Aufgabenpaaren mit zuverlässigen Werten basieren, was seine Effizienz und Robustheit gewährleistet. Unsere theoretische Analyse zeigt, dass unter milden Annahmen, die rekonstruierte Matrix perfekt mit der zugrunde liegenden "wahren" Ähnlichkeitsmatrix mit einer überwältigenden Wahrscheinlichkeit übereinstimmt.die endgültige Aufgabe Partition wird durch die Anwendung eines effizienten spektralen Clustering-Algorithmus auf die wiederhergestellte Matrix berechnet.unsere Ergebnisse zeigen, dass die neue Aufgabe Clustering-Methode kann Aufgabe Clustern, die sowohl Multi-Task-Lernen und wenige-shot-Lernen Setups für Sentiment-Klassifizierung und Dialog Absicht Klassifizierung Aufgaben profitieren zu entdecken.
Die Ähnlichkeit zwischen den Methoden, die bei der Untersuchung der Quanten-Vielkörperphysik und beim maschinellen Lernen verwendet werden, hat viel Aufmerksamkeit auf sich gezogen. Insbesondere Tensornetzwerke (TNs) und Deep-Learning-Architekturen weisen auffällige Ähnlichkeiten auf, so dass TNs für das maschinelle Lernen verwendet werden können.Frühere Ergebnisse verwendeten eindimensionale TNs bei der Bilderkennung, die eine begrenzte Skalierbarkeit und eine hohe Bindungsdimension erfordern.In dieser Arbeit trainieren wir zweidimensionale hierarchische TNs, um Bilderkennungsprobleme zu lösen, indem wir einen Trainingsalgorithmus verwenden, der aus dem Multipartite Entanglement Renormalization Ansatz (MERA) abgeleitet ist. Dieser Ansatz überwindet Skalierbarkeitsprobleme und impliziert neuartige mathematische Verbindungen zwischen Quanten-Vielteilchenphysik, Quanteninformationstheorie und maschinellem Lernen. Während die TN in der Trainingsphase unitär bleiben, können TN-Zustände definiert werden, die jede Klasse der Bilder optimal in einen Quanten-Vielteilchenzustand kodieren. Wir untersuchen die Quanteneigenschaften der TN-Zustände, einschließlich der Quantenverschränkung und -treue, und schlagen vor, dass diese Größen neuartige Eigenschaften sein könnten, die sowohl die Bildklassen als auch die Aufgaben des maschinellen Lernens charakterisieren, und dass unsere Arbeit auch auf die Identifizierung möglicher Quanteneigenschaften bestimmter Methoden der künstlichen Intelligenz angewendet werden könnte.
Eine Vielzahl solcher Aufgaben beinhaltet kontinuierliche physikalische Systeme, die durch partielle Differentialgleichungen (PDEs) mit vielen Freiheitsgraden beschrieben werden können. Bestehende Methoden, die darauf abzielen, die Dynamik solcher Systeme zu kontrollieren, sind typischerweise auf relativ kurze Zeiträume oder eine kleine Anzahl von Interaktionsparametern beschränkt. Wir stellen ein neuartiges hierarchisches Prädiktor-Korrektor-Schema vor, das es neuronalen Netzen ermöglicht, zu lernen, komplexe nichtlineare physikalische Systeme über lange Zeiträume zu verstehen und zu kontrollieren. Zu diesem Zweck führen wir ein Prädiktornetzwerk ein, das optimale Trajektorien plant, und ein Kontrollnetzwerk, das die entsprechenden Kontrollparameter ableitet. Beide Stufen werden Ende-zu-Ende mit einem differenzierbaren PDE-Löser trainiert. Wir zeigen, dass unsere Methode erfolgreich ein Verständnis komplexer physikalischer Systeme entwickelt und lernt, sie für Aufgaben mit PDEs wie den inkompressiblen Navier-Stokes-Gleichungen zu kontrollieren.
Die Fähigkeit von überparametrisierten tiefen Netzwerken, sich gut zu verallgemeinern, wurde mit der Tatsache in Verbindung gebracht, dass der stochastische Gradientenabstieg (SGD) Lösungen findet, die in flachen, weiten Minima des Trainingsverlustes liegen - Minima, in denen die Ausgabe des Netzwerks gegenüber kleinem Zufallsrauschen, das seinen Parametern hinzugefügt wird, widerstandsfähig ist. Bisher wurde diese Beobachtung verwendet, um Generalisierungsgarantien nur für neuronale Netze zu geben, deren Parameter entweder \textit{stochastisch} oder \textit{komprimiert} sind. In dieser Arbeit stellen wir ein allgemeines PAC-Bayesian Framework vor, das diese Beobachtung nutzt, um eine Schranke für das ursprünglich gelernte Netz zu liefern - ein Netz, das deterministisch und nicht komprimiert ist.  Was uns dazu befähigt, ist eine wesentliche Neuerung in unserem Ansatz: Unser Rahmenwerk ermöglicht es uns zu zeigen, dass, wenn bei Trainingsdaten die Interaktionen zwischen den Gewichtsmatrizen bestimmte Bedingungen erfüllen, die ein breites Trainingsverlustminimum implizieren, diese Bedingungen selbst auf die Interaktionen zwischen den Matrizen bei Testdaten verallgemeinert werden können, wodurch ein breites Testverlustminimum impliziert wird. Anschließend wenden wir unseren allgemeinen Rahmen in einem Setup an, in dem wir davon ausgehen, dass die Voraktivierungswerte des Netzwerks nicht zu klein sind (obwohl wir dies nur für die Trainingsdaten annehmen). In diesem Setup bieten wir eine Generalisierungsgarantie für das ursprüngliche (deterministische, unkomprimierte) Netzwerk, die nicht mit dem Produkt der Spektralnormen der Gewichtsmatrizen skaliert - eine Garantie, die mit früheren Ansätzen nicht möglich gewesen wäre.
Das Training von neuronalen Netzen, die nachweislich robust sind, ist von entscheidender Bedeutung, um ihre Sicherheit gegen gegnerische Angriffe zu gewährleisten, aber es ist derzeit sehr schwierig, ein neuronales Netz zu trainieren, das sowohl genau als auch nachweislich robust ist. Wir beweisen, dass für jede kontinuierliche Funktion $f$ ein Netzwerk $n$ existiert, das:(i) $n$ $f$ beliebig nahe approximiert, und(ii) durch einfache intervallgebundene Propagierung einer Region $B$ durch $n$ ein Ergebnis liefert, das der optimalen Ausgabe von $f$ auf $B$ beliebig nahe kommt.Unser Ergebnis kann als universeller Approximationssatz für intervallzertifizierte ReLU-Netzwerke angesehen werden.Nach unserem Wissen ist dies die erste Arbeit, die die Existenz genauer, intervallzertifizierter Netzwerke beweist.
Wir verwenden zwei Arten von Beschleunigungsmethoden: Pruning und Hints.Pruning kann die Modellgröße reduzieren, indem Kanäle von Schichten entfernt werden.Hints können die Leistung des Schülermodells verbessern, indem Wissen vom Lehrermodell übertragen wird.Wir zeigen, dass Pruning und Hints sich gegenseitig ergänzen.Einerseits können Hints vom Pruning profitieren, indem ähnliche Merkmalsrepräsentationen erhalten bleiben. Wir zeigen, dass Pruning und Hints sich gegenseitig ergänzen.Einerseits kann Pruning davon profitieren, dass ähnliche Merkmalsrepräsentationen beibehalten werden, andererseits ist das vom Lehrermodell beschnittene Modell eine gute Initialisierung für das Schülermodell, was die Übertragbarkeit zwischen den beiden Netzwerken erhöht.Unser Ansatz führt die Pruning- und Hints-Phase iterativ durch, um die Leistung weiter zu verbessern.Darüber hinaus schlagen wir einen Algorithmus vor, um die Parameter der Hints-Schicht zu rekonstruieren und das beschnittene Modell für Hints besser geeignet zu machen.Experimente wurden mit verschiedenen Aufgaben durchgeführt, darunter Klassifizierung und Posenschätzung.Die Ergebnisse auf CIFAR-10, ImageNet und COCO zeigen die Verallgemeinerung und Überlegenheit unseres Systems.
Für typische Sequenzvorhersageprobleme wie die Sprachgenerierung wird üblicherweise die Maximum-Likelihood-Schätzung (MLE) verwendet, da sie die höchste Wahrscheinlichkeit für das Auftreten der vorhergesagten Sequenz bietet, die am besten mit der tatsächlichen Sequenz übereinstimmt. MLE konzentriert sich jedoch auf eine einmalige Übereinstimmung zwischen der vorhergesagten Sequenz und dem Goldstandard und behandelt folglich alle falschen Vorhersagen als gleich falsch.Wir bezeichnen diesen Nachteil in dieser Arbeit als {\it negative diversity ignorance}.Alle falschen Vorhersagen als gleich zu behandeln, spielt die Nuancen der detaillierten tokenweisen Struktur dieser Sequenzen unfairerweise herunter. Um dem entgegenzuwirken, erweitern wir den MLE-Verlust durch die Einführung eines zusätzlichen Kullback-Leibler-Divergenz-Terms, der durch den Vergleich eines datenabhängigen Gauß-Prior und der detaillierten Trainingsvorhersage abgeleitet wird. Das vorgeschlagene datenabhängige Gauß-Prior-Ziel (D2GPo) ist über eine prioritäre topologische Ordnung der Token definiert und unterscheidet sich deutlich von dem datenunabhängigen Gauß-Prior (L2-Regularisierung), der üblicherweise zur Glättung des MLE-Trainings verwendet wird. Experimentelle Ergebnisse zeigen, dass die vorgeschlagene Methode einen detaillierteren Prior in den Daten effektiv nutzt und eine verbesserte Leistung bei typischen Aufgaben der Spracherzeugung aufweist, einschließlich überwachter und unbeaufsichtigter maschineller Übersetzung, Textzusammenfassung, Geschichtenerzählen und Bildunterschriftenerstellung.
Wir schlagen einen interaktiven Klassifizierungsansatz für natürlichsprachliche Abfragen vor: Anstatt nur die natürlichsprachliche Abfrage zu klassifizieren, bitten wir den Benutzer um zusätzliche Informationen, indem wir eine Abfolge von binären und Multiple-Choice-Fragen verwenden, wobei wir bei jedem Durchgang einen Policy-Controller verwenden, um zu entscheiden, ob wir eine Frage präsentieren oder dem Benutzer die endgültige Antwort geben, und die beste Frage auswählen, um den Informationsgewinn des Systems zu maximieren. Unsere Evaluierung zeigt, dass die Interaktion dem System hilft, seine Genauigkeit zu erhöhen und mit mehrdeutigen Anfragen umzugehen, während unser Ansatz die Anzahl der Fragen und die endgültige Genauigkeit effektiv ausgleicht.
Faltungsneuronale Netze (Convolutional Neural Networks, CNNs) haben bei der Erkennung und Darstellung von Audio, Bildern, Videos und 3D-Volumina, also in Bereichen, in denen die Eingabe durch eine regelmäßige Graphenstruktur charakterisiert werden kann, Spitzenleistungen erzielt. Die Verallgemeinerung von CNNs auf unregelmäßige Domänen wie 3D-Netze ist jedoch eine Herausforderung, da die Trainingsdaten für 3D-Netze oft begrenzt sind. In dieser Arbeit verallgemeinern wir Faltungs-Autoencoder auf Netzoberflächen, führen eine spektrale Zerlegung von Netzen durch und wenden Faltungen direkt im Frequenzraum an. Wir konstruieren einen komplexen Datensatz von 20.466 hochauflösenden Meshes mit extremen Gesichtsausdrücken und kodieren ihn mit unserem Convolutional Mesh Autoencoder.Trotz begrenzter Trainingsdaten übertrifft unsere Methode die modernsten PCA-Modelle von Gesichtern mit einem 50% geringeren Fehler, während sie 75% weniger Parameter benötigt.
Die Berechnung von Abständen zwischen Beispielen ist das Herzstück vieler Lernalgorithmen für Zeitreihen, weshalb viel Arbeit in die Entwicklung effektiver Abstandsmaße für Zeitreihen geflossen ist.Wir präsentieren Jiffy, eine einfache und skalierbare Abstandsmetrik für multivariate Zeitreihen. Unser Ansatz besteht darin, die Aufgabe als ein Problem des Darstellungslernens neu zu formulieren - anstatt eine komplizierte Abstandsfunktion zu entwerfen, verwenden wir ein CNN, um eine Einbettung zu erlernen, so dass der euklidische Abstand effektiv ist.Durch aggressives Max-Pooling und Downsampling sind wir in der Lage, diese Einbettung mit einem sehr kompakten neuronalen Netzwerk zu konstruieren.
Inspiriert von der Funktionalität und Konnektivität des präfrontalen Kortex (PFC) sowie dem Prozess der menschlichen Verhaltensbildung, schlagen wir eine neuartige modulare Architektur neuronaler Netze mit einem Verhaltensmodul (BM) und einer entsprechenden End-to-End-Trainingsstrategie vor.  Diese Eigenschaft ist besonders nützlich für die Benutzermodellierung (z.B. für Dialogagenten) und für Empfehlungsaufgaben, da sie das Lernen personalisierter Repräsentationen verschiedener Benutzerzustände ermöglicht.  Im Experiment mit Videospielen zeigen die Ergebnisse, dass die vorgeschlagene Methode eine Trennung der Ziele und Verhaltensweisen der Hauptaufgabe zwischen verschiedenen BMs ermöglicht.Die Experimente zeigen auch die Erweiterbarkeit des Netzwerks durch unabhängiges Lernen von neuen Verhaltensmustern.Darüber hinaus demonstrieren wir eine Strategie für einen effizienten Transfer von neu gelernten BMs auf unbekannte Aufgaben.
Eine Hauptkomponente der Überanpassung beim modellfreien Reinforcement Learning (RL) ist der Fall, dass der Agent fälschlicherweise die Belohnung mit bestimmten falschen Merkmalen aus den vom Markov Decision Process (MDP) generierten Beobachtungen korreliert. Wenn ein Agent sich an verschiedene Beobachtungsräume anpasst, auch wenn die zugrundeliegende MDP-Dynamik fixiert ist, bezeichnen wir dies als Beobachtungsüberanpassung (observational overfitting). Unsere Experimente zeigen faszinierende Eigenschaften, insbesondere im Hinblick auf die implizite Regularisierung, und bestätigen auch Ergebnisse aus früheren Arbeiten zur RL-Generalisierung und zum überwachten Lernen (SL).
Wir schlagen ein neuronales Sprachmodell vor, das in der Lage ist, unbeaufsichtigt syntaktische Strukturen zu induzieren, und das die Strukturinformationen nutzt, um bessere semantische Repräsentationen und eine bessere Sprachmodellierung zu bilden.Standardmäßige rekurrente neuronale Netze sind durch ihre Struktur begrenzt und können syntaktische Informationen nicht effizient nutzen. In diesem Papier schlagen wir ein neuartiges neuronales Sprachmodell vor, das so genannte Parsing-Reading-Predict Networks (PRPN), das gleichzeitig die syntaktische Struktur aus unkommentierten Sätzen ableiten und die abgeleitete Struktur nutzen kann, um ein besseres Sprachmodell zu erlernen.In unserem Modell kann der Gradient direkt vom Verlust des Sprachmodells in das neuronale Parsing-Netzwerk zurückgeführt werden.Experimente zeigen, dass das vorgeschlagene Modell die zugrundeliegende syntaktische Struktur entdecken und eine Spitzenleistung bei Sprachmodellaufgaben auf Wort-/Zeichenebene erreichen kann.
Unüberwachtes Einbettungslernen zielt darauf ab, gute Repräsentationen aus Daten zu extrahieren, ohne die Verwendung von menschlich kommentierten Etiketten. Solche Techniken sind offensichtlich im Rampenlicht wegen der Herausforderungen bei der Sammlung von massiven Etiketten für überwachtes Lernen erforderlich. Mehrere Verluste, die in Super-AND definiert sind, sorgen dafür, dass ähnliche Proben auch in einem Raum mit geringer Dichte gesammelt werden und die Merkmale gegenüber Augmentierungen invariant bleiben. Infolgedessen übertrifft unser Modell bestehende Ansätze in verschiedenen Benchmark-Datensätzen und erreicht eine Genauigkeit von 89,2 % in CIFAR-10 mit dem Resnet18-Backbone-Netz, was einer Verbesserung von 2,9 % gegenüber dem Stand der Technik entspricht.
Die Analyse histopathologischer Objektträger ist ein entscheidender Schritt für viele Diagnosen, insbesondere in der Onkologie, wo sie den Goldstandard definiert. Bei der digitalen histopathologischen Analyse müssen hochqualifizierte Pathologen riesige Objektträgerbilder mit extremer digitaler Auflösung (100.000^2 Pixel) in mehreren Zoomstufen überprüfen, um abnormale Zellregionen oder in einigen Fällen einzelne Zellen unter Millionen von Zellen zu lokalisieren. Die Anwendung von Deep Learning auf dieses Problem wird nicht nur durch kleine Stichprobengrößen erschwert, da typische Datensätze nur ein paar hundert Proben enthalten, sondern auch durch die Generierung von lokalisierten Annotationen für das Training interpretierbarer Klassifizierungs- und Segmentierungsmodelle. Auch ohne Annotationen auf Pixelebene sind wir in der Lage, eine Leistung zu demonstrieren, die mit Modellen vergleichbar ist, die mit starken Annotationen auf der Camelyon-16-Lymphknoten-Metastasen-Erkennungsherausforderung trainiert wurden. Wir erreichen dies durch die Verwendung von vortrainierten tiefen Faltungsnetzen, Feature-Einbettung sowie Lernen über Top-Instanzen und negative Evidenz, eine Mehrfachinstanz-Lerntechnik aus dem Bereich der semantischen Segmentierung und Objekterkennung.
Eine Herausforderung bei massiven Multi-Label-Problemen besteht darin, dass es oft eine langschwänzige Häufigkeitsverteilung für die Labels gibt, was zu wenigen positiven Beispielen für die seltenen Labels führt. Wir schlagen eine Lösung für dieses Problem vor, indem wir die Ausgabeschicht eines neuronalen Netzwerks modifizieren, um ein Bayes'sches Netzwerk von Sigmoiden zu erstellen, das die ontologischen Beziehungen zwischen den Labels nutzt, um Informationen zwischen den seltenen und den häufigeren Labels zu teilen.  Wir wenden diese Methode auf die beiden massiven Multi-Label-Aufgaben der Krankheitsvorhersage (ICD-9-Codes) und der Proteinfunktionsvorhersage (Gene Ontology-Terms) an und erzielen signifikante Verbesserungen des AUROC pro Label und der durchschnittlichen Präzision.
Generative Adversarial Networks haben die Datengenerierung in verschiedenen Anwendungsfällen ermöglicht, aber im Falle komplexer, hochdimensionaler Verteilungen kann es schwierig sein, sie zu trainieren, da Konvergenzprobleme und das Auftreten von Mode-Kollaps auftreten. Sliced Wasserstein GANs und insbesondere die Anwendung der Max-Sliced Wasserstein-Distanz ermöglichten es, die Wasserstein-Distanz während des Trainings auf effiziente und stabile Weise zu approximieren und halfen, die Konvergenzprobleme dieser Architekturen zu lindern.Diese Methode transformiert die Probenzuordnung und die Distanzberechnung in eine Sortierung der eindimensionalen Projektion der Proben, was eine ausreichende Approximation der hochdimensionalen Wasserstein-Distanz ergibt. In diesem Beitrag wird gezeigt, dass die Annäherung der Wasserstein-Distanz durch die Sortierung der Stichproben nicht immer der optimale Ansatz ist und die gierige Zuordnung der echten und unechten Stichproben zu einer schnelleren Konvergenz und besseren Annäherung an die ursprüngliche Verteilung führen kann.
Beim lebenslangen maschinellen Lernen geht es darum, sich an neue Aufgaben anzupassen, ohne die alten Aufgaben zu vergessen, während beim few-shot learning eine einzelne Aufgabe mit einer geringen Datenmenge erlernt werden soll.Diese beiden unterschiedlichen Forschungsbereiche sind für die künstliche allgemeine Intelligenz von entscheidender Bedeutung, allerdings sind die vorhandenen Studien beim Training der Modelle von einigen unpraktischen Einstellungen ausgegangen.Beim lebenslangen Lernen wird davon ausgegangen, dass die Art (oder die Menge) der während der Inferenzzeit eingehenden Aufgaben zum Trainingszeitpunkt bekannt ist.Beim few-shot learning wird üblicherweise davon ausgegangen, dass während des Trainings eine große Anzahl von Aufgaben verfügbar ist. Inspiriert von der Arbeitsweise des menschlichen Gehirns schlagen wir ein neuartiges Modell vor, das Slow Thinking to Learn (STL) genannt wird und durch die iterative Berücksichtigung von Interaktionen zwischen aktuellen und zuvor gesehenen Aufgaben zur Laufzeit anspruchsvolle (und etwas langsamere) Vorhersagen macht.
Hypernetworks sind neuronale Metanetzwerke, die Gewichte für ein neuronales Hauptnetzwerk in einer durchgängig differenzierbaren Weise generieren.Trotz umfangreicher Anwendungen, die vom Multi-Task-Lernen bis zum Bayesian Deep Learning reichen, wurde das Problem der Optimierung von Hypernetworks bisher nicht untersucht. Wir stellen fest, dass klassische Gewichtungsinitialisierungsmethoden wie Glorot & Bengio (2010) und He et al. (2015), wenn sie direkt auf ein Hypernetz angewendet werden, nicht in der Lage sind, Gewichte für das Hauptnetz in der richtigen Größenordnung zu erzeugen.Wir entwickeln prinzipielle Techniken für die Gewichtungsinitialisierung in Hypernetzen und zeigen, dass sie zu stabileren Hauptnetzgewichten, geringerem Trainingsverlust und schnellerer Konvergenz führen.
Für die bidirektionale gemeinsame Modellierung von Bild und Text entwickeln wir das Variational Hetero-Encoder (VHE) Randomized Generative Adversarial Network (GAN), ein vielseitiges tiefes generatives Modell, das einen probabilistischen Textdecoder, einen probabilistischen Bildcodierer und ein GAN in einen kohärenten End-to-End-Multimodalitäts-Lernrahmen integriert.VHE Randomized GAN (VHE-GAN) codiert ein Bild, um den zugehörigen Text zu decodieren, und speist das Variational Posterior als Zufallsquelle in den GAN-Bildgenerator ein. Wir fügen drei handelsübliche Module, darunter ein tiefes Themenmodell, einen ladder-structured image encoder und StackGAN++, in VHE-GAN ein, das bereits eine konkurrenzfähige Leistung erreicht, was uns zur Entwicklung von VHE-raster-scan-GAN motiviert, das fotorealistische Bilder nicht nur in mehreren Maßstäben von niedriger bis hoher Auflösung, sondern auch in einer hierarchisch-semantischen Grob-zu-Fein-Methode erzeugt. Durch die Erfassung und Verknüpfung von hierarchischen semantischen und visuellen Konzepten mit einem durchgängigen Training erreicht VHE-raster-scan-GAN eine Spitzenleistung in einer Vielzahl von multimodalen Bild-Text-Lern- und Generierungsaufgaben.
Die meisten klassischen Planer sind sehr erfolgreich bei der Suche nach (nicht-optimalen) Plänen, selbst für große Planungsinstanzen, und stützen sich dabei auf eine Vorverarbeitungsstufe, die eine geerdete Darstellung der Aufgabe berechnet, Um dieses Problem anzugehen, führen wir einen Ansatz zur partiellen Erdung ein, der nur eine Projektion der Aufgabe erdet, wenn eine vollständige Erdung nicht möglich ist.Wir schlagen einen Leitmechanismus vor, der für eine gegebene Domäne die Teile einer Aufgabe identifiziert, die für die Suche nach einem Plan relevant sind, indem er handelsübliche Methoden des maschinellen Lernens einsetzt.Unsere empirische Bewertung bestätigt, dass der Ansatz in der Lage ist, Planungsinstanzen zu lösen, die zu groß sind, um vollständig geerdet zu werden.
In diesem Beitrag stellen wir eine neuartige Methode zur Interpretation von rekurrenten neuronalen Netzen (RNNs) vor, insbesondere von Netzen mit langem Kurzzeitgedächtnis (LSTMs) auf zellulärer Ebene. Wir schlagen eine systematische Pipeline zur Interpretation der Dynamik einzelner verborgener Zustände innerhalb des Netzes unter Verwendung von Antwortcharakterisierungsmethoden vor. Als Ergebnis ist unsere Methode in der Lage, Neuronen mit aufschlussreicher Dynamik eindeutig zu identifizieren, Beziehungen zwischen dynamischen Eigenschaften und Testgenauigkeit durch Ablationsanalyse zu quantifizieren und die Auswirkungen der Netzwerkkapazität auf die dynamische Verteilung eines Netzwerks zu interpretieren.Schließlich demonstrieren wir die Verallgemeinerbarkeit und Skalierbarkeit unserer Methode durch die Auswertung einer Reihe von verschiedenen sequenziellen Benchmark-Datensätzen.
Der entorhinale Kortex (EC) des Säugetiergehirns enthält eine Vielzahl räumlicher Korrelate, darunter Gitterzellen, die den Raum mit Hilfe von Mosaikmustern kodieren. Die Mechanismen und die funktionelle Bedeutung dieser räumlichen Repräsentationen bleiben jedoch weitgehend rätselhaft.Um diese neuronalen Repräsentationen besser zu verstehen, haben wir rekurrente neuronale Netze (RNNs) trainiert, um Navigationsaufgaben in 2D-Arenen auf der Grundlage von Geschwindigkeitseingaben durchzuführen. Überraschenderweise stellen wir fest, dass in den trainierten Netzen gitterartige räumliche Reaktionsmuster auftreten, zusammen mit Einheiten, die andere räumliche Korrelate aufweisen, einschließlich Randzellen und bandartige Zellen.all diese verschiedenen funktionellen Arten von Neuronen wurden experimentell beobachtet.die Reihenfolge des Auftretens von gitterartigen und Randzellen stimmt auch mit Beobachtungen aus Entwicklungsstudien überein.zusammengenommen deuten unsere Ergebnisse darauf hin, dass Gitterzellen, Randzellen und andere, wie sie in der EG beobachtet werden, angesichts der vorherrschenden rekurrenten Verbindungen in den neuronalen Schaltkreisen eine natürliche Lösung für die effiziente Darstellung von Raum sein können.
Während End-to-End-Modelle, die direkt die Wellenform generieren, bei vielen Audiosyntheseproblemen Stand der Technik sind, generieren die besten Multi-Instrumenten-Quellentrennungsmodelle Masken auf dem Magnitudenspektrum und erreichen Leistungen, die weit über den aktuellen End-to-End-Wellenform-Modellen liegen. Wir stellen eine eingehende Analyse einer neuen Architektur vor, die wir Demucs nennen und die auf einem (transponierten) Faltungs-Autoencoder mit einem bidirektionalen LSTM auf der Bottleneck-Schicht und Skip-Verbindungen wie in U-Networks (Ronneberger et al., Verglichen mit dem State-of-the-Art-Waveform-to-Waveform-Modell Wave-U-Net (Stoller et al., 2018) sind die Hauptmerkmale unseres Ansatzes neben dem Bi-LSTM die Verwendung von trans-posed Faltungsschichten anstelle von Upsampling-Faltungsblöcken, die Verwendung von gated linearen Einheiten, die exponentielle Zunahme der Anzahl der Kanäle mit der Tiefe und eine neue sorgfältige Initialisierung der Gewichte.  Die Ergebnisse auf dem MusDB-Datensatz zeigen, dass unsere Architektur ein um fast 2,2 Punkte höheres Signal-Verzerrungs-Verhältnis (SDR) erreicht als der beste Waveform-to-Waveform-Konkurrent (von 3,2 bis 5,4 SDR). Damit erreicht unser Modell die gleiche Leistung wie der Stand der Technik auf diesem Datensatz und schließt die Leistungslücke zwischen Modellen, die auf dem Spektrogramm arbeiten, und End-to-End-Ansätzen.
Obwohl die Bewertung von Strategieprofilen in großen verbundenen Lernernetzwerken eine Herausforderung darstellt, ist sie für die nächste Welle von Anwendungen des maschinellen Lernens von entscheidender Bedeutung.Kürzlich wurde $\alpha$-Rank, ein evolutionärer Algorithmus, als Lösung für die Einstufung gemeinsamer Strategieprofile in Multiagentensystemen vorgeschlagen. In diesem Beitrag beweisen wir formal, dass diese Behauptung nicht begründet ist, und zeigen, dass $\alpha$-Rank eine exponentielle Komplexität mit der Anzahl der Agenten aufweist, was seine Anwendung über eine kleine endliche Anzahl von gemeinsamen Profilen hinaus behindert. Unsere Methode kombiniert evolutionäre Dynamik mit stochastischer Optimierung und doppelten Orakeln für ein wirklich skalierbares Ranking mit linearer Zeit- und Speicherkomplexität (in der Anzahl der Agenten). Unsere Beiträge erlauben es uns zum ersten Mal, groß angelegte Evaluierungsexperimente von Multi-Agenten-Systemen durchzuführen, bei denen wir erfolgreiche Ergebnisse auf großen gemeinsamen Strategieprofilen mit Größen in der Größenordnung von $\mathcal{O}(2^{25})$ (d.h.) zeigen, $\mathcal{O}(2^{25})$ (d.h. ca. $\text{$33$ Millionen Strategien}$) - eine Einstellung, die mit aktuellen Techniken nicht evaluierbar ist.
Zahlreiche Anstrengungen wurden unternommen, um die Annotationskosten beim Lernen mit tiefen Netzwerken zu reduzieren. Zwei prominente Richtungen sind das Lernen mit verrauschten Etiketten und das halbüberwachte Lernen durch die Nutzung von nicht etikettierten Daten.In dieser Arbeit schlagen wir DivideMix vor, einen neuartigen Rahmen für das Lernen mit verrauschten Etiketten durch die Nutzung von halbüberwachten Lerntechniken. Insbesondere modelliert DivideMix die Verlustverteilung pro Probe mit einem Mischmodell, um die Trainingsdaten dynamisch in einen beschrifteten Satz mit sauberen Proben und einen unbeschrifteten Satz mit verrauschten Proben aufzuteilen, und trainiert das Modell sowohl auf den beschrifteten als auch auf den unbeschrifteten Daten in einer halbüberwachten Weise. Während der halbüberwachten Trainingsphase verbessern wir die MixMatch-Strategie, indem wir eine gemeinsame Verfeinerung der Beschriftung und eine gemeinsame Einschätzung der Beschriftung für beschriftete bzw. unbeschriftete Proben durchführen. Experimente mit mehreren Benchmark-Datensätzen zeigen erhebliche Verbesserungen gegenüber den modernsten Methoden. Der Code ist unter https://github.com/LiJunnan1992/DivideMix verfügbar.
Wir stellen einen neuen Algorithmus zum Trainieren eines robusten neuronalen Netzes gegen Angriffe von außen vor. Unser Algorithmus ist durch die folgenden zwei Ideen motiviert: Erstens, obwohl neuere Arbeiten gezeigt haben, dass die Integration von Zufälligkeit die Robustheit neuronaler Netze verbessern kann (Liu 2017), haben wir festgestellt, dass das blinde Hinzufügen von Rauschen zu allen Schichten nicht der optimale Weg ist, um Zufälligkeit einzubauen. Stattdessen modellieren wir die Zufälligkeit im Rahmen des Bayes'schen Neuronalen Netzes (BNN), um die posteriore Verteilung der Modelle auf eine skalierbare Art und Weise formal zu erlernen, und formulieren das Mini-Max-Problem im BNN, um die beste Modellverteilung unter nachteiligen Angriffen zu erlernen, was zu einem nachteilig trainierten Bayes'schen Neuronalen Netz führt. Experimentelle Ergebnisse zeigen, dass der vorgeschlagene Algorithmus unter starken Angriffen die beste Leistung erzielt: Auf CIFAR-10 mit VGG-Netzwerk führt unser Modell zu einer 14%igen Verbesserung der Genauigkeit im Vergleich zu gegnerischem Training (Madry 2017) und zufälligem Self-Ensemble (Liu, 2017) unter PGD-Angriff mit 0,035 Verzerrung, und der Abstand wird sogar noch größer auf einer Teilmenge von ImageNet.
Föderiertes Lernen verteilt das Modelltraining auf eine Vielzahl von Agenten, die unter Berücksichtigung des Datenschutzes das Training mit ihren lokalen Daten durchführen, aber nur die Aktualisierungen der Modellparameter zur iterativen Aggregation auf dem Server teilen. In dieser Arbeit untersuchen wir die Bedrohung durch Modellvergiftungsangriffe auf föderiertes Lernen, die von einem einzelnen, nicht kolludierenden böswilligen Agenten initiiert werden, dessen Ziel es ist, das Modell dazu zu bringen, einen Satz ausgewählter Eingaben mit hohem Vertrauen falsch zu klassifizieren.Wir untersuchen eine Reihe von Strategien zur Durchführung dieses Angriffs, beginnend mit der einfachen Verstärkung der Aktualisierung des böswilligen Agenten, um die Auswirkungen der Aktualisierungen anderer Agenten zu überwinden. Um die Heimlichkeit des Angriffs zu erhöhen, schlagen wir eine alternierende Minimierungsstrategie vor, die abwechselnd den Trainingsverlust und das gegnerische Ziel optimiert.Wir verwenden anschließend eine Parameterschätzung für die Aktualisierungen der gutartigen Agenten, um den Angriffserfolg zu verbessern.Schließlich verwenden wir eine Reihe von Interpretationsverfahren, um visuelle Erklärungen von Modellentscheidungen sowohl für gutartige als auch für bösartige Modelle zu generieren, und zeigen, dass die Erklärungen visuell kaum zu unterscheiden sind. Unsere Ergebnisse zeigen, dass selbst ein stark eingeschränkter Angreifer Modellvergiftungsangriffe durchführen und gleichzeitig unbemerkt bleiben kann, was die Verwundbarkeit der föderierten Lernumgebung und die Notwendigkeit der Entwicklung effektiver Verteidigungsstrategien verdeutlicht.
Trotz der rasanten Fortschritte in der Spracherkennung reagieren aktuelle Modelle empfindlich auf oberflächliche Störungen ihrer Eingaben. Geringe Mengen an Rauschen können die Leistung eines ansonsten hochmodernen Modells zerstören. Um Modelle gegen Hintergrundrauschen zu härten, führen Praktiker häufig eine Datenerweiterung durch, indem sie künstlich verrauschte Beispiele zur Trainingsmenge hinzufügen und die ursprüngliche Kennzeichnung übernehmen. In diesem Papier stellen wir die Hypothese auf, dass ein sauberes Beispiel und seine oberflächlich gestörten Gegenstücke nicht nur derselben Klasse zugeordnet werden sollten, sondern auch derselben Repräsentation.Wir schlagen das invariante Repräsentationslernen (IRL) vor: Bei jeder Trainingsiteration nehmen wir für jedes Trainingsbeispiel ein verrauschtes Gegenstück und wenden dann einen Strafterm an, um übereinstimmende Repräsentationen auf jeder Schicht (oberhalb einer ausgewählten Schicht) zu erzwingen.(i) IRL reduziert signifikant die Zeichenfehlerraten (CER) sowohl bei "sauberen" (3. 3% vs. 6,5%) und "andere" (11,0% vs. 18,1%) Test-Sets;(ii) auf mehrere Out-of-Domain-Noise-Einstellungen (anders als die während des Trainings gesehen), IRL's Vorteile sind sogar noch ausgeprägter.Sorgfältige Ablationen bestätigen, dass unsere Ergebnisse nicht einfach auf schrumpfende Aktivierungen an den ausgewählten Schichten sind.
Dieses Modell ordnet konventionelle Faltung und spärliche Faltung so an, dass die globalen harmonischen Muster, die von der spärlichen Faltung erfasst werden, aus einer ausreichenden Anzahl lokaler Muster bestehen, die von den Schichten der konventionellen Faltung erfasst werden.Wenn das harmonische Modell auf dem MAPS-Datensatz trainiert wird, übertrifft es alle bestehenden Tonhöhenerkennungssysteme, die auf dem gleichen Datensatz trainiert wurden. Am beeindruckendsten ist, dass das harmonische Modell mit einer LSTM-Schicht auf dem MAPS-Datensatz ein aktuelles, komplexeres Tonhöhenerkennungssystem übertrifft, das auf dem MAESTRO-Datensatz trainiert wurde, auf den eine komplizierte Datenerweiterung angewandt wird und dessen Trainingssplit um eine Größenordnung größer ist als der Trainingssplit von MAPS.Das harmonische Modell hat gezeigt, dass es für fortschrittliche automatische Musiktranskriptionssysteme (AMT) verwendet werden kann.
Bisherige Methoden, die auf Domäneninvarianz basieren, übersehen jedoch die zugrundeliegende Abhängigkeit der Klassen von den Domänen, die für den Kompromiss zwischen Klassifikationsgenauigkeit und Invarianz verantwortlich ist. Diese Studie schlägt eine neuartige Methode vor, die die Domäneninvarianz innerhalb eines Bereichs maximiert, der die Genauigkeit nicht beeinträchtigt. Empirische Validierungen zeigen, dass die Leistung von AFLAC der von Basismethoden überlegen ist, was die Bedeutung der Berücksichtigung der Abhängigkeit und die Wirksamkeit der vorgeschlagenen Methode zur Überwindung des Problems unterstützt.
Nach ihrer erfolgreichen Anwendung in der Bildverarbeitung und im Repräsentationslernen ist ein wichtiger nächster Schritt die Betrachtung von Videos. Das Erlernen generativer Modelle für Videos ist eine viel schwierigere Aufgabe, da ein Modell die zeitliche Dynamik einer Szene zusätzlich zur visuellen Darstellung von Objekten erfassen muss. Während die jüngsten generativen Modelle von Video haben einige Erfolge gehabt, ist der aktuelle Fortschritt durch das Fehlen von qualitativen Metriken, die visuelle Qualität, zeitliche Kohärenz und Vielfalt der Proben berücksichtigen behindert.Zu diesem Zweck schlagen wir Fréchet Video Distance (FVD), eine neue Metrik für generative Modelle von Video auf FID basiert.Wir tragen eine groß angelegte menschliche Studie, die bestätigt, dass FVD korreliert gut mit qualitativen menschlichen Beurteilung der erzeugten Videos.
Trotz der Fortschritte im Deep Learning lernen künstliche neuronale Netze nicht auf die gleiche Weise wie Menschen.Heute können neuronale Netze mehrere Aufgaben lernen, wenn sie gemeinsam trainiert werden, können aber die Leistung bei gelernten Aufgaben nicht beibehalten, wenn die Aufgaben einzeln präsentiert werden - dieses Phänomen, das katastrophales Vergessen genannt wird, ist eine grundlegende Herausforderung, die überwunden werden muss, bevor neuronale Netze kontinuierlich aus eingehenden Daten lernen können.In dieser Arbeit lassen wir uns vom menschlichen Gedächtnis inspirieren, um eine Architektur zu entwickeln, die in der Lage ist, kontinuierlich aus sequentiell eingehenden Aufgaben zu lernen und gleichzeitig katastrophales Vergessen zu vermeiden. Unser Modell besteht aus einer dualen Speicherarchitektur, die die komplementären Lernsysteme (Hippocampus und Neocortex) des menschlichen Gehirns nachahmt und ein konsolidiertes Langzeitgedächtnis durch generative Wiedergabe vergangener Erfahrungen aufrechterhält.Wir(i) untermauern unsere Behauptung, dass die Wiedergabe generativ sein sollte,(ii) zeigen die Vorteile der generativen Wiedergabe und des dualen Speichers anhand von Experimenten, und(iii) demonstrieren eine verbesserte Leistungserhaltung selbst für kleine Modelle mit geringer Kapazität.Unsere Architektur weist viele wichtige Merkmale des menschlichen Gedächtnisses auf und bietet Einblicke in den Zusammenhang zwischen Schlaf und Lernen beim Menschen.
Wir stellen LAMOL vor, eine einfache, aber effektive Methode für lebenslanges Sprachenlernen (LLL), die auf Sprachmodellierung basiert.LAMOL spielt Pseudo-Proben früherer Aufgaben nach, ohne dass zusätzlicher Speicher oder Modellkapazität benötigt wird.LAMOL ist ein Sprachmodell, das gleichzeitig lernt, die Aufgaben zu lösen und Trainingsproben zu erzeugen. Die Ergebnisse zeigen, dass LAMOL ein katastrophales Vergessen ohne Anzeichen von Unnachgiebigkeit verhindert und fünf sehr unterschiedliche Sprachaufgaben nacheinander mit nur einem Modell lösen kann. Insgesamt übertrifft LAMOL frühere Methoden deutlich und ist nur 2-3% schlechter als Multitasking, das üblicherweise als Obergrenze für LLL angesehen wird. Der Quellcode ist unter https://github.com/jojotenya/LAMOL verfügbar.
Eine diskrete Sequenz von Wörtern kann viel einfacher in nachgelagerte neuronale Schichten integriert werden, wenn sie als eine Sequenz kontinuierlicher Vektoren dargestellt wird. Auch semantische Beziehungen zwischen Wörtern, die aus einem Textkorpus gelernt wurden, können in den relativen Konfigurationen der Einbettungsvektoren kodiert werden. Die Speicherung und der Zugriff auf die Einbettungsvektoren für alle Wörter in einem Wörterbuch erfordert jedoch eine große Menge an Speicherplatz und kann Systeme mit begrenztem GPU-Speicher beeinträchtigen. wort2ket und word2ketXS sind zwei verwandte Methoden, die vom Quantencomputing inspiriert sind und die Speicherung der Einbettungsmatrix von Wörtern während des Trainings und der Inferenz auf eine hocheffiziente Weise ermöglichen.
Einer der charakteristischen Aspekte der menschlichen Sprache ist ihre Kompositionalität, die es uns erlaubt, komplexe Umgebungen mit begrenztem Vokabular zu beschreiben. Bisher wurde gezeigt, dass Agenten in neuronalen Netzen lernen können, in einer hochstrukturierten, möglicherweise kompositionellen Sprache auf der Grundlage von unstrukturiertem Input (z.B. handgefertigten Merkmalen) zu kommunizieren. Menschen lernen jedoch nicht, auf der Grundlage von gut zusammengefassten Merkmalen zu kommunizieren. In dieser Arbeit trainieren wir neuronale Agenten, um gleichzeitig eine visuelle Wahrnehmung aus rohen Bildpixeln zu entwickeln und zu lernen, mit einer Folge von diskreten Symbolen zu kommunizieren. Die Agenten spielen ein Bildbeschreibungsspiel, bei dem das Bild Faktoren wie Farben und Formen enthält. Wir trainieren die Agenten mit der Obverter-Technik, bei der ein Agent introspektiv Nachrichten generiert, die sein eigenes Verständnis maximieren. Durch qualitative Analyse, Visualisierung und einen Zero-Shot-Test zeigen wir, dass die Agenten aus rohen Bildpixeln eine Sprache mit kompositorischen Eigenschaften entwickeln können, wenn ein angemessener Druck von der Umgebung ausgeht.
Die Fähigkeit, eine Reihe von wahrscheinlichen, aber unterschiedlichen möglichen zukünftigen Verhaltensweisen eines Agenten (z.B. zukünftige Trajektorien eines Fußgängers) vorherzusagen, ist für sicherheitskritische Wahrnehmungssysteme (z.B. autonome Fahrzeuge) unerlässlich, Insbesondere muss eine Menge möglicher zukünftiger Verhaltensweisen, die vom System generiert werden, vielfältig sein, um alle möglichen Ergebnisse zu berücksichtigen, damit die notwendigen Sicherheitsvorkehrungen getroffen werden können.Es reicht nicht aus, eine Menge der wahrscheinlichsten zukünftigen Ergebnisse zu erhalten, da die Menge möglicherweise nur Störungen eines einzigen dominierenden Ergebnisses (Hauptmodus) enthält.) Während generative Modelle wie Variations-Autoencoder (VAEs) sich als leistungsfähiges Werkzeug zum Erlernen einer Verteilung über zukünftige Trajektorien erwiesen haben, können zufällig gezogene Stichproben aus dem erlernten impliziten Likelihood-Modell nicht vielfältig sein - das Likelihood-Modell ist von der Verteilung der Trainingsdaten abgeleitet und die Stichproben werden sich um den Hauptmodus der Daten konzentrieren. In dieser Arbeit schlagen wir vor, eine Diversity-Sampling-Funktion (DSF) zu erlernen, die einen vielfältigen, aber wahrscheinlichen Satz zukünftiger Trajektorien erzeugt: Die DSF bildet Vorhersagekontextmerkmale auf einen Satz latenter Codes ab, die von einem generativen Modell (z. B. VAE) in einen Satz vielfältiger Trajektorien decodiert werden können, Um die Parameter des DSF zu erlernen, wird die Diversität der Trajektorienmuster durch einen Diversitätsverlust auf der Grundlage eines deterministischen Punktprozesses (DPP) bewertet.) Unsere Methode ist eine neuartige Anwendung von DPPs zur Optimierung eines Satzes von Elementen (prognostizierte Trajektorien) im kontinuierlichen Raum. Wir demonstrieren die Vielfalt der Trajektorien, die durch unseren Ansatz sowohl bei niedrigdimensionalen 2D-Trajektoriendaten als auch bei hochdimensionalen menschlichen Bewegungsdaten erzeugt werden.
Es gibt immer mehr Belege dafür, dass Pretraining für neuronale Netzwerk-Sprachverstehensmodelle wertvoll sein kann, aber wir haben noch kein klares Verständnis dafür, wie die Wahl des Pretraining-Ziels die Art der linguistischen Informationen beeinflusst, die Modelle lernen. Wir stellen fest, dass Repräsentationen von Sprachmodellen bei unseren syntaktischen Hilfsvorhersageaufgaben durchweg am besten abschneiden, selbst wenn sie auf relativ kleinen Datenmengen trainiert werden, was darauf hindeutet, dass die Sprachmodellierung die beste datenreiche Vortrainingsaufgabe für Transfer-Learning-Anwendungen sein könnte, die syntaktische Informationen erfordern.Wir stellen auch fest, dass ein zufällig initialisiertes, eingefrorenes Modell bei unseren Hilfsaufgaben auffallend gut abschneiden kann, aber dass dieser Effekt verschwindet, wenn die Menge der Trainingsdaten für die Hilfsaufgaben reduziert wird.
Wir betrachten das Problem der Generierung von Konfigurationen, die physikalische Bedingungen für ein optimales Design von Material-Nanomustern erfüllen, wobei mehrere (und oft widersprüchliche) Eigenschaften gleichzeitig erfüllt werden müssen.  Man denke beispielsweise an den Kompromiss zwischen Wärmewiderstand, elektrischer Leitfähigkeit und mechanischer Stabilität, der erforderlich ist, um ein nanoporöses Muster mit optimaler thermoelektrischer Effizienz zu entwerfen.  Zu diesem Zweck nutzen wir den Rahmen der posterioren Regularisierung und zeigen, dass dieses Problem der Erfüllung von Beschränkungen als Stichprobenziehung aus einer Gibbs-Verteilung formuliert werden kann.  Um diese Schwierigkeiten zu überwinden, führen wir die Surrogat-basierte eingeschränkte Langevin-Dynamik für das Black-Box-Sampling ein.Wir untersuchen zwei Surrogat-Ansätze.Der erste Ansatz nutzt die Annäherung der Gradienten nullter Ordnung im Langevin-Sampling und wir bezeichnen ihn als Langevin nullter Ordnung.In der Praxis kann dieser Ansatz unerschwinglich sein, da wir immer noch häufig die teuren PDE-Löser abfragen müssen. Der zweite Ansatz approximiert die Gradienten in der Langevin-Dynamik mit tiefen neuronalen Netzen, was uns eine effiziente Sampling-Strategie unter Verwendung des Surrogatmodells ermöglicht. wir beweisen die Konvergenz dieser beiden Ansätze, wenn die Zielverteilung log-konkav und glatt ist. wir zeigen die Effektivität beider Ansätze bei der Entwicklung optimaler nano-poröser Materialkonfigurationen, bei denen das Ziel darin besteht, Nano-Muster-Vorlagen mit niedriger Wärmeleitfähigkeit und angemessener mechanischer Stabilität herzustellen.
Es gibt ein wachsendes Interesse an geometrisch inspirierten Einbettungen für das Lernen von Hierarchien, partiellen Ordnungen und Gitterstrukturen, mit natürlichen Anwendungen auf transitive relationale Daten wie Entailment-Graphen. Jüngste Arbeiten haben diese Ideen über deterministische Hierarchien hinaus auf probabilistisch kalibrierte Modelle ausgedehnt, die das Lernen aus unsicherer Überwachung und das Ableiten von Soft-Inklusionen zwischen Konzepten ermöglichen, während die geometrisch-induktive Ausrichtung von hierarchischen Einbettungsmodellen erhalten bleibt. Wir bauen auf dem Box Lattice-Modell von Vilnis et al. (2018) auf, das vielversprechende Ergebnisse bei der Modellierung von Soft-Inclusions durch eine überlappende Hierarchie von Mengen zeigt, die als hochdimensionale Hyperrechtecke (Boxen) parametrisiert sind. Die harten Kanten der Boxen stellen jedoch Schwierigkeiten für die standardmäßige gradientenbasierte Optimierung dar; in dieser Arbeit wurde eine spezielle Ersatzfunktion für den disjunkten Fall verwendet, aber wir halten diese Methode für anfällig.  In dieser Arbeit stellen wir ein neuartiges hierarchisches Einbettungsmodell vor, inspiriert durch eine Entspannung von Box-Einbettungen in parametrisierte Dichtefunktionen unter Verwendung von Gaußschen Faltungen über die Boxen. Wir demonstrieren eine verbesserte oder übereinstimmende Leistung bei der Vorhersage von WordNet-Hypernymen, beim Entailment von Flickr-Beschriftungen und bei einem MovieLens-basierten Marktkorb-Datensatz und zeigen besonders deutliche Verbesserungen im Fall von spärlichen Daten, bei denen viele bedingte Wahrscheinlichkeiten niedrig sein sollten und daher die Boxen nahezu disjunkt sein sollten.
Wir stellen einen schwach überwachten Ansatz zur Datenerweiterung vor, um die Erkennung von benannten Entitäten (NER) in einem anspruchsvollen Bereich zu verbessern: die Extraktion biomedizinischer Entitäten (z.B. Proteine) aus der wissenschaftlichen Literatur.Zunächst trainieren wir ein neuronales NER-Modell (NNER) über einen kleinen Satz vollständig beschrifteter Beispiele.Zweitens verwenden wir einen Referenzsatz von Entitätsnamen (z.B. Proteine in UniProt), um die Erkennung von Entitäten zu verbessern, Zweitens verwenden wir eine Referenzmenge von Entitätsnamen (z.B. Proteine in UniProt), um Entitätserwähnungen mit hoher Präzision, aber niedrigem Wiedererkennungswert in einem unmarkierten Korpus zu identifizieren.Drittens verwenden wir das NNER-Modell, um dem Korpus schwache Markierungen zuzuweisen.Schließlich trainieren wir unser NNER-Modell iterativ über die erweiterte Trainingsmenge, einschließlich der Saat, der Beispiele der Referenzmenge und der schwach markierten Beispiele, was zu verfeinerten Markierungen führt.Wir zeigen empirisch, dass dieser erweiterte Bootstrapping-Prozess die NER-Leistung erheblich verbessert, und diskutieren die Faktoren, die die Wirksamkeit des Ansatzes beeinflussen.
Während die Verfügbarkeit von Daten für das Training von maschinellen Lernmodellen ständig zunimmt, ist es oft viel einfacher, Merkmalsvektoren zu sammeln, als die entsprechenden Bezeichnungen zu erhalten.Einer der Ansätze zur Lösung dieses Problems ist die Verwendung von halbüberwachtem Lernen, das nicht nur die bezeichneten Beispiele, sondern auch die nicht bezeichneten Merkmalsvektoren nutzt. Der Algorithmus nutzt die jüngsten Fortschritte in der Quanten-Sample-basierten Hamilton-Simulation, um den bestehenden Quanten-LS-SVM-Algorithmus so zu erweitern, dass er mit dem semi-supervised-Term im Verlust umgehen kann, während er die gleiche Quantengeschwindigkeit wie die Quanten-LS-SVM beibehält.
In unserer Arbeit schlagen wir eine Brücke zwischen dem Design von tiefen neuronalen Netzen und numerischen Differentialgleichungen und zeigen, dass viele effektive Netze wie ResNet, PolyNet, FractalNet und RevNet als verschiedene numerische Diskretisierungen von Differentialgleichungen interpretiert werden können, was uns eine völlig neue Perspektive auf das Design effektiver tiefer Architekturen eröffnet. Als Beispiel schlagen wir eine lineare Mehrschritt-Architektur (LM-Architektur) vor, die von der linearen Mehrschritt-Methode zur Lösung gewöhnlicher Differentialgleichungen inspiriert ist und eine effektive Struktur darstellt, die für alle ResNet-artigen Netzwerke verwendet werden kann. Insbesondere zeigen wir, dass LM-ResNet und LM-ResNeXt (d. h. die Netze, die durch Anwendung der LM-Architektur auf ResNet bzw. ResNeXt erhalten werden) sowohl bei CIFAR als auch bei ImageNet mit einer vergleichbaren Anzahl von trainierbaren Parametern eine deutlich höhere Genauigkeit als ResNet und ResNeXt erreichen können, und dass LM-ResNet/LM-ResNeXt sowohl bei CIFAR als auch bei ImageNet die ursprünglichen Netze bei ähnlicher Leistung erheblich komprimieren kann (>50%), was mathematisch mit dem Konzept der modifizierten Gleichung aus der numerischen Analyse erklärt werden kann. Nicht zuletzt stellen wir auch eine Verbindung zwischen stochastischer Kontrolle und Rauschinjektion im Trainingsprozess her, die zur Verbesserung der Generalisierung der Netze beiträgt. indem wir die stochastische Trainingsstrategie mit dem stochastischen dynamischen System in Verbindung bringen, können wir das stochastische Training leicht auf die Netze mit der LM-Architektur anwenden. als Beispiel haben wir die stochastische Tiefe in LM-ResNet eingeführt und eine signifikante Verbesserung gegenüber dem ursprünglichen LM-ResNet auf CIFAR10 erreicht.
Die Umwandlung eines Screenshots einer grafischen Benutzeroberfläche, der von einem Designer erstellt wurde, in Computercode ist eine typische Aufgabe, die von einem Entwickler durchgeführt wird, um kundenspezifische Software, Websites und mobile Anwendungen zu erstellen. In diesem Papier zeigen wir, dass Deep-Learning-Methoden genutzt werden können, um ein Modell durchgängig zu trainieren, um automatisch Code aus einem einzigen Eingabebild mit einer Genauigkeit von über 77 % für drei verschiedene Plattformen (d. h. iOS, Android und webbasierte Technologien) zu erzeugen.
Bildverarbeitungsaufgaben wie Bildklassifizierung, Bildabfrage und Lernen mit wenigen Bildern werden derzeit von euklidischen und sphärischen Einbettungen dominiert, so dass die endgültigen Entscheidungen über die Klassenzugehörigkeit oder den Grad der Ähnlichkeit mit Hilfe von linearen Hyperebenen, euklidischen Abständen oder sphärischen geodätischen Abständen (Kosinusähnlichkeit) getroffen werden.In dieser Arbeit zeigen wir, dass hyperbolische Einbettungen in vielen praktischen Szenarien eine bessere Alternative darstellen.
Da die menschliche Kognition nicht dafür optimiert ist, in hochdimensionalen Räumen zu arbeiten, könnten diese Bereiche von interpretierbaren niedrigdimensionalen Repräsentationen profitieren. Die meisten Algorithmen zum Erlernen von Repräsentationen für Zeitreihendaten sind jedoch schwer zu interpretieren, da die Abbildungen von Datenmerkmalen auf hervorstechende Eigenschaften der Repräsentation nicht intuitiv sind und die Repräsentation nicht gleichmäßig über die Zeit verläuft. Um dieses Problem zu lösen, schlagen wir einen neuen Rahmen für das Lernen von Repräsentationen vor, der auf Ideen aus der interpretierbaren diskreten Dimensionalitätsreduktion und der tiefen generativen Modellierung aufbaut und es uns ermöglicht, diskrete Repräsentationen von Zeitreihen zu lernen, die zu glatten und interpretierbaren Einbettungen mit überlegener Clusterleistung führen. Wir stellen eine neue Methode zur Überwindung der Nicht-Differenzierbarkeit beim Lernen diskreter Repräsentationen vor und präsentieren eine gradientenbasierte Version des traditionellen selbstorganisierenden Kartenalgorithmus, die leistungsfähiger ist als das Original.um eine probabilistische Interpretation unserer Methode zu ermöglichen, integrieren wir ein Markov-Modell in den Repräsentationsraum.dieses Modell deckt die zeitliche Übergangsstruktur auf, verbessert die Clustering-Leistung noch weiter und liefert zusätzliche erklärende Erkenntnisse sowie eine natürliche Darstellung der Unsicherheit. Wir evaluieren unser Modell in Bezug auf Clustering-Leistung und Interpretierbarkeit auf statischen (Fashion-)MNIST-Daten, einer Zeitreihe von linear interpolierten (Fashion-)MNIST-Bildern, einem chaotischen Lorenz-Attraktor-System mit zwei Makrozuständen sowie auf einer anspruchsvollen medizinischen Zeitreihenanwendung auf dem eICU-Datensatz.Unsere gelernten Repräsentationen schneiden im Vergleich zu Konkurrenzmethoden gut ab und erleichtern nachgelagerte Aufgaben auf den Realweltdaten.
Wir schlagen Significance-Offset Convolutional Neural Network vor, eine tiefe Faltungsnetzwerkarchitektur für die Regression von multivariaten asynchronen Zeitreihen.  Das Modell ist inspiriert von standardmäßigen autoregressiven (AR) Modellen und Gating-Mechanismen, die in rekurrenten neuronalen Netzen verwendet werden.  Es beinhaltet ein AR-ähnliches Gewichtungssystem, bei dem der endgültige Prädiktor als gewichtete Summe angepasster Regressoren erhalten wird, während die Gewichte datenabhängige Funktionen sind, die durch ein Faltungsnetzwerk erlernt werden.Die Architektur wurde für Anwendungen auf asynchrone Zeitreihen entwickelt und wird an solchen Datensätzen bewertet: einem Hedgefonds-eigenen Datensatz von über 2 Millionen Notierungen für einen Kreditderivat-Index, einer künstlich erzeugten verrauschten autoregressiven Reihe und einem Stromverbrauchsdatensatz.   Der Code für die numerischen Experimente und die Implementierung der Architektur wird online zur Verfügung gestellt, um die Forschung reproduzierbar zu machen.
MixUp ist ein Datenerweiterungsschema, bei dem Paare von Trainingsproben und ihre entsprechenden Etiketten mit linearen Koeffizienten gemischt werden. Ohne Etikettenmischung wird MixUp zu einem konventionellen Schema: Eingabeproben werden verschoben, aber ihre ursprünglichen Etiketten werden beibehalten. Da Proben bevorzugt in Richtung anderer Klassen \iffalse - die typischerweise im Eingaberaum geclustert sind - verschoben werden, bezeichnen wir diese Methode als direktionales adversariales Training oder DAT. Wir definieren untied MixUp (UMixUp), eine Obermenge von MixUp, bei der die Trainingsetiketten mit anderen linearen Koeffizienten gemischt werden als die entsprechenden Stichproben, und zeigen, dass untied MixUp unter denselben milden Bedingungen gegen die gesamte Klasse der DAT-Verfahren konvergiert. Motiviert durch das Verständnis, dass UMixUp sowohl eine Verallgemeinerung von MixUp als auch eine Form von adversarialem Training ist, experimentieren wir mit verschiedenen Datensätzen und Verlustfunktionen, um zu zeigen, dass UMixUp eine bessere Leistung als MixUp bietet.
Planerkennung zielt darauf ab, nach Zielplänen zu suchen, die die beobachteten Handlungen auf der Grundlage von Planbibliotheken und/oder Domänenmodellen am besten erklären.Trotz des Erfolgs früherer Ansätze zur Planerkennung beruhen diese meist auf korrekten Handlungsbeobachtungen. Die jüngsten Fortschritte auf dem Gebiet der visuellen Aktivitätserkennung haben das Potenzial, Anwendungen wie die automatisierte Videoüberwachung zu ermöglichen.Effektive Ansätze für solche Probleme würden die Fähigkeit erfordern, die Pläne von Agenten aus Videoinformationen zu erkennen.Traditionelle Planerkennungsalgorithmen sind auf den Zugang zu detaillierten Planungsdomänenmodellen angewiesen.Eine neue vielversprechende Richtung beinhaltet das Lernen von approximativen (oder flachen) Domänenmodellen direkt aus den beobachteten Aktivitätssequenzen. Die Ergebnisse der visuellen Inferenz sind jedoch oft verrauscht und unsicher und werden typischerweise als Verteilung über mögliche Aktionen dargestellt.In dieser Arbeit entwickeln wir ein visuelles Planerkennungssystem, das Pläne mit einem approximativen Domänenmodell erkennt, das aus unsicheren visuellen Daten gelernt wurde.
Wir betrachten die Aufgabe, komplexe Multi-Hop-Fragen zu beantworten, indem wir einen Korpus als virtuelle Wissensbasis (KB) verwenden. Insbesondere beschreiben wir ein neuronales Modul, DrKIT, das Textdaten wie eine virtuelle KB durchläuft, indem es sanft den Pfaden der Beziehungen zwischen den Erwähnungen von Entitäten im Korpus folgt. Bei jedem Schritt verwendet die Operation eine Kombination aus Sparse-Matrix-TFIDF-Indizes und maximaler innerer Produktsuche (MIPS) auf einem speziellen Index kontextueller Darstellungen. Dieses Modul ist differenzierbar, so dass das gesamte System vollständig durchgängig mit gradientenbasierten Methoden trainiert werden kann, ausgehend von natürlichsprachlichen Eingaben.Wir beschreiben auch ein Vortrainingsschema für den Indexerwähnungscodierer, indem wir harte Negativbeispiele unter Verwendung bestehender Wissensdatenbanken generieren.Wir zeigen, dass DrKIT die Genauigkeit bei 3-Hop-Fragen im MetaQA-Datensatz um 9 Punkte verbessert und damit die Lücke zwischen textbasierten und KB-basierten State-of-the-Art-Systemen um 70 % verringert.DrKIT ist auch sehr effizient und verarbeitet bis zu 10-mal mehr Anfragen pro Sekunde als bestehende State-of-the-Art-QA-Systeme.
Trotz ihres großen Erfolges unterstützen traditionelle Faktorisierungsalgorithmen typischerweise keine Merkmale (z.B., Trotz ihres Erfolgs unterstützen traditionelle Faktorisierungsalgorithmen typischerweise keine Merkmale (z.B. Matrix Factorization) oder ihre Komplexität skaliert quadratisch mit der Anzahl der Merkmale (z.B. Factorization Machine).Neuronale Methoden hingegen erlauben große Merkmalsmengen, sind aber oft für eine bestimmte Anwendung konzipiert.Wir schlagen neuartige Deep Factorization Methoden vor, die eine effiziente und flexible Merkmalsrepräsentation ermöglichen. Wir zeigen, dass unsere Architektur einige zuvor veröffentlichte neuronale Einzweck-Architekturen verallgemeinern kann. Unsere Experimente deuten auf verbesserte Trainingszeiten und Genauigkeit im Vergleich zu flachen Methoden hin.
Augmented Reality (AR) kann bei physischen Aufgaben wie dem Zusammenbau von Objekten durch die Verwendung von "situierten Anweisungen" helfen, die in Form von Videos, Bildern, Text oder Animationen vorliegen können, wobei die Auswahl der hilfreichsten Medien sowohl vom Benutzer als auch von der Art der Aufgabe abhängt.Unsere Arbeit unterstützt die Erstellung von AR-Tutorials für Montageaufgaben mit geringem Aufwand, der über die Ausführung der Aufgabe selbst hinausgeht. Das vorgestellte System AuthAR reduziert den Zeit- und Arbeitsaufwand für die Erstellung interaktiver AR-Tutorials, indem es automatisch die Schlüsselkomponenten des AR-Tutorials generiert, während der Autor die physischen Teile zusammensetzt, und die Autoren durch den Prozess des Hinzufügens von Videos, Bildern, Text und Animationen zum Tutorial führt.
Die Überwachung von Patienten auf der Intensivstation ist eine schwierige und kostspielige Aufgabe, so dass die Vorhersage des Zustands der Patienten während ihres Aufenthalts auf der Intensivstation zu einer besseren Akutversorgung und zur Planung der Krankenhausressourcen beitragen kann.Es gab kontinuierliche Fortschritte in der Forschung zum maschinellen Lernen für das Management auf der Intensivstation, und die meisten dieser Arbeiten konzentrierten sich auf die Verwendung von Zeitreihensignalen, die von Instrumenten auf der Intensivstation aufgezeichnet wurden. In unserer Arbeit zeigen wir, dass das Hinzufügen von klinischen Notizen als eine weitere Modalität die Leistung des Modells für drei Benchmark-Aufgaben verbessert: Vorhersage der Sterblichkeit im Krankenhaus, Modellierung der Dekompensation und Vorhersage der Verweildauer, die eine wichtige Rolle im Management der Intensivstation spielen.Während die Zeitreihendaten in regelmäßigen Abständen gemessen werden, werden die Arztnotizen zu unregelmäßigen Zeiten aufgezeichnet, was es schwierig macht, sie zusammen zu modellieren.Wir schlagen eine Methode vor, um sie gemeinsam zu modellieren, und erreichen damit eine beträchtliche Verbesserung bei den Benchmark-Aufgaben gegenüber dem Basis-Zeitreihenmodell.
Bestehende Arbeiten im Bereich des Multi-Agent Reinforcement Learning (MARL) konzentrieren sich hauptsächlich auf die Koordination von kooperativen Agenten, um bestimmte Aufgaben gemeinsam zu erledigen, aber in vielen Fällen der realen Welt sind Agenten eigennützig, wie z.B. Angestellte in einem Unternehmen oder Vereine in einer Liga, Die Hauptschwierigkeiten der teuren Koordination sind, dassi) der Anführer den Langzeiteffekt berücksichtigen und das Verhalten der Anhänger vorhersagen muss, wenn er die Boni zuteilt undii) die komplexen Interaktionen zwischen den Anhängern den Trainingsprozess schwer konvergieren lassen, besonders wenn sich die Politik des Anführers mit der Zeit ändert.In dieser Arbeit gehen wir dieses Problem durch einen ereignisbasierten tiefen RL-Ansatz an.Unsere Hauptbeiträge sind dreifach. (1) Wir modellieren den Entscheidungsprozess des Anführers als Semi-Markov-Entscheidungsprozess und schlagen einen neuartigen ereignisbasierten Policy-Gradienten vor, um die langfristige Politik des Anführers zu erlernen.(2) Wir nutzen das Konsistenzschema zwischen Anführer und Gefolgschaft, um ein Follower-bewusstes Modul und ein Follower-spezifisches Aufmerksamkeitsmodul zu entwerfen, um das Verhalten der Gefolgschaft vorherzusagen und eine genaue Reaktion auf ihr Verhalten zu geben. (3) Wir schlagen einen auf Handlungsabstraktion basierenden Policy-Gradienten-Algorithmus vor, um den Entscheidungsraum der Follower zu reduzieren und so den Trainingsprozess der Follower zu beschleunigen.Experimente in Ressourcensammlungen, Navigation und dem Räuber-Beute-Spiel zeigen, dass unser Ansatz die State-of-the-Art-Methoden dramatisch übertrifft.
Jüngste Arbeiten haben die Entstehung von Sprache unter Deep Reinforcement Learning Agenten untersucht, die zusammenarbeiten müssen, um eine Aufgabe zu lösen.Von besonderem Interesse sind die Faktoren, die dazu führen, dass Sprache kompositorisch ist - d.h., Evolutionäre Linguisten haben herausgefunden, dass zusätzlich zu strukturellen Prioritäten, wie sie bereits im Deep Learning untersucht wurden, die Dynamik der Übertragung von Sprache von Generation zu Generation wesentlich zur Entstehung von Kompositionalität beiträgt.In dieser Arbeit führen wir diese kulturelle evolutionäre Dynamik in die Sprachentstehung ein, indem wir periodisch Agenten in einer Population ersetzen, um eine Wissenslücke zu schaffen, die implizit eine kulturelle Übertragung von Sprache induziert.Wir zeigen, dass diese implizite kulturelle Übertragung die resultierenden Sprachen dazu ermutigt, eine bessere kompositionelle Generalisierung zu zeigen.
Basierend auf unserer Beobachtung, dass es einen dramatischen Abfall der Singulärwerte der vollständig verbundenen Schichten oder einer einzelnen Merkmalskarte der Faltungsschicht gibt und dass die Dimension des verketteten Merkmalsvektors fast der Summe der Dimensionen der einzelnen Merkmalskarten entspricht, schlagen wir einen auf der Singulärwertzerlegung (SVD) basierenden Ansatz vor, um die Dimension der tiefen Mannigfaltigkeiten für ein typisches neuronales Faltungsnetzwerk VGG19 zu schätzen. Wir wählen drei Kategorien aus dem ImageNet, nämlich Persische Katze, Containerschiff und Vulkan, und bestimmen die lokale Dimension der tiefen Mannigfaltigkeiten der tiefen Schichten durch den Tangentenraum eines Zielbildes. Durch verschiedene Augmentierungsmethoden haben wir herausgefunden, dass die Methode mit Gaußschem Rauschen näher an der intrinsischen Dimension liegt, da wir uns durch das Hinzufügen von zufälligem Rauschen zu einem Bild in einer beliebigen Dimension bewegen, und wenn sich der Rang der Merkmalsmatrix der augmentierten Bilder nicht erhöht, sind wir der lokalen Dimension der Mannigfaltigkeit sehr nahe. Unsere Ergebnisse zeigen, dass die Dimensionen der verschiedenen Kategorien nahe beieinander liegen und entlang der Faltungsschichten und der voll verbundenen Schichten schnell abnehmen. Darüber hinaus zeigen wir, dass die Dimensionen innerhalb der Conv5-Schicht schnell abnehmen.Unsere Arbeit bietet neue Einblicke in die intrinsische Struktur tiefer neuronaler Netze und hilft, die innere Organisation der Blackbox tiefer neuronaler Netze zu enthüllen.
Große vortrainierte Transformatoren wie BERT haben sich bei vielen NLP-Aufgaben als äußerst effektiv erwiesen. Allerdings ist die Inferenz in diesen Modellen mit großer Kapazität unerschwinglich langsam und teuer. Transformatoren sind im Wesentlichen ein Stapel von Selbstbeobachtungsschichten, die jede Eingabeposition unter Verwendung der gesamten Eingabesequenz als Kontext kodieren. Wir stellen jedoch fest, dass es nicht unbedingt notwendig ist, diese teure sequenzweite Selbstbeobachtung auf allen Ebenen anzuwenden. Auf der Grundlage dieser Beobachtung schlagen wir eine Zerlegung in einen vortrainierten Transformator vor, der es den unteren Schichten ermöglicht, Segmente der Eingabe unabhängig voneinander zu verarbeiten, was Parallelität und Zwischenspeicherung ermöglicht. Wir zeigen, dass der Informationsverlust, der durch diese Dekomposition entsteht, in den oberen Schichten durch zusätzliche Überwachung während der Feinabstimmung wieder ausgeglichen werden kann.  Wir evaluieren die Dekomposition mit vortrainierten BERT-Modellen an fünf verschiedenen Aufgaben mit gepaarter Eingabe in den Bereichen Fragebeantwortung, Satzähnlichkeit und Inferenz natürlicher Sprache.  Die Ergebnisse zeigen, dass die Dekomposition eine schnellere Inferenz (bis zu 4x) und eine signifikante Speicherreduktion (bis zu 70%) ermöglicht, während der Großteil (bis zu 99%) der ursprünglichen Leistung erhalten bleibt. Wir werden den Code unter<anonymized url> veröffentlichen.
Da die gelernte Repräsentation von den beobachteten Daten abhängt, kommt der Explorationsstrategie eine entscheidende Rolle zu.Der populäre DQN-Algorithmus hat die Fähigkeiten von ReinforcementLearning (RL)-Algorithmen zum Lernen von Zustandsrepräsentationen aus Rohdaten erheblich verbessert, verwendet jedoch eine naive Explorationsstrategie, die statistisch ineffizient ist.Der RandomizedLeast Squares Value Iteration (RLSVI)-Algorithmus (Osband et al., 2016) hingegen exploriert und verallgemeinert effizient über linear parametrisierte Wertfunktionen.Er basiert jedoch auf einer von Hand entworfenen Zustandsrepräsentation, die für jede Umgebung eine vorherige Entwicklungsarbeit erfordert.In diesem Papier schlagen wir eine DeepLearning-Anpassung für RLSVI vor.Anstatt eine von Hand entworfene Zustandsrepräsentation zu verwenden, verwenden wir eine Zustandsrepräsentation, die direkt aus den Daten durch einenDQN -Agenten gelernt wird. Da die Repräsentation während des Lernprozesses optimiert wird, ist eine Schlüsselkomponente für die vorgeschlagene Methode ein Likelihood-Matching-Mechanismus, der sich an die sich ändernden Repräsentationen anpasst.Wir demonstrieren die Bedeutung der verschiedenen Eigenschaften unseres Algorithmus an einem Spielzeugproblem und zeigen, dass unsere Methode DQN in fünf Atari-Benchmarks übertrifft und konkurrenzfähige Ergebnisse mit dem Rainbow-Algorithmus erreicht.
Wir zeigen, dass diese Undurchsichtigkeit Angreifern die Möglichkeit bietet, unbeabsichtigte Funktionalitäten in Form von Trojanischen Pferden in das Netzwerk einzubetten.Unser neuartiges Framework verbirgt die Existenz eines bösartigen Netzwerks innerhalb eines gutartigen Transportnetzwerks. Wir beweisen theoretisch, dass die Entdeckung des bösartigen Netzwerks rechnerisch nicht machbar ist und zeigen empirisch, dass das Transportnetzwerk seine Tarnung nicht kompromittiert.Unser Angriff deckt eine wichtige, bisher unbekannte Lücke auf, die eine neue Richtung in der Sicherheit des maschinellen Lernens aufzeigt.
In diesem Beitrag stellen wir Random Path Generative Adversarial Network (RPGAN) --- ein alternatives Schema von GANs vor, das als Werkzeug für die Analyse generativer Modelle dienen kann.Während der latente Raum eines typischen GANs aus Eingabevektoren besteht, die zufällig aus der Standard-Gauß-Verteilung gezogen werden, besteht der latente Raum von RPGAN aus zufälligen Pfaden in einem Generator-Netzwerk. Anhand von Experimenten mit Standard-Benchmarks zeigen wir, dass RPGAN eine Reihe interessanter Erkenntnisse über die Rolle der verschiedenen Schichten bei der Bilderzeugung liefert und neben der Interpretierbarkeit auch eine konkurrenzfähige Erzeugungsqualität und effizientes inkrementelles Lernen auf neuen Daten ermöglicht.
Tiefe künstliche neuronale Netze können bei identisch verteilten Trainings- und Testdatensätzen eine extrem geringe Differenz zwischen Trainings- und Testgenauigkeit erreichen, was ein Standardmaß für die Generalisierung ist. Um dieses Problem zu lösen, formulieren wir zunächst einen Klassifizierungsalgorithmus als ein Verfahren zur Suche nach einem Quellcode, der Eingabemerkmale auf Klassen abbildet, und leiten dann eine notwendige und hinreichende Bedingung für die Generalisierung ab, indem wir eine universelle kognitive Ähnlichkeitsmetrik, nämlich die Informationsdistanz, auf der Grundlage der Kolmogorov-Komplexität verwenden. Um dieses Ziel zu erreichen, erweitern wir die Eingabemerkmale durch Verkettung von Kodierungen und trainieren dann den Klassifikator auf den erweiterten Merkmalen. Zur Veranschaulichung dieser Idee konzentrieren wir uns auf die Bildklassifizierung, bei der wir Kanalcodes für die Eingabemerkmale als systematische Methode verwenden, um den Grad der Repräsentativität der Trainings- und Testmengen für die empirische Stichprobe zu verbessern. Um unsere theoretischen Erkenntnisse zu veranschaulichen, zeigen wir anhand umfangreicher systematischer Experimente, dass ein auf kodierten Eingangsmerkmalen trainiertes Modell als Ergebnis des Lernens einer allgemeineren Klassifizierungsfunktion wesentlich robuster gegenüber häufigen Verfälschungen ist, z. B, Gauß- und Schrotrauschen, sowie gegen negative Störungen, z. B. solche, die durch projizierten Gradientenabstieg gefunden werden, wesentlich robuster ist als ein Modell, das auf nicht kodierten Eingangsmerkmalen trainiert wurde.
Wir formulieren die kausale Entdeckung als ein auf der Grenzwahrscheinlichkeit basierendes Bayes'sches Modellauswahlproblem und verwenden eine Parametrisierung, die auf dem Begriff der Unabhängigkeit kausaler Mechanismen basiert, wodurch Markov-äquivalente Graphen unterschieden werden können, und ergänzen dies mit einem empirischen Bayes'schen Ansatz zur Festlegung von Prioren, so dass dem tatsächlichen zugrunde liegenden kausalen Graphen eine höhere Grenzwahrscheinlichkeit zugewiesen wird als seinen Alternativen. Die Annahme eines Bayes'schen Ansatzes ermöglicht auch eine unkomplizierte Modellierung unbeobachteter Störvariablen, für die wir einen Variationsalgorithmus zur Annäherung der Grenzwahrscheinlichkeit bereitstellen, da diese wünschenswerte Eigenschaft die Berechnung der Grenzwahrscheinlichkeit schwierig macht. Wir sind der Meinung, dass der Bayes'sche Ansatz zur Kausalerkennung sowohl die Anwendung der reichhaltigen Methodik der Bayes'schen Inferenz auf verschiedene schwierige Aspekte dieses Problems ermöglicht, als auch einen vereinheitlichenden Rahmen für die Kausalerkennungsforschung bietet.Wir zeigen vielversprechende Ergebnisse in Experimenten mit realen Daten, die unseren Modellierungsansatz und unsere Inferenzmethodik unterstützen.
  Das Ziel von Compressed Sensing ist es, ein strukturiertes Signal $x$ aus einer begrenzten Anzahl von verrauschten linearen Messungen $y\approx Ax$ zu lernen.  Bei der traditionellen komprimierten Abtastung wird die "Struktur" durch Sparsamkeit in einer bekannten Basis dargestellt.  Inspiriert durch den Erfolg von Deep Learning bei der Modellierung von Bildern, haben neuere Arbeiten, beginnend mit~\cite{BDJP17}, stattdessen die Struktur als Ergebnis eines generativen Modells $G: \R^k \bis \R^n$ betrachtet.  Wir präsentieren zwei Ergebnisse, die die Schwierigkeit der letztgenannten Aufgabe belegen und zeigen, dass die bestehenden Schranken eng sind.  Erstens liefern wir eine untere Schranke, die der oberen Schranke von~\cite{BDJP17} für Compressed Sensing aus $L$-Lipschitz generativen Modellen $G$ entspricht.  Insbesondere gibt es eine solche Funktion, die ungefähr $\Omega(k \log L)$ lineare Messungen erfordert, damit eine spärliche Wiederherstellung möglich ist.  Dies gilt sogar für das entspanntere Ziel der \emph{nonuniform} recovery.  Zweitens zeigen wir, dass generative Modelle Sparsamkeit als Repräsentation von Struktur verallgemeinern.  Insbesondere konstruieren wir ein ReLU-basiertes neuronales Netzwerk $G: \R^{2k} \to \R^n$ mit $O(1)$ Schichten und $O(kn)$ Aktivierungen pro Schicht, so dass der Bereich von $G$ alle $k$-sparse Vektoren enthält.
Wir stellen die Hypothese auf, dass neuronale End-to-End-Bildbeschriftungssysteme scheinbar gut funktionieren, weil sie die "Verteilungsähnlichkeit" in einem multimodalen Merkmalsraum ausnutzen und erlernen, indem sie ein Testbild auf ähnliche Trainingsbilder in diesem Raum abbilden und eine Beschriftung aus demselben Raum generieren. Um unsere Hypothese zu überprüfen, konzentrieren wir uns auf die "Bild"-Seite der Bildbeschriftung und variieren die Eingangsbildrepräsentation, halten aber das RNN-Textgenerierungsmodell eines CNN-RNN konstant. Wir fanden heraus, dass Bildbeschriftungsmodelle(i) in der Lage sind, Strukturen von verrauschten Eingabedarstellungen zu trennen;(ii) praktisch keine signifikanten Leistungseinbußen erfahren, wenn eine hochdimensionale Darstellung in einen niedrigeren dimensionalen Raum komprimiert wird;(iii) Bilder mit ähnlichen visuellen und sprachlichen Informationen zusammen gruppieren;(iv) stark auf Testmengen mit einer ähnlichen Verteilung wie die Trainingsmenge angewiesen sind;(v) wiederholt dieselben Beschriftungen erzeugen, indem sie Bilder abgleichen und eine Beschriftung im gemeinsamen visuell-textuellen Raum "abrufen". Unsere Experimente deuten alle auf eine Tatsache hin: dass unsere Hypothese der Verteilungsähnlichkeit zutrifft. Wir kommen zu dem Schluss, dass Bildbeschriftungssysteme, unabhängig von der Bildrepräsentation, Bilder abgleichen und Beschriftungen in einem gemeinsamen semantischen Bild-Text-Unterraum erzeugen.
Um der Tatsache Rechnung zu tragen, dass es typischerweise mehrere korrekte SQL-Abfragen mit der gleichen oder einer sehr ähnlichen Semantik gibt, lassen wir uns von syntaktischen Parsing-Techniken inspirieren und schlagen vor, unsere Sequence-to-Action-Modelle mit nicht-deterministischen Orakeln zu trainieren. Wir evaluieren unsere Modelle auf dem WikiSQL-Datensatz und erreichen eine Ausführungsgenauigkeit von 83,7 % auf dem Testsatz, eine absolute Verbesserung von 2,1 % gegenüber den Modellen, die mit traditionellen statischen Orakeln trainiert wurden, die eine einzige korrekte Ziel-SQL-Abfrage voraussetzen.2 In Kombination mit der ausführungsgesteuerten Dekodierungsstrategie erreicht unser Modell eine neue Spitzenleistung mit einer Ausführungsgenauigkeit von 87,1 %.
In ENAS lernt ein Controller, neuronale Netzwerkarchitekturen zu entdecken, indem er nach einem optimalen Pfad innerhalb eines größeren Modells sucht. Der Controller wird mit einem Policy-Gradienten trainiert, um einen Pfad auszuwählen, der die erwartete Belohnung in der Validierungsmenge maximiert. Auf dem Penn Treebank-Datensatz kann ENAS eine neuartige Architektur entdecken, die eine Test-Perplexität von 57,8 erreicht, was unter den automatischen Modell-Design-Methoden auf dem Penn Treebank-Datensatz der Stand der Technik ist. Auf dem CIFAR-10-Datensatz kann ENAS neuartige Architekturen entwerfen, die einen Testfehler von 2,89 % erreichen, was nahe an den 2,65 % liegt, die von Standard-NAS erreicht werden (Zoph et al, Vor allem aber zeigen unsere Experimente, dass ENAS mehr als 10x schneller und 100x weniger ressourcenintensiv als NAS ist.
Heutzutage sind tiefe neuronale Netze (DNNs) das Hauptinstrument für maschinelle Lernaufgaben in einer Vielzahl von Bereichen, einschließlich Vision, NLP und Sprache, geworden. Insbesondere gibt es keine ausreichenden Beweise dafür, dass Deep-Learning-Maschinen es ermöglichen, Methoden zu konstruieren, die Gradient-Boosting-Entscheidungsbäume (GBDT) übertreffen, die oft die erste Wahl für tabellarische Probleme sind.In diesem Papier stellen wir Neural Oblivious Decision Ensembles (NODE) vor, eine neue Deep-Learning-Architektur, die für die Arbeit mit beliebigen tabellarischen Daten entwickelt wurde. Kurz gesagt, die vorgeschlagene NODE-Architektur verallgemeinert Ensembles von oblivious Entscheidungsbäume, sondern profitiert sowohl von Ende-zu-Ende-Gradient-basierte Optimierung und die Macht der mehrschichtigen hierarchischen Repräsentation learning.With einen umfangreichen experimentellen Vergleich zu den führenden GBDT Pakete auf eine große Anzahl von tabellarischen Datensätzen, zeigen wir den Vorteil der vorgeschlagenen NODE-Architektur, die die Konkurrenten auf die meisten Aufgaben übertrifft.We Open-Source die PyTorch-Implementierung von NODE und glauben, dass es ein universelles Rahmenwerk für maschinelles Lernen auf tabellarischen Daten werden.
Das Ziel der Re-Identifikation von Personen (Re-Id) ist es, die Bilder derselben Personen über verschiedene Kameras hinweg zu identifizieren. Die Unterschiede zwischen den verschiedenen Datensätzen stellen jedoch eine offensichtliche Herausforderung für die Anpassung des Re-Id-Modells, das auf einem Datensatz trainiert wurde, an einen anderen Datensatz dar.Modernste unüberwachte Domänenanpassungsmethoden für die Re-Identifikation von Personen übertragen das gelernte Wissen aus der Quelldomäne, indem sie mit Pseudo-Etiketten optimiert werden, die durch Clustering-Algorithmen auf der Zieldomäne erstellt wurden.Obwohl sie Spitzenleistungen erzielten, wurde das unvermeidliche Etikettenrauschen, das durch das Clustering-Verfahren verursacht wird, ignoriert. Um die Auswirkungen der verrauschten Pseudo-Etiketten abzuschwächen, schlagen wir vor, die Pseudo-Etiketten in der Zieldomäne sanft zu verfeinern, indem wir einen unbeaufsichtigten Rahmen, Mutual Mean-Teaching (MMT), vorschlagen, um bessere Merkmale aus der Zieldomäne über offline verfeinerte harte Pseudo-Etiketten und online verfeinerte weiche Pseudo-Etiketten in einer alternativen Trainingsart zu lernen.  Darüber hinaus ist es gängige Praxis, sowohl den Klassifizierungsverlust als auch den Triplett-Verlust gemeinsam zu verwenden, um eine optimale Leistung bei Personenidentifizierungsmodellen zu erzielen, wobei der herkömmliche Triplett-Verlust jedoch nicht mit weich verfeinerten Etiketten arbeiten kann. Um dieses Problem zu lösen, wird ein neuartiger Softmax-Triplet-Verlust vorgeschlagen, der das Lernen mit weichen Pseudo-Triplet-Etiketten unterstützt, um eine optimale Leistung bei der Domänenanpassung zu erreichen. Der vorgeschlagene MMT-Rahmen erzielt beträchtliche Verbesserungen von 14,4 %, 18,2 %, 13,1 % und 16,4 % mAP bei Market-to-Duke, Duke-to-Market, Market-to-MSMT und Duke-to-MSMT unüberwachten Domänenanpassungsaufgaben.
Im Vergleich zu bestehenden Methoden ermöglicht unser Ansatz die Analyse sowohl der gesamten Audioverarbeitungsstufe als auch rekurrenter neuronaler Netzwerkarchitekturen (z.B. LSTM).Die Audioverarbeitung wird mit Hilfe neuartiger konvexer Relaxationen verifiziert, die auf die in der Audiotechnik verwendeten Merkmalsextraktionsoperationen (z.B. Fast Fourier Transformation) zugeschnitten sind, Wir zeigen, dass der Verifizierer auf große Netzwerke skaliert und dabei deutlich engere Grenzen als bestehende Methoden für gängige Audioklassifizierungsbenchmarks berechnet: Auf dem anspruchsvollen Google Speech Commands-Datensatz zertifizieren wir 95 % mehr Eingaben als die Intervallapproximation (die einzige vorherige skalierbare Methode) für eine Störung von -90 dB.
Da tiefe neuronale Netze überparametrisiert sind, können sie verrauschte Beispiele auswendig lernen. Wir befassen uns mit dem Problem des Auswendiglernens in Anwesenheit von Annotationsrauschen. Aus der Tatsache, dass tiefe neuronale Netze Nachbarschaften der durch Auswendiglernen erworbenen Merkmale nicht verallgemeinern können, stellen wir die Hypothese auf, dass verrauschte Beispiele unter einer bestimmten Störung nicht durchgängig zu kleinen Verlusten für das Netzwerk führen. Auf dieser Grundlage schlagen wir eine neuartige Trainingsmethode namens Learning with Ensemble Consensus (LEC) vor, die eine Überanpassung von verrauschten Beispielen verhindert, indem sie diese durch den Konsens eines Ensembles von gestörten Netzwerken eliminiert. LTEC, eine der vorgeschlagenen LECs, übertrifft die aktuellen State-of-the-Art-Methoden bei verrauschten MNIST, CIFAR-10 und CIFAR-100 auf effiziente Weise.
Eine beliebte Formulierung des Problems ist eine $\ell_1$ regularisierte Maximum-Likelihood-Schätzung.Viele konvexe Optimierungsalgorithmen wurden entwickelt, um diese Formulierung zu lösen, um die Graphenstruktur wiederherzustellen.In letzter Zeit gibt es einen Anstieg des Interesses, Algorithmen direkt auf der Grundlage von Daten zu lernen, und in diesem Fall zu lernen, empirische Kovarianz auf die spärliche Präzisionsmatrix abzubilden. Wir schlagen eine Deep-Learning-Architektur, GLAD, vor, die einen Alternating Minimization (AM)-Algorithmus als induktiven Modell-Bias verwendet und die Modellparameter über überwachtes Lernen lernt. Wir zeigen, dass GLAD ein sehr kompaktes und effektives Modell für die Wiederherstellung von spärlichen Graphen aus Daten lernt.
Der ursprüngliche CFR-Algorithmus funktioniert jedoch nur für diskrete Zustände und Aktionsräume, und die resultierende Strategie wird als tabellarische Darstellung beibehalten. Diese tabellarische Darstellung schränkt die direkte Anwendung der Methode auf große Spiele ein.In diesem Papier schlagen wir eine doppelte neuronale Darstellung für die IIGs vor, bei der ein neuronales Netzwerk das kumulative Bedauern und das andere die durchschnittliche Strategie darstellt.  Um das Lernen effizient zu gestalten, haben wir außerdem mehrere neuartige Techniken entwickelt, darunter eine robuste Sampling-Methode und eine Mini-Batch-Monte-Carlo-Methode zur Minimierung des kontrafaktischen Bedauerns (MCCFR), die unabhängig voneinander von Interesse sein können.  Empirisch gesehen konvergieren neuronale Strategien, die mit unserem Algorithmus trainiert wurden, bei Spielen, die für tabellarische Ansätze nachvollziehbar sind, vergleichbar mit ihren tabellarischen Gegenstücken und übertreffen diejenigen, die auf Deep Reinforcement Learning basieren, deutlich.  Bei extrem großen Spielen mit Milliarden von Entscheidungsknoten erreichte unser Ansatz eine starke Leistung und benötigte dabei hundertmal weniger Speicher als der tabellarische CFR. Bei Kopf-an-Kopf-Spielen beim No-Limit-Texas Hold'em schlug unser neuronaler Agent den starken Agenten ABS-CFR um 9,8$pm4,1$ Chips pro Spiel.
Wir präsentieren die erste Verifikation, dass ein neuronales Netzwerk für Wahrnehmungsaufgaben eine korrekte Ausgabe innerhalb einer spezifizierten Toleranz für jede Eingabe von Interesse produziert.Wir definieren Korrektheit relativ zu einer Spezifikation, die 1) einen Zustandsraum, der aus allen relevanten Zuständen der Welt besteht, und 2) einen Beobachtungsprozess identifiziert, der Eingaben des neuronalen Netzwerks aus den Zuständen der Welt erzeugt. Das Kacheln der Zustands- und Eingaberäume mit einer endlichen Anzahl von Kacheln, das Erhalten von Grundwahrheitsgrenzen aus den Zustandskacheln und von Netzwerkausgangsgrenzen aus den Eingabekacheln und das anschließende Vergleichen der Grundwahrheits- und Netzwerkausgangsgrenzen liefert eine obere Schranke des Netzwerkausgangsfehlers für eine beliebige Eingabe von Interesse.Ergebnisse von zwei Fallstudien unterstreichen die Fähigkeit unserer Technik, enge Fehlergrenzen für alle Eingaben von Interesse zu liefern und zeigen, wie die Fehlergrenzen über die Zustands- und Eingaberäume variieren.
Tiefe generative Modelle haben in den letzten Jahren bemerkenswerte Fortschritte gemacht.Trotz dieser Fortschritte bleibt die quantitative Bewertung und der Vergleich generativer Modelle eine der wichtigsten Herausforderungen.Eine der beliebtesten Metriken zur Bewertung generativer Modelle ist die Log-Likelihood. Während die direkte Berechnung der Log-Likelihood schwierig sein kann, wurde kürzlich gezeigt, dass die Log-Likelihood einiger der interessantesten generativen Modelle wie Variational Autoencoders (VAE) oder generative adversarial networks (GAN) mit Hilfe von annealed importance sampling (AIS) effizient geschätzt werden kann. In dieser Arbeit argumentieren wir, dass die Log-Likelihood-Metrik allein nicht alle unterschiedlichen Leistungsmerkmale generativer Modelle darstellen kann, und schlagen vor, Ratenverzerrungskurven zu verwenden, um tiefe generative Modelle zu bewerten und zu vergleichen. Wir evaluieren die verlustbehafteten Kompressionsraten verschiedener tiefer generativer Modelle wie VAEs, GANs (und deren Varianten) und adversarialer Autocodierer (AAE) auf MNIST und CIFAR10 und kommen zu einer Reihe von Erkenntnissen, die mit Log-Likelihoods allein nicht zu erreichen sind.
Obwohl Reinforcement-Learning-Methoden in der Simulation beeindruckende Ergebnisse erzielen können, stellt die reale Welt zwei große Herausforderungen dar: Die Generierung von Stichproben ist äußerst kostspielig, und unerwartete Störungen oder ungesehene Situationen führen dazu, dass tüchtige, aber spezialisierte Strategien zum Testzeitpunkt versagen.Angesichts der Tatsache, dass es unpraktisch ist, separate Strategien zu trainieren, um alle Situationen zu berücksichtigen, die der Agent in der realen Welt sehen kann, schlägt diese Arbeit vor, zu lernen, wie man sich schnell und effektiv online an neue Aufgaben anpasst.Um stichprobeneffizientes Lernen zu ermöglichen, betrachten wir das Erlernen von Online-Anpassung im Kontext von modellbasiertem Reinforcement-Learning. Unsere Experimente demonstrieren die Online-Anpassung für kontinuierliche Steuerungsaufgaben sowohl bei simulierten als auch bei realen Agenten.Wir zeigen zunächst, wie simulierte Agenten ihr Verhalten online an neuartige Terrains, verkrüppelte Körperteile und hochdynamische Umgebungen anpassen.Wir veranschaulichen auch die Bedeutung der Online-Anpassung für autonome Agenten, die in der realen Welt agieren, indem wir unsere Methode auf einen realen dynamischen Milliroboter mit Beinen anwenden: Wir demonstrieren die erlernte Fähigkeit des Agenten, sich schnell online an ein fehlendes Bein anzupassen, sich auf neuartiges Terrain und Neigungen einzustellen, Fehlkalibrierungen oder Fehler bei der Posenschätzung zu berücksichtigen und das Ziehen von Nutzlasten zu kompensieren.
Modellfreie Deep Reinforcement Learning-Ansätze haben in simulierten Umgebungen übermenschliche Leistungen gezeigt (z. B., Während des Trainings konstruieren diese Ansätze oft implizit einen latenten Raum, der Schlüsselinformationen für die Entscheidungsfindung enthält. In diesem Papier lernen wir ein Vorwärtsmodell auf diesem latenten Raum und wenden es auf die modellbasierte Planung in einem Miniatur-Echtzeit-Strategiespiel mit unvollständigen Informationen (MiniRTS) an.) Wir zeigen zunächst, dass der latente Raum, der aus bestehenden Akteur-Kritik-Modellen konstruiert wurde, relevante Informationen des Spiels enthält, und entwerfen ein Trainingsverfahren, um Vorwärtsmodelle zu erlernen. Wir zeigen auch, dass unser erlerntes Vorwärtsmodell sinnvolle zukünftige Zustände vorhersagen kann und für die Monte-Carlo-Baumsuche (MCTS) im latenten Raum in Bezug auf die Gewinnraten gegen regelbasierte Agenten verwendbar ist.
Diese Techniken haben es den Praktikern ermöglicht, erfolgreich tiefe Faltungsnetzwerke mit Hunderten von Schichten zu trainieren. Insbesondere wurde eine neuartige Art der Verbindung von Schichten als das Dense Convolutional Network (DenseNet) eingeführt und hat bei relevanten Bilderkennungsaufgaben Spitzenleistungen erzielt. In dieser Arbeit gehen wir dieses Problem an, indem wir die Auswirkung der Schichtverknüpfung auf die Gesamtausdrucksstärke eines Faltungsnetzes analysieren, wobei wir insbesondere die in DenseNet verwendeten Verbindungen mit anderen Arten der Schichtverknüpfung vergleichen. Wir führen eine Tensor-Analyse der Ausdrucksstärke von Verbindungen auf arithmetischen Faltungsschaltungen (ConvACs) durch und beziehen unsere Ergebnisse auf Standard-Faltungsnetzwerke. Die Analyse führt zu Leistungsgrenzen und praktischen Richtlinien für den Entwurf von ConvACs.Die Verallgemeinerung dieser Ergebnisse wird für andere Arten von Faltungsnetzwerken über verallgemeinerte Tensor-Zerlegungen diskutiert.
Wir betrachten die folgende zentrale Frage im Bereich des Deep Reinforcement Learning (DRL):Wie können wir implizites menschliches Feedback nutzen, um das Training eines DRL-Algorithmus zu beschleunigen und zu optimieren?State-of-the-Art-Methoden verlassen sich darauf, dass jegliches menschliches Feedback explizit bereitgestellt wird, was die aktive Teilnahme von Menschen erfordert (z.B., In dieser Arbeit untersuchen wir ein alternatives Paradigma, bei dem Menschen, die keine Experten sind, den Agenten, der mit der Umgebung interagiert, still beobachten (und bewerten). Die intrinsischen Reaktionen des Menschen auf das Verhalten des Agenten werden als implizites Feedback erfasst, indem Elektroden auf der menschlichen Kopfhaut platziert werden und die sogenannten ereigniskorrelierten elektrischen Potenziale überwacht werden. Wir entwickeln ein System, um das implizite menschliche Feedback (insbesondere fehlerbezogene elektrische Potentiale) für Zustands-Aktions-Paare in einer Atari-artigen Umgebung zu erhalten und genau zu dekodieren. Als Basisbeitrag demonstrieren wir die Machbarkeit der Erfassung von Fehlerpotentialen eines menschlichen Beobachters, der einen Agenten beim Erlernen verschiedener Atari-Spiele mit Hilfe einer Elektroenzephalogramm (EEG)-Kappe beobachtet, und dekodieren dann die Signale entsprechend und verwenden sie als zusätzliche Belohnungsfunktion für einen DRL-Algorithmus mit der Absicht, dessen Lernen des Spiels zu beschleunigen. Aufbauend auf dieser Grundlage leisten wir in unserer Arbeit die folgenden neuen Beiträge:(i) Wir argumentieren, dass die Definition von Fehlerpotenzialen über verschiedene Umgebungen hinweg verallgemeinerbar ist; insbesondere zeigen wir, dass Fehlerpotenziale eines Beobachters für ein bestimmtes Spiel erlernt werden können und die Definition unverändert für ein anderes Spiel verwendet werden kann, ohne dass die Fehlerpotenziale neu erlernt werden müssen.  (ii) Wir schlagen zwei verschiedene Frameworks vor, um die jüngsten Fortschritte in DRL in das auf Fehlerpotenzialen basierende Feedbacksystem auf eine stichprobeneffiziente Weise zu kombinieren, so dass Menschen implizites Feedback während des Trainings in der Schleife oder vor dem Training des RL-Agenten geben können.(iii) Schließlich skalieren wir das implizite menschliche Feedback (über ErrP) basierende RL auf einigermaßen komplexe Umgebungen (Spiele) und zeigen die Bedeutung unseres Ansatzes durch synthetische und reale Benutzerexperimente.
Kürzlich wurden Konsensnetzwerke (Consensus Networks, CN) vorgeschlagen, um den Mangel an Daten durch die Verwendung von Merkmalen aus mehreren Modalitäten zu lindern, aber auch sie wurden durch die Größe der beschrifteten Daten begrenzt. In diesem Papier erweitern wir CN zu transduktiven Konsensnetzwerken (TCNs), die sich für halbüberwachtes Lernen eignen. In TCNs werden verschiedene Modalitäten des Inputs zu latenten Repräsentationen komprimiert, die wir dazu ermutigen, während des iterativen adversen Trainings ununterscheidbar zu werden. Um TCNs zwei Mechanismen, Konsens und Klassifizierung, zu verstehen, stellen wir seine drei Varianten in Ablation Studien über diese Mechanismen.Um weiter zu untersuchen TCN Modelle, behandeln wir die latenten Darstellungen als Wahrscheinlichkeitsverteilungen und messen ihre Ähnlichkeiten als die negativen relativen Jensen-Shannon Divergenzen. Wir zeigen, dass ein Konsenszustand, der für die Klassifizierung von Vorteil ist, eine stabile, aber unvollkommene Ähnlichkeit zwischen den Repräsentationen erfordert. Insgesamt übertreffen TCNs die besten Benchmark-Algorithmen bei 20 bis 200 gelabelten Proben in den Datensätzen Bank Marketing und DementiaBank oder stimmen mit ihnen überein.
Mehrere stochastische Optimierungsmethoden erster Ordnung, die üblicherweise im euklidischen Bereich verwendet werden, wie z.B. stochastischer Gradientenabstieg (SGD), beschleunigter Gradientenabstieg oder varianzreduzierte Methoden, wurden bereits an bestimmte riemannische Umgebungen angepasst, aber einige der populärsten dieser Optimierungswerkzeuge - nämlich Adam, Adagrad und das neuere Amsgrad - müssen noch auf riemannische Mannigfaltigkeiten verallgemeinert werden. Wir erörtern die Schwierigkeiten bei der Verallgemeinerung solcher adaptiven Schemata auf die agnostische Riemannsche Umgebung und liefern dann Algorithmen und Konvergenzbeweise für geodätisch konvexe Ziele im besonderen Fall eines Produkts von Riemannschen Mannigfaltigkeiten, in dem Adaptivität über Mannigfaltigkeiten im kartesischen Produkt implementiert ist. Unsere Verallgemeinerung ist eng in dem Sinne, dass die Wahl des euklidischen Raums als Riemannsche Mannigfaltigkeit dieselben Algorithmen und Regret-Grenzen ergibt, die bereits für die Standardalgorithmen bekannt waren. Experimentell zeigen wir eine schnellere Konvergenz und einen niedrigeren Zugverlustwert für Riemannsche adaptive Methoden im Vergleich zu ihren entsprechenden Basislinien bei der realistischen Aufgabe, die WordNet-Taxonomie in die Poincare-Kugel einzubetten.
Zunächst zeigen wir, dass die beiden skalierbarsten und effektivsten Methoden zum Erlernen robuster Modelle, das gegnerische Training mit PGD-Angriffen und die zufällige Glättung, nur eine sehr begrenzte Wirksamkeit gegen drei der bekanntesten physischen Angriffe aufweisen. Als Nächstes schlagen wir ein neues abstraktes gegnerisches Modell vor, rechteckige Okklusionsangriffe, bei denen ein Gegner ein kleines, gegnerisch gestaltetes Rechteck in einem Bild platziert, und entwickeln zwei Ansätze zur effizienten Berechnung der resultierenden gegnerischen Beispiele.Schließlich zeigen wir, dass gegnerisches Training mit unserem neuen Angriff Bildklassifizierungsmodelle hervorbringt, die eine hohe Robustheit gegen die physisch realisierbaren Angriffe aufweisen, die wir untersuchen, und damit die erste wirksame generische Verteidigung gegen solche Angriffe bieten.
Kontinuierliches Lernen ist das Problem des sequentiellen Erlernens neuer Aufgaben oder neuen Wissens bei gleichzeitigem Schutz des zuvor erworbenen Wissens, wobei das katastrophale Vergessen eine große Herausforderung für neuronale Netze darstellt, die einen solchen Lernprozess durchführen, so dass neuronale Netze, die in der realen Welt eingesetzt werden, oft mit Szenarien zu kämpfen haben, in denen die Datenverteilung nicht stationär (Konzeptdrift), unausgewogen oder nicht immer vollständig verfügbar ist, d.h. seltene Randfälle, Wir schlagen ein differenzierbares Hebbian-Konsolidierungsmodell vor, das aus einer Softmax-Schicht mit differenzierbarer Hebbian-Plastizität (DHP) besteht, die eine schnell lernende plastische Komponente (komprimiertes episodisches Gedächtnis) zu den festen (sich langsam ändernden) Parametern der Softmax-Ausgangsschicht hinzufügt, wodurch gelernte Repräsentationen über einen längeren Zeitraum beibehalten werden können. Wir demonstrieren die Flexibilität unserer Methode, indem wir bekannte aufgabenspezifische synaptische Konsolidierungsmethoden integrieren, um Änderungen in den langsamen Gewichten zu bestrafen, die für jede Zielaufgabe wichtig sind. wir evaluieren unseren Ansatz auf den Benchmarks Permuted MNIST, Split MNIST und Vision Datasets Mixture und führen eine unausgewogene Variante von Permuted MNIST ein - ein Datensatz, der die Herausforderungen von Klassenungleichgewicht und Konzeptdrift kombiniert. unser vorgeschlagenes Modell erfordert keine zusätzlichen Hyperparameter und übertrifft vergleichbare Baselines, indem es das Vergessen reduziert.
Um eine neuronale Netzwerkarchitektur zu wählen, die für ein bestimmtes Modellierungsproblem effektiv ist, muss man die Einschränkungen verstehen, die jede der möglichen Optionen mit sich bringt. In dieser Arbeit werden die topologischen Einschränkungen untersucht, die die Architektur eines neuronalen Netzes den Levelsets aller Funktionen, die es approximieren kann, auferlegt - ein neuer Ansatz, der sowohl die Art der Einschränkungen als auch die Tatsache berücksichtigt, dass sie unabhängig von der Netztiefe für eine breite Familie von Aktivierungsfunktionen sind.
Die Ableitung von Schleifeninvarianten ist eine der größten Herausforderungen bei der automatisierten Verifikation von realen Programmen, die oft viele Schleifen enthalten.In diesem Beitrag stellen wir ein Continuous Logic Network (CLN) vor, eine neuartige neuronale Architektur zum automatischen Lernen von Schleifeninvarianten direkt aus Programmausführungsspuren.Im Gegensatz zu bestehenden neuronalen Netzwerken können CLNs präzise und explizite Darstellungen von Formeln in Satisfiability Modulo Theories (SMT) für Schleifeninvarianten aus Programmausführungsspuren lernen. Wir entwickeln ein neues, solides und vollständiges semantisches Mapping für die Zuordnung von SMT-Formeln zu kontinuierlichen Wahrheitswerten, mit dem CLNs effizient trainiert werden können. Wir verwenden CLNs, um ein neues Inferenzsystem für Schleifeninvarianten, CLN2INV, zu implementieren, das bestehende Ansätze auf dem beliebten Code2Inv-Datensatz deutlich übertrifft. CLN2INV ist das erste Tool, das alle 124 theoretisch lösbaren Probleme des Code2Inv-Datensatzes lösen kann und dabei im Durchschnitt nur 1,1 Sekunden für jedes Problem benötigt, was 40 Mal schneller ist als bestehende Ansätze.
Einzelzell-RNA-Sequenzierung (scRNAseq) Technologie ermöglicht die Quantifizierung der Genexpression Profile von einzelnen Zellen innerhalb Krebs.Dimension Reduktion Methoden wurden häufig für die Zelle Clustering-Analyse und Visualisierung der Daten.Aktuelle Dimension Reduktion Methoden neigen dazu, übermäßig zu beseitigen die Expression Variationen entsprechen weniger dominierenden Eigenschaften, so dass wir nicht finden, die homogenen Eigenschaften der Krebsentwicklung. In diesem Papier, schlugen wir eine neue und Clustering-Analyse-Methode für scRNAseq-Daten, nämlich BBSC, über die Umsetzung einer Binarisierung der Gen-Expressionsprofil in on / off Frequenzänderungen mit einem Booleschen Matrix factorization.The niedrigen Rang Darstellung der Expression Matrix durch BBSC erholt erhöhen die Auflösung bei der Identifizierung von verschiedenen Zelltypen oder functions.Application von BBSC auf zwei Krebs scRNAseq Daten erfolgreich entdeckt sowohl homogene und heterogene Krebszelle clusters.Further Feststellung zeigte Potenzial bei der Verhinderung von Krebs Progression.
Während normalisierende Flüsse zu bedeutenden Fortschritten bei der Modellierung hochdimensionaler kontinuierlicher Verteilungen geführt haben, bleibt ihre Anwendbarkeit auf diskrete Verteilungen unbekannt.in diesem Papier zeigen wir, dass Flüsse in der Tat auf diskrete Ereignisse ausgedehnt werden können - und unter einer einfachen Variablenwechselformel, die keine log-determinant-Jacobschen Berechnungen erfordert. Diskrete Flüsse haben zahlreiche Anwendungen; wir zeigen Proofs of Concept für 2 Flussarchitekturen: diskrete autoregressive Flüsse ermöglichen Bidirektionalität, so dass z.B. Token in Texten sowohl von links-nach-rechts als auch von rechts-nach-links Kontexten in einem exakten Sprachmodell abhängen; und diskrete bipartite Flüsse (d.h., mit der Schichtstruktur von RealNVP) ermöglichen eine parallele Generierung wie z.B. eine exakte nicht-autoregressive Textmodellierung.
Wir stellen ein Deep Neural Network mit Spike Assisted Feature Extraction (SAFE-DNN) vor, um die Robustheit der Klassifikation bei stochastischen Störungen der Eingaben zu verbessern. Das vorgeschlagene Netzwerk ergänzt ein DNN mit unbeaufsichtigtem Lernen von Low-Level-Features unter Verwendung eines Spiking Neuron Network (SNN) mit Spike-Time-Dependent-Plasticity (STDP). Das komplette Netzwerk lernt, lokale Störungen zu ignorieren, während es eine globale Merkmalserkennung und -klassifizierung durchführt. Die experimentellen Ergebnisse auf CIFAR-10 und ImageNet zeigen eine verbesserte Robustheit gegen Rauschen für mehrere DNN-Architekturen, ohne dass die Genauigkeit bei sauberen Bildern beeinträchtigt wird.
Neuronale Einbettungen wurden mit großem Erfolg in der natürlichen Sprachverarbeitung (NLP) eingesetzt, wo sie kompakte Darstellungen liefern, die Wortähnlichkeit kapseln und Spitzenleistungen in einer Reihe von linguistischen Aufgaben erreichen.Der Erfolg der neuronalen Einbettungen hat erhebliche Mengen an Forschung in Anwendungen in anderen Bereichen als Sprache veranlasst.Eine solche Domäne sind graphenstrukturierte Daten, wo Einbettungen von Scheitelpunkten gelernt werden können, die Scheitelpunktähnlichkeit kapseln und die Leistung bei Aufgaben wie Kantenvorhersage und Scheitelpunktbeschriftung verbessern. Für NLP und graphbasierte Aufgaben wurden Einbettungen in hochdimensionalen euklidischen Räumen erlernt; neuere Arbeiten haben jedoch gezeigt, dass der geeignete isometrische Raum für die Einbettung komplexer Netzwerke nicht der flache euklidische Raum ist, sondern ein negativ gekrümmter hyperbolischer Raum; wir stellen ein neues Konzept vor, das diese jüngsten Erkenntnisse ausnutzt, und schlagen vor, neuronale Einbettungen von Graphen im hyperbolischen Raum zu erlernen; wir liefern experimentelle Beweise dafür, dass hyperbolische Einbettungen euklidische Einbettungen bei Vertex-Klassifizierungsaufgaben für mehrere reale öffentliche Datensätze deutlich übertreffen.
Die International Competition on Knowledge Engineering for Planning and Scheduling (ICKEPS) spielt eine zentrale Rolle bei der Förderung der Entwicklung neuer Knowledge-Engineering (KE)-Werkzeuge und bei der Betonung der Bedeutung von prinzipiellen Ansätzen für alle verschiedenen KE-Aspekte, die für den erfolgreichen langfristigen Einsatz von Planung in realen Anwendungen erforderlich sind.  In diesem Beitrag überprüfen wir das Format früherer ICKEPS-Wettbewerbe, um alternative Formate für künftige Wettbewerbe vorzuschlagen und idealerweise jemanden zu motivieren, die nächsten Wettbewerbe zu organisieren, um so eine Synthese zu schaffen und Gedanken und Diskussionen anzuregen.
Wir zeigen, dass die Generierung englischer Wikipedia-Artikel als eine Multi-Dokument-Zusammenfassung von Quelldokumenten betrachtet werden kann. Wir verwenden extraktive Zusammenfassungen, um wichtige Informationen grob zu identifizieren, und ein neuronales Abstraktionsmodell, um den Artikel zu generieren.Für das Abstraktionsmodell führen wir eine reine Decoder-Architektur ein, die skalierbar auf sehr lange Sequenzen reagieren kann, viel länger als typische Encoder-Decoder-Architekturen, die bei der Sequenztransduktion verwendet werden. Wir zeigen, dass dieses Modell in der Lage ist, flüssige, kohärente mehrsätzige Absätze und sogar ganze Wikipedia-Artikel zu generieren, und dass es in der Lage ist, aus Referenzdokumenten relevante Sachinformationen zu extrahieren, was sich in Perplexität, ROUGE-Scores und menschlichen Bewertungen widerspiegelt.
Abstrakt Stochastic Gradient Descent (SGD) und Adam werden häufig verwendet, um tiefe neuronale Netze zu optimieren, aber die Wahl eines bedeutet in der Regel Kompromisse zwischen Geschwindigkeit, Genauigkeit und Stabilität.Hier präsentieren wir eine Intuition für, warum die Kompromisse existieren sowie eine Methode für die Vereinheitlichung der beiden in einer kontinuierlichen way.This macht es möglich, die Art und Weise Modelle trainiert werden in viel größerem Detail.We zeigen, dass für Standard-Parameter, der neue Algorithmus gleich oder übertrifft SGD und Adam über eine Reihe von Modellen für Bild-Klassifikation Aufgaben und übertrifft SGD für Sprache Modellierung Aufgaben.
Die Verwendung von Nachahmungslernen zum Erlernen einer einzigen Strategie für eine komplexe Aufgabe, die mehrere Modi oder eine hierarchische Struktur hat, kann eine Herausforderung sein. frühere Arbeiten haben gezeigt, dass, wenn die Modi bekannt sind, das Erlernen separater Strategien für jeden Modus oder jede Teilaufgabe die Leistung des Nachahmungslernens erheblich verbessern kann. Unser Ansatz maximiert den gerichteten Informationsfluss im grafischen Modell zwischen latenten Variablen der Teilaufgaben und ihren generierten Trajektorien. Wir zeigen auch, wie unser Ansatz mit dem bestehenden Options-Rahmenwerk verbunden ist, das üblicherweise zum Lernen hierarchischer Strategien verwendet wird.
Das Convolutional Neural Network (CNN) wurde in den letzten Jahrzehnten in vielen Bereichen erfolgreich eingesetzt, jedoch fehlt ihm die Fähigkeit, vorheriges Domänenwissen zu nutzen, wenn es mit vielen realistischen Problemen zu tun hat. wir präsentieren ein Framework namens Geometric Operator Convolutional Neural Network (GO-CNN), das Domänenwissen nutzt, wobei der Kernel der ersten Faltungsschicht durch einen Kernel ersetzt wird, der durch eine geometrische Operatorfunktion erzeugt wird. Unter bestimmten Bedingungen analysieren wir theoretisch die Konvergenz und die Grenze der Generalisierungsfehler zwischen GO-CNNs und gewöhnlichen CNNs. Obwohl die geometrischen Operator-Faltungskerne weniger trainierbare Parameter haben als gewöhnliche Faltungskerne, zeigen die experimentellen Ergebnisse, dass GO-CNNs bei CIFAR-10/100 genauer sind als gewöhnliche CNNs. Außerdem verringert GO-CNNs die Abhängigkeit von der Menge der Trainingsbeispiele und verbessert die Stabilität der Gegner.
Determinantal Punkt Prozesse (DPPs) ist ein wirksames Instrument, um Vielfalt auf mehrere maschinelles Lernen und Computer Vision tasks.Under tiefen Lernen Rahmen, DPP ist in der Regel über die Annäherung, die nicht einfach ist und hat einige Konflikt mit Vielfalt requirement.We beachten, jedoch hat es keine tiefen Lernen Paradigmen zu optimieren DPP direkt, da es Matrix-Inversion, die in sehr rechnerische Instabilität führen kann.This Tatsache stark behindert die breite Nutzung von DPP auf einige spezifische Ziele, wo DPP dient als ein Begriff, um die Funktion Vielfalt zu messen. In diesem Papier entwickeln wir einen einfachen, aber effektiven Algorithmus, um dieses Problem zu lösen und den DPP-Term direkt mit L-Ensemble im Spektralbereich über die Grammatrix zu optimieren, was flexibler ist als das Lernen auf parametrischen Kernen. Durch die Berücksichtigung einiger geometrischer Einschränkungen versucht unser Algorithmus, gültige Sub-Gradienten des DPP-Terms zu generieren, wenn die DPP-Gramm-Matrix nicht invertierbar ist (in diesem Fall gibt es keine Gradienten), so dass unser Algorithmus leicht in mehrere Deep-Learning-Aufgaben integriert werden kann.
Die Qualität eines maschinellen Übersetzungssystems hängt weitgehend von der Verfügbarkeit umfangreicher paralleler Korpora ab. Für die kürzlich populäre neuronale maschinelle Übersetzung (NMT) kann das Problem der Datenarmut sogar noch schwerwiegender werden: Mit einer großen Menge an einstellbaren Parametern kann sich das NMT-Modell zu sehr an die bestehenden Sprachpaare anpassen, während es die allgemeine Vielfalt der Sprache nicht versteht. In dieser Arbeit plädieren wir dafür, jedes Satzpaar in zwei Gruppen ähnlicher Sätze aufzuteilen, um die Vielfalt der sprachlichen Ausdrücke zu berücksichtigen, die wir als parallele Cluster bezeichnen, und dann einen allgemeineren Cluster-zu-Cluster-Korrespondenzwert zu definieren und unser Modell zu trainieren, um diesen Wert zu maximieren. Da eine direkte Maximierung schwierig ist, leiten wir die untere Schranke als unser Ersatzziel ab, das die Punkt-zu-Punkt-Maximum-Likelihood-Schätzung (MLE) und die Punkt-zu-Cluster-Reward-Augmented-Maximum-Likelihood-Algorithmen (RAML) als Spezialfälle verallgemeinert.Auf der Grundlage dieser neuartigen Zielfunktion definieren wir vier potenzielle Systeme zur Realisierung unseres Cluster-zu-Cluster-Rahmens und testen ihre Leistungen in drei anerkannten Übersetzungsaufgaben, wobei jede Aufgabe eine Vorwärts- und eine Rückwärtsübersetzungsrichtung hat. In jedem der sechs Experimente haben unsere vier vorgeschlagenen parallelen Systeme durchweg bewiesen, dass sie die MLE-Basislinie, RL (Reinforcement Learning) und RAML-Systeme signifikant übertreffen, und schließlich haben wir eine Fallstudie durchgeführt, um die Stärke des Cluster-to-Cluster-NMT-Rahmens empirisch zu analysieren.
Die Fähigkeit, interpretierbare und selbsterklärende Entscheidungen zu treffen, ist für die Entwicklung verantwortungsbewusster maschineller Lernsysteme unerlässlich.In dieser Arbeit untersuchen wir das Lernen, um das Problem im Rahmen der induktiven logischen Programmierung (ILP) zu erklären.Wir schlagen Neural Logic Inductive Learning (NLIL) vor, ein effizientes differenzierbares ILP-Framework, das logische Regeln erster Ordnung lernt, die die Muster in den Daten erklären können. In Experimenten haben wir herausgefunden, dass NLIL im Vergleich zu State-of-the-Art-Modellen in der Lage ist, nach Regeln zu suchen, die x10-mal länger sind und dabei x3-mal schneller sind. Wir zeigen auch, dass NLIL auf große Bilddatensätze skalieren kann, z.B. Visual Genome mit 1 Mio. Entitäten.
Neuronale Netze sind jedoch anfällig für negative Beispiele, d.h. sorgfältig gestörte Eingaben, die dazu führen, dass sich die Netze auf willkürlich gewählte Weise falsch verhalten. Wenn sie mit Standardmethoden erzeugt werden, können diese Beispiele einen Klassifikator in der realen Welt aufgrund einer Kombination aus Blickpunktverschiebungen, Kamerarauschen und anderen natürlichen Transformationen nicht durchgängig täuschen.Negative Beispiele, die mit Standardtechniken erzeugt werden, erfordern eine vollständige Kontrolle über die direkte Eingabe in den Klassifikator, was in vielen realen Systemen unmöglich ist. Wir stellen die erste Methode zur Konstruktion realer 3D-Objekte vor, die ein neuronales Netzwerk über eine breite Verteilung von Winkeln und Standpunkten hinweg konsistent täuschen. Schließlich wenden wir den Algorithmus an, um beliebige physische 3D-gedruckte Gegenbeispiele zu erzeugen, und demonstrieren, dass unser Ansatz durchgängig in der realen Welt funktioniert.Unsere Ergebnisse zeigen, dass Gegenbeispiele ein praktisches Problem für reale Systeme sind.
Zu diesem Zweck schlagen wir ein Permutations-Optimierungsmodul vor, das lernt, wie man eine Menge Ende-zu-Ende permutiert. Die permutierte Menge kann weiterverarbeitet werden, um eine permutationsinvariante Repräsentation dieser Menge zu lernen, wodurch ein Engpass in traditionellen Mengenmodellen vermieden wird. Wir demonstrieren die Fähigkeit unseres Modells, Permutationen und Mengenrepräsentationen entweder mit expliziter oder impliziter Überwachung auf vier Datensätzen zu erlernen, auf denen wir modernste Ergebnisse erzielen: Zahlensortierung, Bildmosaike, Klassifizierung von Bildmosaiken und Beantwortung visueller Fragen.
Das physische Design eines Roboters und die Strategie, die seine Bewegung steuert, sind inhärent gekoppelt, aber bestehende Ansätze ignorieren diese Kopplung weitgehend und entscheiden sich stattdessen dafür, zwischen getrennten Design- und Kontrollphasen abzuwechseln, was durchgehend Expertenintuition erfordert und das Risiko der Konvergenz zu suboptimalen Designs birgt. Während des Trainings verfeinern wir die Verteilung des Roboters, um die erwartete Belohnung zu maximieren. Dies führt zu einer Zuordnung der Roboterparameter und der Politik des neuronalen Netzwerks, die gemeinsam optimal sind. Wir evaluieren unseren Ansatz im Kontext der Fortbewegung von Beinen und zeigen, dass er neuartige Roboterdesigns und Laufwege für verschiedene Morphologien entdeckt und eine Leistung erzielt, die mit der von handgefertigten Designs vergleichbar oder besser ist.
Deep Learning ermöglicht das Training großer und flexibler Funktionsapproximatoren von Grund auf auf Kosten großer Datenmengen.Anwendungen neuronaler Netze betrachten oft das Lernen im Kontext einer einzigen Aufgabe.In vielen Szenarien ist das, was wir zu lernen hoffen, jedoch nicht nur eine einzige Aufgabe, sondern ein Modell, das verwendet werden kann, um mehrere verschiedene Aufgaben zu lösen.Solche Multi-Task-Learning-Einstellungen haben das Potenzial, die Dateneffizienz und Generalisierung durch die gemeinsame Nutzung von Daten und Repräsentationen über Aufgaben zu verbessern. In einigen anspruchsvollen Multi-Task-Learning-Settings, insbesondere beim Reinforcement Learning, ist es jedoch sehr schwierig, ein einzelnes Modell zu erlernen, das alle Aufgaben lösen kann und gleichzeitig die Dateneffizienz und die Leistungsvorteile realisiert.Das Erlernen jeder einzelnen Aufgabe von Grund auf kann in solchen Settings tatsächlich eine bessere Leistung erbringen, aber es profitiert nicht von der gemeinsamen Nutzung von Repräsentationen, die das Multi-Task-Learning potenziell bieten kann. Zu diesem Zweck führen wir Matrix-Interleaving (Mint) ein, eine Modifikation von Standardmodellen neuronaler Netze, die die Aktivierungen für jede Aufgabe in einen anderen gelernten Unterraum projiziert, der durch eine Matrix pro Aufgabe und pro Schicht dargestellt wird. Durch das gemeinsame Lernen dieser Matrizen mit den anderen Modellparametern kann der Optimierer selbst entscheiden, in welchem Umfang die Repräsentationen zwischen den Aufgaben geteilt werden sollen. Bei drei anspruchsvollen Multitasking-Problemen mit überwachtem Lernen und Verstärkungslernen mit unterschiedlichem Grad an geteilter Aufgabenstruktur haben wir festgestellt, dass dieses Modell durchgängig mit dem gemeinsamen Training und dem unabhängigen Training übereinstimmt oder diese übertrifft, indem es die besten Elemente von beiden kombiniert.
Die Ausbildung von Agenten, die in einer Umgebung arbeiten, führt oft zu überangepassten Modellen, die nicht in der Lage sind, sich auf die Veränderungen in dieser Umgebung zu verallgemeinern, aber aufgrund der zahlreichen Variationen, die in der realen Welt auftreten können, muss der Agent oft robust sein, um nützlich zu sein, was bei Agenten, die mit Algorithmen des Verstärkungslernens (RL) ausgebildet werden, nicht der Fall ist. Wir schlagen eine Regularisierungsmethode vor, die RL mit Methoden des überwachten Lernens kombiniert, indem wir dem RL-Ziel einen Term hinzufügen, der die Invarianz einer Politik gegenüber Variationen in den Beobachtungen fördert, die die getroffenen Maßnahmen nicht beeinflussen sollten.
Obwohl visuelle Informationen zur Verbesserung der neuronalen maschinellen Übersetzung (NMT) eingeführt wurden, hängt ihre Effektivität stark von der Verfügbarkeit großer Mengen zweisprachiger paralleler Satzpaare mit manuellen Bildannotationen ab.In diesem Papier stellen wir eine universelle visuelle Repräsentation vor, die über einsprachige Korpora mit Bildannotationen gelernt wurde, die den Mangel an großen zweisprachigen Satz-Bild-Paaren überwindet und dadurch die Anwendbarkeit von Bildern in der NMT erweitert. Im Detail wird eine Gruppe von Bildern mit ähnlichen Themen wie der Ausgangssatz aus einer leichten Themen-Bild-Lookup-Tabelle, die über die vorhandenen Satz-Bild-Paare gelernt wurde, abgerufen und dann als Bildrepräsentationen durch ein vortrainiertes ResNet kodiert. Eine Aufmerksamkeitsschicht mit einer Gated Weighting ist es, die visuellen Informationen und Textinformationen als Eingabe in den Decoder für die Vorhersage von Zielübersetzungen zu verschmelzen.Insbesondere ermöglicht die vorgeschlagene Methode die visuellen Informationen in großen Text-only NMT zusätzlich zu den Multimodel NMT integriert werden.Experimente auf vier weit verbreitete Übersetzung Datensätze, einschließlich der WMT'16 Englisch-Rumänisch, WMT'14 Englisch-Deutsch, WMT'14 Englisch-Französisch, und Multi30K, zeigen, dass der vorgeschlagene Ansatz erhebliche Verbesserungen gegenüber starken Grundlinien erreicht.
Um dieses Ziel zu erreichen, führen wir eine Reihe von Schlüsselideen aus der traditionellen Algorithmik und Komplexitätstheorie ein: Zunächst stellen wir eine neue Verbindung zwischen primär-dualen Methoden und Reinforcement Learning her. Wir testen unsere neuen Ideen an einer Reihe von Optimierungsproblemen wie dem AdWords-Problem, dem Online-Knapselproblem und dem Sekretärinnenproblem. Unsere Ergebnisse zeigen, dass die Modelle Verhaltensweisen gelernt haben, die mit den traditionellen optimalen Algorithmen für diese Probleme übereinstimmen.
Trotz ihrer Popularität und ihrer Erfolge sind tiefe neuronale Netze theoretisch kaum verstanden und werden als "Black Box"-Systeme behandelt. Die Verwendung einer funktionalen Sichtweise dieser Netze gibt uns eine nützliche neue Linse, mit der wir sie verstehen können. Dies ermöglicht uns, die Eigenschaften dieser Netze theoretisch oder experimentell zu untersuchen, einschließlich der Auswirkungen von Standardinitialisierungen, des Wertes der Tiefe, der zugrunde liegenden Verlustoberfläche und der Ursprünge der Generalisierung. Ein zentrales Ergebnis ist, dass die Generalisierung aus der Glätte der funktionalen Annäherung in Kombination mit einer flachen anfänglichen Annäherung resultiert, die mit der Anzahl der Einheiten zunimmt und erklärt, warum massiv überparamaterisierte Netze weiterhin gut generalisieren.
Es ist bekannt, dass tiefere neuronale Netze schwieriger zu trainieren sind als flachere. In diesem kurzen Beitrag verwenden wir das (vollständige) Eigenwertspektrum der Hessian, um zu untersuchen, wie sich die Verlustlandschaft ändert, wenn das Netz tiefer wird und wenn Restverbindungen zur Architektur hinzugefügt werden. Durch die Berechnung einer Reihe von quantitativen Maßen auf dem Hess'schen Spektrum zeigen wir, dass die Hess'sche Eigenwertverteilung in tieferen Netzen wesentlich schwerere Schwänze hat (d.h. mehr Ausreißereigenwerte), was die Optimierung des Netzes mit Methoden erster Ordnung erschwert.Wir zeigen, dass das Hinzufügen von Restverbindungen diesen Effekt erheblich abschwächt, was auf einen Mechanismus hindeutet, durch den Restverbindungen das Training verbessern.
Im Zusammenhang mit der Optimierung, ein Gradient eines neuronalen Netzes zeigt die Menge ein bestimmtes Gewicht sollte in Bezug auf den Verlust zu ändern.daher, kleine Gradienten zeigen einen guten Wert des Gewichts, die keine Änderung erfordert und kann während des Trainings eingefroren werden.dieses Papier bietet eine experimentelle Studie über die Bedeutung eines neuronalen Netzes Gewichte, und in welchem Umfang müssen sie aktualisiert werden.wir möchten zeigen, dass ab der dritten Epoche, Einfrieren Gewichte, die keine informativen Gradienten haben und sind weniger wahrscheinlich, dass während des Trainings geändert werden, führt zu einem sehr geringen Rückgang der Gesamtgenauigkeit (und in manchmal besser). Wir experimentieren mit den Datensätzen MNIST, CIFAR10 und Flickr8k unter Verwendung verschiedener Architekturen (VGG19, ResNet-110 und DenseNet-121) und zeigen, dass das Einfrieren von 80 % der VGG19-Netzwerkparameter ab der dritten Epoche zu einem Genauigkeitsverlust von 0,24 % führt, während das Einfrieren von 50 % der Resnet-110-Parameter zu einem Genauigkeitsverlust von 0,9 % und schließlich das Einfrieren von 70 % der Densnet-121-Parameter zu einem Genauigkeitsverlust von 0,57 % führt. Um mit realen Anwendungen zu experimentieren, trainieren wir ein Modell für Bildunterschriften mit einem Aufmerksamkeitsmechanismus auf dem Flickr8k-Datensatz unter Verwendung von LSTM-Netzen, wobei wir 60% der Parameter ab der dritten Epoche einfrieren, was zu einer besseren BLEU-4-Punktzahl führt als das vollständig trainierte Modell.Unser Quellcode ist im Anhang zu finden.
Während herkömmliche Ansätze zum Lernen nach Lehrplänen die Schwierigkeit der Proben als zentrale inkrementelle Strategie betonen, zwingt dies die Netze dazu, aus kleinen Teilmengen von Daten zu lernen und gleichzeitig einen Overhead für die Vorberechnung einzuführen. In dieser Arbeit schlagen wir das Lernen mit inkrementellen Labels und adaptiver Kompensation (LILAC) vor, das einen neuartigen Ansatz für das Curriculum-Lernen einführt.LILAC betont das inkrementelle Lernen von Labels anstelle des inkrementellen Lernens schwieriger Proben.Es arbeitet in zwei verschiedenen Phasen: zuerst, in der inkrementellen Label-Einführungsphase, demaskieren wir Ground-Truth-Labels in festen Schritten während des Trainings, um den Ausgangspunkt zu verbessern, von dem Netzwerke lernen. In der adaptiven Kompensationsphase kompensieren wir fehlgeschlagene Vorhersagen, indem wir den Zielvektor adaptiv in eine glattere Verteilung umwandeln. Wir evaluieren LILAC im Vergleich zu den am ehesten vergleichbaren Methoden in den Bereichen Batch- und Curriculum-Lernen und Label-Glättung anhand von drei Standard-Bild-Benchmarks, CIFAR-10, CIFAR-100 und STL-10. Wir zeigen, dass unsere Methode das Batch-Lernen mit einer höheren mittleren Erkennungsgenauigkeit und einer geringeren Standardabweichung über alle Benchmarks hinweg übertrifft. Wir erweitern LILAC auf eine State-of-the-Art-Performance bei CIFAR-10, indem wir eine einfache Datenerweiterung verwenden und dabei neben anderen wichtigen Eigenschaften auch eine Invarianz der Labelreihenfolge aufweisen.
Wörter in der natürlichen Sprache folgen einer Zipf'schen Verteilung, bei der einige Wörter häufig, die meisten aber selten sind, und das Lernen von Repräsentationen für Wörter im "langen Schwanz" dieser Verteilung enorme Datenmengen erfordert. Repräsentationen von seltenen Wörtern, die direkt auf Endaufgaben trainiert werden, sind in der Regel schlecht, so dass wir Einbettungen auf externen Daten vortrainieren oder alle seltenen Wörter als Wörter außerhalb des Vokabulars mit einer einzigartigen Repräsentation behandeln müssen.Wir bieten eine Methode zur Vorhersage von Einbettungen von seltenen Wörtern on the fly aus kleinen Mengen von Hilfsdaten mit einem Netzwerk, das Ende-zu-Ende für die nachgelagerte Aufgabe trainiert wurde.Wir zeigen, dass dies die Ergebnisse gegenüber den Grundlinien verbessert, bei denen Einbettungen auf die Endaufgabe für das Leseverständnis, die Erkennung von Textentailment und Sprachmodellierung trainiert werden.
In dieser Arbeit schlagen wir einen tiefen generativen Klassifikator vor, der durch die Integration des Konzepts der Gaußschen Diskriminanzanalyse in tiefe neuronale Netze sowohl Proben außerhalb der Verteilung erkennen als auch Proben innerhalb der Verteilung klassifizieren kann. Im Gegensatz zum diskriminativen (oder Softmax-) Klassifikator, der sich nur auf die Entscheidungsgrenze konzentriert, die seinen latenten Raum in mehrere Regionen aufteilt, zielt unser generativer Klassifikator darauf ab, die klassenbedingten Verteilungen explizit als trennbare Gauß-Verteilungen zu modellieren, wodurch wir den Vertrauenswert durch den Abstand zwischen einer Testprobe und dem Zentrum jeder Verteilung definieren können.
Eines der häufigsten Symptome in der älteren Bevölkerung, die Demenz, kann durch Klassifikatoren erkannt werden, die auf linguistischen Merkmalen trainiert wurden, die aus narrativen Transkripten extrahiert wurden, wobei diese linguistischen Merkmale auf ähnliche, aber unterschiedliche Weise durch den normalen Alterungsprozess beeinflusst werden. In diesem Papier zeigen wir, dass tiefe neuronale Netze (DNN) Klassifikatoren können Alter von sprachlichen Merkmalen, die eine Verflechtung, die zu Unfairness zwischen den Altersgruppen führen könnte ableiten.Wir zeigen, dass dieses Problem durch unerwünschte Aktivierungen von v-Strukturen in Kausalitätsdiagramme verursacht wird, und es könnte mit fairen Repräsentation Lernen adressiert werden.Wir bauen neuronale Netzwerk-Klassifikatoren, die niedrig-dimensionale Repräsentationen, die die Auswirkungen der Demenz widerspiegeln, aber die Auswirkungen des Alters zu verwerfen lernen. Um diese Klassifikatoren zu bewerten, spezifizieren wir eine modellagnostische Punktzahl $\Delta_{eo}^{(N)}$, die misst, wie die Klassifikatorergebnisse vom Alter entkoppelt werden.Unsere besten Modelle übertreffen die Basisklassifikatoren neuronaler Netze bei der Entkopplung, während sie die Genauigkeit um nur 2,56\% bzw. 2,25\% in der DementiaBank und dem Famous People-Datensatz beeinträchtigen.
Bei der Entwicklung von tiefen neuronalen Netzen lag der Schwerpunkt auf der Verbesserung der Genauigkeit, was zu leistungsfähigeren, aber hochkomplexen Netzarchitekturen führte, die in der Praxis nur schwer eingesetzt werden können.  Infolgedessen ist in letzter Zeit ein Interesse an der Entwicklung quantitativer Metriken zur Bewertung tiefer neuronaler Netze entstanden, die mehr als nur die Modellgenauigkeit als einzigen Indikator für die Leistung des Netzes berücksichtigen.  In dieser Studie setzen wir die Diskussion über universelle Metriken zur Bewertung der Leistung von tiefen neuronalen Netzen für den praktischen Einsatz auf Geräten fort, indem wir NetScore vorstellen, eine neue Metrik, die speziell für eine quantitative Bewertung des Gleichgewichts zwischen Genauigkeit, Rechenkomplexität und Komplexität der Netzwerkarchitektur eines tiefen neuronalen Netzes entwickelt wurde.  In einer der umfangreichsten vergleichenden Analysen zwischen tiefen neuronalen Netzen in der Literatur wurden die NetScore-Metrik, die Top-1-Genauigkeitsmetrik und die populäre Informationsdichtemetrik mit einer Reihe von 60 verschiedenen tiefen neuronalen Faltungsnetzen für die Bildklassifizierung auf dem ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2012) Datensatz verglichen.  Die Bewertungsergebnisse dieser drei Metriken für diese verschiedenen Netzwerke werden in dieser Studie vorgestellt, um als Referenz für Praktiker auf diesem Gebiet zu dienen.  
Deep Neural Networks (DNNs) sind anfällig für gegnerische Angriffe, insbesondere für gezielte White-Box-Angriffe. Dieses Papier untersucht das Problem, wie aggressiv gezielte White-Box-Angriffe sein können, um über die weit verbreiteten Top-1-Angriffe hinauszugehen. Wir schlagen vor, geordnete Top-k-Angriffe (k>=1) zu erlernen, die erzwingen, dass die Top-k vorhergesagten Labels eines gegnerischen Beispiels die k (zufällig) ausgewählten und geordneten Labels sind (das Ground-Truth-Label ist exklusiv). Wir stellen zwei Methoden vor: Erstens erweitern wir die Carlini-Wagner-Methode (C&W) und verwenden sie als solide Grundlage; zweitens stellen wir ein Verfahren zur Destillation von gegnerischen Beispielen vor, das aus zwei Komponenten besteht: (i) Berechnung einer gegnerischen Wahrscheinlichkeitsverteilung für beliebige geordnete Top-$k$-Zielmarken. (ii) Lernen von adversen Beispielen durch Minimierung der Kullback-Leibler (KL) Divergenz zwischen der adversen Verteilung und der vorhergesagten Verteilung, zusammen mit der Störungsenergie-Strafe.Bei der Berechnung von adversen Verteilungen erforschen wir, wie man semantische Ähnlichkeiten der Labels ausnutzen kann, was zu wissensorientierten Angriffen führt. In Experimenten testen wir Top-k (k=1,2,5,10) Angriffe im ImageNet-1000 val Datensatz unter Verwendung von zwei populären DNNs, die mit dem sauberen ImageNet-1000 train Datensatz trainiert wurden, ResNet-50 und DenseNet-121.Insgesamt erzielt der Ansatz der adversen Destillation die besten Ergebnisse, insbesondere mit großem Abstand, wenn das Berechnungsbudget begrenzt ist. Er reduziert die Störenergie konsistent mit der gleichen Angriffserfolgsrate für alle vier k's und verbessert die Angriffserfolgsrate mit großem Abstand gegenüber der modifizierten C&W Methode für k=10.   
Neuronale Message-Passing-Algorithmen für die halbüberwachte Klassifizierung auf Graphen haben in letzter Zeit große Erfolge erzielt. Allerdings berücksichtigen diese Methoden für die Klassifizierung eines Knotens nur Knoten, die ein paar Propagationsschritte entfernt sind, und die Größe dieser verwendeten Nachbarschaft ist schwer zu erweitern.In diesem Papier verwenden wir die Beziehung zwischen Graph Convolutional Networks (GCN) und PageRank, um ein verbessertes Propagationsschema auf der Grundlage des personalisierten PageRank abzuleiten. Wir nutzen dieses Propagierungsverfahren, um ein einfaches Modell, die personalisierte Propagierung neuronaler Vorhersagen (PPNP), und seine schnelle Annäherung, APPNP, zu konstruieren. Die Trainingszeit unseres Modells ist gleich oder schneller und die Anzahl der Parameter gleich oder geringer als bei früheren Modellen. Es nutzt eine große, anpassbare Nachbarschaft für die Klassifizierung und kann leicht mit jedem neuronalen Netzwerk kombiniert werden.
Die überwiegende Mehrheit der Lexikoninduktions-Evaluierungswörterbücher sind zwischen Englisch und einer anderen Sprache, und der englische Einbettungsraum wird standardmäßig als Knotenpunkt ausgewählt, wenn in einer mehrsprachigen Umgebung gelernt wird.Mit dieser Arbeit stellen wir diese Praktiken jedoch in Frage.Erstens zeigen wir, dass die Wahl der Knotenpunktsprache die nachgelagerte Lexikoninduktionsleistung erheblich beeinflussen kann. Zweitens erweitern wir die derzeitige Sammlung von Evaluierungswörterbüchern, um alle Sprachpaare mit Triangulation einzubeziehen, und erstellen auch neue Wörterbücher für unterrepräsentierte Sprachen. Die Evaluierung etablierter Methoden über all diese Sprachpaare wirft ein Licht auf ihre Eignung und stellt neue Herausforderungen für das Feld dar.
Die Interpretation des Trainings von generativen adversen Netzwerken (GANs) als annähernde Divergenzminimierung war theoretisch aufschlussreich, hat die Diskussion beflügelt und zu theoretisch und praktisch interessanten Erweiterungen wie f-GANs und Wasserstein-GANs geführt.Sowohl für klassische GANs als auch für f-GANs gibt es eine ursprüngliche Trainingsvariante und eine "nicht-sättigende" Variante, die eine alternative Form der Generatoraktualisierung verwendet.Die ursprüngliche Variante ist theoretisch einfacher zu untersuchen, aber die alternative Variante schneidet häufig besser ab und wird für den Einsatz in der Praxis empfohlen. Die alternative Generatoraktualisierung wird oft als einfache Modifikation zur Lösung von Optimierungsproblemen angesehen, und es scheint ein weit verbreiteter Irrglaube zu sein, dass die beiden Varianten dieselbe Divergenz minimieren.In dieser kurzen Notiz leiten wir die Divergenzen ab, die durch die ursprüngliche und die alternative Variante des GAN- und f-GAN-Trainings annähernd minimiert werden.Dies zeigt wichtige Unterschiede zwischen den beiden Varianten auf. Wir zeigen zum Beispiel, dass die alternative Variante des KL-GAN-Trainings tatsächlich die umgekehrte KL-Divergenz minimiert und dass die alternative Variante des konventionellen GAN-Trainings eine "abgeschwächte" Version der umgekehrten KL-Divergenz minimiert.Wir hoffen, dass diese Ergebnisse zur Klärung der theoretischen Diskussion über die Divergenzminimierung beim GAN-Training beitragen.
REINFORCE kann verwendet werden, um Modelle in strukturierten Vorhersage-Settings zu trainieren, um das Testzeit-Ziel direkt zu optimieren. Allerdings ist der übliche Fall, eine Vorhersage pro Datenpunkt (Input) zu ziehen, datenineffizient. Wir zeigen, dass wir durch das Ziehen mehrerer Stichproben (Vorhersagen) pro Datenpunkt mit deutlich weniger Daten lernen können, da wir frei eine REINFORCE-Basislinie erhalten, um die Varianz zu reduzieren. Kombiniert mit einer neuen Technik, um Sequenzen ohne Ersetzung mit Hilfe der stochastischen Balkensuche zu probieren, verbessert dies das Trainingsverfahren für ein Sequenzmodell, das die Lösung des Travelling Salesman Problems vorhersagt.
Verstärkungslernen (Reinforcement Learning, RL) ist eine leistungsfähige Technik zum Trainieren eines Agenten für die Ausführung einer Aufgabe.  Ein mit RL trainierter Agent ist jedoch nur in der Lage, die einzige Aufgabe zu erfüllen, die durch seine Belohnungsfunktion vorgegeben ist.   Ein solcher Ansatz eignet sich nicht für Situationen, in denen ein Agent eine Vielzahl von Aufgaben erfüllen muss, z. B. die Navigation zu verschiedenen Positionen in einem Raum oder das Bewegen von Objekten an verschiedene Orte.  Stattdessen schlagen wir eine Methode vor, die es einem Agenten ermöglicht, automatisch die Bandbreite der Aufgaben zu entdecken, die er in seiner Umgebung ausführen kann.  Wir verwenden ein Generator-Netzwerk, um dem Agenten Aufgaben vorzuschlagen, die er zu erfüllen versucht, wobei jede Aufgabe durch das Erreichen einer bestimmten parametrisierten Teilmenge des Zustandsraums spezifiziert wird.  Das Generatoren-Netzwerk wird mit Hilfe von adversarialem Training optimiert, um Aufgaben zu erzeugen, die immer den richtigen Schwierigkeitsgrad für den Agenten haben.  Unsere Methode erzeugt somit automatisch ein Curriculum von Aufgaben, die der Agent lernen kann.  Wir zeigen, dass ein Agent mit Hilfe dieses Rahmens effizient und automatisch lernen kann, eine große Anzahl von Aufgaben auszuführen, ohne dass dafür Vorkenntnisse über seine Umgebung erforderlich sind (Videos und Code verfügbar unter: https://sites.google.com/view/goalgeneration4rl). Unsere Methode kann auch lernen, Aufgaben mit spärlichen Belohnungen zu erfüllen, die für herkömmliche RL-Methoden eine große Herausforderung darstellen.
Es wurde eine Vielzahl von Verteidigungsmaßnahmen vorgeschlagen, um neuronale Netze gegen feindliche Angriffe zu schützen, aber es hat sich ein Muster herauskristallisiert, bei dem die meisten Verteidigungsmaßnahmen gegen feindliche Angriffe schnell durch neue Angriffe gebrochen werden.  Angesichts des mangelnden Erfolgs bei der Entwicklung robuster Verteidigungsmaßnahmen stellt sich eine grundlegende Frage:  In diesem Beitrag werden Beispiele von Angriffen aus einer theoretischen Perspektive analysiert und grundlegende Grenzen für die Anfälligkeit eines Klassifizierers für Angriffe durch Angreifer aufgezeigt.   Wir zeigen, dass für bestimmte Klassen von Problemen gegnerische Beispiele unvermeidbar sind.  Anhand von Experimenten untersuchen wir die Auswirkungen der theoretischen Garantien auf reale Probleme und erörtern, wie Faktoren wie Dimensionalität und Bildkomplexität die Robustheit eines Klassifizierers gegenüber gegnerischen Beispielen einschränken.
Eine Möglichkeit, diesen großen Speicherbedarf zu reduzieren, ist die Verringerung der Genauigkeit der Aktivierungen. Frühere Arbeiten haben jedoch gezeigt, dass die Verringerung der Genauigkeit der Aktivierungen die Modellgenauigkeit beeinträchtigt. Wir untersuchen Schemata, um Netzwerke von Grund auf mit Aktivierungen geringerer Genauigkeit zu trainieren, ohne die Genauigkeit zu beeinträchtigen. Wir reduzieren die Genauigkeit der Aktivierungskarten (zusammen mit den Modellparametern) und erhöhen die Anzahl der Filterkarten in einer Schicht und stellen fest, dass dieses Schema die Genauigkeit des Basisnetzes mit voller Genauigkeit erreicht oder übertrifft. Wir nennen unser Schema WRPN - wide reduced-precision networks.We berichten über die Ergebnisse und zeigen, dass das WRPN-Schema besser ist als die bisher berichteten Genauigkeiten auf dem ILSVRC-12-Datensatz, während es im Vergleich zu den bisher berichteten Netzwerken mit reduzierter Genauigkeit rechnerisch weniger teuer ist.
Wir untersuchen Methoden für semi-supervised learning (SSL) eines neuronalen linearen Zufallsfeldes (CRF) für Named Entity Recognition (NER), indem wir den Tagger als amortisiertes Variationsposteriori in einem generativen Modell des Textes mit Tags behandeln. Wir untersuchen dann eine Reihe von zunehmend komplexen tiefen generative Modelle von Token gegeben Tags durch Ende-zu-Ende-Optimierung ermöglicht, den Vergleich der vorgeschlagenen Modelle gegen überwacht und starke CRF SSL baselines auf der Ontonotes5 NER Datensatz.Wir finden, dass unsere beste vorgeschlagene Modell konsequent verbessert die Leistung um $\approx 1\%$ F1 in niedrigen und moderaten Ressourcen Regime und leicht Adressen degenerieren Modell Verhalten in einem schwierigeren, teilweise überwachten Einstellung.
Um tiefe neuronale Netze in ressourcenbeschränkten Umgebungen (wie z.B. mobilen Geräten) praktikabel zu machen, ist es vorteilhaft, Modelle zu quantisieren, indem man Gewichte mit geringer Genauigkeit verwendet.Eine gängige Technik zur Quantisierung neuronaler Netze ist die Straight-Through-Gradient-Methode, die Backpropagation durch die Quantisierungsabbildung ermöglicht.Trotz ihres empirischen Erfolgs ist wenig darüber bekannt, warum die Straight-Through-Gradient-Methode funktioniert. Aufbauend auf einer neuartigen Beobachtung, dass die Straight-Through-Gradient-Methode in der Tat identisch ist mit dem bekannten Nesterov's Dual-Averaging-Algorithmus auf einem quantisierungsbeschränkten Optimierungsproblem, schlagen wir einen prinzipielleren alternativen Ansatz vor, der ProxQuant genannt wird und das quantisierte Netztraining stattdessen als reguliertes Lernproblem formuliert und mit der Prox-Gradient-Methode optimiert. ProxQuant führt Backpropagation auf dem zugrundeliegenden Vollpräzisionsvektor durch und wendet einen effizienten Prox-Operator zwischen den stochastischen Gradientenschritten an, um die Quantisierung zu fördern. Bei der Quantisierung von ResNets und LSTMs übertrifft ProxQuant die State-of-the-Art-Ergebnisse bei der binären Quantisierung und liegt bei der Multi-Bit-Quantisierung gleichauf mit dem Stand der Technik. Für die binäre Quantisierung zeigt unsere Analyse sowohl theoretisch als auch experimentell, dass ProxQuant stabiler ist als die Straight-Through-Gradient-Methode (d.h. BinaryConnect), was die Unverzichtbarkeit der Straight-Through-Gradient-Methode in Frage stellt und eine leistungsstarke Alternative darstellt.
Die Anfälligkeit von tiefen neuronalen Netzen gegenüber nachteiligen Beispielen ist zu einem bedeutenden Problem für den Einsatz dieser Modelle in sensiblen Bereichen geworden. Die Entwicklung einer definitiven Verteidigung gegen solche Angriffe hat sich als schwierig erwiesen, und die Methoden, die sich auf die Erkennung von nachteiligen Beispielen stützen, sind nur dann gültig, wenn der Angreifer den Erkennungsmechanismus nicht kennt.In diesem Papier betrachten wir das Problem der nachteiligen Erkennung im Rahmen der robusten Optimierung. Wir zeigen, dass AAT das Lernen von klassenbedingten Verteilungen fördert, was zu generativen Erkennungs-/Klassifizierungsansätzen führt, die sowohl robust als auch besser interpretierbar sind. Wir bieten umfassende Evaluierungen der oben genannten Methoden und demonstrieren ihre konkurrenzfähigen Leistungen und überzeugenden Eigenschaften bei der Erkennung von Widersachern und robusten Klassifizierungsproblemen.
Exploration ist eine Schlüsselkomponente für erfolgreiches Reinforcement Learning, aber optimale Ansätze sind rechnerisch schwer zu bewältigen, so dass sich Forscher auf die manuelle Entwicklung von Mechanismen konzentriert haben, die auf Explorationsboni und intrinsischen Belohnungen basieren, von denen einige durch merkwürdiges Verhalten in natürlichen Systemen inspiriert sind.  In dieser Arbeit schlagen wir eine Strategie vor, um Neugier-Algorithmen als Programme in einer domänenspezifischen Sprache zu kodieren und während einer Meta-Lernphase nach Algorithmen zu suchen, die RL-Agenten befähigen, in neuen Domänen gute Leistungen zu erbringen.  Unsere reichhaltige Programmiersprache, die neuronale Netze mit anderen Bausteinen, wie z.B. Nearest-Neighbor-Modulen, kombinieren und eigene Verlustfunktionen wählen kann, ermöglicht die Formulierung hochgradig verallgemeinerbarer Programme, die in so unterschiedlichen Domänen wie Gitternavigation mit Bildeingabe, Akrobot, Mondlandefähre, Ameise und Hopper gute Leistungen erbringen.   Um diesen Ansatz realisierbar zu machen, entwickeln wir mehrere Pruning-Techniken, einschließlich des Lernens, den Erfolg eines Programms auf der Grundlage seiner syntaktischen Eigenschaften vorherzusagen.   Wir demonstrieren die Wirksamkeit des Ansatzes empirisch, indem wir Neugier-Strategien finden, die denen in der veröffentlichten Literatur ähneln, sowie neuartige Strategien, die mit ihnen konkurrieren und sich gut verallgemeinern lassen.
Viele Algorithmen für maschinelles Lernen repräsentieren Eingabedaten mit Vektoreinbettungen oder diskreten Codes. Wenn Eingaben eine kompositorische Struktur aufweisen (z. B. Objekte, die aus Teilen aufgebaut sind, oder Prozeduren aus Unterprogrammen), stellt sich natürlich die Frage, ob sich diese kompositorische Struktur in den gelernten Repräsentationen der Eingaben widerspiegelt.Während die Bewertung der Kompositionalität in Sprachen in der Linguistik und in angrenzenden Bereichen große Aufmerksamkeit erregt hat, fehlen in der Literatur zum maschinellen Lernen allgemeine Werkzeuge zur Erstellung abgestufter Messungen der kompositorischen Struktur in allgemeineren (z. B. vektorwertigen) Bereichen. Wir beschreiben ein Verfahren zur Bewertung der Kompositionalität, indem wir messen, wie gut das wahre Modell, das die Repräsentation erzeugt, durch ein Modell approximiert werden kann, das eine Sammlung von abgeleiteten Repräsentationsprimitiven explizit zusammensetzt.wir verwenden das Verfahren, um formale und empirische Charakterisierungen der Kompositionsstruktur in einer Vielzahl von Umgebungen zu liefern und die Beziehung zwischen Kompositionalität und Lerndynamik, menschlichen Urteilen, Repräsentationsähnlichkeit und Generalisierung zu untersuchen.
In diesem Papier schlagen wir ein End-to-End-Deep-Learning-Modell, genannt E2Efold, für RNA-Sekundärstruktur-Vorhersage, die effektiv berücksichtigen können die inhärenten Einschränkungen in das problem.the Schlüssel Idee von E2Efold ist es, direkt vorherzusagen, die RNA-Base-Pairing-Matrix, und verwenden Sie eine unrolled Constrained Programming-Algorithmus als ein Baustein in der Architektur zu erzwingen Einschränkungen. Mit umfassenden Experimenten an Benchmark-Datensätzen demonstrieren wir die überlegene Leistung von E2Efold: Es sagt signifikant bessere Strukturen im Vergleich zu früheren SOTA voraus (in einigen Fällen 29,7 % Verbesserung der F1-Werte und sogar eine noch größere Verbesserung für pseudoknotierte Strukturen) und läuft so effizient wie die schnellsten Algorithmen in Bezug auf die Inferenzzeit.
Lernen in rekurrenten neuronalen Netzen (RNNs) wird meist durch Gradientenabstieg mit Backpropagation durch die Zeit (BPTT) implementiert, aber BPTT modelliert nicht genau, wie das Gehirn lernt. Stattdessen lassen sich viele experimentelle Ergebnisse zur synaptischen Plastizität als Drei-Faktor-Lernregeln zusammenfassen, die Eignungsspuren der lokalen neuronalen Aktivität und einen dritten Faktor beinhalten. Wir stellen hier die eligibility propagation (e-prop) vor, eine neue Faktorisierung der Verlustgradienten in RNNs, die in den Rahmen der Drei-Faktoren-Lernregeln passt, wenn sie für biophysikalische spiking neuronale Modelle abgeleitet wird.Bei Tests mit dem TIMIT Spracherkennungs-Benchmark ist sie konkurrenzfähig mit BPTT sowohl für das Training künstlicher LSTM-Netzwerke als auch spiking RNNs.Weitere Analysen legen nahe, dass die Vielfalt der Lernsignale und die Berücksichtigung langsamer interner neuronaler Dynamiken entscheidend für die Lerneffizienz von e-prop sind.
Rekurrente neuronale Netze (RNNs) sind eine effektive Repräsentation von Kontrollstrategien für eine Vielzahl von Verstärkungs- und Nachahmungslernproblemen.RNN-Politiken sind jedoch besonders schwierig zu erklären, zu verstehen und zu analysieren, da sie kontinuierlich bewertete Speichervektoren und Beobachtungsmerkmale verwenden.In diesem Papier führen wir eine neue Technik ein, die Quantized Bottleneck Insertion, um endliche Repräsentationen dieser Vektoren und Merkmale zu lernen. Das Ergebnis ist eine quantisierte Repräsentation des RNN, die analysiert werden kann, um unser Verständnis der Speichernutzung und des allgemeinen Verhaltens zu verbessern. Wir präsentieren Ergebnisse dieses Ansatzes auf synthetischen Umgebungen und sechs Atari-Spielen. Die resultierenden endlichen Repräsentationen sind in einigen Fällen überraschend klein, mit nur 3 diskreten Speicherzuständen und 10 Beobachtungen für eine perfekte Pong-Politik.
Aufbauend auf den jüngsten Erfolg von Deep Reinforcement Learning-Methoden, untersuchen wir die Möglichkeit der On-Policy Reinforcement Learning Verbesserung durch die Wiederverwendung der Daten aus mehreren aufeinanderfolgenden policies.On-Policy-Methoden bringen viele Vorteile, wie die Fähigkeit, jede resultierende policy.However, sie in der Regel verwerfen alle Informationen über die Politik, die zuvor existierte.In dieser Arbeit schlagen wir Anpassung der Replay-Puffer-Konzept, entlehnt aus der Off-Policy-Lernen Einstellung, um die On-Policy-Algorithmen. Die Methode nutzt die Optimierung von Vertrauensbereichen und vermeidet dabei einige der üblichen Probleme von Algorithmen wie TRPO oder ACKTR: Sie verwendet Hyperparameter, um die Heuristiken zur Auswahl von Vertrauensbereichen zu ersetzen, sowie die trainierbare Kovarianzmatrix anstelle der festen Matrix. In vielen Fällen verbessert die Methode nicht nur die Ergebnisse im Vergleich zu den modernsten On-Policy-Lernalgorithmen für Vertrauensbereiche wie ACKTR und TRPO, sondern auch im Vergleich zu ihrem Off-Policy-Gegenstück DDPG.  
Faltungsneuronale Netze (Convolutional Neural Networks, CNN) haben sich bei der Verarbeitung von Datensignalen bewährt, die im räumlichen Bereich gleichmäßig abgetastet werden (z. B. Bilder), Die meisten Datensignale liegen jedoch nicht von Natur aus auf einem Gitter vor und erleiden bei der Abtastung auf einem einheitlichen physikalischen Gitter erhebliche Aliasing-Fehler und Informationsverluste. Darüber hinaus können Signale in verschiedenen topologischen Strukturen vorliegen, z. B. als Punkte, Linien, Flächen und Volumen. Zu diesem Zweck entwickeln wir mathematische Formulierungen für nicht-uniforme Fourier-Transformationen (NUFT), um direkt und optimal nicht-uniforme Datensignale verschiedener Topologien, die auf einem Simplex-Gitter definiert sind, ohne räumlichen Abtastfehler in den Spektralbereich zu übertragen: (1) der Prozess verursacht keinen räumlichen Abtastfehler während der anfänglichen Abtastung, (2) die Allgemeingültigkeit dieses Ansatzes bietet einen einheitlichen Rahmen für die Verwendung von CNNs, um Signale von gemischten Topologien zu analysieren, (3) es erlaubt uns, modernste Backbone-CNN-Architekturen für effektives Lernen zu nutzen, ohne eine bestimmte Architektur für eine bestimmte Datenstruktur ad-hoc zu entwerfen, und (4) die Darstellung erlaubt gewichtete Netze, bei denen jedes Element ein unterschiedliches Gewicht hat (d.h. Textur), das lokale Eigenschaften anzeigt, Wir erzielen gute Ergebnisse auf dem Niveau des Stands der Technik für die 3D-Formensuche und einen neuen Stand der Technik für die Rekonstruktion von Punktwolken zu Oberflächen.
Die Entdeckung kausaler Strukturen zwischen einer Reihe von Variablen ist ein grundlegendes Problem in vielen empirischen Wissenschaften.Traditionelle Score-basierte Methoden zur Entdeckung von Zusammenhängen stützen sich auf verschiedene lokale Heuristiken, um nach einem gerichteten azyklischen Graphen (DAG) gemäß einer vordefinierten Score-Funktion zu suchen.Während diese Methoden, z.B., Während diese Methoden, wie z.B. die gierige Äquivalenzsuche, bei unendlichen Stichproben und bestimmten Modellannahmen attraktive Ergebnisse liefern können, sind sie in der Praxis aufgrund endlicher Daten und möglicher Verletzungen der Annahmen weniger zufriedenstellend.Motiviert durch die jüngsten Fortschritte in der neuronalen kombinatorischen Optimierung schlagen wir vor, Reinforcement Learning (RL) zu verwenden, um nach dem DAG mit dem besten Scoring zu suchen.Unser Encoder-Decoder-Modell nimmt beobachtbare Daten als Eingabe und generiert Graphen-Adjazenzmatrizen, die zur Berechnung von Belohnungen verwendet werden. Im Gegensatz zu typischen RL-Anwendungen, bei denen das Ziel darin besteht, eine Strategie zu erlernen, verwenden wir RL als Suchstrategie und unsere endgültige Ausgabe wäre der Graph unter allen Graphen, die während des Trainings erzeugt wurden, der die beste Belohnung erzielt.
Um das aussagekräftigere Thema zu generieren, das das gegebene Dokument besser repräsentiert, haben wir eine universelle Methode vorgeschlagen, die in der Vorverarbeitungsphase der Daten verwendet werden kann. Die Methode besteht aus drei Schritten: Erstens generiert sie das Wort/Wort-Paar aus jedem einzelnen Dokument, zweitens wendet sie einen parallelen TF-IDF-Algorithmus auf das Wort/Wort-Paar an, um es semantisch zu filtern, und drittens verwendet sie den k-means-Algorithmus, um die Wortpaare zusammenzuführen, die eine ähnliche semantische Bedeutung haben. Die Experimente werden mit der Open Movie Database (OMDb), dem Reuters-Datensatz und dem 20NewsGroup-Datensatz durchgeführt, wobei die mittlere durchschnittliche Präzision als Bewertungsmaßstab verwendet wird. Unsere Ergebnisse werden mit anderen State-of-the-Art-Themenmodellen wie der Latent-Dirichlet-Allokation und traditionellen Restricted-Boltzmann-Maschinen verglichen. Die von uns vorgeschlagene Datenvorverarbeitung kann die generierte Themengenauigkeit um bis zu 12,99% verbessern.
Durch die Ausnutzung der Beziehungen zwischen den verschiedenen Aufgaben, Multi-Task-Lernen Rahmen kann die Leistung erheblich verbessern.Allerdings sind die meisten der bestehenden Arbeiten unter der Annahme, dass die vordefinierten Aufgaben miteinander verwandt sind.So sind ihre Anwendungen auf die reale Welt begrenzt, weil seltene reale Probleme sind eng miteinander verbunden.Außerdem hat das Verständnis der Beziehungen zwischen den Aufgaben von den meisten der aktuellen Methoden ignoriert worden. Um die Effektivität unseres Modells und die Wichtigkeit der Berücksichtigung von mehrstufigen Abhängigkeitsbeziehungen zu zeigen, führen wir Experimente mit verschiedenen öffentlichen Datensätzen durch, bei denen wir signifikante Verbesserungen gegenüber den aktuellen Methoden erzielen.
 Wir entwerfen eine einfache und quantifizierbare Prüfung der globalen Übersetzungsinvarianz in Deep-Learning-Modellen, die auf dem MNIST-Datensatz trainiert wurden. Experimente mit Faltungsnetzwerken und neuronalen Kapseln zeigen, dass beide Modelle eine schlechte Leistung im Umgang mit globaler Übersetzungsinvarianz haben; die Leistung verbesserte sich jedoch durch die Verwendung von Datenvergrößerung.Obwohl das Kapselnetzwerk auf dem MNIST-Testdatensatz besser ist, hat das Faltungsnetzwerk generell eine bessere Leistung bei der Übersetzungsinvarianz.
Gauß'sche Prozesse sind in der Natur und in der Technik allgegenwärtig, z.B. in einer Klasse von neuronalen Netzen mit unendlicher Breite, deren Prioren Gauß'schen Prozessen entsprechen, und wir erweitern diese Korrespondenz perturbativ auf neuronale Netze mit endlicher Breite, wodurch sich nicht-gauß'sche Prozesse als Prioren ergeben. Die hier entwickelte Methodik ermöglicht es uns, den Fluss der Voraktivierungsverteilungen zu verfolgen, indem wir Zufallsvariablen von niedrigeren zu höheren Schichten integrieren, was an den Fluss der Renormierungsgruppe erinnert, und wir entwickeln eine perturbative Vorschrift, um Bayes'sche Inferenz mit schwach nicht-gaußschen Prioren durchzuführen.
Destillation ist eine Methode, um Wissen von einem Modell auf ein anderes zu übertragen und erreicht oft eine höhere Genauigkeit bei gleicher Kapazität.In diesem Beitrag versuchen wir, ein theoretisches Verständnis dafür zu schaffen, was hauptsächlich bei der Destillation hilft.Unsere Antwort ist "frühes Stoppen".Unter der Annahme, dass das Lehrernetzwerk überparametrisiert ist, argumentieren wir, dass das Lehrernetzwerk im Wesentlichen dunkles Wissen aus den Daten durch frühes Stoppen erntet. Dies kann durch ein neues Konzept gerechtfertigt werden, Anisotropic In- formation Retrieval (AIR), was bedeutet, dass das neuronale Netz dazu neigt, die informativen Informationen zuerst und die nicht-informativen Informationen (einschließlich Rauschen) später anzupassen.Motiviert durch die jüngsten Entwicklungen bei der theoretischen Analyse von überparametrisierten neuronalen Netzen, können wir AIR durch den Eigenraum des Neural Tangent Kernel (NTK) charakterisieren.AIR ermöglicht ein neues Verständnis der Destillation. Wir schlagen einen Algorithmus zur Selbstdestillation vor, der das Wissen aus dem Netz in der vorherigen Trainingsepoche sequentiell destilliert, um zu vermeiden, dass falsche Bezeichnungen gespeichert werden. Theoretisch beweisen wir die Konvergenz des vorgeschlagenen Algorithmus zu den Grundwahrheits-Etiketten für zufällig initialisierte überparametrisierte neuronale Netze in Bezug auf die L2-Distanz, während das frühere Ergebnis auf Konvergenz in 0-1 Verlust war.Das theoretische Ergebnis stellt sicher, dass das gelernte neuronale Netz eine Marge auf die Trainingsdaten genießt, was zu einer besseren Verallgemeinerung führt.Empirisch erreichen wir eine bessere Testgenauigkeit und vermeiden vollständig das frühe Stoppen, was den Algorithmus benutzerfreundlicher macht.
Lebenslanges Lernen stellt erhebliche Herausforderungen an die Effektivität (Minimierung von Vorhersagefehlern für alle Aufgaben) und die allgemeine rechnerische Überschaubarkeit für Echtzeitleistungen.  Dieses Papier befasst sich mit kontinuierlichem, lebenslangem Multitask-Lernen, indem es die Beziehungen zwischen den Aufgaben (\textit{output} kernel) und die Parameter des Modells pro Aufgabe in jeder Runde gemeinsam neu bewertet, wobei davon ausgegangen wird, dass die Daten in einem Streaming-Verfahren ankommen.\textit{Online Output Kernel Learning Algorithm} (Um die Speicherexplosion zu vermeiden, schlagen wir eine robuste budgetbegrenzte Version des vorgeschlagenen Algorithmus vor, die die Beziehung zwischen den Aufgaben effizient nutzt, um die Gesamtzahl der repräsentativen Beispiele in der Unterstützungsmenge zu begrenzen.  Darüber hinaus schlagen wir ein zweistufiges budgetiertes Schema vor, um die aufgabenspezifischen Budgetbeschränkungen beim lebenslangen Lernen effizient zu bewältigen.
Minecraft ist ein Videospiel, das viele interessante Herausforderungen für KI-Systeme bietet.In diesem Beitrag konzentrieren wir uns auf Konstruktionsszenarien, in denen ein Agent ein komplexes Bauwerk aus einzelnen Blöcken errichten muss.Da übergeordnete Objekte aus untergeordneten Objekten gebildet werden, kann die Konstruktion natürlich als hierarchisches Aufgabennetzwerk modelliert werden.Wir modellieren ein Hausbauszenario in klassischer und HTN-Planung und vergleichen die Vor- und Nachteile beider Arten von Modellen.
Wir stellen eine Taxonomie von Einschränkungen vor, um diese Angriffe zu kategorisieren, und präsentieren für jede Einschränkung einen realen Anwendungsfall und eine Methode, um zu messen, wie gut die generierten Beispiele die Einschränkung durchsetzen. Diese Angriffe behaupten, dass ihre gegnerischen Störungen die Semantik und syntaktische Korrektheit der Eingaben bewahren, aber unsere Analyse zeigt, dass diese Beschränkungen nicht stark durchgesetzt werden. Unsere Analyse zeigt jedoch, dass diese Beschränkungen nicht streng durchgesetzt werden. Bei einem erheblichen Teil dieser gegnerischen Beispiele stellt ein Grammatikprüfprogramm einen Anstieg der Fehler fest, und menschliche Studien zeigen, dass viele dieser gegnerischen Beispiele in ihrer semantischen Bedeutung von der Eingabe abweichen oder nicht von Menschen geschrieben zu sein scheinen.
Wenn ein Algorithmus beispielsweise ein bestimmtes Pathologiebild als bösartigen Tumor einstuft, muss der Arzt wissen, welche Teile des Bildes den Algorithmus zu dieser Einstufung veranlasst haben.Die Interpretation von Blackbox-Prädiktoren ist daher ein wichtiger und aktiver Forschungsbereich.  In diesem Beitrag zeigen wir, dass die Interpretation von Deep-Learning-Vorhersagen in folgendem Sinne extrem fragil ist: Zwei wahrnehmungsmäßig nicht unterscheidbaren Eingaben mit demselben vorhergesagten Label können sehr unterschiedliche}Interpretationen zugewiesen werden.Wir charakterisieren systematisch die Fragilität der Interpretationen, die von mehreren weit verbreiteten Interpretationsmethoden für die Bedeutung von Merkmalen (Saliency Maps, Integrated Gradient und DeepLIFT) auf ImageNet und CIFAR-10 erzeugt werden. Unsere Experimente zeigen, dass selbst kleine zufällige Störungen die Merkmalsbedeutung verändern können und neue systematische Störungen zu dramatisch unterschiedlichen Interpretationen führen können, ohne das Label zu verändern.Wir erweitern diese Ergebnisse, um zu zeigen, dass Interpretationen, die auf Exemplaren basieren (z.B. Einflussfunktionen), ähnlich fragil sind.Unsere Analyse der Geometrie der Hessian-Matrix gibt einen Einblick, warum Fragilität eine grundlegende Herausforderung für die aktuellen Interpretationsansätze sein könnte.
Die stochastische AUC-Maximierung hat aufgrund der besseren Anpassung an die Klassifizierung von unausgewogenen Daten zunehmendes Interesse geweckt, aber die vorhandenen Arbeiten beschränken sich auf die stochastische AUC-Maximierung mit einem linearen Vorhersagemodell, was die Vorhersagekraft bei extrem komplexen Daten einschränkt.In dieser Arbeit betrachten wir das Problem der stochastischen AUC-Maximierung mit einem tiefen neuronalen Netz als Vorhersagemodell.Auf der Grundlage der Sattelpunktformulierung eines Ersatzverlustes der AUC kann das Problem in ein nicht-konvexes konkaves Min-Max-Problem umgewandelt werden. Insbesondere schlagen wir vor, die Polyak-\L{}ojasiewicz (PL)-Bedingung zu erforschen, die im tiefen Lernen bewiesen und beobachtet wurde, was uns ermöglicht, neue stochastische Algorithmen mit noch schnellerer Konvergenzrate und praktischerem Schrittgrößenschema zu entwickeln.Ein AdaGrad-ähnlicher Algorithmus wird auch unter der PL-Bedingung mit adaptiver Konvergenzrate analysiert.Unsere experimentellen Ergebnisse zeigen die Wirksamkeit der vorgeschlagenen Algorithmen.
Letzteres ist besonders problematisch bei der Anwendung von Reinforcement Learning (RL) in der Robotik, wo das Erkennen, ob die gewünschte Konfiguration erreicht wurde, eine beträchtliche Überwachung und Instrumentierung erfordern kann.Darüber hinaus sind wir oft daran interessiert, eine breite Palette von Konfigurationen zu erreichen, so dass es unpraktisch sein kann, jedes Mal eine andere Belohnung festzulegen.Methoden wie Hindsight Experience Replay (HER) haben kürzlich gezeigt, dass sie vielversprechend sind, um Strategien zu lernen, die viele Ziele erreichen können, ohne eine Belohnung zu benötigen. In dieser Arbeit untersuchen wir verschiedene Ansätze zur Einbeziehung von Demonstrationen, um die Konvergenz zu einer Strategie, mit der jedes Ziel erreicht werden kann, drastisch zu beschleunigen und die Leistung eines Agenten, der mit anderen Algorithmen des Nachahmungslernens trainiert wurde, zu übertreffen.Darüber hinaus kann unsere Methode verwendet werden, wenn nur Trajektorien ohne Expertenaktionen verfügbar sind, was kinästhetische oder Drittperson-Demonstrationen nutzen kann.
Bayes'sche neuronale Netze, die sowohl die negative Log-Likelihood-Verlustfunktion verwenden als auch ihre Vorhersagen mit Hilfe eines gelernten Posteriors über die Parameter mitteln, sind in vielen wissenschaftlichen Bereichen erfolgreich eingesetzt worden, was zum Teil auf ihre Fähigkeit zurückzuführen ist, gewünschte Darstellungen aus vielen großen Datensätzen "mühelos" zu extrahieren. In diesem Beitrag stellen wir eine neue PAC-Bayessche Verallgemeinerungsschranke für den negativen Log-Likelihood-Verlust vor, die das \emph{Herbst-Argument} für die log-Sobolev-Ungleichung verwendet, um die momentgenerierende Funktion des Lernerrisikos zu begrenzen.
Datenerweiterungstechniken, z. B. Flipping oder Cropping, die den Trainingsdatensatz systematisch vergrößern, indem sie explizit mehr Trainingsmuster erzeugen, sind wirksam bei der Verbesserung der Generalisierungsleistung von tiefen neuronalen Netzen.In der überwachten Umgebung besteht eine gängige Praxis für die Datenerweiterung darin, allen erweiterten Mustern derselben Quelle dasselbe Label zuzuweisen, Um diese Herausforderung zu bewältigen, schlagen wir eine einfache, aber effektive Idee des Lernens der gemeinsamen Verteilung der ursprünglichen und selbstüberwachten Beschriftungen der augmentierten Stichproben vor, die einfacher zu trainieren ist und eine aggregierte Inferenz ermöglicht, die die Vorhersagen von verschiedenen augmentierten Stichproben kombiniert, um die Leistung zu verbessern. Um den Aggregationsprozess zu beschleunigen, schlagen wir außerdem eine Wissenstransfertechnik vor, die das Wissen der Augmentierung in das Modell selbst überträgt, und demonstrieren die Effektivität unseres Daten-Augmentierungs-Frameworks in verschiedenen vollständig überwachten Einstellungen, einschließlich der Szenarien mit wenigen Aufnahmen und unausgeglichener Klassifizierung.
Netzwerke mit Langzeitgedächtnis (LSTM) ermöglichen ein zeitlich dynamisches Verhalten mit Rückkopplungsverbindungen und scheinen eine natürliche Wahl für das Lernen von Sequenzen von 3D-Netzen zu sein.Wir stellen einen Ansatz für dynamische Netzrepräsentationen vor, wie sie für numerische Simulationen von Autounfällen verwendet werden. Um die Komplikationen bei der Verwendung von 3D-Maschen zu umgehen, transformieren wir die Sequenzen der Oberflächenmaschen in spektrale Deskriptoren, die die Form effizient kodieren.Eine auf zwei Zweigen basierende LSTM-Netzwerkarchitektur wird gewählt, um die Repräsentationen und die Dynamik des Unfalls während der Simulation zu erlernen.Die Architektur basiert auf einer unbeaufsichtigten Videovorhersage durch ein LSTM ohne eine Faltungsschicht. Auf dieser Repräsentation führt ein Decoder-LSTM die Rekonstruktion der Eingabesequenz durch, während das andere Decoder-LSTM das zukünftige Verhalten vorhersagt, indem es die ersten Schritte der Sequenz als Seed erhält. Das räumlich-zeitliche Fehlerverhalten des Modells wird analysiert, um zu untersuchen, wie gut das Modell die gelernten spektralen Deskriptoren in die Zukunft extrapolieren kann, d.h. wie gut es gelernt hat, die zugrundeliegende dynamische Strukturmechanik zu repräsentieren.In Anbetracht der Tatsache, dass nur wenige Trainingsbeispiele zur Verfügung stehen, was der typische Fall für numerische Simulationen ist, schneidet das Netzwerk sehr gut ab.
In diesem Beitrag versuchen wir, ein Ganzhirn-Kodierungsmodell für die auditive Wahrnehmung in einer naturalistischen Stimulationsumgebung zu schätzen. Wir analysieren Daten aus einem offenen Datensatz, in dem 16 Probanden einen kurzen Film ansahen, während ihre Gehirnaktivität mit funktioneller MRT gemessen wurde. Wir extrahierten Merkmalsvektoren, die mit dem Timing des Tons aus dem Film übereinstimmten, auf verschiedenen Schichten eines Deep Neural Network, das auf die Klassifizierung auditiver Szenen trainiert wurde. Die fMRI-Daten wurden mithilfe von hierarchischem Clustering auf 500 Pakete parzelliert, und die Kodierungsmodelle wurden mithilfe eines vollständig verbundenen neuronalen Netzwerks mit einer versteckten Schicht geschätzt, das darauf trainiert wurde, die Signale für jedes Paket aus den DNN-Merkmalen vorherzusagen. Einzelne Kodierungsmodelle wurden erfolgreich trainiert und sagten die Hirnaktivität bei ungesehenen Daten in Parzellen im oberen Temporallappen sowie in dorsolateralen präfrontalen Regionen voraus, die üblicherweise als an der auditorischen und sprachlichen Verarbeitung beteiligte Bereiche angesehen werden.Insgesamt erweitert dieser Beitrag frühere Versuche zur Schätzung von Kodierungsmodellen, indem er zeigt, dass die Hirnaktivität mit einem generischen DNN (d. h. nicht speziell für diesen Zweck trainiert) modelliert werden kann, um auditorische Merkmale zu extrahieren, was auf eine gewisse Ähnlichkeit zwischen internen DNN-Darstellungen und der Hirnaktivität in natürlichen Umgebungen hindeutet.
In diesem Beitrag beschreiben wir den "impliziten Autoencoder" (IAE), einen generativen Autoencoder, bei dem sowohl der generative Pfad als auch der Erkennungspfad durch implizite Verteilungen parametrisiert sind. wir verwenden zwei generative adversarische Netzwerke, um die Rekonstruktions- und Regularisierungskostenfunktionen des impliziten Autoencoders zu definieren, und leiten die Lernregeln auf der Grundlage des Maximum-Likelihood-Lernens ab. die Verwendung impliziter Verteilungen ermöglicht es uns, aussagekräftigere posteriore und bedingte Likelihood-Verteilungen für den Autoencoder zu lernen. Wir zeigen zum Beispiel, dass implizite Autokodierer die globalen und lokalen Informationen entflechten und deterministische oder stochastische Rekonstruktionen der Bilder durchführen können. Wir zeigen außerdem, dass implizite Autokodierer diskrete zugrundeliegende Variationsfaktoren von den kontinuierlichen Faktoren in einer unbeaufsichtigten Weise entflechten und Clustering und halb-überwachtes Lernen durchführen können.
Kinder nutzen die gegenseitige Ausschließlichkeit (ME), um die Zuordnung von Wörtern zu Begriffen zu klären, wobei sie davon ausgehen, dass ein Objekt, das eine Bezeichnung hat, keine andere braucht. in diesem Beitrag untersuchen wir, ob Standard-Neuralarchitekturen eine ME-Vorspannung haben, und zeigen, dass ihnen diese Lernannahme fehlt. Darüber hinaus zeigen wir, dass ihre induktiven Verzerrungen schlecht zu lebenslangen Lernformulierungen der Klassifizierung und Übersetzung passen. Wir zeigen, dass es zwingende Gründe gibt, neuronale Netzwerke zu entwerfen, die durch gegenseitige Ausschließlichkeit argumentieren, was eine offene Herausforderung bleibt.
Kortikale Neuronen verarbeiten und integrieren Informationen auf mehreren Zeitskalen, und diese Zeitskalen bzw. zeitlichen rezeptiven Felder sind funktional und hierarchisch organisiert, z. B. nutzen für das Arbeitsgedächtnis wichtige Bereiche wie der präfrontale Kortex Neuronen mit stabilen zeitlichen rezeptiven Feldern und langen Zeitskalen, um zuverlässige Repräsentationen von Reizen zu unterstützen. Trotz der jüngsten Fortschritte bei den experimentellen Techniken sind die zugrundeliegenden Mechanismen für die Entstehung neuronaler Zeitskalen, die lang genug sind, um das WM zu unterstützen, unklar und schwierig experimentell zu untersuchen. hier zeigen wir, dass spikende rekurrente neuronale Netze (RNNs), die für die Durchführung einer WM-Aufgabe entwickelt wurden, zuvor beobachtete experimentelle Ergebnisse reproduzieren und dass diese Modelle in Zukunft genutzt werden könnten, um zu untersuchen, wie neuronale Zeitskalen, die spezifisch für das WM sind, entstehen.
Herkömmliche Deep-Learning-Klassifikatoren sind statisch in dem Sinne, dass sie auf einem vordefinierten Satz von Klassen trainiert werden und das Erlernen der Klassifizierung einer neuen Klasse typischerweise ein erneutes Training erfordert.In dieser Arbeit befassen wir uns mit dem Problem der Low-Shot-Netzwerk-Expansionlearning. Wir stellen einen Lernrahmen vor, der es ermöglicht, ein vortrainiertes (Basis-) tiefes Netzwerk zu erweitern, um neue Klassen zu klassifizieren, wenn die Anzahl der Beispiele für die neuen Klassen besonders gering ist.Wir präsentieren eine einfache, aber leistungsstarke Destillationsmethode, bei der das Basisnetzwerk mit zusätzlichen Gewichten erweitert wird, um die neuen Klassen zu klassifizieren, während die Gewichte des Basisnetzwerks unverändert bleiben. Wir bezeichnen dieses Lernverfahren als harte Destillation, da wir die Reaktion des Netzes auf die alten Klassen sowohl im Basisnetz als auch im erweiterten Netz gleich halten.Wir zeigen, dass sich die harte Destillation für Trainingsszenarien mit geringem Umfang eignet, da nur eine kleine Anzahl von Gewichten trainiert werden muss. Schließlich zeigen wir, dass die Erweiterung eines Low-Shot-Netzes mit einem sehr geringen Speicherbedarf durchgeführt werden kann, indem ein kompaktes generatives Modell der Trainingsdaten der Basisklassen verwendet wird - mit einer vernachlässigbaren Verschlechterung im Vergleich zum Lernen mit dem vollständigen Trainingssatz.
Menschen stellen Fragen, die viel reicher, informativer und kreativer sind als aktuelle KI-Systeme.Wir schlagen ein neuronales Programm Generation Rahmen für die Modellierung der menschlichen Fragen, die Fragen als formale Programme darstellt und generiert Programme mit einem Encoder-Decoder-basierte tiefe neuronale network.From umfangreiche Experimente mit einer Informationssuche Spiel, zeigen wir, dass unsere Methode kann optimale Fragen in synthetischen Einstellungen zu stellen, und vorhersagen, welche Fragen Menschen sind wahrscheinlich in uneingeschränkten settings.We auch eine neuartige Grammatik-basierte Frage Generation Rahmen mit Verstärkung Lernen trainiert, die in der Lage, kreative Fragen ohne überwachte Daten zu generieren ist.
Die Klassifizierung von Bildern, die in speziellen Bildgebungsumgebungen außer Luft aufgenommen wurden, ist die erste Herausforderung bei der Erweiterung der Anwendungen von Deep Learning.Wir berichten über ein UW-Net (Underwater Network), ein neues Faltungsneuronales Netzwerk (CNN), das auf einem Netzwerk für die Klassifizierung von Unterwasserbildern basiert.In diesem Modell simulieren wir die visuelle Korrelation der Hintergrundaufmerksamkeit mit dem Bildverständnis für spezielle Umgebungen wie Nebel und Unterwasser, indem wir ein Inception-Attention (I-A) Modul konstruieren.Die experimentellen Ergebnisse zeigen, dass das vorgeschlagene UW-Net eine Genauigkeit von 99. Die experimentellen Ergebnisse zeigen, dass das vorgeschlagene UW-Netz eine Genauigkeit von 99,3% bei der Klassifizierung von Unterwasserbildern erreicht, was deutlich besser ist als andere Bildklassifizierungsnetze wie AlexNet, InceptionV3, ResNet und Se-ResNet.Darüber hinaus zeigen wir, dass das vorgeschlagene IA-Modul verwendet werden kann, um die Leistung der bestehenden Objekterkennungsnetze zu steigern.Durch die Ersetzung des Inception-Moduls durch das I-A-Modul erreicht das Inception-ResnetV2-Netz eine 10,7%ige Top1-Fehlerrate und eine 0%ige Top5-Fehlerrate bei der Teilmenge von ILSVRC-2012, was die Funktion der Hintergrundaufmerksamkeit bei der Bildklassifizierung weiter verdeutlicht.
Eine bekannte Schwäche ist jedoch, dass sie nicht gut abschneiden, wenn sie auf Daten ausgewertet werden, die sich von der Trainingsverteilung unterscheiden, selbst wenn diese Unterschiede sehr gering sind, wie es bei gegnerischen Beispielen der Fall ist.  Wir schlagen \emph{Fortified Networks} vor, eine einfache Transformation bestehender Netzwerke, die die versteckten Schichten in einem tiefen Netzwerk "verstärkt", indem sie identifiziert, wann die versteckten Zustände außerhalb der Datenvielfalt liegen, und diese versteckten Zustände auf Teile der Datenvielfalt zurückführt, in denen das Netzwerk gute Leistungen erbringt. Unser Hauptbeitrag besteht darin zu zeigen, dass die Verstärkung dieser versteckten Zustände die Robustheit von tiefen Netzwerken verbessert. Unsere Experimente (i) zeigen eine verbesserte Robustheit gegenüber Standardangriffen sowohl in Black-Box- als auch in White-Box-Bedrohungsmodellen; (ii) deuten darauf hin, dass unsere Verbesserungen nicht in erster Linie auf das Problem der trügerisch guten Ergebnisse aufgrund der verschlechterten Qualität des Gradientensignals (das Gradientenmaskierungsproblem) zurückzuführen sind, und (iii) zeigen den Vorteil, diese Verstärkung in den versteckten Schichten statt im Eingaberaum durchzuführen.  Wir demonstrieren Verbesserungen in der Robustheit gegen Angriffe auf drei Datensätze (MNIST, Fashion MNIST, CIFAR10), über mehrere Angriffsparameter, sowohl White-Box- als auch Black-Box-Einstellungen und die am häufigsten untersuchten Angriffe (FGSM, PGD, Carlini-Wagner).  Wir zeigen, dass diese Verbesserungen bei einer Vielzahl von Hyperparametern erreicht werden.  
Neuronale Netze können Eingaben falsch klassifizieren, die sich geringfügig von ihren Trainingsdaten unterscheiden, was auf einen kleinen Abstand zwischen ihren Entscheidungsgrenzen und dem Trainingsdatensatz hindeutet. In dieser Arbeit untersuchen wir die binäre Klassifizierung von linear trennbaren Datensätzen und zeigen, dass lineare Klassifizierer auch Entscheidungsgrenzen haben können, die nahe an ihrem Trainingsdatensatz liegen, wenn der Kreuzentropieverlust für das Training verwendet wird. Insbesondere zeigen wir, dass, wenn die Merkmale des Trainingsdatensatzes in einem niedrigdimensionalen affinen Unterraum liegen und der Cross-Entropie-Verlust mit Hilfe einer Gradientenmethode minimiert wird, der Abstand zwischen den Trainingspunkten und der Entscheidungsgrenze viel kleiner als der optimale Wert sein könnte.Dieses Ergebnis steht im Gegensatz zu den Schlussfolgerungen neuerer verwandter Arbeiten wie (Soudry et al, Wir zeigen, dass die Entscheidungsgrenze eines linearen Klassifizierers, der mit differentiellem Training trainiert wurde, in der Tat die maximale Marge erreicht.Die Ergebnisse enthüllen die Verwendung von Cross-Entropie-Verlusten als einen der versteckten Übeltäter von ungünstigen Beispielen und führen eine neue Richtung ein, um neuronale Netze robust gegen sie zu machen.
Die Konzepte der unitären Evolutionsmatrizen und des Assoziativspeichers haben den Bereich der rekurrenten neuronalen Netze (RNN) auf den neuesten Stand der Technik in einer Vielzahl von sequentiellen Aufgaben gebracht.  Allerdings sind RNN nach wie vor nur begrenzt in der Lage, das Langzeitgedächtnis zu manipulieren.  Um diese Schwäche zu umgehen, verwenden die erfolgreichsten Anwendungen von RNN externe Techniken wie z.B. Aufmerksamkeitsmechanismen.In diesem Beitrag schlagen wir ein neuartiges RNN-Modell vor, das die modernsten Ansätze vereint: Das Herzstück von RUM ist die Rotationsoperation, die natürlich eine unitäre Matrix ist und Architekturen mit der Fähigkeit ausstattet, langfristige Abhängigkeiten zu lernen, indem sie das Problem der verschwindenden und explodierenden Gradienten überwindet.  Darüber hinaus dient die Rotationseinheit auch als Assoziativspeicher. Wir evaluieren unser Modell anhand von synthetischen Gedächtnis-, Fragebeantwortungs- und Sprachmodellierungsaufgaben.   RUM lernt die Aufgabe "Kopieren des Gedächtnisses" vollständig und verbessert das State-of-the-Art-Ergebnis bei der Aufgabe "Abrufen".  Wir verbessern auch den Stand der Technik Ergebnis auf 1,189 Bits pro Zeichen (BPC) Verlust in der Character Level Penn Treebank (PTB) Aufgabe, die die Anwendungen von RUM auf realen sequenziellen Daten zu bedeuten hat.Die Universalität unserer Konstruktion, im Kern der RNN, etabliert RUM als ein vielversprechender Ansatz zur Sprachmodellierung, Spracherkennung und maschinelle Übersetzung.
Während sich viele der jüngsten Fortschritte im Bereich des Deep Reinforcement Learning auf modellfreie Methoden stützen, bleiben modellbasierte Ansätze aufgrund ihres Potenzials, unbeaufsichtigte Daten zum Erlernen der Umgebungsdynamik zu nutzen, eine verlockende Perspektive. Eine Perspektive besteht darin, hybride Ansätze zu verfolgen, wie bei AlphaGo, das Monte-Carlo Tree Search (MCTS) - eine modellbasierte Methode - mit Deep-Q-Netzwerken (DQNs) - einer modellfreien Methode - kombiniert. In diesem Papier schlagen wir vor, Rollouts zu simulieren, indem wir die neuesten Durchbrüche in der Bild-zu-Bild-Transduktion, nämlich Pix2Pix GANs, nutzen, um die Dynamik der Umgebung vorherzusagen. Unser vorgeschlagener Algorithmus, generative adversarische Baumsuche (GATS), simuliert Rollouts bis zu einer bestimmten Tiefe, indem er sowohl ein GAN-basiertes Dynamikmodell als auch einen Belohnungsprädiktor verwendet. Unsere theoretische Analyse zeigt einige günstige Eigenschaften von GATS im Hinblick auf den Kompromiss zwischen Verzerrung und Varianz, und die empirischen Ergebnisse zeigen, dass bei fünf beliebten Atari-Spielen die Dynamik- und Belohnungsprädiktoren schnell zu genauen Lösungen konvergieren. Bemerkenswert ist, dass MCTS in diesen Experimenten nur kurze Rollouts hat (bis zu einer Baumtiefe von 4), während frühere Erfolge von MCTS eine Baumtiefe von mehreren hundert haben.
Die neue Modellarchitektur ist von der Information Bottleneck (IB)-Theorie inspiriert und wird daher IB-GAN genannt. Das Ziel von IB-GAN ähnelt dem von InfoGAN, weist aber einen entscheidenden Unterschied auf: Es wird eine Kapazitätsregulierung für die gegenseitige Information angenommen, dank derer der Generator von IB-GAN eine latente Repräsentation in entwirrter und interpretierbarer Weise nutzen kann. Um die Optimierung von IB-GAN in der Praxis zu erleichtern, wird eine neue Variationsobergrenze abgeleitet.Mit Experimenten auf CelebA-, 3DChairs- und dSprites-Datensätzen zeigen wir, dass die visuelle Qualität der von IB-GAN erzeugten Proben oft besser ist als die von β-VAEs.Darüber hinaus erreicht IB-GAN eine viel höhere Entwirrungsmetrik als β-VAEs oder InfoGAN auf dem dSprites-Datensatz.
Wir untersuchen das Training und die Leistung von generativen adversen Netzwerken mit der Maximum Mean Discrepancy (MMD) als Kritiker, bezeichnet als MMD GANs.Als unser wichtigster theoretischer Beitrag klären wir die Situation mit Verzerrungen in GAN-Verlustfunktionen, die durch neuere Arbeiten aufgeworfen wurden: wir zeigen, dass Gradientenschätzer, die im Optimierungsprozess sowohl für MMD GANs als auch für Wasserstein GANs verwendet werden, unverzerrt sind, aber das Lernen eines Diskriminators, der auf Stichproben basiert, führt zu verzerrten Gradienten für die Generatorparameter. Wir erörtern auch die Frage der Wahl des Kernels für den MMD-Kritiker und charakterisieren den Kernel, der der Energiedistanz entspricht, die für den Cramér-GAN-Kritiker verwendet wird. Da der MMD eine integrale Wahrscheinlichkeitsmetrik ist, profitiert er von den Trainingsstrategien, die kürzlich für Wasserstein-GANs entwickelt wurden. In Experimenten ist das MMD GAN in der Lage, ein kleineres kritisches Netzwerk als das Wasserstein GAN zu verwenden, was zu einem einfacheren und schnelleren Trainingsalgorithmus mit übereinstimmender Leistung führt.Wir schlagen auch ein verbessertes Maß für die GAN-Konvergenz vor, die Kernel Inception Distance, und zeigen, wie man es verwendet, um die Lernraten während des GAN-Trainings dynamisch anzupassen.
Wir erweitern das Consensus Network Framework auf das Transductive Consensus Network (TCN), ein semi-supervised multi-modal classification framework, und identifizieren dessen zwei Mechanismen: Konsens und Klassifikation. indem wir drei Varianten als Ablationsstudien vorschlagen, zeigen wir, dass beide Mechanismen zusammen funktionieren sollten. insgesamt übertreffen TCNs die besten Benchmark-Algorithmen oder gleichen sich ihnen an, wenn nur 20 bis 200 markierte Datenpunkte verfügbar sind.
Die Trennung von gemischten Verteilungen ist eine langjährige Herausforderung für das maschinelle Lernen und die Signalverarbeitung.Anwendungen umfassen: Ein-Kanal-Multi-Sprecher-Trennung (Cocktail-Party-Problem), Gesang Stimme Trennung und Trennung von Reflexionen aus Bildern.Die meisten aktuellen Methoden entweder auf die Herstellung von starken Annahmen über die Quelle Verteilungen (zB Sparsity, niedrigen Rang, Wiederholbarkeit) oder auf die Ausbildung Proben von jeder Quelle in der Mischung.In dieser Arbeit, die wir angehen das Szenario der Extrahierung einer unbeobachteten Verteilung additiv gemischt mit einem Signal aus einer beobachteten (arbitrary) distribution.We eine neue Methode vorstellen: Neural Egg Separation - eine iterative Methode, die lernt, die bekannte Verteilung von immer feineren Schätzungen der unbekannten Verteilung zu trennen.In einigen Einstellungen ist Neural Egg Separation initialisierungssensitiv, daher führen wir GLO Masking ein, das eine gute Initialisierung sicherstellt.Umfangreiche Experimente zeigen, dass unsere Methode aktuelle Methoden, die das gleiche Maß an Überwachung verwenden, übertrifft und oft eine ähnliche Leistung wie die vollständige Überwachung erreicht.
Im Gesundheitsbereich wird maschinelles Lernen immer häufiger eingesetzt, doch die Einbettung (Repräsentation) von neuronalen Netzen wird bei physiologischen Signalen wohl zu wenig genutzt.  Diese Unzulänglichkeit steht in krassem Gegensatz zu traditionelleren Bereichen der Informatik, wie z. B. Computer Vision (CV) und Natural Language Processing (NLP).  Bei physiologischen Signalen ist das Erlernen von Merkmalseinbettungen eine natürliche Lösung für den Datenmangel, der durch die Sorge um die Privatsphäre der Patienten verursacht wird - anstatt Daten zu teilen, können Forscher informative Einbettungsmodelle (d. h. Repräsentationsmodelle) teilen, die Patientendaten auf eine Ausgabeeinbettung abbilden.   Hier stellen wir das PHASE (PHysiologicAl Signal Embeddings)-Rahmenwerk vor, das aus drei Komponenten besteht: i) Lernen von Einbettungen physiologischer Signale durch neuronale Netze, ii) Vorhersage von Ergebnissen auf der Grundlage der gelernten Einbettung und iii) Interpretation der Vorhersageergebnisse durch Schätzung von Merkmalszuweisungen in den "gestapelten" Modellen (d. h. Merkmalseinbettungsmodell gefolgt von Vorhersagemodell).  PHASE ist in dreierlei Hinsicht neuartig: 1) Unseres Wissens nach ist PHASE das erste Beispiel für die Übertragung neuronaler Netze zur Erstellung physiologischer Signaleinbettungen. 2) Wir stellen eine nachvollziehbare Methode vor, um Merkmalszuweisungen durch gestapelte Modelle zu erhalten.  Wir beweisen, dass unsere gestapelten Modell-Attributionen Shapley-Werte annähern können - Attributionen, von denen bekannt ist, dass sie wünschenswerte Eigenschaften haben - für beliebige Sätze von Modellen.3) PHASE wurde ausgiebig in einer krankenhausübergreifenden Umgebung mit öffentlich verfügbaren Daten getestet.  In unseren Experimenten zeigen wir, dass PHASE alternative Einbettungen - wie rohe, exponentiell gleitende Mittelwerte/Varianzen und Autoencoder - deutlich übertrifft, die derzeit verwendet werden, und dass die Übertragung von Lernern zur Einbettung/Repräsentation neuronaler Netze zwischen verschiedenen Krankenhäusern immer noch leistungsfähige Einbettungen ergibt.
Wir betrachten das Wörterbuch-Lernproblem, bei dem das Ziel darin besteht, die gegebenen Daten als lineare Kombination einiger Spalten einer Matrix zu modellieren, die als Wörterbuch bekannt ist, wobei die spärlichen Gewichte, die die lineare Kombination bilden, als Koeffizienten bekannt sind.Da das Wörterbuch und die Koeffizienten, die das lineare Modell parametrisieren, unbekannt sind, ist die entsprechende Optimierung von Natur aus nicht konvex. Dies war bis vor kurzem eine große Herausforderung, als beweisbare Algorithmen für das Lernen von Wörterbüchern vorgeschlagen wurden, die jedoch nur Garantien für die Wiederherstellung des Wörterbuchs bieten, ohne explizite Wiederherstellungsgarantien für die Koeffizienten. Zu diesem Zweck entwickeln wir NOODL: einen einfachen, neuronal plausiblen, auf alternierender Optimierung basierenden Online-Wörterbuch-Lernalgorithmus, der sowohl das Wörterbuch als auch die Koeffizienten mit einer geometrischen Rate wiederherstellt, wenn er entsprechend initialisiert wird. Unser Algorithmus, NOODL, ist auch skalierbar und eignet sich für groß angelegte verteilte Implementierungen in neuronalen Architekturen, was bedeutet, dass er nur einfache lineare und nicht-lineare Operationen beinhaltet.
In dieser Arbeit stellen wir die Data Augmented Relation Extraction (DARE) vor, eine einfache Methode zur Erweiterung der Trainingsdaten durch eine entsprechende Feinabstimmung von GPT2, um Beispiele für bestimmte Beziehungstypen zu erzeugen. In einer Reihe von Experimenten zeigen wir die Vorteile unserer Methode, die zu Verbesserungen von bis zu 11 F1-Punkten im Vergleich zu einer starken Basislinie führt. DARE erreicht außerdem neue Spitzenwerte in drei weit verbreiteten biomedizinischen RE-Datensätzen und übertrifft die bisher besten Ergebnisse um durchschnittlich 4,7 F1-Punkte.
Dieses Papier befasst sich mit dem Problem der Darstellung des Glaubens eines Systems unter Verwendung von multivariaten Normalverteilungen (MND), wenn das zugrunde liegende Modell auf einem tiefen neuronalen Netzwerk (DNN) basiert.Die größte Herausforderung bei DNNs ist die Rechenkomplexität, die erforderlich ist, um die Modellunsicherheit unter Verwendung von MNDs zu erhalten.Um eine skalierbare Methode zu erreichen, schlagen wir einen neuartigen Ansatz vor, der das Parameter-Posterior in Form von spärlichen Informationen ausdrückt. Unser Inferenzalgorithmus basiert auf einem neuartigen Laplace-Approximationsschema, das eine diagonale Korrektur der Kronecker-faktorisierten Eigenbasis beinhaltet. Da dies die Inversion der Informationsmatrix unpraktikabel macht - eine Operation, die für eine vollständige Bayes'sche Analyse erforderlich ist - entwickeln wir eine Low-Rank-Approximation dieser Eigenbasis und ein speichereffizientes Abtastschema. Wir liefern sowohl eine theoretische Analyse als auch eine empirische Bewertung auf verschiedenen Benchmark-Datensätzen, die die Überlegenheit unseres Ansatzes gegenüber bestehenden Methoden zeigt.
Im Vergleich zum überwachten Lernen besteht die Hauptschwierigkeit beim halbüberwachten Lernen darin, die vielfältigen Informationen, die von den unbeschrifteten Daten geliefert werden, voll auszunutzen, indem wir eine neuartige Regularisierung vorschlagen, die tangential-normale adversarische Regularisierung, die aus zwei Teilen besteht. Die beiden Teile ergänzen sich und erzwingen gemeinsam die Glätte entlang zweier verschiedener Richtungen, die für das semi-supervised Lernen entscheidend sind: Einer wird entlang des Tangentenraums der Datenvielfalt angewendet, um die lokale Invarianz des Klassifikators auf der Vielfältigkeit zu erzwingen, während der andere auf dem normalen Raum orthogonal zum Tangentenraum durchgeführt wird, um dem Klassifikator Robustheit gegenüber dem Rauschen aufzuerlegen, das die beobachteten Daten von der zugrunde liegenden Datenvielfalt abweichen lässt.  Beide Regularisierer werden durch die Strategie des virtuellen gegnerischen Trainings erreicht. Unsere Methode hat bei halbüberwachten Lernaufgaben sowohl bei künstlichen als auch bei praktischen Datensätzen Spitzenleistungen erzielt.
Diese Eigenschaft ist jedoch nicht das einzige Merkmal, das neuronale Netze einzigartig macht, da es eine Vielzahl anderer Ansätze mit ähnlichen Eigenschaften gibt. Ein weiteres Merkmal, das diese Modelle interessant macht, ist, dass sie mit dem Backpropagation-Algorithmus trainiert werden können, der eine effiziente Gradientenberechnung ermöglicht und diesen universellen Approximatoren die Fähigkeit verleiht, komplexe Mannigfaltigkeiten aus einer großen Menge von Daten in verschiedenen Domänen effizient zu lernen. Trotz ihres häufigen Einsatzes in der Praxis werden neuronale Netze noch immer nicht gut verstanden, und ein breites Spektrum laufender Forschungsarbeiten zielt darauf ab, die Interpretierbarkeit neuronaler Netze zu untersuchen. In dieser Arbeit nutzen wir ein universelles Approximationstheorem aus der algebraischen Topologie, um eine Verbindung zwischen der TDA und dem üblichen Trainingsrahmen für neuronale Netze herzustellen, indem wir den Begriff der automatischen Unterteilung einführen und eine besondere Art von neuronalen Netzen für Regressionsaufgaben entwickeln: Simplicial Complex Networks (SCNs).SCN's Architektur ist mit einer Reihe von Bias-Funktionen zusammen mit einer bestimmten Politik während der Vorwärts-Pass, der die gemeinsame Architektur Suchrahmen in neuronalen Netzen alterniert definiert.Wir glauben, dass die Ansicht der SCNs kann als ein Schritt in Richtung Gebäude interpretierbar tiefes Lernen models.Finally verwendet werden, überprüfen wir ihre Leistung auf eine Reihe von Regression Probleme.
Das Ziel des Lernens von Netzwerkrepräsentationen ist es, niedrigdimensionale Knoteneinbettungen zu erlernen, die die Struktur des Graphen erfassen und für die Lösung nachgelagerter Aufgaben nützlich sind, aber trotz der Verbreitung solcher Methoden gibt es derzeit keine Studie über ihre Robustheit gegenüber nachteiligen Angriffen. Wir zeigen, dass unsere Angriffe übertragbar sind, da sie sich auf viele Modelle verallgemeinern lassen und auch dann erfolgreich sind, wenn der Angreifer eingeschränkt ist.
Hierarchische Reinforcement-Learning-Methoden bieten ein leistungsfähiges Mittel, um flexibles Verhalten in komplizierten Domänen zu planen.Allerdings bleibt das Erlernen einer geeigneten hierarchischen Zerlegung einer Domäne in Teilaufgaben eine große Herausforderung.Wir stellen einen neuartigen Algorithmus für die Entdeckung von Teilaufgaben vor, der auf dem kürzlich eingeführten Multitask-linear-lösbaren Markov-Entscheidungsprozess (MLMDP) Framework basiert. Der MLMDP kann nie zuvor gesehene Aufgaben als Linearkombination einer zuvor gelernten Basismenge von Aufgaben darstellen, so dass das Problem der Entdeckung von Teilaufgaben auf natürliche Weise als eine optimale Annäherung mit niedrigem Rang an die Menge der Aufgaben, mit denen der Agent in einer Domäne konfrontiert wird, gestellt werden kann. Unsere Methode hat mehrere qualitativ wünschenswerte Eigenschaften: sie ist nicht auf das Lernen von Teilaufgaben mit einzelnen Zielzuständen beschränkt, sondern lernt stattdessen verteilte Muster bevorzugter Zustände; sie lernt qualitativ unterschiedliche hierarchische Zerlegungen in ein und derselben Domäne, abhängig von der Gesamtheit der Aufgaben, mit denen der Agent konfrontiert wird; und sie kann einfach iteriert werden, um tiefere hierarchische Zerlegungen zu erhalten.
Wir führen ein neues speichererweitertes neuronales Modell ein, bei dem der Speicher nicht rücksetzbar ist (d.h. die Informationen, die nach der Verarbeitung eines Eingabebeispiels im Speicher gespeichert werden, bleiben für die nächsten Beispiele erhalten). Unser Modell war in der Lage, einen handgefertigten Löser für binäre lineare Programmierung (Binary LP) zu übertreffen. Das vorgeschlagene Modell wurde an verschiedenen binären LP-Instanzen mit einer großen Anzahl von Variablen (bis zu 1000 Variablen) und Einschränkungen (bis zu 700 Einschränkungen) getestet.
Sequence-to-Sequence (Seq2Seq) Modelle mit Aufmerksamkeit haben sich bei Aufgaben, die die Generierung von Sätzen in natürlicher Sprache beinhalten, wie z.B. maschinelle Übersetzung, Bildunterschriften und Spracherkennung, hervorgetan.Die Leistung wurde weiter verbessert, indem unbeschriftete Daten, oft in Form eines Sprachmodells, genutzt wurden. Wir zeigen, dass Seq2Seq-Modelle mit Cold Fusion in der Lage sind, Sprachinformationen besser zu nutzen, indem siei) eine schnellere Konvergenz und eine bessere Generalisierung undii) eine fast vollständige Übertragung auf eine neue Domäne bei Verwendung von weniger als 10 % der gelabelten Trainingsdaten genießen.
Eine zentrale Fähigkeit intelligenter Systeme ist die Fähigkeit, kontinuierlich auf früheren Erfahrungen aufzubauen, um das Erlernen neuer Aufgaben zu beschleunigen und zu verbessern.Zwei unterschiedliche Forschungsparadigmen haben diese Frage untersucht.Meta-Lernen betrachtet dieses Problem als Lernen eines Priors über Modellparameter, der für eine schnelle Anpassung an eine neue Aufgabe geeignet ist, geht aber typischerweise davon aus, dass die Menge der Aufgaben zusammen als ein Stapel verfügbar ist.Im Gegensatz dazu betrachtet Online-Lernen (regret based) eine sequentielle Umgebung, in der Probleme nacheinander aufgedeckt werden, trainiert aber konventionell nur ein einziges Modell ohne aufgabenspezifische Anpassung. Diese Arbeit führt eine Online-Meta-Learning-Umgebung ein, die Ideen aus den beiden oben genannten Paradigmen zusammenführt, um den Geist und die Praxis des kontinuierlichen lebenslangen Lernens besser zu erfassen.Wir schlagen den Follow-the-Meta-Leader (FTML)-Algorithmus vor, der den MAML-Algorithmus auf diese Umgebung erweitert.Theoretisch bietet diese Arbeit eine O(logT)-Regret-Garantie für den FTML-Algorithmus.Unsere experimentelle Auswertung auf drei verschiedenen großen Aufgaben legt nahe, dass der vorgeschlagene Algorithmus Alternativen, die auf traditionellen Online-Lernansätzen basieren, deutlich übertrifft.
Wir untersuchen das Ausmaß, in dem individuelle Aufmerksamkeitsköpfe in vortrainierten Transformator-Sprachmodellen wie BERT und RoBERTa implizit syntaktische Abhängigkeitsbeziehungen erfassen. Wir verwenden zwei Methoden - die maximale Aufmerksamkeitsgewichtung und die Berechnung des maximal überspannenden Baums -, um implizite Abhängigkeitsbeziehungen aus den Aufmerksamkeitsgewichten jeder Schicht/jedes Kopfes zu extrahieren und vergleichen sie mit der Grundwahrheit der Universal Dependency (UD) Bäume. Wir zeigen, dass es für einige UD-Beziehungstypen Köpfe gibt, die den Abhängigkeitstyp signifikant besser wiederherstellen können als die Basislinien auf geparstem englischen Text, was darauf hindeutet, dass einige Selbstaufmerksamkeitsköpfe als Proxy für die syntaktische Struktur fungieren. Wir analysieren BERT auch feinabgestimmt auf zwei Datensätzen - dem syntaxorientierten CoLA und dem semantikorientierten MNLI - um zu untersuchen, ob die Feinabstimmung die Muster ihrer Selbstaufmerksamkeit beeinflusst, aber wir beobachten keine wesentlichen Unterschiede in den mit unseren Methoden extrahierten Abhängigkeitsbeziehungen insgesamt. Unsere Ergebnisse deuten darauf hin, dass diese Modelle einige spezialisierte Aufmerksamkeitsköpfe haben, die einzelne Abhängigkeitstypen verfolgen, aber keinen generalistischen Kopf, der holistisches Parsing signifikant besser durchführt als eine triviale Basislinie, und dass eine direkte Analyse der Aufmerksamkeitsgewichte nicht viel von dem syntaktischen Wissen enthüllt, das BERT-artige Modelle bekanntermaßen lernen.
Die meisten dieser Methoden nutzen jedoch Gesichtsstrukturen und Identitätsinformationen nicht gut aus und haben Schwierigkeiten, mit Gesichtsbildern umzugehen, die große Posenvariationen und Ausrichtungsfehler aufweisen. In diesem Papier schlagen wir eine neuartige Gesichts-Superauflösungsmethode vor, die explizit 3D-Gesichtsprioritäten einbezieht, die die scharfen Gesichtsstrukturen erfassen. Erstens wird der 3D-Gesichtsrendering-Zweig eingerichtet, um 3D-Prioren von hervorstechenden Gesichtsstrukturen und Identitätswissen zu erhalten, und zweitens wird der räumliche Aufmerksamkeitsmechanismus verwendet, um diese hierarchischen Informationen (d.h. Intensitätsähnlichkeit, 3D-Gesichtsstruktur, Identitätsinhalt) für das Superauflösungsproblem besser zu nutzen.Ausführliche Experimente zeigen, dass der vorgeschlagene Algorithmus überlegene Superauflösungsergebnisse erzielt und den Stand der Technik übertrifft.
Wir stellen Cross-View Training (CVT) vor, eine einfache, aber effektive Methode für tiefes, halbüberwachtes Lernen: Auf markierten Beispielen wird das Modell mit dem Standard-Cross-Entropie-Verlust trainiert, und auf einem nicht markierten Beispiel führt das Modell zunächst eine Inferenz durch (es fungiert als "Lehrer"), um weiche Ziele zu erzeugen. Wir weichen von früheren Arbeiten ab, indem wir dem Modell mehrere zusätzliche Studentenvorhersageschichten hinzufügen, wobei die Eingabe für jede Studentenschicht ein Teilnetz des vollständigen Modells ist, das eine eingeschränkte Sicht auf die Eingabe hat (z. B. nur eine Region eines Bildes), Gleichzeitig verbessern die Studenten die Qualität der vom Lehrer verwendeten Repräsentationen, da sie lernen, Vorhersagen mit begrenzten Daten zu treffen. In Kombination mit Virtual Adversarial Training verbessert CVT den aktuellen Stand der Technik bei semi-supervised CIFAR-10 und semi-supervised SVHN. Wir wenden CVT auch an, um Modelle für fünf Aufgaben zur Verarbeitung natürlicher Sprache zu trainieren, wobei wir Hunderte von Millionen von Sätzen mit unbeschrifteten Daten verwenden. Bei allen Aufgaben übertrifft CVT das überwachte Lernen allein erheblich und führt zu Modellen, die den aktuellen Stand der Technik übertreffen oder mit ihm konkurrieren können.
Ich zeige, wie es vorteilhaft sein kann, Metropolis-Entscheidungen über die Annahme oder Ablehnung als Vergleich mit einem einheitlichen [0,1]-Wert auszudrücken und diesen einheitlichen Wert dann nicht-reversibel als Teil des Markov-Ketten-Zustands zu aktualisieren, anstatt ihn bei jeder Iteration unabhängig abzufragen.  Es führt zu einer größeren Verbesserung bei der Verwendung von Langevin-Updates mit persistenten Impulsen und bietet eine Leistung, die mit der von Hamiltonian Monte Carlo (HMC) mit langen Trajektorien vergleichbar ist.  Dies ist von Bedeutung, wenn einige Variablen durch andere Methoden aktualisiert werden, da diese Aktualisierungen bei der HMC nur zwischen den Trajektorien durchgeführt werden können, während sie bei der Langevin-Aktualisierung häufiger durchgeführt werden können.  Dies zeigt sich bei einem Bayes'schen Modell eines neuronalen Netzes, bei dem die Verbindungsgewichte durch persistentes Langevin oder HMC aktualisiert werden, während die Hyperparameter durch Gibbs-Sampling aktualisiert werden.
Wir stellen ES-MAML vor, einen neuen Rahmen für die Lösung des Model Agnostic Meta Learning (MAML)-Problems auf der Grundlage von Evolutionsstrategien (ES). Bestehende Algorithmen für MAML basieren auf Policy-Gradienten und stoßen auf erhebliche Schwierigkeiten, wenn sie versuchen, zweite Ableitungen mit Backpropagation auf stochastischen Policies zu schätzen. Wir zeigen, wie ES auf MAML angewandt werden kann, um einen Algorithmus zu erhalten, der das Problem der Schätzung zweiter Ableitungen vermeidet und zudem konzeptionell einfach und leicht zu implementieren ist.ES-MAML kann außerdem neue Arten von nicht-glatten Anpassungsoperatoren handhaben, und andere Techniken zur Verbesserung der Leistung und Schätzung von ES-Methoden werden anwendbar.Wir zeigen empirisch, dass ES-MAML mit bestehenden Methoden konkurrenzfähig ist und oft eine bessere Anpassung mit weniger Abfragen liefert.
Die Transformation einer Wahrscheinlichkeitsverteilung in eine andere ist ein leistungsfähiges Werkzeug in der Bayes'schen Inferenz und im maschinellen Lernen. Einige prominente Beispiele sind eingeschränkte-unbeschränkte Transformationen von Verteilungen für die Verwendung in Hamiltonian Monte-Carlo und die Konstruktion flexibler und lernfähiger Dichten, wie z.B. normalisierende Flüsse.Wir stellen Bijectors.jl vor, ein Softwarepaket zur Transformation von Verteilungen, das in Julia implementiert ist und unter github.com/TuringLang/Bijectors.jl verfügbar ist.Das Paket bietet eine flexible und zusammensetzbare Möglichkeit, Transformationen von Verteilungen zu implementieren, ohne an einen Berechnungsrahmen gebunden zu sein. Wir demonstrieren die Verwendung von Bijectors.jl zur Verbesserung der Variationsinferenz durch die Kodierung bekannter statistischer Abhängigkeiten in das Variationsposteriori unter Verwendung normalisierender Flüsse, was einen allgemeinen Ansatz zur Lockerung der Mean-Field-Annahme darstellt, die normalerweise in der Variationsinferenz verwendet wird.
Unsere empirische Motivation sind automatisierte Mikroskopiedaten, bei denen kultivierte Zellen abgebildet werden, nachdem sie bekannten und unbekannten chemischen Störungen ausgesetzt wurden, und jeder Datensatz weist eine signifikante experimentelle Verzerrung auf.Dieses Papier stellt einen Ansatz für adversariales Lernen in mehreren Domänen, MuLANN, vor, um mehrere Datensätze mit überlappenden, aber unterschiedlichen Klassensätzen in einer halb überwachten Umgebung zu nutzen. Unsere Beiträge umfassen:i) eine Begrenzung des durchschnittlichen und schlechtesten Domänenrisikos in MDL, die mit Hilfe der H-Divergenz erreicht wird;ii) einen neuen Verlust, um halbüberwachtes Multidomänenlernen und Domänenanpassung zu ermöglichen;iii) die experimentelle Validierung des Ansatzes, die den Stand der Technik bei zwei Standard-Bildbenchmarks und einem neuen Bioimage-Datensatz, Cell, verbessert.
Im Vergleich zu bestehenden Methoden lernt DRN mit weniger Modellparametern und lässt sich leicht auf mehrere Eingangs- und Ausgangsverteilungen ausdehnen. Auf synthetischen und realen Datensätzen erzielt DRN ähnliche oder bessere Leistungen als der Stand der Technik. Darüber hinaus verallgemeinert DRN das herkömmliche mehrschichtige Perzeptron (MLP). Im Rahmen von MLP kodiert jeder Knoten eine reelle Zahl, während bei DRN jeder Knoten eine Wahrscheinlichkeitsverteilung kodiert.
Bestehende Sequenzvorhersagemethoden befassen sich meist mit zeitunabhängigen Sequenzen, bei denen die tatsächliche Zeitspanne zwischen den Ereignissen irrelevant ist und der Abstand zwischen den Ereignissen einfach die Differenz zwischen ihren Reihenfolgepositionen in der Sequenz ist.Während diese zeitunabhängige Sichtweise von Sequenzen für Daten wie natürliche Sprachen anwendbar ist, z. B. wenn es um Wörter in einem Satz geht, ist sie für viele Ereignisse in der realen Welt, die zu ungleichmäßig beabstandeten Zeitpunkten beobachtet und gesammelt werden, ungeeignet und ineffizient, da sie auf natürliche Weise auftreten, z. B, Die Zeitspanne zwischen den Ereignissen kann wichtige Informationen über die Sequenzabhängigkeit menschlicher Verhaltensweisen enthalten. In dieser Arbeit schlagen wir eine Reihe von Methoden für die Verwendung von Zeit in der Sequenzvorhersage vor. Da neuronale Sequenzmodelle wie RNN für die Verarbeitung von Token-ähnlichen Eingaben besser geeignet sind, schlagen wir zwei Methoden für die zeitabhängige Ereignisdarstellung vor, die auf der Intuition basieren, wie Zeit im täglichen Leben in Token umgewandelt wird, sowie auf früheren Arbeiten zur Einbettung von Kontextualisierung. Wir diskutieren diese Methoden auf der Basis von rekurrenten neuronalen Netzen und evaluieren diese Methoden sowie Basismodelle auf fünf Datensätzen, die einer Vielzahl von Sequenzvorhersageaufgaben ähneln. Die Experimente haben gezeigt, dass die vorgeschlagenen Methoden einen Genauigkeitsgewinn gegenüber Basismodellen in einer Reihe von Einstellungen bieten.
Off-Policy-Verstärkungslernalgorithmen versprechen, in Umgebungen anwendbar zu sein, in denen nur ein fester Datensatz (Batch) von Umgebungsinteraktionen verfügbar ist und keine neuen Erfahrungen gesammelt werden können.Diese Eigenschaft macht diese Algorithmen für reale Probleme wie Robotersteuerung attraktiv.In der Praxis versagen jedoch Standard-Off-Policy-Algorithmen in der Batch-Einstellung für kontinuierliche Steuerung.In diesem Papier schlagen wir eine einfache Lösung für dieses Problem vor. Sie erlaubt die Verwendung von Daten, die durch beliebige Verhaltensrichtlinien generiert werden, und verwendet einen gelernten Prior - das vorteilsgewichtete Verhaltensmodell (ABM) - um die RL-Richtlinie in Richtung von Aktionen zu lenken, die zuvor ausgeführt wurden und bei der neuen Aufgabe wahrscheinlich erfolgreich sind.Unsere Methode kann als eine Erweiterung der jüngsten Arbeit über Batch-RL gesehen werden, die stabiles Lernen aus widersprüchlichen Datenquellen ermöglicht.Wir finden Verbesserungen gegenüber konkurrierenden Basislinien in einer Vielzahl von RL-Aufgaben - einschließlich Standard-Benchmarks für kontinuierliche Steuerung und Multi-Task-Lernen für simulierte und reale Roboter.
Eine der größten Herausforderungen bei der Anwendung graphischer neuronaler Netze auf Gen-Interaktionsdaten ist das mangelnde Verständnis des Vektorraums, zu dem sie gehören, und auch die inhärenten Schwierigkeiten, die mit der Darstellung dieser Interaktionen auf einer wesentlich niedrigeren Dimension, nämlich dem euklidischen Raum, verbunden sind. Wir stellen eine systematische, verallgemeinerte Methode vor, genannt iSOM-GSN, mit der ``multi-omische'' Daten mit höheren Dimensionen auf ein zweidimensionales Gitter transformiert werden können, und wenden anschließend ein neuronales Faltungsnetzwerk an, um Krankheitszustände verschiedener Arten vorherzusagen.Basierend auf der Idee der selbstorganisierenden Karte von Kohonen erzeugen wir für jede Probe ein zweidimensionales Gitter für einen gegebenen Satz von Genen, die ein Gen-Ähnlichkeitsnetzwerk darstellen.  Wir haben das Modell zur Vorhersage von Brust- und Prostatakrebs unter Verwendung von Genexpression, DNA-Methylierung und Kopienzahlveränderung getestet und dabei Vorhersagegenauigkeiten im Bereich von 94-98% für Tumorstadien von Brustkrebs und berechnete Gleason-Scores von Prostatakrebs mit nur 11 Eingangsgenen für beide Fälle erzielt.Das Schema liefert nicht nur eine nahezu perfekte Klassifizierungsgenauigkeit, sondern bietet auch ein verbessertes Schema für Darstellungslernen, Visualisierung, Dimensionalitätsreduktion und Interpretation der Ergebnisse.
Es scheint, dass das Vortrainingsverfahren eine sehr gute gemeinsame Initialisierung für das weitere Training für verschiedene Aufgaben des natürlichen Sprachverständnisses lernt, so dass nur wenige Schritte im Parameterraum unternommen werden müssen, um jede Aufgabe zu lernen. In dieser Arbeit verifizieren wir diese Hypothese am Beispiel der Bidirektionalen Encoder-Repräsentationen von Transformatoren (BERT), indem wir zeigen, dass aufgabenspezifische, fein abgestimmte Sprachmodelle im Parameterraum sehr nahe am vortrainierten Modell liegen. Unter Ausnutzung solcher Beobachtungen zeigen wir weiter, dass die fein abgestimmten Versionen dieser riesigen Modelle, die in der Größenordnung von $10^8$ Fließkommaparametern liegen, sehr recheneffizient gemacht werden können: Erstens reicht es aus, nur einen Bruchteil der kritischen Schichten fein abzustimmen, und zweitens kann die Feinabstimmung durch das Lernen einer binären multiplikativen Maske auf den vortrainierten Gewichten, d.h. durch Parameter-Sparsamkeit, adäquat durchgeführt werden: (1) Lernen, um bestimmte Aufgaben zu erfüllen, (2) Einsparung von Speicherplatz, indem nur binäre Masken bestimmter Schichten für jede Aufgabe gespeichert werden, und (3) Einsparung von Rechenleistung auf geeigneter Hardware, indem spärliche Operationen mit Modellparametern durchgeführt werden.  
Wir stellen einen einfachen und effektiven Algorithmus vor, der entwickelt wurde, um das Problem der Kovariatenverschiebung beim Imitationslernen anzugehen, indem er ein Ensemble von Richtlinien auf den Demonstrationsdaten der Experten trainiert und die Varianz ihrer Vorhersagen als Kosten verwendet, die mit RL zusammen mit den Kosten eines überwachten Verhaltensklonens minimiert werden. Wir beweisen eine Bedauernsschranke für den Algorithmus in der tabellarischen Einstellung, die linear im Zeithorizont ist, multipliziert mit einem Koeffizienten, von dem wir zeigen, dass er für bestimmte Probleme, bei denen das behaviorale Klonen versagt, niedrig ist.Wir evaluieren unseren Algorithmus empirisch über mehrere pixelbasierte Atari-Umgebungen und kontinuierliche Kontrollaufgaben und zeigen, dass er mit dem behavioralen Klonen und dem generativen adversarischen Nachahmungslernen übereinstimmt oder diese deutlich übertrifft.
Wir präsentieren und diskutieren eine einfache Bildvorverarbeitungsmethode für das Erlernen unentwirrbarer latenter Faktoren. Insbesondere nutzen wir die implizite induktive Verzerrung, die in den Merkmalen von Netzwerken enthalten ist, die in der ImageNet-Datenbank trainiert wurden. Wir verstärken diesen Bias durch explizite Feinabstimmung solcher vortrainierter Netzwerke auf Aufgaben, die für die NeurIPS2019 Herausforderung der Entwirrung nützlich sind, wie z.B. Winkel- und Positionsschätzung oder Farbklassifizierung.
Zusätzlich zu ihrem Potenzial, wünschenswertes Verhalten auf unbekannte Ziele zu verallgemeinern, können solche Strategien auch eine Planung auf höherer Ebene ermöglichen, die auf Unterzielen basiert. In Umgebungen mit spärlicher Belohnung scheint die Fähigkeit, Informationen über den Grad zu nutzen, in dem ein beliebiges Ziel erreicht wurde, während ein anderes Ziel beabsichtigt war, entscheidend zu sein, um effizientes Lernen zu ermöglichen. In diesem Papier demonstrieren wir, wie Rückblicke in Policy-Gradienten-Methoden eingeführt werden können, indem wir diese Idee auf eine breite Klasse erfolgreicher Algorithmen verallgemeinern. Unsere Experimente mit einer vielfältigen Auswahl an spärlichen Belohnungsumgebungen zeigen, dass Rückblicke zu einer bemerkenswerten Steigerung der Stichprobeneffizienz führen.
Computer Vision hat eine dramatische Revolution in der Leistung, zum großen Teil durch tiefe Funktionen auf große überwachte Datensätze trainiert getrieben unterzogen.Allerdings haben viele dieser Verbesserungen auf statische Bildanalyse konzentriert; Video Verständnis hat eher bescheidenen Verbesserungen gesehen.Obwohl neue Datensätze und räumlich-zeitliche Modelle wurden vorgeschlagen, einfache Frame-by-Frame-Klassifizierung Methoden bleiben oft noch wettbewerbsfähig.Wir gehen davon aus, dass aktuelle Video-Datensätze mit impliziten Verzerrungen über Szene und Objektstruktur, die Variationen in der zeitlichen Struktur zwergen kann geplagt sind. In dieser Arbeit bauen wir einen Videodatensatz mit vollständig beobachtbaren und kontrollierbaren Objekt- und Szenenverzerrungen auf, der wirklich ein raum-zeitliches Verständnis erfordert, um gelöst zu werden. Unser Datensatz, genannt CATER, wird synthetisch mit einer Bibliothek von Standard-3D-Objekten gerendert und testet die Fähigkeit, Kompositionen von Objektbewegungen zu erkennen, die eine langfristige Argumentation erfordern. CATER ist nicht nur ein anspruchsvoller Datensatz, sondern bietet auch eine Fülle von Diagnosewerkzeugen zur Analyse moderner raum-zeitlicher Videoarchitekturen, da er vollständig beobachtbar und kontrollierbar ist.
Wir befassen uns mit den Effizienzproblemen, die durch den Nachzügler-Effekt beim kürzlich entstandenen föderierten Lernen verursacht werden, bei dem ein Modell auf dezentralisierten, nicht unabhängigen und identisch verteilten Daten über massive Arbeitsgeräte hinweg kollaborativ trainiert wird, ohne dass Trainingsdaten in den unzuverlässigen und heterogenen Netzwerken ausgetauscht werden.Wir schlagen eine neuartige zweistufige Analyse der Fehlergrenzen des allgemeinen föderierten Lernens vor, die praktische Einblicke in die Optimierung bietet. Als Ergebnis schlagen wir einen neuartigen, einfach zu implementierenden föderierten Lernalgorithmus vor, der asynchrone Einstellungen und Strategien verwendet, um Diskrepanzen zwischen dem globalen Modell und verzögerten Modellen zu kontrollieren und die Anzahl der lokalen Epochen mit der Schätzung der Staleness anzupassen, um die Konvergenz zu beschleunigen und einer Leistungsverschlechterung durch Nachzügler zu widerstehen.
Long Short-Term Memory (LSTM)-Einheiten haben die Fähigkeit, langfristige Abhängigkeiten zwischen Eingaben zu speichern und zu nutzen, um Vorhersagen für Zeitreihendaten zu generieren.Wir führen das Konzept der Modifizierung des Zellzustands (Speichers) von LSTMs ein, indem wir Rotationsmatrizen verwenden, die durch einen neuen Satz trainierbarer Gewichte parametrisiert werden.Dieser Zusatz zeigt signifikante Leistungssteigerungen bei einigen Aufgaben aus dem bAbI-Datensatz.
Wir befassen uns mit dem Problem der marginalen Inferenz für eine exponentielle Familie, die über die Menge der Permutationsmatrizen definiert ist. Dieses Problem ist bekannt dafür, dass es mit zunehmender Größe der Permutation schnell unlösbar wird, da es die Berechnung der Permanenz einer Matrix beinhaltet, ein #P-schweres Problem. Wir führen die Sinkhorn-Variationsrandinferenz als skalierbare Alternative ein, eine Methode, deren Gültigkeit letztlich durch die so genannte Sinkhorn-Approximation der Permanenz gerechtfertigt ist.
Die Robustheit von neuronalen Netzen gegenüber gegnerischen Beispielen hat große Aufmerksamkeit aufgrund von Sicherheitsimplikationen erhalten.Trotz verschiedener Angriffsansätze, um visuell nicht wahrnehmbare gegnerische Beispiele zu erstellen, wurde wenig in Richtung eines umfassenden Maßes der Robustheit entwickelt.In diesem Beitrag liefern wir eine theoretische Begründung für die Umwandlung der Robustheitsanalyse in ein lokales Lipschitz-Konstanten-Schätzproblem und schlagen vor, die Extremwerttheorie für eine effiziente Bewertung zu verwenden.Unsere Analyse ergibt eine neuartige Robustheitsmetrik namens CLEVER, die kurz für Cross Lipschitz Extreme Value for nEtwork Robustness steht. Experimentelle Ergebnisse auf verschiedenen Netzwerken, darunter ResNet, Inception-v3 und MobileNet, zeigen, dass(i) CLEVER mit der Robustheitsindikation übereinstimmt, die durch die $\ell_2$- und $\ell_\infty$-Normen von gegnerischen Beispielen aus mächtigen Angriffen gemessen wird, und(ii) verteidigte Netzwerke, die defensive Destillation oder Bounded ReLU verwenden, tatsächlich bessere CLEVER-Werte liefern. Unseres Wissens nach ist CLEVER die erste angriffsunabhängige Robustheitsmetrik, die auf beliebige Klassifikatoren neuronaler Netze angewendet werden kann.
Die Zusammenarbeit mehrerer Agenten ist für zahlreiche reale Probleme erforderlich. Obwohl praktische Systeme in der Regel dezentral arbeiten, sind Kommunikation und Informationsaggregation auf lokaler Ebene immer noch wichtig, um komplexe Aufgaben zu erfüllen. Für das Multi-Agenten-Verstärkungslernen wurden viele frühere Studien dem Entwurf einer effektiven Kommunikationsarchitektur gewidmet, Die meisten von ihnen geben einen bestimmten Kommunikationsmodus vor, indem sie eine feste zeitliche Frequenz und einen räumlichen Bereich für die Agenten vorgeben, in dem sie unabhängig von der Notwendigkeit kommunizieren müssen.Ein solches Design ist nicht in der Lage, mit Multi-Agenten-Szenarien umzugehen, die launisch und kompliziert sind, insbesondere wenn nur partielle Informationen verfügbar sind.Motiviert durch diese Tatsache, argumentieren wir, dass die Lösung darin besteht, ein spontanes und selbstorganisierendes Kommunikations-Lernschema (SSoC) zu entwickeln. Die Agenten lernen auch, wie sie adaptiv die empfangenen Nachrichten und ihre eigenen verborgenen Zustände aggregieren können, um Aktionen auszuführen.Verschiedene Experimente wurden durchgeführt, um zu zeigen, dass SSoC wirklich intelligente Nachrichtenübermittlung zwischen Agenten lernt, die weit voneinander entfernt sind.Mit solch agiler Kommunikation beobachten wir, dass effektive Kollaborationstaktiken auftauchen, die von den verglichenen Basissystemen nicht gemeistert wurden.
Wir untersuchen das BERT-Sprachrepräsentationsmodell und das Sequenzgenerierungsmodell mit BERT-Kodierer für Multi-Label-Textklassifizierungsaufgaben.Wir experimentieren mit beiden Modellen und erforschen ihre besonderen Qualitäten für diese Aufgabe.Wir stellen auch ein gemischtes Modell vor und untersuchen es experimentell, das ein Ensemble aus Multi-Label-BERT- und Sequenzgenerierungs-BERT-Modellen ist.Unsere Experimente haben gezeigt, dass BERT-basierte Modelle und insbesondere das gemischte Modell die aktuellen Basislinien in mehreren Metriken übertreffen und State-of-the-Art-Ergebnisse auf drei gut untersuchten Multi-Label-Klassifizierungsdatensätzen mit englischen Texten und zwei privaten Yandex-Taxi-Datensätzen mit russischen Texten erzielen.
Wir schlagen ein neuartiges Modell für CTR-Aufgaben vor, das Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM) genannt wird. Anstatt die Cross-Features direkt zu lernen, nimmt DeepEnFM den Transformer-Encoder als Rückgrat, um die Feature-Einbettungen mit den Hinweisen aus anderen Bereichen abzugleichen. DeepEnFM verwendet einen bilinearen Ansatz, um verschiedene Ähnlichkeitsfunktionen in Bezug auf verschiedene Feldpaare zu generieren, und die Max-Pooling-Methode ermöglicht es DeepEnFM, sowohl die ergänzenden als auch die unterdrückenden Informationen zwischen verschiedenen Aufmerksamkeitsköpfen zu erfassen.Unser Modell wurde an den Datensätzen von Criteo und Avazu validiert und erreicht eine State-of-the-Art-Leistung.
Damit autonome Agenten in der realen Welt erfolgreich agieren können, ist die Fähigkeit, zukünftige Szenenzustände zu antizipieren, eine Schlüsselkompetenz.In realen Szenarien werden zukünftige Zustände zunehmend unsicherer und multimodaler, insbesondere auf langen Zeithorizonten. Dropout-basierte Bayes'sche Inferenz bietet einen rechnerisch überschaubaren, theoretisch gut begründeten Ansatz zum Erlernen verschiedener Hypothesen/Modelle, um mit unsicheren Zukunftszuständen umzugehen und Vorhersagen zu treffen, die gut mit Beobachtungen übereinstimmen - gut kalibriert sind.Es stellt sich jedoch heraus, dass solche Ansätze nicht ausreichen, um komplexe Szenen in der realen Welt zu erfassen, und sogar in der Genauigkeit zurückfallen, wenn man sie mit den einfachen deterministischen Ansätzen vergleicht. In dieser Arbeit schlagen wir eine neuartige Bayes'sche Formulierung für die Vorhersage zukünftiger Szenenzustände vor, die synthetische Likelihoods nutzt, die das Erlernen verschiedener Modelle fördern, um die multimodale Natur zukünftiger Szenenzustände genau zu erfassen. wir zeigen, dass unser Ansatz genaue Vorhersagen auf dem neuesten Stand der Technik und kalibrierte Wahrscheinlichkeiten durch umfangreiche Experimente für die Vorhersage von Szenen auf dem Cityscapes-Datensatz erreicht.
Conditional generative adversarial networks (cGAN) haben zu großen Verbesserungen bei der Aufgabe der bedingten Bilderzeugung geführt, die im Zentrum der Computer Vision liegt. Der Hauptfokus lag bisher auf der Leistungsverbesserung, während es wenig Anstrengungen gab, cGAN robuster gegenüber Rauschen zu machen. In dieser Arbeit stellen wir ein neuartiges bedingtes GAN-Modell, genannt RoCGAN, vor, das die Struktur im Zielraum des Modells nutzt, um dieses Problem anzugehen. Unser Modell erweitert den Generator mit einem unbeaufsichtigten Pfad, der die Ausgaben des Generators dazu bringt, die Zielvielfalt selbst in Anwesenheit von starkem Rauschen zu überspannen. wir beweisen, dass RoCGAN ähnliche theoretische Eigenschaften wie GAN hat, und überprüfen experimentell, dass unser Modell die bestehenden hochmodernen cGAN-Architekturen in einer Vielzahl von Bereichen, einschließlich Bildern von natürlichen Szenen und Gesichtern, deutlich übertrifft.
Obwohl tiefe neuronale Netze haben den Stand der Technik Leistung in der visuellen Klassifikation erreicht, haben neuere Studien gezeigt, dass sie alle anfällig für den Angriff von adversarial examples.to das Problem zu lösen, einige Regularisierung adversarial Ausbildung Methoden, die Einschränkung der Ausgabe Label oder logit, wurden untersucht.in diesem Papier, schlagen wir eine neuartige regularisierte adversarial Ausbildung Rahmen ATLPA, nämlich Adversarial Tolerant Logit Pairing with Attention.instead of constraining eine harte Verteilung (zB.), Anstatt eine harte Verteilung (z.B. One-Hot-Vektoren oder Logit) im adversen Training zu beschränken, verwendet ATLPA Tolerant Logit, das aus einer Konfidenzverteilung für die Top-k-Klassen besteht und Ähnlichkeiten zwischen den Klassen auf Bildebene erfasst.ATLPA minimiert nicht nur den empirischen Verlust, sondern fördert auch die Aufmerksamkeitskarte für Paare von Beispielen, die ähnlich sind. Wir bewerten ATLPA mit dem Stand der Technik Algorithmen, die experimentellen Ergebnisse zeigen, dass unsere Methode übertrifft diese Grundlinien mit höherer Genauigkeit.Im Vergleich zu früheren Arbeiten, ist unsere Arbeit unter sehr anspruchsvollen PGD Angriff bewertet: die maximale Störung $\epsilon $ ist 64 und 128 mit 10 bis 200 Angriff Iterationen.
Ein grundlegendes Merkmal von Intelligenz ist die Fähigkeit, Ziele angesichts neuartiger Umstände zu erreichen.In dieser Arbeit befassen wir uns mit einer solchen Einstellung, die die Lösung einer Aufgabe mit einer neuartigen Reihe von Aktionen erfordert.Empowering Maschinen mit dieser Fähigkeit erfordert Verallgemeinerung in der Art und Weise ein Agent wahrnimmt, seine verfügbaren Aktionen zusammen mit der Art und Weise verwendet sie diese Aktionen, um Aufgaben zu lösen.Daher schlagen wir einen Rahmen, um die Verallgemeinerung über diese beiden Aspekte zu ermöglichen: Verständnis einer Aktion die Funktionalität und die Verwendung von Aktionen zur Lösung von Aufgaben durch Verstärkung Lernen. Wir verwenden eine Architektur des Verstärkungslernens, die über diese Aktionsrepräsentationen arbeitet, und schlagen Regularisierungsmetriken vor, die für die Generalisierung in einer Strategie unerlässlich sind. Wir veranschaulichen die Generalisierbarkeit der Methode des Repräsentationslernens und der Strategie, um eine Zero-Shot-Generalisierung auf zuvor unbekannte Aktionen in anspruchsvollen sequenziellen Entscheidungsumgebungen zu ermöglichen. Unsere Ergebnisse und Videos finden Sie unter sites.google.com/view/action-generalization/
Zeitliche Punktprozesse sind das vorherrschende Paradigma für die Modellierung von Sequenzen von Ereignissen, die in unregelmäßigen Abständen stattfinden. Die Standardmethode für das Lernen in solchen Modellen ist die Schätzung der bedingten Intensitätsfunktion.  Wir zeigen, wie man die Beschränkungen der intensitätsbasierten Ansätze überwinden kann, indem man die bedingte Verteilung der Zeit zwischen den Ereignissen direkt modelliert.  Wir stützen uns auf die Literatur zur Normalisierung von Flüssen, um Modelle zu entwerfen, die flexibel und effizient sind, und schlagen zusätzlich ein einfaches Mischungsmodell vor, das der Flexibilität von flussbasierten Modellen entspricht, aber auch Stichproben und die Berechnung von Momenten in geschlossener Form ermöglicht.  Die vorgeschlagenen Modelle erreichen Spitzenleistungen bei Standardvorhersageaufgaben und eignen sich für neuartige Anwendungen wie das Erlernen von Sequenzeinbettungen und das Imputieren fehlender Daten.
Die Methode basiert auf dem Training einer Autoencoder-Struktur, bei der der Engpass den Raum der Themenverteilung und die Decoder-Ausgänge den Raum der Wortverteilungen über die Themen repräsentieren.Wir nutzen einen Hilfsdecoder, um das Zusammenbrechen der Modi in unserem Modell zu verhindern.  Eine Schlüsseleigenschaft für eine effektive Themenmodellierungsmethode ist es, spärliche Themen- und Wortverteilungen zu haben, wobei es einen Kompromiss zwischen dem Sparsamkeitsniveau von Themen und Wörtern gibt.Diese Eigenschaft wird in unserem Modell durch L-2 Regularisierung implementiert und die Modellhyperparameter kümmern sich um den Kompromiss.  Wir zeigen in unseren Experimenten, dass unser Modell trotz seiner einfachen Architektur und Trainingsprozedur konkurrenzfähige Ergebnisse im Vergleich zu den State-of-the-Art Deep Models für Topic Modelling erzielt.Die "New York Times" und "20 Newsgroups" Datasets werden in den Experimenten verwendet.
Frühere Ansätze in der Literatur finden in erster Linie ein Mapping zwischen den gegebenen Quelle-Ziel-Sprecher-Paaren. Die Entwicklung von Mapping-Techniken für Many-to-Many-VC mit nicht-parallelen Daten, einschließlich Zero-Shot-Lernen, bleibt ein weniger erforschter Bereich in VC. Die meisten der Many-to-Many-VC-Architekturen erfordern Trainingsdaten von allen Zielsprechern, für die wir die Stimmen konvertieren wollen. In diesem Papier schlagen wir eine neuartige Style-Transfer-Architektur vor, die auch erweitert werden kann, um Stimmen auch für Zielsprecher zu erzeugen, deren Daten nicht im Training verwendet wurden (d.h., Insbesondere schlagen wir Adaptive Generative Adversarial Network (AdaGAN), neue architektonische Trainingsprozedur Hilfe beim Lernen normalisierte Sprecher-unabhängige latente Repräsentation, die verwendet werden, um Sprache mit verschiedenen Sprechstilen im Zusammenhang mit VC zu erzeugen.Wir vergleichen unsere Ergebnisse mit dem Stand der Technik StarGAN-VC-Architektur.Insbesondere erreicht die AdaGAN 31. Die Hauptstärke der vorgeschlagenen Architekturen ist, dass sie diese Ergebnisse mit geringerer Rechenkomplexität erzielt. AdaGAN ist 88,6% weniger komplex als StarGAN-VC in Bezug auf FLoating Operation Per Second (FLOPS) und 85,46% weniger komplex in Bezug auf die trainierbaren Parameter.  
Die Selbstaufmerksamkeit ist in der Lage, langfristige Abhängigkeiten zu modellieren, kann aber unter der Extraktion irrelevanter Informationen im Kontext leiden. Um dieses Problem zu lösen, schlagen wir ein neuartiges Modell namens Sparse Transformer vor. Sparse Transformer ist in der Lage, die Konzentration der Aufmerksamkeit auf den globalen Kontext durch eine explizite Auswahl der relevantesten Segmente zu verbessern.Umfangreiche experimentelle Ergebnisse zu einer Reihe von natürlichen Sprachverarbeitungsaufgaben, einschließlich neuronaler maschineller Übersetzung, Bildbeschriftung und Sprachmodellierung, zeigen alle die Vorteile von Sparse Transformer in der Modellleistung.   Sparse Transformer erreicht in den IWSLT 2015 Englisch-Vietnamesisch-Übersetzung und IWSLT 2014 Deutsch-Englisch-Übersetzung die beste Leistung.
Wir stellen die Hypothese auf, dass eine dateneffiziente Erkennung durch Repräsentationen ermöglicht wird, die die Variabilität in natürlichen Signalen vorhersehbarer machen, wie dies durch neuere Wahrnehmungsnachweise nahegelegt wird.Wir überarbeiten und verbessern daher Contrastive Predictive Coding, einen kürzlich vorgeschlagenen Rahmen für unüberwachtes Lernen, und gelangen zu einer Repräsentation, die eine Generalisierung aus kleinen Mengen von markierten Daten ermöglicht. Wenn nur 1% der ImageNet-Labels (d.h. 13 pro Klasse) zur Verfügung gestellt werden, behält dieses Modell eine starke Klassifizierungsleistung, 73% Top-5-Genauigkeit, und übertrifft überwachte Netzwerke um 28% (eine 65%ige relative Verbesserung) und modernste halbüberwachte Methoden um 14%. Wir finden auch, dass diese Repräsentation als nützliches Substrat für die Objekterkennung auf dem PASCAL-VOC 2007-Datensatz dient und sich der Leistung von Repräsentationen nähert, die mit einem vollständig annotierten ImageNet-Datensatz trainiert wurden.
Wir demonstrieren die Möglichkeit dessen, was wir spärliches Lernen nennen: beschleunigtes Training von tiefen neuronalen Netzen, die spärliche Gewichte während des gesamten Trainings beibehalten und gleichzeitig dichte Leistungsniveaus erreichen, indem wir spärliches Momentum entwickeln, einen Algorithmus, der exponentiell geglättete Gradienten (Momentum) verwendet, um Schichten und Gewichte zu identifizieren, die den Fehler effizient reduzieren. Innerhalb einer Schicht wächst sparse momentum Gewichte entsprechend der Impulsgröße von nullwertigen Gewichten.Wir demonstrieren State-of-the-Art sparse Leistung auf MNIST, CIFAR-10 und ImageNet, die Verringerung der mittleren Fehler um eine relative 8%, 15% und 6% im Vergleich zu anderen sparse Algorithmen.Darüber hinaus zeigen wir, dass sparse momentum zuverlässig reproduziert dichten Leistungsniveaus bei gleichzeitiger Bereitstellung von bis zu 5,61x schneller training.In unserer Analyse, Ablationen zeigen, dass die Vorteile der Impuls Umverteilung und Wachstum mit der Tiefe und Größe des Netzes zu erhöhen.
Um prinzipielle Wege für den Entwurf geeigneter Deep Neural Network (DNN)-Modelle zu finden, ist es wichtig, die Verlustfläche von DNNs unter realistischen Annahmen zu verstehen.Wir stellen interessante Aspekte für das Verständnis der lokalen Minima und der Gesamtstruktur der Verlustfläche vor.Der Parameterbereich der Verlustfläche kann in Regionen zerlegt werden, in denen Aktivierungswerte (Null oder Eins für gleichgerichtete lineare Einheiten) konsistent sind. Wir haben herausgefunden, dass die Verlustfläche in jeder Region ähnliche Eigenschaften hat wie lineare neuronale Netze, bei denen jedes lokale Minimum ein globales Minimum ist, d.h. jedes differenzierbare lokale Minimum ist das globale Minimum der entsprechenden Region.Wir beweisen das für ein neuronales Netz mit einer versteckten Schicht unter Verwendung von gleichgerichteten linearen Einheiten unter realistischen Annahmen.Es gibt schlechte Regionen, die zu schlechten lokalen Minima führen, und wir erklären, warum solche Regionen sogar in den überparametrisierten DNNs existieren.
Es hat sich gezeigt, dass kontextualisierte Wortrepräsentationen wie ELMo und BERT bei einer Reihe von semantischen und strukturellen (syntaktischen) Aufgaben gut abschneiden. In dieser Arbeit befassen wir uns mit der Aufgabe der unbeaufsichtigten Entflechtung zwischen Semantik und Struktur in neuronalen Sprachrepräsentationen: Wir versuchen, eine Transformation der kontextualisierten Vektoren zu erlernen, die die lexikalische Semantik verwirft, aber die strukturelle Information behält. Zu diesem Zweck generieren wir automatisch Gruppen von Sätzen, die strukturell ähnlich, aber semantisch unterschiedlich sind, und verwenden einen metrischen Lernansatz, um eine Transformation zu erlernen, die die strukturelle Komponente, die in den Vektoren kodiert ist, hervorhebt.wir zeigen, dass unsere Transformation Vektoren im Raum nach strukturellen Eigenschaften und nicht nach lexikalischer Semantik clustert.schließlich demonstrieren wir die Nützlichkeit unserer destillierten Repräsentationen, indem wir zeigen, dass sie die ursprünglichen kontextualisierten Repräsentationen in einer "few-shot" Parsing-Umgebung übertreffen.
Das vorgeschlagene semiparametrische topologische Gedächtnis (SPTM) besteht aus einem (nicht-parametrischen) Graphen mit Knoten, die Orten in der Umgebung entsprechen, und einem (parametrischen) tiefen Netzwerk, das in der Lage ist, Knoten aus dem Graphen auf der Grundlage von Beobachtungen abzurufen.der Graph speichert keine metrischen Informationen, sondern nur die Konnektivität der Orte, die den Knoten entsprechen. Wir verwenden SPTM als Planungsmodul in einem Navigationssystem. 5 Minuten Filmmaterial eines zuvor unbekannten Labyrinths genügen, um einen SPTM-basierten Navigationsagenten eine topologische Karte der Umgebung erstellen zu lassen und ihn dazu zu verwenden, sicher zu Zielen zu navigieren. Die durchschnittliche Erfolgsrate des SPTM-Agenten bei der zielgerichteten Navigation in den Testumgebungen ist um den Faktor drei höher als die der leistungsstärksten Baseline.
Die verfügbare Auflösung in unserer visuellen Welt ist extrem hoch, wenn nicht sogar unendlich. Bestehende CNNs können auf Bilder mit beliebiger Auflösung angewendet werden, aber mit zunehmender Größe der Eingabe können sie keine kontextuellen Informationen erfassen. Darüber hinaus skalieren die Rechenanforderungen linear mit der Anzahl der Eingabepixel, und die Ressourcen werden gleichmäßig über die Eingabe verteilt, unabhängig davon, wie informativ die verschiedenen Bildregionen sind.Wir versuchen, diese Probleme zu lösen, indem wir eine neuartige Architektur vorschlagen, die eine Bildpyramide von oben nach unten durchläuft, während sie einen harten Aufmerksamkeitsmechanismus verwendet, um selektiv nur die informativsten Bildteile zu verarbeiten. Wir führen Experimente mit MNIST- und ImageNet-Datensätzen durch und zeigen, dass unsere Modelle vollständig gefaltete Gegenstücke signifikant übertreffen können, wenn die Auflösung des Inputs so groß ist, dass das rezeptive Feld der Basislinien die interessierenden Objekte nicht angemessen abdecken kann, und dass Leistungsgewinne aufgrund der selektiven Verarbeitung, die wir verfolgen, für weniger FLOPs erzielt werden.
Die Hyperparameter-Optimierung kann als ein zweistufiges Optimierungsproblem formuliert werden, bei dem die optimalen Parameter auf der Trainingsmenge von den Hyperparametern abhängen. Wir zeigen, wie man skalierbare Best-Response-Approximationen für neuronale Netze konstruieren kann, indem man die Best-Response als ein einzelnes Netz modelliert, dessen versteckte Einheiten abhängig vom Regularisierer reguliert werden.Wir rechtfertigen diese Approximation, indem wir zeigen, dass die exakte Best-Response für ein flaches lineares Netz mit L2-Regularisierung durch einen ähnlichen Gating-Mechanismus dargestellt werden kann. Wir passen dieses Modell mit Hilfe eines gradientenbasierten Hyperparameter-Optimierungsalgorithmus an, der zwischen der Annäherung der Best-Response um die aktuellen Hyperparameter herum und der Optimierung der Hyperparameter unter Verwendung der approximativen Best-Response-Funktion wechselt.Im Gegensatz zu anderen gradientenbasierten Ansätzen ist es bei uns nicht erforderlich, den Trainingsverlust in Bezug auf die Hyperparameter zu differenzieren, so dass wir diskrete Hyperparameter, Datenergänzungs-Hyperparameter und Dropout-Wahrscheinlichkeiten abstimmen können. Da die Hyperparameter online angepasst werden, entdeckt unser Ansatz Hyperparameterpläne, die feste Hyperparameterwerte übertreffen können. Empirisch gesehen übertrifft unser Ansatz konkurrierende Hyperparameter-Optimierungsmethoden bei großen Deep-Learning-Problemen. Wir nennen unsere Netzwerke, die ihre eigenen Hyperparameter während des Trainings online aktualisieren, Self-Tuning Networks (STNs).
Trotz herausragender Fortschritte beinhaltet die quantitative Bewertung solcher Modelle oft mehrere unterschiedliche Metriken, um verschiedene wünschenswerte Eigenschaften zu bewerten, wie z.B. Bildqualität, bedingte Konsistenz und Diversität innerhalb der Konditionierung.In diesem Umfeld wird das Benchmarking von Modellen zu einer Herausforderung, da jede Metrik ein anderes "bestes" Modell anzeigen kann. In diesem Beitrag schlagen wir die Frechet Joint Distance (FJD) vor, die als Frechet-Distanz zwischen den gemeinsamen Verteilungen von Bildern und Konditionierung definiert ist und es ermöglicht, die oben genannten Eigenschaften implizit in einer einzigen Metrik zu erfassen.Wir führen Proof-of-Concept-Experimente auf einem kontrollierbaren synthetischen Datensatz durch, die durchweg die Vorteile der FJD im Vergleich zu den derzeit etablierten Metriken hervorheben. Darüber hinaus verwenden wir die neu eingeführte Metrik, um bestehende cGAN-basierte Modelle für eine Vielzahl von Konditionierungsmodalitäten zu vergleichen (z.B. Klassenetiketten, Objektmasken, Bounding Boxes, Bilder und Textbeschriftungen) Wir zeigen, dass FJD als vielversprechende Einzelmetrik für das Benchmarking von Modellen verwendet werden kann.
Die Kombination von tiefem modellfreiem Reinforcement Learning mit Online-Planung ist ein vielversprechender Ansatz, um auf den Erfolgen von Deep RL aufzubauen.Online-Planung mit Look-Ahead-Bäumen hat sich in Umgebungen, in denen Übergangsmodelle a priori bekannt sind, als erfolgreich erwiesen.In komplexen Umgebungen, in denen Übergangsmodelle aus Daten erlernt werden müssen, haben die Unzulänglichkeiten erlernter Modelle jedoch ihren Nutzen für die Planung eingeschränkt.Um diese Herausforderungen zu bewältigen, schlagen wir TreeQN vor, ein differenzierbares, rekursives, baumstrukturiertes Modell, das als Drop-In-Ersatz für jedes Wertfunktionsnetzwerk in Deep RL mit diskreten Aktionen dient. TreeQN konstruiert dynamisch einen Baum, indem es rekursiv ein Übergangsmodell in einem gelernten abstrakten Zustandsraum anwendet und dann die vorhergesagten Belohnungen und Zustandswerte mit Hilfe eines Baum-Backups aggregiert, um die Q-Werte zu schätzen.Wir schlagen auch ATreeC vor, eine akteurskritische Variante, die TreeQN mit einer Softmax-Schicht erweitert, um ein stochastisches Policy-Netzwerk zu bilden. Beide Ansätze werden Ende-zu-Ende trainiert, so dass das gelernte Modell für seine tatsächliche Verwendung im Baum optimiert wird.Wir zeigen, dass TreeQN und ATreeC n-step DQN und A2C bei einer Box-Pushing-Aufgabe übertreffen, sowie n-step DQN und Wertvorhersagenetzwerke (Oh et al., Darüber hinaus präsentieren wir Ablationsstudien, die den Effekt verschiedener Hilfsverluste auf das Lernen von Übergangsmodellen demonstrieren.
Multi-Label-Klassifikation (MLC) ist die Aufgabe der Zuweisung einer Reihe von Ziel-Etiketten für eine gegebene Probe.Modellierung der kombinatorischen Label-Interaktionen in MLC wurde eine langwierige Herausforderung.Rekurrente neuronale Netze (RNN) basierte Encoder-Decoder-Modelle haben gezeigt, State-of-the-Art-Leistung für die Lösung von MLC.Allerdings ist die sequentielle Natur der Modellierung Label Abhängigkeiten durch ein RNN begrenzt seine Fähigkeit in parallele Berechnung, Vorhersage dichten Etiketten, und die Bereitstellung interpretierbare Ergebnisse. In diesem Papier schlagen wir Message Passing Encoder-Decoder (MPED) Netzwerke vor, die darauf abzielen, schnelle, genaue und interpretierbare MLC zu liefern. MPED Netzwerke modellieren die gemeinsame Vorhersage von Labels, indem sie alle RNNs in der Encoder-Decoder Architektur durch Message Passing Mechanismen ersetzen und auf autoregressive Inferenz vollständig verzichten.  Die vorgeschlagenen Modelle sind einfach, schnell, genau, interpretierbar und strukturunabhängig (sie können für bekannte oder unbekannte strukturierte Daten verwendet werden). Experimente mit sieben realen MLC-Datensätzen zeigen, dass die vorgeschlagenen Modelle die autoregressiven RNN-Modelle in fünf verschiedenen Metriken übertreffen und die Trainings- und Testzeit deutlich verkürzen.
Bisherige Arbeiten zum few-shot learning haben sich hauptsächlich auf Klassifikation und Reinforcement Learning konzentriert.In diesem Papier schlagen wir ein few-shot meta-learning System vor, das sich ausschließlich auf Regressionsaufgaben konzentriert.Unser Modell basiert auf der Idee, dass der Freiheitsgrad der unbekannten Funktion signifikant reduziert werden kann, wenn sie als Linearkombination einer Reihe von sparsifying Basisfunktionen dargestellt wird. Wir entwerfen ein Basis Function Learner Netzwerk, um die Basisfunktionen für eine Aufgabenverteilung zu kodieren, und ein Weights Generator Netzwerk, um den Gewichtsvektor für eine neue Aufgabe zu generieren, und zeigen, dass unser Modell die aktuellen Meta-Lernmethoden in verschiedenen Regressionsaufgaben übertrifft.
In diesem Papier stellen wir einen neuartigen Ansatz vor, um diese Herausforderung in einer generischen Sequenz-zu-Sequenz (Seq2Seq)-Einstellung anzugehen.Wir schlagen zunächst eine neue Aufgabe vor, Conditional Masked Language Modeling (C-MLM), um eine Feinabstimmung von BERT auf einem Zieltextgenerierungsdatensatz zu ermöglichen.Das feinabgestimmte BERT (d.h., Die fein abgestimmte BERT (d.h. Lehrer) wird dann als zusätzliche Überwachung genutzt, um konventionelle Seq2Seq-Modelle (d.h. Schüler) für die Texterzeugung zu verbessern, Durch die Nutzung der eigenwilligen bidirektionalen Natur von BERT kann die Destillation des von BERT gelernten Wissens auto-regressive Seq2Seq-Modelle dazu ermutigen, vorauszuplanen und eine globale Überwachung auf Sequenzebene für eine kohärente Texterzeugung zu erzwingen.Experimente zeigen, dass der vorgeschlagene Ansatz starke Baselines von Transformer bei mehreren Texterzeugungsaufgaben, einschließlich maschineller Übersetzung (MÜ) und Textzusammenfassung, deutlich übertrifft.Unser vorgeschlagenes Modell erzielt auch neue Spitzenergebnisse bei den IWSLT-MÜ-Datensätzen Deutsch-Englisch und Englisch-Vietnamesisch.
Viele Studien haben nahegelegt, dass dieses Merkmal des menschlichen Sehens aus der Interaktion zwischen Feedforward-Signalen von Bottom-Up-Pfaden des visuellen Kortex und Feedback-Signalen von Top-Down-Pfaden resultiert.Motiviert durch eine solche Interaktion, schlagen wir ein neues neuro-inspiriertes Modell vor, nämlich Convolutional Neural Networks with Feedback (CNN-F).CNN-F erweitert CNN mit einem Feedback-generativen Netzwerk, das Bottom-Up- und Top-Down-Inferenz kombiniert, um eine approximative loopy belief propagation durchzuführen.  Wir zeigen, dass die iterative Inferenz von CNN-F die Entflechtung latenter Variablen über mehrere Schichten hinweg ermöglicht und validieren die Vorteile von CNN-F gegenüber dem Basis-CNN. Unsere experimentellen Ergebnisse deuten darauf hin, dass CNN-F robuster gegenüber Bildverschlechterungen wie Pixelrauschen, Okklusion und Unschärfe ist.  Darüber hinaus zeigen wir, dass das CNN-F in der Lage ist, Originalbilder mit hoher Rekonstruktionsgenauigkeit aus degradierten Bildern wiederherzustellen und dabei vernachlässigbare Artefakte zu erzeugen.
Es wird gezeigt, dass ein CNN vom Typ ResNet ein universeller Approximator ist und seine Ausdrucksfähigkeit nicht schlechter ist als die von vollverknüpften neuronalen Netzen (FNNs) mit einer \textit{block-sparse} Struktur, selbst wenn die Größe jeder Schicht im CNN festgelegt ist. Unser Ergebnis ist allgemein in dem Sinne, dass wir jede Approximationsrate, die durch blocksparse FNNs erreicht wird, automatisch in die von CNNs übersetzen können. Dank der allgemeinen Theorie wird gezeigt, dass das Lernen auf CNNs Optimalität bei der Approximation und Schätzung mehrerer wichtiger Funktionsklassen erfüllt. Als Anwendungen betrachten wir zwei Arten von zu schätzenden Funktionsklassen: die Barron-Klasse und die H\"older-Klasse. Wir beweisen, dass der geclippte empirische Risikominimierungs-(ERM)-Schätzer die gleiche Rate wie FNNs erreichen kann, selbst wenn die Kanalgröße, die Filtergröße und die Breite von CNNs konstant in Bezug auf die Stichprobengröße sind. Unser Beweis basiert auf hochentwickelten Auswertungen der abdeckenden Anzahl von CNNs und der nicht-trivialen Parameter-Reskalierungstechnik zur Kontrolle der Lipschitz-Konstante der zu konstruierenden CNNs.
Generierung der Klassifizierung Gewichte wurde in vielen Meta-Learning-Ansätze für wenige Schuss Bild Klassifizierung aufgrund seiner Einfachheit und Wirksamkeit angewandt.Allerdings argumentieren wir, dass es schwierig ist, die genaue und universelle Klassifizierung Gewichte für alle verschiedenen Abfrage Proben aus sehr wenigen Training samples.In dieser Arbeit führen wir aufmerksame Gewichte Generation für wenige Schuss Lernen über Information Maximization (AWGIM), die aktuellen Probleme durch zwei neue Beiträge adressiert. i) AWGIM generiert verschiedene Klassifikationsgewichte für verschiedene Abfrageproben, indem jede Abfrageprobe die gesamte Unterstützungsmenge berücksichtigt.ii) Um zu garantieren, dass die generierten Gewichte an verschiedene Abfrageproben angepasst sind, formulieren wir das Problem neu, um die untere Schranke der gegenseitigen Information zwischen den generierten Gewichten und der Abfrage sowie den Unterstützungsdaten zu maximieren.Soweit wir sehen können, ist dies der erste Versuch, die Informationsmaximierung mit dem "few shot learning" zu vereinen.
Conversational question answering (CQA) ist eine neuartige QA-Aufgabe, die das Verständnis des Dialogkontextes erfordert. Anders als das traditionelle maschinelle Leseverständnis (MRC) mit nur einem Zug ist CQA eine umfassende Aufgabe, die aus dem Lesen von Passagen, der Auflösung von Koreferenzen und dem Verständnis des Kontextes besteht.In diesem Papier schlagen wir ein innovatives kontextualisiertes aufmerksamkeitsbasiertes tiefes neuronales Netzwerk, SDNet, vor, um Kontext in traditionelle MRC-Modelle zu integrieren. Unser Modell nutzt sowohl die Inter-Attention als auch die Self-Attention, um die Konversation und die Passage zu verstehen.Darüber hinaus demonstrieren wir eine neuartige Methode, um das BERT-Kontextmodell als Untermodul in unser Netzwerk zu integrieren.Empirische Ergebnisse zeigen die Effektivität von SDNet.Auf dem CoQA-Leaderboard übertrifft es die F1-Punktzahl des vorherigen besten Modells um 1,6 %.Unser Ensemble-Modell verbessert die F1-Punktzahl weiter um 2,7 %.
Generative adversarische Netzwerke (GANs) sind einer der beliebtesten Ansätze für das Training generativer Modelle, wobei Varianten von Wasserstein-GANs der Standard-GAN-Formulierung in Bezug auf Lernstabilität und Probenqualität überlegen sind. Training mit einem Regularisierungsterm, der die Verletzung der Lipschitz-Beschränkung explizit bestraft, anstatt durch die Norm des Gradienten, hat sich in den meisten Situationen als praktisch nicht durchführbar erwiesen. Inspiriert von Virtual Adversarial Training schlagen wir eine Methode vor, die Adversarial Lipschitz Regularization genannt wird, und zeigen, dass die Verwendung einer expliziten Lipschitz-Strafe in der Tat praktikabel ist und zu einer konkurrenzfähigen Leistung führt, wenn sie auf Wasserstein-GANs angewendet wird, was eine wichtige Verbindung zwischen Lipschitz-Regularisierung und adversarialem Training aufzeigt.
Multi-Task-Lernen verspricht weniger Daten, Parameter und Zeit zu verbrauchen als das Training von separaten Single-Task-Modellen.Aber die Realisierung dieser Vorteile in der Praxis ist eine Herausforderung.Insbesondere ist es schwierig, eine geeignete Architektur zu definieren, die genug Kapazität hat, um viele Aufgaben zu unterstützen, während sie nicht übermäßig viel Rechenleistung für jede einzelne Aufgabe erfordert.Es gibt schwierige Kompromisse bei der Entscheidung, wie Parameter und Schichten über eine große Menge von Aufgaben zuzuordnen.Um dies zu adressieren, schlagen wir eine Methode für die automatische Suche über Multi-Task-Architekturen vor, die die Ressourcenbeschränkungen berücksichtigt. Wir definieren eine Parametrisierung von Feature-Sharing-Strategien zur effektiven Abdeckung und Auswahl von Architekturen und stellen eine Methode zur schnellen Evaluierung solcher Architekturen mit Feature-Destillation vor. Zusammen ermöglichen uns diese Beiträge eine schnelle Optimierung für parameter-effiziente Multi-Task-Modelle. Wir führen ein Benchmarking mit Visual Decathlon durch und zeigen, dass wir automatisch nach Architekturen suchen und diese identifizieren können, die effektiv Kompromisse zwischen den Anforderungen an die Aufgabenressourcen eingehen und gleichzeitig ein hohes Maß an endgültiger Leistung beibehalten.
Mit der Entwicklung und Diversifizierung verteilter Ansätze zur Semantik natürlicher Sprache spielen Einbettungen für sprachliche Einheiten, die größer als Wörter sind (z.B. Sätze), eine zunehmend wichtige Rolle.  Bislang wurden solche Einbettungen mit Hilfe von Benchmark-Aufgaben (z.B. GLUE) und linguistischen Sonden evaluiert.  Wir schlagen einen vergleichenden Ansatz vor, den Nearest Neighbor Overlap (N2O), der die Ähnlichkeit zwischen Embeddern aufgabenunabhängig quantifiziert.  N2O erfordert nur eine Sammlung von Beispielen und ist einfach zu verstehen: Zwei Einbettungen sind ähnlicher, wenn es für dieselbe Menge von Eingaben eine größere Überlappung zwischen den nächsten Nachbarn der Eingaben gibt.  Wir verwenden N2O, um 21 Satzeinbettungen zu vergleichen und zeigen die Auswirkungen verschiedener Designentscheidungen und Architekturen.
Generative Adversarial Networks (GANs) können bei generativen Modellierungsaufgaben eine hochmoderne Stichprobenqualität erreichen, leiden aber unter dem Problem des Modus-Kollapses.Variational Autoencoders (VAEs) hingegen maximieren explizit eine rekonstruktionsbasierte Datenlog-Likelihood und zwingen sie, alle Modi abzudecken, leiden aber unter einer schlechteren Stichprobenqualität.Jüngste Arbeiten haben hybride VAE-GAN-Frameworks vorgeschlagen, die eine GAN-basierte synthetische Likelihood in das VAE-Ziel integrieren, um sowohl das Problem des Modus-Kollapses als auch das der Stichprobenqualität zu lösen - mit begrenztem Erfolg. Wir schlagen ein neuartiges Ziel mit einem "Best-of-Many-Samples"-Rekonstruktionsaufwand und einer stabilen direkten Schätzung der synthetischen Likelihood vor, das es unserem hybriden VAE-GAN-Rahmenwerk ermöglicht, gleichzeitig eine hohe Datenlog-Likelihood und eine geringe Divergenz zum latenten Prior zu erreichen, und das sowohl gegenüber hybriden VAE-GANS als auch gegenüber einfachen GANs eine signifikante Verbesserung der Modusabdeckung und -qualität zeigt.
Um die menschliche Fähigkeit des kontinuierlichen Wissenserwerbs und -transfers über verschiedene Aufgaben hinweg zu imitieren, muss ein lernendes System die Fähigkeit zum lebenslangen Lernen besitzen und die zuvor erworbenen Fähigkeiten effektiv nutzen, wobei die zentrale Herausforderung darin besteht, das von einer Aufgabe gelernte Wissen auf andere Aufgaben zu übertragen und zu verallgemeinern, um Störungen durch vorheriges Wissen zu vermeiden und die Gesamtleistung zu verbessern. Die Methode verwendet statistische Leverage-Score-Informationen, um die Bedeutung der Datenproben in jeder Aufgabe zu messen und nimmt häufige Richtungen Ansatz, um ein lebenslanges Lernen property.This effektiv unterhält eine konstante Trainingsgröße über alle tasks.We zunächst einige mathematische Intuition für die Methode und dann zeigen ihre Wirksamkeit mit Experimenten auf Varianten von MNIST und CIFAR100 Datensätze.
Wir erweitern den Begriff der Äquivarianz in CNNs durch das Polar Transformer Network (PTN).PTN kombiniert Ideen aus dem Spatial Transformer Network (STN) und kanonischen Koordinatendarstellungen.Das Ergebnis ist ein Netzwerk, das invariant zu Translation und äquivariant zu Rotation und Skala ist. PTN wird Ende-zu-Ende trainiert und besteht aus drei verschiedenen Stufen: einem polaren Ursprungsprädiktor, dem neu eingeführten Polartransformatormodul und einem Klassifikator. PTN erreicht den neuesten Stand der Technik auf gedrehten MNIST- und dem neu eingeführten SIM2MNIST-Datensatz, einer MNIST-Variation, die durch Hinzufügen von Unordnung und Störung von Ziffern durch Translation, Rotation und Skalierung erhalten wird.Die Ideen von PTN sind auf 3D erweiterbar, was wir durch das zylindrische Transformatornetzwerk demonstrieren.
Meta-Lernen ermöglicht es einem intelligenten Agenten, frühere Lernepisoden als Grundlage für eine schnelle Leistungsverbesserung bei einer neuen Aufgabe zu nutzen.Bayesianische hierarchische Modellierung bietet einen theoretischen Rahmen für die Formalisierung von Meta-Lernen als Inferenz für eine Reihe von Parametern, die über Aufgaben hinweg geteilt werden.Hier formulieren wir den modellagnostischen Meta-Lern-Algorithmus (MAML) von Finn et al. (2017) als eine Methode für probabilistische Inferenz in einem hierarchischen Bayesianischen Modell neu. Im Gegensatz zu früheren Methoden für das Meta-Lernen über hierarchische Bayes ist MAML durch die Verwendung eines skalierbaren Gradientenabstiegsverfahrens für die Posterior-Inferenz natürlich auf komplexe Funktionsapproximatoren anwendbar. Darüber hinaus bietet die Identifizierung von MAML als hierarchisches Bayes-Verfahren eine Möglichkeit, die Funktionsweise des Algorithmus als Meta-Learning-Verfahren zu verstehen, sowie die Möglichkeit, Rechenstrategien für eine effiziente Inferenz zu nutzen.Wir nutzen diese Gelegenheit, um eine Verbesserung des MAML-Algorithmus vorzuschlagen, die Techniken der approximativen Inferenz und der Krümmungsschätzung nutzt.
Autostacker verbessert die Vorhersagegenauigkeit von Machine-Learning-Baselines, indem es eine innovative hierarchische Stacking-Architektur und einen effizienten Parametersuchalgorithmus einsetzt. Es ist weder ein vorheriges Domänenwissen über die Daten noch eine Vorverarbeitung von Merkmalen erforderlich. Mit einem von der Natur inspirierten Algorithmus - Parallel Hill Climbing (PHC) - reduzieren wir die Zeit für AutoML erheblich. Durch die Fokussierung auf den Modellierungsprozess bricht Autostacker mit der Tradition, Pipelines fester Reihenfolge zu folgen, indem er nicht nur einzelne Modell-Pipelines, sondern auch innovative Kombinationen und Strukturen erforscht. Wie wir im Abschnitt über Experimente zeigen werden, erzielt Autostacker eine deutlich bessere Leistung sowohl in Bezug auf die Testgenauigkeit als auch auf die Zeitkosten im Vergleich zu ersten Versuchen mit Menschen und dem kürzlich populären AutoML-System.
Surrogatmodelle können verwendet werden, um die approximative Bayes'sche Berechnung (ABC) zu beschleunigen. In einem solchen Rahmen wird die Diskrepanz zwischen simulierten und beobachteten Daten mit einem Gauß-Prozess modelliert. Bisher wurden prinzipielle Strategien nur für die sequentielle Auswahl der Simulationsorte vorgeschlagen. Um diese Einschränkung zu beheben, entwickeln wir optimale Bayes'sche Designstrategien, um die teuren Simulationen zu parallelisieren.
Zufällig initialisierte Optimierungsalgorithmen erster Ordnung sind die Methode der Wahl für die Lösung vieler hochdimensionaler, nicht-konvexer Probleme im Bereich des maschinellen Lernens, jedoch können allgemeine theoretische Garantien die Konvergenz zu kritischen Punkten mit schlechten Zielwerten nicht ausschließen. Für einige hochstrukturierte, nicht-konvexe Probleme kann der Erfolg des Gradientenabstiegs jedoch durch das Studium der Geometrie des Ziels verstanden werden. Die resultierenden Raten skalieren als Polynome niedriger Ordnung in der Dimension, obwohl das Ziel eine exponentielle Anzahl von Sattelpunkten besitzt. Diese effiziente Konvergenz kann als eine Folge der negativen Krümmung normal zu den stabilen Mannigfaltigkeiten, die mit Sattelpunkten verbunden sind, angesehen werden, und wir liefern Beweise dafür, dass dieses Merkmal auch von anderen wichtigen nicht-konvexen Problemen geteilt wird.
Die Codierungstheorie ist eine zentrale Disziplin, die drahtgebundene und drahtlose Modems unterstützt, die die Arbeitspferde des Informationszeitalters sind.Der Fortschritt in der Codierungstheorie wird weitgehend durch individuellen menschlichen Einfallsreichtum mit sporadischen Durchbrüchen im letzten Jahrhundert angetrieben.In diesem Papier untersuchen wir, ob es möglich ist, die Entdeckung von Decodierungsalgorithmen durch tiefes Lernen zu automatisieren.Wir untersuchen eine Familie von sequentiellen Codes, die durch rekurrente neuronale Netzwerkarchitekturen (RNN) parametrisiert sind. Wir zeigen, dass kreativ entworfene und trainierte RNN-Architekturen bekannte sequentielle Codes wie Faltungscodes und Turbocodes mit nahezu optimaler Leistung auf dem Kanal mit additivem weißem Gaußschen Rauschen (AWGN) dekodieren können, was wiederum durch bahnbrechende Algorithmen unserer Zeit erreicht wird (Viterbi- und BCJR-Dekoder, die dynamisches Programmieren und Vorwärts-Rückwärts-Algorithmen repräsentieren).Wir zeigen starke Genrealisierungen, d.h., wir trainieren bei einem bestimmten Signal-Rausch-Verhältnis und einer bestimmten Blocklänge, testen aber bei einem breiten Spektrum dieser Größen, sowie Robustheit und Anpassungsfähigkeit bei Abweichungen von der AWGN-Einstellung.
Es hat sich gezeigt, dass Adam in bestimmten Fällen nicht zur optimalen Lösung konvergiert.Forscher haben kürzlich mehrere Algorithmen vorgeschlagen, um das Problem der Nichtkonvergenz von Adam zu vermeiden, aber ihre Effizienz erweist sich in der Praxis als unbefriedigend.In diesem Beitrag geben wir einen neuen Einblick in das Problem der Nichtkonvergenz von Adam und anderen adaptiven Lernratenmethoden. Wir argumentieren, dass es eine unangemessene Korrelation zwischen dem Gradienten $g_t$ und dem Term des zweiten Moments $v_t$ in Adam gibt ($t$ ist der Zeitschritt), was dazu führt, dass ein großer Gradient wahrscheinlich eine kleine Schrittweite hat, während ein kleiner Gradient eine große Schrittweite haben kann. Wir zeigen, dass solche unausgewogenen Schrittgrößen die grundlegende Ursache für die Nicht-Konvergenz von Adam sind, und wir beweisen weiter, dass die Entkorrelation von $v_t$ und $g_t$ zu einer unvoreingenommenen Schrittgröße für jeden Gradienten führt und somit das Nicht-Konvergenz-Problem von Adam löst.Schließlich schlagen wir AdaShift vor, eine neuartige adaptive Lernratenmethode, die $v_t$ und $g_t$ durch zeitliche Verschiebung entkorreliert, d.h, Die experimentellen Ergebnisse zeigen, dass AdaShift in der Lage ist, das Nicht-Konvergenz-Problem von Adam zu lösen und dabei eine konkurrenzfähige Leistung mit Adam sowohl in Bezug auf die Trainingsgeschwindigkeit als auch auf die Generalisierung aufrechtzuerhalten.
Die meisten Methoden der Domänenanpassung betrachten das Problem der Übertragung von Wissen auf die Zieldomäne aus einem einzigen Quelldatensatz.In praktischen Anwendungen haben wir jedoch in der Regel Zugang zu mehreren Quellen.In diesem Papier schlagen wir den ersten Ansatz für Multi-Source Domain Adaptation (MSDA) auf der Grundlage von Generative Adversarial Networks.Our Methode wird durch die Beobachtung, dass das Aussehen eines bestimmten Bildes hängt von drei Faktoren inspiriert: die Domäne, den Stil (in Bezug auf Low-Level-Features Variationen) und den Inhalt. Aus diesem Grund schlagen wir vor, die Bildmerkmale auf einen Raum zu projizieren, in dem nur die Abhängigkeit vom Inhalt beibehalten wird, und dann diese invariante Darstellung auf den Pixelraum unter Verwendung der Zieldomäne und des Stils neu zu projizieren.Auf diese Weise können neue beschriftete Bilder generiert werden, die verwendet werden, um einen endgültigen Zielklassifikator zu trainieren.Wir testen unseren Ansatz unter Verwendung gängiger MSDA-Benchmarks und zeigen, dass er die modernsten Methoden übertrifft.
Die Ableitung der wahrscheinlichsten Konfiguration für eine Teilmenge von Variablen einer gemeinsamen Verteilung in Anbetracht der verbleibenden Variablen - die wir als Co-Generierung bezeichnen - ist eine wichtige Herausforderung, die für alle außer den einfachsten Einstellungen rechnerisch anspruchsvoll ist.Diese Aufgabe hat eine beträchtliche Menge an Aufmerksamkeit erhalten, insbesondere für klassische Wege der Modellierung von Verteilungen wie strukturierte Vorhersage. Im Gegensatz dazu ist fast nichts über diese Aufgabe bekannt, wenn es um kürzlich vorgeschlagene Techniken zur Modellierung hochdimensionaler Verteilungen geht, insbesondere generative adversarische Netze (GANs).Daher untersuchen wir in diesem Papier die auftretenden Herausforderungen für die Co-Generierung mit GANs.Um diese Herausforderungen anzugehen, entwickeln wir einen annealed importance sampling (AIS) basierten Hamiltonian Monte Carlo (HMC) Co-Generierungsalgorithmus.Der vorgestellte Ansatz übertrifft die klassischen gradientenbasierten Methoden auf synthetischen Daten und auf CelebA deutlich.
Wir entwickeln eine einfache Methode zur Schätzung von Parametern in impliziten Modellen, die keine Kenntnis der Form der Likelihood-Funktion oder abgeleiteter Größen erfordert, aber unter bestimmten Bedingungen äquivalent zur Maximierung der Likelihood ist.Unser Ergebnis gilt in der nicht-asymptotischen parametrischen Einstellung, in der sowohl die Kapazität des Modells als auch die Anzahl der Datenbeispiele endlich sind.Wir zeigen auch ermutigende experimentelle Ergebnisse.
Während sich die meisten Ansätze zum Problem des inversen Verstärkungslernens (IRL) auf die Schätzung einer Belohnungsfunktion konzentrieren, die die Politik oder das gezeigte Verhalten eines Expertenagenten bei einer Kontrollaufgabe am besten erklärt, ist es oft der Fall, dass ein solches Verhalten durch eine einfache Belohnung in Kombination mit einer Reihe von harten Einschränkungen prägnanter dargestellt wird. Wir formulieren das Problem der IRL auf Markov-Entscheidungsprozessen (MDPs) neu, so dass wir bei einem nominalen Modell der Umgebung und einer nominalen Belohnungsfunktion versuchen, Zustands-, Aktions- und Merkmalsbeschränkungen in der Umgebung zu schätzen, die das Verhalten eines Agenten motivieren. Unser Ansatz basiert auf dem Maximum-Entropy-IRL-Rahmen, der es uns ermöglicht, die Wahrscheinlichkeit der Demonstrationen eines Expertenagenten zu bestimmen, wenn wir ein MDP kennen. Mit unserer Methode können wir ableiten, welche Beschränkungen dem MDP hinzugefügt werden können, um die Wahrscheinlichkeit der Beobachtung dieser Demonstrationen zu erhöhen. Wir stellen einen Algorithmus vor, der iterativ die Maximum-Likelihood-Beschränkung ermittelt, die das beobachtete Verhalten am besten erklärt, und wir bewerten seine Wirksamkeit anhand von simuliertem Verhalten und aufgezeichneten Daten von Menschen, die ein Hindernis umfahren.
In dieser Arbeit erörtern wir die erforderlichen Elemente eines Robotersystems, das sich anhand von in der realen Welt gesammelten Daten kontinuierlich und autonom verbessern kann, und schlagen eine besondere Ausführungsform eines solchen Systems vor. Anschließend untersuchen wir eine Reihe von Herausforderungen des Lernens ohne Instrumentierung - einschließlich des Fehlens von episodischen Resets, Zustandsabschätzung, und Hand-engineered Belohnungen - und schlagen einfache, skalierbare Lösungen für diese Herausforderungen.Wir demonstrieren die Wirksamkeit unseres vorgeschlagenen Systems auf geschickte Roboter Manipulation Aufgaben in der Simulation und der realen Welt, und bieten auch eine aufschlussreiche Analyse und Ablation Studie der Herausforderungen im Zusammenhang mit diesem Lernparadigma.
Wir untersuchen das Problem der sprachübergreifenden Sprachkonvertierung in nicht-parallelen Sprachkorpora und in einer One-Shot-Lernumgebung. Die meisten früheren Arbeiten erfordern entweder parallele Sprachkorpora oder eine ausreichende Menge an Trainingsdaten eines Zielsprechers, aber wir konvertieren einen beliebigen Satz eines beliebigen Ausgangssprechers in den des Zielsprechers, wenn nur eine Trainingsäußerung des Zielsprechers vorliegt. Um dies zu erreichen, formulieren wir das Problem als Lernen von entkoppelten sprecherspezifischen und kontextspezifischen Repräsentationen und folgen der Idee von [1], die den Factorized Hierarchical Variational Autoencoder (FHVAE) verwendet. Nach dem Training des FHVAE auf Trainingsdaten mit mehreren Sprechern schätzen wir diese latenten Repräsentationen und rekonstruieren dann die gewünschte Äußerung der konvertierten Stimme in die des Zielsprechers. Wir verwenden ein mehrsprachiges Sprachkorpus, um ein universelles Modell zu erlernen, das für alle Sprachen funktioniert, und untersuchen die Verwendung einer One-Hot-Spracheinbettung, um das Modell auf die Sprache der abgefragten Äußerung zu konditionieren, und zeigen die Effektivität des Ansatzes: Wir führen Stimmumwandlungsexperimente mit unterschiedlich großen Trainingsäußerungen durch, und es war möglich, selbst mit nur einer Trainingsäußerung eine angemessene Leistung zu erzielen. In den subjektiven Tests für die einsprachige und die sprachübergreifende Sprachkonvertierung erzielte unser Ansatz mäßig bessere oder vergleichbare Ergebnisse in Bezug auf Sprachqualität und Ähnlichkeit im Vergleich zur Basislinie.
Wir schlagen ein durchgängig trainierbares Aufmerksamkeitsmodul für Faltungsneuronale Netze (CNN) vor, die für die Bildklassifizierung entwickelt wurden. Das Modul nimmt die 2D-Merkmalsvektorkarten als Eingabe, die die Zwischendarstellungen des Eingabebildes in den verschiedenen Phasen der CNN-Pipeline bilden, und gibt eine 2D-Matrix von Bewertungen für jede Karte aus. Standard-CNN-Architekturen werden durch den Einbau dieses Moduls modifiziert und unter der Bedingung trainiert, dass eine konvexe Kombination der 2D-Merkmalsvektoren, die durch die Score-Matrizen parametrisiert sind, allein für die Klassifizierung verwendet werden muss, wobei ein Anreiz besteht, das Relevante zu verstärken und das Irrelevante oder Irreführende zu unterdrücken. Unsere experimentellen Beobachtungen liefern eindeutige Beweise für diesen Effekt: Die gelernten Aufmerksamkeitskarten heben die interessierenden Regionen deutlich hervor und unterdrücken gleichzeitig die Hintergrundstörung. Bei der Binarisierung übertreffen unsere Aufmerksamkeitskarten andere CNN-basierte Aufmerksamkeitskarten, traditionelle Saliency Maps und Top-Objektvorschläge für schwach überwachte Segmentierung, wie auf dem Object Discovery-Datensatz demonstriert wurde, und wir demonstrieren auch eine verbesserte Robustheit gegenüber der schnellen Gradientenzeichen-Methode für gegnerische Angriffe.
Recurrent neural network (RNN) ist ein effektives neuronales Netzwerk bei der Lösung sehr komplexer überwachter und unbeaufsichtigter Aufgaben.es hat eine erhebliche Verbesserung in RNN Bereich wie natürliche Sprachverarbeitung, Sprachverarbeitung, Computer Vision und andere mehrere domains.this Papier befasst sich mit RNN Anwendung auf verschiedene Anwendungsfälle wie Incident Detection, Fraud Detection, und Android Malware Klassifizierung. Das Netzwerk wird bis zu 1000 Epochen mit einer Lernrate im Bereich von 0,01 bis 0,5 betrieben. Offensichtlich hat RNN im Vergleich zu klassischen maschinellen Lernalgorithmen sehr gut abgeschnitten, was vor allem darauf zurückzuführen ist, dass RNN implizit die zugrundeliegenden Merkmale extrahiert und die Eigenschaften der Daten identifiziert, was zu einer besseren Genauigkeit führt.
Anatomische Studien zeigen, dass das Gehirn die Eingabedaten neu formatiert, um zuverlässige Antworten für die Durchführung von Berechnungen zu generieren, aber es bleibt unklar, wie neuronale Schaltkreise komplexe räumlich-zeitliche Muster kodieren. Wir zeigen, dass die neuronale Dynamik stark von der Phasenausrichtung zwischen dem Input und der spontanen chaotischen Aktivität beeinflusst wird. Die Ausrichtung des Inputs entlang der dominanten chaotischen Projektionen führt dazu, dass die chaotischen Trajektorien zu stabilen Kanälen (oder Attraktoren) werden und somit die Rechenleistung eines rekurrenten Netzwerks verbessern.Mit Hilfe der Analyse des mittleren Feldes leiten wir die Auswirkungen der Ausrichtung des Inputs auf die Gesamtstabilität der gebildeten Attraktoren ab.
Generative adversarial networks (GANs) sind ein Lernrahmen, der sich auf die Ausbildung eines Diskriminators stützt, um ein Maß für die Differenz zwischen einer Ziel- und einer erzeugten Verteilung zu schätzen. GANs, wie sie normalerweise formuliert sind, stützen sich darauf, dass die erzeugten Proben vollständig differenzierbar sind, was die generativen Parameter betrifft, und funktionieren daher nicht für diskrete Daten. Wir stellen eine Methode für das Training von GANs mit diskreten Daten vor, die das geschätzte Differenzmaß aus dem Diskriminator verwendet, um Wichtigkeitsgewichte für die generierten Proben zu berechnen und so einen Policy-Gradienten für das Training des Generators bereitzustellen.Die Wichtigkeitsgewichte haben eine starke Verbindung zur Entscheidungsgrenze des Diskriminators, und wir nennen unsere Methode boundary-seeking GANs (BGANs).Wir demonstrieren die Effektivität des vorgeschlagenen Algorithmus mit diskreten Bildern und zeichenbasierter natürlicher Sprachgenerierung.  Darüber hinaus erstreckt sich das Boundary-Seeking-Ziel auch auf kontinuierliche Daten, was zur Verbesserung der Stabilität des Trainings genutzt werden kann. Wir demonstrieren dies anhand von Celeba, Large-Scene Understanding (LSUN) bedrooms und Imagenet ohne Konditionierung.
Policy-Gradienten-Methoden haben großen Erfolg beim Deep Reinforcement Learning, leiden aber unter der hohen Varianz der Gradientenschätzungen, die sich besonders bei Problemen mit langen Horizonten oder hochdimensionalen Aktionsräumen bemerkbar macht. Um dieses Problem zu entschärfen, leiten wir eine verzerrungsfreie handlungsabhängige Basislinie zur Varianzreduktion ab, die die strukturelle Form der stochastischen Politik selbst vollständig ausnutzt und keine zusätzlichen Annahmen über das MDP macht. Wir demonstrieren und quantifizieren den Nutzen der handlungsabhängigen Basislinie sowohl durch theoretische Analyse als auch durch numerische Ergebnisse, einschließlich einer Analyse der Suboptimalität der optimalen zustandsabhängigen Basislinie. Unsere experimentellen Ergebnisse zeigen, dass aktionsabhängige Baselines ein schnelleres Lernen bei Standard-Benchmarks für Reinforcement Learning und bei hochdimensionalen Handmanipulations- und synthetischen Aufgaben ermöglichen. Schließlich zeigen wir, dass die allgemeine Idee, zusätzliche Informationen in Baselines einzubeziehen, um die Varianzreduktion zu verbessern, auf teilweise beobachtete und Multi-Agenten-Aufgaben ausgeweitet werden kann.
Das Problem wird noch verschärft, wenn das überwachte Lernen auf eine Reihe von korrelierten Aufgaben gleichzeitig angewendet wird, da die Menge der erforderlichen Beschriftungen mit der Anzahl der Aufgaben skaliert.Um dieses Problem zu entschärfen, schlagen wir einen aktiven Multitasking-Lernalgorithmus vor, der einen Wissenstransfer zwischen den Aufgaben erreicht. Unser Ansatz reduziert die Anzahl der Abfragen, die während des Trainings benötigt werden, während eine hohe Genauigkeit bei den Testdaten beibehalten wird. Empirische Ergebnisse auf Benchmark-Datensätzen zeigen signifikante Verbesserungen sowohl bei der Genauigkeit als auch bei der Anzahl der Abfragen.
Die Erkennung von Fotomanipulationen beruht auf subtilen statistischen Spuren, die bekanntermaßen durch aggressive verlustbehaftete Online-Komprimierung entfernt werden, und wir zeigen, dass die End-to-End-Modellierung komplexer Foto-Verbreitungskanäle eine Codec-Optimierung mit expliziten Provenance-Zielen ermöglicht. Wir entwerfen einen leichtgewichtigen, trainierbaren, verlustbehafteten Bildcodec, der eine wettbewerbsfähige Ratenverzerrungsleistung liefert, die mit den besten von Hand entwickelten Alternativen vergleichbar ist, aber auf modernen GPU-fähigen Plattformen einen geringeren Rechenaufwand hat.Unsere Ergebnisse zeigen, dass signifikante Verbesserungen in der Manipulationserkennungsgenauigkeit bei geringen Kosten für Bandbreite/Speicher möglich sind.Unser Codec verbesserte die Genauigkeit von 37% auf 86% selbst bei sehr niedrigen Bitraten, die weit unter der Praktikabilität von JPEG (QF 20) liegen.
Rekurrente neuronale Netze sind seit langem die vorherrschende Wahl für die Sequenzmodellierung, haben jedoch zwei gravierende Probleme: Sie sind nicht in der Lage, sehr langfristige Abhängigkeiten zu erfassen, und sie sind nicht in der Lage, die sequenzielle Berechnungsprozedur zu parallelisieren, weshalb in letzter Zeit viele nicht-rekurrente Sequenzmodelle vorgeschlagen wurden, die auf Faltung und Aufmerksamkeitsoperationen basieren. Trotz ihres Erfolges fehlt es diesen Modellen jedoch an den notwendigen Komponenten, um lokale Strukturen in Sequenzen zu modellieren, und sie verlassen sich stark auf Positionseinbettungen, die nur begrenzte Auswirkungen haben und einen beträchtlichen Designaufwand erfordern.In diesem Papier schlagen wir den R-Transformer vor, der sowohl die Vorteile von RNNs als auch des Multi-Head-Attention-Mechanismus genießt und gleichzeitig ihre jeweiligen Nachteile vermeidet. Das vorgeschlagene Modell kann sowohl lokale Strukturen als auch globale langfristige Abhängigkeiten in Sequenzen effektiv erfassen, ohne dass Positionseinbettungen verwendet werden. Wir evaluieren R-Transformer durch umfangreiche Experimente mit Daten aus einer Vielzahl von Domänen und die empirischen Ergebnisse zeigen, dass R-Transformer die State-of-the-Art-Methoden in den meisten Aufgaben weit übertrifft.
In dieser Arbeit schlagen wir einen strukturierten latent-variablen Ansatz vor, der diskrete Kontrollzustände innerhalb eines standardmäßigen autoregressiven neuronalen Paradigmas hinzufügt. Im Rahmen dieser Formulierung können wir eine Reihe von reichhaltigen, posterioren Einschränkungen einbeziehen, um aufgabenspezifisches Wissen zu erzwingen, das effektiv in das neuronale Modell trainiert wird. In Experimenten werden Anwendungen dieses Ansatzes für die Texterzeugung und Part-of-Speech-Induktion untersucht. Bei der Erzeugung natürlicher Sprache zeigt sich, dass diese Methode besser ist als Standard-Benchmarks und gleichzeitig eine feinkörnige Steuerung ermöglicht.
Angenommen, ein tiefes Klassifizierungsmodell wird mit Proben trainiert, die aus Gründen des Datenschutzes oder der Vertraulichkeit geheim gehalten werden müssen: Kann ein Angreifer die privaten Proben erhalten, wenn das Klassifizierungsmodell an den Angreifer weitergegeben wird?wir bezeichnen dieses Reverse-Engineering gegen das Klassifizierungsmodell als Classifier-to-Generator (C2G)-Angriff.diese Situation tritt auf, wenn das Klassifizierungsmodell in mobile Geräte für Offline-Vorhersagen eingebettet wird (z. B. Objekterkennung für das automatisch fahrende Auto und die Gegen den C2G-Angriff führen wir ein neuartiges GAN ein, PreImageGAN. In PreImageGAN ist der Generator so konzipiert, dass er die Stichprobenverteilung schätzt, die durch das Vorabbild des Klassifikationsmodells $f$, $P(X|f(X)=y)$, bedingt ist, wobei $X$ die Zufallsvariable im Stichprobenraum und $y$ der Wahrscheinlichkeitsvektor ist, der das vom Gegner willkürlich festgelegte Ziellabel repräsentiert.In Experimenten zeigen wir, dass PreImageGAN erfolgreich bei der Erkennung von handgeschriebenen Zeichen und der Gesichtserkennung funktioniert. Bei der Zeichenerkennung zeigen wir, dass PreImageGAN es dem Gegner ermöglicht, aus einem Erkennungsmodell für handgeschriebene Ziffern Bilder von Buchstaben zu extrahieren, ohne zu wissen, dass das Modell für Bilder von Buchstaben erstellt wurde.Bei der Gesichtserkennung zeigen wir, dass PreImageGAN es dem Gegner ermöglicht, aus einem Gesichtserkennungsmodell für eine Gruppe von Personen Bilder von bestimmten Personen zu extrahieren, die in der Gruppe enthalten sind, selbst wenn der Gegner keine Kenntnis über die Gesichter der Personen hat.
Kürzlich wurde gezeigt, dass deutlich weniger Messungen erforderlich sind, wenn die Annahme der Sparsamkeit durch die Annahme ersetzt wird, dass der unbekannte Vektor in der Nähe des Bereichs eines geeignet gewählten generativen Modells liegt.  Insbesondere wurde in (Bora {\em et al.}, 2017) gezeigt, dass etwa $O(k\log L)$ zufällige Gauß-Messungen für eine genaue Wiederherstellung ausreichen, wenn das generative Modell mit $k$-Eingabe begrenzt und $L$-Lipschitz ist, und dass $O(kd \log w)$ Messungen für ReLU-Netze mit $k$-Eingabe, Tiefe $d$ und Breite $w$ ausreichen.  In dieser Arbeit werden entsprechende algorithmusunabhängige untere Schranken für die Stichprobenkomplexität mit Hilfe von Werkzeugen der statistischen Minimax-Analyse aufgestellt.  In Übereinstimmung mit den obigen oberen Schranken lassen sich unsere Ergebnisse wie folgt zusammenfassen: (i) Wir konstruieren ein generatives $L$-Lipschitz-Modell, das in der Lage ist, gruppenparse Signale zu erzeugen, und zeigen, dass die resultierende Anzahl von Messungen $\Omega(k \log L)$ beträgt;(ii) Unter Verwendung ähnlicher Ideen konstruieren wir zweischichtige ReLU-Netze mit hoher Breite, die $\Omega(k \log w)$-Messungen erfordern, sowie tiefe ReLU-Netze mit geringerer Breite, die $\Omega(k d)$-Messungen erfordern.  Als Ergebnis stellen wir fest, dass die in (Bora {\em et al.}, 2017) abgeleiteten Skalierungsgesetze optimal oder nahezu optimal sind, wenn keine weiteren Annahmen getroffen werden.
Hier untersuchen wir, ob modernes Deep Reinforcement Learning verwendet werden kann, um Agenten zu trainieren, um kausale Schlussfolgerungen zu ziehen.Wir verwenden einen Meta-Lernansatz, bei dem der Agent eine Strategie für die Durchführung von Experimenten durch kausale Interventionen lernt, um eine nachfolgende Aufgabe zu unterstützen, die genaue kausale Schlussfolgerungen belohnt.Wir fanden auch heraus, dass der Agent in der Lage ist, anspruchsvolle kontrafaktische Vorhersagen zu machen und zu lernen, kausale Schlussfolgerungen aus reinen Beobachtungsdaten zu ziehen. Unsere Ergebnisse deuten darauf hin, dass kausale Schlussfolgerungen in komplexen Umgebungen von leistungsstarken lernbasierten Ansätzen profitieren können. Allgemeiner gesagt, kann diese Arbeit neue Strategien für die strukturierte Erforschung von Verstärkungslernen bieten, indem sie Agenten die Möglichkeit gibt, Experimente durchzuführen und zu interpretieren.
Die Klassifizierung von Gefühlen ist ein aktives Forschungsgebiet mit verschiedenen Anwendungen, darunter die Analyse politischer Meinungen, die Klassifizierung von Kommentaren, Filmrezensionen, Nachrichten und Produktrezensionen.Um eine regelbasierte Gefühlsklassifizierung anzuwenden, benötigen wir Gefühlslexika. Um die manuelle Entwicklungszeit und die Kosten zu umgehen, haben wir versucht, amharische Sentiment-Lexika zu erstellen, die sich auf einen korpusbasierten Ansatz stützen.Die Absicht dieses Ansatzes ist es, Sentiment-Begriffe, die spezifisch für die amharische Sprache sind, aus dem amharischen Korpus zu verarbeiten.Eine kleine Menge von Seed-Begriffen wird manuell aus drei Teilen der Sprache wie Substantiv, Adjektiv und Verb vorbereitet.Wir haben Algorithmen entwickelt, um amharische Sentiment-Lexika automatisch aus dem amharischen Nachrichtenkorpus zu erstellen. Zunächst erstellen wir eine Wort-Kontext-Unigramm-Häufigkeitsmatrix und wandeln sie in eine Matrix mit punktweiser gegenseitiger Information um. Anhand dieser Matrix berechnen wir den Kosinusabstand zwischen dem Mittelwertvektor der Seed-Listen und jedem Wort im Korpusvokabular. Basierend auf dem Schwellenwert werden die Wörter, die dem Mittelwertvektor der Seed-Liste am nächsten liegen, dem Lexikon hinzugefügt. Dann wird der Mittelwertvektor der neuen Sentiment-Seed-Liste aktualisiert und der Prozess wird so lange wiederholt, bis genügend Begriffe im Lexikon vorhanden sind. Durch die Verwendung von PPMI mit einem Schwellenwert von 100 und 200 haben wir korpusbasierte amharische Sentiment-Lexika der Größe 1811 bzw. 3794 erhalten, indem wir 519 Seeds erweitert haben.Schließlich wird das mit dem korpusbasierten Ansatz generierte Lexikon evaluiert.
Optimistische Initialisierung ist eine effektive Strategie für effiziente Exploration beim Reinforcement Learning (RL), auf die sich alle nachweislich effizienten modellfreien Algorithmen stützen, während modellfreie tiefe RL-Algorithmen keine optimistische Initialisierung verwenden, obwohl sie von diesen nachweislich effizienten tabellarischen Algorithmen inspiriert sind. Insbesondere in Szenarien mit nur positiven Belohnungen werden die Q-Werte aufgrund der üblicherweise verwendeten Netzwerkinitialisierungsschemata mit den niedrigsten möglichen Werten initialisiert, was eine pessimistische Initialisierung darstellt.Die bloße Initialisierung des Netzwerks, um optimistische Q-Werte auszugeben, reicht nicht aus, da wir nicht sicherstellen können, dass sie für neue Zustands-Aktions-Paare optimistisch bleiben, was für die Exploration entscheidend ist. Wir schlagen eine einfache, auf der Anzahl basierende Erweiterung der pessimistisch initialisierten Q-Werte vor, die die Quelle des Optimismus vom neuronalen Netz trennt, und zeigen, dass dieses Schema in der tabellarischen Umgebung nachweislich effizient ist und erweitern es auf die tiefe RL-Einstellung. Unser Algorithmus, Optimistic Pessimistically Initialised Q-Learning (OPIQ), ergänzt die Q-Wert-Schätzungen eines DQN-basierten Agenten mit von der Anzahl abgeleiteten Boni, um Optimismus sowohl bei der Aktionsauswahl als auch beim Bootstrapping zu gewährleisten. Wir zeigen, dass OPIQ nicht-optimistische DQN-Varianten, die eine Pseudoanzahl-basierte intrinsische Motivation verwenden, bei schwierigen Explorationsaufgaben übertrifft und dass es optimistische Schätzungen für neuartige Zustands-Aktions-Paare vorhersagt.
Modellfreie Deep Reinforcement Learning (RL)-Algorithmen haben sich in einer Reihe von anspruchsvollen Entscheidungs- und Steuerungsaufgaben bewährt, leiden jedoch typischerweise unter zwei großen Herausforderungen: einer sehr hohen Komplexität der Stichproben und brüchigen Konvergenzeigenschaften, die eine sorgfältige Abstimmung der Hyperparameter erfordern. Beide Herausforderungen schränken die Anwendbarkeit solcher Methoden auf komplexe, reale Domänen stark ein.In diesem Papier schlagen wir Soft Actor-Critic vor, einen Off-Policy Actor-Critic Deep RL-Algorithmus, der auf dem Maximum Entropy Reinforcement Learning Framework basiert. In diesem Rahmen zielt der Akteur darauf ab, die erwartete Belohnung zu maximieren und gleichzeitig die Entropie zu maximieren, d.h. die Aufgabe erfolgreich zu bewältigen und dabei so zufällig wie möglich zu agieren.Frühere Deep-RL-Methoden, die auf diesem Rahmen basieren, wurden entweder als Off-Policy-Q-Learning- oder On-Policy-Policy-Gradienten-Methoden formuliert. Durch die Kombination von Off-Policy-Updates mit einer stabilen stochastischen Actor-Critic-Formulierung erreicht unsere Methode bei einer Reihe von kontinuierlichen Kontroll-Benchmark-Aufgaben eine State-of-the-Art-Performance und übertrifft damit frühere On-Policy- und Off-Policy-Methoden.Darüber hinaus zeigen wir, dass unser Ansatz im Gegensatz zu anderen Off-Policy-Algorithmen sehr stabil ist und eine sehr ähnliche Performance bei verschiedenen zufälligen Seeds erreicht.
Die gebräuchlichsten Ansätze in diesem Zusammenhang sind Behaviour Cloning (BC) und Inverse Reinforcement Learning (IRL). Neuere IRL-Methoden haben gezeigt, dass sie in der Lage sind, effektive Strategien mit einer sehr begrenzten Anzahl von Demonstrationen zu erlernen - ein Szenario, in dem BC-Methoden oft versagen. Leider bietet ein direkter Vergleich der Algorithmen für diese Methoden keine adäquate Intuition, um diesen Leistungsunterschied zu verstehen.Dies ist der motivierende Faktor für unsere Arbeit.Wir beginnen mit der Vorstellung von $f$-MAX, einer Verallgemeinerung von AIRL (Fu et al., 2018), einer modernen IRL-Methode.$f$-MAX bietet eine Grundlage für einen direkteren Vergleich der Ziele für LfD.Wir zeigen, dass $f$-MAX, und durch Vererbung AIRL, eine Teilmenge des kostenregulierten IRL-Rahmens ist, der von Ho & Ermon (2016) dargelegt wurde.Wir schließen mit einer empirischen Bewertung der Faktoren des Unterschieds zwischen verschiedenen LfD-Zielen in der kontinuierlichen Steuerungsdomäne.
Wertbasierte Methoden stellen eine fundamentale Methodik in der Planung und im tiefen Reinforcement Learning (RL) dar.In diesem Papier schlagen wir vor, die zugrundeliegenden Strukturen der State-Action-Wertfunktion, d.h. die Q-Funktion, sowohl für die Planung als auch für das tiefe Reinforcement Learning zu nutzen, Wenn die zugrundeliegende Systemdynamik zu einigen globalen Strukturen der Q-Funktion führt, sollte man in der Lage sein, die Funktion besser abzuleiten, indem man solche Strukturen ausnutzt.Insbesondere untersuchen wir die Low-Rank-Struktur, die weithin für große Datenmatrizen existiert.Wir verifizieren empirisch die Existenz von Low-Rank-Q-Funktionen im Kontext von Kontroll- und tiefen RL-Aufgaben (Atari-Spiele). Unser Hauptbeitrag besteht darin, dass wir durch den Einsatz von Matrix Estimation (ME)-Techniken einen allgemeinen Rahmen vorschlagen, um die zugrundeliegende Low-Rank-Struktur in Q-Funktionen auszunutzen, was zu einem effizienteren Planungsverfahren für die klassische Steuerung führt, und darüber hinaus ein einfaches Schema, das auf alle wertbasierten RL-Techniken angewendet werden kann, um durchgängig eine bessere Leistung bei "Low-Rank"-Aufgaben zu erzielen.Umfangreiche Experimente an Steuerungsaufgaben und Atari-Spielen bestätigen die Wirksamkeit unseres Ansatzes.
Gelernte Repräsentationen von Quellcode ermöglichen es verschiedenen Softwareentwicklungswerkzeugen, z.B., Das Herzstück von Quellcodedarstellungen sind oft Worteinbettungen von Bezeichnernamen im Quellcode, da Bezeichner den Großteil des Quellcodewortschatzes ausmachen und wichtige semantische Informationen vermitteln.Leider gibt es derzeit keine allgemein akzeptierte Methode zur Bewertung der Qualität von Worteinbettungen von Bezeichnern, und aktuelle Bewertungen sind auf bestimmte nachgelagerte Aufgaben ausgerichtet.In diesem Beitrag wird IdBench vorgestellt, der erste Benchmark zur Bewertung, inwieweit Worteinbettungen von Bezeichnern semantische Verwandtschaft und Ähnlichkeit darstellen. Wir verwenden IdBench, um modernste Einbettungstechniken, die für natürliche Sprache vorgeschlagen wurden, eine Einbettungstechnik, die speziell für Quellcode entwickelt wurde, und lexikalische String-Distanzfunktionen zu evaluieren, da diese häufig in aktuellen Entwicklertools verwendet werden.Unsere Ergebnisse zeigen, dass die Effektivität von Einbettungen über verschiedene Einbettungstechniken hinweg erheblich variiert und dass die besten verfügbaren Einbettungen erfolgreich semantische Verwandtschaft darstellen.Auf der anderen Seite bietet keine bestehende Einbettung eine zufriedenstellende Darstellung semantischer Ähnlichkeiten, z.B, Dies kann zu fatalen Fehlern in nachgelagerten Entwicklungswerkzeugen führen. IdBench bietet einen Goldstandard, um die Entwicklung neuer Einbettungen zu leiten, die die derzeitigen Einschränkungen beheben.
Generative adversarische Netze (GANs) werden häufig verwendet, um den Datenerfassungsprozess zu erlernen, und ihre Leistung kann stark von den Verlustfunktionen abhängen, wenn ein begrenztes Rechenbudget zur Verfügung steht.Diese Studie überarbeitet MMD-GAN, das die maximale mittlere Diskrepanz (MMD) als Verlustfunktion für GAN verwendet, und leistet zwei Beiträge. Erstens argumentieren wir, dass die bestehende MMD-Verlustfunktion das Erlernen feiner Details in den Daten behindern kann, da sie versucht, die Diskriminatorausgänge der realen Daten zu kontrahieren.Um dieses Problem anzugehen, schlagen wir eine abstoßende Verlustfunktion vor, um aktiv den Unterschied zwischen den realen Daten zu erlernen, indem wir einfach die Terme in MMD neu anordnen. Zweitens, inspiriert von der Scharnier-Verlust, schlagen wir eine begrenzte Gauß-Kernel, um die Ausbildung von MMD-GAN mit dem repulsiven loss function.The vorgeschlagenen Methoden sind auf die unbeaufsichtigte Bilderzeugung Aufgaben auf CIFAR-10, STL-10, CelebA und LSUN Schlafzimmer Datensätze angewendet. Die Ergebnisse zeigen, dass die repulsive Verlustfunktion die MMD-Verlustfunktion ohne zusätzliche Rechenkosten erheblich verbessert und andere repräsentative Verlustfunktionen übertrifft. Die vorgeschlagenen Methoden erreichen einen FID-Score von 16,21 auf dem CIFAR-10-Datensatz unter Verwendung eines einzelnen DCGAN-Netzwerks und spektraler Normalisierung.
Leider sind die meisten aktuellen tiefen neuronalen Netzwerke enorme Cloud-basierte Strukturen, die viel Speicherplatz benötigen, was die Skalierung von Deep Learning as a Service (DLaaS) und die Verwendung für erweiterte Intelligenz auf Geräten einschränkt.  Die grundlegende Einsicht, die eine geringere Rate als bei naiven Ansätzen ermöglicht, ist die Erkenntnis, dass die bipartiten Graphenschichten von Feedforward-Netzwerken eine Art Permutationsinvarianz in Bezug auf die Beschriftung der Knoten haben, was die Inferenzoperationen angeht, und dass die Inferenzoperationen lokal von den direkt mit ihnen verbundenen Kanten abhängen.Wir liefern auch experimentelle Ergebnisse unseres Ansatzes auf dem MNIST-Datensatz.
Generative adversarial networks (GANs) bilden eine generative Modellierung Ansatz für die Herstellung von ansprechenden Proben bekannt, aber sie sind bemerkenswert schwierig zu trainieren.eine gemeinsame Möglichkeit, dieses Problem anzugehen wurde, um neue Formulierungen der GAN Ziel vorzuschlagen.jedoch überraschend wenige Studien haben sich mit Optimierungsmethoden für diese adversarial training.in dieser Arbeit, gießen wir GAN Optimierungsprobleme in der allgemeinen Variationsungleichheit Rahmen. Indem wir die Literatur zur mathematischen Programmierung anzapfen, begegnen wir einigen verbreiteten Missverständnissen über die Schwierigkeiten der Sattelpunkt-Optimierung und schlagen vor, Methoden, die für Variationsungleichungen entwickelt wurden, auf das Training von GANs auszuweiten.Wir wenden Mittelwertbildung, Extrapolation und eine rechnerisch günstigere Variante, die wir Extrapolation aus der Vergangenheit nennen, auf die stochastische Gradientenmethode (SGD) und Adam an.
Um mit kleinen Datenmengen effizient auf neue Aufgaben zu lernen, überträgt Meta-Learning das Wissen aus früheren Aufgaben auf die neuen Aufgaben. Eine kritische Herausforderung beim Meta-Learning ist jedoch die Heterogenität der Aufgaben, die von traditionellen, global verteilten Meta-Learning-Methoden nicht gut bewältigt werden kann. Darüber hinaus können aktuelle aufgabenspezifische Meta-Learning-Methoden entweder unter dem handwerklichen Design der Strukturen leiden oder nicht in der Lage sein, komplexe Beziehungen zwischen den Aufgaben zu erfassen. In diesem Papier schlagen wir, motiviert durch die Art und Weise der Wissensorganisation in Wissensdatenbanken, ein automatisiertes relationales Meta-Learning (ARML) Framework vor, das automatisch die aufgabenübergreifenden Beziehungen extrahiert und den Meta-Wissensgraphen konstruiert. Wenn eine neue Aufgabe ankommt, kann es schnell die relevanteste Struktur finden und das gelernte Strukturwissen auf den Meta-Lerner zuschneiden. Als Ergebnis adressiert der vorgeschlagene Rahmen nicht nur die Herausforderung der Aufgabenheterogenität durch einen gelernten Meta-Wissensgraphen, sondern erhöht auch die Modellinterpretierbarkeit. Wir führen umfangreiche Experimente zur 2D-Spielzeugregression und zur Klassifizierung von Bildern mit wenigen Aufnahmen durch, und die Ergebnisse zeigen die Überlegenheit von ARML gegenüber dem Stand der Technik auf.
In diesem Papier wird ein Deep-Boosting-Algorithmus entwickelt, um einen diskriminanteren Ensemble-Klassifikator zu erlernen, indem eine Reihe von tiefen Basis-CNNs (Basisexperten) mit unterschiedlichen Fähigkeiten nahtlos kombiniert werden, z. B. werden diese tiefen Basis-CNNs nacheinander trainiert, um eine Reihe von Objektklassen auf eine einfache bis schwierige Weise entsprechend ihrer Lernkomplexität zu erkennen.
Diese Methode basiert auf dem unbeaufsichtigten Training eines Multi-Domain-Wavennet-Auto-Codierers mit einem gemeinsamen Codierer und einem domänenunabhängigen latenten Raum, der Ende-zu-Ende auf Wellenformen trainiert wird.Durch die Verwendung eines vielfältigen Trainingsdatensatzes und einer großen Netzkapazität ermöglicht es der einzelne Codierer, auch Musikdomänen zu übersetzen, die beim Training nicht gesehen wurden. Wir evaluieren unsere Methode anhand eines Datensatzes, der von professionellen Musikern gesammelt wurde, und erzielen überzeugende Übersetzungen. Wir untersuchen auch die Eigenschaften der erhaltenen Übersetzung und demonstrieren, dass wir sogar von einer Pfeife übersetzen können, was möglicherweise die Erstellung von Instrumentalmusik durch ungeübte Menschen ermöglicht.
In der Realität ist der spezifische Angriff selten im Voraus bekannt, und die Angreifer können Bilder auf eine Art und Weise verändern, die außerhalb eines festen Verzerrungsmodells liegt; zum Beispiel liegen gegnerische Drehungen außerhalb der Menge der L_p-begrenzten Verzerrungen.In dieser Arbeit plädieren wir für die Messung der Robustheit gegenüber einem viel breiteren Spektrum unvorhergesehener Angriffe, Angriffe, deren genaue Form während der Entwicklung der Verteidigung unbekannt ist. Wir schlagen mehrere neue Angriffe und eine Methode zur Bewertung einer Verteidigung gegen eine Vielzahl von unvorhergesehenen Verzerrungen vor: Zunächst konstruieren wir neuartige JPEG-, Fog-, Gabor- und Snow-Verzerrungen, um unterschiedlichste Angreifer zu simulieren, und führen dann UAR ein, eine zusammenfassende Metrik, die die Robustheit einer Verteidigung gegen eine bestimmte Verzerrung misst.  Wir stellen fest, dass die Bewertung gegen bestehende L_p-Angriffe redundante Informationen liefert, die sich nicht auf andere Angriffe verallgemeinern lassen; wir empfehlen stattdessen eine Bewertung gegen unsere wesentlich vielfältigeren Angriffe.  Diese Ergebnisse unterstreichen die Notwendigkeit, die Robustheit gegenüber unvorhergesehenen Verzerrungen zu bewerten und zu untersuchen.
Tiefe neuronale Netze (DNNs) haben sich in diesem Jahr als leistungsfähiger Ansatz erwiesen, indem sie seit langem bestehende überwachte und nicht überwachte Aufgaben der Künstlichen Intelligenz (KI) in der Verarbeitung natürlicher Sprache, der Sprachverarbeitung, der Computer Vision und anderen Bereichen lösen: Diese Anwendungsfälle sind Teil der Cybersecurity Data Mining Competition (CDMC) 2017.Die effiziente Netzwerkarchitektur für DNNs wird durch die Durchführung von verschiedenen Pfaden von Experimenten für Netzwerk-Parameter und Netzwerk-Strukturen ausgewählt.Die Experimente von solchen ausgewählten effizienten Konfigurationen von DNNs sind bis zu 1000 Epochen mit Lernrate im Bereich [0,01-0,5] laufen. Die Experimente mit DNNs schnitten im Vergleich zu klassischen maschinellen Lernalgorithmen in allen Anwendungsfällen der Cybersicherheit gut ab, was darauf zurückzuführen ist, dass DNNs implizit bessere Merkmale extrahieren und aufbauen und die Eigenschaften der Daten identifizieren, die zu einer höheren Genauigkeit führen. Die beste Genauigkeit, die DNNs und XGBoost bei der Klassifizierung von Android-Malware erreicht haben, liegt bei 0,940 bzw. 0,741, bei der Erkennung von Vorfällen bei 1,00 bzw. 0,997 und bei der Erkennung von Betrug bei 0,972 bzw. 0,916. Die von DNNs erreichte Genauigkeit variiert um -0,05 %, +0,02 % und -0,01 % von der des am besten bewerteten Systems bei den CDMC 2017-Aufgaben.
Aktuelle Ansätze zur Zerlegung von Demonstrationen in Primitive gehen oft von manuell definierten Primitiven aus und umgehen die Schwierigkeit, diese Primitive zu entdecken.Auf der anderen Seite setzen Ansätze zur Entdeckung von Primitiven restriktive Annahmen über die Komplexität eines Primitivs, die die Anwendbarkeit auf enge Aufgaben beschränken.Unser Ansatz versucht, diese Herausforderungen zu umgehen, indem sowohl die zugrundeliegenden motorischen Primitive als auch die Neuzusammensetzung dieser Primitive gemeinsam gelernt werden, um die ursprüngliche Demonstration zu bilden. Wir zeigen sowohl qualitativ als auch quantitativ, dass unsere gelernten Primitive semantisch bedeutsame Aspekte einer Demonstration erfassen, was es uns ermöglicht, diese Primitive in einem hierarchischen Verstärkungslern-Setup zu komponieren, um robotische Manipulationsaufgaben wie Greifen und Schieben effizient zu lösen.
Die Verwendung moderner Deep-Learning-Modelle, um Vorhersagen über Zeitreihendaten von tragbaren Sensoren zu treffen, erfordert in der Regel große Mengen an markierten Daten, deren Markierung jedoch sowohl mühsam als auch kostspielig sein kann.In diesem Papier wenden wir schwache Überwachung auf Zeitreihendaten an und markieren programmatisch einen Datensatz von Sensoren, die von Parkinson-Patienten getragen werden.Wir haben dann ein LSTM-Modell erstellt, das vorhersagt, wann diese Patienten klinisch relevantes Freezing-Verhalten zeigen (Unfähigkeit, effektive Vorwärtsschritte zu machen). Wir zeigen, dass (1) wenn unser Modell mit patientenspezifischen Daten (vorherige Sensorsitzungen) trainiert wird, wir innerhalb von 9 % AUROC eines Modells liegen, das mit handbeschrifteten Daten trainiert wurde, und (2) wenn wir davon ausgehen, dass keine vorherigen Beobachtungen von Probanden vorliegen, unser schwach überwachtes Modell die gleiche Leistung wie handbeschriftete Daten erbringt.Diese Ergebnisse zeigen, dass eine schwache Überwachung dazu beitragen kann, die Notwendigkeit einer mühsamen Handbeschriftung von Zeitserien-Trainingsdaten zu verringern.
Das Erlernen semantischer Korrespondenz zwischen strukturierten Daten (z.B. Slot-Value-Paaren) und zugehörigen Texten ist ein Kernproblem für viele nachgelagerte NLP-Anwendungen, z.B. die Daten-Text-Generierung, Allerdings sind die gesammelten Daten-Text-Paare für das Training in der Regel lose korrespondiert, wo Texte enthalten zusätzliche oder widersprüchliche Informationen im Vergleich zu seinen gepaarten Eingang.In diesem Papier, schlagen wir eine lokale-to-globale Ausrichtung (L2GA) Rahmen zu lernen semantische Korrespondenzen aus lose verwandte Daten-Text-Paare.Zunächst wird eine lokale Ausrichtung Modell auf Multi-Instanz-Lernen basiert auf die semantische Korrespondenzen innerhalb eines Daten-Text-Paar zu bauen. Anschließend wird ein globales Alignment-Modell auf der Grundlage einer speichergesteuerten CRF-Schicht (Conditional Random Field) entwickelt, um die Abhängigkeiten zwischen den Alignments im gesamten Trainingskorpus auszunutzen, wobei der Speicher zur Integration der vom lokalen Alignment-Modell gelieferten Alignment-Hinweise verwendet wird. Experimente mit aktuellen Restaurantdaten zeigen, dass die von uns vorgeschlagene Methode die Ausrichtungsgenauigkeit verbessern kann, und als Nebenprodukt ist unsere Methode auch geeignet, semantisch äquivalente Trainingsdaten-Text-Paare für neuronale Generierungsmodelle zu induzieren.
Durch die Maximierung der Wahrscheinlichkeit guter Handlungen, die von einem Experten-Demonstrator vorgegeben werden, kann überwachtes Imitationslernen wirksame Richtlinien ohne die algorithmische Komplexität und die Optimierungsherausforderungen des Verstärkungslernens erzeugen, allerdings um den Preis, dass ein Expertendemonstrator - typischerweise eine Person - die Demonstrationen vorführen muss. Die Schlüsselbeobachtung, die dies möglich macht, ist, dass in der Multi-Task-Umgebung Trajektorien, die durch eine suboptimale Politik erzeugt werden, immer noch als optimale Beispiele für andere Aufgaben dienen können. Auf der Grundlage dieser Beobachtung schlagen wir einen sehr einfachen Algorithmus zum Erlernen von Verhaltensweisen vor, der ohne Demonstrationen, vom Benutzer bereitgestellte Belohnungsfunktionen oder komplexe Methoden des Verstärkungslernens auskommt: Unsere Methode maximiert einfach die Wahrscheinlichkeit von Handlungen, die der Agent in seinen eigenen früheren Rollouts tatsächlich ausgeführt hat, unter der Voraussetzung, dass das Ziel der Zustand ist, den er tatsächlich erreicht hat. Obwohl verwandte Varianten dieses Ansatzes bereits zuvor für das Nachahmungslernen mit Beispielen vorgeschlagen wurden, stellen wir die erste Instanz dieses Ansatzes als Methode zum Erlernen von Zielerreichungsstrategien von Grund auf vor.Wir präsentieren ein theoretisches Ergebnis, das selbstüberwachtes Nachahmungslernen und Verstärkungslernen miteinander verbindet, sowie empirische Ergebnisse, die zeigen, dass es mit komplexeren Verstärkungslernmethoden bei einer Reihe von anspruchsvollen Zielerreichungsproblemen konkurrieren kann.
Diese Netzwerke haben oft eine große Anzahl von Parametern und benötigen daher viele Daten zum Trainieren.Wenn die Anzahl der Trainingsdaten jedoch klein ist, wird ein Netzwerk mit hoher Flexibilität die Trainingsdaten schnell überanpassen, was zu einer großen Modellvarianz und einer schlechten Generalisierungsleistung führt.Um dieses Problem zu lösen, schlagen wir eine neue Ensemble-Lernmethode namens InterBoost für die Bildklassifikation mit kleinen Stichproben vor. In der Trainingsphase generiert InterBoost zunächst zufällig zwei komplementäre Datensätze, um zwei Basisnetzwerke derselben Struktur separat zu trainieren, und dann zwei weitere komplementäre Datensätze für das weitere Training der Netzwerke durch Interaktion (oder Informationsaustausch) zwischen den beiden zuvor trainierten Basisnetzwerken.Dieser interaktive Trainingsprozess wird iterativ fortgesetzt, bis ein Stoppkriterium erfüllt ist.In der Testphase werden die Ausgaben der beiden Netzwerke kombiniert, um eine endgültige Punktzahl für die Klassifizierung zu erhalten.Eine detaillierte Analyse der Methode wird für ein tieferes Verständnis ihres Mechanismus bereitgestellt.
In diesem Beitrag entwickeln wir ein neuartiges Verfahren zur Erkennung statistischer Interaktionen, die von einem mehrschichtigen neuronalen Netz mit Vorwärtskopplung erfasst werden, indem wir die gelernten Gewichte direkt interpretieren.Je nach den gewünschten Interaktionen kann unsere Methode im Vergleich zum Stand der Technik eine deutlich bessere oder ähnliche Leistung bei der Erkennung von Interaktionen erzielen, ohne einen exponentiellen Lösungsraum möglicher Interaktionen durchsuchen zu müssen. Wir erreichen diese Genauigkeit und Effizienz durch die Beobachtung, dass Interaktionen zwischen Eingangsmerkmalen durch den nicht-additiven Effekt nichtlinearer Aktivierungsfunktionen erzeugt werden und dass interagierende Pfade in Gewichtsmatrizen kodiert werden Wir demonstrieren die Leistung unserer Methode und die Bedeutung der entdeckten Interaktionen durch experimentelle Ergebnisse sowohl auf synthetischen Datensätzen als auch auf realen Anwendungsdatensätzen.
Das neuronale lineare Modell ist ein einfaches adaptives Bayes'sches lineares Regressionsverfahren, das in letzter Zeit für eine Reihe von Problemen verwendet wurde, die von Bayes'scher Optimierung bis hin zu Reinforcement Learning reichen. In dieser Arbeit charakterisieren wir diese auf den UCI-Datensätzen, einem beliebten Benchmark für Bayes'sche Regressionsmodelle, sowie auf den kürzlich eingeführten ''Gap''-Datensätzen, die bessere Tests für Unsicherheiten außerhalb der Verteilung darstellen, und zeigen, dass das neuronale lineare Modell eine einfache Methode ist, die bei diesen Aufgaben konkurrenzfähig ist.
In diesem Beitrag stellen wir eine Fallstudie vor, in der wir die Ergebnisse eines bahnbrechenden Algorithmus, AlphaZero, reproduzieren, einem Reinforcement-Learning-System, das lernt, wie man Go auf einem übermenschlichen Niveau spielt, wenn man nur die Spielregeln kennt.Wir beschreiben Minigo, eine Reproduktion des AlphaZero-Systems unter Verwendung der öffentlich zugänglichen Google Cloud Platform-Infrastruktur und Google Cloud TPUs. Mit zehn Tagen Training auf 800 Cloud TPUs kann Minigo gleichmäßig gegen LeelaZero und ELF OpenGo spielen, zwei der stärksten öffentlich verfügbaren Go-KIs. Wir diskutieren die Schwierigkeiten bei der Skalierung eines Verstärkungslernsystems und die Überwachungssysteme, die erforderlich sind, um das komplexe Zusammenspiel von Hyperparameterkonfigurationen zu verstehen.
Generative adversarial networks (GANs) trainieren implizite generative Modelle durch das Lösen von Minimax-Problemen, die als nicht-konvex-nicht-konkav bekannt sind und für die die Dynamik von Methoden erster Ordnung nicht gut verstanden wird. In diesem Papier betrachten wir GANs in der Art der integralen Wahrscheinlichkeitsmetriken (IPMs) mit dem Generator, der durch ein überparametrisiertes neuronales Netz dargestellt wird, wenn der Diskriminator in jeder Iteration annähernd optimal gelöst wird. Darüber hinaus beweisen wir, dass der erhaltene stationäre Punkt einem Generator entspricht, der eine Verteilung liefert, die in Bezug auf die Gesamtvariation nahe an der Verteilung der beobachteten Daten liegt, wenn die Breite des Generatorennetzwerks ausreichend groß ist und die Klasse der Diskriminatorfunktion eine ausreichende Unterscheidungsfähigkeit besitzt.
Wir stellen Algorithmen zur Einbettung von Netzwerken vor, die Informationen über einen Knoten aus der lokalen Verteilung von Knotenattributen um ihn herum erfassen, wie sie bei zufälligen Spaziergängen beobachtet werden, wobei wir einen Ansatz ähnlich dem Skip-Gram verfolgen.  Wir beweisen theoretisch, dass die Matrizen der punktuellen gegenseitigen Informationen zwischen Knoten und Merkmalen implizit durch die Einbettungen faktorisiert werden. Experimente zeigen, dass unsere Algorithmen robust und rechnerisch effizient sind und vergleichbare Modelle in sozialen, Web- und Zitationsnetzwerk-Datensätzen übertreffen.
Few-shot classiﬁcation zielt darauf ab, einen classiﬁer zu erlernen, um ungesehene Klassen während des Trainings mit begrenzten gelabelten Beispielen zu erkennen.Während signiﬁkante Fortschritte gemacht wurden, machen die wachsende Komplexität von Netzwerkdesigns, Meta-Lernalgorithmen und Unterschiede in der Implementierung Details einen fairen Vergleich difﬁcult. In dieser Arbeit präsentieren wir1) eine konsistente vergleichende Analyse mehrerer repräsentativer Algorithmen zur Klassifizierung von wenigen Aufnahmen, wobei die Ergebnisse zeigen, dass tiefere Backbones den Abstand zwischen den Methoden, einschließlich der Baseline, deutlich verringern, 2) eine leicht modifizierte Basismethode, die überraschenderweise eine konkurrenzfähige Leistung im Vergleich zum Stand der Technik sowohl im MiniImageNet als auch in den CUB-Datensätzen erreicht, und3) eine neue experimentelle Umgebung zur Bewertung der bereichsübergreifenden Generalisierungsfähigkeit von few-shot-Klassifizierungsalgorithmen. Unsere Ergebnisse zeigen, dass die Verringerung der Intraklassenvariation ein wichtiger Faktor ist, wenn das Feature-Backbone flach ist, aber nicht so kritisch, wenn tiefere Backbones verwendet werden.
Temporale Logiken sind nützlich, um dynamisches Systemverhalten zu beschreiben, und wurden erfolgreich als Sprache für Zieldefinitionen während der Aufgabenplanung verwendet.Frühere Arbeiten zur Ableitung temporaler Logikspezifikationen haben sich auf die "Zusammenfassung" des Eingabedatensatzes konzentriert - d.h, In dieser Arbeit untersuchen wir das Problem der Ableitung von Spezifikationen, die zeitliche Unterschiede zwischen zwei Sätzen von Plankurven beschreiben.Wir formalisieren das Konzept der Bereitstellung solcher kontrastiven Erklärungen und stellen dann ein Bayes'sches probabilistisches Modell für die Ableitung kontrastiver Erklärungen als lineare temporal-logische Spezifikationen vor.Wir demonstrieren die Wirksamkeit, Skalierbarkeit und Robustheit unseres Modells für die Ableitung korrekter Spezifikationen in verschiedenen Benchmark-Planungsdomänen und für eine simulierte Luftkampfmission.
Wir verwenden die tropische Geometrie, eine neue Entwicklung im Bereich der algebraischen Geometrie, um die Entscheidungsgrenzen eines einfachen neuronalen Netzes der Form (Affin, ReLU, Affin) zu charakterisieren und zu verstehen. Insbesondere zeigen wir, dass die Entscheidungsgrenzen eine Teilmenge einer tropischen Hyperfläche sind, die eng mit einem Polytop verbunden ist, das durch die konvexe Hülle von zwei Zonotopen gebildet wird, deren Generatoren präzise Funktionen der Parameter des neuronalen Netzes sind. Dabei schlagen wir eine neue tropische Perspektive für die Lotterielos-Hypothese vor, bei der wir die Auswirkungen verschiedener Initialisierungen auf die tropische geometrische Darstellung der Entscheidungsgrenzen sehen. Wir untersuchen den Einsatz dieser Regularisierer beim Pruning von neuronalen Netzen (Entfernen von Netzparametern, die nicht zur tropischen geometrischen Darstellung der Entscheidungsgrenzen beitragen) und bei der Generierung von Angriffen auf gegnerische Eingaben (mit Eingabestörungen, die explizit die Geometrie der Entscheidungsgrenzen stören, um die Netzvorhersage der Eingabe zu ändern).
Methoden erster Ordnung wie der stochastische Gradientenabstieg (SGD) sind derzeit der Standardalgorithmus für das Training tiefer neuronaler Netze. Methoden zweiter Ordnung werden trotz ihrer besseren Konvergenzrate in der Praxis aufgrund der hohen Rechenkosten für die Berechnung der Informationen zweiter Ordnung selten verwendet. In diesem Papier schlagen wir einen neuartigen Gram-Gauß-Newton (GGN) Algorithmus vor, um tiefe neuronale Netze für Regressionsprobleme mit quadratischen Verlusten zu trainieren. Unsere Methode ist inspiriert von der Verbindung zwischen neuronaler Netzoptimierung und Kernelregression des neuronalen Tangentenkerns (NTK). Im Gegensatz zu typischen Methoden zweiter Ordnung, die in jeder Iteration hohe Rechenkosten verursachen, hat GGN nur einen geringen Overhead im Vergleich zu Methoden erster Ordnung wie SGD. Wir geben auch theoretische Ergebnisse, um zu zeigen, dass für ausreichend große neuronale Netze die Konvergenzrate von GGN quadratisch ist. Darüber hinaus geben wir eine Konvergenzgarantie für den GGN-Algorithmus im Mini-Batch-Verfahren, was unseres Wissens nach das erste Konvergenzergebnis für die Mini-Batch-Version einer Methode zweiter Ordnung auf überparametrisierten neuronalen Netzen ist.Vorläufige Experimente mit Regressionsaufgaben zeigen, dass unser GGN-Algorithmus für das Training von Standardnetzen viel schneller konvergiert und eine bessere Leistung als SGD erreicht.
Aktuelle vortrainierte Satzkodierer erzielen Spitzenergebnisse bei Sprachverstehensaufgaben, aber bedeutet dies, dass sie implizites Wissen über syntaktische Strukturen haben?Wir stellen ein grammatikalisch annotiertes Entwicklungsset für den Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) vor, das wir verwenden, um das grammatikalische Wissen von drei vortrainierten Kodierern zu untersuchen, einschließlich des beliebten OpenAI Transformer (Radford et al., Einige Phänomene, z. B. Modifikation durch Adjunkte, sind für alle Modelle leicht zu erlernen, während andere, z. B. Langstreckenbewegung, nur von Modellen mit starker Gesamtleistung effektiv erlernt werden, und wieder andere, z. B. morphologische Übereinstimmung, werden von kaum einem Modell erlernt.
Bei gleichzeitiger Betrachtung einer endlichen Anzahl von Aufgaben ermöglicht das Multi-Ausgangs-Lernen, die Ähnlichkeit der Aufgaben durch geeignete Regularisierer zu berücksichtigen. Wir schlagen eine Verallgemeinerung der klassischen Einstellung auf ein Kontinuum von Aufgaben vor, indem wir vektorwertige RKHSs verwenden.
Wir analysieren die gemeinsame Wahrscheinlichkeitsverteilung auf die Längen der Vektoren der versteckten Variablen in verschiedenen Schichten eines voll vernetzten tiefen Netzwerks, wenn die Gewichte und Verzerrungen zufällig gemäß Gauß'schen Verteilungen gewählt werden und die Eingabe binärwertig ist. Wir zeigen, dass, wenn die Aktivierungsfunktion einen minimalen Satz von Annahmen erfüllt, die von allen Aktivierungsfunktionen erfüllt werden, die wir kennen und die in der Praxis verwendet werden, dann konvergiert der ``Längenprozess'' mit großer Wahrscheinlichkeit zu einer Längenkarte, die als einfache Funktion der Varianzen der zufälligen Gewichte und Vorspannungen und der Aktivierungsfunktion bestimmt wird, wenn die Breite des Netzes groß wird.
Data Augmentation ist einer der effektivsten Ansätze zur Verbesserung der Genauigkeit von modernen Machine-Learning-Modelle, und es ist auch unerlässlich, um eine tiefe Modell für Meta-Learning zu trainieren.Allerdings sind die meisten aktuellen Daten Augmentation Implementierungen in Meta-Learning angewendet die gleichen wie die in der konventionellen Bildklassifikation verwendet.In diesem Papier stellen wir eine neue Daten Augmentation Methode für Meta-Learning, die als ``Task Level Data Augmentation'' (Task Aug genannt). Die Grundidee von Task Aug ist es, die Anzahl der Bildklassen zu erhöhen und nicht die Anzahl der Bilder in jeder Klasse.Im Gegensatz dazu, mit einer größeren Anzahl von Klassen, können wir mehr verschiedene Aufgabe Instanzen während des Trainings.Dies ermöglicht es uns, ein tiefes Netzwerk durch Meta-Learning-Methoden mit wenig over-fitting.Experimentelle Ergebnisse zeigen, dass unser Ansatz erreicht state-of-the-art Leistung auf miniImageNet, CIFAR-FS, und FC100 few-shot Lernen Benchmarks.Sobald Papier akzeptiert wird, werden wir den Link zu Code.
In diesem Papier stellen wir ein allgemeines Rahmenwerk für die Destillation von Erwartungen in Bezug auf die Bayes'sche Posterior-Verteilung eines tiefen neuronalen Netzwerks vor und erweitern damit die bisherige Arbeit an einer Methode, die als ``Bayesian Dark Knowledge'' bekannt ist.  Unser verallgemeinerter Rahmen gilt für den Fall von Klassifizierungsmodellen und nimmt als Input die Architektur eines ``Lehrer"-Netzwerks, eine allgemeine Posterior-Erwartung von Interesse und die Architektur eines ``Schüler"-Netzwerks. Die Destillationsmethode führt eine Online-Kompression der ausgewählten Posterior-Erwartung durch, indem sie iterativ generierte Monte-Carlo-Stichproben aus dem Parameter-Posterior des Lehrermodells verwendet.Wir betrachten außerdem das Problem der Optimierung der Architektur des Schülermodells in Bezug auf einen Kompromiss zwischen Genauigkeit, Geschwindigkeit und Speicherplatz. Wir stellen experimentelle Ergebnisse vor, die mehrere Datensätze, Destillationsziele, Lehrermodell-Architekturen und Ansätze zur Suche nach Schülermodell-Architekturen untersuchen, und stellen fest, dass die Destillation in ein Schülermodell mit einer Architektur, die mit dem Lehrermodell übereinstimmt, wie es bei Bayesian Dark Knowledge der Fall ist, zu einer suboptimalen Leistung führen kann, und zeigen schließlich, dass Suchmethoden für Schülermodelle eine signifikant verbesserte Leistung erzielen können.
Variationale Autoencoder (VAEs) haben sich als leistungsstarke Modelle für latente Variablen erwiesen, allerdings kann die Form des approximativen Posteriors die Aussagekraft des Modells einschränken.Kategoriale Verteilungen sind flexible und nützliche Bausteine, z.B. in neuronalen Speicherschichten.Wir stellen den Hierarchischen Diskreten Variationalen Autoencoder (HD-VAE) vor: eine Hierarchie von Variationalen Speicherschichten. Die Concrete/Gumbel-Softmax-Relaxation ermöglicht die Maximierung eines Surrogats der Evidence Lower Bound durch stochastischen Gradientenanstieg.Wir zeigen, dass HD-VAE bei der Verwendung einer begrenzten Anzahl von latenten Variablen die Gauß-Basislinie bei der Modellierung mehrerer binärer Bilddatensätze übertrifft.Die Ausbildung von sehr tiefen HD-VAE bleibt eine Herausforderung aufgrund der Relaxationsverzerrung, die durch die Verwendung eines Surrogat-Ziels induziert wird.Wir stellen eine formale Definition vor und führen eine vorläufige theoretische und empirische Studie der Verzerrung durch.
In diesem Papier schlagen wir eine neuartige Technik zur Verbesserung des stochastischen Gradientenabstiegs (SGD) vor, um tiefe Netzwerke zu trainieren, die wir \emph{PowerSGD} nennen. Die vorgeschlagene PowerSGD-Methode erhöht den stochastischen Gradienten während der Iterationen einfach auf eine bestimmte Potenz $\gamma\in[0,1]$ und führt nur einen zusätzlichen Parameter ein, nämlich den Potenzexponenten $\gamma$ (wenn $\gamma=1$ ist, reduziert sich PowerSGD auf SGD).Wir schlagen ferner PowerSGD mit Momentum vor, das wir als \emph{PowerSGDM} bezeichnen, und liefern eine Konvergenzratenanalyse sowohl für PowerSGD- als auch PowerSGDM-Methoden. Die empirischen Ergebnisse zeigen, dass die vorgeschlagenen PowerSGD- und PowerSGDM-Methoden eine schnellere anfängliche Trainingsgeschwindigkeit als adaptive Gradientenmethoden, eine mit SGD vergleichbare Generalisierungsfähigkeit und eine verbesserte Robustheit gegenüber der Auswahl von Hyperparametern und verschwindenden Gradienten aufweisen.PowerSGD ist im Wesentlichen ein Gradientenmodifikator über eine nichtlineare Transformation und als solcher orthogonal und komplementär zu anderen Techniken zur Beschleunigung gradientenbasierter Optimierung.
Wir entwickeln eine Architektur, die in der Lage ist, eine erstaunlich flexible, aufgabenorientierte motorische Steuerung eines humanoiden Körpers mit relativ hohem DoF zu realisieren, indem wir das Vortraining von Low-Level-Motor-Controllern mit einem aufgabenorientierten High-Level-Controller kombinieren, der zwischen Low-Level-Sub-Policies umschaltet. Das resultierende System ist in der Lage, einen physisch simulierten humanoiden Körper zu steuern, um Aufgaben zu lösen, die eine Kopplung der visuellen Wahrnehmung einer unstabilisierten egozentrischen RGB-Kamera während der Fortbewegung in der Umgebung erfordern. https://youtu.be/fBoir7PNxPk
Durch die Beobachtung, dass ein ReLU-Neuron ein Produkt aus einer linearen Funktion und einem Gate ist (letzteres bestimmt, ob das Neuron aktiv ist oder nicht), wobei beide einen gemeinsam trainierten Gewichtsvektor teilen, schlagen wir vor, die beiden zu entkoppeln.Wir führen GaLU-Netzwerke ein - Netzwerke, in denen jedes Neuron ein Produkt aus einer linearen Einheit, die durch einen trainierten Gewichtsvektor definiert ist, und einem Gate ist, das durch einen anderen Gewichtsvektor definiert ist, der nicht trainiert wird. Wir zeigen, dass GaLU-Netze auf Standarddatensätzen ähnlich wie ReLU-Netze funktionieren und beginnen eine Studie ihrer theoretischen Eigenschaften, die zeigt, dass sie tatsächlich einfacher zu analysieren sind. Wir glauben, dass die weitere Erforschung von GaLU-Netzen für die Entwicklung einer Theorie des Deep Learning fruchtbar sein kann.
Mit ihrem zunehmenden Einsatz in kritischen Anwendungen wird es wichtig, Systeme zu entwickeln, die in der Lage sind, ihre prädiktive Unsicherheit genau zu quantifizieren und diese anomalen Eingaben auszusondern.Im Gegensatz zu Standard-Lernaufgaben gibt es jedoch derzeit keine gut etablierten Leitprinzipien für die Entwicklung von Architekturen, die Unsicherheit genau quantifizieren können.Darüber hinaus sind die allgemein verwendeten OoD-Erkennungsansätze anfällig für Fehler und weisen OoD-Proben manchmal sogar höhere Wahrscheinlichkeiten zu. Um diese Probleme anzugehen, versuchen wir zunächst, Leitprinzipien für den Entwurf von Architekturen zu identifizieren, die Unsicherheit berücksichtigen, indem wir die Neural Architecture Distribution Search (NADS) vorschlagen. Mit dieser Formulierung sind wir in der Lage, ein stochastisches Ausreißer-Erkennungsziel zu optimieren und ein Ensemble von Modellen zu konstruieren, um OoD-Erkennung durchzuführen.
In dieser Arbeit schlagen wir einen neuartigen Ansatz zur Erkennung von Bildausreißern (kurz: IOD) vor, der die modernsten Bildklassifizierer nutzt, um Ausreißer zu entdecken, ohne dass ein markierter Ausreißer verwendet wird. Wir stellen fest, dass das Vertrauen, das ein Faltungsneuronales Netzwerk (CNN) in die Zugehörigkeit eines Bildes zu einer bestimmten Klasse hat, zwar intuitiv als Ausreißermaß für jedes Bild dienen könnte, die direkte Anwendung dieses Vertrauens zur Erkennung von Ausreißern jedoch nicht gut funktioniert. Um dieses Problem zu lösen, schlagen wir einen Deep Neural Forest-basierten Ansatz vor, der die widersprüchlichen Anforderungen einer genauen Klassifizierung von Bildern und der korrekten Erkennung von Ausreißern in Einklang bringt. Unsere Experimente mit mehreren Benchmark-Bilddatensätzen, darunter MNIST, CIFAR-10, CIFAR-100 und SVHN, zeigen die Wirksamkeit unseres IOD-Ansatzes für die Ausreißererkennung, der mehr als 90 % der Ausreißer erfasst, die durch die Injektion eines Bilddatensatzes in einen anderen erzeugt werden, während die Klassifizierungsgenauigkeit des Mehrklassen-Klassifizierungsproblems erhalten bleibt.
In diesem Papier wird CloudLSTM vorgestellt, ein neuer Zweig rekurrenter neuronaler Modelle, der auf die Vorhersage von Datenströmen zugeschnitten ist, die von geospatialen Punktwolkenquellen erzeugt werden, und der einen dynamischen Punktwolkenfaltungsoperator (D-Conv) als Kernkomponente von CloudLSTMs entwickelt, der die Faltung direkt über Punktwolken durchführt und lokale räumliche Merkmale aus Gruppen benachbarter Punkte extrahiert, die verschiedene Elemente der Eingabe umgeben. Dieser Operator erhält die Permutationsinvarianz von Sequenz-zu-Sequenz-Lernverfahren aufrecht, während er die Korrelationen zwischen benachbarten Punkten in jedem Zeitschritt darstellt - ein wichtiger Aspekt beim raumzeitlichen Vorhersagelernen. Der D-Conv-Operator löst die Anforderungen an die Gitterstruktur von bestehenden raumzeitlichen Vorhersagemodellen und kann leicht in traditionelle LSTM-Architekturen mit Sequenz-zu-Sequenz-Lern- und Aufmerksamkeitsmechanismen integriert werden.    Wir wenden unsere vorgeschlagene Architektur auf zwei repräsentative, praktische Anwendungsfälle an, die Punkt-Wolken-Ströme beinhalten, d.h. Vorhersage des Verkehrsaufkommens von Mobilfunkdiensten und Vorhersage von Luftqualitätsindikatoren. Unsere Ergebnisse, die wir mit realen Datensätzen erhalten haben, die in verschiedenen Szenarien für jeden Anwendungsfall gesammelt wurden, zeigen, dass CloudLSTM genaue langfristige Vorhersagen liefert und eine Vielzahl von neuronalen Netzwerkmodellen übertrifft.
Um einen einfachen Zugang zu statistischen Ansätzen für relationale Daten zu ermöglichen, wurden mehrere Methoden zur Einbettung von Wissensgraphen als Komponenten von R^d eingeführt. Wir schlagen TransINT vor, eine neuartige und interpretierbare Methode zur Einbettung von Wissensgraphen, die die Reihenfolge der Implikationen zwischen den Relationen im Einbettungsraum isomorph beibehält.TransINT bildet eine Menge von Entitäten (die durch eine Relation verbunden sind) auf kontinuierliche Mengen von Vektoren ab, die isomorph zu den Implikationen der Relationen geordnet sind. Mit einem neuartigen Schema zur gemeinsamen Nutzung von Parametern ermöglicht TransINT das automatische Training auf fehlenden, aber implizierten Fakten ohne Regelerstellung. Wir erreichen neue State-of-the-Art-Leistungen mit signifikanten Margen in der Link-Vorhersage und Triple-Klassifikation auf FB122-Datensatz, mit gesteigerter Leistung auch auf Testinstanzen, die nicht durch logische Regeln abgeleitet werden können.Die Winkel zwischen den kontinuierlichen Mengen, die von TransINT eingebettet werden, bieten einen interpretierbaren Weg, um semantische Verwandtschaft und Implikationsregeln zwischen Beziehungen zu ermitteln.
Unsupervised Domain Adaptive Object Detection zielt darauf ab, einen robusten Detektor auf der Domain-Shift-Umstand zu lernen, wo die Ausbildung (Quelle) Domäne ist Label-rich mit Bounding Box Annotationen, während die Prüfung (Ziel) Domäne ist Label-agnostisch und die Funktion Verteilungen zwischen Ausbildung und Prüfung Domänen sind unähnlich oder sogar völlig anders. In diesem Papier schlagen wir eine auf Stacked Complementary Losses (SCL) basierende Gradient Detach-Methode vor, die das Erkennungsziel (Kreuzentropie und glatte l1-Regression) als primäres Ziel verwendet und mehrere Hilfsverluste in verschiedenen Netzwerkphasen einfügt, um Informationen aus den Komplementärdaten (Zielbilder) zu nutzen, die bei der Anpassung von Modellparametern an Quell- und Zieldomänen wirksam sein können. Wir argumentieren, dass das konventionelle Training mit primärem Ziel hauptsächlich die Informationen aus der Quelldomäne zur Maximierung der Wahrscheinlichkeit nutzt und die Komplementärdaten in den flachen Schichten der Netzwerke ignoriert, was zu einer unzureichenden Integration innerhalb verschiedener Domänen führt. Wir führen umfassende Experimente mit sieben Datensätzen durch, und die Ergebnisse zeigen, dass unsere Methode mit großem Abstand besser abschneidet als die modernsten Methoden. 37,9 % mAP erreichen wir z. B. von Cityscapes zu FoggyCityscapes und übertreffen damit die bisherige Methode Strong-Weak um 3,6 %.
Während CNNs besonders gut geeignet sind, um eine angemessene Hierarchie von Konzepten aus realen Bildern zu erfassen, sind sie auf Bereiche beschränkt, in denen Daten im Überfluss vorhanden sind. Jüngste Versuche haben versucht, dieses Problem der Datenknappheit zu entschärfen, indem sie ihr ursprüngliches Single-Task-Problem in ein neues Multi-Task-Learning (MTL)-Problem übertragen haben. Das Hauptziel dieses induktiven Transfermechanismus ist es, domänenspezifische Informationen aus verwandten Aufgaben zu nutzen, um die Generalisierung auf die Hauptaufgabe zu verbessern.Während die jüngsten Ergebnisse in der Deep Learning (DL)-Gemeinschaft das vielversprechende Potenzial des Trainings aufgabenspezifischer CNNs in einem Soft-Parameter-Sharing-Rahmen gezeigt haben, ist die Integration der jüngsten DL-Fortschritte zur Verbesserung des Wissensaustauschs immer noch ein offenes Problem.In diesem Papier schlagen wir das Deep Collaboration Network (DCNet) vor, einen neuartigen Ansatz für die Verbindung aufgabenspezifischer CNNs in einem MTL-Rahmen. Basierend auf der Beobachtung, dass die Relevanz der Aufgabe von der Tiefe abhängt, verwenden unsere Transformationsblöcke Skip-Verbindungen, wie sie von Residualnetzwerk-Ansätzen vorgeschlagen werden, um unverbundene aufgabenabhängige Merkmale leichter zu deaktivieren. Die experimentellen Ergebnisse zeigen, dass wir eine relative Verbesserung von bis zu 24,31% bei der Ausfallrate von Landmarken im Vergleich zu anderen MTL-Ansätzen auf dem neuesten Stand der Technik erreichen können. Schließlich führen wir eine Ablationsstudie durch, die zeigt, dass unser Ansatz effektiv die gemeinsame Nutzung von Wissen ermöglicht, indem er domänenspezifische Merkmale in bestimmten Tiefen von Aufgaben nutzt, von denen wir wissen, dass sie verwandt sind.
Zero-Shot Learning (ZSL) ist eine Klassifizierungsaufgabe, bei der einige Klassen, die als ungesehene Klassen bezeichnet werden, keine beschrifteten Trainingsbilder haben, sondern nur Nebeninformationen (oder Beschreibungen) über gesehene und ungesehene Klassen, oft in Form von semantischen oder beschreibenden Attributen. Der Mangel an Trainingsbildern aus einer Reihe von Klassen schränkt die Verwendung von Standard-Klassifizierungsverfahren und Verlusten ein, einschließlich des beliebten Cross-Entropie-Verlustes. Der wichtigste Schritt bei der Bewältigung des ZSL-Problems ist die Überbrückung des visuellen mit dem semantischen Raum durch das Erlernen einer nichtlinearen Einbettung, wobei ein etablierter Ansatz darin besteht, die semantische Repräsentation der visuellen Informationen zu erhalten und die Klassifizierung im semantischen Raum durchzuführen. In diesem Papier schlagen wir eine neuartige Architektur vor, die ZSL als vollständig verbundenes neuronales Netz mit Cross-Entropie-Verlust nutzt, um den visuellen Raum in den semantischen Raum einzubetten.während des Trainings, um ungesehene visuelle Informationen in das Netz einzuführen, nutzen wir Soft-Labeling auf der Grundlage semantischer Ähnlichkeiten zwischen gesehenen und ungesehenen Klassen. Wir evaluieren das vorgeschlagene Modell auf fünf Benchmark-Datensätzen für Zero-Shot-Learning, AwA1, AwA2, aPY, SUN und CUB, und zeigen, dass unser Ansatz trotz seiner Einfachheit die State-of-the-Art-Performance in der Generalized-ZSL-Einstellung auf allen diesen Datensätzen erreicht und bei einigen Datensätzen die State-of-the-Art-Performance übertrifft.
Bei komplexen Aufgaben, wie z.B. solchen mit großen kombinatorischen Aktionsräumen, kann zufällige Exploration zu ineffizient sein, um sinnvolle Lernfortschritte zu erzielen.In dieser Arbeit verwenden wir einen Lehrplan mit progressiv wachsenden Aktionsräumen, um das Lernen zu beschleunigen.Wir gehen davon aus, dass die Umgebung außerhalb unserer Kontrolle liegt, aber dass der Agent einen internen Lehrplan festlegen kann, indem er anfangs seinen Aktionsraum einschränkt. Unser Ansatz verwendet Off-Policy Reinforcement Learning, um optimale Wertfunktionen für mehrere Aktionsräume gleichzeitig zu schätzen und Daten, Wertschätzungen und Zustandsrepräsentationen von eingeschränkten Aktionsräumen effizient auf die vollständige Aufgabe zu übertragen. Wir zeigen die Wirksamkeit unseres Ansatzes in Proof-of-Concept-Kontrollaufgaben und bei anspruchsvollen, groß angelegten StarCraft-Mikromanagementaufgaben mit großen Multi-Agenten-Aktionsräumen.
Kürzlich haben Forscher entdeckt, dass die modernsten Objektklassifizierer durch kleine, für das menschliche Auge unbemerkte Störungen in der Eingabe leicht getäuscht werden können. Es ist bekannt, dass ein Angreifer starke Gegenbeispiele erzeugen kann, wenn er die Parameter des Klassifizierers kennt. Umgekehrt kann ein Verteidiger den Klassifikator durch Umschulung robust machen, wenn er die gegnerischen Beispiele kennt. In diesem Beitrag stellen wir eine auf neuronalen Netzen basierende Angriffsklasse vor, um eine größere, aber schwer zu lösende Klasse von Angriffen zu approximieren, und formulieren die Angreifer-Verteidiger-Interaktion als Nullsummenspiel zwischen Anführer und Verfolger. Wir präsentieren empfindlichkeitsbestrafende Optimierungsalgorithmen, um Minimax-Lösungen zu finden, die die beste Worst-Case-Verteidigung gegen Whitebox-Angriffe darstellen. Die Vorteile der lernbasierten Angriffe und Verteidigungen im Vergleich zu gradientenbasierten Angriffen und Verteidigungen werden mit MNIST und CIFAR-10 demonstriert.
Einige Arbeiten haben kürzlich rekurrente neuronale Netzmodelle vorgestellt, die mit Unregelmäßigkeiten umgehen können, aber die meisten von ihnen verlassen sich auf komplexe Mechanismen, um eine bessere Leistung zu erzielen. Diese Arbeit schlägt eine neuartige Methode zur Darstellung von Zeitstempeln (Stunden oder Daten) als dichte Vektoren unter Verwendung von Sinusfunktionen vor, die Time Embeddings genannt wird und als Dateneingabemethode auf die meisten maschinellen Lernmodelle angewendet werden kann.Die Methode wurde mit zwei prädiktiven Aufgaben aus MIMIC III, einem Datensatz von unregelmäßig abgetasteten Zeitreihen elektronischer Gesundheitsakten, evaluiert.Unsere Tests zeigten eine Verbesserung gegenüber LSTM-basierten und klassischen maschinellen Lernmodellen, insbesondere bei sehr unregelmäßigen Daten.
Die Erkennung von Gemeinschaften in Graphen kann über spektrale Methoden oder Posterior-Inferenz unter bestimmten probabilistischen grafischen Modellen gelöst werden. Mit dem Schwerpunkt auf zufälligen Graphenfamilien wie dem stochastischen Blockmodell haben neuere Forschungen beide Ansätze vereinheitlicht und sowohl statistische als auch rechnerische Erkennungsschwellen in Bezug auf das Signal-Rausch-Verhältnis identifiziert. Wir zeigen, dass sie auf datengesteuerte Weise und ohne Zugang zu den zugrunde liegenden generativen Modellen die Leistung des Glaubensausbreitungsalgorithmus auf binären und mehrklassigen stochastischen Blockmodellen erreichen oder sogar übertreffen können, von dem man annimmt, dass er in diesen Fällen die Berechnungsschwelle erreicht. Insbesondere schlagen wir vor, GNNs mit dem Non-Backtracking-Operator zu erweitern, der auf dem Liniendiagramm der Kantenadjazenzen definiert ist.  Darüber hinaus führen wir die erste Analyse der Optimierungslandschaft durch, die sich aus der Verwendung von (linearen) GNNs zur Lösung von Community-Detection-Problemen ergibt, und zeigen, dass unter bestimmten Vereinfachungen und Annahmen der Verlustwert bei jedem lokalen Minimum nahe dem Verlustwert beim globalen Minimum/Minimum liegt.
Residuale Netze (Resnets) sind zu einer prominenten Architektur im Deep Learning geworden, aber ein umfassendes Verständnis von Resnets ist immer noch ein Thema der laufenden Forschung.Eine neuere Ansicht argumentiert, dass Resnets iterative Verfeinerung von Merkmalen durchführen.Wir versuchen, die Eigenschaften dieses Aspekts weiter aufzudecken.Zu diesem Zweck untersuchen wir Resnets sowohl analytisch als auch empirisch.Wir formalisieren den Begriff der iterativen Verfeinerung in Resnets, indem wir zeigen, dass residuale Architekturen natürlich Merkmale ermutigen, sich während der Feedforward-Phase entlang des negativen Gradienten des Verlustes zu bewegen. Wir zeigen, dass Resnets in der Lage sind, sowohl Repräsentationslernen als auch iterative Verfeinerung durchzuführen, wobei ein Resnet-Block im Allgemeinen dazu neigt, das Repräsentationslernverhalten auf die ersten paar Schichten zu konzentrieren, während höhere Schichten eine iterative Verfeinerung von Merkmalen durchführen.Schließlich stellen wir fest, dass die gemeinsame Nutzung von Restschichten auf naive Weise zu einer Repräsentationsexplosion führt und die Generalisierungsleistung beeinträchtigt, und zeigen, dass einfache bestehende Strategien helfen können, dieses Problem zu lindern.
Wir entwickeln gelernte End-to-End-Rekonstruktionen für linsenlose, maskenbasierte Kameras, einschließlich eines experimentellen Systems zur Erfassung von ausgerichteten linsenlosen und linsenbehafteten Bildern für das Training.  Es werden verschiedene Rekonstruktionsmethoden erforscht, die von klassischen iterativen Ansätzen (basierend auf dem physikalischen Abbildungsmodell) bis hin zu tiefgreifenden gelernten Methoden mit vielen gelernten Parametern reichen.  Die Netzwerkstruktur kombiniert das Wissen über das physikalische Abbildungsmodell mit gelernten Parametern, die aus den Daten aktualisiert werden und Artefakte kompensieren, die durch physikalische Approximationen verursacht werden. Unser unrollierter Ansatz ist 20-mal schneller als klassische Methoden und liefert eine bessere Rekonstruktionsqualität als die klassischen und tiefen Methoden auf unserem experimentellen System.  
Deep Learning, ein Rebranding von Deep Neural Network Forschungsarbeiten, hat einen bemerkenswerten Erfolg in den letzten Jahren erreicht.mit mehreren versteckten Schichten, Deep Learning-Modelle zielen auf die Berechnung der hierarchischen Funktion Repräsentationen der Beobachtungsdaten.inzwischen aufgrund seiner schweren Nachteile in der Datenverbrauch, Rechenleistung, Parameter Tuning Kosten und der Mangel an Ergebnis Erklärbarkeit, Deep Learning hat auch unter viel Kritik gelitten. In diesem Beitrag wird ein neues Modell des Repräsentationslernens vorgestellt, nämlich das "Sample-Ensemble Genetic Evolutionary Network" (SEGEN), das als alternativer Ansatz für Deep-Learning-Modelle dienen kann: Anstatt ein einziges tiefes Modell zu erstellen, das auf einer Reihe von Teilinstanzen basiert, verwendet SEGEN eine genetisch-evolutionäre Lernstrategie, um eine Gruppe von Einheitsmodellen Generation für Generation aufzubauen. Bei den in SEGEN integrierten Einheitsmodellen kann es sich entweder um herkömmliche maschinelle Lernmodelle oder um neuere Deep-Learning-Modelle mit einer viel "engeren" und "flacheren" Architektur handeln, wobei die Lernergebnisse jeder Instanz in der letzten Generation durch diffusive Ausbreitung und Ensemble-Lernstrategien effektiv aus jedem Einheitsmodell kombiniert werden. Aus rechnerischer Sicht benötigt SEGEN weitaus weniger Daten, weniger Rechenressourcen und weniger Aufwand für die Parameterabstimmung, hat aber eine solide theoretische Interpretierbarkeit des Lernprozesses und der Ergebnisse.umfangreiche Experimente wurden mit verschiedenen realen Benchmark-Datensätzen durchgeführt, und die experimentellen Ergebnisse von SEGEN haben seine Vorteile gegenüber den modernsten Repräsentationslernmodellen gezeigt.
Wie können wir künstlichen Agenten beibringen, die menschliche Sprache flexibel zu nutzen, um Probleme in einer realen Umgebung zu lösen?Es gibt ein Beispiel in der Natur, dass Agenten in der Lage sind, dieses Problem zu lösen: menschliche Babys lernen schließlich, die menschliche Sprache zu nutzen, um Probleme zu lösen, und sie werden von einem erwachsenen Menschen in der Schleife unterrichtet.Leider sind die derzeitigen Methoden des maschinellen Lernens (z.B. aus dem Deep Reinforcement Learning) zu komplex. Leider sind die derzeitigen Methoden des maschinellen Lernens (z. B. Deep Reinforcement Learning) zu ineffizient, um eine Sprache auf diese Weise zu erlernen (3). Ein herausragendes Ziel ist es, einen Algorithmus mit einem geeigneten "Sprachlern-Prior" zu finden, der es ihm ermöglicht, die menschliche Sprache zu erlernen, während die Anzahl der erforderlichen menschlichen Interaktionen minimiert wird.) Bei L2C trainieren wir einen Meta-Lernagenten in der Simulation, um mit Populationen von vortrainierten Agenten zu interagieren, von denen jeder sein eigenes Kommunikationsprotokoll hat. Sobald der Meta-Lernagent in der Lage ist, sich schnell an jede Population von Agenten anzupassen, kann er in neuen Populationen eingesetzt werden, die während des Trainings nicht gesehen wurden, einschließlich menschlicher Populationen. Um das Versprechen des L2C-Rahmens zu zeigen, führen wir einige vorläufige Experimente in einem Lewis-Signalisierungsspiel (4) durch, in denen wir zeigen, dass Agenten, die mit L2C trainiert wurden, in der Lage sind, eine einfache Form der menschlichen Sprache (dargestellt durch eine handcodierte Kompositionssprache) in weniger Iterationen zu lernen als zufällig initialisierte Agenten.
Wir untersuchen das Problem der Anpassung von aufgabenspezifischen Lernratenplänen aus der Perspektive der Hyperparameteroptimierung.  Wir beschreiben die Struktur des Gradienten eines Validierungsfehlers in Bezug auf die Lernraten, den Hypergradienten, und stellen auf dieser Grundlage einen neuartigen Online-Algorithmus vor.Unsere Methode interpoliert adaptiv zwischen zwei kürzlich vorgeschlagenen Techniken (Franceschi et al., 2017; Baydin et al., 2018), die sich durch erhöhte Stabilität und schnellere Konvergenz auszeichnen.Wir zeigen empirisch, dass die vorgeschlagene Technik in Bezug auf die endgültige Testgenauigkeit mit Baselines und verwandten Methoden gut abschneidet.
In den letzten Jahren gab es einige spannende Entwicklungen im Bereich der Generierung von Bildern aus szenenbasierten Textbeschreibungen.Diese Ansätze haben sich in erster Linie auf die Generierung von Bildern aus einer statischen Textbeschreibung konzentriert und sind auf die Generierung von Bildern in einem einzigen Durchgang beschränkt.Sie sind nicht in der Lage, ein Bild interaktiv auf der Grundlage einer inkrementell additiven Textbeschreibung zu generieren (etwas, das intuitiver ist und der Art und Weise ähnelt, wie wir ein Bild beschreiben). Wir schlagen eine Methode zur inkrementellen Erzeugung eines Bildes auf der Grundlage einer Folge von Graphen von Szenenbeschreibungen (Szenengraphen) vor. Wir schlagen eine rekurrente Netzwerkarchitektur vor, die den in früheren Schritten erzeugten Bildinhalt beibehält und das kumulative Bild entsprechend der neu bereitgestellten Szeneninformationen modifiziert. Unser Modell nutzt Graph Convolutional Networks (GCN), um variabel große Szenegraphen zusammen mit Generative Adversarial Bildübersetzungsnetzwerken zu bedienen, um realistische Multi-Objekt-Bilder zu generieren, ohne eine Zwischenüberwachung während des Trainings zu benötigen. Wir experimentieren mit dem Coco-Stuff-Datensatz, der Multi-Objekt-Bilder zusammen mit Annotationen enthält, die die visuelle Szene beschreiben, und zeigen, dass unser Modell andere Ansätze auf dem gleichen Datensatz bei der Generierung visuell konsistenter Bilder für inkrementell wachsende Szenegraphen deutlich übertrifft.
Die meisten Convolutional Neural Networks (CNNs) für die Bildklassifikation wurden jedoch anhand von Datensätzen entwickelt, die große Objekte in meist zentralen Bildpositionen enthalten. Um zu beurteilen, ob klassische CNN-Architekturen für die Klassifizierung von kleinen Objekten geeignet sind, haben wir eine umfassende Testumgebung aufgebaut, die zwei Datensätze enthält: einen aus MNIST-Ziffern und einen aus Histopathologie-Bildern, die kontrollierte Experimente zur Belastungsprüfung von CNN-Architekturen mit einem breiten Spektrum von Signal-Rausch-Verhältnissen ermöglichen: (1) Es gibt einen Grenzwert für das Signal-Rausch-Verhältnis, unterhalb dessen CNNs nicht verallgemeinern können, und dass dieser Grenzwert von der Größe des Datensatzes beeinflusst wird - mehr Daten führen zu besseren Leistungen; die Menge an Trainingsdaten, die für die Verallgemeinerung des Modells erforderlich ist, skaliert jedoch schnell mit dem Kehrwert des Objekt-Bild-Verhältnisses (2) im Allgemeinen zeigen Modelle mit höherer Kapazität eine bessere Verallgemeinerung; (3) wenn die ungefähren Objektgrößen bekannt sind, ist die Anpassung des rezeptiven Feldes vorteilhaft; und (4) bei einem sehr kleinen Signal-Rausch-Verhältnis beeinflusst die Wahl der globalen Pooling-Operation die Optimierung, während bei relativ großen Signal-Rausch-Werten alle getesteten globalen Pooling-Operationen eine ähnliche Leistung aufweisen.
Jüngste Trends zur Einbeziehung von Aufmerksamkeitsmechanismen in die Bildverarbeitung haben Forscher dazu veranlasst, die Überlegenheit von Faltungsschichten als primären Baustein zu überdenken: Ramachandran et al. (2019) haben gezeigt, dass Aufmerksamkeitsschichten nicht nur CNNs dabei helfen können, weitreichende Abhängigkeiten zu handhaben, sondern auch Faltungsschichten vollständig ersetzen können und bei Bildverarbeitungsaufgaben die beste Leistung erzielen.Dies wirft die Frage auf, ob gelernte Aufmerksamkeitsschichten ähnlich wie Faltungsschichten funktionieren. Diese Arbeit liefert den Beweis, dass Aufmerksamkeitsschichten eine Faltung durchführen können und in der Tat in der Praxis oft lernen, dies zu tun.Insbesondere beweisen wir, dass eine Multi-Head-Selbstaufmerksamkeitsschicht mit einer ausreichenden Anzahl von Köpfen mindestens so aussagekräftig ist wie jede Faltungsschicht.Unsere numerischen Experimente zeigen dann, dass Selbstaufmerksamkeitsschichten auf Pixel-Gitter-Muster ähnlich wie CNN-Schichten reagieren, was unsere Analyse bestätigt.Unser Code ist öffentlich zugänglich.
Wir stellen einen "lernbasierten" Algorithmus für das Low-Rank-Decomposition-Problem vor: Bei einer $n-mal-d$-Matrix $A$ und einem Parameter $k$ ist eine Rang-$k$-Matrix $A'$ zu berechnen, die den Approximationsverlust $||A- A'||_F$ minimiert.Der Algorithmus verwendet eine Trainingsmenge von Eingabematrizen, um seine Leistung zu optimieren. Einige der effizientesten Approximationsalgorithmen zur Berechnung von Approximationen mit niedrigem Rang gehen von der Berechnung einer Projektion $SA$ aus, wobei $S$ eine dünne zufällige $m \mal n$ "Skizzenmatrix" ist, und führen dann die Singulärwertzerlegung von $SA$ durch. Wir zeigen, wie man die Zufallsmatrix $S$ durch eine "gelernte" Matrix der gleichen Sparsamkeit ersetzt, um den Fehler zu reduzieren. unsere Experimente zeigen, dass eine gelernte Skizzenmatrix für mehrere Arten von Datensätzen den Annäherungsverlust im Vergleich zu einer Zufallsmatrix $S$ erheblich reduzieren kann, manchmal um eine Größenordnung. wir untersuchen auch gemischte Matrizen, bei denen nur einige der Zeilen trainiert werden und die restlichen zufällig sind, und zeigen, dass die Matrizen immer noch eine verbesserte Leistung bieten, während die Worst-Case-Garantien erhalten bleiben.
Neuronale Konversationsmodelle sind in Anwendungen wie persönlichen Assistenten und Chatbots weit verbreitet und scheinen eine bessere Leistung zu erbringen, wenn sie auf Wortebene arbeiten, aber für Fusionssprachen wie Französisch, Russisch und Polnisch ist die Größe des Vokabulars manchmal nicht machbar, da die meisten Wörter viele Wortformen haben. Unser Modell nutzt effizient die Korrespondenz zwischen normalisierten und Zielwörtern und übertrifft die Modelle auf Zeichenebene deutlich, während es beim Training 2x schneller ist und bei der Auswertung 20\% schneller ist.Wir schlagen auch eine neue Pipeline für den Aufbau von Konversationsmodellen vor: zuerst wird eine normalisierte Antwort generiert und dann mit Hilfe unseres Netzwerks in eine grammatikalisch korrekte Antwort umgewandelt.Die vorgeschlagene Pipeline bietet eine bessere Leistung als Konversationsmodelle auf Zeichenebene, wie die Tests der Prüfer zeigen.
In diesem Beitrag wird die Verwendung von Spektralelement-Methoden (\citep{canuto_spectral_1988}) für ein schnelles und genaues Training neuronaler gewöhnlicher Differentialgleichungen (ODE-Netze; \citealp{Chen2018NeuralOD}) zur Systemidentifikation vorgeschlagen, indem deren Dynamik als abgeschnittene Reihe von Legendre-Polynomen ausgedrückt wird. Die Koeffizienten der Reihe sowie die Gewichte des Netzes werden durch Minimierung der gewichteten Summe der Verlustfunktion und der Verletzung der ODE-Netzdynamik berechnet. Das Problem wird durch Koordinatenabstieg gelöst, der abwechselnd in Bezug auf die Koeffizienten und die Gewichte zwei nicht eingeschränkte Teilprobleme unter Verwendung von Standard-Backpropagation und Gradientenmethoden minimiert. Der experimentelle Vergleich mit Standardmethoden, wie Backpropagation durch explizite Solver und die adjungierte Technik \citep{Chen2018NeuralOD}, beim Training von Surrogatmodellen kleiner und mittelgroßer dynamischer Systeme zeigt, dass das Verfahren mindestens eine Größenordnung schneller ist, um einen vergleichbaren Wert der Verlustfunktion zu erreichen, und dass der entsprechende Test-MSE ebenfalls eine Größenordnung kleiner ist, was auf eine Verbesserung der Generalisierungsfähigkeiten schließen lässt.
Viele moderne Methoden verwenden intrinsische Motivation, um das spärliche extrinsische Belohnungssignal zu ergänzen und dem Agenten mehr Möglichkeiten zu geben, während der Exploration Feedback zu erhalten. Üblicherweise werden diese Signale als Bonusbelohnungen hinzugefügt, was zu einer gemischten Politik führt, die weder die Exploration noch die Aufgabenerfüllung resolut durchführt.In diesem Papier lernen wir stattdessen separate intrinsische und extrinsische Aufgabenpolitiken und planen zwischen diesen verschiedenen Antrieben, um die Exploration zu beschleunigen und das Lernen zu stabilisieren. Darüber hinaus führen wir eine neue Art von intrinsischer Belohnung ein, die wir als Nachfolger-Feature-Control (SFC) bezeichnen und die allgemein und nicht aufgabenspezifisch ist. Sie berücksichtigt Statistiken über komplette Trajektorien und unterscheidet sich damit von bisherigen Methoden, die nur lokale Informationen zur Bewertung der intrinsischen Motivation verwenden.Wir bewerten unseren vorgeschlagenen geplanten intrinsischen Antrieb (SID) anhand von drei verschiedenen Umgebungen mit rein visuellen Eingaben: VizDoom, DeepMind Lab und DeepMind Control Suite.Die Ergebnisse zeigen eine wesentlich verbesserte Explorationseffizienz mit SFC und die hierarchische Nutzung der intrinsischen Antriebe.Ein Video unserer experimentellen Ergebnisse finden Sie unter https://gofile.io/?c=HpEwTd.
Meta-Reinforcement-Learning-Ansätze zielen darauf ab, Lernverfahren zu entwickeln, die sich mit Hilfe weniger Beispiele schnell an eine Verteilung von Aufgaben anpassen können.Die Entwicklung effizienter Explorationsstrategien, die in der Lage sind, die nützlichsten Proben zu finden, wird in solchen Umgebungen kritisch.Bestehende Ansätze zum Finden effizienter Explorationsstrategien fügen Hilfsziele hinzu, um die Exploration durch die Pre-Update-Politik zu fördern, was jedoch die Anpassung mit wenigen Gradientenschritten erschwert, da die Pre-Update- (Exploration) und Post-Update- (Exploitation) Politiken sehr unterschiedlich sind. Wir zeigen, dass die Verwendung von selbstüberwachten oder überwachten Lernzielen für die Anpassung den Trainingsprozess stabilisiert und demonstrieren auch die überlegene Leistung unseres Modells im Vergleich zu früheren Arbeiten in diesem Bereich.
Das "Supersymmetrische Künstliche Neuronale Netz" im Deep Learning (bezeichnet als (x; θ, bar{θ})Tw), setzt sich für die Berücksichtigung biologischer Beschränkungen ein, um die Rückwärtsfortpflanzung weiter zu verallgemeinern. Betrachtet man die Entwicklung der "Lösungsgeometrien", so hat der Übergang von der SO(n)-Darstellung (z. B. Perceptron-ähnliche Modelle) zur SU(n)-Darstellung (z. B. UnitaryRNNs) immer reichhaltigere Darstellungen im Gewichtsraum des künstlichen neuronalen Netzes gewährleistet, so dass immer bessere Hypothesen generiert werden konnten.Das supersymmetrische künstliche neuronale Netz erforscht einen natürlichen Schritt nach vorn, nämlich die SU(m|n)-Darstellung. Diese supersymmetrischen biologischen Gehirnrepräsentationen (Perez et al.) können durch die superladungsverträgliche spezielle unitäre Notation SU(m|n) oder (x; θ, bar{θ})Tw dargestellt werden, die durch θ, bar{θ} parametrisiert sind, welche supersymmetrische Richtungen sind, im Gegensatz zu θ, das in dem typischen nicht-supersymmetrischen Deep-Learning-Modell gesehen wird.Bemerkenswert ist, dass supersymmetrische Werte mehr Informationen kodieren oder darstellen können als das typische Deep-Learning-Modell, z. B. in Form von "Partnerpotential"-Signalen.
In den meisten praktischen Optimierungsszenarien mit verrauschten Daten und/oder Gradienten ist es jedoch möglich, dass der stochastische Gradientenabstieg versehentlich kritische Parameter ändert.In diesem Papier argumentieren wir für die Bedeutung der direkten Regularisierung von Optimierungstrajektorien. Wir leiten eine neue Ko-Natur-Gradienten-Aktualisierungsregel für kontinuierliches Lernen ab, bei der die neuen Aufgaben-Gradienten mit den empirischen Fisher-Informationen der zuvor gelernten Aufgaben vorkonditioniert werden, und zeigen, dass die Verwendung des Ko-Natur-Gradienten das Vergessen beim kontinuierlichen Lernen systematisch reduziert.
Wir untersuchen das Problem der Generierung von Quellcode in einer stark typisierten, Java-ähnlichen Programmiersprache, wenn ein Label (z.B. eine Reihe von API-Aufrufen oder Typen) mit einer kleinen Menge an Informationen über den gewünschten Code gegeben ist.Von den generierten Programmen wird erwartet, dass sie eine "realistische" Beziehung zwischen Programmen und Labels respektieren, wie es durch einen Korpus von gelabelten Programmen, die während des Trainings verfügbar sind, veranschaulicht wird.Zwei Herausforderungen bei einer solchen *bedingten Programmerzeugung* sind, dass die generierten Programme einen reichen Satz von syntaktischen und semantischen Einschränkungen erfüllen müssen und dass der Quellcode viele Low-Level-Merkmale enthält, die das Lernen behindern. Wir gehen diese Probleme an, indem wir einen neuronalen Generator nicht auf Code, sondern auf *Programmskizzen* trainieren, d.h. auf Modellen der Programmsyntax, die Namen und Operationen abstrahieren, die sich nicht über Programme hinweg verallgemeinern lassen. Während der Generierung leiten wir eine bessere Verteilung über Skizzen ab und konkretisieren dann Stichproben aus dieser Verteilung mit kombinatorischen Techniken zu typsicheren Programmen. Wir implementieren unsere Ideen in einem System zur Generierung vonAPI-lastigem Java-Code und zeigen, dass es oft den gesamten Körper einer Methode vorhersagen kann, wenn nur wenige API-Aufrufe oder Datentypen in der Methode vorkommen.
Wir schlagen einen Ansatz für die Sequenzmodellierung vor, der auf autoregressiven normalisierenden Flüssen basiert.Jede autoregressive Transformation, die über die Zeit hinweg wirkt, dient als beweglicher Bezugsrahmen für die Modellierung von Dynamiken auf höherer Ebene.Diese Technik bietet eine einfache, allgemeine Methode zur Verbesserung der Sequenzmodellierung, mit Verbindungen zu bestehenden und klassischen Techniken.Wir demonstrieren den vorgeschlagenen Ansatz sowohl mit eigenständigen Modellen als auch als Teil größerer sequentieller latenter Variablenmodelle.Die Ergebnisse werden anhand von drei Benchmark-Videodatensätzen vorgestellt, bei denen flussbasierte Dynamiken die Log-Likelihood-Leistung gegenüber Basismodellen verbessern.
Es ist bekannt, dass viele Modelle des maschinellen Lernens anfällig für Angriffe sind, bei denen ein Angreifer einen Klassifikator umgeht, indem er kleine Störungen an den Eingaben vornimmt.In diesem Beitrag wird erörtert, wie industrielle Tools zur Erkennung von Urheberrechten, die eine zentrale Rolle im Internet spielen, anfällig für Angriffe sind.Wir erörtern eine Reihe von Systemen zur Erkennung von Urheberrechten und warum sie besonders anfällig für Angriffe sind.  Diese Schwachstellen sind vor allem bei Systemen, die auf neuronalen Netzen basieren, offensichtlich.  Zum Beweis des Konzepts beschreiben wir eine bekannte Musikerkennungsmethode und implementieren dieses System in Form eines neuronalen Netzes. Anschließend greifen wir dieses System mit einfachen Gradientenmethoden an. Auf diese Weise erzeugte angreifbare Musik täuscht erfolgreich industrielle Systeme, einschließlich des AudioTag-Urheberrechtsdetektors und des Content-ID-Systems von YouTube. Unser Ziel ist es, das Bewusstsein für die Bedrohung durch angreifbare Beispiele in diesem Bereich zu schärfen und die Bedeutung der Härtung von Urheberrechtserkennungssystemen gegen Angriffe hervorzuheben.
Equilibrium Propagation (EP) ist ein Lernalgorithmus, der eine Brücke zwischen maschinellem Lernen und Neurowissenschaften schlägt, indem er Gradienten berechnet, die denen der Backpropagation Through Time (BPTT) sehr ähnlich sind, aber mit einer Lernregel, die im Raum lokal ist.Bei einer Eingabe x und einem zugehörigen Ziel y verläuft EP in zwei Phasen: In der ersten Phase entwickeln sich die Neuronen frei in Richtung eines ersten stabilen Zustands; in der zweiten Phase werden die Ausgangsneuronen in Richtung y gestoßen, bis sie einen zweiten stabilen Zustand erreichen. In bestehenden Implementierungen von EP ist die Lernregel jedoch zeitlich nicht lokal: Die Gewichtsaktualisierung wird durchgeführt, nachdem die Dynamik der zweiten Phase konvergiert hat, und erfordert Informationen aus der ersten Phase, die physisch nicht mehr verfügbar sind, was ein großes Hindernis für die biologische Plausibilität von EP und seine effiziente Hardware-Implementierung darstellt. In dieser Arbeit schlagen wir eine Version von EP mit dem Namen Continual Equilibrium Propagation (C-EP) vor, bei der die Neuronen- und Synapsen-Dynamik während der zweiten Phase gleichzeitig auftritt, so dass die Gewichtsaktualisierung zeitlich lokal wird. Wir beweisen theoretisch, dass, vorausgesetzt die Lernraten sind ausreichend klein, bei jedem Zeitschritt der zweiten Phase die Dynamik der Neuronen und Synapsen den Gradienten des Verlustes folgt, der durch BPTT gegeben ist (Theorem 1). Wir demonstrieren das Training mit C-EP auf MNIST und verallgemeinern C-EP auf neuronale Netze, bei denen die Neuronen durch asymmetrische Verbindungen miteinander verbunden sind, und zeigen anhand von Experimenten, dass das Training umso besser funktioniert, je mehr die Aktualisierungen des Netzes den Gradienten der BPTT folgen.Diese Ergebnisse bringen EP einen Schritt näher an die Biologie heran, wobei die enge Verbindung zur Backpropagation erhalten bleibt.
Es gibt zwei Hauptforschungsrichtungen im Bereich des visuellen Reasonings: das neuronale Modulnetzwerk (NMN) mit explizitem Multi-Hop-Reasoning durch handgefertigte neuronale Module und das monolithische Netzwerk mit implizitem Reasoning im latenten Merkmalsraum.ersteres zeichnet sich durch Interpretierbarkeit und Kompositionalität aus, während letzteres in der Regel eine bessere Leistung aufgrund von Modellflexibilität und Parametereffizienz erzielt.  Um die Lücke zwischen den beiden zu schließen, stellen wir das Meta Module Network (MMN) vor, einen neuartigen hybriden Ansatz, der ein Meta-Modul effizient für die Ausführung vielseitiger Funktionen nutzen kann und gleichzeitig Kompositionalität und Interpretierbarkeit durch modularisiertes Design bewahrt.Das vorgeschlagene Modell analysiert zunächst eine Eingabefrage in ein funktionales Programm durch einen Programmgenerator.Anstatt ein aufgabenspezifisches Netzwerk zu erstellen, um jede Funktion wie ein traditionelles NMN zu repräsentieren, verwenden wir den Recipe Encoder, um die Funktionen in ihre entsprechenden Rezepte (Spezifikationen) zu übersetzen, die zur dynamischen Instanzierung des Meta-Moduls in Instanzmodule verwendet werden. Um verschiedene Instanzmodule mit bestimmten Funktionen auszustatten, wird ein Lehrer-Schüler-Framework vorgeschlagen, bei dem ein symbolischer Lehrer die Szenegraphen vorab ausführt, um den instanziierten Modulen (Schülern) Richtlinien zu geben, denen sie folgen sollen.Kurz gesagt, MMN verwendet das Metamodul, um seine Parametrisierungseffizienz zu erhöhen, und verwendet Rezeptkodierung, um seine Generalisierungsfähigkeit gegenüber NMN zu verbessern: (1) MMN erreicht eine signifikante Verbesserung gegenüber NMN und monolithischen Netzwerken; (2) MMN ist in der Lage, auf unbekannte, aber verwandte Funktionen zu verallgemeinern.
Unser Hauptbeitrag ist CopyCAT, ein gezielter Angriff, der in der Lage ist, einen Agenten dazu zu verleiten, die Politik eines Außenstehenden zu befolgen; er ist vorberechnet, daher schnell abgeleitet und könnte daher in einem Echtzeit-Szenario verwendet werden; wir zeigen seine Effektivität bei Atari 2600-Spielen in einer neuartigen Nur-Lese-Einstellung. Im letzteren Fall kann der Gegner nicht direkt den Zustand des Agenten - seine Repräsentation der Umgebung - verändern, sondern nur seine Beobachtung - seine Wahrnehmung der Umgebung - angreifen, was einen Schreibzugriff auf das Innenleben des Agenten erfordern würde.
Bisherige hybride Empfehlungsmethoden sind effektiv, um mit den Cold-Start-Problemen umzugehen, indem sie reale latente Faktoren von Cold-Start-Elementen (Benutzern) aus Seiteninformationen extrahieren, aber sie leiden immer noch unter der geringen Effizienz in der Online-Empfehlung, die durch die teure Ähnlichkeitssuche im realen latenten Raum verursacht wird.Dieses Papier stellt ein kollaboratives generiertes Hashing (CGH) vor, um die Effizienz zu verbessern, indem Benutzer und Elemente als binäre Codes bezeichnet werden, was für verschiedene Einstellungen gilt: Cold-Start-Benutzer, Cold-Start-Elemente und Warm-Start-Elemente. CGH wurde entwickelt, um Hash-Funktionen von Nutzern und Artikeln durch das Prinzip der minimalen Beschreibungslänge (Minimum Description Length, MDL) zu erlernen und kann somit verschiedene Empfehlungsumgebungen abdecken. Darüber hinaus initiiert CGH eine neue Marketingstrategie durch das Mining potenzieller Nutzer in einem generativen Schritt. Um effektive Nutzer zu rekonstruieren, wird das MDL-Prinzip verwendet, um kompakte und informative binäre Codes aus den Inhaltsdaten zu lernen.
Jüngste Bemühungen, Repräsentationslernen mit formalen Methoden zu kombinieren, die allgemein als neurosymbolische Methoden bekannt sind, haben zu einem neuen Trend geführt, reichhaltige neuronale Architekturen anzuwenden, um klassische kombinatorische Optimierungsprobleme zu lösen.In diesem Papier schlagen wir einen neuronalen Rahmen vor, der lernen kann, das Circuit Satisfiability Problem zu lösen. Unser Framework basiert auf zwei grundlegenden Beiträgen: einer reichhaltigen Embedding-Architektur, die die Problemstruktur kodiert, und einem differenzierbaren End-to-End-Trainingsverfahren, das Reinforcement Learning nachahmt und das Modell direkt auf die Lösung des SAT-Problems trainiert.Die experimentellen Ergebnisse zeigen die überlegene Generalisierungsleistung unseres Frameworks im Vergleich zur kürzlich entwickelten NeuroSAT-Methode.
Sequence Generation Modelle wie rekurrente Netze können mit einer Vielzahl von Lernalgorithmen trainiert werden.z.B. Maximum Likelihood Learning ist einfach und effizient, leidet aber unter dem Exposure Bias Problem.Reinforcement Learning wie Policy Gradient adressiert das Problem, kann aber eine prohibitiv schlechte Explorationseffizienz haben.eine Vielzahl von anderen Algorithmen wie RAML, SPG, und Data Noising, wurden auch in verschiedenen Perspektiven entwickelt.dieses Papier stellt eine formale Verbindung zwischen diesen Algorithmen. Wir stellen eine verallgemeinerte Entropie regulierte Politik Optimierung Formulierung, und zeigen, dass die scheinbar divergierenden Algorithmen können alle als spezielle Instanzen des Rahmens, mit dem einzigen Unterschied, die Konfigurationen der Belohnungsfunktion und ein paar Hyperparameter reformuliert werden.Die einheitliche Auslegung bietet einen systematischen Blick auf die unterschiedlichen Eigenschaften der Exploration und Lernen Effizienz.Außerdem, auf der Grundlage des Rahmens, präsentieren wir einen neuen Algorithmus, der dynamisch interpoliert zwischen den bestehenden Algorithmen für verbesserte Lernen.Experimente auf maschinelle Übersetzung und Text Zusammenfassung zeigen die Überlegenheit des vorgeschlagenen Algorithmus.
Wir berichten über das SHINRA-Projekt, ein Projekt zur Strukturierung von Wikipedia mit kollaborativem Konstruktionsschema. Ziel des Projekts ist es, eine riesige und gut strukturierte Wissensbasis zu schaffen, die in NLP-Anwendungen wie QA, Dialogsystemen und erklärbaren NLP-Systemen verwendet werden kann. Sie wird nach dem Schema "Resource by Collaborative Contribution (RbCC)" erstellt. Es gibt maschinenlesbare Wissensdatenbanken wie CYC, DBpedia, YAGO, Freebase Wikidata usw., aber jede von ihnen hat ein Problem, das es zu lösen gilt. CYC hat ein Abdeckungsproblem, und andere haben ein Kohärenzproblem aufgrund der Tatsache, dass sie auf Wikipedia basieren und/oder von vielen, aber von Natur aus inkohärenten Crowdworkern erstellt werden. Um das letztgenannte Problem zu lösen, haben wir ein Projekt zur Strukturierung von Wikipedia mit Hilfe einer gemeinsamen Aufgabe zur automatischen Erstellung von Wissensdatenbanken gestartet.Die gemeinsamen Aufgaben zur automatischen Erstellung von Wissensdatenbanken sind seit Jahrzehnten beliebt und gut erforscht.Allerdings sind diese Aufgaben nur dazu gedacht, die Leistungen verschiedener Systeme zu vergleichen und herauszufinden, welches System bei begrenzten Testdaten am besten abschneidet.Die Ergebnisse der teilnehmenden Systeme werden nicht geteilt, und die Systeme können nach Beendigung der Aufgabe aufgegeben werden.Wir glauben, dass diese Situation durch die folgenden Änderungen verbessert werden kann:1. die gemeinsame Aufgabe so zu gestalten, dass eine Wissensdatenbank erstellt wird und nicht nur begrenzte Testdaten ausgewertet werden2. Die Ergebnisse aller Systeme werden öffentlich zugänglich gemacht, so dass wir das Ensemble-Lernen durchführen können, um bessere Ergebnisse als die besten Systeme zu erzielen. 3. Die Aufgabe wird wiederholt, so dass wir die Aufgabe mit den größeren und besseren Trainingsdaten aus den Ergebnissen der vorherigen Aufgabe durchführen können (Bootstrapping und aktives Lernen)Wir haben "SHINRA2018" mit dem oben genannten Schema durchgeführt und berichten in diesem Papier über die Ergebnisse und die zukünftigen Richtungen des Projekts. Wir haben die meisten Entitäten in der japanischen Wikipedia (nämlich 730 Tausend Entitäten) in die 200 ENE-Kategorien kategorisiert und auf der Grundlage dieser Daten die Werte der Attribute aus den Wikipedia-Seiten extrahiert. Wir haben 600 Trainingsdaten zur Verfügung gestellt, und die Teilnehmer müssen die Attributwerte für alle verbleibenden Entitäten desselben Kategorietyps übermitteln. 100 Daten davon werden für jede Kategorie verwendet, um die Systemausgabe in der gemeinsamen Aufgabe zu bewerten. Wir haben ein vorläufiges Ensemble-Lernen an den Ausgaben durchgeführt und eine Verbesserung von 15 F1-Scores für eine Kategorie und durchschnittlich 8 F1-Scores für alle 5 getesteten Kategorien im Vergleich zu einer starken Basislinie festgestellt. Aufgrund dieser vielversprechenden Ergebnisse haben wir beschlossen, 2019 drei Aufgaben durchzuführen: eine mehrsprachige Kategorisierungsaufgabe (ML), die Extraktion derselben 5 Kategorien auf Japanisch mit größeren Trainingsdaten (JP-5) und die Extraktion von 34 neuen Kategorien auf Japanisch (JP-34).
Jüngste Studien zur Bildsuperauflösung (SR) nutzen sehr tiefe neuronale Faltungsnetzwerke und die reichhaltigen hierarchischen Merkmale, die sie bieten, was zu einer besseren Rekonstruktionsleistung als herkömmliche Methoden führt. Die kleinen rezeptiven Felder im Up-Sampling- und Rekonstruktionsprozess dieser Modelle hindern sie jedoch daran, den vollen Nutzen aus den globalen Kontextinformationen zu ziehen, was zu Problemen bei der weiteren Leistungsverbesserung führt.In diesem Papier schlagen wir, inspiriert von den Bildrekonstruktionsprinzipien des menschlichen visuellen Systems, ein globales Bildsuperauflösungs-Schlussfolgernetzwerk (SRGRN) vor, um die Korrelationen zwischen verschiedenen Regionen eines Bildes durch globales Schlussfolgern effektiv zu lernen. Konkret schlagen wir ein Global Reasoning Up-Sampling Modul (GRUM) und einen Global Reasoning Reconstruction Block (GRRB) vor, die ein Graphenmodell konstruieren, um Relation Reasoning auf Regionen von Bildern mit niedriger Auflösung (LR) durchzuführen. Die von uns vorgeschlagenen SRGRN sind robuster und können mit niedrig aufgelösten Bildern umgehen, die durch verschiedene Arten von Degradation beschädigt sind. umfangreiche Experimente mit verschiedenen Benchmark-Datensätzen zeigen, dass unser Modell andere State-of-the-Art-Methoden übertrifft.
Graph Neural Networks (GNNs) haben in letzter Zeit aufgrund ihrer Leistungsfähigkeit bei der Verarbeitung von Graphdaten für verschiedene nachgelagerte Aufgaben in unterschiedlichen Anwendungsbereichen enorme Aufmerksamkeit erhalten. Es fehlt jedoch noch eine gründliche Analyse über (1) ob es einen besten Filter gibt, der für alle Graphdaten die beste Leistung erbringt; (2) welche Graph-Eigenschaften die optimale Wahl des Graph-Filters beeinflussen; (3) wie man einen geeigneten Filter entwirft, der sich an die Graphdaten anpasst.In diesem Papier konzentrieren wir uns auf die Beantwortung der oben genannten drei Fragen.Wir schlagen zunächst ein neuartiges Bewertungswerkzeug vor, um die Effektivität von Graph-Faltungsfiltern für einen bestimmten Graph zu bewerten. Mit Hilfe des Bewertungswerkzeugs finden wir heraus, dass es keinen einzelnen Filter als "Silberkugel" gibt, der für alle möglichen Graphen die beste Leistung erbringt.Darüber hinaus beeinflussen unterschiedliche Eigenschaften der Graphenstruktur die Wahl des optimalen Graphenfaltungsfilters.Auf der Grundlage dieser Erkenntnisse entwickeln wir das Adaptive Filter Graph Neural Network (AFGNN), ein einfaches, aber leistungsstarkes Modell, das adaptiv aufgabenspezifische Filter lernen kann. Für einen gegebenen Graphen nutzt es die Bewertung von Graphenfiltern als Regularisierung und lernt, aus einem Satz von Basisfiltern zu kombinieren. Experimente an synthetischen und realen Benchmark-Datensätzen zeigen, dass das von uns vorgeschlagene Modell in der Tat einen geeigneten Filter erlernen kann und bei Graphenaufgaben gut abschneidet.
Der Fortschritt der Knoten-Pooling-Operationen in Graph Neural Networks (GNNs) ist hinter dem fieberhaften Design neuer Message-Passing-Techniken zurückgeblieben, und Pooling bleibt ein wichtiges und herausforderndes Unterfangen für das Design von Deep-Architekturen.In diesem Papier schlagen wir eine Pooling-Operation für GNNs vor, die einen differenzierbaren unbeaufsichtigten Verlust auf der Grundlage des minCut-Optimierungsziels nutzt.Für jeden Knoten lernt unsere Methode einen weichen Cluster-Zuweisungsvektor, der von den Knotenmerkmalen, der Ziel-Inferenzaufgabe (z.B., Für jeden Knoten lernt unsere Methode einen weichen Clusterzuweisungsvektor, der von den Knotenmerkmalen, der Zielinferenzaufgabe (z.B. einem Graphenklassifizierungsverlust) und, dank des minCut-Ziels, auch von der Konnektivitätsstruktur des Graphen abhängt.Das Graphen-Pooling wird durch die Anwendung der Zuweisungsvektor-Matrix auf die Adjazenzmatrix und die Knotenmerkmale erhalten.Wir validieren die Effektivität der vorgeschlagenen Pooling-Methode bei einer Vielzahl von überwachten und unbeaufsichtigten Aufgaben.
Wir schlagen TuckER vor, ein relativ einfaches, aber leistungsfähiges lineares Modell, das auf der Tucker-Dekomposition der binären Tensordarstellung von Wissensgraphen-Tripeln basiert. Durch die Verwendung dieser speziellen Dekomposition werden die Parameter zwischen den Relationen geteilt, was Multitasking-Lernen ermöglicht.TuckER übertrifft frühere State-of-the-Art-Modelle in mehreren Standard-Link-Vorhersagedatensätzen.
Durch Innovationen im Architekturdesign liefern tiefere und breitere neuronale Netzwerkmodelle eine bessere Leistung bei einer Vielzahl von Aufgaben. Der erhöhte Speicherbedarf dieser Modelle stellt jedoch eine Herausforderung beim Training dar, wenn alle Zwischenschichtaktivierungen für die Backpropagation gespeichert werden müssen. Begrenzter GPU-Speicher zwingt Praktiker dazu, suboptimale Entscheidungen zu treffen: entweder ineffizientes Training mit kleineren Stapeln von Beispielen oder Begrenzung der Architektur auf eine geringere Tiefe und Breite und weniger Schichten bei höheren räumlichen Auflösungen.Diese Arbeit führt eine Annäherungsstrategie ein, die den Speicherbedarf eines Netzwerks während des Trainings erheblich reduziert, aber vernachlässigbare Auswirkungen auf die Trainingsleistung und den Rechenaufwand hat. Während des Vorwärtsdurchlaufs ersetzen wir Aktivierungen durch Näherungen mit geringerer Genauigkeit, unmittelbar nachdem sie von nachfolgenden Schichten verwendet wurden, und geben so Speicher frei. Die angenäherten Aktivierungen werden dann während des Rückwärtsdurchlaufs verwendet. Experimente mit CIFAR und ImageNet zeigen, dass die Verwendung unseres Ansatzes mit 8- und sogar 4-Bit-Festkomma-Approximationen von 32-Bit-Gleitkomma-Aktivierungen nur eine geringe Auswirkung auf die Trainings- und Validierungsleistung hat, während gleichzeitig erhebliche Einsparungen bei der Speichernutzung erzielt werden.
Das Problem wird in der Regel mit Streaming-Algorithmen angegangen, die sehr große Datenmengen mit begrenztem Speicherplatz verarbeiten können. Die heutigen Streaming-Algorithmen können jedoch keine Muster in ihren Eingabedaten ausnutzen, um die Leistung zu verbessern.   Die vorgeschlagenen Algorithmen kombinieren die Vorteile des maschinellen Lernens mit den formalen Garantien der Algorithmentheorie.  Wir beweisen, dass unsere lernbasierten Algorithmen geringere Schätzfehler aufweisen als ihre nicht lernenden Gegenstücke.  Außerdem evaluieren wir unsere Algorithmen an zwei realen Datensätzen und demonstrieren empirisch ihre Leistungssteigerung.
Die Vorhersage von Verbindungen in einfachen Graphen ist ein grundlegendes Problem, bei dem neue Verbindungen zwischen Knoten auf der Grundlage der beobachteten Struktur des Graphen vorhergesagt werden. In vielen realen Anwendungen besteht jedoch die Notwendigkeit, Beziehungen zwischen Knoten zu modellieren, die über paarweise Assoziationen hinausgehen. Obwohl sich Graph Convolutional Networks (GCN) in letzter Zeit als leistungsfähiger Deep-Learning-Ansatz für die Vorhersage von Links in einfachen Graphen erwiesen haben, ist ihre Eignung für die Vorhersage von Links in Hypergraphen noch nicht erforscht - wir füllen diese Lücke in diesem Papier und schlagen Neural Hyperlink Predictor (NHP) vor. NHP passt GCNs für die Vorhersage von Links in Hypergraphen an. Wir schlagen zwei Varianten von NHP vor - NHP-U und NHP-D - für die Vorhersage von Links in ungerichteten bzw. gerichteten Hypergraphen.NHP-D ist unseres Wissens die erste Methode für die Vorhersage von Links in gerichteten Hypergraphen.Durch umfangreiche Experimente mit mehreren realen Datensätzen zeigen wir die Effektivität von NHP.
Diese Methode kann verwendet werden, um die Repräsentation eines Eingabesatzes in mehrere unabhängige Vektoren zu zerlegen, wobei jeder Vektor für einen bestimmten Aspekt des Eingabesatzes verantwortlich ist.Wir evaluieren die vorgeschlagene Methode anhand von zwei Fallstudien: die Umwandlung zwischen verschiedenen sozialen Registern und diachrone Sprachveränderungen.Wir zeigen, dass die vorgeschlagene Methode in der Lage ist, feinkörnige, kontrollierte Veränderungen dieser Aspekte des Eingabesatzes durchzuführen. Wir zeigen, dass das vorgeschlagene Verfahren in der Lage ist, eine feinkörnige, kontrollierte Veränderung dieser Aspekte des Eingabesatzes zu erlernen, z.B. ist unser Modell in der Lage, eine kontinuierliche (statt kategorische) Repräsentation des Satzstils zu erlernen, die der Realität des Sprachgebrauchs entspricht.Das Modell verwendet ein adversarial-motivationales Training und beinhaltet einen speziellen motivationalen Verlust, der dem Diskriminator entgegenwirkt und eine bessere Dekomposition fördert.Schließlich evaluieren wir die erhaltenen Bedeutungseinbettungen an einer nachgelagerten Aufgabe der Erkennung von Parasphrasen und zeigen, dass sie signifikant besser sind als Einbettungen eines regulären Autoencoders.
Wir wurden von einer Gruppe von Gesundheitsdienstleistern angesprochen, die sich mit der Pflege chronisch Kranker befassen und auf der Suche nach potenziellen Technologien sind, die den Prozess der Überprüfung von Patientendaten während klinischer Besuche erleichtern.  Um die Einstellung der Leistungserbringer zur Auswertung von Patientendaten zu verstehen, haben wir (1) eine Fokusgruppe mit einer gemischten Gruppe von Leistungserbringern durchgeführt.2 Um die Perspektive der Patienten zu gewinnen, haben wir (2) acht chronische Patienten interviewt, eine Stichprobe ihrer Daten gesammelt und eine Reihe von Visualisierungen entworfen, die die von uns gesammelten Patientendaten darstellen.3 Schließlich haben wir (3) die Leistungserbringer, die um diese Untersuchung gebeten hatten, um Feedback zu den Visualisierungsdesigns gebeten. Auf der Grundlage der Ergebnisse unserer Studien haben wir die Bedeutung der Gestaltung von patientengenerierten Visualisierungen für den Einzelnen diskutiert, indem wir sowohl den Patienten als auch den Gesundheitsdienstleister in Betracht gezogen haben, anstatt sie mit dem Ziel der Verallgemeinerung zu gestalten, und wir haben Richtlinien für die Gestaltung zukünftiger patientengenerierter Datenvisualisierungen aufgestellt.
Während kurzfristige Bewegungen genau vorhergesagt werden können, leiden langfristige Vorhersagen unter der Akkumulation von Eingabe- und Vorhersagefehlern, die zu plausiblen, aber unterschiedlichen Trajektorien führen können, die von der Grundwahrheit abweichen.  Ein System, das Verteilungen der zukünftigen physikalischen Zustände für lange Zeithorizonte auf der Grundlage seiner Unsicherheit vorhersagt, ist daher eine vielversprechende Lösung.  In dieser Arbeit stellen wir eine neuartige robuste Monte-Carlo-Sampling-Methode vor, die auf einer Graph-Faltungs-Dropout-Methode basiert und es uns ermöglicht, mehrere plausible Trajektorien für einen Anfangszustand zu samplen, wenn ein auf einem neuronalen Netz basierender Vorwärtsdynamik-Prädiktor vorliegt.  Wir zeigen, dass die langfristigen Vorhersagefehler unseres Modells bei komplizierten physikalischen Interaktionen von starren und deformierbaren Objekten verschiedener Formen signifikant niedriger sind als die der existierenden starken Baselines.Schließlich demonstrieren wir, wie die Erzeugung mehrerer Trajektorien mit unserer Monte-Carlo-Dropout-Methode verwendet werden kann, um modellfreie Reinforcement-Learning-Agenten schneller zu trainieren und bessere Lösungen für einfache Manipulationsaufgaben zu finden.
Sowohl in der Vergangenheit als auch in jüngster Zeit gab es ein großes Interesse an den relativen Vorteilen verschiedener Familien universeller Funktionsapproximatoren, z. B. neuronaler Netze, Polynome, rationaler Funktionen usw. Die aktuelle Forschung konzentriert sich jedoch fast ausschließlich auf das Verständnis dieses Problems in einer Worst-Case-Umgebung: z. B. die Charakterisierung der besten L1- oder L_{infty}-Approximation in einer Box (oder manchmal sogar unter einer nachteilig konstruierten Datenverteilung). In dieser Umgebung können viele klassische Werkzeuge der Approximationstheorie effektiv eingesetzt werden. In typischen Anwendungen erwarten wir jedoch, dass die Daten hochdimensional, aber strukturiert sind - es wäre also nur wichtig, die gewünschte Funktion auf dem relevanten Teil ihrer Domäne gut zu approximieren, z. B. auf einer kleinen Mannigfaltigkeit, auf der die realen Eingabedaten tatsächlich liegen.Darüber hinaus kann die gewünschte Qualität der Approximation sogar innerhalb dieser Domäne nicht einheitlich sein; zum Beispiel bei Klassifizierungsproblemen muss die Approximation in der Nähe der Entscheidungsgrenze genauer sein.Diese Fragen sind unseres Wissens bisher unerforscht geblieben.	Vor diesem Hintergrund analysieren wir die Leistung von neuronalen Netzen und Polynomkernen in einer natürlichen Regressionsumgebung, in der die Daten eine spärliche latente Struktur aufweisen und die Beschriftungen auf einfache Weise von den latenten Variablen abhängen.wir geben eine nahezu genaue theoretische Analyse der Leistung von neuronalen Netzen und Polynomen für dieses Problem und überprüfen unsere Theorie mit Simulationen.unsere Ergebnisse beinhalten neue (komplex-analytische) Techniken, die von unabhängigem Interesse sein können, und zeigen wesentliche qualitative Unterschiede zu dem, was im schlimmsten Fall bekannt ist.
Aktuelle tiefe generative Modelle können sowohl fotorealistische Bilder als auch visuelle oder textuelle Einbettungen liefern, die für verschiedene Aufgaben im Bereich des Computersehens und der Verarbeitung natürlicher Sprache nützlich sind.Ihre Nützlichkeit wird jedoch oft durch die mangelnde Kontrolle über den generativen Prozess oder das schlechte Verständnis der gelernten Repräsentation eingeschränkt.Um diese Probleme zu überwinden, haben neuere Arbeiten das Interesse an der Untersuchung der Semantik des latenten Raums generativer Modelle gezeigt. In dieser Arbeit schlagen wir vor, die Interpretierbarkeit des latenten Raums von generativen Modellen zu verbessern, indem wir eine neue Methode einführen, um sinnvolle Richtungen im latenten Raum eines generativen Modells zu finden, entlang derer wir uns bewegen können, um bestimmte Eigenschaften des generierten Bildes, wie Position oder Skalierung des Objekts im Bild, genau zu kontrollieren.Unsere Methode ist schwach überwacht und eignet sich besonders gut für die Suche nach Richtungen, die einfache Transformationen des generierten Bildes kodieren, wie z.B. Translation, Zoom oder Farbvariationen.Wir demonstrieren die Effektivität unserer Methode qualitativ und quantitativ, sowohl für GANs als auch für Variations-Autocodierer.
Moderne neuronale Netzarchitekturen verwenden strukturierte lineare Transformationen wie Low-Rank-Matrizen, dünn besetzte Matrizen, Permutationen und die Fourier-Transformation, um die Inferenzgeschwindigkeit zu erhöhen und den Speicherbedarf im Vergleich zu allgemeinen linearen Karten zu reduzieren. Wir verfolgen einen anderen Ansatz: Wir stellen eine Familie von Matrizen vor, die Kaleidoskop-Matrizen (K-Matrizen) genannt werden und nachweislich jede strukturierte Matrix mit nahezu optimaler Komplexität in Bezug auf Raum (Parameter) und Zeit (arithmetische Operationen) erfassen. Beispielsweise verbessert das Ersetzen von Channel Shuffles in ShuffleNet die Klassifizierungsgenauigkeit bei ImageNet um bis zu 5 %.Lernbare K-Matrizen können auch handgefertigte Pipelines vereinfachen - wir ersetzen die Berechnung von Filterbankmerkmalen in der Sprachdatenvorverarbeitung durch eine Kaleidoskop-Schicht, was zu einem Genauigkeitsverlust von nur 0,4 % bei der TIMIT-Spracherkennungsaufgabe führt. K-Matrizen können auch latente Strukturen in Modellen erfassen: Bei einer anspruchsvollen Klassifizierungsaufgabe für permutierte Bilder kann das Hinzufügen einer K-Matrix zu einer standardmäßigen Faltungsarchitektur das Erlernen der latenten Permutation ermöglichen und die Genauigkeit um mehr als 8 Punkte verbessern.Wir bieten eine praktisch effiziente Implementierung unseres Ansatzes und verwenden K-Matrizen in einem Transformer-Netzwerk, um bei einer Sprachübersetzungsaufgabe eine 36 % schnellere End-to-End-Inferenzgeschwindigkeit zu erreichen.
Wir schlagen eine Methode vor, die als Label Embedding Network bezeichnet wird und mit der während des Trainingsprozesses von Deep Networks eine Label-Repräsentation (Label Embedding) gelernt werden kann. Experimentelle Ergebnisse, die auf konkurrierenden Aufgaben basieren, zeigen die Effektivität der vorgeschlagenen Methode, und die gelernte Einbettung der Bezeichnungen ist vernünftig und interpretierbar.Die vorgeschlagene Methode erzielt vergleichbare oder sogar bessere Ergebnisse als die modernsten Systeme.
Da aktuelle Ansätze durch die Netzwerkbandbreite begrenzt sind, schlagen wir die Verwendung von Kommunikationskompression im dezentralisierten Trainingskontext vor. wir zeigen, dass Choco-SGD eine lineare Beschleunigung in der Anzahl der Arbeiter für beliebig hohe Kompressionsverhältnisse auf allgemeinen nicht-konvexen Funktionen und nicht-IID Trainingsdaten erreicht. Wir demonstrieren die praktische Leistung des Algorithmus in zwei Schlüsselszenarien: das Training von Deep-Learning-Modellen(i) über dezentralisierte Benutzergeräte, die über ein Peer-to-Peer-Netzwerk verbunden sind, und(ii) in einem Rechenzentrum.
Wir zeigen, dass Entropie-SGD (Chaudhari et al., 2017), wenn es als Lernalgorithmus betrachtet wird, eine PAC-Bayes-Schranke für das Risiko eines Gibbs-Klassifikators (posterior) optimiert, d. h., Entropie-SGD funktioniert durch die Optimierung des Priors der Schranke, was die Hypothese des PAC-Bayes-Theorems verletzt, dass der Prior unabhängig von den Daten gewählt wird. Um eine gültige Generalisierungsschranke zu erhalten, zeigen wir, dass ein ε-differentiell privater Prior eine gültige PAC-Bayes-Schranke liefert, was eine direkte Folge der Ergebnisse ist, die Generalisierung mit differentieller Privatsphäre verbinden. Unter Verwendung der stochastischen Gradienten-Langevin-Dynamik (SGLD) zur Annäherung an den bekannten exponentiellen Freisetzungsmechanismus stellen wir fest, dass der Generalisierungsfehler bei MNIST (gemessen an zurückgehaltenen Daten) innerhalb der (empirisch nicht eindeutigen) Grenzen liegt, die unter der Annahme berechnet werden, dass SGLD perfekte Stichproben erzeugt.
In diesem Papier untersuchen wir das Lernen der tiefen neuronalen Netze für die automatisierte optische Inspektion in der industriellen Fertigung.unser vorläufiges Ergebnis hat die erstaunliche Leistungsverbesserung durch Transfer-Lernen von der völlig unähnlichen Quelle Domain gezeigt: Das experimentelle Ergebnis zeigt, dass es einen vernachlässigbaren Genauigkeitsabfall in dem Netzwerk gibt, das durch Transfer-Lernen gelernt wurde, bis es auf 1/128 Reduktion der Anzahl der Faltungsfilter komprimiert wird. Dieses Ergebnis steht im Gegensatz zu der Kompression ohne Transfer-Lernen, die mehr als 5% Genauigkeit bei der gleichen Kompressionsrate verliert.
Das klassische Label-Propagation-Verfahren hat trotz seiner Popularität eine begrenzte Modellierungsfähigkeit, da es nur Grapheninformationen für Vorhersagen nutzt.In diesem Papier betrachten wir Label-Propagation aus der Perspektive der Graphen-Signalverarbeitung und zerlegen sie in drei Komponenten: Signal, Filter und Klassifikator.Durch die Erweiterung der drei Komponenten schlagen wir ein einfaches verallgemeinertes Label-Propagation-Verfahren (GLP) für semi-supervised Learning vor. GLP integriert auf natürliche Weise Graphen- und Dateninformationen und bietet die Flexibilität, geeignete Filter und domänenspezifische Klassifikatoren für verschiedene Anwendungen auszuwählen.Interessanterweise bietet GLP auch neue Einblicke in das populäre Graphenfaltungsnetzwerk und erläutert seine Arbeitsmechanismen.Umfangreiche Experimente mit drei Zitationsnetzwerken, einem Wissensgraphen und einem Bilddatensatz zeigen die Effizienz und Effektivität von GLP.
Da die Wahl und die Einstellung des Optimierers die Geschwindigkeit und letztlich die Leistung des Deep Learning beeinflusst, gibt es in diesem Bereich eine Vielzahl von Forschungsarbeiten aus der Vergangenheit und der jüngeren Vergangenheit, aber, was vielleicht überraschend ist, es gibt kein allgemein anerkanntes Protokoll für die quantitative und reproduzierbare Bewertung von Optimierungsstrategien für Deep Learning. Wir schlagen Routinen und Benchmarks für die stochastische Optimierung vor, mit besonderem Augenmerk auf die einzigartigen Aspekte des Deep Learning, wie Stochastizität, Abstimmbarkeit und Generalisierung.Als primären Beitrag präsentieren wir DeepOBS, ein Python-Paket von Deep Learning Optimierungsbenchmarks.Das Paket adressiert die wichtigsten Herausforderungen bei der quantitativen Bewertung von stochastischen Optimierern und automatisiert die meisten Schritte des Benchmarkings. Die Bibliothek enthält eine breite und erweiterbare Auswahl an gebrauchsfertigen, realistischen Optimierungsproblemen, wie z.B. das Training von Residualnetzwerken für die Bildklassifikation auf ImageNet oder Sprachvorhersagemodelle auf Zeichenebene, sowie beliebte Klassiker wie MNIST und CIFAR-10. Das Paket bietet auch realistische Basisergebnisse für die beliebtesten Optimierer auf diesen Testproblemen, um einen fairen Vergleich mit der Konkurrenz beim Benchmarking neuer Optimierer zu gewährleisten, ohne dass kostspielige Experimente durchgeführt werden müssen.Es wird mit Ausgabe-Backends geliefert, die direkt LaTeX-Code für die Aufnahme in akademische Publikationen erzeugen.Es unterstützt TensorFlow und ist als Open Source verfügbar.
Pre-trained word embeddings are the primarymethod for transfer learning in several Natural Language Processing (NLP) tasks.Recent works have focused on using unsupervisedtechniques such as language modeling to obtain these embeddings.In contrast, this workfocused on extracting representations frommultiple pre-trained supervised models, whichenriches word embeddings with task and domain specific knowledge. Experimente, die in aufgaben-, domänen- und sprachübergreifenden Kontexten durchgeführt wurden, zeigen, dass solche überwachten Einbettungen hilfreich sind, insbesondere in ressourcenarmen Kontexten, aber das Ausmaß der Gewinne hängt von der Art der Aufgabe und der Domäne ab.
Wir entwickeln einen theoretischen Rahmen für das Verständnis praktischer Meta-Learning-Methoden, der die Integration anspruchsvoller Formalisierungen der Aufgabenähnlichkeit mit der umfangreichen Literatur über konvexe Online-Optimierung und sequenzielle Vorhersagealgorithmen ermöglicht, um Leistungsgarantien innerhalb einer Aufgabe zu bieten. Unser Ansatz verbessert die jüngsten Analysen des Parameter-Transfers, indem er es ermöglicht, die Aufgabenähnlichkeit adaptiv zu erlernen und die Grenzen des Transferrisikos im Rahmen des statistischen Lernens zu verbessern, und er führt auch zu einfachen Ableitungen der Grenzen des durchschnittlichen Bedauerns für effiziente Algorithmen in Situationen, in denen sich die Aufgabenumgebung dynamisch ändert oder die Aufgaben eine bestimmte geometrische Struktur aufweisen.
In dieser Arbeit schlagen wir eine selbstüberwachte Methode zum Erlernen von Satzrepräsentationen mit einer Injektion von linguistischem Wissen vor. Mehrere linguistische Rahmenwerke schlagen verschiedene Satzstrukturen vor, aus denen semantische Bedeutung aus kompositorischen Wortoperationen ausgedrückt werden könnte. Durch die Gegenüberstellung verschiedener linguistischer Sichtweisen wollen wir Einbettungen aufbauen, die die semantische Bedeutung besser erfassen und weniger empfindlich auf die äußere Form des Satzes reagieren.
Das periphere Nervensystem stellt das Input-/Output-System für das Gehirn dar, das mit Hilfe von implantierten Manschettenelektroden beobachtet und kontrolliert werden kann. Die von diesen Elektroden erzeugten Daten weisen jedoch ein geringes Signal-Rausch-Verhältnis und einen komplexen Signalinhalt auf.In diesem Beitrag befassen wir uns mit der Analyse neuronaler Daten, die vom Vagusnerv in Tiermodellen aufgezeichnet wurden, und entwickeln einen unüberwachten Lerner auf der Grundlage von neuronalen Faltungsnetzen, der in der Lage ist, die Daten gleichzeitig zu entrauschen und nach Signalinhalt zu clustern.
Stochastische Eingabetransformationsmethoden wurden vorgeschlagen, bei denen die Idee darin besteht, das Bild durch eine zufällige Transformation vor Angriffen zu schützen und die Mehrheitsentscheidung als Konsens unter den Zufallsstichproben zu nehmen. Während es intuitiv ist, dass sich die Genauigkeit auf sauberen Bildern verschlechtern würde, ist der genaue Mechanismus, wie dies geschieht, unklar.In diesem Papier untersuchen wir die Verteilung von Softmax, die durch stochastische Transformationen induziert wird. Wir beobachten, dass mit zufälligen Transformationen auf den sauberen Bildern, obwohl die Masse der Softmax-Verteilung könnte auf die falsche Klasse zu verschieben, die sich ergebende Verteilung der Softmax könnte verwendet werden, um die Vorhersage zu korrigieren.Darüber hinaus auf die gegnerischen Gegenstücke, mit dem Bild Transformation, die sich ergebenden Formen der Verteilung der Softmax sind ähnlich wie die Verteilungen aus den sauberen Bildern.Mit diesen Beobachtungen, schlagen wir eine Methode zur Verbesserung der bestehenden Transformation-basierte Abwehr. Wir trainieren einen separaten, leichtgewichtigen Verteilungsklassifikator, um unterschiedliche Merkmale in den Verteilungen von Softmax-Ausgängen transformierter Bilder zu erkennen. Unsere empirischen Studien zeigen, dass unser Verteilungsklassifikator durch Training auf Verteilungen, die nur von sauberen Bildern erhalten wurden, die Mehrheitsabstimmung sowohl für saubere als auch für gegnerische Bilder übertrifft.Unsere Methode ist generisch und kann in bestehende transformationsbasierte Verteidigungsmaßnahmen integriert werden.
Wir stellen einen Algorithmus zur Akteurskritik vor, der dezentralisierte Richtlinien in Multi-Agenten-Szenarien trainiert, indem er zentral berechnete Kritiken verwendet, die einen Aufmerksamkeitsmechanismus teilen, der relevante Informationen für jeden Agenten in jedem Zeitschritt auswählt. Unser Ansatz ist nicht nur auf kooperative Umgebungen mit geteilten Belohnungen anwendbar, sondern auch auf individualisierte Belohnungsumgebungen, einschließlich gegnerischer Umgebungen, und er macht keine Annahmen über die Aktionsräume der Agenten und ist daher flexibel genug, um auf die meisten Multi-Agenten-Lernprobleme angewendet zu werden.
Die jüngste Ausweitung von Anwendungen des maschinellen Lernens auf die Molekularbiologie hat sich als wichtiger Beitrag zum Verständnis biologischer Systeme und insbesondere der Funktionsweise des Genoms erwiesen. Technologische Fortschritte ermöglichten die Sammlung großer epigenetischer Datensätze, einschließlich Informationen über verschiedene DNA-Bindefaktoren (ChIP-Seq) und die räumliche Struktur der DNA (Hi-C). In dieser Arbeit konzentrieren wir uns auf Methoden des maschinellen Lernens zur Vorhersage von Faltungsmustern der DNA in einem klassischen Modellorganismus, Drosophila melanogaster, und betrachten lineare Modelle mit vier Arten der Regularisierung, Gradient Boosting und rekurrente neuronale Netze für die Vorhersage von Chromatin-Faltungsmustern aus epigenetischen Markierungen. Das bidirektionale LSTM-RNN-Modell übertraf alle anderen Modelle und erzielte die besten Vorhersageergebnisse, was die Nutzung komplexer Modelle und die Bedeutung des Gedächtnisses von sequenziellen DNA-Zuständen für die Chromatinfaltung demonstriert.
Frühere Arbeiten haben empirisch gezeigt, dass große neuronale Netze unter Beibehaltung ihrer Genauigkeit erheblich verkleinert werden können. Die meisten Kompressionsmethoden basieren auf Heuristiken und bieten keine Worst-Case-Garantien für den Kompromiss zwischen der Kompressionsrate und dem Approximationsfehler für eine beliebige neue Probe. Unsere Methode basiert auf dem Coreset-Framework, das eine kleine gewichtete Teilmenge von Punkten findet, die nachweislich die ursprünglichen Eingaben approximiert. Insbesondere approximieren wir die Ausgabe einer Schicht von Neuronen durch ein Coreset von Neuronen in der vorherigen Schicht und verwerfen den Rest.Wir wenden dieses Framework schichtweise von oben nach unten an. Im Gegensatz zu früheren Arbeiten ist unser Coreset datenunabhängig, d.h. es garantiert nachweislich die Genauigkeit der Funktion für jede Eingabe $x\in \mathbb{R}^d$, einschließlich einer gegnerischen.Wir demonstrieren die Effektivität unserer Methode auf populären Netzwerkarchitekturen.Insbesondere ergeben unsere Coresets eine 90%ige Kompression der LeNet-300-100-Architektur auf MNIST bei gleichzeitiger Verbesserung der Genauigkeit.
Planungsprobleme in teilweise beobachtbaren Umgebungen können nicht direkt mit Faltungsnetzwerken gelöst werden und erfordern eine Form von Speicher, aber selbst Speichernetzwerke mit ausgefeilten Adressierungsschemata sind aufgrund der Komplexität des gleichzeitigen Lernens von Speicherzugriff und Planung nicht in der Lage, intelligentes Denken zufriedenstellend zu erlernen.Um diese Herausforderungen zu mildern, schlagen wir das Memory Augmented Control Network (MACN) vor. Auf einer niedrigeren Ebene lernt es, in einem lokal beobachteten Raum zu planen, und auf einer höheren Ebene verwendet es eine Sammlung von Richtlinien, die auf lokal beobachteten Räumen berechnet werden, um einen optimalen Plan in der globalen Umgebung zu lernen, in der es operiert.
Kontextualisierte Wortrepräsentationen wie ELMo und BERT sind de facto zum Ausgangspunkt für die Einbeziehung von vortrainierten Repräsentationen für nachgelagerte NLP-Aufgaben geworden und haben ihre statischen Einbettungsvorgänger wie Word2Vec und GloVe weitgehend überflüssig gemacht. In dieser Arbeit stellen wir einfache Methoden zur Erzeugung von statischen Lookup-Table-Einbettungen aus bestehenden, vortrainierten kontextuellen Repräsentationen vor und zeigen, dass sie die Word2Vec- und GloVe-Einbettungen bei einer Reihe von Wortähnlichkeits- und Wortverwandtschaftsaufgaben übertreffen. Dabei zeigen unsere Ergebnisse auch Erkenntnisse, die für nachfolgende Aufgaben nützlich sein können, bei denen unsere Einbettungen oder die ursprünglichen kontextuellen Modelle verwendet werden. Unsere Analyse stellt die umfassendste Studie über soziale Verzerrungen in kontextuellen Wortrepräsentationen dar (über den Proxy unserer destillierten Einbettungen) und zeigt eine Reihe von Ungereimtheiten in aktuellen Techniken zur Quantifizierung von sozialen Verzerrungen in Worteinbettungen.
Das Gehirn führt unüberwachtes Lernen und (vielleicht) gleichzeitig überwachtes Lernen durch, was die Frage aufwirft, ob ein Hybrid aus überwachten und unüberwachten Methoden ein besseres Lernen ermöglicht.Inspiriert durch den reichhaltigen Raum der Hebbian-Lernregeln, haben wir uns vorgenommen, die unüberwachte Lernregel auf der Basis lokaler Informationen direkt zu lernen, die ein überwachtes Signal am besten ergänzt. Wir stellen den Hebbian-augmentierten Trainingsalgorithmus (HAT) vor, der gradientenbasiertes Lernen mit einer unbeaufsichtigten Regel für präsynaptische Aktivitäten, postsynaptische Aktivitäten und aktuelle Gewichte kombiniert.Wir testen die Wirkung von HAT auf ein einfaches Problem (Fashion-MNIST) und finden eine durchgängig höhere Leistung als beim überwachten Lernen allein.Dieses Ergebnis liefert empirische Beweise dafür, dass unbeaufsichtigtes Lernen auf synaptischen Aktivitäten ein starkes Signal liefert, das zur Erweiterung gradientenbasierter Methoden verwendet werden kann.        Wir stellen außerdem fest, dass die meta-gelernte Aktualisierungsregel eine zeitvariable Funktion ist; daher ist es schwierig, eine interpretierbare Hebbsche Aktualisierungsregel zu finden, die beim Training hilft.  Wir stellen fest, dass der Meta-Learner schließlich zu einer nicht-hebbianischen Regel degeneriert, die wichtige Gewichte beibehält, um die Konvergenz des Lerners nicht zu stören.
Tiefe Faltungsnetzwerke fügen ihren Faltungsoperationen häufig additive konstante Terme ("Bias") hinzu, die ein reichhaltigeres Repertoire an funktionalen Zuordnungen ermöglichen. Biases werden auch verwendet, um das Training zu erleichtern, indem die mittlere Reaktion über Stapel von Trainingsbildern subtrahiert wird (eine Komponente der "Stapelnormalisierung"). Wir zeigen jedoch, dass die in den meisten CNNs verwendeten Bias-Terme (additive Konstanten, einschließlich der für die Stapelnormalisierung verwendeten) die Interpretierbarkeit dieser Netze beeinträchtigen, die Leistung nicht verbessern und sogar die Verallgemeinerung der Leistung auf Rauschpegel verhindern, die nicht in den Trainingsdaten enthalten sind. Diese Analysen liefern Interpretationen der Netzwerkfunktionalität in Form von Projektionen auf eine Vereinigung von niedrigdimensionalen Unterräumen und verbinden die lernbasierte Methode mit traditionelleren Entrauschungsmethoden.
Die Auswahl der anfänglichen Parameterwerte für die gradientenbasierte Optimierung von tiefen neuronalen Netzen ist eine der einflussreichsten Hyperparameter-Entscheidungen in tiefen Lernsystemen, die sich sowohl auf die Konvergenzzeiten als auch auf die Leistung des Modells auswirkt.Trotz umfangreicher empirischer und theoretischer Analysen ist relativ wenig über die konkreten Auswirkungen verschiedener Initialisierungsschemata bewiesen worden.In dieser Arbeit analysieren wir die Auswirkungen der Initialisierung in tiefen linearen Netzen und liefern zum ersten Mal einen strengen Beweis dafür, dass das Ziehen der anfänglichen Gewichte aus der orthogonalen Gruppe die Konvergenz im Vergleich zur Standard-Gauß-Initialisierung mit iid-Gewichten beschleunigt. Wir zeigen, dass für tiefe Netzwerke die Breite, die für eine effiziente Konvergenz zu einem globalen Minimum mit orthogonalen Initialisierungen benötigt wird, unabhängig von der Tiefe ist, während die Breite, die für eine effiziente Konvergenz mit Gauß'schen Initialisierungen benötigt wird, linear mit der Tiefe skaliert Unsere Ergebnisse zeigen, wie die Vorteile einer guten Initialisierung während des gesamten Lernprozesses bestehen bleiben können, was eine Erklärung für die jüngsten empirischen Erfolge liefert, die durch die Initialisierung sehr tiefer nichtlinearer Netzwerke nach dem Prinzip der dynamischen Isometrie gefunden wurden.
Die Schätzung von Überlebensfunktionen wird in vielen Disziplinen eingesetzt, am häufigsten jedoch in der medizinischen Analytik in Form des Kaplan-Meier-Schätzers, bei dem sensible Daten (Patientenakten) ohne explizite Kontrolle des Informationsverlustes verwendet werden, was ein erhebliches Problem für den Datenschutz darstellt. Wir schlagen einen ersten differenziell privaten Schätzer der Überlebensfunktion vor und zeigen, dass er leicht erweitert werden kann, um differenziell private Konfidenzintervalle und Teststatistiken zu liefern, ohne ein zusätzliches Datenschutzbudget aufzuwenden.Wir bieten außerdem Erweiterungen für die differenziell private Schätzung der kumulativen Inzidenzfunktion mit konkurrierendem Risiko.Anhand von neun realen klinischen Datensätzen liefern wir empirische Beweise, dass unsere vorgeschlagene Methode einen guten Nutzen bietet und gleichzeitig starke Datenschutzgarantien bietet.
Neuronale Netze können lernen, statistische Eigenschaften aus Daten zu extrahieren, aber sie machen selten Gebrauch von strukturierten Informationen aus dem Label-Raum, um das Repräsentationslernen zu unterstützen. Obwohl einige Label-Strukturen implizit erhalten werden können, wenn sie auf riesigen Datenmengen trainiert werden, kann in einem "few-shot"-Lernkontext, in dem nur wenige Daten zur Verfügung stehen, die explizite Nutzung der Label-Struktur das Modell informieren, um den Repräsentationsraum umzugestalten, um einen globalen Sinn für Klassenabhängigkeiten zu reflektieren.  Wir schlagen einen Meta-Lernrahmen vor, Conditional class-Aware Meta-Learning (CAML), der die Merkmalsrepräsentationen basierend auf einem metrischen Raum, der trainiert wurde, um die Abhängigkeiten zwischen den Klassen zu erfassen, bedingt transformiert, was eine bedingte Modulation der Merkmalsrepräsentationen des Basis-Lerners ermöglicht, um Regelmäßigkeiten zu erzwingen, die durch den Label-Raum informiert werden.
Das Ziel der Multiset-Vorhersage ist es, einen Prädiktor zu trainieren, der eine Eingabe auf ein Multiset abbildet, das aus mehreren Elementen besteht. Im Gegensatz zu bestehenden Problemen des überwachten Lernens, wie Klassifizierung, Ranking und Sequenzgenerierung, ist die Reihenfolge der Elemente in einem Ziel-Multiset nicht bekannt, und jedes Element im Multiset kann mehr als einmal vorkommen, was dieses Problem extrem schwierig macht. In diesem Papier schlagen wir eine neuartige Multiset-Verlustfunktion vor, indem wir dieses Problem aus der Perspektive der sequentiellen Entscheidungsfindung betrachten. die vorgeschlagene Multiset-Verlustfunktion wird empirisch auf zwei Familien von Datensätzen evaluiert, eine synthetische und die andere reale, mit unterschiedlichen Schwierigkeitsgraden, gegen verschiedene Grundlinien-Verlustfunktionen, einschließlich Verstärkungslernen, Sequenz- und aggregierte Verteilungsvergleichs-Verlustfunktionen. die Experimente zeigen die Wirksamkeit der vorgeschlagenen Verlustfunktion gegenüber den anderen.
Das Verständnis der theoretischen Eigenschaften von tiefen und lokal verbundenen nichtlinearen Netzwerken, wie z.B. tiefe Faltungsneuronale Netze (DCNN), ist trotz ihres empirischen Erfolges immer noch ein schwieriges Problem.In diesem Papier schlagen wir einen neuen theoretischen Rahmen für solche Netzwerke mit ReLU-Nichtlinearität vor. Der Rahmen überbrückt Datenverteilung mit Gradientenabstiegsregeln, begünstigt entwirrte Darstellungen und ist kompatibel mit gängigen Regularisierungstechniken wie Batch Norm, nach einer neuartigen Entdeckung seiner Projektionsnatur.Der Rahmen ist auf Lehrer-Schüler-Einstellung aufgebaut, indem der Vorwärts-/Rückwärtsdurchlauf des Schülers auf den Berechnungsgraphen des Lehrers projiziert wird.Wir setzen keine unrealistischen Annahmen voraus (z.B., Unser Rahmen könnte dazu beitragen, die theoretische Analyse vieler praktischer Probleme zu erleichtern, z.B. entkoppelte Repräsentationen in tiefen Netzwerken.
Viele Aufgaben beinhalten individuelle Anreize, die nicht mit dem Gemeinwohl übereinstimmen, dennoch sind eine Vielzahl von Organismen, von Bakterien über Insekten bis hin zu Menschen, in der Lage, ihre Unterschiede zu überwinden und zusammenzuarbeiten. Durch die Kombination von MARL mit angemessen strukturierter natürlicher Selektion zeigen wir, dass individuelle induktive Neigungen zur Kooperation modellfrei erlernt werden können. Um dies zu erreichen, führen wir eine innovative modulare Architektur für Deep Reinforcement Learning-Agenten ein, die eine mehrstufige Selektion unterstützt. Wir präsentieren Ergebnisse in zwei anspruchsvollen Umgebungen und interpretieren diese im Kontext der kulturellen und ökologischen Evolution.
Bei gegnerischen Angriffen auf Klassifizierer des maschinellen Lernens werden der korrekt klassifizierten Eingabe kleine Störungen hinzugefügt, die zu gegnerischen Beispielen führen, die von der ungestörten Eingabe praktisch nicht zu unterscheiden sind und dennoch falsch klassifiziert werden.In standardmäßigen neuronalen Netzen, die für Deep Learning verwendet werden, können Angreifer aus den meisten Eingaben gegnerische Beispiele erstellen, um eine Fehlklassifizierung ihrer Wahl zu verursachen. Wir führen eine neue Art von Netzwerkeinheiten ein, die so genannten RBFI-Einheiten, deren nichtlineare Struktur sie inhärent resistent gegen negative Angriffe macht: Auf der Permutationsinvariante MNIST erreichen Netzwerke mit RBFI-Einheiten ohne negative Angriffe die Leistung von Netzwerken mit Sigmoid-Einheiten und liegen leicht unter der Genauigkeit von Netzwerken mit ReLU-Einheiten. Wenn sie nachteiligen Angriffen ausgesetzt werden, die auf Methoden des projizierten Gradientenabstiegs oder schnellen Gradientensigns basieren, behalten Netzwerke mit RBFI-Einheiten eine Genauigkeit von über 75 %, während die Genauigkeit von ReLU- oder Sigmoid-Einheiten auf unter 1 % sinkt. Die nichtlineare Struktur von RBFI-Einheiten macht es schwierig, sie mit Hilfe von Standard-Gradientenabstieg zu trainieren. Wir zeigen, dass RBFI-Netzwerke mit RBFI-Einheiten effizient zu hohen Genauigkeiten trainiert werden können, indem Pseudogradienten verwendet werden, die mit Funktionen berechnet werden, die speziell zur Erleichterung des Lernens anstelle ihrer wahren Ableitungen entwickelt wurden.
Wir zeigen, dass es ein inhärentes Spannungsverhältnis zwischen dem Ziel der Robustheit gegenüber widrigen Umständen und dem Ziel der Standardgeneralisierung gibt. Wir zeigen, dass dieser Zielkonflikt zwischen der Standardgenauigkeit eines Modells und seiner Robustheit gegenüber gegnerischen Störungen sogar in einer recht einfachen und natürlichen Umgebung nachweisbar ist und bestätigen damit ein ähnliches Phänomen, das in der Praxis beobachtet wurde. Die Ergebnisse bestätigen auch ein ähnliches Phänomen, das in der Praxis zu beobachten ist, und wir argumentieren, dass dieses Phänomen eine Folge davon ist, dass robuste Klassifizierer grundlegend andere Merkmalsrepräsentationen lernen als Standardklassifizierer, die unerwartete Vorteile mit sich bringen: Die von robusten Modellen gelernten Merkmale stimmen tendenziell besser mit auffälligen Datenmerkmalen und der menschlichen Wahrnehmung überein.
In dieser Arbeit schlagen wir Mixup vor, ein einfaches Lernprinzip, um diese Probleme zu lindern. mixup trainiert ein neuronales Netzwerk auf konvexen Kombinationen von Beispielpaaren und ihren Bezeichnungen.  Auf diese Weise reguliert mixup das neuronale Netz, um ein einfaches lineares Verhalten zwischen den Trainingsbeispielen zu begünstigen.  Unsere Experimente mit den Datensätzen ImageNet-2012, CIFAR-10, CIFAR-100, Google-Befehle und UCI zeigen, dass mixup die Generalisierung von modernen neuronalen Netzwerkarchitekturen verbessert.  Wir stellen außerdem fest, dass Mixup die Speicherung fehlerhafter Markierungen reduziert, die Robustheit gegenüber gegnerischen Beispielen erhöht und das Training generativer, gegnerischer Netzwerke stabilisiert.
Wir stellen einen neuartigen Ansatz zur Spike-Sortierung für Multielektroden-Sonden mit hoher Dichte vor, bei dem der Neural Clustering Process (NCP) zum Einsatz kommt, eine kürzlich eingeführte neuronale Architektur, die eine skalierbare amortisierte approximative Bayes'sche Inferenz für effizientes probabilistisches Clustering durchführt. Das NCP-Spike-Sortiermodell, das ausschließlich auf gelabelten synthetischen Spikes eines einfachen generativen Modells trainiert wurde, zeigt eine vielversprechende Leistung beim Clustern von mehrkanaligen Spike-Wellenformen. Das Modell bietet eine höhere Clustering-Qualität als ein alternativer Bayes-Algorithmus, findet mehr Spike-Templates mit klaren rezeptiven Feldern auf realen Daten und stellt mehr Ground-Truth-Neuronen auf hybriden Testdaten im Vergleich zu einem neueren Spike-Sorting-Algorithmus wieder her.NCP ist darüber hinaus in der Lage, die Clustering-Unsicherheit von mehrdeutigen kleinen Spikes durch GPU-parallelisiertes Posterior-Sampling zu behandeln.der Quellcode ist öffentlich verfügbar.
Es wird gezeigt, dass der Bayes'sche Ansatz zu einer Lösung führt, die von der Statistik der Trainingsdaten und nicht von einzelnen Proben abhängt. Die Lösung ist bei Störungen der Trainingsdaten stabil, da sie durch einen integralen Beitrag mehrerer Maxima der Wahrscheinlichkeit und nicht durch ein einzelnes globales Maximum definiert ist. Konkret wird die Bayes'sche Wahrscheinlichkeitsverteilung der Parameter (Gewichte) eines probabilistischen Modells, das durch ein neuronales Netz gegeben ist, über rekurrente Variationsapproximationen geschätzt. Die abgeleiteten rekurrenten Aktualisierungsregeln entsprechen SGD-Regeln, um ein Minimum eines effektiven Verlusts zu finden, der ein Durchschnitt einer ursprünglichen negativen Log-Likelihood über die Gauß'schen Verteilungen der Gewichte ist, was ihn zu einer Funktion von Mittelwerten und Varianzen macht. Unter den stationären Lösungen der Aktualisierungsregeln gibt es triviale Lösungen mit Null-Varianzen an lokalen Minima des ursprünglichen Verlusts und eine einzige nicht-triviale Lösung mit endlichen Varianzen, die ein kritischer Punkt am Ende der Konvexität des effektiven Verlusts im Mittelwert-Varianz-Raum ist. Die empirische Studie bestätigt, dass der kritische Punkt die am besten verallgemeinerbare Lösung darstellt. Während die Lage des kritischen Punktes im Gewichtsraum von den Besonderheiten des verwendeten probabilistischen Modells abhängt, sind einige Eigenschaften des kritischen Punktes universell und modellunabhängig.
Menschen verlassen sich ständig auf das episodische Gedächtnis, wenn sie sich an den Namen von jemandem erinnern, den sie vor 10 Minuten getroffen haben, an die Handlung eines Films, der sich gerade abspielt, oder daran, wo sie ihr Auto geparkt haben.Die Ausstattung von Agenten für Reinforcement Learning mit episodischem Gedächtnis ist ein wichtiger Schritt auf dem Weg zu einer menschenähnlichen allgemeinen Intelligenz.Wir analysieren, warum Standard-RL-Agenten heute kein episodisches Gedächtnis haben und warum bestehende RL-Aufgaben es nicht erfordern. Um das episodische Gedächtnis zu evaluieren, definieren wir eine RL-Aufgabe, die auf dem gewöhnlichen Kinderspiel Konzentration basiert, und stellen fest, dass ein MEM-RL-Agent das episodische Gedächtnis effektiv nutzt, um Konzentration zu meistern, im Gegensatz zu den von uns getesteten Basisagenten.
Anstatt die Parameter von Grund auf neu zu lernen und das Lernen durch Optimierung zu ersetzen, schlagen wir einen Rahmen vor, der auf der Theorie des optimalen Transports aufbaut, um Modellparameter durch die Entdeckung von Korrespondenzen zwischen Modellen und Daten anzupassen und so die Trainingskosten erheblich zu amortisieren.Wir demonstrieren unsere Idee an dem anspruchsvollen Problem der Erstellung probabilistischer räumlicher Darstellungen für autonome Roboter. Obwohl neuere Kartierungstechniken eine robuste Belegungskartierung ermöglicht haben, erfordert das Erlernen aller räumlich unterschiedlichen Parameter in solchen approximativen Bayes'schen Modellen eine beträchtliche Rechenzeit, was sie davon abhält, in der realen Welt der Roboterkartierung eingesetzt zu werden.In Anbetracht der Tatsache, dass die geometrischen Merkmale, die ein Roboter mit seinen Sensoren beobachten würde, in verschiedenen Umgebungen ähnlich sind, zeigen wir in diesem Papier, wie man Parameter und Hyperparameter, die in verschiedenen Domänen gelernt wurden, wiederverwenden kann. Eine Reihe von Experimenten, die in realistischen Umgebungen durchgeführt wurden, hat gezeigt, dass es möglich ist, Tausende solcher Parameter mit vernachlässigbaren Zeit- und Speicherkosten zu übertragen, was eine groß angelegte Kartierung in städtischen Umgebungen ermöglicht.
Es wird ein Algorithmus zum Erlernen einer prädiktiven Zustandsrepräsentation mit Off-Policy Temporal Difference (TD) Learning vorgestellt, der dann zum Erlernen der Fahrzeugsteuerung mit Reinforcement Learning verwendet wird.  Es werden drei Komponenten gleichzeitig gelernt:  (1) die Off-Policy-Vorhersagen als kompakte Repräsentation des Zustands, (2) die Verhaltenspolitik-Verteilung zur Schätzung der Off-Policy-Vorhersagen und (3) der deterministische Politik-Gradient zum Erlernen des Handelns.  Ein Verhaltenspolitik-Diskriminator wird erlernt und zur Schätzung der wichtigen Stichprobenverhältnisse verwendet, die zum Erlernen der prädiktiven Repräsentation außerhalb der Politik mit allgemeinen Wertfunktionen (GVFs) erforderlich sind.  Eine lineare deterministische Gradientenmethode wird verwendet, um den Agenten nur mit den prädiktiven Repräsentationen zu trainieren, während die Vorhersagen gelernt werden.  Alle drei Komponenten werden kombiniert, demonstriert und anhand des Problems der Fahrzeugsteuerung anhand von Bildern in der TORCS-Rennsimulatorumgebung bewertet.  Experimente zeigen, dass die vorgeschlagene Methode in der Lage ist, reibungslos zu steuern und viele, aber nicht alle der in TORCS verfügbaren Strecken mit einer Leistung zu navigieren, die DDPG übertrifft, wenn nur Bilder als Eingabe verwendet werden, und sich der Leistung eines idealen, nicht visuell basierten Kinematikmodells nähert.
In zahlreichen Bereichen, wie z.B. Erdbeobachtung, medizinische Bildgebung, Astrophysik,..., weisen die verfügbaren Bild- und Signaldatensätze oft unregelmäßige Raum-Zeit-Abtastmuster und große Fehlmengen auf, In diesem Beitrag befassen wir uns mit dem durchgängigen Lernen von Repräsentationen von Signalen, Bildern und Bildsequenzen aus unregelmäßig abgetasteten Daten, d.h. wenn die Trainingsdaten fehlende Daten enthalten.In Analogie zur Bayes'schen Formulierung betrachten wir energiebasierte Repräsentationen.Zwei Energieformen werden untersucht: eine, die von Auto-Encodern abgeleitet ist, und eine, die sich auf Gibbs'sche Energien bezieht. Die Lernphase dieser energiebasierten Repräsentationen (oder Prioren) beinhaltet ein gemeinsames Interpolationsproblem, das auf die Lösung eines Energieminimierungsproblems unter Beobachtungsbeschränkungen zurückgreift. Durch die Verwendung einer neuronalen Netzwerk-basierten Implementierung der betrachteten Energieformen können wir ein End-to-End-Lernschema aus unregelmäßig abgetasteten Daten angeben.
Die Credit-Zuweisung beim Meta-Reinforcement Learning (Meta-RL) ist noch wenig verstanden.Bestehende Methoden vernachlässigen entweder die Credit-Zuweisung zum Voradaptionsverhalten oder implementieren sie naiv.Dies führt zu einer schlechten Stichproben-Effizienz während des Meta-Trainings sowie zu ineffektiven Aufgaben-Identifikations-Strategien.Dieses Papier bietet eine theoretische Analyse der Credit-Zuweisung im gradientenbasierten Meta-RL. Aufbauend auf den gewonnenen Erkenntnissen entwickeln wir einen neuartigen Meta-Learning-Algorithmus, der sowohl das Problem der schlechten Credit-Zuweisung als auch frühere Schwierigkeiten bei der Schätzung von Meta-Politik-Gradienten überwindet.Durch die Kontrolle des statistischen Abstands sowohl der Pre-Adaptation als auch der adaptierten Politiken während der Meta-Politik-Suche ermöglicht der vorgeschlagene Algorithmus ein effizientes und stabiles Meta-Learning.Unser Ansatz führt zu einem überlegenen Pre-Adaptation-Politik-Verhalten und übertrifft durchweg frühere Meta-RL-Algorithmen in Bezug auf Stichproben-Effizienz, Wall-Clock-Zeit und asymptotische Leistung.
Wenn man ein neuronales Netzwerk für eine bestimmte Aufgabe trainiert, kann man es vorziehen, ein vortrainiertes Netzwerk zu adaptieren, anstatt mit einem zufällig initialisierten Netzwerk zu beginnen - sei es, weil man nicht genügend Trainingsdaten hat, sei es, weil man ein lebenslanges Lernen durchführt, bei dem das System eine neue Aufgabe lernen muss, während es zuvor für andere Aufgaben trainiert wurde, oder sei es, weil man Prioritäten im Netzwerk über voreingestellte Gewichte kodieren möchte.Die am häufigsten verwendeten Ansätze für die Netzwerkadaption sind unter anderem die Feinabstimmung und die Verwendung des vortrainierten Netzwerks als fester Merkmalsextraktor. In diesem Beitrag schlagen wir eine unkomplizierte Alternative vor: Beim Side-Tuning wird ein vortrainiertes Netz durch das Training eines leichtgewichtigen "Side"-Netzes angepasst, das mit dem (unveränderten) vortrainierten Netz durch einen einfachen additiven Prozess verschmolzen wird. Diese einfache Methode funktioniert genauso gut oder besser als bestehende Lösungen, während sie einige der grundlegenden Probleme des Fine-Tunings, fester Merkmale und mehrerer anderer gängiger Grundkonzepte löst. Insbesondere ist Side-Tuning weniger anfällig für Überanpassung, wenn nur wenige Trainingsdaten zur Verfügung stehen, liefert bessere Ergebnisse als die Verwendung eines festen Feature-Extraktors und leidet nicht unter katastrophalem Vergessen beim lebenslangen Lernen.  Wir demonstrieren die Leistung von Side-Tuning in verschiedenen Szenarien, darunter lebenslanges Lernen (iCIFAR, Taskonomy), Verstärkungslernen, Imitationslernen (visuelle Navigation in Habitat), NLP-Fragenbeantwortung (SQuAD v2) und Single-Task-Transfer-Lernen (Taskonomy), mit durchweg vielversprechenden Ergebnissen.
Wir fügen eine Gaußsche Rauschschicht in den Diskriminator eines generativen adversen Netzwerks ein, um die Ausgabe und die Gradienten in Bezug auf die Trainingsdaten differentiell privat zu machen, und verwenden dann die Generator-Komponente, um einen die Privatsphäre bewahrenden künstlichen Datensatz zu synthetisieren.Unsere Experimente zeigen, dass wir mit einem relativ kleinen Budget für die Privatsphäre in der Lage sind, Daten von hoher Qualität zu erzeugen und erfolgreich maschinelle Lernmodelle auf diesen künstlichen Daten zu trainieren.
Im Gegensatz zu Faltungsstudien, die Bilderscheinungen visualisieren, die dem Netzwerk-Output oder einer neuronalen Aktivierung aus einer globalen Perspektive entsprechen, zielt unsere Forschung darauf ab, zu klären, wie eine bestimmte Eingabeeinheit (Dimension) mit anderen Einheiten (Dimensionen) zusammenarbeitet, um Inferenzmuster des neuronalen Netzwerks zu bilden und somit zum Netzwerk-Output beizutragen. Die Analyse von lokalen Kontexteffekten in Bezug auf bestimmte Eingabeeinheiten ist in realen Anwendungen von besonderem Wert. Insbesondere haben wir unsere Methoden eingesetzt, um die Spielstrategie des alphaGo Zero-Modells in Experimenten zu erklären, und unsere Methode hat erfolgreich die Gründe für jeden Zug während des Spiels entschlüsselt.
Das Hauptziel des Network Pruning ist es, dem neuronalen Netzwerk Sparsamkeit aufzuerlegen, indem die Anzahl der Parameter mit Nullwerten erhöht wird, um die Größe der Architektur zu reduzieren und die Rechengeschwindigkeit zu erhöhen.
Die meisten früheren Arbeiten geben Garantien für die Wiederherstellung von Parametern für Netze mit einer versteckten Schicht, aber die in der Praxis verwendeten Netze haben mehrere nichtlineare Schichten. In dieser Arbeit zeigen wir, wie wir solche Ergebnisse auf tiefere Netze ausweiten können - wir behandeln das Problem der Aufdeckung der untersten Schicht in einem tiefen neuronalen Netz unter der Annahme, dass die unterste Schicht einen hohen Schwellenwert verwendet, bevor sie die Aktivierung anwendet, das obere Netz als wohlbehütetes Polynom modelliert werden kann und die Eingangsverteilung gaußförmig ist.
Während Federated Averaging (FedAvg) die führende Optimierungsmethode für das Training nicht-konvexer Modelle in dieser Umgebung ist, ist ihr Verhalten in realistischen föderierten Umgebungen nicht gut verstanden, wenn die Geräte/Aufgaben statistisch heterogen sind, d.h., In dieser Arbeit stellen wir einen Rahmen vor, der FedProx genannt wird, um statistische Heterogenität zu bewältigen.FedProx umfasst FedAvg als einen Spezialfall.Wir bieten Konvergenzgarantien für FedProx durch eine Annahme der Unähnlichkeit der Geräte.Unsere empirische Auswertung validiert unsere theoretische Analyse und zeigt die verbesserte Robustheit und Stabilität von FedProx für das Lernen in heterogenen Netzwerken.
Referentielle Spiele bieten eine fundierte Lernumgebung für neuronale Agenten, die der Tatsache Rechnung trägt, dass Sprache funktional zur Kommunikation verwendet wird, jedoch eine zweite Einschränkung nicht berücksichtigt, die als grundlegend für die Form der menschlichen Sprache angesehen wird: dass sie von neuen Sprachlernern erlernt werden kann und daher einen Übertragungsengpass überwinden muss.In dieser Arbeit fügen wir einen solchen Engpass in ein referentielles Spiel ein, indem wir eine wechselnde Population von Agenten einführen, in der neue Agenten durch das Spielen mit erfahreneren Agenten lernen. Wir zeigen, dass die bloße kulturelle Übertragung zu einer erheblichen Verbesserung der Spracheffizienz und des kommunikativen Erfolgs führt, gemessen an der Konvergenzgeschwindigkeit, dem Grad der Struktur in den entstandenen Sprachen und der Konsistenz der Sprache innerhalb der Population.Wir zeigen jedoch, dass die optimale Situation darin besteht, Sprache und Agenten gemeinsam zu entwickeln.Wenn wir der Agentenpopulation erlauben, sich durch genotypische Evolution weiterzuentwickeln, erzielen wir durchweg Verbesserungen bei allen betrachteten Metriken.Diese Ergebnisse unterstreichen, dass für Studien zur Sprachentwicklung die kulturelle Evolution wichtig ist, aber auch die Eignung der Architektur selbst berücksichtigt werden sollte.
Die Notwendigkeit großer Mengen von Trainingsbilddaten mit klar definierten Merkmalen ist ein Haupthindernis für die Anwendung von generativen adversen Netzwerken (GAN) bei der Bilderzeugung, wenn die Trainingsdaten begrenzt, aber vielfältig sind, da eine unzureichende Repräsentation latenter Merkmale in den bereits knappen Daten oft zu Instabilität und Zusammenbruch des Modus während des GAN-Trainings führt. Um die Hürde begrenzter Daten bei der Anwendung von GAN auf begrenzte Datensätze zu überwinden, schlagen wir in diesem Papier die Strategie der \textit{parallelen rekurrenten Datenerweiterung} vor, bei der das GAN-Modell seine Trainingsmenge schrittweise mit Beispielbildern anreichert, die von GANs konstruiert wurden, die in aufeinanderfolgenden Trainingsepochen parallel trainiert wurden.Experimente an einer Vielzahl kleiner, aber vielfältiger Datensätze zeigen, dass unsere Methode mit geringen modellspezifischen Überlegungen Bilder von besserer Qualität erzeugt als die Bilder, die ohne eine solche Strategie erzeugt werden.Der Quellcode und die erzeugten Bilder dieses Papiers werden nach der Überprüfung veröffentlicht.
Wir entwickeln einen stochastischen Ganzhirn- und Körpersimulator des Fadenwurms Caenorhabditis elegans (C. elegans) und zeigen, dass er ausreichend regulierend ist, um die Imputation latenter Membranpotentiale aus partiellen Kalzium-Fluoreszenz-Bildgebungsbeobachtungen zu ermöglichen. Dies ist der erste uns bekannte Versuch, den Kreis zu schließen, bei dem ein anatomisch fundierter Ganzkonnektom-Simulator verwendet wird, um einen zeitlich veränderlichen "Gehirn"-Zustand mit Einzelzell-Treue aus in der Praxis messbaren Kovariaten zu berechnen.  Durch den Einsatz modernster Bayes'scher Methoden des maschinellen Lernens, die auf leicht zugänglichen Daten beruhen, ebnet unsere Methode den Weg für Neurowissenschaftler, interpretierbare konnektomweite Zustandsdarstellungen zu gewinnen, physiologisch relevante Parameterwerte automatisch aus Daten zu schätzen und Simulationen zur Untersuchung intelligenter Lebensformen in silico durchzuführen.
Die qualitativ hochwertigen Knoteneinbettungen, die aus den Graph Neural Networks (GNNs) gelernt wurden, wurden auf eine Vielzahl von knotenbasierten Anwendungen angewandt, und einige von ihnen haben eine State-of-the-Art-Leistung (SOTA) erreicht. Bei der Anwendung von Knoteneinbettungen, die aus GNNs gelernt wurden, um Grapheneinbettungen zu erzeugen, reicht die skalare Knotenrepräsentation jedoch möglicherweise nicht aus, um die Knoten-/Grapheneigenschaften effizient zu erhalten, was zu suboptimalen Grapheneinbettungen führt. In Anlehnung an das Capsule Neural Network (CapsNet) schlagen wir das Capsule Graph Neural Network (CapsGNN) vor, das das Konzept der Kapseln aufgreift, um die Schwächen bestehender GNN-basierter Grapheneinbettungsalgorithmen zu beheben. Durch die Extraktion von Knotenmerkmalen in Form von Kapseln kann ein Routing-Mechanismus genutzt werden, um wichtige Informationen auf der Graphebene zu erfassen, wodurch unser Modell mehrere Einbettungen für jeden Graphen erzeugt, um Grapheigenschaften unter verschiedenen Aspekten zu erfassen. Unsere umfangreichen Auswertungen mit 10 Graphen-strukturierten Datensätzen zeigen, dass CapsGNN einen leistungsfähigen Mechanismus hat, der die makroskopischen Eigenschaften des gesamten Graphen datengesteuert erfasst und andere SOTA-Techniken bei verschiedenen Graphen-Klassifizierungsaufgaben übertrifft, dank des neuen Instruments.
Wir stellen einen neuartigen Rahmen für generative Modelle auf der Grundlage von Restricted Kernel Machines (RKMs) mit Multi-View-Generierung und unkorrelierten Feature-Learning-Fähigkeiten, genannt Gen-RKM.Um Multi-View-Generierung zu integrieren, verwendet dieser Mechanismus eine gemeinsame Darstellung von Daten aus verschiedenen views.The Mechanismus ist flexibel, um sowohl Kernel-basierte, (tief) neuronales Netz und Faltungs-basierte Modelle innerhalb der gleichen Einstellung.To aktualisieren die Parameter des Netzes, schlagen wir eine neuartige Ausbildung Verfahren, das gemeinsam lernt die Funktionen und gemeinsame Darstellung.Experimente zeigen das Potenzial des Rahmens durch qualitative Bewertung der generierten Proben.
Die Arbeit an dem Problem der kontextualisierten Wortrepräsentation - die Entwicklung von wiederverwendbaren neuronalen Netzwerkkomponenten für das Satzverständnis - hat in letzter Zeit eine Welle von Fortschritten gesehen, die sich auf die unbeaufsichtigte Pretraining-Aufgabe der Sprachmodellierung mit Methoden wie ELMo (Peters et al., Die primären Ergebnisse der Studie unterstützen die Verwendung von Sprachmodellierung als Pretraining Aufgabe und setzen einen neuen Stand der Technik unter vergleichbaren Modellen mit Multitasking-Lernen mit Sprachmodellen.Allerdings ein genauerer Blick auf diese Ergebnisse zeigt besorgniserregend starke Grundlinien und auffallend unterschiedliche Ergebnisse über Zielaufgaben, was darauf hindeutet, dass die weit verbreitete Paradigma der Pretraining und Einfrieren Satz Encoder möglicherweise nicht eine ideale Plattform für die weitere Arbeit sein.
In diesem Beitrag untersuchen wir das Problem des Angriffs und der Verteidigung beim Deep Learning aus der Perspektive der Fourier-Analyse: Wir berechnen zunächst explizit die Fourier-Transformation von tiefen neuronalen ReLU-Netzen und zeigen, dass es im Fourier-Spektrum neuronaler Netze abklingende, aber von Null verschiedene Hochfrequenzkomponenten gibt. Auf der Grundlage dieser Analyse schlagen wir vor, eine einfache Technik zur Nachmittelung zu verwenden, um diese hochfrequenten Komponenten zu glätten und so die Robustheit neuronaler Netze gegenüber Angriffen zu verbessern. Experimentelle Ergebnisse auf dem ImageNet- und dem CIFAR-10-Datensatz haben gezeigt, dass die von uns vorgeschlagene Methode universell wirksam ist, um viele in der Literatur vorgeschlagene schädliche Angriffsmethoden abzuwehren, einschließlich FGSM-, PGD-, DeepFool- und C&W-Angriffe. Unsere Methode der Nachmittelung ist einfach, da sie kein erneutes Training erfordert, und kann inzwischen über 80-96 % der von diesen Methoden erzeugten schädlichen Proben erfolgreich abwehren, ohne eine signifikante Leistungsverschlechterung (weniger als 2 %) auf den ursprünglichen sauberen Bildern einzuführen.
Methoden des Verstärkungslernens, die kontinuierlich neuronale Netze durch Episodengenerierung mit Spielbaumsuche erlernen, waren in deterministischen Zweipersonenspielen mit vollständiger Information wie Schach, Shogi und Go erfolgreich. Durch die Betrachtung des gesamten Systems als Spielbaumsuche kann die neue Methode den Kompromiss zwischen Ausbeutung und Erkundung während der Episodengenerierung handhaben, und die Experimente mit einem kleinen Problem haben gezeigt, dass sie im Vergleich zu der bestehenden Methode Alpha Zero eine robuste Leistung aufweist.
Zwei grundlegende Schwächen der Aggregatoren von MPNNs schränken jedoch ihre Fähigkeit ein, graphenstrukturierte Daten darzustellen: Sie verlieren die strukturelle Information der Knoten in den Nachbarschaften und sind nicht in der Lage, weitreichende Abhängigkeiten in disassortativen Graphen zu erfassen. Ausgehend von den Beobachtungen zum klassischen neuronalen Netz und zur Netzgeometrie schlagen wir ein neuartiges geometrisches Aggregationsschema für graphische neuronale Netze vor, um die beiden Schwächen zu überwinden.  Das vorgeschlagene Aggregationsschema ist permutationsinvariant und besteht aus drei Modulen, der Knoteneinbettung, der strukturellen Nachbarschaft und der Zwei-Ebenen-Aggregation. Wir präsentieren auch eine Implementierung des Schemas in Graph-Faltungsnetzen, genannt Geom-GCN, um transduktives Lernen auf Graphen durchzuführen.
Wir betrachten die Aufgabe der Programmsynthese in Anwesenheit einer Belohnungsfunktion über die Ausgabe von Programmen, wo das Ziel ist, Programme mit maximalen Belohnungen zu finden. Wir führen ein neuartiges iteratives Optimierungsschema ein, bei dem wir ein RNN auf einem Datensatz von K besten Programmen aus einer Prioritätswarteschlange der bisher generierten Programme trainieren und dann neue Programme synthetisieren und sie der Prioritätswarteschlange hinzufügen, indem wir aus dem RNN eine Stichprobe ziehen. Wir vergleichen unseren Algorithmus namens Priority Queue Training (PQT) mit genetischen Algorithmen und Reinforcement Learning-Baselines auf einer einfachen, aber ausdrucksstarken Turing-Programmiersprache namens BF. Unsere experimentellen Ergebnisse zeigen, dass unser täuschend einfacher PQT-Algorithmus die Baselines deutlich übertrifft, indem wir der Belohnungsfunktion eine Strafe für die Programmlänge hinzufügen.
Wir präsentieren das Graph-Wavelet-Neural-Network (GWNN), ein neuartiges Graph-Convolutional-Neural-Network (CNN), das die Graph-Wavelet-Transformation nutzt, um die Unzulänglichkeiten früherer spektraler Graph-CNN-Methoden zu beheben, die auf der Graph-Fourier-Transformation beruhen.Anders als die Graph-Fourier-Transformation kann die Graph-Wavelet-Transformation über einen schnellen Algorithmus erhalten werden, ohne dass eine Matrix-Eigenkomposition mit hohen Rechenkosten erforderlich ist. Darüber hinaus sind Graph-Wavelets spärlich und in der Vertex-Domäne lokalisiert, was eine hohe Effizienz und gute Interpretierbarkeit für die Graphenfaltung bietet. Das vorgeschlagene GWNN übertrifft die bisherigen spektralen Graph-CNNs bei der Aufgabe der graphbasierten halbüberwachten Klassifizierung auf drei Benchmark-Datensätzen erheblich: Cora, Citeseer und Pubmed.
Learning rate decay (lrDecay) ist eine \emph{de facto} Technik für das Training moderner neuronaler Netze. lrDecay beginnt mit einer großen Lernrate und lässt diese dann mehrfach ab. lrDecay hilft empirisch gesehen sowohl bei der Optimierung als auch bei der Generalisierung. Die gängige Meinung darüber, wie lrDecay funktioniert, stammt aus der Optimierungsanalyse des (stochastischen) Gradientenabstiegs:1) eine anfänglich hohe Lernrate beschleunigt das Training oder hilft dem Netzwerk, ungewollte lokale Minima zu vermeiden;2) das Abklingen der Lernrate hilft dem Netzwerk, zu einem lokalen Minimum zu konvergieren und Oszillation zu vermeiden.Trotz der Popularität dieser gängigen Meinungen deuten Experimente darauf hin, dass sie unzureichend sind, um die allgemeine Effektivität von lrDecay beim Training moderner neuronaler Netze zu erklären, die tief, breit und nicht konvex sind. Die vorgeschlagene Erklärung wird an einem sorgfältig konstruierten Datensatz mit überschaubarer Musterkomplexität validiert, und ihre Implikation, dass zusätzliche Muster, die in späteren Phasen von lrDecay gelernt werden, komplexer und damit weniger übertragbar sind, ist in realen Datensätzen gerechtfertigt.
Wir zeigen, wie ein Ensemble von $Q^*$-Funktionen für eine effektivere Exploration beim Deep Reinforcement Learning genutzt werden kann.Wir bauen auf gut etablierten Algorithmen aus dem Bandit-Setting auf und passen sie an das $Q$-Learning-Setting an.Wir schlagen eine Explorationsstrategie vor, die auf Upper-Confidence Bounds (UCB) basiert.Unsere Experimente zeigen signifikante Gewinne beim Atari Benchmark.
Wir konzentrieren uns auf das Problem des Lernens eines einzelnen Motormoduls, das flexibel eine Reihe von Verhaltensweisen für die Steuerung von hochdimensionalen, physikalisch simulierten Humanoiden ausdrücken kann, und schlagen dazu eine Motorarchitektur vor, die die allgemeine Struktur eines inversen Modells mit einem latent-variablen Engpass hat. Wir zeigen, dass es möglich ist, dieses Modell vollständig offline zu trainieren, um Tausende von Expertenrichtlinien zu komprimieren und einen Raum für die Einbettung von Motorprimitiven zu erlernen.Das trainierte neuronale probabilistische Motorprimitivsystem kann das Verhalten von humanoiden Ganzkörpern in einem Zug nachahmen und dabei robust ungesehene Trajektorien imitieren. Um das Training unseres Modells zu unterstützen, vergleichen wir zwei Ansätze für das Offline-Policy-Cloning, darunter eine erfahrungseffiziente Methode, die wir lineares Feedback-Policy-Cloning nennen. Wir empfehlen den Lesern, sich ein ergänzendes Video (https://youtu.be/CaDEf-QcKwA) anzusehen, das unsere Ergebnisse zusammenfasst.
Die Datenerweiterung ist eine nützliche Technik, um die Größe des Trainingssatzes zu vergrößern und eine Überanpassung für verschiedene Aufgaben des maschinellen Lernens zu verhindern, wenn nur wenige Trainingsdaten zur Verfügung stehen.1 Die derzeitigen Datenerweiterungstechniken sind jedoch in hohem Maße auf menschliches Design und Domänenwissen angewiesen, und die vorhandenen automatisierten Ansätze müssen die latenten Merkmale im Trainingsdatensatz noch vollständig ausschöpfen. In diesem Beitrag schlagen wir eine parallele adaptive GAN-Datenerweiterung (PAGANDA) vor, bei der sich der Trainingsdatensatz adaptiv mit Beispielbildern anreichert, die automatisch aus parallel trainierten generativen adversen Netzwerken (GANs) erstellt werden. Wir zeigen anhand von Experimenten, dass unsere Strategie der Datenanreicherung ohne modellspezifische Überlegungen leicht an bereichsübergreifende Deep Learning/Machine Learning-Aufgaben wie Bildklassifikation und Image Inpainting angepasst werden kann und dabei die Modellleistung in beiden Aufgaben deutlich verbessert.
Um dieses Problem zu entschärfen, schlagen wir eine neue Regularisierungsmethode vor, die die prädiktive Verteilung zwischen ähnlichen Proben bestraft, insbesondere destillieren wir die prädiktive Verteilung zwischen verschiedenen Proben desselben Etiketts und erweiterten Proben derselben Quelle während des Trainings, d.h. wir regularisieren das dunkle Wissen (d.h. das Wissen über falsche Vorhersagen) eines einzelnen Netzwerks, d.h. eine Technik zur Destillation des Selbstwissens, um es zu zwingen, aussagekräftigere Vorhersagen auszugeben.  Wir demonstrieren die Wirksamkeit der vorgeschlagenen Methode anhand von Experimenten mit verschiedenen Bildklassifizierungsaufgaben: Sie verbessert nicht nur die Generalisierungsfähigkeit, sondern auch die Kalibrierungsgenauigkeit moderner neuronaler Netze.
In diesem Papier betrachten wir das spezifische Problem der Sprachmodellierung auf Wortebene und untersuchen Strategien zur Regularisierung und Optimierung von LSTM-basierten Modellen. Wir schlagen das gewichtsgetrennte LSTM vor, das DropConnect auf versteckte-zu-versteckte Gewichte als eine Form der rekurrenten Regularisierung verwendet. Darüber hinaus stellen wir NT-ASGD, eine nicht-monoton getriggerte (NT) Variante der gemittelten stochastischen Gradientenmethode (ASGD) vor, bei der der Mittelungsauslöser unter Verwendung einer NT-Bedingung bestimmt wird, anstatt vom Benutzer eingestellt zu werden.Unter Verwendung dieser und anderer Regularisierungsstrategien erreicht unser ASGD Weight-Dropped LSTM (AWD-LSTM) modernste Wortlevel-Perplexitäten auf zwei Datensätzen: 57. Bei der Untersuchung der Effektivität eines neuronalen Caches in Verbindung mit unserem vorgeschlagenen Modell erreichen wir eine noch niedrigere State-of-the-Art-Perplexität von 52,8 in der Penn Treebank und 52,0 in WikiText-2. Wir untersuchen auch die Realisierbarkeit der vorgeschlagenen Regularisierungs- und Optimierungsstrategien im Kontext des quasi-rekurrenten neuronalen Netzwerks (QRNN) und zeigen eine vergleichbare Leistung wie das AWD-LSTM-Pendant. Der Code zur Reproduktion der Ergebnisse ist als Open Source verfügbar unter https://github.com/salesforce/awd-lstm-lm.
Mit dem Ziel, die Funktionsweise neuronaler Netze besser zu verstehen, wurden verschiedene Methoden zur Messung der Einheitenselektivität entwickelt.  In einem Versuch, die Objektselektivität besser zu charakterisieren, vergleichen wir verschiedene Selektivitätsmaße für eine große Anzahl von Einheiten in AlexNet, einschließlich der lokalistischen Selektivität, der Präzision, der klassenbedingten mittleren Aktivitätsselektivität (CCMAS), der Netzwerkzerlegung, der menschlichen Interpretation von Aktivierungsmaximierungsbildern (AM) und Standard-Signaldetektionsmaßen.  Wir stellen fest, dass die verschiedenen Maße unterschiedliche Schätzungen der Objektselektivität liefern, wobei die Präzisions- und CCMAS-Maße irreführend hohe Schätzungen liefern, da die selektivsten Einheiten eine schlechte Trefferquote oder eine hohe Fehlalarmrate (oder beides) bei der Objektklassifizierung aufweisen, was sie zu schlechten Objektdetektoren macht.  Um diese Ergebnisse zu verallgemeinern, haben wir die Selektivitätsmaße einiger Einheiten in VGG-16 und GoogLeNet verglichen, die auf den ImageNet- oder Places-365-Datensätzen trainiert wurden, die als "Objektdetektoren" beschrieben wurden, und auch hier fanden wir schlechte Trefferquoten und hohe Fehlalarmquoten bei der Objektklassifizierung.
Die Faltungsstruktur des DNA-Moleküls in Verbindung mit Hilfsmolekülen, auch als Chromatin bezeichnet, ist für die funktionellen Eigenschaften der DNA von großer Bedeutung. Die Chromatinstruktur wird weitgehend durch die zugrunde liegende primäre DNA-Sequenz bestimmt, obwohl die Interaktion noch nicht vollständig verstanden ist. Die Methode wurde so entwickelt, dass sie in der Lage ist, Interaktionen zwischen distalen Elementen in der DNA-Sequenz zu erkennen, von denen bekannt ist, dass sie sehr relevant sind.
Die wichtigste Erkenntnis ist, dass wir zusätzlich zu den Merkmalen auch Ähnlichkeitsinformationen übertragen können, und dass dies ausreicht, um eine Ähnlichkeitsfunktion und ein Clusternetzwerk zu lernen, um sowohl eine Domänenanpassung als auch ein aufgabenübergreifendes Transferlernen durchzuführen. Diese Ähnlichkeit ist kategorieunabhängig und kann aus Daten in der Quelldomäne mit Hilfe eines Ähnlichkeitsnetzes erlernt werden. Wir stellen dann zwei neuartige Ansätze für die Durchführung von Transferlernen unter Verwendung dieser Ähnlichkeitsfunktion vor: Erstens entwerfen wir für die unbeaufsichtigte Domänenanpassung eine neue Verlustfunktion, um die Klassifizierung mit einem eingeschränkten Clustering-Verlust zu regulieren und somit ein Clustering-Netz mit der übertragenen Ähnlichkeitsmetrik zu erlernen, das die Trainingsinputs generiert, und zweitens für aufgabenübergreifendes Lernen (d. h, Da das Ähnlichkeitsnetz verrauscht ist, liegt der Schlüssel darin, einen robusten Clustering-Algorithmus zu verwenden, und wir zeigen, dass unsere Formulierung robuster ist als die alternativen eingeschränkten und unbeschränkten Clustering-Ansätze. Die Ergebnisse zeigen, dass wir semantische Cluster mit hoher Genauigkeit rekonstruieren können. Anschließend bewerten wir die Leistung des domänenübergreifenden Transfers anhand von Bildern aus den Office-31- und SVHN-MNIST-Aufgaben und präsentieren die höchste Genauigkeit in beiden Datensätzen.  Unser Ansatz geht nicht explizit auf die Diskrepanz zwischen den Domänen ein und zeigt eine weitere Verbesserung, wenn wir ihn mit einem Verlust der Domänenanpassung kombinieren.
Jüngste Fortschritte im Bereich der sprachübergreifenden Worteinbettungen beruhen in erster Linie auf Mapping-basierten Methoden, bei denen vortrainierte Worteinbettungen aus verschiedenen Sprachen durch eine lineare Transformation in einen gemeinsamen Raum projiziert werden; diese Ansätze gehen jedoch davon aus, dass die Worteinbettungsräume zwischen verschiedenen Sprachen isomorph sind, was sich in der Praxis als nicht zutreffend erwiesen hat (Søgaard et al, Dies motiviert die Untersuchung gemeinsamer Lernmethoden, die dieses Hindernis überwinden können, indem sie gleichzeitig Einbettungen über Sprachen hinweg über einen sprachübergreifenden Term im Trainingsziel lernen.Angesichts der Fülle an verfügbaren parallelen Daten (Tiedemann, 2012), schlagen wir eine zweisprachige Erweiterung der CBOW-Methode vor, die satzorientierte Korpora nutzt, um robuste sprachübergreifende Wort- und Satzrepräsentationen zu erhalten. Unser Ansatz verbessert signifikant die Leistung der sprachübergreifenden Satzsuche gegenüber allen anderen Ansätzen und übertrifft überzeugend die Mapping-Methoden, während er die Parität mit gemeinsam trainierten Methoden zur Wortübersetzung beibehält. er erreicht auch die Parität mit einer tiefen RNN-Methode bei einer Zero-Shot-Aufgabe zur sprachübergreifenden Dokumentenklassifikation, wobei er viel weniger Rechenressourcen für das Training und die Inferenz benötigt. als zusätzlicher Vorteil verbessert unsere zweisprachige Methode auch die Qualität der einsprachigen Wortvektoren trotz des Trainings auf viel kleineren Datensätzen.  Wir machen unseren Code und unsere Modelle öffentlich zugänglich.
Um in komplexen Umgebungen handeln und planen zu können, sollten Agenten über einen mentalen Simulator der Welt verfügen, der drei Eigenschaften aufweist:(a) er sollte einen abstrakten Zustand aufbauen, der den Zustand der Welt repräsentiert;(b) er sollte eine Überzeugung bilden, die die Ungewissheit über die Welt repräsentiert;(c) er sollte über eine einfache Schritt-für-Schritt-Simulation hinausgehen und eine zeitliche Abstraktion aufweisen. Motiviert durch das Fehlen eines Modells, das all diese Anforderungen erfüllt, schlagen wir TD-VAE vor, ein generatives Sequenzmodell, das Repräsentationen erlernt, die explizite Überzeugungen über Zustände enthalten, die mehrere Schritte in die Zukunft reichen, und das direkt ohne Einzelschrittübergänge ausgeführt werden kann.TD-VAE wird auf Paaren von zeitlich getrennten Zeitpunkten trainiert, unter Verwendung eines Analogons des zeitlichen Differenzlernens, das beim Verstärkungslernen verwendet wird.
In diesem Papier wird ein informationstheoretisches Co-Trainingsziel für unüberwachtes Lernen eingeführt. Wir betrachten das Problem der Vorhersage der Zukunft. Anstatt zukünftige Wahrnehmungen (Bildpixel oder Schallwellen) vorherzusagen, sagen wir ``Hypothesen'' voraus, die durch zukünftige Wahrnehmungen bestätigt werden. Formal gesehen gehen wir von einer Populationsverteilung für Paare $(x,y)$ aus, wobei wir uns $x$ als vergangene Empfindung und $y$ als zukünftige Empfindung vorstellen können. Wir trainieren sowohl ein Vorhersagemodell $P_\Phi(z|x)$ als auch ein Bestätigungsmodell $P_\Psi(z|y)$, wobei wir $z$ als Hypothesen (wenn vorhergesagt) oder Fakten (wenn bestätigt) betrachten. Für eine Populationsverteilung auf Paaren $(x,y)$ konzentrieren wir uns auf das Problem der Messung der gegenseitigen Information zwischen $x$ und $y$. Durch die Ungleichung der Datenverarbeitung ist diese gegenseitige Information mindestens so groß wie die gegenseitige Information zwischen $x$ und $z$ unter der Verteilung auf Tripel $(x,z,y)$, die durch das Bestätigungsmodell $P_\Psi(z|y)$ definiert ist. Das informationstheoretische Trainingsziel für $P_\Phi(z|x)$ und $P_\Psi(z|y)$ kann als eine Form des Co-Trainings angesehen werden, bei dem die Vorhersage von $x$ mit der Bestätigung von $y$ übereinstimmen soll.Wir führen Experimente für Anwendungen zum Lernen von Phonetik auf dem TIMIT-Datensatz durch.
Generative Modelle für die Gesangsstimme haben sich hauptsächlich mit der Aufgabe der "Gesangsstimmsynthese" beschäftigt, d.h., In dieser Arbeit erforschen wir eine neuartige, aber anspruchsvolle Alternative: die Erzeugung von Gesangsstimmen ohne vorgegebene Partituren und Texte, sowohl in der Trainings- als auch in der Inferenzzeit.Insbesondere experimentieren wir mit drei verschiedenen Schemata:1) freier Sänger, bei dem das Modell Gesangsstimmen ohne jegliche Bedingungen erzeugt;2) begleiteter Sänger, bei dem das Modell Gesangsstimmen über eine Wellenform von Instrumentalmusik erzeugt; und3) Solosänger, bei dem das Modell zunächst eine Akkordfolge improvisiert und diese dann zur Erzeugung von Stimmen verwendet. Wir skizzieren die damit verbundenen Herausforderungen und schlagen eine Pipeline zur Bewältigung dieser neuen Aufgaben vor, die die Entwicklung von Quellentrennungs- und Transkriptionsmodellen für die Datenaufbereitung, von adversen Netzwerken für die Audiogenerierung und von maßgeschneiderten Metriken für die Bewertung umfasst.
Distillation ist eine Technik zur Komprimierung von Netzwerken, die versucht, Wissen aus einem großen Modell in ein kleineres zu übertragen. Wir verwenden die Lehrer-Schüler-Destillation, um die Effizienz des Biaffine-Abhängigkeitsparsers zu verbessern, der in Bezug auf Genauigkeit und Parsing-Geschwindigkeit die beste Leistung erzielt (Dozat & Manning, 2016).  Bei der Destillation auf 20 % der trainierbaren Parameter des ursprünglichen Modells beobachten wir nur einen durchschnittlichen Rückgang von ∼1 Punkt sowohl für UAS als auch für LAS über eine Reihe von verschiedenen Universal Dependency Treebanks, während wir bei der Inferenzzeit 2,26x (1,21x) schneller sind als das Basismodell auf CPU (GPU).  Schließlich erreichen wir durch Destillation einen Parser, der nicht nur schneller, sondern auch genauer ist als der schnellste moderne Parser auf der Penn Treebank.
Während Autoencoder eine Schlüsseltechnik beim Erlernen von Repräsentationen für kontinuierliche Strukturen wie Bilder oder Wellenformen sind, hat sich die Entwicklung von Universal-Autoencodern für diskrete Strukturen wie Textsequenzen oder diskretisierte Bilder als schwieriger erwiesen.Insbesondere machen es diskrete Eingaben schwieriger, einen glatten Encoder zu erlernen, der die komplexen lokalen Beziehungen im Eingaberaum bewahrt.In dieser Arbeit schlagen wir einen adversarisch regularisierten Autoencoder (ARAE) mit dem Ziel vor, robustere Repräsentationen im diskreten Raum zu erlernen. ARAE trainiert sowohl einen reichhaltigen Kodierer für den diskreten Raum, wie z.B. ein RNN, als auch eine einfachere Generatorfunktion für den kontinuierlichen Raum, während das Training eines generativen adversen Netzwerks (GAN) dazu dient, die Verteilungen auf Ähnlichkeit zu beschränken. Experimente mit Text und diskretisierten Bildern zeigen, dass das GAN-Modell saubere Interpolationen erzeugt und die Multimodalität des ursprünglichen Raums einfängt, und dass der Autoencoder Verbesserungen beim halbüberwachten Lernen sowie hochmoderne Ergebnisse bei der unausgerichteten Textstil-Transferaufgabe erzielt, bei der nur eine gemeinsame kontinuierliche Raumdarstellung verwendet wird.
Wenn Daten aus mehreren latenten Teilpopulationen stammen, schätzen maschinelle Lernverfahren in der Regel die Parameterwerte unabhängig für jede Teilpopulation.In diesem Papier schlagen wir vor, diese Grenzen zu überwinden, indem wir Proben als Aufgaben in einem Multitasking-Lernverfahren betrachten.
In dieser Arbeit führen wir zunächst eine mathematische Analyse des Speichers durch, der als eine Funktion definiert ist, die ein Element in einer Sequenz auf die aktuelle Ausgabe dreier RNN-Zellen abbildet, nämlich des einfachen rekurrenten neuronalen Netzes (SRN), des Lang-Kurzzeit-Speichers (LSTM) und der Gated Recurrent Unit (GRU).Auf der Grundlage der Analyse schlagen wir ein neues Design vor, das wir als erweiterten Lang-Kurzzeit-Speicher (ELSTM) bezeichnen, um die Speicherlänge einer Zelle zu erweitern. Als Nächstes stellen wir ein Multi-Task-RNN-Modell vor, das gegenüber früheren fehlerhaften Vorhersagen robust ist, das sogenannte dependentbidirectional recurrent neural network (DBRNN), für das Sequence-in-Sequenceout(SISO)-Problem.Schließlich wird die Leistung des DBRNN-Modells mit derELSTM-Zelle durch experimentelle Ergebnisse demonstriert.
Viele kürzlich trainierte neuronale Netze verwenden eine große Anzahl von Parametern, um eine gute Leistung zu erzielen. Intuitiv kann man die Anzahl der erforderlichen Parameter als groben Maßstab für die Schwierigkeit eines Problems verwenden... Aber wie genau sind solche Vorstellungen? Wie viele Parameter werden wirklich benötigt? Der Ansatz ist einfach zu implementieren, rechnerisch nachvollziehbar und führt zu mehreren aufschlussreichen Schlussfolgerungen: Viele Probleme haben kleinere intrinsische Dimensionen, als man vermuten könnte, und die intrinsische Dimension für einen gegebenen Datensatz variiert nur wenig über eine Familie von Modellen mit sehr unterschiedlichen Größen. Die intrinsische Dimension ermöglicht einen quantitativen Vergleich der Schwierigkeit von Problemen bei überwachtem, verstärktem und anderem Lernen, wobei wir zum Beispiel zu dem Schluss kommen, dass die Lösung des Problems des umgekehrten Pendels 100 Mal einfacher ist als die Klassifizierung von Ziffern aus MNIST, und dass das Spielen von Atari Pong aus Pixeln ungefähr so schwierig ist wie die Klassifizierung von CIFAR-10. Die Methode liefert nicht nur eine neue Kartographie der objektiven Landschaften, die von parametrisierten Modellen durchwandert werden, sondern ist auch eine einfache Technik, um konstruktiv eine Obergrenze für die minimale Beschreibungslänge einer Lösung zu erhalten.
Graph Neural Networks als eine Kombination von Graph Signal Processing und Deep Convolutional Networks zeigt große Macht in der Mustererkennung in nicht-euklidischen Domains.in diesem Papier schlagen wir eine neue Methode zum Einsatz von zwei Pipelines auf der Grundlage der Dualität eines Graphen, um die Genauigkeit zu verbessern.durch die Erforschung der primären Graphen und seine dualen Graphen, wo Knoten und Kanten können als einander behandelt werden, haben wir die Vorteile der beiden Vertex-Features und Edge-Features ausgenutzt.als Ergebnis haben wir zu einem Rahmen, der großes Potenzial in beiden semisupervised und unbeaufsichtigte Lernen hat erreicht.
Das erste Modell ist ein Local Autoencoding Parser (LAP), der die Eingabe mit kontinuierlichen latenten Variablen sequentiell kodiert; das zweite Modell ist ein Global Autoencoding Parser (GAP), der die Eingabe in Abhängigkeitsbäume als latente Variablen kodiert, mit exakter Inferenz. Beide Modelle bestehen aus zwei Teilen: einem Kodierer, der durch tiefe neuronale Netze (DNN) erweitert wird, die die Kontextinformationen nutzen können, um die Eingabe in latente Variablen zu kodieren, und einem Dekodierer, der ein generatives Modell ist, das in der Lage ist, die Eingabe zu rekonstruieren.LAP und GAP haben eine einheitliche Struktur mit unterschiedlichen Verlustfunktionen für gelabelte und nicht gelabelte Daten mit gemeinsamen Parametern.Wir haben Experimente mit WSJ- und UD-Abhängigkeits-Parsing-Datensätzen durchgeführt, die zeigen, dass unsere Modelle die nicht gelabelten Daten nutzen können, um die Leistung bei einer begrenzten Menge gelabelter Daten zu steigern.
Ein Gate-Mechanismus aktualisiert die schnellen Gewichte bei jedem Zeitschritt einer Sequenz durch zwei separate Matrizen auf der Basis von Außenprodukten, die von den langsamen Teilen des Netzes erzeugt werden. Das System wird auf einer komplexen Sequenz-zu-Sequenz-Variation des assoziativen Retrieval-Problems mit etwa 70-mal mehr Zeitspeicher (d. h. zeitvariablen Variablen) trainiert. In Bezug auf die Genauigkeit und die Anzahl der Parameter übertrifft unsere Architektur eine Vielzahl von RNNs, einschließlich Long Short-Term Memory, Hypernetworks und ähnliche Architekturen mit schneller Gewichtung.
Im Bereich des Deep Learning wird nach einer Optimierungsmethode gesucht, die hervorragende Eigenschaften sowohl für die Optimierung als auch für die Verallgemeinerung aufweist.  Wir schlagen eine Methode zur mathematischen Optimierung vor, die auf Flüssen entlang von Geodäten basiert, d.h. den kürzesten Pfaden zwischen zwei Punkten, in Bezug auf die Riemannsche Metrik, die durch eine nichtlineare Funktion induziert wird.In unserer Methode beziehen sich die Flüsse auf Exponentially Decaying Flows (EDF), da sie so entworfen werden können, dass sie exponentiell auf die lokalen Lösungen konvergieren.In diesem Papier führen wir Experimente durch, um die hohe Leistung bei Optimierungsbenchmarks (d.h., (d.h. Konvergenzeigenschaften) sowie ihr Potenzial für die Erstellung guter Benchmarks für maschinelles Lernen (d.h. Generalisierungseigenschaften).
Wir stellen Quantum Graph Neural Networks (QGNN) vor, eine neue Klasse von Quanten-Neural-Network-Ansatzen, die darauf zugeschnitten sind, Quantenprozesse zu repräsentieren, die eine Graphenstruktur haben, und die besonders geeignet sind, auf verteilten Quantensystemen über ein Quantennetzwerk ausgeführt zu werden. Neben dieser allgemeinen Klasse von Ansatzen stellen wir weitere spezialisierte Architekturen vor, nämlich Quantum Graph Recurrent Neural Networks (QGRNN) und Quantum Graph Convolutional Neural Networks (QGCNN). Wir geben vier Anwendungsbeispiele für QGNNs: Lernen der Hamilton-Dynamik von Quantensystemen, Lernen, wie man multipartite Verschränkung in einem Quantennetzwerk erzeugt, unbeaufsichtigtes Lernen für spektrales Clustering und überwachtes Lernen für die Klassifizierung von Graphenisomorphismus.
Unsere Forschung betrifft die Wahrnehmung von Datenschutzverletzungen durch die Nutzer, insbesondere Fragen der Verantwortlichkeit. Eine vorläufige Studie ergab, dass viele Menschen die Problematik nur unzureichend verstehen und sich in gewisser Weise selbst verantwortlich fühlen. Wir vermuteten, dass dieser Eindruck von den Kommunikationsstrategien der Unternehmen herrühren könnte. Wir verglichen daher Texte von Unternehmen mit externen Quellen, wie z. B. den Nachrichtenmedien. Dies deutete darauf hin, dass Unternehmen bekannte Methoden der Krisenkommunikation verwenden, um ihren Reputationsschaden zu verringern, und dass diese Strategien mit der Neupositionierung der in der Geschichte enthaltenen narrativen Elemente übereinstimmen. Die Ergebnisse dieser Studie stimmen mit unserer Dokumentenanalyse überein und deuten darauf hin, dass die Unternehmenskommunikation die Wahrnehmung der Nutzer in Bezug auf die Viktimisierung, die Einstellung zum Datenschutz und die Verantwortlichkeit beeinflusst.
Zielerkennung ist das Problem der Ableitung des korrekten Ziels, auf das ein Agent einen Plan ausführt, wenn eine Reihe von Zielhypothesen, ein Domänenmodell und eine (möglicherweise verrauschte) Probe des ausgeführten Plans vorliegen.   Dies ist ein Schlüsselproblem sowohl bei kooperativen als auch bei kompetitiven Agenteninteraktionen, und neuere Ansätze haben schnelle und genaue Algorithmen zur Zielerkennung hervorgebracht.   In dieser Arbeit nutzen wir die Fortschritte in der Operator-Counting-Heuristik, die mit Hilfe von linearen Programmen über Nebenbedingungen aus klassischen Planungsproblemen berechnet wird, um Probleme der Zielerkennung zu lösen.   Unser Ansatz verwendet zusätzliche Operator-Counting-Beschränkungen, die aus den Beobachtungen abgeleitet werden, um effizient auf das richtige Ziel zu schließen, und dient als Grundlage für eine Reihe weiterer Methoden mit zusätzlichen Beschränkungen.
Um dieses Problem zu lösen, schlagen wir die Verwendung von Wissensdestillation vor, bei der Single-Task-Modelle ein Multi-Task-Modell unterrichten. Wir verbessern dieses Training mit Teacher Annealing, einer neuartigen Methode, die das Modell allmählich von Destillation auf überwachtes Lernen umstellt und dem Multi-Task-Modell hilft, seine Single-Task-Lehrer zu übertreffen. Wir bewerten unseren Ansatz durch Multi-Task-Feintuning von BERT auf dem GLUE-Benchmark.
Sicherheitskritische Anwendungen, wie z.B. das autonome Fahren, würden von der Fähigkeit profitieren, die ungewöhnlichen Objekte zu lokalisieren, die dazu führen, dass das Bild außerhalb der Verteilung liegt. Darüber hinaus werden die angepassten Methoden experimentell mit zwei neuen Datensätzen verglichen, die aus bestehenden semantischen Segmentierungsdatensätzen mit PSPNet- und DeeplabV3+-Architekturen abgeleitet wurden, und es wird eine neue Metrik für die Aufgabe vorgeschlagen.Die Auswertung zeigt, dass die Leistungseinstufung der verglichenen Methoden nicht auf die neue Aufgabe übertragbar ist und jede Methode deutlich schlechter abschneidet als ihre Gegenstücke auf Bildebene.
Wir stellen eine Domänenanpassungsmethode vor, um neuronale Repräsentationen von markenreichen Quelldomänen auf unmarkierte Zieldomänen zu übertragen. Neuere adversarische Methoden, die für diese Aufgabe vorgeschlagen wurden, lernen, Merkmale über Domänen hinweg anzugleichen, indem sie ein spezielles Domänenklassifizierungsnetzwerk "austricksen". Ein Nachteil dieses Ansatzes ist jedoch, dass der Domänenklassifizierer die erzeugten Merkmale einfach als in der Domäne oder nicht in der Domäne markiert, ohne die Grenzen zwischen den Klassen zu berücksichtigen. Wir schlagen einen neuartigen Ansatz vor, die Adversarial Dropout Regularization (ADR), die den Generator dazu ermutigt, diskriminantere Merkmale für die Zieldomäne auszugeben. Unsere Schlüsselidee ist es, den traditionellen Domänenkritiker durch einen Kritiker zu ersetzen, der nicht-diskriminierende Merkmale durch die Verwendung von Dropout auf dem Klassifizierernetzwerk erkennt. Der Generator lernt dann, diese Bereiche des Merkmalsraums zu vermeiden und erzeugt so bessere Merkmale.
Die Verwendung von Deep Learning für eine breite Palette von Datenproblemen hat die Notwendigkeit für das Verständnis und die Diagnose dieser Modelle erhöht, und Deep-Learning-Interpretationstechniken sind ein wesentliches Werkzeug für Datenanalysten geworden.Obwohl zahlreiche Modellinterpretationsmethoden in den letzten Jahren vorgeschlagen wurden, basieren die meisten dieser Verfahren auf Heuristiken mit wenig oder keinen theoretischen Garantien.In dieser Arbeit schlagen wir einen statistischen Rahmen für die Saliency-Schätzung für Black-Box-Computer-Vision-Modelle.Wir bauen ein Modell-agnostische Schätzverfahren, das statistisch konsistent ist und die Saliency-Prüfungen von Adebayo et al. (Durch unsere theoretische Analyse legen wir eine obere Schranke für die Anzahl der Modellevaluierungen fest, die erforderlich sind, um die Region der Wichtigkeit mit hoher Wahrscheinlichkeit wiederherzustellen, und entwickeln ein neues Störungsschema für die Schätzung lokaler Gradienten, das sich als effizienter erweist als die üblicherweise verwendeten Zufallsstörungsschemata.Die Gültigkeit der neuen Methode wird durch eine Sensitivitätsanalyse nachgewiesen.
Wir erforschen die Idee der kompositorischen Mengeneinbettung, die verwendet werden kann, um nicht nur eine einzelne Klasse, sondern die Menge der Klassen, die mit den Eingabedaten verbunden sind (z.B., Dies kann beispielsweise bei der Erkennung mehrerer Objekte in Bildern oder bei der Diarisierung mehrerer Sprecher (One-Shot-Learning) in Audiodaten nützlich sein. Insbesondere entwickeln und implementieren wir zwei neuartige Modelle, bestehend aus (1) einer Einbettungsfunktionf, die gemeinsam mit einer "zusammengesetzten" Funktion g trainiert wird, die Operationen zur Mengenvereinigung zwischen den in zwei Einbettungsvektoren kodierten Klassen berechnet; und (2) einer Einbettungf, die gemeinsam mit einer "Abfrage"-Funktion h trainiert wird, die berechnet, ob die in einer Einbettung kodierten Klassen die in einer anderen Einbettung kodierten Klassen umfassen. Im Gegensatz zu früheren Arbeiten müssen diese Modelle sowohl die mit den Eingabebeispielen assoziierten Klassen wahrnehmen als auch die Beziehungen zwischen verschiedenen Klassensätzen kodieren. In Experimenten, die mit simulierten Daten, OmniGlot und COCO-Datensätzen durchgeführt wurden, übertreffen die vorgeschlagenen zusammengesetzten Einbettungsmodelle die auf traditionellen Einbettungsansätzen basierenden Grundlinien.
In dieser Arbeit schlagen wir eine zielgerichtete kollaborative Aufgabe vor, die Sprache, Sehen und Handeln in einer virtuellen Umgebung als Kernkomponenten enthält. Wir entwickeln ein kollaboratives Bild-Zeichen-Spiel zwischen zwei Agenten, genannt CoDraw. Der Erzähler sieht eine abstrakte Szene, die mehrere Cliparts in einer semantisch sinnvollen Konfiguration enthält, während der Zeichner versucht, die Szene auf einer leeren Leinwand mit verfügbaren Cliparts zu rekonstruieren. Wir sammeln den CoDraw-Datensatz von ~10.000 Dialogen, die aus ~138.000 Nachrichten bestehen, die zwischen menschlichen Agenten ausgetauscht werden, und definieren Protokolle und Metriken, um die Effektivität von gelernten Agenten auf diesem Testbett zu evaluieren, wobei wir die Notwendigkeit einer neuartigen "Crosstalk"-Bedingung hervorheben, die Agenten, die unabhängig voneinander auf disjunkten Teilmengen der Trainingsdaten trainiert wurden, für die Evaluierung zusammenbringt. Wir stellen Modelle für unsere Aufgabe vor, einschließlich einfacher, aber effektiver Baselines und neuronaler Netzwerkansätze, die mit einer Kombination aus Nachahmungslernen und zielorientiertem Training trainiert werden, wobei alle Modelle sowohl durch eine vollautomatische Auswertung als auch durch das Spielen des Spiels mit menschlichen Agenten getestet werden.
Das Vorhandensein von Verzerrungen und Störfaktoren ist zweifelsohne eine der größten Herausforderungen bei Anwendungen des maschinellen Lernens, die in den letzten Jahren zu zentralen Debatten geführt hat - von falschen Assoziationen durch Störvariablen in medizinischen Studien bis hin zur Verzerrung durch die Rasse in Systemen zur Geschlechts- oder Gesichtserkennung. Eine Lösung besteht darin, Datensätze zu verbessern und so zu organisieren, dass sie keine Verzerrungen enthalten, was eine mühsame und intensive Aufgabe ist. Die Alternative besteht darin, die verfügbaren Daten zu nutzen und Modelle zu erstellen, die diese Verzerrungen berücksichtigen. In diesem Papier schlagen wir eine Methode vor, die auf der adversen Trainingsstrategie basiert, um diskriminierende Merkmale zu erlernen, die unvoreingenommen und invariant gegenüber den Störfaktoren sind, was durch die Einbeziehung einer neuen adversen Verlustfunktion ermöglicht wird, die eine verschwindende Korrelation zwischen dem Bias und den erlernten Merkmalen fördert. Wir wenden unsere Methode auf einen synthetischen, einen medizinischen Diagnose- und einen Geschlechtsklassifizierungsdatensatz (Gender Shades) an, und unsere Ergebnisse zeigen, dass die von unserer Methode gelernten Merkmale nicht nur zu einer besseren Vorhersageleistung führen, sondern auch nicht mit den Bias- oder Confounder-Variablen korreliert sind. http://blinded_for_review/ ist der Code verfügbar.
Bestehende neuronale Modelle zur Beantwortung von Fragen (QA) müssen bei den meisten großen QA-Datensätzen über einen langen Kontext nachdenken und komplizierte Schlussfolgerungen ziehen. Jüngste Arbeiten haben gezeigt, dass ein Satzselektionsmodul, das einen kürzeren Kontext auswählt und in das nachgeschaltete QA-Modell einspeist, eine vergleichbare Leistung erzielt wie ein QA-Modell, das auf dem vollständigen Kontext trainiert wurde, und dabei auch besser interpretierbar ist. Wir stellen die Hypothese auf, dass das Satzauswahlmodul fremden Kontext herausfiltern kann und es dem nachgeschalteten QA-Modell dadurch ermöglicht, sich auf die Teile des Kontexts zu konzentrieren, die für die Frage relevant sind, und darüber nachzudenken. Wir zeigen jedoch, dass eine Pipeline, die aus einem Satzauswahlmodul und einem anschließenden QA-Modell besteht, im Vergleich zu einem QA-Modell, das auf dem vollständigen Kontext trainiert wurde, robuster gegenüber Angriffen von außen ist.
Obwohl Wissensdatenbanken in der Regel eine baumartige oder zyklische Struktur aufweisen, kann keiner der bestehenden Ansätze diese Daten in einen kompatiblen Raum einbetten, der mit der Struktur übereinstimmt. Um dieses Problem zu überwinden, wird in diesem Papier ein neuartiger Rahmen, genannt Riemannian TransE, vorgeschlagen, um die Entitäten in eine Riemannsche Mannigfaltigkeit einzubetten. Riemannian TransE modelliert jede Relation als eine Bewegung zu einem Punkt und definiert spezifische neuartige Distanzdifferenzen für jede Relation, so dass alle Relationen auf natürliche Weise in Übereinstimmung mit der Struktur der Daten eingebettet sind.Experimente mit verschiedenen Aufgaben zur Vervollständigung von Wissensdatenbanken haben gezeigt, dass Riemannian TransE bei einer geeigneten Wahl der Mannigfaltigkeit auch mit deutlich reduzierten Parametern eine gute Leistung erzielt.
Das katastrophale Vergessen in neuronalen Netzen ist eines der bekanntesten Probleme des kontinuierlichen Lernens.Bisherige Versuche, das Problem zu lösen, konzentrieren sich darauf, zu verhindern, dass sich wichtige Gewichte ändern.Solche Methoden erfordern oft Aufgabengrenzen, um effektiv zu lernen, und unterstützen kein Rückwärts-Transfer-Lernen.In diesem Papier schlagen wir einen Meta-Lernalgorithmus vor, der lernt, die Gradienten alter Aufgaben zu rekonstruieren. Wir schlagen einen Meta-Lernalgorithmus vor, der lernt, die Gradienten alter Aufgaben im Hinblick auf die aktuellen Parameter zu rekonstruieren und diese rekonstruierten Gradienten mit dem aktuellen Gradienten zu kombinieren, um kontinuierliches Lernen und Rückwärtstransferlernen von der aktuellen Aufgabe zu früheren Aufgaben zu ermöglichen.
 Wir geben eine formale Prozedur für die Berechnung von Vorabbildern von Faltungsnetzwerken an, indem wir die duale Basis verwenden, die aus dem Satz von Hyperebenen definiert ist, die mit den Schichten des Netzwerks assoziiert sind, und weisen auf die besondere Symmetrie hin, die mit Anordnungen von Hyperebenen von Faltungsnetzwerken verbunden ist, die die Form regelmäßiger mehrdimensionaler polyedrischer Kegel haben. Wir erörtern die Effizienz einer großen Anzahl von Schichten verschachtelter Kegel, die aus inkrementellen Faltungen kleiner Größe resultieren, um einen guten Kompromiss zwischen effizienter Kontraktion von Daten auf niedrige Dimensionen und der Formung von Vorabbild-Mannigfaltigkeiten zu erzielen.Wir demonstrieren, wie ein spezifisches Netzwerk eine nicht lineare Eingangs-Mannigfaltigkeit zu einer affinen Ausgangs-Mannigfaltigkeit abflacht und diskutieren seine Bedeutung für das Verständnis der Klassifizierungseigenschaften von tiefen Netzwerken.
Meta-Lernen hat beeindruckende Fortschritte bei der schnellen Modellanpassung gemacht, aber es gibt nur wenige Arbeiten, die sich mit der schnellen Unsicherheitsanpassung bei der Bayes'schen Modellierung befassen.In diesem Papier schlagen wir vor, das Ziel zu erreichen, indem wir Meta-Lernen auf den Raum der Wahrscheinlichkeitsmaße anwenden und das Konzept des Meta-Samplings für die schnelle Unsicherheitsanpassung einführen. Der Meta-Sampler wird durch die Annahme einer neuronal-inversen autoregressiven Flussstruktur (NIAF), einer Variante der kürzlich vorgeschlagenen neuronalen autoregressiven Flüsse, konstruiert, um effizient Meta-Samples zu erzeugen, die angepasst werden sollen. Der Probenadapter verschiebt Metastichproben zu aufgabenspezifischen Proben, basierend auf einer neu vorgeschlagenen und allgemeinen Bayes'schen Sampling-Technik, genannt Optimal-Transport Bayes'sches Sampling.Die Kombination der beiden Komponenten erlaubt es, ein einfaches Lernverfahren für den Meta-Sampler zu entwickeln, das über Standard-Back-Propagation effizient optimiert werden kann.Umfangreiche experimentelle Ergebnisse zeigen die Effizienz und Effektivität des vorgeschlagenen Rahmens, der im Vergleich zu verwandten Methoden eine bessere Qualität der Proben und eine schnellere Anpassung der Ungewissheit erreicht.
Wir schlagen ein kontextadaptives Entropiemodell für den Einsatz in der durchgängig optimierten Bildkompression vor, das zwei Arten von Kontexten nutzt, nämlich bitverbrauchende Kontexte und bitfreie Kontexte, die danach unterschieden werden, ob eine zusätzliche Bitallokation erforderlich ist, und die es dem Modell ermöglichen, die Verteilung jeder latenten Repräsentation mit einer verallgemeinerten Form der Approximationsmodelle genauer zu schätzen, was zu einer verbesserten Kompressionsleistung führt. Basierend auf den experimentellen Ergebnissen übertrifft die vorgeschlagene Methode die traditionellen Bildcodecs, wie BPG und JPEG2000, sowie andere frühere, auf künstlichen neuronalen Netzen (ANN) basierende Ansätze in Bezug auf den Spitzenwert des Signal-Rausch-Verhältnisses (PSNR) und den Index der strukturellen Ähnlichkeit auf mehreren Ebenen (MS-SSIM). Der Testcode ist öffentlich verfügbar unter https://github.com/JooyoungLeeETRI/CA_Entropy_Model.
Tiefe Netzwerke erbringen oft gute Leistungen in der Datenverteilung, auf der sie trainiert wurden, geben jedoch falsche (und oft sehr sichere) Antworten, wenn sie auf Punkten außerhalb der Trainingsverteilung ausgewertet werden.  Im Idealfall würde ein Modell Punkten, die nicht aus der Trainingsverteilung stammen, ein geringeres Vertrauen zuweisen.  Wir schlagen einen Regularizer vor, der dieses Problem angeht, indem er mit interpolierten versteckten Zuständen trainiert und den Klassifikator dazu anregt, an diesen Punkten weniger Vertrauen zu haben.  Da die versteckten Zustände gelernt werden, hat dies den wichtigen Effekt, dass die versteckten Zustände für eine Klasse so konzentriert werden, dass die Interpolationen innerhalb derselben Klasse oder zwischen zwei verschiedenen Klassen sich nicht mit den realen Datenpunkten anderer Klassen überschneiden.  Dies hat den großen Vorteil, dass es die Unteranpassung vermeidet, die durch Interpolation im Eingaberaum entstehen kann.  Wir beweisen, dass die genaue Bedingung für die Vermeidung dieses Problems der Unteranpassung durch Manifold Mixup darin besteht, dass die Dimensionalität der verborgenen Zustände die Anzahl der Klassen übersteigt, was in der Praxis häufig der Fall ist.  Darüber hinaus kann man davon ausgehen, dass diese Konzentration die Merkmale in früheren Schichten diskriminanter macht.  Wir zeigen, dass Manifold Mixup trotz des geringen zusätzlichen Rechenaufwands große Verbesserungen im Vergleich zu starken Baselines beim überwachten Lernen, bei der Robustheit gegenüber einstufigen Angriffen, beim halbüberwachten Lernen und bei der Negativen Log-Likelihood auf herausgehaltenen Proben erzielt.
Bei der stereotaktischen Radiochirurgie (SRS) ist es manchmal erforderlich, eine Region von Interesse (ROI) wie z. B. Krebs zu packen, um einen Behandlungsplan zu erstellen.  Wir haben einen Kugelpackungsalgorithmus entwickelt, der nicht-überschneidende Kugeln innerhalb der ROI packt.  Die Region von Interesse sind in unserem Fall die Voxel, die als Krebsgewebe identifiziert wurden.  Epsilon-Rotationsinvarianz bedeutet, dass die 3D-ROI beliebig gedreht werden kann, während die Volumeneigenschaften innerhalb einer gewissen Grenze von Epsilon (fast) gleich bleiben. Die angewandten Rotationen erzeugen eine Kugelpackung, die hoch korreliert bleibt, wenn wir die geometrischen Eigenschaften der Kugelpackung vor und nach der Rotation der Volumendaten für die ROI analysieren. Unser neuartiger Kugelpackungsalgorithmus hat einen hohen Grad an Rotationsinvarianz im Bereich von +/- epsilon.Our Methode verwendet einen Formdeskriptor, der aus den Werten der disjunkten Menge von Kugeln aus dem abstandsbasierten Kugelpackungsalgorithmus abgeleitet wird, um den invarianten Deskriptor aus der ROI zu extrahieren.We demonstriert durch die Umsetzung dieser Ideen mit Slicer3D Plattform für unsere Forschung zur Verfügung.  Die Daten basieren auf sing MRI Stereotactic images.We präsentiert mehrere Leistungsergebnisse auf verschiedenen Benchmarks Daten von über 30 Patienten in Slicer3D Plattform.
Wir zeigen, dass sich implizite Sparsamkeit auf Filterebene in Faltungsneuronalen Netzen (CNNs) manifestiert, die Batch-Normalisierung und ReLU-Aktivierung verwenden und mit adaptiven Gradientenabstiegstechniken mit L2-Regularisierung oder Gewichtsabnahme trainiert werden.Durch eine umfangreiche empirische Studie (Anonymous, 2019) stellen wir eine Hypothese über den Mechanismus hinter dem Sparsifizierungsprozess auf.Wir stellen fest, dass das Zusammenspiel verschiedener Phänomene die Stärke von L2- und Gewichtsabnahme-Regularisatoren beeinflusst, was dazu führt, dass die vermeintlich nicht sparsamkeitsinduzierenden Regularisatoren Filtersparsamkeit induzieren.  In diesem Workshop-Artikel fassen wir einige unserer wichtigsten Erkenntnisse und Experimente zusammen und präsentieren zusätzliche Ergebnisse auf modernen Netzwerkarchitekturen wie ResNet-50.
Menschen können im Laufe ihres Lebens eine Vielzahl von Konzepten und Fähigkeiten inkrementell erlernen und dabei eine Reihe wünschenswerter Eigenschaften aufweisen, wie z.B. Nicht-Vergessen, Wiederholung von Konzepten, Vorwärts- und Rückwärtstransfer von Wissen, Lernen mit wenigen Schüssen und selektives Vergessen.Bisherige Ansätze für lebenslanges maschinelles Lernen können nur Teilmengen dieser Eigenschaften demonstrieren, oft durch Kombination mehrerer komplexer Mechanismen.  In dieser Perspektive schlagen wir einen leistungsfähigen einheitlichen Rahmen vor, der alle Eigenschaften durch die Verwendung einer kleinen Anzahl von Gewichtskonsolidierungsparametern in tiefen neuronalen Netzen demonstrieren kann.Darüber hinaus sind wir in der Lage, viele Parallelen zwischen den Verhaltensweisen und Mechanismen unseres vorgeschlagenen Rahmens und denjenigen zu ziehen, die das menschliche Lernen betreffen, wie z. B. Gedächtnisverlust oder Schlafentzug.Diese Perspektive dient als Kanal für wechselseitige Inspiration, um lebenslanges Lernen bei Maschinen und Menschen besser zu verstehen.
Die CT-Rekonstruktion mit begrenztem Winkel ist ein unterbestimmtes lineares inverses Problem, das zur Lösung geeignete Regularisierungstechniken erfordert. In dieser Arbeit untersuchen wir, wie vortrainierte generative adversarische Netzwerke (GANs) verwendet werden können, um verrauschte, stark artefaktbeladene Rekonstruktionen von konventionellen Techniken zu bereinigen, indem sie effektiv auf die abgeleitete Bildvielfalt projiziert werden. Insbesondere verwenden wir eine robuste Version des weit verbreiteten GAN-Prior für inverse Probleme, basierend auf einer neuen Technik, die Korruptionsnachahmung genannt wird und die Rekonstruktionsqualität erheblich verbessert Der vorgeschlagene Ansatz arbeitet direkt im Bildraum, wodurch er nicht trainiert werden muss und keinen Zugang zum Messmodell benötigt.
Wir zeigen, wie die Überwachung auf Satzebene verwendet werden kann, um die Kodierung einzelner Sätze zu verbessern und um zu lernen, welche Eingabesätze eher die Beziehung zwischen einem Paar von Entitäten ausdrücken.Wir stellen auch eine neuartige neuronale Architektur zum Sammeln von Signalen aus mehreren Eingabesätzen vor, die die Vorteile von Aufmerksamkeit und Maxpooling kombiniert.Die vorgeschlagene Methode erhöht die AUC um 10 % (von 0,261 auf 0,284) und übertrifft die kürzlich veröffentlichten Ergebnisse auf dem FB-NYT-Datensatz.
Die Aufmerksamkeitsschicht in einem neuronalen Netzmodell bietet Einblicke in die Überlegungen des Modells hinter seiner Vorhersage, die in der Regel als undurchsichtig kritisiert werden.In letzter Zeit sind scheinbar widersprüchliche Standpunkte über die Interpretierbarkeit von Aufmerksamkeitsgewichten aufgetaucht (Jain & Wallace, 2019; Vig & Belinkov, 2019).Inmitten dieser Verwirrung entsteht die Notwendigkeit, den Aufmerksamkeitsmechanismus systematischer zu verstehen.In dieser Arbeit versuchen wir, diese Lücke zu füllen, indem wir eine umfassende Erklärung geben, die beide Arten von Beobachtungen rechtfertigt (d.h., Durch eine Reihe von Experimenten zu verschiedenen NLP-Aufgaben validieren wir unsere Beobachtungen und untermauern unsere Behauptung der Interpretierbarkeit von Aufmerksamkeit durch manuelle Auswertung.
Wir stellen ein neuartiges Modell für dieses Problem vor, das einen Graphen verwendet, um den Zwischenzustand des generierten Outputs zu repräsentieren.Unser Modell generiert Code, indem es grammatikgesteuerte Expansionsschritte mit Graphenerweiterung und neuronalen Nachrichtenübermittlungsschritten verschachtelt.Eine experimentelle Evaluierung zeigt, dass unser neues Modell semantisch sinnvolle Ausdrücke generieren kann und dabei eine Reihe von starken Grundmodellen übertrifft.
Die Ableitung von Modellen, die Vorhersage zukünftiger Symbole und die Schätzung der Entropierate von zeit- und ereignisdiskreten Prozessen ist ein altbekanntes Verfahren, doch viele Zeitreihen lassen sich besser als zeit- und ereignisdiskrete Prozesse konzeptualisieren. Die Methoden beruhen auf einer Erweiterung der Bayes'schen Strukturinferenz, die die universelle Approximationskraft neuronaler Netze ausnutzt. Auf der Grundlage von Experimenten mit einfachen synthetischen Daten scheinen diese neuen Methoden mit den modernsten Methoden zur Vorhersage und Entropieratenschätzung konkurrenzfähig zu sein, solange das richtige Modell abgeleitet wird.
Jüngste Studien zeigen, dass Faltungsneuronale Netze (CNNs) unter verschiedenen Bedingungen anfällig sind, z. B. bei gegnerischen Beispielen, Backdoor-Angriffen und Verteilungsverschiebungen.  Motiviert durch die Erkenntnis, dass das menschliche visuelle System bei der Erkennung stärker auf die globale Struktur (z. B., Motiviert durch die Erkenntnisse, dass das menschliche visuelle System bei der Erkennung eher auf globale Strukturen (z. B. Form) achtet, während CNNs eher auf lokale Texturmerkmale in Bildern ausgerichtet sind, schlagen wir ein einheitliches Rahmenwerk EdgeGANRob vor, das auf robusten Kantenmerkmalen basiert, um die Robustheit von CNNs im Allgemeinen zu verbessern. Dabei werden zunächst explizit Form-/Strukturmerkmale aus einem gegebenen Bild extrahiert und dann ein neues Bild rekonstruiert, indem die Texturinformationen mit einem trainierten generativen adversen Netzwerk (GAN) aufgefüllt werden. Um die Empfindlichkeit des Kantenerkennungsalgorithmus gegenüber Störungen zu verringern, schlagen wir einen robusten Kantenerkennungsansatz Robust Canny vor, der auf dem Vanilla Canny-Algorithmus basiert. Um weitere Einblicke zu gewinnen, vergleichen wir EdgeGANRob mit dem vereinfachten Verfahren EdgeNetRob, das Lernaufgaben direkt auf den extrahierten robusten Kantenmerkmalen durchführt.Wir stellen fest, dass EdgeNetRob die Robustheit des Modells erheblich verbessern kann, allerdings auf Kosten der Genauigkeit des reinen Modells.EdgeGANRob hingegen ist in der Lage, die Genauigkeit des reinen Modells im Vergleich zu EdgeNetRob zu verbessern, ohne die Vorteile der Robustheit zu verlieren, die durch EdgeNetRob eingeführt wurden.Umfangreiche Experimente zeigen, dass EdgeGANRob in verschiedenen Lernaufgaben unter unterschiedlichen Bedingungen widerstandsfähig ist.
Durch die Extraktion von High-Level-Abstraktionen im Bottom-Up-Inferenzprozess wird jedoch das Ziel der Erhaltung aller Variationsfaktoren für die Top-Down-Generierung gefährdet.Motiviert durch das Konzept des "klein anfangen", stellen wir eine Strategie vor, um schrittweise unabhängige hierarchische Repräsentationen von hohen zu niedrigen Abstraktionsebenen zu erlernen.Das Modell beginnt mit dem Lernen der abstraktesten Repräsentation und erweitert dann schrittweise die Netzwerkarchitektur, um neue Repräsentationen auf verschiedenen Abstraktionsebenen einzuführen. Wir demonstrieren quantitativ die Fähigkeit des vorgestellten Modells, die Entflechtung im Vergleich zu bestehenden Arbeiten an zwei Benchmark-Datensätzen zu verbessern, indem wir drei Entflechtungsmetriken verwenden, einschließlich einer neuen Metrik, die wir vorgeschlagen haben, um die zuvor vorgestellte Metrik der gegenseitigen Informationslücke zu ergänzen. Indem wir die jeweiligen Vorteile des Lernens hierarchischer Repräsentationen und des progressiven Lernens nutzen, ist dies unseres Wissens der erste Versuch, die Entflechtung zu verbessern, indem die Kapazität der VAE zum Lernen hierarchischer Repräsentationen progressiv gesteigert wird.
Zu diesem Zweck wenden wir eine Copula-Transformation an, die durch die Wiederherstellung der Invarianzeigenschaften der Information Bottleneck-Methode zu einer Entflechtung der Merkmale im latenten Raum führt, und zeigen darauf aufbauend, wie diese Transformation zu einer Sparsamkeit des latenten Raums im neuen Modell führt.  Wir evaluieren unsere Methode an künstlichen und realen Daten.
Wir stellen eine neuartige Methode vor, um solche Benchmarks systematisch zu konstruieren, indem wir die zusammengesetzte Divergenz maximieren und gleichzeitig eine kleine atomare Divergenz zwischen Trainings- und Testdatensätzen garantieren, und wir vergleichen diese Methode quantitativ mit anderen Ansätzen zur Erstellung von Benchmarks für kompositionelle Generalisierung. Wir stellen einen großen und realistischen Datensatz zur Beantwortung von Fragen in natürlicher Sprache vor, der nach dieser Methode konstruiert wurde, und verwenden ihn, um die kompositionelle Generalisierungsfähigkeit von drei Architekturen des maschinellen Lernens zu analysieren. Wir stellen fest, dass sie nicht kompositionell verallgemeinern können und dass es eine überraschend starke negative Korrelation zwischen Verbunddivergenz und Genauigkeit gibt.
Um zu verstehen, wie sich das Sehen von Objekten im Säuglings- und Kindesalter entwickelt, müssen überprüfbare Computermodelle entwickelt werden. Tiefe neuronale Netze (DNNs) haben sich als Modelle für das Sehen Erwachsener bewährt, aber es ist noch nicht klar, ob sie als Modelle für die Entwicklung taugen. Wir quantifizierten die Entwicklung expliziter Objektrepräsentationen auf jeder Ebene dieses Netzwerks durch Training, indem wir die Faltungsschichten einfroren und eine zusätzliche lineare Dekodierungsschicht trainierten. Wir evaluierten die Dekodierungsgenauigkeit auf der gesamten ImageNet-Validierungsmenge und auch für einzelne visuelle Klassen. CORnet verwendet jedoch überwachtes Training, und da Kleinkinder nur einen äußerst unzureichenden Zugang zu Labels haben, müssen sie stattdessen auf unbeaufsichtigte Weise lernen. Wir haben daher auch das Lernen in einem hochmodernen unbeaufsichtigten Netzwerk (DeepCluster) gemessen.CORnet und DeepCluster unterscheiden sich sowohl in der Überwachung als auch in den Faltungsnetzwerken, die ihnen zugrunde liegen. Wir machen Vorhersagen darüber, wie sich das Lernen in verschiedenen Hirnregionen bei Säuglingen entwickeln sollte. In allen drei Netzwerken haben wir auch auf eine Beziehung zwischen der Reihenfolge, in der Säuglinge und Maschinen visuelle Klassen erwerben, getestet und nur Hinweise auf eine kontraintuitive Beziehung gefunden. Wir diskutieren die möglichen Gründe dafür.
Wir zeigen, dass in einer Vielzahl von groß angelegten Deep-Learning-Szenarien der Gradient dynamisch zu einem sehr kleinen Unterraum nach einer kurzen Zeit des Trainings konvergiert.der Unterraum wird durch einige Top-Eigenvektoren des Hessian (gleich der Anzahl der Klassen im Datensatz) aufgespannt, und ist meist über lange Zeiträume des Trainings erhalten.ein einfaches Argument legt dann nahe, dass Gradientenabstieg kann vor allem in diesem Unterraum passieren.wir geben ein Beispiel für diesen Effekt in einem lösbaren Modell der Klassifizierung, und wir kommentieren mögliche Auswirkungen für die Optimierung und Lernen.
Die Beantwortung von Fragen, die Multi-Hop-Argumentation auf Web-Skala erfordert das Abrufen mehrerer Beweise Dokumente, von denen eine oft wenig lexikalische oder semantische Beziehung zu der Frage.Dieses Papier stellt einen neuen Graphen-basierte rekurrente Retrieval-Ansatz, der lernt, um Argumentation Pfade über den Wikipedia-Graphen, um Multi-Hop-Open-Domain-Fragen zu beantworten.Unsere Retriever-Modell trainiert ein rekurrentes neuronales Netz, das lernt, um sequentiell abrufen Beweis Absätze in der Argumentation Pfad durch Konditionierung auf die zuvor abgerufenen Dokumente. Unser Lesemodell bewertet die Argumentationspfade und extrahiert die Antwortspanne, die im besten Argumentationspfad enthalten ist.Experimentelle Ergebnisse zeigen State-of-the-Art-Ergebnisse in drei Open-Domain-QA-Datensätzen, die die Effektivität und Robustheit unserer Methode unter Beweis stellen.Insbesondere erreicht unsere Methode eine signifikante Verbesserung in HotpotQA und übertrifft das vorherige beste Modell um mehr als 14 Punkte.
In den letzten Jahren haben Stereo-Matching-Algorithmen, die auf Deep Learning basieren, eine hervorragende Leistung erzielt und sind zur Hauptforschungsrichtung geworden.Bestehende Algorithmen verwenden in der Regel tiefe Faltungsneuronale Netze (DCNNs), um abstraktere semantische Informationen zu extrahieren, aber wir glauben, dass die detaillierten Informationen der räumlichen Struktur für Stereo-Matching-Aufgaben wichtiger sind. Das Netzwerk besteht aus drei Teilen: einem Modul zur primären Merkmalsextraktion, einem ASPP-Modul (Atrous Spatial Pyramid Pooling) und einem Modul zur Merkmalsfusion.Das primäre Merkmalsextraktionsnetzwerk enthält nur drei Faltungsschichten. Dieses Netzwerk nutzt die grundlegende Fähigkeit der Merkmalsextraktion des flachen Netzwerks, um die detaillierten Informationen der räumlichen Struktur zu extrahieren und zu bewahren. In diesem Papier wird das Modul der erweiterten Faltung und des räumlichen Pyramidenpoolings (ASPP) eingeführt, um die Größe des rezeptiven Feldes zu erhöhen. Wir haben den Teil der Merkmalsextraktion der bestehenden Stereo-Matching-Algorithmen durch unser flaches Merkmalsextraktionsnetzwerk ersetzt und eine Spitzenleistung auf dem KITTI 2015-Datensatz erzielt. Im Vergleich zum Referenznetzwerk wurde die Anzahl der Parameter um 42 % reduziert und die Matching-Genauigkeit um 1,9 % verbessert.
Wir schlagen eine neue Ausgabeschicht für tiefe neuronale Netze vor, die die Verwendung von protokolliertem kontextuellem Bandit-Feedback für das Training erlaubt, Zu diesem Zweck schlagen wir einen Ansatz zur kontrafaktischen Risikominimierung (Counterfactual Risk Minimization, CRM) für das Training von tiefen Netzen unter Verwendung eines äquivarianten empirischen Risikoschätzers mit Varianzregulierung, BanditNet, vor und zeigen, wie das resultierende Ziel so zerlegt werden kann, dass ein Training mit stochastischem Gradientenabstieg (SGD) möglich ist. Wir demonstrieren empirisch die Effektivität der Methode, indem wir zeigen, wie tiefe Netzwerke - insbesondere ResNets - für die Objekterkennung ohne konventionell beschriftete Bilder trainiert werden können.
Protein-Klassifikation ist verantwortlich für die biologische Sequenz, kamen wir mit einer Idee, die sich mit der Klassifizierung von Proteomics mit Deep Learning Algorithmus.Dieser Algorithmus konzentriert sich hauptsächlich auf Sequenzen von Protein-Vektor, der für die Darstellung von Proteomics verwendet wird zu klassifizieren. Die Auswahl der Art der Proteindarstellung ist eine Herausforderung, von der die Ausgabe in Bezug auf die Genauigkeit abhängt. Die hier verwendete Proteindarstellung ist ein n-Gramm, d.h. ein 3-Gramm und Kerasembedding, das für biologische Sequenzen wie Proteine verwendet wird.In diesem Papier arbeiten wir an der Proteinklassifizierung, um die Stärke und Darstellung der biologischen Sequenz der Proteine zu zeigen.
In dieser Arbeit versuchen wir, eine kritische Frage zu beantworten: ob es eine Eingabesequenz gibt, die ein gut trainiertes Sequenz-zu-Sequenz (seq2seq)-Modell für ein diskretes neuronales Netzwerk dazu veranlasst, ungeheuerliche Ausgaben (aggressiv, bösartig, angreifend usw.) zu generieren, und wenn es solche Eingaben gibt, wie man sie effizient findet. Wir wenden eine empirische Methodik an, bei der wir zunächst Listen von ungeheuerlichen Ausgabesequenzen erstellen und dann einen diskreten Optimierungsalgorithmus entwerfen, um Eingabesequenzen zu finden, die das Modell dazu veranlassen, diese zu generieren, wobei der Optimierungsalgorithmus für die Suche nach großen Vokabeln erweitert und darauf beschränkt wird, nach Eingabesequenzen zu suchen, die wahrscheinlich von realen Benutzern eingegeben werden: Wir zeigen, dass angesichts der Auslöser-Eingaben, die unser Algorithmus findet, eine signifikante Anzahl von bösartigen Sätzen vom Modell mit hoher Wahrscheinlichkeit zugewiesen wird, was eine unerwünschte Folge des standardmäßigen seq2seq-Trainings darstellt.
Beim Problem des unüberwachten Lernens von unzusammenhängenden Repräsentationen ist eine der vielversprechendsten Methoden, die Gesamtkorrelation der gesampelten latenten Variablen zu bestrafen.  Leider scheitert diese gut motivierte Strategie oft an einer problematischen Differenz zwischen der gesampelten latenten Repräsentation und ihrer entsprechenden mittleren Repräsentation.  Wir liefern eine theoretische Erklärung dafür, dass eine niedrige Gesamtkorrelation der Stichprobenverteilung keine Garantie für eine niedrige Gesamtkorrelation der mittleren Repräsentation ist und beweisen, dass es für die mittlere Repräsentation mit beliebig hoher Gesamtkorrelation Verteilungen latenter Variablen mit hoher Gesamtkorrelation gibt.  Dennoch glauben wir, dass die Gesamtkorrelation ein Schlüssel zur Entflechtung des unüberwachten repräsentativen Lernens sein könnte, und wir schlagen eine Abhilfe, RTC-VAE, vor, die die Gesamtkorrelationsstrafe korrigiert.   Experimente zeigen, dass unser Modell eine vernünftigere Verteilung der mittleren Repräsentation im Vergleich zu Basismodellen, z.B. β-TCVAE und FactorVAE, aufweist.
Um die Bildstruktur besser mit dem generierten Text zu verknüpfen, ersetzen wir in dieser Arbeit den traditionellen Softmax-Aufmerksamkeitsmechanismus durch zwei alternative, die Sparsamkeit fördernde Transformationen: Sparsemax und Total-Variation Sparse Attention (TVmax).  Um die Sparsamkeit zu fördern und die Verschmelzung von benachbarten räumlichen Orten zu unterstützen, schlagen wir TVmax vor.  Durch die Auswahl relevanter Gruppen von Merkmalen verbessert die TVmax-Transformation die Interpretierbarkeit der Daten, die wir in den Datensätzen Microsoft COCO und Flickr30k präsentieren.  TVmax übertrifft die anderen verglichenen Aufmerksamkeitsmechanismen in Bezug auf die von Menschen bewertete Qualität der Bildunterschriften und die Relevanz der Aufmerksamkeit.
Obwohl es mehr als 65.000 Sprachen in der Welt gibt, klingen die Aussprachen vieler Phoneme in allen Sprachen ähnlich.Wenn Menschen eine Fremdsprache lernen, spiegelt ihre Aussprache oft die Eigenschaften ihrer Muttersprache wider.Das motiviert uns zu untersuchen, wie das Sprachsynthese-Netzwerk die Aussprache lernt, wenn mehrsprachige Datensätze gegeben sind.In dieser Studie trainieren wir das Sprachsynthese-Netzwerk zweisprachig in Englisch und Koreanisch, und analysieren, wie das Netzwerk die Beziehungen der Phonem-Aussprache zwischen den Sprachen lernt. Unser experimentelles Ergebnis zeigt, dass die gelernten Phonemeinbettungsvektoren näher beieinander liegen, wenn ihre Aussprache in den verschiedenen Sprachen ähnlich ist. Basierend auf diesem Ergebnis zeigen wir auch, dass es möglich ist, Netzwerke zu trainieren, die die koreanische Sprache eines englischen Sprechers synthetisieren und umgekehrt.
Generative Adversarial Networks (GANs) haben in letzter Zeit beeindruckende Ergebnisse für viele reale Anwendungen erzielt, und viele GAN-Varianten sind mit Verbesserungen bei der Qualität der Proben und der Trainingsstabilität aufgetaucht.Allerdings fehlen weitgehend die Visualisierung und das Verständnis von GANs.Wie repräsentiert ein GAN unsere visuelle Welt intern?Was verursacht die Artefakte in GAN-Ergebnissen? Die Beantwortung dieser Fragen könnte es uns ermöglichen, neue Einsichten und bessere Modelle zu entwickeln.In dieser Arbeit stellen wir einen analytischen Rahmen vor, um GANs auf der Ebene von Einheiten, Objekten und Szenen zu visualisieren und zu verstehen.Wir identifizieren zunächst eine Gruppe von interpretierbaren Einheiten, die eng mit Objektkonzepten verbunden sind, mit einer segmentierungsbasierten Netzwerkzerlegungsmethode. Schließlich untersuchen wir die kontextuelle Beziehung zwischen diesen Einheiten und ihrer Umgebung, indem wir die entdeckten Objektkonzepte in neue Bilder einfügen.Wir zeigen mehrere praktische Anwendungen, die durch unser Framework ermöglicht werden, vom Vergleich interner Repräsentationen über verschiedene Schichten, Modelle und Datensätze bis hin zur Verbesserung von GANs durch das Auffinden und Entfernen von Artefakt-verursachenden Einheiten und zur interaktiven Manipulation von Objekten in der Szene.Wir stellen Open-Source-Interpretationswerkzeuge zur Verfügung, um Peer-Forschern und Praktikern ein besseres Verständnis ihrer GAN-Modelle zu ermöglichen.
In der Vergangenheit war das Streben nach effizienter Inferenz eine der treibenden Kräfte hinter der Erforschung neuer Deep-Learning-Architekturen und -Bausteine. Zu den jüngsten Beispielen gehören: das Squeeze-and-Excitation-Modul von (Hu et al., 2018), tiefenweise separierbare Faltungen in Xception (Chollet, 2017) und der umgekehrte Flaschenhals in MobileNet v2 (Sandler et al., 2018).  In all diesen Fällen ermöglichten die resultierenden Bausteine nicht nur eine höhere Effizienz, sondern auch eine höhere Genauigkeit und fanden eine breite Akzeptanz in der Praxis. In dieser Arbeit erweitern wir das Arsenal effizienter Bausteine für neuronale Netzwerkarchitekturen, aber anstatt Standardprimitive (wie Faltung) zu kombinieren, plädieren wir für den Ersatz dieser dichten Primitive durch ihre spärlichen Gegenstücke.  Die Idee, die Anzahl der Parameter durch Sparsamkeit zu verringern, ist zwar nicht neu (Mozer & Smolensky, 1989), aber die gängige Meinung ist, dass diese Verringerung der theoretischen FLOPs sich nicht in realen Effizienzgewinnen niederschlägt.  Wir wollen dieses Missverständnis korrigieren, indem wir eine Familie von effizienten Sparse-Kerneln für verschiedene Hardware-Plattformen einführen, die wir zum Nutzen der Community als Open Source zur Verfügung stellen wollen: Ausgestattet mit unserer effizienten Implementierung von Sparse-Primitiven zeigen wir, dass Sparse-Versionen der MobileNet v1- und MobileNet v2-Architekturen die starken Dense-Baselines auf der Effizienz-Genauigkeits-Kurve deutlich übertreffen.   Auf dem Snapdragon 835 übertreffen unsere spärlichen Netzwerke ihre dichten Äquivalente um das 1,1-2,2-fache - was ungefähr einer ganzen Generation an Verbesserungen entspricht.  Wir hoffen, dass unsere Ergebnisse eine breitere Anwendung von Sparsity als Werkzeug zur Erstellung effizienter und genauer Deep-Learning-Architekturen ermöglichen.
In dieser Arbeit befassen wir uns mit der halbüberwachten Klassifizierung von Graphdaten, bei der die Kategorien dieser nicht beschrifteten Knoten aus beschrifteten Knoten sowie Graphstrukturen abgeleitet werden.Neuere Arbeiten lösen dieses Problem oft mit der fortgeschrittenen Graphfaltung auf herkömmliche überwachte Weise, aber die Leistung könnte stark beeinträchtigt werden, wenn beschriftete Daten knapp sind.Hier schlagen wir ein Graph Inference Learning (GIL) Framework vor, um die Leistung der Knotenklassifizierung durch das Erlernen der Ableitung von Knotenbeschriftungen auf der Graphentopologie zu steigern. Um die Verbindung zwischen zwei Knoten zu überbrücken, definieren wir formal eine Strukturrelation, indem wir Knotenattribute, Pfade zwischen den Knoten und lokale topologische Strukturen zusammen kapseln, so dass die Inferenz bequem von einem Knoten zu einem anderen Knoten abgeleitet werden kann. Um den Inferenzprozess zu erlernen, führen wir außerdem eine Meta-Optimierung der Strukturbeziehungen von Trainingsknoten zu Validierungsknoten ein, so dass die erlernte Fähigkeit zur Grapheninferenz besser an Testknoten angepasst werden kann. Umfassende Evaluierungen an vier Benchmark-Datensätzen (einschließlich Cora, Citeseer, Pubmed und NELL) zeigen die Überlegenheit unserer GIL im Vergleich zu anderen State-of-the-Art-Methoden bei der semi-supervised Knotenklassifizierung.
In diesem Beitrag wird eine Methode zum effizienten Training von Q-Funktionen für Markov-Entscheidungsprozesse (MDP) mit kontinuierlichen Zuständen vorgeschlagen, so dass die Spuren der resultierenden Politiken eine Eigenschaft der linearen temporalen Logik (LTL) erfüllen.LTL, eine modale Logik, kann eine breite Palette von zeitabhängigen logischen Eigenschaften, einschließlich Sicherheit und Liveness, ausdrücken.Wir konvertieren die LTL-Eigenschaft in einen limitierten deterministischen Buchi-Automaten, mit dem ein synchronisiertes Produkt-MDP konstruiert wird. Die vorgeschlagene Methode wird in einer numerischen Studie evaluiert, um die Qualität der generierten Kontrollpolitik zu testen, und wird mit konventionellen Methoden zur Politiksynthese wie MDP-Abstraktion (Voronoi Quantizer) und approximativer dynamischer Programmierung (Fit-Value-Iteration) verglichen.
Wir stellen zwei Ansätze zur Durchführung effizienter Bayes'scher Inferenz in stochastischen Simulatoren vor, die verschachtelte stochastische Teilprozeduren enthalten, d.h., Die sich daraus ergebende Klasse von Simulatoren wird in den Wissenschaften häufig verwendet und kann als probabilistisches generatives Modell interpretiert werden. Allerdings stellt das Ziehen von Schlussfolgerungen aus diesen Simulatoren eine große Herausforderung dar, da es nicht möglich ist, auch nur ihre unnormalisierte Dichte zu bewerten, was die Verwendung vieler Standard-Inferenzverfahren wie Markov Chain Monte Carlo (MCMC) verhindert. Um dieses Problem zu lösen, führen wir Inferenzalgorithmen ein, die auf einem zweistufigen Ansatz beruhen, der zunächst die bedingten Dichten der einzelnen Teilverfahren approximiert, bevor diese Approximationen verwendet werden, um MCMC-Methoden auf dem gesamten Programm auszuführen.da die Teilverfahren separat behandelt werden können und niedriger dimensioniert sind als das Gesamtproblem, ermöglicht dieser zweistufige Prozess, sie zu isolieren und somit gut handhabbar zu machen, ohne die Gesamtdimensionalität des Problems einzuschränken.wir demonstrieren den Nutzen unseres Ansatzes an einem einfachen, künstlich konstruierten Simulator.
Während adversariales Training die robuste Genauigkeit (gegen einen Gegner) verbessern kann, schadet es manchmal der Standardgenauigkeit (wenn es keinen Gegner gibt). frühere Arbeiten haben diesen Kompromiss zwischen Standard- und robuster Genauigkeit untersucht, aber nur in der Situation, in der kein Prädiktor beide Ziele im unendlichen Datenlimit gut erfüllt. Da unsere Konstruktion auf einem konvexen Lernproblem basiert, können wir Optimierungsaspekte ausschließen und so eine grundlegende Spannung zwischen Robustheit und Generalisierung aufdecken.Schließlich zeigen wir, dass robustes Selbsttraining diesen Kompromiss größtenteils eliminiert, indem es unbeschriftete Daten nutzt.
Skip-Verbindungen werden zunehmend von tiefen neuronalen Netzen genutzt, um die Genauigkeit und Kosteneffizienz zu verbessern. Insbesondere das neue DenseNet ist effizient in der Berechnung und bei den Parametern und erreicht hochmoderne Vorhersagen, indem es jede Merkmalsschicht direkt mit allen vorhergehenden verbindet. In dieser Arbeit wird erstmals experimentell gezeigt, dass ein wesentlicher Vorteil von Skip-Verbindungen darin besteht, dass die Abstände zwischen den Merkmalsebenen während der Backpropagation kurz sind, d. h., dass bei Verwendung einer festen Anzahl von Skip-Verbindungen die Verbindungsmuster mit kürzeren Backpropagationsabständen zwischen den Ebenen genauere Vorhersagen ermöglichen. Dieser Erkenntnis folgend schlagen wir eine Verbindungsvorlage, Log-DenseNet, vor, die im Vergleich zu DenseNet die Backpropagationsabstände zwischen den Schichten nur geringfügig von 1 auf ($1 + \log_2 L$) erhöht, aber nur $L\log_2 L$ Gesamtverbindungen anstelle von $O(L^2)$ verwendet. Daher sind \logdenses einfacher zu skalieren als DenseNets und erfordern keine sorgfältige GPU-Speicherverwaltung mehr. Wir demonstrieren die Effektivität unseres Designprinzips, indem wir eine bessere Leistung als DenseNets bei der semantischen Tabula-Rasa-Segmentierung und konkurrenzfähige Ergebnisse bei der visuellen Erkennung zeigen.
Der Aufbau von Agenten für die Interaktion mit dem Web würde erhebliche Verbesserungen im Wissensverständnis und Repräsentationslernen ermöglichen.Web-Navigationsaufgaben sind jedoch für aktuelle Deep Reinforcement Learning (RL)-Modelle aufgrund des großen diskreten Aktionsraums und der unterschiedlichen Anzahl von Aktionen zwischen den Zuständen schwierig.In dieser Arbeit stellen wir DOM-Q-NET vor, eine neuartige Architektur für RL-basierte Web-Navigation, um diese beiden Probleme anzugehen.Es parametrisiert Q-Funktionen mit separaten Netzwerken für verschiedene Aktionskategorien: Klicken auf ein DOM-Element und Eingabe einer Zeichenkette.  Unser Modell verwendet ein neuronales Graphen-Netzwerk, um die baumartige HTML-Struktur einer Standard-Webseite darzustellen.  Wir demonstrieren die Fähigkeiten unseres Modells in der MiniWoB-Umgebung, in der wir mit bestehenden Arbeiten mithalten oder sie sogar übertreffen können, ohne dass wir auf die Demonstration von Experten angewiesen sind, und wir zeigen eine zweifache Verbesserung der Effizienz beim Training in der Multi-Task-Umgebung, die es unserem Modell ermöglicht, gelernte Verhaltensweisen aufgabenübergreifend zu übertragen.
Kürzlich hat die Besorgnis über die Realisierbarkeit des Lernens von entflochtenen Repräsentationen in einer rein unbeaufsichtigten Art und Weise eine Verschiebung in Richtung der Einbeziehung von schwacher Überwachung angeregt. Um dieses Problem anzugehen, stellen wir einen theoretischen Rahmen - einschließlich eines Kalküls der Entflechtung - zur Verfügung, um die Entflechtungsgarantien (oder deren Fehlen) zu analysieren, die durch schwache Überwachung gewährt werden, wenn sie mit Lernalgorithmen gekoppelt sind, die auf Verteilungsanpassung basieren.Wir verifizieren empirisch die Garantien und Einschränkungen mehrerer schwacher Überwachungsmethoden (eingeschränkte Kennzeichnung, Match-Pairing und Rank-Pairing) und demonstrieren die Vorhersagekraft und Nützlichkeit unseres theoretischen Rahmens.
Trotz des wachsenden Interesses am kontinuierlichen Lernen wurden die meisten aktuellen Arbeiten in einem eher eingeschränkten Rahmen untersucht, in dem Aufgaben klar unterscheidbar sind und die Aufgabengrenzen während des Trainings bekannt sind. Wenn unser Ziel jedoch darin besteht, einen Algorithmus zu entwickeln, der so lernt wie der Mensch, ist dieser Rahmen alles andere als realistisch, und es ist von entscheidender Bedeutung, eine Methode zu entwickeln, die aufgabenunabhängig funktioniert. Unser Modell, genannt Continual Neural Dirichlet Process Mixture (CN-DPM), besteht aus einer Reihe von neuronalen Netzwerk-Experten, die für eine Teilmenge der Daten zuständig sind.CN-DPM erweitert die Anzahl der Experten auf prinzipielle Weise unter dem Bayes'schen nichtparametrischen Rahmen.Mit umfangreichen Experimenten zeigen wir, dass unser Modell erfolgreich aufgabenfreies kontinuierliches Lernen sowohl für diskriminative als auch generative Aufgaben wie Bildklassifikation und Bildgenerierung durchführt.
Der stochastische Gradientenabstieg (SGD) mit Nesterov-Momentum ist ein weit verbreitetes Optimierungsverfahren im Deep Learning, das eine hervorragende Generalisierungsleistung aufweist, aber aufgrund der großen Stochastizität ist SGD mit Nesterov-Momentum nicht robust, d.h., In dieser Arbeit schlagen wir das Amortisierte Nesterov-Momentum vor, eine spezielle Variante des Nesterov-Momentums, die robustere Iterate, schnellere Konvergenz in der Anfangsphase und höhere Effizienz aufweist.Unsere experimentellen Ergebnisse zeigen, dass dieses neue Momentum ähnliche (manchmal bessere) Generalisierungsleistungen mit wenig bis gar keinem Tuning erreicht.Im konvexen Fall liefern wir optimale Konvergenzraten für unsere neuen Methoden und diskutieren, wie die Theoreme die empirischen Ergebnisse erklären.
Ein generatives Modell auf dem neuesten Stand der Technik, ein "factorized action variational autoencoder (FAVAE)", wird für das Erlernen von entwirrten und interpretierbaren Repräsentationen aus sequentiellen Daten über den Informationsengpass ohne Überwachung vorgestellt. Wir haben uns auf die entwirrte Repräsentation von sequentiellen Daten konzentriert, weil es eine breite Palette von potenziellen Anwendungen gibt, wenn die entwirrte Repräsentation auf sequentielle Daten wie Video-, Sprach- und Börsenkursdaten ausgedehnt wird.Sequentielle Daten sind durch dynamische Faktoren und statische Faktoren gekennzeichnet: dynamische Faktoren sind zeitabhängig, und statische Faktoren sind zeitunabhängig. Bisherige Arbeiten haben es geschafft, statische Faktoren und dynamische Faktoren zu entflechten, indem sie explizit die Prioritäten latenter Variablen modelliert haben, um zwischen statischen und dynamischen Faktoren zu unterscheiden, aber dieses Modell kann die Repräsentationen zwischen dynamischen Faktoren nicht entflechten, wie z.B. die Entflechtung von "Greifen" und "Werfen" bei Roboteraufgaben. Da unsere Methode keine Modellierungsprioritäten erfordert, ist sie in der Lage, dynamische Faktoren "zwischen" dynamischen Faktoren zu entflechten. In Experimenten zeigen wir, dass FAVAE die entflechteten dynamischen Faktoren extrahieren kann.
Generative Netzwerke sind vielversprechende Modelle für die Spezifikation von visuellen Transformationen.Leider ist die Zertifizierung von generativen Modellen eine Herausforderung, da man genügend Nicht-Konvexität erfassen muss, um präzise Grenzen für die Ausgabe zu erzeugen.Bestehende Verifikationsmethoden sind entweder nicht auf generative Netzwerke skalierbar oder erfassen nicht genug Nicht-Konvexität. In dieser Arbeit stellen wir einen neuen Verifizierer namens ApproxLine vor, der nicht-triviale Eigenschaften von generativen Netzwerken zertifizieren kann.ApproxLine führt sowohl deterministische als auch probabilistische abstrakte Interpretationen durch und erfasst unendliche Mengen von Ausgaben generativer Netzwerke.Wir zeigen, dass ApproxLine interessante Interpolationen im latenten Raum des Netzwerks verifizieren kann.
Multi-View-Video-Zusammenfassung (MVS) fehlt die Aufmerksamkeit der Forscher aufgrund ihrer großen Herausforderungen der Inter-View-Korrelationen und Überschneidungen von Kameras.Die meisten der früheren MVS Arbeiten sind offline, die sich nur auf die Zusammenfassung, benötigen zusätzliche Kommunikationsbandbreite und Übertragungszeit mit keinem Fokus auf unsichere Umgebungen. Im Gegensatz zu den bestehenden Methoden schlagen wir eine auf Kantenintelligenz basierende MVS und eine auf räumlich-zeitlichen Merkmalen basierende Aktivitätserkennung für IoT-Umgebungen vor. Wir segmentieren die Multiview-Videos auf jedem Slave-Gerät über die Kante in Aufnahmen unter Verwendung eines leichtgewichtigen CNN-Objekterkennungsmodells und berechnen die gegenseitige Information zwischen ihnen, um eine Zusammenfassung zu erstellen. Unser System verlässt sich nicht nur auf die Zusammenfassung, sondern kodiert und überträgt sie an ein Master-Gerät mit einem neuronalen Rechenstab (NCS) zur intelligenten Berechnung von Inter-View-Korrelationen und zur effizienten Erkennung von Aktivitäten, wodurch Rechenressourcen, Kommunikationsbandbreite und Übertragungszeit eingespart werden.Experimente zeigen einen Anstieg von 0,4 in der F-Messung auf dem MVS-Office-Datensatz sowie 0. Die Zeitkomplexität wurde von 1,23 auf 0,45 Sekunden für die Verarbeitung eines Einzelbildes reduziert, wodurch die MVS um 0,75 Sekunden schneller wurde. Darüber hinaus haben wir einen neuen Datensatz erstellt, indem wir synthetisch Nebel zu einem MVS-Datensatz hinzugefügt haben, um die Anpassungsfähigkeit unseres Systems sowohl für sichere als auch für unsichere Überwachungsumgebungen zu zeigen.
Ein zentrales Ziel bei der Untersuchung des visuellen Kortex von Primaten und hierarchischer Modelle für die Objekterkennung ist es, zu verstehen, wie und warum einzelne Einheiten einen Kompromiss zwischen Invarianz und Empfindlichkeit gegenüber Bildtransformationen eingehen. Wir diskutieren die Konsequenzen dieser Beziehung für die KI: tiefe Netze gewichten natürlich invariante Einheiten gegenüber sensiblen Einheiten, und dies kann durch Training gestärkt werden, was möglicherweise zur Generalisierungsleistung beiträgt. Unsere Ergebnisse sagen eine Signaturbeziehung zwischen Invarianz und dynamischem Bereich voraus, die nun in zukünftigen neurophysiologischen Studien getestet werden kann.
Wir untersuchen die Verwendung der Wave-U-Net-Architektur für die Sprachanhebung, ein von Stoller et al. eingeführtes Modell für die Trennung von Musikgesang und -begleitung.  Diese End-to-End-Lernmethode für die Trennung von Audioquellen arbeitet direkt im Zeitbereich, ermöglicht die integrierte Modellierung von Phaseninformationen und ist in der Lage, große zeitliche Kontexte zu berücksichtigen.  Unsere Experimente zeigen, dass die vorgeschlagene Methode mehrere Metriken, nämlich PESQ, CSIG, CBAK, COVL und SSNR, gegenüber dem Stand der Technik in Bezug auf die Aufgabe der Sprachanhebung auf dem Voice Bank Corpus (VCTK) Datensatz verbessert. Wir stellen fest, dass eine reduzierte Anzahl von versteckten Schichten für die Sprachverbesserung im Vergleich zu dem ursprünglichen System, das für die Trennung von Gesangsstimmen in der Musik entwickelt wurde, ausreichend ist.
Verstärkungslernen (Reinforcement Learning, RL) ist ein leistungsfähiges Verfahren, um Probleme zu lösen, indem man Fehler erforscht und aus ihnen lernt. Im Kontext der autonomen Fahrzeugsteuerung (AV) kann es jedoch in der realen Welt sehr gefährlich und kostspielig sein, von einem Agenten zu verlangen, dass er Fehler macht, oder sogar Fehler zuzulassen. Da diese Simulationen unvollkommene Darstellungen haben, insbesondere in Bezug auf Grafik, Physik und menschliche Interaktion, finden wir Motivation für einen Rahmen ähnlich RL, geeignet für die reale Welt.Zu diesem Zweck formulieren wir einen Lernrahmen, der aus eingeschränkter Exploration lernt, indem ein menschlicher Demonstrator die Exploration durchführt.Bestehende Arbeiten zum Lernen aus Demonstration typischerweise entweder davon ausgegangen, dass die gesammelten Daten von einem optimalen Experten durchgeführt werden, oder erfordert potenziell gefährliche Exploration, um die optimale Politik zu finden. Eine unserer wichtigsten Erkenntnisse ist, dass das Problem überschaubar wird, wenn der Feedback-Score, der die Demonstration bewertet, für die einzelne Aktion gilt und nicht für die gesamte Sequenz von Aktionen. Wir verwenden menschliche Experten, um Fahrdaten zu sammeln und die Fahrdaten durch einen Rahmen zu kennzeichnen, den wir ``Backseat Driver'' nennen, wodurch wir Zustands-Aktions-Paare erhalten, die mit skalaren Werten verbunden sind, die den Score für die Aktion darstellen. Wir nennen den allgemeineren Lernrahmen ReNeg, da er eine Regression von Zuständen zu Handlungen erlernt, die sowohl durch negative als auch durch positive Beispiele gegeben sind.Wir validieren empirisch mehrere Modelle im ReNeg-Rahmen, indem wir das Spurhalten mit begrenzten Daten testen.Wir stellen fest, dass die beste Lösung in diesem Kontext das behaviorale Klonen übertrifft und starke Verbindungen zu stochastischen Policy-Gradient-Ansätzen hat.
Wir erforschen die Verwendung von vektorquantisierten Autoencoder-Modellen (VQ-VAE) für die Bilderzeugung in großem Maßstab. Zu diesem Zweck skalieren und verbessern wir die in VQ-VAE verwendeten autoregressiven Prioren, um synthetische Stichproben mit viel höherer Kohärenz und Genauigkeit als zuvor zu erzeugen.  Wir verwenden einfache Feed-Forward-Encoder- und Decoder-Netzwerke, so dass unser Modell ein attraktiver Kandidat für Anwendungen ist, bei denen die Kodierungs- und Dekodierungsgeschwindigkeit von entscheidender Bedeutung ist, was es uns außerdem ermöglicht, nur im komprimierten latenten Raum autoregressiv zu samplen, was um eine Größenordnung schneller ist als das Sampling im Pixelraum, insbesondere bei großen Bildern. Wir zeigen, dass eine mehrstufige hierarchische Organisation von VQ-VAE, ergänzt mit leistungsstarken Prioren über die latenten Codes, in der Lage ist, Stichproben mit einer Qualität zu generieren, die mit dem Stand der Technik von Generative Adversarial Networks auf facettenreichen Datensätzen wie ImageNet konkurriert, während sie nicht unter den bekannten Unzulänglichkeiten von GANs wie Mode-Kollaps und mangelnde Diversität leidet.
Wir stellen einen Ansatz für die Monte-Carlo-Baumsuche für das klassische Travelling-Salesman-Problem (TSP) vor, der auf einem Greedy-Algorithmus basiert, um die optimale Lösung für das TSP durch sukzessives Hinzufügen von Knoten zu finden.Ein neuronales Netzwerk (GNN) wird trainiert, um die lokale und globale Graphenstruktur zu erfassen und die Wahrscheinlichkeit für die Auswahl jedes Knotens in jedem Schritt zu bestimmen. Die vorherige Wahrscheinlichkeit liefert eine Heuristik für MCTS, und die MCTS-Ausgabe ist eine verbesserte Wahrscheinlichkeit für die Auswahl des nächsten Knotens, da sie die Rückkopplungsinformation durch die Verschmelzung der vorherigen mit der Scouting-Prozedur ist.Experimentelle Ergebnisse auf TSP bis zu 100 Knoten zeigen, dass die vorgeschlagene Methode kürzere Touren als andere lernbasierte Methoden erhält.
Diese Modelle repräsentieren ein Molekül als einen Graphen, indem sie nur den Abstand zwischen den Atomen (Knoten) und nicht die räumliche Richtung von einem Atom zum anderen verwenden.Richtungsinformationen spielen jedoch eine zentrale Rolle in empirischen Potentialen für Moleküle, z.B. in Winkelpotentialen.Um diese Einschränkung zu mildern, schlagen wir direktionale Nachrichtenübermittlung vor, in der wir die Nachrichten zwischen den Atomen anstelle der Atome selbst einbetten.Jede Nachricht ist mit einer Richtung im Koordinatenraum verbunden. Diese direktionalen Nachrichteneinbettungen sind rotationsäquivariant, da die zugehörigen Richtungen mit dem Molekül rotieren. wir schlagen ein Nachrichtenübermittlungsschema vor, das analog zur Glaubensausbreitung ist und die direktionalen Informationen nutzt, indem es Nachrichten basierend auf dem Winkel zwischen ihnen transformiert. Darüber hinaus verwenden wir sphärische Bessel-Funktionen, um eine theoretisch gut begründete, orthogonale radiale Basis zu konstruieren, die eine bessere Leistung als die derzeit vorherrschenden radialen Gauß'schen Basisfunktionen erzielt, während sie mehr als viermal weniger Parameter verwendet.Wir nutzen diese Innovationen, um das direktionale neuronale Nachrichtenübermittlungsnetz (DimeNet) zu konstruieren.DimeNet übertrifft frühere GNNs im Durchschnitt um 77% bei MD17 und um 41% bei QM9.
Das Training eines Agenten zur Lösung von Steuerungsaufgaben direkt aus hochdimensionalen Bildern mit modellfreiem Reinforcement Learning (RL) hat sich als schwierig erwiesen.Der Agent muss eine latente Repräsentation zusammen mit einer Steuerungspolitik lernen, um die Aufgabe zu erfüllen.Das Anpassen eines Encoders mit hoher Kapazität unter Verwendung eines knappen Belohnungssignals ist nicht nur extrem ineffizient, sondern auch anfällig für suboptimale Konvergenz.Zwei Möglichkeiten zur Verbesserung der Sammeleffizienz sind das Lernen einer guten Merkmalsrepräsentation und die Verwendung von Off-Policy-Algorithmen. Wir analysieren verschiedene Ansätze zum Erlernen guter latenter Merkmale und kommen zu dem Schluss, dass der Bildrekonstruktionsverlust der wesentliche Bestandteil ist, der effizientes und stabiles Repräsentationslernen im bildbasierten RL ermöglicht. In Anlehnung an diese Erkenntnisse entwickeln wir einen Off-Policy-Actor-Critic-Algorithmus mit einem Hilfsdecoder, der Ende-zu-Ende trainiert und sowohl bei modellfreien als auch bei modellbasierten Algorithmen bei vielen anspruchsvollen Kontrollaufgaben dem Stand der Technik entspricht.
 Wir präsentieren einen neuartigen Operator für neuronale Netze, chopout, mit dem neuronale Netze trainiert werden, sogar in einem einzigen Trainingsprozess, so dass abgeschnittene Subnetze die bestmögliche Leistung erbringen.Chopout ist einfach zu implementieren und in die meisten Arten von bestehenden neuronalen Netzen zu integrieren.Darüber hinaus ermöglicht es, die Größe von Netzen und latenten Repräsentationen auch nach dem Training zu reduzieren, indem einfach Schichten abgeschnitten werden.Wir zeigen seine Wirksamkeit durch mehrere Experimente.
Generative Adversarial Networks (GANs) haben gezeigt, dass sie mit bemerkenswertem Erfolg realistisch aussehende synthetische Bilder erzeugen können, aber ihre Leistung scheint weniger beeindruckend zu sein, wenn der Trainingssatz sehr vielfältig ist.Um eine bessere Anpassung an die Zieldatenverteilung zu erreichen, wenn der Datensatz viele verschiedene Klassen enthält, schlagen wir eine Variante des grundlegenden GAN-Modells vor, ein Multi-Modal Gaussian-Mixture GAN (GM-GAN), bei dem die Wahrscheinlichkeitsverteilung über den latenten Raum eine Mischung von Gaußschen ist. Um die Leistung des Modells zu bewerten, schlagen wir eine neue Bewertungsmethode vor, die zwei (in der Regel widersprüchliche) Maße - Diversität und Qualität der generierten Daten - separat berücksichtigt.  Durch eine Reihe von Experimenten, die sowohl synthetische als auch reale Datensätze verwenden, zeigen wir quantitativ, dass GM-GANs besser abschneiden als die Grundmodelle, sowohl bei der Bewertung mit dem allgemein verwendeten Inception Score als auch bei der Bewertung mit unserer eigenen alternativen Bewertungsmethode. Wir zeigen, wie dieses Phänomen für die Aufgabe des unüberwachten Clusterns genutzt werden kann, und liefern eine quantitative Auswertung, die die Überlegenheit unserer Methode für das unüberwachte Clustern von Bilddatensätzen zeigt. Schließlich demonstrieren wir ein Merkmal, das unser Modell weiter von anderen GAN-Modellen abhebt: die Möglichkeit, den Kompromiss zwischen Qualität und Diversität zu kontrollieren, indem man nach dem Training die Wahrscheinlichkeitsverteilung des latenten Raums ändert, so dass man je nach Bedarf Stichproben mit höherer Qualität und geringerer Diversität oder umgekehrt nehmen kann.
Mehrere Ansätze wurden vorgeschlagen, um den Kommunikations-Overhead beim verteilten Training zu reduzieren, z. B. die Synchronisierung erst nach Durchführung mehrerer lokaler SGD-Schritte und dezentrale Methoden (z. B, Obwohl diese Methoden schneller laufen als AllReduce-basierte Methoden, die blockierende Kommunikation vor jeder Aktualisierung verwenden, können die resultierenden Modelle nach der gleichen Anzahl von Aktualisierungen weniger genau sein.Inspiriert von der BMUF-Methode von Chen & Huo (2016), schlagen wir ein Slow-Momentum (SloMo)-Framework vor, bei dem Arbeiter nach mehreren Iterationen eines Basis-Optimierungsalgorithmus periodisch synchronisieren und ein Momentum-Update durchführen. Experimente zur Bildklassifikation und maschinellen Übersetzung zeigen, dass SloMo im Vergleich zum Basisoptimierer durchgängig Verbesserungen in der Optimierungs- und Verallgemeinerungsleistung liefert, selbst wenn der zusätzliche Overhead über viele Aktualisierungen amortisiert wird, so dass die SloMo-Laufzeit mit der des Basisoptimierers vergleichbar ist.Wir bieten theoretische Konvergenzgarantien, die zeigen, dass SloMo zu einem stationären Punkt mit glatten nicht-konvexen Verlusten konvergiert.Da BMUF eine besondere Instanz des SloMo-Rahmens ist, entsprechen unsere Ergebnisse auch den ersten theoretischen Konvergenzgarantien für BMUF.
In dieser Arbeit fügen wir eine Planungsphase in die neuronale maschinelle Übersetzung ein, um die grobe Struktur der Ausgabesätze zu kontrollieren. Das Modell generiert zunächst einige Planer-Codes und sagt dann die tatsächlichen Ausgabewörter voraus, die von diesen Codes abhängen.Die Codes werden gelernt, um die grobe Struktur des Zielsatzes zu erfassen. Um die Codes zu lernen, entwerfen wir ein neuronales End-to-End-Netzwerk mit einem Diskretisierungsengpass, der die vereinfachten Part-of-Speech-Tags von Zielsätzen vorhersagt.Experimente zeigen, dass die Übersetzungsleistung im Allgemeinen durch Vorausplanung verbessert wird.Wir finden auch, dass Übersetzungen mit unterschiedlichen Strukturen durch Manipulation der Planer-Codes erhalten werden können.
Die Interpolation von Daten in tiefen neuronalen Netzen ist zu einem Thema von großem Forschungsinteresse geworden.  Wir beweisen, dass überparametrisierte, einschichtige, voll verbundene Autokoder nicht nur interpolieren, sondern Trainingsdaten speichern: Sie produzieren Ausgaben in der Spanne der Trainingsbeispiele (eine nicht-lineare Version davon).Im Gegensatz zu voll verbundenen Autokodern beweisen wir, dass Tiefe für die Speicherung in Faltungsautokodern notwendig ist.  Darüber hinaus stellen wir fest, dass das Hinzufügen von Nichtlinearität zu tiefen Faltungs-Autoencodern zu einer stärkeren Form der Memorisierung führt: Anstatt Punkte in der Spannweite der Trainingsbilder auszugeben, neigen tiefe Faltungs-Autoencoder dazu, einzelne Trainingsbilder auszugeben.  Da Faltungs-Autoencoder-Komponenten Bausteine von tiefen Faltungsnetzwerken sind, stellen wir uns vor, dass unsere Ergebnisse Licht auf die wichtige Frage der induktiven Verzerrung in überparametrisierten tiefen Netzwerken werfen werden.
Der enorme Erfolg von tiefen neuronalen Netzen hat dazu geführt, dass die grundlegenden Eigenschaften dieser Netze besser verstanden werden müssen, aber viele der vorgeschlagenen theoretischen Ergebnisse gelten nur für flache Netze. \in \mathbb{R}^{k \mal n}$ die innerste Gewichtsmatrix eines beliebigen neuronalen Feed-Forward-Netzwerks $M sein: \mathbb{R}^n \zu \mathbb{R}$, so dass $M(x)$ geschrieben werden kann als $M(x) = \sigma(\mathbf{A} x)$, für irgendein Netzwerk $\sigma: \mathbb{R}^k \zu \mathbb{R}$.Das Ziel ist dann, die Zeilenspanne von $\mathbf{A}$ wiederherzustellen, wenn man nur Orakelzugriff auf den Wert von $M(x)$ hat. Wir zeigen, dass, wenn $M$ ein mehrschichtiges Netz mit ReLU-Aktivierungsfunktionen ist, eine partielle Wiederherstellung möglich ist: nämlich, dass wir $k/2$ linear unabhängige Vektoren in der Zeilenspanne von $\mathbf{A}$ mit poly$(n)$ nicht-adaptiven Abfragen an $M(x)$ nachweislich wiederherstellen können.  Für den Fall, dass $M$ differenzierbare Aktivierungsfunktionen hat, zeigen wir außerdem, dass die Wiederherstellung der vollen Spanne auch dann möglich ist, wenn die Ausgabe zuerst durch eine Schwellenwertfunktion mit Vorzeichen oder $0/1$ geleitet wird; in diesem Fall ist unser Algorithmus adaptiv.Empirisch bestätigen wir, dass die Wiederherstellung der vollen Spanne nicht immer möglich ist, sondern nur für unrealistisch dünne Schichten. Darüber hinaus demonstrieren wir die Nützlichkeit von Span Recovery als Angriff, indem wir neuronale Netze dazu bringen, Daten, die durch kontrolliertes Zufallsrauschen verschleiert sind, als sensible Eingaben zu klassifizieren. 
In diesem Papier untersuchen wir die implizite Regularisierung des Gradientenabstiegsalgorithmus in homogenen neuronalen Netzen, einschließlich voll vernetzter und faltiger neuronaler Netze mit ReLU- oder LeakyReLU-Aktivierungen, insbesondere den Gradientenabstieg oder Gradientenfluss (d.h., Wir untersuchen insbesondere den Gradientenabstieg oder den Gradientenfluss (d.h. den Gradientenabstieg mit infinitesimaler Schrittweite) zur Optimierung des logistischen Verlusts oder des Cross-Entropie-Verlusts eines beliebigen homogenen (möglicherweise nicht-glatten) Modells und zeigen, dass wir, wenn der Trainingsverlust unter einen bestimmten Schwellenwert fällt, eine geglättete Version der normalisierten Marge definieren können, die im Laufe der Zeit zunimmt.Wir formulieren auch ein natürliches eingeschränktes Optimierungsproblem, das mit der Margenmaximierung zusammenhängt, und beweisen, dass sowohl die normalisierte Marge als auch ihre geglättete Version an einem KKT-Punkt des Optimierungsproblems zum Zielwert konvergieren. Unsere Ergebnisse verallgemeinern die bisherigen Ergebnisse für logistische Regression mit ein- oder mehrschichtigen linearen Netzen und liefern quantitativere Konvergenzergebnisse mit schwächeren Annahmen als die bisherigen Ergebnisse für homogene glatte neuronale Netze.
Standard rekurrente Modelle sind ineffektiv, da sie anfällig für Fehlerfortpflanzung sind und Korrelationen höherer Ordnung nicht effektiv erfassen können. Eine mögliche Lösung ist die Erweiterung auf räumlich-zeitliche rekurrente Modelle höherer Ordnung. Ein solches Modell erfordert jedoch eine große Anzahl von Parametern und Operationen, wodurch es in der Praxis schwer zu erlernen ist und anfällig für Überanpassung ist. In dieser Arbeit schlagen wir Convolutional Tensor-Train LSTM (Conv-TT-LSTM), die höherer Ordnung lerntConvolutional LSTM (ConvLSTM) effizient mit Convolutional Tensor-Train Decomposition (CTTD).Unser vorgeschlagenes Modell natürlich umfasst höherer Ordnung räumlich-zeitliche Informationen zu einem geringen Kosten für Speicher und Berechnung durch die Verwendung von effizienten Low-Rank Tensor Darstellungen. Wir evaluieren unser Modell an Moving-MNIST- und KTH-Datensätzen und zeigen Verbesserungen im Vergleich zu Standard-ConvLSTM und bessere/vergleichbare Ergebnisse zu anderen ConvLSTM-basierten Ansätzen, jedoch mit viel weniger Parametern.
Während tiefe neuronale Netze hervorragende Ergebnisse in einer Vielzahl von Anwendungen gezeigt haben, ist das Lernen aus einer sehr begrenzten Anzahl von Beispielen immer noch eine herausfordernde Aufgabe.Trotz der Schwierigkeiten des Lernens mit wenigen Beispielen haben metrische Lerntechniken das Potenzial der neuronalen Netze für diese Aufgabe gezeigt.Obwohl diese Methoden gut funktionieren, liefern sie keine zufriedenstellenden Ergebnisse.In dieser Arbeit wird die Idee des metrischen Lernens mit dem Arbeitsmechanismus von Support Vector Machines (SVM) erweitert, der für seine Generalisierungsfähigkeiten auf einem kleinen Datensatz bekannt ist. Darüber hinaus wird in dieser Arbeit ein durchgängiger Lernrahmen für die Ausbildung von adaptiven Kernel-SVMs vorgestellt, der das Problem der Auswahl eines korrekten Kerns und guter Merkmale für SVMs beseitigt.Als nächstes wird das One-Shot-Lernproblem für Audiosignale neu definiert. Der Algorithmus, der den Omniglot-Datensatz verwendet, verbesserte die Genauigkeit von 98,1 % auf 98,5 % bei der One-Shot-Klassifizierung und von 98,9 % auf 99,3 % bei der Klassifizierung von wenigen Shots.
Die meisten existierenden 3D CNN-Strukturen für das Lernen von Videodarstellungen sind Clip-basierte Methoden und berücksichtigen nicht die zeitliche Entwicklung der räumlich-zeitlichen Merkmale auf Video-Ebene. In diesem Papier schlagen wir 4D Convolutional Neural Networks auf Video-Ebene, nämlich V4D, vor, um die Entwicklung der räumlich-zeitlichen Darstellung mit 4D-Faltungen zu modellieren und die räumlich-zeitlichen 3D-Darstellungen mit Restverbindungen zu erhalten. Wir stellen außerdem die Trainings- und Inferenzmethoden für das vorgeschlagene V4D vor. Ausführliche Experimente werden mit drei Videoerkennungs-Benchmarks durchgeführt, bei denen V4D hervorragende Ergebnisse erzielt und die aktuellen 3D-CNNs weit übertrifft.
Wir untersuchen das Problem des Lernens und Optimierens durch physikalische Simulationen mittels differenzierbarer Programmierung.Wir stellen DiffSim vor, eine neue differenzierbare Programmiersprache, die für den Aufbau von leistungsstarken differenzierbaren physikalischen Simulationen zugeschnitten ist.Wir demonstrieren die Leistung und Produktivität unserer Sprache in gradientenbasierten Lern- und Optimierungsaufgaben auf 10 verschiedenen physikalischen Simulatoren.Zum Beispiel ist ein differenzierbarer elastischer Objektsimulator, der in unserer Sprache geschrieben wurde, 4. Ein differenzierbarer elastischer Objektsimulator, der in unserer Sprache geschrieben wurde, ist beispielsweise 4,6 Mal schneller als die von Hand entwickelte CUDA-Version und läuft genauso schnell und 188 Mal schneller als TensorFlow.Mit unseren differenzierbaren Programmen werden neuronale Netzwerk-Controller typischerweise innerhalb von nur zehn Iterationen optimiert.Schließlich teilen wir die Lektionen, die wir aus unserer Erfahrung bei der Entwicklung dieser Simulatoren gelernt haben, nämlich dass die Differenzierung von physikalischen Simulatoren nicht immer nützliche Gradienten des zu simulierenden physikalischen Systems liefert.Wir untersuchen systematisch die zugrundeliegenden Gründe und schlagen Lösungen zur Verbesserung der Gradientenqualität vor.
Bestehende Techniken sind oft auf einen bestimmten Typ von Prädiktoren beschränkt oder basieren auf der Auffälligkeit von Eingaben, die unerwünscht empfindlich auf Faktoren reagieren können, die nichts mit dem Entscheidungsprozess des Modells zu tun haben.Wir schlagen stattdessen ausreichende Eingabemengen vor, die minimale Untermengen von Merkmalen identifizieren, deren beobachtete Werte allein ausreichen, um dieselbe Entscheidung zu treffen, selbst wenn alle anderen Eingabewerte fehlen. Unser Ansatz ist konzeptionell unkompliziert, völlig modellunabhängig, einfach zu implementieren durch instanzielle Rückwärtsselektion und in der Lage, prägnantere Begründungen zu liefern als existierende Techniken.Wir demonstrieren den Nutzen unserer Interpretationsmethode an neuronalen Netzwerkmodellen, die auf Text- und Bilddaten trainiert wurden.
In diesem Papier befassen wir uns mit dem Problem der Erkennung von Stichproben, die nicht aus der Trainingsverteilung gezogen wurden, d.h., Viele frühere Studien haben versucht, dieses Problem zu lösen, indem sie Proben mit geringer Klassifizierungszuverlässigkeit als OOD-Beispiele unter Verwendung von tiefen neuronalen Netzen (DNNs) betrachtet haben. Bei schwierigen Datensätzen oder Modellen mit geringer Klassifizierungsfähigkeit betrachten diese Methoden jedoch fälschlicherweise Proben in der Verteilung nahe der Entscheidungsgrenze als OOD-Proben. In Experimenten übertrifft unsere Methode die bestehenden Methoden um ein Vielfaches und erreicht die beste Erkennungsleistung auf verschiedenen Datensätzen und Klassifizierungsmodellen. Zum Beispiel erhöht unsere Methode den AUROC-Wert früherer Arbeiten (83,8%) auf 99,8% in DenseNet auf den CIFAR-100- und Tiny-ImageNet-Datensätzen.
Wir untersuchen Modelle, die in der Lage sind, die Attribute mehrerer Eingaben so zu kombinieren, dass eine resynthetisierte Ausgabe trainiert wird, um einen gegnerischen Diskriminator für reale und synthetische Daten zu täuschen. Darüber hinaus untersuchen wir die Verwendung einer solchen Architektur im Kontext des halbüberwachten Lernens, bei dem wir eine Mischfunktion erlernen, deren Ziel es ist, Interpolationen versteckter Zustände oder maskierte Kombinationen latenter Repräsentationen zu erzeugen, die mit einem konditionierten Klassenlabel übereinstimmen.Wir zeigen quantitative und qualitative Beweise dafür, dass eine solche Formulierung ein interessanter Weg der Forschung ist.
In dieser Arbeit analysieren wir die zeitliche Inkonsistenz von Streaming-Wireless-Signalen im Kontext der gerätefreien passiven Lokalisierung in Innenräumen. Wir zeigen, dass Daten aus WiFi Channel State Information (CSI) verwendet werden können, um ein robustes System zu trainieren, das in der Lage ist, eine Lokalisierung auf Raumebene durchzuführen. Eine der größten Herausforderungen für ein solches System ist die Verschiebung der Eingangsdatenverteilung in einen unerforschten Raum im Laufe der Zeit, was zu einer unerwünschten Verschiebung in den gelernten Grenzen des Ausgangsraums führt.In dieser Arbeit schlagen wir einen phasen- und magnitudenerweiterten Merkmalsraum zusammen mit einer Standardisierungstechnik vor, die nur wenig von Drifts betroffen ist.Wir zeigen, dass diese robuste Darstellung der Daten eine bessere Lerngenauigkeit ergibt und eine geringere Anzahl von Umschulungen erfordert.
Föderiertes Lernen verbessert den Datenschutz und die Effizienz beim maschinellen Lernen in Netzwerken verteilter Geräte, wie z. B. Mobiltelefone, IoT- und tragbare Geräte usw. Dennoch können Modelle, die mit föderiertem Lernen trainiert wurden, aufgrund des Problems der Domänenverschiebung nicht auf neue Geräte verallgemeinert werden: Die Domänenverschiebung tritt auf, wenn sich die von den Quellknoten gesammelten beschrifteten Daten statistisch von den unbeschrifteten Daten des Zielknotens unterscheiden. In dieser Arbeit stellen wir einen prinzipiellen Ansatz für das Problem der föderierten Domänenanpassung vor, der darauf abzielt, die Repräsentationen, die zwischen den verschiedenen Knoten gelernt wurden, mit der Datenverteilung des Zielknotens abzugleichen.Unser Ansatz erweitert adversarische Anpassungstechniken auf die Einschränkungen der föderierten Einstellung.Darüber hinaus entwickeln wir einen dynamischen Aufmerksamkeitsmechanismus und nutzen die Entflechtung von Merkmalen, um den Wissenstransfer zu verbessern.Empirisch führen wir umfangreiche Experimente an verschiedenen Bild- und Textklassifizierungsaufgaben durch und zeigen vielversprechende Ergebnisse unter unbeaufsichtigter föderierter Domänenanpassung Einstellung.
Wir beschreiben eine Version von Kapseln, in der jede Kapsel eine logistische Einheit hat, um das Vorhandensein einer Entität zu repräsentieren und eine 4x4-Matrix, die lernen könnte, die Beziehung zwischen dieser Entität und dem Betrachter (die Pose) zu repräsentieren. Eine Kapsel in einer Schicht stimmt für die Pose-Matrix vieler verschiedener Kapseln in der darüber liegenden Schicht ab, indem sie ihre eigene Pose-Matrix mit trainierbaren sichtpunktinvarianten Transformationsmatrizen multipliziert, die lernen können, Teil-Ganzes-Beziehungen darzustellen. Diese Koeffizienten werden iterativ für jedes Bild mit Hilfe des Erwartungs-Maximierungs-Algorithmus aktualisiert, so dass die Ausgabe jeder Kapsel an eine Kapsel in der darüber liegenden Schicht weitergeleitet wird, die eine Gruppe ähnlicher Stimmen erhält. Die Transformationsmatrizen werden diskriminativ trainiert, indem sie durch die unrollierten Iterationen von EM zwischen jedem Paar benachbarter Kapselschichten zurückverfolgt werden. Bei der kleinenNORB-Benchmark reduzieren Kapseln die Anzahl der Testfehler um 45\% im Vergleich zum Stand der Technik.
Sowohl die logische Spezifikation als auch die Grammatik haben komplexe Strukturen und können von Aufgabe zu Aufgabe variieren, was das Lernen über verschiedene Aufgaben hinweg vor erhebliche Herausforderungen stellt.Darüber hinaus sind Trainingsdaten für domänenspezifische Syntheseaufgaben oft nicht verfügbar.Um diese Herausforderungen zu bewältigen, schlagen wir einen Meta-Lernrahmen vor, der eine übertragbare Strategie aus nur schwacher Überwachung lernt. Unser Framework besteht aus drei Komponenten:1) einem Encoder, der sowohl die logische Spezifikation als auch die Grammatik gleichzeitig mit Hilfe eines neuronalen Graphen-Netzwerks einbettet;2) einem grammatikalisch adaptiven Policy-Netzwerk, das das Lernen einer übertragbaren Policy ermöglicht; und3) einem Reinforcement-Learning-Algorithmus, der die Einbettung und die adaptive Policy gemeinsam trainiert.Wir evaluieren das Framework anhand von 214 kryptographischen Schaltungssyntheseaufgaben. Das Ergebnis ist vergleichbar mit zwei klassischen Synthese-Engines auf dem neuesten Stand der Technik, die 129 bzw. 153 Aufgaben lösen. In der Meta-Solver-Einstellung kann sich das Framework effizient an ungesehene Aufgaben anpassen und erreicht eine Beschleunigung von 2x bis 100x.
Simultane maschinelle Übersetzungsmodelle beginnen mit der Generierung einer Zielsequenz, bevor sie die Quellsequenz kodiert oder gelesen haben. Jüngste Ansätze für diese Aufgabe wenden entweder eine feste Politik auf den Transformator oder eine lernfähige monotone Aufmerksamkeit auf eine schwächere rekurrente neuronale Netzwerkstruktur an. Wir wenden MMA auf die Aufgabe der maschinellen Simultanübersetzung an und zeigen einen besseren Kompromiss zwischen Latenz und Qualität im Vergleich zu MILk, dem bisherigen State-of-the-Art-Ansatz.code will be released upon publication.
Dieses Papier stellt einen neuartigen zweistufigen Ansatz für das grundlegende Problem des Lernens eine optimale Karte von einer Verteilung zu einem anderen.Erstens lernen wir einen optimalen Transport (OT) Plan, der als eine ein-zu-viele Karte zwischen den beiden Verteilungen gedacht werden kann.Zu diesem Zweck schlagen wir eine stochastische duale Ansatz der regulierten OT, und zeigen empirisch, dass es besser als eine neuere verwandte Ansatz skaliert, wenn die Menge der Proben ist sehr groß. Zweitens schätzen wir eine Monge-Karte als ein tiefes neuronales Netz, das durch Annäherung an die baryzentrische Projektion des zuvor erhaltenen OT-Plans gelernt wird.Diese Parametrisierung ermöglicht die Verallgemeinerung der Abbildung außerhalb der Unterstützung des Eingabemaßes.Wir beweisen zwei theoretische Stabilitätsergebnisse der regularisierten OT, die zeigen, dass unsere Schätzungen zur OT- und Monge-Karte zwischen den zugrundeliegenden kontinuierlichen Maßen konvergieren.Wir stellen unseren vorgeschlagenen Ansatz in zwei Anwendungen vor: Domänenanpassung und generative Modellierung.
Während die berühmte Word2Vec-Technik semantisch reichhaltige Wortrepräsentationen liefert, ist es weniger klar, ob Satz- oder Dokumentrepräsentationen auf Wortrepräsentationen oder von Grund auf neu aufgebaut werden sollten.Jüngste Arbeiten haben gezeigt, dass ein Distanzmaß zwischen Dokumenten namens \emph{Word Mover's Distance} (WMD), das semantisch ähnliche Wörter abgleicht, eine beispiellose KNN-Klassifizierungsgenauigkeit liefert.WMD ist jedoch sehr teuer in der Berechnung und schwieriger anzuwenden als einfache KNN als Merkmalseinbettungen.In diesem Papier schlagen wir die \emph{Word Mover's Embedding } (Unsere Technik erweitert die Theorie der \emph{Random Features}, um die Konvergenz des inneren Produkts zwischen WMEs zu einem positiv-definiten Kernel zu zeigen, der als eine weiche Version der (inversen) WMD interpretiert werden kann. Die vorgeschlagene Einbettung ist in vielen Situationen effizienter und flexibler als WMD. Beispielsweise reduziert WME mit einem einfachen linearen Klassifikator die Rechenkosten von WMD-basiertem KNN \emph{von kubisch auf linear} in der Dokumentenlänge und \emph{von quadratisch auf linear} in der Anzahl der Stichproben, während gleichzeitig die Genauigkeit verbessert wird.In Experimenten mit 9 Benchmark-Textklassifizierungsdatensätzen und 22 Textähnlichkeitsaufgaben entspricht die vorgeschlagene Technik durchgängig dem Stand der Technik oder übertrifft diese, mit deutlich höherer Genauigkeit bei Problemen von kurzer Länge.
Wir stellen einen neuen Ansatz zur Bewertung der Robustheit neuronaler Netze vor, der auf der Schätzung des Anteils der Eingaben basiert, für die eine Eigenschaft verletzt wird, insbesondere schätzen wir die Wahrscheinlichkeit des Ereignisses, dass die Eigenschaft unter einem Eingabemodell verletzt wird.Unser Ansatz unterscheidet sich kritisch von dem formalen Verifikationsrahmen, da er, wenn die Eigenschaft verletzt werden kann, einen informativen Begriff davon liefert, wie robust das Netz ist, und nicht nur die konventionelle Behauptung, dass das Netz nicht verifizierbar ist. Der Schlüssel zum praktischen Erfolg unseres Ansatzes ist die Anpassung des Multi-Level-Splittings, eines Monte-Carlo-Ansatzes zur Schätzung der Wahrscheinlichkeit seltener Ereignisse, an unseren statistischen Robustheitsrahmen. wir zeigen, dass unser Ansatz in der Lage ist, formale Verifikationsverfahren auf Benchmark-Problemen zu emulieren, während er auf größere Netzwerke skaliert und zuverlässige zusätzliche Informationen in Form von genauen Schätzungen der Verletzungswahrscheinlichkeit liefert.
Neuere vortrainierte transformatorbasierte Sprachmodelle haben in verschiedenen NLP-Datensätzen Spitzenleistungen erbracht, aber trotz ihrer großen Fortschritte leiden sie unter verschiedenen strukturellen und syntaktischen Verzerrungen.In dieser Arbeit untersuchen wir die Verzerrung durch lexikalische Überschneidungen, z.B., Um die Robustheit zu verbessern, reichern wir die Eingabesätze der Trainingsdaten mit ihren automatisch erkannten Prädikat-Argument-Strukturen an. Diese erweiterte Darstellung ermöglicht es den transformatorbasierten Modellen, verschiedene Aufmerksamkeitsmuster zu lernen, indem sie sich auf die wichtigsten semantisch und syntaktisch wichtigen Teile der Sätze konzentrieren und diese erkennen.  Wir evaluieren unsere Lösung für die Aufgaben der natürlichsprachlichen Inferenz und der fundierten Commonsense-Inferenz unter Verwendung der Modelle BERT, RoBERTa und XLNET.Wir evaluieren das Verständnis der Modelle für syntaktische Variationen, Antonym-Relationen und benannte Entitäten bei lexikalischer Überlappung.Unsere Ergebnisse zeigen, dass die Einbeziehung von Prädikat-Argument-Strukturen während der Feinabstimmung die Robustheit erheblich verbessert, z.B. um etwa 20pp bei der Unterscheidung verschiedener benannter Entitäten, während es keine zusätzlichen Kosten bei der Testzeit verursacht und keine Änderung des Modells oder des Trainingsverfahrens erfordert.
Wir zeigen, dass jedes Ziel als eine Stichprobe aus einer Dirichlet-Verteilung modelliert werden kann, wobei die Parameter der Dirichlet-Verteilung durch die Ausgabe eines neuronalen Netzes bereitgestellt werden, und dass das kombinierte Modell unter Verwendung des Gradienten der Datenwahrscheinlichkeit trainiert werden kann. Dieser Ansatz liefert interpretierbare Vorhersagen in Form von mehrdimensionalen Verteilungen anstelle von Punktschätzungen, aus denen man Konfidenzintervalle erhalten oder das Risiko bei der Entscheidungsfindung quantifizieren kann.Darüber hinaus zeigen wir, dass derselbe Ansatz verwendet werden kann, um Ziele in Form von empirischen Zählungen als Stichproben aus der Dirichlet-Multinomial-Verbundverteilung zu modellieren.In Experimenten überprüfen wir, dass unser Ansatz diese Vorteile bietet, ohne die Leistung der Punktschätzungsvorhersagen bei zwei verschiedenen Anwendungen zu beeinträchtigen: (1) Destillation von tiefen Faltungsnetzen, die auf CIFAR-100 trainiert wurden, und (2) Vorhersage des Ortes von Teilchenkollisionen im XENON1T-Detektor für Dunkle Materie.
Das DenseNet, eine der kürzlich vorgeschlagenen neuronalen Netzwerkarchitekturen, hat die modernste Leistung in vielen visuellen Aufgaben erreicht, weist jedoch aufgrund der dichten Verbindungen der internen Struktur eine große Redundanz auf, was zu hohen Rechenkosten beim Training solcher dichten Netzwerke führt. Um dieses Problem anzugehen, entwerfen wir ein Reinforcement Learning Framework, um nach effizienten DenseNet-Architekturen mit schichtweisem Pruning (LWP) für verschiedene Aufgaben zu suchen, während die ursprünglichen Vorteile von DenseNet, wie die Wiederverwendung von Merkmalen, kurze Pfade usw., erhalten bleiben. In diesem Rahmen bewertet ein Agent die Wichtigkeit jeder Verbindung zwischen zwei Blockschichten und entfernt die überflüssigen Verbindungen. Zusätzlich wird ein neuartiger Reward-Shaping-Trick eingeführt, mit dem DenseNet einen besseren Kompromiss zwischen Genauigkeit und Float-Point-Operationen (FLOPs) erreicht.Unsere Experimente zeigen, dass DenseNet mit LWP kompakter und effizienter ist als bestehende Alternativen.   
Wir zeigen, dass Modelle, die für die Vorhersage propriozeptiver Informationen über den Körper des Agenten trainiert wurden, Objekte in der Außenwelt repräsentieren. Obwohl sie nur mit intern verfügbaren Signalen trainiert wurden, repräsentieren diese dynamischen Körpermodelle externe Objekte durch die Notwendigkeit, ihre Auswirkungen auf den eigenen Körper des Agenten vorherzusagen, d.h. das Modell lernt ganzheitliche, dauerhafte Repräsentationen von Objekten in der Welt, obwohl die einzigen Trainingssignale Körpersignale sind. Unser Dynamikmodell ist in der Lage, erfolgreich Verteilungen über 132 Sensormesswerte über 100 Schritte in die Zukunft vorherzusagen, und wir zeigen, dass die latenten Variablen des Dynamikmodells auch dann noch die Form eines Objekts repräsentieren, wenn der Körper nicht mehr in Kontakt mit diesem ist. Wir zeigen, dass die aktive Datenerfassung durch Maximierung der Entropie von Vorhersagen über den Körper - Berührungssensoren, Propriozeption und vestibuläre Informationen - zum Erlernen von dynamischen Modellen führt, die bei der Steuerung eine überlegene Leistung zeigen.Wir sammeln auch Daten von einer echten Roboterhand und zeigen, dass dieselben Modelle verwendet werden können, um Fragen über Eigenschaften von Objekten in der realen Welt zu beantworten.Videos mit qualitativen Ergebnissen unserer Modelle sind unter https://goo.gl/mZuqAV verfügbar.
Inspiriert durch den Erfolg von generativen adversen Netzwerken (GANs) in Bildbereichen, stellen wir eine neuartige hierarchische Architektur zum Lernen charakteristischer topologischer Merkmale aus einem einzigen beliebigen Eingabegraphen mittels GANs vor.die hierarchische Architektur, die aus mehreren GANs besteht, bewahrt sowohl lokale als auch globale topologische Merkmale und unterteilt den Eingabegraphen automatisch in repräsentative Stufen zum Lernen von Merkmalen. Die Stadien erleichtern die Rekonstruktion und können als Indikatoren für die Bedeutung der zugehörigen topologischen Strukturen verwendet werden.Experimente zeigen, dass unsere Methode Subgraphen erzeugt, die eine breite Palette von topologischen Merkmalen beibehalten, sogar in frühen Rekonstruktionsstadien.Dieses Papier enthält Original-Forschung über die Kombination der Verwendung von GANs und Graph topologische Analyse.
In der chinesischen Gesellschaft hat der Aberglaube einen hohen Stellenwert, und Kfz-Kennzeichen mit begehrten Nummern können bei Auktionen sehr hohe Preise erzielen. Im Gegensatz zu anderen wertvollen Gegenständen wird für Kfz-Kennzeichen vor der Auktion kein Schätzpreis festgelegt. Ich schlage vor, dass die Aufgabe der Vorhersage von Kennzeichenpreisen als eine Aufgabe der natürlichen Sprachverarbeitung (NLP) angesehen werden kann, da der Wert von der Bedeutung jedes einzelnen Zeichens auf dem Kennzeichen und seiner Semantik abhängt.Ich konstruiere ein tiefes rekurrentes neuronales Netzwerk (RNN), um die Preise von Kfz-Kennzeichen in Hongkong auf der Grundlage der Zeichen auf einem Kennzeichen vorherzusagen. Anhand von 13 Jahren historischer Auktionspreise wird gezeigt, dass die Vorhersagen des tiefen RNN über 80 Prozent der Preisschwankungen erklären können und damit frühere Modelle deutlich übertreffen.  Ich zeige auch, wie das Modell erweitert werden kann, um eine Suchmaschine für Platten zu werden und um Schätzungen der erwarteten Preisverteilung zu liefern.
Wir stellen ein neues latentes Modell natürlicher Bilder vor, das auf großen Datensätzen erlernt werden kann: Der Lernprozess liefert eine latente Einbettung für jedes Bild im Trainingsdatensatz sowie ein tiefes Faltungsnetzwerk, das den latenten Raum auf den Bildraum abbildet. Nach dem Training bietet das neue Modell eine starke und universelle Bildvorhersage für eine Vielzahl von Bildwiederherstellungsaufgaben, wie z. B. Inpainting mit großen Löchern, Superresolution und Kolorierung. Um hochauflösende natürliche Bilder zu modellieren, verwendet unser Ansatz latente Räume mit sehr hoher Dimensionalität (ein bis zwei Größenordnungen höher als frühere latente Bildmodelle). Um diese hohe Dimensionalität zu bewältigen, verwenden wir latente Räume mit einer speziellen Mannigfaltigkeitsstruktur (Faltungsmannigfaltigkeiten), die durch ein ConvNet einer bestimmten Architektur parametrisiert sind. In den Experimenten vergleichen wir die gelernten latenten Modelle mit latenten Modellen, die von Autocodierern, fortgeschrittenen Varianten generativer adversarialer Netze und einem starken Basissystem, das eine einfachere Parametrisierung des latenten Raums verwendet, gelernt wurden, und unser Modell übertrifft die konkurrierenden Ansätze in einer Reihe von Restaurierungsaufgaben.
Die meisten neueren AES-Lösungen verwenden auf tiefen neuronalen Netzen (DNN) basierende Modelle mit Regression, bei denen der auf neuronalen Netzen basierende Kodierer eine Aufsatzdarstellung lernt, die bei der Unterscheidung zwischen den Aufsätzen hilft, und die entsprechende Aufsatzbewertung durch einen Regressor abgeleitet wird. Ein solcher DNN-Ansatz erfordert in der Regel viele von Experten bewertete Aufsätze als Trainingsdaten, um eine gute Aufsatzrepräsentation für eine genaue Bewertung zu erlernen, was jedoch in der Regel teuer ist und daher nur wenige Daten zur Verfügung stehen. Inspiriert von der Beobachtung, dass Menschen einen Aufsatz in der Regel bewerten, indem sie ihn mit einigen Referenzen vergleichen, schlagen wir ein siamesisches Framework namens Referee Network (RefNet) vor, das es dem Modell ermöglicht, die Qualität von zwei Aufsätzen zu vergleichen, indem es die relativen Merkmale erfasst, die das Aufsatzpaar unterscheiden können. Das vorgeschlagene Framework kann als Erweiterung von Regressionsmodellen eingesetzt werden, da es zusätzlich zu den internen Informationen weitere relative Merkmale erfassen kann und somit ideal für den Umgang mit spärlichen Daten geeignet ist.Experimente zeigen, dass unser Framework die bestehenden Regressionsmodelle deutlich verbessern und eine akzeptable Leistung erzielen kann, selbst wenn die Trainingsdaten stark reduziert sind.
Eine der grundlegenden Aufgaben im Verständnis der Genomik ist das Problem der Vorhersage von Transkriptionsfaktor-Bindungsstellen (TFBSs).Mit mehr als Hunderten von Transkriptionsfaktoren (TFs) als Etiketten, Genom-Sequenz-basierte TFBS Vorhersage ist eine anspruchsvolle Multi-Label-Klassifizierung task.There sind zwei wichtige biologische Mechanismen für TF-Bindung: (1) sequenzspezifische Bindungsmuster auf Genomen, die als "Motive" bekannt sind, und (2) Interaktionen zwischen TFs, die als Co-Bindungseffekte bekannt sind. In diesem Papier schlagen wir eine neuartige tiefe Architektur vor, das Prototype Matching Network (PMN), um die TF-Bindungsmechanismen nachzuahmen. Unser PMN-Modell extrahiert automatisch Prototypen ("Motiv"-ähnliche Merkmale) für jeden TF durch einen neuartigen Prototyp-Matching-Verlust. In Anlehnung an die Ideen von "few-shot" Matching-Modellen verwenden wir den Begriff des Support-Sets von Prototypen und ein LSTM, um zu lernen, wie TFs interagieren und an genomische Sequenzen binden. Auf einem TFBS-Referenzdatensatz mit 2,1 Millionen genomischen Sequenzen übertrifft PMN die Grundlinien signifikant und validiert unsere Designentscheidungen empirisch.Unseres Wissens nach ist dies die erste Deep-Learning-Architektur, die Prototyp-Lernen einführt und TF-TF-Interaktionen für die Vorhersage von TFBS in großem Maßstab berücksichtigt.Die vorgeschlagene Architektur ist nicht nur genau, sondern modelliert auch die zugrunde liegende Biologie.
Frühere Arbeiten haben gezeigt, dass eine adversarisch robuste Generalisierung eine größere Probenkomplexität und denselben Datensatz erfordert, z. B., Da das Sammeln neuer Trainingsdaten kostspielig sein könnte, konzentrieren wir uns darauf, die gegebenen Daten besser zu nutzen, indem wir Regionen mit hoher Probendichte im Merkmalsraum induzieren, was zu lokal ausreichenden Proben für robustes Lernen führen könnte.Wir zeigen zunächst formal, dass der Softmax-Cross-Entropie-Verlust (SCE) und seine Varianten unangemessene Überwachungssignale vermitteln, die die gelernten Merkmalspunkte dazu ermutigen, sich beim Training spärlich über den Raum zu verteilen. Der MMC-Verlust ermutigt das Modell, sich auf das Erlernen geordneter und kompakter Repräsentationen zu konzentrieren, die sich um die voreingestellten optimalen Zentren für verschiedene Klassen gruppieren. Wir zeigen empirisch, dass die Anwendung des MMC-Verlusts die Robustheit selbst bei starken adaptiven Angriffen erheblich verbessern kann, während die Genauigkeit auf sauberen Eingaben mit geringem Mehraufwand im Vergleich zum SCE-Verlust auf dem neuesten Stand bleibt.
Wir stellen einen analytischen Rahmen vor, um GANs auf der Ebene von Einheiten, Objekten und Szenen zu visualisieren und zu verstehen. Wir identifizieren zunächst eine Gruppe interpretierbarer Einheiten, die eng mit Objektkonzepten verbunden sind, mit einer segmentierungsbasierten Netzwerkzerlegungsmethode. Schließlich untersuchen wir die kontextuelle Beziehung zwischen diesen Einheiten und ihrer Umgebung, indem wir die entdeckten Objektkonzepte in neue Bilder einfügen. Wir zeigen mehrere praktische Anwendungen, die durch unseren Rahmen ermöglicht werden, vom Vergleich interner Repräsentationen über verschiedene Schichten und Modelle bis hin zur Verbesserung von GANs durch das Auffinden und Entfernen von Artefakt verursachenden Einheiten und zur interaktiven Manipulation von Objekten in der Szene.
Wir stellen einen einfachen NN-Ansatz (Nearest Neighbor) vor, der hochfrequente fotorealistische Bilder aus einem ``unvollständigen'' Signal wie einem niedrig aufgelösten Bild, einer Oberflächennormalen-Karte oder Kanten synthetisiert.Aktuelle, dem Stand der Technik entsprechende tiefe generative Modelle, die für eine solche bedingte Bildsynthese entwickelt wurden, haben zwei wichtige Mängel: (1) sie sind nicht in der Lage, eine große Menge von verschiedenen Ausgaben zu generieren, aufgrund der Mode-Kollaps-Problem.(2) sie sind nicht interpretierbar, so dass es schwierig ist, die synthetisierte Ausgabe zu kontrollieren.Wir zeigen, dass NN-Ansätze potenziell solche Einschränkungen zu adressieren, aber leiden in der Genauigkeit auf kleine Datensätze. Wir entwerfen eine einfache Pipeline, die das Beste aus beiden Welten kombiniert: In der ersten Phase wird ein Faltungsneuronales Netzwerk (CNN) verwendet, um die Eingabe auf ein (übermäßig geglättetes) Bild abzubilden, und in der zweiten Phase wird eine pixelweise Methode der nächsten Nachbarschaft verwendet, um die geglättete Ausgabe auf mehrere hochwertige, hochfrequente Ausgaben in einer kontrollierbaren Weise abzubilden.Wichtig ist, dass der pixelweise Abgleich es unserer Methode ermöglicht, neue hochfrequente Inhalte durch Ausschneiden und Einfügen von Pixeln aus verschiedenen Trainingsbeispielen zusammenzustellen.  Wir demonstrieren unseren Ansatz für verschiedene Eingabemodalitäten und für verschiedene Bereiche wie menschliche Gesichter, Haustiere, Schuhe und Handtaschen.
Neuronale Netze sind anfällig für ungünstige Beispiele und Forscher haben viele heuristische Angriffs- und Verteidigungsmechanismen vorgeschlagen.  Für glatte Verluste erreicht unser Verfahren nachweislich ein moderates Maß an Robustheit mit geringen rechnerischen oder statistischen Kosten im Vergleich zur empirischen Risikominimierung, und unsere statistischen Garantien ermöglichen es uns, die Robustheit für den Populationsverlust effizient zu zertifizieren. für nicht wahrnehmbare Störungen entspricht unsere Methode heuristischen Ansätzen oder übertrifft diese.
Die Qualität der posterioren Inferenz wird weitgehend durch zwei Faktoren bestimmt: a) die Fähigkeit der Variationsverteilung, das wahre Posterior zu modellieren, und b) die Fähigkeit des Erkennungsnetzwerks, die Inferenz über alle Datenpunkte zu verallgemeinern.Wir analysieren die approximative Inferenz in Variations-Auto-Codern im Hinblick auf diese Faktoren. Wir zeigen, dass die suboptimale Inferenz oft auf die Amortisierung der Inferenz zurückzuführen ist und nicht auf die begrenzte Komplexität der approximierenden Verteilung. Wir zeigen, dass dies zum Teil darauf zurückzuführen ist, dass der Generator lernt, sich an die Wahl der Approximation anzupassen, und dass die Parameter, die verwendet werden, um die Aussagekraft der Approximation zu erhöhen, eine Rolle bei der Verallgemeinerung der Inferenz spielen und nicht nur die Komplexität der Approximation verbessern.
Um semi-supervised Modelle zu nutzen, müssen wir zunächst automatisch Beschriftungen, sogenannte Pseudo-Labels, generieren. Wir stellen fest, dass frühere Ansätze zur Generierung von Pseudo-Labels die Clustering-Leistung aufgrund ihrer geringen Genauigkeit beeinträchtigen. Stattdessen verwenden wir ein Ensemble von Deep Networks, um einen Ähnlichkeitsgraphen zu erstellen, aus dem wir Pseudo-Labels mit hoher Genauigkeit extrahieren. Wir zeigen, dass unser Ansatz die Ergebnisse des State-of-the-Art-Clustering für mehrere Bild- und Textdatensätze übertrifft. So erreichen wir beispielsweise eine Genauigkeit von 54,6 % für CIFAR-10 und 43,9 % für 20news und übertreffen damit den State-of-the-Art in absoluten Zahlen um 8-12 %.
Diese Arbeit befasst sich mit dem Lernen von Wörterbüchern, d.h., Wir zeigen, dass ein subgradienter Abstiegsalgorithmus mit zufälliger Initialisierung orthogonale Wörterbücher auf einer natürlichen, nicht-glatten, nicht-konvexen L1-Minimierungsformulierung des Problems wiederherstellen kann, und zwar unter milden statistischen Annahmen bezüglich der Daten. Unsere Analyse entwickelt mehrere Werkzeuge zur Charakterisierung von Landschaften nicht-glatter Funktionen, die von unabhängigem Interesse für das beweisbare Training von tiefen Netzen mit nicht-glatten Aktivierungen (z.B. ReLU) sein könnten, Vorläufige synthetische und reale Experimente bestätigen unsere Analyse und zeigen, dass unser Algorithmus empirisch gut funktioniert, um orthogonale Wörterbücher wiederherzustellen.
Wir untersuchen die Modellwiederherstellung für die Datenklassifizierung, wobei die Trainingsetiketten von einem einschichtigen, vollständig verbundenen neuronalen Netz mit sigmoiden Aktivierungen generiert werden und das Ziel darin besteht, die Gewichtsvektoren des neuronalen Netzes wiederherzustellen.wir beweisen, dass die empirische Risikofunktion unter Verwendung der Kreuzentropie bei Gauß'schen Eingaben starke Konvexität und Glätte in einer lokalen Nachbarschaft der Grundwahrheit aufweist, sobald die Probenkomplexität ausreichend groß ist. Dies impliziert, dass der Gradientenabstieg linear zu einem kritischen Punkt konvergiert, der nachweislich nahe an der Grundwahrheit liegt, wenn er in dieser Nachbarschaft initialisiert wird, was durch die Tensormethode erreicht werden kann, ohne dass bei jeder Iteration ein neuer Satz von Stichproben erforderlich ist.Soweit wir wissen, ist dies die erste globale Konvergenzgarantie, die für die empirische Risikominimierung unter Verwendung von Kreuzentropie über den Gradientenabstieg für das Lernen von einschichtigen neuronalen Netzen bei nahezu optimaler Stichproben- und Rechenkomplexität in Bezug auf die Eingangsdimension des Netzes aufgestellt wurde.
Mit dem Einsatz von neuronalen Netzen auf mobilen Geräten und der Notwendigkeit, neuronale Netze über begrenzte oder teure Kanäle zu übertragen, wurde die Dateigröße des trainierten Modells als Engpass identifiziert. wir schlagen einen Codec für die Komprimierung von neuronalen Netzen vor, der auf Transformationskodierung für Faltungsschichten und dichte Schichten und auf Clustering für Biases und Normalisierungen basiert. mit diesem Codec erreichen wir durchschnittliche Komprimierungsfaktoren zwischen 7,9 und 9,3, während die Genauigkeit der komprimierten Netze für die Bildklassifizierung nur um jeweils 1 % bis 2 % abnimmt.
Eine pragmatische Lösung bieten vertrauenswürdige Ausführungsumgebungen (Trusted Execution Environments, TEEs), die mit Hilfe von Hardware- und Softwareschutz sensible Berechnungen vom nicht vertrauenswürdigen Software-Stack isolieren. In dieser Arbeit wird die Ausführung von Deep Neural Networks (DNNs) in TEEs untersucht, indem die DNN-Berechnungen effizient zwischen vertrauenswürdigen und nicht vertrauenswürdigen Geräten aufgeteilt werden.1 Aufbauend auf einem effizienten Outsourcing-Schema für die Matrixmultiplikation schlagen wir Slalom vor, ein Framework, das die Ausführung aller linearen Schichten in einem DNN sicher von einem TEE (z.B. Intel SGX oder Sanctum) delegiert, Wir evaluieren Slalom, indem wir DNNs in einer Intel SGX Enklave laufen lassen, die selektiv Arbeit an eine nicht vertrauenswürdige GPU delegiert. Für kanonische DNNs (VGG16-, MobileNet- und ResNet-Varianten) erhalten wir eine 6- bis 20-fache Steigerung des Durchsatzes für verifizierbare Inferenz und eine 4- bis 11-fache Steigerung für verifizierbare und private Inferenz.
Tiefe neuronale Netze sind zwar eine sehr erfolgreiche Modellklasse, doch ihr großer Speicherbedarf belastet den Energieverbrauch, die Kommunikationsbandbreite und die Speicheranforderungen erheblich, so dass die Reduzierung der Modellgröße zu einem der wichtigsten Ziele beim tiefen Lernen geworden ist. Ein typischer Ansatz besteht darin, einen Satz deterministischer Gewichte zu trainieren und dabei bestimmte Techniken wie Pruning und Quantisierung anzuwenden, damit die empirische Gewichtsverteilung für Kodierungsschemata nach Shannon geeignet ist. Wie in diesem Papier gezeigt wird, ermöglicht die Lockerung des Gewichtsdeterminismus und die Verwendung einer vollständigen Variationsverteilung über die Gewichte jedoch effizientere Kodierungsschemata und folglich höhere Kompressionsraten. In Anlehnung an das klassische Bits-Back-Argument kodieren wir die Netzwerkgewichte mit Hilfe einer Zufallsstichprobe und benötigen nur eine Anzahl von Bits, die der Kullback-Leibler-Divergenz zwischen der Variationsverteilung der Stichprobe und der Kodierungsverteilung entspricht. Es kann gezeigt werden, dass das verwendete Kodierungsschema nahe an der optimalen informationstheoretischen unteren Schranke liegt, in Bezug auf die verwendete Variationsfamilie, und dass unsere Methode einen neuen Stand der Technik in der Kompression neuronaler Netze setzt, da sie frühere Ansätze im Pareto-Sinn deutlich dominiert: Bei den Benchmarks LeNet-5/MNIST und VGG-16/CIFAR-10 liefert unser Ansatz die beste Testleistung für ein festes Speicherbudget, und umgekehrt erreicht er die höchsten Kompressionsraten für eine feste Testleistung.
Die meisten bestehenden neuronalen Netze zum Erlernen von Graphen behandeln das Problem der Permutationsinvarianz, indem sie das Netz als ein Nachrichtenübermittlungsschema konzipieren, bei dem jeder Knoten die von seinen Nachbarn stammenden Merkmalsvektoren summiert. Wir argumentieren, dass dies eine Beschränkung ihrer Darstellungsleistung darstellt, und schlagen stattdessen eine neue allgemeine Architektur zur Darstellung von Objekten vor, die aus einer Hierarchie von Teilen besteht, die wir kovariante kompositionelle Netze (CCNs) nennen. Wir erreichen die Kovarianz, indem wir jede Aktivierung entsprechend einer Tensordarstellung der Permutationsgruppe transformieren und die entsprechenden Tensor-Aggregationsregeln ableiten, die jedes Neuron implementieren muss.Experimente zeigen, dass CCNs konkurrierende Methoden bei einigen Standard-Graph-Lernbenchmarks übertreffen können.
 In den letzten Jahren sind dreidimensionale Faltungsneuronale Netze (3D CNN) intensiv in der Videoanalyse und Handlungserkennung angewandt und erhält eine gute Leistung. 3D CNN führt jedoch zu massiven Berechnungen und Speicherverbrauch, die ihren Einsatz auf mobilen und eingebetteten Geräten behindert.In diesem Papier schlagen wir eine dreidimensionale Regularisierung-basierte Pruning-Methode, um verschiedene Regularisierungsparameter zu verschiedenen Gewichtsgruppen auf der Grundlage ihrer Bedeutung für das Netzwerk zuweisen.Unsere Experimente zeigen, dass die vorgeschlagene Methode übertrifft andere beliebte Methoden in diesem Bereich.
In diesem Beitrag schlagen wir Datenerklärungen als Designlösung und professionelle Praxis für Technologen der natürlichen Sprachverarbeitung vor, sowohl in der Forschung als auch in der Entwicklung - durch die Annahme und den weit verbreiteten Einsatz von Datenerklärungen kann das Feld beginnen, kritische wissenschaftliche und ethische Fragen anzugehen, die sich aus der Verwendung von Daten aus bestimmten Bevölkerungsgruppen bei der Entwicklung von Technologien für andere Bevölkerungsgruppen ergeben.Wir stellen eine Form vor, die Datenerklärungen annehmen können, und untersuchen die Auswirkungen ihrer Annahme als Teil der regelmäßigen Praxis. Wir argumentieren, dass Datenerklärungen dazu beitragen werden, Probleme im Zusammenhang mit Ausgrenzung und Voreingenommenheit in der Sprachtechnologie zu lindern; sie werden zu präziseren Behauptungen darüber führen, wie NLP-Forschung verallgemeinert werden kann, und damit zu besseren technischen Ergebnissen; sie werden Unternehmen vor öffentlichen Peinlichkeiten schützen; und sie werden schließlich zu einer Sprachtechnologie führen, die ihre Benutzer in ihrem eigenen bevorzugten linguistischen Stil trifft und sie darüber hinaus nicht falsch gegenüber anderen darstellt.** Erscheint in TACL **
Wir stellen CGNN vor, ein Framework zum Erlernen funktionaler Kausalmodelle als generative neuronale Netze, die mittels Backpropagation trainiert werden, um die maximale mittlere Diskrepanz zu den beobachteten Daten zu minimieren. Im Gegensatz zu früheren Ansätzen nutzt CGNN sowohl bedingte Abhängigkeiten als auch Verteilungsasymmetrien, um nahtlos bivariate und multivariate Kausalstrukturen zu entdecken, mit oder ohne versteckte Variablen. CGNN schätzt nicht nur die kausale Struktur, sondern auch ein vollständiges und differenzierbares generatives Modell der Daten. Durch eine Vielzahl von Experimenten illustrieren wir die konkurrenzfähigen Ergebnisse von CGNN im Vergleich zu hochmodernen Alternativen bei der kausalen Entdeckung von Beobachtungen sowohl auf simulierten als auch auf realen Daten in den Bereichen Ursache-Wirkungs-Inferenz, Identifikation von V-Strukturen und multivariater kausaler Entdeckung.
Ein typischer Pruning-Algorithmus besteht aus einer dreistufigen Pipeline, d.h., In dieser Arbeit machen wir eine eher überraschende Beobachtung: Die Feinabstimmung eines beschnittenen Modells führt nur zu einer vergleichbaren oder sogar schlechteren Leistung als das Training dieses Modells mit zufällig initialisierten Gewichten. Unsere Ergebnisse haben mehrere Implikationen:1) das Training eines großen, überparametrisierten Modells ist nicht notwendig, um ein effizientes Endmodell zu erhalten,2) gelernte "wichtige" Gewichte des großen Modells sind nicht notwendigerweise für das kleine beschnittene Modell nützlich,3) die beschnittene Architektur selbst, und nicht ein Satz von vererbten Gewichten, ist das, was zu dem Effizienzvorteil im Endmodell führt, was darauf hindeutet, dass einige Beschneidungsalgorithmen als Suche nach Netzwerkarchitekturen angesehen werden könnten.
Leider ist es im Allgemeinen nicht trivial, k-means zu erweitern, um Datenpunkte jenseits der Gauß-Verteilung zu clustern, insbesondere die Cluster mit nicht-konvexen Formen (Beliakov & King, 2006). Zu diesem Zweck führen wir zum ersten Mal die Extremwerttheorie (EVT) ein, um die Clustering-Fähigkeit von k-means zu verbessern. Wir schlagen daher einen neuartigen Algorithmus vor, der Extreme Value k-means (EV k-means) genannt wird, einschließlich GEV k-means und GPD k-means.Darüber hinaus führen wir auch Tricks ein, um die Berechnung des euklidischen Abstands zu beschleunigen, um die Recheneffizienz von klassischem k-means zu verbessern.Außerdem wird unser EV k-means zu einer Online-Version erweitert, d.h., Umfassende Experimente werden durchgeführt, um unsere EV k-means und online EV k-means auf synthetischen und realen Datensätzen zu validieren. Die experimentellen Ergebnisse zeigen, dass unsere Algorithmen in den meisten Fällen die Konkurrenz deutlich übertreffen.
Deep Reinforcement Learning Algorithmen haben sich in einer Vielzahl von Domänen als erfolgreich erwiesen, jedoch bleiben Aufgaben mit spärlichen Belohnungen eine Herausforderung, wenn der Zustandsraum groß ist.Zielorientierte Aufgaben gehören zu den typischsten Problemen in dieser Domäne, bei denen eine Belohnung nur erhalten werden kann, wenn das endgültige Ziel erreicht wurde.In dieser Arbeit schlagen wir eine mögliche Lösung für solche Probleme mit der Einführung eines erfahrungsbasierten Tendenz-Belohnungs-Mechanismus vor, der den Agenten mit zusätzlichen Hinweisen versorgt, die auf einem diskriminierenden Lernen auf vergangenen Erfahrungen während eines automatisierten Reverse-Curriculums basieren. Dieser Mechanismus liefert nicht nur dichte zusätzliche Lernsignale darüber, welche Zustände zum Erfolg führen, sondern erlaubt es dem Agenten auch, nur diese Tendenzbelohnung anstelle der gesamten Erfahrungsgeschichte während des mehrphasigen Lehrplanlernens zu behalten. Wir untersuchen ausführlich die Vorteile unserer Methode in Standarddomänen mit spärlichen Belohnungen wie Maze und Super Mario Bros. und zeigen, dass unsere Methode bei Aufgaben mit langen Zeithorizonten und großem Zustandsraum effizienter und robuster ist als frühere Ansätze. Darüber hinaus zeigen wir, dass unser Ansatz unter Verwendung eines optionalen Keyframe-Schemas mit einer sehr geringen Anzahl von Schlüsselzuständen schwierige Robotermanipulationsaufgaben direkt aus der Wahrnehmung und spärlichen Belohnungen lösen kann.
Die menschliche Szenenwahrnehmung geht über das Erkennen einer Sammlung von Objekten und ihrer paarweisen Beziehungen hinaus. Wir verstehen übergeordnete, abstrakte Regelmäßigkeiten innerhalb der Szene wie Symmetrie und Wiederholung. Experimente zeigen, dass unser Modell gut auf synthetischen Daten funktioniert und sich auf reale Bilder mit einer solchen kompositorischen Struktur übertragen lässt.Die Verwendung von Szenenprogrammen hat eine Reihe von Anwendungen ermöglicht, wie z.B. komplexe visuelle Analogiebildung und Szenenextrapolation.
Moderne neuronale Netze erfordern oft tiefe Kompositionen von hochdimensionalen nichtlinearen Funktionen (breite Architektur), um eine hohe Testgenauigkeit zu erreichen, und können daher eine überwältigende Anzahl von Parametern haben. Wir führen einen effizienten Mechanismus ein, die umgestaltete Tensorzerlegung, um neuronale Netze zu komprimieren, indem wir drei Arten von invarianten Strukturen ausnutzen: Periodizität, Modulation und niedriger Rang. Unsere Methode der umgestalteten Tensorzerlegung nutzt solche invarianten Strukturen mit einer Technik, die Tensorisierung genannt wird (Umgestaltung der Schichten in Tensoren höherer Ordnung), kombiniert mit Tensorzerlegungen höherer Ordnung auf den tensorisierten Schichten. Experimente mit LeNet-5 (MNIST), ResNet-32 (CI-FAR10) und ResNet-50 (ImageNet) zeigen, dass unsere umgestaltete Tensorzerlegung bei gleicher Kompressionsrate die hochmodernen Approximationstechniken mit niedrigem Rang übertrifft (Verbesserung der Testgenauigkeit um 5 % durchgängig bei CIFAR10) und zudem um Größenordnungen schnellere Konvergenzraten erzielt.
Mit dem zunehmenden Einsatz von Deep-Learning-Methoden in sicherheitskritischen Szenarien ist die Interpretierbarkeit wichtiger denn je. Obwohl viele verschiedene Richtungen in Bezug auf die Interpretierbarkeit für visuelle Modalitäten erforscht wurden, wurden Zeitseriendaten vernachlässigt und nur eine Handvoll Methoden aufgrund ihrer schlechten Verständlichkeit getestet. Wir nähern uns dem Problem der Interpretierbarkeit auf eine neuartige Art und Weise, indem wir TSInsight vorschlagen, bei dem wir einen Auto-Encoder mit einer sparsamkeitsinduzierenden Norm auf seiner Ausgabe an den Klassifikator anhängen und ihn auf der Grundlage der Gradienten des Klassifikators und einer Rekonstruktionsstrafe feinabstimmen.Der Auto-Encoder lernt, Merkmale zu erhalten, die für die Vorhersage durch den Klassifikator wichtig sind, und unterdrückt diejenigen, die irrelevant sind, d. h. er dient als Merkmalszuordnung. Mit anderen Worten, wir fordern das Netzwerk auf, nur die Teile zu rekonstruieren, die für den Klassifikator nützlich sind, d.h. die mit der Vorhersage korreliert oder kausal sind.Im Gegensatz zu den meisten anderen Attributionsverfahren ist TSInsight in der Lage, sowohl instanzbasierte als auch modellbasierte Erklärungen zu erzeugen. Wir haben TSInsight zusammen mit anderen gebräuchlichen Attributionsmethoden auf einer Reihe von verschiedenen Zeitseriendatensätzen evaluiert, um seine Wirksamkeit zu validieren, und wir haben die Eigenschaften analysiert, die TSInsight von vornherein erreicht, einschließlich der Robustheit gegenüber Angriffen und der Kontraktion des Ausgaberaums.Die erzielten Ergebnisse sprechen dafür, dass TSInsight ein effektives Werkzeug für die Interpretierbarkeit von tiefen Zeitserienmodellen sein kann.
Varianzreduktionsmethoden, die eine Mischung aus großen und kleinen Batch-Gradienten verwenden, wie SVRG (Johnson & Zhang, 2013) und SpiderBoost (Wang et al., 2018), benötigen deutlich mehr Rechenressourcen pro Update als SGD (Robbins & Monro, 1951).Wir reduzieren die Rechenkosten pro Update von Varianzreduktionsmethoden durch die Einführung eines Sparse-Gradienten-Operators, der den Top-K-Operator (Stich et al, 2018; Aji & Heafield, 2017) und dem randomisierten Koordinatenabstiegsoperator.Während die Rechenkosten für die Berechnung der Ableitung eines Modellparameters konstant sind, machen wir die Beobachtung, dass die Gewinne bei der Varianzreduktion proportional zur Größe der Ableitung sind.In diesem Papier zeigen wir, dass ein spärlicher Gradient, der auf der Größe vergangener Gradienten basiert, die Rechenkosten für Modellaktualisierungen ohne einen signifikanten Verlust bei der Varianzreduktion reduziert.Theoretisch ist unser Algorithmus mindestens so gut wie der beste verfügbare Algorithmus (z. B. SpiderBoost). Theoretisch ist unser Algorithmus mindestens so gut wie der beste verfügbare Algorithmus (z. B. SpiderBoost) unter geeigneten Parametereinstellungen und kann viel effizienter sein, wenn es unserem Algorithmus gelingt, die Sparsamkeit der Gradienten zu erfassen.Empirisch übertrifft unser Algorithmus durchgehend SpiderBoost unter Verwendung verschiedener Modelle, um verschiedene Bildklassifizierungsaufgaben zu lösen.Wir liefern auch empirische Beweise, um die Intuition hinter unserem Algorithmus durch eine einfache Gradientenentropie-Berechnung zu unterstützen, die dazu dient, die Gradienten-Sparsamkeit bei jeder Iteration zu quantifizieren.
Trotz vielversprechender Fortschritte bei der Imputation von unimodalen Daten (z.B. Image Inpainting) sind die Modelle für die Imputation von multimodalen Daten bei weitem nicht zufriedenstellend.In dieser Arbeit schlagen wir einen Variational Selective Autoencoder (VSAE) für diese Aufgabe vor. VSAE lernt nur von teilweise beobachteten Daten und kann die gemeinsame Verteilung von beobachteten/unbeobachteten Modalitäten und die Imputationsmaske modellieren, was zu einem vereinheitlichten Modell für verschiedene nachgelagerte Aufgaben einschließlich Datengenerierung und Imputation führt.
In vielen Bereichen, insbesondere bei der Analyse von Unternehmenstexten, gibt es eine Fülle von Daten, die für die Entwicklung neuer KI-gestützter intelligenter Erfahrungen genutzt werden können, um die Produktivität der Menschen zu verbessern. Allerdings gibt es starke Datenschutzgarantien, die eine breite Probenahme und Kennzeichnung persönlicher Textdaten zum Erlernen oder Evaluieren von Modellen von Interesse verhindern. In diesem Beitrag untersuchen wir die Herausforderungen bei der Übertragung von Informationen von einem E-Mail-Datensatz auf einen anderen, um die Absicht des Nutzers vorherzusagen. Insbesondere stellen wir Ansätze zur Charakterisierung der Übertragungslücke in Textkorpora sowohl aus intrinsischer als auch aus extrinsischer Sicht vor und bewerten mehrere in der Literatur vorgeschlagene Methoden zur Überbrückung dieser Lücke.
Hierarchisches Reinforcement Learning ist ein vielversprechender Ansatz für Entscheidungsprobleme mit langen Zeithorizonten und spärlichen Belohnungen.Leider entkoppeln die meisten Methoden immer noch den Prozess des Erwerbs von Fähigkeiten auf niedrigerer Ebene und das Training einer höheren Ebene, die die Fähigkeiten in einer neuen Aufgabe kontrolliert.Die Behandlung der Fähigkeiten als feststehend kann zu erheblicher Suboptimalität in der Transferumgebung führen.In dieser Arbeit schlagen wir einen neuartigen Algorithmus vor, um eine Reihe von Fähigkeiten zu entdecken und sie zusammen mit der höheren Ebene kontinuierlich anzupassen, selbst wenn sie auf einer neuen Aufgabe trainiert werden. Wir führen die Hierarchical Proximal Policy Optimization (HiPPO) ein, eine On-Policy-Methode, um alle Ebenen der Hierarchie gleichzeitig effizient zu trainieren. Zweitens schlagen wir eine Methode zum Trainieren von Zeitabstraktionen vor, die die Robustheit der gewonnenen Fähigkeiten gegenüber Umgebungsänderungen verbessert.Code und Ergebnisse sind unter sites.google.com/view/hippo-rl verfügbar.
Um zu quantifizieren, wie schwierig es ist, eine Aufgabe zu erlernen, berechnen wir einen beobachteten Wert für die gegenseitige Information, indem wir die geschätzte gegenseitige Information durch die Entropie der Eingabe teilen.Wir begründen diesen Wert analytisch, indem wir zeigen, dass die geschätzte gegenseitige Information einen Fehler hat, der mit der Entropie der Daten zunimmt. Experimentell analysieren wir bildbasierte Eingabedatendarstellungen und zeigen, dass die Leistungsergebnisse umfangreicher Netzwerkarchitektur-Suchen gut mit der berechneten Punktzahl übereinstimmen. Um bessere Lernergebnisse zu gewährleisten, müssen Darstellungen daher möglicherweise sowohl auf die Aufgabe als auch auf das Modell zugeschnitten sein, um mit der impliziten Verteilung des Modells übereinzustimmen.
Sepsis ist eine lebensbedrohliche Komplikation einer Infektion und eine der Hauptursachen für die Sterblichkeit in Krankenhäusern.  Während die frühzeitige Erkennung von Sepsis die Ergebnisse für die Patienten verbessert, gibt es kaum einen Konsens über genaue Behandlungsrichtlinien, und die Behandlung septischer Patienten bleibt ein offenes Problem.  Wir modellieren kontinuierliche physiologische Zeitreihen von Patienten unter Verwendung von Gauß'schen Prozessen mit mehreren Ausgängen, einem probabilistischen Modell, das fehlende Werte und unregelmäßige Beobachtungszeiten leicht handhabt und gleichzeitig Schätzungen der Unsicherheit beibehält.der Gauß'sche Prozess ist direkt mit einem tiefen rekurrenten Q-Netz verknüpft, das klinisch interpretierbare Behandlungsrichtlinien lernt, und beide Modelle werden zusammen Ende-zu-Ende gelernt.  Wir evaluieren unseren Ansatz auf einem heterogenen Datensatz von septischen über 15 Monate von unserer Universität Gesundheitssystem, und finden, dass unsere gelernte Politik könnte die Patientensterblichkeit um so viel wie 8,2 \% von einer Gesamtbasislinie Sterblichkeitsrate von 13,3 \% zu reduzieren.  Unser Algorithmus könnte verwendet werden, um Ärzten Behandlungsempfehlungen als Teil eines Entscheidungsunterstützungstools zu geben, und der Rahmen lässt sich leicht auf andere Verstärkungslernprobleme anwenden, die auf spärlich abgetasteten und häufig fehlenden multivariaten Zeitreihendaten basieren.
Unüberwachtes und halbüberwachtes Lernen sind wichtige Probleme, die bei komplexen Daten wie natürlichen Bildern eine besondere Herausforderung darstellen, und der Fortschritt bei diesen Problemen würde sich beschleunigen, wenn wir Zugang zu geeigneten generativen Modellen hätten, um die damit verbundenen Inferenzaufgaben zu stellen. Inspiriert durch den Erfolg von Convolutional Neural Networks (CNNs) für die überwachte Vorhersage in Bildern, entwerfen wir das Neural Rendering Model (NRM), ein neues hierarchisches probabilistisches generatives Modell, dessen Inferenzberechnungen denen in einem CNN entsprechen.Das NRM führt einen kleinen Satz latenter Variablen auf jeder Ebene des Modells ein und erzwingt Abhängigkeiten zwischen allen latenten Variablen über eine konjugierte Prior-Verteilung. Wir zeigen, dass dieser Regularisierer die Generalisierung sowohl in der Theorie als auch in der Praxis verbessert. Die Likelihood-Schätzung im NRM liefert den neuen Max-Min-Kreuzentropie-Trainingsverlust, der eine neue tiefe Netzwerkarchitektur - das Max-Min-Netzwerk - vorschlägt, die den Stand der Technik für halbüberwachtes und überwachtes Lernen auf SVHN, CIFAR10 und CIFAR100 übertrifft oder ihm entspricht.
Deep Reinforcement Learning (RL)-Agenten sind oft nicht in der Lage, sich auf unbekannte Umgebungen zu verallgemeinern (obwohl sie den trainierten Agenten semantisch ähnlich sind), insbesondere wenn sie auf hochdimensionalen Zustandsräumen, wie z.B. Bildern, trainiert werden.In diesem Papier schlagen wir eine einfache Technik vor, um die Verallgemeinerungsfähigkeit von Deep RL-Agenten zu verbessern, indem wir ein randomisiertes (konvolutionelles) neuronales Netzwerk einführen, das die Eingangsbeobachtungen zufällig stört. Darüber hinaus betrachten wir eine Inferenzmethode, die auf der Monte-Carlo-Approximation basiert, um die durch diese Randomisierung induzierte Varianz zu reduzieren. Wir demonstrieren die Überlegenheit unserer Methode in 2D CoinRun, 3D DeepMind Lab Exploration und 3D Robotik-Steuerungsaufgaben: Sie übertrifft verschiedene Regularisierungs- und Datenerweiterungsmethoden für den gleichen Zweck deutlich.
Um besser zu verstehen, ob mehrere Befehlszuordnungen sich gegenseitig stören oder die Übertragung und Beibehaltung beeinflussen können, haben wir einen Prototyp mit drei Drucktasten auf einer Smartphone-Hülle entwickelt, die verwendet werden können, um dem System erweiterte Eingaben zu liefern. Wir haben diese Tasten drei verschiedenen Aktionen zugeordnet und eine Studie durchgeführt, um herauszufinden, ob sich mehrere Zuordnungen auf das Lernen und die Leistung, den Transfer und das Behalten auswirken. Die Übertragung auf eine realistischere Aufgabe war erfolgreich, wenn auch mit einer leichten Verringerung der Genauigkeit.Beibehaltung nach einer Woche war zunächst schlecht, aber Experte Leistung wurde schnell wiederhergestellt.Unsere Arbeit bietet neue Informationen über die Gestaltung und Verwendung von Augmented Input in mobilen Interaktionen.
Neuronale Netze sind in der Verarbeitung natürlicher Sprache weit verbreitet, doch trotz ihrer empirischen Erfolge ist ihr Verhalten spröde: Sie reagieren sowohl überempfindlich auf kleine Eingabeänderungen als auch unterempfindlich auf das Löschen großer Teile des Eingabetextes. Wir entwickeln eine neuartige Technik zur formalen Verifikation dieser Spezifikation für Modelle, die auf dem beliebten Mechanismus der dekomponierbaren Aufmerksamkeit basieren, indem wir den effizienten und effektiven Ansatz der Intervallschrankenpropagation (IBP) verwenden. In unseren Experimenten mit den SNLI- und MNLI-Datensätzen stellen wir fest, dass das IBP-Training zu einer signifikant verbesserten Verifizierungsgenauigkeit führt. Auf dem SNLI-Testsatz können wir 18,4 % der Proben verifizieren, was eine erhebliche Verbesserung gegenüber nur 2,8 % beim Standardtraining darstellt.
Das Training mit einer größeren Anzahl von Parametern bei gleichzeitiger Beibehaltung schneller Iterationen ist eine zunehmend angewandte Strategie und ein Trend zur Entwicklung leistungsfähigerer Modelle für tiefe neuronale Netze (DNN), was einen größeren Speicherbedarf und höhere Rechenanforderungen für das Training erfordert. Wir nennen diese Methode Shifted and Squeezed FP8 (S2FP8). Wir zeigen, dass die vorgeschlagene Methode im Gegensatz zu früheren 8-Bit-Präzisions-Trainingsmethoden für repräsentative Modelle sofort funktioniert: Die Methode kann die Modellgenauigkeit beibehalten, ohne dass eine Feinabstimmung der Verlustskalierungsparameter erforderlich ist oder bestimmte Schichten in einfacher Präzision gehalten werden müssen.Wir führen zwei erlernbare Statistiken der DNN-Tensoren ein - verschobene und gequetschte Faktoren, die verwendet werden, um den Bereich der Tensoren in 8-Bit optimal anzupassen und so den Informationsverlust aufgrund der Quantisierung zu minimieren.
Variational Auto Encoders (VAE) sind in der Lage, realistische Bilder, Töne und Videosequenzen zu erzeugen.Aus der Sicht von Praktikern sind wir in der Regel daran interessiert, Probleme zu lösen, bei denen Aufgaben sequentiell gelernt werden, und zwar auf eine Weise, die es vermeidet, alle vorherigen Daten in jeder Phase erneut zu betrachten.Wir gehen dieses Problem an, indem wir einen konzeptionell einfachen und skalierbaren End-to-End-Ansatz einführen, der vergangenes Wissen einbezieht, indem er Prior direkt aus den Daten lernt.  Wir betrachten eine skalierbare Boosting-ähnliche Annäherung an die schwer zu erreichenden theoretischen optimalen Prioren und führen empirische Studien an zwei häufig verwendeten Benchmarks durch, nämlich MNIST und Fashion MNIST für disjunkte sequenzielle Bilderzeugungsaufgaben. Für jeden Datensatz liefert die vorgeschlagene Methode die besten Ergebnisse unter vergleichbaren Ansätzen und vermeidet katastrophales Vergessen auf vollautomatische Weise mit einer festen Modellarchitektur.
Die meisten dieser Fortschritte kamen dadurch zustande, dass man das few-shot learning als ein Meta-Lernproblem betrachtete. Model Agnostic Meta Learning oder MAML ist derzeit einer der besten Ansätze für few-shot learning via Meta-Lernen. MAML ist einfach, elegant und sehr leistungsfähig, hat jedoch eine Reihe von Problemen, wie z.B. sehr empfindlich auf neuronale Netzwerkarchitekturen, was oft zu Instabilität während des Trainings führt, und erfordert mühsame Hyperparametersuchen, um das Training zu stabilisieren und eine hohe Generalisierung zu erreichen, und ist sowohl beim Training als auch bei der Inferenz sehr rechenaufwändig.In diesem Papier schlagen wir verschiedene Modifikationen an MAML vor, die nicht nur das System stabilisieren, sondern auch die Generalisierungsleistung, die Konvergenzgeschwindigkeit und den Rechenaufwand von MAML erheblich verbessern, was wir MAML++ nennen.
Die Erkennung von Gemeinschaften in Graphen ist von zentraler Bedeutung in den Bereichen Graph Mining, maschinelles Lernen und Netzwerkwissenschaft. Die Erkennung von sich überschneidenden Gemeinschaften ist eine besondere Herausforderung und bleibt ein offenes Problem.  Motiviert durch den Erfolg des Graphen-basierten tiefen Lernens in anderen Graphen-bezogenen Aufgaben, untersuchen wir die Anwendbarkeit dieses Rahmens für die Erkennung überlappender Gemeinschaften. Wir schlagen ein probabilistisches Modell zur Erkennung überlappender Gemeinschaften vor, das auf der Architektur eines neuronalen Graphen-Netzwerks basiert.  Trotz seiner Einfachheit übertrifft unser Modell die bestehenden Ansätze bei der Erkennung von Gemeinschaften um ein Vielfaches.  Darüber hinaus ist das vorgeschlagene Modell aufgrund der induktiven Formulierung in der Lage, die Erkennung von Gemeinschaften außerhalb der Stichprobe für Knoten durchzuführen, die zum Zeitpunkt des Trainings nicht vorhanden waren.
Die Suche nach neuronalen Architekturen (NAS) zielt darauf ab, den Entwurf von tiefen Netzwerken für neue Aufgaben zu erleichtern.Bestehende Verfahren beruhen auf zwei Phasen: Suche im Architekturraum und Validierung der besten Architektur.NAS-Algorithmen werden derzeit ausschließlich auf der Grundlage ihrer Ergebnisse bei der nachgelagerten Aufgabe verglichen.Obwohl dies intuitiv ist, wird die Effektivität ihrer Suchstrategien nicht explizit bewertet.In diesem Papier schlagen wir vor, die NAS-Suchphase zu bewerten. Wir stellen fest, dass:(i) die modernsten NAS-Algorithmen im Durchschnitt eine ähnliche Leistung erbringen wie die zufällige Strategie;(ii) die weit verbreitete Strategie der Gewichtsteilung die Rangfolge der NAS-Kandidaten so weit verschlechtert, dass sie ihre wahre Leistung nicht mehr widerspiegelt, was die Effektivität des Suchprozesses verringert.Wir glauben, dass unser Bewertungsrahmen der Schlüssel zum Entwurf von NAS-Strategien sein wird, die beständig Architekturen finden, die den zufälligen überlegen sind.
In dieser Arbeit nutzen wir die geometrischen Eigenschaften des optimalen Transportproblems (OT) und die Wasserstein-Distanzen, um eine Prioritätsverteilung für den latenten Raum eines Auto-Encoders zu definieren. Wir führen Sliced-Wasserstein-Auto-Encoder (SWAE) ein, die es ermöglichen, die Verteilung des latenten Raums in eine beliebige, abtastbare Wahrscheinlichkeitsverteilung zu formen, ohne dass ein gegnerisches Netzwerk trainiert werden muss oder eine Likelihood-Funktion spezifiziert werden muss. Wir zeigen, dass die vorgeschlagene Formulierung eine effiziente numerische Lösung bietet, die ähnliche Fähigkeiten wie Wasserstein-Auto-Encoder (WAE) und Variations-Auto-Encoder (VAE) bietet, während sie von einer peinlich einfachen Implementierung profitiert.Wir bieten eine umfassende Fehleranalyse für unseren Algorithmus und zeigen seine Vorzüge an drei Benchmark-Datensätzen.
Der Hamilton-Formalismus spielt eine zentrale Rolle in der klassischen Physik und der Quantenphysik. Hamiltonianer sind das wichtigste Werkzeug zur Modellierung der kontinuierlichen zeitlichen Entwicklung von Systemen mit konservierten Größen, und sie sind mit vielen nützlichen Eigenschaften ausgestattet, wie z.B. zeitliche Reversibilität und glatte Interpolation in der Zeit. Diese Eigenschaften sind für viele Probleme des maschinellen Lernens wichtig - von der Sequenzvorhersage bis hin zum Reinforcement Learning und der Dichtemodellierung -, werden aber in der Regel nicht von Standardwerkzeugen wie rekurrenten neuronalen Netzen unterstützt. In diesem Papier stellen wir das Hamiltonian Generative Network (HGN) vor, den ersten Ansatz, der in der Lage ist, Hamiltonian-Dynamiken aus hochdimensionalen Beobachtungen (wie z. B. Bildern) ohne restriktive Domänenannahmen konsistent zu lernen. Wir zeigen, wie eine einfache Modifikation der Netzwerkarchitektur HGN in ein leistungsfähiges normalisierendes Flussmodell, genannt Neural Hamiltonian Flow (NHF), verwandelt, das die Hamilton'sche Dynamik zur Modellierung ausdrucksstarker Dichten verwendet, und wir hoffen, dass unsere Arbeit als erste praktische Demonstration des Wertes dient, den der Hamilton'sche Formalismus dem maschinellen Lernen bringen kann. http://tiny.cc/hgn
Bei der Cloud-Migration werden die Daten, Anwendungen und Dienste des Kunden von der ursprünglichen IT-Plattform auf eine oder mehrere Cloud-Umgebungen übertragen, um die Leistung des IT-Systems zu verbessern und gleichzeitig die IT-Verwaltungskosten zu senken.Die Cloud-Migrationsprojekte auf Unternehmensebene sind in der Regel komplex und umfassen die dynamische Planung und Neuplanung verschiedener Arten von Transformationen für bis zu 10.000 Endpunkte. Derzeit werden die Planung und Neuplanung in der Cloud-Migration in der Regel manuell oder halb-manuell mit starker Abhängigkeit vom Domänenwissen des Migrationsexperten durchgeführt, was Tage oder sogar Wochen für jede Runde der Planung oder Neuplanung in Anspruch nimmt. Daher ist eine automatisierte Planungsmaschine, die in der Lage ist, einen qualitativ hochwertigen Migrationsplan in kurzer Zeit zu generieren, für die Migrationsindustrie besonders wünschenswert.In diesem kurzen Beitrag stellen wir kurz die Vorteile der Verwendung von KI-Planung in der Cloud-Migration, einen vorläufigen Prototyp sowie die Herausforderungen vor, die die Aufmerksamkeit der Planungs- und Planungsgesellschaft erfordern.
Unüberwachte Domänenanpassung zielt darauf ab, die in einer Quelldomäne trainierte Hypothese auf eine unbeschriftete Zieldomäne zu verallgemeinern.Ein beliebter Ansatz für dieses Problem ist das Erlernen einer domäneninvarianten Repräsentation für beide Domänen.In dieser Arbeit untersuchen wir theoretisch und empirisch die explizite Auswirkung der Einbettung auf die Verallgemeinerung auf die Zieldomäne.Insbesondere wirkt sich die Komplexität der Klasse der Einbettungen auf eine Obergrenze für das Risiko der Zieldomäne aus.Dies spiegelt sich auch in unseren Experimenten wider.
Der Bereich der Zeitreihenvorhersage wurde ausgiebig untersucht, da er in vielen realen Anwendungen von grundlegender Bedeutung ist.Wettervorhersage, Verkehrsflussvorhersage oder Verkauf sind überzeugende Beispiele für sequenzielle Phänomene.Vorhersagemodelle nutzen im Allgemeinen die Beziehungen zwischen vergangenen und zukünftigen Werten.Im Fall von stationären Zeitreihen hängen die beobachteten Werte jedoch auch drastisch von einer Reihe exogener Merkmale ab, die zur Verbesserung der Vorhersagequalität genutzt werden können. In dieser Arbeit schlagen wir einen Paradigmenwechsel vor, der darin besteht, solche Merkmale in Einbettungsvektoren innerhalb rekurrenter neuronaler Netze zu erlernen, und wenden unseren Rahmen auf die Vorhersage von Smartcard-Tap-in-Protokollen im Pariser U-Bahn-Netz an, wobei die Ergebnisse zeigen, dass in den Kontext eingebettete Modelle bei der Vorhersage in einem und mehreren Schritten quantitativ besser abschneiden.
Die Fähigkeit eines Agenten, seine eigenen Lernziele zu entdecken, wird seit langem als ein Schlüsselelement für künstliche allgemeine Intelligenz angesehen.Durchbrüche bei der autonomen Entscheidungsfindung und beim Verstärkungslernen wurden hauptsächlich in Bereichen erzielt, in denen das Ziel des Agenten klar umrissen ist: z. B. ein Spiel zu spielen, um zu gewinnen, oder sicher zu fahren. Mehrere Studien haben gezeigt, dass das Erlernen extramuraler Teilaufgaben und zusätzlicher Vorhersagen (1) das Erlernen einzelner, vom Menschen vorgegebener Aufgaben, (2) den Lerntransfer und (3) die erlernte Repräsentation der Welt durch den Agenten verbessern kann.In all diesen Beispielen wurde der Agent angewiesen, worüber er lernen sollte. Wir untersuchen einen Rahmen für die Entdeckung: Kuratieren eine große Sammlung von Vorhersagen, die verwendet werden, um den Agenten Darstellung der Welt zu konstruieren.Insbesondere unterhält unser System eine große Sammlung von Vorhersagen, kontinuierlich beschneiden und ersetzen Vorhersagen.Wir unterstreichen die Bedeutung der Berücksichtigung der Stabilität als Konvergenz für ein solches System, und entwickeln eine adaptive, regulierte Algorithmus zu diesem Ziel.Wir bieten mehrere Experimente in computergestützten Mikrowelten zeigen, dass dieser einfache Ansatz kann wirksam sein für die Entdeckung nützliche Vorhersagen autonom.
Die Schätzung des Aufnahmeortes eines Bildes allein auf der Grundlage des Bildinhaltes ist selbst für Menschen eine schwierige Aufgabe, da die korrekte Kennzeichnung eines Bildes auf diese Weise stark von Kontextinformationen abhängt und nicht so einfach ist wie die Identifizierung eines einzelnen Objekts im Bild. Diese Arbeit leistet einen Beitrag zum Stand der Forschung auf dem Gebiet der Geolokalisierung von Bildern, indem sie eine neuartige globale Vernetzungsstrategie vorstellt, eine Reihe von Trainingsverfahren zur Überwindung der erheblichen Datenbeschränkungen beim Training dieser Modelle beschreibt und zeigt, wie die Einbeziehung zusätzlicher Informationen zur Verbesserung der Gesamtleistung eines Geolokalisierungsmodells verwendet werden kann.  Darüber hinaus lassen sich der Zeitpunkt der Veröffentlichung, gelernte Benutzer-Alben und andere Metadaten leicht einbeziehen, um die Genauigkeit der Geolokalisierung um bis zu 11 % für Orte auf Länderebene (750 km) und 3 % für Orte auf Stadtebene (25 km) zu verbessern.
Hierarchische Bayes-Methoden haben das Potenzial, viele verwandte Aufgaben zu vereinheitlichen (z. B. k-shot-Klassifikation, bedingte und unbedingte Generierung), indem sie jede als Inferenz innerhalb eines einzigen generativen Modells betrachten. wir zeigen, dass bestehende Ansätze für das Lernen solcher Modelle auf ausdrucksstarken generativen Netzwerken wie PixelCNNs versagen können, indem sie die globale Verteilung mit wenig Vertrauen auf latente Variablen beschreiben. Um dies zu beheben, entwickeln wir eine Modifikation des Variational Autoencoders, bei der kodierte Beobachtungen in neue Elemente derselben Klasse dekodiert werden; das Ergebnis, das wir Variational Homoencoder (VHE) nennen, kann als Training eines hierarchischen latenten Variablenmodells verstanden werden, das latente Variablen in diesen Fällen besser nutzt. Mit diesem Rahmen können wir ein hierarchisches PixelCNN für den Omniglot-Datensatz trainieren, das alle bestehenden Modelle in Bezug auf die Likelihood des Testsatzes übertrifft. Mit einem einzigen Modell erreichen wir sowohl eine starke One-Shot-Generierung als auch eine Klassifizierung auf nahezu menschlichem Niveau, die mit hochmodernen diskriminativen Klassifizierern konkurrieren kann. Das VHE-Ziel lässt sich natürlich auf reichere Datensatzstrukturen wie faktorielle oder hierarchische Kategorien ausdehnen, wie wir durch das Training von Modellen veranschaulichen, die den Zeicheninhalt von einfachen Variationen im Zeichenstil trennen und den Stil eines Alphabets auf neue Zeichen verallgemeinern.
Um natürliche Sprache zu verstehen, ist ein gesunder Menschenverstand oder Hintergrundwissen erforderlich, aber in den meisten neuronalen Systemen zum Verstehen natürlicher Sprache (NLU) wird das erforderliche Hintergrundwissen indirekt aus statischen Korpora erworben.Wir entwickeln eine neue Lesearchitektur für die dynamische Integration von explizitem Hintergrundwissen in NLU-Modelle. Ein neues aufgabenspezifisches Lesemodul liefert einer aufgabenspezifischen NLU-Architektur verfeinerte Wortrepräsentationen, indem es Hintergrundwissen in Form von Freitextaussagen zusammen mit den aufgabenspezifischen Eingaben verarbeitet.Starke Leistungen bei den Aufgaben der Beantwortung von Dokumentenfragen (DQA) und der Erkennung von textuellem Entailment (RTE) zeigen die Effektivität und Flexibilität unseres Ansatzes.Die Analyse zeigt, dass unsere Modelle lernen, Wissen selektiv und auf semantisch angemessene Weise zu nutzen.
Trotz der Vorteile der gemischt-präzisen Arithmetik in Bezug auf die Verringerung des Bedarfs an wichtigen Ressourcen wie Speicherbandbreite oder Registerdateigröße, hat es eine begrenzte Kapazität für die Verringerung der Rechenkosten und erfordert 32 Bits, um seine Ausgabeoperanden zu repräsentieren.Dieses Papier schlägt zwei Ansätze vor, um gemischt-präzise für halbpräzise Arithmetik während eines großen Teils des Trainings zu ersetzen. Der erste Ansatz erreicht Genauigkeitsverhältnisse etwas langsamer als der Stand der Technik durch die Verwendung von Halbpräzisions-Arithmetik während mehr als 99% der Ausbildung.Der zweite Ansatz erreicht die gleiche Genauigkeit wie der Stand der Technik durch dynamisches Umschalten zwischen Halb- und gemischte Präzision arithmetische während training.It verwendet Halbpräzision während mehr als 94% der Ausbildung.This Papier ist das erste in der Demonstration, dass Halbpräzision kann für einen sehr großen Teil der DNNs Ausbildung verwendet werden und immer noch erreichen Stand der Technik Genauigkeit.
Wir führen die "inverse Quadratwurzel-Lineareinheit" (ISRLU) ein, um das Lernen in tiefen neuronalen Netzen zu beschleunigen.ISRLU hat eine bessere Leistung als ELU, bietet aber viele der gleichen Vorteile.ISRLU und ELU haben ähnliche Kurven und Eigenschaften. ISRLU und ELU haben ähnliche Kurven und Eigenschaften. Beide haben negative Werte, die es ihnen ermöglichen, die mittlere Aktivierung der Einheit näher an den Nullpunkt zu bringen und den normalen Gradienten näher an den natürlichen Gradienten der Einheit zu bringen, was einen störungsresistenten Deaktivierungszustand sicherstellt und das Risiko einer Überanpassung verringert.Der signifikante Leistungsvorteil von ISRLU auf herkömmlichen CPUs überträgt sich auch auf effizientere HW-Implementierungen auf HW/SW-Codesign für CNNs/RNNs. In Experimenten mit TensorFlow führt ISRLU zu schnellerem Lernen und besserer Generalisierung als ReLU auf CNNs.Diese Arbeit schlägt auch eine rechnerisch effiziente Variante vor, die als "inverse Quadratwurzel-Einheit" (ISRU) bezeichnet wird und für RNNs verwendet werden kann.Viele RNNs verwenden entweder Long Short Memory (LSTM) und Gated Recurrent Units (GRU), die mit tanh- und sigmoid-Aktivierungsfunktionen implementiert werden.ISRU hat eine geringere rechnerische Komplexität, hat aber dennoch eine ähnliche Kurve wie tanh und sigmoid.
Ein im Allgemeinen intelligenter Lerner sollte sich auf komplexere Aufgaben verallgemeinern, als er zuvor kennengelernt hat, aber die beiden gängigen Paradigmen des maschinellen Lernens - entweder das Trainieren eines separaten Lerners pro Aufgabe oder das Trainieren eines einzelnen Lerners für alle Aufgaben - haben beide Schwierigkeiten mit einer solchen Verallgemeinerung, weil sie die kompositionelle Struktur der Aufgabenverteilung nicht nutzen.Dieses Papier führt den kompositionellen Problemgraphen als einen breit anwendbaren Formalismus ein, um Aufgaben unterschiedlicher Komplexität in Form von Problemen mit gemeinsamen Teilproblemen in Beziehung zu setzen.Wir schlagen das kompositionelle Verallgemeinerungsproblem vor, um zu messen, wie bereitwillig altes Wissen wiederverwendet und somit darauf aufgebaut werden kann. Als ersten Schritt zur Lösung des Problems der kompositionellen Generalisierung führen wir den kompositionellen rekursiven Lerner ein, einen domänenübergreifenden Rahmen für das Erlernen von algorithmischen Prozeduren zum Zusammensetzen von Repräsentationstransformationen, der einen Lerner erzeugt, der durch Analogien zu zuvor gesehenen Problemen über die auszuführenden Berechnungen nachdenkt.Wir zeigen auf einer symbolischen und einer hochdimensionalen Domäne, dass unser kompositioneller Ansatz auf komplexere Probleme verallgemeinert werden kann, als sie dem Lerner zuvor begegnet sind, wohingegen Grundlinien, die nicht ausdrücklich kompositionell sind, dies nicht tun.
Die Informationsengpass-Methode bietet eine informationstheoretische Methode für das Lernen von Repräsentationen, indem ein Encoder darauf trainiert wird, alle Informationen zu behalten, die für die Vorhersage des Labels relevant sind, während die Menge anderer, überflüssiger Informationen in der Repräsentation minimiert wird.  Eine theoretische Analyse führt zur Definition eines neuen Multiview-Modells, das auf dem Sketchy-Datensatz und auf markierungsbeschränkten Versionen des MIR-Flickr-Datensatzes Spitzenergebnisse liefert.  Wir erweitern unsere Theorie auch auf den Single-View-Bereich, indem wir standardmäßige Datenerweiterungstechniken nutzen und empirisch bessere Generalisierungsfähigkeiten im Vergleich zu traditionellen unüberwachten Ansätzen für Repräsentationslernen zeigen.
Die biologische Plausibilität des Backpropagation-Algorithmus wird von Neurowissenschaftlern seit langem angezweifelt, da die Neuronen in der Vorwärts- und Rückwärtsphase zwei verschiedene Arten von Signalen senden müssten und die Neuronenpaare über symmetrische bidirektionale Verbindungen kommunizieren müssten.Wir stellen ein einfaches zweiphasiges Lernverfahren für rekurrente Festpunktnetze vor, das diese beiden Probleme löst.In unserem Modell führen die Neuronen eine undichte Integration durch und die synaptischen Gewichte werden durch einen lokalen Mechanismus aktualisiert. Als Folge dieser Verallgemeinerung berechnet der Algorithmus nicht den wahren Gradienten der Zielfunktion, sondern approximiert ihn mit einer Genauigkeit, die nachweislich in direktem Zusammenhang mit dem Symmetriegrad der Feedforward- und Feedback-Gewichte steht.Wir zeigen experimentell, dass die intrinsischen Eigenschaften des Systems zu einer Angleichung der Feedforward- und Feedback-Gewichte führen und dass unser Algorithmus die Zielfunktion optimiert.
In dieser Arbeit stellen wir die erste Methode vor, die unseres Wissens nach ein generatives Modell von 3D-Formen aus natürlichen Bildern völlig unbeaufsichtigt erlernt, d.h. wir verwenden keine 3D- oder 2D-Anmerkungen, kein Stereovideo und keine Ego-Motion während des Trainings. Unser Ansatz folgt der allgemeinen Strategie von Generative Adversarial Networks, bei denen ein Bildgenerator-Netzwerk lernt, Bildmuster zu erzeugen, die realistisch genug sind, um ein Diskriminator-Netzwerk zu täuschen, so dass es glaubt, dass es sich um natürliche Bilder handelt.Im Gegensatz dazu ist in unserem Ansatz die Bildgenerierung in zwei Stufen aufgeteilt.In der ersten Stufe gibt ein Generator-Netzwerk 3D-Objekte aus.In der zweiten Stufe erzeugt ein differenzierbarer Renderer ein Bild des 3D-Objekts aus einem zufälligen Blickwinkel. Die wichtigste Beobachtung ist, dass ein realistisches 3D-Objekt aus jedem plausiblen Blickwinkel ein realistisches Rendering ergeben sollte, so dass unser vorgeschlagenes Training durch die zufällige Wahl des Blickwinkels das Generatornetz dazu zwingt, eine interpretierbare 3D-Darstellung unabhängig vom Blickwinkel zu erlernen. In dieser Arbeit besteht eine 3D-Darstellung aus einem Dreiecksnetz und einer Texturkarte, die zum Einfärben der Dreiecksfläche mit Hilfe der UV-Mapping-Technik verwendet wird. Wir analysieren unseren Lernansatz, zeigen seine Unklarheiten auf und zeigen, wie sie überwunden werden können.Experimentell zeigen wir, dass unsere Methode realistische 3D-Formen von Gesichtern lernen kann, indem wir nur die natürlichen Bilder des FFHQ-Datensatzes verwenden.
Wir konzentrieren uns auf das Problem der Black-Box-Angriffe, bei denen das Ziel darin besteht, mit Hilfe von Informationen, die sich auf die Bewertung von Verlustfunktionen von Eingabe-Ausgabe-Paaren beschränken, Gegenbeispiele zu generieren.Wir verwenden Bayes'sche Optimierung (BO), um speziell auf Szenarien mit geringen Abfrage-Budgets einzugehen und abfrageeffiziente Gegenangriffe zu entwickeln. Unser vorgeschlagener Ansatz erreicht eine Leistung, die mit dem Stand der Technik vergleichbar ist, wenn auch mit einer viel geringeren durchschnittlichen Anzahl von Abfragen. Insbesondere bei niedrigen Abfragebudgets reduziert die von uns vorgeschlagene Methode die Anzahl der Abfragen um bis zu 80% im Vergleich zum Stand der Technik.
Moderne tiefe neuronale Netze (DNNs) haben in der Regel Dutzende Millionen von Parametern, die möglicherweise nicht in die oberen Ebenen der Speicherhierarchie passen, wodurch sich die Inferenzzeit und der Energieverbrauch erheblich erhöhen und ihr Einsatz auf Endgeräten wie Mobiltelefonen nicht möglich ist. Die Kompression von DNN-Modellen ist daher in letzter Zeit zu einem aktiven Forschungsgebiet geworden, wobei sich \emph{connection pruning} als eine der erfolgreichsten Strategien herauskristallisiert hat. Ein sehr natürlicher Ansatz ist es, Verbindungen von DNNs über $\ell_1$ Regularisierung zu beschneiden, aber neuere empirische Untersuchungen haben nahegelegt, dass dies im Kontext der DNN-Kompression nicht so gut funktioniert. In dieser Arbeit greifen wir diese einfache Strategie wieder auf und analysieren sie rigoros, um zu zeigen, dass:(a) die Anzahl der Nicht-Null-Elemente eines $\ell_1$-regulierten schichtweisen Pruning-Ziels durch die Anzahl der bestraften Prädiktionslogits begrenzt ist, unabhängig von der Stärke der Regularisierung;(b) erfolgreiches Pruning in hohem Maße von einem genauen Optimierungslöser abhängt und ein Kompromiss zwischen Kompressionsgeschwindigkeit und Verzerrung der Prädiktionsgenauigkeit besteht, der von der Stärke der Regularisierung gesteuert wird. Unsere theoretischen Ergebnisse deuten darauf hin, dass $\ell_1$ Pruning erfolgreich sein könnte, vorausgesetzt, dass wir einen genauen Optimierungslöser verwenden; wir bestätigen dies in unseren Experimenten, in denen wir zeigen, dass eine einfache $\ell_1$ Regularisierung mit einem Adamax-L1(kumulativen) Löser ein Pruning-Verhältnis ergibt, das mit dem Stand der Technik konkurriert.
Wir betrachten die Vorteile und Grenzen eines solchen Ansatzes, wenn eine Maschine den Fehler in einer Regressionsaufgabe messen muss: Wie kann eine Maschine den Fehler von Regressionsunterkomponenten messen, wenn sie nicht über die Grundwahrheit für die korrekten Vorhersagen verfügt?Ein auf das Fehlersignal der Regressoren angewandter Compressed-Sensing-Ansatz kann deren Präzisionsfehler ohne Grundwahrheit wiederherstellen. Seine Lösungen sind jedoch nicht eindeutig - eine Eigenschaft der Lösungen für die Bodenwahrheitsinferenz.Durch Hinzufügen von $\ell_1$---Minimierung als Bedingung kann die korrekte Lösung in Einstellungen wiederhergestellt werden, in denen eine Fehlerkorrektur möglich ist.Wir erörtern kurz die Ähnlichkeit der Mathematik der Bodenwahrheitsinferenz für Regressoren mit der für Klassifikatoren.
Wir schlagen eine neue Repräsentation vor, die Ein-Pixel-Signatur, die verwendet werden kann, um die Eigenschaften der Faltungsneuronalen Netze (CNNs) aufzudecken. Hier wird jeder CNN-Klassifikator mit einer Signatur assoziiert, die erstellt wird, indem Pixel für Pixel ein negativer Wert erzeugt wird, der das Ergebnis der größten Änderung der Klassenvorhersage ist. Die Ein-Pixel-Signatur ist unabhängig von den Design-Entscheidungen der CNN-Architekturen, wie z. B. Typ, Tiefe, Aktivierungsfunktion und Art des Trainings, und kann für einen Blackbox-Klassifikator ohne Zugriff auf die Netzwerkparameter effizient berechnet werden. Klassische Netzwerke wie LetNet, VGG, AlexNet und ResNet zeigen unterschiedliche Eigenschaften in ihren Signaturbildern.Für die Anwendung konzentrieren wir uns auf das Klassifikator-Backdoor-Erkennungsproblem, bei dem ein CNN-Klassifikator böswillig mit einem unbekannten Trojaner eingefügt wurde.Wir zeigen die Wirksamkeit der Ein-Pixel-Signatur bei der Erkennung von Backdoored CNN.Unsere vorgeschlagene Ein-Pixel-Signaturdarstellung ist allgemein und kann bei Problemen angewendet werden, bei denen diskriminierende Klassifikatoren, insbesondere auf der Basis neuronaler Netzwerke, charakterisiert werden sollen.
Wir erweitern das Paradigma des Lernens aus Demonstrationen, indem wir eine Methode zum Erlernen unbekannter, aufgabenübergreifender Beschränkungen bereitstellen, indem wir Demonstrationen der Aufgaben, ihre Kostenfunktionen und das Wissen über die Systemdynamik und die Kontrollbeschränkungen verwenden.Bei sicheren Demonstrationen verwendet unsere Methode Hit-and-Run-Sampling, um kostengünstigere und damit unsichere Trajektorien zu erhalten. Sowohl sichere als auch unsichere Trajektorien werden verwendet, um eine konsistente Repräsentation der unsicheren Menge zu erhalten, indem ein gemischtes ganzzahliges Programm gelöst wird. Zusätzlich modifizieren wir unsere Methode, indem wir eine bekannte Parametrisierung des Constraints nutzen, um parametrische Constraints in hohen Dimensionen zu lernen.
Wir verwenden Siamesische Netze, um das Problem der kleinsten Quadrate multidimensionaler Skalierung zu lösen, um Abbildungen zu erzeugen, die geodätische Distanzen auf der Mannigfaltigkeit bewahren. Im Gegensatz zu früheren parametrischen Methoden zum Lernen von Mannigfaltigkeiten zeigen wir eine erhebliche Reduzierung des Trainingsaufwands, der durch die Berechnung geodätischer Distanzen in einer Farthest-Point-Sampling-Strategie ermöglicht wird. Darüber hinaus reduziert die Verwendung eines Netzwerks zur Modellierung der abstandserhaltenden Karte die Komplexität des mehrdimensionalen Skalierungsproblems und führt zu einer verbesserten nicht-lokalen Generalisierung der Mannigfaltigkeit im Vergleich zu analogen nicht-parametrischen Gegenstücken.wir demonstrieren unsere Behauptungen auf Punktwolken-Daten und auf Bild-Mannigfaltigkeiten und zeigen eine numerische Analyse unserer Technik, um ein größeres Verständnis für die Darstellungsleistung von neuronalen Netzen bei der Modellierung von Mannigfaltigkeitsdaten zu ermöglichen.
Exploration ist ein grundlegender Aspekt des Reinforcement Learning, der typischerweise durch stochastische Aktionsauswahl implementiert wird.Exploration kann jedoch effizienter sein, wenn sie auf die Gewinnung neuen Weltwissens gerichtet ist.Besuchszähler haben sich sowohl in der Praxis als auch in der Theorie als nützlich für gerichtete Exploration erwiesen.Eine wesentliche Einschränkung von Zählern ist jedoch ihre Lokalität.Während es einige modellbasierte Lösungen für dieses Manko gibt, fehlt noch ein modellfreier Ansatz. Wir schlagen $E$-Werte vor, eine Verallgemeinerung von Zählern, die verwendet werden können, um den sich ausbreitenden Explorationswert über Zustands-Aktions-Trajektorien zu bewerten.Wir vergleichen unseren Ansatz mit allgemein verwendeten RL-Techniken und zeigen, dass die Verwendung von $E$-Werten das Lernen und die Leistung im Vergleich zu traditionellen Zählern verbessert.Wir zeigen auch, wie unsere Methode mit Funktionsapproximation implementiert werden kann, um effizient kontinuierliche MDPs zu lernen.Wir demonstrieren dies, indem wir zeigen, dass unser Ansatz den Stand der Technik in dem Spiel Freeway Atari 2600 übertrifft.
Deep Neuroevolution und Deep Reinforcement Learning (Deep RL)-Algorithmen sind zwei populäre Ansätze für die Politiksuche: Erstere sind weithin anwendbar und recht stabil, leiden aber unter einer geringen Stichprobeneffizienz, während letztere zwar stichprobeneffizienter sind, aber die stichprobeneffizientesten Varianten sind auch recht instabil und reagieren sehr empfindlich auf die Einstellung von Hyperparametern.Bisher wurden diese Methodenfamilien meist als konkurrierende Werkzeuge verglichen.Ein neuer Ansatz besteht jedoch darin, sie zu kombinieren, um das Beste aus beiden Welten zu erhalten. Zwei bereits existierende Kombinationen verwenden entweder einen Ad-hoc-Evolutionsalgorithmus oder einen Zielexplorationsprozess zusammen mit dem Deep-Deterministic-Policy-Gradient-Algorithmus (DDPG), einem effizienten Off-Policy Deep-RL-Algorithmus.In diesem Papier schlagen wir ein anderes Kombinationsschema vor, das die einfache Cross-Entropie-Methode (CEM) und Twin Delayed Deep Deterministic Policy Gradient (TD3) verwendet, einen weiteren Off-Policy Deep-RL-Algorithmus, der DDPG übertrifft. Wir evaluieren die resultierende Methode, CEM-RL, auf einer Reihe von Benchmarks, die klassischerweise in Deep RL verwendet werden. Wir zeigen, dass CEM-RL von mehreren Vorteilen gegenüber seinen Konkurrenten profitiert und einen zufriedenstellenden Kompromiss zwischen Leistung und Stichprobeneffizienz bietet.
Die jüngsten Fortschritte im Bereich des Deep Reinforcement Learning haben erhebliche Fortschritte bei der Leistung von Anwendungen wie Go und Atari-Spielen gemacht. Die Entwicklung praktischer Methoden zum Ausgleich von Exploration und Ausnutzung in komplexen Domänen bleibt jedoch weitgehend ungelöst.Thompson Sampling und seine Erweiterung auf das Reinforcement Learning bieten einen eleganten Ansatz für die Exploration, der nur Zugang zu posterioren Stichproben des Modells erfordert.Gleichzeitig haben Fortschritte bei approximativen Bayes'schen Methoden die posteriore Approximation für flexible neuronale Netzwerkmodelle praktisch gemacht. Um die Auswirkungen der Verwendung eines approximativen Posteriors auf Thompson Sampling zu verstehen, vergleichen wir etablierte und neu entwickelte Methoden für approximatives Posterior Sampling in Kombination mit Thompson Sampling mit einer Reihe von kontextuellen Bandit-Problemen.Wir haben festgestellt, dass viele Ansätze, die im Rahmen des überwachten Lernens erfolgreich waren, im Szenario der sequentiellen Entscheidungsfindung unterlegen sind.
In diesem Papier untersuchen wir mathematisch die Eigenschaften der aktuellen Varianten von GANs, die Informationen über Klassenetiketten nutzen. Mit klassenbewussten Gradienten und Cross-Entropie-Zerlegung zeigen wir, wie Klassenetiketten und damit verbundene Verluste das Training von GANs beeinflussen. Umfassende Experimente wurden durchgeführt, um unsere Analyse zu validieren und die Effektivität unserer Lösung zu evaluieren. AM-GAN übertrifft dabei andere starke Baselines und erreicht einen State-of-the-Art Inception Score (8.91) auf CIFAR-10. Darüber hinaus zeigen wir, dass der Inception Score mit dem Inception ImageNet-Klassifikator hauptsächlich die Diversität des Generators abbildet, es jedoch keine verlässlichen Beweise dafür gibt, dass er die tatsächliche Qualität der Probe widerspiegeln kann.Wir schlagen daher eine neue Metrik vor, die wir AM Score nennen, um eine genauere Schätzung der Probenqualität zu ermöglichen.
Moderne neuronale Netze sind überparametrisiert, d.h. jede gleichgerichtete lineare verborgene Einheit kann durch einen multiplikativen Faktor modifiziert werden, indem die Eingangs- und Ausgangsgewichte angepasst werden, ohne den Rest des Netzes zu verändern.Inspiriert durch den Sinkhorn-Knopp-Algorithmus führen wir eine schnelle iterative Methode zur Minimierung der l2-Norm der Gewichte ein, d.h. den Gewichtsabnahme-Regulierer. Die Verschachtelung unseres Algorithmus mit SGD während des Trainings verbessert die Testgenauigkeit und bietet bei kleinen Stapeln eine Alternative zur Stapel- und Gruppennormalisierung auf CIFAR-10 und ImageNet mit ResNet-18.
In dieser Arbeit betrachten wir die Worst-Case-Analyse von Agenten über Umgebungseinstellungen, um herauszufinden, ob es Richtungen gibt, in denen Agenten bei der Generalisierung versagt haben. Konkret betrachten wir eine 3D-Ego-Aufgabe, bei der Agenten durch prozedural generierte Labyrinthe navigieren müssen, und bei der Reinforcement-Learning-Agenten kürzlich eine durchschnittliche Leistung auf menschlichem Niveau erreicht haben. Indem wir die Struktur der Labyrinthe optimieren, stellen wir fest, dass Agenten trotz ihrer beeindruckenden Durchschnittsleistung selbst bei überraschend einfachen Labyrinthen katastrophale Fehler erleiden und das Ziel nicht finden können, und dass sich diese Fehler auf verschiedene Agenten und sogar deutlich unterschiedliche Architekturen übertragen. Wir glauben, dass unsere Ergebnisse eine wichtige Rolle für die Worst-Case-Analyse bei der Identifizierung von Richtungen hervorheben, in denen Agenten bei der Generalisierung versagt haben, und hoffen, dass die Fähigkeit zur automatischen Identifizierung von Generalisierungsfehlern die Entwicklung allgemeinerer und robusterer Agenten erleichtern wird.
Um dieses Problem zu beheben, wurde vorgeschlagen, die Likelihood durch eine Pseudo-Likelihood zu ersetzen, d.h. die Exponentialfunktion einer Verlustfunktion mit geeigneten Robustheitseigenschaften.In diesem Beitrag bauen wir eine Pseudo-Likelihood auf, die auf der Maximum Mean Discrepancy (MMD) basiert, die durch eine Einbettung von Wahrscheinlichkeitsverteilungen in einen reproduzierenden Kernel-Hilbert-Raum definiert ist. Wir zeigen, dass dieses MMD-Bayes-Posterior konsistent und robust gegenüber Modellfehlspezifizierungen ist.Da das auf diese Weise erhaltene Posterior unlösbar sein könnte, beweisen wir auch, dass vernünftige Variationsapproximationen dieses Posters die gleichen Eigenschaften besitzen.Wir liefern Details zu einem stochastischen Gradientenalgorithmus, um diese Variationsapproximationen zu berechnen.Numerische Simulationen deuten in der Tat darauf hin, dass unser Schätzer robuster gegenüber Fehlspezifizierungen ist als die auf der Likelihood basierenden.
Wir stellen einen unverzerrten Schätzer der logarithmischen marginalen Likelihood und ihrer Gradienten für latente Variablenmodelle vor, der auf der randomisierten Abschneidung unendlicher Reihen basiert, wenn er durch eine Kodierer-Dekodierer-Architektur parametrisiert ist, können die Parameter des Kodierers optimiert werden, um die Varianz dieses Schätzers zu minimieren. Wir zeigen, dass Modelle, die mit unserem Schätzer trainiert werden, bei gleichem durchschnittlichem Rechenaufwand bessere Likelihoods für den Testsatz liefern als ein standardmäßiger, auf Wichtigkeitsabtastung basierender Ansatz, und dass dieser Schätzer auch die Verwendung von latenten Variablenmodellen für Aufgaben ermöglicht, bei denen unvoreingenommene Schätzer anstelle von unteren Schranken für die marginale Likelihood bevorzugt werden, wie z.B. die Minimierung von umgekehrten KL-Divergenzen und die Schätzung von Score-Funktionen.
Dieses Papier leistet zwei Beiträge zum Verständnis, wie sich die Hyperparameter des stochastischen Gradientenabstiegs auf den endgültigen Trainingsverlust und die Testgenauigkeit neuronaler Netze auswirken: Erstens argumentieren wir, dass der stochastische Gradientenabstieg zwei Regime mit unterschiedlichem Verhalten aufweist; ein vom Rauschen dominiertes Regime, das typischerweise bei kleinen oder mittleren Losgrößen auftritt, und ein von der Krümmung dominiertes Regime, das typischerweise bei großen Losgrößen auftritt. Im rauschdominierten Regime steigt die optimale Lernrate mit zunehmender Chargengröße, und der Trainingsverlust und die Testgenauigkeit sind bei konstantem Epochenbudget unabhängig von der Chargengröße; im krümmungsdominierten Regime ist die optimale Lernrate unabhängig von der Chargengröße, und der Trainingsverlust und die Testgenauigkeit nehmen mit zunehmender Chargengröße ab. Wir untermauern diese Behauptungen mit Experimenten zu einer Reihe von Architekturen, einschließlich ResNets, LSTMs und Autoencodern, und führen immer eine Rastersuche über die Lernraten bei allen Stapelgrößen durch. 2. zeigen wir, dass kleine oder mäßig große Stapelgrößen weiterhin sehr große Stapel auf der Testmenge übertreffen, selbst wenn beide Modelle für die gleiche Anzahl von Schritten trainiert werden und ähnliche Trainingsverluste erreichen. Darüber hinaus sinkt beim Training von Wide-ResNets auf CIFAR-10 mit einer konstanten Batchgröße von 64 die optimale Lernrate zur Maximierung der Testgenauigkeit nur um den Faktor 2, wenn das Epochenbudget um den Faktor 128 erhöht wird, während die optimale Lernrate zur Minimierung des Trainingsverlustes um den Faktor 16 sinkt. Diese Ergebnisse bestätigen, dass das Rauschen in stochastischen Gradienten eine vorteilhafte implizite Regularisierung einführen kann.
Counterfactual Regret Minimization (CFR) ist der erfolgreichste Algorithmus, um annähernde Nash-Gleichgewichte in Spielen mit unvollkommener Information zu finden, aber die Abhängigkeit von vollständigen Spielbaum-Traversalen schränkt die Skalierbarkeit und Allgemeinheit von CFR ein. Daher wird der Zustands- und Aktionsraum des Spiels für CFR oft abstrahiert (d.h. vereinfacht), und die resultierende Strategie wird dann auf das vollständige Spiel zurückgeführt. Dies erfordert umfangreiches Expertenwissen, ist bei vielen Spielen außerhalb des Pokerspiels nicht praktikabel und führt oft zu hochgradig ausbeutbaren Strategien. Eine kürzlich vorgeschlagene Methode, Deep CFR, wendet Deep Learning direkt auf CFR an und erlaubt es dem Agenten, intrinsisch zu abstrahieren und über den Zustandsraum von Stichproben zu verallgemeinern, ohne Expertenwissen zu benötigen.In diesem Papier stellen wir Single Deep CFR (SD-CFR) vor, eine Variante von Deep CFR, die einen geringeren Gesamtnäherungsfehler hat, indem sie das Training eines durchschnittlichen Strategie-Netzwerks vermeidet.Wir zeigen, dass SD-CFR aus theoretischer Sicht attraktiver ist und empirisch Deep CFR in Bezug auf Ausbeutbarkeit und Eins-gegen-Eins-Spiel im Poker übertrifft.
Während generative Modelle große Erfolge bei der Generierung hochdimensionaler Stichproben in Abhängigkeit von niedrigdimensionalen Deskriptoren gezeigt haben (z.B. beim Lernen der Strichstärke in MNIST, der Haarfarbe in CelebA oder der Sprecheridentität in Wavenet), wirft ihre Generierung außerhalb der Stichprobe grundlegende Probleme auf. Der Conditional Variational Autoencoder (CVAE) als einfaches bedingtes generatives Modell setzt die Bedingungen während des Trainings nicht explizit in Beziehung und hat daher keinen Anreiz, eine kompakte gemeinsame Verteilung über die Bedingungen hinweg zu lernen.Wir überwinden diese Einschränkung, indem wir ihre Verteilungen mit Hilfe der maximalen mittleren Diskrepanz (MMD) in der Decoderschicht, die dem Engpass folgt, anpassen. Dies führt zu einer starken Regularisierung sowohl für die Rekonstruktion von Proben innerhalb der gleichen Bedingung als auch für die Transformation von Proben über die Bedingungen hinweg, was zu einer deutlich verbesserten Generalisierung führt. trVAE wird im Benchmarking mit hochdimensionalen Bild- und Tabellendaten verglichen und zeigt eine höhere Robustheit und Genauigkeit als bestehende Ansätze. Insbesondere zeigen wir qualitativ verbesserte Vorhersagen für zelluläre Störungen als Reaktion auf Behandlung und Krankheit auf der Grundlage von hochdimensionalen Einzelzell-Genexpressionsdaten, indem wir zuvor problematische Minderheitsklassen und multiple Bedingungen angehen. Für allgemeine Aufgaben verbessern wir die Pearson-Korrelationen von hochdimensionalen geschätzten Mittelwerten und Varianzen mit ihren Grundwahrheiten von 0,89 auf 0,97 bzw. 0,75 auf 0,87.
Wir analysieren die Konvergenzgeschwindigkeit zum globalen Optimum für das Gradientenabstiegstraining eines tiefen linearen neuronalen Netzes durch Minimierung des L2-Verlustes über geweißte Daten.  Die Konvergenz mit einer linearen Rate ist garantiert, wenn die folgenden Bedingungen erfüllt sind: (i) die Dimensionen der verborgenen Schichten sind mindestens das Minimum der Eingangs- und Ausgangsdimensionen; (ii) die Gewichtsmatrizen sind bei der Initialisierung annähernd ausgeglichen; und (iii) der anfängliche Verlust ist kleiner als der Verlust jeder Lösung mit Rangdefizit.  Die Annahmen zur Initialisierung (Bedingungen (ii) und (iii)) sind insofern notwendig, als die Verletzung einer dieser Bedingungen zu einem Konvergenzfehler führen kann.  Darüber hinaus sind sie im wichtigen Fall der Ausgangsdimension 1, d. h. der skalaren Regression, erfüllt, so dass die Konvergenz zum globalen Optimum mit konstanter Wahrscheinlichkeit unter einem zufälligen Initialisierungsschema gegeben ist.  Unsere Ergebnisse erweitern frühere Analysen, z. B. von tiefen linearen Residualnetzwerken, erheblich (Bartlett et al., 2018).
Eines der grundlegenden Probleme bei der überwachten Klassifizierung und beim maschinellen Lernen im Allgemeinen ist die Modellierung nicht-parametrischer Invarianten, die in den Daten vorhanden sind.Die meisten bisherigen Arbeiten haben sich auf die Durchsetzung von Prioritäten in Form von Invarianten zu parametrischen Störungstransformationen konzentriert, von denen erwartet wird, dass sie in den Daten vorhanden sind.Das Lernen nicht-parametrischer Invarianten direkt aus den Daten bleibt jedoch ein wichtiges offenes Problem.In diesem Papier stellen wir eine neue architektonische Schicht für Faltungsnetzwerke vor, die in der Lage ist, allgemeine Invarianten aus den Daten selbst zu lernen. Diese Schicht kann Invarianz zu nicht-parametrischen Transformationen erlernen und motiviert interessanterweise permanente zufällige Konnektome, die als Permanent Random Connectome Non-Parametric Transformation Networks (PRC-NPTN) bezeichnet werden.PRC-NPTN-Netzwerke werden mit zufälligen Verbindungen (nicht nur Gewichten) initialisiert, die eine kleine Teilmenge der Verbindungen in einer vollständig verbundenen Faltungsschicht darstellen.  Wir motivieren zufällig initialisierte Verbindungen als eine einfache Methode, um Invarianz aus den Daten selbst zu lernen und gleichzeitig Invarianz gegenüber mehreren störenden Transformationen geltend zu machen, und stellen fest, dass diese zufällig initialisierten permanenten Verbindungen positive Auswirkungen auf die Generalisierung haben und viel größere ConvNet-Baselines sowie das kürzlich vorgeschlagene nichtparametrische Transformationsnetzwerk (NPTN) bei Benchmarks, die das Lernen von Invarianten aus den Daten selbst erzwingen, übertreffen.
Im Gegensatz zu bestehenden bedingten generativen Modellen, die Konditionierungsinformationen durch Verkettung einbeziehen, führen wir eine spezielle Netzwerkkomponente ein, die bedingte Transformationseinheit (CTU), die entwickelt wurde, um die latenten Raumtransformationen zu erlernen, die bestimmten Zielansichten entsprechen. Darüber hinaus wird ein Konsistenzverlustterm definiert, um das Netzwerk zum Erlernen der gewünschten latenten Raumabbildungen zu leiten, ein aufgabenteiliger Decoder wird konstruiert, um die Qualität der generierten Ansichten zu verfeinern, und ein adaptiver Diskriminator wird eingeführt, um den adversen Trainingsprozess zu verbessern. Die Allgemeingültigkeit der vorgeschlagenen Methodik wird anhand von drei verschiedenen Aufgaben demonstriert: Rekonstruktion von Tiefenbildern aus mehreren Blickwinkeln auf realen Händen, Synthese von realen und synthetischen Gesichtern und Rotation starrer Objekte.
Wir beschreiben drei Ansätze, die es einem extrem rechenschwachen eingebetteten Scheduler ermöglichen, eine kleine Anzahl von alternativen Aktivitäten auf der Grundlage der Ressourcenverfügbarkeit zu berücksichtigen.Wir betrachten den Fall, in dem der Scheduler so rechenschwach ist, dass er keine Backtrack-Suche durchführen kann.Die ersten beiden Ansätze kompilieren Ressourcenprüfungen (so genannte Guards) vor, die die Auswahl einer bevorzugten alternativen Aktivität nur dann ermöglichen, wenn schätzungsweise genügend Ressourcen für die Planung der verbleibenden Aktivitäten verfügbar sind. Der letzte Ansatz imitiert das Backtracking, indem er den Scheduler mehrfach mit den alternativen Aktivitäten aufruft. Wir präsentieren eine Evaluierung dieser Techniken anhand von Missionsszenarien (Sol-Typen genannt) aus dem nächsten planetaren Rover der NASA, wo diese Techniken für die Aufnahme in einen Onboard-Scheduler evaluiert werden.
Inspiriert von der Modularität und dem Lebenszyklus biologischer Neuronen stellen wir Continual Learning via Neural Pruning (CLNP) vor, eine neue Methode, die auf lebenslanges Lernen in Modellen mit fester Kapazität abzielt und auf der Beschneidung von Neuronen mit geringer Aktivität basiert.Bei dieser Methode wird ein L1-Regulator verwendet, um das Vorhandensein von Neuronen mit null oder geringer Aktivität zu fördern, deren Verbindungen zu zuvor aktiven Neuronen am Ende des Trainings dauerhaft getrennt werden. Nachfolgende Aufgaben werden mit diesen beschnittenen Neuronen nach der Reinitialisierung trainiert und führen zu keiner Verschlechterung der Leistung der vorherigen Aufgaben.
In diesem Beitrag untersuchen wir empirisch verschiedene Möglichkeiten zur Verbesserung der Leistung von neuronalen Faltungsnetzwerken (CNNs) bei spektralen Audiomerkmalen, insbesondere drei Aspekte des CNN-Designs: die Tiefe des Netzwerks, die Verwendung von Restblöcken zusammen mit der Verwendung von gruppierter Faltung und die globale Aggregation über die Zeit. Der Anwendungskontext ist die Klassifizierung von Sängern und die Einbettung der Gesangsleistung, und wir glauben, dass die Schlussfolgerungen auch für andere Arten der Musikanalyse mit Hilfe von neuronalen Faltungsnetzwerken gelten.Die Ergebnisse zeigen, dass die globale zeitliche Aggregation die Leistung von CNNs am meisten verbessert.Ein weiterer Beitrag dieses Papiers ist die Veröffentlichung eines Datensatzes für Gesangsaufnahmen, der für das Training und die Bewertung verwendet werden kann.
Die Softmax-Funktion wird in der letzten Schicht fast aller bestehenden Sequenz-zu-Sequenz-Modelle für die Spracherzeugung verwendet, ist jedoch in der Regel die am langsamsten zu berechnende Schicht, die die Größe des Vokabulars auf eine Teilmenge der häufigsten Typen beschränkt, und hat einen großen Speicherbedarf. Unsere wichtigsten Neuerungen sind ein neuartiger probabilistischer Verlust und ein Trainings- und Inferenzverfahren, bei dem wir eine Wahrscheinlichkeitsverteilung über vortrainierte Worteinbettungen generieren, anstatt einer Multinomialverteilung über das Vokabular, das durch Softmax erhalten wird. Wir evaluieren diese neue Klasse von Sequenz-zu-Sequenz-Modellen mit kontinuierlichen Ausgaben für die Aufgabe der neuronalen maschinellen Übersetzung und zeigen, dass unsere Modelle eine bis zu 2,5-fache Beschleunigung der Trainingszeit erreichen, während sie in Bezug auf die Übersetzungsqualität mit den State-of-the-Art-Modellen mithalten können und in der Lage sind, sehr große Wortschätze zu verarbeiten, ohne Kompromisse bei der Übersetzungsqualität einzugehen.
Die Korrelationen zwischen den Gewichten innerhalb derselben Schicht können durch Symmetrien in dieser Schicht beschrieben werden, und die Netzwerke verallgemeinern besser, wenn solche Symmetrien gebrochen werden, um die Redundanzen der Gewichte zu reduzieren. Mithilfe einer Zwei-Parameter-Feldtheorie finden wir heraus, dass das Netzwerk solche Symmetrien gegen Ende des Trainings selbst brechen kann, und zwar in einem Prozess, der in der Physik als spontanes Symmetriebrechen bekannt ist, d.h. ein Netzwerk generalisiert sich selbst ohne Benutzereingabe-Schichten, um die Symmetrie zu brechen, sondern durch Kommunikation mit benachbarten Schichten, 2015), zeigen wir, dass die verbleibenden Symmetrien, die die nicht-linearen Schichten überleben, spontan gebrochen werden, basierend auf empirischen Ergebnissen.Der Lagrange für die nicht-linearen und Gewichtsschichten zusammen hat auffallende Ähnlichkeiten mit dem in der Quantenfeldtheorie eines Skalars.Unter Verwendung von Ergebnissen aus der Quantenfeldtheorie zeigen wir, dass unser Rahmen in der Lage ist, viele experimentell beobachtete Phänomene zu erklären, wie z. B. das Training auf zufällige Etiketten mit Null-Fehler (Zhang et al., 2017), den Informationsengpass und den Phasenübergang aus diesem heraus (Shwartz-Ziv & Tishby, 2017), zerschmetterte Gradienten (Balduzzi et al., 2017) und viele mehr.
In den letzten Jahren wurde dank künstlicher tiefer neuronaler Netze ein bemerkenswerter Durchbruch im Bereich der KI erzielt, der zu einem großen Erfolg bei maschinellen Lernaufgaben in den Bereichen Computer Vision, Verarbeitung natürlicher Sprache, Spracherkennung, Malware-Erkennung usw. geführt hat, wobei diese Netze jedoch sehr anfällig für leicht zu erstellende gegnerische Beispiele sind. Die bisher robusteste bekannte Methode ist der so genannte C&W-Angriff [1], aber eine Gegenmaßnahme, das so genannte Fea-ture Squeezing, in Verbindung mit einer Ensemble-Verteidigung hat gezeigt, dass die meisten dieser Angriffe zerstört werden können [6]. In diesem Papier stellen wir eine neue Methode vor, die wir CenteredInitial Attack (CIA) nennen und die zwei Vorteile hat: Erstens stellt sie durch Konstruktion sicher, dass die maximale Störung kleiner als ein vorher festgelegter Schwellenwert ist, ohne den Clipping-Prozess, der die Qualität der Angriffe verschlechtert, und zweitens ist sie robust gegen kürzlich eingeführte Abwehrmaßnahmen wie Feature Squeezing, JPEG-Encoding und sogar gegen ein Ensemble von Abwehrmaßnahmen. Während die Anwendung nicht auf Bilder beschränkt ist, veranschaulichen wir dies anhand von fünf der derzeit besten Klassifizierer des ImageNet-Datensatzes, von denen zwei absichtlich umtrainiert wurden, um robust gegen Angriffe zu sein. Bei einer festen maximalen Störung von nur 1,5 % auf einem beliebigen Pixel überlisten etwa 80 % der (gezielten) Angriffe die Voting-Ensemble-Verteidigung, und fast 100 %, wenn die Störung nur 6 % beträgt.Während dies zeigt, wie schwierig es ist, sich gegen CIA-Angriffe zu verteidigen, gibt der letzte Abschnitt des Papiers einige Leitlinien zur Begrenzung ihrer Auswirkungen.
Die Beantwortung komplexer logischer Abfragen in großen unvollständigen Wissensgraphen (KGs) ist eine fundamentale, aber dennoch herausfordernde Aufgabe. Ein vielversprechender Ansatz zur Lösung dieses Problems ist die Einbettung der KG-Entitäten sowie der Abfrage in einen Vektorraum, so dass die Entitäten, die die Abfrage beantworten, in der Nähe der Abfrage eingebettet sind. Darüber hinaus können bisherige Arbeiten nur Abfragen behandeln, die Konjunktionen ($\wedge$) und Existenzquantoren ($\exists$) verwenden. Die Behandlung von Abfragen mit logischen Disjunktionen ($\vee$) bleibt ein offenes Problem. Hier schlagen wir query2box vor, ein auf Einbettung basierendes Framework zur Argumentation über beliebige Abfragen mit $\wedge$-, $\vee$- und $\exists$-Operatoren in massiven und unvollständigen KGs.Unsere Haupterkenntnis ist, dass Abfragen als Boxen eingebettet werden können (d.h., Wir zeigen, dass Konjunktionen auf natürliche Weise als Schnittpunkte von Boxen dargestellt werden können und beweisen auch ein negatives Ergebnis, dass die Behandlung von Disjunktionen eine Einbettung mit einer Dimension proportional zur Anzahl der KG-Entitäten erfordern würde. Wir zeigen jedoch, dass query2box durch die Umwandlung von Abfragen in eine disjunktive Normalform in der Lage ist, beliebige logische Abfragen mit $\wedge$, $\vee$, $\exists$ in einer skalierbaren Weise zu behandeln. Wir demonstrieren die Effektivität von query2box an zwei großen KGs und zeigen, dass query2box eine relative Verbesserung von bis zu 25% gegenüber dem Stand der Technik erreicht.
Angetrieben durch den Bedarf an parallelisierbaren Hyperparameter-Optimierungsmethoden werden in dieser Arbeit Open-Loop-Suchmethoden untersucht: Sequenzen, die vorbestimmt sind und generiert werden können, bevor eine einzelne Konfiguration evaluiert wird.Beispiele hierfür sind die Gittersuche, die gleichförmige Zufallssuche, Sequenzen mit geringer Diskrepanz und andere Stichprobenverteilungen.Insbesondere schlagen wir die Verwendung von k-determinanten Punktprozessen in der Hyperparameter-Optimierung über die Zufallssuche vor.Im Vergleich zur konventionellen gleichförmigen Zufallssuche, bei der die Hyperparameter-Einstellungen unabhängig voneinander abgetastet werden, fördert ein k-DPP die Vielfalt.  Wir beschreiben einen Ansatz, der Hyperparameter-Suchräume für die effiziente Verwendung mit einem k-DPP transformiert. Darüber hinaus stellen wir einen neuartigen Metropolis-Hastings-Algorithmus vor, der von k-DPPs abtasten kann, die über einen beliebigen Raum definiert sind, aus dem einheitliche Stichproben gezogen werden können, einschließlich Räumen mit einer Mischung aus diskreten und kontinuierlichen Dimensionen oder Baumstruktur.Unsere Experimente zeigen signifikante Vorteile in realistischen Szenarien mit einem begrenzten Budget für das Training überwachter Lerner, egal ob in Serie oder parallel.
Grapheneinbettungstechniken werden zunehmend in einer Vielzahl verschiedener Anwendungen eingesetzt, die das Lernen auf nicht-euklidischen Daten beinhalten. bestehende Grapheneinbettungsmodelle berücksichtigen jedoch entweder keine Knotenattributinformationen während des Trainings oder leiden unter Knotenattributrauschen, was die Genauigkeit beeinträchtigt. In diesem Papier schlagen wir GraphZoom vor, ein mehrstufiges Framework zur Verbesserung der Genauigkeit und Skalierbarkeit von unüberwachten Grapheneinbettungsalgorithmen.GraphZoom führt zunächst eine Graphenfusion durch, um einen neuen Graphen zu erzeugen, der die Topologie des ursprünglichen Graphen und die Knotenattributinformationen effektiv kodiert. GraphZoom ermöglicht es, alle bestehenden Einbettungsmethoden auf den vergröberten Graphen anzuwenden, bevor es die auf der gröbsten Ebene erhaltenen Einbettungen schrittweise zu immer feineren Graphen verfeinert.Wir haben unseren Ansatz an einer Reihe beliebter Graphendatensätze sowohl für transduktive als auch für induktive Aufgaben evaluiert.Unsere Experimente zeigen, dass GraphZoom die Klassifizierungsgenauigkeit erhöht und die Laufzeit im Vergleich zu den modernsten unüberwachten Einbettungsmethoden deutlich reduziert.
Tiefe generative Modelle sind erfolgreich bei der Modellierung kontinuierlicher Daten, aber es bleibt eine Herausforderung, die Repräsentationen für diskrete Strukturen mit formalen Grammatiken und Semantiken zu erfassen, z.B. Computerprogramme und molekulare Strukturen, Inspiriert von der Compiler-Theorie, bei der die Überprüfung von Syntax und Semantik über eine syntaxgeleitete Übersetzung (SDT) erfolgt, schlagen wir einen neuartigen syntaxgeleiteten Variations-Auto-Codierer (SD-VAE) vor, der stochastische Lazy-Attribute einführt und die Offline-SDT-Prüfung in eine on-the-fly generierte Anleitung zur Einschränkung des Decoders umsetzt. Wir evaluieren das vorgeschlagene Modell mit Anwendungen in Programmiersprachen und Molekülen, einschließlich Rekonstruktion und Programm/Molekül-Optimierung. Die Ergebnisse zeigen die Effektivität bei der Einbeziehung von syntaktischen und semantischen Einschränkungen in diskrete generative Modelle, die deutlich besser ist als aktuelle state-of-the-art Ansätze.
Mit der jüngsten Verfügbarkeit großer annotierter Daten ist es möglich geworden, komplexe Modelle wie neuronale Netze für die Inferenz natürlicher Sprache (NLI) zu trainieren, die eine Spitzenleistung erreicht haben.Obwohl es relativ große annotierte Daten gibt, können Maschinen alles Wissen, das für die Durchführung von NLI erforderlich ist, aus den Daten lernen? Wenn nicht, wie können NLI-Modelle von externem Wissen profitieren und wie kann man NLI-Modelle bauen, um dieses Wissen zu nutzen? in diesem Beitrag versuchen wir, diese Fragen zu beantworten, indem wir die modernsten neuronalen Modelle für die Inferenz natürlicher Sprache mit externem Wissen anreichern. wir zeigen, dass die vorgeschlagenen Modelle mit externem Wissen den Stand der Technik auf dem Stanford Natural Language Inference (SNLI)-Datensatz weiter verbessern.
Hohe Intra-Klassen-Vielfalt und Inter-Klassen-Ähnlichkeit ist ein Merkmal der Fernerkundung Szene Bilddatensätze derzeit erhebliche Schwierigkeiten für Deep-Learning-Algorithmen auf Klassifizierung tasks.To verbessern Genauigkeit, post-classificationmethods wurden vorgeschlagen, für die Glättung Ergebnisse der Modellvorhersagen.However, diese Ansätze erfordern ein zusätzliches neuronales Netzwerk, um die Glättung Betrieb, der Overhead für die task.We hinzufügen, schlagen einen Ansatz, der das Lernen tief Features direkt über benachbarte Szene Bilder, ohne die Verwendung eines Cleanup-Modell. Unser Ansatz verwendet ein siamesisches Netzwerk, um die Unterscheidungskraft von neuronalen Faltungsnetzwerken auf einem Paar benachbarter Bilder zu verbessern und nutzt dann die semantische Kohärenz zwischen diesem Paar, um den Merkmalsvektor des Bildes, für das wir ein Label vorhersagen wollen, zu bereichern.Empirische Ergebnisse zeigen, dass dieser Ansatz eine brauchbare Alternative zu bestehenden Methoden darstellt. Zum Beispiel verbesserte unser Modell die Vorhersagegenauigkeit um einen Prozentpunkt und verringerte den mittleren quadratischen Fehlerwert um 0,02 gegenüber dem Ausgangswert bei einer Aufgabe zur Schätzung der Krankheitsdichte, wobei diese Leistungssteigerungen mit den Ergebnissen bestehender Nachklassifizierungsmethoden vergleichbar sind, und zwar ohne Implementierungsaufwand.
Der Diskretisierungsfehler wird noch größer, wenn die spärlichen Daten unregelmäßig verteilt sind, so dass die Daten auf einem unstrukturierten Gitter definiert sind, was es schwierig macht, Deep-Learning-Modelle zu bauen, um physikalische Beobachtungen auf dem unstrukturierten Gitter zu verarbeiten. In diesem Papier schlagen wir eine neuartige Architektur mit dem Namen Physics-aware Difference Graph Networks (PA-DGN) vor, die Nachbarschaftsinformationen ausnutzt, um endliche Differenzen zu lernen, die von physikalischen Gleichungen inspiriert sind. PA-DGN nutzt darüber hinaus datengesteuertes End-to-End-Lernen, um die zugrunde liegenden dynamischen Beziehungen zwischen den räumlichen und zeitlichen Unterschieden in gegebenen Beobachtungen zu entdecken.
Hierarchisches Reinforcement Learning (HRL) hat das Versprechen, die Fähigkeiten von RL-Agenten durch den Betrieb auf verschiedenen Ebenen der zeitlichen Abstraktion zu verbessern. Trotz des Erfolges der jüngsten Arbeiten im Umgang mit inhärenter Nicht-Stationarität und Stichprobenkomplexität, bleibt es schwierig, auf ungesehene Umgebungen zu verallgemeinern und verschiedene Ebenen der Politik auf andere Agenten zu übertragen. In dieser Arbeit schlagen wir eine neuartige HRL-Architektur vor, Hierarchical Decompositional Reinforcement Learning (HiDe), die eine Zerlegung der hierarchischen Schichten in unabhängige Teilaufgaben erlaubt und gleichzeitig ein gemeinsames Training aller Schichten in einer durchgängigen Weise ermöglicht.Die wichtigste Erkenntnis ist die Kombination einer Kontrollpolitik auf einer unteren Ebene mit einer bildbasierten Planungspolitik auf einer höheren Ebene.Wir evaluieren unsere Methode an verschiedenen komplexen kontinuierlichen Kontrollaufgaben für die Navigation und zeigen, dass eine Verallgemeinerung über Umgebungen hinweg und eine Übertragung von Politiken auf höherer Ebene erreicht werden kann.Siehe Videos https://sites.google.com/view/hide-rl
Lautliche Korrespondenzmuster spielen eine entscheidende Rolle für die linguistische Rekonstruktion.Linguisten nutzen sie zum Nachweis von Sprachverwandtschaft, zur Rekonstruktion von Urformen und für die klassische phylogenetische Rekonstruktion auf der Basis gemeinsamer Innovationen.Kognate Wörter, die nicht den erwarteten Mustern entsprechen, können auf verschiedene Arten von Ausnahmen im Lautwandel hinweisen, wie z.B. Analogie oder Assimilation häufiger Wörter.Hier stellen wir ein automatisches Verfahren für die Inferenz von lautlichen Korrespondenzmustern über mehrere Sprachen hinweg vor, das auf einem Netzwerkansatz basiert. Die Aufgabe, alle kompatiblen Korrespondenzmengen abzuleiten, kann dann als das bekannte Minimum-Clique-Cover-Problem der Graphentheorie behandelt werden, bei dem es im Wesentlichen darum geht, den Graphen in die kleinste Anzahl von Cliquen zu unterteilen, in denen jeder Knoten durch genau eine Clique repräsentiert wird. Die resultierenden Partitionen stellen alle Korrespondenzmuster dar, die für einen gegebenen Datensatz abgeleitet werden können. Durch den Ausschluss von Mustern, die nur in einigen wenigen Kognatensätzen vorkommen, kann der Kern regelmäßig wiederkehrender Klangkorrespondenzen abgeleitet werden.Auf der Grundlage dieser Idee wird eine Methode zur automatischen Erkennung von Korrespondenzmustern vorgestellt, die als Teil einer Python-Bibliothek implementiert ist, die das Papier ergänzt. Um die Nützlichkeit der Methode zu veranschaulichen, werden verschiedene Tests vorgestellt und konkrete Beispiele für die Ausgabe der Methode geliefert.Zusätzlich zum Quellcode wird die Studie durch ein kurzes interaktives Tutorial ergänzt, das die Anwendung der neuen Methode und die Überprüfung ihrer Ergebnisse veranschaulicht.
Wir beschreiben Kernel RNN Learning (KeRNL), eine rangreduzierte, zeitliche Eignungsspur-basierte Annäherung an Backpropagation through Time (BPTT) für das Training rekurrenter neuronaler Netze (RNNs), die bei Aufgaben mit langer Zeitabhängigkeit eine konkurrenzfähige Leistung zu BPTT bietet.Die Annäherung ersetzt einen Rang-4-Gradiententensor, der beschreibt, wie vergangene Aktivierungen versteckter Einheiten den aktuellen Zustand beeinflussen, durch ein einfaches rangreduziertes Produkt aus einem Empfindlichkeitsgewicht und einer zeitlichen Eignungsspur. Die Regel stellt einen weiteren Schritt in Richtung einer biologisch plausiblen oder neuronal inspirierten ML dar, mit geringerer Komplexität in Form von entspannten architektonischen Anforderungen (keine symmetrischen Rücklaufgewichte), einem geringeren Speicherbedarf (keine Entfaltung und Speicherung von Zuständen über die Zeit) und einer kürzeren Rückkopplungszeit.
Wir stellen Deep Graph Infomax (DGI) vor, einen allgemeinen Ansatz für das Lernen von Knotenrepräsentationen in graphenstrukturierten Daten auf unüberwachte Weise. DGI basiert auf der Maximierung der gegenseitigen Information zwischen Patch-Repräsentationen und entsprechenden High-Level-Zusammenfassungen von Graphen - beides abgeleitet aus etablierten Graphen-Faltungsnetzwerk-Architekturen. Die gelernten Patch-Repräsentationen fassen Teilgraphen zusammen, die um die interessierenden Knoten zentriert sind, und können daher für nachgelagerte knotenweise Lernaufgaben wiederverwendet werden. Im Gegensatz zu den meisten früheren Ansätzen für unüberwachtes Lernen mit GCNs verlässt sich DGI nicht auf Random-Walk-Ziele und ist sowohl auf transduktive als auch auf induktive Lernkonfigurationen anwendbar.Wir zeigen eine konkurrenzfähige Leistung bei einer Vielzahl von Knotenklassifizierungs-Benchmarks, die manchmal sogar die Leistung des überwachten Lernens übertrifft.
In vielen Umgebungen bringt nur eine kleine Teilmenge aller Zustände eine hohe Belohnung.  In diesen Fällen liefern nur wenige der Interaktionen mit der Umgebung ein relevantes Lernsignal, so dass wir bevorzugt auf diese hochbelohnten Zustände und die wahrscheinlichen Trajektorien, die zu ihnen führen, trainieren sollten. Zu diesem Zweck plädieren wir für die Verwendung eines Rückverfolgungsmodells, das die vorangehenden Zustände vorhersagt, die in einem bestimmten Zustand mit hoher Belohnung enden.  Wir können ein Modell trainieren, das ausgehend von einem hochwertigen Zustand (oder einem Zustand, der als hochwertig eingeschätzt wird) vorhersagt und abtastet, welche (Zustands-, Aktions-)Tupel zu diesem hochwertigen Zustand geführt haben könnten Diese Spuren von (Zustands-, Aktions-)Paaren, die wir als Recall Traces bezeichnen und die von diesem Backtracking-Modell ausgehend von einem hochwertigen Zustand abgetastet werden, sind informativ, da sie in guten Zuständen enden, und daher können wir diese Spuren zur Verbesserung einer Strategie verwenden. Wir liefern eine Variationsinterpretation für diese Idee und einen praktischen Algorithmus, in dem das Backtracking-Modell aus einer approximativen Posterior-Verteilung über Trajektorien, die zu großen Belohnungen führen, Stichproben zieht.  
Präpositionen gehören zu den häufigsten Wörtern und eine gute Repräsentation von Präpositionen ist von großem syntaktischen und semantischen Interesse in der Computerlinguistik.Bestehende Methoden zur Repräsentation von Präpositionen behandeln Präpositionen entweder als Inhaltswörter (z.B, word2vec und GloVe) oder hängen stark von externen linguistischen Ressourcen ab, einschließlich syntaktischem Parsing, Trainingsaufgaben und datensatzspezifischen Repräsentationen.In dieser Arbeit verwenden wir Wort-Dreifachzählungen (eines der Wörter ist eine Präposition), um die Interaktion der Präposition mit ihrem Kopf und ihren Kindern zu erfassen.Präpositionale Einbettungen werden über Tensordiskompositionen auf einem großen, unmarkierten Korpus abgeleitet.  Wir zeigen eine neue Geometrie mit Hadamard-Produkten auf und demonstrieren empirisch ihre Nützlichkeit bei der Paraphrasierung von Phrasalverben. Darüber hinaus werden unsere Präpositionaleinbettungen als einfache Merkmale für zwei anspruchsvolle nachgelagerte Aufgaben verwendet: Präpositionsauswahl und Disambiguierung von Präpositionalanhängen.  
Eine vielversprechende Klasse von generativen Modellen bildet Punkte von einer einfachen Verteilung auf eine komplexe Verteilung durch ein invertierbares neuronales Netzwerk ab.   Das wahrscheinlichkeitsbasierte Training dieser Modelle erfordert die Einschränkung ihrer Architekturen, um eine kostengünstige Berechnung der Jacobi-Determinanten zu ermöglichen.  Alternativ kann die Jacobi-Spur verwendet werden, wenn die Transformation durch eine gewöhnliche Differentialgleichung spezifiziert ist. In diesem Papier verwenden wir den Hutchinson-Spur-Schätzer, um eine skalierbare, unverzerrte Schätzung der Log-Dichte zu erhalten.  Das Ergebnis ist ein zeitkontinuierliches, invertierbares generatives Modell mit unverzerrter Dichteschätzung und One-Pass-Sampling, das unbeschränkte neuronale Netzwerkarchitekturen erlaubt. Wir demonstrieren unseren Ansatz an hochdimensionaler Dichteschätzung, Bilderzeugung und Variationsinferenz und erreichen den Stand der Technik bei exakten Likelihood-Methoden mit effizientem Sampling.
Unser Ansatz, Generative Feature Matching Networks (GFMN), nutzt vortrainierte neuronale Netze wie Autoencoder und ConvNet-Klassifikatoren zur Merkmalsextraktion. Wir führen eine Vielzahl von Experimenten mit verschiedenen anspruchsvollen Datensätzen, einschließlich ImageNet, durch. Unsere experimentellen Ergebnisse zeigen, dass unser Ansatz aufgrund der Ausdruckskraft der Merkmale von vortrainierten ImageNet-Klassifikatoren, selbst durch den bloßen Abgleich von Statistiken erster Ordnung, modernste Ergebnisse für anspruchsvolle Benchmarks wie CIFAR10 und STL10 erzielen kann.
Wir schlagen eine neuartige Architektur für die k-shot-Klassifikation auf dem Omniglot-Datensatz vor, die auf prototypischen Netzwerken aufbaut und deren Architektur wir als prototypische Gauß-Netzwerke bezeichnen. prototypische Netzwerke lernen eine Karte zwischen Bildern und Einbettungsvektoren und verwenden deren Clustering für die Klassifikation. In unserem Modell wird ein Teil des Encoder-Outputs als eine Schätzung des Vertrauensbereichs über den Einbettungspunkt interpretiert und als Gaußsche Kovarianzmatrix ausgedrückt, und unser Netzwerk konstruiert dann eine richtungs- und klassenabhängige Distanzmetrik auf dem Einbettungsraum, wobei die Unsicherheiten einzelner Datenpunkte als Gewichte verwendet werden. Wir zeigen, dass Gaußsche prototypische Netzwerke eine bevorzugte Architektur gegenüber prototypischen Vanille-Netzwerken mit einer äquivalenten Anzahl von Parametern sind.Wir berichten über Ergebnisse, die mit der State-of-the-Art-Performance in der 1-Shot- und 5-Shot-Klassifikation sowohl im 5-Wege- als auch im 20-Wege-Regime auf dem Omniglot-Datensatz übereinstimmen.Wir erforschen das künstliche Down-Sampling eines Teils der Bilder im Trainingssatz, was unsere Leistung verbessert.Unsere Experimente führen uns daher zu der Hypothese, dass Gaußsche prototypische Netzwerke in weniger homogenen, verrauschten Datensätzen, die in realen Anwendungen üblich sind, besser abschneiden könnten.
Wir zeigen, dass die Ausgabe eines (Rest-)CNN mit einem geeigneten Prior über die Gewichte und Verzerrungen ein GP im Grenzfall von unendlich vielen Faltungsfiltern ist, und erweitern damit ähnliche Ergebnisse für dichte Netze. für ein CNN kann der äquivalente Kernel genau berechnet werden und hat im Gegensatz zu "tiefen Kerneln" nur sehr wenige Parameter: nur die Hyperparameter des ursprünglichen CNN. Darüber hinaus zeigen wir, dass dieser Kernel zwei Eigenschaften besitzt, die eine effiziente Berechnung ermöglichen; die Kosten für die Auswertung des Kernels für ein Bildpaar sind vergleichbar mit einem einzigen Vorwärtsdurchlauf durch das ursprüngliche CNN mit nur einem Filter pro Schicht.Der einem 32-Schicht-ResNet äquivalente Kernel erzielt einen Klassifikationsfehler von 0,84% bei MNIST, ein neuer Rekord für GP mit einer vergleichbaren Anzahl von Parametern.
Im Gegensatz zu früheren Methoden, die die Architektur des neuronalen Netzes, die Beschneidungspolitik und die Quantisierungspolitik separat optimieren, optimieren wir sie gemeinsam in einer End-to-End-Weise.Um mit dem größeren Designraum umzugehen, trainieren wir einen quantisierungsbewussten Genauigkeitsprädikator, der in die evolutionäre Suche eingespeist wird, um die beste Anpassung auszuwählen.Wir generieren zunächst einen großen Datensatz von <NN-Architektur, ImageNet-Genauigkeit>-Paaren, ohne jede Architektur zu trainieren, sondern indem wir ein vereinheitlichtes Supernet abfragen. Dann verwenden wir diese Daten, um einen Genauigkeitsprädikator ohne Quantisierung zu trainieren, und nutzen die Prädikator-Transfer-Technik, um einen quantisierungsfähigen Prädikator zu erhalten, der die Zeit für die Feinabstimmung nach der Quantisierung reduziert.Ausführliche Experimente mit ImageNet zeigen die Vorteile der End-to-End-Methodik: Sie erreicht die gleiche Genauigkeit (75. 1%) als das ResNet34 Float-Modell und spart dabei 2,2× BitOps im Vergleich zum 8-Bit-Modell; wir erreichen die gleiche Genauigkeit wie MobileNetV2+HAQ und erzielen gleichzeitig 2×/1,3× Latenz-/Energieeinsparungen; die End-to-End-Optimierung übertrifft separate Optimierungen mit ProxylessNAS+AMC+HAQ um 2,3% Genauigkeit und reduziert gleichzeitig GPU-Stunden und CO2-Emissionen um Größenordnungen.
Ein weit verbreitetes Merkmal verteilter Optimierungstechniken ist die Anforderung, dass alle Knoten ihre zugewiesenen Aufgaben in jeder Berechnungsepoche abschließen müssen, bevor das System mit der nächsten Epoche fortfahren kann.In solchen Einstellungen können langsame Knoten, so genannte Nachzügler, den Fortschritt stark verlangsamen.Um die Auswirkungen von Nachzüglern abzuschwächen, schlagen wir eine verteilte Online-Optimierungsmethode vor, die Anytime Minibatch genannt wird.Bei diesem Ansatz erhalten alle Knoten eine feste Zeit, um die Gradienten von so vielen Datenproben wie möglich zu berechnen. Die Arbeiter erhalten dann eine feste Kommunikationszeit, um ihre Minibatch-Gradienten über mehrere Konsensrunden zu mitteln, die dann verwendet werden, um die primären Variablen über duale Mittelung zu aktualisieren.Anytime Minibatch verhindert, dass Nachzügler das System aufhalten, ohne die Arbeit zu verschwenden, die Nachzügler erledigen können. Unsere numerischen Ergebnisse zeigen, dass unser Ansatz in Amazon EC2 bis zu 1,5 Mal schneller ist und bis zu fünf Mal schneller, wenn es größere Schwankungen in der Rechenknotenleistung gibt.
Viele Bildklassifizierer für maschinelles Lernen sind anfällig für gegnerische Angriffe, d. h. Eingaben mit Störungen, die darauf abzielen, absichtlich eine Fehlklassifizierung auszulösen. aktuelle gegnerische Methoden verändern direkt die Pixelfarben und bewerten sie anhand von Pixelnormkugeln: Pixelstörungen, die kleiner als eine bestimmte Größe sind, entsprechend einer Messnorm. Diese Bewertung hat jedoch nur einen begrenzten praktischen Nutzen, da Störungen im Pixelraum nicht den zugrunde liegenden realen Phänomenen der Bildentstehung entsprechen, die zu ihnen führen, und hat keine Sicherheitsmotivation.Pixel in natürlichen Bildern sind Messungen von Licht, das mit der Geometrie einer physischen Szene interagiert hat. Ein Beitrag, den wir vorstellen, ist ein physikalisch basierter differenzierbarer Renderer, der es uns ermöglicht, Pixelgradienten in den parametrischen Raum der Beleuchtung und Geometrie zu übertragen. Unser Ansatz ermöglicht physikalisch basierte gegnerische Angriffe, und unser differenzierbarer Renderer nutzt Modelle aus der interaktiven Rendering-Literatur, um die Leistungs- und Genauigkeitskompromisse auszugleichen, die für einen speichereffizienten und skalierbaren gegnerischen Datenerweiterungsworkflow erforderlich sind.
Alternativen zu rekurrenten neuronalen Netzen, insbesondere Architekturen, die auf Aufmerksamkeit oder Faltung beruhen, haben für die Verarbeitung von Eingabesequenzen an Bedeutung gewonnen.Trotz ihrer Relevanz sind die rechnerischen Eigenschaften dieser Alternativen noch nicht vollständig erforscht worden.Wir untersuchen die Rechenleistung von zwei der paradigmatischsten Architekturen, die diese Mechanismen veranschaulichen: der Transformer (Vaswani et al., Wir zeigen, dass beide Modelle ausschließlich aufgrund ihrer Fähigkeit, interne dichte Repräsentationen der Daten zu berechnen und darauf zuzugreifen, Turing-vollständig sind.Insbesondere benötigen weder der Transformer noch die Neural GPU Zugriff auf einen externen Speicher, um Turing-vollständig zu werden.Unsere Studie zeigt auch einige minimale Sätze von Elementen, die erforderlich sind, um diese Vollständigkeitsergebnisse zu erhalten.
Da es für ein lernendes System in der Regel schwierig ist, seltene Ereignisse korrekt vorherzusagen, und dies auch für Segmentierungsalgorithmen gilt, möchten wir ein Alarmsystem entwickeln, das Alarm schlägt, wenn das Segmentierungsergebnis möglicherweise nicht zufriedenstellend ist. Eine plausible Lösung besteht darin, die Segmentierungsergebnisse in einen niedrigdimensionalen Merkmalsraum zu projizieren und dann Klassifikatoren/Regressoren im Merkmalsraum zu erlernen, um die Qualität der Segmentierungsergebnisse vorherzusagen.In diesem Papier bilden wir den Merkmalsraum unter Verwendung des Formmerkmals, das eine starke Vorabinformation ist, die zwischen verschiedenen Daten geteilt wird, so dass es in der Lage ist, die Qualität der Segmentierungsergebnisse bei verschiedenen Segmentierungsalgorithmen auf verschiedenen Datensätzen vorherzusagen. Das Formmerkmal eines Segmentierungsergebnisses wird mit dem Wert der Verlustfunktion erfasst, wenn das Segmentierungsergebnis mit einem Variational Auto-Encoder (VAE) getestet wird. der VAE wird nur mit den Grundwahrheitsmasken trainiert, daher werden schlechte Segmentierungsergebnisse mit schlechten Formen zu seltenen Ereignissen für den VAE und führen zu einem großen Verlustwert. durch Ausnutzung dieser Tatsache ist der VAE in der Lage, alle Arten von Formen zu erkennen, die aus der Verteilung normaler Formen in der Grundwahrheit (GT) herausfallen. Schließlich lernen wir die Darstellung im eindimensionalen Merkmalsraum, um die Qualitäten der Segmentierungsergebnisse vorherzusagen. Wir bewerten unser Alarmsystem auf mehreren aktuellen Segmentierungsalgorithmen für die medizinische Segmentierungsaufgabe. Die Segmentierungsalgorithmen führen unterschiedlich auf verschiedenen Datensätzen, aber unser System bietet durchweg zuverlässige Vorhersage über die Qualitäten der Segmentierungsergebnisse.
Die linearen Transformationen in konvergenten tiefen Netzen zeigen einen schnellen Eigenwertabfall. Die Verteilung der Eigenwerte sieht wie eine Heavy-tail-Verteilung aus, bei der die überwiegende Mehrheit der Eigenwerte klein, aber nicht wirklich Null ist und nur wenige Spitzen großer Eigenwerte existieren. Wir verwenden einen stochastischen Approximator, um Histogramme der Eigenwerte zu erzeugen.
Die Erfassung weitreichender Merkmalsbeziehungen ist ein zentrales Thema bei Faltungsneuronalen Netzen (CNNs), weshalb Versuche, ein durchgängig trainierbares Aufmerksamkeitsmodul in CNNs zu integrieren, weit verbreitet sind.Das Hauptziel dieser Arbeiten ist die Anpassung von Merkmalskarten unter Berücksichtigung der Raum-Kanal-Korrelation innerhalb einer Faltungsschicht. In dieser Arbeit konzentrieren wir uns auf die Modellierung der Beziehungen zwischen den Schichten und schlagen eine neuartige Struktur vor, das "Recurrent Layer Attention network", das die Hierarchie der Merkmale in rekurrenten neuronalen Netzen (RNNs) speichert, die sich gleichzeitig mit CNNs ausbreiten und die Merkmalsvolumina aller Schichten adaptiv skalieren.Wir stellen außerdem mehrere strukturelle Derivate vor, um die Kompatibilität mit aktuellen Aufmerksamkeitsmodulen und die Erweiterbarkeit des vorgeschlagenen Netzwerks zu demonstrieren.Für das semantische Verständnis der gelernten Merkmale visualisieren wir auch Zwischenschichten und zeichnen die Kurve der Schichtskalierungskoeffizienten (d.h. die Schichtaufmerksamkeit), Das rekurrente Layer-Attention-Netzwerk erreicht eine signifikante Leistungssteigerung, die eine leichte Erhöhung der Parameter in einer Bildklassifizierungsaufgabe mit dem CIFAR- und ImageNet-1K 2012-Datensatz und einer Objekterkennungsaufgabe mit dem Microsoft COCO 2014-Datensatz erfordert.
Biologische neuronale Netze (BNNs) zeichnen sich durch schnelles Lernen aus, was bedeutet, dass sie hochinformative Merkmale extrahieren. Insbesondere das Insektengeruchsnetz lernt neue Gerüche sehr schnell, und zwar durch drei Schlüsselelemente: Eine konkurrierende Inhibitionsschicht; randomisierte, spärliche Konnektivität in eine hochdimensionale, spärliche plastische Schicht; und Hebbian-Updates der synaptischen Gewichte.In dieser Arbeit setzen wir MothNet, ein Computermodell des Mottengeruchsnetzes, als automatischen Merkmalsgenerator ein.als Front-End-Präprozessor angebracht, liefern die Readout-Neuronen von MothNet neue Merkmale, die von den ursprünglichen Merkmalen abgeleitet sind, zur Verwendung durch Standard-ML-Klassifikatoren. Diese "Insekten-Cyborgs" (teils BNN, teils ML-Methode) haben eine signifikant bessere Leistung als ML-Methoden allein auf vektorisierten MNIST- und Omniglot-Datensätzen, indem sie den Fehlerdurchschnitt des Testsatzes um 20% bis 55% reduzieren. Der MothNet-Merkmal-Generator übertrifft auch andere Merkmal-Generierungsmethoden, einschließlich PCA, PLS und NNs, erheblich.
Wir stellen einen Ansatz für jederzeitige Vorhersagen in tiefen neuronalen Netzen (DNNs) vor: Für jede Testprobe erzeugt ein jederzeitiger Prädiktor schnell ein grobes Ergebnis und verfeinert es dann so lange, bis das Testzeit-Rechenbudget erschöpft ist. In dieser Arbeit untersuchen wir eine Erweiterung von Feed-Forward-Netzwerken, um beliebige neuronale Netze (ANNs) mit Hilfe von Hilfsvorhersagen und Verlusten zu bilden, und weisen auf einen blinden Fleck in den jüngsten Studien über solche ANNs hin: die Bedeutung einer hohen Endgenauigkeit. Wir zeigen anhand mehrerer Erkennungsdatensätze und -architekturen, dass wir durch nahezu optimale Endvorhersagen in kleinen Modellen die Geschwindigkeit von großen Modellen verdoppeln können, um das entsprechende Genauigkeitsniveau zu erreichen, und zwar durch einfache Gewichtung von Verlusten, die während des Trainings oszillieren.
In jüngster Zeit werden kritische Aspekte wie Versuchsplanung und Bildprioritäten durch tiefe neuronale Netze optimiert, die durch Abrollen von Iterationen klassischer physikbasierter Rekonstruktionen gebildet werden (so genannte physikbasierte Netze). In dieser Arbeit schlagen wir ein speichereffizientes Lernverfahren vor, das die Reversibilität der Netzwerkschichten ausnutzt, um ein datengesteuertes Design für die computergestützte Bildgebung in großem Maßstab zu ermöglichen. Wir demonstrieren die Praxistauglichkeit unserer Methode an zwei großen Systemen: superauflösende optische Mikroskopie und Mehrkanal-Magnetresonanzbildgebung.
Existierende Multi-Agent Reinforcement Learning (MARL)-Kommunikationsmethoden haben sich auf eine vertrauenswürdige dritte Partei (TTP) verlassen, um Belohnungen an Agenten zu verteilen, was sie in Peer-to-Peer-Umgebungen unanwendbar macht.Dieses Papier schlägt eine Belohnungsverteilung unter Verwendung von {\em Neuron as an Agent} (NaaA) in MARL ohne TTP mit zwei Schlüsselideen: (i) Belohnungsverteilung zwischen Agenten und (ii) Auktionstheorie.Auktionstheorie wird eingeführt, weil die Belohnungsverteilung zwischen Agenten für die Optimierung unzureichend ist.Agenten in NaaA maximieren ihre Gewinne (die Differenz zwischen Belohnung und Kosten) und, als theoretisches Ergebnis, wird gezeigt, dass der Auktionsmechanismus Agenten autonom kontrafaktische Renditen als die Werte anderer Agenten bewerten lässt. Schließlich bestätigen numerische Experimente (eine Ein-Agenten-Umgebung von OpenAI Gym und eine Multi-Agenten-Umgebung von ViZDoom), dass die Optimierung des NaaA-Rahmens zu einer besseren Leistung beim Reinforcement Learning führt.
Eine Vergrößerung des Modells beim Vortraining von Repräsentationen natürlicher Sprache führt oft zu einer verbesserten Leistung bei nachgelagerten Aufgaben, aber ab einem gewissen Punkt wird eine weitere Vergrößerung des Modells aufgrund von GPU/TPU-Speicherbeschränkungen, längeren Trainingszeiten und unerwarteter Modellverschlechterung schwieriger.Um diese Probleme anzugehen, stellen wir zwei Techniken zur Parameterreduktion vor, die den Speicherverbrauch senken und die Trainingsgeschwindigkeit von BERT erhöhen. Wir verwenden auch einen selbstüberwachten Verlust, der sich auf die Modellierung der Kohärenz zwischen den Sätzen konzentriert, und zeigen, dass er bei nachgelagerten Aufgaben mit Eingaben aus mehreren Sätzen konsistent hilft. Als Ergebnis erzielt unser bestes Modell neue Spitzenergebnisse bei den GLUE-, RACE- und SQuAD-Benchmarks, während es im Vergleich zu BERT-large weniger Parameter hat.
Gradient Boosting Trees, Support Vector Machine, Random Forest und Logistic Regression werden typischerweise für Klassifizierungsaufgaben auf tabellarischen Daten verwendet. Die jüngste Arbeit der Super Characters-Methode, die zweidimensionale Worteinbettungen verwendet, erzielte Spitzenergebnisse bei Textklassifizierungsaufgaben, was das Potenzial dieses neuen Ansatzes zeigt. In diesem Papier schlagen wir die SuperTML-Methode vor, die die Idee der Super Characters-Methode und der zweidimensionalen Einbettungen aufgreift, um das Problem der Klassifizierung von Tabellendaten zu lösen. Für jede Eingabe von Tabellendaten werden die Merkmale zunächst in zweidimensionale Einbettungen wie ein Bild projiziert, und dann wird dieses Bild in fein abgestimmte ImageNet CNN-Modelle für die Klassifizierung eingespeist.Experimentelle Ergebnisse haben gezeigt, dass die vorgeschlagene SuperTML-Methode sowohl bei großen als auch bei kleinen Datensätzen Spitzenergebnisse erzielt hat.
Obwohl diese Bereiche einen gemeinsamen Ursprung haben, haben sie sich weitgehend unabhängig voneinander entwickelt.Wir skizzieren Verbindungen und Kontraste zwischen diesen Bereichen und nutzen ihre Beziehungen, um neue Parallelen zwischen maschinellem Lernen und Neurowissenschaften zu identifizieren.Wir diskutieren dann spezifische Grenzen an dieser Schnittstelle: Backpropagation, normalisierende Flüsse und Aufmerksamkeit, mit gegenseitigem Nutzen für beide Bereiche.
Adaptive Optimierungsmethoden wie AdaGrad, RMSprop und Adam wurden vorgeschlagen, um einen schnellen Trainingsprozess mit einem elementweisen Skalierungsterm auf Lernraten zu erreichen. Obwohl sie vorherrschend sind, wird beobachtet, dass sie im Vergleich zu SGD schlecht verallgemeinern oder sogar aufgrund von instabilen und extremen Lernraten nicht konvergieren.Jüngste Arbeiten haben einige Algorithmen wie AMSGrad vorgeschlagen, um dieses Problem anzugehen, aber sie konnten keine beträchtliche Verbesserung gegenüber den bestehenden Methoden erreichen.In unserem Beitrag zeigen wir, dass extreme Lernraten zu einer schlechten Leistung führen können. Wir bieten neue Varianten von Adam und AMSGrad, genannt AdaBound bzw. AMSBound, die dynamische Grenzen für die Lernraten verwenden, um einen allmählichen und sanften Übergang von adaptiven Methoden zu SGD zu erreichen, und erbringen einen theoretischen Nachweis der Konvergenz.Wir führen außerdem Experimente mit verschiedenen populären Aufgaben und Modellen durch, was in früheren Arbeiten oft unzureichend ist. Die experimentellen Ergebnisse zeigen, dass die neuen Varianten die Generalisierungslücke zwischen adaptiven Methoden und SGD beseitigen und gleichzeitig eine höhere Lerngeschwindigkeit zu Beginn des Trainings beibehalten können, was insbesondere bei komplexen tiefen Netzen eine signifikante Verbesserung gegenüber den Vorbildern darstellt.Die Implementierung des Algorithmus ist unter https://github.com/Luolc/AdaBound zu finden.
Ein wichtiges Problem, das sich beim Reinforcement Learning und bei Monte-Carlo-Methoden stellt, ist die Schätzung von Größen, die durch die stationäre Verteilung einer Markov-Kette definiert sind.In vielen realen Anwendungen ist der Zugang zum zugrunde liegenden Übergangsoperator auf einen festen Satz von Daten beschränkt, die bereits gesammelt wurden, ohne dass eine zusätzliche Interaktion mit der Umgebung möglich ist.Wir zeigen, dass eine konsistente Schätzung auch in diesem Szenario möglich ist und dass eine effektive Schätzung in wichtigen Anwendungen immer noch erreicht werden kann. Unser Ansatz basiert auf der Schätzung eines Verhältnisses, das die Diskrepanz zwischen der stationären und der empirischen Verteilung korrigiert, abgeleitet aus den grundlegenden Eigenschaften der stationären Verteilung, und auf der Nutzung von Umformulierungen von Einschränkungen, die auf der Minimierung der Variationsdivergenz beruhen.der resultierende Algorithmus, GenDICE, ist einfach und effektiv.wir beweisen die Konsistenz der Methode unter allgemeinen Bedingungen, liefern eine detaillierte Fehleranalyse und demonstrieren eine starke empirische Leistung bei Benchmark-Aufgaben, einschließlich des Offline-PageRank und der Bewertung von Offline-Politiken.
Die Stärke neuronaler Netze liegt in ihrer Fähigkeit, sich auf unbekannte Daten zu verallgemeinern, doch die Gründe für dieses Phänomen sind nach wie vor schwer fassbar.Zahlreiche rigorose Versuche wurden unternommen, um die Verallgemeinerung zu erklären, aber die verfügbaren Grenzen sind immer noch recht locker, und die Analyse führt nicht immer zu einem wirklichen Verständnis.Das Ziel dieser Arbeit ist es, die Verallgemeinerung intuitiver zu machen.Mit Hilfe von Visualisierungsmethoden diskutieren wir das Geheimnis der Verallgemeinerung, die Geometrie von Verlustlandschaften und wie der Fluch (oder eher der Segen) der Dimensionalität Optimierer dazu bringt, sich in Minima einzurichten, die sich gut verallgemeinern.
Wir argumentieren, dass Symmetrie eine wichtige Überlegung ist, um das Problem der Systematizität anzugehen, und untersuchen zwei Formen der Symmetrie, die für symbolische Prozesse relevant sind. Wir implementieren diesen Ansatz in Form einer Faltung und zeigen, dass er für eine effektive Generalisierung in drei Spielzeugproblemen verwendet werden kann: Regellernen, Komposition und Grammatiklernen.
Wichtige äquatoriale Klimaphänomene wie QBO und ENSO konnten nie angemessen als deterministische Prozesse erklärt werden, obwohl neuere Forschungen immer mehr Hinweise auf vorhersagbares Verhalten zeigen.Diese Studie wendet die grundlegenden Laplace-Gezeitengleichungen mit vereinfachenden Annahmen entlang des Äquators an - d.h. keine Corioliskraft und eine Annäherung an kleine Winkel.Die Lösungen der partiellen Differentialgleichungen sind hochgradig nichtlinear und mit Navier-Stokes verwandt, und nur Suchansätze können zur Anpassung an die Daten verwendet werden.
Im Bereich des kontinuierlichen Lernens besteht das Ziel darin, mehrere Aufgaben nacheinander zu erlernen, ohne auf die Daten früherer Aufgaben zurückgreifen zu können. Es wurden mehrere Lösungen für dieses Problem vorgeschlagen, die jedoch in der Regel davon ausgehen, dass der Benutzer weiß, welche der Aufgaben zum Testzeitpunkt an einer bestimmten Probe durchgeführt werden soll, oder sich auf kleine Stichproben aus früheren Daten stützen, und die meisten von ihnen leiden unter einem erheblichen Genauigkeitsabfall, wenn sie mit Stapeln von jeweils nur einer Klasse aktualisiert werden.In diesem Artikel schlagen wir eine neue Methode, OvA-INN, vor, die in der Lage ist, eine Klasse nach der anderen zu erlernen, ohne vorherige Daten zu speichern. Um dies zu erreichen, trainieren wir für jede Klasse ein spezifisches invertierbares neuronales Netz, um den Nullvektor für seine Klasse auszugeben. Zum Testzeitpunkt können wir die Klasse einer Probe vorhersagen, indem wir feststellen, welches Netz den Vektor mit der kleinsten Norm ausgibt. Auf diese Weise sind wir in der Lage, modernste Ansätze zu übertreffen, die sich auf das Lernen von Merkmalen für die Datensätze MNIST und CIFAR-100 stützen. In unseren Experimenten erreichen wir eine Genauigkeit von 72% bei CIFAR-100, nachdem wir unser Modell klassenweise trainiert haben.
Konversationssysteme können grob in zwei Kategorien eingeteilt werden: Retrieval-basierte und Generations-basierte Systeme.Retrieval-Systeme suchen eine vom Benutzer eingegebene Äußerung (nämlich eine Anfrage) in einem großen Konversationsspeicher und geben eine Antwort zurück, die am besten mit der Anfrage übereinstimmt.Generative Ansätze synthetisieren neue Antworten.Beide Wege haben gewisse Vorteile, leiden aber unter ihren eigenen Nachteilen. Wir schlagen ein neuartiges Ensemble aus abruf- und generativem Konversationssystem vor: Die abgerufenen Kandidaten werden zusätzlich zur ursprünglichen Abfrage über ein neuronales Netzwerk an einen Antwortgenerator weitergeleitet, so dass das Modell mehr Informationen erhält, und die generierte Antwort nimmt zusammen mit den abgerufenen Antworten an einem Re-Ranking-Prozess teil, um die endgültige Antwort zu finden.
Tiefe neuronale Netze, die auf einer Vielzahl von Datensätzen trainiert wurden, zeigen eine beeindruckende Übertragbarkeit: Tiefe Funktionen erscheinen allgemein, da sie auf viele Datensätze und Aufgaben anwendbar sind, eine Eigenschaft, die in realen Anwendungen weit verbreitet ist: Ein neuronales Netz, das auf großen Datensätzen wie ImageNet trainiert wurde, kann die Generalisierung erheblich steigern und das Training beschleunigen, wenn es auf einen kleineren Zieldatensatz abgestimmt wird. Trotz der weiten Verbreitung wurde bisher nur wenig Aufwand betrieben, um den Grund für die Übertragbarkeit von tiefen Merkmalsrepräsentationen aufzudecken. Dieses Papier versucht, die Übertragbarkeit aus den Perspektiven der verbesserten Generalisierung, der Optimierung und der Machbarkeit der Übertragbarkeit zu verstehen. Wir zeigen, dass1) Übertragene Modelle dazu neigen, flachere Minima zu finden, da ihre Gewichtsmatrizen in der Nähe der ursprünglichen flachen Region der vortrainierten Parameter bleiben, wenn sie auf einen ähnlichen Zieldatensatz übertragen werden;2) Übertragene Repräsentationen machen die Verlustlandschaft mit verbesserter Lipschitzness günstiger, was das Training erheblich beschleunigt und stabilisiert.3) Die Verbesserung ist größtenteils auf die Tatsache zurückzuführen, dass die Hauptkomponente des Gradienten in den vortrainierten Parametern unterdrückt wird, wodurch die Größe des Gradienten in der Backpropagation stabilisiert wird. ) Die Durchführbarkeit der Übertragbarkeit hängt mit der Ähnlichkeit der Eingabe und des Labels zusammen, und eine überraschende Entdeckung ist, dass die Durchführbarkeit auch von den Trainingsstufen beeinflusst wird, indem die Übertragbarkeit zuerst während des Trainings zunimmt und dann abnimmt.3 Wir bieten außerdem eine theoretische Analyse, um unsere Beobachtungen zu überprüfen.
Wir gehen der folgenden Frage nach: Zwei solcher Transformationen sind für Feed-Forward-Architekturen bekannt: die Permutation von Neuronen innerhalb einer Schicht und die positive Skalierung aller eingehenden Gewichte eines Neurons, gekoppelt mit der inversen Skalierung seiner ausgehenden Gewichte.In dieser Arbeit zeigen wir für Architekturen mit nicht-steigenden Breiten, dass Permutation und Skalierung tatsächlich die einzigen funktionserhaltenden Gewichtstransformationen sind. Für jede in Frage kommende Architektur geben wir eine explizite Konstruktion eines neuronalen Netzes an, so dass jedes andere Netz, das dieselbe Funktion implementiert, durch die Anwendung von Permutationen und Skalierungen aus dem ursprünglichen Netz erhalten werden kann.Der Beweis beruht auf einem geometrischen Verständnis der Grenzen zwischen linearen Regionen von ReLU-Netzen, und wir hoffen, dass die entwickelten mathematischen Werkzeuge von unabhängigem Interesse sind.
Ein allgemeines Problem, das in letzter Zeit viel Aufmerksamkeit erhalten hat, ist die Frage, wie man mehrere Aufgaben im selben Netzwerk ausführen und dabei sowohl die Effizienz als auch die Vorhersagegenauigkeit maximieren kann.Ein beliebter Ansatz besteht aus einer Mehrzweigarchitektur auf einem gemeinsamen Backbone, der gemeinsam auf einer gewichteten Summe von Verlusten trainiert wird.In vielen Fällen führt die gemeinsame Darstellung jedoch zu einer nicht optimalen Leistung, vor allem aufgrund einer Interferenz zwischen konkurrierenden Gradienten unkorrelierter Aufgaben.Neuere Ansätze lösen dieses Problem durch eine kanalweise Modulation der Merkmalskarten entlang des gemeinsamen Backbone mit aufgabenspezifischen Vektoren, die manuell oder dynamisch abgestimmt werden. Wir gehen einen Schritt weiter und schlagen eine neuartige Architektur vor, die das Erkennungsnetzwerk sowohl kanalweise als auch räumlich mit einem effizienten bildabhängigen Top-Down-Berechnungsschema moduliert.Unsere Architektur verwendet weder aufgabenspezifische Verzweigungen noch aufgabenspezifische Module, sondern ein Top-Down-Modulationsnetzwerk, das von allen Aufgaben gemeinsam genutzt wird.Wir zeigen die Effektivität unseres Schemas, indem wir sowohl bei korrelierten als auch bei unkorrelierten Aufgabensätzen gleichwertige oder bessere Ergebnisse als alternative Ansätze erzielen.Wir zeigen auch unsere Vorteile in Bezug auf die Modellgröße, die Hinzufügung neuer Aufgaben und die Interpretierbarkeit. Der Code wird veröffentlicht.
Die Leistung eines Clustering-Algorithmus hängt von den Merkmalen ab, die aus den Daten extrahiert werden. Bei der Zeitreihenanalyse hat sich jedoch das Problem ergeben, dass die herkömmlichen Methoden, die auf der Signalform basieren, bei Phasenverschiebungen, Amplituden- und Signallängenvariationen instabil sind. In diesem Papier schlagen wir einen neuen Clustering-Algorithmus vor, der sich auf den Aspekt des dynamischen Systems des Signals konzentriert, indem er ein rekurrentes neuronales Netzwerk und die Variational-Bayes-Methode verwendet.
Zwei wichtige Themen beim Deep Learning sind die Einbeziehung des Menschen in den Modellierungsprozess: Modellprioritäten übertragen Informationen vom Menschen auf ein Modell, indem sie die Parameter des Modells regulieren; Modellattributionen übertragen Informationen vom Modell auf den Menschen, indem sie das Verhalten des Modells erklären.Bisherige Arbeiten haben wichtige Schritte unternommen, um diese Themen durch verschiedene Formen der Gradientenregulierung zu verbinden.Wir finden jedoch, dass bestehende Methoden, die Attributionen verwenden, um das Verhalten eines Modells mit der menschlichen Intuition abzugleichen, ineffektiv sind. Wir entwickeln eine effiziente und theoretisch fundierte Methode zur Attribution von Merkmalen, erwartete Gradienten, und ein neuartiges Rahmenwerk, Attributionsprioritäten, um Prioritätserwartungen über das Verhalten eines Modells während des Trainings durchzusetzen.
Rekurrente neuronale Netze (RNNs) haben sich bei der Verarbeitung von Sequenzdaten als sehr leistungsfähig erwiesen, sind jedoch aufgrund ihrer rekursiven Natur sowohl komplex als auch speicherintensiv, was die Einbettung von RNNs in mobile Geräte, die Echtzeitprozesse mit begrenzten Hardware-Ressourcen erfordern, erschwert. Um die oben genannten Probleme anzugehen, stellen wir eine Methode vor, die binäre und ternäre Gewichte während der Trainingsphase erlernen kann, um die Hardware-Implementierung von RNNs zu erleichtern, was dazu führt, dass alle Multiplikations-Akkumulations-Operationen durch einfache Akkumulationen ersetzt werden, was erhebliche Vorteile für kundenspezifische Hardware in Bezug auf Siliziumfläche und Stromverbrauch mit sich bringt. Auf der Softwareseite evaluieren wir die Leistung (in Bezug auf die Genauigkeit) unserer Methode unter Verwendung von Langkurzzeitspeichern (LSTMs) und Gated Recurrent Units (GRUs) bei verschiedenen sequentiellen Modellen, einschließlich Sequenzklassifizierung und Sprachmodellierung, und zeigen, dass unsere Methode bei den genannten Aufgaben konkurrenzfähige Ergebnisse erzielt, während sie während der Laufzeit binäre/ternäre Gewichte verwendet. Auf der Hardwareseite stellen wir kundenspezifische Hardware zur Beschleunigung der rekurrenten Berechnungen von LSTMs mit binären/ternären Gewichten vor und zeigen, dass LSTMs mit binären/ternären Gewichten eine bis zu 12-fache Speicherersparnis und eine bis zu 10-fache Beschleunigung der Inferenz im Vergleich zu Hardware-Implementierungen mit voller Genauigkeit erreichen können.
Wir verwenden eine neue GAN-ähnliche Deep-Architektur, die auf das unbeaufsichtigte Lernen einer Bildrepräsentation abzielt, die latente Objektteile kodiert und somit gut auf ungesehene Klassen in unserer Aufgabe der Erkennung von wenigen Aufnahmen generalisiert. Wir leisten zwei Beiträge: Erstens erweitern wir das Vanilla GAN mit Rekonstruktionsverlusten, um den Diskriminator zu zwingen, die wichtigsten Merkmale von "gefälschten" Bildern zu erfassen, die aus zufällig abgetasteten Codes erzeugt wurden. Zweitens stellen wir einen Trainingssatz von Triplett-Bildbeispielen zusammen, um den Triplett-Verlust beim metrischen Lernen zu schätzen, indem wir ein Bildmaskierungsverfahren verwenden, das geeignet ist, latente Objektteile zu identifizieren. metrisches Lernen stellt also sicher, dass die tiefe Repräsentation von Bildern, die ähnliche Objektklassen zeigen und einige Teile gemeinsam haben, näher ist als die Repräsentationen von Bildern, die keine gemeinsamen Teile haben. Unsere Ergebnisse zeigen, dass wir den Stand der Technik deutlich übertreffen und eine ähnliche Leistung wie das übliche episodische Training für das vollständig überwachte few-shot Lernen auf den Mini-Imagenet und Tiered-Imagenet Datensätzen erzielen.
Bestehende Blackbox-Angriffe auf tiefe neuronale Netze (DNNs) haben sich bisher weitgehend auf die Übertragbarkeit konzentriert, wobei eine für ein lokal trainiertes Modell erzeugte gegnerische Instanz "übertragen" werden kann, um andere Lernmodelle anzugreifen.In diesem Papier schlagen wir neuartige Gradient Estimation Blackbox-Angriffe für Gegner mit Abfragezugriff auf die Klassenwahrscheinlichkeiten des Zielmodells vor, die nicht auf Übertragbarkeit beruhen. Wir schlagen außerdem Strategien vor, um die Anzahl der Abfragen, die zur Generierung jeder gegnerischen Probe erforderlich sind, von der Dimensionalität der Eingabe zu entkoppeln. Eine iterative Variante unseres Angriffs erreicht Erfolgsquoten von nahezu 100 % sowohl für gezielte als auch für ungezielte Angriffe auf DNNs. Wir führen umfangreiche Experimente für eine gründliche vergleichende Bewertung von Blackbox-Angriffen durch und zeigen, dass die vorgeschlagenen Gradient Estimation-Angriffe alle auf Übertragbarkeit basierenden Blackbox-Angriffe übertreffen, die wir sowohl auf MNIST- als auch auf CIFAR-10-Datensätzen getestet haben, und dabei ähnliche Erfolgsquoten wie bekannte, hochmoderne Whitebox-Angriffe erzielen. Wir wenden die Gradient Estimation Angriffe auch erfolgreich gegen einen realen Content Moderation Classiﬁer an, der von Clarifai gehostet wird, und evaluieren Blackbox-Angriffe gegen modernste Verteidigungsmaßnahmen.
Wir schlagen eine neuartige Subgraphen-Bilddarstellung für die Klassifizierung von Netzwerkfragmenten vor, wobei das Ziel ihre übergeordneten Netzwerke sind.Die Graphen-Bilddarstellung basiert auf 2D-Bildeinbettungen von Adjazenzmatrizen.Wir verwenden diese Bilddarstellung in zwei Modi.Erstens als Eingabe für einen maschinellen Lernalgorithmus. Unsere Schlussfolgerungen aus mehreren Datensätzen sind, dass 1. Deep Learning mit strukturierten Bildmerkmalen im Vergleich zu Graph-Kernel- und klassischen merkmalsbasierten Methoden am besten abschneidet und 2. reines Transfer-Lernen mit minimalen Eingriffen durch den Benutzer effektiv funktioniert und robust gegenüber kleinen Daten ist.
Unsere Technik ist von der Mean-Teacher-Variante (Tarvainen et al. 2017) des temporalen Ensembling (Laine et al. 2017) abgeleitet, einer Technik, die im Bereich des halbüberwachten Lernens den Stand der Technik erreicht hat.Wir stellen eine Reihe von Modifikationen ihres Ansatzes für anspruchsvolle Domänenanpassungsszenarien vor und bewerten ihre Wirksamkeit. Unser Ansatz erreicht in einer Reihe von Benchmarks den Stand der Technik, einschließlich unseres siegreichen Beitrags in der VISDA-2017 visual domain adaptation challenge.in kleinen Bild-Benchmarks, unser Algorithmus nicht nur übertrifft den Stand der Technik, sondern kann auch eine Genauigkeit erreichen, die nahe an der eines überwachten Klassifizierers ist.
Wir nennen die Fähigkeit, Bilder von neuartigen semantischen Konzepten zu erstellen visuell geerdet imagination.In diesem Papier, zeigen wir, wie wir modifizieren können variational Auto-Encoder, um diese Aufgabe zu erfüllen.Our Methode verwendet eine neuartige Ausbildung Ziel, und ein neuartiges Produkt von Experten Inferenz-Netzwerk, das teilweise spezifiziert (abstrakte) Konzepte in einer prinzipiellen und effiziente Weise behandeln kann. Wir schlagen auch eine Reihe von einfach zu berechnenden Bewertungsmetriken vor, die unsere intuitiven Vorstellungen von einer guten visuellen Vorstellungskraft erfassen, nämlich Korrektheit, Abdeckung und Kompositionalität (die 3 K's), 2017 und die BiVCCA-Methode von Wang et al., 2016), indem wir sie auf zwei Datensätze anwenden: den MNIST-with-attributes-Datensatz (den wir hier vorstellen) und den CelebA-Datensatz (Liu et al., 2015).
Wir stellen "Search with Amortized Value Estimates" (SAVE) vor, einen Ansatz zur Kombination von modellfreiem Q-Learning mit modellbasierter Monte-Carlo Tree Search (MCTS). Bei SAVE wird ein gelernter Prior über State-Action-Werte verwendet, um MCTS zu leiten, das einen verbesserten Satz von State-Action-Werten schätzt. Die neuen Q-Schätzungen werden dann in Kombination mit echten Erfahrungen verwendet, um den Prior zu aktualisieren. SAVE kann auf jedem Q-Learning-Agenten mit Zugang zu einem Modell implementiert werden, was wir durch die Integration in Agenten demonstrieren, die anspruchsvolle physikalische Argumentationsaufgaben und Atari ausführen.SAVE erreicht durchgängig höhere Belohnungen mit weniger Trainingsschritten und - im Gegensatz zu typischen modellbasierten Suchansätzen - eine starke Leistung mit sehr kleinen Suchbudgets.Durch die Kombination von realer Erfahrung mit Informationen, die während der Suche berechnet werden, zeigt SAVE, dass es möglich ist, sowohl die Leistung des modellfreien Lernens als auch die Rechenkosten der Planung zu verbessern.
Wir beziehen dieses Ergebnis auf die bekannte konvexe Dualität der Shannon-Entropie und der Softmax-Funktion, die auch als Donsker-Varadhan-Formel bekannt ist und einen kurzen Beweis für die Äquivalenz liefert. Anschließend interpretieren wir diese Dualität weiter und nutzen Ideen der konvexen Analyse, um eine neue Ungleichheit der Politik in Bezug auf das Soft-Q-Learning zu beweisen.
Computersimulationen bieten einen automatischen und sicheren Weg für das Training von Robotersteuerungsstrategien, um komplexe Aufgaben, wie z.B. die Fortbewegung, zu bewältigen. Eine in der Simulation trainierte Strategie lässt sich jedoch aufgrund der Unterschiede zwischen den beiden Umgebungen in der Regel nicht direkt auf die reale Hardware übertragen.Transferlernen unter Verwendung von Domänenrandomisierung ist ein vielversprechender Ansatz, der jedoch in der Regel davon ausgeht, dass die Zielumgebung nahe an der Verteilung der Trainingsumgebungen liegt und somit stark von einer genauen Systemidentifikation abhängt.In diesem Beitrag stellen wir einen anderen Ansatz vor, der die Domänenrandomisierung für die Übertragung von Steuerungsstrategien auf bekannte Umgebungen nutzt. Der Schlüsselgedanke ist, dass wir, anstatt eine einzelne Politik in der Simulation zu lernen, gleichzeitig eine Familie von Politiken lernen, die unterschiedliche Verhaltensweisen aufweisen.Wenn wir in der Zielumgebung getestet werden, suchen wir direkt nach der besten Politik in der Familie auf der Grundlage der Aufgabenleistung, ohne die Notwendigkeit, die dynamischen Parameter zu identifizieren.Wir bewerten unsere Methode auf fünf simulierten Robotersteuerungsproblemen mit unterschiedlichen Diskrepanzen in der Trainings- und Testumgebung und zeigen, dass unsere Methode größere Modellierungsfehler im Vergleich zum Training einer robusten Politik oder einer adaptiven Politik überwinden kann.
Wir schlagen vq-wav2vec vor, um diskrete Repräsentationen von Audiosegmenten durch eine selbstüberwachte Kontextvorhersageaufgabe im Stil von wav2vec zu erlernen.Der Algorithmus verwendet entweder eine Gumbel-Softmax oder ein Online-K-Means-Clustering, um die dichten Repräsentationen zu quantisieren.Die Diskretisierung ermöglicht die direkte Anwendung von Algorithmen aus der NLP-Gemeinschaft, die diskrete Eingaben erfordern.Experimente zeigen, dass das BERT-Pre-Training einen neuen Stand der Technik bei der TIMIT-Phonemklassifikation und der WSJ-Spracherkennung erreicht.
Deep Reinforcement Learning-Algorithmen führen zu Agenten, die schwierige Entscheidungsprobleme in komplexen Umgebungen lösen können. Viele schwierige Multi-Agenten-Wettbewerbsspiele, insbesondere Echtzeit-Strategiespiele, werden jedoch immer noch als jenseits der Fähigkeiten aktueller Deep Reinforcement Learning-Algorithmen angesehen, obwohl es in letzter Zeit einige Bemühungen gab, dies zu ändern \citep{openai_2017_dota, vinyals_2017_starcraft}. Wenn die Gegner in einem kompetitiven Spiel suboptimal sind, sind die aktuellen, nach einem Nash-Gleichgewicht strebenden Selbstspiel-Algorithmen außerdem oft nicht in der Lage, ihre Strategien auf Gegner zu verallgemeinern, die Strategien spielen, die sich stark von ihren eigenen unterscheiden, was darauf hindeutet, dass ein Lernalgorithmus erforderlich ist, der über das herkömmliche Selbstspiel hinausgeht. Wir entwickeln Hierarchical Agent with Self-play (HASP), einen Lernansatz, um hierarchisch strukturierte Strategien zu erhalten, die eine höhere Leistung als konventionelle Self-Play-Algorithmen in kompetitiven Spielen durch die Verwendung eines vielfältigen Pools von Sub-Strategien, die wir von Counter Self-Play (CSP) erhalten, erreichen können.Wir zeigen, dass die Ensemble-Strategie, die von HASP generiert wird, eine bessere Leistung erreichen kann, während sie mit unbekannten Gegnern konfrontiert wird, die suboptimale Strategien verwenden. Anhand eines motivierenden iterierten Rock-Paper-Scissor-Spiels und eines teilweise beobachtbaren Echtzeit-Strategiespiels (http://generals.io/) kommen wir zu dem Schluss, dass HASP besser abschneidet als herkömmliches Self-Play und eine Gewinnrate von 77% gegen FloBot erreicht, einen Open-Source-Agenten, der in den Online-Ranglisten auf Platz 2 steht.
Wir führen adaptive Eingabedarstellungen für die neuronale Sprachmodellierung ein, die die adaptive Softmax von Grave et al. (2017) auf Eingabedarstellungen mit variabler Kapazität erweitern.Es gibt mehrere Möglichkeiten, wie die Eingabe- und Ausgabeschichten faktorisiert werden können und ob Wörter, Zeichen oder Unterworteinheiten modelliert werden sollen.Wir führen einen systematischen Vergleich der beliebten Möglichkeiten für eine selbstaufmerksame Architektur durch. Unsere Experimente zeigen, dass Modelle, die mit adaptiven Einbettungen ausgestattet sind, mehr als doppelt so schnell zu trainieren sind als die beliebten CNN mit Zeicheneingabe, während sie eine geringere Anzahl von Parametern haben. Beim WikiText-103 Benchmark erreichen wir 18,7 Perplexität, eine Verbesserung von 10,5 Perplexität im Vergleich zum zuvor besten veröffentlichten Ergebnis und beim Billion Word Benchmark erreichen wir 23,02 Perplexität.
Die meisten Objektdetektoren formulieren die Regression von Bounding Boxen als eine unimodale Aufgabe (d.h., Die meisten Objekterkennungssysteme formulieren die Bounding-Box-Regression als unimodale Aufgabe (d.h. sie regressieren einen einzigen Satz von Bounding-Box-Koordinaten unabhängig voneinander), aber wir stellen fest, dass die Bounding-Box-Grenzen eines verdeckten Objekts mehrere plausible Konfigurationen haben können, und dass die verdeckten Bounding-Box-Grenzen mit den sichtbaren korrelieren.Motiviert durch diese beiden Beobachtungen schlagen wir ein tiefes multivariates Gauß-Gemisch-Modell für die Bounding-Box-Regression bei Verdeckung vor. Die Mischungskomponenten lernen potentiell verschiedene Konfigurationen eines verdeckten Teils, und die Kovarianzen zwischen den Variablen helfen, die Beziehung zwischen den verdeckten Teilen und den sichtbaren zu lernen.Quantitativ verbessert unser Modell die AP der Baselines um 3,9% bzw. 1,2% auf CrowdHuman und MS-COCO mit fast keinem Rechen- oder Speicher-Overhead.Qualitativ genießt unser Modell Erklärbarkeit, da wir die resultierenden Bounding Boxen über die Kovarianzmatrizen und die Mischungskomponenten interpretieren können.
Negative Beispiele sind ein weit verbreitetes Phänomen bei Modellen des maschinellen Lernens, bei denen scheinbar unmerkliche Störungen der Eingabe zu Fehlklassifizierungen bei ansonsten statistisch genauen Modellen führen.  Adversariales Training, eine der erfolgreichsten empirischen Verteidigungen gegen adversarische Beispiele, bezieht sich auf das Training auf adversarischen Beispielen, die innerhalb einer geometrischen Randbedingung generiert werden, wobei die am häufigsten verwendete geometrische Randbedingung eine $L_p$-Kugel mit dem Radius $\epsilon$ in einer Norm ist. Wir zeigen, dass adversariales Training mit Voronoi-Constraints robuste Modelle hervorbringt, die den Stand der Technik bei MNIST deutlich übertreffen und bei CIFAR-10 konkurrenzfähig sind.
Jüngste Forschungsanstrengungen ermöglichen Studien zur natürlichsprachlich fundierten Navigation in fotorealistischen Umgebungen, z.B, Bestehende Methoden neigen jedoch dazu, Trainingsdaten in gesehenen Umgebungen zu stark anzupassen und können in zuvor ungesehenen Umgebungen nicht gut generalisiert werden. Um die Lücke zwischen gesehenen und ungesehenen Umgebungen zu schließen, zielen wir darauf ab, ein verallgemeinerbares Navigationsmodell aus zwei neuartigen Perspektiven zu erlernen:(1) wir stellen ein Multitasking-Navigationsmodell vor, das nahtlos sowohl auf Vision-Language Navigation (VLN) als auch auf Navigation aus Dialoghistorie (NDH) Aufgaben trainiert werden kann, das von einer reichhaltigeren natürlichsprachlichen Anleitung profitiert und Wissen effektiv über Aufgaben hinweg transferiert;(2) wir schlagen vor, umgebungsunabhängige Repräsentationen für die Navigationspolitik zu erlernen, die zwischen den Umgebungen invariant sind und sich somit besser auf ungesehene Umgebungen verallgemeinern lassen. Ausführliche Experimente zeigen, dass unser umgebungsunabhängiges Multitasking-Navigationsmodell den Leistungsunterschied zwischen gesehenen und ungesehenen Umgebungen signifikant reduziert und die Basislinien in ungesehenen Umgebungen um 16% (relatives Maß für die Erfolgsrate) bei VLN und 120% (Zielfortschritt) bei NDH übertrifft, was den neuen Stand der Technik für NDH-Aufgaben begründet.
In diesem Papier schlagen wir vor, Modell-Ensembling in einer Multiklassen- oder Multilabel-Lernumgebung unter Verwendung von Wasserstein (W.) barycenters.Optimal Transportmetriken, wie die Wasserstein-Distanz, ermöglichen die Einbeziehung semantischer Informationen wie Worteinbettungen.using W. Wir zeigen Anwendungen von Wasserstein-Ensembling in der attributbasierten Klassifikation, im Multilabel-Lernen und in der Generierung von Bildunterschriften. Diese Ergebnisse zeigen, dass das W.-Ensembling eine brauchbare Alternative zum geometrischen oder arithmetischen Mittelwert-Ensembling ist.
Während Deep Learning bei der Modellierung von Aufgaben mit großen, sorgfältig kuratierten beschrifteten Datensätzen unglaublich erfolgreich war, bleibt seine Anwendung auf Probleme mit begrenzten beschrifteten Daten eine Herausforderung.Das Ziel der vorliegenden Arbeit ist es, die Beschriftungseffizienz großer neuronaler Netze, die auf Audiodaten arbeiten, durch eine Kombination von Multitasking-Lernen und selbstüberwachtem Lernen auf unbeschrifteten Daten zu verbessern.Wir trainierten einen End-to-End-Audio-Feature-Extraktor auf der Grundlage von WaveNet, der in einfache, aber vielseitige aufgabenspezifische neuronale Netze einfließt. Wir zeigen, dass in Szenarien mit begrenzten Trainingsdaten die Leistung von drei verschiedenen überwachten Klassifizierungsaufgaben durch gleichzeitiges Training mit diesen zusätzlichen selbstüberwachten Aufgaben um bis zu 6 % verbessert werden kann und dass die Einbeziehung von Datenerweiterungen in unsere Multitasking-Einstellung zu weiteren Leistungssteigerungen führt.
Neuere neuronale Netz- und Sprachmodelle basieren auf Softmax-Verteilungen mit einer extrem großen Anzahl von Kategorien, wobei die Berechnung der Softmax-Normalisierungskonstante sehr teuer ist, was zu einer wachsenden Anzahl von effizient berechenbaren, aber verzerrten Softmax-Schätzungen geführt hat. In diesem Beitrag stellen wir die ersten beiden unvoreingenommenen Algorithmen zur Maximierung der Softmax-Likelihood vor, deren Arbeit pro Iteration unabhängig von der Anzahl der Klassen und Datenpunkte ist (und keine zusätzliche Arbeit am Ende jeder Epoche erfordert). Wir vergleichen die empirische Leistung unserer unvoreingenommenen Methoden mit dem Stand der Technik auf sieben realen Datensätzen, wo sie alle Konkurrenten umfassend übertreffen.
Die Erstellung von Gegenbeispielen für diskrete Eingaben wie Textsequenzen unterscheidet sich grundlegend von der Generierung solcher Beispiele für kontinuierliche Eingaben wie Bilder.Dieses Papier versucht, die Frage zu beantworten: Können wir in einer Black-Box-Einstellung automatisch Gegenbeispiele erstellen, um Deep-Learning-Klassifikatoren für Texte durch unmerkliche Änderungen effektiv zu täuschen?Unsere Antwort ist ein klares Ja. Bisherige Bemühungen beruhen zumeist auf der Verwendung von Gradienten-Evidenz und sind weniger effektiv, da es entweder schwierig ist, das nächstgelegene Wort (in Bezug auf die Bedeutung) automatisch zu finden, oder weil sie sich stark auf handgefertigte linguistische Regeln stützen.  Unsere experimentellen Ergebnisse zeigen, dass MCTSBug Deep-Learning-Klassifikatoren mit Erfolgsquoten von 95 % in sieben großen Benchmark-Datensätzen täuschen kann, indem es nur wenige Zeichen stört.  Überraschenderweise ist MCTSBug, ohne sich überhaupt auf Gradienteninformationen zu verlassen, effektiver als die gradientenbasierte White-Box-Baseline.Dank der Natur des Homoglyphen-Angriffs sind die erzeugten Störungen für das menschliche Auge fast nicht wahrnehmbar.
  Wir stellen ein Modell vor, das lernt, einfache Handzeichnungen in Grafikprogramme umzuwandeln, die in einer Untermenge von LaTeX geschrieben sind. Das Modell kombiniert Techniken des Deep Learning und der Programmsynthese.  Das Modell kombiniert Techniken aus dem Deep Learning und der Programmsynthese. Wir lernen ein neuronales Faltungsnetzwerk, das plausible Zeichnungsprimitive vorschlägt, die ein Bild erklären. Diese Zeichnungsprimitive sind wie eine Spur des Satzes von primitiven Befehlen, die von einem Grafikprogramm ausgegeben werden. Wir lernen ein Modell, das Programmsynthesetechniken verwendet, um ein Grafikprogramm aus dieser Spur wiederherzustellen.  Zusammengenommen sind diese Ergebnisse ein Schritt in Richtung Agenten, die nützliche, für den Menschen lesbare Programme aus Wahrnehmungseingaben erstellen.
Dieses Papier stützt sich auf Background Check (Perello-Nieto et al., 2016), eine Technik in der Modellkalibrierung, um neuronale Netze in zwei Klassen bei der Erkennung von negativen Beispielen zu unterstützen, wobei die eindimensionale Differenz zwischen Logit-Werten als zugrunde liegendes Maß verwendet wird.Diese Methode neigt interessanterweise dazu, den höchsten durchschnittlichen Recall auf Bildsätzen zu erzielen, die mit großen Störungsvektoren generiert werden, was sich von der bestehenden Literatur über negative Angriffe unterscheidet (Cubuk et al., 2017), Die vorgeschlagene Methode benötigt keine Kenntnis der Angriffsparameter oder -methoden zum Zeitpunkt des Trainings, im Gegensatz zu einem großen Teil der Literatur, die auf Deep Learning basierende Methoden zur Erkennung gegnerischer Beispiele verwendet, wie z. B. Metzen et al. (2017), was der vorgeschlagenen Methode zusätzliche Flexibilität verleiht.
Unser System lernt Wort- und Satzeinbettungen gemeinsam, indem es ein mehrsprachiges Skip-Gram-Modell zusammen mit einem sprachübergreifenden Satzähnlichkeitsmodell trainiert. Wir konstruieren Satzeinbettungen, indem wir Worteinbettungen mit einem LSTM verarbeiten und einen Durchschnitt der Ausgaben nehmen. Unsere Architektur kann auf transparente Weise sowohl einsprachige als auch satzausgerichtete zweisprachige Korpora verwenden, um mehrsprachige Einbettungen zu erlernen, und deckt damit ein Vokabular ab, das wesentlich größer ist als das Vokabular der zweisprachigen Korpora allein.Unser Modell zeigt eine konkurrenzfähige Leistung in einer Standardaufgabe zur sprachübergreifenden Dokumentenklassifikation.Wir zeigen auch die Effektivität unserer Methode in einem Szenario mit geringen Ressourcen.
Wasserstein GAN (WGAN) ist ein vielversprechender Rahmen, um mit dem Instabilitätsproblem umzugehen, da es eine gute Konvergenzeigenschaft hat. Ein Nachteil des WGAN ist, dass es die Wasserstein-Distanz in der dualen Domäne auswertet, was eine gewisse Annäherung erfordert, so dass es möglicherweise nicht die wahre Wasserstein-Distanz optimiert. Experimente auf dem MNIST-Datensatz zeigen, dass unsere Methode deutlich stabiler konvergiert und die niedrigste Wasserstein-Distanz unter den WGAN-Varianten auf Kosten einer gewissen Schärfe der erzeugten Bilder erreicht. Experimente auf dem 8-Gauß-Spielzeugdatensatz zeigen, dass mit unserer Methode bessere Gradienten für den Generator erzielt werden und die vorgeschlagene Methode eine flexiblere generative Modellierung als WGAN ermöglicht.
Neural Architecture Search (NAS) ist ein aufregendes neues Feld, das verspricht, genauso viel zu verändern wie Convolutional Neural Networks im Jahr 2012.Trotz vieler großartiger Arbeiten, die zu erheblichen Verbesserungen bei einer Vielzahl von Aufgaben führen, ist der Vergleich zwischen verschiedenen Methoden immer noch ein sehr offenes Thema.Während die meisten Algorithmen auf den gleichen Datensätzen getestet werden, gibt es kein gemeinsames experimentelles Protokoll, das von allen befolgt wird.Als solches und aufgrund der unzureichenden Nutzung von Ablationsstudien, gibt es einen Mangel an Klarheit darüber, warum bestimmte Methoden effektiver sind als andere. Um die Hürde des Vergleichs von Methoden mit unterschiedlichen Suchräumen zu überwinden, schlagen wir vor, die relative Verbesserung einer Methode gegenüber der zufällig ausgewählten Durchschnittsarchitektur zu verwenden, wodurch Vorteile, die sich aus von Experten entwickelten Suchräumen oder Trainingsprotokollen ergeben, effektiv beseitigt werden. Wir führen weitere Experimente mit dem weit verbreiteten DARTS-Suchraum durch, um den Beitrag der einzelnen Komponenten der NAS-Pipeline zu verstehen. Diese Experimente verdeutlichen, dass:(i) die Verwendung von Tricks im Bewertungsprotokoll eine überwiegende Auswirkung auf die gemeldete Leistung der Architekturen hat;(ii) der zellenbasierte Suchraum einen sehr engen Genauigkeitsbereich aufweist, so dass der Seed einen beträchtlichen Einfluss auf die Rangfolge der Architekturen hat;(iii) die von Hand entworfene Makrostruktur (Zellen) wichtiger ist als die gesuchte Mikrostruktur (Operationen); und(iv) die Tiefenlücke ein echtes Phänomen ist, was durch die Veränderung der Rangfolge zwischen Architekturen mit 8 und 20 Zellen belegt wird. Abschließend schlagen wir bewährte Verfahren vor, die sich hoffentlich als nützlich für die Gemeinschaft erweisen und dazu beitragen, die derzeitigen Fallstricke der NAS zu entschärfen, z. B. Schwierigkeiten bei der Reproduzierbarkeit und dem Vergleich von Suchmethoden. Der verwendete Code ist unter https://github.com/antoyang/NAS-Benchmark verfügbar.
Jüngste Fortschritte in der domänenübergreifenden Bildzuordnung haben sich auf die Übersetzung von Bildern zwischen den Domänen konzentriert. Obwohl die erzielten Fortschritte beeindruckend sind, reicht die visuelle Wiedergabetreue oft nicht aus, um das passende Beispiel aus der anderen Domäne zu identifizieren.In diesem Beitrag befassen wir uns mit genau dieser Aufgabe, exakte Analogien zwischen Datensätzen zu finden, d.h. für jedes Bild aus Domäne A ein analoges Bild in Domäne B zu finden: Wir zeigen außerdem, dass die domänenübergreifende Zuordnungsaufgabe in zwei Teile aufgeteilt werden kann: Domänenausrichtung und Lernen der Zuordnungsfunktion.
Die effektive Ableitung von diskriminierenden und kohärenten latenten Themen von kurzen Texten ist eine kritische Aufgabe für viele Anwendungen in der realen Welt.Dennoch ist die Aufgabe erwiesenermaßen eine große Herausforderung für traditionelle Themenmodelle aufgrund der Daten Sparsamkeit Problem durch die Eigenschaften von kurzen Texten induziert.Darüber hinaus wird die komplexe Inferenz-Algorithmus auch ein Engpass für diese traditionellen Modelle schnell zu erkunden Variationen. In diesem Papier schlagen wir ein neuartiges Modell mit dem Namen Neural Variational Sparse Topic Model (NVSTM) vor, das auf einem sparsamen Themenmodell mit dem Namen Sparse Topical Coding (STC) basiert und bei dem die Hilfsworteinbettungen verwendet werden, um die Erzeugung von Darstellungen zu verbessern. Der Variational Autoencoder (VAE)-Ansatz wird angewandt, um das Modell effizient zu inferenzieren, was es einfach macht, Erweiterungen für seinen Black-Box-Inferenzprozess zu erforschen.Experimentelle Ergebnisse aufWeb Snippets, 20Newsgroups, BBC und biomedizinischen Datensätzen zeigen die Effektivität und Effizienz des Modells.
Neural Tangents ist eine Bibliothek für die Arbeit mit neuronalen Netzen unendlicher Breite und bietet eine High-Level-API für die Spezifikation komplexer und hierarchischer neuronaler Netzarchitekturen, die dann entweder wie üblich mit endlicher Breite oder in ihrer unendlichen Breite trainiert und ausgewertet werden können. Für Netze mit unendlicher Breite führt Neural Tangents eine exakte Inferenz entweder über die Bayes-Regel oder einen Gradientenabstieg durch und generiert die entsprechenden Neural Network Gaussian Process- und Neural Tangent-Kerne.Darüber hinaus bietet Neural Tangents Werkzeuge zur Untersuchung der Trainingsdynamik von breiten, aber endlichen Netzen mit Gradientenabstieg. Alle Berechnungen können automatisch auf mehrere Beschleuniger verteilt werden, wobei die Anzahl der Geräte nahezu linear skaliert werden kann. Zusätzlich zum unten stehenden Repository stellen wir ein interaktives Colab-Notebook unter https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb zur Verfügung.
Symbolische Logik ermöglicht es Praktikern, Systeme zu entwickeln, die regelbasierte Schlussfolgerungen ziehen, die interpretierbar sind und leicht durch Vorwissen ergänzt werden können. Solche Systeme sind jedoch aufgrund der großen linguistischen Variabilität der Sprache traditionell schwer auf Probleme mit natürlicher Sprache anzuwenden. Wir schlagen vor, die Vorteile beider Ansätze zu nutzen, indem wir eine Kombination aus neuronalen Netzen und logischer Programmierung auf die Beantwortung von Fragen in natürlicher Sprache anwenden: Wir schlagen vor, einen externen, nicht-differenzierbaren Prolog-Prover zu verwenden, der eine Ähnlichkeitsfunktion über vortrainierte Satzkodierer nutzt.  Wir evaluieren das vorgeschlagene System an zwei verschiedenen Aufgaben zur Beantwortung von Fragen und zeigen, dass es zwei sehr starke Grundsysteme - BIDAF (Seo et al., 2016a) und FASTQA (Weissenborn et al., 2017) - ergänzt und beide übertrifft, wenn es in einem Ensemble verwendet wird.
Trainingsetiketten sind teuer zu beschaffen und können von unterschiedlicher Qualität sein, da einige von vertrauenswürdigen Experten stammen können, während andere von Heuristiken oder anderen Quellen mit schwacher Überwachung wie Crowd-Sourcing stammen können.  Wir argumentieren, dass wir das Beste aus beiden Welten erhalten könnten, wenn der Lernende die Qualität der Beschriftung irgendwie kennen und berücksichtigen könnte.   Zu diesem Zweck stellen wir "fidelity-weighted learning" (FWL) vor, einen halbüberwachten Schüler-Lehrer-Ansatz für das Training von tiefen neuronalen Netzen mit schwach etikettierten Daten. FWL moduliert die Parameteraktualisierungen für ein Schülernetzwerk, das für die Aufgabe, die uns interessiert, auf einer pro-Probe-Basis trainiert wird, entsprechend der posterioren Konfidenz seiner Label-Qualität, die von einem Lehrer geschätzt wird, der Zugang zu begrenzten Proben mit hochwertigen Labels hat.
Insbesondere liefern wir empirische und theoretische Beweise dafür, dass Methoden erster Ordnung, wie z.B. der Gradientenabstieg, trotz Überparametrisierung unter einem reichhaltigen Datenmodell nachweislich robust gegenüber Rauschen/Verfälschung auf einem konstanten Anteil der Labels sind. i) Erstens zeigen wir, dass in den ersten paar Iterationen, in denen die Updates noch in der Nähe der Initialisierung liegen, diese Algorithmen nur auf die korrekten Labels passen und die verrauschten Labels im Wesentlichen ignorieren. ii) Zweitens zeigen wir, dass sich die Algorithmen weit vom Ausgangsmodell entfernen müssen, um eine Überanpassung an die verrauschten Etiketten zu erreichen, was erst nach vielen weiteren Iterationen der Fall sein kann, was wiederum zeigt, dass der Gradientenabstieg mit frühem Stoppen nachweislich robust gegenüber Etikettenrauschen ist und die empirische Robustheit von tiefen Netzen sowie allgemein angenommene Heuristiken zum frühen Stoppen beleuchtet.
Obwohl durch Pruning eine beträchtliche Menge an Gewichten in einem Netzwerk entfernt wird, war die Reduzierung des Speicherbedarfs begrenzt, da herkömmliche spärliche Matrixformate eine beträchtliche Menge an Speicher benötigen, um indexbezogene Informationen zu speichern. Darüber hinaus sind die Berechnungen, die mit solchen Sparse-Matrix-Formaten verbunden sind, langsam, da der sequenzielle Sparse-Matrix-Dekodierungsprozess hochparallele Rechensysteme nicht effizient nutzt.Als Versuch, Indexinformationen zu komprimieren und gleichzeitig den Dekodierungsprozess zu parallelisieren, wurde das Viterbi-basierte Pruning vorgeschlagen. In diesem Papier schlagen wir ein neues Sparse-Matrix-Format vor, um einen hochparallelen Dekodierungsprozess der gesamten Sparse-Matrix zu ermöglichen. Die vorgeschlagene Sparse-Matrix wird durch die Kombination von Pruning und Gewichtsquantisierung konstruiert. Für die neuesten RNN-Modelle auf PTB- und WikiText-2-Korpus wird der LSTM-Parameter-Speicherbedarf 19x komprimiert, indem das vorgeschlagene Sparse-Matrix-Format im Vergleich zum Basismodell verwendet wird. Simulationsergebnisse zeigen, dass das vorgeschlagene Schema die Parameter 20 % bis 106 % schneller in die Verarbeitungselemente einspeisen kann als der Fall, in dem die Werte der dichten Matrix direkt aus dem DRAM kommen.
Die Verwendung von Deep-Learning-Modellen als Priors für Compressive-Sensing-Aufgaben stellt ein neues Potenzial für die kostengünstige seismische Datenerfassung dar.Ein geeignetes generatives adversariales Netzwerk von Wasserstein wird auf der Grundlage einer generativen adversarischen Netzwerkarchitektur entwickelt, die auf mehreren historischen Erhebungen trainiert wurde und in der Lage ist, die statistischen Eigenschaften der seismischen Wavelets zu erlernen.Die Anwendung der Validierung und Leistungsprüfung des Compressive Sensing erfolgt in drei Schritten.Zunächst wird die Existenz einer spärlichen Darstellung mit unterschiedlichen Kompressionsraten für seismische Erhebungen untersucht. Das primäre Ziel des vorgeschlagenen Deep-Learning-Modells ist es, die Grundlagen für ein optimales Design für die seismische Erfassung zu schaffen, mit weniger Verlusten bei der Bildqualität. In diesem Sinne wird ein Compressive-Sensing-Design eines ungleichmäßigen Gitters über einer Anlage im Golf von Mexiko vorgeschlagen, im Gegensatz zu einem traditionellen seismischen Überwachungsgitter, das Daten gleichmäßig alle paar Meter sammelt, und das die vorgeschlagene Methode nutzt.
In den letzten Jahren haben wir rasche Fortschritte bei einer Reihe von Benchmark-Problemen im Bereich der künstlichen Intelligenz gesehen, wobei moderne Methoden bei Go, Poker und Dota fast oder sogar übermenschliche Leistungen erzielt haben. Ein gemeinsamer Aspekt all dieser Herausforderungen besteht darin, dass sie von vornherein kontraproduktiv oder, technisch gesprochen, Nullsummenspiele sind. Im Gegensatz zu diesen Situationen erfordert der Erfolg in der realen Welt in der Regel, dass Menschen zusammenarbeiten und mit anderen kommunizieren, und zwar in Situationen, die zumindest teilweise kooperativ sind. Im letzten Jahr hat sich das Kartenspiel Hanabi als neue Benchmark-Umgebung für KI etabliert, um diese Lücke zu füllen. Hanabi ist insbesondere für Menschen interessant, da es sich ganz auf die Theory of Mind konzentriert, d.h. die Fähigkeit, die Absichten, Überzeugungen und Sichtweisen anderer Agenten effektiv zu verstehen, wenn sie deren Handlungen beobachten.Zu lernen, informativ zu sein, wenn man von anderen beobachtet wird, ist eine interessante Herausforderung für Reinforcement Learning (RL): Grundsätzlich erfordert RL, dass Agenten zu erforschen, um gute Politik zu entdecken.Allerdings, wenn naiv getan, wird diese Zufälligkeit inhärent machen ihre Aktionen weniger informativ für andere während der Ausbildung.Wir präsentieren eine neue tiefe Multi-Agenten RL-Methode, die Simplified Action Decoder (SAD), die diesen Widerspruch unter Ausnutzung der zentralisierten Trainingsphase löst. Durch die Kombination dieser einfachen Intuition mit einer Hilfsaufgabe für die Zustandsvorhersage und den besten Praktiken für Multi-Agenten-Lernen, etabliert SAD einen neuen Stand der Technik für 2-5 Spieler im Selbstspielteil der Hanabi-Herausforderung.
Wir stellen einen durchgängig trainierbaren Ansatz für die optische Zeichenerkennung (OCR) auf gedruckten Dokumenten vor, der auf der Vorhersage eines zweidimensionalen Zeichenrasters ("chargrid") als semantische Segmentierungsaufgabe basiert. Um einzelne Zeicheninstanzen aus dem chargrid zu identifizieren, betrachten wir Zeichen als Objekte und verwenden Objekterkennungstechniken aus der Computer Vision.
Wir schlagen NovoGrad vor, eine adaptive stochastische Gradientenabstiegsmethode mit schichtweiser Gradientennormalisierung und entkoppeltem Gewichtsabfall, die in unseren Experimenten mit neuronalen Netzen für Bildklassifikation, Spracherkennung, maschinelle Übersetzung und Sprachmodellierung gleich gut oder besser abschneidet als gut abgestimmte SGD mit Momentum und Adam/AdamW. Darüber hinaus ist NovoGrad (1) robust gegenüber der Wahl der Lernrate und der Initialisierung der Gewichte, (2) funktioniert gut in einer großen Batch-Umgebung und (3) hat einen zweimal kleineren Speicherbedarf als Adam.
Die meisten generativen Audiomodelle erzeugen direkt Samples in einem von zwei Bereichen: Zeit oder Frequenz. Diese Darstellungen reichen zwar aus, um jedes Signal auszudrücken, sind aber ineffizient, da sie das vorhandene Wissen darüber, wie Klang erzeugt und wahrgenommen wird, nicht nutzen. Ein dritter Ansatz (Vocoder/Synthesizer) nutzt erfolgreich das Wissen über Signalverarbeitung und -wahrnehmung, wurde aber aufgrund der begrenzten Ausdrucksfähigkeit und der Schwierigkeiten bei der Integration mit modernen, auf Autodifferenzierung basierenden maschinellen Lernmethoden weniger aktiv erforscht. In diesem Beitrag stellen wir die Differentiable Digital Signal Processing (DDSP)-Bibliothek vor, die eine direkte Integration von klassischen Signalverarbeitungselementen mit Deep-Learning-Methoden ermöglicht. Mit dem Fokus auf Audiosynthese erreichen wir eine High-Fidelity-Generierung ohne große autoregressive Modelle oder adversarische Verluste. Wir zeigen, dass DDSP die Nutzung starker induktiver Verzerrungen ermöglicht, ohne die Ausdruckskraft neuronaler Netzwerke zu verlieren. Darüber hinaus zeigen wir, dass die Kombination interpretierbarer Module die Manipulation jeder einzelnen Modellkomponente ermöglicht, mit Anwendungen wie der unabhängigen Kontrolle von Tonhöhe und Lautstärke, der realistischen Extrapolation auf Tonhöhen, die während des Trainings nicht gesehen wurden, der blinden Dereverberation der Raumakustik, der Übertragung der extrahierten Raumakustik auf neue Umgebungen und der Transformation der Klangfarbe zwischen unterschiedlichen Quellen. Kurz gesagt, DDSP ermöglicht einen interpretierbaren und modularen Ansatz zur generativen Modellierung, ohne die Vorteile von Deep Learning zu opfern. Die Bibliothek ist unter https://github.com/magenta/ddsp verfügbar und wir ermutigen weitere Beiträge aus der Community und von Fachleuten.
Spectral Graph Convolutional Networks (GCNs) sind eine Verallgemeinerung von Faltungsnetzen für das Lernen auf graphenstrukturierten Daten.Anwendungen von spektralen GCNs sind erfolgreich, aber auf einige wenige Probleme beschränkt, bei denen der Graph fixiert ist, wie z.B. Formkorrespondenz und Knotenklassifizierung. In dieser Arbeit befassen wir uns mit dieser Einschränkung, indem wir eine bestimmte Familie von spektralen Graphen-Netzwerken, die Tschebyscheff-GCNs, überarbeiten und ihre Wirksamkeit bei der Lösung von Graphen-Klassifizierungsaufgaben mit einer variablen Graphenstruktur und -größe zeigen. Wir modellieren explizit verschiedene Arten von Kanten: kommentierte Kanten, gelernte Kanten mit abstrakter Bedeutung und hierarchische Kanten.Wir experimentieren auch mit verschiedenen Möglichkeiten, die aus verschiedenen Kantenarten extrahierten Repräsentationen zu verschmelzen.Diese Einschränkung wird manchmal von einem Datensatz impliziert, aber wir lockern diese Einschränkung für alle Arten von Datensätzen.Wir erreichen State-of-the-Art-Ergebnisse auf einer Vielzahl von chemischen, sozialen und Vision-Graph-Klassifizierung Benchmarks.
Tiefe neuronale Netze haben sich kürzlich als anfällig für Backdoor-Angriffe erwiesen, d.h. ein Angreifer kann durch die Veränderung einer kleinen Menge von Trainingsbeispielen eine Hintertür installieren, die während der Inferenz verwendet werden kann, um das Verhalten des Modells vollständig zu kontrollieren.Obwohl der Angriff sehr mächtig ist, hängt er entscheidend davon ab, dass der Angreifer in der Lage ist, willkürliche, oft eindeutig falsch gekennzeichnete Eingaben in die Trainingsmenge einzuführen, und kann daher sogar durch eine recht rudimentäre Datenfilterung entdeckt werden. In diesem Beitrag stellen wir einen neuen Ansatz zur Durchführung von Backdoor-Angriffen vor, bei dem Beispiele von Angreifern und GAN-generierte Daten verwendet werden.
Trotz ihrer Fähigkeit, sich große Datensätze zu merken, erreichen tiefe neuronale Netze oft eine gute Generalisierungsleistung, wobei die Unterschiede zwischen den gelernten Lösungen von Netzen, die generalisieren, und solchen, die dies nicht tun, unklar bleiben.Zusätzlich wurden die Tuning-Eigenschaften einzelner Richtungen (definiert als die Aktivierung einer einzelnen Einheit oder einer linearen Kombination von Einheiten als Reaktion auf eine Eingabe) hervorgehoben, aber ihre Bedeutung wurde nicht bewertet. Hier verbinden wir diese Untersuchungslinien, um zu zeigen, dass die Abhängigkeit eines Netzwerks von einzelnen Richtungen ein guter Prädiktor für seine Generalisierungsleistung ist, und zwar über Netzwerke hinweg, die auf Datensätzen mit unterschiedlichen Anteilen an verfälschten Bezeichnungen trainiert wurden, über Ensembles von Netzwerken hinweg, die auf Datensätzen mit unveränderten Bezeichnungen trainiert wurden, über verschiedene Hyperparameter hinweg und über den Verlauf des Trainings. Während Dropout diese Größe nur bis zu einem gewissen Punkt reguliert, entmutigt die Batch-Normalisierung implizit die Abhängigkeit von einer einzigen Richtung, zum Teil durch die Verringerung der Klassenselektivität einzelner Einheiten.Schließlich finden wir, dass die Klassenselektivität ein schlechter Prädiktor für die Wichtigkeit der Aufgabe ist, was nicht nur darauf hindeutet, dass Netzwerke, die gut verallgemeinern, ihre Abhängigkeit von einzelnen Einheiten durch die Verringerung ihrer Selektivität minimieren, sondern auch, dass individuell selektive Einheiten für eine starke Netzwerkleistung nicht notwendig sind.
Anstelle eines Encoder-Decoder-Paares können wir ein einzelnes Inferenznetzwerk direkt aus den Daten trainieren, indem wir eine Kostenfunktion verwenden, die nicht nur über Stichproben, sondern auch über Abfragen stochastisch ist. Mit diesem Netzwerk können wir die gleichen Inferenzaufgaben durchführen wie in einem ungerichteten grafischen Modell mit verborgenen Variablen, ohne uns mit der schwer zu lösenden Partitionsfunktion auseinandersetzen zu müssen.  Wir zeigen, dass sich unser Ansatz auf ungesehene probabilistische Abfragen auf ebenfalls ungesehenen Testdaten verallgemeinern lässt und somit eine schnelle und flexible Inferenz ermöglicht.Experimente zeigen, dass dieser Ansatz PCD und AdVIL auf 9 Benchmark-Datensätzen übertrifft oder gleichwertig ist.
Vor dem Wiederaufleben des Deep Learning war die dominierende Familie zur Lösung solcher Optimierungsprobleme die numerische Optimierung, z.B. Gauss-Newton (GN).In jüngerer Zeit wurden mehrere Versuche unternommen, lernfähige GN-Schritte als Kaskadenregressionsarchitekturen zu formulieren.In diesem Papier untersuchen wir neuere Architekturen des maschinellen Lernens, wie z.B. tiefe neuronale Netze mit Restverbindungen, unter der oben genannten Perspektive. Zu diesem Zweck zeigen wir zunächst, wie Residualblöcke (wenn sie als Diskretisierung von ODEs betrachtet werden) als GN-Schritte angesehen werden können, und gehen dann einen Schritt weiter und schlagen einen neuen Residualblock vor, der an die Newton-Methode in der numerischen Optimierung erinnert und eine schnellere Konvergenz aufweist.Wir bewerten das vorgeschlagene Newton-ResNet gründlich, indem wir Experimente zur Bild- und Sprachklassifizierung sowie zur Bilderzeugung unter Verwendung von vier Datensätzen durchführen.Alle Experimente zeigen, dass Newton-ResNet weniger Parameter benötigt, um die gleiche Leistung wie das ursprüngliche ResNet zu erzielen.
In Wettbewerbssituationen können Agenten zur Erreichung ihrer Ziele Aktionen ausführen, die unwissentlich die Ziele eines Gegners erleichtern: (Der Benutzer und der Angreifer konkurrieren, um unterschiedliche Ziele zu erreichen.Wenn es eine Diskrepanz im Domänenwissen des Benutzers und des Angreifers gibt, kann der Angreifer die Unvertrautheit des Benutzers mit der Domäne zu seinem Vorteil nutzen und sein eigenes Ziel vorantreiben.In dieser Situation muss der Beobachter, dessen Ziel es ist, den Benutzer zu unterstützen, möglicherweise eingreifen, und dieser Eingriff muss online, rechtzeitig und genau erfolgen. Wir haben einen Klassifikator trainiert, der domänenunabhängige Merkmale verwendet, die aus dem Entscheidungsraum des Beobachters extrahiert werden, um die "Kritikalität" des aktuellen Zustands zu bewerten. Das trainierte Modell wird dann in einer Online-Umgebung auf IPC-Benchmarks verwendet, um Beobachtungen zu identifizieren, die ein Eingreifen rechtfertigen.Unsere Beiträge legen eine Grundlage für weitere Arbeiten auf dem Gebiet der Entscheidung, wann eingegriffen werden muss.
Während frühere Arbeiten sich auf die Ausnutzung statistischer Unabhängigkeit konzentrierten, um latente Faktoren zu unterscheiden, argumentieren wir, dass diese Anforderung vorteilhaft gelockert werden kann und schlagen stattdessen einen nicht-statistischen Rahmen vor, der sich auf die Identifizierung einer modularen Organisation des Netzwerks stützt, basierend auf kontrafaktischen Manipulationen. Unsere Experimente belegen, dass die Modularität zwischen Gruppen von Kanälen bis zu einem gewissen Grad auf einer Vielzahl von generativen Modellen erreicht wird, was den Entwurf gezielter Eingriffe in komplexe Bilddatensätze ermöglicht und den Weg für Anwendungen wie rechnerisch effizienten Stiltransfer und die automatische Bewertung der Robustheit gegenüber kontextuellen Veränderungen in Mustererkennungssystemen öffnet.
Wir schlagen eine Differentiable Hebbian Plasticity (DHP) Softmax-Schicht vor, die den langsamen Gewichten der Softmax-Ausgangsschicht eine schnell lernende plastische Komponente hinzufügt, die sich wie ein komprimiertes episodisches Gedächtnis verhält, das bestehende Gedächtnisspuren reaktiviert und gleichzeitig neue erzeugt. Wir demonstrieren die Flexibilität unseres Modells, indem wir es mit bekannten Konsolidierungsmethoden kombinieren, um katastrophales Vergessen zu verhindern. Wir evaluieren unseren Ansatz an den Benchmarks Permuted MNIST und Split MNIST und führen Imbalanced Permuted MNIST ein - ein Datensatz, der die Herausforderungen von Klassenungleichgewicht und Konzeptdrift kombiniert. Unser Modell benötigt keine zusätzlichen Hyperparameter und übertrifft vergleichbare Baselines, indem es das Vergessen reduziert.
Während reale Gehirnnetzwerke funktionale Modularität aufweisen, untersuchen wir, ob funktionale Modularität auch in Deep Neural Networks (DNN) existiert, die durch Backpropagation trainiert werden. Unter der Hypothese, dass DNN auch in aufgabenspezifischen Modulen organisiert sind, versuchen wir in diesem Papier, eine versteckte Schicht in disjunkte Gruppen von aufgabenspezifischen versteckten Neuronen mit Hilfe von relativ gut untersuchten Neuronen-Attributionsmethoden zu zerlegen. Wir argumentieren, dass solche Gruppen von Neuronen, die wir als funktionale Module bezeichnen, als grundlegende funktionale Einheit in DNN dienen können.Wir schlagen eine vorläufige Methode vor, um funktionale Module über Bi-Cluster-Attributions-Scores von versteckten Neuronen zu identifizieren.Wir stellen fest, dass die funktionalen Neuronen zunächst wenig überraschend sehr spärlich sind, d.h. nur eine kleine Untermenge von Neuronen, Wir stellen fest, dass die funktionalen Neuronen sehr spärlich sind, d.h. nur eine kleine Untergruppe von Neuronen ist wichtig für die Vorhersage einer kleinen Untergruppe von Datenproben, und dass, obwohl wir keine Label-Überwachung verwenden, Proben, die derselben Gruppe (Bicluster) entsprechen, überraschend kohärente Merkmalsmuster zeigen.Wir zeigen auch, dass diese funktionalen Module eine entscheidende Rolle bei der Unterscheidung von Datenproben durch Ablationsexperimente spielen.
Power-effiziente CNN Domain Specific Accelerator (CNN-DSA) Chips sind derzeit für den breiten Einsatz in mobilen Geräten zur Verfügung.Diese Chips sind vor allem in Computer-Vision-Anwendungen verwendet.Allerdings hat die jüngste Arbeit von Super Characters Methode für Text-Klassifikation und Sentiment-Analyse Aufgaben mit zweidimensionalen CNN-Modelle auch erreicht State-of-the-Art-Ergebnisse durch die Methode des Transfer-Lernens von Vision zu text.In diesem Papier, implementiert wir die Text-Klassifikation und Sentiment-Analyse-Anwendungen auf mobilen Geräten mit CNN-DSA-Chips. Kompakte Netzwerkrepräsentationen mit einer Genauigkeit von einem Bit und drei Bits für Koeffizienten und fünf Bits für Aktivierungen werden im CNN-DSA-Chip mit einer Leistungsaufnahme von weniger als 300 mW verwendet. Für Edge-Geräte mit Speicher- und Rechenbeschränkungen wird das Netzwerk weiter komprimiert, indem die externen Fully Connected (FC)-Schichten innerhalb des CNN-DSA-Chips approximiert werden.Auf dem Workshop haben wir zwei Systemdemonstrationen für NLP-Aufgaben.Die erste Demo klassifiziert den eingegebenen englischen Wikipedia-Satz in eine der 14 Klassen.Die zweite Demo klassifiziert die chinesische Online-Shopping-Rezension in positiv oder negativ.
Um einen effizienten Vergleich zu ermöglichen, führen wir ein amortisiertes Variationsinferenzverfahren ein, das eine schnelle und zuverlässige Posterior-Schätzung für Modelle der gleichen Architektur ermöglicht. Unser Any Parameter Encoder (APE) erweitert das neuronale Encodernetzwerk, das in der amortisierten Inferenz üblich ist, um sowohl einen Datenmerkmalvektor als auch einen Modellparametervektor als Eingabe zu verwenden. In Experimenten zum Vergleich von Themenmodellen für synthetische Daten und Produktrezensionen liefert unser Any-Parameter-Encoder vergleichbare Posterioren zu teureren Methoden in weitaus kürzerer Zeit, insbesondere wenn die Encoder-Architektur in modellbasierter Weise entworfen wird.
Die Fähigkeit, Wissen auf neuartige Umgebungen und Aufgaben zu übertragen, ist ein sinnvolles Desiderat für allgemeine Lernagenten. Trotz der offensichtlichen Versprechungen ist die Übertragung in RL immer noch ein offenes und wenig genutztes Forschungsgebiet.In diesem Papier nehmen wir eine brandneue Perspektive über die Übertragung: wir schlagen vor, dass die Fähigkeit, Kredit zuzuweisen, strukturelle Invarianten in den Aufgaben enthüllt, die übertragen werden können, um RL effizienter zu machen. Unser Hauptbeitrag ist Secret, ein neuartiger Ansatz zum Transfer-Lernen für RL, der einen rückwärtsgerichteten Credit-Zuweisungsmechanismus verwendet, der auf einer selbst-aufmerksamen Architektur basiert. Zwei Aspekte sind der Schlüssel zu seiner Allgemeingültigkeit: Er lernt die Credit-Zuweisung als einen separaten, offline überwachten Prozess und modifiziert ausschließlich die Belohnungsfunktion.
Die jüngsten Fortschritte auf dem Gebiet der neuronalen Variationsinferenz ermöglichten eine Renaissance der latenten Variablenmodelle in einer Vielzahl von Bereichen mit hochdimensionalen Daten.In diesem Papier stellen wir zwei generische Variationsinferenz-Frameworks für generative Modelle von Wissensgraphen vor: Latent Fact Model und Latent Information Model.  Während traditionelle Variationsmethoden eine analytische Näherung für die schwer zu fassende Verteilung über die latenten Variablen ableiten, konstruieren wir hier ein Inferenznetzwerk, das auf der symbolischen Repräsentation von Entitäten und Beziehungstypen im Wissensgraphen basiert, um die Variationsverteilungen bereitzustellen. Der neue Rahmen kann Modelle erstellen, die in der Lage sind, die zugrundeliegende probabilistische Semantik für die symbolische Repräsentation zu entdecken, indem parametrisierbare Verteilungen verwendet werden, die ein Training durch Back-Propagation im Kontext der neuronalen Variationsinferenz ermöglichen, was zu einer hochskalierbaren Methode führt.im Rahmen eines Bernoulli-Sampling-Rahmens bieten wir eine alternative Rechtfertigung für häufig verwendete Techniken in der groß angelegten stochastischen Variationsinferenz, die die Trainingszeit drastisch reduziert, allerdings um den Preis einer zusätzlichen Annäherung an die Variationsuntergrenze.  Der generative Rahmen ist flexibel genug, um das Training unter jeder Prior-Verteilung zu ermöglichen, die einen Re-Parametrisierungs-Trick erlaubt, sowie unter jeder Scoring-Funktion, die eine Maximum-Likelihood-Schätzung der Parameter erlaubt.Experimentelle Ergebnisse zeigen das Potenzial und die Effizienz dieses Rahmens durch die Verbesserung mehrerer Benchmarks mit Gauß'schen Prior-Darstellungen.Code öffentlich verfügbar auf Github.
Die generative Abbildung vom latenten Raum in den Datenraum folgt einem dynamischen System, in dem eine erlernbare Potentialfunktion ein kompressibles Fluid dazu bringt, in Richtung der Zieldichteverteilung zu fließen.Das Training des Modells läuft auf die Lösung eines optimalen Steuerungsproblems hinaus. Wir wenden den Ansatz auf die unbeaufsichtigte Dichteabschätzung des MNIST-Datensatzes und die Variationsberechnung des zweidimensionalen Ising-Modells am kritischen Punkt an. Dieser Ansatz bringt Einsichten und Techniken aus der Monge-Ampere-Gleichung, dem optimalen Transport und der Fluiddynamik in generative Modelle mit reversibler Strömung ein.
Graph Neural Networks (GNNs) sind eine Klasse von tiefen Modellen, die auf Daten mit beliebiger Topologie und ordnungsinvarianter Struktur arbeiten, die als Graphen dargestellt werden.Wir stellen eine effiziente Speicherschicht für GNNs vor, die lernen kann, Graphenrepräsentationslernen und Graphenpooling gemeinsam durchzuführen.Wir stellen auch zwei neue Netzwerke vor, die auf unserer Speicherschicht basieren: Memory-Based Graph Neural Network (MemGNN) und Graph Memory Network (GMN), die hierarchische Graphenrepräsentationen durch Vergröberung des Graphen in den Speicherschichten erlernen können.Die experimentellen Ergebnisse zeigen, dass die vorgeschlagenen Modelle in sechs von sieben Graphenklassifizierungs- und Regressionsbenchmarks Spitzenergebnisse erzielen.Wir zeigen auch, dass die erlernten Repräsentationen den chemischen Merkmalen in den Moleküldaten entsprechen könnten.
Um dieses Problem zu lösen, schlagen wir ein neuartiges Kodierungsschema vor, das {-1,+1} verwendet, um quantisierte neuronale Netze (QNNs) in binäre Netze mit mehreren Zweigen zu zerlegen, die effizient durch bitweise Operationen (xnor und bitcount) implementiert werden können, um eine Modellkompression, Rechenbeschleunigung und Ressourceneinsparung zu erreichen. Unsere Methode kann maximal ~59 Beschleunigung und ~32 Speicherersparnis im Vergleich zu ihren Gegenstücken mit voller Präzision erreichen, so dass Benutzer leicht verschiedene Kodiergenauigkeiten entsprechend ihren Anforderungen und Hardwareressourcen erreichen können.unser Mechanismus ist sehr geeignet für die Verwendung von FPGA und ASIC in Bezug auf die Datenspeicherung und Berechnung, die eine machbare Idee für intelligente Chips bietet.wir validieren die Wirksamkeit unserer Methode auf beiden groß angelegten Bildklassifizierung (z. B., ImageNet) und Objekterkennungsaufgaben.
Diese Methode basiert auf der Verschmelzung von Merkmalen aus den generierten und bedingten Informationen im Merkmalsraum und ermöglicht es dem Diskriminator, Statistiken höherer Ordnung aus den Daten besser zu erfassen und erhöht die Stärke der Signale, die durch das Netzwerk geleitet werden, wenn die realen oder generierten Daten und die bedingten Daten übereinstimmen. Die vorgeschlagene Methode ist konzeptionell einfacher als die gemeinsamen Modelle aus Faltungsneuronalem Netz und bedingtem Markov-Zufallsfeld (CNN-CRF) und erzwingt die Konsistenz höherer Ordnung, ohne auf eine sehr spezifische Klasse von Potenzialen höherer Ordnung beschränkt zu sein.Experimentelle Ergebnisse zeigen, dass diese Methode zu einer Verbesserung bei einer Vielzahl verschiedener strukturierter Vorhersageaufgaben führt, darunter Bildsynthese, semantische Segmentierung und Tiefenschätzung.
Intelligente Lebewesen können ihre Umgebung erforschen und nützliche Fähigkeiten ohne Aufsicht erlernen. In diesem Papier schlagen wir ``Diversity is All You Need'' (DIAYN) vor, eine Methode zum Erlernen nützlicher Fähigkeiten ohne eine Belohnungsfunktion. Unsere vorgeschlagene Methode lernt Fähigkeiten durch Maximierung eines informationstheoretischen Ziels unter Verwendung einer maximalen Entropiepolitik. In einer Reihe von Reinforcement-Learning-Benchmark-Umgebungen ist unsere Methode in der Lage, eine Fähigkeit zu erlernen, die die Benchmark-Aufgabe löst, obwohl sie nie die tatsächliche Belohnung für die Aufgabe erhält.Wir zeigen, wie vortrainierte Fähigkeiten eine gute Parameterinitialisierung für nachgelagerte Aufgaben bieten und hierarchisch zusammengesetzt werden können, um komplexe, spärliche Belohnungsaufgaben zu lösen.Unsere Ergebnisse deuten darauf hin, dass die unbeaufsichtigte Entdeckung von Fähigkeiten als effektiver Pre-Training-Mechanismus dienen kann, um die Herausforderungen der Exploration und der Dateneffizienz beim Reinforcement Learning zu überwinden.
Lexikalische Mehrdeutigkeit, d.h., Obwohl der Einsatz von rekurrenten neuronalen Netzen und Aufmerksamkeitsmechanismen dieses Problem lösen soll, sind maschinelle Übersetzungssysteme nicht immer in der Lage, lexikalisch mehrdeutige Sätze korrekt zu übersetzen. In dieser Arbeit versuche ich, das Problem der lexikalischen Mehrdeutigkeit in neuronalen maschinellen Übersetzungssystemen für Englisch-Japanisch zu lösen, indem ich ein vortrainiertes BERT-Sprachmodell (Bidirectional Encoder Representations from Transformer), das kontextualisierte Worteinbettungen erzeugen kann, mit einem Transformer-Übersetzungsmodell kombiniere, das eine State-of-the-Art-Architektur für die Aufgabe der maschinellen Übersetzung ist. Darüber hinaus erreicht eines der vorgeschlagenen Modelle, Transformer_BERT-WE, einen höheren BLEU-Score im Vergleich zum Transformer-Vanilla-Modell, was ein konkreter Beweis dafür ist, dass die Verwendung von kontextualisierten Worteinbettungen aus BERT nicht nur das Problem der lexikalischen Mehrdeutigkeit lösen, sondern auch die Übersetzungsqualität im Allgemeinen verbessern kann.
Wir präsentieren eine neuartige und skalierbare Anwendung von Generative Adversarial Networks (GANs) für die Modellierung und Generierung von Daten zur menschlichen Mobilität.Wir nutzen tatsächliche Mitfahranfragen von Ride-Sharing/Hailing-Diensten aus vier großen Städten in den USA, um unser GAN-Modell zu trainieren.Unser Modell erfasst die räumliche und zeitliche Variabilität der Mitfahranfragen, die für alle vier Städte an einem typischen Tag und in einer typischen Woche beobachtet wurden. In früheren Arbeiten wurden die räumlichen und zeitlichen Eigenschaften von Mobilitätsdatensätzen mit Hilfe der fraktalen Dimensionalität bzw. des Verdichtungsgesetzes kurz charakterisiert, die wir zur Validierung unserer mit GANs erzeugten synthetischen Datensätze verwenden Solche synthetischen Datensätze können Bedenken hinsichtlich des Datenschutzes ausräumen und sind für Forscher und politische Entscheidungsträger im Bereich der städtischen Mobilität und des intelligenten Verkehrs äußerst nützlich.
Während tiefe neuronale Netze eine sehr erfolgreiche Modellklasse sind, stellt ihr großer Speicherbedarf eine erhebliche Belastung für den Energieverbrauch, die Kommunikationsbandbreite und die Speicheranforderungen dar, so dass die Reduzierung der Modellgröße zu einem der wichtigsten Ziele des tiefen Lernens geworden ist.In Anlehnung an das klassische Bits-Back-Argument kodieren wir die Netzwerkgewichte mit Hilfe einer Zufallsstichprobe und benötigen nur eine Anzahl von Bits, die der Kullback-Leibler-Divergenz zwischen der abgetasteten Variationsverteilung und der kodierenden Verteilung entspricht. Das verwendete Kodierungsschema liegt nachweislich nahe an der optimalen informationstheoretischen unteren Schranke in Bezug auf die verwendete Variationsfamilie. bei den Benchmarks LeNet-5/MNIST und VGG-16/CIFAR-10 liefert unser Ansatz die beste Testleistung bei einem festen Speicherbudget und erreicht umgekehrt die höchsten Kompressionsraten bei einer festen Testleistung.
Das zugrundeliegende Prinzip ist, dass neuronale Netze, die auf großen Datensätzen trainiert wurden, empirisch gezeigt haben, dass sie in der Lage sind, natürliche Bilder aus einer niedrigdimensionalen latenten Repräsentation des Bildes zu generieren, und dass ein verrauschtes Bild entrauscht werden kann, indem das nächstgelegene Bild im Bereich des Priors gefunden wird, aber es gibt wenig Theorie, um diesen Erfolg zu rechtfertigen, geschweige denn, um die Entrauschungsleistung als Funktion der Netzparameter vorherzusagen. In diesem Papier betrachten wir das Problem der Entrauschung eines Bildes von additivem Gaußschen Rauschen unter der Annahme, dass das Bild gut durch ein tiefes neuronales Netzwerk mit ReLu-Aktivierungsfunktionen beschrieben wird, das einen k-dimensionalen latenten Raum auf ein n-dimensionales Bild abbildet. Wir beschreiben und analysieren einen einfachen Gradientenabstieg-ähnlichen iterativen Algorithmus, der eine nicht-konvexe Verlustfunktion minimiert und nachweislich einen Bruchteil (1 - O(k/n)) der Rauschenergie entfernt.
Deep Learning liefert großartige Ergebnisse in vielen Bereichen, von der Spracherkennung über die Bildklassifizierung bis hin zur Übersetzung.Aber für jedes Problem muss ein Deep Model gut funktionieren, was die Erforschung der Architektur und eine lange Abstimmungsphase erfordert.Wir stellen ein einzelnes Modell vor, das gute Ergebnisse bei einer Reihe von Problemen liefert, die mehrere Domänen umfassen.Insbesondere wird dieses einzelne Modell gleichzeitig auf ImageNet, mehreren Übersetzungsaufgaben, Bildunterschriften (COCO-Datensatz), einem Spracherkennungskorpus und einer englischen Parsing-Aufgabe trainiert. Unsere Modellarchitektur enthält Bausteine aus mehreren Bereichen: Faltungsschichten, einen Aufmerksamkeitsmechanismus und dünn besetzte Schichten. Jeder dieser Rechenblöcke ist für eine Teilmenge der Aufgaben, die wir trainieren, von entscheidender Bedeutung. Interessanterweise stellen wir fest, dass selbst wenn ein Block für eine Aufgabe nicht entscheidend ist, das Hinzufügen dieses Blocks die Leistung nie beeinträchtigt und in den meisten Fällen bei allen Aufgaben verbessert.
Ein solches Ziel kann mit Konzepten erreicht werden, die der kontinentalen Philosophie entlehnt sind und mit Hilfe von Werkzeugen aus der mathematischen Kategorientheorie formalisiert werden.Illustrationen dieses Ansatzes werden an einem cyberphysischen System vorgestellt: dem Slotcar-Spiel und auch an Atari 2600-Spielen.
Audiosignale werden mit hohen zeitlichen Auflösungen abgetastet, und das Erlernen der Audiosynthese erfordert die Erfassung von Strukturen über eine Reihe von Zeitskalen.Generative adversarische Netzwerke (GANs) haben großen Erfolg bei der Erzeugung von Bildern, die sowohl lokal als auch global kohärent sind, aber sie haben wenig Anwendung auf die Audio-Generierung gesehen.In diesem Beitrag stellen wir WaveGAN vor, einen ersten Versuch der Anwendung von GANs auf die unüberwachte Synthese von Roh-Wellenform-Audio. WaveGAN ist in der Lage, eine Sekunde Scheiben von Audio-Wellenformen mit globaler Kohärenz, geeignet für Sound-Effekt generation.Our Experimente zeigen, dass - ohne Etiketten-WaveGAN lernt, verständliche Worte zu produzieren, wenn auf einem kleinen Wortschatz Sprache Datensatz trainiert, und kann auch synthetisieren Audio aus anderen Bereichen wie Trommeln, Vogelgesang, und piano.We vergleichen WaveGAN zu einer Methode, die GANs für die Bilderzeugung auf Bild-ähnliche Audio-Merkmal Repräsentationen entwickelt, finden beide Ansätze vielversprechend zu sein.
Die Schwierigkeit, ausreichend beschriftete Daten für überwachtes Lernen zu erhalten, hat die Domänenanpassung motiviert, bei der ein Klassifikator in einer Domäne, der Quelldomäne, trainiert wird, aber in einer anderen, der Zieldomäne, arbeitet. Die Verringerung der Domänendiskrepanz hat die Leistung verbessert, wird aber durch die eingebetteten Merkmale behindert, die keine klar trennbaren und ausgerichteten Cluster bilden. Insbesondere beweisen wir, dass Zyklus-Konsistenz führt die eingebetteten Features entfernt von allen, aber ein Cluster, wenn die Quelle domain is ideal clustered.we zusätzlich nutzen mehr Informationen aus approximierten lokalen manifold und verfolgen lokale manifold consistency für weitere improvement.Results für verschiedene Domain-Anpassung Szenarien zeigen engere Clustering und eine Verbesserung der Klassifizierung Genauigkeit.
Ähnlich wie bei früheren Arbeiten modellieren wir die Unsicherheit nur in der letzten Schicht des Netzes und behandeln den Rest des Netzes als Merkmalsextraktor, wodurch wir dank der effizienten, in geschlossener Form vorliegenden Unsicherheitsschätzungen für lineare Modelle ein erfolgreiches Gleichgewicht zwischen Exploration und Ausnutzung herstellen können. Wir leiten eine differenzielle Approximation in geschlossener Form für dieses Ziel ab und zeigen empirisch anhand verschiedener Modelle und Datensätze, dass das Training des restlichen Netzwerks auf diese Weise zu einer besseren Online- und Offline-Leistung im Vergleich zu anderen Methoden führt.
	Obwohl es immer mehr Literatur zur Erklärung neuronaler Netze gibt, wurde bisher kein Konsens darüber erzielt, wie eine Entscheidung eines neuronalen Netzes zu erklären ist oder wie eine Erklärung zu bewerten ist.	In dieser Arbeit leisten wir einen doppelten Beitrag: Erstens untersuchen wir Schemata zur Kombination von Erklärungsmethoden und zur Verringerung der Modellunsicherheit, um eine einzige aggregierte Erklärung zu erhalten, die robuster ist und besser mit dem neuronalen Netz übereinstimmt als jede einzelne Erklärungsmethode.	Zweitens schlagen wir einen neuen Ansatz zur Evaluierung von Erklärungsmethoden vor, der die Notwendigkeit einer manuellen Evaluierung umgeht und nicht auf die Übereinstimmung von neuronalen Netzen und menschlichen Entscheidungsprozessen angewiesen ist.
Wir stellen SOSELETO (SOurce SELEction for Target Optimization) vor, eine neue Methode zur Nutzung eines Quelldatensatzes zur Lösung eines Klassifizierungsproblems in einem Zieldatensatz.  SOSELETO basiert auf der folgenden einfachen Intuition: Einige Quellbeispiele sind für das Zielproblem informativer als andere.  Um dieser Intuition gerecht zu werden, werden den Quellbeispielen Gewichte zugewiesen; diese Gewichte werden gemeinsam mit dem Quell- und dem Zielklassifizierungsproblem über ein zweistufiges Optimierungsschema gelöst.  Das Ziel kann daher die Quellstichproben auswählen, die für seine eigene Klassifizierungsaufgabe am informativsten sind.  Darüber hinaus wirkt die zweistufige Optimierung als eine Art Regularisierung für das Ziel, die eine Überanpassung verhindert.  SOSELETO kann sowohl auf klassisches Transfer-Lernen als auch auf das Problem des Trainings auf Datensätzen mit verrauschten Etiketten angewandt werden; wir zeigen Ergebnisse auf dem neuesten Stand der Technik für beide Probleme.
Wir stellen einen neuartigen Ansatz für das Training neuronaler abstrakter Architekturen vor, der eine (partielle) Überwachung der interpretierbaren Komponenten der Maschine einschließt, Basierend auf unserer Methode haben wir eine detaillierte experimentelle Evaluation sowohl mit den NTM- als auch mit den NRAM-Architekturen durchgeführt und gezeigt, dass der Ansatz zu einer signifikant besseren Konvergenz und Generalisierungsfähigkeit der Lernphase führt, als wenn nur mit Input-Output-Beispielen trainiert wird.
Bayesianisches Lernen von Modellparametern in neuronalen Netzen ist in Szenarien wichtig, in denen Schätzungen mit gut kalibrierter Unsicherheit wichtig sind. In diesem Papier schlagen wir Bayesianische quantisierte Netze (BQNs) vor, quantisierte neuronale Netze (QNNs), für die wir eine Posterior-Verteilung über ihre diskreten Parameter lernen. Wir stellen eine Reihe effizienter Algorithmen für das Lernen und die Vorhersage in BQNs zur Verfügung, ohne dass eine Stichprobe aus ihren Parametern oder Aktivierungen gezogen werden muss, was nicht nur ein differenzierbares Lernen in quantisierten Modellen ermöglicht, sondern auch die Varianz in der Gradientenschätzung reduziert. Wir evaluieren BQNs auf MNIST-, Fashion-MNIST- und KMNIST-Klassifizierungsdatensätzen im Vergleich zum Bootstrap-Ensemble von QNNs (E-QNN) und zeigen, dass BQNs sowohl geringere Vorhersagefehler als auch besser kalibrierte Unsicherheiten als E-QNN erreichen (mit weniger als 20 % der negativen Log-Likelihood).
Die Injektion negativer Beispiele während des Trainings, bekannt als adversariales Training, kann die Robustheit gegen einstufige Angriffe verbessern, aber nicht für unbekannte iterative Angriffe. um diese Herausforderung anzugehen, zeigen wir zunächst, dass iterativ generierte adversarische Bilder leicht zwischen Netzwerken übertragen werden können, die mit der gleichen Strategie trainiert wurden. Wir trainieren ein Netzwerk von Grund auf, indem wir iterativ generierte Angriffsbilder, die aus bereits verteidigten Netzwerken stammen, zusätzlich zu den einstufigen Angriffsbildern aus dem zu trainierenden Netzwerk einspeisen. Wir schlagen außerdem vor, den Einbettungsraum sowohl für die Klassifizierung als auch für das Ähnlichkeitslernen auf niedriger Ebene (Pixelebene) zu nutzen, um unbekannte Störungen auf Pixelebene zu ignorieren. Während des Trainings injizieren wir gegnerische Bilder, ohne die entsprechenden sauberen Bilder zu ersetzen, und bestrafen den Abstand zwischen den beiden Einbettungen (sauber und gegnerisch). Die experimentellen Ergebnisse zeigen, dass Kaskaden-Training zusammen mit dem von uns vorgeschlagenen Low-Level-Ähnlichkeitslernen die Robustheit gegen iterative Angriffe effizient verbessert, allerdings auf Kosten einer geringeren Robustheit gegen einstufige Angriffe.
Während man glaubt, dass Techniken wie Adam, Batch-Normalisierung und neuerdings SeLU-Nichtlinearitäten das Problem der explodierenden Gradienten "lösen", zeigen wir, dass dies nicht der Fall ist und dass in einer Reihe von populären MLP-Architekturen explodierende Gradienten existieren und dass sie die Tiefe begrenzen, bis zu der Netze effektiv trainiert werden können, sowohl in der Theorie als auch in der Praxis.Wir erklären, warum explodierende Gradienten auftreten und heben das {\it collapsing domain problem} hervor, das in Architekturen auftreten kann, die explodierende Gradienten vermeiden. ResNets haben deutlich niedrigere Gradienten und können daher das Problem der explodierenden Gradienten umgehen, was das effektive Training von viel tieferen Netzwerken ermöglicht, was, wie wir zeigen, eine Folge einer überraschenden mathematischen Eigenschaft ist.Durch die Feststellung, dass {\it jedes neuronale Netzwerk ein Residualnetzwerk ist}, entwickeln wir den {\it Residualtrick}, der offenbart, dass die Einführung von Skip-Verbindungen das Netzwerk mathematisch vereinfacht, und dass diese Einfachheit der Hauptgrund für ihren Erfolg sein kann.
In diesem Papier interessieren wir uns für zwei scheinbar unterschiedliche Konzepte: \Textit{adversarial training} und \textit{generative adversarial networks (GANs)}.Insbesondere, wie diese Techniken arbeiten, um sich gegenseitig zu verbessern.Zu diesem Zweck analysieren wir die Begrenzung des adversarial training als Verteidigungsmethode, beginnend mit der Frage, wie gut die Robustheit eines Modells verallgemeinern kann. Dann verbessern wir erfolgreich die Generalisierbarkeit durch Datenerweiterung durch die ``fake'' Bilder, die von generativen adversarischen Netzwerken gesampelt werden.Danach sind wir überrascht zu sehen, dass der resultierende robuste Klassifikator zu einem besseren Generator führt, kostenlos.Wir erklären intuitiv dieses interessante Phänomen und lassen die theoretische Analyse für zukünftige Arbeit. Durch diese Beobachtungen motiviert, schlagen wir ein System vor, das Generator, Diskriminator und gegnerischen Angreifer in einem einzigen Netzwerk vereint: Nach einem End-to-End-Training und einer Feinabstimmung kann unsere Methode gleichzeitig die Robustheit der Klassifizierer, gemessen an der Genauigkeit bei starken gegnerischen Angriffen, und die Qualität der Generatoren, die sowohl ästhetisch als auch quantitativ bewertet wird, verbessern. In Bezug auf den Klassifikator erreichen wir eine bessere Robustheit als der hochmoderne adversarische Trainingsalgorithmus, der in (Madry \textit{et al.}, 2017) vorgeschlagen wurde, während unser Generator eine konkurrenzfähige Leistung im Vergleich zu SN-GAN (Miyato und Koyama, 2018) erreicht.
Um ähnliche binäre Netzwerke zu erhalten, stützen sich bestehende Methoden auf die Vorzeichen-Aktivierungsfunktion, die jedoch keine Gradienten für Werte ungleich Null hat, was die Standard-Backpropagation unmöglich macht. Um die Schwierigkeit zu umgehen, ein Netzwerk zu trainieren, das sich auf die Vorzeichen-Aktivierungsfunktion stützt, wechseln diese Methoden während des Trainings zwischen Gleitkomma- und Binärdarstellungen des Netzwerks, was suboptimal und ineffizient ist. Wir nähern uns der Aufgabe der Binarisierung, indem wir mit einer eindeutigen Darstellung trainieren, die eine glatte Aktivierungsfunktion beinhaltet, die während des Trainings iterativ geschärft wird, bis sie zu einer binären Darstellung wird, die der Vorzeichenaktivierungsfunktion entspricht. Unsere binären Netze haben nicht nur den Vorteil eines geringeren Speicher- und Rechenaufwands im Vergleich zu konventionellen Fließkomma- und binären Netzen, sondern zeigen auch eine höhere Klassifizierungsgenauigkeit als existierende State-of-the-Art-Methoden in mehreren Benchmark-Datensätzen.
  In vielen Anwendungen sind die Trainingsdaten für eine maschinelle Lernaufgabe über mehrere Knoten verteilt, und die Aggregation dieser Daten kann aufgrund von Speicher-, Kommunikations- oder Datenschutzbeschränkungen undurchführbar sein.In dieser Arbeit stellen wir Good-Enough Model Spaces (GEMS) vor, einen neuartigen Rahmen für das Lernen eines globalen, zufriedenstellenden (d.h. "guten") Modells innerhalb eines Netzwerks. "In Experimenten mit Benchmark- und medizinischen Datensätzen übertrifft unser Ansatz andere grundlegende Aggregationstechniken wie Ensembling oder Modell-Mittelwertbildung und schneidet vergleichbar mit idealen, nicht verteilten Modellen ab.
Wir schlagen den Wasserstein-Auto-Encoder (WAE) vor - einen neuen Algorithmus zur Erstellung eines generativen Modells der Datenverteilung. WAE minimiert eine bestrafte Form der Wasserstein-Distanz zwischen der Modellverteilung und der Zielverteilung, was zu einem anderen Regularisierer führt als der, der vom Variational Auto-Encoder (VAE) verwendet wird. Wir vergleichen unseren Algorithmus mit verschiedenen anderen Techniken und zeigen, dass er eine Verallgemeinerung von adversen Auto-Encodern (AAE) ist. Unsere Experimente zeigen, dass WAE viele der Eigenschaften von VAEs teilt (stabiles Training, Encoder-Decoder-Architektur, schöne latente Mannigfaltigkeitsstruktur), während er Proben von besserer Qualität erzeugt.
In diesem Papier zielen wir darauf ab, einen neuartigen Mechanismus zu entwickeln, um die differentielle Privatsphäre (DP) beim gegnerischen Lernen für tiefe neuronale Netze zu bewahren, mit nachweisbarer Robustheit gegenüber gegnerischen Beispielen.Wir nutzen die sequentielle Kompositionstheorie in DP, um eine neue Verbindung zwischen DP-Erhaltung und nachweisbarer Robustheit herzustellen. Um den Kompromiss zwischen Modellnutzen, Verlust der Privatsphäre und Robustheit anzugehen, entwerfen wir eine originelle, differentiell private, gegnerische Zielfunktion, die auf der Post-Processing-Eigenschaft in DP basiert, um die Empfindlichkeit unseres Modells zu erhöhen.
In hochdimensionalen Verstärkungslernumgebungen mit spärlichen Belohnungen ist die Durchführung einer effektiven Exploration, um überhaupt ein Belohnungssignal zu erhalten, eine offene Herausforderung.Während modellbasierte Ansätze eine bessere Exploration durch Planung versprechen, ist es extrem schwierig, einen ausreichend zuverlässigen Markov-Entscheidungsprozess (MDP) in hohen Dimensionen zu lernen (z.B. über 10^100 Zustände), In diesem Papier schlagen wir vor, ein abstraktes MDP über eine viel geringere Anzahl von Zuständen (z.B. 10^5) zu lernen, über die wir für eine effektive Exploration planen können.Wir nehmen an, dass wir eine Abstraktionsfunktion haben, die konkrete Zustände (z.B. rohe Pixel) auf abstrakte Zustände abbildet (z.B, In unserem Ansatz unterhält ein Manager eine abstrakte MDP über eine Teilmenge der abstrakten Zustände, die durch gezielte Exploration monoton wächst (was aufgrund der abstrakten MDP möglich ist).Gleichzeitig lernen wir eine Worker-Policy, um zwischen den abstrakten Zuständen zu reisen; der Worker befasst sich mit der Unordnung der konkreten Zustände und präsentiert dem Manager eine saubere Abstraktion.Bei drei der schwierigsten Spiele aus der Arcade-Lernumgebung (Montezuma's, Pitfall! (Montezuma's, Pitfall! und Private Eye) übertrifft unser Ansatz den bisherigen Stand der Technik in jedem Spiel um mehr als den Faktor 2. In Pitfall! ist unser Ansatz der erste, der übermenschliche Leistungen ohne Demonstrationen erreicht.
Die meisten Deep Reinforcement Learning (RL)-Systeme sind nicht in der Lage, effektiv aus Off-Policy-Daten zu lernen, vor allem, wenn sie nicht online in der Umgebung erkunden können. Daher entwickeln wir eine neue Klasse von Off-Policy Batch-RL-Algorithmen, die KL-Kontrolle verwenden, um Divergenz von einem vorher trainierten Modell wahrscheinlicher Aktionen zu bestrafen. Dieser Way Off-Policy (WOP) Algorithmus wurde sowohl an traditionellen RL-Aufgaben von OpenAI Gym als auch an dem Problem der Dialoggenerierung in offenen Bereichen getestet, einem anspruchsvollen Reinforcement Learning Problem mit einem 20.000-dimensionalen Aktionsraum. Wir testen die Generalisierung in der realen Welt, indem wir Dialogmodelle live einsetzen, um mit Menschen in einer offenen Domäne zu kommunizieren, und zeigen, dass WOP signifikante Verbesserungen gegenüber dem Stand der Technik früherer Methoden im Batch Deep RL erreicht.
Die Übertragung und Anpassung an neue unbekannte Umgebungsdynamiken ist eine zentrale Herausforderung für das Reinforcement Learning (RL). Eine noch größere Herausforderung besteht darin, in einem einzigen Versuch zur Testzeit nahezu optimale Leistungen zu erbringen, möglicherweise ohne Zugang zu dichten Belohnungen, was von aktuellen Methoden, die mehrere Erfahrungsrollouts für die Anpassung erfordern, nicht adressiert wird.Um eine Übertragung in einer einzigen Episode in einer Familie von Umgebungen mit verwandten Dynamiken zu erreichen, schlagen wir einen allgemeinen Algorithmus vor, der eine Sonde und ein Inferenzmodell optimiert, um zugrundeliegende latente Variablen der Testdynamik schnell zu schätzen, die dann sofort als Input für eine universelle Kontrollpolitik verwendet werden. Dieser modulare Ansatz ermöglicht die Integration modernster Algorithmen für Variationsinferenz oder RL. Darüber hinaus erfordert unser Ansatz keinen Zugang zu Belohnungen zum Testzeitpunkt, so dass er auch in Umgebungen eingesetzt werden kann, in denen bestehende adaptive Ansätze nicht funktionieren. In verschiedenen experimentellen Domänen mit einer einzigen Testbedingung übertrifft unsere Methode bestehende adaptive Ansätze deutlich und zeigt eine günstige Leistung im Vergleich zu den Basislinien für robusten Transfer.
Domänenspezifische zielorientierte Dialogsysteme erfordern typischerweise die Modellierung von drei Arten von Eingaben, nämlich, (i) die mit der Domäne assoziierte Wissensbasis,(ii) der Gesprächsverlauf, der eine Abfolge von Äußerungen darstellt, und(iii) die aktuelle Äußerung, für die eine Antwort generiert werden muss. Bei der Modellierung dieser Eingaben ignorieren aktuelle State-of-the-Art-Modelle wie Mem2Seq typischerweise die reichhaltige Struktur, die dem Wissensgraphen und den Sätzen im Gesprächskontext innewohnt. Inspiriert durch den jüngsten Erfolg strukturbewusster Graph Convolutional Networks (GCNs) für verschiedene NLP-Aufgaben wie maschinelle Übersetzung, semantische Rollenbeschriftung und Dokumentendatierung, schlagen wir ein speichererweitertes GCN für zielorientierte Dialoge vor. Unser Modell nutzt(i) den Entitätsbeziehungsgraphen in einer Wissensdatenbank und(ii) den Abhängigkeitsgraphen, der mit einer Äußerung verbunden ist, um reichhaltigere Repräsentationen für Wörter und Entitäten zu berechnen, und berücksichtigt die Tatsache, dass in bestimmten Situationen, z.B. wenn die Konversation in einer Code-Mischsprache stattfindet, möglicherweise keine Abhängigkeits-Parser verfügbar sind. Wir experimentieren mit dem modifizierten DSTC2-Datensatz und seinen kürzlich veröffentlichten Code-Mixed-Versionen in vier Sprachen und zeigen, dass unsere Methode die bestehenden State-of-the-Art-Methoden übertrifft, indem wir eine breite Palette von Bewertungsmetriken verwenden.
Wir erreichen dies, indem wir (i) zunächst Vektoreinbettungen einzelner Graphknoten lernen und (ii) diese dann zusammensetzen, um Knotensequenzen kompakt darzustellen. Konkret schlagen wir SENSE-S (Semantically Enhanced Node Sequence Embedding - for Single nodes) vor, einen neuartigen, auf Skip-Gram basierenden Einbettungsmechanismus für einzelne Graphknoten, der sowohl die Graphstruktur als auch deren textuelle Beschreibungen erfasst. Wir zeigen, dass SENSE-S-Vektoren die Genauigkeit von Multi-Label-Klassifizierungsaufgaben um bis zu 50% und Link-Vorhersage-Aufgaben um bis zu 78% unter einer Vielzahl von Szenarien mit realen Datensätzen erhöhen.Basierend auf SENSE-S, schlagen wir als nächstes generisches SENSE vor, um zusammengesetzte Vektoren zu berechnen, die eine Sequenz von Knoten repräsentieren, bei denen die Erhaltung der Knotenreihenfolge wichtig ist.Wir beweisen, dass dieser Ansatz bei der Einbettung von Knotensequenzen effizient ist, und unsere Experimente mit realen Daten bestätigen seine hohe Genauigkeit bei der Dekodierung der Knotenreihenfolge.
Jüngste Erkenntnisse zeigen, dass Faltungsneuronale Netze (CNNs) in Richtung Texturen voreingenommen sind, so dass CNNs nicht robust gegenüber nachteiligen Störungen über Texturen sind, während traditionelle robuste visuelle Merkmale wie SIFT (scale-invariant feature transforms) so konzipiert sind, dass sie über einen erheblichen Bereich von affinen Verzerrungen, Hinzufügen von Rauschen, etc. mit der Nachahmung der menschlichen Wahrnehmung Natur robust sind. Dieses Papier zielt darauf ab, die guten Eigenschaften von SIFT zu nutzen, um CNN-Architekturen in Richtung einer besseren Genauigkeit und Robustheit zu renovieren. Wir leihen uns die Skalenraum-Extremwert-Idee von SIFT und schlagen EVPNet (extreme value preserving network) vor, das drei neuartige Komponenten zur Modellierung der Extremwerte enthält: (1) parametrische Gauß-Differenzen (DoG) zur Extraktion von Extrema, (2) abgeschnittene ReLU zur Unterdrückung nicht stabiler Extrema und (3) projizierte Normalisierungsschicht (PNL) zur Nachahmung der PCA-SIFT-ähnlichen Merkmalsnormalisierung.Experimente zeigen, dass EVPNets eine ähnliche oder bessere Genauigkeit als herkömmliche CNNs erreichen können, während sie eine viel bessere Robustheit gegenüber einer Reihe von gegnerischen Angriffen (FGSM, PGD, etc.) auch ohne gegnerisches Training erreichen.
Komprimierte Repräsentationen generalisieren besser (Shamir et al., Die Information Bottleneck (IB)-Methode (Tishby et al. (2000)) bietet einen aufschlussreichen und prinzipientreuen Ansatz für das Abwägen von Kompression und Vorhersage beim Lernen von Repräsentationen. Das IB-Ziel I(X; Z) - βI(Y ; Z) verwendet einen Lagrange-Multiplikator β, um diesen Kompromiss abzustimmen. Es gibt jedoch nur wenige theoretische Anleitungen für die Auswahl von β. Es gibt auch einen Mangel an theoretischem Verständnis über die Beziehung zwischen β, dem Datensatz, der Modellkapazität und der Lernfähigkeit.In dieser Arbeit zeigen wir, dass, wenn β falsch gewählt wird, kein Lernen stattfinden kann: die triviale Darstellung P(Z|X) = P(Z) wird zum globalen Minimum des IB-Ziels. Wir zeigen, wie dies vermieden werden kann, indem wir einen scharfen Phasenübergang zwischen dem Nicht-Lernbaren und dem Lernbaren identifizieren, der sich ergibt, wenn β variiert.Dieser Phasenübergang definiert das Konzept der IB-Lernbarkeit.Wir beweisen mehrere hinreichende Bedingungen für IB-Lernbarkeit, die eine theoretische Anleitung für die Auswahl von β liefern. Wir zeigen weiter, dass IB-Lernbarkeit durch die größte vertrauenswürdige, typische und unausgewogene Teilmenge der Trainingsbeispiele bestimmt wird.Wir geben einen praktischen Algorithmus zur Schätzung des minimalen β für einen gegebenen Datensatz.Wir testen unsere theoretischen Ergebnisse auf synthetischen Datensätzen, MNIST und CIFAR10 mit verrauschten Etiketten, und machen die überraschende Beobachtung, dass die Genauigkeit nicht monoton in β sein kann.
 Wir betrachten eine neue Klasse von \emph{data poisoning}-Angriffen auf neuronale Netze, bei denen der Angreifer die Kontrolle über ein Modell übernimmt, indem er kleine Störungen an einer Teilmenge der Trainingsdaten vornimmt.  Wir formulieren die Aufgabe, Gifte zu finden, als ein zweistufiges Optimierungsproblem, das mit Methoden aus der Meta-Learning-Community gelöst werden kann.  Im Gegensatz zu früheren Vergiftungsstrategien kann das Meta-Poisoning Netze vergiften, die von Grund auf mit einer dem Angreifer unbekannten Initialisierung trainiert wurden, und über Hyperparameter hinweg übertragen werden. Außerdem zeigen wir, dass unsere Angriffe vielseitiger sind: Sie können eine Fehlklassifizierung des Zielbildes in eine beliebig gewählte Klasse bewirken.Unsere Ergebnisse zeigen eine Erfolgsquote von über 50%, wenn nur 3-10% des Trainingsdatensatzes vergiftet werden.
Wir geben einen neuen Algorithmus für das Lernen eines zweischichtigen neuronalen Netzes unter einer sehr allgemeinen Klasse von Eingangsverteilungen an. y = A \sigma(Wx) + \xi, wobei A, W Gewichtsmatrizen sind, \xi für Rauschen steht und die Anzahl der Neuronen in der versteckten Schicht nicht größer ist als der Eingang oder der Ausgang, garantiert unserem Algorithmus die Wiederherstellung der Parameter A, W des grundrichtigen Netzes. Die einzige Anforderung an die Eingabe x ist, dass sie symmetrisch ist, was auch sehr komplizierte und strukturierte Eingaben zulässt. Wir verwenden spektrale Algorithmen, um die komplizierte, nicht-konvexe Optimierung beim Lernen neuronaler Netze zu vermeiden. Experimente zeigen, dass unser Algorithmus mit einer kleinen Anzahl von Stichproben für viele symmetrische Eingangsverteilungen das neuronale Netz der Grundwahrheit robust lernen kann.
Wenn der Lehrer und der Schüler jedoch neuronale Netze sind, sind die Beispiele, die das Lehrernetz lernt, um den Schülern etwas beizubringen, typischerweise nicht interpretierbar.Wir zeigen, dass das Training des Schülers und des Lehrers iterativ statt gemeinsam interpretierbare Lehrstrategien hervorbringen kann. Wir bewerten die Interpretierbarkeit, indem wir (1) die Ähnlichkeit der entstehenden Strategien des Lehrers mit intuitiven Strategien in jeder Domäne messen und (2) Experimente mit Menschen durchführen, um zu bewerten, wie effektiv die Strategien des Lehrers beim Unterrichten von Menschen sind.Wir zeigen, dass das Lehrernetzwerk lernt, interpretierbare, pädagogische Beispiele auszuwählen oder zu generieren, um regelbasierte, probabilistische, boolesche und hierarchische Konzepte zu unterrichten.
Der stochastische Gradientenabstieg (SGD), der verrauschte Gradientenaktualisierungen gegen Recheneffizienz eintauscht, ist der De-facto-Optimierungsalgorithmus zur Lösung von Problemen des maschinellen Lernens in großem Maßstab.SGD kann schnelle Lernfortschritte erzielen, indem Aktualisierungen unter Verwendung von unterabgetasteten Trainingsdaten durchgeführt werden, aber die verrauschten Aktualisierungen führen auch zu langsamer asymptotischer Konvergenz.   Mehrere Varianzreduktionsalgorithmen wie SVRG führen Kontrollvariablen ein, um eine niedrigere Varianzgradientenschätzung und eine schnellere Konvergenz zu erreichen.  Die traditionelle asymptotische Analyse in der stochastischen Optimierung bietet nur einen begrenzten Einblick in das Training von Deep-Learning-Modellen unter einer festen Anzahl von Epochen.In diesem Papier präsentieren wir eine nicht-asymptotische Analyse von SVRG unter einem verrauschten Kleinste-Quadrate-Regressionsproblem.Unser Hauptaugenmerk liegt auf dem Vergleich des exakten Verlusts von SVRG mit dem von SGD bei jeder Iteration t. Wir zeigen, dass die Lerndynamik unseres Regressionsmodells eng mit der von neuronalen Netzen auf MNIST und CIFAR-10 übereinstimmt, sowohl für die unterparametrisierten als auch für die überparametrisierten Modelle.Unsere Analyse und experimentellen Ergebnisse deuten darauf hin, dass es einen Kompromiss zwischen den Rechenkosten und der Konvergenzgeschwindigkeit in unterparametrisierten neuronalen Netzen gibt.SVRG übertrifft SGD nach einigen Epochen in diesem Regime.SGD übertrifft jedoch immer SVRG im überparametrisierten Regime.
Nicht-autoregressive maschinelle Übersetzungssysteme (NAT) prognostizieren eine Sequenz von Output-Tokens parallel und erreichen dadurch erhebliche Verbesserungen in der Generierungsgeschwindigkeit im Vergleich zu autoregressiven Modellen.Bestehende NAT-Modelle verlassen sich in der Regel auf die Technik der Wissensdestillation, die die Trainingsdaten aus einem vortrainierten autoregressiven Modell für eine bessere Leistung erstellt.Wissensdestillation ist empirisch nützlich und führt zu großen Genauigkeitsgewinnen für NAT-Modelle, aber der Grund für diesen Erfolg ist bisher unklar.In diesem Beitrag entwerfen wir zunächst systematische Experimente, um zu untersuchen, warum Wissensdestillation für das NAT-Training entscheidend ist. Wir stellen fest, dass Wissensdestillation die Komplexität von Datensätzen reduzieren und NAT dabei helfen kann, die Variationen in den Ausgabedaten zu modellieren. Darüber hinaus wird eine starke Korrelation zwischen der Kapazität eines NAT-Modells und der optimalen Komplexität der destillierten Daten für die beste Übersetzungsqualität beobachtet. Auf der Grundlage dieser Erkenntnisse schlagen wir außerdem mehrere Ansätze vor, die die Komplexität von Datensätzen verändern können, um die Leistung von NAT-Modellen zu verbessern.
Aufgrund ihrer Fähigkeit, Informationen über lange Zeithorizonte hinweg effektiv zu integrieren und auf große Datenmengen zu skalieren, haben Self-Attention-Architekturen in letzter Zeit einen bahnbrechenden Erfolg in der Verarbeitung natürlicher Sprache (NLP) gezeigt und in Bereichen wie Sprachmodellierung und maschineller Übersetzung Spitzenergebnisse erzielt. Die Nutzung der Fähigkeit des Transformators, lange Zeithorizonte von Informationen zu verarbeiten, könnte einen ähnlichen Leistungsschub in teilweise beobachtbaren Bereichen des Reinforcement Learning (RL) bieten, aber die groß angelegten Transformatoren, die in NLP verwendet werden, müssen noch erfolgreich auf die RL-Einstellung angewendet werden. In dieser Arbeit zeigen wir, dass die Standard-Transformator-Architektur schwer zu optimieren ist, was zuvor im Rahmen des überwachten Lernens beobachtet wurde, aber bei RL-Zielen besonders ausgeprägt ist. Wir schlagen architektonische Modifikationen vor, die die Stabilität und Lerngeschwindigkeit des ursprünglichen Transformers und der XL-Variante erheblich verbessern. Die vorgeschlagene Architektur, der Gated Transformer-XL (GTrXL), übertrifft LSTMs in anspruchsvollen Speicherumgebungen und erzielt State-of-the-Art-Ergebnisse in der Multi-Task DMLab-30 Benchmark-Suite und übertrifft die Leistung einer externen Speicherarchitektur. Wir zeigen, dass GTrXL, das mit den gleichen Verlusten trainiert wurde, eine Stabilität und Leistung aufweist, die durchweg mit einer konkurrierenden LSTM-Basislinie übereinstimmt oder diese übertrifft, auch bei reaktiveren Aufgaben, bei denen der Speicher weniger kritisch ist. GTrXL bietet eine leicht zu trainierende, einfach zu implementierende, aber wesentlich ausdrucksstärkere architektonische Alternative zu den standardmäßigen mehrschichtigen LSTM, die für RL-Agenten in teilweise beobachtbaren Umgebungen allgegenwärtig sind.  
Um diese kompakten Modelle jedoch für den Einsatz in der realen Welt geeignet zu machen, müssen wir nicht nur die Leistungslücke verkleinern, sondern sie auch robuster gegenüber häufig auftretenden und nachteiligen Störungen machen.Rauschen durchdringt jede Ebene des Nervensystems, von der Wahrnehmung sensorischer Signale bis zur Erzeugung motorischer Reaktionen. Wir glauben daher, dass Rauschen ein entscheidendes Element bei der Verbesserung des Trainings neuronaler Netze sein könnte und die scheinbar widersprüchlichen Ziele der Verbesserung sowohl der Generalisierung als auch der Robustheit des Modells angeht.Inspiriert von der Variabilität von Versuch zu Versuch im Gehirn, die aus mehreren Rauschquellen resultieren kann, führen wir Variabilität durch Rauschen entweder auf der Eingangsebene oder den Überwachungssignalen ein. Unsere Ergebnisse zeigen, dass Rauschen sowohl die Generalisierung als auch die Robustheit des Modells verbessern kann. "Fickle Teacher", das Dropouts im Lehrermodell als Quelle für Antwortvariationen nutzt, führt zu einer signifikanten Verbesserung der Generalisierung. "Soft Randomization", die die Ausgangsverteilung des Schülermodells auf dem Bild mit Gaußschem Rauschen an die des Lehrers auf dem Originalbild anpasst, verbessert die Robustheit des gegnerischen Modells um ein Vielfaches im Vergleich zu dem mit Gaußschem Rauschen trainierten Schülermodell. Die Studie hebt die Vorteile des Hinzufügens von konstruktivem Rauschen im Rahmen der Wissensdestillation hervor und hofft, weitere Arbeiten auf diesem Gebiet zu inspirieren.
Unser Clustering-Ziel basiert auf der Optimierung normalisierter Schnitte, einem Kriterium, das sowohl die Ähnlichkeit innerhalb von Clustern als auch die Unähnlichkeit zwischen Clustern misst, und wir definieren eine differenzierbare Verlustfunktion, die den erwarteten normalisierten Schnitten entspricht. Im Gegensatz zu vielen Arbeiten im Bereich des unbeaufsichtigten Deep Learning gibt unser trainiertes Modell direkt die endgültigen Clusterzuordnungen aus und nicht die Einbettungen, die weiterverarbeitet werden müssen, um brauchbar zu sein Unser Ansatz lässt sich auf ungesehene Datensätze in einer Vielzahl von Domänen verallgemeinern, einschließlich Text und Bild, (z.B. MNIST, Reuters, CIFAR-10 und CIFAR-100) und übertreffen die stärksten Baselines um bis zu 10,9 %. Unsere Generalisierungsergebnisse sind um bis zu 21,9 % besser als die des aktuell leistungsstärksten Clustering-Ansatzes mit der Fähigkeit zur Generalisierung.
Wir stellen den größten (öffentlich zugänglichen) Datensatz für die Erkennung von kyrillischem handgeschriebenem Text und den ersten Datensatz für die Erkennung von kyrillischem Text in freier Wildbahn vor und schlagen eine Methode zur Erkennung von kyrillischem handgeschriebenem Text und Text in freier Wildbahn vor. auf der Grundlage dieses Ansatzes entwickeln wir ein System, mit dem die Bearbeitungszeit für einen der größten mathematischen Wettbewerbe in der Ukraine um 12 Tage und die Menge des verwendeten Papiers um 0,5 Tonnen reduziert werden kann.
Wir führen eine Phrasendarstellung ein (die auch auf Sätze anwendbar ist), bei der jede Phrase einen eigenen Satz von Multimode-Codebuch-Einbettungen hat, um verschiedene semantische Facetten der Bedeutung der Phrase zu erfassen. Die Codebucheinbettungen können als Clusterzentren betrachtet werden, die die Verteilung von möglicherweise zusammen auftretenden Wörtern in einem vortrainierten Worteinbettungsraum zusammenfassen.wir schlagen ein durchgängig trainierbares neuronales Modell vor, das die Menge der Clusterzentren direkt aus der Eingabetextsequenz (z.B. einer Phrase oder einem Satz) während der Analyse vorhersagt, Wir stellen fest, dass die Codebook-Einbettungen pro Phrase/Satz nicht nur eine besser interpretierbare semantische Repräsentation liefern, sondern auch starke Baselines (mit großem Abstand in einigen Aufgaben) in Benchmark-Datensätzen für unbeaufsichtigte Phrasenähnlichkeit, Satzähnlichkeit, Hypernym-Erkennung und extraktive Zusammenfassung übertreffen.
Während des Trainings lernen wir ein Multi-Sprecher-Modell mit einem gemeinsamen konditionalen WaveNet-Kern und unabhängigen Einbettungen für jeden Sprecher. Das Ziel des Trainings ist es nicht, ein neuronales Netzwerk mit festen Gewichten zu erzeugen, das dann als TTS-System eingesetzt wird, sondern ein Netzwerk zu erzeugen, das nur wenige Daten zum Einsatzzeitpunkt benötigt, um sich schnell an neue Sprecher anzupassen. Wir stellen drei Strategien vor und vergleichen sie miteinander:(i) Lernen der Sprechereinbettung unter Beibehaltung des WaveNet-Kerns,(ii) Feinabstimmung der gesamten Architektur mit stochastischem Gradientenabstieg und(iii) Vorhersage der Sprechereinbettung mit einem trainierten neuronalen Netzwerk-Encoder.Die Experimente zeigen, dass diese Ansätze bei der Anpassung des neuronalen Netzwerks mit mehreren Sprechern an neue Sprecher erfolgreich sind und mit nur wenigen Minuten Audiodaten von neuen Sprechern sowohl bei der Natürlichkeit der Proben als auch bei der Ähnlichkeit der Stimme Spitzenergebnisse erzielen.
Der neue Ansatz nutzt nicht nur die merkmalsbasierte Repräsentation, die von einem neuronalen Netz aus der ursprünglichen Aufgabe gelernt wurde (Repräsentationstransfer), sondern auch Informationen über die Klassen (Konzepttransfer). Wir zeigen, dass selbst ein einfaches probabilistisches Modell auf einem Standard-K-Shot-Lerndatensatz mit großem Abstand den Stand der Technik erreicht und darüber hinaus in der Lage ist, Unsicherheiten genau zu modellieren, was zu gut kalibrierten Klassifikatoren führt, und im Gegensatz zu vielen neueren Ansätzen zum K-Shot-Lernen leicht erweiterbar und flexibel ist.
Aufbauend auf den jüngsten Erfolgen des verteilten Trainings von RL-Agenten untersuchen wir in diesem Papier das Training von RNN-basierten RL-Agenten aus verteilter priorisierter Erfahrungswiedergabe, untersuchen die Auswirkungen von Parameterverzögerungen, die zu Repräsentationsdrift und rekurrentem Zustand führen, und leiten empirisch eine verbesserte Trainingsstrategie ab. Unter Verwendung einer einzigen Netzwerkarchitektur und eines festen Satzes von Hyperparametern vervierfacht der resultierende Agent, Recurrent Replay Distributed DQN, den bisherigen Stand der Technik auf Atari-57 und entspricht dem Stand der Technik auf DMLab-30. Es ist der erste Agent, der die Leistung auf menschlichem Niveau in 52 der 57 Atari-Spiele übertrifft.
Das derzeitige State-of-the-Art-Modell für die durchgängige semantische Rollenerkennung (SRL) ist eine tiefe neuronale Netzwerkarchitektur ohne explizite linguistische Merkmale. In dieser Arbeit stellen wir linguistisch informierte Selbstaufmerksamkeit (LISA) vor: ein neues neuronales Netzwerkmodell, das Multi-Head-Selbstaufmerksamkeit mit Multi-Task-Lernen in den Bereichen Dependency Parsing, Part-of-Speech, Prädikaterkennung und SRL kombiniert. So wird zum Beispiel die Syntax einbezogen, indem einer der Aufmerksamkeitsköpfe darauf trainiert wird, syntaktische Eltern für jedes Token zu beachten. In Experimenten mit dem CoNLL-2005 SRL-Datensatz erreicht LISA eine Steigerung von 2,5 F1 absolut gegenüber dem bisherigen Stand der Technik bei Newswire mit vorhergesagten Prädikaten und mehr als 2,0 F1 bei Out-of-Domain-Daten. Bei ConLL-2012 English SRL zeigen wir ebenfalls eine Verbesserung von mehr als 3,0 F1, was einer Fehlerreduzierung von 13% entspricht.
Bottleneck-Strukturen mit Identitäts- (z.B. Residual-) Verbindung sind heute ein beliebtes Paradigma für den Entwurf tiefer Faltungsneuronaler Netze (CNN), um große Merkmale effizient zu verarbeiten.In dieser Arbeit konzentrieren wir uns auf die informationserhaltende Natur der Identitätsverbindung und nutzen diese, um einer Faltungsschicht eine neue Funktionalität der Kanalselektivität zu ermöglichen, d.h, Insbesondere schlagen wir die Selective Convolutional Unit (SCU) vor, eine weithin anwendbare architektonische Einheit, die die Parametereffizienz verschiedener moderner CNNs mit Engpässen verbessert. Während des Trainings lernt die SCU schrittweise die Kanalselektivität on-the-fly durch die alternative Verwendung von(a) Beschneidung unwichtiger Kanäle und(b) Neuverdrahtung der beschnittenen Parameter auf wichtige Kanäle. Unsere experimentellen Ergebnisse zeigen, dass die SCU-basierten Modelle ohne Nachbearbeitung im Allgemeinen sowohl eine Modellkomprimierung als auch eine Verbesserung der Genauigkeit im Vergleich zu den Grundmodellen erreichen, und zwar durchgängig für alle getesteten Architekturen.
Ein Beispiel ist die Schätzung von Behandlungseffekten aus Beobachtungsdaten, bei denen eine Teilaufgabe darin besteht, die Wirkung einer Behandlung auf Probanden vorherzusagen, die sich systematisch von denen unterscheiden, die die Behandlung in den Daten erhalten haben.Eine verwandte Art von Verteilungsverschiebung tritt bei der unüberwachten Domänenanpassung auf, bei der wir die Aufgabe haben, auf eine Verteilung von Eingaben zu verallgemeinern, die sich von derjenigen unterscheidet, in der wir Beschriftungen beobachten.Wir stellen beide Probleme als Vorhersage unter einer Verschiebung im Design dar. Gängige Methoden zur Überwindung von Verteilungsänderungen sind oft heuristisch oder beruhen auf Annahmen, die in der Praxis nur selten zutreffen, wie z. B. ein gut spezifiziertes Modell oder die Kenntnis der Strategie, die zu den beobachteten Daten geführt hat, andere Methoden werden dadurch behindert, dass sie eine vorgegebene Metrik zum Vergleich von Beobachtungen benötigen oder schlechte asymptotische Eigenschaften aufweisen. In dieser Arbeit entwickeln wir eine Schranke für den Generalisierungsfehler unter Design Shift, die auf integralen Wahrscheinlichkeitsmetriken und der Neugewichtung von Stichproben basiert, und kombinieren diese Idee mit Repräsentationslernen, indem wir bestehende Ergebnisse in diesem Bereich verallgemeinern und verschärfen, und schließlich schlagen wir einen algorithmischen Rahmen vor, der von unserer Schranke inspiriert ist, und überprüfen seine Wirksamkeit bei der Schätzung kausaler Effekte.
Deep-Ensembles haben sich empirisch als vielversprechender Ansatz zur Verbesserung der Genauigkeit, Unsicherheit und Robustheit von Deep-Learning-Modellen erwiesen. Während Deep-Ensembles theoretisch durch den Bootstrap motiviert wurden, zeigen auch Nicht-Bootstrap-Ensembles, die nur mit zufälliger Initialisierung trainiert wurden, in der Praxis gute Leistungen, was darauf hindeutet, dass es andere Erklärungen dafür geben könnte, warum Deep-Ensembles gut funktionieren. Bayes'sche neuronale Netze, die Verteilungen über die Parameter des Netzes erlernen, sind theoretisch gut durch Bayes'sche Prinzipien motiviert, schneiden aber in der Praxis nicht so gut ab wie tiefe Ensembles, insbesondere bei Datensatzverschiebungen Eine mögliche Erklärung für diese Kluft zwischen Theorie und Praxis ist, dass gängige skalierbare approximative Bayes'sche Methoden dazu neigen, sich auf einen einzigen Modus zu konzentrieren, während tiefe Ensembles dazu neigen, verschiedene Modi im Funktionsraum zu erkunden. Unsere Ergebnisse zeigen, dass zufällige Initialisierungen völlig unterschiedliche Modi erforschen, während Funktionen entlang einer Optimierungsbahn oder aus dem Unterraum davon innerhalb eines einzigen Modus Vorhersagen-weise clustern, während sie oft signifikant im Gewichtsraum abweichen. Wir zeigen, dass es zwar verlustarme Verbindungen zwischen den Modi gibt, diese aber im Raum der Vorhersagen nicht miteinander verbunden sind. Durch die Entwicklung des Konzepts der Diversitäts-/Genauigkeitsebene zeigen wir, dass die Dekorrelationsleistung von Zufallsinitialisierungen von gängigen Unterraum-Sampling-Methoden nicht erreicht wird.
Bestehende Deep-Learning-Ansätze für das Lernen von visuellen Merkmalen neigen dazu, mehr Informationen zu extrahieren, als für die jeweilige Aufgabe erforderlich ist. Aus Sicht der Wahrung der Privatsphäre sind die visuellen Eingabedaten nicht vor dem Modell geschützt, wodurch das Modell intelligenter werden kann, als es trainiert wurde. In dieser Forschungsarbeit schlagen wir einen dreifachen neuen Beitrag vor:(i) eine neuartige Metrik zur Messung des Trust Scores eines trainierten Deep-Learning-Modells,(ii) einen modellagnostischen Lösungsrahmen zur Verbesserung des Trust Scores durch Unterdrückung aller unerwünschten Aufgaben und(iii) einen simulierten Benchmark-Datensatz, PreserveTask, mit fünf verschiedenen grundlegenden Bildklassifizierungsaufgaben, um die Generalisierungseigenschaften von Modellen zu untersuchen.In der ersten Reihe von Experimenten messen und verbessern wir die Trust Scores von fünf populären Deep-Learning-Modellen: VGG16, VGG19, Inception-v1, MobileNet und DenseNet und zeigen, dass Inception-v1 den niedrigsten Vertrauenswert hat.Zusätzlich zeigen wir Ergebnisse unseres Frameworks auf dem Farb-MNIST-Datensatz und praktische Anwendungen der Erhaltung von Gesichtsattributen in Diversity in Faces (DiF) und IMDB-Wiki-Datensatz.
  In dieser Arbeit schlagen wir vor, eine Policy zu trainieren und dabei explizit die Diskrepanz zwischen diesen beiden Verteilungen über einen festen Zeithorizont zu bestrafen. Wir tun dies, indem wir ein gelerntes Modell der Umgebungsdynamik verwenden, das für mehrere Zeitschritte abgerollt wird, und ein Policy-Netzwerk trainieren, um die differenzierbaren Kosten über diese abgerollte Trajektorie zu minimieren. Wir schlagen vor, diese zweiten Kosten zu messen, indem wir die Unsicherheit des dynamischen Modells in Bezug auf seine eigenen Vorhersagen verwenden, indem wir aktuelle Ideen aus der Unsicherheitsabschätzung für tiefe Netzwerke nutzen. Wir bewerten unseren Ansatz anhand eines großen Beobachtungsdatensatzes von Fahrverhalten, der von Verkehrskameras aufgezeichnet wurde, und zeigen, dass wir in der Lage sind, effektive Fahrstrategien aus reinen Beobachtungsdaten ohne Interaktion mit der Umgebung zu lernen.
Dynamische Systemmodelle (einschließlich RNNs) sind oft nicht in der Lage, die Sequenzgenerierung oder -vorhersage an einen bestimmten Kontext anzupassen, was ihre Anwendung in der realen Welt einschränkt. In diesem Papier zeigen wir, dass hierarchische dynamische Multitasking-Systeme (MTDSs) eine direkte Benutzerkontrolle über die Sequenzgenerierung bieten, indem sie einen latenten Code z verwenden, der die Anpassung an die individuelle Datensequenz spezifiziert. Wir zeigen, dass MTDS die Vorhersagen über latente Code-Interpolation verbessern und die langfristige Leistungsverschlechterung von Standard-RNN-Ansätzen vermeiden kann.
Die meisten der existierenden adversen Trainingsansätze basieren jedoch auf einer spezifischen Art von adversen Angriffen, die möglicherweise nicht ausreichend repräsentative Proben aus dem adversen Bereich liefern, was zu einer schwachen Generalisierungsfähigkeit bei adversen Beispielen aus anderen Angriffen führt. Diese Arbeit konzentriert sich hauptsächlich auf die adversarial Ausbildung noch effiziente FGSM adversary.In diesem Szenario ist es schwierig, ein Modell mit großer Verallgemeinerung aufgrund des Mangels an repräsentativen adversarial Proben zu trainieren, aka die Proben sind nicht in der Lage, genau zu reflektieren die adversarial Domain. Um dieses Problem zu lindern, schlagen wir eine neuartige ATDA-Methode (Adversarial Training with Domain Adaptation) vor, bei der das Training auf der FGSM-Domäne als eine Domänenanpassungsaufgabe mit einer begrenzten Anzahl von Proben der Zieldomäne betrachtet wird, wobei die Hauptidee darin besteht, eine Repräsentation zu erlernen, die sowohl auf der reinen als auch auf der gegnerischen Domäne semantisch sinnvoll und domäneninvariant ist. Empirische Auswertungen auf Fashion-MNIST, SVHN, CIFAR-10 und CIFAR-100 zeigen, dass ATDA die Generalisierung von adversarialem Training und die Glattheit der gelernten Modelle stark verbessern kann und die State-of-the-Art-Methoden auf Standard-Benchmark-Datensätzen übertrifft. Um die Übertragbarkeit unserer Methode zu zeigen, erweitern wir ATDA auch auf das adversariales Training auf iterative Angriffe wie PGD-Adversial Training (PAT) und die Verteidigungsleistung wird erheblich verbessert.
Die Modellierung von Sprache auf Zeichenebene ist eine wichtige, aber schwierige Aufgabe in der natürlichen Sprachverarbeitung. Frühere Arbeiten haben sich auf die Identifizierung langfristiger Abhängigkeiten zwischen Zeichen konzentriert und haben tiefere und breitere Netzwerke für eine bessere Leistung aufgebaut. Ihre Modelle erfordern jedoch erhebliche Rechenressourcen, was die Verwendbarkeit von Sprachmodellen auf Zeichenebene in Anwendungen mit begrenzten Ressourcen behindert.In diesem Papier schlagen wir ein leichtgewichtiges Modell vor, genannt Group-Transformer, das die Ressourcenanforderungen für einen Transformer reduziert, eine vielversprechende Methode zur Modellierung von Sequenzen mit langfristigen Abhängigkeiten. Das Ergebnis ist, dass der Group-Transformer im Vergleich zum leistungsstärksten LSTM-basierten Modell nur 18,2 % der Parameter benötigt, während er bei zwei Benchmark-Aufgaben, enwik8 und text8, eine bessere Leistung erbringt.Im Vergleich zu Transformers mit einer vergleichbaren Anzahl von Parametern und Zeitkomplexität zeigt das vorgeschlagene Modell eine bessere Leistung.Der Implementierungscode wird verfügbar sein.
  Die Domänenadaption befasst sich mit dem Problem des Transfers von Wissen aus einer markenreichen Quelldomäne in eine unmarkierte oder markenarme Zieldomäne. In letzter Zeit hat das domänenadversarische Training (DAT) vielversprechende Fähigkeiten gezeigt, einen domäneninvarianten Merkmalsraum durch Umkehrung der Gradientenfortpflanzung eines Domänenklassifikators zu erlernen. Allerdings ist DAT noch anfällig in mehreren Aspekten, einschließlich (1) Ausbildung Instabilität aufgrund der überwältigenden diskriminative Fähigkeit der Domain-Klassifikator in adversarial Ausbildung, (2) restriktive Feature-Level-Ausrichtung, und (3) Mangel an Interpretierbarkeit oder systematische Erklärung der gelernten Feature-Raum.In diesem Papier, schlagen wir eine neuartige Max-Margin Domain-Adversarial Training (MDAT) durch die Gestaltung eines Adversarial Reconstruction Network (ARN). Die vorgeschlagene MDAT stabilisiert die Gradientenumkehr in ARN durch Ersetzen der Domain-Klassifikator mit einem Rekonstruktionsnetzwerk, und auf diese Weise ARN führt sowohl Feature-Level-und Pixel-Level-Domain-Alignment ohne zusätzliche Netzwerk-Strukturen.Darüber hinaus zeigt ARN starke Robustheit auf eine breite Palette von Hyper-Parameter-Einstellungen, die große Erleichterung der Aufgabe der Modellauswahl.Umfangreiche empirische Ergebnisse bestätigen, dass unser Ansatz übertrifft andere State-of-the-Art-Domain-Alignment-Methoden.Darüber hinaus sind die rekonstruierten Ziel Proben visualisiert, um die Domain-invariante Feature-Raum, der mit unserer Intuition entspricht zu interpretieren.
Wir stellen einen Ansatz zur Erweiterung von Taxonomien mit Synonymen oder Aliasen vor, der auf große Einkaufstaxonomien mit Tausenden von Knoten abzielt. Ein umfassender Satz von Entitätsaliasen ist eine wichtige Komponente bei der Identifizierung von Entitäten in unstrukturierten Texten wie Produktbewertungen oder Suchanfragen.Unsere Methode besteht aus zwei Schritten: Wir generieren Synonymkandidaten aus WordNet und Einkaufssuchanfragen und verwenden dann einen binären Klassifikator, um die Kandidaten zu filtern. Wir verarbeiten Taxonomien mit Tausenden von Synonymen, um über 90.000 Synonyme zu generieren, und zeigen, dass die Verwendung der Taxonomie zur Ableitung von Kontextmerkmalen die Klassifizierungsleistung gegenüber der Verwendung von Merkmalen aus dem Zielknoten allein verbessert, und dass unser Ansatz das Potenzial für Transferlernen zwischen verschiedenen Taxonomiedomänen hat, was die Notwendigkeit, Trainingsdaten für neue Taxonomien zu sammeln, verringert.
Relationale Argumentation, die Fähigkeit, Interaktionen und Beziehungen zwischen Objekten zu modellieren, ist wertvoll für robuste Multi-Objekt-Tracking und entscheidend für Trajektorie prediction.In diesem Papier schlagen wir MOHART, eine Klasse-agnostische, End-to-End-Multi-Objekt-Tracking und Trajektorie prediction Algorithmus, der explizit für Permutation Invarianz in seiner relationalen reasoning.We erforschen eine Reihe von Permutation invariant Architekturen und zeigen, dass Multi-Headed-Selbst-Aufmerksamkeit übertrifft die zur Verfügung gestellten Grundlinien und besser berücksichtigt komplexe physikalische Wechselwirkungen in einem anspruchsvollen Spielzeug Experiment. Anhand von drei realen Tracking-Datensätzen zeigen wir, dass das Hinzufügen von relationalen Schlussfolgerungen die Tracking- und Trajektorienvorhersageleistung erhöht, insbesondere bei Ego-Bewegungen, Verdeckungen, überfüllten Szenen und fehlerhaften Sensoreingaben.Unseres Wissens nach ist MOHART der erste vollständige End-to-End-Ansatz für die Verfolgung von mehreren Objekten mit Hilfe von Bildverarbeitung, der auf reale Daten angewendet wird und in der Literatur beschrieben ist.
Wir untersuchen die Kombination von akteurskritischen Verstärkungslernalgorithmen mit einheitlicher, groß angelegter Erfahrungswiedergabe und schlagen Lösungen für zwei Herausforderungen vor:(a) effizientes akteurskritisches Lernen mit Erfahrungswiedergabe(b) Stabilität von sehr abweichendem Lernen.Wir nutzen diese Erkenntnisse, um Hyper-Parameter-Sweeps zu beschleunigen, bei denen alle teilnehmenden Agenten gleichzeitig laufen und ihre Erfahrungen über ein gemeinsames Wiedergabemodul austauschen.Zu diesem Zweck analysieren wir die Verzerrungs-Varianz-Kompromisse in V-Trace, einer Form von Wichtigkeitssampling für akteurskritische Methoden. Basierend auf unserer Analyse plädieren wir für die Mischung von Erfahrungen aus Replay und On-Policy-Erfahrungen und schlagen ein neues Schema für Vertrauensbereiche vor, das effektiv auf Datenverteilungen skaliert, bei denen V-Trace instabil wird. Wir bieten eine umfassende empirische Validierung der vorgeschlagenen Lösung und zeigen die Vorteile dieses Aufbaus durch die Demonstration der modernsten Dateneffizienz auf Atari bei Agenten, die bis zu 200 Millionen Frames in der Umgebung trainiert wurden.
Stochastischer Gradientenabstieg (SGD) ist das Arbeitspferd des modernen maschinellen Lernens.Manchmal gibt es viele verschiedene potentielle Gradientenschätzer, die verwendet werden können.Wenn dies der Fall ist, ist es wichtig, denjenigen mit dem besten Kompromiss zwischen Kosten und Varianz zu wählen.Dieses Papier analysiert die Konvergenzraten von SGD als Funktion der Zeit, anstatt der Iterationen.Dies führt zu einer einfachen Regel, um den Schätzer zu wählen, der zu der besten Optimierungskonvergenzgarantie führt. Inspiriert von diesem Prinzip schlagen wir eine Technik zur automatischen Auswahl eines Schätzers vor, wenn ein endlicher Pool von Schätzern gegeben ist, den wir dann auf unendliche Pools von Schätzern erweitern, bei denen jeder durch Kontrollvariablengewichte indiziert ist, was durch eine Reduktion auf ein gemischt-ganzzahliges quadratisches Programm ermöglicht wird.
Die Diskrepanz zwischen dem Minimax- und dem Maximin-Zielwert könnte als Proxy für die Schwierigkeiten dienen, auf die der alternierende Gradientenabstieg bei der Optimierung von GANs stößt.In dieser Arbeit geben wir neue Ergebnisse zu den Vorteilen einer Multi-Generator-Architektur von GANs.Wir zeigen, dass die Minimax-Lücke auf \epsilon schrumpft, wenn die Anzahl der Generatoren mit der Rate O(1/\epsilon) steigt. Der Kern unserer Techniken ist eine neuartige Anwendung des Shapley-Folkman-Lemmas auf das generische Minimax-Problem, von dem in der Literatur nur bekannt war, dass es funktioniert, wenn die Zielfunktion auf die Lagrange-Funktion eines Optimierungsproblems mit Einschränkungen beschränkt ist. Das von uns vorgeschlagene Stackelberg-GAN schneidet experimentell sowohl in synthetischen als auch in realen Datensätzen gut ab und verbessert die Frechet Inception Distance um 14,61% gegenüber den vorherigen Multi-Generator-GANs in den Benchmark-Datensätzen.
Nesterov SGD wird häufig für das Training moderner neuronaler Netze und anderer Modelle des maschinellen Lernens verwendet, doch seine Vorteile gegenüber SGD sind theoretisch nicht geklärt, denn wie wir in diesem Papier zeigen, bietet Nesterov SGD mit einer beliebigen Parameterauswahl im Allgemeinen keine Beschleunigung gegenüber gewöhnlicher SGD, und außerdem kann Nesterov SGD bei Schrittgrößen, die die Konvergenz gewöhnlicher SGD gewährleisten, divergieren. Um das Problem der Nicht-Beschleunigung zu lösen, führen wir einen Kompensationsterm in Nesterov SGD ein, der für die gleichen Schrittgrößen wie SGD konvergiert. Wir beweisen, dass MaSS eine beschleunigte Konvergenzrate gegenüber SGD für jede Mini-Batch-Größe in der linearen Umgebung erreicht.  Für eine volle Charge entspricht die Konvergenzrate von MaSS der bekannten beschleunigten Rate der Nesterov-Methode. Wir analysieren auch die praktisch wichtige Frage der Abhängigkeit der Konvergenzrate und der optimalen Hyper-Parameter von der Mini-Batch-Größe und zeigen drei verschiedene Regime: lineare Skalierung, abnehmende Erträge und Sättigung. Die experimentelle Bewertung von MaSS für mehrere Standardarchitekturen von tiefen Netzen, einschließlich ResNet und Faltungsnetze, zeigt eine verbesserte Leistung gegenüber SGD, Nesterov SGD und Adam.
Wir schlagen die Fixed Grouping Layer (FGL) vor; eine neuartige Feedforward-Schicht, die entwickelt wurde, um den induktiven Bias der strukturierten Glätte in ein Deep-Learning-Modell zu integrieren.FGL erreicht dieses Ziel, indem es Knoten über Schichten hinweg basierend auf räumlicher Ähnlichkeit verbindet.Die Verwendung von strukturierter Glätte, wie sie von FGL implementiert wird, ist durch Anwendungen auf strukturierte räumliche Daten motiviert, die wiederum durch Domänenwissen motiviert sind.Die vorgeschlagene Modellarchitektur übertrifft konventionelle neuronale Netzwerkarchitekturen in einer Vielzahl von simulierten und realen Datensätzen mit strukturierter Glätte.
Um dieses Problem zu überwinden, wird in neueren Arbeiten zur Deformationsmodellierung versucht, die Daten räumlich so zu rekonfigurieren, dass eine gemeinsame Anordnung entsteht und die semantische Erkennung weniger unter der Deformation leidet. Dies geschieht in der Regel durch die Ergänzung statischer Operatoren mit gelernten Freiform-Sampling-Gittern im Bildraum, die dynamisch auf die Daten und die Aufgabe zur Anpassung des rezeptiven Feldes abgestimmt sind. Die Anpassung des rezeptiven Feldes erreicht jedoch nicht ganz das eigentliche Ziel - was für das Netzwerk wirklich wichtig ist, ist das *effektive* rezeptive Feld (ERF), das widerspiegelt, wie viel jedes Pixel beiträgt. In dieser Arbeit instanziieren wir eine mögliche Lösung als Deformable Kernels (DKs), eine Familie neuartiger und generischer Faltungsoperatoren zur Behandlung von Objektdeformationen durch direkte Anpassung des ERF, während das rezeptive Feld unangetastet bleibt.Das Herzstück unserer Methode ist die Fähigkeit, den ursprünglichen Kernelraum neu abzutasten, um die Deformation von Objekten wiederherzustellen.Dieser Ansatz wird durch theoretische Einsichten gerechtfertigt, dass das ERF strikt durch Datenabtaststellen und Kernelwerte bestimmt wird. Wir implementieren DKs als generischen Drop-in-Ersatz für starre Kernel und führen eine Reihe empirischer Studien durch, deren Ergebnisse mit unseren Theorien übereinstimmen: Bei mehreren Aufgaben und Standard-Basismodellen schneidet unser Ansatz im Vergleich zu früheren Arbeiten, die sich während der Laufzeit anpassen, gut ab.
Die Ableitung der strukturellen Eigenschaften eines Proteins aus seiner Aminosäuresequenz ist ein anspruchsvolles, aber wichtiges Problem in der Biologie: Die Strukturen der meisten Proteinsequenzen sind nicht bekannt, aber die Struktur ist entscheidend für das Verständnis der Funktion. Bestehende Ansätze zur Erkennung struktureller Ähnlichkeit zwischen Proteinen aus der Sequenz sind nicht in der Lage, strukturelle Muster zu erkennen und zu nutzen, wenn die Sequenzen zu weit auseinander liegen, was unsere Fähigkeit einschränkt, Wissen zwischen strukturell verwandten Proteinen zu übertragen.Wir nähern uns diesem Problem neu durch die Linse des Repräsentationslernens.Wir stellen einen Rahmen vor, der jede Proteinsequenz auf eine Sequenz von Vektoreinbettungen abbildet - eine pro Aminosäureposition - die strukturelle Informationen kodieren. Wir trainieren bidirektionale LSTM-Modelle (Long Short Memory) auf Proteinsequenzen mit einem zweiteiligen Feedback-Mechanismus, der Informationen aus (i) globaler struktureller Ähnlichkeit zwischen Proteinen und (ii) paarweisen Restkontaktkarten für einzelne Proteine einbezieht.Um das Lernen aus strukturellen Ähnlichkeitsinformationen zu ermöglichen, definieren wir ein neuartiges Ähnlichkeitsmaß zwischen beliebig langen Sequenzen von Vektoreinbettungen, das auf einer weichen symmetrischen Ausrichtung (SSA) zwischen ihnen basiert. Unsere Methode ist in der Lage, nützliche positionsspezifische Einbettungen zu erlernen, obwohl keine direkten Beobachtungen der Korrespondenz auf Positionsebene zwischen den Sequenzen vorliegen, und wir zeigen empirisch, dass unser Multitasking-Rahmenwerk andere sequenzbasierte Methoden und sogar eine leistungsstarke strukturbasierte Alignment-Methode bei der Vorhersage struktureller Ähnlichkeit, unserem Ziel, übertrifft, und schließlich zeigen wir, dass unsere erlernten Einbettungen auf andere Proteinsequenz-Probleme übertragen werden können, um den Stand der Technik bei der Vorhersage von Transmembrandomänen zu verbessern.
Inspiriert durch das Anpassungsphänomen biologischer neuronaler Feuerung, schlagen wir eine Regularitätsnormalisierung vor: eine Neuparametrisierung der Aktivierung im neuronalen Netz, die die statistische Regelmäßigkeit im impliziten Raum berücksichtigt, indem der Optimierungsprozess des neuronalen Netzes als ein Modellauswahlproblem betrachtet wird. Wir führen eine inkrementelle Version der Berechnung dieses universellen Codes als normalisierte maximale Wahrscheinlichkeit ein und demonstrieren seine Flexibilität, um Datenprioritäten wie Top-Down-Aufmerksamkeit und andere Orakelinformationen einzubeziehen, sowie seine Kompatibilität, um in die Batch-Normalisierung und die Layer-Normalisierung einbezogen zu werden. Die vorläufigen Ergebnisse zeigten, dass die vorgeschlagene Methode übertrifft bestehende Normalisierung Methoden bei der Bewältigung der begrenzten und unausgewogenen Daten aus einer nicht-stationären Verteilung Benchmarking auf Computer Vision task.as eine unbeaufsichtigte Aufmerksamkeit Mechanismus gegeben Input-Daten, diese biologisch plausibel Normalisierung hat das Potenzial, mit anderen komplizierten realen Szenarien sowie Verstärkung Lernen Einstellung, wo die Belohnungen sind spärlich und ungleichmäßig.Further Forschung wird vorgeschlagen, diese Szenarien zu entdecken und erkunden Sie die Verhaltensweisen zwischen den verschiedenen Varianten.
In diesem Beitrag untersuchen wir die generative Modellierung mittels Autoencodern unter Verwendung der eleganten geometrischen Eigenschaften des optimalen Transportproblems (OT) und der Wasserstein-Distanzen. Wir zeigen, dass die vorgeschlagene Formulierung eine effiziente numerische Lösung bietet, die ähnliche Fähigkeiten wie Wasserstein-Autoencoder (WAE) und Variations-Autoencoder (VAE) bietet, während sie von einer peinlich einfachen Implementierung profitiert. 
Das Ziel des Imitationslernens (IL) ist es, eine gute Politik aus qualitativ hochwertigen Demonstrationen zu lernen, aber die Qualität der Demonstrationen kann in der Realität unterschiedlich sein, da es einfacher und billiger ist, Demonstrationen von einer Mischung aus Experten und Amateuren zu sammeln.IL in solchen Situationen kann eine Herausforderung sein, vor allem, wenn das Niveau der Expertise der Demonstranten unbekannt ist.Wir schlagen ein neues IL-Paradigma namens Variational Imitation Learning with Diverse-quality demonstrations (VILD) vor, in dem wir explizit das Niveau der Expertise der Demonstranten mit einem probabilistischen grafischen Modell modellieren und es zusammen mit einer Belohnungsfunktion schätzen. Wir zeigen, dass ein naiver Schätzungsansatz nicht für große Zustands- und Aktionsräume geeignet ist, und beheben dieses Problem durch die Verwendung eines Variationsansatzes, der leicht mit bestehenden Methoden des Reinforcement Learning implementiert werden kann.Experimente mit kontinuierlichen Kontroll-Benchmarks zeigen, dass VILD die modernsten Methoden übertrifft.Unsere Arbeit ermöglicht skalierbare und dateneffiziente IL unter realistischeren Bedingungen als bisher.
Mit einer wachsenden Anzahl von verfügbaren Diensten, die alle leicht unterschiedliche Parameter, Vorbedingungen und Auswirkungen haben, wird die automatisierte Planung von allgemeinen semantischen Diensten sehr relevant. Die meisten existierenden Planer berücksichtigen jedoch nur PDDL, oder wenn sie behaupten, OWL-S zu verwenden, übersetzen sie es normalerweise in PDDL, wobei sie auf dem Weg viel von der Semantik verlieren. In diesem Beitrag schlagen wir eine neue domänenunabhängige Heuristik vor, die auf einer semantischen Distanz basiert und von generischen Planungsalgorithmen wie A* für die automatisierte Planung von semantischen Diensten, die mit OWL-S beschrieben sind, verwendet werden kann. Damit die Heuristik mehr relevante Informationen enthält, berechnen wir die Heuristik zur Laufzeit.
In farbigen Graphen werden Knotenklassen oft entweder mit der Klasse ihrer Nachbarn oder mit Informationen assoziiert, die nicht in den mit jedem Knoten assoziierten Graphen einfließen.Wir schlagen hier vor, dass Knotenklassen auch mit topologischen Merkmalen der Knoten assoziiert werden.Wir nutzen diese Assoziation, um maschinelles Graphenlernen im Allgemeinen und speziell Graph Convolutional Networks (GCN) zu verbessern. Erstens zeigen wir, dass selbst bei Abwesenheit jeglicher externer Informationen über Knoten eine gute Genauigkeit bei der Vorhersage der Knotenklasse erreicht werden kann, indem entweder topologische Merkmale oder die Klasse der Nachbarn als Eingabe für ein GCN verwendet werden. Zweitens zeigen wir, dass das explizite Hinzufügen der Topologie als Eingabe für die GCN die Genauigkeit nicht verbessert, wenn sie mit externen Informationen über die Knoten kombiniert wird, aber das Hinzufügen einer zusätzlichen Adjazenzmatrix mit Kanten zwischen entfernten Knoten mit ähnlicher Topologie zur GCN verbessert die Genauigkeit erheblich und führt zu Ergebnissen, die besser sind als alle Methoden nach dem Stand der Technik in mehreren Datensätzen.
Fliegen und Mäuse sind Arten, die durch 600 Millionen Jahre Evolution voneinander getrennt sind, und doch haben sie Geruchssysteme entwickelt, die in ihrer anatomischen und funktionellen Organisation viele Gemeinsamkeiten aufweisen.Welche Funktionen erfüllen diese gemeinsamen anatomischen und funktionellen Merkmale, und sind sie optimal für die Geruchswahrnehmung?In dieser Studie befassen wir uns mit der Optimalität des evolutionären Designs in Geruchsschaltkreisen, indem wir künstliche neuronale Netze untersuchen, die auf die Wahrnehmung von Gerüchen trainiert wurden. Wir haben herausgefunden, dass künstliche neuronale Netze die Strukturen des Riechsystems quantitativ rekapitulieren, einschließlich der Bildung von Glomeruli auf einer Kompressionsschicht und einer spärlichen und zufälligen Konnektivität auf einer Expansionsschicht, und wir bieten theoretische Begründungen für jedes Ergebnis.
Die jüngste Richtung der ungepaarten Bild-zu-Bild-Übersetzung ist einerseits sehr aufregend, da sie die große Belastung bei der Erlangung einer markierungsintensiven Pixel-zu-Pixel-Überwachung mindert, aber andererseits ist sie aufgrund des Vorhandenseins von Artefakten und degenerierten Transformationen nicht völlig zufriedenstellend. In diesem Papier, nehmen wir eine vielfältige Sicht des Problems durch die Einführung einer Glätte Begriff über die Probe Graph zu harmonischen Funktionen zu erreichen, um konsistente Mappings während der translation.We erzwingen HarmonicGAN entwickeln, um bidirektionale Übersetzungen zwischen der Quelle und dem Ziel domains.With der Hilfe von Ähnlichkeit-Konsistenz, die inhärente Selbst-Konsistenz Eigenschaft der Proben beibehalten werden kann. Unter einer identischen Problemstellung wie CycleGAN, ohne zusätzliche manuelle Eingaben und mit nur geringen Kosten für die Trainingszeit, zeigt HarmonicGAN eine signifikante qualitative und quantitative Verbesserung gegenüber dem Stand der Technik, sowie eine verbesserte Interpretierbarkeit. Wir zeigen experimentelle Ergebnisse in einer Reihe von Anwendungen, einschließlich medizinischer Bildgebung, Objekttransfiguration und semantischer Beschriftung, und wir übertreffen die konkurrierenden Methoden in allen Aufgaben, und insbesondere für eine medizinische Bildgebungsaufgabe verwandelt unsere Methode CycleGAN von einem Misserfolg in einen Erfolg, indem sie den mittleren quadratischen Fehler halbiert und Bilder erzeugt, die Radiologen in 95% der Fälle gegenüber konkurrierenden Methoden bevorzugen.
Neuere Methoden gehen dieses Problem mit Hilfe von Deep Learning an, das semantische Hinweise nutzen kann, um mit Herausforderungen wie texturlosen und reflektierenden Regionen umzugehen. In diesem Beitrag stellen wir ein faltbares neuronales Netzwerk namens DPSNet (Deep Plane Sweep Network) vor, dessen Design von bewährten Verfahren traditioneller geometriebasierter Ansätze inspiriert ist. Anstatt die Tiefe und/oder die Korrespondenz des optischen Flusses direkt aus Bildpaaren zu schätzen, wie es in vielen früheren Deep-Learning-Methoden der Fall ist, verfolgt DPSNet einen Planesweep-Ansatz, der den Aufbau eines Kostenvolumens aus tiefen Merkmalen unter Verwendung des Planesweep-Algorithmus, die Regularisierung des Kostenvolumens über eine kontextabhängige Kostenaggregation und die Regression der Tiefenkarte aus dem Kostenvolumen beinhaltet. Durch die effektive Einbindung konventioneller Multiview-Stereokonzepte in einen Deep-Learning-Rahmen erzielt DPSNet modernste Rekonstruktionsergebnisse bei einer Vielzahl von anspruchsvollen Datensätzen.
Die Fähigkeit, biologische Strukturen wie DNA oder Proteine zu entwerfen, hätte beträchtliche medizinische und industrielle Auswirkungen und stellt ein herausforderndes Black-Box-Optimierungsproblem dar, das sich durch große Chargen und niedrige Rundenzahlen auszeichnet, da arbeitsintensive Nasslaborauswertungen erforderlich sind.Als Antwort darauf schlagen wir die Verwendung von Reinforcement Learning (RL) auf der Basis von Proximal-Policy-Optimierung (PPO) für biologisches Sequenzdesign vor. Wir schlagen eine modellbasierte Variante der PPO, DyNA-PPO, vor, um die Effizienz der Probenahme zu verbessern, bei der die Strategie für eine neue Runde offline mit einem Simulator trainiert wird, der an funktionale Messungen aus früheren Runden angepasst ist. Um die wachsende Anzahl von Beobachtungen über Runden hinweg zu berücksichtigen, wird das Simulatormodell in jeder Runde automatisch aus einem Pool verschiedener Modelle mit unterschiedlicher Kapazität ausgewählt.  Bei den Aufgaben, DNA-Transkriptionsfaktor-Bindungsstellen zu entwerfen, antimikrobielle Proteine zu entwerfen und die Energie von Ising-Modellen auf der Grundlage der Proteinstruktur zu optimieren, stellen wir fest, dass DyNA-PPO in Situationen, in denen eine Modellierung möglich ist, deutlich besser abschneidet als bestehende Methoden, während es in Situationen, in denen kein zuverlässiges Modell gelernt werden kann, nicht schlechter abschneidet.
Um maschinelle Intelligenz zu erreichen, ist eine nahtlose Integration von Wahrnehmung und logischem Denken erforderlich, doch die bisher entwickelten Modelle neigen dazu, sich auf das eine oder das andere zu spezialisieren; eine ausgefeilte Manipulation von Symbolen, die aus reichhaltigen Wahrnehmungsräumen gewonnen wurden, hat sich bisher als schwer fassbar erwiesen.Betrachten Sie eine visuelle Rechenaufgabe, bei der das Ziel darin besteht, einfache arithmetische Algorithmen auf Ziffern anzuwenden, die unter natürlichen Bedingungen präsentiert werden (z. B. handgeschrieben, zufällig angeordnet). Die untere Ebene besteht aus einer heterogenen Sammlung von Informationsverarbeitungsmodulen, zu denen vortrainierte tiefe neuronale Netze zum Auffinden und Extrahieren von Zeichen aus dem Bild gehören können, sowie Module, die symbolische Transformationen an den durch die Wahrnehmung extrahierten Darstellungen vornehmen. Die übergeordnete Ebene besteht aus einem Controller, der mit Hilfe von Reinforcement Learning trainiert wird und die Module koordiniert, um die Aufgabe auf hoher Ebene zu lösen. So kann der Controller beispielsweise lernen, in welchen Kontexten die Wahrnehmungsnetzwerke auszuführen sind und welche symbolischen Transformationen auf ihre Ausgaben anzuwenden sind.Das resultierende Modell ist in der Lage, eine Vielzahl von Aufgaben im Bereich der visuellen Arithmetik zu lösen, und hat mehrere Vorteile gegenüber standardmäßigen, architektonisch homogenen Feedforward-Netzwerken, einschließlich einer verbesserten Stichprobeneffizienz.
In dieser Arbeit modellieren wir den sozialen Einfluss in das Schema des Verstärkungslernens, so dass die Agenten sowohl von der Umwelt als auch von ihren Artgenossen lernen können.Wir definieren zunächst eine Metrik zur Messung des Abstands zwischen den Strategien und leiten dann quantitativ die Definition der Einzigartigkeit ab. Im Gegensatz zu früheren Ansätzen der gemeinsamen Optimierung wird die Motivation der sozialen Einzigartigkeit in unserer Arbeit als Einschränkung auferlegt, um den Agenten zu ermutigen, eine andere Strategie als die vorhandenen Agenten zu erlernen und dennoch die ursprüngliche Aufgabe zu lösen.
Generative adversarische Netzwerke (GANs) haben sich bei der Approximation komplexer Verteilungen hochdimensionaler Eingabedaten als äußerst effektiv erwiesen, und es wurden erhebliche Fortschritte beim Verständnis und bei der Verbesserung der Leistung von GANs sowohl in der Theorie als auch in der Anwendung erzielt. Aus diesem Grund haben wir, obwohl viele GAN-Varianten vorgeschlagen werden, relativ wenig Verständnis für ihre relativen Fähigkeiten.in diesem Papier, bewerten wir die Leistung der verschiedenen Arten von GANs mit Divergenz und Abstand Funktionen, die in der Regel nur für die Ausbildung.wir beobachten Konsistenz über die verschiedenen vorgeschlagenen Metriken und, interessanterweise, die Test-Zeit-Metriken nicht bevorzugen Netzwerke, die die gleiche Ausbildung-Zeit-Kriterium.wir vergleichen auch die vorgeschlagenen Metriken zu menschlichen Wahrnehmungswerte.
Wissensdatenbanken (KB) werden oft als eine Sammlung von Fakten in der Form (HEAD, PREDICATE, TAIL) dargestellt, wobei HEAD und TAIL Entitäten sind, während PREDICATE eine binäre Beziehung ist, die die beiden verbindet.Es ist eine bekannte Tatsache, dass Wissensdatenbanken bei weitem nicht vollständig sind, und daher die Fülle der Forschung auf KB-Vervollständigungsmethoden, insbesondere auf Link-Vorhersage.Allerdings, wenn auch häufig ignoriert, enthalten diese Repositories auch numerische Fakten.numerische Fakten verbinden Entitäten mit numerischen Werten über numerische Prädikate; z.B., (Um dieses Problem anzugehen, führen wir das Problem der numerischen Attributvorhersage ein.Dieses Problem beinhaltet eine neue Art von Abfrage, bei der die Beziehung ein numerisches Prädikat ist.Folglich und im Gegensatz zur Linkvorhersage ist die Antwort auf diese Abfrage ein numerischer Wert. Wir argumentieren, dass die numerischen Werte, die mit Entitäten assoziiert sind, bis zu einem gewissen Grad die relationale Struktur der Wissensbasis erklären und nutzen daher Methoden zur Einbettung der Wissensbasis, um Repräsentationen zu erlernen, die nützliche Prädiktoren für die numerischen Attribute sind.eine umfangreiche Reihe von Experimenten auf Benchmark-Versionen von FREEBASE und YAGO zeigen, dass unsere Ansätze weitgehend vernünftige Baselines übertreffen.wir machen die Datensätze unter einer freizügigen BSD-3 Lizenz verfügbar.
Wir schlagen Verfahren für die Bewertung und Stärkung der kontextuellen Einbettung Alignment und zeigen, dass sie nützlich sind bei der Analyse und Verbesserung der mehrsprachigen BERT.Insbesondere nach unserer vorgeschlagenen Alignment-Verfahren, zeigt BERT deutlich verbessert Null-Shot-Performance auf XNLI im Vergleich zu dem Basismodell, bemerkenswert passend pseudo-voll überwachten übersetzen-Train-Modelle für Bulgarisch und Griechisch. Um den Grad des Alignments zu messen, führen wir eine kontextuelle Version der Wortsuche ein und zeigen, dass diese gut mit dem Zero-Shot-Transfer korreliert. Mit Hilfe dieser Wortsuchaufgabe analysieren wir auch BERT und stellen fest, dass es systematische Mängel aufweist, z.B. schlechteres Alignment für offene Wortteile und Wortpaare in verschiedenen Schriften, die durch das Alignment-Verfahren korrigiert werden.Diese Ergebnisse unterstützen kontextuelles Alignment als nützliches Konzept für das Verständnis großer mehrsprachiger vortrainierter Modelle.
Neuronale Netze haben hervorragende Leistungen bei der Lösung verschiedener inverser Probleme in der Bildverarbeitung erbracht. Die Nachteile von End-to-End-Lernansätzen im Vergleich zu klassischen Variationsverfahren sind jedoch die Notwendigkeit eines teuren Neutrainings selbst für geringfügig veränderte Problemstellungen und das Fehlen nachweisbarer Fehlergrenzen während der Inferenz. In neueren Arbeiten wurde das erste Problem angegangen, indem Netzwerke, die für die Gaußsche Bildentrauschung trainiert wurden, als generische Plug-and-Play-Regulierer in Energieminimierungsalgorithmen verwendet wurden. In neueren Arbeiten wurde vorgeschlagen, Netzwerke so zu trainieren, dass sie Abstiegsrichtungen in Bezug auf eine gegebene Energiefunktion mit einer nachweisbaren Garantie für die Konvergenz zu einem Minimierer dieser Energie ausgeben.Allerdings erfordert jedes Problem und jede Energie das Training eines separaten Netzwerks.In diesem Papier betrachten wir die Kombination beider Ansätze, indem wir die Ausgaben eines Plug-and-Play-Entrauschungsnetzwerks auf den Kegel der Abstiegsrichtungen zu einer gegebenen Energie projizieren.Auf diese Weise kann ein einziges vortrainiertes Netzwerk für eine Vielzahl von Rekonstruktionsaufgaben verwendet werden.Unsere Ergebnisse zeigen Verbesserungen im Vergleich zu klassischen Energieminimierungsmethoden, während immer noch nachweisbare Konvergenzgarantien bestehen.
Wir zeigen, dass populäre Einbettungsmodelle in der Tat unkalibriert sind, was bedeutet, dass Wahrscheinlichkeitsschätzungen, die mit vorhergesagten Tripeln verbunden sind, unzuverlässig sind.Wir stellen eine neuartige Methode vor, um ein Modell zu kalibrieren, wenn negative Grundwahrheiten nicht verfügbar sind, was der übliche Fall in Wissensgraphen ist.Wir schlagen vor, Platt-Skalierung und isotonische Regression neben unserer Methode zu verwenden. Experimente an drei Datensätzen mit Negativen zeigen, dass unser Beitrag zu gut kalibrierten Modellen führt, wenn man ihn mit dem Goldstandard der Verwendung von Negativen vergleicht.Wir erhalten signifikant bessere Ergebnisse als die unkalibrierten Modelle aller Kalibrierungsmethoden.Wir zeigen, dass die isotonische Regression die beste Gesamtleistung bietet, nicht ohne Kompromisse.Wir zeigen auch, dass kalibrierte Modelle eine State-of-the-Art-Genauigkeit erreichen, ohne die Notwendigkeit, beziehungsspezifische Entscheidungsschwellen zu definieren.
In jüngster Zeit wird die Idee von Mining-basierten Strategien übernommen, um die falsch klassifizierten Proben hervorzuheben und vielversprechende Ergebnisse zu erzielen. Während des gesamten Trainingsprozesses betonen die vorherigen Methoden jedoch entweder nicht explizit die Probe auf der Grundlage ihrer Wichtigkeit, was dazu führt, dass die harten Proben nicht vollständig ausgenutzt werden, oder sie betonen explizit die Auswirkungen der halbharten/harten Proben sogar in der frühen Trainingsphase, was zu Konvergenzproblemen führen kann. In dieser Arbeit schlagen wir einen neuartigen adaptiven Curriculum-Learning-Verlust (CurricularFace) vor, der die Idee des Curriculum-Lernens in die Verlustfunktion einbettet, um eine neuartige Trainingsstrategie für die tiefe Gesichtserkennung zu erreichen, die hauptsächlich leichte Proben in der frühen Trainingsphase und schwere Proben in der späteren Phase berücksichtigt. Unser CurricularFace passt die relative Wichtigkeit von leichten und schweren Mustern während der verschiedenen Trainingsphasen adaptiv an, wobei in jeder Phase den verschiedenen Mustern entsprechend ihrer Schwierigkeit unterschiedliche Wichtigkeit zugewiesen wird.Umfangreiche experimentelle Ergebnisse auf populären Benchmarks zeigen die Überlegenheit unseres CurricularFace gegenüber dem Stand der Technik.
Adversarial examples are perturbed inputs designed to fool machine learning models.Adversarial training injects such examples into training data to increase robustness.To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. Wir zeigen, dass diese Form des adversen Trainings zu einem degenerierten globalen Minimum konvergiert, in dem kleine Krümmungsartefakte in der Nähe der Datenpunkte eine lineare Annäherung an den Verlust verschleiern. Infolgedessen stellen wir fest, dass das adversarische Training anfällig für Blackbox-Angriffe bleibt, bei denen wir Störungen übertragen, die auf unverteidigten Modellen berechnet wurden, sowie für einen leistungsstarken neuen Ein-Schritt-Angriff, der die nicht-glatte Umgebung der Eingabedaten durch einen kleinen Zufallsschritt umgeht. Darüber hinaus stellen wir Ensemble Adversarial Training vor, eine Technik, die Trainingsdaten mit Störungen aus anderen Modellen anreichert. Auf ImageNet führt Ensemble Adversarial Training zu Modellen mit hoher Robustheit gegenüber Blackbox-Angriffen. Insbesondere unser robustestes Modell gewann die erste Runde des NIPS 2017-Wettbewerbs zum Thema Verteidigung gegen Adversarial-Angriffe.
Ein adversarial feature learning (AFL) ist ein leistungsfähiges Framework, um Repräsentationen zu lernen, die invariant zu einem störenden Attribut sind, das ein adversariales Spiel zwischen einem Feature-Extraktor und einem kategorialen Attribut-Klassifikator verwendet.Theoretisch klingt es im Sinne der Maximierung der bedingten Entropie zwischen Attribut und Repräsentation. Wir schlagen ein {\em attribute perception matching} als alternativen Ansatz vor, der auf der Neuformulierung der bedingten Entropiemaximierung als {\em pair-wise distribution matching} basiert. Obwohl der naive Ansatz für die Realisierung des paarweisen Verteilungsabgleichs eine beträchtliche Anzahl von Parametern erfordert, benötigt die vorgeschlagene Methode die gleiche Anzahl von Parametern wie AFL, hat aber eine bessere Konvergenzeigenschaft.Experimente an Spielzeug- und realen Datensätzen beweisen, dass die von uns vorgeschlagene Methode wesentlich schneller zu einer besseren invarianten Darstellung konvergiert als AFL.  
Wir untersuchen die Eigenschaften allgemeiner Verlustflächen anhand ihrer Hessian-Matrix und zeigen empirisch, dass das Spektrum der Hessian-Matrix im Kontext des Deep Learning aus zwei Teilen besteht: (1) dem Hauptteil, der in der Nähe von Null zentriert ist, (2) und Ausreißern, die vom Hauptteil entfernt sind.Wir präsentieren numerische Beweise und mathematische Begründungen für die folgenden Vermutungen, die von Sagun et.al. (2016) aufgestellt wurden: Wenn man die Daten fixiert und die Anzahl der Parameter erhöht, skaliert man lediglich den Hauptteil des Spektrums; wenn man die Dimension fixiert und die Daten verändert (z.B. indem man mehr Cluster hinzufügt oder die Daten weniger trennbar macht), wirkt sich das nur auf die Ausreißer aus.Wir glauben, dass unsere Beobachtungen bemerkenswerte Implikationen für die nicht-konvexe Optimierung in hohen Dimensionen haben. Erstens impliziert die *Flächigkeit* solcher Landschaften (die durch die Singularität der Hessian gemessen werden kann), dass die klassischen Vorstellungen von Anziehungsbereichen ziemlich irreführend sein können und dass die Diskussion über breite/schmale Bereiche eine neue Perspektive in Bezug auf Überparametrisierung und Redundanz benötigt, die in der Lage sind, *große* zusammenhängende Komponenten am Boden der Landschaft zu schaffen. Zweitens kann die Abhängigkeit einer kleinen Anzahl großer Eigenwerte von der Datenverteilung mit dem Spektrum der Kovarianzmatrix der Gradienten von Modelloutputs in Verbindung gebracht werden, so dass wir die Verbindungen innerhalb des Daten-Architektur-Algorithmus-Rahmens eines Modells neu bewerten können, in der Hoffnung, dass dies Licht auf die Geometrie hochdimensionaler und nicht-konvexer Räume in modernen Anwendungen werfen würde. Insbesondere stellen wir einen Fall vor, der die beiden Beobachtungen miteinander verbindet: Klein- und Großserien-Gradientenabstieg scheinen zu unterschiedlichen Anziehungsbereichen zu konvergieren, aber wir zeigen, dass sie in Wirklichkeit durch ihre flache Region verbunden sind und daher zum selben Bereich gehören.
Die Zuteilung von Rechenressourcen im Backbone ist ein entscheidendes Thema bei der Objekterkennung, aber die Klassifizierungszuweisung wird normalerweise direkt in den Objektdetektor übernommen, was sich als suboptimal erweist. Um die belegten Rechenressourcen effizienter umzuverteilen, stellen wir CR-NAS (Computation Reallocation Neural Architecture Search) vor, das Strategien zur Umverteilung von Rechenressourcen über verschiedene Merkmalsauflösungen und räumliche Positionen hinweg direkt auf dem Zielerkennungsdatensatz erlernen kann.Ein zweistufiger Umverteilungsraum wird sowohl für die Stufe als auch für die räumliche Umverteilung vorgeschlagen. Wir wenden CR-NAS auf mehrere Backbones an und erzielen konsistente Verbesserungen: Unser CR-ResNet50 und CR-MobileNetV2 übertreffen die Baseline um 1,9% bzw. 1,7% COCO AP ohne zusätzliches Berechnungsbudget. Die von CR-NAS entdeckten Modelle können mit anderen leistungsstarken Erkennungshälsen/-köpfen ausgestattet und leicht auf andere Datensätze, z.B. PASCAL VOC, und andere Sehaufgaben, z.B. Instanzsegmentierung, übertragen werden.Unser CR-NAS kann als Plugin verwendet werden, um die Leistung verschiedener Netzwerke zu verbessern, was sehr anspruchsvoll ist.
Unsicherheit ist ein sehr wichtiges Merkmal der Intelligenz und hilft dem Gehirn, ein flexibles, kreatives und leistungsfähiges intelligentes System zu werden.Die Crossbar-basierten neuromorphen Computerchips, in denen die Berechnung hauptsächlich durch analoge Schaltungen durchgeführt wird, haben die Unsicherheit und können verwendet werden, um das Gehirn zu imitieren.Allerdings haben die meisten der aktuellen tiefen neuronalen Netze nicht die Unsicherheit des neuromorphen Computerchips berücksichtigt. In dieser Arbeit haben wir das Unsicherheitsanpassungstrainingsschema (UATS) vorgeschlagen, das die Unsicherheit dem neuronalen Netz im Trainingsprozess mitteilt. Die experimentellen Ergebnisse zeigen, dass die neuronalen Netze vergleichbare Inferenzleistungen auf dem unsicheren neuromorphen Computerchip im Vergleich zu den Ergebnissen auf den ursprünglichen Plattformen erreichen können und viel besser sind als die Leistungen ohne dieses Trainingsschema.
Eine wichtige Eigenschaft von Bildklassifizierungssystemen in der realen Welt besteht darin, dass sie sowohl Objekte aus Zielklassen (``Bekannte'') genau klassifizieren als auch unbekannte Objekte (``Unbekannte''), die zu Klassen gehören, die in den Trainingsdaten nicht vorhanden sind, sicher zurückweisen.Leider sorgt die starke Generalisierungsfähigkeit bestehender CNNs zwar für ihre Genauigkeit bei der Klassifizierung bekannter Objekte, führt aber auch dazu, dass sie Unbekannte oft mit hohem Vertrauen einer Zielklasse zuordnen. In dieser Arbeit schlagen wir ein Unknown-aware Deep Neural Network (kurz UDN) vor, um dieses schwierige Problem zu lösen. Die Schlüsselidee von UDN ist es, bestehende CNNs zu verbessern, um eine Produktoperation zu unterstützen, die die Produktbeziehung zwischen den Merkmalen modelliert, die von Faltungsschichten erzeugt werden. Auf diese Weise wird das Fehlen eines einzigen Schlüsselmerkmals einer Zielklasse die Wahrscheinlichkeit, ein Objekt dieser Klasse zuzuordnen, stark reduzieren.UDN verwendet ein erlerntes Ensemble dieser Produktoperationen, was es ermöglicht, die widersprüchlichen Anforderungen der genauen Klassifizierung bekannter Objekte und der korrekten Zurückweisung von Unbekannten auszugleichen.Um die Leistung von UDN bei der Erkennung von Unbekannten weiter zu verbessern, schlagen wir eine informationstheoretische Regularisierungsstrategie vor, die das Ziel der Zurückweisung von Unbekannten in den Lernprozess von UDN integriert. Wir experimentieren mit Benchmark-Bilddatensätzen, darunter MNIST, CIFAR-10, CIFAR-100 und SVHN, und fügen Unbekannte hinzu, indem wir einen Datensatz in einen anderen injizieren. Unsere Ergebnisse zeigen, dass UDN die State-of-the-Art-Methoden bei der Zurückweisung von Unbekannten deutlich übertrifft, indem es die Genauigkeit um 25 Prozentpunkte verbessert, während die Klassifizierungsgenauigkeit erhalten bleibt.
Multivariate Zeitreihen mit fehlenden Werten sind in Bereichen wie dem Gesundheits- und Finanzwesen weit verbreitet und haben im Laufe der Jahre an Zahl und Komplexität zugenommen, was die Frage aufwirft, ob Deep-Learning-Methoden die klassischen Datenimputationsmethoden in diesem Bereich übertreffen können. naive Anwendungen von Deep Learning liefern jedoch keine zuverlässigen Konfidenzschätzungen und sind nicht interpretierbar. Die nichtlineare Dimensionalitätsreduktion bei fehlenden Daten wird durch einen VAE-Ansatz mit einer neuartigen strukturierten Variationsapproximation erreicht. Wir zeigen, dass unser Ansatz mehrere klassische und Deep-Learning-basierte Datenimputationsmethoden für hochdimensionale Daten aus den Bereichen Computer Vision und Gesundheitswesen übertrifft, während er zusätzlich die Glattheit der Imputationen verbessert und interpretierbare Unsicherheitsschätzungen liefert.
In der Praxis zeigt sich oft, dass große, überparametrisierte neuronale Netze besser verallgemeinern als ihre kleineren Gegenstücke, eine Beobachtung, die im Widerspruch zu klassischen Vorstellungen von Funktionskomplexität zu stehen scheint, die typischerweise kleinere Modelle bevorzugen.In dieser Arbeit untersuchen wir dieses Spannungsverhältnis zwischen Komplexität und Verallgemeinerung durch eine umfassende empirische Untersuchung zweier natürlicher Metriken der Komplexität, die mit der Empfindlichkeit gegenüber Eingabestörungen zusammenhängen.Unsere Experimente untersuchen Tausende von Modellen mit verschiedenen Architekturen, Optimierern und anderen Hyperparametern sowie vier verschiedene Bildklassifizierungsdatensätze. Wir stellen fest, dass trainierte neuronale Netze robuster gegenüber Eingabestörungen in der Nähe der Trainingsdaten sind, gemessen an der Eingabe-Ausgabe-Jakobiante des Netzes, und dass dies gut mit der Generalisierung korreliert. Wir stellen außerdem fest, dass Faktoren, die mit schlechter Generalisierung verbunden sind - wie Full-Batch-Training oder die Verwendung von Zufallsetiketten - einer höheren Sensitivität entsprechen, während Faktoren, die mit guter Generalisierung verbunden sind - wie Datenvergrößerung und ReLU-Nichtlinearitäten - zu robusteren Funktionen führen.
Eine Alternative besteht darin, den Parametern des Agenten direkt Rauschen hinzuzufügen, was zu einer konsistenteren Exploration und einem reicheren Satz von Verhaltensweisen führen kann. Wir zeigen, dass sowohl Off- als auch On-Policy-Methoden von diesem Ansatz profitieren, indem wir DQN, DDPG und TRPO in hochdimensionalen, diskreten Aktionsumgebungen sowie in kontinuierlichen Steuerungsaufgaben experimentell vergleichen.
Vorgefertigte Sprachmodelle mit tiefen neuronalen Netzen wie ELMo, GPT, BERT und XLNet haben in jüngster Zeit bei einer Reihe von Sprachverstehensaufgaben Spitzenleistungen erzielt, sind jedoch aufgrund ihrer Größe für eine Reihe von Szenarien, insbesondere auf mobilen Geräten und Edge-Geräten, unpraktisch. Vor allem die Einbettungsmatrix der Eingabewörter macht aufgrund des großen Eingabevokabulars und der Einbettungsdimensionen einen beträchtlichen Teil des Speicherbedarfs des Modells aus.2 Techniken zur Wissensdestillation waren erfolgreich bei der Komprimierung großer neuronaler Netzwerkmodelle, aber sie sind unwirksam, wenn es darum geht, Schülermodelle mit Vokabularen zu erstellen, die sich von den ursprünglichen Lehrermodellen unterscheiden. Wir stellen eine neuartige Technik zur Wissensdestillation vor, um ein Schülermodell mit einem deutlich kleineren Wortschatz sowie niedrigeren Einbettungs- und versteckten Zustandsdimensionen zu trainieren, indem wir einen dualen Trainingsmechanismus einsetzen, der das Lehrer- und das Schülermodell gleichzeitig trainiert, um optimale Worteinbettungen für das Schülervokabular zu erhalten. Unsere Methode ist in der Lage, das BERT-BASE-Modell um mehr als das 60-fache zu komprimieren, wobei sich die nachgelagerten Aufgabenmetriken nur geringfügig verschlechtern, was zu einem Sprachmodell mit einer Größe von weniger als 7 MB führt.Experimentelle Ergebnisse zeigen auch eine höhere Komprimierungseffizienz und Genauigkeit im Vergleich zu anderen modernen Komprimierungstechniken.
Die Nebenprodukte einer solchen Argumentation sind Invarianten, die Muster über Beispiele hinweg erfassen, wie z.B. "wenn jemand irgendwo hingegangen ist, dann ist er dort", ohne bestimmte Personen oder Orte zu erwähnen.Menschen lernen schon in jungen Jahren, was Variablen sind und wie sie zu verwenden sind, und die Frage, die in diesem Papier behandelt wird, ist, ob Maschinen auch lernen und Variablen allein aus Beispielen verwenden können, ohne dass menschliches Pre-Engineering erforderlich ist. Wir schlagen Unification Networks vor, die Soft Unification in neuronale Netze integrieren, um Variablen zu erlernen und dadurch Beispiele in Invarianten zu heben, die dann zur Lösung einer bestimmten Aufgabe verwendet werden können.Wir evaluieren unseren Ansatz an vier Datensätzen, um zu zeigen, dass das Erlernen von Invarianten Muster in den Daten erfasst und die Leistung im Vergleich zu den Basislinien verbessern kann.
Wir schlagen einen neuen lernbasierten Ansatz vor, um schlecht gestellte inverse Probleme in der Bildgebung zu lösen. Wir befassen uns mit dem Fall, in dem Bodenwahrheitstrainingsproben selten sind und das Problem stark schlecht gestellt ist - sowohl wegen der zugrunde liegenden Physik als auch weil wir nur wenige Messungen erhalten können. Diese Einstellung ist in der geophysikalischen Bildgebung und Fernerkundung üblich. Wir zeigen, dass in diesem Fall der übliche Ansatz, die Abbildung der Messdaten auf die Rekonstruktion direkt zu erlernen, instabil wird. Stattdessen schlagen wir vor, zunächst ein Ensemble von einfacheren Abbildungen von den Daten auf Projektionen des unbekannten Bildes in zufällige, stückweise konstante Unterräume zu erlernen und dann die Projektionen zu kombinieren, um eine endgültige Rekonstruktion zu bilden, indem wir ein Dekonvolution-ähnliches Problem lösen.Wir zeigen experimentell, dass die vorgeschlagene Methode robuster gegenüber Messrauschen und Störungen ist, die während des Trainings nicht gesehen werden, als eine direkt erlernte Umkehrung.
Wir schlagen eine Technik vor, um die Auswahl der Architektur zu beschleunigen, indem wir ein zusätzliches HyperNet lernen, das die Gewichte eines Hauptmodells in Abhängigkeit von der Architektur des Modells generiert.Durch den Vergleich der relativen Validierungsleistung von Netzwerken mit HyperNet-generierten Gewichten können wir effektiv über eine breite Palette von Architekturen zu den Kosten eines einzigen Trainingslaufs suchen. Zur Erleichterung dieser Suche entwickeln wir einen flexiblen Mechanismus, der auf Lese- und Schreibvorgängen im Speicher basiert und es uns ermöglicht, eine breite Palette von Netzwerkkonnektivitätsmustern zu definieren, mit ResNet-, DenseNet- und FractalNet-Blöcken als Spezialfälle. Wir validieren unsere Methode (SMASH) auf CIFAR-10 und CIFAR-100, STL-10, ModelNet10 und Imagenet32x32 und erreichen eine konkurrenzfähige Leistung mit ähnlich großen, von Hand entworfenen Netzwerken.
Das Standardprotokoll für die Sammlung von NLI wurde nicht für die Erstellung von Pretraining-Daten konzipiert und ist für diesen Zweck wahrscheinlich alles andere als ideal. Mit Blick auf diese Anwendung schlagen wir vier alternative Protokolle vor, die jeweils darauf abzielen, entweder die Leichtigkeit, mit der Annotatoren solide Trainingsbeispiele produzieren können, oder die Qualität und Vielfalt dieser Beispiele zu verbessern. Unsere primären Ergebnisse sind größtenteils negativ, wobei keine dieser neuen Methoden wesentliche Verbesserungen beim Transferlernen zeigt. Wir machen jedoch mehrere Beobachtungen, die in die künftige Arbeit mit NLI-Daten einfließen sollten, wie z. B., dass die Verwendung automatisch bereitgestellter Seed-Sätze zur Inspiration die Qualität der resultierenden Daten in den meisten Bereichen verbessert, und dass alle von uns untersuchten Interventionen die zuvor beobachteten Probleme mit Annotationsartefakten drastisch reduzieren.
Ereignisse in der realen Welt können stochastisch und unvorhersehbar sein, und die hohe Dimensionalität und Komplexität natürlicher Bilder erfordert, dass das Vorhersagemodell ein komplexes Verständnis der natürlichen Welt aufbaut.Viele bestehende Methoden gehen dieses Problem an, indem sie vereinfachende Annahmen über die Umgebung treffen.Eine häufige Annahme ist, dass das Ergebnis deterministisch ist und es nur eine plausible Zukunft gibt.Dies kann in realen Umgebungen mit stochastischer Dynamik zu qualitativ schlechten Vorhersagen führen. In diesem Beitrag entwickeln wir eine stochastische variable Videovorhersage (SV2P), die für jede Probe der latenten Variablen eine andere mögliche Zukunft vorhersagt. Wir demonstrieren die Fähigkeit der vorgeschlagenen Methode bei der Vorhersage detaillierter zukünftiger Frames von Videos auf mehreren realen Datensätzen, sowohl handlungsfrei als auch handlungsbedingt, und stellen fest, dass die von uns vorgeschlagene Methode im Vergleich zu demselben Modell ohne Stochastik und zu anderen stochastischen Videovorhersagemethoden wesentlich bessere Videovorhersagen liefert.Unsere SV2P-Implementierung wird nach der Veröffentlichung als Open Source verfügbar sein.
In Multi-Agenten-Systemen kommt es aufgrund der hohen Korrelationen zwischen den Agenten zu komplexem interaktiven Verhalten, wobei bisherige Arbeiten zur Modellierung von Multi-Agenten-Interaktionen anhand von Demonstrationen in erster Linie durch die Annahme der Unabhängigkeit zwischen den Strategien und ihren Belohnungsstrukturen eingeschränkt sind. In dieser Arbeit wird das Problem der Modellierung von Multi-Agenten-Interaktionen in einen Rahmen des Multi-Agenten-Nachahmungslernens mit expliziter Modellierung korrelierter Politiken durch Annäherung an die Politiken der Gegner gegossen, wodurch die Politiken der Agenten wiederhergestellt werden können, die ähnliche Interaktionen generieren können. Infolgedessen entwickeln wir einen dezentralisierten Algorithmus für adversariales Nachahmungslernen mit korrelierten Strategien (CoDAIL), der dezentralisiertes Training und Ausführung ermöglicht. Verschiedene Experimente zeigen, dass CoDAIL komplexe Interaktionen besser regenerieren kann, die nahe an den Demonstratoren liegen, und die modernsten Methoden des Multi-Agenten-Nachahmungslernens übertrifft.
Bestehende Systeme zur Erkennung von Plagiaten zeigen eine gute Leistung und haben eine riesige Quellendatenbank, so dass es jetzt nicht mehr ausreicht, den Text so zu kopieren, wie er ist, um die ursprüngliche Arbeit zu erhalten.Daher wird eine andere Art von Plagiaten populär - sprachübergreifende Plagiate.Wir präsentieren ein CrossLang-System für diese Art von Plagiatserkennung für Englisch-Russisch Sprachpaar.
Datenparallelität ist zu einer dominanten Methode geworden, um das Training von tiefen neuronalen Netzen (DNN) über mehrere Knoten zu skalieren.  Da die Synchronisierung der lokalen Modelle oder Gradienten ein Engpass für ein groß angelegtes verteiltes Training sein kann, hat die Komprimierung des Kommunikationsverkehrs in letzter Zeit große Beachtung gefunden.  Unter mehreren kürzlich vorgeschlagenen Kompressionsalgorithmen ist die Residual Gradient Compression (RGC) einer der erfolgreichsten Ansätze - sie kann die Größe der übertragenden Nachricht (0,1 % der Gradientengröße) jedes Knotens erheblich komprimieren und dennoch die Genauigkeit bewahren. Allerdings konzentriert sich die Literatur über die Komprimierung von tiefen Netzwerken fast ausschließlich auf das Erreichen einer guten Kompressionsrate, während die Effizienz von RGC in der realen Implementierung weniger untersucht wurde.In diesem Papier entwickeln wir eine RGC-Methode, die eine erhebliche Verbesserung der Trainingszeit in realen Multi-GPU-Systemen erreicht. Wir untersuchen die Leistung von RedSync auf zwei verschiedenen Multi-GPU-Plattformen, darunter ein Supercomputer und ein Multi-Card-Server. Unsere Testfälle umfassen Bildklassifikation auf Cifar10 und ImageNet sowie Sprachmodellierungsaufgaben auf Penn Treebank und Wiki2-Datensätzen. Für DNNs mit hohem Kommunikations-zu-Berechnungs-Verhältnis, die lange Zeit als schlecht skalierbar galten, zeigt RedSync eine signifikante Leistungssteigerung.
Backpropagation ist die treibende Kraft hinter den heutigen künstlichen neuronalen Netzen, aber trotz umfangreicher Forschung bleibt es unklar, ob das Gehirn diesen Algorithmus implementiert.Unter Neurowissenschaftlern werden Algorithmen des Reinforcement Learning (RL) oft als realistische Alternative angesehen, aber die Konvergenzrate eines solchen Lernens skaliert schlecht mit der Anzahl der beteiligten Neuronen. Hier schlagen wir einen hybriden Lernansatz vor, bei dem jedes Neuron eine RL-ähnliche Strategie verwendet, um zu lernen, wie man sich den Gradienten annähert, die Backpropagation liefern würde.Wir zeigen, dass unser Ansatz lernt, sich dem Gradienten anzunähern, und die Leistung des gradientenbasierten Lernens auf vollständig verbundenen und faltigen Netzen erreichen kann.Das Lernen von Rückkopplungsgewichten bietet einen biologisch plausiblen Mechanismus, um eine gute Leistung zu erzielen, ohne dass präzise, vorab festgelegte Lernregeln erforderlich sind.
Die meisten tiefen neuronalen Netze (DNNs) erfordern komplexe Modelle, um eine hohe Leistung zu erzielen.Parameterquantisierung ist weit verbreitet, um die Implementierungskomplexität zu reduzieren.Frühere Studien zur Quantisierung basierten meist auf umfangreichen Simulationen mit Trainingsdaten.Wir wählen einen anderen Ansatz und versuchen, die Kapazität von DNN-Modellen pro Parameter zu messen und die Ergebnisse zu interpretieren, um Einblicke in die optimale Quantisierung von Parametern zu erhalten.Diese Forschung verwendet künstlich erzeugte Daten und generische Formen von voll verbundenen DNNs, faltigen neuronalen Netzen und rekurrenten neuronalen Netzen. Wir führen Speicher- und Klassifikationstests durch, um die Auswirkungen der Anzahl und Präzision der Parameter auf die Leistung zu untersuchen. Das Modell und die Kapazitäten pro Parameter werden durch die Messung der gegenseitigen Information zwischen der Eingabe und der klassifizierten Ausgabe bewertet. Wir erweitern die Ergebnisse der Speicherkapazitätsmessung auch auf Bildklassifizierungs- und Sprachmodellierungsaufgaben.
Inspiriert durch die Kombination von Feedforward- und iterativen Berechnungen im visuellen Kortex und unter Ausnutzung der Fähigkeit von denoising autoencoders, den Wert einer gemeinsamen Verteilung zu schätzen, schlagen wir einen neuartigen Ansatz zur iterativen Inferenz vor, um die komplexe gemeinsame Verteilung von Ausgangsvariablen, die von einigen Eingangsvariablen abhängig sind, zu erfassen und zu nutzen. Dieser Ansatz wird auf die pixelweise Segmentierung von Bildern angewandt, wobei die geschätzte bedingte Punktzahl verwendet wird, um einen Gradientenanstieg in Richtung eines Modus der geschätzten bedingten Verteilung durchzuführen, was frühere Arbeiten zur Punktzahlschätzung durch entrauschende Autoencoder auf den Fall einer bedingten Verteilung ausweitet, wobei eine neuartige Verwendung eines korrumpierten Feedforward-Prädiktors die Gaußsche Korruption ersetzt. Ein Vorteil dieses Ansatzes gegenüber klassischeren Methoden der iterativen Inferenz für strukturierte Ausgänge, wie z.B. bedingte Zufallsfelder (CRFs), ist, dass es nicht mehr notwendig ist, eine explizite Energiefunktion zu definieren, die die Ausgangsvariablen verknüpft, um die Berechnungen überschaubar zu halten. Wir haben experimentell herausgefunden, dass die vorgeschlagene iterative Inferenz aus der bedingten Score-Schätzung durch bedingte denoising autoencoders besser abschneidet als vergleichbare Modelle auf der Basis von CRFs oder solche, die keine explizite Modellierung der bedingten gemeinsamen Verteilung der Outputs verwenden.
Inspiriert vom Erfolg des Selbstaufmerksamkeitsmechanismus und der Transformer-Architektur in Sequenztransduktions- und Bilderzeugungsanwendungen schlagen wir neuartige, auf Selbstaufmerksamkeit basierende Architekturen vor, um die Leistung von auf adversen latenten Codes basierenden Schemata in der Texterzeugung zu verbessern.Die auf adversen latenten Codes basierende Texterzeugung hat in letzter Zeit aufgrund ihrer vielversprechenden Ergebnisse viel Aufmerksamkeit erregt.In diesem Papier unternehmen wir einen Schritt zur Stärkung der in diesen Setups verwendeten Architekturen, insbesondere AAE und ARAE. In unseren Experimenten wird der Google-Satzkomprimierungsdatensatz verwendet, um unsere Methode mit diesen Methoden unter Verwendung verschiedener objektiver und subjektiver Maße zu vergleichen. Die Experimente zeigen, dass die vorgeschlagenen (selbst-)aufmerksamkeitsbasierten Modelle den Stand der Technik bei der adversarialcodebasierten Texterzeugung übertreffen.
Einige Arbeiten versuchen, Angriffsauslöser auf ihren gegnerischen Proben zu verstecken, wenn sie neuronale Netze angreifen, und andere wollen neuronale Netze mit Wasserzeichen versehen, um ihr Eigentum vor Plagiaten zu schützen. In diesem Beitrag stellen wir eine allgemeine Encoder-Decoder-Trainingsmethode vor, die von generativen adversen Netzwerken (GANs) inspiriert ist. Im Gegensatz zu GANs kooperieren unsere neuronalen Encoder- und Decoder-Netzwerke jedoch, um das beste Wasserzeichenschema für die Datenproben zu finden. Anders als bei GANs arbeiten unsere neuronalen Netze jedoch zusammen, um bei gegebenen Datenmustern das beste Wasserzeichenschema zu finden. Mit anderen Worten, wir entwerfen keine neue Wasserzeichenstrategie, sondern unsere beiden vorgeschlagenen neuronalen Netze finden die am besten geeignete Methode selbst.Nach dem Training kann der Decoder in andere neuronale Netze implantiert werden, um diese anzugreifen oder zu schützen (siehe Anhang für Anwendungsfälle und reale Implementierungen). Zu diesem Zweck sollte der Decoder sehr klein sein, um keinen Overhead zu verursachen, wenn er mit anderen neuronalen Netzen verbunden ist, aber gleichzeitig sehr hohe Erfolgsraten bei der Dekodierung bieten, was eine große Herausforderung darstellt.Unsere gemeinsame Trainingsmethode löst das Problem erfolgreich und hält in unseren Experimenten fast 100\% Erfolgsraten bei der Kodierung und Dekodierung für mehrere Datensätze aufrecht, wobei nur sehr wenige Änderungen an den Datenproben vorgenommen werden, um Wasserzeichen zu verstecken.Im Anhang stellen wir außerdem mehrere reale Anwendungsfälle vor.
Wir zeigen, dass unser Schätzer als Rao-Blackwellisierung von drei verschiedenen Schätzern abgeleitet werden kann. Durch die Kombination unseres Schätzers mit REINFORCE erhalten wir einen Policy-Gradienten-Schätzer und reduzieren seine Varianz mit Hilfe einer eingebauten Kontrollvariante, die ohne zusätzliche Modellevaluierungen erhalten wird. Experimente mit einem Spielzeugproblem, einem kategorialen Variational Auto-Encoder und einem strukturierten Vorhersageproblem zeigen, dass unser Schätzer der einzige ist, der sowohl bei hoher als auch bei niedriger Entropie konstant zu den besten Schätzern gehört.
Wir führen ein Schema zur gemeinsamen Nutzung von Parametern ein, bei dem verschiedene Schichten eines Faltungsneuronalen Netzes (CNN) durch eine gelernte Linearkombination von Parametertensoren aus einer globalen Bank von Vorlagen definiert werden.  Die Beschränkung der Anzahl der Templates führt zu einer flexiblen Hybridisierung von traditionellen CNNs und rekurrenten Netzen.  Unser einfaches Schema für die gemeinsame Nutzung von Parametern, das zwar über weiche Gewichte definiert ist, führt in der Praxis häufig zu trainierten Netzen mit einer nahezu strikten rekurrenten Struktur; mit vernachlässigbaren Nebeneffekten werden sie zu Netzen mit tatsächlichen Schleifen. Obwohl wir nur den Aspekt der rekurrenten Verbindungen berücksichtigen, erreichen unsere trainierten Netze eine Genauigkeit, die mit denen konkurriert, die mit modernsten neuronalen Architektur-Suchverfahren (NAS) erstellt wurden.  Insbesondere bei synthetischen Aufgaben, die algorithmischer Natur sind, trainieren unsere hybriden Netze sowohl schneller als auch extrapolieren sie besser auf Testbeispiele außerhalb der Trainingsmenge.
Die Gradientenbeschneidung ist eine weit verbreitete Technik beim Training von tiefen Netzen und wird im Allgemeinen aus einer Optimierungsperspektive heraus motiviert: Informell steuert sie die Dynamik von Iteraten und verbessert so die Konvergenzrate zu einem lokalen Minimum. Diese Intuition wurde in einer Reihe neuerer Arbeiten präzisiert, die zeigen, dass ein geeignetes Clipping zu einer deutlich schnelleren Konvergenz führen kann als der Vanille-Gradientenabstieg. In diesem Papier schlagen wir eine neue Sichtweise für die Untersuchung des Gradientenclippings vor, nämlich die Robustheit: Inoffiziell erwartet man, dass das Clipping Robustheit gegenüber Rauschen bietet, da man keiner einzelnen Probe übermäßig vertraut. Überraschenderweise beweisen wir, dass das Standard-Gradient Clipping für das häufige Problem des Etikettenrauschens bei der Klassifizierung im Allgemeinen keine Robustheit bietet. andererseits zeigen wir, dass eine einfache Variante des Gradient Clipping nachweislich robust ist und einer geeigneten Modifizierung der zugrunde liegenden Verlustfunktion entspricht.
 Unter den tiefen generativen Modellen unterscheiden sich Flow-basierte Modelle, die in dieser Arbeit einfach als Flow-Modelle bezeichnet werden, von anderen Modellen dadurch, dass sie eine nachvollziehbare Likelihood bieten: Sie sind nicht nur eine Bewertungsmetrik für synthetisierte Daten, sondern sollen auch robust gegenüber Out-of-Distribution (OoD)-Eingaben sein, da sie keine Informationen der Eingaben verwerfen. Es wurde jedoch beobachtet, dass Flows, die auf FashionMNIST trainiert wurden, den OoD-Proben aus MNIST höhere Likelihoods zuweisen. Diese kontraintuitive Beobachtung wirft die Frage nach der Robustheit der Likelihood von Flows auf.In diesem Papier untersuchen wir die Korrelation zwischen der Likelihood von Flows und der Bildsemantik: Unsere Experimente zeigen eine überraschend schwache Korrelation zwischen den Wahrscheinlichkeiten von Flüssen und der Bildsemantik: Die Vorhersagewahrscheinlichkeiten von Flüssen können durch triviale Transformationen, die die Bildsemantik unverändert lassen, stark beeinflusst werden, was wir als semantisch-invariante Transformationen (SITs) bezeichnen. Wir untersuchen drei SITs~(alle kleine Modifikationen auf Pixelebene): Bildpixelverschiebung, zufällige Störung durch Rauschen, Nullsetzung latenter Faktoren~(beschränkt auf Flüsse, die eine Mehrskalenarchitektur verwenden, z. B. Glow). Diese Ergebnisse, obwohl kontraintuitiv, stimmen mit der Tatsache überein, dass die Vorhersagewahrscheinlichkeit eines Flusses die gemeinsame Wahrscheinlichkeit aller Bildpixel ist, so dass die Likelihoods von Flüssen, die auf den Intensitäten auf Pixelebene modelliert sind, nicht in der Lage sind, die Existenzwahrscheinlichkeit der hochrangigen Bildsemantik anzuzeigen.
In diesem Beitrag werden Strategien untersucht, die Angriffe auf Bildklassifizierungssysteme durch Transformation der Eingaben vor der Einspeisung in das System abwehren, insbesondere die Anwendung von Bildtransformationen wie Bittiefenreduktion, JPEG-Kompression, Minimierung der Gesamtvarianz und Bildversteppung vor der Einspeisung des Bildes in einen Faltungsnetzwerkklassifizierer. Unsere Experimente mit ImageNet zeigen, dass die totale Varianzminimierung und das Image Quilting in der Praxis sehr effektiv sind, insbesondere wenn das Netzwerk auf transformierten Bildern trainiert wird. 60 % der starken Gray-Box-Angriffe und 90 % der starken Black-Box-Angriffe durch eine Vielzahl von Angriffsmethoden werden durch unsere beste Verteidigung eliminiert.
	Wir beweisen, dass A2BCD linear zu einer Lösung des konvexen Minimierungsproblems konvergiert, und zwar mit der gleichen Geschwindigkeit wie NU_ACDM, solange die maximale Verzögerung nicht zu groß ist.Dies ist der erste asynchrone Nesterov-beschleunigte Algorithmus, der eine nachweisbare Beschleunigung erreicht.Außerdem beweisen wir, dass beide Algorithmen eine optimale Komplexität haben. In Experimenten konnten wir beobachten, dass A2BCD der leistungsstärkste Koordinatenabstiegsalgorithmus ist und auf einigen Datensätzen bis zu 4-5x schneller konvergiert als NU_ACDM, was die Wanduhrzeit angeht.Um unsere Theorie und Beweistechniken zu motivieren, leiten wir auch ein zeitkontinuierliches Analogon unseres Algorithmus ab, analysieren es und beweisen, dass es mit der gleichen Rate konvergiert.
Ein Rahmenwerk für effiziente Bayes'sche Inferenz in probabilistischen Programmen wird durch die Einbettung eines Samplers in eine variantenreiche Posterior-Approximation eingeführt, deren Stärke sowohl in der einfachen Implementierung als auch in der automatischen Abstimmung der Sampler-Parameter liegt, um die Mischzeit zu beschleunigen.Mehrere Strategien zur Annäherung an die Berechnung der unteren Schranke der Evidenz (ELBO) werden vorgestellt, einschließlich einer Umschreibung des ELBO-Ziels. Experimentelle Beweise werden durch die Durchführung von Experimenten mit einer unbedingten VAE für Dichteschätzungsaufgaben, die Lösung eines Einflussdiagramms in einem hochdimensionalen Raum mit einem bedingten Variations-Autoencoder (cVAE) als Deep-Bayes-Klassifikator und Zustandsraummodelle für Zeitreihendaten gezeigt.
Die Punktschätzungen von ReLU-Klassifizierungsnetzen, der wohl am weitesten verbreiteten Architektur neuronaler Netze, haben kürzlich gezeigt, dass sie weit von den Trainingsdaten entfernt ein beliebig hohes Vertrauen haben, Die theoretische Analyse solcher Bayes'schen Approximationen ist begrenzt, auch für ReLU-Klassifikationsnetzwerke. Wir präsentieren eine Analyse von approximativen Gauß'schen Posterior-Verteilungen auf den Gewichten von ReLU-Netzwerken. Wir zeigen, dass selbst eine vereinfachte (und damit billige), nicht-bayesianische Gauß-Verteilung das asymptotische Problem der Overconfidence behebt, und dass die Konfidenz besser kalibriert wird, wenn eine, wenn auch einfache, Bayes-Methode verwendet wird, um die Gauß-Verteilung zu erhalten.Dieses theoretische Ergebnis motiviert eine Reihe von Laplace-Approximationen entlang eines Fidelity-Cost-Trade-Offs.Wir validieren diese Erkenntnisse empirisch anhand von Experimenten mit gängigen tiefen ReLU-Netzwerken.
Wortalignments sind nützlich für Aufgaben wie statistische und neuronale maschinelle Übersetzung (NMT) und Annotationsprojektion.Statistische Wortaligner erbringen gute Leistungen, ebenso wie Methoden, die Alignments gemeinsam mit Übersetzungen in NMT extrahieren.Die meisten Ansätze erfordern jedoch parallele Trainingsdaten, und die Qualität nimmt ab, je weniger Trainingsdaten zur Verfügung stehen.Wir schlagen Wortalignment-Methoden vor, die wenig oder gar keine parallelen Daten benötigen.Die Schlüsselidee ist, mehrsprachige Worteinbettungen - sowohl statisch als auch kontextualisiert - für das Wortalignment zu nutzen. Wir stellen fest, dass traditionelle statistische Aligner von kontextualisierten Einbettungen übertroffen werden - sogar in Szenarien mit reichlich parallelen Daten. Zum Beispiel erreichen kontextualisierte Einbettungen für einen Satz von 100k parallelen Sätzen eine Wortausrichtung F1, die mehr als 5% höher ist (absolut) als eflomal.
Jüngste Studien haben die Anfälligkeit von Reinforcement Learning (RL)-Modellen in verrauschten Umgebungen gezeigt, wobei sich die Quellen des Rauschens von Szenario zu Szenario unterscheiden, z. B. ist der beobachtete Belohnungskanal in der Praxis oft mit Rauschen behaftet (z. B, In diesem Beitrag betrachten wir verrauschte RL-Probleme, bei denen die von RL-Agenten beobachteten Belohnungen mit einer Belohnungskonfusionsmatrix generiert werden. Wir bezeichnen solche beobachteten Belohnungen als gestörte Belohnungen. Wir entwickeln einen unverzerrten Belohnungsschätzer mit Hilfe eines robusten RL-Rahmens, der es RL-Agenten ermöglicht, in verrauschten Umgebungen zu lernen, während sie nur gestörte Belohnungen beobachten. Wir beweisen die Konvergenz und Stichprobenkomplexität unseres Ansatzes.Umfangreiche Experimente auf verschiedenen DRL-Plattformen zeigen, dass Politiken, die auf unseren geschätzten Surrogat-Belohnungen basieren, höhere erwartete Belohnungen erzielen können und schneller konvergieren als existierende Basislinien.Zum Beispiel ist der State-of-the-Art PPO-Algorithmus in der Lage, 67,5% und 46,7% Verbesserungen im Durchschnitt auf fünf Atari-Spiele zu erzielen, wenn die Fehlerraten 10% bzw. 30% betragen.
Das Training rekurrenter neuronaler Netze (RNNs) auf langen Sequenzen unter Verwendung von Backpropagation through Time (BPTT) bleibt eine grundlegende Herausforderung. Es hat sich gezeigt, dass das Hinzufügen eines lokalen, nicht überwachten Verlustterms in das Optimierungsziel das Training von RNNs auf langen Sequenzen effektiver macht. Während die Bedeutung einer nicht überwachten Aufgabe im Prinzip durch einen Koeffizienten in der Zielfunktion kontrolliert werden kann, beeinflussen die Gradienten in Bezug auf den nicht überwachten Verlustterm immer noch alle versteckten Zustandsdimensionen, was dazu führen kann, dass wichtige Informationen über die überwachte Aufgabe verschlechtert oder gelöscht werden. Im Vergleich zu bestehenden halbüberwachten Sequenzlernmethoden konzentriert sich dieses Papier auf einen traditionell übersehenen Mechanismus - eine Architektur mit explizit entworfenen privaten und gemeinsamen versteckten Einheiten, die den schädlichen Einfluss des zusätzlichen unbeaufsichtigten Verlustes auf die überwachte Hauptaufgabe abschwächen soll. Wir erreichen dies, indem wir den versteckten Raum des RNN in einen privaten Raum für die überwachte Aufgabe und einen gemeinsamen Raum für die überwachte und die nicht überwachte Aufgabe aufteilen und präsentieren umfangreiche Experimente mit dem vorgeschlagenen Rahmenwerk auf mehreren langen Sequenzmodellierungs-Benchmark-Datensätzen.
Um die Übertragung von Aufgabeninformationen besser zu verstehen, untersuchen wir eine Architektur mit einem gemeinsamen Modul für alle Aufgaben und einem separaten Ausgabemodul für jede Aufgabe.Wir untersuchen die Theorie dieser Einstellung auf lineare und ReLU-aktivierte Modelle.Unsere wichtigste Beobachtung ist, dass, ob die Daten der Aufgaben gut ausgerichtet sind oder nicht, die Leistung des Multi-Task-Lernens erheblich beeinflussen kann.Wir zeigen, dass eine falsche Ausrichtung zwischen den Aufgabendaten zu einer negativen Übertragung führen kann (oder die Leistung beeinträchtigt) und bieten ausreichende Bedingungen für eine positive Übertragung. Inspiriert von den theoretischen Erkenntnissen zeigen wir, dass das Ausrichten der Einbettungsschichten von Aufgaben zu Leistungssteigerungen beim Multi-Task-Training und Transfer-Lernen auf dem GLUE-Benchmark und bei Sentiment-Analyse-Aufgaben führt; zum Beispiel erzielten wir eine durchschnittliche Verbesserung der GLUE-Punktzahl um 2,35% bei 5 GLUE-Aufgaben im Vergleich zu BERT LARGE mit unserer Ausrichtungsmethode.Wir entwerfen auch ein SVD-basiertes Schema zur Neugewichtung von Aufgaben und zeigen, dass es die Robustheit des Multi-Task-Trainings auf einem Multi-Label-Bilddatensatz verbessert.
Wir überprüfen drei Einschränkungen von BLEU und ROUGE - den beliebtesten Metriken zur Bewertung von Referenzzusammenfassungen im Vergleich zu Hypothesenzusammenfassungen -, entwickeln Kriterien dafür, wie sich eine gute Metrik verhalten sollte, schlagen konkrete Möglichkeiten zur detaillierten Bewertung der Leistung einer Metrik vor und zeigen das Potenzial von Transformers-basierten Sprachmodellen zur Bewertung von Referenzzusammenfassungen im Vergleich zu Hypothesenzusammenfassungen.
Meta-Learning hat sich in der Bildverarbeitung bewährt, wo Muster auf niedriger Ebene auf verschiedene Lernaufgaben übertragbar sind. Die direkte Anwendung dieses Ansatzes auf Text ist jedoch eine Herausforderung - lexikalische Merkmale, die für eine Aufgabe sehr informativ sind, können für eine andere unbedeutend sein. Unser Modell wird in einem Meta-Lernsystem trainiert, um diese Signaturen in Aufmerksamkeitsbewertungen umzuwandeln, die dann verwendet werden, um die lexikalischen Repräsentationen von Wörtern zu gewichten.Wir zeigen, dass unser Modell durchweg besser abschneidet als prototypische Netzwerke, die auf der Grundlage von lexikalischem Wissen gelernt wurden (Snell et al, 2017) sowohl bei der Klassifizierung von Texten mit wenigen Schüssen als auch bei der Klassifizierung von Beziehungen in sechs Benchmark-Datensätzen deutlich übertrifft (durchschnittlich 19,96 % bei der 1-Schuss-Klassifizierung).
Die Beschreibung neuronaler Berechnungen im Bereich der Neurowissenschaften stützt sich auf zwei konkurrierende Sichtweisen:(i) eine klassische Einzelzellsichtweise, die die Aktivität einzelner Neuronen mit sensorischen oder Verhaltensvariablen in Beziehung setzt und sich darauf konzentriert, wie verschiedene Zellklassen auf Berechnungen abgebildet werden;(ii) eine neuere Populationssichtweise, die stattdessen Berechnungen in Form kollektiver neuronaler Trajektorien charakterisiert und sich auf die Dimensionalität dieser Trajektorien konzentriert, wenn Tiere Aufgaben ausführen. Wie die beiden Schlüsselkonzepte der Zellklassen und der niedrigdimensionalen Trajektorien interagieren, um neuronale Berechnungen zu gestalten, ist jedoch derzeit nicht verstanden. Hier gehen wir diese Frage an, indem wir Werkzeuge des maschinellen Lernens für das Training von RNNs mit Reverse-Engineering und theoretischen Analysen der Netzwerkdynamik kombinieren. In diesen Netzwerken steuert der Rang der Konnektivität die Dimensionalität der Dynamik, während die Anzahl der Komponenten in der Gauß'schen Mischung der Anzahl der Zellklassen entspricht. Mit Hilfe von Backpropagation bestimmen wir den minimalen Rang und die Anzahl der Zellklassen, die für die Umsetzung von neurowissenschaftlichen Aufgaben mit zunehmender Komplexität erforderlich sind. Wir zeigen, dass der Rang den Phasenraum bestimmt, der für Dynamiken zur Verfügung steht, die Input-Output-Mappings implementieren, während mehrere Zellklassen es den Netzwerken ermöglichen, flexibel zwischen verschiedenen Arten von Dynamiken im verfügbaren Phasenraum zu wechseln.Unsere Ergebnisse haben Auswirkungen auf die Analyse neurowissenschaftlicher Experimente und die Entwicklung erklärbarer KI.
Wir schlagen den Fusionsdiskriminator vor, einen einheitlichen Rahmen für die Einbeziehung bedingter Informationen in ein generatives adversariales Netzwerk (GAN) für eine Vielzahl verschiedener strukturierter Vorhersageaufgaben, einschließlich Bildsynthese, semantischer Segmentierung und Tiefenschätzung. Ähnlich wie die häufig verwendeten CNN-CRF-Modelle (Convolutional Neural Network - Conditional Markov Random Field) ist die vorgeschlagene Methode in der Lage, Konsistenz höherer Ordnung im Modell zu erzwingen, ohne jedoch auf eine sehr spezifische Klasse von Potenzialen beschränkt zu sein.
Modellbasiertes Verstärkungslernen (MBRL) hat sich als leistungsfähiger Rahmen für das dateneffiziente Erlernen der Kontrolle über kontinuierliche Aufgaben erwiesen. Neuere Arbeiten im Bereich MBRL haben sich hauptsächlich auf die Verwendung fortgeschrittener Funktionsapproximatoren und Planungsschemata konzentriert, wobei der allgemeine Rahmen seit seiner Konzeption praktisch unverändert blieb. Im Kontext von MBRL charakterisieren wir die objektive Diskrepanz zwischen dem Training des Vorwärtsdynamikmodells in Bezug auf die Wahrscheinlichkeit der Ein-Schritt-Vorhersage und dem Gesamtziel der Verbesserung der Leistung bei einer nachgelagerten Steuerungsaufgabe. Dieses Problem kann sich beispielsweise aus der Erkenntnis ergeben, dass Dynamikmodelle, die für eine bestimmte Aufgabe effektiv sind, nicht unbedingt global genau sein müssen, und dass umgekehrt global genaue Modelle möglicherweise lokal nicht genau genug sind, um eine gute Regelungsleistung bei einer bestimmten Aufgabe zu erzielen.In unseren Experimenten untersuchen wir diese objektive Diskrepanz und zeigen, dass die Wahrscheinlichkeit der Vorhersage eines Schrittes nicht immer mit der nachgelagerten Regelungsleistung korreliert. Wir schlagen eine erste Methode vor, um das Mismatch-Problem durch eine Neugewichtung des dynamischen Modelltrainings zu entschärfen, und schließen darauf aufbauend mit einer Diskussion über andere mögliche zukünftige Forschungsrichtungen zur Lösung dieses Problems.
In letzter Zeit gab es eine hitzige Debatte (z. B. Schwartz-Ziv & Tishby (2017), Saxe et al. (2018), Noshad et al. (2018), Goldfeld et al. (2018)) über die Messung des Informationsflusses in tiefen neuronalen Netzen mit Hilfe von Techniken aus der Informationstheorie. Es wird behauptet, dass Tiefe Neuronale Netze im Allgemeinen gute Verallgemeinerungsfähigkeiten haben, da sie nicht nur lernen, wie sie von einer Eingabe auf eine Ausgabe abbilden, sondern auch, wie sie Informationen über die eingegebenen Trainingsdaten komprimieren können (Schwartz-Ziv & Tishby, 2017), d. h. sie abstrahieren die Eingabeinformationen und entfernen alle unnötigen oder zu spezifischen Informationen. Wenn dies der Fall ist, könnte die Methode der Nachrichtenkompression, Information Bottleneck (IB), als natürlicher Komparator für die Netzwerkleistung verwendet werden, da diese Methode eine optimale Grenze für die Informationskompression liefert.Diese Behauptung wurde später sowohl widerlegt als auch bestätigt (z.B. Saxe et al. (2018), Achille et al. (2017), Noshad et al. (2018)), da die angewandte Methode der Messung der gegenseitigen Information eigentlich nicht die Information, sondern die Clusterung der internen Schichtrepräsentationen misst (Goldfeld et al. (2018)).In diesem Beitrag werden wir eine detaillierte Erklärung der Entwicklung in der Informationsebene (IP) präsentieren, die ein Plot-Typ ist, der die gegenseitige Information vergleicht, um die Kompression zu beurteilen (Schwartz-Ziv & Tishby (2017)), wenn Rauschen rückwirkend hinzugefügt wird (unter Verwendung der Binning-Schätzung).  Wir erklären auch, warum verschiedene Aktivierungsfunktionen unterschiedliche Trajektorien auf der IP zeigen. Außerdem haben wir die Wirkung von Clustering auf den Netzwerkverlust durch frühes und perfektes Stoppen unter Verwendung der Informationsebene untersucht und wie Clustering zur Unterstützung des Netzwerkprunings verwendet werden kann.
Das formale Verständnis des induktiven Bias hinter tiefen Faltungsnetzwerken, d.h. der Beziehung zwischen den architektonischen Merkmalen des Netzwerks und den Funktionen, die es modellieren kann, ist begrenzt.In dieser Arbeit stellen wir eine grundlegende Verbindung zwischen den Bereichen Quantenphysik und Deep Learning her und nutzen sie, um neue theoretische Beobachtungen bezüglich des induktiven Bias von Faltungsnetzwerken zu machen. Insbesondere zeigen wir eine strukturelle Äquivalenz zwischen der Funktion, die durch eine arithmetische Faltungsschaltung (ConvAC) realisiert wird, und einer Quanten-Vielteilchen-Wellenfunktion, die die Verwendung von Quantenverschränkungsmaßen als Quantifizierer der Ausdrucksfähigkeit eines tiefen Netzwerks zur Modellierung von Korrelationen erleichtert. Dies ermöglicht uns eine graphentheoretische Analyse eines Faltungsnetzwerks, indem wir seine Ausdruckskraft an einen Minimalschnitt in dem zugrunde liegenden Graphen binden und ein praktisches Ergebnis in Form einer direkten Kontrolle über den induktiven Bias durch die Anzahl der Kanäle (Breite) jeder Schicht demonstrieren. Die Beschreibung eines tiefen Faltungsnetzes mit wohldefinierten graphentheoretischen Werkzeugen und die strukturelle Verbindung zur Quantenverschränkung sind zwei interdisziplinäre Brücken, die durch diese Arbeit geschlagen werden.
Deep-Learning-Algorithmen werden zunehmend bei der Modellierung chemischer Prozesse eingesetzt, aber Black-Box-Vorhersagen ohne Rationale sind in praktischen Anwendungen, wie z.B. dem Design von Medikamenten, nur begrenzt anwendbar.Zu diesem Zweck lernen wir, molekulare Substrukturen - Rationale - zu identifizieren, die mit der angestrebten chemischen Eigenschaft (z.B. Toxizität) verbunden sind, Wir formulieren dieses Problem als ein Verstärkungslernproblem über den molekularen Graphen, parametrisiert durch zwei Faltungsnetzwerke, die der Auswahl der Rationale und der darauf basierenden Vorhersage entsprechen, wobei letztere die Belohnungsfunktion induziert. Wir zeigen, dass unser Modell unter der zusätzlichen Bedingung, dass die Vorhersagen strikt den Rationalen folgen, eine hohe Leistung beibehält, und validieren die extrahierten Rationalen durch Vergleich mit denen, die in der chemischen Literatur beschrieben sind, und durch synthetische Experimente.
In den letzten Jahren wurden beträchtliche Fortschritte auf dem Gebiet der Graphenfaltungsnetzwerke (GCN) gemacht.In diesem Papier analysieren wir zum ersten Mal theoretisch die Verbindungen zwischen GCN und Matrixfaktorisierung (MF) und vereinheitlichen GCN als Matrixfaktorisierung mit Co-Training und Unitisierung.Darüber hinaus schlagen wir unter der Leitung dieser theoretischen Analyse ein alternatives Modell zu GCN vor, das wir Co-Training und Unitisierte Matrixfaktorisierung (CUMF) nennen.Die Richtigkeit unserer Analyse wird durch gründliche Experimente überprüft. Die experimentellen Ergebnisse zeigen, dass CUMF erreicht ähnliche oder überlegene Leistungen im Vergleich zu GCN.Darüber hinaus erbt CUMF die Vorteile der MF-basierten Methoden zu unterstützen natürlich Konstruieren Mini-Batches, und ist freundlicher zu verteilten Computing im Vergleich zu GCN.Die verteilte CUMF auf semi-supervised Knoten Klassifizierung deutlich übertrifft verteilte GCN Methoden.So, CUMF stark profitiert großen Maßstab und komplexe reale Anwendungen.
Das Problem ist anspruchsvoll, da es nicht nur die Erzeugung von chemisch gültigen molekularen Strukturen erfordert, sondern auch die Optimierung ihrer chemischen Eigenschaften in der Zwischenzeit.Inspiriert von den jüngsten Fortschritten in der tiefen generativen Modelle, in diesem Papier schlagen wir eine Flow-basierte autoregressive Modell für Graphen Generation genannt GraphAF.GraphAF kombiniert die Vorteile der beiden autoregressive und Flow-basierte Ansätze und genießt: (1) hohe Modellflexibilität für die Schätzung der Datendichte; (2) effiziente parallele Berechnung für das Training; (3) ein iterativer Sampling-Prozess, der es ermöglicht, chemisches Domänenwissen für die Wertigkeitsprüfung zu nutzen.Experimentelle Ergebnisse zeigen, dass GraphAF in der Lage ist, 68\% chemisch gültige Moleküle auch ohne chemische Wissensregeln und 100\% gültige Moleküle mit chemischen Regeln zu erzeugen. Nach der Feinabstimmung des Modells für die zielgerichtete Eigenschaftsoptimierung mit Hilfe von Reinforcement Learning erreicht GraphAF sowohl bei der Optimierung chemischer Eigenschaften als auch bei der Optimierung eingeschränkter Eigenschaften die beste Leistung.
Wir untersuchen zwei Arten von Vorkonditionierern und vorkonditionierten Methoden des stochastischen Gradientenabstiegs (SGD) in einem einheitlichen Rahmen, von denen wir den ersten aufgrund seiner engen Beziehung zur Newton-Methode als Newton-Typ und den zweiten als Fisher-Typ bezeichnen, da sein Vorkonditionierer eng mit der Umkehrung der Fisher-Informationsmatrix verwandt ist.Beide Vorkonditionierer können aus einem Rahmen abgeleitet und effizient für beliebige Matrix-Lie-Gruppen geschätzt werden, die vom Benutzer mit natürlichem oder relativem Gradientenabstieg bestimmt werden, wobei bestimmte Kriterien für die Schätzung des Vorkonditionierers minimiert werden.Viele bestehende Vorkonditionierer und Methoden, z.B, RMSProp, Adam, KFAC, äquilibrierte SGD, Batch-Normalisierung usw., sind Spezialfälle des Newton-Typs oder des Fisher-Typs oder eng mit ihnen verwandt.Experimentelle Ergebnisse zu relativ großen Problemen des maschinellen Lernens werden für die Leistungsstudie berichtet.
EDA besteht aus vier einfachen, aber leistungsstarken Operationen: Synonymer Ersatz, zufällige Einfügung, zufällige Vertauschung und zufällige Löschung. Bei fünf Textklassifizierungsaufgaben zeigen wir, dass EDA die Leistung sowohl für faltbare als auch für rekurrente neuronale Netze verbessert. EDA zeigt besonders starke Ergebnisse bei kleineren Datensätzen; im Durchschnitt über fünf Datensätze hinweg erreichte das Training mit EDA bei Verwendung von nur 50 % des verfügbaren Trainingssatzes die gleiche Genauigkeit wie das normale Training mit allen verfügbaren Daten.Wir haben auch umfangreiche Ablationsstudien durchgeführt und schlagen Parameter für den praktischen Einsatz vor.
Wir schlagen ein Modell vor, das in der Lage ist, physikalische Parameterschätzungen von Systemen aus Videos durchzuführen, bei denen die Differentialgleichungen, die die Dynamik der Szene regeln, bekannt sind, aber beschriftete Zustände oder Objekte nicht verfügbar sind.Bestehende physikalische Methoden zum Verstehen von Szenen erfordern entweder die Überwachung von Objektzuständen oder integrieren sich nicht mit differenzierbarer Physik, um interpretierbare Systemparameter und -zustände zu erlernen.Wir lösen dieses Problem durch einen Ansatz, der Vision-as-inverse-Graphics und differenzierbare Physik-Engines zusammenbringt, bei dem Objekte und explizite Zustands- und Geschwindigkeitsrepräsentationen durch das Modell entdeckt werden. Unser Ansatz ist verwandten unüberwachten Methoden bei der langfristigen Vorhersage zukünftiger Bilder von Systemen mit interagierenden Objekten (wie z. B. Kugel-Feder- oder Dreikörper-Gravitationssystemen) deutlich überlegen, da er in der Lage ist, die Dynamik als induktive Vorspannung in das Modell zu integrieren. Wir zeigen den Wert dieser engen Integration von Bildverarbeitung und Physik, indem wir das dateneffiziente Lernen einer bildverarbeitungsgesteuerten, modellbasierten Steuerung für ein Pendelsystem demonstrieren, und wir zeigen, dass die Interpretierbarkeit des Controllers einzigartige Fähigkeiten in der zielgerichteten Steuerung und der physikalischen Argumentation für eine datenfreie Anpassung bietet.
Anstatt die synthetischen Proben direkt zu annotieren, sucht ASAL ähnliche Proben aus dem Pool und bezieht sie in das Training mit ein, wodurch die Qualität der neuen Proben hoch und die Annotationen zuverlässig sind.  ASAL eignet sich besonders für große Datensätze, da es eine bessere Laufzeitkomplexität (sublinear) für die Stichprobenauswahl erreicht als herkömmliche Unsicherheitsstichproben (linear).Wir stellen eine umfassende Reihe von Experimenten an zwei Datensätzen vor und zeigen, dass ASAL ähnliche Methoden übertrifft und die etablierte Baseline (Zufallsstichproben) deutlich übertrifft.  Im Diskussionsteil analysieren wir, in welchen Situationen ASAL am besten abschneidet und warum es manchmal schwierig ist, die Zufallsstichprobenauswahl zu übertreffen. Unseres Wissens nach ist dies die erste adversarische aktive Lerntechnik, die für Mehrklassenprobleme mit tiefen Faltungs-Klassifikatoren angewendet wird und eine bessere Leistung als die Zufallsstichprobenauswahl zeigt.
Jedes Mal, wenn ein Modell trainiert wird, erhält man ein anderes Ergebnis aufgrund zufälliger Faktoren im Trainingsprozess, zu denen zufällige Parameterinitialisierung und zufällige Datenmischung gehören. Die Berichterstattung über die beste Einzelmodellleistung geht nicht angemessen auf diese Stochastizität ein.
Ein beliebter Ansatz für dieses Problem ist das Erlernen von domäneninvarianten Einbettungen für beide Domänen. In dieser Arbeit untersuchen wir theoretisch und empirisch die Auswirkung der Komplexität der Einbettung auf die Generalisierung auf die Zieldomäne. Als nächstes spezifizieren wir unseren theoretischen Rahmen auf mehrschichtige neuronale Netze und entwickeln eine Strategie, die die Empfindlichkeit gegenüber der Einbettungskomplexität abschwächt und empirisch eine Leistung erreicht, die mit dem besten schichtabhängigen Komplexitätskompromiss vergleichbar oder besser ist.
Wir schlagen eine neue Architektur vor, die als Dual Adversarial Transfer Network (DATNet) bezeichnet wird, um die ressourcenarme Named Entity Recognition (NER) zu adressieren, und zwar in zwei Varianten von DATNet, d.h., Um den verrauschten und unausgewogenen Trainingsdaten gerecht zu werden, schlagen wir einen neuartigen Generalized Resource-Adversarial Discriminator (GRAD) vor, der die Modellgeneralisierung durch adversariales Training verbessert. Wir untersuchen die Auswirkungen der verschiedenen Komponenten in DATNet über Domänen und Sprachen hinweg und zeigen, dass eine signifikante Verbesserung vor allem für Daten mit geringen Ressourcen erzielt werden kann. Ohne zusätzliche handgefertigte Merkmale zu ergänzen, erreichen wir neue Spitzenleistungen bei CoNLL und Twitter NER - 88,16% F1 für Spanisch, 53,43% F1 für WNUT-2016 und 42,83% F1 für WNUT-2017.
Obwohl vor kurzem gezeigt wurde, dass das GAN-Training konvergiert, enden GAN-Modelle oft in lokalen Nash-Gleichgewichten, die mit einem Mode-Kollaps verbunden sind oder die Zielverteilung nicht modellieren.Wir stellen Coulomb-GANs vor, die das GAN-Lernproblem als ein Potentialfeld darstellen, in dem die generierten Proben von den Proben der Trainingsmenge angezogen werden, sich aber gegenseitig abstoßen. Der Diskriminator lernt ein Potentialfeld, während der Generator die Energie verringert, indem er seine Proben entlang des Vektor-(Kraft-)Feldes bewegt, das durch den Gradienten des Potentialfeldes bestimmt wird. Wir beweisen, dass Coulomb-GANs nur ein Nash-Gleichgewicht besitzen, das optimal in dem Sinne ist, dass die Modellverteilung gleich der Zielverteilung ist, und wir zeigen die Wirksamkeit von Coulomb-GANs bei LSUN-Schlafzimmern, CelebA-Gesichtern, CIFAR-10 und der Google Billion Word Textgenerierung.
Die Vorhersage von Eigenschaften von Knoten in einem Graphen ist ein wichtiges Problem mit Anwendungen in einer Vielzahl von Bereichen. Graphenbasierte Semi Supervised Learning (SSL)-Methoden zielen darauf ab, dieses Problem zu lösen, indem sie eine kleine Teilmenge der Knoten als Seeds beschriften und dann die Graphenstruktur nutzen, um Label-Scores für den Rest der Knoten im Graphen vorherzusagen. Kürzlich haben Graph Convolutional Networks (GCNs) eine beeindruckende Leistung bei der graphbasierten SSL-Aufgabe erreicht.Zusätzlich zu den Label-Scores ist es auch wünschenswert, eine Konfidenz-Score mit ihnen verbunden zu haben.Leider ist die Konfidenzschätzung im Kontext von GCN bisher nicht erforscht worden. Wir füllen diese wichtige Lücke in diesem Papier und schlagen ConfGCN vor, das Label-Scores zusammen mit ihren Konfidenzwerten gemeinsam in GCN-basierten Einstellungen schätzt.ConfGCN verwendet diese geschätzten Konfidenzwerte, um den Einfluss eines Knotens auf einen anderen während der Nachbarschaftsaggregation zu bestimmen und dadurch anisotrope Fähigkeiten zu erlangen.Durch umfangreiche Analysen und Experimente mit Standard-Benchmarks finden wir heraus, dass ConfGCN in der Lage ist, den Stand der Technik-Baselines deutlich zu übertreffen.Wir haben den Quellcode von ConfGCN verfügbar gemacht, um reproduzierbare Forschung zu fördern.
Unser erster Benchmark, ImageNet-C, standardisiert und erweitert das Thema Korruptionsrobustheit und zeigt, welche Klassifikatoren in sicherheitskritischen Anwendungen zu bevorzugen sind, und wir schlagen einen neuen Datensatz namens ImageNet-P vor, der es Forschern ermöglicht, die Robustheit eines Klassifikators gegenüber gängigen Störungen zu bewerten. Wir stellen fest, dass es vernachlässigbare Änderungen in der relativen Robustheit von AlexNet-Klassifizierern gegenüber ResNet-Klassifizierern gibt, und wir entdecken Möglichkeiten, die Robustheit gegenüber Korruption und Störungen zu verbessern.
Das Papier erforscht eine neuartige Methodik in der Quellcode-Verschleierung durch die Anwendung von textbasierten rekurrenten neuronalen Netzwerken (RNN) Encoder-Decoder-Modellen in der Chiffriertext-Generierung und Schlüsselgenerierung. Quantitative Benchmark-Vergleiche mit bestehenden Obfuskationsmethoden deuten auf eine signifikante Verbesserung der Tarnung und der Ausführungskosten für die vorgeschlagene Lösung hin, und Experimente zu den Eigenschaften des Modells ergaben positive Ergebnisse hinsichtlich der Zeichenvariation, der Unähnlichkeit mit der ursprünglichen Codebasis und der konsistenten Länge des obfuskierten Codes.
Wir zeigen, dass GAN eine gute High-Level-Darstellung von Zieldaten hat, die leicht auf semantische Segmentierungsmasken projiziert werden kann.Diese Methode kann verwendet werden, um einen Trainingsdatensatz für das Unterrichten eines separaten semantischen Segmentierungsnetzwerks zu erstellen.Unsere Experimente zeigen, dass ein solches Segmentierungsnetzwerk erfolgreich auf realen Daten verallgemeinert.Darüber hinaus übertrifft die Methode das überwachte Training, wenn die Anzahl der Trainingsproben klein ist, und funktioniert auf einer Vielzahl von verschiedenen Szenen und Klassen.Der Quellcode der vorgeschlagenen Methode wird öffentlich zugänglich sein.
Vision-Language Navigation (VLN) ist eine Aufgabe, bei der ein Agent angewiesen wird, in fotorealistischen, unbekannten Umgebungen mit Anweisungen in natürlicher Sprache zu navigieren.Bisherige Forschung zu VLN wurde hauptsächlich auf dem Room-to-Room (R2R) Datensatz mit ausschließlich englischen Anweisungen durchgeführt.  Das ultimative Ziel von VLN ist es jedoch, Menschen zu bedienen, die beliebige Sprachen sprechen.Um mehrsprachiges VLN mit zahlreichen Sprachen zu ermöglichen, sammeln wir einen sprachübergreifenden R2R-Datensatz, der den ursprünglichen Benchmark um entsprechende chinesische Anweisungen erweitert.Es ist jedoch zeitaufwendig und teuer, umfangreiche menschliche Anweisungen für jede existierende Sprache zu sammeln.Auf der Grundlage des neu eingeführten Datensatzes schlagen wir ein allgemeines sprachübergreifendes VLN-Framework vor, um eine anweisungsgeleitete Navigation für verschiedene Sprachen zu ermöglichen.Wir untersuchen zunächst die Möglichkeit, einen sprachübergreifenden Agenten zu bauen, wenn keine Trainingsdaten der Zielsprache verfügbar sind. Der sprachübergreifende Agent ist mit einem Meta-Learner ausgestattet, um sprachübergreifende Repräsentationen zu aggregieren und ein visuell geerdetes sprachübergreifendes Ausrichtungsmodul, um textuelle Repräsentationen verschiedener Sprachen auszurichten.Unter dem Zero-Shot-Lernszenario zeigt unser Modell konkurrenzfähige Ergebnisse, sogar im Vergleich zu einem Modell, das mit allen zielsprachlichen Anweisungen trainiert wurde.Darüber hinaus führen wir einen adversen Domänenanpassungsverlust ein, um die Übertragungsfähigkeit unseres Modells zu verbessern, wenn eine bestimmte Menge an zielsprachlichen Daten gegeben ist.Unsere Methoden und Daten zeigen das Potenzial des Aufbaus eines sprachübergreifenden Agenten, um Sprecher mit verschiedenen Sprachen zu bedienen.
Tiefe generative Modelle haben den Stand der Technik in der semi-supervised Klassifikation, aber ihre Fähigkeit zur Ableitung nützlicher diskriminierende Merkmale in einer völlig unbeaufsichtigten Art und Weise für die Klassifizierung in schwierigen realen Datensätzen, wo eine angemessene Mannigfaltigkeit Trennung erforderlich ist nicht ausreichend erforscht worden.Die meisten Methoden auf die Definition einer Pipeline der Ableitung von Merkmalen über generative Modellierung und dann die Anwendung Clustering-Algorithmen, die Trennung der Modellierung und diskriminierende Prozesse. Wir schlagen ein tiefes hierarchisches generatives Modell vor, das eine Mischung aus diskreten und kontinuierlichen Verteilungen verwendet, um zu lernen, die verschiedenen Datenmanifalten effektiv zu trennen, und das durchgängig trainierbar ist. Wir zeigen, dass wir durch die Spezifizierung der Form der diskreten Variablenverteilung den latenten Repräsentationen des Modells eine spezifische Struktur auferlegen und testen die Unterscheidungsleistung unseres Modells bei der Aufgabe der CLL-Diagnose im Vergleich zu Baselines aus dem Bereich der computergestützten FC sowie der Variational Autoencoder-Literatur.
Repräsentation Lernen über Graphen strukturierte Daten hat erhebliche Aufmerksamkeit vor kurzem aufgrund seiner allgegenwärtigen applicability.However, die meisten Fortschritte wurden in statischen Graphen Einstellungen gemacht worden, während die Bemühungen für das gemeinsame Lernen dynamisch des Graphen und dynamisch auf dem Graphen sind noch in einem frühen Stadium.Two grundlegende Fragen stellen sich beim Lernen über dynamische Graphen: (i) Wie elegant Modell dynamischen Prozessen über Graphen? (Wir stellen DyRep vor - einen neuartigen Modellierungsrahmen für dynamische Graphen, der das Lernen von Repräsentationen als einen latenten Vermittlungsprozess darstellt, der zwei beobachtete Prozesse überbrückt, nämlich die Dynamik des Netzwerks (realisiert als topologische Evolution) und die Dynamik im Netzwerk (realisiert als Aktivitäten zwischen Knoten). Konkret schlagen wir ein tiefes zeitliches Punktprozessmodell mit zwei Zeitskalen vor, das die verschachtelte Dynamik der beobachteten Prozesse erfasst. Dieses Modell wird durch ein zeitlich-aufmerksames Repräsentationsnetzwerk parametrisiert, das zeitlich sich entwickelnde strukturelle Informationen in Knotenrepräsentationen kodiert, die wiederum die nichtlineare Entwicklung der beobachteten Graphendynamik antreiben. Wir zeigen, dass DyRep bei der Vorhersage von dynamischen Verbindungen und bei der Zeitvorhersage besser abschneidet als herkömmliche Verfahren und präsentieren umfangreiche qualitative Einblicke in unser Verfahren.
Mit dem Erfolg des modernen maschinellen Lernens wird es immer wichtiger zu verstehen und zu kontrollieren, wie Lernalgorithmen interagieren.Leider zeigen negative Ergebnisse aus der Spieltheorie, dass es wenig Hoffnung gibt, allgemeine n-Spieler-Spiele zu verstehen oder zu kontrollieren.Wir stellen daher glatte Märkte (SM-Spiele) vor, eine Klasse von n-Spieler-Spielen mit paarweisen Nullsummen-Interaktionen.SM-Spiele kodieren ein gemeinsames Design-Muster im maschinellen Lernen, das einige GANs, adversariales Training und andere neuere Algorithmen einschließt.Wir zeigen, dass SM-Spiele für die Analyse und Optimierung mit Methoden erster Ordnung zugänglich sind.
Während Zählmaschinen in der theoretischen Informatik seit den 1960er Jahren wenig Beachtung fanden, haben sie in jüngster Zeit eine neue Relevanz im Bereich der Verarbeitung natürlicher Sprache (NLP) erlangt.Neuere Arbeiten deuten darauf hin, dass einige leistungsstarke rekurrente neuronale Netze ihren Speicher als Zähler nutzen.Ein möglicher Weg, den Erfolg dieser Netze zu verstehen, besteht also darin, die Theorie der Zählerberechnung zu überdenken.Daher untersuchen wir die Fähigkeiten von Echtzeit-Zählmaschinen als formale Grammatiken.Wir zeigen zunächst, dass mehrere Varianten der Zählmaschine konvergieren, um die gleiche Klasse formaler Sprachen auszudrücken. Als nächstes zeigen wir, dass Zählmaschinen keine booleschen Ausdrücke auswerten können, obwohl sie deren Syntax schwach validieren können, was Auswirkungen auf die Interpretierbarkeit und Evaluierung von neuronalen Netzwerken hat: Die erfolgreiche Anpassung syntaktischer Muster garantiert nicht, dass ein Zählmaschinen-ähnliches Modell die zugrunde liegenden semantischen Strukturen korrekt repräsentiert, und schließlich gehen wir der Frage nach, ob Zählsprachen semilinear sind.
Aufmerksamkeitsgesteuerte, RNN-basierte Encoder-Decoder-Modelle für abstrakte Zusammenfassungen haben bei kurzen Eingabe- und Ausgabesequenzen eine gute Leistung erzielt. Bei längeren Dokumenten und Zusammenfassungen enthalten diese Modelle jedoch häufig sich wiederholende und inkohärente Phrasen.Wir stellen ein neuronales Netzmodell mit einer neuartigen Intra-Attention vor, das die Eingabe und die kontinuierlich erzeugte Ausgabe separat überwacht, sowie eine neue Trainingsmethode, die standardmäßige überwachte Wortvorhersage und Reinforcement Learning (RL) kombiniert. Wenn jedoch die Standard-Wortvorhersage mit der globalen Sequenzvorhersage des RL-Trainings kombiniert wird, werden die resultierenden Zusammenfassungen besser lesbar.Wir evaluieren dieses Modell auf den CNN/Daily Mail- und New York Times-Datensätzen.Unser Modell erreicht einen ROUGE-1-Score von 41,16 auf dem CNN/Daily Mail-Datensatz, eine Verbesserung gegenüber früheren State-of-the-Art-Modellen.Die menschliche Bewertung zeigt auch, dass unser Modell qualitativ hochwertigere Zusammenfassungen produziert.
Knowledge Distillation (KD) ist eine gängige Methode, um das von einem maschinellen Lernmodell (dem Lehrer) gelernte ``Wissen'' auf ein anderes Modell (den Schüler) zu übertragen, wobei der Lehrer in der Regel über eine größere Kapazität verfügt (z.B, Unseres Wissens nach übersehen die bestehenden Methoden die Tatsache, dass, obwohl der Schüler zusätzliches Wissen vom Lehrer absorbiert, beide Modelle dieselben Eingabedaten verwenden - und diese Daten sind das einzige Medium, mit dem das Wissen des Lehrers demonstriert werden kann. Auf der anderen Seite kann ein menschlicher Lehrer ein Stück Wissen mit individualisierten Beispielen demonstrieren, die an einen bestimmten Schüler angepasst sind, z.B. in Bezug auf seinen kulturellen Hintergrund und seine Interessen.Inspiriert von diesem Verhalten, entwerfen wir Datenerweiterungsagenten mit unterschiedlichen Rollen, um die Wissensdestillation zu erleichtern. Wir konzentrieren uns speziell auf KD, wenn das Netzwerk des Lehrers eine höhere Präzision (Bitbreite) hat als das Netzwerk des Schülers.Wir finden empirisch, dass speziell zugeschnittene Datenpunkte es ermöglichen, das Wissen des Lehrers dem Schüler effektiver zu demonstrieren.Wir vergleichen unseren Ansatz mit existierenden KD-Methoden beim Training populärer neuronaler Architekturen und zeigen, dass rollenweise Datenerweiterung die Effektivität von KD gegenüber starken vorherigen Ansätzen verbessert.Der Code zur Reproduktion unserer Ergebnisse wird öffentlich zugänglich gemacht.
Viele Ansätze zur neuronalen abstrakten Zusammenfassung beinhalten die Einführung signifikanter induktiver Verzerrungen, wie z.B. Pointer-Generator-Architekturen, Coverage und teilweise extraktive Verfahren, die die menschliche Zusammenfassung imitieren sollen. Wir zeigen, dass es möglich ist, eine konkurrenzfähige Leistung zu erzielen, indem wir die Zusammenfassung direkt als Sprachmodellierung betrachten. Wir stellen ein einfaches Verfahren vor, das auf vortrainierten Decoder-Transformatoren aufbaut, um konkurrenzfähige ROUGE-Ergebnisse zu erzielen, indem wir nur einen Sprachmodellierungsverlust verwenden, ohne Strahlensuche oder andere Dekodierungszeit-Optimierung, und uns stattdessen auf effizientes Nukleus-Sampling und gierige Dekodierung verlassen.
Wir gehen davon aus, dass jede Domäne nacheinander kommt und wir nur auf Daten in der aktuellen Domäne zugreifen können. Das Ziel von IDA ist es, ein einheitliches Modell zu erstellen, das in allen angetroffenen Domänen gut funktioniert. Wir schlagen vor, ein rekurrentes neuronales Netz (RNN) mit einer direkt parametrisierten Speicherbank zu erweitern, die durch einen Aufmerksamkeitsmechanismus bei jedem Schritt des RNN-Übergangs abgerufen wird. Die Speicherbank bietet eine natürliche Möglichkeit der IDA: Bei der Anpassung unseres Modells an eine neue Domäne fügen wir der Speicherbank schrittweise neue Slots hinzu, was die Modellkapazität erhöht. Experimente zeigen, dass unser Ansatz die naive Feinabstimmung und frühere Arbeiten zu IDA, einschließlich der elastischen Gewichtskonsolidierung und des progressiven neuronalen Netzes, deutlich übertrifft.  Verglichen mit der Erweiterung versteckter Zustände ist unser Ansatz robuster für alte Domänen, was sowohl durch empirische als auch theoretische Ergebnisse belegt wird.
In dieser Arbeit entwickeln wir einen neuen Algorithmus zur Wiederherstellung spärlicher Signale unter Verwendung von Reinforcement Learning (RL) und Monte Carlo Tree Search (MCTS). Ähnlich wie beim orthogonalen Matching Pursuit (OMP) wählt unser RL+MCTS-Algorithmus die Unterstützung des Signals sequentiell aus. Die wichtigste Neuerung ist, dass der vorgeschlagene Algorithmus lernt, wie die nächste Unterstützung zu wählen, im Gegensatz zu einer vorgefertigten Regel wie in OMP.Empirische Ergebnisse werden zur Verfügung gestellt, um die überlegene Leistung des vorgeschlagenen RL + MCTS Algorithmus über bestehende spärliche Signal Recovery-Algorithmen zu demonstrieren.
Ein vielversprechender Ansatz besteht darin, die hochdimensionalen Beobachtungen in einen niedrigdimensionalen latenten Repräsentationsraum einzubetten, das latente Dynamikmodell zu schätzen und dieses Modell dann für die Steuerung im latenten Raum zu verwenden. Eine wichtige offene Frage ist, wie man eine Repräsentation lernt, die für bestehende Steuerungsalgorithmen geeignet ist. Durch die Formulierung und Analyse des Problems des Repräsentationslernens aus der Perspektive der optimalen Steuerung legen wir drei grundlegende Prinzipien fest, die die gelernte Repräsentation umfassen sollte: 1) genaue Vorhersage im Beobachtungsraum, 2) Konsistenz zwischen der Dynamik im latenten und im Beobachtungsraum und 3) geringe Krümmung in den Übergängen im latenten Raum. Diese Prinzipien entsprechen natürlich einer Verlustfunktion, die aus drei Begriffen besteht: Vorhersage, Konsistenz und Krümmung (PCC). Um PCC nachvollziehbar zu machen, leiten wir eine amortisierte Variationsschranke für die PCC-Verlustfunktion ab. Umfangreiche Experimente auf Benchmark-Domänen zeigen, dass der neue Variations-PCC-Lernalgorithmus von einem wesentlich stabileren und reproduzierbaren Training profitiert und zu einer überlegenen Kontrollleistung führt.  Weitere Ablationsstudien belegen die Bedeutung aller drei PCC-Komponenten für das Erlernen eines guten latenten Raums für die Kontrolle.
Das Zusammenspiel zwischen der Topologie interneuronaler Netze und der Kognition wurde von Connectomics-Forschern und Netzwerkwissenschaftlern eingehend untersucht, was für das Verständnis der bemerkenswerten Effizienz biologischer neuronaler Netze von entscheidender Bedeutung ist.1 Seltsamerweise hat die Deep-Learning-Revolution, die neuronale Netze wiederbelebt hat, topologischen Aspekten nicht viel Aufmerksamkeit geschenkt. Wir schließen diese Lücke, indem wir erste Ergebnisse von Deep Connectomics Networks (DCNs) als DNNs mit Topologien präsentieren, die von realen neuronalen Netzwerken inspiriert sind. Wir zeigen eine hohe Klassifizierungsgenauigkeit von DCNs, deren Architektur von den biologischen neuronalen Netzwerken von C. Elegans und dem visuellen Kortex der Maus inspiriert wurde.
Faltungsneuronale Netze und rekurrente neuronale Netze verfügen über Netzwerkstrukturen, die sich gut für räumliche bzw. sequentielle Daten eignen, während die Struktur standardmäßiger neuronaler Feed-Forward-Netze (FNNs) einfach ein Stapel vollständig verbundener Schichten ist, unabhängig von den Merkmalskorrelationen in den Daten, und die Anzahl der Schichten und Neuronen manuell auf Validierungsdaten abgestimmt wird, was zeitaufwendig ist und zu suboptimalen Netzwerken führen kann. Unsere Methode bestimmt die Anzahl der Schichten, die Anzahl der Neuronen auf jeder Schicht und die spärliche Konnektivität zwischen benachbarten Schichten automatisch aus den Daten. Die resultierenden Modelle werden Backbone-Skippath Neural Networks (BSNNs) genannt. 17 Experimente zeigen, dass BSNNs im Vergleich zu FNNs eine bessere oder vergleichbare Klassifizierungsleistung mit viel weniger Parametern erreichen können.
Trotz ihrer zahlreichen Anwendungen ist die Bayes'sche Inferenz mit Herausforderungen konfrontiert, wenn es um die Ableitung von Feldern geht, die diskrete Repräsentationen von großer Dimension haben und/oder priorisierte Verteilungen haben, die mathematisch schwer zu charakterisieren sind.In dieser Arbeit zeigen wir, wie die ungefähre Verteilung, die von einem generativen adversen Netzwerk (GAN) gelernt wurde, als priorisierte Verteilung in einer Bayes'schen Aktualisierung verwendet werden kann, um diese beiden Herausforderungen zu bewältigen. Wir demonstrieren die Wirksamkeit dieses Ansatzes durch die Ableitung und Quantifizierung von Unsicherheiten in einem physikalisch basierten inversen Problem und einem inversen Problem aus der Computer Vision.
Wir argumentieren, dass die weit verbreiteten Omniglot- und miniImageNet-Benchmarks zu einfach sind, da ihre Klassensemantik nicht über Episoden hinweg variiert, was den beabsichtigten Zweck der Bewertung von Klassifizierungsmethoden mit wenigen Aufnahmen verfehlt. Da die Klassensemantik von Omniglot immer "Zeichen" und die Klassensemantik von miniImageNet "Objektkategorie" ist, schlagen wir eine neue Methode namens Centroid Networks vor, die bei Omniglot und miniImageNet überraschend hohe Genauigkeiten erreichen kann, ohne dass zur Zeit der Metaevaluierung irgendwelche Labels verwendet werden. Unsere Ergebnisse deuten darauf hin, dass diese Benchmarks nicht für überwachte few-shot-Klassifikation geeignet sind, da die Überwachung selbst während der Meta-Evaluation nicht notwendig ist. Unter Verwendung unserer Methode leiten wir eine neue Metrik ab, das Class Semantics Consistency Criterion, und verwenden es, um die Schwierigkeit des Meta-Datasets zu quantifizieren.Schließlich zeigen wir unter einigen restriktiven Annahmen, dass Centroid Networks schneller und genauer ist als eine State-of-the-Art Learning-to-Cluster-Methode (Hsu et al., 2018).
Wir lernen, Entscheidungszustände zu identifizieren, d.h. eine überschaubare Menge von Zuständen, in denen Entscheidungen die zukünftigen Zustände, die ein Agent in einer Umgebung erreichen kann, sinnvoll beeinflussen.Wir nutzen den VIC-Rahmen, der die "Ermächtigung" eines Agenten maximiert, d.h. die Fähigkeit, eine vielfältige Menge von Zuständen zuverlässig zu erreichen, und formulieren eine Sandwich-Schranke für das Ermächtigungsziel, die die Identifizierung von Entscheidungszuständen ermöglicht. Im Gegensatz zu früheren Arbeiten werden unsere Entscheidungszustände ohne extrinsische Belohnungen entdeckt - einfach durch Interaktion mit der Welt. Unsere Ergebnisse zeigen, dass unsere Entscheidungszustände:1) oft interpretierbar sind, und2) zu einer besseren Exploration bei nachgelagerten zielorientierten Aufgaben in teilweise beobachtbaren Umgebungen führen.
Inverse Probleme sind in den Naturwissenschaften allgegenwärtig und beziehen sich auf die herausfordernde Aufgabe, komplexe und potenziell multimodale posteriore Verteilungen über verborgene Parameter aus einer Reihe von Beobachtungen abzuleiten.Typischerweise steht ein Modell des physikalischen Prozesses in Form von Differentialgleichungen zur Verfügung, was jedoch zu einer schwer lösbaren Inferenz über seine Parameter führt.Während die Vorwärtspropagation von Parametern durch das Modell die Evolution des Systems simuliert, ist das inverse Problem, die Parameter aus der Sequenz von Zuständen zu finden, nicht eindeutig. In dieser Arbeit schlagen wir eine Verallgemeinerung des Bayes'schen Optimierungsrahmens vor, um die Inferenz zu approximieren. Die resultierende Methode lernt Annäherungen an die Posterior-Verteilung durch Anwendung des Stein'schen Variationsgradientenabstiegs auf die Schätzungen eines Gauß'schen Prozessmodells. Erste Ergebnisse zeigen die Leistung der Methode bei der likelihood-freien Inferenz für Verstärkungslernumgebungen.
Wir stellen einen datengesteuerten Ansatz vor, um eine Bibliothek von rückgekoppelten Bewegungsprimitiven für nicht-holonomische Fahrzeuge zu konstruieren, die einen begrenzten Fehler bei der Verfolgung beliebig langer Trajektorien garantiert, wodurch sichergestellt wird, dass eine Neuplanung der Bewegung vermieden werden kann, solange die Störungen des Fahrzeugs innerhalb einer bestimmten Grenze bleiben und möglicherweise auch, wenn die Hindernisse innerhalb einer bestimmten Grenze verschoben werden. Wir liefern hinreichende Bedingungen für die Konstruktion solcher robusten Bewegungsprimitive für eine große Klasse nichtlinearer Dynamiken, einschließlich häufig verwendeter Modelle, wie das Standardmodell von Reeds-Shepp, und wenden den Algorithmus für die Bewegungsplanung und -steuerung eines Rover mit Schlupf ohne vorherige Modellierung an.
Es hat sich gezeigt, dass Deep Convolutional Networks (DCNs) empfindlich auf Universal Adversarial Perturbations (UAPs) reagieren, d.h. auf eingabeunabhängige Störungen, die ein Modell in großen Teilen eines Datensatzes verfälschen, wobei diese UAPs interessante visuelle Muster aufweisen. Unsere Arbeit zeigt, dass visuell ähnliche prozedurale Rauschmuster ebenfalls als UAPs fungieren, insbesondere zeigen wir, dass verschiedene DCN-Architekturen empfindlich auf Gabor-Rauschmuster reagieren, und dass dieses Verhalten, seine Ursachen und Implikationen weitere Untersuchungen verdienen.
Tiefe neuronale Netze (DNNs) erbringen gute Leistungen bei einer Vielzahl von Aufgaben, obwohl die meisten in der Praxis verwendeten Netze stark überparametrisiert sind und sogar in der Lage sind, sich perfekt an zufällig beschriftete Daten anzupassen.Jüngste Erkenntnisse deuten darauf hin, dass die Entwicklung "komprimierbarer" Darstellungen der Schlüssel zur Anpassung der Komplexität überparametrierter Netze an die jeweilige Aufgabe und zur Vermeidung von Überanpassung ist (Arora et al., 2018; Zhou et al, 2018).In diesem Papier liefern wir neue empirische Beweise, die diese Hypothese unterstützen, indem wir zwei unabhängige Mechanismen identifizieren, die entstehen, wenn die Breite des Netzwerks erhöht wird: Robustheit (Einheiten, die entfernt werden können, ohne die Genauigkeit zu beeinträchtigen) und Redundanz (Einheiten mit ähnlicher Aktivität). In einer Reihe von Experimenten mit AlexNet-, ResNet- und Inception-Netzwerken in den CIFAR-10- und ImageNet-Datensätzen sowie mit flachen Netzwerken mit synthetischen Daten zeigen wir, dass DNNs bei größeren Breiten für einen umfassenden Satz von Hyperparametern entweder ihre Robustheit, ihre Redundanz oder beides konsequent erhöhen.
Deep Networks realisieren komplexe Mappings, die oft durch ihr lokal lineares Verhalten an oder um interessante Punkte herum verstanden werden. z.B. verwenden wir die Ableitung des Mappings in Bezug auf seine Eingaben für Sensitivitätsanalysen oder um eine Vorhersage zu erklären (Koordinatenrelevanz zu erhalten). eine zentrale Herausforderung besteht darin, dass solche Ableitungen selbst von Natur aus instabil sind. in diesem Papier schlagen wir ein neues Lernproblem vor, um Deep Networks zu ermutigen, stabile Ableitungen über größere Regionen zu haben. Unser Algorithmus besteht aus einem Inferenzschritt, der eine Region um einen Punkt identifiziert, in der die lineare Approximation nachweislich stabil ist, und einem Optimierungsschritt, um solche Regionen zu erweitern. Wir schlagen eine neuartige Entspannung vor, um den Algorithmus auf realistische Modelle zu skalieren.
In den letzten Jahren gab es zwei scheinbar gegensätzliche Entwicklungen bei tiefen neuronalen Faltungsnetzen (CNNs): Einerseits wird die Dichte von CNNs durch das Hinzufügen von schichtübergreifenden Verbindungen erhöht, um eine höhere Genauigkeit zu erreichen, andererseits werden durch Regularisierungs- und Pruning-Methoden spärliche Strukturen geschaffen, was zu geringeren Rechenkosten führt.In diesem Papier schlagen wir eine Brücke zwischen diesen beiden Entwicklungen, indem wir eine neue Netzwerkstruktur mit lokal dichten, aber extern spärlichen Verbindungen vorschlagen. Diese neue Struktur verwendet dichte Module als Grundbausteine und verbindet diese Module während des Trainingsprozesses über einen neuartigen Algorithmus spärlich miteinander. Experimentelle Ergebnisse zeigen, dass die lokal dichte, aber extern spärliche Struktur eine konkurrenzfähige Leistung bei Benchmark-Aufgaben (CIFAR10, CIFAR100 und ImageNet) erzielen kann, während die Netzwerkstruktur schlank bleibt.
Die meisten modernen Inferenzalgorithmen sind Varianten der Markov-Chain-Monte-Carlo-Methode (MCMC) oder der Variationsinferenz (VI), wobei beide Methoden in der Praxis mit Einschränkungen zu kämpfen haben: MCMC-Methoden können sehr rechenintensiv sein; VI-Methoden können große Verzerrungen aufweisen. In dieser Arbeit zielen wir darauf ab, MCMC und VI durch eine neuartige hybride Methode zu verbessern, die auf der Idee basiert, die Simulationsverzerrung von MCMC-Ketten mit endlicher Länge durch gradientenbasierte Optimierung zu reduzieren.die vorgeschlagene Methode kann Proben mit geringer Verzerrung generieren, indem sie die Länge der MCMC-Simulation erhöht und die MCMC-Hyperparameter optimiert, was ein attraktives Gleichgewicht zwischen Approximationsverzerrung und Recheneffizienz bietet.wir zeigen, dass unsere Methode vielversprechende Ergebnisse bei beliebten Benchmarks im Vergleich zu aktuellen hybriden MCMC- und VI-Methoden liefert.
Um diesen Effekt zu demonstrieren, trainieren wir ein neues "Meta"-Netz, um entweder aus der endgültigen Ausgabe des zugrundeliegenden "Basis"-Netzes oder aus der Ausgabe einer der Zwischenschichten des Basisnetzes vorherzusagen, ob das Basisnetz für eine bestimmte Eingabe korrekt oder inkorrekt sein wird.Wir stellen fest, dass das Meta-Netz über einen weiten Bereich von Aufgaben und Basisnetzen eine Genauigkeit von 65% - 85% bei dieser Bestimmung erreichen kann.
Wir entwickeln einen neuen Algorithmus für das Nachahmungslernen aus einer einzigen Expertendemonstration: Im Gegensatz zu vielen früheren One-Shot-Ansätzen für das Nachahmungslernen setzt unser Algorithmus nicht den Zugang zu mehr als einer Expertendemonstration während der Trainingsphase voraus, sondern nutzt eine Explorationsstrategie, um unbeaufsichtigte Trajektorien zu erhalten, die dann zum Trainieren eines Encoders und einer kontextabhängigen Nachahmungsstrategie verwendet werden. Die Optimierungsverfahren für den Encoder, den Imitationslerner und die Explorationspolitik sind eng miteinander verknüpft, so dass eine Rückkopplungsschleife entsteht, in der die Explorationspolitik neue Demonstrationen sammelt, die den Imitationslerner herausfordern, während der Encoder versucht, die Imitationspolitik bestmöglich zu unterstützen.Wir evaluieren unseren Algorithmus an 6 MujoCo-Robotikaufgaben.
Wir untersuchen dieses Problem im Kontext der Aufgabenplanung.Wir stellen das Agent Resource-Constrained Project Scheduling Problem (ARCPSP) vor, eine Erweiterung des Resource-Constrained Project Scheduling Problems, die ein Konzept von Agenten beinhaltet, die Aufgaben parallel ausführen. Wir skizzieren einen generischen Rahmen, der auf der effizienten Aufzählung minimaler unbefriedigender Mengen (MUS) und maximaler befriedigender Mengen (MSS) basiert, um kleine Beschreibungen der Quelle der Unerfüllbarkeit zu erstellen. Diese Beschreibungen werden mit potenziellen Relaxationen ergänzt, die die Unerfüllbarkeit innerhalb der Probleminstanz beheben würden. Wir veranschaulichen, wie diese Methode auf das ARCPSP angewendet werden kann, und demonstrieren, wie verschiedene Arten von Erklärungen für eine übermäßig eingeschränkte Instanz des ARCPSP erzeugt werden können.
Wir zeigen, dass Gradienten-basierte Salienz-Ansätze nicht in der Lage sind, diese Verschiebung zu erfassen, und entwickeln eine neue Verteidigung, die stattdessen negative Beispiele auf der Grundlage von erlernten Salienz-Modellen erkennt.Wir untersuchen zwei Ansätze: ein CNN, das darauf trainiert ist, zwischen natürlichen und negativen Bildern zu unterscheiden, indem es die Salienz-Masken verwendet, die von unserem erlernten Salienz-Modell erzeugt werden, und ein CNN, das auf den salienten Pixeln selbst als Eingabe trainiert wird. Auf MNIST, CIFAR-10 und ASSIRA sind unsere Abwehrmechanismen in der Lage, verschiedene Angriffe zu erkennen, einschließlich starker Angriffe wie C&W und DeepFool, im Gegensatz zu gradientenbasierten Salienz- und Detektoren, die sich auf das Eingabebild stützen. Letztere sind nicht in der Lage, Angriffe auf Bilder zu erkennen, wenn die L_2- und L_infinity-Normen der Störungen zu klein sind.
In dieser Arbeit evaluieren wir empirisch die Leistung von sechs unbeaufsichtigten Entwirrungsansätzen auf dem mpi3d Spielzeugdatensatz, der für die NeurIPS 2019 Disentanglement Challenge kuratiert und veröffentlicht wurde.Die in dieser Arbeit untersuchten Methoden sind Beta-VAE, Faktor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE und Beta-TCVAE. Die Kapazitäten aller Modelle wurden während des Trainings schrittweise erhöht, und die Hyperparameter wurden in allen Experimenten beibehalten. Die Methoden wurden anhand von fünf Entflechtungsmetriken bewertet, nämlich DCI, Faktor-VAE, IRS, MIG und SAP-Score. Im Rahmen dieser Studie wurde festgestellt, dass der Beta-TCVAE-Ansatz seine Alternativen in Bezug auf die normalisierte Summe der Metriken übertrifft. Eine qualitative Untersuchung der kodierten Latenzen zeigt jedoch, dass es keine konsistente Korrelation zwischen den berichteten Metriken und dem Entflechtungspotenzial des Modells gibt.
Die meisten früheren Arbeiten über Multi-Agenten-Verstärkungslernen (MARL) erreichen eine optimale Zusammenarbeit durch direktes Lernen einer Strategie für jeden Agenten, um eine gemeinsame Belohnung zu maximieren.In diesem Papier wollen wir dies aus einem anderen Blickwinkel angehen.Insbesondere betrachten wir Szenarien, in denen es eigennützige Agenten (d.h. Arbeiter-Agenten) gibt, die ihren eigenen Verstand (Präferenzen, Absichten, Fähigkeiten, etc.) haben und nicht diktiert werden können, um Aufgaben auszuführen, die sie nicht tun wollen.Um eine optimale Koordination zwischen diesen Agenten zu erreichen, trainieren wir einen Super-Agenten (d.h. den Manager), um sie zu leiten, indem wir zuerst die Agenten in die Lage versetzen, ihre Aufgaben zu erfüllen, Um eine optimale Koordination zwischen diesen Agenten zu erreichen, trainieren wir einen Superagenten (d.h. den Manager), der sie verwaltet, indem er zunächst ihre Gedanken auf der Grundlage aktueller und vergangener Beobachtungen ableitet und dann Verträge initiiert, um den Arbeitern geeignete Aufgaben zuzuweisen und ihnen entsprechende Prämien zu versprechen, damit sie sich bereit erklären, zusammenzuarbeiten.Das Ziel des Managers ist es, die Gesamtproduktivität zu maximieren und die Zahlungen an die Arbeiter für Ad-hoc-Teams zu minimieren. Um den Manager zu trainieren, schlagen wir Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) vor, das aus Agentenmodellierung und Policy Learning besteht. Wir haben unseren Ansatz in zwei Umgebungen, Resource Collection und Crafting, evaluiert, um Multi-Agenten-Management-Probleme mit verschiedenen Aufgabenstellungen und mehreren Designs für die Arbeiter-Agenten zu simulieren.Die experimentellen Ergebnisse haben die Effektivität unseres Ansatzes bei der Online-Modellierung von Arbeiter-Agenten und bei der Erreichung optimaler Ad-hoc-Teams mit guter Generalisierung und schneller Anpassung validiert.
Wir schlagen eine Netzwerkarchitektur vor, die zeitlich rekurrente Verbindungen für den internen Zustand der weit verbreiteten Residualblöcke einführt. wir zeigen, dass mit diesen Verbindungen faltbare neuronale Netze robuster stabile zeitliche Zustände lernen können, die zwischen den Auswertungen bestehen bleiben. wir demonstrieren ihr Potenzial für die Ableitung von hochqualitativen hochauflösenden Bildern aus niedrig aufgelösten Bildern, die mit Echtzeit-Renderern erzeugt wurden. Diese Daten kommen in einer Vielzahl von Anwendungen vor und stellen eine besondere Herausforderung dar, da sie ein stark verfremdetes Signal enthalten, das sich erheblich von den glatten Eingaben in natürlichen Videos unterscheidet und mit den vorhandenen Techniken nicht in akzeptabler Qualität erzeugt werden kann.Wir schlagen außerdem eine Reihe sorgfältiger Anpassungen typischer generativer gegnerischer Architekturen für die Superauflösung von Videos vor, um zu einem ersten Modell zu gelangen, das detaillierte, aber zeitlich kohärente Bilder aus einem verfremdeten Eingabestrom eines Echtzeit-Renderers erzeugen kann.
Der Backpropagation-Algorithmus ist heutzutage der beliebteste Algorithmus für das Training neuronaler Netze, leidet jedoch unter den Problemen der Vorwärts-, Rückwärts- und Aktualisierungssperre, insbesondere wenn ein neuronales Netz so groß ist, dass seine Schichten über mehrere Geräte verteilt sind. Bestehende Lösungen können entweder nur ein Locking-Problem behandeln oder führen zu schwerwiegenden Genauigkeitsverlusten oder Speicherineffizienz.Darüber hinaus berücksichtigt keine von ihnen das Nachzüglerproblem zwischen den Geräten.In diesem Papier schlagen wir \textbf{Layer-wise Staleness} und einen neuen effizienten Trainingsalgorithmus, \textbf{Diversely Stale Parameters} (Wir analysieren auch die Konvergenz von DSP mit zwei populären gradientenbasierten Methoden und beweisen, dass beide garantiert zu kritischen Punkten für nicht-konvexe Probleme konvergieren. Schließlich zeigen umfangreiche experimentelle Ergebnisse zum Training von tiefen konvolutionellen neuronalen Netzen, dass der von uns vorgeschlagene DSP-Algorithmus eine signifikante Trainingsbeschleunigung mit stärkerer Robustheit und besserer Generalisierung als vergleichbare Methoden erreichen kann.
Die Emergenz von Sprache in Multi-Agenten-Settings ist eine vielversprechende Forschungsrichtung, um natürliche Sprache in simulierten Agenten zu verankern.Wenn KI in der Lage wäre, die Bedeutung von Sprache durch ihre Verwendung zu verstehen, könnte sie diese auch flexibel auf andere Situationen übertragen.Das wird als wichtiger Schritt zur Erreichung einer allgemeinen KI gesehen.Der Umfang der emergenten Kommunikation ist bisher jedoch noch begrenzt.Es ist notwendig, die Lernmöglichkeiten für die mit der Kommunikation verbundenen Fähigkeiten zu verbessern, um die emergente Komplexität zu erhöhen. Wir haben ein Beispiel aus dem menschlichen Spracherwerb und der Bedeutung der empathischen Verbindung in diesem Prozess genommen.Wir schlagen einen Ansatz vor, um den Begriff der Empathie in Multi-Agenten Deep Reinforcement Learning einzuführen.Wir erweitern bestehende Ansätze auf referentielle Spiele mit einer zusätzlichen Aufgabe für den Sprecher, um die Gedankenänderung des Zuhörers vorherzusagen, was die Lernzeit verbessert.Unsere Experimente zeigen das hohe Potenzial dieses architektonischen Elements durch die Verdoppelung der Lerngeschwindigkeit der Testanordnung.
Bestehende typische Deep-Learning-basierte Modelle für Bildunterschriften bestehen aus einem Bild-Encoder, um visuelle Merkmale zu extrahieren, und einem Sprachmodell-Decoder, der vielversprechende Ergebnisse bei der Generierung einzelner Sätze auf hoher Ebene gezeigt hat. Die Inkonsistenz zwischen der parallelen Extraktion von visuellen Merkmalen und der sequentiellen Textüberwachung schränkt den Erfolg ein, wenn die Länge des generierten Textes lang ist (mehr als 50 Wörter). In diesem Papier schlagen wir ein neues Modul vor, das Text Embedding Bank (TEB)-Modul, um das Problem der Bilduntertitelung zu lösen. Dieses Modul verwendet das Absatzvektormodell, um Merkmalsrepräsentationen mit fester Länge aus einem Absatz mit variabler Länge zu erlernen. Wir bezeichnen das Merkmal mit fester Länge als TEB. Dieses TEB-Modul spielt zwei Rollen, um die Leistung der Absatzbeschriftung zu verbessern: Erstens fungiert es als eine Form der globalen und kohärenten tiefen Überwachung, um die visuelle Merkmalsextraktion im Bildcodierer zu regulieren. Zweitens fungiert es als verteilter Speicher, um dem Sprachmodell die Merkmale des gesamten Absatzes zur Verfügung zu stellen, wodurch das Problem der langfristigen Abhängigkeit gemildert wird. Die Hinzufügung dieses Moduls zu zwei bestehenden State-of-the-Art-Methoden erzielt mit großem Abstand ein neues State-of-the-Art-Ergebnis auf dem Visual-Genome-Datensatz für Absatzbeschriftungen.
Mehrere Algorithmen wurden für dieses Problem entwickelt, darunter das informationstheoretische metrische Lernen (ITML) [Davis et al. 2007] und die Large Margin Nearest Neighbor (LMNN) Klassifizierung [Weinberger und Saul 2009].  Wir betrachten eine Formulierung des metrischen Mahalanobis-Lernens als Optimierungsproblem, bei dem das Ziel darin besteht, die Anzahl der verletzten Ähnlichkeits-/Dissimilaritätsbeschränkungen zu minimieren.  Wir zeigen, dass für jede feste Umgebungsdimension ein vollständig polynomiales Approximationsschema (FPTAS) mit nahezu linearer Laufzeit existiert, das mit Hilfe von Werkzeugen aus der Theorie der linearen Programmierung in niedrigen Dimensionen gewonnen wird, diskutieren Verbesserungen des Algorithmus in der Praxis und präsentieren experimentelle Ergebnisse auf synthetischen und realen Datensätzen.
Standard-Bildbeschriftungsaufgaben wie COCO und Flickr30k sind sachlich, neutral im Ton und geben (für einen Menschen) das Offensichtliche an (z.B. "ein Mann spielt Gitarre"). Während solche Aufgaben nützlich sind, um zu überprüfen, ob eine Maschine den Inhalt eines Bildes versteht, sind sie für Menschen als Beschriftungen nicht ansprechend.   Vor diesem Hintergrund definieren wir eine neue Aufgabe, Personality-Captions, deren Ziel es ist, durch die Einbeziehung von kontrollierbaren Stil- und Persönlichkeitsmerkmalen so ansprechend wie möglich zu sein. 201.858 solcher Bildunterschriften, konditioniert auf 215 mögliche Merkmale, werden von uns gesammelt und veröffentlicht.  Wir erstellen Modelle, die bestehende Arbeiten aus (i) Satzrepräsentationen (Mazaré et al., 2018) mit Transformers, die auf 1,7 Milliarden Dialogbeispielen trainiert wurden, und (ii) Bildrepräsentationen (Mahajan et al., 2018) mit ResNets, die auf 3,5 Milliarden Social Media-Bildern trainiert wurden, kombinieren.  Wir erhalten State-of-the-Art-Leistung auf Flickr30k und COCO, und starke Leistung auf unsere neue Aufgabe.Schließlich, Online-Evaluierungen validieren, dass unsere Aufgabe und Modelle sind ansprechend für Menschen, mit unserem besten Modell in der Nähe der menschlichen Leistung.
Modelle des maschinellen Lernens (ML), die durch differentiell privaten stochastischen Gradientenabstieg (DP-SGD) trainiert werden, haben einen viel geringeren Nutzen als die nicht-privaten Modelle.Um diese Verschlechterung abzumildern, schlagen wir einen DP-Laplacian-Glättungs-SGD (DP-LSSGD) vor, um ML-Modelle mit differentiellen Datenschutzgarantien (DP) zu trainieren.Das Herzstück von DP-LSSGD ist die Laplacian-Glättung, die das im Gauß-Mechanismus verwendete Gauß-Rauschen glättet. In der Praxis macht DP-LSSGD das Training von konvexen und nicht-konvexen ML-Modellen stabiler und ermöglicht es den trainierten Modellen, sich besser zu verallgemeinern. Der vorgeschlagene Algorithmus ist einfach zu implementieren und die zusätzliche Rechenkomplexität und der Speicher-Overhead im Vergleich zu DP-SGD sind vernachlässigbar.DP-LSSGD ist anwendbar, um eine Vielzahl von ML-Modellen, einschließlich DNNs, zu trainieren.
Wir untersuchen das robuste Ein-Bit-Compressed-Sensing-Problem, dessen Ziel es ist, einen Algorithmus zu entwerfen, der jeden spärlichen Zielvektor $\theta_0\in\mathbb{R}^d$ \emph{uniformly} aus $m$ quantisierten verrauschten Messungen treu wiederherstellt. Unter der Annahme, dass die Messungen sub-Gauß'sch sind, benötigt der beste bekannte rechnerisch nachvollziehbare Algorithmus, um jedes $k$-sparse $\theta_0$ ($k\ll d$) \emph{uniformly} bis zu einem Fehler $\varepsilon$ mit hoher Wahrscheinlichkeit wiederherzustellen,\footnote{Hier ist ein Algorithmus ``rechnerisch nachvollziehbar'', wenn er nachweisbare Konvergenzgarantien besitzt. Die Notation $\tilde{\mathcal{O}}(\cdot)$ lässt einen Logarithmusfaktor von $\varepsilon^{-1}$ weg.} $m\geq\tilde{\mathcal{O}}(k\log d/\varepsilon^4)$.In dieser Arbeit betrachten wir einen neuen Rahmen für das Ein-Bit-Erfassungsproblem, bei dem die Sparsamkeit implizit durch die Abbildung einer niedrigdimensionalen Repräsentation $x_0$ durch ein bekanntes $n$-schichtiges generatives ReLU-Netzwerk $G:\mathbb{R}^k\rightarrow\mathbb{R}^d$ erzwungen wird. Wir schlagen vor, das Ziel $G(x_0)$ über ein unbeschränktes empirisches Risikominimierungsproblem (ERM) unter einer viel schwächeren \emph{sub-exponentiellen Messannahme} wiederherzustellen. Für ein solches Problem führen wir eine gemeinsame statistische und rechnerische Analyse durch. Insbesondere beweisen wir, dass der ERM-Schätzer in diesem neuen Rahmen eine verbesserte statistische Rate von $m=\tilde{\mathcal{O}} (kn\log d /\epsilon^2)$ eine verbesserte statistische Rate von $m=\tilde{\mathcal{O}}} (kn\log d /\epsilon^2)$ erreicht, die jedes $G(x_0)$ gleichmäßig bis zu einem Fehler $\varepsilon$ wiederherstellt. Darüber hinaus beweisen wir, dass unter geeigneten Bedingungen für die ReLU-Gewichte das von uns vorgeschlagene empirische Risiko trotz Nicht-Konvexität keinen stationären Punkt außerhalb kleiner Nachbarschaften um die wahre Darstellung $x_0$ und ihr negatives Vielfaches hat. Unsere Analyse wirft ein Licht auf die Möglichkeit, ein tiefes generatives Modell unter partiellen und quantisierten Messungen zu invertieren und ergänzt damit den jüngsten Erfolg der Verwendung tiefer generativer Modelle für inverse Probleme.
Wir stellen einen unbeaufsichtigten Strukturlernalgorithmus für tiefe, vorwärtsgerichtete neuronale Netze vor und schlagen eine neue Interpretation für die Tiefe und die Konnektivität zwischen den Schichten vor, bei der eine Hierarchie von Unabhängigkeiten in der Eingangsverteilung in der Netzstruktur kodiert wird, was zu Strukturen führt, die es den Neuronen ermöglichen, sich mit Neuronen in jeder tieferen Schicht zu verbinden und Zwischenschichten zu überspringen. Darüber hinaus kodieren Neuronen in tieferen Schichten Abhängigkeiten niedriger Ordnung (kleine Bedingungsmengen) und haben einen weiten Bereich des Inputs, während Neuronen in den ersten Schichten Abhängigkeiten höherer Ordnung (größere Bedingungsmengen) kodieren und einen engeren Bereich haben, wodurch die Tiefe des Netzes automatisch bestimmt wird - gleich der maximalen Ordnung der Unabhängigkeit in der Inputverteilung, die die Rekursionstiefe des Algorithmus darstellt. Der vorgeschlagene Algorithmus konstruiert zwei grafische Hauptmodelle:1) einen generativen latenten Graphen (ein tiefes Glaubensnetzwerk), der aus Daten gelernt wird, und2) einen tiefen diskriminativen Graphen, der aus dem generativen latenten Graphen konstruiert wird.Wir beweisen, dass bedingte Abhängigkeiten zwischen den Knoten im gelernten generativen latenten Graphen im klassenbedingten diskriminativen Graphen erhalten bleiben.Schließlich wird eine tiefe neuronale Netzwerkstruktur auf der Grundlage des diskriminativen Graphen konstruiert. Anhand von Bildklassifizierungs-Benchmarks wird gezeigt, dass der Algorithmus die tiefsten Schichten (Faltungsschichten und dichte Schichten) herkömmlicher Faltungsnetze ersetzt und dabei eine hohe Klassifizierungsgenauigkeit erreicht, während gleichzeitig deutlich kleinere Strukturen konstruiert werden.Der vorgeschlagene Algorithmus zum Strukturlernen erfordert nur geringe Rechenkosten und läuft effizient auf einer Standard-Desktop-CPU.
Um dieses Phänomen zu verstehen, untersuchen wir, wie und warum das Training bei starker Regularisierung scheitert. Insbesondere untersuchen wir, wie sich die Gradienten im Laufe der Zeit für verschiedene Regularisierungsstärken verändern und liefern eine Analyse, warum die Gradienten so schnell abnehmen. Wir stellen fest, dass es eine Toleranzschwelle für die Stärke der Regularisierung gibt, bei der das Lernen vollständig scheitert, wenn die Stärke der Regularisierung darüber hinausgeht.Wir schlagen eine einfache, aber neuartige Methode vor, die verzögerte starke Regularisierung, um die Toleranzschwelle zu moderieren.Experimentelle Ergebnisse zeigen, dass der von uns vorgeschlagene Ansatz tatsächlich eine starke Regularisierung sowohl für L1- als auch für L2-Regularisierer erreicht und sowohl die Genauigkeit als auch die Sparsamkeit auf öffentlichen Datensätzen verbessert.Unser Quellcode wird veröffentlicht.
In Bereichen, in denen viel auf dem Spiel steht (z.B. in der medizinischen Diagnostik), ist es entscheidend, Einblicke in das Netzwerk zu erhalten, um Vertrauen zu gewinnen und angenommen zu werden.Eine der Möglichkeiten, die Interpretierbarkeit eines neuronalen Netzwerks zu verbessern, besteht darin, die Bedeutung eines bestimmten Konzepts (z.B. Geschlecht) bei der Vorhersage zu erklären, Diese Arbeit zielt darauf ab, quantitative Antworten auf die Frage nach der relativen Bedeutung von Konzepten von Interesse über Konzeptaktivierungsvektoren (CAV) zu geben. Insbesondere ermöglicht dieser Rahmen es Experten, die nicht mit maschinellem Lernen zu tun haben, Konzepte von Interesse auszudrücken und Hypothesen anhand von Beispielen zu testen (z. B, Wir zeigen, dass CAV mit einer relativ kleinen Menge von Beispielen erlernt werden kann.Testen mit CAV kann zum Beispiel beantworten, ob ein bestimmtes Konzept (z.B. Geschlecht) wichtiger für die Vorhersage einer bestimmten Klasse (z.B. Arzt) ist als andere Konzepte.Interpretieren mit CAV erfordert kein Umlernen oder Modifikation des Netzwerks.Wir zeigen, dass viele Ebenen von sinnvollen Konzepten erlernt werden (z.B., Wir zeigen, dass viele Ebenen von bedeutungsvollen Konzepten erlernt werden (z.B. Farbe, Textur, Objekte, der Beruf einer Person), und wir präsentieren CAV's \textit{empirical deepdream} - wo wir eine Aktivierung maximieren, indem wir einen Satz von Beispielbildern verwenden.Wir zeigen, wie verschiedene Erkenntnisse aus dem relativen Wichtigkeitstest mit CAV gewonnen werden können.
Wir stellen eine neue Familie von Zielfunktionen vor, die wir als Conditional Entropy Bottleneck (CEB) bezeichnen und die durch das Minimum Necessary Information (MNI) Kriterium motiviert sind und demonstrieren die Anwendung von CEB auf Klassifikationsaufgaben. Wir zeigen, dass CEB: gut kalibriert Vorhersagen; starke Erkennung von anspruchsvollen Out-of-Distribution Beispiele und leistungsstarke Whitebox gegnerische Beispiele; und erhebliche Robustheit zu diesen adversaries.Finally, wir berichten, dass CEB nicht von Informationen-freie Datensätze zu lernen, eine mögliche Lösung für das Problem der Generalisierung in Zhang et al. beobachtet (2016).
Deep Repräsentation Learning hat sich zu einem der am weitesten verbreiteten Ansätze für die visuelle Suche, Empfehlung und Identifizierung.Retrieval solcher Darstellungen aus einer großen Datenbank ist jedoch rechnerisch challenging.Approximate Methoden auf der Grundlage Lernen kompakte Darstellungen, wurden weithin für dieses Problem, wie locality sensitive Hashing, Produkt Quantisierung und PCA erforscht. Im Gegensatz zum Erlernen kompakter Repräsentationen schlagen wir in dieser Arbeit vor, hochdimensionale und spärliche Repräsentationen zu erlernen, die eine ähnliche Darstellungskapazität wie dichte Einbettungen haben, aber aufgrund der spärlichen Matrixmultiplikationsoperationen, die viel schneller sein können als dichte Multiplikationen, effizienter sind. Wir folgen der wichtigen Erkenntnis, dass die Anzahl der Operationen quadratisch mit der Spärlichkeit der Einbettungen abnimmt, vorausgesetzt, dass die Nicht-Null-Einträge gleichmäßig über die Dimensionen verteilt sind. Wir schlagen einen neuartigen Ansatz vor, um solche verteilten spärlichen Einbettungen durch die Verwendung einer sorgfältig konstruierten Regularisierungsfunktion zu erlernen, die direkt eine kontinuierliche Relaxation der Anzahl der Gleitkommaoperationen (FLOPs) minimiert, die während des Abrufs anfallen.Unsere Experimente zeigen, dass unser Ansatz mit den anderen Grundlinien konkurrenzfähig ist und einen ähnlichen oder besseren Kompromiss zwischen Geschwindigkeit und Genauigkeit auf praktischen Datensätzen liefert.
Im Bereich der intuitiven Physik untersuchen wir die Aufgabe, die Stabilität von Blocktürmen visuell vorherzusagen, mit dem Ziel, die Argumentation des Modells zu verstehen und zu beeinflussen.Unsere Beiträge sind zweifach. Erstens führen wir neuronale Stethoskope als ein Rahmenwerk ein, um den Grad der Wichtigkeit spezifischer Einflussfaktoren in tiefen Netzwerken zu quantifizieren sowie Informationen aktiv zu fördern und zu unterdrücken, wie es angemessen ist.Dabei vereinen wir Konzepte aus dem Multitasking-Lernen sowie dem Training mit Hilfs- und adversen Verlusten.Zweitens setzen wir das Stethoskop-Rahmenwerk ein, um eine eingehende Analyse eines hochmodernen tiefen neuronalen Netzwerks für die Stabilitätsvorhersage zu liefern, wobei wir insbesondere dessen physikalische Argumentation untersuchen. Wir zeigen, dass das Basismodell anfällig dafür ist, durch falsche visuelle Hinweise in die Irre geführt zu werden, was zu einem Leistungsabfall auf das Niveau von zufälligem Raten führt, wenn es auf Szenarien trainiert wird, in denen visuelle Hinweise umgekehrt mit der Stabilität korreliert sind.Die Verwendung von Stethoskopen zur Förderung einer sinnvollen Merkmalsextraktion erhöht die Leistung von 51% auf 90% Vorhersagegenauigkeit. Umgekehrt erlernt das Basismodell beim Training auf einem einfachen Datensatz, bei dem visuelle Hinweise positiv mit der Stabilität korreliert sind, eine Verzerrung, die zu einer schlechten Leistung auf einem schwierigeren Datensatz führt. Durch die Verwendung eines gegnerischen Stethoskops wird die Verzerrung des Netzwerks erfolgreich aufgehoben, was zu einer Leistungssteigerung von 66 % auf 88 % führt.
Flow-basierte Modelle wie Real NVP sind ein äußerst leistungsfähiger Ansatz zur Dichteschätzung, aber bestehende Flow-basierte Modelle sind darauf beschränkt, kontinuierliche Dichten über einen kontinuierlichen Eingaberaum in ähnlich kontinuierliche Verteilungen über kontinuierliche latente Variablen zu transformieren, was sie für die Modellierung und Darstellung diskreter Strukturen in Datenverteilungen, z. B. Klassenzugehörigkeit oder diskrete Symmetrien, schlecht geeignet macht.  Dieser RAD-Ansatz (Real and Discrete) behält die wünschenswerten Normalisierungsfluss-Eigenschaften der exakten Probenahme, der exakten Inferenz und der analytisch berechenbaren Wahrscheinlichkeiten bei, während er gleichzeitig die gleichzeitige Modellierung von kontinuierlichen und diskreten Strukturen in einer Datenverteilung ermöglicht.
Wir untersuchen die Verlustfläche von neuronalen Netzen und beweisen, dass selbst für einschichtige Netze mit "geringster" Nichtlinearität die empirischen Risiken in den meisten Fällen ungewollte lokale Minima aufweisen, was darauf hindeutet, dass im Allgemeinen "keine ungewollten lokalen Minima" eine Eigenschaft sind, die auf tiefe lineare Netze beschränkt ist, und dass die aus linearen Netzen gewonnenen Erkenntnisse möglicherweise nicht robust sind, insbesondere beweisen wir für ReLU(-ähnliche) Netze konstruktiv, dass für fast alle praktischen Datensätze unendlich viele lokale Minima existieren. Wir präsentieren auch ein Gegenbeispiel für allgemeinere Aktivierungen (sigmoid, tanh, arctan, ReLU, etc.), für die es ein schlechtes lokales Minimum gibt.Unsere Ergebnisse machen die am wenigsten restriktiven Annahmen im Vergleich zu bestehenden Ergebnissen über falsche lokale Optima in neuronalen Netzen.Wir schließen unsere Diskussion ab, indem wir eine umfassende Charakterisierung der globalen Optimalität für tiefe lineare Netze präsentieren, die andere Ergebnisse zu diesem Thema vereinheitlicht.
Wenn der Agent jedoch weiß, welche Eigenschaften der Umgebung wir als wichtig erachten, dann kann er, nachdem er gelernt hat, wie sich seine Aktionen auf diese Eigenschaften auswirken, in der Lage sein, dieses Wissen zu nutzen, um komplexe Aufgaben zu lösen, ohne speziell dafür zu trainieren.Zu diesem Zweck betrachten wir ein Setup, in dem eine Umgebung mit einer Reihe von benutzerdefinierten Attributen erweitert wird, die die Merkmale von Interesse parametrisieren. Wir schlagen ein Modell vor, das eine Strategie für den Übergang zwischen "nahegelegenen" Attributssätzen lernt und einen Graphen möglicher Übergänge unterhält. Bei einer Aufgabe zur Testzeit, die durch einen Zielsatz von Attributen und einen aktuellen Zustand ausgedrückt werden kann, leitet unser Modell die Attribute des aktuellen Zustands ab und sucht über Pfade durch den Attributraum, um einen Plan auf hoher Ebene zu erhalten, und verwendet dann seine Strategie auf niedriger Ebene, um den Plan auszuführen. Wir zeigen anhand von Grid-World-Spielen und dem Stapeln von 3D-Blöcken, dass unser Modell in der Lage ist, längere, komplexere Aufgaben zur Testzeit zu verallgemeinern, selbst wenn es zur Trainingszeit nur kurze, einfache Aufgaben sieht.
Die Fähigkeit, neue Konzepte mit kleinen Datenmengen zu erlernen, ist ein entscheidender Aspekt der Intelligenz, der sich für Deep-Learning-Methoden als Herausforderung erwiesen hat.Meta-Learning hat sich als vielversprechende Technik zur Nutzung von Daten aus früheren Aufgaben herausgestellt, um ein effizientes Lernen neuer Aufgaben zu ermöglichen.Die meisten Meta-Learning-Algorithmen setzen jedoch implizit voraus, dass sich die Meta-Trainingsaufgaben gegenseitig ausschließen, so dass kein einzelnes Modell alle Aufgaben auf einmal lösen kann. Wenn dies nicht der Fall ist, kann der Meta-Learner die Trainingsdaten für die Aufgabe ignorieren und ein einzelnes Modell erlernen, das alle Meta-Trainingsaufgaben von null auf hundert erfüllt, sich aber nicht effektiv an neue Bildklassen anpassen kann.  Diese Anforderung bedeutet, dass der Benutzer beim Entwurf der Aufgaben große Sorgfalt walten lassen muss, z.B. durch das Mischen von Beschriftungen oder das Entfernen von aufgabenidentifizierenden Informationen aus den Eingaben.In einigen Domänen macht dies das Meta-Lernen völlig unanwendbar.In diesem Papier gehen wir diese Herausforderung an, indem wir ein Meta-Regularisierungsziel unter Verwendung der Informationstheorie entwerfen, das der datengesteuerten Anpassung Vorrang einräumt.Dies veranlasst den Meta-Lerner zu entscheiden, was aus den Aufgabentrainingsdaten gelernt werden muss und was aus den Aufgabentesteingaben abgeleitet werden sollte. Wir demonstrieren seine Anwendbarkeit sowohl auf kontextuelle als auch auf gradientenbasierte Meta-Learning-Algorithmen und wenden ihn in praktischen Umgebungen an, in denen die Anwendung von Standard-Meta-Learning schwierig war, und unser Ansatz übertrifft Standard-Meta-Learning-Algorithmen in diesen Umgebungen erheblich. 
Das Erlernen einer effizienten Aktualisierungsregel aus Daten, die ein schnelles Lernen neuer Aufgaben aus derselben Verteilung fördert, bleibt ein offenes Problem im Bereich des Meta-Learnings. Einerseits wird bei der direkten Erzeugung von Aktualisierungen auf einen nützlichen induktiven Bias verzichtet, was leicht zu nicht konvergierendem Verhalten führen kann, andererseits werden bei Ansätzen, die versuchen, eine gradientenbasierte Aktualisierungsregel zu steuern, typischerweise Gradienten durch den Lernprozess berechnet, um ihre Meta-Gradienten zu erhalten, was zu Methoden führt, die nicht über eine Anpassung an wenige Aufgaben hinausgehen können. In dieser Arbeit schlagen wir Warped Gradient Descent (WarpGrad) vor, eine Methode, die diese Ansätze überschneidet, um ihre Beschränkungen zu mildern.WarpGrad lernt eine effizient parametrisierte Vorkonditionierungsmatrix, die den Gradientenabstieg über die Aufgabenverteilung erleichtert.Vorkonditionierung entsteht durch die Verschachtelung von nicht-linearen Schichten, die als Warp-Schichten bezeichnet werden, zwischen den Schichten eines Task-Learners. Warp-Schichten sind meta-learned ohne Backpropagating durch die Aufgabe Trainingsprozess in einer Art und Weise ähnlich wie Methoden, die lernen, direkt zu produzieren updates.WarpGrad ist rechnerisch effizient, einfach zu implementieren, und kann auf beliebig große Meta-Learning-Probleme skalieren.We bieten eine geometrische Interpretation des Ansatzes und bewerten ihre Wirksamkeit in einer Vielzahl von Einstellungen, einschließlich few-shot, Standard überwacht, kontinuierliche und Verstärkung Lernen.
In den letzten Jahren haben sich tiefe neuronale Netze für maschinelle Lernaufgaben, einschließlich der Klassifizierung, durchgesetzt, aber es hat sich gezeigt, dass sie anfällig für Störungen sind: Sorgfältig ausgearbeitete kleine Störungen können zu einer Fehlklassifizierung legitimer Bilder führen.Wir schlagen Defense-GAN vor, ein neues Framework, das die Ausdrucksfähigkeit generativer Modelle nutzt, um tiefe neuronale Netze gegen solche Angriffe zu verteidigen.Defense-GAN wird so trainiert, dass es die Verteilung ungestörter Bilder modelliert. Die von uns vorgeschlagene Methode kann mit jedem Klassifizierungsmodell verwendet werden und verändert weder die Struktur des Klassifizierers noch das Trainingsverfahren. Sie kann auch als Verteidigung gegen jeden Angriff verwendet werden, da sie keine Kenntnis des Prozesses zur Erzeugung der gegnerischen Beispiele voraussetzt.
Wir untersuchen das Problem des Lernens von Ähnlichkeitsfunktionen über sehr große Korpora unter Verwendung von Einbettungsmodellen für neuronale Netze. Diese Modelle werden typischerweise unter Verwendung von SGD mit Zufallsstichproben von unbeobachteten Paaren trainiert, mit einer Stichprobengröße, die quadratisch mit der Korpusgröße wächst, was die Skalierung teuer macht.Wir schlagen neue effiziente Methoden vor, um diese Modelle zu trainieren, ohne unbeobachtete Paare abtasten zu müssen. Wir zeigen, dass der Gradient dieses Terms effizient berechnet werden kann, indem die Schätzungen der Gramians beibehalten werden, und entwickeln Varianzreduktionsschemata, um die Qualität der Schätzungen zu verbessern.Wir führen groß angelegte Experimente durch, die eine signifikante Verbesserung sowohl der Trainingszeit als auch der Generalisierungsleistung im Vergleich zu Stichprobenmethoden zeigen.
Damit die Technologie zum Verstehen natürlicher Sprache (Natural Language Understanding, NLU) maximal nützlich ist, muss sie in der Lage sein, Sprache auf eine Art und Weise zu verarbeiten, die nicht ausschließlich auf eine einzelne Aufgabe, ein Genre oder einen Datensatz beschränkt ist.Um dieses Ziel zu erreichen, stellen wir den General Language Understanding Evaluation (GLUE) Benchmark vor, eine Sammlung von Werkzeugen zur Bewertung der Leistung von Modellen in einer Vielzahl von bestehenden NLU-Aufgaben.Durch die Einbeziehung von Aufgaben mit begrenzten Trainingsdaten soll GLUE Modelle begünstigen und fördern, die allgemeines linguistisches Wissen über Aufgaben hinweg teilen. GLUE beinhaltet auch eine handgefertigte diagnostische Testsuite, die eine detaillierte linguistische Analyse der Modelle ermöglicht. Wir evaluieren Grundlinien, die auf aktuellen Methoden für Transfer- und Repräsentationslernen basieren, und stellen fest, dass Multitasking-Training für alle Aufgaben besser abschneidet als das Training eines separaten Modells pro Aufgabe.
Eine Vielzahl von kooperativen Multi-Agenten-Kontrollproblemen erfordert, dass Agenten individuelle Ziele erreichen und gleichzeitig zum kollektiven Erfolg beitragen. Diese Multi-Ziel-Multi-Agenten-Einstellung stellt aktuelle Algorithmen vor Schwierigkeiten, die in erster Linie auf Einstellungen mit einer einzigen globalen Belohnung abzielen, und zwar aufgrund von zwei neuen Herausforderungen: effiziente Exploration für das Erlernen sowohl der individuellen Zielerreichung als auch der Kooperation für den Erfolg anderer Agenten und die Kredit-Zuweisung für Interaktionen zwischen Aktionen und Zielen verschiedener Agenten. Um beide Herausforderungen zu bewältigen, strukturieren wir das Problem in ein neuartiges zweistufiges Curriculum um, in dem die Zielerreichung eines einzelnen Agenten vor dem Erlernen der Kooperation mehrerer Agenten erlernt wird, und wir leiten einen neuen Multi-Ziel-Multi-Agenten-Politik-Gradienten mit einer Kreditfunktion für die lokalisierte Kreditvergabe ab.Wir verwenden ein Funktionserweiterungsschema, um Wert- und Politikfunktionen über das Curriculum zu überbrücken. Die komplette Architektur, genannt CM3, lernt signifikant schneller als direkte Anpassungen bestehender Algorithmen bei drei herausfordernden Multi-Ziel-Multi-Agenten-Problemen: kooperative Navigation in schwierigen Formationen, Verhandeln von Fahrbahnwechseln mit mehreren Fahrzeugen im SUMO-Verkehrssimulator und strategische Kooperation in einer Checkers-Umgebung.
Das erste Problem sind die impliziten Verzerrungen in den Belohnungsfunktionen, die in diesen Algorithmen verwendet werden. Während diese Verzerrungen in einigen Umgebungen gut funktionieren, können sie in anderen Umgebungen zu suboptimalem Verhalten führen. Zweitens: Obwohl diese Algorithmen von wenigen Expertendemonstrationen lernen können, benötigen sie eine prohibitiv große Anzahl von Interaktionen mit der Umgebung, um den Experten für viele reale Anwendungen zu imitieren. Um diese Probleme zu lösen, schlagen wir einen neuen Algorithmus mit dem Namen Discriminator-Actor-Critic vor, der Off-Policy Reinforcement Learning verwendet, um die Komplexität der Policy-Umwelt-Interaktionsproben um einen durchschnittlichen Faktor 10 zu reduzieren.
Capsule Networks haben vielversprechende Ergebnisse auf defacto Benchmark-Computer-Vision-Datensätzen wie MNIST, CIFAR und smallNORB gezeigt, obwohl sie noch auf Aufgaben getestet werden müssen, bei denen (1) die erkannten Entitäten inhärent komplexere interne Repräsentationen haben und (2) es nur sehr wenige Instanzen pro Klasse gibt, von denen gelernt werden kann, und (3) bei denen eine punktweise Klassifizierung nicht geeignet ist.Daher führt dieses Papier Experimente zur Gesichtsverifikation in kontrollierten und unkontrollierten Umgebungen durch, die diese Punkte gemeinsam angehen. Das Modell wird mit Hilfe von kontrastiven Verlusten mit l2-normalisierten, kapselkodierten Pose-Merkmalen trainiert. Wir stellen fest, dass siamesische Kapselnetzwerke in beiden paarweisen Lerndatensätzen gut gegen starke Grundlinien abschneiden und die besten Ergebnisse in der Einstellung mit wenigen Aufnahmen erzielen, in der die Bildpaare im Testsatz ungesehene Personen enthalten.
Tiere zeichnen sich dadurch aus, dass sie ihre Absichten, ihre Aufmerksamkeit und ihre Handlungen an die Umgebung anpassen können. Das macht sie bemerkenswert effizient bei der Interaktion mit einer reichhaltigen, unvorhersehbaren und sich ständig verändernden Außenwelt, eine Eigenschaft, die intelligenten Maschinen derzeit fehlt. Diese Anpassungseigenschaft hängt stark von der zellulären Neuromodulation ab, dem biologischen Mechanismus, der die intrinsischen Eigenschaften von Neuronen und die Reaktion auf externe Reize dynamisch und kontextabhängig steuert. Die Anpassungsfähigkeiten des Netzwerks werden anhand von Navigations-Benchmarks in einem Meta-Lernkontext getestet und mit modernsten Ansätzen verglichen. Die Ergebnisse zeigen, dass Neuromodulation in der Lage ist, einen Agenten an verschiedene Aufgaben anzupassen, und dass auf Neuromodulation basierende Ansätze einen vielversprechenden Weg zur Verbesserung der Anpassung von künstlichen Systemen darstellen.
Um CNNs effizienter zu machen, wurden viele Methoden vorgeschlagen, um entweder leichtgewichtige Netzwerke zu entwerfen oder Modelle zu komprimieren. Obwohl einige effiziente Netzwerkstrukturen vorgeschlagen wurden, wie MobileNet oder ShuffleNet, stellen wir fest, dass es immer noch redundante Informationen zwischen den Faltungskernen gibt. Um dieses Problem zu lösen, schlagen wir in diesem Papier eine neuartige dynamische Faltungsmethode mit dem Namen \textbf{DyNet} vor, die adaptiv Faltungskerne auf der Grundlage von Bildinhalten erzeugen kann.Um die Effektivität zu demonstrieren, wenden wir DyNet auf mehrere moderne CNNs an. Die Ergebnisse des Experiments zeigen, dass DyNet die Berechnungskosten erheblich reduzieren kann, während die Leistung nahezu unverändert bleibt. Insbesondere für ShuffleNetV2 (1.0), MobileNetV2 (1.0), ResNet18 und ResNet50 reduziert DyNet 40,0%, 56,7%, 68,2% bzw. 72,4% FLOPs, während sich die Top-1-Genauigkeit bei ImageNet nur um +1,0%, -0,27%, -0. In der Zwischenzeit beschleunigt DyNet die Inferenzgeschwindigkeit von MobileNetV2 (1.0), ResNet18 und ResNet50 um das 1,87-fache, 1,32-fache bzw. 1,48-fache auf der CPU-Plattform. Um die Skalierbarkeit zu überprüfen, wenden wir DyNet auch auf Segmentierungsaufgaben an.
Der Betrieb von tiefen neuronalen Netzen auf Geräten mit begrenzten Ressourcen erfordert die Reduzierung ihres Speicherbedarfs und ihrer Rechenanforderungen.in diesem Papier stellen wir eine Trainingsmethode vor, die so genannte Look-up-Table-Quantisierung (LUT-Q), die ein Wörterbuch lernt und jedes Gewicht einem der Werte des Wörterbuchs zuordnet.wir zeigen, dass diese Methode sehr flexibel ist und dass viele andere Techniken als Spezialfälle von LUT-Q angesehen werden können. Wir zeigen, dass die Methode sehr flexibel ist und dass viele andere Techniken als Spezialfälle von LUT-Q angesehen werden können. Zum Beispiel können wir das mit LUT-Q trainierte Wörterbuch einschränken, um Netzwerke mit beschnittenen Gewichtsmatrizen zu erzeugen oder das Wörterbuch auf Zweierpotenzen beschränken, um Multiplikationen zu vermeiden.
Eine unbeabsichtigte Folge der gemeinsamen Nutzung von Merkmalen ist die Anpassung des Modells an korrelierte Aufgaben innerhalb des Datensatzes, die so genannte negative Übertragung.  In diesem Beitrag greifen wir das Problem des negativen Transfers in Multitasking-Settings erneut auf und stellen fest, dass seine ätzenden Auswirkungen auf eine breite Palette von linearen und nichtlinearen Modellen, einschließlich neuronaler Netze, anwendbar sind.Wir untersuchen zunächst die Auswirkungen des negativen Transfers auf prinzipielle Weise und zeigen, dass zuvor vorgeschlagene Gegenmaßnahmen unzureichend sind, insbesondere für trainierbare Merkmale. Wir schlagen einen gegnerischen Trainingsansatz vor, um die Auswirkungen des negativen Transfers abzuschwächen, indem wir das Problem im Rahmen einer Domänenanpassung betrachten. Schließlich bestätigen empirische Ergebnisse zur Attributvorhersage-Multitasking auf AWA- und CUB-Datensätzen die Notwendigkeit einer Korrektur des negativen Austauschs in einer End-to-End-Umgebung.
Aktuelle theoretische und experimentelle Ergebnisse deuten auf die Möglichkeit hin, aktuelle und zukünftige Quanten-Hardware in anspruchsvollen Sampling-Aufgaben zu verwenden.In diesem Beitrag stellen wir das auf freier Energie basierende Verstärkungslernen (FERL) als eine Anwendung von Quanten-Hardware vor.Wir schlagen eine Methode zur Verarbeitung der gemessenen Qubit-Spinkonfigurationen eines Quanten-Vernetzers vor, um die freie Energie einer Quanten-Boltzmann-Maschine (QBM) zu approximieren. Die experimentellen Ergebnisse zeigen, dass unsere Technik eine vielversprechende Methode ist, um die Macht des Quanten-Samplings bei Aufgaben des Verstärkungslernens zu nutzen.
Deep-Learning-Modelle sind anfällig für gegnerische Beispiele, die durch die Anwendung von für den Menschen nicht wahrnehmbaren Störungen auf gutartige Eingaben erstellt werden, aber im Rahmen der Blackbox-Einstellung sind die meisten existierenden Gegner oft schlecht übertragbar, um andere Verteidigungsmodelle anzugreifen. In dieser Arbeit betrachten wir die Generierung von Gegenbeispielen als Optimierungsprozess und schlagen zwei neue Methoden vor, um die Übertragbarkeit von Gegenbeispielen zu verbessern, nämlich die Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) und die Scale-Invariant Attack Method (SIM).NI-FGSM zielt darauf ab, den beschleunigten Nesterov-Gradienten in die iterativen Angriffe zu integrieren, um die Übertragbarkeit von Gegenbeispielen zu verbessern. SIM basiert auf unserer Entdeckung der skaleninvarianten Eigenschaft von Deep-Learning-Modellen, die wir nutzen, um die gegnerischen Störungen über die Skalenkopien der Eingabebilder zu optimieren, um eine "Überanpassung" des angegriffenen White-Box-Modells zu vermeiden und übertragbare gegnerische Beispiele zu erzeugen. NI-FGSM und SIM können auf natürliche Weise integriert werden, um einen robusten gradientenbasierten Angriff aufzubauen, der mehr übertragbare Gegenbeispiele gegen die Verteidigungsmodelle generiert.Empirische Ergebnisse auf dem ImageNet-Datensatz zeigen, dass unsere Angriffsmethoden eine höhere Übertragbarkeit aufweisen und höhere Angriffserfolgsraten erzielen als modernste gradientenbasierte Angriffe.
In dieser Arbeit stellen wir eine probabilistische Trainingsmethode für neuronale Netze mit binären Gewichten und Aktivierungen vor, genannt PBNet. Durch die Einbeziehung der Stochastik während des Trainings umgehen wir die Notwendigkeit, den Gradienten von Funktionen zu approximieren, deren Ableitung fast immer Null ist, wie z.B. $\textrm{sign}(\cdot)$, und erhalten dennoch ein vollständig binäres neuronales Netzwerk zur Testzeit. Da alle Operationen in einer Schicht des PBNet auf Zufallsvariablen beruhen, führen wir stochastische Versionen der Batch-Normalisierung und des Max-Poolings ein, die sich gut auf ein deterministisches Netzwerk zur Testzeit übertragen lassen.  Wir evaluieren zwei verwandte Trainingsmethoden für das PBNet: eine, bei der die Aktivierungsverteilungen im gesamten Netz propagiert werden, und eine, bei der binäre Aktivierungen in jeder Schicht abgetastet werden Unsere Experimente zeigen, dass die Abtastung der binären Aktivierungen ein wichtiges Element für das stochastische Training von binären Neuronalen Netzen ist.
Das Ziel generativer Modelle ist es, die zugrunde liegende Datenverteilung eines stichprobenbasierten Datensatzes zu modellieren.Unsere Intuition ist, dass ein genaues Modell im Prinzip auch den stichprobenbasierten Datensatz als Teil seiner induzierten Wahrscheinlichkeitsverteilung einschließen sollte.Um dies zu untersuchen, betrachten wir vollständig trainierte generative Modelle unter Verwendung des GenerativeAdversarial Networks (GAN) Rahmens und analysieren den resultierenden Generator auf seine Fähigkeit, sich den Datensatz zu merken.Weiterhin zeigen wir, dass die Größe des initialen latenten Raums von entscheidender Bedeutung ist, um eine genaue Rekonstruktion der Trainingsdaten zu ermöglichen. Dies stellt eine Verbindung zur Kompressionstheorie her, bei der Autoencoder (AE) verwendet werden, um die Rekonstruktionsfähigkeiten unseres generativen Modells zu begrenzen.Hier beobachten wir ähnliche Ergebnisse wie beim Tradeoff zwischen Wahrnehmung und Verzerrung (Blau & Michaeli (2018)). Bei einem kleinen latenten Raum erzeugt das AE eine niedrige Qualität und das GAN eine hohe Qualität der Ergebnisse aus wahrnehmungsbezogener Sicht.Im Gegensatz dazu ist der Verzerrungsfehler für das AE kleiner.Wenn die Dimensionalität des latenten Raums erhöht wird, sinkt die Verzerrung für beide Modelle, aber die wahrnehmungsbezogene Qualität steigt nur für das AE.
Wir befassen uns mit dem Problem der Open-Set-Autorenschaftsüberprüfung, einer Klassifizierungsaufgabe, die darin besteht, Texte unbekannter Autorenschaft einem bestimmten Autor zuzuordnen, wenn die unbekannten Dokumente in der Testmenge aus der Trainingsmenge ausgeschlossen sind.Wir stellen einen durchgängigen Modellbildungsprozess vor, der universell auf eine Vielzahl von Korpora mit wenig bis gar keiner Modifikation oder Feinabstimmung anwendbar ist.Er beruht auf dem Transferlernen eines tiefen Sprachmodells und verwendet ein generatives adversarisches Netzwerk und eine Reihe von Textaugmentierungstechniken, um die Generalisierungsfähigkeit des Modells zu verbessern. Das Sprachmodell kodiert Dokumente bekannter und unbekannter Urheberschaft in einem bereichsinvarianten Raum, indem es Dokumentenpaare als Eingabe für den Klassifikator ausrichtet, während es sie getrennt hält. Die resultierenden Einbettungen werden verwendet, um ein Ensemble rekurrenter und quasirekurrenter neuronaler Netze zu trainieren. Die gesamte Pipeline ist bidirektional; die Ergebnisse des Vorwärts- und Rückwärtsdurchlaufs werden gemittelt. Wir führen Experimente mit vier traditionellen Datensätzen zur Verifizierung der Autorenschaft, einer Sammlung von Papieren, die durch maschinelles Lernen aus dem Internet gewonnen wurden, und einem großen Amazon-Reviews-Datensatz durch. Die experimentellen Ergebnisse übertreffen die Grundlagen und den aktuellen Stand der Technik und bestätigen den vorgeschlagenen Ansatz.
Wir betrachten die Lösung eines RL-Problems für einen einzelnen Agenten, indem wir es auf $n$ Lerner verteilen. Diese Lerner, die Berater genannt werden, versuchen, das Problem von einem anderen Fokus aus zu lösen. Wir zeigen, dass die lokale Planungsmethode für die Berater kritisch ist und dass keine der in der Literatur gefundenen Methoden fehlerfrei ist: die \textit{egocentric}-Planung überschätzt Werte von Zuständen, bei denen die anderen Berater nicht übereinstimmen, und die \textit{agnostic}-Planung ist ineffizient in der Nähe von Gefahrenzonen.Wir stellen einen neuartigen Ansatz vor, der \textit{empathic} genannt wird, und diskutieren seine theoretischen Aspekte.Wir untersuchen und validieren unsere theoretischen Erkenntnisse empirisch an einer Obstsammelaufgabe.
Wir stellen einen hybriden Rahmen vor, der den Kompromiss zwischen zeitlicher und frequenzbezogener Präzision in Audiodarstellungen nutzt, um die Leistung von Sprachverbesserungsaufgaben zu verbessern.Wir zeigen zunächst, dass herkömmliche Ansätze, die spezifische Darstellungen wie Roh-Audio und Spektrogramme verwenden, jeweils effektiv auf verschiedene Arten von Rauschen abzielen.Durch die Integration beider Ansätze kann unser Modell Multi-Skalen- und Multi-Domänen-Merkmale erlernen und effektiv Rauschen entfernen, das in verschiedenen Regionen des Zeit-Frequenz-Raums auf komplementäre Weise vorhanden ist.Experimentelle Ergebnisse zeigen, dass das vorgeschlagene hybride Modell eine bessere Leistung und Robustheit liefert als die Verwendung jedes Modells einzeln.
Die konsequente Überprüfung der statistischen Signifikanz von experimentellen Ergebnissen ist der erste obligatorische Schritt auf dem Weg zu einer reproduzierbaren Wissenschaft. In diesem Beitrag wird ein Leitfaden für rigorose Vergleiche von Verstärkungslernalgorithmen vorgestellt. Neben Simulationen vergleichen wir die empirischen Verteilungen, die wir durch die Ausführung von Soft-Actor Critic und Twin-Delayed Deep Deterministic Policy Gradient auf Half-Cheetah erhalten haben, und stellen abschließend Richtlinien und Code zur Verfügung, um strenge Vergleiche der Leistungen von RL-Algorithmen durchzuführen.
In der Information Bottleneck (IB), bei der Abstimmung der relativen Stärke zwischen Kompression und Vorhersage Begriffe, wie verhalten sich die beiden Begriffe, und was ist ihre Beziehung mit dem Datensatz und die gelernte Darstellung?In diesem Papier, wir versuchen, diese Fragen zu beantworten, indem mehrere Phasenübergänge in der IB Ziel: IB_β[p(z|x)] = I(X; Z) - βI(Y; Z), definiert auf der Kodierungsverteilung p(z|x) für Eingabe X, Ziel Y und Repräsentation Z, wobei plötzliche Sprünge von dI(Y; Z)/dβ und Vorhersagegenauigkeit mit zunehmendem β beobachtet werden. Wir führen eine Definition für IB-Phasenübergänge als qualitative Veränderung der IB-Verlustlandschaft ein und zeigen, dass die Übergänge dem Beginn des Lernens neuer Klassen entsprechen. Mit Hilfe der Variationsrechnung zweiter Ordnung leiten wir eine Formel ab, die eine praktische Bedingung für IB-Phasenübergänge darstellt, und stellen ihre Verbindung zur Fisher-Informationsmatrix für parametrisierte Modelle her. Wir bieten zwei Perspektiven, um die Formel zu verstehen, und zeigen, dass jeder IB-Phasenübergang eine Komponente der maximalen (nichtlinearen) Korrelation zwischen X und Y orthogonal zur gelernten Repräsentation ist, in enger Analogie zur kanonischen Korrelationsanalyse (CCA) in linearen Einstellungen.Basierend auf der Theorie stellen wir einen Algorithmus zur Entdeckung von Phasenübergangspunkten vor.Schließlich überprüfen wir, dass unsere Theorie und unser Algorithmus Phasenübergänge in kategorischen Datensätzen genau vorhersagen, den Beginn des Lernens neuer Klassen und die Klassenschwierigkeit in MNIST vorhersagen und markante Phasenübergänge in CIFAR10 vorhersagen.
Wir schlagen zwei Ansätze für lokal adaptive Aktivierungsfunktionen vor, nämlich schichtweise und neuronenweise lokal adaptive Aktivierungsfunktionen, die die Leistung von tiefen und physikalisch informierten neuronalen Netzen verbessern. Die Einführung der neuronenweisen Aktivierungsfunktion wirkt wie eine Vektoraktivierungsfunktion im Gegensatz zur traditionellen skalaren Aktivierungsfunktion, die durch feste, globale und schichtweise Aktivierungen gegeben ist. In numerischen Experimenten wird eine nichtlineare diskontinuierliche Funktion mit Hilfe eines tiefen neuronalen Netzes mit schichtweisen und neuronenweisen lokal adaptiven Aktivierungsfunktionen mit und ohne Steigungsrückgewinnungsterm approximiert und mit dem globalen Gegenstück verglichen, und die Lösung der nichtlinearen Burgergleichung, die steile Gradienten aufweist, wird ebenfalls mit den vorgeschlagenen Methoden erzielt. Auf der theoretischen Seite beweisen wir, dass in der vorgeschlagenen Methode die Gradientenabstiegsalgorithmen nicht zu suboptimalen kritischen Punkten oder lokalen Minima unter praktischen Bedingungen für die Initialisierung und Lernrate angezogen werden. Darüber hinaus wird gezeigt, dass die vorgeschlagenen adaptiven Aktivierungsfunktionen mit der Steigungsrückgewinnung den Trainingsprozess in standardmäßigen Deep-Learning-Benchmarks mit CIFAR-10, CIFAR-100, SVHN, MNIST, KMNIST, Fashion-MNIST und Semeion-Datensätzen mit und ohne Datenerweiterung beschleunigen.
Das Lernen in Gauß'schen Prozessmodellen erfolgt durch die Anpassung von Hyperparametern des Mittelwerts und der Kovarianzfunktion, wobei der klassische Ansatz die Maximierung der Grenzwahrscheinlichkeit beinhaltet, die zu Festpunktschätzungen führt (ein Ansatz, der als Typ II Maximum Likelihood oder ML-II bezeichnet wird). Ein alternatives Lernverfahren besteht darin, das Posterior über Hyperparameter in einer hierarchischen Spezifikation von GPs abzuleiten, die wir Fully Bayesian Gaussian Process Regression (GPR) nennen. In dieser Arbeit werden zwei Annäherungen an das schwierige Hyperparameter-Posterior betrachtet: 1) Hamiltonian Monte Carlo (HMC), das eine auf Stichproben basierende Annäherung liefert, und2) Variational Inference (VI), bei der das Posterior über Hyperparameter durch einen faktorisierten Gauß (Mean-Field) oder einen vollwertigen Gauß, der Korrelationen zwischen Hyperparametern berücksichtigt, angenähert wird.Wir analysieren die Vorhersageleistung für Fully Bayesian GPR auf einer Reihe von Benchmark-Datensätzen.
Damit solche Modelle eine komprimierte binäre Sequenz erzeugen können, die die universelle Datendarstellung in einer digitalen Welt ist, muss die latente Darstellung einer Entropiecodierung unterzogen werden. Die Bereichscodierung als Entropiecodierungstechnik ist optimal, kann aber katastrophal versagen, wenn sich die Berechnung des Priors auf der Sende- und der Empfangsseite auch nur geringfügig unterscheidet. Leider ist dies ein häufiges Szenario, wenn Fließkommamathematik verwendet wird und Sender und Empfänger auf unterschiedlichen Hardware- oder Softwareplattformen arbeiten, da die numerische Abrundung oft plattformabhängig ist. Wir schlagen die Verwendung ganzzahliger Netzwerke als universelle Lösung für dieses Problem vor und zeigen, dass sie eine zuverlässige plattformübergreifende Kodierung und Dekodierung von Bildern mit Variationsmodellen ermöglichen.
Neuronale Netze, die mit externem Speicher betrieben werden, simulieren das Verhalten von Computern und können Algorithmen und andere komplexe Aufgaben erlernen, indem sie den Speicher nutzen, um Daten für einen neuronalen Controller zu speichern.In dieser Arbeit führen wir einen neuen Speicher ein, um Gewichte für den Controller zu speichern, analog zum Speicher für gespeicherte Programme in modernen Computerarchitekturen. Das vorgeschlagene Modell, das wir Neuronaler Programmspeicher nennen, ergänzt die derzeitigen speichererweiterten neuronalen Netze und schafft differenzierbare Maschinen, die Programme im Laufe der Zeit umschalten und sich an variable Kontexte anpassen können und damit der Universellen Turing-Maschine voll und ganz ähneln.Eine breite Palette von Experimenten zeigt, dass die resultierenden Maschinen nicht nur bei klassischen algorithmischen Problemen brillieren, sondern auch das Potenzial für kompositorische, kontinuierliche, kurzzeitige Lern- und Fragebeantwortungsaufgaben haben.
Wir zeigen, dass man in der Regel die gleiche Lernkurve auf Trainings- und Testsätzen erhalten kann, indem man stattdessen die Stapelgröße während des Trainings erhöht. Dieses Verfahren ist erfolgreich für stochastischen Gradientenabstieg (SGD), SGD mit Momentum, Nesterov-Momentum und Adam. Es erreicht äquivalente Testgenauigkeiten nach der gleichen Anzahl von Trainingsepochen, aber mit weniger Parameteraktualisierungen, was zu größerer Parallelität und kürzeren Trainingszeiten führt. Wir können die Anzahl der Parameteraktualisierungen weiter reduzieren, indem wir die Lernrate $\epsilon$ erhöhen und die Stapelgröße $B \propto \epsilon$ skalieren.Schließlich kann man den Impulskoeffizienten $m$ erhöhen und $B \propto auf 1/(1-m)$ skalieren, obwohl dies die Testgenauigkeit tendenziell leicht verringert.Entscheidend ist, dass unsere Verfahren es uns ermöglichen, bestehende Trainingspläne für das Training großer Stapel ohne Hyper-Parameter-Abstimmung wiederzuverwenden.Wir trainieren ResNet-50 auf ImageNet bis zu einer Validierungsgenauigkeit von 76,1% in weniger als 30 Minuten.
Traditionelle Modelle für die Beantwortung von Fragen optimieren mit Hilfe von Cross-Entropie-Verlusten, die exakte Antworten auf Kosten der Bestrafung von nahegelegenen oder sich überschneidenden Antworten fördern, die manchmal genauso genau sind.Wir schlagen ein gemischtes Ziel vor, das Cross-Entropie-Verluste mit selbstkritischem Policy-Lernen kombiniert und Belohnungen verwendet, die von Wortüberschneidungen abgeleitet sind, um die Fehlanpassung zwischen Bewertungsmetrik und Optimierungsziel zu lösen. Unsere Vorschläge verbessern die Leistung des Modells bei allen Fragetypen und Eingabelängen, insbesondere bei langen Fragen, die die Fähigkeit zur Erfassung langfristiger Abhängigkeiten erfordern. Auf dem Stanford Question Answering Dataset erreicht unser Modell mit 75,1 % exakter Treffergenauigkeit und 83,1 % F1 den Stand der Technik, während das Ensemble 78,9 % exakte Treffergenauigkeit und 86,0 % F1 erreicht.
Die neuronale Architektursuche (NAS) hat in den letzten Jahren in einer Vielzahl von Anwendungen bahnbrechende Erfolge erzielt.Es könnte an der Zeit sein, einen Schritt zurückzutreten und die guten und schlechten Aspekte auf dem Gebiet der NAS zu analysieren.Eine Vielzahl von Algorithmen suchen Architekturen in verschiedenen Suchräumen.Diese gesuchten Architekturen werden mit verschiedenen Setups trainiert, z.B., Dies wirft ein Vergleichsproblem auf, wenn man die Leistung verschiedener NAS-Algorithmen vergleicht.NAS-Bench-101 hat sich als erfolgreich erwiesen, um dieses Problem zu lindern.In dieser Arbeit schlagen wir eine Erweiterung von NAS-Bench-101 vor: NAS-Bench-201 mit einem anderen Suchraum, Ergebnissen auf mehreren Datensätzen und mehr diagnostischen Informationen.NAS-Bench-201 hat einen festen Suchraum und bietet einen einheitlichen Benchmark für fast alle aktuellen NAS-Algorithmen. Das Design unseres Suchraums ist inspiriert von demjenigen, das in den populärsten zellbasierten Suchalgorithmen verwendet wird, wo eine Zelle als gerichteter azyklischer Graph dargestellt wird. Jede Kante ist hier mit einer Operation verbunden, die aus einem vordefinierten Operationsset ausgewählt wird. Um für alle NAS-Algorithmen anwendbar zu sein, umfasst der in NAS-Bench-201 definierte Suchraum alle möglichen Architekturen, die durch 4 Knoten und 5 zugehörige Operationsoptionen erzeugt werden, was zu insgesamt 15.625 neuronalen Zellkandidaten führt. Dies ermöglicht es Forschern, unnötiges wiederholtes Training für ausgewählte Architekturen zu vermeiden und sich ausschließlich auf den Suchalgorithmus selbst zu konzentrieren. Die eingesparte Trainingszeit für jede Architektur verbessert auch die Effizienz der meisten NAS-Algorithmen und stellt eine rechenkostenfreundlichere NAS-Gemeinschaft für ein breiteres Spektrum von Forschern dar. Um die vorgeschlagene NAS-Bench-102 weiter zu unterstützen, haben wir sie unter vielen Aspekten analysiert und 10 aktuelle NAS-Algorithmen einem Benchmarking unterzogen, was ihre Anwendbarkeit bestätigt.
Generative Adversarial Networks (GANs) sind eines der populärsten Werkzeuge für das Lernen komplexer hochdimensionaler Verteilungen, aber die Generalisierungseigenschaften von GANs sind noch nicht gut verstanden worden.In diesem Papier analysieren wir die Generalisierung von GANs in praktischen Einstellungen.Wir zeigen, dass Diskriminatoren, die auf diskreten Datensätzen mit dem ursprünglichen GAN-Verlust trainiert wurden, eine schlechte Generalisierungsfähigkeit haben und den theoretisch optimalen Diskriminator nicht annähern. Wir schlagen eine null-zentrierte Gradientenstrafe vor, um die Generalisierung des Diskriminators zu verbessern, indem wir ihn in Richtung des optimalen Diskriminators schieben.Die Strafe garantiert die Generalisierung und Konvergenz von GANs.Experimente auf synthetischen und großen Datensätzen verifizieren unsere theoretische Analyse.
Generative adversarial networks (GANs) sind eine Klasse von tiefen generativen Modellen, die darauf abzielen, eine Zielverteilung in einer unbeaufsichtigten Art und Weise zu erlernen. Während sie erfolgreich auf viele Probleme angewandt wurden, ist das Training eines GANs eine notorisch herausfordernde Aufgabe und erfordert eine beträchtliche Menge an Hyperparameter-Abstimmung, neuronales Architektur-Engineering und eine nicht-triviale Menge von ``Tricks''. In dieser Arbeit nehmen wir einen nüchternen Blick auf den aktuellen Stand der GANs aus einer praktischen Perspektive.Wir reproduzieren den aktuellen Stand der Technik und gehen über eine faire Erkundung der GAN-Landschaft hinaus.Wir diskutieren häufige Fallstricke und Reproduzierbarkeit Probleme, Open-Source unseren Code auf Github, und bieten pre-trained Modelle auf TensorFlow Hub.
Da Deep Reinforcement Learning (RL) auf immer mehr Aufgaben angewandt wird, besteht die Notwendigkeit, das Verhalten der gelernten Agenten zu visualisieren und zu verstehen. Saliency Maps erklären das Verhalten der Agenten, indem sie die Merkmale des Eingabezustands hervorheben, die für den Agenten bei der Durchführung einer Aktion am wichtigsten sind. Unser Ansatz generiert fokussiertere Saliency Maps, indem er zwei Aspekte (Spezifität und Relevanz) ausbalanciert, die verschiedene Desiderata der Saliency erfassen: Der erste erfasst die Auswirkung der Störung auf die relative erwartete Belohnung der zu erklärenden Aktion.  Der zweite Aspekt gewichtet irrelevante Merkmale, die den relativen erwarteten Nutzen anderer Handlungen als der zu erklärenden Handlung verändern, herunter.  Wir vergleichen unseren Ansatz mit bestehenden Ansätzen für Agenten, die für das Spielen von Brettspielen (Schach und Go) und Atari-Spielen (Breakout, Pong und Space Invaders) trainiert wurden.  Anhand von anschaulichen Beispielen (Schach, Atari, Go), menschlichen Studien (Schach) und automatisierten Bewertungsmethoden (Schach) zeigen wir, dass unser Ansatz Auffälligkeitskarten erzeugt, die für Menschen besser interpretierbar sind als bestehende Ansätze.
Um das Innenleben tiefer neuronaler Netze zu verstehen und mögliche theoretische Erklärungen zu liefern, untersuchen wir die tiefen Repräsentationen anhand der untrainierten CNN-DCN-Architektur mit zufälliger Gewichtung.CNN bezeichnet als Faltungs-Autocodierer den Teil eines neuronalen Faltungsnetzes, der von der Eingabe bis zu einer Faltungs-Zwischenschicht reicht, und DCN den entsprechenden dekonvolutionären Teil.Im Vergleich zum DCN-Training für vortrainierte CNN konvergiert das Training des DCN für CNN mit zufälliger Gewichtung schneller und führt zu einer qualitativ besseren Bildrekonstruktion. Um einen besseren Einblick in die zwischengeschaltete Zufallsdarstellung zu erhalten, untersuchen wir die Auswirkungen der Netzwerkbreite im Vergleich zur Tiefe, die Anzahl der Zufallskanäle und die Größe der Zufallskerne auf die Rekonstruktionsqualität und liefern theoretische Begründungen für empirische Beobachtungen.Wir bieten außerdem eine schnelle Stilübertragungsanwendung, die die CNN-DCN-Architektur mit Zufallsgewicht verwendet, um das Potenzial unserer Beobachtung zu zeigen.
 Der derzeitige Kompromiss zwischen Tiefe und Rechenkosten macht es schwierig, tiefe neuronale Netze für viele industrielle Anwendungen zu übernehmen, insbesondere wenn die Rechenleistung begrenzt ist.Hier sind wir von der Idee inspiriert, dass, während tiefere Einbettungen benötigt werden, um schwierige Proben zu unterscheiden, eine große Anzahl von Proben können gut über viel flacher Einbettungen unterschieden werden. In dieser Studie führen wir das Konzept der Entscheidungsgatter (d-gate) ein, Module, die darauf trainiert sind, zu entscheiden, ob eine Probe in eine tiefere Einbettung projiziert werden muss oder ob eine frühe Vorhersage am d-gate gemacht werden kann, wodurch die Berechnung dynamischer Repräsentationen in verschiedenen Tiefen ermöglicht wird.  Die vorgeschlagenen d-gate-Module können in jedes tiefe neuronale Netz integriert werden und reduzieren die durchschnittlichen Rechenkosten der tiefen neuronalen Netze unter Beibehaltung der Modellierungsgenauigkeit. Experimentelle Ergebnisse zeigen, dass die Nutzung der vorgeschlagenen d-gate-Module zu einer ~38%igen Beschleunigung und ~39%igen FLOPS-Reduzierung bei ResNet-101 und zu einer ~46%igen Beschleunigung und $\sim$36\%igen FLOPS-Reduzierung bei DenseNet-201 führte, das auf dem CIFAR10-Datensatz trainiert wurde, wobei die Genauigkeit nur um ~2% abnahm.
Wir betrachten neuronale Netze mit einer versteckten Schicht und zeigen, dass beim Lernen von symmetrischen Funktionen die Anfangsbedingungen so gewählt werden können, dass das Standard-SGD-Training effizient Generalisierungsgarantien erzeugt.Wir verifizieren dies empirisch und zeigen, dass dies nicht gilt, wenn die Anfangsbedingungen zufällig gewählt werden.Der Nachweis der Konvergenz untersucht die Interaktion zwischen den beiden Schichten des Netzes.Unsere Ergebnisse unterstreichen die Bedeutung der Verwendung von Symmetrie bei der Entwicklung neuronaler Netze.
Die Suche nach Architekturen zielt darauf ab, automatisch neuronale Architekturen zu finden, die mit den von menschlichen Experten entworfenen Architekturen konkurrieren können.Während die jüngsten Ansätze eine hochmoderne Vorhersageleistung für die Bilderkennung erreicht haben, sind sie aus zwei Gründen problematisch, wenn die Ressourcen begrenzt sind: (1) die gefundenen neuronalen Architekturen sind ausschließlich auf eine hohe Vorhersageleistung optimiert, ohne dass ein übermäßiger Ressourcenverbrauch bestraft wird; (2) die meisten Methoden zur Architektursuche erfordern enorme Rechenressourcen.Wir beheben das erste Manko, indem wir LEMONADE vorschlagen, einen evolutionären Algorithmus für die multizentrische Architektursuche, der eine Annäherung an die Pareto-Front von Architekturen unter mehreren Zielen, wie Vorhersageleistung und Anzahl der Parameter, in einem einzigen Durchlauf der Methode ermöglicht. Den zweiten Mangel beheben wir, indem wir einen Lamarck'schen Vererbungsmechanismus für LEMONADE vorschlagen, der Kindernetzwerke generiert, die mit der Vorhersageleistung ihrer trainierten Eltern warmgestartet werden, indem wir (approximative) Netzwerk-Morphismus-Operatoren zur Generierung von Kindern verwenden. Die Kombination dieser beiden Beiträge ermöglicht es, Modelle zu finden, die mit unterschiedlich großen NASNets, MobileNets, MobileNets V2 und Wide Residual Networks auf CIFAR-10 und ImageNet64x64 innerhalb von nur einer Woche auf acht GPUs gleichziehen oder diese sogar übertreffen. Das ist etwa 20-40x weniger Rechenleistung als frühere Architektur-Suchmethoden, die State-of-the-Art-Leistung liefern.
Wir gehen zwei Herausforderungen der probabilistischen Themenmodellierung an, um die Wahrscheinlichkeit eines Wortes in einem gegebenen Kontext, d.h. P(wordjcontext), besser zu schätzen: (1) Keine Sprachstruktur im Kontext: Probabilistische Themenmodelle ignorieren die Wortreihenfolge, indem sie einen gegebenen Kontext als "bag-of-word" zusammenfassen, und folglich geht die Semantik der Wörter im Kontext verloren.In dieser Arbeit beziehen wir die Sprachstruktur ein, indem wir ein neuronales autoregressives Themenmodell (TM) mit einem LSTM-basierten Sprachmodell (LSTM-LM) in einem einzigen probabilistischen Rahmen kombinieren. Das LSTM-LM lernt eine Vektorraumrepräsentation jedes Worts, indem es die Wortreihenfolge in lokalen Kollokationsmustern berücksichtigt, während das TM gleichzeitig eine latente Repräsentation aus dem gesamten Dokument lernt, Wir vereinen zwei komplementäre Paradigmen des Lernens der Bedeutung von Wortvorkommen durch die Kombination eines Themenmodells und eines Sprachmodells in einem einheitlichen probabilistischen Rahmen, der als ctx-DocNADE bezeichnet wird.(2) Begrenzter Kontext und/oder kleineres Trainingskorpus von Dokumenten: In Situationen mit einer geringen Anzahl von Wortvorkommen (d.h., Wir gehen diese Herausforderung an, indem wir externes Wissen in neuronale autoregressive Themenmodelle über einen Sprachmodellierungsansatz einbeziehen: Wir verwenden Worteinbettungen als Input eines LSTM-LMs mit dem Ziel, die Wort-Themen-Zuordnung auf einem kleineren und/oder kurzen Textkorpus zu verbessern. Die vorgeschlagene DocNADE-Erweiterung wird als ctx-DocNADEe bezeichnet. Wir präsentieren neuartige neuronale autoregressive Themenmodellvarianten, die mit neuronalen Sprachmodellen und Einbettungsprioren gekoppelt sind und die den Stand der Technik bei generativen Themenmodellen in Bezug auf Generalisierung (Perplexität), Interpretierbarkeit (Themenkohärenz) und Anwendbarkeit (Abruf und Klassifizierung) über 6 Langtext- und 8 Kurztext-Datensätze aus verschiedenen Domänen durchweg übertreffen.
Modellkompressionsmethoden reduzieren effektiv den Speicherbedarf dieser Modelle, in der Regel durch die Anwendung von Transformationen wie Weight Pruning oder Quantisierung. In diesem Papier stellen wir ein neuartiges Schema für verlustbehaftete Gewichtskodierung vor, das herkömmliche Kompressionstechniken ergänzt. Die Kodierung basiert auf dem Bloomier-Filter, einer probabilistischen Datenstruktur, die auf Kosten der Einführung von Zufallsfehlern Platz sparen kann. Durch Ausnutzung der Fähigkeit neuronaler Netze, diese Unvollkommenheiten zu tolerieren und durch Umschulung um die Fehler herum, kann die vorgeschlagene Technik, Weightless, DNN-Gewichte um das bis zu 496-fache komprimieren; bei gleicher Modellgenauigkeit führt dies zu einer bis zu 1,51-fachen Verbesserung gegenüber dem Stand der Technik.
Neuere Arbeiten haben gezeigt, dass kontextualisierte Wortrepräsentationen, die aus der neuronalen maschinellen Übersetzung (NMT) abgeleitet werden, eine brauchbare Alternative zu solchen aus einfachen Wortvorhersageaufgaben sind, da das interne Verständnis, das aufgebaut werden muss, um von einer Sprache in eine andere übersetzen zu können, viel umfassender ist. Leider können NMT-Modelle aufgrund von Rechen- und Speicherbeschränkungen derzeit keine großen Wortvokabulare verwenden, so dass Alternativen wie Teilworteinheiten (BPE und morphologische Segmentierungen) und Zeichen verwendet werden.Wir untersuchen hier die Auswirkungen der Verwendung verschiedener Arten von Einheiten auf die Qualität der resultierenden Darstellungen, wenn sie zur Modellierung von Syntax, Semantik und Morphologie verwendet werden.  Wir fanden heraus, dass Repräsentationen, die aus Teilwörtern abgeleitet werden, etwas besser für die Modellierung der Syntax sind, während zeichenbasierte Repräsentationen für die Modellierung der Morphologie besser geeignet sind und auch robuster gegenüber verrauschten Eingaben sind.
Few-Shot-Lernen ist der Prozess des Lernens neuer Klassen mit nur wenigen Beispielen und es bleibt eine herausfordernde Aufgabe im maschinellen Lernen. Viele hochentwickelte Few-Shot-Lernalgorithmen wurden vorgeschlagen, die auf der Vorstellung basieren, dass Netzwerke sich leicht an neue Beispiele anpassen können, wenn sie einfach mit nur wenigen Beispielen fein abgestimmt werden. In dieser Studie zeigen wir, dass die Fine-Tuning-Methode in dem häufig verwendeten niedrig aufgelösten mini-ImageNet-Datensatz eine höhere Genauigkeit als die üblichen few-shot-Lernalgorithmen in der 1-shot-Aufgabe und fast die gleiche Genauigkeit wie die des State-of-the-Art-Algorithmus in der 5-shot-Aufgabe erreicht. Bei beiden Aufgaben zeigen wir, dass unsere Methode eine höhere Genauigkeit erreicht als herkömmliche few-shot Lernalgorithmen. Wir analysieren die experimentellen Ergebnisse weiter und zeigen, dass:1) der Umschulungsprozess durch die Verwendung einer niedrigen Lernrate stabilisiert werden kann,2) die Verwendung von adaptiven Gradientenoptimierern während der Feinabstimmung die Testgenauigkeit erhöhen kann, und3) die Testgenauigkeit durch die Aktualisierung des gesamten Netzwerks verbessert werden kann, wenn eine große Domänenverschiebung zwischen Basis- und neuen Klassen besteht.
Wir schlagen einen Ansatz zur Generierung realistischer und originalgetreuer Börsendaten auf der Grundlage generativer adverser Netzwerke vor. Wir modellieren den Auftragsstrom als stochastischen Prozess mit endlicher historischer Abhängigkeit und verwenden ein bedingtes Wasserstein-GAN, um die historische Abhängigkeit von Aufträgen auf einem Aktienmarkt zu erfassen. Wir testen unseren Ansatz mit tatsächlichen Marktdaten und synthetischen Daten zu einer Reihe verschiedener Statistiken und stellen fest, dass die generierten Daten den realen Daten sehr nahe kommen.
Wir stellen einen neuartigen Black-Box-Angriffsalgorithmus mit modernsten Modellumgehungsraten für die Abfrageeffizienz unter $\ell_\infty$ und $\ell_2$ Metriken vor. Er nutzt einen \textit{sign-based}, statt magnitudenbasierten, Gradientenschätzungsansatz, der die Gradientenschätzung von kontinuierlicher zu binärer Black-Box-Optimierung verschiebt. Es konstruiert adaptiv Abfragen, um den Gradienten zu schätzen, wobei eine Abfrage auf der vorherigen aufbaut, anstatt den Gradienten bei jedem Schritt mit einer zufälligen Abfragekonstruktion neu zu schätzen, wobei die Verwendung von Vorzeichenbits zu einem geringeren Speicherbedarf führt und weder eine Abstimmung der Hyperparameter noch eine Dimensionalitätsreduktion erforderlich ist. Darüber hinaus ist seine theoretische Leistung garantiert und er kann gegnerische Unterräume besser charakterisieren als White-Box-Gradienten-ausgerichtete Unterräume. Bei zwei öffentlichen Black-Box-Angriffsherausforderungen und einem Modell, das robust gegen Transferangriffe trainiert wurde, übertreffen die Umgehungsraten des Algorithmus alle eingereichten Angriffe. Für eine Reihe veröffentlichter Modelle ist der Algorithmus 3,8-mal$ weniger fehleranfällig und benötigt 2,5-mal$ weniger Abfragen als die beste Kombination von Algorithmen nach dem Stand der Technik. So umgeht er beispielsweise ein Standard-MNIST-Modell mit durchschnittlich nur 12$ Abfragen.
Rekurrente Neuronale Netze (RNNs) sind weit verbreitete Modelle für Sequenzdaten.wie bei Feedforward-Netzen ist es üblich geworden, "tiefe" RNNs zu bauen, d.h. mehrere rekurrente Schichten zu stapeln, um Abstraktionen der Daten auf höherer Ebene zu erhalten.dies funktioniert jedoch nur für eine Handvoll Schichten.im Gegensatz zu Feedforward-Netzen schadet das Stapeln von mehr als ein paar rekurrenten Einheiten (z.B., Wir untersuchen das Training von mehrschichtigen RNNs und untersuchen die Größe der Gradienten, während sie sich durch das Netzwerk ausbreiten. Wir zeigen, dass, abhängig von der Struktur der rekurrenten Basiseinheit, die Gradienten systematisch abgeschwächt oder verstärkt werden, so dass sie mit zunehmender Tiefe dazu neigen, zu verschwinden bzw. zu explodieren. Basierend auf unserer Analyse entwerfen wir eine neue Art von Gated Cell, die die Gradientengröße besser bewahrt und es daher ermöglicht, tiefere RNNs zu trainieren. Wir validieren unser Design experimentell mit fünf verschiedenen Sequenzmodellierungsaufgaben auf drei verschiedenen Datensätzen. Die vorgeschlagene stapelbare rekurrente (STAR) Zelle ermöglicht wesentlich tiefere rekurrente Architekturen mit verbesserter Leistung.
Trotz des empirischen Erfolges sind die theoretischen Grundlagen der Stabilität, Konvergenz und Beschleunigungseigenschaften der Batch-Normalisierung (BN) nach wie vor schwer fassbar. In diesem Papier gehen wir dieses Problem von einem Modellierungsansatz aus an, indem wir eine gründliche theoretische Analyse der BN durchführen, die auf ein vereinfachtes Modell angewendet wird: die gewöhnlichen kleinsten Quadrate (OLS). Wir entdecken, dass der Gradientenabstieg auf OLS mit BN interessante Eigenschaften hat, darunter ein Skalierungsgesetz, Konvergenz für beliebige Lernraten für die Gewichte, asymptotische Beschleunigungseffekte sowie Unempfindlichkeit gegenüber der Wahl der Lernraten.wir zeigen dann numerisch, dass diese Erkenntnisse nicht spezifisch für das OLS-Problem sind und qualitativ für komplexere überwachte Lernprobleme gelten.dies weist auf eine neue Richtung zur Aufdeckung der mathematischen Prinzipien, die der Batch-Normalisierung zugrunde liegen.
Da das Ziel des Agenten darin besteht, die akkumulierte Belohnung zu maximieren, lernt er oft, Schlupflöcher und Fehlspezifikationen im Belohnungssignal auszunutzen, was zu unerwünschtem Verhalten führt. In dieser Arbeit stellen wir einen neuartigen Ansatz für die Optimierung eingeschränkter Richtlinien vor, der als "Reward Constrained Policy Optimization" (RCPO) bezeichnet wird und ein alternatives Bestrafungssignal verwendet, um die Richtlinie in Richtung einer zufriedenstellenden Richtlinie zu lenken.Wir beweisen die Konvergenz unseres Ansatzes und liefern empirische Beweise für seine Fähigkeit, zufriedenstellende Richtlinien zu trainieren.
Das Handheld Virtual Panel (HVP) ist ein virtuelles Panel, das in der Virtuellen Realität (VR) an den Controller der nicht-dominanten Hand angeschlossen wird und die gängige Technik ist, um Menüs und Toolboxen in VR-Geräten zu aktivieren. In diesem Beitrag untersuchen wir die Zielerfassungsleistung für das HVP in Abhängigkeit von vier Faktoren: Zielbreite, Zielentfernung, Annäherungsrichtung in Bezug auf die Schwerkraft und Annäherungswinkel.Unsere Ergebnisse zeigen, dass alle vier Faktoren signifikante Auswirkungen auf die Benutzerleistung haben.Auf der Grundlage der Ergebnisse schlagen wir Richtlinien für die ergonomische und leistungsfähige Gestaltung der HVP-Schnittstellen vor.
Wir stellen fest, dass viele erfolgreiche Netzwerke dennoch oft eine große Anzahl redundanter Kanten enthalten, von denen viele einen vernachlässigbaren Beitrag zur Gesamtleistung des Netzwerks leisten.In diesem Beitrag schlagen wir ein neuartiges iSparse-Framework vor und zeigen experimentell, dass wir das Netzwerk um 30-50% sparsamer gestalten können, ohne die Netzwerkleistung zu beeinträchtigen. Darüber hinaus kann iSparse sowohl während des Trainings eines Modells als auch auf ein bereits trainiertes Modell angewandt werden, was es zu einem umtrainingsfreien Ansatz macht und zu einem minimalen Rechenaufwand führt. Vergleiche von iSparse mit PFEC, NISP, DropConnect und Retraining-Free auf Benchmark-Datensätzen zeigen, dass iSparse zu effektiven Netzwerksparsifizierungen führt.
Sprachmodellierungsaufgaben, bei denen Wörter auf der Grundlage eines lokalen Kontexts vorhergesagt werden, haben sich als sehr effektiv für das Lernen von Worteinbettungen und kontextabhängigen Repräsentationen von Phrasen erwiesen. Motiviert durch die Beobachtung, dass Bemühungen, Wissen aus der Welt in maschinenlesbare Wissensdatenbanken zu kodieren, dazu tendieren, entitätszentriert zu sein, untersuchen wir die Verwendung einer Lückentextaufgabe, um kontextunabhängige Repräsentationen von Entitäten aus den Kontexten zu lernen, in denen diese Entitäten erwähnt wurden. Wir zeigen, dass das Training von neuronalen Modellen in großem Maßstab es uns ermöglicht, extrem genaue Informationen über die Typisierung von Entitäten zu erlernen, was wir mit der Rekonstruktion von Wikipedia-Kategorien mit wenigen Aufnahmen demonstrieren.
Variationale Autokodierer (VAEs) sind erfolgreich beim Lernen einer niedrigdimensionalen Mannigfaltigkeit aus hochdimensionalen Daten mit komplexen Abhängigkeiten.in ihrem Kern bestehen sie aus einem leistungsstarken Bayes'schen probabilistischen Inferenzmodell, um die hervorstechenden Merkmale der Daten zu erfassen.beim Training nutzen sie die Leistungsfähigkeit der variationalen Inferenz, indem sie eine untere Schranke für die Modellevidenz optimieren.die latente Repräsentation und die Leistung von VAEs werden stark von der Art der als Kostenfunktion verwendeten Schranke beeinflusst. Durch die Nutzung des q-deformierten Logarithmus in den traditionellen unteren Schranken ELBO und IWAE und der oberen Schranke CUBO leisten wir einen Beitrag zu dieser Richtung der Forschung. In dieser Proof-of-Concept-Studie untersuchen wir verschiedene Möglichkeiten zur Erstellung dieser q-deformierten Schranken, die enger sind als die klassischen Schranken, und wir zeigen Verbesserungen in der Leistung solcher VAEs auf dem binarisierten MNIST-Datensatz.
Breiman zeigt jedoch ein Dilemma auf (Breiman, 1999), dass eine einheitliche Verbesserung der Randverteilung nicht notwendigerweise den Generalisierungsfehler reduziert. In diesem Beitrag greifen wir Breimans Dilemma in tiefen neuronalen Netzen mit kürzlich vorgeschlagenen normalisierten Rändern unter Verwendung von Lipschitz-Konstanten auf, die durch spektrale Normprodukte gebunden sind. Anhand einer vereinfachten Theorie und umfangreicher Experimente wird gezeigt, dass Breimans Dilemma auf der Dynamik normalisierter Randverteilungen beruht, die den Kompromiss zwischen der Ausdrucksstärke des Modells und der Komplexität der Daten widerspiegelt. Wenn die Komplexität der Daten mit der Ausdrucksstärke des Modells in dem Sinne vergleichbar ist, dass die Trainings- und Testdaten ähnliche Phasenübergänge in der Dynamik der normalisierten Margen aufweisen, werden zwei effiziente Wege über klassische margenbasierte Generalisierungsgrenzen abgeleitet, um den Trend des Generalisierungsfehlers erfolgreich vorherzusagen.Andererseits können überexprimierte Modelle, die gleichmäßige Verbesserungen der normalisierten Margen beim Training aufweisen, eine solche Vorhersagekraft verlieren und das Overfitting nicht verhindern. 
Es war eine offene Forschungsherausforderung, ein durchgängiges aufgabenorientiertes Multidomänen-Dialogsystem zu entwickeln, in dem ein Mensch mit dem Dialogagenten konversieren kann, um Aufgaben in mehr als einer Domäne zu erledigen.Erstens ist die Verfolgung von Glaubenszuständen von Multidomänendialogen schwierig, da der Dialogagent die vollständigen Glaubenszustände von allen relevanten Domänen erhalten muss, von denen jede sowohl gemeinsame Slots für alle Domänen als auch einzigartige Slots nur für die Domäne haben kann. Zweitens muss der Dialogagent auch verschiedene Arten von Informationen verarbeiten, einschließlich Kontextinformationen aus dem Dialogkontext, dekodierte Dialogzustände der aktuellen Dialogrunde und abgefragte Ergebnisse aus einer Wissensdatenbank, um kontextbewusste und aufgabenspezifische Antworten für den Menschen semantisch zu gestalten. Um diese Herausforderungen zu bewältigen, schlagen wir eine neuronale End-to-End-Architektur für aufgabenorientierte Dialoge in mehreren Domänen vor. Wir schlagen einen neuartigen Multi-level Neural Belief Tracker vor, der die Glaubenszustände des Dialogs verfolgt, indem er Signale sowohl auf Slot- als auch auf Domänenebene unabhängig voneinander lernt. In Anlehnung an jüngste Arbeiten im Bereich der End-to-End-Dialogsysteme integrieren wir den Belief Tracker mit Generierungskomponenten, um End-to-End-Dialogaufgaben zu lösen. 50,91% gemeinsame Zielgenauigkeit und konkurrenzfähige Ergebnisse bei der Aufgabenerfüllung und der Generierung von Antworten sind das Ergebnis der MultiWOZ2.1 Benchmark.
Score-Matching bietet einen effektiven Ansatz zum Lernen flexibler unnormalisierter Modelle, aber seine Skalierbarkeit ist durch die Notwendigkeit, eine Ableitung zweiter Ordnung auszuwerten, begrenzt.  Diese Verbindung ermöglicht es uns, eine skalierbare Annäherung an diese Ziele zu entwerfen, mit einer Form, die der einstufigen kontrastiven Divergenz ähnelt. Wir stellen Anwendungen für das Training von impliziten Variations- und Wasserstein-Autocodierern mit mannigfaltigen Prioren vor.
Dieses Papier entwickelt variationales kontinuierliches Lernen (VCL), ein einfaches, aber allgemeines Rahmenwerk für kontinuierliches Lernen, das Online-Variationsinferenz (VI) und jüngste Fortschritte in der Monte-Carlo-VI für neuronale Netze vereint.Das Rahmenwerk kann erfolgreich sowohl tiefe diskriminative Modelle als auch tiefe generative Modelle in komplexen kontinuierlichen Lernumgebungen trainieren, in denen sich bestehende Aufgaben im Laufe der Zeit weiterentwickeln und völlig neue Aufgaben auftauchen.Experimentelle Ergebnisse zeigen, dass VCL die modernsten Methoden des kontinuierlichen Lernens in einer Vielzahl von Aufgaben übertrifft und katastrophales Vergessen auf vollautomatische Weise vermeidet.
In partiell beobachtbaren (PO) Umgebungen leiden Deep Reinforcement Learning (RL)-Agenten oft unter einer unbefriedigenden Leistung, da zwei Probleme zusammen angegangen werden müssen: wie man Informationen aus den Rohbeobachtungen extrahiert, um die Aufgabe zu lösen, und wie man die Strategie verbessert.In dieser Studie schlagen wir einen RL-Algorithmus zur Lösung von PO-Aufgaben vor.Unsere Methode besteht aus zwei Teilen: einem variationalen rekurrenten Modell (VRM) zur Modellierung der Umgebung und einem RL-Controller, der sowohl auf die Umgebung als auch auf das VRM Zugriff hat. Der vorgeschlagene Algorithmus wurde in zwei Arten von PO-Robotersteuerungsaufgaben getestet, nämlich solchen, bei denen entweder Koordinaten oder Geschwindigkeiten nicht beobachtbar sind, und solchen, die eine Langzeitspeicherung erfordern.Unsere Experimente zeigen, dass der vorgeschlagene Algorithmus bei Aufgaben, bei denen unbeobachtete Zustände nicht auf einfache Weise aus Rohbeobachtungen abgeleitet werden können, eine bessere Dateneffizienz erreicht und/oder eine optimalere Strategie lernt als andere alternative Ansätze.
In diesem Artikel wird das Problem der Online-Algorithmenauswahl im Kontext des Reinforcement Learning (RL) formalisiert: Bei einer episodischen Aufgabe und einer endlichen Anzahl von RL-Algorithmen mit Off-Policy muss ein Meta-Algorithmus entscheiden, welcher RL-Algorithmus während der nächsten Episode die Kontrolle übernimmt, um den erwarteten Ertrag zu maximieren. In diesem Artikel wird ein neuartiger Meta-Algorithmus mit dem Namen Epochal Stochastic Bandit Algorithm Selection (ESBAS) vorgestellt, dessen Prinzip darin besteht, die Aktualisierung der Politik in jeder Epoche einzufrieren und einen neu gestarteten stochastischen Bandit mit der Auswahl des Algorithmus zu betrauen. ESBAS wird zunächst empirisch an einer Dialogaufgabe evaluiert, bei der sich zeigt, dass es jeden einzelnen Algorithmus in den meisten Konfigurationen übertrifft.ESBAS wird dann an eine echte Online-Umgebung angepasst, bei der die Algorithmen ihre Richtlinien nach jedem Übergang aktualisieren, was wir SSBAS nennen.SSBAS wird an einer Obstsammelaufgabe evaluiert, bei der sich zeigt, dass es den Schrittgrößenparameter effizienter anpasst als der klassische hyperbolische Zerfall, und an einem Atari-Spiel, bei dem es die Leistung um ein Vielfaches verbessert.
Im Vergleich zum klassischen generativen Prozess, bei dem jeder beobachtete Datenpunkt aus einer individuellen latenten Variable generiert wird, geht unser Ansatz von einer globalen latenten Variable aus, um die gesamte Menge der beobachteten Datenpunkte zu generieren. Im Vergleich zum Standard-ELBO-Ziel, bei dem das Variationsposteriori für jeden Datenpunkt mit der Prior-Verteilung übereinstimmen soll, gleicht das WiSE-ALE-Ziel das gemittelte Posterior über alle Stichproben mit dem Prior ab, wodurch die stichprobenweisen Posterior-Verteilungen einen breiteren Bereich akzeptabler Einbettungsmittelwerte und Varianzen aufweisen können, was zu einer besseren Rekonstruktionsqualität im Auto-Encoding-Prozess führt.Anhand verschiedener Beispiele und im Vergleich zu anderen hochmodernen VAE-Modellen zeigen wir, dass WiSE-ALE hervorragende Informationseinbettungseigenschaften aufweist und gleichzeitig die Fähigkeit besitzt, eine glatte, kompakte Darstellung zu lernen.
Wir verbessern die Robustheit von tiefen neuronalen Netzen gegenüber Angriffen durch Angreifer, indem wir eine interpolierende Funktion als Ausgangsaktivierung verwenden.   Diese datenabhängige Aktivierungsfunktion verbessert sowohl die Klassifizierungsgenauigkeit als auch die Stabilität gegenüber gegnerischen Störungen. Zusammen mit der Minimierung der Gesamtvariation der gegnerischen Bilder und dem erweiterten Training erreichen wir beim stärksten Angriff eine Verbesserung der Genauigkeit um bis zu 20,6 %, 50,7 % bzw. 68,7 % gegenüber der schnellen Gradienten-Zeichen-Methode, der iterativen schnellen Gradienten-Zeichen-Methode und den Carlini-Wagner-L2-Angriffen.  Unsere Verteidigungsstrategie ist zu vielen der bestehenden Methoden additiv.  Wir geben eine intuitive Erklärung unserer Verteidigungsstrategie durch die Analyse der Geometrie des Merkmalsraumes.Zur Reproduzierbarkeit wird der Code auf GitHub verfügbar sein.
Um dies zu erreichen, bauten wir die größte bestehende visuelle Spracherkennung Datensatz, bestehend aus Paaren von Text und Videoclips von Gesichtern sprechen (3.886 Stunden Video).Im Tandem, entwarfen wir und trainierten ein integriertes Lippenlesen System, bestehend aus einer Video-Pipeline, die rohen Video-Maps zu stabilen Videos von Lippen und Sequenzen von Phonemen, eine skalierbare tiefe neuronale Netzwerk, das die Lippen-Videos, um Sequenzen von Phonem-Verteilungen Karten, und eine Produktionsebene Sprachdecoder, die Ausgaben Sequenzen von Wörtern. Das vorgeschlagene System erreicht eine Wortfehlerrate (WER) von 40,9 %, gemessen an einem Hold-Out-Set. Im Vergleich dazu erreichen professionelle Lippenleser entweder 86,4 % oder 92,9 % WER auf demselben Datensatz, wenn sie Zugang zu zusätzlichen Arten von Kontextinformationen haben. Unser Ansatz verbessert deutlich frühere Lippenleseansätze, einschließlich Varianten von LipNet und von Watch, Attend, and Spell (WAS), die nur 89,8 % bzw. 76,8 % WER erreichen können.
Frühere Arbeiten (Bowman et al., 2015; Yang et al., Um das Problem zu lösen, dass der Decoder Informationen aus dem Encoder ignoriert (posteriorer Kollaps), schwächen diese bisherigen Modelle die Kapazität des Decoders, um das Modell zu zwingen, Informationen aus latenten Variablen zu verwenden. Diese Strategie ist jedoch nicht ideal, da sie die Qualität des generierten Textes verschlechtert und die Hyperparameter erhöht. Wir zeigen, dass unser Modell gut konditionierte Sätze generieren kann, ohne die Kapazität des Dekoders zu schwächen, und dass die multimodale Prior-Verteilung die Interpretierbarkeit der erworbenen Repräsentationen verbessert.
Modellbeschneidung zielt darauf ab, Sparsamkeit in den verschiedenen Verbindungsmatrizen eines tiefen neuronalen Netzwerks zu induzieren und dadurch die Anzahl der nicht nullwertigen Parameter im Modell zu reduzieren, Dies deutet auf die Möglichkeit hin, dass die Basismodelle in diesen Experimenten vielleicht von Anfang an stark überparametrisiert sind und eine praktikable Alternative für die Modellkompression darin bestehen könnte, einfach die Anzahl der versteckten Einheiten zu reduzieren, während die dichte Verbindungsstruktur des Modells beibehalten wird, was einen ähnlichen Kompromiss zwischen Modellgröße und Genauigkeit darstellt. Wir untersuchen diese beiden unterschiedlichen Wege der Modellkompression im Kontext energieeffizienter Inferenz in ressourcenbeschränkten Umgebungen und schlagen eine neue graduelle Pruning-Technik vor, die einfach und unkompliziert auf eine Vielzahl von Modellen/Datensätzen mit minimaler Anpassung angewendet und nahtlos in den Trainingsprozess integriert werden kann. Wir vergleichen die Genauigkeit von großen, aber beschnittenen Modellen (large-sparse) und ihren kleineren, aber dichten (small-dense) Gegenstücken mit identischem Speicherplatzbedarf. Über eine breite Palette von neuronalen Netzwerkarchitekturen (deep CNNs, stacked LSTM und seq2seq LSTM-Modelle) hinweg stellen wir fest, dass large-sparse-Modelle durchweg besser abschneiden als small-dense-Modelle und eine bis zu 10-fache Reduktion der Anzahl von Nicht-Null-Parametern bei minimalem Genauigkeitsverlust erreichen.
Die Steuerung von Attributen der generierten Sprache (z.B. Themen- oder Stimmungswechsel) ist jedoch schwierig, ohne die Modellarchitektur zu modifizieren oder eine Feinabstimmung auf attributspezifischen Daten vorzunehmen, was mit erheblichen Kosten für ein erneutes Training verbunden ist. Wir schlagen eine einfache Alternative vor: das Plug-and-Play-Sprachmodell (PPLM) für eine steuerbare Sprachgenerierung, das ein vortrainiertes LM mit einem oder mehreren einfachen Attributklassifikatoren kombiniert, die die Texterzeugung ohne weiteres Training des LM steuern. Im kanonischen Szenario, das wir vorstellen, sind die Attributmodelle einfache Klassifikatoren, die aus einer benutzerspezifischen Tasche von Wörtern oder einer einzelnen gelernten Schicht mit 100.000 Mal weniger Parametern als das LM bestehen. Die Probenahme umfasst einen Vorwärts- und Rückwärtsdurchlauf, bei dem die Gradienten des Attributmodells die versteckten Aktivierungen des LM vorantreiben und so die Generierung steuern. PPLMs sind insofern flexibel, als dass jede beliebige Kombination von differenzierbaren Attributmodellen zur Steuerung der Texterzeugung verwendet werden kann, was vielfältige und kreative Anwendungen über die in diesem Papier genannten Beispiele hinaus ermöglicht.
Bestehende Deep-Multitask-Learning (MTL)-Ansätze ordnen Schichten, die zwischen Aufgaben geteilt werden, in einer parallelen Reihenfolge an. Eine solche Organisation schränkt die Arten von geteilten Strukturen, die gelernt werden können, erheblich ein. Die Ergebnisse deuten darauf hin, dass eine flexible Anordnung eine effektivere gemeinsame Nutzung ermöglichen kann, was die Entwicklung eines weichen Ordnungsansatzes motiviert, der lernt, wie gemeinsame Schichten auf unterschiedliche Weise für verschiedene Aufgaben eingesetzt werden.deep MTL mit weicher Anordnung übertrifft parallele Ordnungsmethoden in einer Reihe von Domänen.diese Ergebnisse deuten darauf hin, dass die Stärke von deep MTL aus dem Lernen sehr allgemeiner Bausteine kommt, die zusammengesetzt werden können, um die Anforderungen jeder Aufgabe zu erfüllen.
Wir schlagen einen generischen Rahmen vor, um die Genauigkeit und das Vertrauen (Score) einer Vorhersage durch stochastische Inferenzen in tiefen neuronalen Netzen zu kalibrieren, indem wir zunächst die Beziehung zwischen der Variation mehrerer Modellparameter für eine einzelne Beispielinferenz und der Varianz der entsprechenden Vorhersage-Scores durch Bayes'sche Modellierung der stochastischen Regularisierung analysieren.Unsere empirische Beobachtung zeigt, dass die Genauigkeit und der Score einer Vorhersage stark mit der Varianz mehrerer stochastischer Inferenzen korreliert sind, die durch stochastische Tiefe oder Dropout gegeben sind. Motiviert durch diese Tatsachen, entwerfen wir eine neuartige varianzgewichtete vertrauensintegrierte Verlustfunktion, die sich aus zwei Kreuzentropie-Verlusttermen in Bezug auf die Grundwahrheit und die Gleichverteilung zusammensetzt, die durch die Varianz der stochastischen Vorhersageergebnisse ausgeglichen werden.Die vorgeschlagene Verlustfunktion ermöglicht es uns, tiefe neuronale Netze zu erlernen, die vertrauenskalibrierte Ergebnisse mit einer einzigen Schlussfolgerung vorhersagen. Unser Algorithmus bietet eine hervorragende Vertrauenskalibrierungsleistung und verbessert die Klassifizierungsgenauigkeit mit zwei beliebten stochastischen Regularisierungstechniken - stochastische Tiefe und Dropout - in mehreren Modellen und Datensätzen; er lindert das Problem des Übervertrauens in tiefen neuronalen Netzen erheblich, indem er die Netze so trainiert, dass die Vorhersagegenauigkeit proportional zum Vertrauen in die Vorhersage ist.
Real-Life-Control-Aufgaben beinhalten Angelegenheiten von verschiedenen Substanzen - starre oder weiche Körper, Flüssigkeiten, Gase - mit jeweils unterschiedlichen physikalischen Verhaltensweisen, was eine Herausforderung für die traditionellen Rigid-Body-Physik-Engines darstellt.Partikelbasierte Simulatoren wurden entwickelt, um die Dynamik dieser komplexen Szenen zu modellieren; da sie sich jedoch auf Annäherungstechniken stützen, weicht ihre Simulation oft von der realen Physik ab, insbesondere auf lange Sicht.In diesem Papier schlagen wir vor, einen partikelbasierten Simulator für komplexe Steuerungsaufgaben zu lernen. Die Kombination von Lernen mit partikelbasierten Systemen bringt zwei große Vorteile mit sich: Erstens kann der gelernte Simulator, genau wie andere partikelbasierte Systeme, auf Objekte unterschiedlicher Materialien einwirken; zweitens bietet die partikelbasierte Repräsentation einen starken induktiven Bias für das Lernen: Partikel desselben Typs haben die gleiche Dynamik in sich. Wir demonstrieren, dass Roboter mit dem gelernten Simulator komplexe Manipulationsaufgaben bewältigen können, wie z.B. die Manipulation von Flüssigkeiten und verformbarem Schaumstoff, sowohl in der Simulation als auch in der realen Welt. Unsere Studie trägt dazu bei, die Grundlage für das Lernen von Robotern in dynamischen Szenen mit partikelbasierten Repräsentationen zu legen.
Generative Adversarial Networks (GANs) sind dafür bekannt, dass sie, wenn sie auf großen Datensätzen mit verschiedenen Modi trainiert werden, verschmolzene Bilder erzeugen, die nicht eindeutig zu einem der Modi gehören.Wir stellen die Hypothese auf, dass dieses Problem durch die Interaktion zwischen zwei Tatsachen entsteht: (1) Bei Datensätzen mit großer Vielfalt ist es wahrscheinlich, dass die Modi auf separaten Mannigfaltigkeiten liegen.(2) Der Generator (G) ist als kontinuierliche Funktion formuliert, und das Eingaberauschen wird von einer zusammenhängenden Menge abgeleitet, wodurch die Ausgabe von G eine zusammenhängende Menge ist. Wenn G alle Modi abdeckt, dann muss es einen Teil der Ausgabe von G geben, der sie miteinander verbindet, was zu unerwünschten, verschmolzenen Bildern führt. Wir ergänzen die GAN-Formulierung mit einem Klassifikator C, der vorhersagt, welche Rauschpartition/welcher Generator die Ausgangsbilder erzeugt hat, und fördern so die Vielfalt zwischen den einzelnen Partitionen/Generatoren. Wir experimentieren mit MNIST, celebA, STL-10 und einem schwierigen Datensatz mit klar unterscheidbaren Modi und zeigen, dass die Rauschpartitionen verschiedenen Modi der Datenverteilung entsprechen und Bilder von besserer Qualität erzeugen.
Um Crowd-Sourced-Daten zu nutzen, um Text-to-Speech-Modelle (TTS) mit mehreren Sprechern zu trainieren, die saubere Sprache für alle Sprecher synthetisieren können, ist es wichtig, entkoppelte Repräsentationen zu erlernen, die unabhängig voneinander die Sprecheridentität und das Hintergrundrauschen in den generierten Signalen kontrollieren können.Das Erlernen solcher Repräsentationen kann jedoch eine Herausforderung sein, da es keine Labels gibt, die die Aufnahmebedingungen jedes Trainingsbeispiels beschreiben, und da Sprecher und Aufnahmebedingungen oft korreliert sind, z.B. da Benutzer oft viele Aufnahmen mit derselben Ausrüstung machen.Dieses Papier schlägt drei Komponenten vor, um dieses Problem zu lösen, indem: (1) Formulierung eines bedingten generativen Modells mit faktorisierten latenten Variablen, (2) Verwendung von Datenerweiterung, um Rauschen hinzuzufügen, das nicht mit der Sprecheridentität korreliert und dessen Bezeichnung während des Trainings bekannt ist, und (3) Verwendung von adversarialer Faktorisierung, um die Entflechtung zu verbessern.Experimentelle Ergebnisse zeigen, dass die vorgeschlagene Methode Sprecher- und Rauschattribute entflechten kann, selbst wenn sie in den Trainingsdaten korreliert sind, und dass sie verwendet werden kann, um konsistent saubere Sprache für alle Sprecher zu synthetisieren.Ablationsstudien verifizieren die Bedeutung jeder vorgeschlagenen Komponente.
LSTM-basierte Sprachmodelle zeigen Kompositionalität in ihren Repräsentationen, aber wie dieses Verhalten im Laufe des Trainings entsteht, ist nicht erforscht worden. Durch die Analyse synthetischer Datenexperimente mit kontextueller Dekomposition finden wir heraus, dass LSTMs weitreichende Abhängigkeiten kompositionell lernen, indem sie sie aus kürzeren Bestandteilen während des Trainings aufbauen.
Das Lernen eines tiefen neuronalen Netzes erfordert die Lösung eines anspruchsvollen Optimierungsproblems: Es handelt sich um ein hochdimensionales, nicht-konvexes und nicht-glattes Minimierungsproblem mit einer großen Anzahl von Termen.Die derzeitige Praxis bei der Optimierung neuronaler Netze beruht auf dem Algorithmus des stochastischen Gradientenabstiegs (SGD) oder seinen adaptiven Varianten. Die adaptiven Varianten des SGD-Algorithmus neigen dazu, Lösungen zu erzeugen, die sich auf ungesehene Daten weniger gut verallgemeinern lassen als der SGD-Algorithmus mit einem von Hand entworfenen Zeitplan.Wir stellen eine Optimierungsmethode vor, die empirisch das Beste aus beiden Welten bietet: Unser Algorithmus liefert eine gute Verallgemeinerungsleistung und benötigt nur einen Hyperparameter. Unser Ansatz basiert auf einem zusammengesetzten proximalen Rahmen, der die kompositorische Natur tiefer neuronaler Netze ausnutzt und leistungsstarke konvexe Optimierungsalgorithmen durch Design nutzen kann, insbesondere verwenden wir den Frank-Wolfe-Algorithmus (FW) für SVM, der eine optimale Schrittgröße in geschlossener Form bei jedem Zeitschritt berechnet. Wir präsentieren Experimente mit den CIFAR- und SNLI-Datensätzen, in denen wir die signifikante Überlegenheit unserer Methode gegenüber Adam, Adagrad sowie den kürzlich vorgeschlagenen BPGrad und AMSGrad demonstrieren. Darüber hinaus vergleichen wir unseren Algorithmus mit SGD mit einem von Hand entworfenen Lernratenplan und zeigen, dass er eine ähnliche Generalisierung bietet, während er oft schneller konvergiert.
In diesem Beitrag zeigen wir, wie neuartige Transfer-Reinforcement-Learning-Techniken auf die komplexe Aufgabe der zielgesteuerten Navigation unter Verwendung des fotorealistischenAI2THOR-Simulators angewendet werden können, und zwar auf der Grundlage des Konzepts der Universal SuccessorFeatures mit einem A3C-Agenten. Wir führen den neuartigen architektonischen1Beitrag einer Successor Feature Dependent Policy (SFDP) ein und übernehmen das Konzept der VariationalInformation Bottlenecks, um eine State-of-the-Art-Leistung zu erreichen.VUSFA, unsere endgültige Architektur, ist ein unkomplizierter Ansatz, der mit unserem Open-Source-Repository implementiert werden kann.Unser Ansatz ist verallgemeinerbar, zeigte eine größere Stabilität beim Training und übertraf neuere Ansätze in Bezug auf die Transfer-Lernfähigkeit.
Eine große Herausforderung beim Lernen von Bildrepräsentationen ist die Entflechtung der Variationsfaktoren, die der Bildbildung zugrunde liegen.  Dies wird in der Regel mit einer Autocodierer-Architektur erreicht, bei der eine Teilmenge der latenten Variablen auf bestimmte Faktoren beschränkt ist und die übrigen Variablen als Störvariablen betrachtet werden. Dieser Ansatz hat einen großen Nachteil: Mit zunehmender Dimension der Störvariablen wird die Bildrekonstruktion verbessert, aber der Decoder hat die Flexibilität, die festgelegten Faktoren zu ignorieren, wodurch die Fähigkeit verloren geht, die Ausgabe auf sie zu konditionieren.  In dieser Arbeit schlagen wir vor, diesen Zielkonflikt zu überwinden, indem wir die Dimension des latenten Codes schrittweise erhöhen, während die Jacobi des Ausgangsbildes in Bezug auf die entwirrten Variablen gleich bleiben muss.  Infolgedessen sind die erhaltenen Modelle sowohl bei der Entflechtung als auch bei der Rekonstruktion wirksam.  Wir demonstrieren die Anwendbarkeit dieser Methode sowohl in unbeaufsichtigten als auch in überwachten Szenarien zum Erlernen entwirrter Repräsentationen. In einer Aufgabe zur Manipulation von Gesichtsattributen erhalten wir eine qualitativ hochwertige Bilderzeugung, während wir Dutzende von Attributen mit einem einzigen Modell reibungslos kontrollieren.
Wir schlagen einen neuen Begriff der "Nichtlinearität" einer Netzwerkschicht in Bezug auf einen Eingabestapel vor, der auf ihrer Nähe zu einem linearen System basiert, das sich im nichtnegativen Rang der Aktivierungsmatrix widerspiegelt. Wir führen Experimente mit voll verknüpften und faltbaren neuronalen Netzen durch, die auf verschiedenen Bild- und Audiodatensätzen trainiert wurden, und unsere Ergebnisse zeigen, dass unsere Technik als Indikator für Memorisierung verwendet werden kann, um ein frühzeitiges Stoppen durchzuführen.
Deep neural networks have achieved state-of-the-art performance in various fields, but they have to be scaled down to be used for real-world applications.as a means to reduce the size of a neural network while preserving its performance, knowledge transfer has brought a lot of attention.one popular method of knowledge transfer is knowledge distillation (KD), where softened outputs of a pre-trained teacher network help train student networks.Since KD, other transfer methods have been proposed, and they mainly focus on loss functions, activations of hidden layers, or additional modules to transfer knowledge well from teacher networks to student networks. In dieser Arbeit konzentrieren wir uns auf die Struktur eines Lehrernetzwerks, um den Effekt mehrerer Lehrernetzwerke ohne zusätzliche Ressourcen zu erzielen.Wir schlagen vor, die Struktur eines Lehrernetzwerks so zu ändern, dass es stochastische Blöcke und Skip-Verbindungen hat.Dadurch wird ein Lehrernetzwerk zum Aggregat einer großen Anzahl von Pfaden. In der Trainingsphase wird jedes Teilnetz durch das zufällige Fallenlassen von stochastischen Blöcken generiert und als Lehrernetzwerk verwendet, was das Training des Schülernetzwerks mit mehreren Lehrernetzwerken ermöglicht und das Schülernetzwerk mit den gleichen Ressourcen in einem einzigen Lehrernetzwerk weiter verbessert.Wir verifizieren, dass die vorgeschlagene Struktur weitere Verbesserungen für Schülernetzwerke in Benchmark-Datensätzen bringt.
Wir haben eine Android-Tastatur mit lexikalischen (wortbasierten) und semantischen (bedeutungsbasierten) Emoji-Vorschlägen entwickelt und diese in zwei verschiedenen Studien verglichen. Um die Wirkung von Emoji-Vorschlägen in Online-Konversationen zu untersuchen, haben wir eine Text-Messaging-Laborstudie mit 24 Teilnehmern und einen 15-tägigen Feldeinsatz mit 18 Teilnehmern durchgeführt. Wir fanden heraus, dass lexikalische Emoji-Vorschläge die Emoji-Nutzung im Vergleich zu einer Tastatur ohne Vorschläge um 31,5 % erhöhten, während semantische Vorschläge die Emoji-Nutzung um 125,1 % steigerten. Die Vorschlagsmechanismen hatten jedoch keinen signifikanten Einfluss auf das Chat-Erlebnis.
Herkömmliche Generative Adversarial Networks (GANs) für die Texterzeugung neigen dazu, Probleme der Belohnungssparsamkeit und des Moduszusammenbruchs zu haben, die sich auf die Qualität und Vielfalt der erzeugten Muster auswirken.Um diese Probleme anzugehen, schlagen wir ein neuartiges Paradigma des selbstadversarischen Lernens (SAL) vor, um die Leistung von GANs bei der Texterzeugung zu verbessern.Im Gegensatz zu Standard-GANs, die einen binären Klassifikator als Diskriminator verwenden, um vorherzusagen, ob ein Muster echt oder erzeugt ist, verwendet SAL einen vergleichenden Diskriminator, der ein paarweiser Klassifikator ist, um die Textqualität zwischen einem Musterpaar zu vergleichen. Dieser Selbstverbesserungs-Belohnungsmechanismus ermöglicht es dem Modell, leichter Kredite zu erhalten und zu vermeiden, dass es bei einer begrenzten Anzahl echter Proben zusammenbricht, was nicht nur dazu beiträgt, das Problem der spärlichen Belohnung zu lindern, sondern auch das Risiko des Zusammenbruchs des Modus verringert.Experimente an Textgenerierungs-Benchmark-Datensätzen zeigen, dass unser vorgeschlagener Ansatz sowohl die Qualität als auch die Vielfalt erheblich verbessert und eine stabilere Leistung im Vergleich zu früheren GANs für die Textgenerierung bietet.
In dieser Studie stellen wir eine neuartige Methode vor, die sich auf SVD stützt, um die Anzahl der latenten Dimensionen zu ermitteln. Das allgemeine Prinzip der Methode besteht darin, die Kurve der singulären Werte der SVD-Zerlegung eines Datensatzes mit der Kurve des randomisierten Datensatzes zu vergleichen. Um unsere Methode zu bewerten, vergleichen wir sie mit konkurrierenden Methoden wie Kaisers Eigenwert-größer-als-eine-Regel (K1), Parallele Analyse (PA), Velicers MAP-Test (Minimum Average Partial).Wir vergleichen unsere Methode auch mit der Silhouettenbreite (SW) Technik, die in verschiedenen Clustering-Methoden verwendet wird, um die optimale Anzahl von Clustern zu bestimmen.Das Ergebnis auf synthetischen Daten zeigt, dass die Parallele Analyse und unsere Methode hatähnliche Ergebnisse und genauer als die anderen Methoden, und dass unsere Methoden etwas besseres Ergebnis als die Parallele Analyse Methode für die spärlichen Datensätze.
Hier konzentrieren wir uns auf das Problem der Erkennung von Angriffen und nicht auf eine robuste Klassifizierung, da die Erkennung eines Angriffs sogar noch wichtiger sein kann als die Vermeidung von Fehlklassifizierungen.Wir bauen auf den Fortschritten im Bereich der Erklärbarkeit auf, wo Aktivitätskarten-ähnliche Erklärungen verwendet werden, um Entscheidungen zu rechtfertigen und zu validieren, indem Merkmale hervorgehoben werden, die an einer Klassifizierungsentscheidung beteiligt sind.Die wichtigste Beobachtung ist, dass es schwierig ist, Erklärungen für falsche Entscheidungen zu erstellen. Wir schlagen EXAID vor, einen neuen Ansatz zur Erkennung von Angriffen, der die Erklärbarkeit von Modellen nutzt, um Bilder zu identifizieren, deren Erklärungen nicht mit der vorhergesagten Klasse übereinstimmen. Konkret verwenden wir SHAP, das Shapley-Werte im Raum des Eingabebildes verwendet, um festzustellen, welche Eingabemerkmale zu einer Klassenentscheidung beitragen. Interessanterweise erfordert dieser Ansatz keine Änderung des angegriffenen Modells und kann ohne Modellierung eines spezifischen Angriffs angewendet werden. Er kann daher erfolgreich eingesetzt werden, um unbekannte Angriffe zu erkennen, die zum Zeitpunkt der Entwicklung des Erkennungsmodells noch nicht bekannt waren. Wir evaluieren EXAID an zwei Benchmark-Datensätzen, CIFAR-10 und SVHN, und im Vergleich zu drei führenden Angriffstechniken, FGSM, PGD und C&W. Wir stellen fest, dass EXAID die SoTA-Erkennungsmethoden in einem weiten Bereich von Rauschpegeln deutlich übertrifft und die Erkennung von 70 % auf über 90 % bei kleinen Störungen verbessert.
Wir beschreiben einen einfachen und allgemeinen Ansatz zur Komprimierung von Gewichten in neuronalen Netzen, bei dem die Netzparameter (Gewichte und Verzerrungen) in einem "latenten" Raum dargestellt werden, was einer Neuparametrisierung gleichkommt.Dieser Raum ist mit einem erlernten Wahrscheinlichkeitsmodell ausgestattet, das verwendet wird, um der Parameterdarstellung während des Trainings einen Entropieabzug aufzuerlegen und die Darstellung nach dem Training mithilfe eines einfachen arithmetischen Codierers zu komprimieren. Die Klassifizierungsgenauigkeit und die Komprimierbarkeit des Modells werden gemeinsam maximiert, wobei der Kompromiss zwischen Bitrate und Genauigkeit durch einen Hyperparameter spezifiziert wird. Wir evaluieren die Methode anhand der MNIST-, CIFAR-10- und ImageNet-Klassifizierungsbenchmarks unter Verwendung von sechs verschiedenen Modellarchitekturen. Unsere Ergebnisse zeigen, dass eine hochmoderne Modellkomprimierung auf skalierbare und allgemeine Weise erreicht werden kann, ohne dass komplexe Verfahren wie mehrstufiges Training erforderlich sind.
In diesem Zusammenhang schlagen wir Ada-Boundary vor, einen neuartigen adaptiven Batch-Selektionsalgorithmus, der einen effektiven Mini-Batch entsprechend dem Lernfortschritt des Modells konstruiert. Unsere Schlüsselidee ist es, verwirrende Proben zu präsentieren, was das wahre Label ist. Wir demonstrieren den Vorteil von Ada-Boundary durch umfangreiche Experimente mit zwei neuronalen Faltungsnetzen für drei Benchmark-Datensätze. Die Ergebnisse zeigen, dass Ada-Boundary die Trainingszeit um bis zu 31,7 % im Vergleich zur State-of-the-Art-Strategie und um bis zu 33,5 % im Vergleich zur Baseline-Strategie verbessert.
Diese Datensätze definieren typischerweise eine Ontologie, um eine Struktur zu schaffen, die diese Sound-Klassen mit abstrakteren Superklassen in Beziehung setzt, so dass die Ontologie als Quelle für die Darstellung des Domänenwissens von Sounds dient. Wir schlagen zwei Ontologie-basierte neuronale Netzwerkarchitekturen für die Klassifizierung von Schallereignissen vor und definieren einen Rahmen, um einfache Netzwerkarchitekturen zu entwerfen, die eine ontologische Struktur beibehalten, die mit zwei der gängigsten Klassifizierungsdatensätze für Schallereignisse trainiert und evaluiert werden.
Die Ergebnisse zeigen eine Verbesserung der Klassifizierungsleistung, die die Vorteile der Einbeziehung der ontologischen Informationen demonstriert. Im Gegensatz zu vollständig verbundenen Netzwerken erreichen Convolutional Neural Networks (CNNs) Effizienz durch das Erlernen von Gewichten, die mit lokalen Filtern mit einer endlichen räumlichen Ausdehnung verbunden sind, was zur Folge hat, dass ein Filter zwar weiß, worauf er schaut, aber nicht, wo er im Bild positioniert ist. In dieser Arbeit testen wir diese Hypothese und zeigen das überraschende Ausmaß an absoluter Positionsinformation, die in gängigen neuronalen Netzen kodiert wird. eine umfassende Reihe von Experimenten zeigt die Gültigkeit dieser Hypothese und gibt Aufschluss darüber, wie und wo diese Information dargestellt wird, während sie Hinweise darauf gibt, woher Positionsinformation in tiefen CNNs stammt.
Die semantische Analyse, die einen natürlichsprachlichen Satz in eine formale, maschinenlesbare Darstellung seiner Bedeutung umwandelt, ist durch die begrenzten annotierten Trainingsdaten stark eingeschränkt. Inspiriert von der Idee der Grob-zu-Fein-Analyse schlagen wir ein allgemeines-zu-detailliertes neuronales Netzwerk (GDNN) vor, indem wir eine domänenübergreifende Skizze (CDS) zwischen Äußerungen und ihren logischen Formen einbeziehen. für Äußerungen in verschiedenen Domänen extrahiert das allgemeine Netzwerk die CDS mit Hilfe eines Kodierer-Dekodierer-Modells in einem Multi-Task-Learning-Setup. Unsere Experimente zeigen, dass CDS im Vergleich zu direktem Multi-Task-Lernen die Leistung bei der semantischen Parsing-Aufgabe verbessert hat, die die Anfragen der Benutzer in eine Bedeutungsrepräsentationssprache (MRL) umwandelt. Wir verwenden auch Experimente, um zu veranschaulichen, dass CDS funktioniert, indem wir einige Einschränkungen zum Zieldekodierungsprozess hinzufügen, was die Effektivität und Rationalität von CDS weiter belegt.
Die Lernfähigkeit verschiedener neuronaler Architekturen kann direkt durch berechenbare Maße der Datenkomplexität charakterisiert werden. In diesem Papier stellen wir das Problem der Architekturauswahl neu dar, indem wir verstehen, wie die Daten die ausdrucksstärksten und verallgemeinerungsfähigsten Architekturen bestimmen, die für diese Daten geeignet sind, jenseits der induktiven Verzerrung. Nachdem wir die algebraische Topologie als Maß für die Datenkomplexität vorgeschlagen haben, zeigen wir, dass die Fähigkeit eines Netzwerks, die topologische Komplexität eines Datensatzes in seiner Entscheidungsgrenze auszudrücken, ein streng begrenzender Faktor für seine Fähigkeit zur Verallgemeinerung ist. Unsere empirische Analyse zeigt, dass neuronale Netze auf jeder Ebene der Datenkomplexität topologische Phasenübergänge und Schichtungen aufweisen. Diese Beobachtung ermöglichte es uns, die bestehende Theorie mit empirisch getriebenen Vermutungen über die Wahl der Architekturen für neuronale Netze mit einer einzigen verborgenen Schicht zu verbinden.
Ein Beispiel aus der Chemie ist das Design von maßgeschneiderten organischen Materialien und Molekülen, das effiziente Methoden zur Erkundung des chemischen Raums erfordert. Wir stellen einen genetischen Algorithmus (GA) vor, der mit einem auf einem neuronalen Netzwerk (DNN) basierenden Diskriminatormodell erweitert wird, um die Vielfalt der generierten Moleküle zu verbessern und gleichzeitig den GA zu steuern.Wir zeigen, dass unser Algorithmus andere generative Modelle bei Optimierungsaufgaben übertrifft.Wir stellen außerdem einen Weg vor, um die Interpretierbarkeit genetischer Algorithmen zu erhöhen, was uns geholfen hat, Designprinzipien abzuleiten
Bidirektionale Encoder-Repräsentationen von Transformatoren (BERT) erreichen in einer Reihe von Aufgaben der natürlichen Sprachverarbeitung Spitzenergebnisse, doch das Verständnis ihrer internen Funktionsweise ist noch unzureichend und unbefriedigend.um BERT und andere Transformator-basierte Modelle besser zu verstehen, präsentieren wir eine schichtweise Analyse der verborgenen Zustände von BERT. Im Gegensatz zu früheren Forschungen, die sich hauptsächlich darauf konzentrierten, Transformer-Modelle durch ihre Boxgewichte zu erklären, argumentieren wir, dass verborgene Zustände ebenso wertvolle Informationen enthalten.Unsere Analyse konzentriert sich insbesondere auf Modelle, die auf die Aufgabe der Beantwortung von Fragen (QA) als Beispiel für eine komplexe nachgelagerte Aufgabe abgestimmt sind. Wir untersuchen, wie QA-Modelle Token-Vektoren transformieren, um die richtige Antwort zu finden. Zu diesem Zweck wenden wir eine Reihe allgemeiner und QA-spezifischer Sondierungsaufgaben an, die die in jeder Repräsentationsebene gespeicherten Informationen offenlegen. Unsere qualitative Analyse der Visualisierungen des verborgenen Zustands bietet zusätzliche Einblicke in den Denkprozess von BERT. Unsere Ergebnisse zeigen, dass die Transformationen innerhalb von BERT Phasen durchlaufen, die mit traditionellen Pipeline-Aufgaben zusammenhängen, und dass das System daher implizit aufgabenspezifische Informationen in seine Token-Repräsentationen einbeziehen kann.
Wir schlagen eine allgemeine Deep Reinforcement Learning-Methode vor und wenden sie auf Robotermanipulationsaufgaben an: Unser Ansatz nutzt Demonstrationsdaten, um einen Reinforcement Learning-Agenten beim Lernen zu unterstützen, um eine breite Palette von Aufgaben zu lösen, vor allem bisher ungelöste. Unsere Experimente zeigen, dass unser Verstärkungs- und Nachahmungsansatz in der Lage ist, kontaktreiche Robotermanipulationsaufgaben zu lösen, die weder mit der modernsten Verstärkungs- noch mit der Nachahmungslernmethode allein gelöst werden können.
Ensembles, bei denen mehrere neuronale Netze einzeln trainiert und ihre Vorhersagen gemittelt werden, haben sich als sehr erfolgreich erwiesen, um sowohl die Genauigkeit als auch die Vorhersageunsicherheit einzelner neuronaler Netze zu verbessern. In diesem Papier schlagen wir BatchEnsemble vor, eine Ensemble-Methode, deren Rechen- und Speicherkosten deutlich niedriger sind als die typischer Ensembles. BatchEnsemble erreicht dies, indem jede Gewichtsmatrix als Hadamard-Produkt eines gemeinsamen Gewichts aller Ensemble-Mitglieder und einer Rang-1-Matrix pro Mitglied definiert wird. Im Gegensatz zu Ensembles ist BatchEnsemble nicht nur geräteübergreifend parallelisierbar, wobei ein Gerät ein Mitglied trainiert, sondern auch innerhalb eines Geräts, wobei mehrere Ensemble-Mitglieder gleichzeitig für ein bestimmtes Mini-Batch aktualisiert werden. Bei CIFAR-10-, CIFAR-100-, WMT14 EN-DE/EN-FR-Übersetzungs- und kontextuellen Bandits-Aufgaben liefert BatchEnsemble wettbewerbsfähige Genauigkeit und Unsicherheiten als typische Ensembles; die Beschleunigung bei der Testzeit beträgt das Dreifache und die Speicherreduzierung das Dreifache bei einem Ensemble der Größe 4. Wir wenden BatchEnsemble auch auf lebenslanges Lernen an, wobei BatchEnsemble auf Split-CIFAR-100 eine vergleichbare Leistung wie progressive neuronale Netze erbringt, während es viel geringere Rechen- und Speicherkosten hat, und wir zeigen, dass BatchEnsemble leicht auf lebenslanges Lernen auf Split-ImageNet skalieren kann, das 100 sequenzielle Lernaufgaben umfasst.
Wir stellen eine neuartige Belohnungsschätzungsmethode vor, die auf einer endlichen Stichprobe optimaler Zustandstrajektorien von Expertendemonstrationen basiert und dazu verwendet werden kann, einen Agenten so zu führen, dass er das Verhalten des Experten nachahmt.Die optimalen Zustandstrajektorien werden verwendet, um ein generatives oder prädiktives Modell der "guten" Zustandsverteilung zu erlernen.Das Belohnungssignal wird durch eine Funktion der Differenz zwischen dem tatsächlichen nächsten Zustand, der vom Agenten erreicht wird, und dem vorhergesagten nächsten Zustand, der durch das erlernte generative oder prädiktive Modell gegeben ist, errechnet. Mit dieser abgeleiteten Belohnungsfunktion führen wir Standardverstärkungslernen in der inneren Schleife durch, um den Agenten zum Erlernen der gegebenen Aufgabe anzuleiten. Experimentelle Auswertungen über eine Reihe von Aufgaben zeigen, dass die vorgeschlagene Methode im Vergleich zum Standardverstärkungslernen mit vollständigen oder spärlichen, von Hand erstellten Belohnungen eine überlegene Leistung erbringt. Außerdem zeigen wir, dass unsere Methode einen Agenten erfolgreich in die Lage versetzt, gute Aktionen direkt von Expertenvideos von Spielen wie Super Mario Bros und Flappy Bird zu lernen.
Um zu erklären, wie das Modell mit der kompositorischen Semantik von Wörtern und Phrasen umgeht, untersuchen wir das hierarchische Erklärungsproblem. Wir betonen, dass die zentrale Herausforderung darin besteht, nicht-additive und kontextunabhängige Bedeutung für einzelne Wörter und Phrasen zu berechnen.Wir zeigen, dass einige frühere Bemühungen um hierarchische Erklärungen, z.B. kontextuelle Zerlegung, die gewünschten Eigenschaften mathematisch nicht erfüllen, was zu einer inkonsistenten Erklärungsqualität in verschiedenen Modellen führt. Wir modifizieren kontextuelle Dekompositionsalgorithmen entsprechend unserer Formulierung und schlagen einen modellunabhängigen Erklärungsalgorithmus mit konkurrenzfähiger Leistung vor. Die menschliche Bewertung und die automatische Bewertung von Metriken sowohl für LSTM-Modelle als auch für fein abgestimmte BERT-Transformer-Modelle auf mehreren Datensätzen zeigen, dass unsere Algorithmen frühere Arbeiten zu hierarchischen Erklärungen robust übertreffen. Wir zeigen, dass unsere Algorithmen helfen, die Kompositionalität der Semantik zu erklären, Klassifizierungsregeln zu extrahieren und das menschliche Vertrauen in Modelle zu verbessern.
Stochastischer Gradientenabstieg (SGD) mit stochastischem Momentum ist ein beliebtes Verfahren in der nicht-konvexen stochastischen Optimierung und insbesondere für das Training tiefer neuronaler Netze: Beim Standard-SGD werden die Parameter durch Verbesserung entlang des Gradientenpfads bei der aktuellen Iteration auf einer Reihe von Beispielen aktualisiert, wobei die Hinzufügung eines "Momentum"-Terms die Aktualisierung in Richtung der vorherigen Parameteränderung verzerrt. In der nicht-stochastischen konvexen Optimierung kann man zeigen, dass eine Momentum-Anpassung die Konvergenzzeit in vielen Fällen nachweislich verkürzt, aber solche Ergebnisse sind in stochastischen und nicht-konvexen Situationen schwer zu finden. In diesem Papier schlagen wir eine Antwort vor: Stochastisches Momentum verbessert das Training von tiefen Netzen, weil es SGD so modifiziert, dass es Sattelpunkten schneller entkommt und folglich schneller einen stationären Punkt zweiter Ordnung findet. Unsere theoretischen Ergebnisse beleuchten auch die damit zusammenhängende Frage, wie man den idealen Momentum-Parameter wählt - unsere Analyse legt nahe, dass $\beta \in [0,1)$ groß sein sollte (nahe bei 1), was mit den empirischen Ergebnissen übereinstimmt.Wir liefern auch experimentelle Ergebnisse, die diese Schlussfolgerungen weiter bestätigen.
GANs bieten einen Rahmen für das Training von generativen Modellen, die eine Datenverteilung nachahmen. In vielen Fällen möchten wir jedoch ein generatives Modell trainieren, um eine zusätzliche Zielfunktion innerhalb der von ihm erzeugten Daten zu optimieren, wie z. B. die Erstellung ästhetisch ansprechenderer Bilder. In einigen Fällen sind diese Zielfunktionen schwer zu bewerten, z. B. können sie menschliche Interaktion erfordern. Um dies zu erreichen, bauen wir ein Modell des menschlichen Verhaltens in der angestrebten Domäne aus einer relativ kleinen Menge von Interaktionen auf und verwenden dieses Verhaltensmodell dann als zusätzliche Verlustfunktion, um das generative Modell zu verbessern.
Semi-überwachtes Lernen (Semi-Supervised Learning, SSL) ist eine Studie, die eine große Menge an unmarkierten Daten effizient ausnutzt, um die Leistung unter den Bedingungen begrenzter markierter Daten zu verbessern Die meisten herkömmlichen SSL-Methoden gehen davon aus, dass die Klassen der unmarkierten Daten in der Menge der Klassen der markierten Daten enthalten sind. Darüber hinaus sortieren diese Methoden unbrauchbare unbeschriftete Proben nicht aus und verwenden alle unbeschrifteten Daten für das Lernen, was für realistische Situationen nicht geeignet ist.In diesem Papier schlagen wir eine SSL-Methode vor, die selektives Selbsttraining (SST) genannt wird und selektiv entscheidet, ob jede unbeschriftete Probe in den Trainingsprozess einbezogen wird. Für die konventionellen SSL-Probleme, die sich mit Daten befassen, bei denen sowohl die beschrifteten als auch die unbeschrifteten Proben die gleichen Klassenkategorien aufweisen, ist die vorgeschlagene Methode nicht nur mit anderen konventionellen SSL-Algorithmen vergleichbar, sondern kann auch mit anderen SSL-Algorithmen kombiniert werden. Während die konventionellen Methoden nicht auf die neuen SSL-Probleme angewendet werden können, bei denen die getrennten Daten nicht die gleichen Klassen haben, zeigt unsere Methode keine Leistungseinbußen, selbst wenn die Klassen der unmarkierten Daten sich von denen der markierten Daten unterscheiden.
Tiefe generative neuronale Netze haben sich sowohl bei der bedingten als auch bei der unbedingten Modellierung komplexer Datenverteilungen als effektiv erwiesen.Die bedingte Generierung ermöglicht eine interaktive Steuerung, aber die Erstellung neuer Steuerungen erfordert oft ein teures Neutraining.In diesem Papier entwickeln wir eine Methode zur bedingten Generierung ohne Neutraining des Modells.Durch post-hoc-Lernen von latenten Einschränkungen identifizieren Wertfunktionen Regionen im latenten Raum, die Ausgaben mit gewünschten Attributen erzeugen. Durch die Kombination von Attributbeschränkungen mit einer universellen "Realismus"-Beschränkung, die eine Ähnlichkeit mit der Datenverteilung erzwingt, generieren wir realistische bedingte Bilder aus einem unbedingten Variations-Autoencoder.Weiterhin demonstrieren wir unter Verwendung von gradientenbasierter Optimierung identitätserhaltende Transformationen, die die minimale Anpassung im latenten Raum vornehmen, um die Attribute eines Bildes zu verändern.Schließlich demonstrieren wir mit diskreten Sequenzen von Musiknoten eine Zero-Shot-Bedingungsgenerierung, die latente Beschränkungen in Abwesenheit von markierten Daten oder einer differenzierbaren Belohnungsfunktion lernt.
Jüngste Fortschritte in der Hardware und Methodik für das Training neuronaler Netze haben eine neue Generation großer Netze hervorgebracht, die mit einer Vielzahl von Daten trainiert werden und bei vielen NLP-Aufgaben erhebliche Genauigkeitsgewinne erzielt haben. In diesem Beitrag machen wir NLP-Forscher auf dieses Problem aufmerksam, indem wir die ungefähren finanziellen und ökologischen Kosten für das Training einer Reihe von kürzlich erfolgreichen neuronalen Netzwerkmodellen für NLP quantifizieren und darauf aufbauend Empfehlungen zur Kostensenkung und Verbesserung der Gerechtigkeit in der NLP-Forschung und -Praxis geben.
Viele Modelle, die auf dem Variationalen Autoencoder basieren, werden vorgeschlagen, um eine Entflechtung latenter Variablen in der Inferenz zu erreichen. Die meisten aktuellen Arbeiten konzentrieren sich jedoch auf die Entwicklung leistungsfähiger Regularisierer zur Entflechtung, während die gegebene Anzahl von Dimensionen für die latente Repräsentation bei der Initialisierung die Entflechtung stark beeinträchtigen könnte. Daher wird ein Pruning-Mechanismus eingeführt, der automatisch die intrinsische Dimension der Daten anstrebt und gleichzeitig die Entwirrung der Repräsentationen fördert. Die vorgeschlagene Methode wird an MPI3D und MNIST validiert, um den Stand der Technik in Bezug auf Entwirrung, Rekonstruktion und Robustheit zu übertreffen. Der Code wird auf https://github.com/WeyShi/FYP-of-Disentanglement bereitgestellt.
Wir erforschen die Eigenschaften von rekurrenten Sprachmodellen auf Byte-Ebene und stellen fest, dass bei ausreichender Kapazität, Trainingsdaten und Rechenzeit die von diesen Modellen gelernten Repräsentationen entwirrte Merkmale enthalten, die High-Level-Konzepten entsprechen, insbesondere finden wir eine einzige Einheit, die eine Stimmungsanalyse durchführt. Wir zeigen auch, dass die Sentiment-Einheit einen direkten Einfluss auf den generativen Prozess des Modells hat: Wenn man ihren Wert einfach auf positiv oder negativ setzt, werden Proben mit dem entsprechenden positiven oder negativen Sentiment erzeugt.
 Das Sampling diskreter latenter Variablen kann aus zwei Gründen zu hochvariablen Gradientenschätzern führen: 1) Verzweigung auf den Stichproben innerhalb des Modells und2) das Fehlen einer pfadweisen Ableitung für die Stichproben. Während die aktuellen State-of-the-Art-Methoden Kontroll-Variate-Schemata für das erste und Continuous-Relaxation-Methoden für das zweite Problem verwenden, ist ihr Nutzen durch die Komplexität der Implementierung und des Trainings effektiver Kontroll-Variate-Schemata und die Notwendigkeit der Auswertung von (potentiell exponentiell) vielen Verzweigungspfaden im Modell begrenzt. Hier stellen wir den RWS-Algorithmus (RWS; Bornschein und Bengio, 2015) erneut vor und zeigen anhand umfangreicher Auswertungen, dass er diese beiden Probleme umgeht und die aktuellen State-of-the-Art-Methoden beim Lernen diskreter latent-variabler Modelle übertrifft. Darüber hinaus stellen wir fest, dass RWS im Gegensatz zum Importance-weighted Autoencoder mit zunehmender Partikelanzahl bessere Modelle und Inferenznetzwerke lernt und dass sich seine Vorteile auch auf kontinuierliche latent-variable Modelle erstrecken.Unsere Ergebnisse deuten darauf hin, dass RWS eine konkurrenzfähige, oft vorzuziehende Alternative für das Lernen tiefer generativer Modelle ist.
Das Ziel beim Deep-Extreme-Multilabel-Learning ist es, gemeinsam Merkmalsrepräsentationen und Klassifikatoren zu erlernen, um Datenpunkte automatisch mit der relevantesten Teilmenge von Etiketten aus einem extrem großen Etikettensatz zu kennzeichnen.Leider sind Deep-Extreme-Klassifikatoren nach dem Stand der Technik entweder nicht skalierbar oder ungenau für kurze Textdokumente. In diesem Papier wird der DeepXML-Algorithmus entwickelt, der beide Einschränkungen durch die Einführung einer neuartigen Architektur behebt, die das Training von Kopf- und Fußetiketten aufteilt.  DeepXML erhöht die Genauigkeit, indem es (a) Worteinbettungen auf Head-Labels lernt und sie durch eine neuartige Restverbindung auf datenarme Tail-Labels überträgt; (b) die Menge der verfügbaren negativen Trainingsdaten durch die Erweiterung modernster Negative-Sub-Sampling-Techniken erhöht; und(c) die Menge der vorhergesagten Labels neu einordnet, um die härtesten Negative für den ursprünglichen Klassifikator zu eliminieren.Alle diese Beiträge werden effizient implementiert, indem der hoch skalierbare Slice-Algorithmus für vortrainierte Einbettungen erweitert wird, um die vorgeschlagene DeepXML-Architektur zu lernen. Das Ergebnis ist, dass DeepXML effizient auf Probleme mit Millionen von Labels skaliert werden kann, die jenseits der Möglichkeiten modernster Deep-Extrem-Klassifikatoren liegen, da es mehr als 10x schneller trainieren kann als XML-CNN und AttentionXML. Gleichzeitig wurde empirisch festgestellt, dass DeepXML bis zu 19% genauer ist als führende Techniken zum Abgleich von Suchmaschinenanfragen mit Gebotsformulierungen von Werbetreibenden.
Robuste Schätzung unter Hubers $\epsilon$-Kontaminationsmodell ist zu einem wichtigen Thema in der Statistik und theoretischen Informatik geworden. Ratenoptimale Verfahren wie Tukeys Median und andere Schätzer, die auf statistischen Tiefenfunktionen basieren, sind aufgrund ihrer rechnerischen Intractabilität unpraktisch. Ähnlich wie bei der Herleitung von f-GAN zeigen wir, dass diese Tiefenfunktionen, die zu ratenoptimalen robusten Schätzern führen, alle als Variationsschranken der totalen Variationsdistanz im Rahmen von f-Learning betrachtet werden können. Insbesondere zeigen wir, dass ein JS-GAN, das einen neuronalen Netzdiskriminator mit mindestens einer versteckten Schicht verwendet, in der Lage ist, die Minimax-Rate der robusten Mittelwertschätzung unter Hubers $\epsilon$-Kontaminationsmodell zu erreichen.Interessanterweise wird gezeigt, dass die versteckten Schichten der neuronalen Netzstruktur in der Diskriminator-Klasse für die robuste Schätzung notwendig sind.
Long Short-Term Memory (LSTM) ist eines der leistungsstärksten Sequenzmodelle.Trotz der starken Leistung, fehlt es jedoch an der schönen Interpretierbarkeit wie in Zustandsraummodellen.In diesem Papier präsentieren wir eine Möglichkeit, das Beste aus beiden Welten zu kombinieren, indem wir State Space LSTM (SSL) einführen, das die frühere Arbeit \cite{zaheer2017latent} der Kombination von Themenmodellen mit LSTM verallgemeinert. Im Gegensatz zu \cite{zaheer2017latent} machen wir jedoch keine Faktorisierungsannahmen in unserem Inferenzalgorithmus.Wir präsentieren einen effizienten Sampler, der auf einer sequentiellen Monte-Carlo-Methode (SMC) basiert, die direkt aus dem gemeinsamen Posterior zieht.Experimentelle Ergebnisse bestätigen die Überlegenheit und Stabilität dieses SMC-Inferenzalgorithmus in einer Vielzahl von Domänen.
In dieser Arbeit wollen wir dieses Problem durch hierarchische Bayes'sche Programminduktion angehen und stellen einen neuartigen Lernalgorithmus vor, der Konzepte als kurze, generative, stochastische Programme ableiten kann, während er gleichzeitig einen globalen Prior über die Programme lernt, um die Generalisierung zu verbessern, und ein Erkennungsnetzwerk für effiziente Inferenz. Unser Algorithmus, Wake-Sleep-Remember (WSR), kombiniert Gradientenlernen für kontinuierliche Parameter mit neural-geführter Suche über Programme.Wir zeigen, dass WSR zwingende latente Programme in zwei harten symbolischen Domänen lernt: zelluläre Automaten und Gaußsche Prozesskerne.Wir sammeln und bewerten auch einen neuen Datensatz, Text-Concepts, um strukturierte Muster in natürlichen Textdaten zu entdecken.
Das Wissen über die Funktion von Proteinen ist notwendig, da es ein klares Bild von biologischen Prozessen gibt.Dennoch gibt es viele Proteinsequenzen, die gefunden und zu den Datenbanken hinzugefügt wurden, aber es fehlt eine funktionale Annotation.Die Laborexperimente nehmen eine beträchtliche Menge an Zeit für die Annotation der Sequenzen.Daraus ergibt sich die Notwendigkeit, computergestützte Techniken zu verwenden, um Proteine basierend auf ihren Funktionen zu klassifizieren. In unserer Arbeit haben wir die Daten von Swiss-Prot gesammelt, die 40433 Proteine enthalten, die in 30 Familien gruppiert sind, die wir an ein rekurrentes neuronales Netzwerk (RNN), ein Long Short Term Memory (LSTM) und ein Gated Recurrent Unit (GRU)-Modell weitergeben und durch die Anwendung von Trigram mit einem tiefen neuronalen Netzwerk und einem flachen neuronalen Netzwerk auf denselben Datensatz vergleichen. 
Algorithmen für die Erkennung von gesprochenen Begriffen (STD) zielen oft darauf ab, die Lücke zwischen den Bewertungen positiver und negativer Beispiele zu maximieren, d.h. sie konzentrieren sich darauf, sicherzustellen, dass Äußerungen, in denen der Begriff vorkommt, höher eingestuft werden als Äußerungen, in denen der Begriff nicht vorkommt, ohne jedoch eine Erkennungsschwelle zwischen den beiden zu bestimmen. Der Vorteil der Minimierung dieser Verlustfunktion während des Trainings besteht darin, dass nicht nur die relativen Rangfolgeergebnisse maximiert werden, sondern das System auch so eingestellt wird, dass ein fester Schwellenwert verwendet wird, wodurch die Robustheit des Systems erhöht und die Erkennungsgenauigkeit maximiert wird. Wir verwenden die neue Verlustfunktion in der strukturierten Vorhersageeinstellung und erweitern den diskriminativen Keyword-Spotting-Algorithmus für das Lernen des Detektors für gesprochene Begriffe mit einem einzigen Schwellenwert für alle Begriffe. Wir demonstrieren außerdem die Wirksamkeit der neuen Verlustfunktion, indem wir sie auf ein tiefes neuronales Siamesisches Netzwerk in einer schwach überwachten Einstellung für die schablonenbasierte Detektion von gesprochenen Begriffen anwenden, wiederum mit einem einzigen festen Schwellenwert.Experimente mit den TIMIT-, WSJ- und Switchboard-Korpora zeigten, dass unser Ansatz nicht nur die Genauigkeitsraten verbesserte, wenn ein fester Schwellenwert verwendet wurde, sondern auch eine höhere Area Under Curve (AUC) erzielte.
Föderiertes Lernen beinhaltet gemeinsames Lernen über massiv verteilte Partitionen von Daten, die auf entfernten Geräten generiert wurden. Eine naive Minimierung einer aggregierten Verlustfunktion in einem solchen Netzwerk kann einige der Geräte unverhältnismäßig begünstigen oder benachteiligen. In dieser Arbeit schlagen wir q-Fair Federated Learning (q-FFL) vor, ein neuartiges Optimierungsziel, das von Strategien zur Ressourcenzuteilung in drahtlosen Netzwerken inspiriert ist und eine gerechtere Verteilung der Genauigkeit auf die Geräte in föderierten Netzwerken fördert. q-FFL wird durch eine skalierbare Methode, q-FedAvg, gelöst, die in föderierten Netzwerken ausgeführt werden kann. q-FFL wird durch Simulationen auf föderierten Datensätzen validiert.
Wir trainieren einen Generator und einen Encoder gemeinsam und in einer adversen Art und Weise.Das Generator-Netzwerk lernt, realistische Objekte zu sampeln.Das Encoder-Netzwerk wiederum wird gleichzeitig trainiert, um die wahre Datenverteilung auf den Prior im latenten Raum abzubilden.Um gute Rekonstruktionen zu gewährleisten, führen wir einen erweiterten adversen Rekonstruktionsverlust ein. Hier trainieren wir einen Diskriminator, um zwei Arten von Paaren zu unterscheiden: ein Objekt mit seiner Augmentation und eines mit seiner Rekonstruktion. Wir zeigen, dass ein solcher adversarialer Verlust Objekte auf der Grundlage des Inhalts und nicht auf der exakten Übereinstimmung vergleicht. Wir zeigen experimentell, dass unser Modell Proben und Rekonstruktionen von einer Qualität erzeugt, die mit dem Stand der Technik auf den Datensätzen MNIST, CIFAR10 und CelebA konkurriert, und gute quantitative Ergebnisse auf CIFAR10 erzielt.
Wir schlagen einen robusten Bayes'schen Deep-Learning-Algorithmus vor, um komplexe Posterioren mit latenten Variablen abzuleiten. Inspiriert von Dropout, einem beliebten Werkzeug für Regularisierung und Modell-Ensemble, weisen wir den Gewichten in tiefen neuronalen Netzen (DNN) spärliche Prioren zu, um ein automatisches "Dropout" zu erreichen und Überanpassung zu vermeiden. Durch alternatives Sampling aus der Posterior-Verteilung mittels stochastischer Gradienten-Markov-Ketten-Monte-Carlo (SG-MCMC) und Optimierung der latenten Variablen mittels stochastischer Approximation (SA) konvergiert die Trajektorie der Zielgewichte nachweislich zur wahren Posterior-Verteilung unter der Bedingung optimaler latenter Variablen. Simulationen von large-p-small-n Regressionen zeigen die Robustheit dieser Methode, wenn sie auf Modelle mit latenten Variablen angewandt wird. Darüber hinaus führt ihre Anwendung auf Faltungsneuronale Netze (CNN) zu einer State-of-the-Art-Performance auf MNIST- und Fashion-MNIST-Datensätzen und zu einer verbesserten Resistenz gegen Angriffe von außen.
Die Aktivität von Populationen sensorischer Neuronen trägt Stimulusinformationen sowohl in der zeitlichen als auch in der räumlichen Dimension, was die Frage aufwirft, wie man alle Informationen, die die Populationscodes über all diese Dimensionen tragen, kompakt darstellen kann. Anhand von Daten, die von retinalen Ganglienzellen mit einer starken Basislinie vor dem Stimulus aufgezeichnet wurden, haben wir gezeigt, dass unsere Erweiterungen in Situationen, in denen der Stimulus eine starke Änderung der Feuerungsrate hervorruft, eine Verbesserung der Stimulusdekodierungsleistung bewirken.
Wir schlagen einen Rahmen für extreme gelernte Bildkompression auf der Basis von Generative Adversarial Networks (GANs) vor, der visuell ansprechende Bilder bei deutlich niedrigeren Bitraten als bisherige Methoden ermöglicht. Wenn eine semantische Labelkarte des Originalbildes verfügbar ist, kann unsere Methode außerdem unwichtige Regionen im dekodierten Bild wie Straßen und Bäume vollständig aus der Labelkarte synthetisieren, so dass nur die Speicherung der erhaltenen Region und der semantischen Labelkarte erforderlich ist.
Gängige Ansätze lernen eine Bewertungsfunktion, die Elemente einzeln (d.h. ohne den Kontext anderer Elemente in der Liste) bewertet, indem ein punktweiser, paarweiser oder listenweiser Verlust optimiert wird. Die Liste wird dann in der absteigenden Reihenfolge der Bewertungen sortiert. Mögliche Wechselwirkungen zwischen Elementen, die in derselben Liste vorhanden sind, werden in der Trainingsphase auf der Verlustebene berücksichtigt. In diesem Papier schlagen wir ein kontextbewusstes neuronales Netzmodell vor, das die Bewertung von Elementen durch Anwendung eines Mechanismus der Selbstaufmerksamkeit lernt, so dass die Relevanz eines bestimmten Elements im Kontext aller anderen Elemente in der Liste bestimmt wird, sowohl beim Training als auch bei der Inferenz.
Wir schlagen eine aktive Lernalgorithmus-Architektur vor, die in der Lage ist, ihren Lernprozess zu organisieren, um ein Feld komplexer Aufgaben durch das Lernen von Sequenzen primitiver motorischer Strategien zu erreichen: Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB) Der Lernende kann seine Erfahrungen verallgemeinern, um kontinuierlich neue Ergebnisse zu erlernen, indem er aktiv auswählt, was und wie er lernt und sich dabei von empirischen Messungen seines eigenen Fortschritts leiten lässt.In diesem Papier betrachten wir das Erlernen einer Reihe miteinander verbundener komplexer Ergebnisse, die hierarchisch organisiert sind.Wir führen einen neuen Rahmen ein, der "Prozeduren" genannt wird und die autonome Entdeckung ermöglicht, wie zuvor erlernte Fähigkeiten kombiniert werden können, um zunehmend komplexere motorische Strategien (Kombinationen primitiver motorischer Strategien) zu erlernen. Wir zeigen in einer simulierten Umgebung, dass unsere neue Architektur in der Lage ist, das Erlernen komplexer motorischer Strategien in Angriff zu nehmen und die Komplexität ihrer Strategien an die jeweilige Aufgabe anzupassen, und wir zeigen auch, dass unsere "Prozeduren" die Fähigkeit des Agenten zum Erlernen komplexer Aufgaben erhöhen.
Monte Carlo Tree Search (MCTS) hat beeindruckende Ergebnisse in einer Reihe von diskreten Umgebungen erzielt, wie z.B. Go, Mario und Arcade-Spiele, aber es hat noch nicht sein wahres Potenzial in kontinuierlichen Domänen erfüllt.In dieser Arbeit stellen wirTPO vor, eine auf Baumsuche basierende Methode zur Optimierung von Richtlinien für kontinuierliche Umgebungen.TPO verfolgt einen hybriden Ansatz zur Richtlinienoptimierung.  Um diese Herausforderungen zu überwinden, schlagen wir vor, den Verzweigungsfaktor der Baumsuche zu begrenzen, indem wir nur wenige Aktionsproben aus der Policy-Verteilung ziehen und eine neue Verlustfunktion definieren, die auf den Mittelwerten und Standardabweichungen der Trajektorien basiert.  Unser Ansatz hat zu einigen nicht-intuitiven Ergebnissen geführt.  Wir haben jedoch festgestellt, dass die Bootstrapping-Baumsuche mit einer vortrainierten Strategie es uns ermöglicht, qualitativ hochwertige Ergebnisse mit einem niedrigen MCTS-Verzweigungsfaktor und einer geringen Anzahl von Simulationen zu erzielen.Ohne das vorgeschlagene Bootstrapping der Strategie würde ein kontinuierliches MCTS einen viel größeren Verzweigungsfaktor und eine viel größere Anzahl von Simulationen erfordern, was es rechnerisch unerschwinglich macht.In unseren Experimenten verwenden wir PPO als unseren Basis-Algorithmus zur Optimierung der Strategie.TPO verbessert die Strategie bei fast allen unseren Benchmarks erheblich.  In komplexen Umgebungen wie z.B. Humanoid erreichen wir eine 2,5fache Verbesserung gegenüber dem Basisalgorithmus.
Der Variational Autoencoder (VAE) hat sich bei der Modellierung der Vielfalt natürlicher Bilder in bestimmten Datensätzen als erfolgreich erwiesen und ermöglicht die Generierung aussagekräftiger Bilder durch Interpolation oder Extrapolation im latenten Kodierraum, aber es ist unklar, ob ähnliche Fähigkeiten für Text angesichts seiner diskreten Natur möglich sind. Wir stellen fest, dass herkömmliche Sequenz-VAEs bis zu einem gewissen Grad entwirrte Repräsentationen durch ihre latenten Codes lernen können, aber sie scheitern oft bei der korrekten Dekodierung, wenn der latente Faktor manipuliert wird, weil die manipulierten Codes oft in Löchern oder leeren Regionen im aggregierten posterioren latenten Raum landen, für deren Verarbeitung das Dekodierungsnetzwerk nicht trainiert ist. Unsere vorgeschlagene Methode entschärft das Problem der latenten Lücken und erzielt den ersten Erfolg beim unüberwachten Lernen von kontrollierbaren Repräsentationen für Text.Empirisch gesehen übertrifft unsere Methode die unüberwachten Baselines signifikant und ist konkurrenzfähig mit stark überwachten Ansätzen zur Übertragung von Textstilen.Wenn der latente Faktor (z.B. Thema) während einer langen Satzgenerierung gewechselt wird, wird die Manipulation innerhalb dieses Simplexes durchgeführt, Wenn der latente Faktor (z.B. das Thema) während der Generierung eines langen Satzes gewechselt wird, kann der von uns vorgeschlagene Rahmen den Satz oft auf eine scheinbar natürliche Weise vervollständigen - eine Fähigkeit, die von früheren Methoden nie versucht wurde.
In dieser Arbeit haben wir ein hierarchisches Netzwerkmodell entwickelt, das wir Hierarchical Prediction Network (HPNet) nennen, um zu verstehen, wie räumlich-zeitliche Erinnerungen in einer Repräsentationshierarchie für die Vorhersage zukünftiger Videobilder erlernt und kodiert werden können.Das Modell ist inspiriert von den Feedforward-, Feedback- und lateralen rekurrenten Schaltkreisen im hierarchischen visuellen System von Säugetieren. Das Modell enthält einen Feed-Forward-Pfad, der räumlich-zeitliche Merkmale sukzessiver Komplexität berechnet und kodiert, und einen Feedback-Pfad, der die Interpretation von einer höheren Ebene auf die darunter liegende Ebene projiziert.Innerhalb jeder Ebene kreuzen sich der Feed-Forward-Pfad und der Feedback-Pfad in einem rekurrenten Gated-Schaltkreis, der ihre Signale sowie die internen Speicherzustände des Schaltkreises integriert, um eine Vorhersage der eingehenden Signale zu erzeugen. Das Netzwerk lernt, indem es die eingehenden Signale mit seiner Vorhersage vergleicht und sein internes Modell der Welt durch Minimierung der Vorhersagefehler auf jeder Ebene der Hierarchie im Stil des prädiktiven selbstüberwachten Lernens aktualisiert. Das Netzwerk verarbeitet Daten in Blöcken von Videobildern und nicht auf der Basis von Einzelbildern.  Wir haben beobachtet, dass die hierarchische Interaktion im Netzwerk zu einer Empfindlichkeit gegenüber Erinnerungen an globale Bewegungsmuster führt, sogar in der Populationsdarstellung der Einheiten auf der frühesten Ebene.Schließlich haben wir neurophysiologische Beweise geliefert, die zeigen, dass Neuronen im frühen visuellen Kortex von wachen Affen eine sehr ähnliche Empfindlichkeit und ein ähnliches Verhalten zeigen.Diese Ergebnisse deuten darauf hin, dass prädiktives selbstüberwachtes Lernen ein wichtiges Prinzip für repräsentatives Lernen im visuellen Kortex sein könnte.  
Wir stellen einen empirischen Ansatz vor, der auf kontrafaktischer Argumentation beruht, um die von Saliency Maps generierten Hypothesen zu testen, und zeigen, dass die von Saliency Maps vorgeschlagenen Erklärungen oft nicht durch Experimente gestützt werden.Unsere Experimente deuten darauf hin, dass Saliency Maps am besten als Forschungsinstrument und nicht als Erklärungsinstrument betrachtet werden sollten.
Eine der ungelösten Fragen beim Deep Learning ist die Art der Lösungen, die entdeckt werden: Wir untersuchen die Sammlung von Lösungen, die von derselben Netzwerkarchitektur mit unterschiedlicher zufälliger Initialisierung der Gewichte und zufälligen Mini-Batches erreicht werden.  Überraschenderweise scheinen alle Netzinstanzen die gleiche Lerndynamik zu haben, wobei zunächst dieselben Trainings- und Testbeispiele vom gelernten Modell richtig erkannt werden, gefolgt von anderen Beispielen, die in etwa der gleichen Reihenfolge gelernt werden. Wenn man die Untersuchung auf heterogene Ansammlungen von neuronalen Netzarchitekturen ausweitet, zeigt sich erneut, dass die Beispiele unabhängig von der Architektur in der gleichen Reihenfolge gelernt werden, obwohl die leistungsfähigere Architektur weiter lernen und somit eine höhere Genauigkeit erreichen kann. Um die Robustheit dieser Phänomene zu zeigen, geben wir eine ausführliche Zusammenfassung unserer empirischen Studie, die Hunderte von Graphen umfasst, die Zehntausende von Netzwerken mit unterschiedlichen NN-Architekturen, Hyperparametern und Domänen beschreiben.Wir diskutieren auch Fälle, in denen dieses Ähnlichkeitsmuster zusammenbricht, was zeigt, dass die gemeldete Ähnlichkeit kein Artefakt der Optimierung durch Gradientenabstieg ist.Vielmehr ist das beobachtete Ähnlichkeitsmuster charakteristisch für das Lernen komplexer Probleme mit großen Netzwerken.Schließlich zeigen wir, dass dieses Ähnlichkeitsmuster stark mit effektiver Generalisierung korreliert zu sein scheint.
Wir schlagen einen einheitlichen Rahmen für den Aufbau von unbeaufsichtigten Repräsentationen einzelner Objekte oder Entitäten (und deren Zusammensetzungen) vor, indem wir jedem Objekt sowohl eine Verteilungsschätzung als auch eine Punktschätzung (Vektoreinbettung) zuordnen, was durch die Verwendung von optimalem Transport ermöglicht wird, der es uns erlaubt, diese zugehörigen Schätzungen aufzubauen, während wir die zugrundeliegende Geometrie des Bodenraums ausnutzen Unsere Methode bietet eine neuartige Perspektive für den Aufbau reichhaltiger und leistungsstarker Merkmalsrepräsentationen, die gleichzeitig Unsicherheit (über eine Verteilungsschätzung) und Interpretierbarkeit (mit der optimalen Transportkarte) erfassen. Empirische Ergebnisse zeigen starke Vorteile, die durch den vorgeschlagenen Rahmen gewonnen werden, und dieser Ansatz kann für jedes unbeaufsichtigte oder überwachte Problem (auf Text oder anderen Modalitäten) mit einer Koinzidenzstruktur, wie z.B. Sequenzdaten, verwendet werden.
In diesem Papier schlagen wir zwei Methoden vor, nämlich die Trace-norm regression (TNR) und die Stable Trace-norm Analysis (StaTNA), um die Leistung von Empfehlungssystemen mit Nebeninformationen zu verbessern Unser Ansatz der Trace-norm regression extrahiert latente Faktoren mit niedrigem Rang, die den Nebeninformationen zugrunde liegen und die Benutzerpräferenzen in verschiedenen Kontexten beeinflussen. Darüber hinaus erfasst unser neuartiger Empfehlungsrahmen StaTNA nicht nur latente Low-Rank-Faktoren, sondern berücksichtigt auch den idiosynkratischen Geschmack einzelner Nutzer. Wir vergleichen die Leistungen von TNR und StaTNA auf den MovieLens-Datensätzen mit State-of-the-Art-Modellen und zeigen, dass StaTNA und TNR diese Methoden im Allgemeinen übertreffen.
Textbasierte Computerspiele beschreiben ihre Welt für den Spieler durch natürliche Sprache und erwarten, dass der Spieler mit dem Spiel durch Text interagiert. Diese Spiele sind von Interesse, da sie als Testumgebung für Sprachverständnis, Problemlösung und Spracherzeugung durch künstliche Agenten angesehen werden können. Darüber hinaus bieten sie eine Lernumgebung, in der diese Fähigkeiten durch Interaktionen mit einer Umgebung erworben werden können, anstatt durch die Verwendung von festen Korpora. Ein Aspekt, der diese Spiele für lernende Agenten besonders herausfordernd macht, ist der kombinatorisch große Aktionsraum.Bestehende Methoden zum Lösen textbasierter Spiele sind auf Spiele beschränkt, die entweder sehr einfach sind oder einen Aktionsraum haben, der auf eine vorgegebene Menge zulässiger Aktionen beschränkt ist.In dieser Arbeit schlagen wir vor, den Explorationsansatz von Go-Explore (Ecoffet et al., Unsere Experimente zeigen, dass dieser Ansatz bestehende Lösungen bei der Lösung textbasierter Spiele übertrifft und in Bezug auf die Anzahl der Interaktionen mit der Umgebung effizienter ist.Darüber hinaus zeigen wir, dass die gelernte Strategie besser als bestehende Lösungen auf unbekannte Spiele verallgemeinert werden kann, ohne dass eine Einschränkung des Aktionsraums verwendet wird.
In der kürzlich erschienenen Arbeit "Lottery Ticket Hypothesis" von Frankle & Carbin wurde gezeigt, dass ein einfacher Ansatz zur Erstellung von spärlichen Netzwerken (Beibehaltung der großen Gewichte) zu Modellen führt, die von Grund auf trainierbar sind, aber nur, wenn man von denselben anfänglichen Gewichten ausgeht.Die Leistung dieser Netzwerke übertrifft oft die Leistung des nicht spärlichen Basismodells, aber aus Gründen, die nicht gut verstanden wurden.In dieser Arbeit untersuchen wir die drei kritischen Komponenten des Lottery Ticket (LT)-Algorithmus und zeigen, dass jede von ihnen erheblich variiert werden kann, ohne die Gesamtergebnisse zu beeinflussen. Wir zeigen, warum es wichtig ist, die Gewichte auf Null zu setzen, wie Vorzeichen ausreichen, um ein neu initialisiertes Netzwerk zu trainieren, und warum sich Maskierung wie Training verhält. Schließlich entdecken wir die Existenz von Supermasken, die auf ein untrainiertes, zufällig initialisiertes Netzwerk angewendet werden können, um ein Modell mit einer Leistung zu erzeugen, die weit über dem Zufall liegt (86% bei MNIST, 41% bei CIFAR-10).
In dieser Studie konzentrierten wir uns auf ein solches Self-Attention-Netzmodell, nämlich BERT, das in Bezug auf das Stapeln von Schichten in verschiedenen Sprachverständnis-Benchmarks gut abgeschnitten hat. In vielen nachgelagerten Aufgaben werden jedoch Informationen zwischen Schichten von BERT für die Feinabstimmung ignoriert. In Anbetracht dieser Vor- und Nachteile wird in diesem Papier SesameBERT vorgeschlagen, eine verallgemeinerte Feinabstimmungsmethode, die (1) die Extraktion globaler Informationen zwischen allen Schichten durch Squeeze und Excitation ermöglicht und (2) lokale Informationen durch die Erfassung benachbarter Kontexte mittels Gaußscher Unschärfe anreichert. Darüber hinaus haben wir die Effektivität unseres Ansatzes im HANS-Datensatz demonstriert, der verwendet wird, um festzustellen, ob Modelle flache Heuristiken übernommen haben, anstatt zugrundeliegende Verallgemeinerungen zu lernen. Die Experimente zeigten, dass SesameBERT BERT in Bezug auf den GLUE-Benchmark und den HANS-Evaluierungssatz übertraf.
Eine wachsende Zahl von Lernmethoden sind eigentlich differenzierbare Spiele, deren Spieler mehrere, voneinander abhängige Ziele parallel optimieren - von GANs und intrinsischer Neugier bis hin zu Multi-Agenten-RL.Opponent Shaping ist ein leistungsfähiger Ansatz zur Verbesserung der Lerndynamik in diesen Spielen, der den Einfluss der Spieler auf die Aktualisierungen der anderen berücksichtigt. Lernen mit Opponent-Learning Awareness (LOLA) ist ein neuerer Algorithmus, der diese Reaktion ausnutzt und zu Kooperation in Situationen wie dem Iterierten Gefangenendilemma führt.Obwohl experimentell erfolgreich, zeigen wir, dass LOLA-Agenten ein "arrogantes" Verhalten zeigen können, das der Konvergenz direkt entgegensteht. In diesem Papier stellen wir Stable Opponent Shaping (SOS) vor, eine neue Methode, die zwischen LOLA und einer stabilen Variante namens LookAhead interpoliert. wir beweisen, dass LookAhead lokal zu Gleichgewichten konvergiert und strikte Sättel in allen differenzierbaren Spielen vermeidet. SOS erbt diese wesentlichen Garantien, während es auch das Lernen der Gegner formt und LOLA experimentell entweder entspricht oder übertrifft.
Viele dieser Fragen sind nicht eindeutig, und eine zuverlässige Identifizierung ähnlicher Fragen würde eine effizientere und effektivere Beantwortung von Fragen ermöglichen.Während sich viele Forschungsbemühungen auf das Problem der allgemeinen Frageähnlichkeit konzentriert haben, lassen sich diese Ansätze nicht gut auf den medizinischen Bereich verallgemeinern, in dem oft medizinische Fachkenntnisse erforderlich sind, um die semantische Ähnlichkeit zu bestimmen. Während andere Pre-Training-Aufgaben eine Genauigkeit von weniger als 78,7 % für diese Aufgabe liefern, erreicht unser Modell eine Genauigkeit von 82,6 % mit der gleichen Anzahl von Trainingsbeispielen, eine Genauigkeit von 80,0 % mit einem viel kleineren Trainingssatz und eine Genauigkeit von 84,5 %, wenn der gesamte Korpus an medizinischen Frage-Antwort-Daten verwendet wird.
Wir untersuchen den Nutzen der gemeinsamen Nutzung von Repräsentationen zwischen Aufgaben, um den effektiven Einsatz von tiefen neuronalen Netzen im Multi-Task Reinforcement Learning zu ermöglichen. wir nutzen die Annahme, dass das Lernen von verschiedenen Aufgaben, die gemeinsame Eigenschaften haben, hilfreich ist, um das Wissen über sie zu verallgemeinern, was zu einer effektiveren Merkmalsextraktion im Vergleich zum Lernen einer einzelnen Aufgabe führt. intuitiv bietet der resultierende Satz von Merkmalen Leistungsvorteile, wenn er von Reinforcement Learning Algorithmen verwendet wird. Wir beweisen dies, indem wir theoretische Garantien liefern, die die Bedingungen hervorheben, unter denen es sinnvoll ist, Repräsentationen zwischen Aufgaben zu teilen, und die bekannten endlichen Zeitgrenzen der Approximate Value-Iteration auf die Multitask-Einstellung ausdehnen.Zusätzlich ergänzen wir unsere Analyse, indem wir Multitask-Erweiterungen von drei Reinforcement-Learning-Algorithmen vorschlagen, die wir empirisch auf weit verbreiteten Reinforcement-Learning-Benchmarks evaluieren, die signifikante Verbesserungen gegenüber den Single-Task-Gegenstücken in Bezug auf Stichprobeneffizienz und Leistung zeigen.
Wir stellen eine 3D-Kapselarchitektur für die Verarbeitung von Punktwolken vor, die bezüglich der SO(3)-Rotationsgruppe, der Translation und der Permutation der ungeordneten Eingabesätze äquivariant ist. Das Netzwerk arbeitet mit einem spärlichen Satz lokaler Referenzrahmen, die aus einer Eingabepunktwolke berechnet werden, und stellt eine Ende-zu-Ende-Äquivarianz durch eine neuartige 3D-Quaternion-Gruppen-Kapselschicht her, einschließlich eines äquivarianten dynamischen Routing-Verfahrens. Dabei verbinden wir den Prozess des dynamischen Routings zwischen den Kapseln theoretisch mit dem bekannten Weiszfeld-Algorithmus, einem Schema zur Lösung iterativer, neu gewichteter kleinster Quadrate (IRLS) mit nachweisbaren Konvergenzeigenschaften, das eine robuste Posenschätzung zwischen den Kapselschichten ermöglicht.Aufgrund der spärlichen äquivarianten Quaternion-Kapseln ermöglicht unsere Architektur eine gemeinsame Objektklassifikation und Orientierungsschätzung, die wir empirisch an gängigen Benchmark-Datensätzen validieren. 
Vektorsemantik, insbesondere Satzvektoren, wurden in letzter Zeit erfolgreich in vielen Bereichen der Verarbeitung natürlicher Sprache eingesetzt.Allerdings hat relativ wenig Arbeit die interne Struktur und Eigenschaften von Räumen von Satzvektoren erforscht.In diesem Papier werden wir die Eigenschaften von Satzvektoren durch die Untersuchung einer bestimmten realen Anwendung zu erkunden: Insbesondere zeigen wir, dass die Kosinus-Ähnlichkeit zwischen Satzvektoren und Dokumentvektoren stark mit der Satzbedeutung korreliert und dass die Vektorsemantik Lücken zwischen den bisher ausgewählten Sätzen und dem Dokument identifizieren und korrigieren kann.Darüber hinaus identifizieren wir spezifische Dimensionen, die mit effektiven Zusammenfassungen verbunden sind.Unseres Wissens ist dies das erste Mal, dass spezifische Dimensionen von Satzeinbettungen mit Satzeigenschaften in Verbindung gebracht wurden.Wir vergleichen auch die Eigenschaften verschiedener Methoden von Satzeinbettungen.Viele dieser Erkenntnisse haben Anwendungen in Anwendungen von Satzeinbettungen weit über die Zusammenfassung hinaus.
Wir stellen Value Propagation (VProp) vor, ein parametereffizientes differenzierbares Planungsmodul, das auf Value Iteration aufbaut und erfolgreich in einer Reinforcement-Learning-Methode trainiert werden kann, um ungesehene Aufgaben zu lösen, die Fähigkeit besitzt, sich auf größere Kartengrößen zu verallgemeinern und zu lernen, in dynamischen Umgebungen zu navigieren. Wir evaluieren das Modul auf Konfigurationen von MazeBase-Gitterwelten mit zufällig generierten Umgebungen verschiedener Größen und zeigen, dass es in der Lage ist, das Planen zu erlernen, wenn die Umgebung auch stochastische Elemente enthält, wodurch ein kosteneffizientes Lernsystem zum Aufbau von größeninvarianten Low-Level-Planern für eine Vielzahl von interaktiven Navigationsproblemen entsteht.
In dieser Arbeit schlagen wir eine Methode vor, mit der die Vorhersagen von Black-Box-Empfehlungssystemen sowohl interpretiert als auch erweitert werden können. Insbesondere schlagen wir vor, Interpretationen von Merkmalsinteraktionen aus einem Quell-Empfehlungsmodell zu extrahieren und diese Interaktionen explizit in einem Ziel-Empfehlungsmodell zu kodieren, wobei sowohl Quell- als auch Zielmodell Black-Boxen sind.  Wir haben herausgefunden, dass unsere Interaktionsinterpretationen sowohl informativ als auch prädiktiv sind, d.h. dass sie bestehende Empfehlungsmodelle signifikant übertreffen.Darüber hinaus kann derselbe Ansatz zur Interpretation von Interaktionen neue Einblicke in Bereiche jenseits der Empfehlung liefern.
Gleichgerichtete lineare Einheiten (ReLUs) sind zu einer bevorzugten Aktivierungsfunktion für künstliche neuronale Netze geworden.In dieser Arbeit betrachten wir das Problem des Lernens eines generativen Modells in Anwesenheit von Nichtlinearität (modelliert durch die ReLU-Funktionen).Gegeben eine Menge von Signalvektoren $\mathbf{y}^i \in \mathbb{R}^d, i =1, 2, \dots , n$, zielen wir darauf ab, die Netzwerkparameter zu lernen, d.h., die $d\mal k$ Matrix $A$, unter dem Modell $\mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i +\mathbf{b})$, wobei $\mathbf{b}\in \mathbb{R}^d$ ein zufälliger Bias-Vektor ist, und {$\mathbf{c}^i \in \mathbb{R}^k$ beliebige unbekannte latente Vektoren sind}. Wir zeigen, dass es möglich ist, den Spaltenraum von $A$ mit einem Fehler von $O(d)$ (in Frobenius-Norm) unter bestimmten Bedingungen für die Verteilung von $\mathbf{b}$ wiederherzustellen.
Methoden, die dichte Vektorrepräsentationen für Merkmale in unstrukturierten Daten - wie z.B. Wörter in einem Dokument - berechnen, haben sich als sehr erfolgreich für die Wissensrepräsentation erwiesen. Wir untersuchen, wie dichte Repräsentationen geschätzt werden können, wenn mehrere Merkmalstypen in einem Datensatz vorhanden sind, und zwar sowohl für das überwachte Lernen, bei dem explizite Beschriftungen verfügbar sind, als auch für das unüberwachte Lernen, bei dem es keine Beschriftungen gibt.Feat2Vec berechnet Einbettungen für Daten mit mehreren Merkmalstypen und erzwingt, dass alle verschiedenen Merkmalstypen in einem gemeinsamen Raum existieren. Im überwachten Fall zeigen wir, dass unsere Methode Vorteile gegenüber kürzlich vorgeschlagenen Methoden hat, wie z.B. eine höhere Vorhersagegenauigkeit ermöglicht und eine Möglichkeit bietet, das Cold-Start-Problem zu vermeiden.Im unbeaufsichtigten Fall deuten unsere Experimente darauf hin, dass Feat2Vec bestehende Algorithmen, die die Struktur der Daten nicht nutzen, deutlich übertrifft.Wir glauben, dass wir die ersten sind, die eine Methode zum Lernen von unbeaufsichtigten Einbettungen vorschlagen, die die Struktur mehrerer Merkmalstypen nutzen.
Wir untersuchen die internen Repräsentationen, die ein rekurrentes neuronales Netz (RNN) verwendet, während es lernt, eine reguläre formale Sprache zu erkennen, indem wir ein RNN auf positive und negative Beispiele einer regulären Sprache trainieren und fragen, ob es eine einfache Dekodierungsfunktion gibt, die Zustände dieses RNN auf Zustände des minimalen deterministischen endlichen Automaten (MDFA) für die Sprache abbildet. Unsere Experimente zeigen, dass eine solche Dekodierungsfunktion tatsächlich existiert und dass sie die Zustände des RNN nicht auf MDFA-Zustände abbildet, sondern auf die Zustände einer Abstraktion, die durch das Clustern kleiner Mengen von MDFA-Zuständen in ``'Superstates'' erhalten wird.Eine qualitative Analyse zeigt, dass die Abstraktion oft eine einfache Interpretation hat.Insgesamt deuten die Ergebnisse auf eine starke strukturelle Beziehung zwischen internen Repräsentationen, die von RNNs und endlichen Automaten verwendet werden, hin und erklären die bekannte Fähigkeit von RNNs, formale grammatische Strukturen zu erkennen. 
Im Gegensatz zu früheren Ansätzen, die Suchalgorithmen auf einen kleinen, von Menschen entworfenen Suchraum anwenden, ohne die Hardwarevielfalt zu berücksichtigen, schlagen wir HURRICANE vor, das die automatische hardwarebewusste Suche über einen viel größeren Suchraum und ein mehrstufiges Suchschema im Koordinatenaufstiegsrahmen erforscht, um maßgeschneiderte Modelle für verschiedene Arten von Hardware zu erzeugen. Ausführliche Experimente auf ImageNet zeigen, dass unser Algorithmus durchweg eine viel niedrigere Inferenzlatenz mit einer ähnlichen oder besseren Genauigkeit als die modernsten NAS-Methoden auf drei Arten von Hardware erreicht. Bemerkenswerterweise erreicht HURRICANE eine 76,63% Top-1-Genauigkeit auf ImageNet mit einer Inferenzlatenz von nur 16,5 ms für DSP, was eine 3,4% höhere Genauigkeit und eine 6. Für VPU erreicht HURRICANE eine um 0,53% höhere Top-1-Genauigkeit als Proxyless-mobile mit einer 1,49-fachen Beschleunigung. Sogar für gut untersuchte mobile CPUs erreicht HURRICANE eine um 1,63% höhere Top-1-Genauigkeit als FBNet-iPhoneX mit einer vergleichbaren Inferenzlatenz.HURRICANE reduziert auch die Trainingszeit um durchschnittlich 54,7% im Vergleich zu SinglePath-Oneshot.
Unsere Methode modelliert explizit die Phrasenstrukturen in Ausgabesequenzen unter Verwendung von Sleep-WAke Networks (SWAN), einer kürzlich vorgeschlagenen segmentierungsbasierten Sequenzmodellierungsmethode.Um die monotone Ausrichtungsanforderung von SWAN abzuschwächen, führen wir eine neue Schicht ein, um eine (weiche) lokale Umordnung von Eingabesequenzen durchzuführen.Anders als bei bestehenden neuronalen maschinellen Übersetzungsansätzen (NMT) verwendet NPMT keine aufmerksamkeitsbasierten Dekodierungsmechanismen.  Unsere Experimente zeigen, dass NPMT bei den maschinellen Übersetzungsaufgaben IWSLT 2014 (Deutsch-Englisch/Englisch-Deutsch) und IWSLT 2015 (Englisch-Vietnamesisch) im Vergleich zu starken NMT-Baselines überragende Leistungen erzielt und dass unsere Methode sinnvolle Sätze in den Ausgabesprachen erzeugt.
Generative Adversarial Networks (GANs) haben beeindruckende Ergebnisse bei der Modellierung von Verteilungen über komplizierte Mannigfaltigkeiten wie die von natürlichen Bildern gezeigt.GANs leiden jedoch oft unter Modus-Kollaps, was bedeutet, dass sie anfällig sind, nur eine einzige oder einige wenige Modi der Datenverteilung zu charakterisieren.Um dieses Problem zu adressieren, schlagen wir ein neuartiges Framework namens LDMGAN vor. Um diesen latenten Raum zu nutzen, schlagen wir einen regularisierten AutoEncoder (AE) vor, der die Datenverteilung auf die vorherige Verteilung im kodierten Raum abbildet. Ausführliche Experimente mit synthetischen Daten und realen Datensätzen zeigen, dass der von uns vorgeschlagene Rahmen die Stabilität und Diversität des GAN deutlich verbessert.
Das derzeitige Paradigma der Ausführung neuronaler Netze stützt sich jedoch entweder auf handoptimierte Bibliotheken, traditionelle Kompilationsheuristiken oder seit kurzem auf genetische Algorithmen und andere stochastische Methoden, die durch häufige, kostspielige Hardwaremessungen nicht nur zu zeitaufwändig, sondern auch suboptimal sind.Wir entwickeln daher eine Lösung, die lernen kann, sich schnell an einen bisher unbekannten Entwurfsraum für die Codeoptimierung anzupassen, was sowohl die Suche beschleunigt als auch die Ausgangsleistung verbessert. Diese Lösung mit dem Namen CHAMELEON nutzt Verstärkungslernen, dessen Lösung weniger Schritte zur Konvergenz benötigt, und entwickelt einen adaptiven Sampling-Algorithmus, der sich nicht nur auf die kostspieligen Stichproben (reale Hardware-Messungen) an repräsentativen Punkten konzentriert, sondern auch eine von Domänenwissen inspirierte Logik verwendet, um die Stichproben selbst zu verbessern.Experimente mit realer Hardware zeigen, dass CHAMELEON die Optimierungszeit im Vergleich zu AutoTVM um das 4,45-fache beschleunigt und gleichzeitig die Inferenzzeit der modernen tiefen Netzwerke um 5,6 % verbessert.
Das Ziel ist es, eine formale Grammatik in Form von differenzierbaren Funktionen und latenten Repräsentationen zu modellieren, so dass ihr Lernen durch Standard-Backpropagation möglich ist. Das Lernen einer formalen Grammatik, die mit latenten Terminals, Nicht-Terminals und Produktionsregeln dargestellt wird, erlaubt es, sequentielle Strukturen mit mehreren Möglichkeiten aus den Daten zu erfassen.Die adversarische Grammatik ist so konzipiert, dass sie stochastische Produktionsregeln aus der Datenverteilung lernen kann.Die Möglichkeit, mehrere Produktionsregeln auszuwählen, führt zu verschiedenen vorhergesagten Ergebnissen und modelliert so effizient viele plausible Zukünfte.  Wir bestätigen den Nutzen der adversen Grammatik bei zwei verschiedenen Aufgaben: der Vorhersage zukünftiger menschlicher 3D-Posen und der Vorhersage zukünftiger Aktivitäten - in allen Fällen übertrifft die vorgeschlagene adverse Grammatik den Stand der Technik und ist in der Lage, viel genauer und weiter in die Zukunft zu prognostizieren als frühere Arbeiten.
Unser Ansatz, den wir Janossy-Pooling nennen, drückt eine permutationsinvariante Funktion als Durchschnitt einer permutationssensitiven Funktion aus, die auf alle Umordnungen der Eingabesequenz angewandt wird, und ermöglicht es uns, die reichhaltige und ausgereifte Literatur über permutationssensitive Funktionen zu nutzen, um neuartige und flexible permutationsinvariante Funktionen zu konstruieren. Um eine rechnerische Nachvollziehbarkeit zu ermöglichen, betrachten wir drei Arten von Approximationen: kanonische Ordnungen von Sequenzen, Funktionen mit k-Ordnungsinteraktionen und stochastische Optimierungsalgorithmen mit zufälligen Permutationen.Unser Rahmen vereinheitlicht eine Vielzahl bestehender Arbeiten in der Literatur und schlägt mögliche Modellierungen und algorithmische Erweiterungen vor.Wir untersuchen einige davon in unseren Experimenten, die eine verbesserte Leistung im Vergleich zu aktuellen State-of-the-Art-Methoden zeigen.
Während die Anzahl der Instanzen und Klassen bei realistischen Aufgaben variieren kann, gehen die existierenden Meta-Learning-Ansätze für die Klassifikation von wenigen Fällen davon aus, dass die Anzahl der Instanzen pro Aufgabe und Klasse fest ist, so dass sie lernen, das Meta-Wissen über alle Aufgaben hinweg gleichmäßig zu nutzen, selbst wenn die Anzahl der Instanzen pro Aufgabe und Klasse stark variiert. Um diese Einschränkungen zu überwinden, schlagen wir ein neuartiges Meta-Lernmodell vor, das den Effekt des Meta-Lernens und des aufgabenspezifischen Lernens innerhalb jeder Aufgabe adaptiv ausbalanciert. Durch das Lernen der Ausgleichsvariablen können wir entscheiden, ob wir eine Lösung erhalten, indem wir uns auf das Metawissen oder das aufgabenspezifische Lernen verlassen.Wir formulieren dieses Ziel in einem Bayes'schen Inferenzrahmen und gehen es mit Hilfe von Variationsinferenz an.Wir validieren unser Bayes'sches aufgabenadaptives Meta-Lernen (Bayes'sches TAML) auf zwei realistischen aufgaben- und klassenungleichen Datensätzen, auf denen es bestehende Meta-Lernansätze deutlich übertrifft.Weitere Ablationsstudien bestätigen die Effektivität jeder Ausgleichskomponente und des Bayes'schen Lernrahmens.
Viele Aufgaben in der künstlichen Intelligenz erfordern die Zusammenarbeit mehrerer Agenten.Wir untersuchen tiefes Verstärkungslernen für Multi-Agenten-Domänen.Jüngste Forschungsbemühungen nehmen oft die Form von zwei scheinbar widersprüchlichen Perspektiven an, die dezentrale Perspektive, bei der jeder Agent seinen eigenen Controller haben soll, und die zentralisierte Perspektive, bei der man davon ausgeht, dass es ein größeres Modell gibt, das alle Agenten steuert.In diesem Zusammenhang greifen wir die Idee der Master-Slave-Architektur wieder auf, indem wir beide Perspektiven in einen Rahmen integrieren. Die Idee, beide Perspektiven zu kombinieren, ist intuitiv und kann durch viele reale Systeme motiviert werden; aus einer Vielzahl möglicher Realisierungen heben wir jedoch drei Schlüsselbestandteile hervor, d.h. zusammengesetzte Handlungsrepräsentation, lernfähige Kommunikation und unabhängiges Schlussfolgern; mit Netzwerkdesigns, die diese explizit erleichtern, übertrifft unser Vorschlag durchweg die neuesten konkurrierenden Methoden sowohl in Synthetikexperimenten als auch bei anspruchsvollen StarCraft-Mikromanagementaufgaben.
Wir untersuchen die implizite Verzerrung von Gradientenabstiegsmethoden bei der Lösung eines binären Klassifizierungsproblems über einen linear trennbaren Datensatz.Der Klassifizierer wird durch ein nichtlineares ReLU-Modell beschrieben und die Zielfunktion nimmt die exponentielle Verlustfunktion an.Wir charakterisieren zunächst die Landschaft der Verlustfunktion und zeigen, dass es neben asymptotischen globalen Minima auch falsche asymptotische lokale Minima geben kann. Wir zeigen dann, dass der Gradientenabstieg (GD) entweder zu einer globalen oder einer lokalen Max-Margin-Richtung konvergieren kann oder in einem allgemeinen Kontext von der gewünschten Max-Margin-Richtung abweicht.Für den stochastischen Gradientenabstieg (SGD) zeigen wir, dass er erwartungsgemäß entweder zur globalen oder zur lokalen Max-Margin-Richtung konvergiert, wenn SGD konvergiert. Des Weiteren untersuchen wir die implizite Verzerrung dieser Algorithmen beim Lernen eines Multineuronennetzes unter bestimmten stationären Bedingungen und zeigen, dass der gelernte Klassifikator die Ränder jeder Musterkartierung unter der ReLU-Aktivierung maximiert.
Modellbeschneidung ist zu einer nützlichen Technik geworden, die die rechnerische Effizienz von Deep Learning verbessert und es ermöglicht, Lösungen in ressourcenbeschränkten Szenarien einzusetzen.Eine weit verbreitete Praxis in einschlägigen Arbeiten geht davon aus, dass ein Parameter oder ein Merkmal mit kleinerer Norm eine weniger informative Rolle zum Zeitpunkt der Inferenz spielt. In diesem Papier schlagen wir eine Kanalbeschneidungstechnik zur Beschleunigung der Berechnungen von tiefen Faltungsneuronalen Netzen (CNNs) vor, die sich nicht kritisch auf diese Annahme stützt, sondern sich auf die direkte Vereinfachung des Kanal-zu-Kanal-Berechnungsgraphen eines CNNs konzentriert, ohne die rechnerisch schwierige und nicht immer nützliche Aufgabe, hochdimensionale Tensoren von CNNs spärlich zu strukturieren, durchzuführen. Unser Ansatz ist zweistufig: Zunächst wird eine stochastische End-to-End-Trainingsmethode angewandt, die die Ausgänge einiger Kanäle schließlich dazu zwingt, konstant zu sein, und dann werden diese konstanten Kanäle aus dem ursprünglichen neuronalen Netzwerk entfernt, indem die Vorspannungen der sie beeinflussenden Schichten so angepasst werden, dass das resultierende kompakte Modell schnell feinabgestimmt werden kann.Unser Ansatz ist aus einer Optimierungsperspektive mathematisch ansprechend und leicht reproduzierbar.Wir haben unseren Ansatz anhand mehrerer Bildlern-Benchmarks erprobt und seine interessanten Aspekte und konkurrenzfähige Leistung demonstriert.
Der stochastische Gradientenabstieg (SGD) ist aufgrund seiner vielen wünschenswerten Eigenschaften die vorherrschende Optimierungsmethode für das Training von tiefen neuronalen Netzen. Eine der bemerkenswertesten und am wenigsten verstandenen Eigenschaften von SGD ist, dass es relativ gut auf ungesehene Daten generalisiert, selbst wenn das neuronale Netz Millionen von Parametern hat. Wir stellen die Hypothese auf, dass es in bestimmten Fällen wünschenswert ist, seine intrinsischen Generalisierungseigenschaften zu lockern und eine Erweiterung von SGD einzuführen, die Deep Gradient Boosting (DGB) genannt wird. Die sich daraus ergebende Formel für die Gewichtsaktualisierung kann auch als Normalisierungsverfahren für die Daten betrachtet werden, die während des Vorwärtsdurchlaufs in jeder Schicht ankommen. Wenn die neue Architektur als separate Eingabenormalisierungsschicht (INN) implementiert wird, zeigt sie im Vergleich zur gleichen Architektur ohne Normalisierungsschichten eine verbesserte Leistung bei Bilderkennungsaufgaben. Im Gegensatz zur Batch-Normalisierung (BN) hat INN keine erlernbaren Parameter, entspricht aber ihrer Leistung bei CIFAR10- und ImageNet-Klassifizierungsaufgaben.
Wir schlagen vor, bestehende Deep Reinforcement Learning (Deep RL) Algorithmen zu erweitern, indem wir ihnen erlauben, zusätzlich Sequenzen von Aktionen als Teil ihrer Strategie zu wählen.  Diese Modifikation zwingt das Netzwerk dazu, die Belohnung von Aktionssequenzen zu antizipieren, was, wie wir zeigen, die Exploration verbessert und zu einer besseren Konvergenz führt. unser Vorschlag ist einfach, flexibel und kann leicht in jedes Deep RL Framework integriert werden. wir zeigen die Leistungsfähigkeit unseres Schemas, indem wir den hochmodernen GA3C-Algorithmus bei mehreren beliebten Atari-Spielen konsequent übertreffen.
Bei der Optimierung eines neuronalen Netzes denkt man oft an die Optimierung seiner Parameter, aber letztlich geht es um die Optimierung der Funktion, die Eingaben auf Ausgaben abbildet. Da eine Änderung der Parameter als schlechter Proxy für die Änderung der Funktion dienen könnte, ist es bedenklich, dass den Parametern Vorrang eingeräumt wird, aber die Korrespondenz nicht getestet wurde. Wir untersuchen, wie sich typische Netzwerke in diesem Raum verhalten, und vergleichen, wie sich die $\ell^2$-Distanzen von Parametern mit den $L^2$-Distanzen von Funktionen zwischen verschiedenen Punkten einer Optimierungskurve vergleichen lassen.Wir stellen fest, dass die beiden Distanzen nicht trivial miteinander verbunden sind. Wir untersuchen dann, wie der $L^2$-Abstand direkt auf die Optimierung angewendet werden kann. Wir schlagen zunächst vor, dass man beim Multitasking-Lernen ein katastrophales Vergessen vermeiden kann, indem man direkt begrenzt, wie stark sich die Input/Output-Funktion zwischen den Aufgaben ändert. Zweitens schlagen wir eine neue Lernregel vor, die die Entfernung, die ein Netzwerk bei einer Aktualisierung durch den $L^2$-Raum zurücklegen kann, einschränkt, so dass neue Beispiele auf eine Weise gelernt werden können, die das zuvor Gelernte nur minimal beeinträchtigt.
Das Informationsengpassproblem (IB) befasst sich mit der Frage, wie man relevante komprimierte Repräsentationen T einer Zufallsvariablen X für die Aufgabe der Vorhersage von Y erhält. Es ist definiert als ein eingeschränktes Optimierungsproblem, das die Information der Repräsentation über die Aufgabe, I(T;Y), maximiert und gleichzeitig sicherstellt, dass ein minimaler Komprimierungsgrad r erreicht wird (d.h., I(X;T) <= r).Aus praktischen Gründen wird das Problem in der Regel durch Maximierung des IB-Lagrangianers für viele Werte des Lagrange-Multiplikators gelöst, wodurch die IB-Kurve (d.h., Es ist bekannt, dass die IB-Kurve nicht erforscht werden kann, wenn Y eine deterministische Funktion von X ist, und dass andere Lagrange-Kurven vorgeschlagen wurden, um dieses Problem zu lösen (z. B, In diesem Papier stellen wir(i) eine allgemeine Familie von Lagrange-Faktoren vor, die die Erkundung der IB-Kurve in allen Szenarien ermöglichen;(ii) beweisen, dass es bei Verwendung dieser Lagrange-Faktoren eine Eins-zu-Eins-Abbildung zwischen dem Lagrange-Multiplikator und der gewünschten Kompressionsrate r für bekannte IB-Kurvenformen gibt, wodurch die Last der Lösung des Optimierungsproblems für viele Werte des Lagrange-Multiplikators entfällt.
Wir schlagen die Neural Logic Machine (NLM) vor, eine neuronal-symbolische Architektur sowohl für induktives Lernen als auch für logische Schlussfolgerungen. NLMs nutzen die Leistungsfähigkeit sowohl von neuronalen Netzen - als Funktionsapproximatoren - als auch von logischer Programmierung - als symbolischer Prozessor für Objekte mit Eigenschaften, Relationen, logischen Verknüpfungen und Quantoren.  In unseren Experimenten erreichen NLMs eine perfekte Generalisierung in einer Reihe von Aufgaben, von relationalen Argumentationsaufgaben auf dem Stammbaum und allgemeinen Graphen bis hin zu Entscheidungsfindungsaufgaben wie dem Sortieren von Arrays, dem Finden kürzester Pfade und dem Spielen der Klötzchenwelt - die meisten dieser Aufgaben sind für neuronale Netze oder induktive Logikprogrammierung allein schwer zu bewältigen.
Sequenz-zu-Sequenz (seq2seq) neuronale Modelle wurden aktiv für die abstrakte Zusammenfassung untersucht. Dennoch erzeugen bestehende neuronale abstrakte Systeme häufig sachlich falsche Zusammenfassungen und sind anfällig für gegnerische Informationen, was auf einen entscheidenden Mangel an semantischem Verständnis hindeutet.In diesem Papier schlagen wir ein neuartiges semantisch bewusstes neuronales abstraktes Zusammenfassungsmodell vor, das lernt, qualitativ hochwertige Zusammenfassungen durch semantische Interpretation über hervorstechende Inhalte zu erzeugen. Ein neuartiges Bewertungsschema mit gegnerischen Stichproben wird eingeführt, um zu messen, wie gut ein Modell Off-Topic-Informationen identifiziert, wobei unser Modell eine deutlich bessere Leistung erbringt als der beliebte Pointer-Generator Summarizer.Human Evaluation bestätigt auch, dass unser System Zusammenfassungen einheitlich informativer und treuer sowie weniger redundant als das seq2seq-Modell sind.
Die jüngste Arbeit von Super Characters Methode mit zweidimensionalen Wort Einbettung erreicht State-of-the-Art-Ergebnisse in Text-Klassifizierung Aufgaben, zeigt das Versprechen dieses neuen Ansatzes.Dieses Papier leiht die Idee der Super Characters Methode und zweidimensionale Einbettung, und schlägt eine Methode zur Erzeugung von Konversations-Antwort für offene Domain-Dialoge.Die experimentellen Ergebnisse auf einem öffentlichen Datensatz zeigt, dass die vorgeschlagene SuperChat Methode erzeugt hohe Qualität Antworten.Eine interaktive Demo ist bereit, auf dem Workshop zu zeigen.Und Code wird auf github bald verfügbar sein.
Trotz ihrer vielversprechenden Leistungen bei vielen Aufgaben stehen CNNs immer noch vor großen Hindernissen auf dem Weg zur idealen maschinellen Intelligenz: Zum einen sind CNNs komplex und schwer zu interpretieren. Eine andere ist, dass Standard-CNNs große Mengen an kommentierten Daten benötigen, die manchmal sehr schwer zu erhalten sind, und es wäre wünschenswert, sie aus wenigen Beispielen lernen zu können.In dieser Arbeit gehen wir diese Einschränkungen von CNNs an, indem wir neuartige, einfache und interpretierbare Modelle für das Lernen aus wenigen Beispielen entwickeln. Unsere Modelle basieren auf der Idee der Kodierung von Objekten in Form von visuellen Konzepten, die interpretierbare visuelle Hinweise sind, die durch die Merkmalsvektoren innerhalb von CNNs repräsentiert werden.Wir passen zunächst das Lernen von visuellen Konzepten an die "few-shot"-Einstellung an und decken dann zwei Schlüsseleigenschaften der Merkmalskodierung mit visuellen Konzepten auf, die wir als Kategorieempfindlichkeit und räumliche Muster bezeichnen. Experimente zeigen, dass unsere Modelle konkurrenzfähige Leistungen erzielen und dabei viel flexibler und interpretierbarer sind als alternative State-of-the-Art-Methoden für das Lernen von wenigen Aufnahmen, und wir kommen zu dem Schluss, dass die Verwendung von visuellen Konzepten dazu beiträgt, die natürlichen Fähigkeiten von CNNs für das Lernen von wenigen Aufnahmen aufzudecken.
Kürzlich wurden verschiedene neuronale Netze für unregelmäßig strukturierte Daten wie Graphen und Mannigfaltigkeiten vorgeschlagen.Unseres Wissens haben alle bestehenden Graphen-Netzwerke diskrete Tiefe.Inspiriert durch neuronale gewöhnliche Differentialgleichung (NODE) für Daten in der euklidischen Domäne, erweitern wir die Idee der kontinuierlichen Tiefe Modelle zu Graph-Daten, und schlagen Graph gewöhnliche Differentialgleichung (GODE).Die Ableitung der versteckten Knoten Zustände sind mit einem Graphen neuronales Netz parametrisiert, und die Ausgabe-Zustände sind die Lösung für diese gewöhnliche Differentialgleichung.Wir zeigen zwei Ende-zu-Ende-Methoden für die effiziente Ausbildung von GODE: (1) indirekte Backpropagation mit der adjungierten Methode; (2) direkte Backpropagation durch den ODE-Löser, der den Gradienten genau berechnet.Wir zeigen, dass die direkte Backpropagation die adjungierte Methode in Experimenten übertrifft.Wir stellen dann eine Familie von bijektiven Blöcken vor, die $\mathcal{O}(1)$ Speicherverbrauch ermöglicht. Wir zeigen, dass GODE leicht an verschiedene existierende neuronale Graphen-Netzwerke angepasst werden kann und die Genauigkeit verbessert.Wir validieren die Leistung von GODE sowohl in halb-überwachten Knoten-Klassifizierungsaufgaben als auch in Graphen-Klassifizierungsaufgaben.Unser GODE-Modell erreicht ein kontinuierliches Modell in der Zeit, Speichereffizienz, genaue Gradientenschätzung und Generalisierbarkeit mit verschiedenen Graphen-Netzwerken.
Unüberwachte Bild-zu-Bild-Übersetzung ist eine kürzlich vorgeschlagene Aufgabe der Übersetzung eines Bildes in einen anderen Stil oder eine andere Domäne, die nur ungepaarte Bildbeispiele zur Trainingszeit enthält.In diesem Papier formulieren wir eine neue Aufgabe der unüberwachten Video-zu-Video-Übersetzung, die ihre eigenen einzigartigen Herausforderungen stellt. Wir untersuchen die Leistung der Video-zu-Video-Übersetzung pro Bild mit Hilfe bestehender Bild-zu-Bild-Übersetzungsnetzwerke und schlagen einen räumlich-zeitlichen 3D-Übersetzer als alternative Lösung für dieses Problem vor. Unsere Ergebnisse zeigen, dass die bildweise Übersetzung realistische Ergebnisse auf Einzelbildebene liefert, aber auf der Skala des gesamten Videos deutlich schlechter abschneidet als unser dreidimensionaler Übersetzungsansatz, der besser in der Lage ist, die komplexe Struktur des Videos und der Bewegung sowie die Kontinuität des Objektaussehens zu erlernen.
Wir stellen eine Variante von Kapselnetzwerken vor, die Abhilfe schaffen soll: Wir stellen fest, dass das Lernen aller paarweisen Teil-Ganzes-Beziehungen zwischen Kapseln aufeinanderfolgender Schichten ineffizient ist. Basierend auf diesen Erkenntnissen schlagen wir einen alternativen Rahmen für Kapselnetzwerke vor, der lernt, die Mannigfaltigkeit von Pose-Variationen, genannt Space-of-Variation (SOV), für jeden Kapseltyp jeder Schicht projektiv zu kodieren, und zwar unter Verwendung einer trainierbaren, äquivarianten Funktion, die über ein Gitter von Gruppentransformationen definiert ist. In der Vorhersagephase des Routings wird die Projektion in die SOV einer tieferen Kapsel mit Hilfe der entsprechenden Funktion durchgeführt. Als spezielle Ausprägung dieser Idee und um die Vorteile einer erhöhten Parameteraufteilung zu nutzen, verwenden wir in dieser Phase typhomogene gruppenäquivariante Faltungen von flacheren Kapseln. Wir zeigen, dass diese spezielle Instanz unseres allgemeinen Modells äquivariant ist und daher die kompositionelle Repräsentation einer Eingabe unter Transformationen bewahrt.Wir führen mehrere Experimente an Standard-Objektklassifizierungsdatensätzen durch, die die erhöhte Transformationsrobustheit sowie die allgemeine Leistung unseres Modells im Vergleich zu mehreren Kapsel-Baselines zeigen.
Kürzlich haben tiefe neuronale Netze gezeigt, dass sie sich Trainingsdaten auch bei verrauschten Etiketten merken können, was die Generalisierungsleistung beeinträchtigt. Um dieses Problem zu mildern, schlagen wir eine einfache, aber effektive Methode vor, die selbst bei starkem Rauschen robust gegenüber verrauschten Etiketten ist.  Unsere Zielsetzung beinhaltet einen Varianzregulierungsterm, der implizit die Jacobi-Norm des neuronalen Netzwerks auf dem gesamten Trainingssatz (einschließlich der verrauschten Daten) bestraft, was die Generalisierung fördert und eine Überanpassung an die verrauschten Etiketten verhindert.Experimente mit verrauschten Benchmarks zeigen, dass unser Ansatz eine State-of-the-Art-Leistung mit einer hohen Toleranz gegenüber starkem Rauschen erreicht.
Jüngste Forschungsergebnisse deuten darauf hin, dass die neuronale maschinelle Übersetzung bei der WMT-Aufgabe zur Übersetzung chinesisch-englischer Nachrichten mit der professionellen menschlichen Übersetzung gleichziehen kann.Wir testen diese Behauptung empirisch mit alternativen Bewertungsprotokollen, indem wir die Bewertung einzelner Sätze und ganzer Dokumente gegenüberstellen. Unsere Ergebnisse unterstreichen die Notwendigkeit, die Bewertung auf Dokumentenebene zu verlagern, da sich die maschinelle Übersetzung in einem Maße verbessert, dass Fehler, die auf Satzebene schwer oder gar nicht zu erkennen sind, für die Unterscheidung der Qualität verschiedener Übersetzungsergebnisse entscheidend werden.
Das Imitationslernen zielt darauf ab, eine Strategie umgekehrt aus Expertendemonstrationen zu erlernen, was in der Literatur sowohl für Einzelagenten mit Markov-Entscheidungsprozess (MDP)-Modell als auch für Multiagenten mit Markov-Spiel (MG)-Modell ausführlich untersucht wurde. Die bestehenden Ansätze für allgemeine Multiagenten-Markov-Spiele sind jedoch nicht auf umfangreiche Multiagenten-Markov-Spiele anwendbar, bei denen die Agenten asynchrone Entscheidungen in einer bestimmten Reihenfolge und nicht gleichzeitig treffen. Wir schlagen einen neuartigen Rahmen für asynchrones generatives adversarisches Imitationslernen (AMAGAIL) unter allgemeinen extensiven Markov-Spielen vor, und die erlernten Expertenstrategien garantieren nachweislich ein subgame perfect equilibrium (SPE), ein allgemeineres und stärkeres Gleichgewicht als ein Nash-Gleichgewicht (NE), umfangreiche Markov-Spiele).
Das Selbsttraining ist eine der frühesten und einfachsten halbüberwachten Methoden, bei der der ursprüngliche markierte Datensatz durch unmarkierte Daten ergänzt wird, die mit der Vorhersage des Modells gepaart sind. In dieser Arbeit zeigen wir zunächst, dass es nicht nur möglich, sondern auch empfehlenswert ist, das Selbsttraining in der Sequenzgenerierung anzuwenden, indem wir die Leistungsgewinne sorgfältig untersuchen. Um diesen Mechanismus weiter zu fördern, schlagen wir vor, Rauschen in den Eingaberaum zu injizieren, was zu einer "verrauschten" Version des Selbsttrainings führt.Empirische Untersuchungen an Standard-Benchmarks für maschinelle Übersetzung und Textzusammenfassung unter verschiedenen Ressourceneinstellungen zeigen, dass verrauschtes Selbsttraining in der Lage ist, unmarkierte Daten effektiv zu nutzen und die Basisleistung um eine große Spanne zu verbessern.
Wir stellen ein durchgängig trainiertes Speichersystem vor, das sich schnell an neue Daten anpasst und Muster wie diese generiert. Inspiriert von Kanervas spärlichem verteiltem Speicher verfügt es über einen robusten verteilten Lese- und Schreibmechanismus. Der Speicher ist analytisch nachvollziehbar, was eine optimale Online-Kompression über eine Bayes'sche Aktualisierungsregel ermöglicht. Wir formulieren es als hierarchisches bedingtes generatives Modell, bei dem der Speicher eine reichhaltige datenabhängige Prioritätsverteilung bereitstellt. Empirisch zeigen wir, dass das adaptive Gedächtnis generative Modelle, die sowohl auf den Omniglot- als auch auf den CIFAR-Datensätzen trainiert wurden, signifikant verbessert. Verglichen mit dem Differentiable Neural Computer (DNC) und seinen Varianten hat unser Speichermodell eine größere Kapazität und ist wesentlich einfacher zu trainieren.
Das Pruning großer neuronaler Netze unter Beibehaltung ihrer Leistung ist aufgrund der geringeren Raum- und Zeitkomplexität oft wünschenswert.In bestehenden Methoden erfolgt das Pruning innerhalb eines iterativen Optimierungsverfahrens mit entweder heuristisch entworfenen Pruning-Zeitplänen oder zusätzlichen Hyperparametern, was ihren Nutzen untergräbt.In dieser Arbeit stellen wir einen neuen Ansatz vor, der ein gegebenes Netz einmal bei der Initialisierung vor dem Training beschneidet.Um dies zu erreichen, führen wir ein auf der Verbindungsempfindlichkeit basierendes Salienzkriterium ein, das strukturell wichtige Verbindungen im Netz für die gegebene Aufgabe identifiziert. Unsere Methode führt zu extrem spärlichen Netzwerken mit praktisch der gleichen Genauigkeit wie das Referenznetzwerk bei den MNIST-, CIFAR-10- und Tiny-ImageNet-Klassifizierungsaufgaben und ist allgemein auf verschiedene Architekturen anwendbar, einschließlich Faltungsnetzwerke, Residualnetzwerke und rekurrente Netzwerke, und im Gegensatz zu bestehenden Methoden können wir mit unserem Ansatz nachweisen, dass die beibehaltenen Verbindungen tatsächlich für die jeweilige Aufgabe relevant sind.
Beim Transferlernen werden die trainierten Gewichte eines Quellmodells als Ausgangsgewichte für das Training eines Zieldatensatzes verwendet.  Eine gut gewählte Quelle mit einer großen Anzahl von gelabelten Daten führt zu einer erheblichen Verbesserung der Genauigkeit.  Wir demonstrieren eine Technik, mit der automatisch große, nicht beschriftete Datensätze beschriftet werden, so dass damit Quellmodelle für das Transfer-Lernen trainiert werden können. Wir evaluieren diese Methode experimentell anhand eines Basisdatensatzes mit von Menschen beschrifteten ImageNet1K-Labels im Vergleich zu fünf Varianten dieser Technik.  Wir zeigen, dass die Leistung dieser automatisch trainierten Modelle im Durchschnitt innerhalb von 17 % der Grundlinie liegt.
In diesem Papier schlagen wir ein einfaches und hochgradig parametrisch effizientes Modul mit dem Namen Baumstrukturiertes Aufmerksamkeitsmodul (TAM) vor, das benachbarte Kanäle rekursiv dazu anregt, zusammenzuarbeiten, um eine räumliche Aufmerksamkeitskarte als Ausgabe zu erzeugen. Im Gegensatz zu anderen Aufmerksamkeitsmodulen, die versuchen, weitreichende Abhängigkeiten in jedem Kanal zu erfassen, konzentriert sich unser Modul darauf, Nichtlinearitäten zwischen den Kanälen durch die Verwendung punktweiser Gruppenfaltung durchzusetzen. Dieses Modul stärkt nicht nur die Repräsentationskraft eines Modells, sondern fungiert auch als Tor, das den Signalfluss steuert.Unser Modul ermöglicht es einem Modell, eine höhere Leistung in einer hochgradig parametereffizienten Weise zu erreichen.Wir validieren die Effektivität unseres Moduls empirisch mit umfangreichen Experimenten auf CIFAR-10/100- und SVHN-Datensätzen.Mit unserem vorgeschlagenen Aufmerksamkeitsmodul erzielen ResNet50- und ResNet101-Modelle eine Verbesserung der Genauigkeit um 2,3 % bzw. 1,2 % bei weniger als 1,5 % Parameter-Overhead.Unser PyTorch-Implementierungscode ist öffentlich verfügbar.
Eine große Herausforderung für die praktische Anwendung von Reinforcement Learning auf reale Probleme ist die Notwendigkeit, eine Orakel-Belohnungsfunktion zu spezifizieren, die eine Aufgabe korrekt definiert.Inverse Reinforcement Learning (IRL) versucht, diese Herausforderung zu vermeiden, indem stattdessen eine Belohnungsfunktion aus dem Verhalten von Experten abgeleitet wird.  Auch wenn es attraktiv ist, kann es unpraktisch teuer sein, Datensätze von Demonstrationen zu sammeln, die die in der realen Welt üblichen Variationen abdecken (z.B. das Öffnen jeder Art von Tür), so dass IRL in der Praxis häufig nur mit einem begrenzten Satz von Demonstrationen durchgeführt werden muss, bei denen es äußerst schwierig sein kann, eine Belohnungsfunktion eindeutig zu bestimmen. In dieser Arbeit nutzen wir die Einsicht, dass Demonstrationen aus anderen Aufgaben verwendet werden können, um die Menge der möglichen Belohnungsfunktionen einzuschränken, indem wir einen "Prior" lernen, der speziell für die Fähigkeit optimiert ist, aus einer begrenzten Anzahl von Demonstrationen aussagekräftige Belohnungsfunktionen abzuleiten.  Wir zeigen, dass unsere Methode effizient Belohnungen aus Bildern für neuartige Aufgaben ableiten kann, und erläutern, wie unser Ansatz mit dem Lernen eines Priors vergleichbar ist.
Vor diesem Hintergrund stellen wir Deepström-Netzwerke vor - eine neue Architektur neuronaler Netze, mit der wir die obersten dichten Schichten standardmäßiger Faltungsarchitekturen durch eine Annäherung an eine Kernel-Funktion ersetzen, indem wir uns auf die Nyström-Approximation stützen. Unser Ansatz ist sehr flexibel, er ist mit jeder Kernel-Funktion kompatibel und ermöglicht die Nutzung mehrerer Kernel. Wir zeigen, dass Deepström-Netzwerke auf Standarddatensätzen wie SVHN und CIFAR100 eine Spitzenleistung erreichen. Ein Vorteil der Methode liegt in der begrenzten Anzahl von lernbaren Parametern, die sie besonders für kleine Trainingsmengen, z.B. von 5 bis 20 Proben pro Klasse, geeignet machen.    
Das Hauptziel dieses kurzen Artikels ist es, die neuronale Kunstgemeinschaft über die ethischen Verzweigungen der Verwendung von Modellen zu informieren, die auf dem Datensatz imagenet trainiert wurden, oder über die Verwendung von Seed-Bildern der Klassen 445 -n02892767- ['Bikini, Zweiteiler'] und 459- n02837789- ['Büstenhalter, BH, Bandeau'] desselben. Wir entdeckten, dass viele der Bilder, die zu diesen Klassen gehören, nachweislich pornografisch waren, in einer nicht-einvernehmlichen Umgebung aufgenommen wurden, voyeuristisch waren und auch minderjährige Nacktheit enthielten. Ähnlich wie bei den Zusammenhängen zwischen Elfenbeinschnitzerei und illegaler Wilderei und zwischen Diamantenschmuckkunst und Blutdiamanten gehen wir davon aus, dass hier ein ähnliches moralisches Rätsel im Spiel ist, und möchten ein Gespräch unter den neuralen Künstlern in der Gemeinschaft anstoßen.
Induktives und unbeaufsichtigtes Graphenlernen ist eine wichtige Technik für Vorhersage- oder Informationsgewinnungsaufgaben, bei denen Beschriftungsinformationen schwer zu erhalten sind. Es ist auch eine Herausforderung, das Graphenlernen induktiv und gleichzeitig unbeaufsichtigt zu gestalten, da Lernprozesse, die von auf Rekonstruktionsfehlern basierenden Verlustfunktionen geleitet werden, unweigerlich eine Bewertung der Graphenähnlichkeit erfordern, die in der Regel rechnerisch schwer zu bewältigen ist.In diesem Beitrag schlagen wir ein allgemeines Rahmenwerk SEED (Sampling, Encoding, and Embedding Distributions) für induktives und unbeaufsichtigtes Repräsentationslernen für graphenstrukturierte Objekte vor. Anstatt sich direkt mit den rechnerischen Herausforderungen zu befassen, die sich aus der Bewertung der Graphenähnlichkeit ergeben, werden im SEED-Rahmenwerk bei einem Eingabegraphen eine Reihe von Untergraphen gesampelt, deren Rekonstruktionsfehler effizient bewertet werden können, die Untergraphen-Samples in eine Sammlung von Untergraphen-Vektoren kodiert und die Einbettung der Untergraphen-Vektor-Verteilung als Ausgabevektor-Darstellung für den Eingabegraphen verwendet. Durch eine theoretische Analyse zeigen wir die enge Verbindung zwischen SEED und Graphenisomorphismus. Unsere empirische Studie zeigt, dass der vorgeschlagene SEED-Rahmen in der Lage ist, im Vergleich zu konkurrierenden Basismethoden eine Verbesserung von bis zu 10 % zu erzielen, wobei öffentliche Benchmark-Datensätze verwendet werden.
Die Reaktionen neuronaler Populationen auf sensorische Stimuli können sowohl eine nichtlineare Stimulusabhängigkeit als auch eine reich strukturierte gemeinsame Variabilität aufweisen. Um der diskreten Natur neuronaler Spike Trains Rechnung zu tragen, verwenden wir die REBAR-Methode, um unverzerrte Gradienten für die adversarische Optimierung neuronaler Kodierungsmodelle zu schätzen.Wir illustrieren unseren Ansatz an Populationsaufzeichnungen aus dem primären visuellen Kortex.Wir zeigen, dass das Hinzufügen latenter Rauschquellen zu einem konvolutionellen neuronalen Netzwerk ein Modell ergibt, das sowohl die Reizabhängigkeit als auch die Rauschkorrelationen der Populationsaktivität erfasst.
Als Kernstück dieses Rahmens führen wir eine neuartige Lernaufgabe für mehrere Instanzen ein, die auf einer Kennzeichnung auf Beutelniveau basiert, die als "unique class count" (ucc) bezeichnet wird und die Anzahl der eindeutigen Klassen unter allen Instanzen innerhalb des Beutels angibt; bei dieser Aufgabe sind keine Anmerkungen zu einzelnen Instanzen innerhalb des Beutels während des Trainings der Modelle erforderlich. Wir haben einen auf einem neuronalen Netzwerk basierenden ucc-Klassifikator konstruiert und experimentell gezeigt, dass die Clustering-Leistung unseres Frameworks mit unserem schwach überwachten ucc-Klassifikator mit der von vollständig überwachten Lernmodellen vergleichbar ist, bei denen die Beschriftungen für alle Instanzen bekannt sind. Außerdem haben wir die Anwendbarkeit unseres Frameworks auf eine reale Aufgabe der semantischen Segmentierung von Brustkrebsmetastasen in histologischen Lymphknotenschnitten getestet und gezeigt, dass die Leistung unseres schwach überwachten Frameworks mit der Leistung eines vollständig überwachten Unet-Modells vergleichbar ist.
Modellfreie Methoden des Verstärkungslernens (Reinforcement Learning, RL) sind in einer wachsenden Anzahl von Aufgaben erfolgreich, unterstützt durch die jüngsten Fortschritte im Deep Learning.  Sie neigen jedoch dazu, unter der hohen Komplexität von Beispielen zu leiden, was ihren Einsatz in realen Domänen behindert.  Alternativ dazu verspricht das modellbasierte Verstärkungslernen, die Komplexität der Stichproben zu reduzieren, erfordert jedoch eine sorgfältige Abstimmung und war bisher hauptsächlich in restriktiven Domänen erfolgreich, in denen einfache Modelle für das Lernen ausreichen.In diesem Papier analysieren wir das Verhalten von Vanillemodell-basierten Verstärkungslernmethoden, wenn tiefe neuronale Netze verwendet werden, um sowohl das Modell als auch die Strategie zu lernen, und zeigen, dass die gelernte Strategie dazu neigt, Regionen zu nutzen, in denen nicht genügend Daten für das zu lernende Modell verfügbar sind, was zu Instabilität beim Training führt. Um dieses Problem zu überwinden, schlagen wir vor, ein Ensemble von Modellen zu verwenden, um die Modellunsicherheit aufrechtzuerhalten und den Lernprozess zu regulieren. Wir zeigen außerdem, dass die Verwendung von Likelihood-Ratio-Ableitungen zu einem wesentlich stabileren Lernen führt als Backpropagation über die Zeit.Insgesamt reduziert unser Ansatz Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) die Musterkomplexität im Vergleich zu modellfreien Deep-RL-Methoden bei anspruchsvollen kontinuierlichen Kontroll-Benchmark-Aufgaben erheblich.
Die hohe Rechen- und Parameterkomplexität neuronaler Netze macht ihr Training sehr langsam und schwierig auf energie- und speicherbeschränkten Rechnersystemen.Viele Techniken zur Reduktion der Netzwerkkomplexität wurden vorgeschlagen, einschließlich der Implementierung von Fixpunkten.Allerdings bleibt ein systematischer Ansatz für das Design von Training und Inferenz von tiefen neuronalen Netzen mit Fixpunkten schwer fassbar.Wir beschreiben eine Präzisionszuweisungsmethodik für das Training neuronaler Netze, bei der alle Netzwerkparameter, d.h. Aktivierungen und Gewichte in den Feeding-Parametern, in der Regel in der ersten Phase des Netzes liegen, Wir beschreiben eine Präzisionszuweisungsmethode für das Training neuronaler Netze, bei der alle Netzparameter, d.h. Aktivierungen und Gewichte im Feedforward-Pfad, Gradienten und Gewichtsakkumulatoren im Feedback-Pfad, nahe der minimalen Präzision zugewiesen werden. Die annähernde Optimalität (Minimalität) der resultierenden Präzisionszuweisung wird empirisch für vier Netze auf den Datensätzen CIFAR-10, CIFAR-100 und SVHN validiert und die Komplexitätsreduktion, die sich aus unserem Ansatz ergibt, mit anderen Designs für neuronale Netze mit Festkomma verglichen.
Die Forschung im Bereich des maschinellen Lernens (ML) hat Prototypen untersucht: Beispiele, die für das zu lernende Verhalten repräsentativ sind. Wir bewerten systematisch fünf Methoden zur Identifizierung von Prototypen, sowohl solche, die bereits eingeführt wurden, als auch neue, die wir vorschlagen, und stellen fest, dass alle von ihnen sinnvolle, aber unterschiedliche Interpretationen liefern.Durch eine menschliche Studie bestätigen wir, dass alle fünf Metriken gut mit der menschlichen Intuition übereinstimmen. Die Untersuchung von Fällen, in denen die Metriken nicht übereinstimmen, bietet eine aufschlussreiche Perspektive auf die Eigenschaften von Daten und Algorithmen, die beim Lernen verwendet werden, mit Auswirkungen auf den Aufbau von Datenkorpora, die Effizienz, die Robustheit gegenüber Gegnern, die Interpretierbarkeit und andere ML-Aspekte.Insbesondere bestätigen wir, dass der Ansatz "train on hard" bei vielen Datensätzen und Aufgaben die Genauigkeit verbessern kann, aber dass er bei vielen falsch beschrifteten oder mehrdeutigen Beispielen deutlich schlechter ist.
In dieser Arbeit schlagen wir das Sparse Deep Scattering Croisé Network (SDCSN) vor, eine neuartige Architektur, die auf dem Deep Scattering Network (DSN) basiert, das durch die Kaskadierung von Wavelet-Transformationsfaltungen mit einem komplexen Modulus und einem zeitinvarianten Operator erreicht wird.Wir erweitern diese Arbeit, indem wir zunächst mehrere Wavelet-Transformationen kreuzen, um die Merkmalsvielfalt zu erhöhen und gleichzeitig jegliches Lernen zu vermeiden. Dadurch erhalten wir eine informativere latente Repräsentation und profitieren von der Entwicklung hochspezialisierter Wavelet-Filter in den letzten Jahrzehnten. Außerdem reduzieren wir durch die Kombination der verschiedenen Wavelet-Repräsentationen die Menge an Vorinformationen, die für die vorliegenden Signale erforderlich sind. Unsere systematische und prinzipielle Lösung spart die latente Repräsentation des Netzwerks, indem sie als lokale Maske fungiert, die zwischen Aktivität und Rauschen unterscheidet. So schlagen wir vor, das DSN zu verbessern, indem wir die Varianz der Streukoeffizientenrepräsentation erhöhen und seine Robustheit in Bezug auf instationäres Rauschen verbessern. Wir zeigen, dass unser neuer Ansatz robuster ist und das DSN bei einer Vogeldetektionsaufgabe übertrifft.
Wir schlagen ein neuronales Clustermodell vor, das sowohl latente Merkmale als auch deren Clusterung erlernt. Im Gegensatz zu ähnlichen Methoden benötigt unser Modell keine vordefinierte Anzahl von Clustern. Unter Verwendung eines überwachten Ansatzes agglomerieren wir latente Merkmale zu zufällig ausgewählten Zielen innerhalb desselben Raums, während wir die Ziele nach und nach entfernen, bis nur noch Ziele übrig bleiben, die Clusterzentren darstellen. Um das Verhalten unseres Modells über verschiedene Modalitäten hinweg zu zeigen, wenden wir unser Modell sowohl auf Text- als auch auf Bilddaten an und erzielen sehr konkurrenzfähige Ergebnisse auf MNIST. Schließlich liefern wir auch Ergebnisse im Vergleich zu Basismodellen für Fashion-MNIST, den 20 Newsgroups-Datensatz und einen von uns selbst erstellten Twitter-Datensatz.
Jüngste Arbeiten über die Generierung von Erklärungen für Entscheidungsprobleme haben den Erklärungsprozess als einen Modellabgleich betrachtet, bei dem ein KI-Agent das menschliche mentale Modell (seiner Fähigkeiten, Überzeugungen und Ziele) im Hinblick auf eine anstehende Aufgabe auf die gleiche Seite bringt.Diese Formulierung erfasst kurz und bündig viele mögliche Arten von Erklärungen und geht explizit auf die verschiedenen Eigenschaften - z.B. die sozialen Aspekte, die Kontrastivität und die Selektivität - von Erklärungen ein, die in den Sozialwissenschaften im Rahmen von Mensch-Mensch-Interaktionen untersucht wurden. In früheren Arbeiten haben wir untersucht, wie solche Erklärungen vom Menschen in der Schleife wahrgenommen werden können, und haben eine mögliche Methode zu ihrer Erzeugung angedeutet.In diesem Beitrag gehen wir auf diese merkwürdige Eigenschaft des Modellabgleichsprozesses näher ein und erörtern ähnliche Auswirkungen auf den allgemeinen Begriff der erklärbaren Entscheidungsfindung.
Unsere Algorithmen basieren auf der Beobachtung, dass sich Mini-Batches von stochastischen Gradienten in aufeinanderfolgenden Iterationen nicht drastisch ändern und folglich vorhersehbar sind. Inspiriert von der ähnlichen Einstellung in der Online-Lernliteratur, die Optimistic Online Learning genannt wird, schlagen wir zwei neue optimistische Algorithmen für AMSGrad bzw. Adam vor, indem wir die Vorhersagbarkeit von Gradienten ausnutzen.  Die neuen Algorithmen kombinieren die Idee der Momentum-Methode, der adaptiven Gradienten-Methode und der Algorithmen des Optimistischen Online-Lernens, was zu einer Beschleunigung des Trainings tiefer neuronaler Netze in der Praxis führt.
Die Verwendung von rekurrenten neuronalen Netzen (RNNs) in Sequenzmodellierungsaufgaben ist vielversprechend, da sie qualitativ hochwertige Ergebnisse liefern, aber aufgrund des speichergebundenen Ausführungsmusters von RNNs eine Herausforderung darstellen, wenn es darum geht, strenge Latenzanforderungen zu erfüllen.Wir schlagen eine Big-Little-Dual-Modul-Inferenz vor, um unnötige Speicherzugriffe und Berechnungen dynamisch zu überspringen, um die RNN-Inferenz zu beschleunigen. Unter Ausnutzung der fehlerresistenten Eigenschaft von nichtlinearen Aktivierungsfunktionen, die in RNNs verwendet werden, schlagen wir vor, ein leichtgewichtiges kleines Modul zu verwenden, das die ursprüngliche RNN-Schicht approximiert, die als großes Modul bezeichnet wird, um Aktivierungen der unempfindlichen Region zu berechnen, die fehlerresistenter sind. Die teuren Speicherzugriffe und Berechnungen des großen Moduls können reduziert werden, da die Ergebnisse nur in der sensiblen Region verwendet werden. Unsere Methode kann den Gesamtspeicherzugriff im Durchschnitt um 40 % reduzieren und eine 1,54- bis 1,75-fache Beschleunigung auf einer CPU-basierten Serverplattform mit vernachlässigbaren Auswirkungen auf die Modellqualität erreichen.
Fine-grained Entity Recognition (FgER) ist die Aufgabe der Erkennung und Klassifizierung von Entitätserwähnungen in einer großen Menge von Typen, die verschiedene Bereiche wie Biomedizin, Finanzen und Sport abdecken.   Wir stellen fest, dass die Erkennung von Entitätserwähnungen für überwachte Lernmodelle zu einer Einschränkung wird, wenn sich die Typenmenge über mehrere Domänen erstreckt.  Der Hauptgrund dafür ist das Fehlen von Datensätzen, in denen die Entitätsgrenzen korrekt annotiert sind und die ein großes Spektrum von Entitätstypen abdecken.  Unsere Arbeit setzt direkt an diesem Problem an.  Wir schlagen Heuristiken vor, die mit Distant Supervision (HAnDS) verbunden sind, um automatisch einen Qualitätsdatensatz zu konstruieren, der für die FgER-Aufgabe geeignet ist.  Das HAnDS-Framework nutzt die enge Verknüpfung zwischen Wikipedia und Freebase in einer Pipeline und reduziert so Annotationsfehler, die durch die naive Verwendung eines Fernüberwachungsansatzes entstehen.  Mithilfe des HAnDS-Frameworks erstellen wir zwei Datensätze, von denen einer für den Aufbau von FgER-Systemen geeignet ist, die bis zu 118 Entitätstypen auf der Grundlage der FIGER-Typenhierarchie erkennen, und ein anderer für bis zu 1115 Entitätstypen auf der Grundlage der TypeNet-Hierarchie.  Unsere umfangreichen empirischen Experimente garantieren die Qualität der generierten Datensätze.  Außerdem stellen wir einen manuell annotierten Datensatz zum Benchmarking von FgER-Systemen zur Verfügung.
Die Implementierung korrekter Methodenaufrufe ist eine wichtige Aufgabe für Softwareentwickler, die jedoch eine große Herausforderung darstellt, da die Struktur der Methodenaufrufe kompliziert sein kann.In diesem Papier schlagen wir InvocMap vor, ein Code-Vervollständigungstool, das es Entwicklern ermöglicht, eine Implementierung mehrerer Methodenaufrufe aus einer Liste von Methodennamen im Codekontext zu erhalten.InvocMap ist in der Lage, die verschachtelten Methodenaufrufe vorherzusagen, deren Namen nicht in der Liste der von den Entwicklern eingegebenen Methodennamen erscheinen. Um dies zu erreichen, analysieren wir die Methodenaufrufe nach vier Abstraktionsebenen und bauen eine maschinelle Übersetzungsmaschine, um die Zuordnung von der ersten Ebene zur dritten Abstraktionsebene mehrerer Methodenaufrufe zu erlernen, was nur erfordert, dass die Entwickler manuell lokale Variablen aus dem generierten Ausdruck hinzufügen, um den endgültigen Code zu erhalten.Wir evaluieren unseren vorgeschlagenen Ansatz auf sechs populären Bibliotheken: JDK, Android, GWT, Joda-Time, Hibernate und Xstream.Mit dem Trainingskorpus von 2,86 Millionen Methodenaufrufen, die aus 1000 Java Github-Projekten extrahiert wurden, und dem Testkorpus, der aus 120 Online-Foren Code-Snippets extrahiert wurde, erreicht InvocMap eine Genauigkeitsrate von bis zu 84 in der F1-Punktzahl, je nachdem, wie viele Kontextinformationen zusammen mit den Methodennamen zur Verfügung gestellt werden, was sein Potenzial für die automatische Codevervollständigung zeigt.
Angreifer in neuronalen Netzen haben seit ihrem ersten Auftreten viel Aufmerksamkeit auf sich gezogen. Während die meisten existierenden Methoden darauf abzielen, Bildklassifizierungsmodelle zur Fehlklassifizierung zu verleiten oder Angriffe für bestimmte Objektinstanzen in der Objekterkennung zu entwickeln, konzentrieren wir uns auf die Entwicklung universeller Angreifer, um Objektdetektoren zu täuschen und Objekte vor den Detektoren zu verstecken. Die von uns untersuchten Angreifer sind in dreierlei Hinsicht universell: (1) Sie sind nicht spezifisch für bestimmte Objektinstanzen; (2) sie sind bildunabhängig; (3) sie können auf verschiedene unbekannte Modelle übertragen werden. Um dies zu erreichen, schlagen wir zwei neuartige Techniken vor, die die Übertragbarkeit der Gegenspieler verbessern: \textit{piling-up} und \textit{monochromatization}. Beide Techniken vereinfachen die Muster der generierten Gegenspieler und führen letztendlich zu einer höheren Übertragbarkeit.
In dieser Arbeit wird der Poincaré-Wasserstein-Autoencoder vorgestellt, eine Neuformulierung des kürzlich vorgeschlagenen Wasserstein-Autoencoder-Frameworks auf einer nicht-euklidischen Mannigfaltigkeit, dem Poincaré-Kugelmodell des hyperbolischen Raums H n. Durch die Annahme, dass der latente Raum hyperbolisch ist, können wir seine intrinsische Hierarchie nutzen, um den erlernten latenten Raumrepräsentationen eine Struktur zu verleihen. Wir zeigen, dass wir für Datensätze mit latenten Hierarchien die Struktur in einem niedrigdimensionalen latenten Raum wiederherstellen können und demonstrieren das Modell in der visuellen Domäne, um einige seiner Eigenschaften zu analysieren, und zeigen konkurrenzfähige Ergebnisse bei der Vorhersage von Graphenverbindungen.
Im Gegensatz zu früheren Arbeiten basiert die vorgeschlagene Methode auf der Umwandlung von Festkomma-Aktivierungen in Vektoren über das kleinste endliche Feld GF(2), gefolgt von nichtlinearen Dimensionalitätsreduktionsschichten (NDR), die in ein DNN eingebettet sind. Wir wenden die vorgeschlagene Netzwerkarchitektur auf die Aufgaben der ImageNet-Klassifikation und der PASCAL VOC-Objekterkennung an. Im Vergleich zu früheren Ansätzen zeigen die durchgeführten Experimente eine Verringerung des Speicherbedarfs um den Faktor 2 bei geringer Verschlechterung der Genauigkeit, während nur bitweise Berechnungen hinzugefügt werden.
Adversariales Training ist eine der stärksten Verteidigungsmaßnahmen gegen adversarische Angriffe, aber es erfordert, dass während der Optimierung für jeden Mini-Batch adversarische Beispiele erzeugt werden.  Der Aufwand für die Erzeugung dieser Beispiele während des Trainings schließt die Verwendung von adversarialem Training bei komplexen Bilddaten aus. In dieser Studie erforschen wir die Mechanismen, durch die das adversarische Training die Robustheit von Klassifikatoren verbessert, und zeigen, dass diese Mechanismen durch einfache Regularisierungsmethoden, einschließlich Label Smoothing und Logit Squeezing, effektiv nachgeahmt werden können.  Bemerkenswerterweise sind wir in der Lage, mit diesen einfachen Regularisierungsmethoden in Kombination mit der Injektion von Gaußschem Rauschen eine starke Widerstandsfähigkeit zu erreichen, die oft die Widerstandsfähigkeit des gegnerischen Trainings übertrifft, und das ohne gegnerische Beispiele.
Ein Hauptziel des unbeaufsichtigten Lernens ist es, Datenrepräsentationen zu entdecken, die für nachfolgende Aufgaben nützlich sind, ohne Zugang zu überwachten Markierungen während des Trainings.Typischerweise beinhaltet dies die Minimierung eines Ersatzziels, wie z.B. die negative log Likelihood eines generativen Modells, mit der Hoffnung, dass Repräsentationen, die für nachfolgende Aufgaben nützlich sind, als Nebeneffekt entstehen.In dieser Arbeit schlagen wir stattdessen vor, direkt auf spätere gewünschte Aufgaben zu zielen, indem wir eine unbeaufsichtigte Lernregel lernen, die zu Repräsentationen führt, die für diese Aufgaben nützlich sind.  Konkret zielen wir auf eine halb-überwachte Klassifizierungsleistung ab und erlernen einen Algorithmus - eine nicht-überwachte Gewichtsaktualisierungsregel - der Darstellungen erzeugt, die für diese Aufgabe nützlich sind.Zusätzlich beschränken wir unsere nicht-überwachte Aktualisierungsregel auf eine biologisch motivierte, neuronale Funktion, die es ermöglicht, sie auf verschiedene neuronale Netzwerkarchitekturen, Datensätze und Datenmodalitäten zu verallgemeinern. Wir zeigen, dass die meta-gelernte Aktualisierungsregel nützliche Merkmale erzeugt und manchmal bestehende unbeaufsichtigte Lerntechniken übertrifft. Wir zeigen außerdem, dass die meta-gelernte unbeaufsichtigte Aktualisierungsregel verallgemeinert werden kann, um Netze mit verschiedenen Breiten, Tiefen und Nichtlinearitäten zu trainieren, und dass sie auch auf Daten mit zufällig permutierten Eingabedimensionen und sogar von Bilddatensätzen auf eine Textaufgabe verallgemeinert werden kann.
Im Bereich des überwachten Lernens gibt es in jüngster Zeit zahlreiche Belege dafür, dass breitere Netzwerke im überparametrisierten Umfeld bessere Testfehler erzielen, d.h. der Kompromiss zwischen Verzerrung und Varianz ist nicht direkt beobachtbar, wenn man die Netzwerkbreite beliebig erhöht. Wir experimentieren mit vier OpenAI Gym-Umgebungen, wobei wir die Breite der Value- und Policy-Netzwerke über die vorgeschriebenen Werte hinaus erhöhen. Unsere empirischen Ergebnisse unterstützen diese Hypothese, wobei die Abstimmung der Hyperparameter jeder Netzwerkbreite separat als wichtige zukünftige Arbeit in Umgebungen/Algorithmen verbleibt, in denen die optimalen Hyperparameter über die Breiten hinweg deutlich variieren und die Ergebnisse verwirren, wenn dieselben Hyperparameter für alle Breiten verwendet werden.
Das Erlernen von entwirrten Repräsentationen von Daten ist eines der zentralen Themen beim unüberwachten Lernen im Allgemeinen und bei der generativen Modellierung im Besonderen.  In dieser Arbeit befassen wir uns mit einem etwas komplizierteren Szenario, bei dem die Beobachtungen aus einer bedingten Verteilung einer bekannten Kontrollvariante und einer latenten Rauschvariante erzeugt werden.  Zu diesem Zweck stellen wir ein hierarchisches Modell und eine Trainingsmethode (CZ-GEM) vor, die einige der jüngsten Entwicklungen bei likelihood-basierten und likelihood-freien generativen Modellen nutzen.  Wir zeigen, dass CZ-GEM durch die Formulierung die richtigen induktiven Verzerrungen einführt, die die Entflechtung der Kontrollvariablen von den Rauschvariablen sicherstellen und gleichzeitig die Komponenten der Kontrollvariablen entflechtet halten, ohne die Qualität der generierten Stichproben zu beeinträchtigen.Unser Ansatz ist einfach, allgemein und kann sowohl in überwachten als auch in unbeaufsichtigten Einstellungen angewendet werden.
In dieser Arbeit zeigen wir, dass die Menge der markierten Trainingsdaten drastisch reduziert werden kann, wenn Deep Learning mit aktivem Lernen kombiniert wird.Während aktives Lernen sample-effizient ist, kann es rechenintensiv sein, da es iteratives Umlernen erfordert.Um dies zu beschleunigen, führen wir eine leichtgewichtige Architektur für NER ein, nämlich, Das Modell erreicht bei Standarddatensätzen für diese Aufgabe nahezu die modernste Leistung und ist gleichzeitig rechnerisch viel effizienter als die Modelle mit der besten Leistung. Wir führen während des Trainingsprozesses inkrementelles aktives Lernen durch und sind in der Lage, die modernste Leistung mit nur 25\% der ursprünglichen Trainingsdaten zu erreichen.
Die meisten Quantisierungsmethoden führen eine Feinabstimmung an einem vortrainierten Netzwerk durch, was jedoch manchmal zu einem großen Genauigkeitsverlust im Vergleich zum ursprünglichen Netzwerk führt. Wir stellen eine neue Technik vor, um quantisierungsfreundliche Netzwerke zu trainieren, die direkt in ein genaues quantisiertes Netzwerk umgewandelt werden können, ohne dass eine zusätzliche Feinabstimmung erforderlich ist.Unsere Technik ermöglicht die Quantisierung der Gewichte und Aktivierungen aller Netzwerkschichten bis hinunter zu 4 Bits, wodurch eine hohe Effizienz erreicht und der Einsatz in der Praxis erleichtert wird. Im Vergleich zu anderen vollständig quantisierten Netzwerken, die mit 4 Bits arbeiten, zeigen wir erhebliche Verbesserungen in der Genauigkeit, z. B. 66,68 % Top-1-Genauigkeit auf ImageNet mit ResNet-18, verglichen mit der bisherigen State-of-the-Art-Genauigkeit von 61,52 % Louizos et al. (2019) und einer vollen Präzision Referenzgenauigkeit von 69,76 %.Wir haben eine gründliche Reihe von Experimenten durchgeführt, um die Wirksamkeit unserer Methode zu testen und auch Ablation Studien zu verschiedenen Aspekten der Methode und Techniken durchgeführt, um die Ausbildung Stabilität und Genauigkeit zu verbessern.Unsere Codebasis und trainierte Modelle sind auf GitHub verfügbar.
Während sich ein Großteil der Arbeit beim Entwurf von Faltungsnetzwerken in den letzten fünf Jahren um die empirische Untersuchung der Bedeutung von Tiefe, Filtergrößen und Anzahl der Merkmalskanäle drehte, haben neuere Studien gezeigt, dass Verzweigung, d.h., Um die Komplexität der Entwurfsentscheidungen in Architekturen mit mehreren Verzweigungen zu bekämpfen, wurden in früheren Arbeiten einfache Strategien angewandt, wie z. B. ein fester Verzweigungsfaktor, dieselbe Eingabe für alle parallelen Verzweigungen und eine additive Kombination der von allen Verzweigungen an den Aggregationspunkten erzeugten Ausgaben. In dieser Arbeit entfernen wir diese vordefinierten Entscheidungen und schlagen einen Algorithmus zum Erlernen der Verbindungen zwischen den Zweigen im Netzwerk vor, der nicht a priori vom menschlichen Designer ausgewählt wird, sondern gleichzeitig mit den Gewichten des Netzwerks erlernt wird, indem eine einzelne Verlustfunktion optimiert wird, die in Bezug auf die Endaufgabe definiert ist.Wir demonstrieren unseren Ansatz am Problem der Multi-Klassen-Bildklassifizierung anhand von vier verschiedenen Datensätzen, wo er im Vergleich zum hochmodernen ``ResNeXt''-Multi-Branch-Netzwerk bei gleicher Lernkapazität eine durchgängig höhere Genauigkeit liefert.
In einem Versuch, die Repräsentationen von tiefen Faltungsnetzwerken zu verstehen, die auf Sprachaufgaben trainiert wurden, zeigen wir, dass einzelne Einheiten selektiv auf spezifische Morpheme, Wörter und Phrasen reagieren, anstatt auf willkürliche und uninterpretierbare Muster zu reagieren. Um dieses faszinierende Phänomen quantitativ zu analysieren, schlagen wir eine Methode zum Konzeptabgleich vor, die darauf basiert, wie die Einheiten auf replizierten Text reagieren. Wir führen Analysen mit verschiedenen Architekturen auf mehreren Datensätzen für Klassifizierungs- und Übersetzungsaufgaben durch und liefern neue Erkenntnisse darüber, wie tiefe Modelle natürliche Sprache verstehen.
Die Anwendung von Verstärkungslernen (Reinforcement Learning, RL) auf reale Probleme erfordert Schlussfolgerungen über die Korrelation von Aktion und Belohnung über lange Zeithorizonte. Hierarchische Methoden des Verstärkungslernens (Reinforcement Learning, HRL) bewältigen dies, indem sie die Aufgabe in Hierarchien aufteilen, oft mit einer manuell abgestimmten Netzwerkstruktur oder vordefinierten Unterzielen. Wir schlagen ein neuartiges HRL-Framework TAIC vor, das die zeitliche Abstraktion aus vergangenen Erfahrungen oder Expertendemonstrationen ohne aufgabenspezifisches Wissen erlernt. Wir formulieren das zeitliche Abstraktionsproblem als Lernen latenter Repräsentationen von Handlungssequenzen und präsentieren einen neuartigen Ansatz zur Regularisierung des latenten Raums durch Hinzufügen informationstheoretischer Einschränkungen. Eine Visualisierung des latenten Raums zeigt, dass unser Algorithmus eine effektive Abstraktion der langen Aktionssequenzen lernt.Die gelernte Abstraktion erlaubt es uns, neue Aufgaben auf höherem Niveau effizienter zu lernen.Wir vermitteln eine signifikante Beschleunigung der Konvergenz über Benchmark-Lernprobleme.Diese Ergebnisse zeigen, dass das Lernen von zeitlichen Abstraktionen eine effektive Technik ist, um die Konvergenzrate und die Stichprobeneffizienz von RL-Algorithmen zu erhöhen.
Rekurrente neuronale Netze (RNNs) haben bei vielen verschiedenen Aufgaben, von der maschinellen Übersetzung bis zur Erkennung chirurgischer Aktivitäten, Spitzenleistungen erzielt, doch die Ausbildung von RNNs zur Erfassung langfristiger Abhängigkeiten bleibt schwierig. Wir verfolgen einen orthogonalen Ansatz und stellen MIST RNNs vor, eine NARX RNN-Architektur, die direkte Verbindungen aus der sehr fernen Vergangenheit erlaubt. Wir zeigen, dass MIST RNNs1) im Vergleich zu LSTM und den zuvor vorgeschlagenen NARX RNNs hervorragende Vanishing-Gradient-Eigenschaften aufweisen;2) weitaus effizienter sind als die zuvor vorgeschlagenen NARX RNN-Architekturen, da sie sogar weniger Berechnungen benötigen als LSTM; und3) die Leistung gegenüber LSTM und Clockwork RNNs bei Aufgaben, die sehr langfristige Abhängigkeiten erfordern, erheblich verbessern.
Ein gut trainiertes Modell sollte Objekte mit einstimmiger Bewertung für jede Kategorie klassifizieren, was voraussetzt, dass die semantischen Merkmale auf hoher Ebene zwischen den Proben gleich sind, trotz einer großen Spanne in der Auflösung, Textur, Deformation usw. Frühere Arbeiten konzentrieren sich auf die Neugestaltung der Verlustfunktion oder schlagen neue Regularisierungsbedingungen für den Verlust vor.In diesem Papier gehen wir dieses Problem aus einer neuen Perspektive an.Für jede Kategorie wird angenommen, dass es zwei Sätze im Merkmalsraum gibt: einen mit zuverlässigeren Informationen und den anderen mit weniger zuverlässigen Quellen. Wir argumentieren, dass die verlässliche Menge das Merkmalslernen der weniger verlässlichen Menge während des Trainings anleiten könnte - im Sinne eines Schülers, der das Verhalten des Lehrers nachahmt und so zu einem kompakteren Klassenschwerpunkt im hochdimensionalen Raum drängt. Es ist bekannt, dass Objekte mit niedriger Auflösung aufgrund des Verlusts von Detailinformationen während des Vorwärtsdurchlaufs des Netzwerks schwieriger zu erkennen sind. Wir betrachten daher Objekte mit hoher Auflösung als die zuverlässige Menge und Objekte mit niedriger Auflösung als die weniger zuverlässige Menge. Wir entwerfen einen historischen Puffer, um alle früheren Proben in der zuverlässigen Menge zu repräsentieren und verwenden sie, um das Feature-Lernen der weniger zuverlässigen Menge zu leiten.Das Design einer effektiven Feature-Repräsentation für die zuverlässige Menge wird weiter untersucht, wo wir den optimalen Transport (OT) Algorithmus in den Rahmen einführen.Proben in der weniger zuverlässigen Menge sind besser mit der zuverlässigen Menge mit Hilfe von OT metric.Incorporated mit einem solchen Plug-and-Play-Intertwiner, erreichen wir eine deutliche Verbesserung gegenüber früheren Stand der Technik auf der COCO-Objekt-Erkennung Benchmark.
In jüngster Zeit wurden mehrere Frameworks entwickelt, die den Einsatz von Deep Learning in diesem Lernszenario ermöglichen. Eine wichtige Entscheidung bei der Modellierung ist die Frage, inwieweit die Architektur für alle Aufgaben gemeinsam genutzt werden sollte. Andererseits ermöglicht die starre Festlegung einer gemeinsamen Komponente und eines aufgabenspezifischen Teils den Aufgabentransfer und begrenzt die Größe des Modells, ist aber anfällig für katastrophales Vergessen und schränkt die Form des Aufgabentransfers ein, die auftreten kann. Im Idealfall sollte das Netzwerk adaptiv ermitteln, welche Teile des Netzwerks datengesteuert gemeinsam genutzt werden sollen. Hier stellen wir einen solchen Ansatz vor, der als Kontinuierliches Lernen mit adaptiven Gewichten (CLAW) bezeichnet wird und auf probabilistischer Modellierung und Variationsinferenz basiert. Experimente zeigen, dass CLAW bei sechs Benchmarks die beste Leistung in Bezug auf die Gesamtleistung des kontinuierlichen Lernens, gemessen an der Klassifizierungsgenauigkeit, und in Bezug auf die Bewältigung des katastrophalen Vergessens erreicht.
Hochdimensionale Daten liegen oft in oder in der Nähe von niedrigdimensionalen Unterräumen. Sparsame Unterraum-Clustering-Methoden mit durch die L0-Norm induzierter Sparsamkeit, wie das L0-Sparse Subspace Clustering (L0-SSC), sind nachweislich effektiver als ihr L1-Gegenstück, das Sparse Subspace Clustering (SSC). Diese auf der L0-Norm basierenden Unterraum-Clustering-Methoden sind jedoch auf saubere Daten beschränkt, die genau in Unterräumen liegen. Reale Daten sind oft verrauscht und können in der Nähe von Unterräumen liegen. Wir schlagen verrauschte L0-SSC vor, um verrauschte Daten zu behandeln und so die Robustheit zu verbessern. Wir zeigen, dass die optimale Lösung des Optimierungsproblems der verrauschten L0-SSC die Subraum-Detektions-Eigenschaft (SDP), ein Schlüsselelement, mit dem Daten aus verschiedenen Unterräumen getrennt werden, unter deterministischen und randomisierten Modellen erreicht.Unsere Ergebnisse bieten eine theoretische Garantie für die Korrektheit der verrauschten L0-SSC in Bezug auf die SDP bei verrauschten Daten. Noisy-DR-L0-SSC projiziert die Daten zunächst durch eine lineare Transformation in einen niedrigeren dimensionalen Raum und führt dann Noisy-L0-SSC auf den dimensional reduzierten Daten durch, um die Effizienz zu verbessern.Die experimentellen Ergebnisse zeigen die Wirksamkeit von Noisy-L0-SSC und Noisy-DR-L0-SSC.
Mode-Connectivity bietet neuartige geometrische Einblicke in die Analyse von Verlustlandschaften und ermöglicht den Aufbau von hochpräzisen Pfaden zwischen gut trainierten neuronalen Netzen.In dieser Arbeit schlagen wir vor, Mode-Connectivity in Verlustlandschaften einzusetzen, um die Robustheit von tiefen neuronalen Netzen gegenüber Angreifern zu untersuchen, und bieten neuartige Methoden zur Verbesserung dieser Robustheit.  Wenn Netzwerkmodelle mit Backdoor- oder Error-Injection-Angriffen manipuliert werden, zeigen unsere Ergebnisse, dass die Pfadverbindung, die mit einer begrenzten Menge an echten Daten erlernt wurde, die negativen Effekte effektiv abschwächen kann, während die ursprüngliche Genauigkeit auf sauberen Daten beibehalten wird.Daher bietet die Moduskonnektivität dem Benutzer die Möglichkeit, Backdoor- oder Error-Injected-Modelle zu reparieren.  Die Experimente zeigen, dass auf dem Pfad, der die regulären und die von den Angreifern trainierten Modelle miteinander verbindet, eine Barriere für den Verlust der Robustheit gegenüber Angreifern besteht.  Es wird eine hohe Korrelation zwischen dem gegnerischen Robustheitsverlust und dem größten Eigenwert der hessischen Eingabematrix beobachtet, wofür theoretische Begründungen geliefert werden.  Unsere Ergebnisse deuten darauf hin, dass die Moduskonnektivität ein ganzheitliches Werkzeug und ein praktisches Mittel zur Bewertung und Verbesserung der Robustheit von adversen Netzen darstellt.
Generative adversarische Netze (GANs) lernen, Stichproben aus einer Rauschverteilung auf eine gewählte Datenverteilung abzubilden. Neuere Arbeiten haben gezeigt, dass GANs folglich empfindlich auf die Form der Rauschverteilung reagieren und durch diese begrenzt werden. Wir adressieren dieses Problem, indem wir lernen, aus mehreren Modellen zu generieren, so dass die Ausgabe des Generators tatsächlich die Kombination aus mehreren verschiedenen Netzwerken ist. Wir stellen eine neuartige Formulierung von Multigeneratormodellen vor, bei der wir einen Prior über die Generatoren lernen, der durch ein neuronales Netzwerk parametrisiert wird, das nicht nur die optimale Abtastrate von jedem Generator lernt, sondern auch das Rauschen, das von jedem Generator empfangen wird, optimal formt.Das resultierende Noise Prior GAN (NPGAN) erreicht eine Ausdruckskraft und Flexibilität, die sowohl Einzelgeneratormodelle als auch frühere Multigeneratormodelle übertrifft.
Die jüngsten Fortschritte bei Generative Adversarial Networks (GANs) - in Bezug auf Architektur, Trainingsstrategien und empirische Tricks - haben zu nahezu fotorealistischen Mustern auf großen Datensätzen wie ImageNet geführt.  Insbesondere bei einem Modell, BigGAN, stimmen Metriken wie Inception Score oder Frechet Inception Distance fast mit denen des Datensatzes überein, was darauf hindeutet, dass diese Modelle der Verteilung des Trainingssatzes nahezu entsprechen.   Angesichts der Qualität dieser Modelle lohnt es sich zu verstehen, inwieweit diese Stichproben zur Datenerweiterung verwendet werden können, eine Aufgabe, die als langfristiges Ziel des GAN-Forschungsprojekts formuliert wurde.  Zu diesem Zweck trainieren wir die ResNet-50-Klassifikatoren entweder mit reinen BigGAN-Bildern oder mit einer Mischung aus ImageNet- und BigGAN-Bildern und testen sie mit dem ImageNet-Validierungsset.Unsere vorläufigen Ergebnisse zeigen sowohl eine gemessene Sicht auf die GAN-Qualität nach dem Stand der Technik als auch die Grenzen der aktuellen Metriken. Bei der ausschließlichen Verwendung von BigGAN-Bildern haben wir festgestellt, dass der Top-1- und Top-5-Fehler um 120 % bzw. 384 % gestiegen ist, und dass das Hinzufügen weiterer BigGAN-Daten zum ImageNet-Trainingssatz die Klassifizierungsleistung bestenfalls geringfügig verbessert.   Diese Ergebnisse deuten darauf hin, dass wir, da GANs zunehmend in nachgelagerten Aufgaben eingesetzt werden, Metriken entwickeln sollten, die die Leistung nachgelagerter Aufgaben besser messen.  Wir schlagen die Klassifizierungsleistung als eine solche Metrik vor, die zusätzlich zur Bewertung der Qualität der Proben pro Klasse besser für solche nachgelagerten Aufgaben geeignet ist.
Moderne föderierte Netzwerke, wie z. B. solche, die aus tragbaren Geräten, Mobiltelefonen oder autonomen Fahrzeugen bestehen, generieren täglich riesige Datenmengen, aus denen Modelle gelernt werden können, die das Benutzererlebnis auf jedem Gerät verbessern. Zu diesem Zweck schlagen wir Leaf vor, ein modulares Benchmarking-Framework für das Lernen in föderierten Umgebungen. Leaf umfasst eine Reihe von Open-Source-Federated-Datensätzen, einen strengen Bewertungsrahmen und eine Reihe von Referenzimplementierungen, die alle darauf ausgerichtet sind, die Hindernisse und Feinheiten praktischer föderierter Umgebungen zu erfassen.
Das Verstehen von Objektbewegungen ist eines der Kernprobleme in der Computer Vision und erfordert die Segmentierung und Verfolgung von Objekten über die Zeit. Es wurden bedeutende Fortschritte in der Instanzsegmentierung gemacht, aber solche Modelle können Objekte nicht verfolgen und, was noch wichtiger ist, sie sind nicht in der Lage, sowohl im 3D-Raum als auch in der Zeit zu argumentieren.Wir schlagen einen neuen räumlich-zeitlichen Einbettungsverlust auf Videos vor, der eine zeitlich konsistente Videoinstanzsegmentierung erzeugt. Darüber hinaus schätzt unser Modell auch die monokulare Tiefe mit einem selbstüberwachten Verlust, da der relative Abstand zu einem Objekt effektiv einschränkt, wo es sich als nächstes befinden kann, was eine zeitkonsistente Einbettung gewährleistet. Schließlich zeigen wir, dass unser Modell Instanzen auch bei Verdeckungen und verpassten Erkennungen genau verfolgen und segmentieren kann, was den Stand der Technik im KITTI-Multi-Objekt- und Tracking-Datensatz verbessert.
Da Reinforcement Learning weiterhin die maschinelle Intelligenz über ihre konventionellen Grenzen hinaus vorantreibt, schränken unzureichende Praktiken in spärlichen Belohnungsumgebungen weitere Anwendungen in einem breiteren Spektrum fortgeschrittener Bereiche stark ein. Motiviert durch die Nachfrage nach einem effektiven tiefen Reinforcement Learning-Algorithmus, der spärliche Belohnungsumgebungen berücksichtigt, stellt dieses Papier Hindsight Trust Region Policy Optimization (HTRPO) vor, eine Methode, die Interaktionen in spärlichen Belohnungsbedingungen effizient nutzt, um Richtlinien innerhalb der Vertrauensregion zu optimieren und in der Zwischenzeit die Lernstabilität zu erhalten. Zunächst passen wir die TRPO-Zielfunktion in Form der erwarteten Rendite der Politik theoretisch an die Verteilung der Rückblicksdaten an, die aus den alternativen Zielen generiert werden, und wenden dann Monte Carlo mit Wichtigkeitsabtastung an, um die KL-Divergenz zwischen zwei Politiken zu schätzen, wobei wir die Rückblicksdaten als Eingabe verwenden.  Experimentelle Ergebnisse sowohl bei diskreten als auch bei kontinuierlichen Benchmark-Aufgaben zeigen, dass HTRPO deutlich schneller konvergiert als frühere Gradienten-Methoden und dass es effektive Leistungen und eine hohe Dateneffizienz beim Training von Strategien in spärlichen Belohnungsumgebungen erreicht.
Die Beantwortung von Fragen in offenen Bereichen (Open-Domain Question Answering, QA) ist ein wichtiges Problem in der KI und NLP, das sich als Indikator für den Fortschritt bei der Verallgemeinerbarkeit von KI-Methoden und -Techniken erweist. In diesem Beitrag konzentrieren wir uns auf den kürzlich eingeführten ARC Challenge-Datensatz, der 2.590 Multiple-Choice-Fragen enthält, die für naturwissenschaftliche Grundschulprüfungen verfasst wurden und die für aktuelle QS-Systeme die größten Herausforderungen darstellen. Wir stellen ein System vor, das eine gegebene Frage in Abfragen umformuliert, die verwendet werden, um unterstützenden Text aus einem großen Korpus wissenschaftsbezogener Texte abzurufen. Unser Rewriter ist in der Lage, Hintergrundwissen aus ConceptNet einzubeziehen und - in Verbindung mit einem generischen textuellen Entailment-System, das auf SciTail trainiert wurde und Unterstützung in den abgerufenen Ergebnissen identifiziert - mehrere starke Baselines bei der End-to-End-QA-Aufgabe zu übertreffen, obwohl er nur trainiert wurde, um wesentliche Begriffe in der ursprünglichen Ausgangsfrage zu identifizieren. Wir verwenden eine verallgemeinerbare Entscheidungsmethodik über die abgerufenen Beweise und Antwortkandidaten, um die beste Antwort auszuwählen. Durch die Kombination von Anfrageumformulierung, Hintergrundwissen und textuellem Entailment ist unser System in der Lage, mehrere starke Baselines auf dem ARC-Datensatz zu übertreffen.
Deep CNNs haben in den letzten Jahren für zahlreiche Aufgaben im Bereich des maschinellen Lernens und der Computer Vision Spitzenleistungen erbracht, doch mit zunehmender Tiefe ist auch die Anzahl der verwendeten Parameter gestiegen, was ihren Einsatz in speicherbeschränkten Umgebungen erschwert und ihre Interpretation erschwert.Die Theorie des maschinellen Lernens impliziert, dass solche Netzwerke stark überparametrisiert sind und dass es möglich sein sollte, ihre Größe zu reduzieren, ohne die Genauigkeit zu beeinträchtigen. In dieser Arbeit gehen wir einen weiteren Schritt in diese Richtung, indem wir einen Filter-Sharing-Ansatz zur Komprimierung tiefer CNNs vorschlagen, der ihren Speicherbedarf durch wiederholte Anwendung einer einzigen Faltungsabbildung gelernter Filter zur Simulation einer CNN-Pipeline reduziert. Anhand von Experimenten mit CIFAR-10, CIFAR-100, Tiny ImageNet und ImageNet zeigen wir, dass wir auf diese Weise die Anzahl der Parameter von Netzwerken, die auf gängigen Designs wie VGGNet und ResNet basieren, um einen Faktor reduzieren können, der proportional zu ihrer Tiefe ist, während ihre Genauigkeit weitgehend unbeeinflusst bleibt.Auf einer breiteren Ebene zeigt unser Ansatz auch, wie die in visuellen Signalen gefundenen Skalen-Raum-Regelmäßigkeiten genutzt werden können, um neuronale Architekturen zu erstellen, die einfacher und besser interpretierbar sind.
Wir erweitern die jüngsten Ergebnisse von (Arora et al., 2019) durch eine Spektralanalyse von Repräsentationen, die Kernel- und neuronalen Einbettungen entsprechen, und zeigen, dass in einem einfachen einschichtigen Netzwerk die Ausrichtung der Beschriftungen an den Eigenvektoren der entsprechenden Gram-Matrix sowohl die Konvergenz der Optimierung während des Trainings als auch die Generalisierungseigenschaften bestimmt.Wir verallgemeinern ihr Ergebnis auf Kernel- und neuronale Repräsentationen und zeigen, dass diese Erweiterungen sowohl die Optimierung als auch die Generalisierung des in (Arora et al., 2019) untersuchten Grundaufbaus verbessern.
In diesem Papier begründen wir theoretisch ein Schema für die Schätzung von Unsicherheiten, das auf einer Stichprobenziehung aus einer Prioritätsverteilung basiert, und zeigen, dass die Unsicherheitsschätzungen in dem Sinne konservativ sind, dass sie niemals eine posteriore Unsicherheit unterschätzen, die durch einen hypothetischen Bayes-Algorithmus erhalten wurde. Wir zeigen auch Konzentration, was bedeutet, dass die Unsicherheitsschätzungen gegen Null konvergieren, wenn wir mehr Daten erhalten.Unsicherheitsschätzungen, die von zufälligen Prioren erhalten werden, können an jede Deep-Network-Architektur angepasst und mit Standard-Pipelines für überwachtes Lernen trainiert werden.Wir bieten eine experimentelle Bewertung von zufälligen Prioren auf Kalibrierung und Erkennung außerhalb der Verteilung auf typische Computer-Vision-Aufgaben und zeigen, dass sie tiefe Ensembles in der Praxis übertreffen.
Das vorgeschlagene Modell ist in der Lage, beliebige partielle Daten auf eine multimodale latente Variationsverteilung abzubilden, wobei das Sampling aus einer solchen Verteilung zu einer stochastischen Imputation führt. Eine vorläufige Evaluierung des MNIST-Datensatzes zeigt eine vielversprechende stochastische Imputation, die auf partiellen Bildern als Input basiert.
Dieses Papier untersucht \emph{Modellinversionsangriffe}, bei denen der Zugang zu einem Modell missbraucht wird, um Informationen über die Trainingsdaten abzuleiten.Seit ihrer ersten Einführung durch~\citet{fredrikson2014privacy} haben solche Angriffe ernsthafte Bedenken aufgeworfen, da die Trainingsdaten normalerweise sensible Informationen enthalten. Bisher wurden erfolgreiche Modellinversionsangriffe nur bei einfachen Modellen wie der linearen Regression und der logistischen Regression demonstriert. Frühere Versuche, neuronale Netze zu invertieren, selbst solche mit einfachen Architekturen, haben keine überzeugenden Ergebnisse erbracht.Wir stellen eine neuartige Angriffsmethode vor, die als \emph{generative model inversion attack} bezeichnet wird und tiefe neuronale Netze mit hohen Erfolgsraten invertieren kann. Anstatt private Trainingsdaten von Grund auf neu zu rekonstruieren, nutzen wir partielle öffentliche Informationen, die sehr generisch sein können, um eine Verteilungspriorität über generative adversarische Netzwerke (GANs) zu erlernen und sie zu verwenden, um den Inversionsprozess zu steuern.Darüber hinaus beweisen wir theoretisch, dass die Vorhersagekraft eines Modells und seine Anfälligkeit für Inversionsangriffe in der Tat zwei Seiten derselben Medaille sind - hochgradig vorhersagende Modelle sind in der Lage, eine starke Korrelation zwischen Merkmalen und Bezeichnungen herzustellen, die genau mit dem übereinstimmt, was ein Angreifer ausnutzt, um die Angriffe durchzuführen. Unsere Experimente zeigen, dass der vorgeschlagene Angriff die Identifizierungsgenauigkeit im Vergleich zu bestehenden Arbeiten um etwa $75\%$ für die Rekonstruktion von Gesichtsbildern aus einem hochmodernen Gesichtserkennungsklassifikator verbessert.
Latent Space-basierte GAN-Methoden und aufmerksamkeitsbasierte Encoder-Decoder-Architekturen haben beeindruckende Ergebnisse bei der Texterzeugung bzw. bei der unüberwachten NMT erzielt.Wir nutzen diese beiden Bereiche und schlagen eine auf dem latenten Raum basierende Architektur vor, die in der Lage ist, parallele Sätze in zwei Sprachen gleichzeitig zu generieren und bidirektional zu übersetzen.Das Ziel der zweisprachigen Generierung wird erreicht, indem eine Stichprobe aus dem latenten Raum gezogen wird, der nachteiligerweise von beiden Sprachen gemeinsam genutzt werden muss. Zunächst wird ein NMT-Modell mit Rückübersetzung und einem adversen Setup trainiert, um einen latenten Zustand zwischen den beiden Sprachen zu erzwingen.Der Encoder und der Decoder werden für die beiden Übersetzungsrichtungen gemeinsam genutzt.Als nächstes wird ein GAN trainiert, um "synthetischen" Code zu erzeugen, der den gemeinsamen latenten Raum der Sprachen nachahmt.Dieser Code wird dann in den Decoder eingespeist, um Text in einer der beiden Sprachen zu erzeugen.Wir führen unsere Experimente mit den Datensätzen Europarl und Multi30k für das Sprachpaar Englisch-Französisch durch und dokumentieren unsere Leistung sowohl mit Supervised als auch Unsupervised NMT.
Da Künstliche Intelligenz (KI) zu einem integralen Bestandteil unseres Lebens wird, ist die Entwicklung einer erklärbaren KI, die den Entscheidungsprozess eines KI- oder Roboteragenten verkörpert, unerlässlich.  Bisherige Arbeiten zur Generierung von Erklärungen konzentrieren sich auf die Unterstützung der Argumente, die hinter dem Verhalten des Roboters stehen, berücksichtigen jedoch nicht die mentale Belastung, die für das Verständnis der erhaltenen Erklärung erforderlich ist. Diese Ansätze berücksichtigen jedoch nicht die mentale Belastung, die notwendig ist, um die erhaltenen Erklärungen zu verstehen. Mit anderen Worten, es wird erwartet, dass der menschliche Teamkollege jede Erklärung versteht, oft schon vor der Ausführung der Aufgabe, unabhängig davon, wie viele Informationen in der Erklärung enthalten sind. Zu diesem Zweck wird eine allgemeine Formulierung der Online-Erklärungsgenerierung zusammen mit drei verschiedenen Implementierungen vorgestellt, die unterschiedliche Online-Eigenschaften erfüllen.Wir stützen unsere Erklärungsgenerierungsmethode auf eine Modellabstimmung, die in unserer früheren Arbeit eingeführt wurde.Unsere Ansätze werden sowohl mit menschlichen Probanden in einem Standard-Planungswettbewerb (IPC), unter Verwendung des NASA Task Load Index (TLX), als auch in der Simulation mit zehn verschiedenen Problemen in zwei IPC-Domänen evaluiert.
Tiefe neuronale Netze verwenden tiefere und breitere Strukturen, um eine bessere Leistung zu erzielen und folglich auch immer mehr GPU-Speicher zu verwenden.Allerdings schränkt der begrenzte GPU-Speicher viele potenzielle Designs neuronaler Netze ein.In diesem Papier schlagen wir einen auf Verstärkungslernen basierenden Variablentausch- und Neuberechnungsalgorithmus vor, um die Speicherkosten zu reduzieren, ohne die Genauigkeit der Modelle zu beeinträchtigen.Variablentausch kann Variablen zwischen CPU- und GPU-Speicher übertragen, um die im GPU-Speicher gespeicherten Variablen zu reduzieren. Die Neuberechnung kann Zeit gegen Platz tauschen, indem einige Merkmalskarten während der Vorwärtspropagation entfernt werden.Vorwärtsfunktionen werden erneut ausgeführt, um die Merkmalskarten vor der Wiederverwendung zu erhalten.Wie jedoch automatisch entschieden werden kann, welche Variablen ausgetauscht oder neu berechnet werden sollen, bleibt ein schwieriges Problem.Um dieses Problem anzugehen, schlagen wir vor, ein tiefes Q-Netz (DQN) zu verwenden, um Pläne zu machen.Durch die Kombination von Variablenswapping und Neuberechnung übertreffen unsere Ergebnisse mehrere bekannte Benchmarks.
Bei der Vanilla Backpropagation (VBP) spielt die Aktivierungsfunktion in Bezug auf Nichtlinearität und Differenzierbarkeit eine wichtige Rolle. Der verschwindende Gradient ist ein wichtiges Problem, das mit der schlechten Wahl der Aktivierungsfunktion beim Deep Learning (DL) zusammenhängt.Diese Arbeit zeigt, dass eine differenzierbare Aktivierungsfunktion für die Fehler-Backpropagation nicht mehr notwendig ist. Die Ableitung der Aktivierungsfunktion kann durch eine iterative zeitliche Differenzierung (ITD) mit fester zufälliger Rückkopplungsgewichtsanpassung (FBA) ersetzt werden. Durch die Verwendung von FBA mit ITD können wir die VBP in einen biologisch plausibleren Ansatz für das Lernen tiefer neuronaler Netzwerkarchitekturen umwandeln.
Inspiriert von den jüngsten Erfolgen von tiefen generativen Modellen für Text-To-Speech (TTS) wie WaveNet (van den Oord et al., 2016) und Tacotron (Wang et al., 2017), schlägt dieser Artikel die Verwendung eines tiefen generativen Modells vor, das für die automatische Spracherkennung (ASR) als primäres akustisches Modell (AM) für ein Gesamterkennungssystem mit einem separaten Sprachmodell (LM) zugeschnitten ist: (1) die Verwendung von Mischdichte-Netzwerken, sowohl autoregressiv als auch nicht-autoregressiv, um Dichtefunktionen zu erzeugen, die in der Lage sind, akustische Eingabesequenzen mit einer viel leistungsfähigeren Konditionierung zu modellieren als die generativen Modelle der ersten Generation für ASR, Gaussian Mixture Models / Hidden Markov Models (GMM/HMMs), und (2) die Verwendung von Standard-LSTMs, im Sinne des ursprünglichen Tandem-Ansatzes, um diskriminative Merkmalsvektoren für die generative Modellierung zu erzeugen. Die Kombination von Mixture-Density-Netzwerken und tiefen diskriminativen Merkmalen führt zu einer neuartigen Dual-Stack-LSTM-Architektur, die direkt mit dem RNN-Transducer (Graves, 2012) verwandt ist, aber die explizite funktionale Form einer Dichte aufweist und natürlich mit einem separaten Sprachmodell unter Verwendung der Bayes-Regel kombiniert wird.Die hier diskutierten generativen Modelle werden experimentell in Bezug auf die Log-Likelihoods und Frame-Genauigkeiten verglichen.
Jüngste Studien zeigen, dass weit verbreitete Deep neural networks (DNNs) sind anfällig für die sorgfältig gestaltete adversarial examples.Many fortschrittliche Algorithmen wurden vorgeschlagen, um adversarial Beispiele durch die Nutzung der L_p Abstand für die Bestrafung perturbations.Different Verteidigung Methoden wurden auch erforscht, um gegen solche adversarial attacks. Während die Effektivität der L_p-Distanz als Maß für die Wahrnehmungsqualität ein aktives Forschungsgebiet bleibt, konzentrieren wir uns in diesem Papier stattdessen auf eine andere Art von Störung, nämlich die räumliche Transformation, im Gegensatz zur direkten Manipulation der Pixelwerte wie in früheren Arbeiten.Störungen, die durch räumliche Transformation erzeugt werden, könnten zu großen L_p-Distanzmaßen führen, aber unsere umfangreichen Experimente zeigen, dass solche räumlich transformierten gegnerischen Beispiele wahrnehmungsmäßig realistisch und schwieriger zu verteidigen sind mit bestehenden Abwehrsystemen. Wir visualisieren die auf räumlicher Transformation basierende Störung für verschiedene Beispiele und zeigen, dass unsere Technik realistische gegnerische Beispiele mit glatter Bildverformung erzeugen kann.Schließlich visualisieren wir die Aufmerksamkeit von tiefen Netzwerken mit verschiedenen Arten von gegnerischen Beispielen, um besser zu verstehen, wie diese Beispiele interpretiert werden.
Die meisten existierenden Deep Reinforcement Learning (DRL) Frameworks betrachten Aktionsräume, die entweder diskret oder kontinuierlich sind.Motiviert durch das Projekt der Design Game AI für King of Glory (KOG), eines der weltweit beliebtesten Handyspiele, betrachten wir das Szenario mit dem diskret-kontinuierlichen hybriden Aktionsraum.Um existierende DLR Frameworks direkt anzuwenden, nähern sich existierende Ansätze entweder dem hybriden Raum durch eine diskrete Menge oder entspannen ihn in eine kontinuierliche Menge, was in der Regel weniger effizient und robust ist. In diesem Papier schlagen wir ein parametrisiertes tiefes Q-Netz (P-DQN) für den hybriden Aktionsraum ohne Approximation oder Relaxation vor.Unser Algorithmus kombiniert DQN und DDPG und kann als eine Erweiterung des DQN auf hybride Aktionen angesehen werden.Die empirische Studie über das Spiel KOG bestätigt die Effizienz und Effektivität unserer Methode.
In den letzten zehn Jahren wurden Wissensgraphen zur Erfassung von strukturiertem Fachwissen immer beliebter. Relationale Lernmodelle ermöglichen die Vorhersage fehlender Verbindungen innerhalb von Wissensgraphen, wobei latente Distanzansätze die Beziehungen zwischen Entitäten über eine Distanz zwischen latenten Repräsentationen modellieren.Translating Embedding Modelle (z.B. TransE) gehören zu den populärsten latenten Distanzansätzen, die eine Distanzfunktion verwenden, um mehrere Beziehungsmuster zu lernen. Sie sind jedoch meist ineffizient bei der Erfassung symmetrischer Relationen, da die Norm des Repräsentationsvektors für alle symmetrischen Relationen gleich Null wird.Sie verlieren auch Informationen, wenn sie Relationen mit reflexiven Mustern lernen, da sie symmetrisch und transitiv werden.Wir schlagen das Modell der Mehrfachdistanzeinbettung (MDE) vor, das diese Einschränkungen behebt, und einen Rahmen, der kollaborative Kombinationen latenter distanzbasierter Terme (MDE) ermöglicht. Unsere Lösung basiert auf zwei Prinzipien:1) Verwendung von limitbasierten Verlusten anstelle von Margin-Ranking-Verlusten und2) durch das Erlernen unabhängiger Einbettungsvektoren für jeden der Begriffe können wir gemeinsam trainieren und Vorhersagen unter Verwendung widersprüchlicher Distanzbegriffe treffen.Wir zeigen außerdem, dass MDE die Modellierung von Beziehungen mit (Anti-)Symmetrie, Inversion und Kompositionsmustern ermöglicht. Wir schlagen MDE als ein neuronales Netzwerkmodell vor, das es uns ermöglicht, nicht-lineare Beziehungen zwischen den Einbettungsvektoren und der erwarteten Ausgabe der Bewertungsfunktion abzubilden Unsere empirischen Ergebnisse zeigen, dass MDE die State-of-the-Art-Einbettungsmodelle auf mehreren Benchmark-Datensätzen übertrifft.
Improved generative adversarial network (Improved GAN) ist eine erfolgreiche Methode zur Verwendung generativer adversarialer Modelle, um das Problem des halbüberwachten Lernens zu lösen, leidet jedoch unter dem Problem der instabilen Ausbildung, und in diesem Papier haben wir festgestellt, dass die Instabilität hauptsächlich auf die verschwindenden Gradienten des Generators zurückzuführen ist. Um dieses Problem zu beheben, schlagen wir eine neue Methode, um kollaborative Ausbildung verwenden, um die Stabilität der semi-supervised GAN mit der Kombination von Wasserstein GAN.The Experimente haben gezeigt, dass unsere vorgeschlagene Methode ist stabiler als die ursprüngliche Improved GAN und erreicht vergleichbare Klassifizierung Genauigkeit auf verschiedenen Datensätzen zu verbessern.
Es ist seit langem bekannt, dass ein einschichtiges, vollständig verbundenes neuronales Netz mit einem i.i.d.-Prior über seine Parameter einem Gauß-Prozess (GP) im Grenzfall einer unendlichen Netzbreite entspricht.  Kürzlich wurden Kernel-Funktionen entwickelt, die mehrschichtige zufällige neuronale Netze imitieren, jedoch nur außerhalb eines Bayes'schen Rahmens. Daher wurde in früheren Arbeiten nicht erkannt, dass diese Kernel als Kovarianzfunktionen für GPs verwendet werden können und eine vollständig Bayes'sche Vorhersage mit einem tiefen neuronalen Netz ermöglichen. In dieser Arbeit leiten wir die exakte Äquivalenz zwischen unendlich breiten, tiefen Netzen und GPs mit einer bestimmten Kovarianzfunktion ab und entwickeln eine rechnerisch effiziente Pipeline, um diese Kovarianzfunktion zu berechnen und verwenden die resultierende GP, um Bayes'sche Inferenz für tiefe neuronale Netze auf MNIST und CIFAR-10 durchzuführen.  Wir stellen fest, dass sich die Genauigkeit des trainierten neuronalen Netzes mit zunehmender Schichtbreite derjenigen des entsprechenden GP annähert und dass die GP-Unsicherheit stark mit dem Vorhersagefehler des trainierten Netzes korreliert ist.
Kürzlich fanden Xie et al. (2019a) heraus, dass zufällig generierte Netze aus derselben Verteilung ähnlich funktionieren, was darauf hindeutet, dass wir nach zufälligen Graphenverteilungen anstelle von Graphen suchen sollten.Wir schlagen Graphon als neuen Suchraum vor. Ein Graphon ist der Grenzwert der Cauchy-Folge von Graphen und einer skalenfreien probabilistischen Verteilung, aus der Graphen mit unterschiedlicher Anzahl von Scheitelpunkten gezogen werden können.Diese Eigenschaft ermöglicht es uns, NAS unter Verwendung von schnellen Modellen mit geringer Kapazität durchzuführen und die gefundenen Modelle bei Bedarf hochzuskalieren.Wir entwickeln einen Algorithmus für NAS im Raum der Graphons und zeigen empirisch, dass er stufenweise Graphen finden kann, die DenseNet und andere Baselines im ImageNet übertreffen.
Wir stellen die Hypothese auf, dass diese schlechte Generalisierung eine Folge des adversen Trainings mit einem gleichmäßigen Störungsradius um jede Trainingsstichprobe ist. Stichproben nahe der Entscheidungsgrenze können unter einem kleinen Störungsbudget in eine andere Klasse umgewandelt werden, und die Erzwingung großer Ränder um diese Stichproben führt zu schlechten Entscheidungsgrenzen, die schlecht verallgemeinert werden können. Ausgehend von dieser Hypothese schlagen wir ein instanzadaptives adverses Training vor - eine Technik, die stichprobenspezifische Störungsspannen um jede Trainingsstichprobe herum erzwingt. wir zeigen, dass sich mit unserem Ansatz die Testgenauigkeit bei ungestörten Stichproben verbessert, während die Robustheit nur geringfügig abnimmt. umfangreiche Experimente mit den Datensätzen CIFAR-10, CIFAR-100 und Imagenet belegen die Wirksamkeit unseres vorgeschlagenen Ansatzes.
Modelle des maschinellen Lernens, einschließlich traditioneller Modelle und neuronaler Netze, können leicht durch negative Beispiele getäuscht werden, die aus natürlichen Beispielen mit kleinen Störungen erzeugt werden.  Dies stellt eine kritische Herausforderung für die Sicherheit des maschinellen Lernens dar und behindert die breite Anwendung des maschinellen Lernens in vielen wichtigen Bereichen wie Computer Vision und Malware-Erkennung.  Leider leiden selbst modernste Verteidigungsansätze wie gegnerisches Training und defensive Destillation immer noch unter großen Einschränkungen und können umgangen werden.  In dieser Arbeit wollen wir zwei wichtige Forschungsfragen aus einem einzigartigen Blickwinkel untersuchen: Sind gegnerische Beispiele von natürlichen Beispielen unterscheidbar?  Sind mit verschiedenen Methoden erzeugte Negativbeispiele voneinander unterscheidbar?  Diese beiden Fragen betreffen die Unterscheidbarkeit von Negativbeispielen.  Die Beantwortung dieser Fragen wird möglicherweise zu einem einfachen, aber effektiven Ansatz führen, der in dieser Arbeit als defensive Unterscheidung unter der Bezeichnung Multi-Label-Klassifikation bezeichnet wird, um sich gegen negative Beispiele zu schützen.  Wir entwerfen und führen Experimente mit dem MNIST-Datensatz durch, um diese beiden Fragen zu untersuchen, und erhalten sehr positive Ergebnisse, die die starke Unterscheidbarkeit von gegnerischen Beispielen zeigen.  Wir empfehlen, dass dieser einzigartige Ansatz zur defensiven Unterscheidung ernsthaft in Betracht gezogen werden sollte, um andere Verteidigungsansätze zu ergänzen.
Lange Zeit galt die Entwicklung leistungsfähiger neuronaler Architekturen als eine dunkle Kunst, die nur von Experten abgestimmt werden konnte. eine der wenigen bekannten Richtlinien für die Entwicklung von Architekturen ist die Vermeidung explodierender oder verschwindender Gradienten. Doch selbst diese Richtlinie ist relativ vage und umständlich geblieben, da es keine wohldefinierte, gradientenbasierte Metrik gibt, die vor Beginn des Trainings berechnet werden kann und die Leistung des Netzwerks nach Abschluss des Trainings zuverlässig vorhersagen kann.Wir stellen die unseres Wissens erste derartige Metrik vor: den Nichtlinearitätskoeffizienten (NLC). Anhand einer umfangreichen empirischen Studie zeigen wir, dass der NLC, der im zufällig initialisierten Zustand des Netzes berechnet wird, ein leistungsfähiger Prädiktor für den Testfehler ist und dass das Erreichen eines NLC in der richtigen Größe wesentlich für das Erreichen eines optimalen Testfehlers ist, zumindest in vollständig verbundenen Feedforward-Netzen. Die NLC ist auch konzeptionell einfach, billig zu berechnen und robust gegenüber einer Reihe von Störfaktoren und architektonischen Designentscheidungen, gegenüber denen vergleichbare Metriken nicht unbedingt robust sind.
Dieses Papier gibt eine rigorose Analyse von trainierten Generalized Hamming Networks (GHN), die von Fan (2017) vorgeschlagen wurden, und enthüllt eine interessante Erkenntnis über GHNs, d.h. gestapelte Faltungsschichten in einem GHN sind äquivalent zu einer einzelnen, aber breiten Faltungsschicht.Die aufgedeckte Äquivalenz kann auf der theoretischen Seite als eine konstruktive Manifestation des universellen Approximationstheorems Cybenko (1989); Hornik (1991) betrachtet werden. Für die Netzwerkvisualisierung bieten die konstruierten tiefen Epitome auf jeder Ebene eine Visualisierung der internen Repräsentation des Netzwerks, die nicht von den Eingabedaten abhängt, und ermöglichen die direkte Extraktion von Merkmalen in nur einem Schritt, ohne auf regulierte Optimierungen zurückgreifen zu müssen, die in bestehenden Visualisierungstools verwendet werden.
Es gibt zwei Hauptparadigmen von White-Box-Angriffen, die versuchen, Eingabestörungen zu erzwingen.  Beim ersten Paradigma, dem so genannten Fix-Störungs-Angriff, werden gegnerische Proben innerhalb eines bestimmten Störungsniveaus hergestellt.  Das zweite Paradigma, der so genannte Zero-Confidence-Angriff, findet die kleinste Störung, die erforderlich ist, um eine Fehlklassifizierung zu verursachen, auch bekannt als die Marge eines Eingangsmerkmals.  Während das erste Paradigma gut gelöst ist, ist es das zweite nicht.  Bestehende Zero-Confidence-Angriffe führen entweder zu erheblichen Approximationsfehlern oder sind zu zeitaufwändig.  Wir schlagen daher MarginAttack vor, einen Zero-Confidence-Angriffsrahmen, der in der Lage ist, die Marge mit verbesserter Genauigkeit und Effizienz zu berechnen.  Unsere Experimente zeigen, dass MarginAttack in der Lage ist, eine kleinere Marge zu berechnen als die modernsten Zero-Confidence-Angriffe und mit den modernsten Fix-Perturbation-Angriffen übereinstimmt.  Darüber hinaus läuft er deutlich schneller als der Carlini-Wagner-Angriff, der derzeit genaueste Null-Vertrauens-Angriffsalgorithmus.
Anstatt Variationen über Filterkanäle oder Pyramidenebenen hinweg aufzuzählen, sagen dynamische Modelle den Maßstab lokal voraus und passen die rezeptiven Felder entsprechend an.Der Grad der Variation und die Vielfalt der Eingaben machen dies zu einer schwierigen Aufgabe.Bestehende Methoden lernen entweder einen Feedforward-Prädiktor, der selbst nicht völlig immun gegen die Maßstabsvariationen ist, denen er entgegenwirken soll, oder sie wählen den Maßstab durch einen festen Algorithmus aus, der nicht aus der gegebenen Aufgabe und den Daten lernen kann. Wir schlagen ein neuartiges Entropie-Minimierungsziel für die Inferenz vor und optimieren über Aufgaben- und Strukturparameter, um das Modell auf jede Eingabe abzustimmen. Die Optimierung während der Inferenz verbessert die semantische Segmentierungsgenauigkeit und verallgemeinert besser auf extreme Skalenvariationen, die die dynamische Feedforward-Inferenz zum Scheitern bringen.
Der Deep Image Prior (DIP, Ulyanov et al., 2017) ist ein faszinierender neuer Ansatz für die Wiederherstellung von Bildern, die natürlich erscheinen, ist aber nicht vollständig verstanden.Diese Arbeit zielt darauf ab, etwas mehr Licht auf diesen Ansatz durch die Untersuchung der Eigenschaften der frühen Ausgaben des DIP.Zuerst zeigen wir, dass diese frühen Iterationen Invarianz zu nachteiligen Störungen durch Klassifizierung progressiver DIP-Ausgänge und mit einem neuartigen Saliency Map Ansatz zeigen. Schließlich untersuchen wir die Invarianz der frühen DIP-Ausgänge gegenüber Störungen und stellen die Hypothese auf, dass diese Ausgänge nicht robuste Bildmerkmale entfernen können, indem wir die Konfidenzwerte für die Klassifizierung vergleichen, was diese Hypothese bestätigt.
In diesem Papier, adressieren wir die Herausforderung der begrenzten beschrifteten Daten und Klasse Ungleichgewicht Problem für maschinelles Lernen-basierte Gerüchte-Erkennung auf Social Media.We präsentieren eine Offline-Daten-Erweiterung Methode basiert auf semantische Verwandtschaft für Gerüchte-Erkennung.Zu diesem Zweck wird nicht beschrifteten Social-Media-Daten genutzt werden, um begrenzte beschriftete Daten zu erweitern.Eine kontextbezogene neuronale Sprachmodell und eine große Glaubwürdigkeit fokussiert Twitter-Korpus eingesetzt werden, um effektive Darstellungen von Gerüchten Tweets für semantische Verwandtschaft Messung lernen. Wir führen Experimente zu sechs verschiedenen realen Ereignissen durch, die auf fünf öffentlich verfügbaren Datensätzen und einem erweiterten Datensatz basieren. Unsere Experimente zeigen, dass die vorgeschlagene Methode es uns ermöglicht, eine größere Anzahl von Trainingsdaten mit angemessener Qualität über eine schwache Überwachung zu generieren.Wir präsentieren vorläufige Ergebnisse, die mit einem hochmodernen neuronalen Netzwerkmodell mit erweiterten Daten zur Gerüchteerkennung erzielt wurden.
Self-Attention ist ein nützlicher Mechanismus, um generative Modelle für Sprache und Bilder zu erstellen, der die Wichtigkeit von Kontextelementen bestimmt, indem er jedes Element mit dem aktuellen Zeitschritt vergleicht.In diesem Papier zeigen wir, dass eine sehr leichtgewichtige Faltung mit den besten berichteten Self-Attention-Ergebnissen konkurrieren kann.Als nächstes führen wir dynamische Faltungen ein, die einfacher und effizienter sind als Self-Attention. Die Anzahl der Operationen, die für diesen Ansatz erforderlich sind, skaliert linear mit der Eingabelänge, während Self-Attention quadratisch ist.Experimente in den Bereichen maschinelle Übersetzung, Sprachmodellierung und abstrakte Zusammenfassung zeigen, dass dynamische Faltungen besser sind als starke Self-Attention-Modelle.Auf dem WMT'14 Englisch-Deutsch-Testsatz erreichen dynamische Faltungen einen neuen Stand der Technik von 29,7 BLEU.
Aufgrund ihrer Verbindung mit generativen adversen Netzwerken (GANs) haben Sattelpunktprobleme in letzter Zeit ein beträchtliches Interesse im maschinellen Lernen und darüber hinaus auf sich gezogen.Zwangsläufig drehen sich die meisten theoretischen Garantien um konvex-konkave (oder sogar lineare) Probleme. Um in dieser Hinsicht Stück für Stück voranzukommen, analysieren wir das Verhalten von Mirror Descent (MD) in einer Klasse von nicht-monotonen Problemen, deren Lösungen mit denen einer natürlich assoziierten Variationsungleichung übereinstimmen - eine Eigenschaft, die wir Kohärenz nennen. Wir zeigen zunächst, dass gewöhnlicher, "vanilla" MD unter einer strikten Version dieser Bedingung konvergiert, aber nicht anderweitig; insbesondere kann er sogar in bilinearen Modellen mit einer eindeutigen Lösung nicht konvergieren.Wir zeigen dann, dass dieser Mangel durch Optimismus gemildert wird: durch einen "extra-gradient" Schritt konvergiert optimistischer Spiegelabstieg (OMD) in allen kohärenten Problemen. Unsere Analyse verallgemeinert und erweitert die Ergebnisse von Daskalakis et al. [2018] für optimistischen Gradientenabstieg (OGD) in bilinearen Problemen und macht konkrete Fortschritte für nachweisbare Konvergenz jenseits von konvex-konkaven Spielen.Wir bieten auch stochastische Analoga dieser Ergebnisse, und wir validieren unsere Analyse durch numerische Experimente in einer breiten Palette von GAN-Modellen (einschließlich Gauß'schen Mischmodellen und den CelebA- und CIFAR-10-Datensätzen).
Die Batch-Normalisierung (BN) wird häufig verwendet, um das Training in tiefen neuronalen Netzen zu stabilisieren und zu beschleunigen. in vielen Fällen verringert sie tatsächlich die Anzahl der Parameteraktualisierungen, die erforderlich sind, um einen niedrigen Trainingsfehler zu erreichen. sie reduziert jedoch auch die Robustheit gegenüber kleinen Störungen der Eingaben durch den Gegner und häufigen Korruptionen im zweistelligen Prozentbereich, wie wir an fünf Standarddatensätzen zeigen. außerdem stellen wir fest, dass der Ersatz der BN durch einen Gewichtsabfall ausreicht, um eine Beziehung zwischen der Anfälligkeit durch den Gegner und der Eingabedimension aufzuheben. Eine kürzlich durchgeführte Mean-Field-Analyse hat ergeben, dass BN eine Gradientenexplosion hervorruft, wenn es auf mehreren Ebenen verwendet wird, aber dies kann die von uns beobachtete Anfälligkeit nicht vollständig erklären, da sie bereits bei einer einzigen BN-Ebene auftritt. Wir erklären diesen Mechanismus explizit an einem linearen Spielzeugmodell und zeigen in Experimenten, dass er auch für nichtlineare "Real-World"-Modelle gilt.
Learning to Optimize ist ein kürzlich vorgeschlagener Rahmen für das Erlernen von Optimierungsalgorithmen mit Hilfe von Reinforcement Learning.In diesem Beitrag untersuchen wir das Erlernen eines Optimierungsalgorithmus für das Training flacher neuronaler Netze.Solche hochdimensionalen stochastischen Optimierungsprobleme stellen eine interessante Herausforderung für bestehende Reinforcement Learning Algorithmen dar. Wir entwickeln eine Erweiterung, die sich für das Lernen von Optimierungsalgorithmen in diesem Umfeld eignet, und zeigen, dass der gelernte Optimierungsalgorithmus andere bekannte Optimierungsalgorithmen auch bei ungesehenen Aufgaben durchgängig übertrifft und robust gegenüber Änderungen der Stochastizität der Gradienten und der Architektur des neuronalen Netzes ist. Genauer gesagt zeigen wir, dass ein Optimierungsalgorithmus, der mit der vorgeschlagenen Methode für das Problem des Trainings eines neuronalen Netzes auf MNIST trainiert wurde, auf die Probleme des Trainings neuronaler Netze auf dem Toronto Faces Dataset, CIFAR-10 und CIFAR-100 verallgemeinert.
Die Abhängigkeit des Generalisierungsfehlers neuronaler Netze von der Größe des Modells und des Datensatzes ist sowohl in der Praxis als auch für das Verständnis der Theorie neuronaler Netze von entscheidender Bedeutung, doch die funktionale Form dieser Abhängigkeit ist nach wie vor schwer fassbar.In dieser Arbeit stellen wir eine funktionale Form vor, die den Generalisierungsfehler in der Praxis gut approximiert, Unsere Konstruktion basiert auf Erkenntnissen, die wir aus Beobachtungen gewonnen haben, die wir über eine Reihe von Modell-/Datenskalen, in verschiedenen Modelltypen und Datensätzen, in Seh- und Sprachaufgaben durchgeführt haben.Wir zeigen, dass die Form sowohl gut zu den Beobachtungen über Skalen hinweg passt, als auch genaue Vorhersagen von kleinen bis großen Modellen und Daten liefert.
Wir stellen einen unüberwachten Lernalgorithmus vor, um Agenten zu trainieren, wahrnehmungsspezifische Ziele zu erreichen, indem wir nur einen Strom von Beobachtungen und Aktionen verwenden. Unser Agent lernt gleichzeitig eine zielbedingte Strategie und eine Zielerreichungs-Belohnungsfunktion, die misst, wie ähnlich ein Zustand dem Zielzustand ist. Diese duale Optimierung führt zu einem kooperativen Spiel, das zu einer erlernten Belohnungsfunktion führt, die die Ähnlichkeit in kontrollierbaren Aspekten der Umgebung widerspiegelt, anstatt den Abstand im Raum der Beobachtungen. Wir demonstrieren die Wirksamkeit unseres Agenten, um auf eine unbeaufsichtigte Weise zu lernen, eine Reihe von Zielen auf drei Domänen zu erreichen - Atari, die DeepMind Control Suite und DeepMind Lab.
Moderne Sequenz-zu-Sequenz-Modelle für große Aufgaben führen eine feste Anzahl von Berechnungen für jede Eingabesequenz durch, unabhängig davon, ob sie leicht oder schwer zu verarbeiten ist. In diesem Papier trainieren wir Transformer-Modelle, die Ausgabevorhersagen in verschiedenen Phasen des Netzwerks machen können, und wir untersuchen verschiedene Möglichkeiten, um vorherzusagen, wie viel Berechnung für eine bestimmte Sequenz erforderlich ist. Im Gegensatz zur dynamischen Berechnung in Universal Transformers, die iterativ denselben Satz von Schichten anwendet, wenden wir bei jedem Schritt verschiedene Schichten an, um sowohl den Umfang der Berechnung als auch die Modellkapazität anzupassen. Bei der IWSLT-Übersetzung Deutsch-Englisch erreicht unser Ansatz die Genauigkeit eines gut abgestimmten Basis-Transformers, während er weniger als ein Viertel der Decoder-Schichten verwendet.
Variationale Autokodierer (VAEs) sind tiefe generative latente Variablenmodelle, die aus zwei Komponenten bestehen: einem generativen Modell, das eine Datenverteilung p(x) durch Transformation einer Verteilung p(z) über den latenten Raum erfasst, und einem Inferenzmodell, das wahrscheinliche latente Codes für jeden Datenpunkt ableitet (Kingma und Welling, 2013).Jüngste Arbeiten zeigen, dass traditionelle Trainingsmethoden dazu neigen, Lösungen zu liefern, die Modellierungsdesiderate verletzen: (1) das gelernte generative Modell erfasst die beobachtete Datenverteilung, ignoriert dabei aber die latenten Codes, was zu Codes führt, die die Daten nicht repräsentieren (z. B. van den Oord et al. (2017); Kim et al. (2018)); (2) die Gesamtheit der gelernten latenten Codes stimmt nicht mit dem Prior p(z) überein. Diese Nichtübereinstimmung bedeutet, dass das gelernte generative Modell nicht in der Lage ist, realistische Daten mit Stichproben von p(z) zu generieren (z. B. Makhzani et al. (2015); Tomczak und Welling (2017)).In diesem Papier zeigen wir, dass beide Probleme aus der Tatsache resultieren, dass die globalen Optima des VAE-Trainingsziels oft unerwünschten Lösungen entsprechen.Unsere Analyse baut auf zwei Beobachtungen auf: (1) das generative Modell ist nicht identifizierbar - es gibt viele generative Modelle, die die Daten gleich gut erklären, jedes mit unterschiedlichen (und potenziell unerwünschten) Eigenschaften und (2) Verzerrungen im VAE-Ziel - das VAE-Ziel kann generative Modelle bevorzugen, die die Daten schlecht erklären, aber Posterioren haben, die leicht zu approximieren sind. Anhand von synthetischen Datensätzen zeigen wir, dass LiBI generative Modelle erlernen kann, die die Datenverteilung erfassen und Inferenzmodelle, die die Modellierungsannahmen besser erfüllen, wenn traditionelle Methoden damit Schwierigkeiten haben.
Die Modellierung von Hypernymen, wie z.B. "Pudel ist ein Hund", ist eine wichtige Verallgemeinerungshilfe für viele NLP-Aufgaben, wie z.B. Entailment, Relationsextraktion und Beantwortung von Fragen. Überwachtes Lernen aus gelabelten Hypernym-Quellen, wie z.B. WordNet, schränkt die Reichweite dieser Modelle ein, was durch das Lernen von Hypernymen aus nicht gelabeltem Text behoben werden kann.  Bestehende unüberwachte Methoden sind entweder nicht für große Vokabularien geeignet oder liefern eine inakzeptabel schlechte Genauigkeit.  In diesem Beitrag wird die Distributional Inclusion Vector Embedding (DIVE) vorgestellt, eine einfach zu implementierende, unbeaufsichtigte Methode zur Entdeckung von Hypernymen durch nicht-negative Vektoreinbettungen pro Wort, die die Eigenschaft der Inklusion von Wortkontexten bewahren. In experimentellen Auswertungen, die umfassender sind als alle anderen uns bekannten Literaturquellen - die Auswertung von 11 Datensätzen unter Verwendung mehrerer bestehender und neu vorgeschlagener Bewertungsfunktionen - stellen wir fest, dass unsere Methode eine bis zu doppelt so hohe Präzision wie frühere unbeaufsichtigte Methoden und die höchste durchschnittliche Leistung bietet, wobei eine viel kompaktere Wortrepräsentation verwendet wird und viele neue State-of-the-Art-Ergebnisse erzielt werden.
Kontinuierliches Lernen zielt darauf ab, neue Aufgaben zu lernen, ohne zuvor gelernte Aufgaben zu vergessen, was eine besondere Herausforderung darstellt, wenn man keinen Zugang zu den Daten früherer Aufgaben hat und wenn das Modell eine feste Kapazität hat.Aktuelle Algorithmen für kontinuierliches Lernen, die auf Regularisierung basieren, benötigen eine externe Repräsentation und zusätzliche Berechnungen, um die Wichtigkeit der Parameter zu messen.Im Gegensatz dazu schlagen wir Uncertainty-guided Continual Bayesian Neural Networks (UCB) vor, bei denen sich die Lernrate entsprechend der Ungewissheit anpasst, die in der Wahrscheinlichkeitsverteilung der Gewichte in Netzwerken definiert ist. Wir zeigen auch eine Variante unseres Modells, die Unsicherheit für das Pruning der Gewichte verwendet und die Leistung der Aufgaben nach dem Pruning beibehält, indem sie binäre Masken pro Aufgabe speichert. Wir evaluieren unseren UCB-Ansatz ausgiebig auf verschiedenen Objektklassifizierungsdatensätzen mit kurzen und langen Aufgabensequenzen und berichten von einer überlegenen oder gleichwertigen Leistung im Vergleich zu bestehenden Ansätzen.Zusätzlich zeigen wir, dass unser Modell nicht notwendigerweise Aufgabeninformationen zum Testzeitpunkt benötigt, d.h. es setzt kein Wissen darüber voraus, zu welcher Aufgabe eine Probe gehört.
Menschen haben eine natürliche Neugier, sich vorzustellen, wie es sich anfühlt, als jemand oder etwas anderes zu existieren. Diese Neugier wird sogar noch stärker für die Haustiere, für die wir sorgen. Menschen können nicht wirklich wissen, wie es ist, unsere Haustiere zu sein, aber wir können unser Verständnis dafür vertiefen, wie es ist, die Welt wie sie wahrzunehmen und zu erforschen.Wir untersuchen, wie Wearables den Menschen die Möglichkeit bieten können, die Welt aus der Perspektive von Tieren zu erleben, die sich von denen unterscheiden, die für uns biologisch natürlich sind. Um das Potenzial von Wearables für die Übernahme der Tierperspektive zu bewerten, haben wir ein sensorisches Wearable entwickelt, das den Trägern katzenähnliche Schnurrhaare verleiht. Anschließend haben wir ein Labyrinth-Erkundungserlebnis geschaffen, bei dem die Teilnehmer mit verbundenen Augen die Schnurrhaare zur Navigation durch das Labyrinth nutzten.
Der Generalisierungsfehler (auch bekannt als Out-of-Sample-Fehler) misst, wie gut sich die aus den Trainingsdaten gelernte Hypothese auf zuvor nicht gesehene Daten verallgemeinern lässt.Der Nachweis enger Grenzen für den Generalisierungsfehler ist eine zentrale Frage in der statistischen Lerntheorie.   In diesem Papier erhalten wir Generalisierungsfehlerschranken für das Lernen allgemeiner nicht-konvexer Ziele, was in den letzten Jahren große Aufmerksamkeit erregt hat.   Wir entwickeln einen neuen Rahmen, der als Bayes-Stabilität bezeichnet wird, um Algorithmus-abhängige Generalisierungsfehlerschranken zu beweisen.  Der neue Rahmen kombiniert Ideen aus der PAC-Bayesianischen Theorie und dem Begriff der algorithmischen Stabilität.  Unter Anwendung der Bayes-Stabilitäts-Methode erhalten wir neue datenabhängige Generalisierungsschranken für die stochastische Gradienten-Langevin-Dynamik (SGLD) und verschiedene andere verrauschte Gradientenverfahren (z.B. mit Momentum, Mini-Batch und Beschleunigung, Entropie-SGD).Unser Ergebnis ist typischerweise enger als ein kürzlich erzieltes Ergebnis in Mou et al. (2018) und verbessert die Ergebnisse in Pensia et al. (2018).  Unsere Experimente zeigen, dass unsere datenabhängigen Schranken zufällig beschriftete Daten von normalen Daten unterscheiden können, was eine Erklärung für die faszinierenden Phänomene liefert, die in Zhang et al. (2017a) beobachtet wurden.Wir untersuchen auch die Situation, in der der Gesamtverlust die Summe aus einem begrenzten Verlust und einem zusätzlichen l`2-Regularisierungsterm ist. Wir erhalten neue Verallgemeinerungsschranken für die kontinuierliche Langevin-Dynamik in dieser Umgebung, indem wir eine neue Log-Sobolev-Ungleichung für die Parameterverteilung zu jedem Zeitpunkt entwickeln.unsere neuen Schranken sind wünschenswerter, wenn das Rauschniveau des Prozesses nicht sehr klein ist, und werden auch dann nicht leer, wenn T gegen unendlich tendiert.
In diesem Papier wird eine neue intrinsische Belohnungsgenerierungsmethode für sparse-reward reinforcement learning vorgeschlagen, die auf einem Ensemble von Dynamikmodellen basiert. In der vorgeschlagenen Methode wird die Mischung aus mehreren Dynamikmodellen verwendet, um die wahre unbekannte Übergangswahrscheinlichkeit anzunähern, und die intrinsische Belohnung wird als das Minimum der Überraschung von jedem Dynamikmodell zur Mischung der Dynamikmodelle entworfen. Um die Effektivität der vorgeschlagenen Methode zur Generierung intrinsischer Belohnungen zu zeigen, wird ein funktionierender Algorithmus konstruiert, indem die vorgeschlagene Methode zur Generierung intrinsischer Belohnungen mit dem Algorithmus zur proximalen Politikoptimierung (PPO) kombiniert wird. numerische Ergebnisse zeigen, dass die vorgeschlagene, auf einem Modell-Ensemble basierende Methode zur Generierung intrinsischer Belohnungen für repräsentative Fortbewegungsaufgaben besser abschneidet als frühere Methoden, die auf einem einzelnen dynamischen Modell basieren.
Angesichts der schnellen Entwicklung von Analysetechniken für NLP- und Sprachverarbeitungssysteme wurden bisher nur wenige systematische Studien durchgeführt, um die Stärken und Schwächen der einzelnen Methoden zu vergleichen. Wir verwenden zwei häufig angewandte Analysetechniken, diagnostische Klassifikatoren und Repräsentationsähnlichkeitsanalysen, um zu quantifizieren, inwieweit neuronale Aktivierungsmuster Phoneme und Phonemsequenzen kodieren.Wir manipulieren zwei Faktoren, die das Ergebnis der Analyse beeinflussen können.Erstens untersuchen wir die Rolle des Lernens, indem wir neuronale Aktivierungen aus trainierten und zufällig initialisierten Modellen vergleichen. Zweitens untersuchen wir den zeitlichen Umfang der Aktivierungen, indem wir sowohl lokale Aktivierungen, die einigen Millisekunden des Sprachsignals entsprechen, als auch globale Aktivierungen, die über die gesamte Äußerung gepoolt sind, untersuchen.Wir kommen zu dem Schluss, dass die Darstellung von Analyseergebnissen mit zufällig initialisierten Modellen von entscheidender Bedeutung ist und dass Methoden mit globalem Umfang tendenziell konsistentere und interpretierbare Ergebnisse liefern, und wir empfehlen ihre Verwendung als Ergänzung zu diagnostischen Methoden mit lokalem Umfang.
Super Resolution (SR) ist eine grundlegende und wichtige Low-Level-Computer-Vision (CV)-Aufgabe.Anders als bei traditionellen SR-Modellen konzentriert sich diese Studie auf eine spezifische, aber realistische SR-Frage: Wie können wir zufriedenstellende SR-Ergebnisse aus komprimierten JPG-Bildern (C-JPG) erzielen, die im Internet weit verbreitet sind.C-JPG kann im Allgemeinen Speicherplatz freisetzen und gleichzeitig eine beträchtliche Qualität des Bildes beibehalten, Um dieses Problem zu lösen, schlagen wir eine neuartige SR-Struktur mit zwei speziell entwickelten Komponenten sowie einen Zyklusverlust vor.Kurz gesagt, gibt es hauptsächlich drei Beiträge zu diesem Papier.Erstens kann unsere Forschung hochqualifizierte SR-Bilder für gängige C-JPG-Bilder erzeugen. Zweitens schlagen wir ein funktionales Untermodell vor, um Informationen für C-JPG-Bilder wiederzugewinnen, anstatt die Perspektive der Rauschunterdrückung in traditionellen SR-Ansätzen zu verwenden.Drittens integrieren wir den Zyklusverlust in den SR-Löser, um eine hybride Verlustfunktion für eine bessere SR-Generierung zu erstellen.Experimente zeigen, dass unser Ansatz eine hervorragende Leistung unter den modernsten Methoden erreicht.
Keyword-Spotting-oder Wakeword-Erkennung-ist ein wesentliches Merkmal für die Freisprecheinrichtung von modernen sprachgesteuerten Geräten.Mit solchen Geräten immer allgegenwärtig, könnten Benutzer wollen eine personalisierte benutzerdefinierte Wakeword wählen.In dieser Arbeit stellen wir DONUT, ein CTC-basierten Algorithmus für Online-Abfrage-by-Beispiel Keyword-Spotting, die benutzerdefinierte Wakeword-Erkennung ermöglicht. Unsere Methode kombiniert die Generalisierung und Interpretierbarkeit von CTC-basiertem Keyword-Spotting mit der Benutzeranpassung und dem Komfort eines konventionellen Query-by-Example-Systems. DONUT hat niedrige Rechenanforderungen und eignet sich sowohl für das Lernen als auch für Inferenzen auf eingebetteten Systemen, ohne dass private Benutzerdaten in die Cloud hochgeladen werden müssen.
Um flexibel und effizient über zeitliche Sequenzen zu schlussfolgern, werden abstrakte Repräsentationen benötigt, die die wichtigen Informationen in der Sequenz kompakt darstellen.eine Möglichkeit, solche Repräsentationen zu konstruieren, besteht darin, sich auf die wichtigen Ereignisse in einer Sequenz zu konzentrieren.in diesem Papier schlagen wir ein Modell vor, das sowohl lernt, solche Schlüsselereignisse (oder Keyframes) zu entdecken, als auch die Sequenz in Bezug auf sie darzustellen.  Wir tun dies unter Verwendung eines hierarchischen Keyframe-Inpainter (KeyIn) Modells, das zuerst Keyframes und deren zeitliche Platzierung generiert und dann die Sequenzen zwischen Keyframes einfärbt. Wir schlagen eine vollständig differenzierbare Formulierung für effizientes Lernen der Keyframe-Platzierung vor und zeigen, dass KeyIn informative Keyframes in verschiedenen Datensätzen mit unterschiedlicher Dynamik findet.
In dieser Arbeit wird das unbeaufsichtigte Lernen von Repräsentationen durch Maximierung der gegenseitigen Information zwischen einer Eingabe und der Ausgabe eines tiefen neuronalen Netzwerk-Encoders untersucht. Unsere Methode, die wir Deep InfoMax (DIM) nennen, übertrifft eine Reihe beliebter unbeaufsichtigter Lernmethoden und schneidet im Vergleich zum vollständig überwachten Lernen bei mehreren Klassifizierungsaufgaben in einigen Standardarchitekturen gut ab.DIM eröffnet neue Wege für das unbeaufsichtigte Lernen von Repräsentationen und ist ein wichtiger Schritt in Richtung flexibler Formulierungen von Repräsentationslernzielen für spezifische Endziele.
Der gebräuchlichste Weg zur Schätzung der atomaren Bedeutung ist die Berechnung der elektronischen Struktur mit Hilfe der Dichtefunktionaltheorie (DFT) und die anschließende Interpretation mit Hilfe des Domänenwissens menschlicher Experten. Dieser konventionelle Ansatz ist jedoch für große Moleküldatenbanken nicht praktikabel, da die DFT-Berechnung einen enormen Rechenaufwand erfordert, insbesondere eine Zeitkomplexität von O(n^4) bezogen auf die Anzahl der Elektronen in einem Molekül. Um dieses Problem zu lösen, nutzen wir zunächst einen auf maschinellem Lernen basierenden Ansatz für die Schätzung der atomaren Bedeutung. Zu diesem Zweck schlagen wir eine umgekehrte Selbstaufmerksamkeit auf graphischen neuronalen Netzen vor und integrieren sie mit einer graphbasierten molekularen Beschreibung. Unsere Methode bietet einen effizienten, automatisierten und zielgerichteten Weg zur Schätzung der atomaren Bedeutung ohne jegliches Fachwissen über Chemie und Physik.
Spektrales Clustering ist eine führende und beliebte Technik in der unüberwachten Datenanalyse.  Zwei ihrer wichtigsten Einschränkungen sind die Skalierbarkeit und die Verallgemeinerung der spektralen Einbettung (d. h., Unser Netzwerk, das wir SpectralNet nennen, lernt eine Karte, die Eingabedatenpunkte in den Eigenraum ihrer zugehörigen Graphen-Laplacian-Matrix einbettet und sie anschließend clustert.Wir trainieren SpectralNet mit einem Verfahren, das eingeschränkte stochastische Optimierung beinhaltet. Die stochastische Optimierung ermöglicht die Skalierung auf große Datensätze, während die Einschränkungen, die mit Hilfe einer speziellen Ausgabeschicht implementiert werden, es uns ermöglichen, die Netzwerkausgabe orthogonal zu halten.Darüber hinaus verallgemeinert die von SpectralNet gelernte Karte die spektrale Einbettung auf natürliche Weise auf ungesehene Datenpunkte.Um die Qualität des Clusterns weiter zu verbessern, ersetzen wir die standardmäßigen paarweisen Gauß-Affinitäten durch Affinitäten, die aus unbeschrifteten Daten mit Hilfe eines siamesischen Netzwerks abgeleitet werden.  Eine zusätzliche Verbesserung kann erreicht werden, indem das Netzwerk auf Code-Repräsentationen angewendet wird, die z.B. von Standard-Autocodierern erzeugt werden.Unser End-to-End-Lernverfahren ist vollständig unbeaufsichtigt.Darüber hinaus wenden wir die VC-Dimensionstheorie an, um eine untere Grenze für die Größe von SpectralNet abzuleiten.  State-of-the-Art Clustering-Ergebnisse werden sowohl für die MNIST- als auch für die Reuters-Datensätze berichtet.
Meta-Learning-Methoden, insbesondere Model-Agnostic Meta-Learning (Finn et al., 2017) oder MAML, haben große Erfolge bei der schnellen Anpassung an neue Aufgaben erzielt, nachdem sie für ähnliche Aufgaben trainiert wurden.Der Mechanismus hinter ihrem Erfolg ist jedoch kaum verstanden.Wir beginnen diese Arbeit mit einer experimentellen Analyse von MAML und stellen fest, dass tiefe Modelle für den Erfolg entscheidend sind, selbst bei einfachen Aufgaben, bei denen ein lineares Modell für jede einzelne Aufgabe ausreichen würde. Darüber hinaus stellen wir bei Bilderkennungsaufgaben fest, dass die frühen Schichten von MAML-trainierten Modellen aufgabeninvariante Merkmale lernen, während spätere Schichten für die Anpassung verwendet werden, was ein weiterer Beweis dafür ist, dass diese Modelle mehr Kapazität benötigen, als für ihre individuellen Aufgaben unbedingt erforderlich ist. Im Anschluss an unsere Erkenntnisse schlagen wir eine Methode vor, die eine bessere Nutzung der Modellkapazität zum Zeitpunkt der Inferenz ermöglicht, indem der Anpassungsaspekt des Meta-Lernens in Parameter aufgeteilt wird, die nur für die Anpassung verwendet werden, aber nicht Teil des Vorwärtsmodells sind.Wir stellen fest, dass unser Ansatz effektiveres Meta-Lernen in kleineren Modellen ermöglicht, die für die einzelnen Aufgaben geeignet sind.
Convolutional Neural Networks treiben den Fortschritt in der 2D- und 3D-Klassifikation von Bildern und Objekten kontinuierlich voran.Die konsequente Anwendung dieses Algorithmus erfordert eine ständige Evaluierung und Aktualisierung der grundlegenden Konzepte, um den Fortschritt aufrechtzuerhalten.Techniken zur Regularisierung von Netzwerken konzentrieren sich typischerweise auf Operationen auf Faltungsschichten, während Pooling-Schichten ohne geeignete Optionen bleiben.Wir stellen Wavelet Pooling als eine weitere Alternative zum traditionellen Nachbarschaftspooling vor. Diese Methode dekomponiert Merkmale in eine zweite Ebene und verwirft die Subbänder der ersten Ebene, um die Merkmalsdimensionen zu reduzieren. Diese Methode adressiert das Overfitting-Problem, das beim Max-Pooling auftritt, und reduziert die Merkmale auf eine strukturell kompaktere Weise als das Pooling über Nachbarschaftsregionen.Experimentelle Ergebnisse auf vier Benchmark-Klassifizierungsdatensätzen zeigen, dass die von uns vorgeschlagene Methode Methoden wie Max-, Mean-, Mixed- und Stochastic-Pooling übertrifft oder vergleichbar ist.
Dynamische Ridesharing-Dienste (DRS) spielen eine wichtige Rolle bei der Verbesserung der Effizienz des städtischen Nahverkehrs. Die Zufriedenheit der Nutzer bei dynamischen Ridesharing-Diensten wird durch mehrere Faktoren wie Fahrzeit, Kosten und soziale Kompatibilität mit Mitfahrern bestimmt. Bestehende DRS optimieren den Gewinn, indem sie den operativen Wert für die Anbieter maximieren oder die Reisezeit für die Nutzer minimieren, aber sie vernachlässigen die sozialen Erfahrungen der Fahrer, die den Gesamtwert des Dienstes für die Nutzer erheblich beeinflussen.Wir schlagen DROPS vor, ein dynamisches Ridesharing-System, das die sozialen Präferenzen der Fahrer in den Matching-Prozess einbezieht, um die Qualität der gebildeten Fahrten zu verbessern. Die Planung von Fahrten für Benutzer ist eine Mehrzieloptimierung, die darauf abzielt, den Betriebswert für den Dienstanbieter zu maximieren und gleichzeitig den Wert der Fahrt für die Benutzer zu maximieren.Der Benutzerwert wird auf der Grundlage der Kompatibilität zwischen Mitfahrern und der Fahrzeit geschätzt.Wir stellen dann einen Echtzeit-Matching-Algorithmus für die Fahrtenbildung vor.Schließlich bewerten wir unseren Ansatz empirisch mit realen Taxifahrten und einem Bevölkerungsmodell, das soziale Präferenzen auf der Grundlage von Benutzerumfragen enthält.Die Ergebnisse zeigen eine Verbesserung der sozialen Kompatibilität der Fahrer, ohne die Fahrzeugkilometer für den Dienstanbieter und die Fahrzeit für die Benutzer wesentlich zu beeinflussen.
Deep Neural Networks (DNNs) haben sich in letzter Zeit als anfällig gegenüber gegnerischen Beispielen erwiesen, d.h. sorgfältig erstellten Instanzen, die DNNs dazu verleiten können, bei der Vorhersage Fehler zu machen.Um solche Angriffe besser zu verstehen, ist eine Charakterisierung der Eigenschaften von Regionen (den sogenannten "gegnerischen Unterräumen") erforderlich, in denen gegnerische Beispiele liegen. Wir gehen diese Herausforderung an, indem wir die dimensionalen Eigenschaften der gegnerischen Regionen mit Hilfe der Local Intrinsic Dimensionality (LID) charakterisieren: LID bewertet die raumfüllende Fähigkeit der Region, die ein Referenzbeispiel umgibt, basierend auf der Distanzverteilung des Beispiels zu seinen Nachbarn. Wir erläutern zunächst, wie die LID-Charakteristik gegnerischer Regionen durch Störungen beeinflusst werden kann, und zeigen dann empirisch, dass die LID-Charakteristik die Unterscheidung von gegnerischen Beispielen, die mit modernsten Angriffen erzeugt wurden, erleichtern kann. Als Proof-of-Concept zeigen wir, dass eine mögliche Anwendung von LID die Unterscheidung von gegnerischen Beispielen ist, und die vorläufigen Ergebnisse zeigen, dass es mehrere State-of-the-Art-Erkennungsmaßnahmen mit großen Margen für fünf Angriffsstrategien, die in diesem Papier über drei Benchmark-Datensätze betrachtet werden, übertreffen kann.Unsere Analyse der LID-Charakteristik für gegnerische Regionen motiviert nicht nur neue Richtungen der effektiven gegnerischen Verteidigung, sondern eröffnet auch mehr Herausforderungen für die Entwicklung neuer Angriffe, um die Schwachstellen von DNNs besser zu verstehen.
In diesem Papier entwerfen und analysieren wir einen neuen stochastischen Optimierungsalgorithmus nullter Ordnung (ZO), ZO-signSGD, der die doppelten Vorteile von gradientenfreien Operationen und signSGD genießt. Letzterer benötigt nur die Vorzeicheninformation von Gradientenschätzungen, ist aber in der Lage, eine vergleichbare oder sogar bessere Konvergenzgeschwindigkeit als SGD-Algorithmen zu erreichen. Unsere Studie zeigt, dass ZO signSGD $\sqrt{d}$ mal mehr Iterationen benötigt als signSGD, was zu einer Konvergenzrate von $O(\sqrt{d}/\sqrt{T})$ unter milden Bedingungen führt, wobei $d$ die Anzahl der Optimierungsvariablen und $T$ die Anzahl der Iterationen ist. Darüber hinaus analysieren wir die Auswirkungen verschiedener Arten von Gradientenschätzern auf die Konvergenz von ZO-signSGD und schlagen zwei Varianten von ZO-signSGD vor, die mindestens $O(\sqrt{d}/\sqrt{T})$ Konvergenzrate erreichen.Auf der Anwendungsseite untersuchen wir die Verbindung zwischen ZO-signSGD und Black-Box-Angriffen im robusten Deep Learning.  Unsere empirischen Auswertungen auf den Bildklassifizierungsdatensätzen MNIST und CIFAR-10 zeigen die überlegene Leistung von ZO-signSGD bei der Generierung von adversen Beispielen aus neuronalen Black-Box-Netzen.
Die nicht-stationäre Charakteristik der Solarenergie macht traditionelle Punktvorhersagemethoden aufgrund großer Vorhersagefehler weniger nützlich, was zu erhöhten Unsicherheiten im Netzbetrieb führt, was sich negativ auf die Zuverlässigkeit auswirkt und zu erhöhten Betriebskosten führt.Dieses Forschungspapier schlägt eine einheitliche Architektur für die Solarvorhersage mit mehreren Zeithorizonten für kurz- und langfristige Vorhersagen unter Verwendung rekurrenter neuronaler Netze (RNN) vor. Die Ergebnisse zeigen, dass die vorgeschlagene Methode, die auf der einheitlichen Architektur basiert, für die Solarvorhersage mit mehreren Horizonten effektiv ist und einen geringeren Vorhersagefehler (root-mean-squared) im Vergleich zu den vorhergehenden leistungsstärksten Methoden erzielt, die ein Modell für jeden Zeithorizont verwenden.Die vorgeschlagene Methode ermöglicht Prognosen mit mehreren Horizonten mit Echtzeit-Eingaben, die ein hohes Potenzial für praktische Anwendungen im sich entwickelnden intelligenten Stromnetz haben.
Das ResNet und die Batch-Normalisierung (BN) erreichten eine hohe Leistung, auch wenn nur wenige beschriftete Daten verfügbar sind, aber die Gründe für die hohe Leistung sind unklar. um die Gründe zu klären, analysierten wir die Wirkung der Skip-Verbindung in ResNet und BN auf die Fähigkeit der Datentrennung, die eine wichtige Fähigkeit für das Klassifikationsproblem ist. Unsere Ergebnisse zeigen, dass im mehrschichtigen Perzeptron mit zufällig initialisierten Gewichten der Winkel zwischen zwei Eingangsvektoren in exponentieller Reihenfolge seiner Tiefe gegen Null konvergiert, dass die Skip-Verbindung diese exponentielle Abnahme zu einer sub-exponentiellen Abnahme macht, und dass das BN diese sub-exponentielle Abnahme zu einer reziproken Abnahme entspannt. Darüber hinaus zeigt unsere Analyse, dass die Beibehaltung des Winkels bei der Initialisierung die trainierten neuronalen Netze dazu ermutigt, Punkte aus verschiedenen Klassen zu trennen, was bedeutet, dass die Skip-Verbindung und die BN die Fähigkeit zur Datentrennung verbessern und eine hohe Leistung erzielen, selbst wenn nur wenige beschriftete Daten verfügbar sind.
Obwohl GAN hat dazu geführt, dass erhebliche Verbesserungen in der Daten-Darstellungen, es hat immer noch mehrere Probleme wie instabile Ausbildung, versteckte Vielzahl von Daten, und riesige rechnerische overhead.GAN neigt dazu, die Daten einfach ohne Informationen über die vielfältigen der Daten, die von der Kontrolle gewünschten Features zu generieren behindert produzieren.Moreover, die meisten GAN's haben eine große Größe der vielfältigen, was zu schlechten Skalierbarkeit. Im Gegensatz zu den konventionellen GAN-Modellen mit versteckter Verteilung des latenten Raums, definieren wir die Verteilungen explizit im Voraus, die trainiert werden, um die Daten auf der Grundlage der entsprechenden Merkmale durch Eingabe der latenten Variablen, die der Verteilung folgen, zu erzeugen. Wir beweisen, dass eine VAE für erstere geeignet ist, und modifizieren eine Verlustfunktion der VAE, um die Daten in den vordefinierten latenten Raum abzubilden, um die rekonstruierten Daten so nah wie möglich an den Eingabedaten entsprechend ihrer Eigenschaften zu lokalisieren, und fügen die KL-Divergenz zur Verlustfunktion von LSC-GAN hinzu, um diesen Prozess zu berücksichtigen. Der Decoder der VAE, der die Daten mit den entsprechenden Merkmalen aus dem vordefinierten latenten Raum generiert, wird als Generator des LSC-GAN verwendet.Mehrere Experimente auf dem CelebA-Datensatz werden durchgeführt, um die Nützlichkeit der vorgeschlagenen Methode zu überprüfen, um die gewünschten Daten stabil und effizient zu generieren, wobei eine hohe Kompressionsrate erreicht wird, die etwa 24 Pixel an Informationen in jeder Dimension des latenten Raums enthalten kann.Außerdem lernt unser Modell die Umkehrung von Merkmalen, wie z.B. nicht lachen (eher Stirnrunzeln), nur mit Daten des gewöhnlichen und lächelnden Gesichtsausdrucks.
Angesichts der ständig steigenden Nachfrage und der daraus resultierenden geringeren Qualität der Dienstleistungen hat sich der Schwerpunkt auf die Entlastung der Netze verlagert, um einen effizienteren Fluss in Systemen wie dem Verkehr, den Versorgungsketten und den Stromnetzen zu ermöglichen.Ein Schritt in diese Richtung besteht darin, das traditionelle, auf Heuristiken basierende Training von Systemen zu überdenken, da dieser Ansatz nicht in der Lage ist, die damit verbundene Dynamik zu modellieren. Zwar kann man Multi-Agent Reinforcement Learning (MARL) auf solche Probleme anwenden, indem man jeden Knoten im Netzwerk als einen Agenten betrachtet, aber die meisten MARL-basierten Modelle gehen davon aus, dass die Agenten unabhängig sind. In vielen realen Aufgaben müssen sich die Agenten als Gruppe und nicht als eine Ansammlung von Individuen verhalten. Wir formulieren das Problem in einer allgemeinen Netzwerkumgebung und demonstrieren die Nützlichkeit von Kommunikation in Netzwerken mit Hilfe einer Fallstudie zu Verkehrssystemen. Darüber hinaus untersuchen wir das emergente Kommunikationsprotokoll und zeigen die Bildung von verschiedenen Gemeinschaften mit geerdetem Vokabular.Nach unserem Wissen ist dies die einzige Arbeit, die emergente Sprache in einer vernetzten MARL-Umgebung untersucht.
Wir stellen den Convolutional Conditional Neural Process (ConvCNP) vor, ein neues Mitglied der Familie der neuronalen Prozesse, das die Übersetzungsäquivarianz in den Daten modelliert.Übersetzungsäquivarianz ist eine wichtige induktive Verzerrung für viele Lernprobleme, einschließlich der Modellierung von Zeitreihen, räumlichen Daten und Bildern.Das Modell bettet Datensätze in einen unendlich-dimensionalen Funktionsraum ein, im Gegensatz zu endlich-dimensionalen Vektorräumen. Um diesen Begriff zu formalisieren, erweitern wir die Theorie der neuronalen Repräsentationen von Mengen um funktionale Repräsentationen und zeigen, dass jede übersetzungsäquivariante Einbettung durch ein tiefes Faltungsset repräsentiert werden kann. Wir evaluieren ConvCNPs in verschiedenen Umgebungen und zeigen, dass sie im Vergleich zu bestehenden NPs die beste Leistung erzielen.
Klassische Modelle beschreiben den primären visuellen Kortex (V1) als eine Filterbank von orientierungsselektiven linear-nichtlinearen (LN) oder Energiemodellen, aber diese Modelle versagen bei der genauen Vorhersage von neuronalen Reaktionen auf natürliche Reize.Neuere Arbeiten zeigen, dass Faltungsneuronale Netze (CNNs) trainiert werden können, um die Aktivität von V1 genauer vorherzusagen, aber es bleibt unklar, welche Merkmale von V1-Neuronen über die Orientierungsselektivität und Phaseninvarianz hinaus extrahiert werden.Hier arbeiten wir daran, die Berechnungen von V1 systematisch zu untersuchen, indem wir Neuronen in Gruppen kategorisieren, die ähnliche Berechnungen durchführen. Wir stellen einen Rahmen für die Identifizierung gemeinsamer Merkmale unabhängig von der Orientierungsselektivität einzelner Neuronen vor, indem wir ein rotations-äquivariantes neuronales Faltungsnetzwerk verwenden, das automatisch jedes Merkmal bei mehreren verschiedenen Orientierungen extrahiert.Wir passen dieses rotations-äquivariante CNN an die Reaktionen einer Population von 6000 Neuronen auf natürliche Bilder an, die im primären visuellen Kortex von Mäusen mit Hilfe der Zwei-Photonen-Bildgebung aufgezeichnet wurden. Wir zeigen, dass unser rotations-äquivariantes Netzwerk ein reguläres CNN mit der gleichen Anzahl von Merkmalskarten übertrifft und eine Reihe gemeinsamer Merkmale aufzeigt, die von vielen V1-Neuronen geteilt und spärlich gepoolt werden, um die neuronale Aktivität vorherzusagen.Unsere Ergebnisse sind ein erster Schritt hin zu einem leistungsstarken neuen Werkzeug zur Untersuchung der nichtlinearen funktionalen Organisation des visuellen Kortex.
In klassischen Arbeiten hat Zellner (1988, 2002) gezeigt, dass die Bayes'sche Inferenz als Lösung für ein informationstheoretisches Funktional abgeleitet werden kann.  Im Folgenden leiten wir eine verallgemeinerte Form dieses Funktionals als Variationsuntergrenze eines prädiktiven Informationsengpassziels ab.  Dieses verallgemeinerte Funktional umfasst die meisten modernen Inferenzverfahren und schlägt neue Verfahren vor.
In vielen Anwendungen ist es wünschenswert, nur die relevanten Informationen aus komplexen Eingabedaten zu extrahieren, was eine Entscheidung darüber beinhaltet, welche Eingabemerkmale relevant sind. Die Informationsengpass-Methode formalisiert dies als informationstheoretisches Optimierungsproblem, indem ein optimaler Kompromiss zwischen Kompression (Weglassen irrelevanter Eingabeinformationen) und Vorhersage des Ziels beibehalten wird. Dies ist typischerweise dann der Fall, wenn wir eine Standardkonditionierungseingabe haben, wie z.B. eine Zustandsbeobachtung, und eine ``privilegierte'' Eingabe, die dem Ziel einer Aufgabe, der Ausgabe eines kostspieligen Planungsalgorithmus oder der Kommunikation mit einem anderen Agenten entsprechen könnte.In solchen Fällen können wir es vorziehen, die privilegierte Eingabe zu komprimieren, entweder um eine bessere Generalisierung zu erreichen (z.B. in Bezug auf Ziele) oder um die Kommunikation mit einem anderen Agenten zu minimieren, Praktische Implementierungen des Informationsengpasses, die auf der Variationsinferenz basieren, erfordern den Zugriff auf die privilegierte Eingabe, um die Engpassvariable zu berechnen, so dass sie zwar eine Komprimierung durchführen, diese Komprimierungsoperation selbst aber einen uneingeschränkten, verlustfreien Zugriff benötigt.In dieser Arbeit schlagen wir den variationalen Bandbreitenengpass vor, der für jedes Beispiel über den geschätzten Wert der privilegierten Information entscheidet, bevor er sie sieht, d.h., Wir formulieren eine vertretbare Annäherung an diesen Rahmen und zeigen in einer Reihe von Experimenten zum Verstärkungslernen, dass sie die Generalisierung verbessern und den Zugang zu rechenintensiven Informationen reduzieren kann.
Atemübungen sind ein zugänglicher Weg, um Stress und viele psychische Krankheitssymptome zu bewältigen.Traditionell wurden Atemübungen unter persönlicher Anleitung oder mit Audio-Aufnahmen erlernt.Die Verlagerung auf mobile Geräte hat zu einer neuen Art des Lernens und der Durchführung von Atemübungen geführt, wie man an der Zunahme mehrerer mobiler Anwendungen mit verschiedenen Atemdarstellungen sehen kann.Allerdings wurden bisher nur wenige Arbeiten durchgeführt, um die Effektivität dieser visuellen Darstellungen bei der Unterstützung des Atemrhythmus, gemessen durch Synchronisation, zu untersuchen. Durch kontrollierte Laborstudien und Interviews haben wir zwei Darstellungen identifiziert, die eindeutige Vorteile gegenüber den anderen haben. Darüber hinaus haben wir festgestellt, dass nicht alle Nutzer die auditive Anleitung bevorzugen. Wir identifizieren potenzielle Probleme mit der Benutzerfreundlichkeit der Darstellungen und schlagen Design-Richtlinien für die zukünftige Entwicklung von App-gestützten Atemübungen vor.
Die derzeitige Arbeit an der neuronalen Codesynthese besteht darin, dass immer ausgefeiltere Architekturen auf stark vereinfachten domänenspezifischen Sprachen trainiert werden, wobei für das Training eine einheitliche Stichprobe aus dem Programmraum dieser Sprachen verwendet wird. Im Vergleich dazu ist der Programmraum für eine C-ähnliche Sprache riesig und in Bezug auf "nützliche" Funktionalitäten extrem dünn besiedelt; dies erfordert einen weitaus intelligenteren Ansatz für die Korpusgenerierung für ein effektives Training. Wir zeigen, dass die Verwendung eines diskriminatorbasierten Trainingskorpus-Generators, der nur mit unmarkierten Problemspezifikationen im klassischen Programming-by-Example-Format trainiert wird, die Leistung des Netzwerks im Vergleich zu den derzeitigen einheitlichen Stichprobenverfahren erheblich verbessert.
Die meisten früheren Arbeiten haben semantisch invariante Textstörungen untersucht, die dazu führen, dass sich die Vorhersage eines Modells ändert, wenn sie sich nicht ändern sollte. In dieser Arbeit konzentrieren wir uns auf das komplementäre Problem: übermäßige Unterempfindlichkeit der Vorhersage, wenn der Eingabetext sinnvoll verändert wird und sich die Vorhersage des Modells nicht ändert, wenn sie sich ändern sollte. Wir formulieren einen verrauschten gegnerischen Angriff, der unter semantischen Variationen von Verständnisfragen sucht, für die ein Modell immer noch fälschlicherweise die gleiche Antwort wie die ursprüngliche Frage liefert - und das mit einer noch höheren Wahrscheinlichkeit.Wir zeigen, dass - obwohl sie unbeantwortbare Fragen enthalten - SQuAD2. 0 und NewsQA trotz unbeantwortbarer Fragen anfällig für diesen Angriff sind und einen beträchtlichen Anteil an Fehlern bei gegnerisch generierten Fragen begehen, was darauf hindeutet, dass aktuelle Modelle - selbst wenn sie die Antwort korrekt vorhersagen können - auf falschen Oberflächenmustern beruhen und nicht unbedingt alle in einer gegebenen Verständnisfrage enthaltenen Informationen kennen. Schließlich zeigen wir, dass adversarisch robuste Modelle in einer verzerrten Datenumgebung mit einer ungleichen Verteilung von Training und Auswertung besser abschneiden; sie neigen weniger dazu, sich zu sehr auf Vorhersagehinweise zu verlassen, die nur in der Trainingsmenge vorhanden sind, und übertreffen ein herkömmliches Modell in der verzerrten Datenumgebung um bis zu 11% F1.
Diese Repräsentation ist sprachunabhängig und erfordert kein Vortraining und erzeugt eine Kodierung ohne Informationsverlust. Sie bietet eine adäquate Beschreibung der Morphologie eines Textes, da sie in der Lage ist, Präfixe, Deklinationen und Flexionen mit ähnlichen Vektoren darzustellen und sogar unbekannte Wörter im Trainingsdatensatz zu repräsentieren. Als Teil dieses Papiers zeigen wir, dass diese Technik besonders effektiv ist, wenn sie mit Faltungsneuronalen Netzen (CNNs) für die Textklassifikation auf Zeichenebene gekoppelt ist.Wir wenden zwei Varianten von CNNs an, die damit gekoppelt sind.Experimentelle Ergebnisse zeigen, dass sie die Anzahl der zu optimierenden Parameter drastisch reduziert, was zu wettbewerbsfähigen Klassifikationsgenauigkeitswerten in nur einem Bruchteil der Zeit führt, die für die One-Hot-Codierung von Repräsentationen aufgewendet wird, und somit das Training in Standard-Hardware ermöglicht.
Modelle zur Vorhersage von Sequenzen können anhand von Beispielsequenzen mit verschiedenen Trainingsalgorithmen erlernt werden: Maximum-Likelihood-Lernen ist einfach und effizient, kann aber zum Testzeitpunkt zu Fehlern führen. Verstärkungslernen, wie z. B. Policy-Gradient, geht auf dieses Problem ein, kann aber eine sehr schlechte Explorationseffizienz aufweisen. Eine Vielzahl anderer Algorithmen, wie z. B. Data Noising, RAML und Softmax-Policy-Gradient, wurden ebenfalls aus verschiedenen Perspektiven entwickelt. In diesem Beitrag stellen wir einen Formalismus der entropie-regulierten Policy-Optimierung vor und zeigen, dass die scheinbar unterschiedlichen Algorithmen, einschließlich MLE, als spezielle Instanzen der Formulierung umformuliert werden können. Die vereinheitlichende Interpretation ermöglicht es uns, die Algorithmen systematisch nebeneinander zu vergleichen und neue Einsichten in die Kompromisse des Algorithmusdesigns zu gewinnen. Die neue Perspektive führt auch zu einem verbesserten Ansatz, der dynamisch zwischen der Familie der Algorithmen interpoliert und das Modell in einer geplanten Weise lernt.
Eine genaue Vorhersage der Kundennachfrage von jedem Ausgangsort zu einem Ziel angesichts einer Reihe früherer Schnappschüsse hilft Mitfahrzentralen, ihre Marktmechanismen besser zu verstehen. Die meisten bestehenden Vorhersagemethoden ignorieren jedoch die Netzwerkstruktur der OD-Flow-Daten und nutzen nicht die topologischen Abhängigkeiten zwischen verwandten OD-Paaren. In diesem Papier schlagen wir ein latentes räumlich-zeitliches Herkunfts-Ziel-Modell (LSTOD) vor, mit einem neuartigen Faltungsneuronalen Netzwerk (CNN)-Filter, um die räumlichen Merkmale von OD-Paaren aus einer Graphenperspektive und einer Aufmerksamkeitsstruktur zu erlernen, um ihre langfristige Periodizität zu erfassen.Experimente an einem realen Kundenanfragedatensatz mit verfügbaren OD-Informationen von einer Ride-Sharing-Plattform zeigen den Vorteil von LSTOD bei der Erzielung von mindestens 6,5% Verbesserung der Vorhersagegenauigkeit gegenüber dem zweitbesten Modell.
Adversariales Training hat sich als eine der effektivsten Methoden für das Training von robusten Modellen erwiesen, um sich gegen gegnerische Beispiele zu verteidigen.Allerdings fehlt es adversarial trainierten Modellen oft an adversarial robuster Generalisierung auf ungesehenen Testdaten.Neuere Arbeiten zeigen, dass adversarial trainierte Modelle eher auf globale Strukturmerkmale ausgerichtet sind. Um die robusten lokalen Merkmale zu erlernen, entwickeln wir eine Random Block Shuffle (RBS)-Transformation, um die globalen Strukturmerkmale auf normalen adversen Beispielen aufzubrechen. Wir schlagen einen neuen Ansatz vor, der sich Robuste Lokale Merkmale für adversariales Training (RLFAT) nennt und zuerst die robusten lokalen Merkmale durch adversariales Training auf den RBS-transformierten adversen Beispielen erlernt und dann die robusten lokalen Merkmale in das Training normaler adverser Beispiele überträgt.Um die Allgemeingültigkeit unseres Arguments zu demonstrieren, implementieren wir RLFAT in den derzeit modernsten adversarialen Trainingssystemen. Ausführliche Experimente mit STL-10, CIFAR-10 und CIFAR-100 zeigen, dass RLFAT sowohl die robuste Verallgemeinerung als auch die Standardverallgemeinerung des gegnerischen Trainings signifikant verbessert und dass unsere Modelle mehr lokale Merkmale des Objekts auf den Bildern erfassen, was der menschlichen Wahrnehmung besser entspricht.
Die Verifikation von Planungsdomänenmodellen ist von entscheidender Bedeutung, um die Sicherheit, Integrität und Korrektheit von planungsbasierten automatisierten Systemen zu gewährleisten.Diese Aufgabe wird in der Regel mithilfe von Model-Checking-Techniken durchgeführt.  Die direkte Anwendung von Model-Checkern zur Verifikation von Planungsdomänenmodellen kann jedoch zu falsch-positiven Ergebnissen führen, d.h. zu Gegenbeispielen, die für einen soliden Planer unerreichbar sind, wenn er die zu verifizierende Domäne während einer Planungsaufgabe verwendet.In diesem Beitrag diskutieren wir die Nachteile einer uneingeschränkten Verifikation von Planungsdomänenmodellen und schlagen eine ausfallsichere Praxis für den Entwurf von Planungsdomänenmodellen vor, die die Sicherheit der erzeugten Pläne im Falle von unentdeckten Fehlern in Domänenmodellen garantieren kann.  Darüber hinaus demonstrieren wir, wie Modellprüfprogramme und Techniken zur Planung von Zustandsbeschränkungen zur Verifizierung von Planungsdomänenmodellen eingesetzt werden sollten, damit keine unerreichbaren Gegenbeispiele zurückgegeben werden.
Um das menschliche Zeichnen zu imitieren, sind Interaktionen zwischen der Umgebung und dem Agenten erforderlich, um Versuche zu ermöglichen. Allerdings ist die Umgebung in der Regel nicht differenzierbar, was zu langsamer Konvergenz und massiven Berechnungen führt.In diesem Papier versuchen wir, die diskrete Natur der Software-Umgebung mit einer intermediären, differenzierbaren Simulation zu adressieren. Wir stellen StrokeNet vor, ein neuartiges Modell, bei dem der Agent auf einer gut ausgearbeiteten neuronalen Annäherung an die Malumgebung trainiert wird. Mit diesem Ansatz war unser Agent in der Lage, das Schreiben von Zeichen wie MNIST-Ziffern schneller zu erlernen als Reinforcement-Learning-Ansätze auf unüberwachte Weise.
Wir zeigen, wie maschinelles Lernen Experimente in der Quantenphysik modellieren kann.Quantenverschränkung ist ein Eckpfeiler für aufkommende Quantentechnologien wie Quantencomputer und Quantenkryptographie.Von besonderem Interesse sind komplexe Quantenzustände mit mehr als zwei Teilchen und einer großen Anzahl verschränkter Quantenniveaus.Bei einem solchen hochdimensionalen Quantenzustand mit mehreren Teilchen ist es in der Regel unmöglich, eine Versuchsanordnung zu rekonstruieren, die ihn erzeugt.Um nach interessanten Experimenten zu suchen, muss man daher Millionen von Versuchsanordnungen auf einem Computer zufällig erzeugen und die jeweiligen Ausgangszustände berechnen. Wir zeigen, dass ein neuronales Netzwerk mit langem Kurzzeitgedächtnis (LSTM) erfolgreich lernen kann, Quantenexperimente zu modellieren, indem es die Eigenschaften der Ausgangszustände für gegebene Versuchsaufbauten korrekt vorhersagt, ohne dass die Zustände selbst berechnet werden müssen.Dieser Ansatz ermöglicht nicht nur eine schnellere Suche, sondern ist auch ein wesentlicher Schritt in Richtung eines automatisierten Entwurfs von hochdimensionalen Mehrteilchen-Quantenexperimenten unter Verwendung generativer maschineller Lernmodelle.
In diesem Papier schlagen wir ein nichtlineares, unbeaufsichtigtes metrisches Lernverfahren vor, um die Leistung von Clustering-Algorithmen zu steigern: Nichtlineares metrisches Distanzlernen und Manifold Embedding werden integriert und gleichzeitig durchgeführt, um die natürlichen Trennungen zwischen den Datenproben zu erhöhen Die metrische Lernkomponente wird durch Merkmalsraumtransformationen implementiert, die durch ein nichtlineares deformierbares Modell namens Coherent Point Drifting (CPD) reguliert werden. Angetrieben durch CPD können Datenpunkte ein höheres Maß an linearer Trennbarkeit erreichen, die anschließend von der Komponente zur Einbettung von Merkmalen (Manifold Embedding) aufgegriffen wird, um gut trennbare Probenprojektionen für das Clustering zu erzeugen.Experimentelle Ergebnisse auf synthetischen und Benchmark-Datensätzen zeigen die Effektivität des von uns vorgeschlagenen Ansatzes im Vergleich zu State-of-the-Art-Lösungen im Bereich des unüberwachten metrischen Lernens.
Da maschinelles Lernen allgegenwärtig wird, müssen die eingesetzten Systeme so genau wie möglich sein. Infolgedessen haben Anbieter von maschinellem Lernen einen steigenden Bedarf an nützlichen, zusätzlichen Trainingsdaten, die dem Training zugute kommen, ohne alle Details über das trainierte Programm preiszugeben.Gleichzeitig möchten die Datenbesitzer ihre Daten für ihren Wert tauschen, ohne zuerst die Daten selbst weggeben zu müssen, bevor sie eine Entschädigung erhalten.Es ist schwierig für Datenanbieter und Modellanbieter, sich auf einen fairen Preis zu einigen, ohne zuerst die Daten oder das trainierte Modell der anderen Seite preiszugeben. Derzeit haben Daten- und Modelleigentümer kein faires Preissystem, das die Notwendigkeit beseitigt, einer dritten Partei zu vertrauen und das Modell auf den Daten zu trainieren, was1) viel Zeit in Anspruch nimmt,2) nicht garantiert, dass nützliche Daten wertvoll bezahlt werden und unbrauchbare Daten nicht, ohne der dritten Partei mit dem Modell und den Daten zu vertrauen. Bestehende Verbesserungen zur Sicherung der Transaktion konzentrieren sich stark auf die Verschlüsselung oder Annäherung der Daten, wie z. B. Training auf verschlüsselten Daten und Varianten des föderierten Lernens. So leistungsfähig diese Methoden auch zu sein scheinen, wir zeigen, dass sie in unserem Anwendungsfall mit realen Annahmen zur Wahrung der Privatsphäre der Dateneigentümer bei Black-Box-Modellen unpraktisch sind. In diesem Beitrag wird eine neuartige Methode zur fairen Preisbildung unter Verwendung von Datenmodell-Bewirkungstechniken wie Einflussfunktionen, Modellextraktion und Modellkompressionsmethoden vorgeschlagen, die sichere Datentransaktionen ermöglicht. Wir zeigen erfolgreich, dass man den Wert der Daten annähern kann, ohne die Daten durch das Modell laufen zu lassen; d.h., wenn sich die Daten als redundant herausstellen, ist die Preisbildung minimal, und wenn die Daten zu einer angemessenen Verbesserung führen, wird ihr Wert richtig eingeschätzt, ohne starke Annahmen über die Natur des Modells zu machen.Zukünftige Arbeit wird sich darauf konzentrieren, ein System mit stärkerer Transaktionssicherheit gegen gegnerische Angriffe zu etablieren, die der anderen Partei Details über das Modell oder die Daten offenbaren.
Wir stellen Line-Storm vor, ein interaktives Computersystem für kreative Leistungen, das wir im Zusammenhang mit dem Schreiben auf Papier unter Verwendung von Line-Storm untersucht haben.Wir haben Selbstauskunftsfragebögen als Teil der Forschung mit menschlichen Teilnehmern verwendet, um Line-Storm zu evaluieren.Line-Storm bestand aus einem Schreibstift und einem Schreibblock, die mit Elektronik erweitert wurden.Der Schreibblock war mit einem Kontaktmikrofon verbunden, und der Schreibstift war mit einem kleinen Mikrocontroller-Board und Peripheriegeräten ausgestattet.Die Signale dieser elektronischen Erweiterungen wurden in die Audiosyntheseumgebung Max/MSP eingespeist, um eine interaktive Klanglandschaft zu erzeugen. Nach einer statistischen Analyse in SPSS konnten wir unsere Forschungshypothese, dass die Präsenz und das Engagement durch Line-Storm verbessert wurden, nicht bestätigen. Die Teilnehmer gaben an, dass sie während der experimentellen Bedingung im Durchschnitt nicht präsenter und engagierter waren als während der Kontrollbedingung. Ein statistisch signifikantes Ergebnis unserer Studie ist, dass einige Teilnehmer positiver auf Line-Storm reagierten als andere. Diese "Bewahrer" von Line-Storm waren eine Gruppe, die sich von den anderen Teilnehmern unterschied, die über eine größere Präsenz und ein größeres Engagement berichteten und mehr Wörter mit Line-Storm als in der Kontrollbedingung schrieben. Wir diskutieren die Ergebnisse unserer Forschung und stellen Line-Storm in einen künstlerisch-technologischen Kontext, wobei wir uns bei der Betrachtung des Wesens von Line-Storm auf Schriften von Martin Heidegger stützen. Zukünftige Arbeiten umfassen die Modifizierung interaktiver Komponenten, die Verbesserung der Ästhetik und die Verwendung von miniaturisierter Elektronik, das Experimentieren mit einer Zeichenaufgabe anstelle einer Schreibaufgabe und die Zusammenarbeit mit einem Komponisten elektronischer Musik, um eine interessantere, immersivere und fesselndere interaktive Klanglandschaft zum Schreiben oder Zeichnen zu schaffen.
In vielen Anwendungen stehen keine parallelen Trainingsdaten zur Verfügung, und die zu übertragenden Quellensätze können beliebige und unbekannte Stile haben.In diesem Beitrag stellen wir ein Encoder-Decoder-Framework für diese Problemstellung vor.Jeder Satz wird in seine latenten Repräsentationen von Inhalt und Stil kodiert.Durch Rekombination des Inhalts mit dem Zielstil können wir einen Satz dekodieren, der im Zielbereich ausgerichtet ist.Um die Kodier- und Dekodierfunktionen angemessen einzuschränken, koppeln wir sie mit zwei Verlustfunktionen. Die erste ist ein Stil-Diskrepanz-Verlust, der erzwingt, dass die Stil-Repräsentation die Stil-Informationen genau kodiert, die durch die Diskrepanz zwischen dem Satz-Stil und dem Ziel-Stil geleitet werden.Die zweite ist ein Zyklus-Konsistenz-Verlust, der sicherstellt, dass der übertragene Satz den Inhalt des ursprünglichen Satzes, der von seinem Stil getrennt ist, beibehält.Wir validieren die Effektivität unseres vorgeschlagenen Modells an zwei Aufgaben: Stimmungsmodifikation von Restaurant-Rezensionen und Dialog-Antwort-Revision mit einem romantischen Stil.
In diesem Papier schlagen wir eine neue Verlustfunktion für die Durchführung der Hauptkomponentenanalyse (PCA) mit linearen Autokodierern (LAEs) vor.die Optimierung der Standard-L2-Verlust führt zu einer Decoder-Matrix, die den Hauptunterraum der Stichprobenkovarianz der Daten aufspannt, aber nicht die exakten Eigenvektoren identifiziert.dieser Nachteil entsteht durch eine Invarianz, die sich in der globalen Karte aufhebt.hier beweisen wir, dass unsere Verlustfunktion dieses Problem beseitigt, d.h. der Decoder konvergiert zu den Eigenvektoren. Wir beweisen, dass unsere Verlustfunktion dieses Problem beseitigt, d. h. der Decoder konvergiert zu den exakten, geordneten, nicht normalisierten Eigenvektoren der Kovarianzmatrix der Stichprobe.für diesen neuen Verlust stellen wir fest, dass alle lokalen Minima globale Optima sind, und zeigen außerdem, dass die Berechnung des neuen Verlusts (und auch seiner Gradienten) die gleiche Komplexität aufweist wie der klassische Verlust.wir berichten über numerische Ergebnisse sowohl bei synthetischen Simulationen als auch bei einem PCA-Experiment mit realen Daten von MNIST (d. h. einer 60.000 x 784 Matrix), (d.h. eine 60.000 x 784 Matrix), die zeigen, dass unser Ansatz praktisch anwendbar ist und die Nachteile früherer LAEs behebt.
Benutzer haben ein enormes Potenzial, bei der Erstellung und Pflege von Wissensdatenbanken (KBs) zu helfen, indem sie Feedback geben, das falsche und fehlende Entitätsattribute und -beziehungen identifiziert. Wenn jedoch neue Daten zur KB hinzugefügt werden, können sich die KB-Entitäten, die durch eine Entitätsauflösung (ER) erstellt werden, ändern, wodurch die beabsichtigten Ziele des Benutzerfeedbacks unbekannt werden - ein Problem, das wir als Identitätsunsicherheit bezeichnen. In dieser Arbeit stellen wir einen Rahmen für die Integration von Benutzer-Feedback in KBs in Anwesenheit von Identitätsunsicherheiten vor. Unser Ansatz basiert darauf, dass Benutzer-Feedback zusammen mit Erwähnungen in ER teilnimmt. Wir schlagen eine spezifische Darstellung von Benutzer-Feedback als Feedback-Erwähnungen vor und stellen einen neuen Online-Algorithmus für die Integration dieser Erwähnungen in eine bestehende KB vor. In Experimenten zeigen wir, dass unser vorgeschlagener Ansatz die Grundlinien in 70% der experimentellen Bedingungen übertrifft.
Algorithmen für maschinelles Lernen sind anfällig für Poisoning-Angriffe: Ein Angreifer kann bösartige Punkte in den Trainingsdatensatz injizieren, um den Lernprozess zu beeinflussen und die Leistung des Algorithmus zu verschlechtern.Optimale Poisoning-Angriffe wurden bereits vorgeschlagen, um Worst-Case-Szenarien zu evaluieren, indem Angriffe als zweistufiges Optimierungsproblem modelliert wurden.Die Lösung dieser Probleme ist rechenintensiv und für einige Modelle, wie z. B. tiefe Netzwerke, nur begrenzt anwendbar.In diesem Papier stellen wir ein neuartiges generatives Modell vor, um systematische Poisoning-Angriffe gegen Machine-Learning-Klassifizierer zu entwickeln, die gegnerische Trainingsbeispiele erzeugen, d. h. Wir schlagen ein generatives adversariales Netz mit drei Komponenten vor: Generator, Diskriminator und Zielklassifikator. Dieser Ansatz ermöglicht es uns, auf natürliche Weise die Erkennbarkeitsbeschränkungen zu modellieren, die bei realistischen Angriffen zu erwarten sind, und die Regionen der zugrundeliegenden Datenverteilung zu identifizieren, die anfälliger für Data Poisoning sein können.Unsere experimentelle Auswertung zeigt die Effektivität unseres Angriffs, um Klassifikatoren für maschinelles Lernen zu kompromittieren, einschließlich tiefer Netzwerke.
Der Expectation-Maximization (EM) Algorithmus ist ein grundlegendes Werkzeug im unüberwachten maschinellen Lernen und wird oft als effizienter Weg zur Lösung von Maximum Likelihood (ML) und Maximum A Posteriori Schätzungsproblemen verwendet, insbesondere für Modelle mit latenten Variablen.Es ist auch der Algorithmus der Wahl, um Mischungsmodelle anzupassen: generative Modelle, die unmarkierte Punkte repräsentieren, die aus $k$ verschiedenen Prozessen stammen, als Stichproben aus $k$ multivariaten Verteilungen.In dieser Arbeit definieren und verwenden wir eine Quantenversion von EM, um ein Gauß-Mischungsmodell anzupassen. Bei Quantenzugriff auf einen Datensatz von $n$ Vektoren der Dimension $d$ hat unser Algorithmus Konvergenz- und Präzisionsgarantien ähnlich dem klassischen Algorithmus, aber die Laufzeit ist nur polylogarithmisch in der Anzahl der Elemente im Trainingssatz und ist polynomial in anderen Parametern - wie der Dimension des Merkmalsraums und der Anzahl der Komponenten in der Mischung. Wir verallgemeinern den Algorithmus weiter, indem wir ein beliebiges Mischungsmodell von Basisverteilungen aus der Exponentialfamilie anpassen, und diskutieren die Leistung des Algorithmus auf Datensätzen, von denen erwartet wird, dass sie von diesen Algorithmen erfolgreich klassifiziert werden können, und argumentieren, dass wir in diesen Fällen starke Garantien für die Laufzeit geben können.
Wir schlagen einen neuen Ansatz vor, der als iterative regularisierte duale Mittelwertbildung (iRDA) bekannt ist, um die Effizienz von Faltungsneuronalen Netzen (CNN) zu verbessern, indem die Redundanz des Modells deutlich reduziert wird, ohne seine Genauigkeit zu verringern.  Die Methode wurde für verschiedene Datensätze getestet und hat sich als wesentlich effizienter erwiesen als die meisten bestehenden Komprimierungstechniken in der Deep-Learning-Literatur.  Für viele populäre Datensätze wie MNIST und CIFAR-10 können mehr als 95 % der Gewichte entfernt werden, ohne dass die Genauigkeit darunter leidet. Insbesondere sind wir in der Lage, ResNet18 mit 95 % Sparsamkeit eine Genauigkeit zu verleihen, die mit der eines viel größeren Modells ResNet50 mit der besten Sparsamkeit von 60 % vergleichbar ist, wie in der Literatur beschrieben.
Das Erlernen der Nachahmung von Expertenverhalten anhand von Demonstrationen kann eine Herausforderung darstellen, insbesondere in Umgebungen mit hochdimensionalen, kontinuierlichen Beobachtungen und unbekannten Dynamiken. Überwachte Lernmethoden, die auf Behavioral Cloning (BC) basieren, leiden unter einer Verteilungsverschiebung: Da der Agent gierig demonstrierte Handlungen nachahmt, kann er sich aufgrund von Fehlerakkumulationen von demonstrierten Zuständen entfernen. Da die wahre Belohnungsfunktion für die Aufgabe nicht bekannt ist, lernen diese Methoden eine Belohnungsfunktion aus den Demonstrationen, oft unter Verwendung komplexer und brüchiger Annäherungstechniken, die ein gegnerisches Training beinhalten. Wir schlagen eine einfache Alternative vor, die immer noch RL verwendet, aber kein Lernen einer Belohnungsfunktion erfordert. Wir erreichen dies, indem wir dem Agenten eine konstante Belohnung von r=+1 für die Übereinstimmung mit der demonstrierten Aktion in einem demonstrierten Zustand und eine konstante Belohnung von r=0 für alle anderen Verhaltensweisen geben. Unsere Methode, die wir Soft Q Imitation Learning (SQIL) nennen, kann mit einer Handvoll kleiner Änderungen an jedem Standard Q-Learning oder Off-Policy Actor-Critic Algorithmus implementiert werden. Theoretisch zeigen wir, dass SQIL als eine regularisierte Variante von BC interpretiert werden kann, die einen Sparsity-Prior verwendet, um Nachahmung mit langem Horizont zu fördern. Empirisch zeigen wir, dass SQIL BC übertrifft und konkurrenzfähige Ergebnisse im Vergleich zu GAIL erzielt, und zwar bei einer Vielzahl von bildbasierten und niedrigdimensionalen Aufgaben in Box2D, Atari und MuJoCo. Dieses Papier ist ein Proof of Concept, das zeigt, wie eine einfache Nachahmungsmethode, die auf RL mit konstanten Belohnungen basiert, genauso effektiv sein kann wie komplexere Methoden, die gelernte Belohnungen verwenden.
Zwei wichtige Ansätze, um dieses Problem anzugehen, sind Clustering und Repräsentationslernen.Es gibt einerseits sehr leistungsfähige Deepclustering-Modelle und andererseits interpretierbare Repräsentationslerntechniken, die sich oft auf latente topologische Strukturen wie selbstorganisierende Karten stützen.Aktuelle Methoden kombinieren diese beiden Ansätze jedoch noch nicht erfolgreich. Wir stellen eine neue tiefe Architektur für probabilistisches Clustering, VarPSOM, und seine Erweiterung auf Zeitseriendaten, VarTPSOM, bestehend aus VarPSOM-Modulen, die durch LSTM-Zellen verbunden sind, vor. Wir zeigen, dass sie im Vergleich zu aktuellen tiefen Clustering-Methoden auf statischen MNIST/Fashion-MNIST-Daten sowie medizinischen Zeitserien eine überlegene Clustering-Leistung erzielen und gleichzeitig eine interpretierbare Repräsentation induzieren.Darüber hinaus sagt VarTPSOM auf medizinischen Zeitserien erfolgreich zukünftige Trajektorien im ursprünglichen Datenraum voraus.
Ein neuronales Netz kann so trainiert werden, dass es mehrere Aufgaben gleichzeitig lösen kann, indem es "Multi-Task-Lernen" verwendet. Dies spart Berechnungen zur Inferenzzeit, da nur ein einziges Netz ausgewertet werden muss. Leider führt dies oft zu einer schlechteren Gesamtleistung, da die Aufgabenziele miteinander konkurrieren, was die Frage aufwirft, welche Aufgaben in einem Netz zusammen gelernt werden sollten und welche nicht. Wir untersuchen systematisch die Kooperation und den Wettbewerb zwischen Aufgaben und schlagen einen Rahmen für die Zuweisung von Aufgaben an mehrere neuronale Netze vor, so dass kooperierende Aufgaben von demselben neuronalen Netz berechnet werden, während konkurrierende Aufgaben von verschiedenen Netzen berechnet werden.
Suchmaschine hat sich zu einer grundlegenden Komponente in verschiedenen Web-und mobile applications.Retrieving relevante Dokumente aus der massiven Datensätze ist eine Herausforderung für eine Suchmaschine System, vor allem, wenn mit ausführlichen oder Schwanz queries.In diesem Papier, wir erforschen einen Vektorraum Suche Rahmen für Dokument Retrieval. Wir haben ein tiefes semantisches Matching-Modell trainiert, so dass jede Anfrage und jedes Dokument als niedrigdimensionale Einbettung kodiert werden kann. Unser Modell wurde auf der Grundlage der BERT-Architektur trainiert und wir haben einen schnellen k-nearest-neighbor-Indexdienst für das Online-Serving eingesetzt.
Semi-Supervised Learning (SSL)-Ansätze sind ein einflussreicher Rahmen für die Verwendung von unmarkierten Daten, wenn im Laufe des Trainings keine ausreichende Menge an markierten Daten zur Verfügung steht.SSL-Methoden, die auf Convolutional Neural Networks (CNNs) basieren, haben in letzter Zeit erfolgreiche Ergebnisse bei Standard-Benchmark-Aufgaben wie der Bildklassifizierung geliefert.In dieser Arbeit betrachten wir die allgemeine Einstellung des SSL-Problems, bei dem die markierten und unmarkierten Daten aus derselben zugrunde liegenden Wahrscheinlichkeitsverteilung stammen. Wir schlagen einen neuen Ansatz vor, der eine Optimal Transport (OT)-Technik verwendet, die als Ähnlichkeitsmetrik zwischen diskreten empirischen Wahrscheinlichkeitsmaßen dient, um Pseudo-Labels für die unmarkierten Daten bereitzustellen, die dann in Verbindung mit den anfänglich markierten Daten verwendet werden können, um das CNN-Modell in einer SSL-Art und Weise zu trainieren.Wir haben unsere vorgeschlagene Methode evaluiert und mit hochmodernen SSL-Algorithmen auf Standarddatensätzen verglichen, um die Überlegenheit und Wirksamkeit unseres SSL-Algorithmus zu demonstrieren.
Die Batch-Normalisierung (Batch-Norm) wird oft verwendet, um das Training in tiefen neuronalen Netzen zu stabilisieren und zu beschleunigen. In vielen Fällen verringert sie tatsächlich die Anzahl der Parameteraktualisierungen, die erforderlich sind, um einen niedrigen Trainingsfehler zu erreichen. Allerdings verringert sie auch die Robustheit gegenüber kleinen Störungen und Rauschen im zweistelligen Prozentbereich, wie wir an fünf Standarddatensätzen zeigen. Darüber hinaus reicht es aus, die Stapelnorm durch einen Gewichtsabfall zu ersetzen, um die Beziehung zwischen der Anfälligkeit des Gegners und der Eingabedimension aufzuheben.
Dieses Papier stellt vorläufige Ideen unserer Arbeit für das auto- matisierte Lernen von hierarchischen Zielnetzen in nichteter- ministischen Domänen vor, die wir derzeit umsetzen.
Tiefe neuronale Netze zeichnen sich durch große Datenmengen aus, haben aber Probleme, wenn die Daten knapp sind oder wenn sie sich schnell an Änderungen in der Aufgabe anpassen müssen.Als Antwort darauf schlagen neuere Arbeiten im Bereich des Meta-Lernens vor, einen Meta-Lerner auf einer Verteilung ähnlicher Aufgaben zu trainieren, in der Hoffnung, dass er sich auf neue, aber verwandte Aufgaben verallgemeinert, indem er eine High-Level-Strategie lernt, die das Wesen des Problems erfasst, das er lösen soll. Wir schlagen eine Klasse von einfachen und generischen Meta-Learning-Architekturen vor, die eine neuartige Kombination von zeitlichen Faltungen und weicher Aufmerksamkeit verwenden; erstere, um Informationen aus der Vergangenheit zu aggregieren, und letztere, um bestimmte Informationen zu lokalisieren.  In der bisher umfangreichsten Reihe von Meta-Learning-Experimenten evaluieren wir den daraus resultierenden Simple Neural AttentIve Learner (oder SNAIL) in mehreren anspruchsvollen Benchmarking-Aufgaben.  Bei allen Aufgaben, sowohl beim überwachten als auch beim verstärkenden Lernen, erreicht SNAIL mit großem Abstand die beste Leistung.
Knowledge Graph Embedding (KGE) hat in den letzten Jahren mehr Aufmerksamkeit auf sich gezogen.die meisten KGE-Modelle lernen aus zeitunbewussten Tripeln.die Einbeziehung von zeitlichen Informationen neben Tripeln würde jedoch die Leistung eines KGE-Modells weiter verbessern. In diesem Zusammenhang schlagen wir LiTSE vor, ein temporales KGE-Modell, das Zeitinformationen in Entitäts-/Relationsrepräsentationen einbezieht, indem es eine lineare Zeitreihendekomposition verwendet.Darüber hinaus berücksichtigen wir die zeitliche Unsicherheit während der Entwicklung von Entitäts-/Relationsrepräsentationen im Laufe der Zeit, indem wir die Repräsentationen temporaler KGEs in den Raum mehrdimensionaler Gaußverteilungen abbilden. Experimente zeigen, dass LiTSE nicht nur den Stand der Technik bei der Vorhersage von Links in temporalen KGs erreicht, sondern auch in der Lage ist, den Zeitpunkt des Auftretens von Fakten mit fehlenden Zeitangaben sowie die Existenz zukünftiger Ereignisse vorherzusagen.
Wir untersuchen die Entstehung von kooperativem Verhalten bei Agenten mit Verstärkungslernen, indem wir eine herausfordernde wettbewerbsfähige Multi-Agenten-Fußballumgebung mit kontinuierlicher simulierter Physik einführen und zeigen, dass dezentralisiertes, populationsbasiertes Training mit Co-Play zu einer Progression im Verhalten der Agenten führen kann: von zufälligem Verhalten über einfaches Balljagen bis hin zu Anzeichen von Kooperation. Insbesondere zeigen wir, dass die automatische Optimierung von einfachen Belohnungen, die an sich nicht zu kooperativem Verhalten führen, zu langfristigem Teamverhalten führen kann. Außerdem wenden wir ein Bewertungsschema an, das auf spieltheoretischen Prinzipien beruht und die Leistung der Agenten ohne vordefinierte Bewertungsaufgaben oder menschliche Referenzwerte bewerten kann.
In den letzten Jahren wurde eine Reihe von neuronalen Ansätzen für die Programmsynthese vorgeschlagen, von denen viele ein Sequenzgenerierungsparadigma ähnlich der neuronalen maschinellen Übersetzung verwenden, bei dem Sequenz-zu-Sequenz-Modelle trainiert werden, um die Wahrscheinlichkeit bekannter Referenzprogramme zu maximieren.Obwohl diese Strategie beeindruckende Ergebnisse erzielt, hat sie zwei wesentliche Einschränkungen: Erstens ignoriert sie Programm-Aliasing, d.h. die Tatsache, dass viele verschiedene Programme eine bestimmte Spezifikation erfüllen können (insbesondere bei unvollständigen Spezifikationen wie einigen wenigen Eingabe-Ausgabe-Beispielen). Durch die Maximierung der Wahrscheinlichkeit eines einzigen Referenzprogramms werden viele semantisch korrekte Programme benachteiligt, was sich negativ auf die Leistung des Synthesizers auswirken kann.Zweitens übersieht diese Strategie die Tatsache, dass Programme eine strenge Syntax haben, die effizient überprüft werden kann.Um die erste Einschränkung zu beheben, führen wir Reinforcementlearning auf einem überwachten Modell mit einem Ziel durch, das explizit die Wahrscheinlichkeit der Erzeugung semantisch korrekter Programme maximiert. Um die zweite Einschränkung zu beheben, führen wir ein Trainingsverfahren ein, das direkt die Wahrscheinlichkeit maximiert, syntaktisch korrekte Programme zu generieren, die die Spezifikation erfüllen.Wir zeigen, dass unsere Beiträge zu einer verbesserten Genauigkeit der Modelle führen, insbesondere in Fällen, in denen die Trainingsdaten begrenzt sind.
Zeitreihenprognosen spielen eine entscheidende Rolle im Marketing, im Finanzwesen und in vielen anderen quantitativen Bereichen. Eine große Anzahl von Methoden wurde zu diesem Thema entwickelt, darunter ARIMA, Holt-Winters usw. Ihre Leistung wird jedoch leicht durch das Vorhandensein von Veränderungspunkten und Anomaliepunkten untergraben, zwei Strukturen, die häufig in realen Daten beobachtet werden, aber in den oben genannten Methoden selten berücksichtigt werden.In diesem Papier schlagen wir ein neuartiges Zustandsraum-Zeitreihenmodell vor, das in der Lage ist, die Struktur von Veränderungspunkten und Anomaliepunkten sowie Trend und Saisonalität zu erfassen. Um alle verborgenen Variablen abzuleiten, entwickeln wir einen Bayes'schen Rahmen, der in der Lage ist, Verteilungen und Prognoseintervalle für die Zeitreihenprognose zu erhalten, mit nachweisbaren theoretischen Eigenschaften.für die Implementierung wird ein iterativer Algorithmus mit Markov-Chain-Monte-Carlo (MCMC), Kalman-Filter und Kalman-Glättung vorgeschlagen.sowohl bei synthetischen Daten als auch bei realen Datenanwendungen liefert unsere Methodik eine bessere Leistung bei der Zeitreihenprognose im Vergleich zu bestehenden Methoden, zusammen mit einer genaueren Erkennung von Änderungspunkten und Anomalien.
Die komplexe Welt um uns herum ist von Natur aus multimodal und sequentiell (kontinuierlich), Informationen sind über verschiedene Modalitäten verstreut und erfordern mehrere kontinuierliche Sensoren, um erfasst zu werden. Die Modellierung beliebig verteilter räumlich-zeitlicher Dynamiken innerhalb und zwischen den Modalitäten ist wohl die größte Herausforderung in diesem Forschungsbereich.In diesem Beitrag stellen wir ein neues Transformatormodell vor, den sogenannten Factorized Multimodal Transformer (FMT) für multimodales sequentielles Lernen.FMT modelliert inhärent die intramodalen und intermodalen (zwei oder mehr Modalitäten einbeziehenden) Dynamiken innerhalb seines multimodalen Inputs in einer faktorisierten Weise. Die vorgeschlagene Faktorisierung ermöglicht es, die Anzahl der Selbstaufmerksamkeiten zu erhöhen, um die vorliegenden multimodalen Phänomene besser zu modellieren, ohne dass es beim Training zu Schwierigkeiten kommt (z.B. Überanpassung), selbst bei relativ geringen Ressourcen.Alle Aufmerksamkeitsmechanismen innerhalb von FMT haben ein vollständiges rezeptives Feld im Zeitbereich, was es ihnen ermöglicht, multimodale Dynamiken mit langer Reichweite asynchron zu erfassen. In unseren Experimenten konzentrieren wir uns auf Datensätze, die die drei häufig untersuchten Modalitäten Sprache, Sehen und Akustik enthalten. Wir führen eine breite Palette von Experimenten durch, die sich über drei gut untersuchte Datensätze und 21 verschiedene Labels erstrecken.FMT zeigt eine überlegene Leistung im Vergleich zu zuvor vorgeschlagenen Modellen und setzt einen neuen Stand der Technik in den untersuchten Datensätzen.
Wir entwickeln einen neuartigen und effizienten Algorithmus zur Optimierung neuronaler Netze, der von einem kürzlich vorgeschlagenen geodätischen Optimierungsalgorithmus inspiriert ist. Unser Algorithmus, den wir Stochastic Geodesic Optimization (SGeO) nennen, verwendet einen adaptiven Koeffizienten auf der Grundlage der Heavy Ball-Methode von Polyak und steuert effektiv die Menge an Gewicht, die auf die vorherige Aktualisierung der Parameter gelegt wird, basierend auf der Richtungsänderung im Optimierungspfad. Experimentelle Ergebnisse für stark konvexe Funktionen mit Lipschitz-Gradienten und tiefe Autoencoder-Benchmarks zeigen, dass SGeO niedrigere Fehler als etablierte Methoden erster Ordnung erreicht und mit niedrigeren oder ähnlichen Fehlern wie eine neuere Methode zweiter Ordnung namens K-FAC (Kronecker-Factored Approximate Curvature) konkurriert.Wir integrieren auch Nesterov-Stil Lookahead-Gradienten in unseren Algorithmus (SGeO-N) und beobachten bemerkenswerte Verbesserungen.
Das MTM basiert auf der Idee der maskierten Sprachmodellierung und unterstützt sowohl autoregressive als auch nicht-autoregressive Dekodierungsstrategien, indem es einfach die Reihenfolge der Maskierung ändert.In Experimenten mit der WMT 2016 Rumänisch-Englisch-Aufgabe zeigt das MTM eine starke Konstant-Zeit-Übersetzungsleistung und schlägt alle verwandten Ansätze mit vergleichbarer Komplexität.Wir vergleichen auch ausführlich verschiedene Dekodierungsstrategien, die vom MTM unterstützt werden, sowie verschiedene Längenmodellierungstechniken und Trainingseinstellungen.
Wir stellen lokale Ensembles vor, eine Methode zur Erkennung von Extrapolation zum Testzeitpunkt in einem vortrainierten Modell. Wir konzentrieren uns auf die Unterbestimmtheit als Schlüsselkomponente der Extrapolation: Wir wollen erkennen, wenn viele mögliche Vorhersagen mit den Trainingsdaten und der Modellklasse übereinstimmen. Unsere Methode verwendet lokale Informationen zweiter Ordnung, um die Varianz der Vorhersagen über ein Ensemble von Modellen derselben Klasse zu approximieren. Wir berechnen diese Annäherung, indem wir die Norm der Komponente des Gradienten eines Testpunkts schätzen, die mit den Richtungen der Hessian mit geringer Krümmung übereinstimmt, und bieten eine nachvollziehbare Methode zur Schätzung dieser Größe. Experimentell zeigen wir, dass unsere Methode in der Lage ist, zu erkennen, wenn ein vortrainiertes Modell auf Testdaten extrapoliert, mit Anwendungen zur Erkennung von Out-of-Distribution, zur Erkennung von falschen Korrelaten und zum aktiven Lernen.
Viele Nationen und Nichtregierungsorganisationen koordinieren sich regelmäßig, um solche Probleme anzugehen, aber ihre Zusammenarbeit wird oft durch die Beschränkung der Informationen, die sie austauschen können, behindert.In diesem Papier betrachten wir die Verwendung einer fortschrittlichen kryptographischen Technik, die sichere Mehrparteienberechnung genannt wird, um Koalitionsmitglieder in die Lage zu versetzen, gemeinsame Ziele zu erreichen und gleichzeitig die Anforderungen an den Datenschutz zu erfüllen. Unser besonderes Augenmerk liegt auf der Planung von Hilfslieferungen durch mehrere Nationen, bei der es darum geht, zu koordinieren, wann und wohin die verschiedenen Hilfsorganisationen nach einer Naturkatastrophe Hilfsgüter liefern werden. Selbst bei der Verwendung von sicherer Mehrparteien-Rechentechnik können Informationen über private Daten nach außen dringen.  Wir beschreiben, wie der aufkommende Bereich des quantitativen Informationsflusses genutzt werden kann, um Dateneigentümern dabei zu helfen, das Ausmaß zu verstehen, in dem private Daten als Ergebnis möglicher oder tatsächlicher Planungsvorgänge angreifbar werden könnten, und um automatische Anpassungen des Planungsprozesses zu ermöglichen, um Datenschutzanforderungen zu gewährleisten.
Verschiedene Methoden, die auf variationalen Autoencodern basieren, wurden vorgeschlagen, um dieses Problem zu lösen, indem die Unabhängigkeit zwischen den Repräsentationen erzwungen und der Regularisierungsterm in der variationalen unteren Schranke modifiziert wird.Jüngste Arbeiten von Locatello et al. (2018) haben jedoch gezeigt, dass die vorgeschlagenen Methoden stark von der Zufälligkeit und der Wahl des Hyperparameters beeinflusst werden.Diese Arbeit baut auf dem gleichen Rahmen in Stufe 1 (Li et al, 2019), aber mit anderen Einstellungen; um es in sich geschlossen zu machen, stellen wir dieses Manuskript zur Verfügung, das dem Bericht für Stufe 1 unvermeidlich sehr ähnlich ist.Im Detail übernehmen wir in dieser Arbeit, anstatt einen neuen Regularisierungsterm zu entwerfen, die FaktorVAE, verbessern aber die Rekonstruktionsleistung und erhöhen die Kapazität des Netzwerks und des Trainingsschritts.Die Strategie erweist sich als sehr effektiv bei der Erreichung der Entflechtung.
Unser Ansatz generiert Klärungsfragen mit dem Ziel, neue Informationen zu erhalten, die den gegebenen Kontext vervollständigen. Wir entwickeln ein generatives adversarisches Netzwerk (GAN), bei dem der Generator ein Sequenz-zu-Sequenz-Modell und der Diskriminator eine Nutzenfunktion ist, die den Wert der Aktualisierung des Kontexts mit der Antwort auf die Klärungsfrage modelliert. Wir evaluieren zwei Datensätze unter Verwendung automatischer Metriken und menschlicher Beurteilungen von Nützlichkeit, Spezifität und Relevanz und zeigen, dass unser Ansatz sowohl ein Retrieval-basiertes Modell als auch Ablationen übertrifft, die das Utility-Modell und das kontradiktorische Training ausschließen.
Experimente haben gezeigt, dass die informativste Art der Kombination von Informationen aus mehreren Musterdatenbanken darin besteht, gesättigte Kostenpartitionierung zu verwenden. In früheren Arbeiten wurden Muster ausgewählt und gesättigte Kostenpartitionierungen über die resultierenden Musterdatenbank-Heuristiken in zwei separaten Schritten berechnet.
Vaswani et.al. (2017) schlagen eine neue Architektur vor, die Rekursion und Faltung vollständig vermeidet und stattdessen nur Selbstaufmerksamkeit und Feed-Forward-Schichten verwendet. Während die vorgeschlagene Architektur bei verschiedenen maschinellen Übersetzungsaufgaben Ergebnisse auf dem neuesten Stand der Technik erzielt, erfordert sie eine große Anzahl von Parametern und Trainingsiterationen, um zu konvergieren. Wir schlagen Weighted Transformer vor, einen Transformer mit modifizierten Aufmerksamkeitsschichten, der nicht nur das Basisnetzwerk in der BLEU-Punktzahl übertrifft, sondern auch um 15-40% schneller konvergiert.Wir ersetzen insbesondere die Multi-Head-Attention durch mehrere Self-Attention-Zweige, die das Modell während des Trainingsprozesses zu kombinieren lernt.Unser Modell verbessert die State-of-the-Art-Leistung um 0,5 BLEU-Punkte bei der WMT 2014-Übersetzungsaufgabe Englisch-Deutsch und um 0,4 bei der Übersetzungsaufgabe Englisch-Französisch.
Das Nachahmungslernen bietet einen attraktiven Rahmen für die autonome Steuerung: Bei vielen Aufgaben können Demonstrationen des bevorzugten Verhaltens leicht von menschlichen Experten erhalten werden, wodurch die Notwendigkeit einer kostspieligen und potenziell gefährlichen Online-Datenerfassung in der realen Welt entfällt. Modellbasiertes Verstärkungslernen (MBRL) bietet wesentlich mehr Flexibilität, da ein aus Daten erlerntes Vorhersagemodell verwendet werden kann, um verschiedene Ziele zum Testzeitpunkt zu erreichen. MBRL leidet jedoch unter zwei Unzulänglichkeiten: Erstens hilft das Modell nicht bei der Auswahl gewünschter oder sicherer Ergebnisse - seine Dynamik schätzt nur ab, was möglich ist, nicht was bevorzugt wird. Zweitens erfordert MBRL in der Regel eine zusätzliche Online-Datenerfassung, um sicherzustellen, dass das Modell in den Situationen, die beim Versuch, die Testzeitziele zu erreichen, tatsächlich angetroffen werden, genau ist.Das Sammeln dieser Daten mit einem nur teilweise trainierten Modell kann gefährlich und zeitaufwändig sein.In diesem Papier versuchen wir, die Vorteile des Nachahmungslernens und MBRL zu kombinieren, und schlagen Nachahmungsmodelle vor: probabilistische Vorhersagemodelle, die in der Lage sind, expertenähnliche Trajektorien zu planen, um beliebige Ziele zu erreichen. Wir stellen fest, dass diese Methode sowohl die direkte Imitation als auch MBRL in einer simulierten autonomen Fahraufgabe deutlich übertrifft und effizient aus einem festen Satz von Expertendemonstrationen ohne zusätzliche Online-Datensammlung erlernt werden kann. Wir zeigen auch, dass unser Modell flexibel Kosten, die vom Benutzer zur Testzeit bereitgestellt werden, einbeziehen kann, dass es Sequenzen von Zielen planen kann und dass es sogar mit ungenauen Zielen, einschließlich Zielen auf der falschen Straßenseite, gut funktioniert.
Unsicherheitsschätzung und Ensembling-Methoden gehen Hand in Hand: Die Unsicherheitsschätzung ist einer der wichtigsten Maßstäbe für die Bewertung der Ensembling-Leistung, und Deep-Learning-Ensembles haben bei der Unsicherheitsschätzung den neuesten Stand der Technik erreicht.In dieser Arbeit konzentrieren wir uns auf die bereichsinterne Unsicherheit bei der Bildklassifizierung, untersuchen die Standards für ihre Quantifizierung und weisen auf die Fallstricke bestehender Metriken hin. Um einen besseren Einblick in den umfassenden Vergleich zu erhalten, führen wir das Deep-Ensemble-Äquivalent (DEE) ein und zeigen, dass viele hochentwickelte Ensembling-Techniken in Bezug auf die Test-Log-Likelihood einem Ensemble aus sehr wenigen, unabhängig voneinander trainierten Netzwerken gleichzusetzen sind.
Obwohl die meisten der bisher entwickelten Techniken Kenntnisse über die Architektur des maschinellen Lernmodells erfordern und sich nur schwer auf komplexe Vorhersagepipelines skalieren lassen, hat sich gezeigt, dass die Methode der randomisierten Glättung viele dieser Hindernisse überwinden kann, da sie nur einen Black-Box-Zugriff auf das zugrunde liegende Modell erfordert und sich auf große Architekturen skalieren lässt. Bisherige Arbeiten zur randomisierten Glättung konzentrierten sich jedoch auf eingeschränkte Klassen von Glättungsmaßen oder Störungen (wie Gaußsche oder diskrete) und waren nur in der Lage, Robustheit in Bezug auf einfache Normschranken zu beweisen. Insbesondere erweitern wir randomisierte Glättungsverfahren, um beliebige Glättungsmaße zu handhaben und beweisen die Robustheit des geglätteten Klassifizierers durch die Verwendung von $f$-Divergenzen. Unsere Methodik erreicht eine nach dem Stand der Technik}-zertifizierte Robustheit bei MNIST, CIFAR-10 und ImageNet sowie bei der Audioklassifizierungsaufgabe Librispeech in Bezug auf verschiedene Klassen von Störungen.
Die Fähigkeit, komplexe Multi-Objekt-Szenen in sinnvolle Abstraktionen wie Objekte zu zerlegen, ist von grundlegender Bedeutung, um Kognition auf höherer Ebene zu erreichen. Bisherige Ansätze für unbeaufsichtigtes objektorientiertes Szenenrepräsentationslernen basieren entweder auf Spatial-Attention- oder Scene-Mixture-Ansätzen und sind in ihrer Skalierbarkeit begrenzt, was ein Haupthindernis für die Modellierung von realen Szenen ist.In diesem Papier schlagen wir ein generatives latentes Variablenmodell vor, das SPACE genannt wird und einen uniﬁed probabilistischen Modellierungsrahmen bietet, der das Beste aus Spatial-Attention- und Scene-Mixture-Ansätzen kombiniert. SPACE kann explizit faktorisierte Objektrepräsentationen für Vordergrundobjekte bereitstellen und gleichzeitig Hintergrundsegmente mit komplexer Morphologie zerlegen. Bisherige Modelle sind gut in einem der beiden Bereiche, aber nicht in beiden.SPACE löst auch die Skalierbarkeitsprobleme bisheriger Methoden, indem es parallele räumliche Aufmerksamkeit einbezieht und somit auf Szenen mit einer großen Anzahl von Objekten ohne Leistungseinbußen anwendbar ist.Wir zeigen anhand von Experimenten auf Atari- und 3D-Räumen, dass SPACE die oben genannten Eigenschaften im Vergleich zu SPAIR, IODINE und GENESIS durchgängig erreicht.Die Ergebnisse unserer Experimente finden Sie auf unserer Projektwebsite: https://sites.google.com/view/space-project-page
Wir schlagen ein einzelnes neuronales probabilistisches Modell vor, das auf einem Variations-Auto-Codierer basiert, der auf eine beliebige Teilmenge von beobachteten Merkmalen konditioniert werden kann und dann die verbleibenden Merkmale in "einem Schuss" abtastet.Die Merkmale können sowohl reell-wertig als auch kategorisch sein.Das Training des Modells wird durch stochastische Variations-Bayes durchgeführt.Die experimentelle Auswertung auf synthetischen Daten sowie Merkmal-Imputation und Bild-Inpainting-Problemen zeigt die Wirksamkeit des vorgeschlagenen Ansatzes und die Vielfalt der erzeugten Proben.
Wir erforschen effiziente Suchmethoden für neuronale Architekturen und zeigen, dass ein einfacher, aber leistungsfähiger evolutionärer Algorithmus neue Architekturen mit exzellenter Leistung entdecken kann. Unser Ansatz kombiniert ein neuartiges hierarchisches genetisches Repräsentationsschema, das das modularisierte Designmuster nachahmt, das üblicherweise von menschlichen Experten verwendet wird, und einen ausdrucksstarken Suchraum, der komplexe Topologien unterstützt. Unser Algorithmus entdeckt effizient Architekturen, die eine große Anzahl von manuell entworfenen Modellen für die Bildklassifizierung übertreffen, und erreicht einen Top-1-Fehler von 3,6 % bei CIFAR-10 und 20,3 %, wenn er auf ImageNet übertragen wird, was mit den besten existierenden Suchansätzen für neuronale Architekturen konkurrenzfähig ist.
Bei der visuellen Planung (VP) lernt ein Agent, zielgerichtetes Verhalten aus offline gewonnenen Beobachtungen eines dynamischen Systems zu planen, z. B, VP-Algorithmen kombinieren im Wesentlichen datengesteuerte Wahrnehmung und Planung und sind unter anderem für Robotermanipulation und -navigation wichtig. Ein neuer und vielversprechender Ansatz für VP ist die semi-parametrische topologische Speichermethode (SPTM), bei der Bildproben als Knoten in einem Graphen behandelt werden und die Konnektivität im Graphen durch tiefe Bildklassifizierung gelernt wird. Der gelernte Graph stellt somit die topologische Konnektivität der Daten dar, und die Planung kann mit herkömmlichen Graphensuchmethoden durchgeführt werden. Noch wichtiger ist, dass SPTM nur begrenzt in der Lage ist, auf Veränderungen in der Domäne zu verallgemeinern, da sein Graph aus direkten Beobachtungen konstruiert wird und daher neue Proben für die Planung gesammelt werden müssen.In diesem Papier schlagen wir Halluzinatives Topologisches Gedächtnis (HTM) vor, das diese Mängel überwindet.In HTM trainieren wir anstelle eines diskriminativen Klassifikators eine Energiefunktion unter Verwendung kontrastiver prädiktiver Kodierung. Zusätzlich erlernen wir ein bedingtes VAE-Modell, das Muster aus einem Kontextbild der Domäne generiert und diese halluzinierten Muster für den Aufbau des Konnektivitätsgraphen verwendet, was eine Generalisierung auf Änderungen in der Domäne zum Nulltarif ermöglicht.In simulierten Domänen übertrifft HTM herkömmliche SPTM- und visuelle Vorausschau-Methoden sowohl hinsichtlich der Planqualität als auch des Erfolgs bei der Planung mit langem Horizont.
Deep Neural Networks (DNNs) haben sich in den letzten Jahren stark entwickelt, wobei die Batch Normalization (BN) eine unverzichtbare Rolle spielt. Es wurde jedoch festgestellt, dass BN aufgrund der Reduktionsoperationen kostspielig ist. Wir stellen fest, dass die Effektivität weniger Datenkorrelation erwartet, während die Effizienz ein regelmäßiges Ausführungsmuster erwartet. Zu diesem Zweck schlagen wir zwei Kategorien von Ansätzen vor: Sampling oder die Erstellung von wenigen unkorrelierten Daten für die Schätzung von Statistiken mit bestimmten Strategiebeschränkungen. Die erste Kategorie umfasst "Batch Sampling (BS)", das zufällig einige Stichproben aus jeder Charge auswählt, und "Feature Sampling (FS)", das zufällig einen kleinen Patch aus jeder Feature Map aller Stichproben auswählt, und die zweite Kategorie ist "Virtual Dataset Normalization (VDN)", die einige synthetische Zufallsstichproben generiert, um die Datenkorrelation für eine genaue Schätzung zu reduzieren und das Ausführungsmuster für die laufende Beschleunigung zu optimieren. Alle vorgeschlagenen Methoden werden umfassend an verschiedenen DNN-Modellen evaluiert, wobei eine Gesamttrainingsbeschleunigung von bis zu 21,7 % auf modernen GPUs praktisch ohne die Unterstützung von Spezialbibliotheken erreicht werden kann und der Verlust an Modellgenauigkeit und Konvergenzrate vernachlässigbar ist; darüber hinaus zeigen unsere Methoden eine starke Leistung bei der Lösung des bekannten "Mikro-Stapel-Normalisierungs"-Problems im Falle einer winzigen Stapelgröße.
Wir schlagen den Algorithmus FedMA (Federated Matched Averaging) vor, der für das föderierte Lernen moderner neuronaler Netzwerkarchitekturen wie z.B. Faltungsneuronaler Netze (CNNs) und LSTMs entwickelt wurde. FedMA konstruiert das gemeinsame globale Modell schichtweise, indem es versteckte Elemente (d.h. Kanäle für Faltungsschichten, versteckte Zustände für LSTMs, Neuronen für vollverknüpfte Schichten) mit ähnlichen Merkmalsextraktionssignaturen abgleicht und mittelt. Unsere Experimente zeigen, dass FedMA populäre State-of-the-Art-Algorithmen für föderiertes Lernen auf tiefen CNN- und LSTM-Architekturen, die auf realen Datensätzen trainiert wurden, übertrifft und gleichzeitig die Kommunikationseffizienz verbessert.
Wir stellen SOSELETO (SOurce SELEction for Target Optimization) vor, eine neue Methode zur Nutzung eines Quelldatensatzes zur Lösung eines Klassifizierungsproblems in einem Zieldatensatz.  SOSELETO basiert auf der folgenden einfachen Intuition: Einige Quellbeispiele sind für das Zielproblem informativer als andere.  Um dieser Intuition gerecht zu werden, werden den Quellbeispielen Gewichte zugewiesen; diese Gewichte werden gemeinsam mit dem Quell- und dem Zielklassifizierungsproblem über ein zweistufiges Optimierungsschema gelöst.  Das Ziel kann daher die Quellstichproben auswählen, die für seine eigene Klassifizierungsaufgabe am informativsten sind.  Darüber hinaus wirkt die zweistufige Optimierung als eine Art Regularisierung für das Ziel, die eine Überanpassung verhindert.  SOSELETO kann sowohl auf klassisches Transfer-Lernen als auch auf das Problem des Trainings auf Datensätzen mit verrauschten Labels angewandt werden; wir zeigen Ergebnisse auf dem neuesten Stand der Technik für beide Probleme.
Die ableitungsfreie Optimierung (DFO) mit Hilfe von Trust-Region-Methoden wird häufig für Anwendungen des maschinellen Lernens verwendet, z. B. für die (Hyper-)Parameteroptimierung, ohne dass die Ableitungen der Zielfunktionen bekannt sind.  Während der erste Explorationsprozess das Minimum der Blackbox-Funktion durch Minimierung einer sich zeitlich entwickelnden Surrogationsfunktion sucht, aktualisiert ein anderer Ausnutzungsprozess die Surrogationsfunktion von Zeit zu Zeit unter Verwendung der vom Explorationsprozess durchlaufenen Punkte, so dass die Effizienz der ableitungsfreien Optimierung von der Art und Weise abhängt, wie die beiden Prozesse gekoppelt sind. In diesem Papier schlagen wir ein neuartiges dynamisches System vor, nämlich \ThePrev---\underline{S}tochastic \underline{H}amiltonian \underline{E}xploration und \underline{E}xploitation, das die Teilbereiche der Blackbox-Funktion mit Hilfe einer sich zeitlich entwickelnden quadratischen Funktion surrogiert und dann das Minimum der quadratischen Funktionen mit Hilfe eines schnell konvergierenden Hamilton-Systems erforscht und verfolgt. Um die Optimierung weiter zu beschleunigen, stellen wir \TheName\ vor, das mehrere \ThePrev\-Threads für die gleichzeitige Erkundung und Nutzung parallelisiert. Die Ergebnisse von Experimenten, die auf einer breiten Palette von Anwendungen für maschinelles Lernen basieren, zeigen, dass \TheName\ unter denselben Einstellungen eine Reihe von derivativ-freien Optimierungsalgorithmen mit schnellerer Konvergenzgeschwindigkeit übertrifft.
 Es hat sich gezeigt, dass binarisierte neuronale Netze (BNNs) die Effizienz des Netzes während der Inferenzphase verbessern, nachdem das Netz trainiert wurde. BNNs binarisieren jedoch nur die Modellparameter und Aktivierungen während der Fortpflanzung, so dass BNNs während des Trainings keine signifikanten Effizienzverbesserungen bieten, da die Gradienten weiterhin mit hoher Präzision fortgepflanzt und verwendet werden.   Wir zeigen, dass es keine inhärenten Schwierigkeiten bei der Ausbildung von BNNs mit "Binarized BackPropagation" (BBP) gibt, bei der wir auch die Gradienten binarisieren.Um eine signifikante Verschlechterung der Testgenauigkeit zu vermeiden, erhöhen wir einfach die Anzahl der Filterkarten in jeder Faltungsschicht.Die Verwendung von BBP auf dedizierter Hardware kann die Ausführungseffizienz potenziell erheblich verbessern (z. B. Die Verwendung von BBP auf dedizierter Hardware kann die Ausführungseffizienz erheblich verbessern (z.B. Verringerung des dynamischen Speicherbedarfs, der Speicherbandbreite und der Rechenenergie) und den Trainingsprozess mit einer geeigneten Hardwareunterstützung beschleunigen, selbst nach einer solchen Erhöhung der Netzwerkgröße, Mit dieser Methode zeigen wir einen minimalen Verlust an Klassifizierungsgenauigkeit bei verschiedenen Datensätzen und Topologien.
Die Gewichtungsinitialisierung und die Aktivierungsfunktion tiefer neuronaler Netze haben einen entscheidenden Einfluss auf die Leistung des Trainingsverfahrens: Eine ungeeignete Auswahl kann zum Verlust von Eingangsinformationen während der Vorwärtspropagation und zum exponentiellen Verschwinden/Explodieren von Gradienten während der Rückwärtspropagation führen. Das Verständnis der theoretischen Eigenschaften von untrainierten Zufallsnetzwerken ist der Schlüssel zur Identifizierung derjenigen tiefen Netzwerke, die erfolgreich trainiert werden können, wie kürzlich von Schoenholz et al. (2017) gezeigt wurde, die zeigten, dass für tiefe neuronale Feedforward-Netzwerke nur eine bestimmte Wahl von Hyperparametern, bekannt als "Rand des Chaos", zu einer guten Leistung führen kann. Wir vervollständigen diese Analyse durch die Bereitstellung quantitativer Ergebnisse, die zeigen, dass sich die Informationen für eine Klasse von ReLU-ähnlichen Aktivierungsfunktionen tatsächlich tiefer ausbreiten, wenn die Initialisierung am Rande des Chaos erfolgt.Durch eine weitere Erweiterung dieser Analyse identifizieren wir eine Klasse von Aktivierungsfunktionen, die die Informationsausbreitung gegenüber ReLU-ähnlichen Funktionen verbessern. Zu dieser Klasse gehört die Swish-Aktivierung $\phi_{swish}(x) = x \cdot \text{sigmoid}(x)$, die in Hendrycks & Gimpel (2016), Elfwing et al. (2017) und Ramachandran et al. (2017) verwendet wurde, und die eine theoretische Grundlage für die hervorragende empirische Leistung von $\phi_{swish}$ bietet, die in diesen Beiträgen beobachtet wurde.Wir ergänzen diese früheren Ergebnisse, indem wir den Vorteil der Verwendung einer zufälligen Initialisierung am Rande des Chaos in diesem Kontext veranschaulichen.
Die treibende Kraft hinter dem jüngsten Erfolg von LSTMs ist ihre Fähigkeit, komplexe und nicht-lineare Beziehungen zu lernen, was dazu geführt hat, dass LSTMs als Blackboxen bezeichnet werden.Zu diesem Zweck führen wir die kontextuelle Dekomposition (CD) ein, einen Interpretationsalgorithmus zur Analyse individueller Vorhersagen, die von Standard-LSTMs gemacht werden, ohne das zugrunde liegende Modell zu verändern. Durch die Zerlegung der Ausgabe eines LSTMs erfasst CD die Beiträge von Wortkombinationen oder Variablen zur endgültigen Vorhersage eines LSTMs. Bei der Stimmungsanalyse mit den Datensätzen von Yelp und SST zeigen wir, dass CD in der Lage ist, Wörter und Phrasen mit gegensätzlichen Stimmungen zuverlässig zu identifizieren und wie sie kombiniert werden, um die endgültige Vorhersage des LSTMs zu erhalten.
Die meisten Deep-Learning-basierten Modelle für die Sprachverbesserung haben sich hauptsächlich auf die Schätzung der Größe des Spektrogramms konzentriert, während die Phase von verrauschter Sprache für die Rekonstruktion wiederverwendet wird, da es schwierig ist, die Phase von sauberer Sprache zu schätzen. Erstens schlagen wir Deep Complex U-Net vor, ein fortschrittliches U-Net strukturiertes Modell, das wohldefinierte komplexwertige Bausteine enthält, um mit komplexwertigen Spektrogrammen umzugehen.Zweitens schlagen wir eine polarkoordinatenweise komplexwertige Maskierungsmethode vor, um die Verteilung komplexer idealer Verhältnismasken zu reflektieren. Drittens definieren wir eine neuartige Verlustfunktion, gewichtete Quelle-zu-Verzerrung-Verhältnis (wSDR) Verlust, der entworfen ist, um direkt mit einem quantitativen Bewertungsmaßstab korrelieren.Unser Modell wurde auf eine Mischung aus der Voice Bank Korpus und DEMAND-Datenbank, die weithin von vielen Deep-Learning-Modelle für die Sprachverbesserung verwendet wurde evaluiert.Ablation Experimente wurden auf dem gemischten Datensatz durchgeführt, die zeigen, dass alle drei vorgeschlagenen Ansätze empirisch gültig sind.Experimentelle Ergebnisse zeigen, dass die vorgeschlagene Methode erreicht state-of-the-art Leistung in allen Metriken und übertrifft frühere Ansätze durch eine große Marge.
Alle lebenden Organismen kämpfen gegen die Kräfte der Natur, um Nischen zu finden, in denen sie einen relativen Stillstand aufrechterhalten können. Wir schlagen vor, dass eine solche Suche nach Ordnung inmitten des Chaos ein vereinheitlichendes Prinzip für die Entstehung nützlicher Verhaltensweisen in künstlichen Agenten bieten könnte.Wir formalisieren diese Idee in eine unüberwachte Verstärkungslernmethode, die als überraschungsminimierendes RL (SMiRL) bezeichnet wird. SMiRL trainiert einen Agenten mit dem Ziel, die Wahrscheinlichkeit von beobachteten Zuständen unter einem Modell zu maximieren, das auf allen zuvor gesehenen Zuständen trainiert wurde. Die resultierenden Agenten erwerben mehrere proaktive Verhaltensweisen, um stabile Zustände zu suchen und aufrechtzuerhalten, wie z.B. Balancieren und Schadensvermeidung, die eng mit den Möglichkeiten der Umgebung und ihren vorherrschenden Entropiequellen, wie Wind, Erdbeben und anderen Agenten, verbunden sind. Wir zeigen, dass unsere überraschungsminimierenden Agenten erfolgreich Tetris und Doom spielen und einen Humanoiden steuern können, um Stürze zu vermeiden, ohne dass eine aufgabenspezifische Belohnungsüberwachung erforderlich ist.  Darüber hinaus zeigen wir, dass SMiRL als unbeaufsichtigtes Vortrainingsobjekt verwendet werden kann, das das anschließende belohnungsgesteuerte Lernen erheblich beschleunigt.
Ein beliebter Ansatz für das Meta-Lernen besteht darin, ein rekurrentes Modell zu trainieren, um einen Trainingsdatensatz als Eingabe einzulesen und die Parameter eines gelernten Modells auszugeben oder Vorhersagen für neue Testeingaben auszugeben.alternativ dazu zielt ein neuerer Ansatz für das Meta-Lernen darauf ab, tiefe Repräsentationen zu erwerben, die mittels Standard-Gradientenabstieg effektiv auf neue Aufgaben abgestimmt werden können. In diesem Beitrag betrachten wir das Problem des Meta-Lernens aus der Perspektive der Universalität, indem wir den Begriff der Annäherung des Lernalgorithmus formalisieren und die Ausdruckskraft der oben erwähnten rekurrenten Modelle mit den neueren Ansätzen vergleichen, die den Gradientenabstieg in den Meta-Lerner einbetten. Insbesondere versuchen wir, die folgende Frage zu beantworten: Verfügt eine tiefe Repräsentation in Kombination mit einem Standard-Gradientenabstieg über eine ausreichende Kapazität, um jeden Lernalgorithmus zu approximieren? Wir finden, dass dies in der Tat der Fall ist, und stellen in unseren Experimenten fest, dass gradientenbasiertes Meta-Lernen durchweg zu Lernstrategien führt, die im Vergleich zu denen, die durch rekurrente Modelle repräsentiert werden, stärker verallgemeinert sind.
Brain-Computer Interfaces (BCI) können Patienten mit eingeschränkten Kommunikationsfähigkeiten aufgrund von neurodegenerativen Erkrankungen helfen, Text oder Sprache durch direkte neuronale Verarbeitung zu produzieren. Allerdings hat sich ihre praktische Umsetzung aufgrund von Beschränkungen in Bezug auf Geschwindigkeit, Genauigkeit und Verallgemeinerbarkeit bestehender Schnittstellen als schwierig erwiesen.Zu diesem Zweck zielen wir darauf ab, ein BCI zu entwickeln, das Text direkt aus neuronalen Signalen dekodiert.Wir implementieren einen Rahmen, der zunächst Frequenzbänder im Eingangssignal isoliert, die differentielle Informationen bezüglich der Produktion verschiedener phonemischer Klassen enthalten. Diese Bänder bilden einen Merkmalsatz, der in ein LSTM einfließt, das zu jedem Zeitpunkt Wahrscheinlichkeitsverteilungen über alle von einer Person gesprochenen Phoneme erkennt. Schließlich glättet ein Partikelfilter-Algorithmus diese Wahrscheinlichkeiten zeitlich, indem er Vorwissen über die englische Sprache einbezieht, um den dem dekodierten Wort entsprechenden Text auszugeben. Der empirische Erfolg des von uns vorgeschlagenen Ansatzes ist vielversprechend für die Verwendung einer solchen Schnittstelle durch Patienten in einer unbeeinflussten, naturalistischen Umgebung.
Transferlernen für die Merkmalsextraktion kann verwendet werden, um tiefe Repräsentationen in Kontexten zu nutzen, in denen es sehr wenige Trainingsdaten gibt, in denen es begrenzte Rechenressourcen gibt oder in denen das Abstimmen der Hyperparameter, die für das Training benötigt werden, keine Option ist.Während frühere Beiträge zur Merkmalsextraktion Einbettungen vorschlagen, die auf einer einzelnen Schicht des Netzwerks basieren, schlagen wir in diesem Papier eine Einbettung für ein ganzes Netzwerk vor, die erfolgreich faltige und vollständig verbundene Merkmale integriert, die aus allen Schichten eines tiefen faltigen neuronalen Netzwerks stammen. Zu diesem Zweck normalisiert die Einbettung die Merkmale im Kontext des Problems und diskretisiert ihre Werte, um Rauschen zu reduzieren und den Einbettungsraum zu regulieren, was auch die Rechenkosten für die Verarbeitung der resultierenden Darstellungen deutlich reduziert. Es wurde gezeigt, dass die vorgeschlagene Methode die Einschicht-Einbettung bei verschiedenen Bildklassifizierungsaufgaben übertrifft und gleichzeitig robuster gegenüber der Wahl des vortrainierten Modells ist, das für die Gewinnung der ursprünglichen Merkmale verwendet wird.
Wir stellen einen neuartigen Algorithmus zur Netzwerkbeschneidung vor, der Dynamic Sparse Training genannt wird und mit dem die optimalen Netzwerkparameter und die Struktur des spärlichen Netzwerks in einem einheitlichen Optimierungsprozess mit trainierbaren Beschneidungsschwellen gemeinsam gefunden werden können, wobei diese Schwellen dynamisch über Backpropagation schichtweise angepasst werden können. Darüber hinaus haben wir mehrere überraschende Beobachtungen gemacht, die die Effektivität und Efﬁzienz unseres Algorithmus belegen. Diese Beobachtungen zeigen die zugrundeliegenden Probleme traditioneller dreistufiger Pruning-Algorithmen auf und präsentieren die potenziellen Hinweise, die unser Algorithmus für den Entwurf kompakterer Netzarchitekturen liefert.
Inwieweit kann erfolgreiches maschinelles Lernen unser Verständnis von biologischem Lernen beeinflussen? ein beliebter Weg der Untersuchung in den letzten Jahren war es, solche Algorithmen direkt in eine realistische Schaltungsimplementierung abzubilden. hier konzentrieren wir uns auf das Lernen in rekurrenten Netzwerken und untersuchen eine Reihe von Lernalgorithmen. unser Ansatz zerlegt sie in ihre rechnerischen Bausteine und diskutiert ihr abstraktes Potenzial als biologische Operationen. diese alternative Strategie bietet eine "faule", aber prinzipielle Möglichkeit, ML-Ideen in Bezug auf ihre biologische Plausibilität zu bewerten.
In den letzten Jahren wurden verschiedene Angriffe und Verteidigungsmaßnahmen vorgeschlagen.Oft erweisen sich scheinbar robuste Modelle als nicht robust, wenn anspruchsvollere Angriffe verwendet werden.Ein Ausweg aus diesem Dilemma sind beweisbare Robustheitsgarantien.Während beweisbare robuste Modelle für bestimmte $l_p$-Störungsmodelle entwickelt wurden, zeigen wir, dass sie keine Garantie gegen andere $l_q$-Störungen bieten. Wir schlagen ein neues Regularisierungsschema, MMR-Universal, für ReLU-Netze vor, das Robustheit gegenüber $l_1$- \textit{und} $l_\infty$-Störungen erzwingt und zeigen, wie dies zu den ersten beweisbar robusten Modellen gegenüber jeder $l_p$-Norm für $p\geq 1$ führt.
Das vorgeschlagene Kontrollproblem enthält eine Wiederherstellungsdynamik, die durch ein RNN modelliert wird. Der bewegliche Endpunkt, der im Wesentlichen der Endzeitpunkt der zugehörigen Dynamik ist, wird durch ein Policy-Netzwerk bestimmt. Numerische Experimente zeigen, dass DURR in der Lage ist, Spitzenleistungen bei der blinden Bildentrauschung und dem Deblocking von JPEG-Bildern zu erzielen, und dass DURR auch für Bilder mit höherem Degradationsgrad geeignet ist, die nicht in der Trainingsphase enthalten sind.
Die Wasserstein-Distanz hat in letzter Zeit viel Aufmerksamkeit in der Gemeinschaft des maschinellen Lernens erhalten, vor allem wegen ihrer prinzipiellen Art, Verteilungen zu vergleichen. Sie hat zahlreiche Anwendungen in verschiedenen schwierigen Problemen gefunden, wie z.B. Domänenanpassung, Dimensionalitätsreduktion oder generative Modelle.Allerdings ist ihre Verwendung immer noch durch einen hohen Rechenaufwand begrenzt.Unser Ziel ist es, dieses Problem zu lindern, indem wir einen Annäherungsmechanismus bereitstellen, der es erlaubt, die inhärente Komplexität zu brechen.Es beruht auf der Suche nach einer Einbettung, bei der die euklidische Distanz die Wasserstein-Distanz nachahmt. Wir zeigen, dass eine solche Einbettung mit einer siamesischen Architektur gefunden werden kann, die mit einem Decoder-Netzwerk verbunden ist, das es erlaubt, sich vom Einbettungsraum zurück in den ursprünglichen Eingaberaum zu bewegen.Sobald diese Einbettung gefunden wurde, kann die Berechnung von Optimierungsproblemen im Wasserstein-Raum (z.B. Baryzentren, Hauptrichtungen oder sogar Archetypen) extrem schnell durchgeführt werden.Numerische Experimente, die diese Idee unterstützen, werden an Bilddatensätzen durchgeführt und zeigen die großen potentiellen Vorteile unserer Methode.
Continuous Bag of Words (CBOW) ist eine leistungsfähige Methode zur Texteinbettung, die aufgrund ihrer starken Fähigkeit, Wortinhalte zu kodieren, bei einer Vielzahl von nachgelagerten Aufgaben eine gute Leistung erbringt und gleichzeitig effizient zu berechnen ist.CBOW ist jedoch nicht in der Lage, die Wortreihenfolge zu erfassen.Der Grund dafür ist, dass die Berechnung der CBOW-Worteinbettungen kommutativ ist, d.h, Um dieses Manko zu beheben, schlagen wir einen Lernalgorithmus für das Continuous Matrix Space Model vor, den wir Continual Multiplication of Words (CMOW) nennen.Unser Algorithmus ist eine Adaption von word2vec, so dass er auf großen Mengen von unbeschriftetem Text trainiert werden kann.Wir zeigen empirisch, dass CMOW linguistische Eigenschaften besser erfasst, aber beim Erfassen von Wortinhalten CBOW unterlegen ist. Unsere Ergebnisse zeigen, dass das hybride CBOW-CMOW-Modell die starke Fähigkeit von CBOW, sich Wortinhalte zu merken, beibehält und gleichzeitig seine Fähigkeit, andere linguistische Informationen zu kodieren, um 8% verbessert.
Dieses Papier schlägt Metagross (Meta Gated Recursive Controller) vor, eine neue Einheit zur Modellierung neuronaler Sequenzen, die sich durch eine rekursive Parametrisierung ihrer Gating-Funktionen auszeichnet, d.h., Dies kann als eine Form des Meta-Gatings und der rekursiven Parametrisierung eines rekurrenten Modells interpretiert werden.wir gehen davon aus, dass der von uns vorgeschlagene induktive Bias Vorteile bei der Modellierung von Lernprozessen mit inhärent hierarchisch strukturierten Sequenzdaten (z.B. Sprache, logische oder musikalische Aufgaben) bietet, Zu diesem Zweck führen wir umfangreiche Experimente mit rekursiven Logikaufgaben (Sortieren, Baum-Traversal, logische Inferenz), sequenzieller Pixel-für-Pixel-Klassifikation, semantischem Parsing, Code-Generierung, maschineller Übersetzung und polyphoner Musikmodellierung durch, die den weitreichenden Nutzen des vorgeschlagenen Ansatzes demonstrieren, d.h., wir erreichen bei allen Aufgaben die beste Leistung (oder nahe daran).
Welches generative Modell eignet sich am besten für kontinuierliches Lernen? in diesem Beitrag werden generative Modelle für disjunkte sequentielle Bilderzeugungsaufgaben bewertet und verglichen. wir untersuchen, wie verschiedene Modelle lernen und vergessen, wobei wir verschiedene Strategien in Betracht ziehen: Rehearsal, Regularisierung, generative Wiederholung und Feinabstimmung. wir verwenden zwei quantitative Metriken, um die Erzeugungsqualität und die Speicherfähigkeit zu bewerten. Wir experimentieren mit sequentiellen Aufgaben auf drei häufig verwendeten Benchmarks für kontinuierliches Lernen (MNIST, Fashion MNIST und CIFAR10) und stellen fest, dass unter allen Modellen das ursprüngliche GAN am besten abschneidet und unter den Strategien des kontinuierlichen Lernens die generative Wiederholung alle anderen Methoden übertrifft, auch wenn wir zufriedenstellende Kombinationen auf MNIST und Fashion MNIST gefunden haben.
Ausgehend von Daten, die durch die aktuelle Strategie generiert wurden, formuliert und löst SPU ein eingeschränktes Optimierungsproblem im nicht-parametrisierten proximalen Strategieraum und konvertiert die optimale nicht-parametrisierte Strategie mithilfe von überwachter Regression in eine parametrisierte Strategie, aus der dann neue Proben gezogen werden. Die Methode ist insofern allgemein, als sie sowohl auf diskrete als auch auf kontinuierliche Aktionsräume anwendbar ist und eine Vielzahl von Proximitätsbeschränkungen für das nicht-parametrisierte Optimierungsproblem handhaben kann. wir zeigen, wie die Probleme Natural Policy Gradient und Trust Region Policy Optimization (NPG/TRPO) sowie das Problem Proximal Policy Optimization (PPO) mit dieser Methode behandelt werden können. Die SPU-Implementierung ist viel einfacher als TRPO, und unsere umfangreichen Experimente zeigen, dass SPU TRPO in simulierten Mujoco-Roboter-Aufgaben und PPO in Atari-Videospiel-Aufgaben übertrifft, was die Effizienz der Stichprobe angeht.
Jüngste Arbeiten zur Modellierung neuronaler Reaktionen im visuellen System von Primaten haben von tiefen neuronalen Netzen profitiert, die für die Erkennung von Objekten in großem Maßstab trainiert wurden, und haben eine hierarchische Übereinstimmung zwischen den Schichten des künstlichen neuronalen Netzes und den Gehirnbereichen entlang des ventralen Sehstroms gefunden. Wir wissen jedoch weder, ob solche aufgabenoptimierten Netzwerke ebenso gute Modelle des visuellen Systems von Nagetieren ermöglichen, noch, ob eine ähnliche hierarchische Übereinstimmung besteht. Hier gehen wir diese Fragen im visuellen System der Maus an, indem wir Merkmale auf mehreren Schichten eines auf ImageNet trainierten neuronalen Faltungsnetzwerks (CNN) extrahieren, um die Reaktionen von Tausenden von Neuronen in vier visuellen Bereichen (V1, LM, AL, RL) auf natürliche Bilder vorherzusagen. Wir fanden heraus, dass die CNN-Merkmale die klassischen Untereinheiten-Energiemodelle übertreffen, fanden aber keine Hinweise auf eine Ordnung der Bereiche, die wir über eine Korrespondenz mit der Hierarchie der CNN-Schichten aufgezeichnet haben, und dass dasselbe CNN, aber mit zufälligen Gewichten, einen gleichwertigen Merkmalsraum für die Vorhersage neuronaler Reaktionen bietet. Unsere Ergebnisse deuten darauf hin, dass die Objekterkennung als High-Level-Aufgabe nicht mehr diskriminierende Merkmale zur Charakterisierung des visuellen Systems der Maus liefert als ein zufälliges Netzwerk. im Gegensatz zu Primaten ist das Training ethologisch relevanter visuell geführter Verhaltensweisen - über die statische Objekterkennung hinaus - möglicherweise erforderlich, um die funktionelle Organisation des visuellen Kortex der Maus zu enthüllen.
Der Reparametrisierungstrick ist zu einem der nützlichsten Werkzeuge im Bereich der Variationsinferenz geworden, basiert jedoch auf der Standardisierungstransformation, die den Anwendungsbereich dieser Methode auf Verteilungen beschränkt, die nachvollziehbare inverse kumulative Verteilungsfunktionen haben oder als deterministische Transformationen solcher Verteilungen ausgedrückt werden können.In diesem Beitrag haben wir den Reparametrisierungstrick verallgemeinert, indem wir eine allgemeine Transformation zulassen. Auf der Grundlage des vorgeschlagenen Gradientenmodells schlagen wir einen neuen polynomialbasierten Gradientenschätzer vor, der unter bestimmten Bedingungen eine bessere theoretische Leistung als der Reparametrisierungstrick aufweist und auf eine größere Klasse von Variationsverteilungen angewendet werden kann.In Studien mit synthetischen und realen Daten zeigen wir, dass der von uns vorgeschlagene Gradientenschätzer eine signifikant geringere Gradientenvarianz als andere State-of-the-Art-Methoden aufweist und somit ein schnelleres Inferenzverfahren ermöglicht.
Zur gleichzeitigen Erfassung von Syntax und Semantik aus einem Textkorpus schlagen wir ein neues Sprachmodell mit größerem Kontext vor, das eine wiederkehrende hierarchische semantische Struktur über ein dynamisches tiefes Themenmodell extrahiert, um die Erzeugung natürlicher Sprache zu steuern. Experimentelle Ergebnisse auf einer Vielzahl von realen Textkorpora zeigen, dass das vorgeschlagene Modell nicht nur die modernsten Sprachmodelle mit größeren Kontexten übertrifft, sondern auch interpretierbare rekurrente mehrschichtige Themen erlernt und verschiedene Sätze und Absätze generiert, die syntaktisch korrekt und semantisch kohärent sind.
Wir stellen einen neuartigen Ansatz vor, um ein natürliches Mediengemälde mit Hilfe von Reinforcement Learning zu trainieren: Ausgehend von einem Referenzbild basiert unsere Formulierung auf einem strichbasierten Rendering, das menschliches Zeichnen imitiert und von Grund auf ohne Aufsicht erlernt werden kann.Unser Malagent berechnet eine Abfolge von Aktionen, die die primitiven Malstriche darstellen. Um sicherzustellen, dass die generierte Strategie vorhersehbar und kontrollierbar ist, verwenden wir eine Methode des eingeschränkten Lernens und trainieren den Malagenten anhand des Umgebungsmodells und folgen den Befehlen, die in einer Beobachtung kodiert sind.Wir haben unseren Ansatz auf viele Benchmarks angewandt und unsere Ergebnisse zeigen, dass unser eingeschränkter Agent mit verschiedenen Malmedien und verschiedenen Einschränkungen im Aktionsraum umgehen kann, um mit Menschen oder anderen Agenten zusammenzuarbeiten.
Bisher erfordern die einzigen Verfahren, die sich explizit mit Wahnvorstellungen befassen, eine umfassende Suche unter Verwendung tabellarischer Wertschätzungen. In diesem Papier entwickeln wir effiziente Methoden, um Wahnvorstellungen durch das Training von Q-Approximatoren mit Etiketten zu verringern, die mit der zugrunde liegenden gierigen Politikklasse "konsistent" sind. Wir führen ein einfaches Bestrafungsschema ein, das Q-Labels, die über Trainingsstapel hinweg verwendet werden, dazu ermutigt, (gemeinsam) mit der aussprechbaren Politikklasse konsistent zu bleiben.Wir schlagen auch einen Suchrahmen vor, der es ermöglicht, mehrere Q-Approximatoren zu generieren und zu verfolgen und so die Auswirkungen von verfrühten (impliziten) Politikverpflichtungen abzuschwächen.Experimentelle Ergebnisse zeigen, dass diese Methoden die Leistung des Q-Lernens in einer Vielzahl von Atari-Spielen verbessern können, manchmal dramatisch.
Das Papier schlägt vor und demonstriert eine Deep Convolutional Neural Network (DCNN)-Architektur zur Identifizierung von Benutzern mit verkleideten Gesichtern, die versuchen, eine betrügerische ATM-Transaktion.Die jüngste Einführung von Disguised Face Identification (DFI) Rahmen beweist die Anwendbarkeit von tiefen neuronalen Netzen für dieses Problem.Alle Geldautomaten heutzutage eine versteckte Kamera in ihnen und erfassen die Aufnahmen von ihren Nutzern.Allerdings ist es unmöglich für die Polizei auf die Spur der Betrüger mit verkleideten Gesichtern aus dem ATM-Material. Das vorgeschlagene Deep Convolutional Neural Network wird trainiert, um in Echtzeit zu erkennen, ob der Benutzer im aufgenommenen Bild versucht, seine Identität zu verschleiern oder nicht.Die Ausgabe des DCNN wird dann an den Geldautomaten gemeldet, um entsprechende Schritte zu unternehmen und den Betrüger daran zu hindern, die Transaktion abzuschließen.Das Netzwerk wird anhand eines Datensatzes von Bildern trainiert, die in ähnlichen Situationen wie bei einem Geldautomaten aufgenommen wurden.Die vergleichsweise geringe Hintergrundunschärfe in den Bildern ermöglicht es dem Netzwerk, eine hohe Genauigkeit bei der Merkmalsextraktion und Klassifizierung für alle verschiedenen Verkleidungen zu zeigen.
Wenn die Aktivierungsfunktion linear ist und die Kodierungsdimension (Breite der versteckten Schicht) kleiner als die Eingabedimension ist, ist bekannt, dass der Auto-Encoder optimiert ist, um die Hauptkomponenten der Datenverteilung zu lernen (Oja 1982). Wenn die Aktivierung jedoch nichtlinear ist und die Breite größer als die Eingabedimension ist (überkomplett), verhält sich der Auto-Encoder anders als die PCA, und es ist bekannt, dass er empirisch gut für spärliche Kodierungsprobleme funktioniert. Wir liefern eine theoretische Erklärung für dieses empirisch beobachtete Phänomen, wenn die gleichgerichtete lineare Einheit (ReLu) als Aktivierungsfunktion angenommen wird und die Breite der versteckten Schicht groß ist. In diesem Fall zeigen wir, dass die Initialisierung der Gewichtsmatrix eines Autoencoders durch Sampling aus einer sphärischen Gauß-Verteilung, gefolgt von stochastischem Gradientenabstiegstraining (SGD), mit signifikanter Wahrscheinlichkeit zur Grundwahrheitsdarstellung für eine Klasse von Lernmodellen für spärliche Wörterbücher konvergiert. Darüber hinaus können wir zeigen, dass unter der Bedingung der Konvergenz die erwartete Konvergenzrate O(1/t) ist, wobei t die Anzahl der Aktualisierungen ist. Unsere Analyse quantifiziert, wie eine Erhöhung der Breite der versteckten Schicht die Trainingsleistung verbessert, wenn eine zufällige Initialisierung verwendet wird, und wie die Norm der Netzwerkgewichte die Geschwindigkeit der SGD-Konvergenz beeinflusst.
Hierarchische Agenten haben das Potenzial, sequenzielle Entscheidungsfindungsaufgaben mit größerer Stichprobeneffizienz zu lösen als ihre nicht-hierarchischen Gegenstücke, da hierarchische Agenten Aufgaben in Gruppen von Teilaufgaben zerlegen können, die nur kurze Entscheidungssequenzen erfordern.  Um dieses Potenzial des schnelleren Lernens auszuschöpfen, müssen hierarchische Agenten in der Lage sein, ihre verschiedenen Ebenen von Strategien parallel zu lernen, damit diese einfacheren Teilprobleme gleichzeitig gelöst werden können.  Das parallele Lernen mehrerer Ebenen von Strategien ist jedoch schwierig, da es inhärent instabil ist: Änderungen in einer Strategie auf einer Ebene der Hierarchie können Änderungen in den Übergangs- und Belohnungsfunktionen auf höheren Ebenen in der Hierarchie verursachen, was das gemeinsame Lernen mehrerer Ebenen von Strategien erschwert.  In diesem Beitrag stellen wir ein neues hierarchisches Reinforcement Learning (HRL) Framework vor, Hierarchical Actor-Critic (HAC), das die Instabilitätsprobleme überwinden kann, die auftreten, wenn Agenten versuchen, gemeinsam mehrere Ebenen von Strategien zu lernen.  Die Hauptidee hinter HAC ist es, jede Ebene der Hierarchie unabhängig von den unteren Ebenen zu trainieren, indem jede Ebene so trainiert wird, als ob die Strategien der unteren Ebenen bereits optimal wären.  Wir demonstrieren experimentell sowohl in der Netzwelt als auch in simulierten Roboterdomänen, dass unser Ansatz das Lernen im Vergleich zu anderen nicht-hierarchischen und hierarchischen Methoden erheblich beschleunigen kann.  In der Tat ist unser System das erste, das erfolgreich 3-stufige Hierarchien parallel in Aufgaben mit kontinuierlichen Zustands- und Aktionsräumen lernt.
Positiv-unbeschriftetes (PU) Lernen befasst sich mit dem Problem des Lernens eines binären Klassifikators aus positiven (P) und unbeschrifteten (U) Daten und wird oft in Situationen angewandt, in denen negative (N) Daten nur schwer vollständig beschriftet werden können, aber das Sammeln einer nicht repräsentativen N-Menge, die nur einen kleinen Teil aller möglichen N-Daten enthält, kann in vielen praktischen Situationen viel einfacher sein. In diesem Beitrag wird ein neuartiger Klassifizierungsrahmen untersucht, der solche verzerrten N-Daten (bN) in das PU-Lernen einbezieht. Unser Ansatz kann als eine Variante traditioneller Algorithmen zur Gewichtung von Beispielen betrachtet werden, wobei die Gewichtung jedes Beispiels durch einen vorbereitenden Schritt berechnet wird, der vom PU-Lernen inspiriert ist.Wir leiten auch eine Schätzfehlergrenze für die vorgeschlagene Methode ab.Experimentelle Ergebnisse zeigen die Effektivität unseres Algorithmus nicht nur in PUbN-Lernszenarien, sondern auch in gewöhnlichen PU-Lernszenarien auf mehreren Benchmark-Datensätzen.
Verstärkungslernen (Reinforcement Learning, RL) wird häufig eingesetzt, um die Leistung bei der Texterzeugung, einschließlich der maschinellen Übersetzung (MT), zu steigern, insbesondere durch den Einsatz von Minimum Risk Training (MRT) und Generative Adversarial Networks (GAN). Es ist jedoch nur wenig darüber bekannt, was und wie diese Methoden im Zusammenhang mit der maschinellen Übersetzung lernen. Wir weisen nach, dass eine der gängigsten RL-Methoden für die MÜ die erwartete Belohnung nicht optimiert und zeigen, dass andere Methoden zu lange brauchen, um zu konvergieren. Unsere Ergebnisse deuten darauf hin, dass RL-Verfahren in der MÜ die Leistung nur dann verbessern, wenn die vortrainierten Parameter bereits nahe an der korrekten Übersetzung liegen.
Seit Anfang 2018 wurden erhebliche Fortschritte in der Modellierung natürlicher Sprachverarbeitung (NLP) gemacht, die es ermöglichen, auch bei wenig gelabelten Daten genaue Ergebnisse zu erzielen, da diese NLP-Modelle vom Training mit aufgabenunabhängigen und aufgabenspezifischen, nicht gelabelten Daten profitieren können, was jedoch mit erheblichen Größen- und Rechenkosten verbunden ist. Dieses Workshop-Papier beschreibt, wie die von uns vorgeschlagene faltige Studentenarchitektur, die durch einen Destillationsprozess aus einem groß angelegten Modell trainiert wurde, eine 300-fache Beschleunigung der Inferenz und eine 39-fache Reduzierung der Parameteranzahl erreichen kann.
Die Kombination mehrerer Funktionsapproximatoren in Modellen des maschinellen Lernens führt in der Regel zu einer besseren Leistung und Robustheit im Vergleich zu einer einzelnen Funktion. Beim Reinforcement Learning sind Ensemble-Algorithmen wie eine Mittelwertbildungsmethode und eine Mehrheitsabstimmungsmethode nicht immer optimal, da jede Funktion grundlegend unterschiedliche optimale Trajektorien aus der Exploration lernen kann. Der Vorteil dieses Algorithmus ist, dass er die Ensemble-Leistung verbessert, indem er die Gewichte der Q-Funktionen reduziert, die mit den aktuellen Trajektorien nicht vertraut sind. Wir liefern experimentelle Ergebnisse für Gridworld- und Atari-Aufgaben, die im Vergleich zu den Basis-Algorithmen deutliche Leistungsverbesserungen zeigen.
Dieses Papier stellt die Behaviour Suite for Reinforcement Learning, oder kurz bsuite, vor. bsuite ist eine Sammlung von sorgfältig entworfenen Experimenten, die Kernfähigkeiten von Reinforcement Learning (RL)-Agenten mit zwei Zielen untersuchen: Erstens, klare, informative und skalierbare Probleme zu sammeln, die Schlüsselaspekte im Design von allgemeinen und effizienten Lernalgorithmen erfassen, und zweitens, das Verhalten von Agenten durch ihre Leistung auf diesen gemeinsamen Benchmarks zu studieren.Um diese Bemühungen zu vervollständigen, stellen wir diese http-URL als Open Source zur Verfügung, die die Auswertung und Analyse jedes Agenten auf bsuite automatisiert. Diese Bibliothek erleichtert die reproduzierbare und zugängliche Forschung zu den Kernproblemen von RL und letztlich die Entwicklung besserer Lernalgorithmen. bsuite ist in Python geschrieben und lässt sich leicht in bestehende Projekte einbinden. bsuite enthält Beispiele für OpenAI Baselines, Dopamine und neue Referenzimplementierungen. bsuite soll in Zukunft noch mehr hervorragende Experimente aus der Forschungsgemeinschaft enthalten.
Sequenz-zu-Sequenz-Aufmerksamkeitsmodelle sind ein vielversprechender Ansatz für die End-to-End-Spracherkennung. Die erhöhte Modellleistung erschwert das Trainingsverfahren, und die Analyse von Fehlermodi dieser Modelle wird aufgrund der End-to-End-Natur schwieriger. In dieser Arbeit stellen wir verschiedene Analysen vor, um die Trainings- und Modelleigenschaften besser zu verstehen. Um ein besseres Verständnis der Funktionsweise des Aufmerksamkeitsprozesses zu erlangen, untersuchen wir den Encoder-Output sowie die Aufmerksamkeitsenergien und -gewichte. Unsere Experimente wurden mit Switchboard, LibriSpeech und Wall Street Journal durchgeführt.
Simulationen sind oft entweder zu einfach, um eine robuste Validierung zu ermöglichen, oder zu komplex, um sie nachvollziehbar zu berechnen. Daher werden approximative Validierungsmethoden benötigt, um Fehler ohne unsichere Vereinfachungen nachvollziehbar zu finden.In diesem Beitrag wird die Theorie hinter einem solchen Black-Box-Ansatz vorgestellt: adaptive Stresstests (AST).Wir geben auch drei Beispiele für Validierungsprobleme, die so formuliert wurden, dass sie mit AST funktionieren.
Mehrstufige gierige Strategien wurden ausgiebig im modellbasierten Reinforcement Learning (RL) und in Fällen, in denen ein Modell der Umgebung verfügbar ist (z.B. im Go-Spiel), verwendet, In dieser Arbeit erforschen wir die Vorteile von mehrstufigen gierigen Strategien im modellfreien RL, wenn sie im Rahmen der mehrstufigen dynamischen Programmierung (DP) eingesetzt werden: mehrstufige Policy- und Value-Iteration Diese Algorithmen lösen iterativ Entscheidungsprobleme mit kurzem Horizont und konvergieren zur optimalen Lösung des ursprünglichen Problems. Durch die Verwendung von modellfreien Algorithmen als Löser von Kurzhorizontproblemen leiten wir vollständig modellfreie Algorithmen ab, die Instanzen des mehrschrittigen DP-Rahmens sind. Da modellfreie Algorithmen anfällig für Instabilitäten bezüglich des Entscheidungsproblemhorizonts sind, kann dieser einfache Ansatz dazu beitragen, diese Instabilitäten zu mildern und führt zu verbesserten modellfreien Algorithmen.
Das von Madry et al. (2018) vorgeschlagene adversarische Trainingsverfahren ist eine der effektivsten Methoden zur Verteidigung gegen adversarische Beispiele in tiefen neuronalen Netzen (DNNs). In unserem Beitrag beleuchten wir die Praktikabilität und die Härte des adversen Trainings, indem wir zeigen, dass die Effektivität (Robustheit im Testset) des adversen Trainings stark mit dem Abstand zwischen einem Testpunkt und der Mannigfaltigkeit der vom Netzwerk eingebetteten Trainingsdaten korreliert ist.Testbeispiele, die relativ weit von dieser Mannigfaltigkeit entfernt sind, sind anfälliger für adverse Angriffe. Folglich ist eine auf adversarialem Training basierende Verteidigung anfällig für eine neue Klasse von Angriffen, den "Blind-Spot-Angriff", bei dem sich die Eingabebilder in "Blind-Spots" (Regionen mit geringer Dichte) der empirischen Verteilung der Trainingsdaten befinden, aber immer noch auf der Grundwahrheitsdaten-Mannigfaltigkeit liegen.für MNIST haben wir herausgefunden, dass diese Blind-Spots leicht durch einfaches Skalieren und Verschieben von Bildpixelwerten gefunden werden können. Vor allem bei großen Datensätzen mit hochdimensionalen und komplexen Daten (CIFAR, ImageNet, etc.) macht die Existenz von Blindspots im gegnerischen Training die Verteidigung bei allen gültigen Testbeispielen aufgrund des Fluchs der Dimensionalität und der Knappheit der Trainingsdaten schwierig.Darüber hinaus stellen wir fest, dass Blindspots auch bei beweisbaren Verteidigungsmaßnahmen wie (Kolter & Wong, 2018) und (Sinha et al, 2018), da diese trainierbaren Robustheitszertifikate nur auf einem begrenzten Satz von Trainingsdaten praktisch optimiert werden können.
Kantenintelligenz, insbesondere binäre neuronale Netze (BNN), haben in letzter Zeit große Aufmerksamkeit in der Gemeinschaft der künstlichen Intelligenz auf sich gezogen.BNNs reduzieren die Rechenkosten, die Modellgröße und den Speicherbedarf erheblich.  Allerdings gibt es immer noch eine Leistungslücke zwischen dem erfolgreichen vollpräzisen neuronalen Netz mit ReLU-Aktivierung und BNNs. Wir argumentieren, dass der Genauigkeitsverlust von BNNs auf ihre Geometrie zurückzuführen ist. Wir analysieren das Verhalten des vollpräzisen neuronalen Netzes mit ReLU-Aktivierung und vergleichen es mit seinem binarisierten Gegenstück. dieser Vergleich schlägt eine zufällige Initialisierung der Verzerrung als Mittel gegen die Aktivierungssättigung in vollpräzisen Netzen vor und führt uns zu einem verbesserten BNN-Training. unsere numerischen Experimente bestätigen unsere geometrische Intuition.
Das Verständnis, wie Menschen Kategorien darstellen, ist ein Kernproblem in der Kognitionswissenschaft, wobei die Flexibilität des menschlichen Lernens ein Goldstandard bleibt, den moderne künstliche Intelligenz und maschinelles Lernen anstreben.Jahrzehnte der psychologischen Forschung haben eine Vielzahl von formalen Theorien von Kategorien hervorgebracht, doch die Validierung dieser Theorien mit naturalistischen Stimuli bleibt eine Herausforderung.Das Problem ist, dass menschliche Kategoriedarstellungen nicht direkt beobachtet werden können und die Durchführung von informativen Experimenten mit naturalistischen Stimuli wie Bildern eine praktikable Darstellung dieser Stimuli erfordert. In diesem Beitrag stellen wir eine Methode zur Schätzung der Struktur menschlicher Kategorien vor, die sich auf Ideen aus der Kognitionswissenschaft und dem maschinellen Lernen stützt und menschenbasierte Algorithmen mit hochmodernen tiefen Repräsentationslernern verbindet. Wir liefern qualitative und quantitative Ergebnisse als Beweis für die Machbarkeit der Methode.
Die Anwendung von tiefen rekurrenten Netzwerken auf die Audiotranskription hat zu beeindruckenden Fortschritten in der automatischen Spracherkennung (ASR) geführt. Viele haben gezeigt, dass kleine Störungen tiefe neuronale Netzwerke dazu bringen können, ein bestimmtes Ziel mit hoher Zuverlässigkeit falsch vorherzusagen. Aktuelle Arbeiten zur Täuschung von ASR-Systemen haben sich auf White-Box-Angriffe konzentriert, bei denen die Modellarchitektur und die Parameter bekannt sind. In diesem Papier verfolgen wir einen Black-Box-Ansatz zur Generierung von Gegenspielern und kombinieren die Ansätze von genetischen Algorithmen und Gradientenschätzung, um die Aufgabe zu lösen. 89,25 % Ähnlichkeit des Ziels des Angriffs erreichen wir nach 3000 Generationen, während 94,6 % Ähnlichkeit der Audiodatei erhalten bleiben.
Jüngste Fortschritte in der physikbasierten Charakteranimation haben beeindruckende Durchbrüche in der menschlichen Bewegungssynthese durch die Imitation von Motion-Capture-Daten über tiefes Verstärkungslernen gezeigt, aber die Ergebnisse wurden meist auf die Imitation eines einzelnen eindeutigen Bewegungsmusters demonstriert und lassen sich nicht auf interaktive Aufgaben verallgemeinern, die flexible Bewegungsmuster aufgrund variierender räumlicher Konfigurationen zwischen Mensch und Objekt erfordern.Um diese Lücke zu schließen, konzentrieren wir uns auf eine Klasse interaktiver Aufgaben - das Sitzen auf einem Stuhl. Wir schlagen ein hierarchisches Reinforcement Learning Framework vor, das sich auf eine Sammlung von Subtask Controllern stützt, die trainiert werden, um einfache, wiederverwendbare Mocap-Bewegungen zu imitieren, und einen Meta-Controller, der trainiert wird, um die Subtasks richtig auszuführen, um die Hauptaufgabe zu erfüllen.Wir demonstrieren experimentell die Stärke unseres Ansatzes gegenüber verschiedenen einstufigen und hierarchischen Baselines.Wir zeigen auch, dass unser Ansatz auf die Bewegungsvorhersage anhand eines Bildeingangs angewendet werden kann.Ein Video-Highlight finden Sie unter https://youtu.be/XWU3wzz1ip8/.
Wir schlagen vor, die GAN-Trainingsdynamik als Regret-Minimierung zu untersuchen, was im Gegensatz zu der weit verbreiteten Ansicht steht, dass es sich um eine konsistente Minimierung einer Divergenz zwischen realen und generierten Verteilungen handelt.Wir analysieren die Konvergenz des GAN-Trainings aus diesem neuen Blickwinkel, um zu verstehen, warum es zu einem Modus-Kollaps kommt.Wir stellen die Hypothese auf, dass die Existenz von unerwünschten lokalen Gleichgewichten in diesem nicht-konvexen Spiel für den Modus-Kollaps verantwortlich ist. Wir zeigen, dass diese degenerierten lokalen Gleichgewichte mit einem Gradientenstrafschema namens DRAGAN vermieden werden können. Wir zeigen, dass DRAGAN ein schnelleres Training ermöglicht, eine verbesserte Stabilität mit weniger Moduszusammenbrüchen erreicht und zu Generatorennetzwerken mit besserer Modellierungsleistung bei einer Vielzahl von Architekturen und Zielfunktionen führt.
Tiefe neuronale Netze (DNNs) haben im letzten Jahrzehnt aufgrund der Vorteile des automatischen Merkmalslernens und der Ausdrucksfreiheit überraschende Erfolge erzielt. Ihre Interpretierbarkeit bleibt jedoch rätselhaft, da DNNs komplexe Kombinationen von linearen und nichtlinearen Transformationen sind. Obwohl viele Modelle vorgeschlagen wurden, um die Interpretierbarkeit von DNNs zu erforschen, bleiben mehrere Herausforderungen ungelöst: 1) das Fehlen von Interpretierbarkeitsmengenmaßen für DNNs, 2) das Fehlen einer Theorie für die Stabilität von DNNs und3) die Schwierigkeit, nichtkonvexe DNN-Probleme mit Interpretierbarkeitsbeschränkungen zu lösen. Um diese Herausforderungen gleichzeitig anzugehen, stellt dieses Papier einen neuartigen intrinsischen Interpretierbarkeitsbewertungsrahmen für DNNs vor, und zwar werden vier unabhängige Eigenschaften der Interpretierbarkeit auf der Grundlage bestehender Arbeiten definiert. Schließlich wird eine erweiterte Version von Deep Learning Alternating Direction Method of Multipliers (dlADMM) vorgeschlagen, um DNN-Probleme mit Interpretierbarkeitsbeschränkungen effizient und genau zu lösen.
Reinforcement Learning (RL) definiert typischerweise einen Diskontierungsfaktor als Teil des Markov Decision Process.  Der Abzinsungsfaktor bewertet künftige Belohnungen nach einem exponentiellen Schema, das zu theoretischen Konvergenzgarantien der Bellman-Gleichung führt. Die Erkenntnisse aus Psychologie, Wirtschaft und Neurowissenschaften legen jedoch nahe, dass Menschen und Tiere stattdessen hyperbolische Zeitpräferenzen haben.  Hier erweitern wir frühere Arbeiten von Kurth-Nelson und Redish und schlagen einen effizienten Deep Reinforcement Learning-Agenten vor, der über hyperbolische Diskontierungs- und andere nicht-exponentielle Diskontierungsmechanismen agiert. Wir zeigen, dass ein einfacher Ansatz hyperbolische Diskontierungsfunktionen annähert, während er gleichzeitig vertraute Zeitdifferenz-Lerntechniken in RL verwendet.  Zusätzlich und unabhängig von der hyperbolischen Diskontierung machen wir die überraschende Entdeckung, dass das gleichzeitige Lernen von Wertfunktionen über mehrere Zeithorizonte eine effektive Hilfsaufgabe ist, die oft besser ist als moderne Methoden.
Traditionelle Ansätze der Emotionserkennung basieren auf Gesichtsbildern, Messungen der Herzfrequenz, des Blutdrucks, der Temperatur, des Tons der Stimme/Sprache usw. Diese Merkmale können jedoch potenziell in falsche Merkmale umgewandelt werden, so dass zur Erkennung versteckter und echter Merkmale, die nicht von der Person gesteuert werden, Daten aus Gehirnsignalen gemessen werden: Das Hauptziel dieser Studie ist die Erkennung von Emotionen auf der Grundlage der Analyse von EEG-Signalen, die vom Gehirn als Reaktion auf visuelle Stimuli aufgezeichnet wurden. 11 gesunden Zielpersonen wurden ausgewählte visuelle Stimuli dargeboten, und die EEG-Signale wurden unter kontrollierten Bedingungen aufgezeichnet, um Artefakte (Muskel- und/oder Augenbewegungen) zu minimieren.  Die Signale wurden gefiltert und die Art des Frequenzbandes wurde berechnet und erkannt.die vorgeschlagene Methode sagt einen Emotionstyp (positiv/negativ) als Reaktion auf die dargebotenen Stimuli voraus.schließlich wurde die Leistung des vorgeschlagenen Ansatzes getestet.die durchschnittliche Genauigkeit der maschinellen Lernalgorithmen (d.h. J48, Bayes Net, Adaboost und Random Forest) sind 78,86, 74,76, 77,82 bzw. 82,46.  In dieser Studie haben wir auch EEG-Anwendungen im Kontext des Neuro-Marketings angewandt. Die Ergebnisse zeigen empirisch die Erkennung der bevorzugten Farbpräferenz von Kunden als Reaktion auf die Logofarbe eines Unternehmens oder einer Dienstleistung.
  Wir präsentieren einen probabilistischen Rahmen für sitzungsbasierte Empfehlungen.  Eine latente Variable für den Benutzerzustand wird aktualisiert, wenn der Benutzer mehr Artikel ansieht und wir mehr über seine Interessen erfahren.  Wir bieten rechnerische Lösungen an, die sowohl den Re-Parametrisierungstrick als auch die Bouchard-Schranke für die Softmax-Funktion verwenden. Außerdem untersuchen wir den Einsatz eines Variations-Autocodierers und eines Variations-Expectation-Maximization-Algorithmus zur Verschärfung der Variations-Schranke.  Schließlich zeigen wir, dass die Bouchard-Schranke dazu führt, dass der Nenner der Softmax-Funktion in eine Summe zerfällt, die schnelle verrauschte Gradienten der Schranke ermöglicht, wodurch ein vollständig probabilistischer Algorithmus entsteht, der an word2vec und einen schnellen Online-EM-Algorithmus erinnert.
Eine wichtige Frage beim Aufgabentransferlernen ist die Bestimmung der Übertragbarkeit von Aufgaben, d.h. bei einer gemeinsamen Eingabedomäne die Abschätzung, inwieweit Repräsentationen, die von einer Ausgangsaufgabe gelernt wurden, beim Lernen einer Zielaufgabe helfen können.Typischerweise wird die Übertragbarkeit entweder experimentell gemessen oder über die Aufgabenverwandtschaft abgeleitet, die oft ohne klare operative Bedeutung definiert wird.In diesem Beitrag stellen wir eine neuartige Metrik vor, den H-Score, eine einfach zu berechnende Bewertungsfunktion, die die Leistung von übertragenen Repräsentationen von einer Aufgabe zu einer anderen in Klassifikationsproblemen abschätzt. Inspiriert von einem prinzipiellen informationstheoretischen Ansatz, hat H-score eine direkte Verbindung zur asymptotischen Fehlerwahrscheinlichkeit der Entscheidungsfunktion, die auf dem übertragenen Merkmal basiert.Diese Formulierung der Übertragbarkeit kann weiter verwendet werden, um eine geeignete Menge von Quellaufgaben in Aufgaben-Transfer-Lernproblemen auszuwählen oder um effiziente Transfer-Lernstrategien zu entwickeln.Experimente mit synthetischen und realen Bilddaten zeigen, dass nicht nur unsere Formulierung der Übertragbarkeit in der Praxis sinnvoll ist, sondern dass sie auch auf Inferenz-Probleme jenseits der Klassifikation verallgemeinert werden kann, wie z.B. Erkennungsaufgaben für das 3D-Innenraum-Szenenverständnis.
In diesem Beitrag wird ein allgemeiner Rahmen vorgestellt, um das entscheidende Problem der Klasseninkongruenz bei der unbeaufsichtigten Domänenanpassung (UDA) für Mehrklassenverteilungen zu bewältigen.  Bisherige adversarische Lernmethoden konditionieren die Domänenanpassung nur auf Pseudo-Etiketten, aber verrauschte und ungenaue Pseudo-Etiketten können die in probabilistischen Vorhersagen eingebettete Multi-Klassen-Verteilung stören und somit das latente Mismatch-Problem nur unzureichend lindern.  Im Vergleich zu Pseudo-Etiketten sind Klassenprototypen genauer und zuverlässiger, da sie alle Instanzen zusammenfassen und in der Lage sind, die inhärente semantische Verteilung zu repräsentieren, die über Domänen hinweg geteilt wird. Daher schlagen wir ein neuartiges Prototype-Assisted Adversarial Learning (PAAL) Schema vor, das probabilistische Instanzvorhersagen und Klassenprototypen zusammen einbezieht, um zuverlässige Indikatoren für adversariales Domain Alignment zu liefern.   Mit dem PAAL-Schema gleichen wir sowohl die Repräsentationen der Instanzmerkmale als auch die Repräsentationen der Klassenprototypen ab, um die Diskrepanz zwischen semantisch unterschiedlichen Klassen zu verringern.   Außerdem nutzen wir die Klassenprototypen als Proxy, um die klasseninterne Varianz in der Zieldomäne zu minimieren und so die Diskrepanz zwischen semantisch ähnlichen Klassen zu mildern.  Wir demonstrieren die gute Leistung und Generalisierbarkeit des PAAL-Schemas und des PACDA-Frameworks an zwei UDA-Aufgaben, nämlich der Objekterkennung (Office-Home, ImageCLEF-DA und Office) und der synthetisch-realen semantischen Segmentierung (GTA5→CityscapesundSynthia→Cityscapes).
Wir stellen eine neue Methodik vor, die eine Familie von \emph{positive definite kernels} aus jedem gegebenen Unähnlichkeitsmaß auf strukturierten Eingaben konstruiert, deren Elemente entweder reellwertige Zeitreihen oder diskrete Strukturen wie Zeichenketten, Histogramme und Graphen sind. Unser Ansatz, den wir D2KE (von Distance to Kernel and Embedding) nennen, lehnt sich an die Literatur über zufällige Merkmale an, doch anstatt zufällige Merkmalskarten aus einem benutzerdefinierten Kernel abzuleiten, um Kernelmaschinen zu approximieren, bauen wir einen Kernel aus einer zufälligen Merkmalskarte auf, die wir anhand des Distanzmaßes spezifizieren. Des Weiteren schlagen wir die Verwendung einer endlichen Anzahl von Zufallsobjekten vor, um eine zufällige Merkmalseinbettung jeder Instanz zu erzeugen. Wir liefern eine theoretische Analyse, die zeigt, dass D2KE eine bessere Verallgemeinerbarkeit aufweist als universelle Nearest-Neighbor-Schätzungen. Einerseits fasst D2KE die weit verbreitete \emph{representative-set method} als Spezialfall zusammen und bezieht sich in einem Grenzfall auf den bekannten \emph{distance substitution kernel}. Andererseits verallgemeinert D2KE die bestehenden \emph{Random Features-Methoden}, die nur auf vektorielle Eingabedarstellungen anwendbar sind, auf komplex strukturierte Eingaben mit variablen Größen. Wir führen Klassifizierungsexperimente für so unterschiedliche Bereiche wie Zeitreihen, Zeichenketten und Histogramme (für Texte und Bilder) durch, für die unser vorgeschlagener Rahmen sowohl in Bezug auf die Testgenauigkeit als auch auf die Rechenzeit mit bestehenden entfernungsbasierten Lernmethoden vergleichbar ist.
Tiefe neuronale Faltungsnetzwerke (Deep Convolution Neural Networks, CNNs), die ihren Ursprung in der Pionierarbeit von \cite{Hinton1986,LeCun1985,Alex2012} haben und in \cite{LeCunBengioHinton2015} zusammengefasst sind, haben sich als sehr nützlich in einer Vielzahl von Bereichen erwiesen.  Die modernsten CNN-Maschinen, wie z. B. das Image Rest Net \cite{He_2016_CVPR}, werden durch Eingaben mit reellen Werten und Kernel-Faltungen beschrieben, gefolgt von lokalen und nicht-linearen gleichgerichteten linearen Ausgaben.  Die Rolle dieser Schichten, ihre Genauigkeit und ihre Grenzen zu verstehen und sie effizienter zu machen (weniger Parameter), sind allesamt aktuelle Forschungsfragen.   Inspiriert von der Quantentheorie schlagen wir die Verwendung von Kernel-Funktionen mit komplexem Wert vor, gefolgt von einem lokalen nichtlinearen absoluten (Modulus-)Operator im Quadrat. Wir untersuchen ein konkretes Problem der Formerkennung und zeigen, dass eine Faltungsschicht mit quanteninspirierten komplexen Kernen das statistische/klassische Kernel-Gegenstück und einen "Bayes'schen Formschätzer" übertrifft, wenn mehrere sich überschneidende Formen deformiert und/oder Störgeräusche hinzugefügt werden, was auf das Quantenphänomen der Interferenz zurückzuführen ist, das in klassischen CNNs nicht vorhanden ist.  
Wir stellen eine Forschungsplattform für künstliche Intelligenz vor, die vom Genre der MMORPGs (Massively Multiplayer Online Role-Playing Games, auch bekannt als MMOs) inspiriert ist, und zeigen, wie diese Plattform genutzt werden kann, um das Verhalten und das Lernen großer Populationen neuronaler Agenten zu untersuchen - im Gegensatz zu den derzeit populären Spielumgebungen unterstützt unsere Plattform persistente Umgebungen mit einer variablen Anzahl von Agenten und offenen Aufgabenbeschreibungen. Die Entstehung von komplexem Leben auf der Erde wird oft dem Wettrüsten zugeschrieben, das durch eine riesige Anzahl von Organismen ausgelöst wurde, die alle um endliche Ressourcen konkurrierten. Unsere Plattform zielt darauf ab, diese Situation im Mikrokosmos zu simulieren: Wir führen eine Reihe von Experimenten durch, um zu testen, wie groß angelegter Multiagenten-Wettbewerb die Entwicklung von geschicktem Verhalten fördern kann.
In diesem Papier schlagen wir einen verbesserten quantitativen Evaluierungsrahmen für Generative Adversarial Networks (GANs) zur Erzeugung domänenspezifischer Bilder vor, wobei wir herkömmliche Evaluierungsmethoden auf zwei Ebenen verbessern: die Merkmalsdarstellung und die Evaluierungsmetrik.Im Gegensatz zu den meisten bestehenden Evaluierungsrahmen, die die Darstellung des ImageNet-Einführungsmodells übertragen, um Bilder auf den Merkmalsraum abzubilden, verwendet unser Rahmen einen spezialisierten Encoder, um eine feinkörnige domänenspezifische Darstellung zu erhalten. Darüber hinaus schlagen wir für Datensätze mit mehreren Klassen die Class-Aware Frechet Distance (CAFD) vor, die ein Gauß'sches Mischungsmodell auf den Merkmalsraum anwendet, um die vielschichtige Merkmalsverteilung besser anzupassen. Unseres Wissens nach sind wir die ersten, die Gegenbeispiele liefern, bei denen FID nicht mit den menschlichen Urteilen übereinstimmt, und es wird in den Experimenten gezeigt, dass unser Rahmenwerk in der Lage ist, die Unzulänglichkeit von FID zu überwinden und die Robustheit zu verbessern.
Dieses Papier stellt ResBinNet vor, das aus zwei miteinander verknüpften Methoden besteht, die darauf abzielen, die langsame Konvergenzgeschwindigkeit und begrenzte Genauigkeit binärer neuronaler Netze zu überwinden.Die erste Methode, die sogenannte Restbinarisierung, lernt eine mehrstufige binäre Darstellung für die Merkmale innerhalb einer bestimmten neuronalen Netzschicht. Die zweite Methode, die sogenannte Temperaturanpassung, binarisiert allmählich die Gewichte einer bestimmten Schicht.Die beiden Methoden lernen gemeinsam eine Reihe von weich-binarisierten Parametern, die die Konvergenzrate und Genauigkeit von binären neuronalen Netzen verbessern.Wir bestätigen die Anwendbarkeit und Skalierbarkeit von ResBinNet durch die Implementierung eines Prototyps Hardware-Beschleuniger.Der Beschleuniger ist rekonfigurierbar in Bezug auf die numerische Genauigkeit der binarisierten Features, bietet einen Kompromiss zwischen Laufzeit und Inferenz Genauigkeit.
In realen Anwendungen des maschinellen Lernens sind große Ausreißer und allgegenwärtiges Rauschen an der Tagesordnung, und der Zugang zu sauberen Trainingsdaten, wie sie von standardmäßigen tiefen Autoencodern benötigt werden, ist unwahrscheinlich. Neuronale Netze mit Autoencodern lernen, normale Bilder zu rekonstruieren, und können daher diese Bilder als anomal klassifizieren, wenn der Rekonstruktionsfehler einen bestimmten Schwellenwert überschreitet.In diesem Papier schlagen wir eine unbeaufsichtigte Methode vor, die auf Subset-Scanning über Autoencoder-Aktivierungen basiert. Die Beiträge unserer Arbeit sind dreifach: Erstens schlagen wir eine neuartige Methode vor, die die Erkennung mit Rekonstruktionsfehlern und Subset-Scanning-Scores kombiniert, um die Anomalie-Scores aktueller Autoencoder zu verbessern, ohne dass eine Umschulung erforderlich ist.Zweitens bieten wir die Möglichkeit, die Menge anomaler Knoten im Rekonstruktionsfehlerraum zu inspizieren und zu visualisieren, die eine Probe verrauscht machen.Drittens zeigen wir, dass Subset-Scanning für die Erkennung von Anomalien in den inneren Schichten des Autoencoders verwendet werden kann.Wir liefern Ergebnisse zur Erkennungsleistung für mehrere ungezielte adversarische Rauschmodelle unter Standarddatensätzen.
Konventionelle KGEs leiden oft unter einer begrenzten Wissensrepräsentation, was zu einer geringeren Genauigkeit führt, vor allem wenn sie auf spärlichen Wissensgraphen trainieren. Um dies zu beheben, stellen wir Pretrain-KGEs vor, einen Trainingsrahmen für das Lernen besserer wissensbasierter Entitäts- und Beziehungseinbettungen, der das reichhaltige linguistische Wissen von vortrainierten Sprachmodellen nutzt. Unsere vorgeschlagene Methode ist modellunabhängig in dem Sinne, dass sie auf jede beliebige Variante von KGE-Modellen angewendet werden kann. Experimentelle Ergebnisse zeigen, dass unsere Methode die Ergebnisse konsistent verbessern kann und mit verschiedenen KGE-Modellen wie TransE und QuatE in vier KG-Benchmark-Datensätzen bei der Vorhersage von Links und der Klassifizierung von Tripletts die beste Leistung erzielt.
Wir beschreiben eine neuartige Methode zur Darstellung einer symbolischen Wissensbasis (KB), die als spärlich-matrixverifizierte KB bezeichnet wird.  Diese Darstellung ermöglicht neuronale Module, die vollständig differenzierbar sind, der ursprünglichen Semantik der KB treu bleiben, ausdrucksstark genug sind, um Multi-Hop-Inferenzen zu modellieren, und skalierbar genug sind, um mit realistisch großen KBs verwendet zu werden.  Die reifizierte KB ermöglicht sehr einfache End-to-End-Architekturen, um bei mehreren Benchmarks, die zwei Aufgabenfamilien repräsentieren, eine konkurrenzfähige Leistung zu erzielen: KB-Vervollständigung und Lernen von semantischen Parsern aus Denotationen.
Wir untersuchen die Cross-Entropy-Methode (CEM) für die nicht-konvexe Optimierung einer kontinuierlichen und parametrisierten Zielfunktion und führen eine differenzierbare Variante (DCEM) ein, die es uns ermöglicht, die Ausgabe der CEM in Bezug auf die Parameter der Zielfunktion zu differenzieren. Wir zeigen Anwendungen in einer synthetischen energiebasierten strukturierten Vorhersageaufgabe und in der nicht-konvexen kontinuierlichen Steuerung. Im Rahmen der Steuerung zeigen wir an den simulierten Geparden- und Walker-Aufgaben, dass wir ihre optimalen Aktionssequenzen mit DCEM einbetten und dann die Richtlinienoptimierung zur Feinabstimmung von Komponenten der Steuerung als Schritt zur Kombination von modellbasiertem und modellfreiem RL verwenden können.
Wir schlagen einen neuen Rahmen für die Extraktion von Entitäten und Ereignissen vor, der auf generativem adversarialem Imitationslernen basiert - einer inversen Methode des Verstärkungslernens, die ein generatives adversariales Netzwerk (GAN) verwendet - und gehen davon aus, dass die Instanzen und Etiketten verschiedene Schwierigkeitsgrade aufweisen und dass die Gewinne und Strafen (Belohnungen) unterschiedlich sein werden.  Experimente zeigen auch, dass der vorgeschlagene Rahmen die modernsten Methoden übertrifft.
Trotz der jüngsten Fortschritte in der generativen Bildmodellierung bleibt die erfolgreiche Generierung hochauflösender, vielfältiger Stichproben aus komplexen Datensätzen wie ImageNet ein schwer zu erreichendes Ziel. Zu diesem Zweck trainieren wir Generative Adversarial Networks auf der bisher größten Skala und untersuchen die für eine solche Skala spezifischen Instabilitäten. Unsere Modifikationen führen zu Modellen, die den neuen Stand der Technik in der klassenbedingten Bildsynthese setzen. Wenn sie auf ImageNet mit einer Auflösung von 128x128 trainiert werden, erreichen unsere Modelle (BigGANs) einen Inception Score (IS) von 166,3 und eine Frechet Inception Distance (FID) von 9,6, was eine Verbesserung gegenüber dem bisher besten IS von 52,52 und FID von 18,65 bedeutet.
Jüngste Studien zeigen, dass ein tiefes neuronales Netzwerk übertragbare Merkmale erlernen kann, die sich gut auf neue Aufgaben verallgemeinern lassen.Ben-David et al. (2010) stellen außerdem eine obere Schranke für den Zielfehler bei der Übertragung des Wissens bereit, die sich als gleichzeitige Minimierung des Quellenfehlers und des Abstands zwischen den Randverteilungen zusammenfassen lässt.Übliche Methoden, die auf dieser Theorie basieren, ignorieren jedoch in der Regel den gemeinsamen Fehler, so dass Proben aus verschiedenen Klassen beim Abgleich der Randverteilungen vermischt werden können. Um dieses Problem zu lösen, schlagen wir eine allgemeine obere Schranke vor, die den gemeinsamen Fehler berücksichtigt, so dass der unerwünschte Fall angemessen bestraft werden kann. Darüber hinaus verwenden wir einen eingeschränkten Hypothesenraum, um eine engere Schranke zu formalisieren, sowie eine neuartige Cross-Margin-Diskrepanz, um die Unähnlichkeit zwischen den Hypothesen zu messen, was die Instabilität während des adversen Lernens mildert.
Ein Zoo von tiefen Netzen ist heutzutage für fast jede Aufgabe verfügbar, und es ist zunehmend unklar, mit welchem Netz man beginnen sollte, wenn man eine neue Aufgabe in Angriff nimmt, oder welches Netz man als Initialisierung für die Feinabstimmung eines neuen Modells verwenden sollte. Die Struktur der Lehrer und des Schülers kann sich beliebig unterscheiden, und sie können auch auf völlig unterschiedliche Aufgaben mit unterschiedlichen Ausgaberäumen trainiert werden.Nach dem Training mit dem Wissensfluss ist der Schüler unabhängig von den Lehrern.Wir demonstrieren unseren Ansatz an einer Reihe von überwachten und verstärkenden Lernaufgaben und übertreffen dabei die Feinabstimmung und andere "Wissensaustausch"-Methoden.
Trotz der beeindruckenden Leistung von tiefen neuronalen Netzen (DNNs) bei zahlreichen Lernaufgaben, zeigen sie immer noch ungehobelte Verhaltensweisen.Ein rätselhaftes Verhalten ist die subtile empfindliche Reaktion von DNNs auf verschiedene Rauschattacken. In dieser Arbeit schlagen wir einen neuen Trainingsregularisierer vor, der darauf abzielt, den probabilistischen erwarteten Trainingsverlust eines DNNs zu minimieren, das einer generischen Gaußschen Eingabe unterliegt. Wir führen umfangreiche Experimente mit LeNet und AlexNet auf verschiedenen Datensätzen wie MNIST, CIFAR10 und CIFAR100 durch, um die Effektivität unseres vorgeschlagenen Regularisierers zu demonstrieren. Insbesondere zeigen wir, dass Netze, die mit dem vorgeschlagenen Regularisierer trainiert werden, von einer Steigerung der Robustheit gegen Gaußsches Rauschen profitieren, die einer 3-21-fachen Vergrößerung verrauschter Daten entspricht. Darüber hinaus zeigen wir empirisch an verschiedenen Architekturen und Datensätzen, dass die Verbesserung der Robustheit gegen Gaußsches Rauschen durch die Verwendung des neuen Regularisierers die Gesamtrobustheit gegen 6 andere Arten von Angriffen um zwei Größenordnungen verbessern kann.
Bestehende neuronale Architekturen skalieren typischerweise nicht auf die gesamte Evidenz und greifen daher auf die Auswahl einer einzelnen Passage im Dokument zurück (entweder durch Trunkierung oder andere Mittel) und suchen sorgfältig nach der Antwort innerhalb dieser Passage. in einigen Fällen kann diese Strategie jedoch suboptimal sein, da es durch die Konzentration auf eine bestimmte Passage schwierig wird, mehrere Erwähnungen der gleichen Antwort im gesamten Dokument zu nutzen. In dieser Arbeit verfolgen wir einen anderen Ansatz, indem wir leichtgewichtige Modelle konstruieren, die in einer Kaskade kombiniert werden, um die Antwort zu finden.Jedes Teilmodell besteht nur aus Feed-Forward-Netzen, die mit einem Aufmerksamkeitsmechanismus ausgestattet sind, wodurch es trivial parallelisierbar ist.Wir zeigen, dass unser Ansatz auf etwa eine Größenordnung größere Beweisdokumente skalieren kann und Informationen aus mehreren Erwähnungen jedes Antwortkandidaten im gesamten Dokument aggregieren kann.Empirisch gesehen erreicht unser Ansatz sowohl in der Wikipedia- als auch in der Web-Domäne des TriviaQA-Datensatzes eine Spitzenleistung und übertrifft komplexere, rekurrente Architekturen.
Im Gegensatz zu analogen Bereichen, in denen große Systeme als hierarchische Wiederholung von kleinen Einheiten aufgebaut werden, stützt sich die derzeitige Praxis des maschinellen Lernens weitgehend auf Modelle mit nicht-repetitiven Komponenten. Im Geiste der molekularen Zusammensetzung mit sich wiederholenden Atomen bringen wir den Stand der Technik bei der Modellkompression voran, indem wir AtomicCompression Networks (ACNs) vorschlagen, eine neuartige Architektur, die durch rekursive Wiederholung einer kleinen Gruppe von Neuronen aufgebaut wird. Empirische Beweise deuten darauf hin, dass ACNs Kompressionsraten von bis zu drei Größenordnungen im Vergleich zu fein abgestimmten, voll verbundenen neuronalen Netzen erreichen (88- bis 1116-fache Reduktion), wobei sich die Klassifizierungsgenauigkeit nur geringfügig verschlechtert (0,15 % bis 5,33 %).Darüber hinaus kann unsere Methode zu sublinearen Modellkomplexitäten führen und ermöglicht das Erlernen tiefer ACNs mit weniger Parametern als eine logistische Regression, ohne dass die Klassifizierungsgenauigkeit abnimmt.
Generative Modelle, die Sequenzen zukünftiger Ereignisse modellieren und vorhersagen können, können im Prinzip lernen, komplexe Phänomene der realen Welt, wie z.B. physikalische Interaktionen, zu erfassen. Eine zentrale Herausforderung bei der Videovorhersage ist jedoch, dass die Zukunft sehr unsicher ist: eine Sequenz vergangener Beobachtungen von Ereignissen kann viele mögliche Zukünfte implizieren. Unseres Wissens ist unsere Arbeit die erste, die eine Multibild-Videovorhersage mit normalisierenden Flüssen vorschlägt, die eine direkte Optimierung der Datenwahrscheinlichkeit ermöglicht und qualitativ hochwertige stochastische Vorhersagen erzeugt.Wir beschreiben einen Ansatz zur Modellierung der Dynamik des latenten Raums und zeigen, dass flussbasierte generative Modelle einen praktikablen und wettbewerbsfähigen Ansatz für die generative Modellierung von Videos bieten.
Entlang dieser Linie untersuchen wir theoretisch eine allgemeine Form der gradientenbasierten Optimierungsdynamik mit unvoreingenommenem Rauschen, die SGD und die Standard-Langevin-Dynamik vereint.Durch die Untersuchung dieser allgemeinen Optimierungsdynamik analysieren wir das Verhalten von SGD bei der Flucht aus Minima und seine Regularisierungseffekte.Ein neuartiger Indikator wird abgeleitet, um die Effizienz der Flucht aus Minima durch Messung der Ausrichtung der Rauschkovarianz und der Krümmung der Verlustfunktion zu charakterisieren. Wir zeigen weiter, dass das anisotrope Rauschen in SGD die beiden Bedingungen erfüllt und somit hilft, aus scharfen und schlechten Minima effektiv zu entkommen, hin zu stabileren und flachen Minima, die sich typischerweise gut verallgemeinern lassen.Wir verifizieren unser Verständnis durch den Vergleich dieser anisotropen Diffusion mit vollem Gradientenabstieg plus isotroper Diffusion (d.h. Langevin-Dynamik) und anderen Arten von positionsabhängigem Rauschen.
In diesem Beitrag zeigen wir, wie man das Modell effektiver nutzen kann, indem wir seine Differenzierbarkeit ausnutzen und einen Algorithmus zur Optimierung der Politik entwickeln, der die pfadweise Ableitung des gelernten Modells und der Politik über zukünftige Zeitschritte hinweg verwendet.Instabilitäten des Lernens über viele Zeitschritte hinweg werden durch die Verwendung einer terminalen Wertfunktion verhindert, indem die Politik auf akteurskritische Weise gelernt wird. Wir zeigen, dass unser Ansatz (i) durchweg stichprobeneffizienter ist als bestehende modellbasierte Algorithmen auf dem neuesten Stand der Technik, (ii) der asymptotischen Leistung modellfreier Algorithmen entspricht und (iii) auf lange Horizonte skaliert, ein Bereich, in dem frühere modellbasierte Ansätze typischerweise Schwierigkeiten hatten.
Meta-Learning-Algorithmen lernen, sich neue Aufgaben schneller anzueignen, indem sie die Erfahrungen aus früheren Aufgaben nutzen, um neue Probleme effizienter zu lösen. Die Leistung von Meta-Learning-Algorithmen hängt von den Aufgaben ab, die für das Meta-Training zur Verfügung stehen: Genauso wie überwachtes Lernen am besten auf Testpunkte verallgemeinert, die aus der gleichen Verteilung wie die Trainingspunkte gezogen werden, verallgemeinern Meta-Learning-Methoden am besten auf Aufgaben aus der gleichen Verteilung wie die Meta-Trainingsaufgaben. In dieser Arbeit gehen wir einen Schritt in diese Richtung, indem wir eine Familie von unbeaufsichtigten Meta-Lernalgorithmen für das Verstärkungslernen vorschlagen. wir motivieren und beschreiben ein allgemeines Rezept für unbeaufsichtigtes Meta-Verstärkungslernen und präsentieren eine Instanziierung dieses Ansatzes. Unsere konzeptionellen und theoretischen Beiträge bestehen darin, das Problem des unbeaufsichtigten Meta-Reinforcement-Learnings zu formulieren und zu beschreiben, wie Aufgabenvorschläge, die auf gegenseitiger Information basieren, prinzipiell verwendet werden können, um optimale Meta-Learner zu trainieren.Unsere experimentellen Ergebnisse zeigen, dass unbeaufsichtigtes Meta-Reinforcement-Learning effektiv beschleunigte Reinforcement-Learning-Verfahren erwirbt, ohne dass manuelle Aufgabendesigns erforderlich sind, und die Leistung des Lernens von Grund auf deutlich übertrifft.
Wir stellen eine neuartige Methode vor, die parametereffizientes Transfer- und Multitasking-Lernen mit tiefen neuronalen Netzen ermöglicht: Der grundlegende Ansatz besteht darin, einen Modell-Patch - einen kleinen Satz von Parametern - zu lernen, der sich auf jede Aufgabe spezialisiert, anstatt die letzte Schicht oder das gesamte Netz fein abzustimmen. In ähnlicher Weise zeigen wir, dass das Neulernen bestehender Schichten mit niedrigen Parametern (wie z.B. Faltungen in der Tiefe), während der Rest des Netzes eingefroren bleibt, ebenfalls die Genauigkeit des Transfer-Lernens erheblich verbessert.Unser Ansatz ermöglicht sowohl simultanes (Multi-Task) als auch sequentielles Transfer-Lernen.Bei mehreren Multi-Task-Lernproblemen erreichen wir trotz der Verwendung von viel weniger Parametern als bei der traditionellen Logit-Feinabstimmung die gleiche Leistung wie bei Single-Task. 
Gegnerische Beispiele haben den enormen Erfolg des maschinellen Lernens (ML) etwas gestört und geben Anlass zur Sorge hinsichtlich seiner Vertrauenswürdigkeit: Während Studien durchgeführt werden, um die intrinsischen Eigenschaften von adversen Beispielen zu entdecken, wie z.B. ihre Übertragbarkeit und Universalität, gibt es keine ausreichende theoretische Analyse, um das Phänomen in einer Weise zu verstehen, die den Designprozess von ML-Experimenten beeinflussen kann.In diesem Beitrag leiten wir ein informationstheoretisches Modell ab, das adversarische Angriffe universell als den Missbrauch von Merkmalsredundanzen in ML-Algorithmen erklärt. Wir beweisen, dass Merkmalsredundanz eine notwendige Bedingung für die Existenz negativer Beispiele ist, und unser Modell hilft, die wichtigsten Fragen zu erklären, die in vielen anekdotischen Studien zu negativen Beispielen aufgeworfen werden.Unsere Theorie wird durch empirische Messungen des Informationsgehalts von gutartigen und negativen Beispielen in Bild- und Textdaten untermauert. Unsere Messungen zeigen, dass typische negative Beispiele gerade genug Redundanz einbringen, um die Entscheidungsfindung eines maschinellen Lerners, der auf entsprechenden positiven Beispielen trainiert wurde, zu überfordern.
Wir schlagen ein neuartiges unüberwachtes generatives Modell, Elastic-InfoGAN, vor, das lernt, die Objektidentität von anderen Low-Level-Aspekten in klassenungleichen Datensätzen zu trennen.Wir untersuchen zunächst die Probleme im Zusammenhang mit den Annahmen über die Einheitlichkeit, die von InfoGAN gemacht werden, und demonstrieren seine Unwirksamkeit, die Objektidentität in unausgeglichenen Daten richtig zu trennen. Unsere Schlüsselidee besteht darin, die Entdeckung des diskreten latenten Variationsfaktors invariant gegenüber identitätserhaltenden Transformationen in realen Bildern zu machen und dies als Signal zu verwenden, um die Parameter der latenten Verteilung zu erlernen. Experimente sowohl mit künstlichen (MNIST) als auch realen (YouTube-Faces) Datensätzen zeigen die Effektivität unseres Ansatzes in unausgewogenen Daten durch: (i) bessere Entflechtung der Objektidentität als latenter Variationsfaktor; und (ii) bessere Annäherung an das Klassenungleichgewicht in den Daten, wie es sich in den erlernten Parametern der latenten Verteilung widerspiegelt.
Viele reale Anwendungen zeigen ein großes Interesse daran, mehrere Aufgaben aus verschiedenen Datenquellen/Modalitäten mit unausgewogenen Stichproben und Dimensionen zu erlernen. Leider können bestehende hochmoderne Deep-Multi-Task-Learning (MTL)-Ansätze nicht direkt auf diese Einstellungen angewendet werden, da entweder die Eingangsdimensionen heterogen sind oder die optimalen Netzwerkarchitekturen der verschiedenen Aufgaben heterogen sind. Daher müssen Mechanismen für die gemeinsame Nutzung von Wissen entwickelt werden, um die inhärenten Diskrepanzen zwischen den Netzwerkarchitekturen der verschiedenen Aufgaben zu bewältigen.1 Zu diesem Zweck schlagen wir einen flexiblen Rahmen für die gemeinsame Nutzung von Wissen vor, um mehrere Aufgaben aus verschiedenen Datenquellen/Modalitäten gemeinsam zu lernen. Der vorgeschlagene Rahmen erlaubt es jeder Aufgabe, ihr eigenes aufgabenspezifisches Netzwerkdesign zu besitzen, indem eine kompakte Tensordarstellung verwendet wird, während die gemeinsame Nutzung durch die teilweise gemeinsam genutzten latenten Kerne erreicht wird.Durch die Bereitstellung einer ausgefeilteren Steuerung der gemeinsamen Nutzung latenter Kerne ist unser Rahmen effektiv bei der Übertragung von aufgabeninvariantem Wissen und gleichzeitig effizient beim Lernen aufgabenspezifischer Merkmale.Experimente sowohl mit einzelnen als auch mit mehreren Datenquellen/Modalitäten zeigen die vielversprechenden Ergebnisse der vorgeschlagenen Methode, die besonders in Szenarien mit unzureichenden Daten günstig sind.
Aufgrund dieser Abhängigkeit von visuellem Feedback sind blinde Spieler im Nachteil, da sie die Karten nicht lesen oder die Position der Spielsteine nicht sehen können und daher nicht in der Lage sind, ein Spiel ohne sehende Hilfe zu spielen.Wir präsentieren Game Changer, einen erweiterten Arbeitsbereich, der sowohl Audiobeschreibungen als auch taktile Ergänzungen bietet, um den Zustand des Brettspiels für blinde und sehbehinderte Spieler zugänglich zu machen. In diesem Beitrag beschreiben wir das Design von Game Changer und präsentieren die Ergebnisse einer Benutzerstudie, in der 7 blinde Teilnehmer Game Changer benutzten, um gegen einen sehenden Partner zu spielen. Die meisten Spieler gaben an, dass das Spiel mit den Ergänzungen von Game Changer zugänglicher war und waren der Meinung, dass Game Changer zur Erweiterung anderer Spiele verwendet werden könnte.
Wir schlagen ein Regel-Exemplar-Modell für die Sammlung menschlicher Überwachung vor, um die Skalierbarkeit von Regeln mit der Qualität von Instanzbezeichnungen zu kombinieren.  Wir schlagen einen Trainingsalgorithmus vor, der Regeln gemeinsam über latente Abdeckungsvariablen denoisiert und das Modell durch einen weichen Implikationsverlust über die Abdeckungs- und Beschriftungsvariablen trainiert.  Die empirische Auswertung von fünf verschiedenen Aufgaben zeigt, dass (1) unser Algorithmus genauer ist als verschiedene existierende Methoden des Lernens aus einer Mischung von sauberer und verrauschter Überwachung, und (2) die gekoppelte Regel-Beispiel-Überwachung effektiv bei der Denoisierung von Regeln ist.
Unsere vorgeschlagene Methode baut auf dem Rahmen von generativen adversen Netzwerken auf und führt das Empowerment-regulierte Maximum-Entropie inverse Reinforcement Learning ein, um nahezu optimale Belohnungen und Strategien zu erlernen.Empowerment-basierte Regularisierung verhindert, dass sich die Strategie zu sehr an die Expertendemonstrationen anpasst, was zu verallgemeinertem Verhalten führt, das zum Erlernen nahezu optimaler Belohnungen führt. Wir evaluieren unseren Ansatz an verschiedenen hochdimensionalen, komplexen Steuerungsaufgaben und testen unsere gelernten Belohnungen in anspruchsvollen Transfer-Learning-Problemen, bei denen sich Trainings- und Testumgebungen in Bezug auf Dynamik oder Struktur voneinander unterscheiden. Die Ergebnisse zeigen, dass die von uns vorgeschlagene Methode nicht nur nahezu optimale Belohnungen und Strategien lernt, die mit dem Verhalten von Experten übereinstimmen, sondern auch deutlich besser abschneidet als die modernsten Algorithmen für inverses Reinforcement Learning.
Rekurrente neuronale Netze (RNNs) können kontinuierliche Vektorrepräsentationen von symbolischen Strukturen wie Sequenzen und Sätzen lernen; diese Repräsentationen weisen oft lineare Regularitäten (Analogien) auf. Solche Regelmäßigkeiten motivieren unsere Hypothese, dass RNNs, die solche Regelmäßigkeiten aufweisen, symbolische Strukturen implizit in Tensorproduktrepräsentationen (TPRs; Smolensky, 1990) kompilieren, die Tensorprodukte von Vektoren, die Rollen (z.B. Sequenzpositionen) repräsentieren, und Vektoren, die Füller (z.B. bestimmte Wörter) darstellen, additiv kombinieren. Um diese Hypothese zu testen, führen wir Tensor Product Decomposition Networks (TPDNs) ein, die TPRs zur Annäherung an bestehende Vektorrepräsentationen verwenden. Wir demonstrieren anhand synthetischer Daten, dass TPDNs erfolgreich lineare und baumbasierte RNN-Autoencoder-Darstellungen approximieren können, was darauf hindeutet, dass diese Darstellungen eine interpretierbare Kompositionsstruktur aufweisen; wir untersuchen die Einstellungen, die RNNs dazu bringen, solche strukturabhängigen Darstellungen zu induzieren.  Im Gegensatz dazu zeigen weitere TPDN-Experimente, dass die Repräsentationen von vier Modellen, die trainiert wurden, um natürlich vorkommende Sätze zu kodieren, weitgehend mit einer Tasche von Wörtern approximiert werden können, mit nur marginalen Verbesserungen durch komplexere Strukturen. Wir kommen zu dem Schluss, dass TPDNs eine leistungsstarke Methode zur Interpretation von Vektordarstellungen bieten und dass Standard-RNNs kompositionelle Sequenzdarstellungen induzieren können, die bemerkenswert gut durch TPRs approximiert werden; gleichzeitig sind bestehende Trainingsaufgaben für das Lernen von Satzdarstellungen möglicherweise nicht ausreichend, um robuste strukturelle Darstellungen zu induzieren.
Unser Ansatz besteht darin, die Idee der Parametrisierung aus der Programmiersprachentheorie zu verallgemeinern, um eine semantische Eigenschaft zu formulieren, die gewöhnliche Algorithmen von beliebigen nicht-algorithmischen Funktionen unterscheidet. Diese Charakterisierung führt auf natürliche Weise zu einem erlernten Datenerweiterungsschema, das RNNs ermutigt, algorithmisches Verhalten zu erlernen, und das Lernen mit kleinen Stichproben bei einer Vielzahl von Listenverarbeitungsaufgaben ermöglicht.
Das Ziel des Multi-Label-Lernens (MLL) ist es, eine gegebene Instanz mit den relevanten Bezeichnungen aus einer Menge von Konzepten zu assoziieren. Bisherige Arbeiten zum MLL konzentrierten sich hauptsächlich auf die Einstellung, in der die Konzeptmenge als fest angenommen wird, während viele reale Anwendungen die Einführung neuer Konzepte in die Menge erfordern, um neuen Anforderungen gerecht zu werden.Eine häufige Notwendigkeit ist es, die ursprünglichen groben Konzepte zu verfeinern und sie in feinere Konzepte aufzuteilen, wobei der Verfeinerungsprozess typischerweise mit begrenzten beschrifteten Daten für die feineren Konzepte beginnt. Um diesen Bedarf zu decken, schlagen wir ein spezielles schwach überwachtes MLL-Problem vor, das sich nicht nur auf die Situation einer begrenzten feinkörnigen Überwachung konzentriert, sondern auch die hierarchische Beziehung zwischen den groben Konzepten und den feinkörnigen Konzepten ausnutzt. Die experimentellen Ergebnisse zeigen, dass die von uns vorgeschlagene Methode in der Lage ist, genaue Pseudo-Labels zuzuweisen und im Vergleich zu anderen bestehenden Methoden eine bessere Klassifizierungsleistung zu erzielen.
Wir entwickeln einen auf Verstärkungslernen basierenden Suchassistenten, der Benutzer durch eine Reihe von Aktionen und eine Abfolge von Interaktionen unterstützen kann, um ihnen zu ermöglichen, ihre Absichten zu verwirklichen.Unser Ansatz ist auf die subjektive Suche ausgerichtet, bei der der Benutzer nach digitalen Assets wie Bildern sucht, was sich grundlegend von den Aufgaben unterscheidet, die objektive und begrenzte Suchmodalitäten haben.Beschriftete Konversationsdaten sind bei solchen Suchaufgaben in der Regel nicht verfügbar und das Training des Agenten durch menschliche Interaktionen kann zeitaufwendig sein. Wir schlagen einen stochastischen virtuellen Benutzer vor, der einen realen Benutzer verkörpert und verwendet werden kann, um das Benutzerverhalten effizient zu testen, um den Agenten zu trainieren, was das Bootstrapping des Agenten beschleunigt.Wir entwickeln eine auf dem A3C-Algorithmus basierende kontexterhaltende Architektur, die es dem Agenten ermöglicht, dem Benutzer kontextuelle Unterstützung zu bieten.Wir vergleichen den A3C-Agenten mit Q-Learning und bewerten seine Leistung anhand der durchschnittlichen Belohnungen und Zustandswerte, die er mit dem virtuellen Benutzer in Validierungsepisoden erhält.Unsere Experimente zeigen, dass der Agent lernt, höhere Belohnungen und bessere Zustände zu erreichen.
Wir stellen einen einfachen Ansatz vor, der auf pixelweisen nächsten Nachbarn basiert, um die Funktionsweise modernster neuronaler Netze für Aufgaben auf Pixelebene zu verstehen und zu interpretieren.Wir zielen darauf ab, die Synthese-/Vorhersagemechanismen modernster konvolutioneller neuronaler Netze zu verstehen und aufzudecken.Zu diesem Zweck analysieren wir in erster Linie den Syntheseprozess generativer Modelle und den Vorhersagemechanismus diskriminativer Modelle. Die Haupthypothese dieser Arbeit ist, dass neuronale Faltungsnetze für Aufgaben auf Pixelebene eine schnelle Synthese-/Prädiktionsfunktion für kompositionelle nächste Nachbarn erlernen.
Das Erlernen der Vorhersage zukünftiger Ereignisse aus Mustern vergangener Ereignisse wird schwieriger, je mehr Arten von Ereignissen berücksichtigt werden. Viele der Muster, die von einem gewöhnlichen LSTM im Datensatz erkannt werden, sind falsch, da die Anzahl potenzieller paarweiser Korrelationen beispielsweise quadratisch mit der Anzahl der Ereignisse wächst. Wir schlagen eine Art faktorielle LSTM-Architektur vor, bei der verschiedene Blöcke von LSTM-Zellen für die Erfassung verschiedener Aspekte des Weltzustands verantwortlich sind und verwenden Datalog-Regeln, um zu spezifizieren, wie die LSTM-Struktur aus einer Datenbank von Fakten über die Entitäten in der Welt abgeleitet werden soll. In beiden Fällen besteht das Ziel darin, nützliche induktive Verzerrungen zu erhalten, indem informierte Unabhängigkeitsannahmen in das Modell kodiert werden.Wir betrachten speziell den neuronalen Hawkes-Prozess, der ein LSTM verwendet, um die Rate momentaner Ereignisse in kontinuierlicher Zeit zu modulieren.Sowohl in synthetischen als auch in realen Domänen zeigen wir, dass wir eine bessere Generalisierung erhalten, wenn wir geeignete faktorielle Designs verwenden, die durch einfache Datalog-Programme spezifiziert werden.
In dieser Arbeit geht es um die Lösung von datengesteuerten Optimierungsproblemen, bei denen das Ziel darin besteht, eine Eingabe zu finden, die eine unbekannte Bewertungsfunktion maximiert, wenn man Zugang zu einem Datensatz von Eingabe- und Bewertungspaaren hat.Eingaben können auf extrem dünnen Mannigfaltigkeiten in hochdimensionalen Räumen liegen, was die Optimierung anfällig für ein Abfallen von der Mannigfaltigkeit macht.Außerdem kann die Auswertung der unbekannten Funktion teuer sein, so dass der Algorithmus in der Lage sein sollte, statische Offline-Daten zu nutzen.Wir schlagen Modellinversionsnetzwerke (MINs) als Ansatz zur Lösung solcher Probleme vor. Im Gegensatz zu früheren Arbeiten skalieren MINs auf extrem hochdimensionale Eingaberäume und können offline protokollierte Datensätze für die Optimierung sowohl in kontextuellen als auch in nicht-kontextuellen Umgebungen effizient nutzen.Wir zeigen, dass MINs auch auf die aktive Umgebung erweitert werden können, die in früheren Arbeiten häufig untersucht wurde, und zwar über ein einfaches, neuartiges und effektives Schema für die aktive Datenerfassung.Unsere Experimente zeigen, dass MINs als leistungsstarke Optimierer für eine Reihe von kontextuellen/nicht-kontextuellen, statischen/aktiven Problemen fungieren, einschließlich der Optimierung von Bildern und Proteindesigns sowie des Lernens aus protokolliertem Bandit-Feedback.
Die Generierung formaler Sprache, die durch relationale Tupel repräsentiert wird, wie z.B. Lisp-Programme oder mathematische Ausdrücke, aus einer natürlich-sprachlichen Eingabe ist eine extrem anspruchsvolle Aufgabe, da sie die explizite Erfassung diskreter symbolischer Strukturinformationen aus der Eingabe erfordert, um die Ausgabe zu generieren.Die meisten neuronalen Sequenzmodelle, die dem Stand der Technik entsprechen, erfassen solche Strukturinformationen nicht explizit und schneiden daher bei diesen Aufgaben nicht gut ab.In diesem Papier schlagen wir ein neues Kodierer-Dekodierer-Modell vor, das auf Tensor-Produkt-Repräsentationen (TPRs) für die Generierung von natürlicher zu formaler Sprache basiert, genannt TP-N2F. Der Kodierer von TP-N2F verwendet die TPR-"Bindung", um symbolische Strukturen in natürlicher Sprache im Vektorraum zu kodieren, und der Dekodierer verwendet die TPR-"Entbindung", um eine Sequenz von relationalen Tupeln, die jeweils aus einer Relation (oder Operation) und einer Anzahl von Argumenten bestehen, im symbolischen Raum zu erzeugen. TP-N2F übertrifft LSTM-basierte Seq2Seq-Modelle beträchtlich und schafft einen neuen Stand der Technik bei zwei Benchmarks: dem MathQA-Datensatz für das Lösen mathematischer Probleme und dem AlgoList-Datensatz für die Programmsynthese.Ablation-Studien zeigen, dass die Verbesserungen hauptsächlich auf die Verwendung von TPRs sowohl im Encoder als auch im Decoder zurückzuführen sind, um explizit relationale Strukturinformationen für symbolische Schlussfolgerungen zu erfassen.
Wir formulieren dieses Modell im Kontext der Vorwärtsmodellierung und nutzen räumliche und sequenzielle Beschränkungen und Korrelationen über Faltungsneuronale Netze bzw. Netze des Langzeitgedächtnisses.Wir evaluieren unseren Ansatz an einem großen Datensatz menschlicher Spiele von StarCraft: Unsere Modelle schlagen durchweg starke regelbasierte Grundlinien und erzeugen qualitativ sinnvolle zukünftige Spielzustände.
In dieser Arbeit untersuchen wir die Generalisierung neuronaler Netze beim gradientenbasierten Meta-Lernen, indem wir verschiedene Eigenschaften der Ziellandschaften analysieren. Wir zeigen experimentell, dass mit fortschreitendem Meta-Training die Meta-Test-Lösungen, die durch Anpassung der Meta-Train-Lösung des Modells an neue Aufgaben über einige Schritte der gradientenbasierten Feinabstimmung erhalten werden, flacher werden, einen geringeren Verlust aufweisen und sich weiter von der Meta-Train-Lösung entfernen. Wir zeigen auch, dass diese Meta-Test-Lösungen flacher werden, selbst wenn die Generalisierung abnimmt, und liefern damit einen experimentellen Beweis gegen die Korrelation zwischen Generalisierung und flachen Minima im Paradigma des gradientenbasierten Meta-Leaning. Darüber hinaus zeigen wir empirisch, dass die Generalisierung auf neue Aufgaben mit der Kohärenz zwischen ihren Anpassungstrajektorien im Parameterraum korreliert, gemessen durch die durchschnittliche Cosinus-Ähnlichkeit zwischen aufgabenspezifischen Trajektorienrichtungen, ausgehend von einer gleichen Meta-Train-Lösung, und dass die Kohärenz von Meta-Test-Gradienten, gemessen durch das durchschnittliche innere Produkt zwischen den aufgabenspezifischen Gradientenvektoren, die bei der Meta-Train-Lösung ausgewertet werden, ebenfalls mit der Generalisierung korreliert.
Es gab mehrere Versuche mit Variations-Autokodierern (VAE), leistungsstarke globale Repräsentationen komplexer Daten zu erlernen, indem eine Kombination aus latenten stochastischen Variablen und einem autoregressiven Modell über die Dimensionen der Daten verwendet wurde. In diesem Beitrag stellen wir einfache Ergänzungen zum VAE-Rahmenwerk vor, die sich auf natürliche Bilder verallgemeinern lassen, indem räumliche Informationen in die stochastischen Schichten eingebettet werden. Wir verbessern die State-of-the-Art-Ergebnisse bei MNIST, OMNIGLOT, CIFAR10 und ImageNet signifikant, wenn die Feature-Map-Parametrisierung der stochastischen Variablen mit dem autoregressiven PixelCNN-Ansatz kombiniert wird.Interessanterweise beobachten wir auch nahezu State-of-the-Art-Ergebnisse ohne den autoregressiven Teil.
Einfache rekurrente Netzwerke leiden stark unter dem Problem des verschwindenden Gradienten, während Gated Neural Networks (GNNs) wie Long-short Term Memory (LSTM) und Gated Recurrent Unit (GRU) durch ausgeklügelte Netzwerkdesigns vielversprechende Ergebnisse in vielen Sequenzlernaufgaben liefern.Dieses Papier zeigt, wie wir dieses Problem in einem einfachen rekurrenten Netzwerk durch Analyse der Gating-Mechanismen in GNNs angehen können. Wir vergleichen dieses Modell mit IRNNs und LSTMs in mehreren Sequenzmodellierungs-Benchmarks und stellen fest, dass die RINs eine konkurrenzfähige Leistung erbringen und in allen Aufgaben schneller konvergieren. Insbesondere erzielen kleine RIN-Modelle eine um 12%-67% höhere Genauigkeit in den Sequential und Permuted MNIST-Datensätzen und erreichen eine State-of-the-Art-Leistung im bAbI Question Answering-Datensatz.
In letzter Zeit ist das Interesse an sicheren und robusten Techniken im Bereich des Reinforcement Learning (RL) stark gestiegen. Wir schlagen einen neuartigen Ansatz zur Fehlertoleranz innerhalb von RL vor, bei dem der Controller eine Strategie lernt, die mit gegnerischen Angriffen und zufälligen Unterbrechungen, die zu Ausfällen der Systemteilkomponenten führen, umgehen kann. Indem wir zeigen, dass die Problemklasse durch eine Variante von SGs repräsentiert wird, beweisen wir die Existenz einer Lösung, die ein eindeutiges Fixpunkt-Gleichgewicht des Spiels darstellt, und charakterisieren das optimale Verhalten des Reglers, indem wir einen Algorithmus zur Annäherung der Wertfunktion vorstellen, der durch Simulation in unbekannten Umgebungen zur Lösung konvergiert.
Während sich ein Großteil der Literatur auf die Wiederherstellung eines Einzelbildes aus einem unscharfen Bild konzentriert, befassen wir uns in dieser Arbeit mit einer anspruchsvolleren Aufgabe, nämlich der Videowiederherstellung aus einem unscharfen Bild.Wir formulieren die Videowiederherstellung aus einem einzelnen unscharfen Bild als ein inverses Problem, indem wir eine saubere Bildsequenz und ihre jeweilige Bewegung als latente Faktoren und das unscharfe Bild als Beobachtung festlegen. Wir entwerfen eine Verlustfunktion und Regularisierer mit komplementären Eigenschaften, um das Training zu stabilisieren und analysieren Variantenmodelle des vorgeschlagenen Netzwerks.Die Effektivität und Übertragbarkeit unseres Netzwerks werden durch eine große Anzahl von Experimenten auf zwei verschiedenen Arten von Datensätzen hervorgehoben: Kameradrehungsunschärfen, die aus Panoramaszenen generiert werden, und dynamische Bewegungsunschärfen in Hochgeschwindigkeitsvideos.Unser Code und unsere Modelle werden öffentlich verfügbar sein.
Diese Faktoren begrenzen die Anwendbarkeit für den Einsatz auf Geräten mit begrenztem Speicher- und Batteriespeicher, wie z. B. Mobiltelefonen oder eingebetteten Systemen. In dieser Arbeit schlagen wir eine neuartige Pruning-Technik vor, die ganze Filter und Neuronen entsprechend ihrer relativen L1-Norm im Vergleich zum Rest des Netzwerks eliminiert, was zu einer stärkeren Komprimierung und geringeren Redundanz der Parameter führt. Wir beweisen die Machbarkeit unserer Methode, indem wir eine Komprimierung von 97,4%, 47,8% bzw. 53% von LeNet-5, ResNet-56 und ResNet-110 erreichen und damit die für ResNet berichteten State-of-the-Art-Komprimierungsergebnisse übertreffen, ohne dass es zu Leistungseinbußen im Vergleich zur Baseline kommt.Unser Ansatz zeigt nicht nur eine gute Leistung, sondern ist auch einfach auf vielen Architekturen zu implementieren.
Der jüngste Erfolg neuronaler Netze bei der Lösung schwieriger Entscheidungsaufgaben hat einen Anreiz geschaffen, intelligente Entscheidungsfindung "at the edge" zu integrieren. Diese Arbeit hat sich jedoch traditionell auf die Inferenz neuronaler Netze und nicht auf das Training konzentriert, da Speicher- und Rechenbeschränkungen bestehen, vor allem in aufkommenden nichtflüchtigen Speichersystemen, bei denen Schreibvorgänge energetisch kostspielig sind und die Lebensdauer verkürzen.Die Fähigkeit, "at the edge" zu trainieren, wird jedoch immer wichtiger, da sie Anwendungen wie Echtzeitanpassung an Geräteabweichungen und Umgebungsvariationen, Benutzeranpassung und geräteübergreifendes föderiertes Lernen ermöglicht. In dieser Arbeit befassen wir uns mit vier zentralen Herausforderungen für das Training auf Edge-Geräten mit nichtflüchtigem Speicher: niedrige Dichte der Gewichtsaktualisierung, Gewichtsquantisierung, geringer Hilfsspeicher und Online-Lernen. Wir stellen ein Low-Rank-Trainingsschema vor, das diese vier Herausforderungen bei gleichzeitiger Beibehaltung der Recheneffizienz angeht.
Wissensextraktionstechniken werden verwendet, um neuronale Netze in symbolische Beschreibungen umzuwandeln, mit dem Ziel, verständlichere Lernmodelle zu erstellen. Die zentrale Herausforderung besteht darin, eine Erklärung zu finden, die verständlicher ist als das ursprüngliche Modell, während sie dieses Modell immer noch getreu darstellt. Die verteilte Natur von tiefen Netzwerken hat viele zu der Annahme veranlasst, dass die verborgenen Merkmale eines neuronalen Netzwerks nicht durch logische Beschreibungen erklärt werden können, die einfach genug sind, um von Menschen verstanden zu werden, und dass die dekompositionelle Wissensextraktion zugunsten anderer Methoden aufgegeben werden sollte.In diesem Papier untersuchen wir diese Frage systematisch, indem wir eine Methode zur Wissensextraktion unter Verwendung von \textit{M-of-N}-Regeln vorschlagen, die es uns ermöglicht, die Komplexitäts-/Genauigkeitslandschaft von Regeln abzubilden, die verborgene Merkmale in einem Convolutional Neural Network (CNN) beschreiben. Wir stellen fest, dass die Regeln mit optimalem Kompromiss in der ersten und letzten Schicht einen hohen Grad an Erklärbarkeit aufweisen, während die Regeln mit optimalem Kompromiss in der zweiten und dritten Schicht weniger erklärbar sind. die Ergebnisse werfen ein Licht auf die Durchführbarkeit der Regelextraktion aus tiefen Netzwerken und weisen auf den Wert der dekompositionellen Wissensextraktion als Methode der Erklärbarkeit hin.
Jüngste Erkenntnisse zeigen, dass tiefe generative Modelle Proben außerhalb der Verteilung als wahrscheinlicher einschätzen können als solche, die aus der gleichen Verteilung wie die Trainingsdaten stammen.In dieser Arbeit konzentrieren wir uns auf Variations-Auto-Encoder (VAEs) und gehen das Problem der falsch ausgerichteten Likelihood-Schätzungen bei Bilddaten an. Wir entwickeln eine neuartige Likelihood-Funktion, die nicht nur auf den von der VAE zurückgegebenen Parametern basiert, sondern auch auf den Merkmalen der Daten, die in einer selbstüberwachten Art und Weise erlernt wurden. Auf diese Weise erfasst das Modell zusätzlich die semantischen Informationen, die von der üblichen VAE-Likelihood-Funktion vernachlässigt werden.Wir demonstrieren die Verbesserungen in der Zuverlässigkeit der Schätzungen mit Experimenten auf den FashionMNIST- und MNIST-Datensätzen.
Direkte Policy-Gradienten-Methoden für Reinforcement Learning und kontinuierliche Kontrollprobleme sind aus verschiedenen Gründen ein beliebter Ansatz: 1) sie sind einfach zu implementieren, ohne explizite Kenntnis des zugrundeliegenden Modells;2) sie sind ein "End-to-End"-Ansatz, der direkt die interessierende Leistungsmetrik optimiert;3) sie ermöglichen von Natur aus reich parametrisierte Richtlinien. Ein bemerkenswerter Nachteil ist, dass diese Methoden selbst bei dem grundlegendsten kontinuierlichen Steuerungsproblem (dem von linearen quadratischen Reglern) ein nicht-konvexes Optimierungsproblem lösen müssen, über dessen Effizienz sowohl aus rechnerischer als auch aus statistischer Sicht wenig bekannt ist.Im Gegensatz dazu haben die Systemidentifikation und die modellbasierte Planung in der optimalen Steuerungstheorie eine viel solidere theoretische Grundlage, über deren rechnerische und statistische Eigenschaften viel bekannt ist.  Die vorliegende Arbeit schließt diese Lücke, indem sie zeigt, dass (modellfreie) Policy-Gradienten-Methoden global zur optimalen Lösung konvergieren und effizient sind (polynomial in relevanten problemabhängigen Größen), was ihre Stichproben- und Berechnungskomplexität betrifft.
Niederdimensionale Vektoreinbettungen, die mit LSTMs oder einfacheren Techniken berechnet werden, sind ein beliebter Ansatz zur Erfassung der "Bedeutung" von Text und eine Form des unüberwachten Lernens, die für nachgelagerte Aufgaben nützlich ist, deren Leistungsfähigkeit jedoch nicht theoretisch verstanden wird. Dies führt zu einem neuen theoretischen Ergebnis über LSTMs: Niedrigdimensionale Einbettungen, die von einem LSTM mit geringem Speicher abgeleitet werden, sind nachweislich mindestens so leistungsfähig bei Klassifizierungsaufgaben, bis hin zu kleinen Fehlern, wie ein linearer Klassifikator über BonG-Vektoren, ein Ergebnis, das umfangreiche empirische Arbeiten bisher nicht zeigen konnten. Unsere Experimente stützen diese theoretischen Erkenntnisse und etablieren starke, einfache und unbeaufsichtigte Baselines auf Standard-Benchmarks, die in einigen Fällen den Stand der Technik unter den Methoden auf Wortebene darstellen. Wir zeigen auch eine überraschende neue Eigenschaft von Einbettungen wie GloVe und word2vec: Sie bilden eine gute Sensing-Matrix für Text, die effizienter ist als zufällige Matrizen, das Standardwerkzeug zur spärlichen Wiederherstellung, was erklären könnte, warum sie in der Praxis zu besseren Darstellungen führen.
    Einige konventionelle Transformationen wie die Diskrete Walsh-Hadamard-Transformation (DWHT) und die Diskrete Cosinus-Transformation (DCT) wurden häufig als Merkmalsextraktoren in der Bildverarbeitung verwendet, aber nur selten in neuronalen Netzen eingesetzt.Wir haben jedoch herausgefunden, dass diese konventionellen Transformationen die Fähigkeit haben, die kanalübergreifenden Korrelationen ohne erlernbare Parameter in DNNs zu erfassen.In diesem Papier wird zunächst vorgeschlagen, konventionelle Transformationen auf die punktweise Faltung anzuwenden, und es wird gezeigt, dass solche Transformationen die Rechenkomplexität von neuronalen Netzen erheblich reduzieren, ohne dass die Genauigkeit beeinträchtigt wird. Speziell für die DWHT sind keine Fließkommamultiplikationen, sondern nur Additionen und Subtraktionen erforderlich, was den Berechnungsaufwand erheblich reduziert. Das von uns vorgeschlagene DWHT-basierte Modell erzielte eine um 1,49% höhere Genauigkeit bei 79,4% reduzierten Parametern und 48,4% reduzierten FLOPs im Vergleich zum Basismodell (MoblieNet-V1) auf dem CIFAR 100-Datensatz.
Wir führen den Begriff des \emph{Gitterrepräsentationslernens} ein, bei dem die Repräsentation für ein Objekt von Interesse (z.B. ein Satz oder ein Bild) ein Gitterpunkt in einem euklidischen Raum ist. Unser wichtigster Beitrag ist ein Ergebnis, mit dem eine Zielfunktion, die eine Gitterquantisierung verwendet, durch eine Zielfunktion ersetzt werden kann, bei der die Quantisierung fehlt, so dass Optimierungsverfahren auf der Grundlage des Gradientenabstiegs angewendet werden können; wir nennen die daraus resultierenden Algorithmen \emph{dithered stochastic gradient descent}-Algorithmen, da sie explizit so konzipiert sind, dass sie ein Optimierungsverfahren ermöglichen, bei dem nur lokale Informationen verwendet werden. Wir argumentieren auch, dass eine Technik, die häufig in Variational Auto-Encoders (Gauß-Prioren und Gauß-approximierte Posterioren) verwendet wird, eng mit der Idee von Gitterdarstellungen verbunden ist, da der Quantisierungsfehler in guten hochdimensionalen Gittern als Gauß-Verteilung modelliert werden kann. Wir verwenden eine herkömmliche Encoder/Decoder-Architektur, um die Idee der gitterartigen Darstellungen zu erforschen, und liefern experimentelle Beweise für das Potenzial der Verwendung von Gitterdarstellungen, indem wir die generische \texttt{OpenNMT-py}-Architektur so modifizieren, dass sie nicht nur Gaußsches Dithering von Darstellungen, sondern auch den bekannten Straight-Through-Schätzer und seine Anwendung auf Vektorquantisierung implementieren kann. 
Es gab viele Versuche, den Kompromiss zwischen Genauigkeit und Robustheit gegenüber Gegnern zu erklären, aber es gab kein klares Verständnis für das Verhalten eines robusten Klassifikators, der eine dem Menschen ähnliche Robustheit aufweist. Wir argumentieren, (1) warum wir die Robustheit von Klassifizierern gegenüber verschiedenen Störungsgrößen berücksichtigen müssen und nicht nur auf einen festen Störungsschwellenwert achten sollten, (2) warum wir verschiedene Methoden verwenden müssen, um nachteilig gestörte Proben zu generieren, die zum Trainieren eines robusten Klassifizierers und zum Messen der Robustheit von Klassifizierern verwendet werden können, und (3) warum wir nachteilige Genauigkeiten mit verschiedenen Größenordnungen priorisieren müssen.Wir führen die lexikographische echte Robustheit (LGR) von Klassifizierern ein, die die oben genannten Anforderungen kombiniert.  Wir schlagen außerdem einen Orakel-Klassifikator vor, der "Optimal Lexicographically Genuinely Robust Classifier (OLGRC)" genannt wird und der die Genauigkeit von aussagekräftigen, nachteilig gestörten Beispielen bevorzugt, die durch kleinere Störungen erzeugt wurden.   Um die lexikografische Optimierung auf neuronale Netze anzuwenden, verwenden wir das Gradient Episodic Memory (GEM), das ursprünglich für kontinuierliches Lernen entwickelt wurde, um katastrophales Vergessen zu verhindern.
Die Generierung komplexer diskreter Verteilungen ist nach wie vor eines der schwierigsten Probleme im Bereich des maschinellen Lernens. Bestehende Verfahren zur Generierung komplexer Verteilungen mit hohen Freiheitsgraden basieren auf generativen Standardmodellen wie Generative Adversarial Networks (GAN), Wasserstein GAN und damit verbundenen Variationen, die auf einer Optimierung des Abstands zwischen zwei kontinuierlichen Verteilungen beruhen. Wir stellen ein diskretes Wasserstein GAN (DWGAN) Modell vor, das auf einer dualen Formulierung der Wasserstein-Distanz zwischen zwei diskreten Verteilungen basiert und leiten einen neuartigen Trainingsalgorithmus und eine entsprechende Netzwerkarchitektur ab, die auf dieser Formulierung basiert.
Wir stellen die offene, modulare, selbstverbessernde KI-Vereinigungsarchitektur Omega vor, die eine Verfeinerung von Solomonoffs Alpha-Architektur ist, die mehrere entscheidende Prinzipien der allgemeinen Intelligenz verkörpert, einschließlich der Vielfalt der Darstellungen, der Vielfalt der Datentypen, des integrierten Speichers, der Modularität und der Kognition höherer Ordnung.Wir behalten den grundlegenden Entwurf eines fundamentalen algorithmischen Substrats bei, das als "KI-Kern" für die Problemlösung und grundlegende kognitive Funktionen wie den Speicher bezeichnet wird, sowie eine größere, modulare Architektur, die den Kern auf vielfältige Weise wiederverwendet. Omega umfasst acht Repräsentationssprachen und sechs Klassen von neuronalen Netzen, die kurz vorgestellt werden. die Architektur ist zunächst für die Automatisierung der Datenwissenschaft gedacht und umfasst daher viele Problemlösungsmethoden für statistische Aufgaben. wir geben einen Überblick über die allgemeine Softwarearchitektur, Kognition höherer Ordnung, Selbstverbesserung, modulare neuronale Architekturen, intelligente Agenten, die Prozess- und Speicherhierarchie, Hardwareabstraktion, Peer-to-Peer-Computing und Datenabstraktionsmöglichkeiten.
Um herkömmliche Single-Turn-Modelle in die Lage zu versetzen, die Geschichte umfassend zu kodieren, stellen wir Flow vor, einen Mechanismus, der Zwischenrepräsentationen, die während des Prozesses der Beantwortung früherer Fragen generiert werden, durch eine alternierende parallele Verarbeitungsstruktur einbeziehen kann.Im Vergleich zu oberflächlichen Ansätzen, die frühere Fragen/Antworten als Input verketten, integriert Flow die latente Semantik der Gesprächsgeschichte tiefer. Unser Modell, FlowQA, zeigt eine überragende Leistung bei zwei kürzlich vorgeschlagenen Konversationsherausforderungen (+7,2% F1 bei CoQA und +4,0% bei QuAC).Die Effektivität von Flow zeigt sich auch bei anderen Aufgaben.Durch die Reduktion von sequentiellem Instruktionsverständnis auf maschinelles Konversationsverstehen übertrifft FlowQA die besten Modelle auf allen drei Domänen in SCONE, mit +1,8% bis +4,4% Verbesserung der Genauigkeit.
Wir betrachten Reinforcement Learning und Bandit-strukturierte Vorhersageprobleme mit sehr spärlichem Verlust-Feedback: nur am Ende einer Episode. Wir stellen einen neuartigen Algorithmus vor, RESIDUAL LOSS PREDICTION (RESLOPE), der solche Probleme durch automatisches Lernen einer internen Darstellung einer dichteren Belohnungsfunktion löst. RESLOPE arbeitet als Reduktion auf kontextuelle Bandits, indem es seine gelernte Verlustrepräsentation verwendet, um das Kreditzuweisungsproblem zu lösen, und ein kontextuelles Bandit-Orakel, um einen Kompromiss zwischen Exploration und Ausbeutung zu finden. RESLOPE genießt eine theoretische Garantie im Stil einer No-Regret-Reduktion und übertrifft den Stand der Technik von Verstärkungslernalgorithmen sowohl in MDP-Umgebungen als auch in strukturierten Bandit-Vorhersageeinstellungen.
Diese Schwierigkeiten ergeben sich aus der Tatsache, dass die optimalen Gewichte für adversarische Netze den Sattelpunkten und nicht den Minimierern der Verlustfunktion entsprechen. Die alternierenden stochastischen Gradientenmethoden, die typischerweise für solche Probleme verwendet werden, konvergieren nicht zuverlässig zu den Sattelpunkten, und wenn sie konvergieren, sind sie oft sehr empfindlich gegenüber den Lernraten. Wir zeigen sowohl in der Theorie als auch in der Praxis, dass die vorgeschlagene Methode zuverlässig zu Sattelpunkten konvergiert, so dass die Wahrscheinlichkeit eines "Zusammenbruchs" negativer Netzwerke geringer ist und ein schnelleres Training mit höheren Lernraten möglich ist.
Eine wichtige Art von Fragen, die in der erklärbaren Planung auftauchen, sind kontrastive Fragen der Form "Warum Aktion A anstelle von Aktion B?" Solche Fragen können mit einer kontrastiven Erklärung beantwortet werden, die die Eigenschaften des ursprünglichen Plans mit A mit denen des kontrastiven Plans mit B vergleicht. In diesem Beitrag werden domänenunabhängige Kompilationen von Benutzerfragen in Constraints eingeführt, die dem Planungsmodell hinzugefügt werden, so dass eine Lösung des neuen Modells den kontrastiven Plan repräsentiert. Wir stellen eine formale Beschreibung der Kompilation von Benutzerfragen zu Constraints in einer temporalen und numerischen PDDL2.1-Planungsumgebung vor.
Methoden, die Repräsentationen von Knoten in einem Graphen erlernen, spielen eine entscheidende Rolle in der Netzwerkanalyse, da sie viele nachgelagerte Lernaufgaben ermöglichen. Wir schlagen Graph2Gauss vor - einen Ansatz, der effizient vielseitige Knoteneinbettungen auf großen (attributierten) Graphen erlernen kann, die eine starke Leistung bei Aufgaben wie Linkvorhersage und Knotenklassifizierung zeigen. Im Gegensatz zu den meisten Ansätzen, die Knoten als Punktvektoren in einem niedrigdimensionalen, kontinuierlichen Raum darstellen, betten wir jeden Knoten als Gauß-Verteilung ein, was es uns ermöglicht, Unsicherheiten in der Darstellung zu erfassen, und schlagen eine unbeaufsichtigte Methode vor, die induktive Lernszenarien handhabt und auf verschiedene Arten von Graphen anwendbar ist: einfach/zugeordnet, gerichtet/ungerichtet. Durch die Nutzung der Netzwerkstruktur und der zugehörigen Knotenattribute sind wir in der Lage, ohne zusätzliches Training auf unbekannte Knoten zu verallgemeinern.Um die Einbettungen zu lernen, verwenden wir eine personalisierte Ranking-Formulierung bezüglich der Knotenabstände, die die natürliche Ordnung der Knoten ausnutzt, die durch die Netzwerkstruktur vorgegeben ist. Experimente mit realen Netzwerken zeigen die hohe Leistungsfähigkeit unseres Ansatzes, der die modernsten Methoden zur Netzwerkeinbettung bei verschiedenen Aufgaben übertrifft.
Während große Fortschritte gemacht wurden, um neuronale Netze in einem breiten Spektrum von Aufgaben effektiv zu machen, sind viele überraschend anfällig für kleine, sorgfältig ausgewählte Störungen ihrer Eingabe, bekannt als adversarische Beispiele.In diesem Papier befürworten wir und experimentell untersuchen die Verwendung von Logit Regularisierung Techniken als adversarische Verteidigung, die in Verbindung mit anderen Methoden für die Schaffung adversarial Robustheit mit wenig bis gar keine Kosten verwendet werden kann. Wir zeigen, dass ein Großteil der Effektivität eines neueren gegnerischen Verteidigungsmechanismus auf die Logit-Regularisierung zurückgeführt werden kann, und wir zeigen, wie die Verteidigung sowohl gegen White-Box- als auch gegen Black-Box-Angriffe verbessert werden kann, wobei ein stärkerer Black-Box-Angriff gegen PGD-basierte Modelle entsteht.
Während es umfangreiche Arbeiten zur automatischen Optimierung von Hyperparametern für einfache Räume gibt, bleiben komplexe Räume wie der Raum der Deep-Architekturen weitgehend unerforscht. Wir schlagen eine erweiterbare und modulare Sprache vor, die es dem menschlichen Experten ermöglicht, komplexe Suchräume über Architekturen und ihre Hyperparameter kompakt darzustellen. Die resultierenden Suchräume sind baumstrukturiert und daher leicht zu durchlaufen. Wir können die Struktur des Suchraums nutzen, um verschiedene Modellsuchalgorithmen einzuführen, wie z.B. die Zufallssuche, die Monte-Carlo-Baumsuche (MCTS) und die sequentielle modellbasierte Optimierung (SMBO). Wir stellen Experimente vor, die die verschiedenen Algorithmen mit CIFAR-10 vergleichen und zeigen, dass MCTS und SMBO die Zufallssuche übertreffen. Diese Experimente zeigen, dass unser Rahmenwerk effektiv für die Modellentdeckung genutzt werden kann, da es möglich ist, ausdrucksstarke Suchräume zu beschreiben und konkurrenzfähige Modelle ohne großen Aufwand für den menschlichen Experten zu entdecken.Der Code für unser Rahmenwerk und unsere Experimente wurde öffentlich zugänglich gemacht
Deep-Learning-Netzwerke haben bei Computer-Vision-Aufgaben wie Bildklassifikation und Objekterkennung eine hohe Genauigkeit erreicht.Die leistungsfähigen Systeme umfassen jedoch in der Regel große Modelle mit zahlreichen Parametern.Nach dem Training ist ein schwieriger Aspekt für solche leistungsstarken Modelle der Einsatz in ressourcenbeschränkten Inferenzsystemen - die Modelle (oft tiefe Netzwerke oder breite Netzwerke oder beides) sind rechen- und speicherintensiv.Numerische Verfahren mit geringer Präzision und Modellkomprimierung durch Wissensdestillation sind beliebte Techniken, um sowohl die Rechenanforderungen als auch den Speicherbedarf dieser eingesetzten Modelle zu senken. In diesem Papier untersuchen wir die Kombination dieser beiden Techniken und zeigen, dass die Leistung von Netzwerken mit geringer Präzision durch die Verwendung von Wissensdestillationstechniken erheblich verbessert werden kann. Wir nennen unseren Ansatz Apprentice und zeigen die modernsten Genauigkeiten unter Verwendung von ternärer Präzision und 4-Bit-Präzision für viele Varianten der ResNet-Architektur auf dem ImageNet-Datensatz.Wir untersuchen drei Schemata, in denen man Wissensdestillationstechniken auf verschiedene Stufen der Train-and-Deploy-Pipeline anwenden kann.
Binäre neuronale Netze (BNN) helfen dabei, den hohen Ressourcenbedarf von DNN zu verringern, da sowohl die Aktivierungen als auch die Gewichte auf 1-Bit begrenzt sind. Wir schlagen eine verbesserte binäre Trainingsmethode (BNN+) vor, indem wir eine Regularisierungsfunktion einführen, die das Training von Gewichten um binäre Werte herum fördert, um die Modellleistung zu verbessern. Diese Ergänzungen basieren auf linearen Operationen, die leicht in den binären Trainingsrahmen implementiert werden können. Wir zeigen experimentelle Ergebnisse auf CIFAR-10 mit einer Genauigkeit von 86,5 % auf AlexNet und 91,3 % mit dem VGG-Netz. Auf ImageNet übertrifft unsere Methode auch die traditionelle BNN-Methode und das XNOR-Netz unter Verwendung von AlexNet mit einer Marge von 4 % bzw. 2 % Top-1 Genauigkeit.
Clustering ist eine grundlegende Methode des maschinellen Lernens, deren Qualität von der Datenverteilung abhängt. Aus diesem Grund können tiefe neuronale Netze zum Erlernen besserer Repräsentationen der Daten verwendet werden.In diesem Papier schlagen wir eine systematische Taxonomie für Clustering mit tiefem Lernen vor, zusätzlich zu einer Überprüfung von Methoden aus dem Bereich. Wir schlagen auch einen neuen Ansatz vor, der auf der Taxonomie aufbaut und einige der Einschränkungen früherer Arbeiten überwindet. unsere experimentelle Auswertung von Bilddatensätzen zeigt, dass sich die Methode der State-of-the-Art-Clustering-Qualität annähert und in einigen Fällen besser abschneidet.
Generative Modelle verwenden oft menschliche Bewertungen, um den Fortschritt zu bestimmen und zu rechtfertigen, aber leider sind die bestehenden menschlichen Bewertungsmethoden ad-hoc: Es gibt derzeit keine standardisierte, validierte Bewertung, die: (1) die Wahrnehmungsgenauigkeit misst, (2) zuverlässig ist, (3) die Modelle in eine klare Rangfolge bringt und (4) eine qualitativ hochwertige Messung ohne unverhältnismäßige Kosten gewährleistet. Als Antwort darauf konstruieren wir die Human-eYe Perceptual Evaluation (HYPE), eine menschliche Metrik, die (1) auf psychophysikalischer Wahrnehmungsforschung beruht, (2) über verschiedene Sätze von zufällig abgetasteten Ausgaben eines Modells hinweg zuverlässig ist, (3) zu trennbaren Modellleistungen führt und (4) effizient in Bezug auf Kosten und Zeit ist.Wir stellen zwei Methoden vor: Die erste, HYPE-Time, misst die visuelle Wahrnehmung unter adaptiven Zeitbeschränkungen, um die minimale Zeitdauer (z. B. 250 ms) zu bestimmen, die für die Ausgabe eines Modells erforderlich ist, Die zweite, HYPE-Infinity, misst die menschliche Fehlerrate bei gefälschten und echten Bildern ohne Zeitbeschränkung, wobei die Stabilität beibehalten und der Zeit- und Kostenaufwand drastisch reduziert wird.Wir testen HYPE mit vier hochmodernen generativen adversen Netzwerken (GANs) bei der bedingungslosen Bilderzeugung unter Verwendung von zwei Datensätzen, dem beliebten CelebA und dem neueren FFHQ mit höherer Auflösung, sowie zwei Stichprobenverfahren für die Modellausgaben. Durch mehrfache Simulation der HYPE-Evaluierung zeigen wir eine konsistente Rangfolge der verschiedenen Modelle und identifizieren StyleGAN mit Trunkierungs-Trick-Sampling (27,6 % HYPE-Infinity-Täuschungsrate, wobei etwa ein Viertel der Bilder von Menschen falsch klassifiziert wurde) als überlegen gegenüber StyleGAN ohne Trunkierung (19,0 %) auf FFHQ.
Die maschinelle Übersetzung hat in letzter Zeit dank der jüngsten Fortschritte im Deep Learning und der Verfügbarkeit großer paralleler Korpora beeindruckende Leistungen erzielt. Es gab zahlreiche Versuche, diese Erfolge auf ressourcenarme Sprachpaare auszuweiten, wofür jedoch Zehntausende paralleler Sätze erforderlich sind. Wir schlagen ein Modell vor, das Sätze aus einsprachigen Korpora in zwei verschiedenen Sprachen nimmt und sie in den gleichen latenten Raum abbildet. Indem es lernt, in beiden Sprachen aus diesem gemeinsamen Merkmalsraum zu rekonstruieren, lernt das Modell effektiv zu übersetzen, ohne irgendwelche markierten Daten zu verwenden. Wir demonstrieren unser Modell auf zwei weit verbreiteten Datensätzen und zwei Sprachpaaren und berichten BLEU-Scores von 32,8 und 15,1 auf den Datensätzen Multi30k und WMT Englisch-Französisch, ohne auch nur einen einzigen parallelen Satz zur Trainingszeit zu verwenden.
Wir leiten eine neue intrinsische soziale Motivation für Multi-Agenten-Verstärkungslernen (MARL) ab, bei der Agenten für ihren kausalen Einfluss auf die Handlungen eines anderen Agenten belohnt werden, wobei der kausale Einfluss durch kontrafaktisches Denken bewertet wird. Wir zeigen, dass die Belohnung für kausalen Einfluss mit der Maximierung der gegenseitigen Information zwischen den Handlungen der Agenten zusammenhängt und testen den Ansatz in anspruchsvollen sozialen Dilemmasituationen, wo er durchweg zu einer verbesserten Kooperation zwischen den Agenten und einer höheren kollektiven Belohnung führt. Schließlich zeigen wir, dass der Einfluss berechnet werden kann, indem jeder Agent mit einem internen Modell ausgestattet wird, das die Handlungen anderer Agenten vorhersagt, so dass die Belohnung für den sozialen Einfluss ohne den Einsatz eines zentralisierten Controllers berechnet werden kann und somit einen wesentlich allgemeineren und skalierbaren induktiven Bias für MARL mit unabhängigen Agenten darstellt.
Wir kombinieren dies in einem verteilten Rahmen für Off-Policy-Lernen, um einen Algorithmus zu entwickeln, den wir Distributed Distributional Deep Deterministic Policy Gradient (D4PG) nennen. Wir kombinieren diese Technik auch mit einer Reihe zusätzlicher, einfacher Verbesserungen wie der Verwendung von N-Schritt-Rückgaben und priorisierter Erfahrungswiederholung. Unsere Ergebnisse zeigen, dass der D4PG-Algorithmus bei einer Vielzahl einfacher Steuerungsaufgaben, schwieriger Manipulationsaufgaben und einer Reihe schwieriger, auf Hindernissen basierender Fortbewegungsaufgaben die beste Leistung erzielt.
Zustands-Aktions-Wertfunktionen (d.h., Wir schlagen einen neuen Begriff des Aktionswerts vor, der durch eine Gauß-geglättete Version des in SARSA verwendeten erwarteten Q-Werts definiert ist. Wir zeigen, dass solche geglätteten Q-Werte immer noch eine Bellman-Gleichung erfüllen, so dass sie auf natürliche Weise aus Erfahrungen aus einer Umgebung erlernt werden können, und dass die Gradienten der erwarteten Belohnung in Bezug auf den Mittelwert und die Kovarianz einer parametrisierten Gaußschen Strategie aus dem Gradienten und der Hessischen Funktion der geglätteten Q-Wert-Funktion gewonnen werden können. Auf der Grundlage dieser Beziehungen entwickeln wir neue Algorithmen für das Training einer Gauß'schen Politik direkt aus einem gelernten Q-Wert-Approximator. Der Ansatz ist auch für proximale Optimierungstechniken geeignet, indem das Ziel mit einer Strafe für die KL-Divergenz von einer vorherigen Politik erweitert wird. wir stellen fest, dass die Fähigkeit, sowohl einen Mittelwert als auch eine Kovarianz während des Trainings zu lernen, es diesem Ansatz ermöglicht, starke Ergebnisse bei standardmäßigen kontinuierlichen Kontrollbenchmarks zu erzielen.
Interaktive fiktive Spiele sind textbasierte Simulationen, in denen ein Agent mit der Welt rein durch natürliche Sprache interagiert. Sie sind ideale Umgebungen, um zu untersuchen, wie man Agenten mit Verstärkungslernen erweitern kann, um die Herausforderungen des Verstehens natürlicher Sprache, der partiellen Beobachtbarkeit und der Aktionsgenerierung in kombinatorisch großen textbasierten Aktionsräumen zu bewältigen. Wir stellen KG-A2C vor, einen Agenten, der einen dynamischen Wissensgraphen aufbaut, während er Aktionen unter Verwendung eines schablonenbasierten Aktionsraums erforscht und generiert. Wir behaupten, dass die doppelte Verwendung des Wissensgraphen, um Rückschlüsse auf den Spielzustand zu ziehen und die Generierung natürlicher Sprache einzuschränken, der Schlüssel zu einer skalierbaren Erforschung kombinatorisch großer natürlichsprachlicher Aktionen ist.
Es ist bekannt, dass neuronale Netze universelle Approximatoren sind, aber dass tiefere Netze in der Praxis tendenziell leistungsfähiger sind als flachere. Wir beleuchten dies, indem wir beweisen, dass die Gesamtzahl der Neuronen m, die erforderlich ist, um natürliche Klassen von multivariaten Polynomen von n Variablen zu approximieren, nur linear mit n für tiefe neuronale Netze wächst, aber exponentiell wächst, wenn nur eine einzige versteckte Schicht erlaubt ist. Wir weisen auch nach, dass bei einer Erhöhung der Anzahl der versteckten Schichten von 1 auf k der Bedarf an Neuronen nicht exponentiell mit n, sondern mit n^{1/k} wächst, was darauf hindeutet, dass die Mindestanzahl der Schichten, die für eine praktische Ausdrucksfähigkeit erforderlich sind, nur logarithmisch mit n wächst.
Faltungsneuronale Netze (Convolutional Neural Networks, CNNs) haben in den letzten Jahren einen dramatischen Einfluss auf Wissenschaft, Technik und Industrie ausgeübt, doch der theoretische Mechanismus der CNN-Architektur bleibt erstaunlich vage.Die CNN-Neuronen, einschließlich ihres charakteristischen Elements, der Faltungsfilter, sind als lernbare Merkmale bekannt, doch ihre individuelle Rolle bei der Erzeugung der Ausgabe ist eher unklar. Die These dieser Arbeit ist, dass nicht alle Neuronen gleich wichtig sind und einige von ihnen mehr nützliche Informationen enthalten, um eine gegebene Aufgabe zu erfüllen, daher schlagen wir vor, die Wichtigkeit der Neuronen zu quantifizieren und einzustufen und die Wichtigkeit der Neuronen direkt in die Zielfunktion unter zwei Formulierungen einzubeziehen: (1) ein spieltheoretischer Ansatz, der auf dem Shapley-Wert basiert, der den marginalen Beitrag jedes Filters berechnet; und (2) ein probabilistischer Ansatz, der auf dem sogenannten Wichtigkeitsschalter basiert, der Variationsinferenz verwendet.Mit diesen beiden Methoden bestätigen wir die allgemeine Theorie, dass einige der Neuronen von Natur aus wichtiger sind als die anderen.Verschiedene Experimente zeigen, dass gelernte Ränge leicht für die strukturierte Netzwerkkompression und die Interpretierbarkeit gelernter Merkmale verwendet werden können.
Diese Arbeit stellt einen modularen und hierarchischen Ansatz zum Erlernen von Strategien für die Erkundung von 3D-Umgebungen vor, der die Stärken sowohl klassischer als auch lernbasierter Methoden nutzt, indem er analytische Pfadplaner mit gelernten Mappern sowie globale und lokale Strategien einsetzt: Der Einsatz von Lernen bietet Flexibilität in Bezug auf Eingabemodalitäten (im Mapper), nutzt strukturelle Regelmäßigkeiten der Welt (in globalen Strategien) und bietet Robustheit gegenüber Fehlern bei der Zustandsschätzung (in lokalen Strategien). Eine solche Nutzung des Lernens innerhalb jedes Moduls behält seine Vorteile, während gleichzeitig die hierarchische Zerlegung und das modulare Training es uns ermöglichen, die hohe Komplexität der Stichproben zu umgehen, die mit dem Training von End-to-End-Policies verbunden ist.Unsere Experimente in visuell und physisch realistischen simulierten 3D-Umgebungen zeigen die Effektivität unseres vorgeschlagenen Ansatzes im Vergleich zu früheren Lern- und Geometrie-basierten Ansätzen.
Deep Learning für Computer Vision hängt vor allem von der Quelle der Aufsicht.Fotorealistische Simulatoren können große automatisch beschriftete synthetische Daten generieren, aber eine Domäne Lücke einführen, die sich negativ auf die Leistung auswirkt.Wir schlagen einen neuen unbeaufsichtigten Domänenanpassungsalgorithmus vor, genannt SPIGAN, der auf Simulator Privileged Information (PI) und Generative Adversarial Networks (GAN) basiert. Wir verwenden interne Daten aus dem Simulator als PI während des Trainings einer Zielaufgabe network.We experimentell bewerten unseren Ansatz auf semantische segmentation.We trainieren die Netze auf realen Welt Cityscapes und Vistas Datensätze, mit nur unbeschriftete reale Bilder und synthetische beschriftete Daten mit z-Puffer (Tiefe) PI aus dem SYNTHIA-Datensatz.Our Methode verbessert über keine Anpassung und Stand der Technik unüberwachten Domain-Anpassung Techniken.
In diesem Papier stellen wir die erste rigorose Studie zur Diagnose von Elementen eines groß angelegten adversen Trainings auf ImageNet vor, die zwei faszinierende Eigenschaften offenbart. Erstens untersuchen wir die Rolle der Normalisierung: Die Stapelnormalisierung (BN) ist ein entscheidendes Element für die Erzielung von Spitzenleistungen bei vielen Bildverarbeitungsaufgaben, aber wir zeigen, dass sie die Netzwerke daran hindern kann, eine starke Robustheit bei gegnerischem Training zu erlangen.Eine unerwartete Beobachtung ist, dass bei Modellen, die mit BN trainiert wurden, das einfache Entfernen von sauberen Bildern aus den Trainingsdaten die Robustheit gegenüber gegnerischen Angriffen erheblich steigert, nämlich um 18,3%, Diese Zwei-Domänen-Hypothese kann das Problem der BN beim Training mit einer Mischung aus sauberen und gegnerischen Bildern erklären, da die Schätzung der Normalisierungsstatistik dieser Mischverteilung eine Herausforderung darstellt.Geleitet von dieser Zwei-Domänen-Hypothese zeigen wir, dass die Entflechtung der Mischverteilung für die Normalisierung, d.h. die Anwendung separater BNs auf saubere und gegnerische Bilder, eine große Herausforderung darstellt, Darüber hinaus stellen wir fest, dass die Erzwingung eines konsistenten Verhaltens von BNs beim Training und beim Testen die Robustheit weiter verbessern kann.Zweitens untersuchen wir die Rolle der Netzwerkkapazität.Wir stellen fest, dass unsere so genannten "tiefen" Netzwerke für die Aufgabe des adversen Lernens immer noch flach sind.Im Gegensatz zu traditionellen Klassifizierungsaufgaben, bei denen die Genauigkeit nur geringfügig durch das Hinzufügen weiterer Schichten zu "tiefen" Netzwerken verbessert wird (z. B, Im Gegensatz zu herkömmlichen Klassifizierungsaufgaben, bei denen die Genauigkeit nur geringfügig durch das Hinzufügen weiterer Schichten zu "tiefen" Netzen verbessert wird (z. B. ResNet-152), sind tiefere Netze bei der Ausbildung von Gegnern sehr viel stärker gefordert, um eine höhere Robustheit gegenüber Gegnern zu erreichen, die selbst bei einer Erhöhung der Netzkapazität auf ein noch nie dagewesenes Maß (z. B. ResNet-638) deutlich und durchgängig zu beobachten ist.  
Der Gradient eines tiefen neuronalen Netzes (DNN) in Bezug auf die Eingabe liefert Informationen, die verwendet werden können, um die Ausgabevorhersage in Bezug auf die Eingabemerkmale zu erklären, und ist weithin untersucht worden, um bei der Interpretation von DNNs zu helfen. In einem linearen Modell (d.h. $g(x)=wx+b$) entspricht der Gradient ausschließlich den Gewichten $w$.Ein solches Modell kann ein glattes nichtlineares DNN lokal linear approximieren, und daher sind die Gewichte dieses lokalen Modells der Gradient.Der andere Teil eines lokalen linearen Modells, d.h., In diesem Beitrag stellen wir fest, dass der Bias in einem DNN auch einen nicht zu vernachlässigenden Beitrag zur Korrektheit der Vorhersagen leistet und daher eine wichtige Rolle beim Verständnis des DNN-Verhaltens spielen kann. Wir schlagen einen Backpropagation-Algorithmus (BBp) vor, der auf der Ausgabeschicht beginnt und iterativ den Bias jeder Schicht auf die Eingangsknoten überträgt sowie den resultierenden Bias-Term der vorherigen Schicht kombiniert. Dieser Prozess endet an der Eingabeschicht, wo die Summierung der Attributionen über alle Eingabemerkmale genau $b$ wiederherstellt. Zusammen mit der Backpropagation des Gradienten, der $w$ erzeugt, können wir das lokal lineare Modell $g(x)=wx+b$ vollständig wiederherstellen. Die Zuordnung der DNN-Ausgänge zu den Eingängen wird somit in zwei Teile zerlegt, den Gradienten $w$ und die Bias-Zuordnung, die separate und komplementäre Erklärungen liefern.Wir untersuchen verschiedene mögliche Zuordnungsmethoden, die auf den Bias jeder Schicht in BBp angewendet werden.In Experimenten zeigen wir, dass BBp zusätzlich zu gradientenbasierten Zuordnungen komplementäre und hoch interpretierbare Erklärungen von DNNs generieren kann.
Es basiert auf der gleichen Idee der Verwendung von Fourier-Transformation und Autokorrelationsfunktion in Vlachos et al. 2005.Während zeigt interessante Ergebnisse dieser Methode nicht gut auf verrauschte Signale oder Signale mit mehreren Periodizitäten.So fügt unsere Methode mehrere neue zusätzliche Schritte (Hinweise Clustering, Filterung und Detrending), um diese Probleme zu beheben.Experimentelle Ergebnisse zeigen, dass die vorgeschlagene Methode übertrifft den Stand der Technik Algorithmen.
Unser Rahmen besteht aus einem Deep Reinforcement Learning (DRL)-Agenten und einem inversen Dynamikmodell, die miteinander konkurrieren. Ersterer sammelt Trainingsproben für Letzteren und sein Ziel ist es, den Fehler des Letzteren zu maximieren. Letzterer wird mit den vom Ersteren gesammelten Proben trainiert und erzeugt Belohnungen für den Ersteren, wenn er die tatsächliche Aktion des Letzteren nicht vorhersagen kann. In einem solchen Wettbewerbsumfeld lernt der DRL-Agent, Proben zu erzeugen, die das inverse Dynamikmodell nicht korrekt vorhersagen kann, und das inverse Dynamikmodell lernt, sich an die herausfordernden Proben anzupassen.Wir schlagen außerdem eine Belohnungsstruktur vor, die sicherstellt, dass der DRL-Agent nur mäßig harte Proben sammelt und keine übermäßig harten, die das inverse Modell daran hindern, effektiv zu imitieren. Wir bewerten die Wirksamkeit unserer Methode auf mehrere OpenAI Fitnessstudio Roboterarm und Hand Manipulation Aufgaben gegen eine Reihe von Baseline-Modelle.Experimentelle Ergebnisse zeigen, dass unsere Methode vergleichbar ist, dass direkt mit Experten Demonstrationen trainiert, und besser als die anderen Baselines auch ohne menschliche Prioritäten.
In diesem Papier wird ein Dual Variational Autoencoder (DualVAE) vorgeschlagen, ein Rahmen für die Erzeugung von Bildern, die Multiklassen-Etiketten entsprechen.Jüngste Forschungen über bedingte generative Modelle, wie die Conditional VAE, zeigen Bildübertragung durch Ändern von Etiketten.Allerdings, wenn die Dimension der Multiklassen-Etiketten groß ist, können diese Modelle nicht ändern Bilder, die Etiketten entsprechen, weil das Lernen mehrere Verteilungen der entsprechenden Klasse notwendig ist, um ein Bild zu übertragen.Dies führt zu einem Mangel an Trainingsdaten.Daher konditionieren wir statt mit Etiketten, mit latenten Vektoren, die Etiketteninformationen enthalten. DualVAE teilt eine Verteilung des latenten Raums durch lineare Entscheidungsgrenzen mit Hilfe von Labels.Folglich kann DualVAE leicht ein Bild durch Verschieben eines latenten Vektors in Richtung einer Entscheidungsgrenze übertragen und ist robust gegenüber den fehlenden Werten von Multiklassen-Labels.Um unsere vorgeschlagene Methode zu evaluieren, führen wir einen Conditional Inception Score (CIS) ein, um zu messen, wie sehr sich ein Bild in die Zielklasse ändert.Wir evaluieren die von DualVAE übertragenen Bilder unter Verwendung des CIS in CelebA-Datensätzen und demonstrieren die State-of-the-Art-Leistung in einer Multiklassen-Umgebung.
Einer der bemerkenswertesten Beiträge des Deep Learning ist die Anwendung von Faltungsneuronalen Netzen (ConvNets) auf die Klassifizierung von strukturierten Signalen und insbesondere auf die Klassifizierung von Bildern. Neben ihrer beeindruckenden Leistung beim überwachten Lernen inspirierte die Struktur solcher Netze die Entwicklung von tiefen Filterbänken, die als Scattering-Transformationen bezeichnet werden. Diese Transformationen wenden eine Kaskade von Wavelet-Transformationen und komplexen Modulus-Operatoren an, um Merkmale zu extrahieren, die invariant gegenüber Gruppenoperationen und stabil gegenüber Deformationen sind. Darüber hinaus inspirierten ConvNets die jüngsten Fortschritte im geometrischen Deep Learning, die darauf abzielen, diese Netzwerke auf Graphdaten zu verallgemeinern, indem Begriffe aus der Graphsignalverarbeitung angewendet werden, um tiefe Graphfilterkaskaden zu erlernen. Wir demonstrieren die Nützlichkeit von Merkmalen, die mit dieser entworfenen tiefen Filterbank extrahiert wurden, in der Graphenklassifizierung von Biochemie- und sozialen Netzwerkdaten (einschließlich State-of-the-Art-Ergebnissen im letzteren Fall), und in der Datenexploration, wo sie Rückschlüsse auf EC-Austauschpräferenzen in der Enzymevolution ermöglichen.
OCR ist unweigerlich mit NLP verknüpft, da die Endausgabe in Textform erfolgt. Fortschritte in der Dokumentenintelligenz führen zu einem Bedarf an einer einheitlichen Technologie, die OCR mit verschiedenen NLP-Aufgaben, insbesondere semantischem Parsing, integriert. In dieser Studie veröffentlichen wir einen konsolidierten Datensatz für das Parsen von Quittungen als ersten Schritt in Richtung Post-OCR-Parsing-Aufgaben. Der Datensatz besteht aus Tausenden von indonesischen Quittungen, die Bilder und Box/Text-Annotationen für OCR und mehrstufige semantische Etiketten für das Parsen enthalten. Der vorgeschlagene Datensatz kann verwendet werden, um verschiedene OCR- und Parsing-Aufgaben zu lösen.
Die zunehmende Komplexität von Convolutional Neural Networks (CNNs) erhöht das Interesse an der Partitionierung eines Netzwerks über mehrere Beschleuniger während des Trainings und an der Pipeline-Verteilung der Backpropagation-Berechnungen über die Beschleuniger.Bestehende Ansätze vermeiden oder begrenzen die Verwendung von Stale-Weights durch Techniken wie Micro-Batching oder Weight Stashing.Diese Techniken nutzen die Beschleuniger entweder zu wenig oder erhöhen den Speicherbedarf.Wir untersuchen die Auswirkungen von Stale-Weights auf die statistische Effizienz und Leistung in einem Pipeline-Backpropagation-Schema, das die Beschleunigerauslastung maximiert und den Speicher-Overhead bescheiden hält. Wir verwenden 4 CNNs (LeNet-5, AlexNet, VGG und ResNet) und zeigen, dass, wenn das Pipelining auf die frühen Schichten in einem Netzwerk beschränkt ist, das Training mit veralteten Gewichten konvergiert und zu Modellen mit vergleichbarer Inferenzgenauigkeit führt, wie sie aus dem Training ohne Pipelining auf MNIST- und CIFAR-10-Datensätzen resultiert; ein Rückgang der Genauigkeit von 0,4 %, 4 %, 0,83 % bzw. 1,45 % für die 4 Netzwerke. Wir schlagen vor, pipelined und non-pipelined Training in einem hybriden Schema zu kombinieren, um diesen Rückgang zu beheben. Wir demonstrieren die Implementierung und Leistung unserer pipelined Backpropagation in PyTorch auf 2 GPUs unter Verwendung von ResNet und erreichen Geschwindigkeitssteigerungen von bis zu 1,8X gegenüber einer 1-GPU-Basislinie, mit einem kleinen Rückgang der Inferenzgenauigkeit.
Trotz ihrer beeindruckenden Leistung zeigen tiefe neuronale Netze auffällige Fehler bei Eingaben außerhalb der Verteilung. Eine Kernidee der Forschung zu gegnerischen Beispielen ist es, Fehler neuronaler Netze bei solchen Verteilungsänderungen aufzudecken. Wir zerlegen diese Fehler in zwei komplementäre Quellen: Empfindlichkeit und Invarianz. Wir zeigen, dass tiefe Netze nicht nur zu empfindlich auf aufgabenirrelevante Änderungen ihrer Eingaben reagieren, wie es von Epsilon-adversen Beispielen bekannt ist, sondern auch zu invariant gegenüber einer großen Bandbreite von aufgabenrelevanten Änderungen sind, wodurch große Regionen im Eingaberaum anfällig für adversarische Angriffe werden. Wir zeigen, dass eine solche übermäßige Invarianz bei verschiedenen Aufgaben und Architekturtypen auftritt: Bei MNIST und ImageNet kann man den klassenspezifischen Inhalt fast jedes Bildes manipulieren, ohne die versteckten Aktivierungen zu verändern.Wir identifizieren eine Unzulänglichkeit des Standard-Cross-Entropie-Verlustes als Grund für diese Fehler.Darüber hinaus erweitern wir dieses Ziel auf der Grundlage einer informationstheoretischen Analyse, so dass es das Modell dazu anregt, alle aufgabenabhängigen Merkmale in seiner Entscheidung zu berücksichtigen.Dies ist der erste Ansatz, der explizit auf die Überwindung der übermäßigen Invarianz und der daraus resultierenden Schwachstellen zugeschnitten ist.
Flussbasierte generative Modelle sind leistungsstarke exakte Likelihood-Modelle mit effizientem Sampling und Inferenz. In diesem Papier untersuchen und verbessern wir drei einschränkende Designentscheidungen, die von flussbasierten Modellen in früheren Arbeiten verwendet wurden: die Verwendung von gleichmäßigem Rauschen für die Dequantisierung, die Verwendung von nicht aussagekräftigen affinen Flüssen und die Verwendung von rein faltungsbasierten Konditionierungsnetzwerken in Kopplungsschichten. Auf der Grundlage unserer Erkenntnisse schlagen wir Flow++ vor, ein neues flussbasiertes Modell, das nun das modernste nicht-autoregressive Modell für die unbedingte Dichteschätzung bei Standard-Bildbenchmarks ist.
Moderne tiefe künstliche neuronale Netze haben beeindruckende Ergebnisse durch Modelle mit Größenordnungen mehr Parametern als Trainingsbeispiele erzielt, die die Überanpassung mit Hilfe von Regularisierung kontrollieren.Regularisierung kann implizit sein, wie im Fall des stochastischen Gradientenabstiegs und des Parameter-Sharings in Faltungsschichten, oder explizit.Explizite Regularisierungstechniken, die häufigsten Formen sind Gewichtsabnahme und Dropout, haben sich als erfolgreich in Bezug auf eine verbesserte Generalisierung erwiesen, aber sie reduzieren blind die effektive Kapazität des Modells, führen empfindliche Hyperparameter ein und erfordern tiefere und breitere Architekturen, um die reduzierte Kapazität auszugleichen. Im Gegensatz dazu nutzen Datenerweiterungstechniken Domänenwissen, um die Anzahl der Trainingsbeispiele zu erhöhen und die Generalisierung zu verbessern, ohne die effektive Kapazität zu reduzieren und ohne modellabhängige Parameter einzuführen, da sie auf die Trainingsdaten angewandt werden.In diesem Papier stellen wir systematisch Datenerweiterung und explizite Regularisierung auf drei populären Architekturen und drei Datensätzen gegenüber.Unsere Ergebnisse zeigen, dass Datenerweiterung allein die gleiche oder eine höhere Leistung als regularisierte Modelle erreichen kann und eine viel höhere Anpassungsfähigkeit an Änderungen in der Architektur und der Menge der Trainingsdaten aufweist.
Adversarial Feature Learning (AFL) ist eine der vielversprechenden Möglichkeiten für explizite Einschränkungen neuronaler Netze, um gewünschte Darstellungen zu lernen; zum Beispiel könnte AFL helfen, anonymisierte Darstellungen zu lernen, um Datenschutzprobleme zu vermeiden.AFL lernen solche Darstellungen durch Training der Netze, um den Gegner zu täuschen, die die sensiblen Informationen aus dem Netzwerk vorherzusagen, und daher hängt der Erfolg der AFL stark von der Wahl des Gegners.Dieses Papier schlägt ein neuartiges Design des Gegners, {\em mehrere Gegner über zufällige Teilräume} (Die vorgeschlagene Methode ist durch die Annahme motiviert, dass die Täuschung eines Gegners keine aussagekräftigen Informationen liefern kann, wenn der Gegner leicht getäuscht werden kann, und Gegner, die sich auf einen einzelnen Klassifikator verlassen, leiden unter diesem Problem. Im Gegensatz dazu ist die vorgeschlagene Methode so konzipiert, dass sie weniger anfällig ist, indem sie ein Ensemble unabhängiger Klassifikatoren verwendet, wobei jeder Klassifikator versucht, sensible Variablen aus einer anderen Teilmenge der Darstellungen vorherzusagen. Die empirischen Validierungen an drei Aufgaben zur Benutzeranonymisierung zeigen, dass die von uns vorgeschlagene Methode in allen drei Datensätzen Spitzenleistungen erzielt, ohne den Nutzen der Daten wesentlich zu beeinträchtigen. Dies ist insofern von Bedeutung, als es neue Erkenntnisse über die Gestaltung des Gegners liefert, die für die Verbesserung der Leistung von AFL wichtig sind.
Ein Schlüsselproblem in den Neurowissenschaften und den Biowissenschaften im Allgemeinen besteht darin, dass Daten durch eine Hierarchie dynamischer Systeme erzeugt werden. Ein Beispiel dafür sind die Daten der Kalziumbildgebung in vitro, wo die Daten durch ein dynamisches System niedrigerer Ordnung erzeugt werden, das den Kalziumfluss in den Neuronen steuert, der wiederum durch ein dynamisches System höherer Ordnung der neuronalen Berechnung angetrieben wird.Idealerweise wären Biowissenschaftler in der Lage, die Dynamik sowohl der Systeme niedrigerer Ordnung als auch der Systeme höherer Ordnung abzuleiten, was jedoch in hochdimensionalen Bereichen schwierig ist. Ein kürzlich vorgestellter Ansatz, bei dem sequenzielle Variations-Autocodierer verwendet werden, hat gezeigt, dass es möglich ist, die latente Dynamik eines einzelnen dynamischen Systems für Berechnungen während des Greifverhaltens im Gehirn zu erlernen, indem Spiking-Daten als Poisson-Prozess modelliert werden. Wir generieren synthetische Daten durch die Erzeugung von Feuerungsraten, die Abtastung von Spike Trains und die Konvertierung von Spike Trains in Fluoreszenz-Transienten von zwei dynamischen Systemen, die in der jüngeren Literatur als wichtige Benchmarks verwendet wurden: ein Lorenz-Attraktor und ein chaotisches rekurrentes neuronales Netzwerk Wir zeigen, dass unser Modell besser in der Lage ist, die Lorenz-Dynamik aus Fluoreszenzdaten zu rekonstruieren als konkurrierende Methoden. Wir zeigen, dass unser Modell besser in der Lage ist, die Lorenz-Dynamik aus Fluoreszenzdaten zu rekonstruieren als konkurrierende Methoden. Obwohl unser Modell die zugrundeliegenden Spike-Raten und Kalzium-Transienten aus dem chaotischen neuronalen Netzwerk gut rekonstruieren kann, ist es nicht so gut in der Lage, die Feuerraten zu rekonstruieren wie die grundlegenden Techniken zur Ableitung von Spikes aus Kalziumdaten.Diese Ergebnisse zeigen, dass VLAEs ein vielversprechender Ansatz für die Modellierung hierarchischer dynamischer Systeme in den Biowissenschaften sind, aber dass die Ableitung der Dynamik von Systemen niedrigerer Ordnung möglicherweise besser mit einfacheren Methoden erreicht werden kann.
Trotz zahlreicher früherer Arbeiten zur Quantisierung von Gewichten oder Aktivierungen für neuronale Netze klafft immer noch eine große Lücke zwischen den Software-Quantisierern und der Implementierung von Beschleunigern mit niedriger Genauigkeit, was entweder die Effizienz der Netze oder die der Hardware aufgrund mangelnder Software- und Hardwarekoordination in der Entwurfsphase beeinträchtigt. In diesem Papier schlagen wir einen erlernten linearen symmetrischen Quantisierer für ganzzahlige neuronale Netzwerkprozessoren vor, der nicht nur neuronale Parameter und Aktivierungen auf ganzzahlige Low-Bit-Werte quantisiert, sondern auch die Hardware-Inferenz durch die Verwendung von Batch-Normalisierungsfusion und Akkumulatoren mit niedriger Genauigkeit (z.B., 16-Bit) und Multiplikatoren (z. B., Wir verwenden eine einheitliche Methode zur Quantisierung von Gewichten und Aktivierungen, und die Ergebnisse übertreffen viele frühere Ansätze für verschiedene Netzwerke wie AlexNet, ResNet und leichtgewichtige Modelle wie MobileNet, während sie die Architektur des Beschleunigers nicht beeinträchtigen.Zusätzlich wenden wir die Methode auch auf Modelle zur Objekterkennung an und beobachten eine hohe Leistung und Genauigkeit in YOLO-v2. Schließlich setzen wir die quantisierten Modelle auf unserem spezialisierten DNN-Beschleuniger ein, um die Effektivität des vorgeschlagenen Quantisierers zu zeigen. Wir zeigen, dass selbst bei linearer symmetrischer Quantisierung die Ergebnisse besser sein können als bei asymmetrischen oder nichtlinearen Methoden in 4-Bit-Netzwerken. In der Auswertung verursacht der vorgeschlagene Quantisierer einen Genauigkeitsabfall von weniger als 0,4 % in ResNet18, ResNet34 und AlexNet, wenn das gesamte Netzwerk quantisiert wird, wie es von den Integer-Prozessoren gefordert wird.
Wir schlagen ein CNN für energiebewusstes dynamisches Routing vor, das EnergyNet genannt wird und eine adaptiv-komplexe Inferenz auf der Grundlage der Eingaben erreicht, was zu einer Gesamtreduzierung der Laufzeit-Energiekosten führt, ohne dass die Genauigkeit merklich sinkt (oder sogar steigt). Dies wird erreicht, indem wir einen Energieverlust vorschlagen, der sowohl die Rechen- als auch die Datenverschiebungskosten erfasst, diesen mit dem genauigkeitsorientierten Verlust kombinieren und eine dynamische Routing-Strategie zum Überspringen bestimmter Schichten in den Netzwerken erlernen, die den hybriden Verlust optimiert.  Unsere empirischen Ergebnisse zeigen, dass EnergyNet im Vergleich zu den Basis-CNNs die Energiekosten während der Inferenz auf den CIFAR10- bzw. Tiny ImageNet-Testsätzen um bis zu 40 % bzw. 65 % senken kann, während die gleichen Testgenauigkeiten beibehalten werden.  Es ist außerdem ermutigend zu beobachten, dass das Energiebewusstsein als Trainingsregulierung dienen und sogar die Vorhersagegenauigkeit verbessern kann: Unsere Modelle können bei CIFAR-10 eine um 0,7 % höhere Top-1-Testgenauigkeit als die Baseline erreichen, wenn sie bis zu 27 % Energie sparen, und eine um 1,0 % höhere Top-5-Testgenauigkeit bei Tiny ImageNet, wenn sie bis zu 50 % Energie sparen.
Während die exakte Inferenz und das Lernen dieser Modelle lineare Zeit erfordert, kann es annähernd in sublinearer Zeit mit starken Konzentrationsgarantien durchgeführt werden. In dieser Arbeit stellen wir LSH Softmax vor, eine Methode, um sublineares Lernen und Inferenz der Softmax-Schicht in der Deep-Learning-Umgebung durchzuführen. Unsere Methode stützt sich auf das populäre Locality-Sensitive Hashing, um einen gut konzentrierten Gradientenschätzer zu erstellen, der die nächsten Nachbarn und gleichmäßige Stichproben verwendet.Wir stellen auch ein Inferenzschema in sublinearer Zeit für LSH Softmax vor, das die Gumbel-Verteilung verwendet.Bei der Sprachmodellierung zeigen wir, dass rekurrente neuronale Netze, die mit LSH Softmax trainiert wurden, die gleiche Leistung erbringen wie die Berechnung der exakten Softmax, während sie sublineare Berechnungen benötigen.
Kann der Erfolg von Reinforcement-Learning-Methoden für einfache kombinatorische Optimierungsprobleme auf die sequentielle Einsatzplanung von mehreren Robotern ausgeweitet werden?Zusätzlich zu der Herausforderung, eine nahezu optimale Leistung bei großen Problemen zu erreichen, ist die Übertragbarkeit auf eine unbekannte Anzahl von Robotern und Aufgaben eine weitere zentrale Herausforderung für reale Anwendungen.In diesem Beitrag schlagen wir eine Methode vor, die den ersten Erfolg bei beiden Herausforderungen für Roboter/Maschinen-Planungsprobleme erzielt.  Erstens zeigen wir, dass jedes Roboterplanungsproblem als zufälliges probabilistisches graphisches Modell (PGM) ausgedrückt werden kann.Wir entwickeln eine Mean-Field-Inferenzmethode für zufällige PGM und verwenden sie für die Inferenz von Q-Funktionen.Zweitens zeigen wir, dass die Übertragbarkeit durch sorgfältiges Entwerfen einer zweistufigen sequentiellen Kodierung des Problemzustands erreicht werden kann.Drittens lösen wir das Problem der rechnerischen Skalierbarkeit der angepassten Q-Iteration, indem wir eine heuristische auktionsbasierte Q-Iterationsanpassungsmethode vorschlagen, die durch die erreichte Übertragbarkeit ermöglicht wird.  Wir wenden unsere Methode auf zeit- und raumdiskrete Probleme (Multi-Robot Reward Collection (MRRC)) an und erreichen mit der Übertragbarkeit eine skalierbare Optimalität von 97%, die auch in stochastischen Kontexten beibehalten wird. Durch die Ausweitung unserer Methode auf zeit- und raumkontinuierliche Formulierungen behaupten wir, die erste lernbasierte Methode mit skalierbarer Leistung in jeder Art von Multi-Maschinen-Planungsproblemen zu sein; unsere Methode erreicht eine vergleichbare Leistung wie populäre Metaheuristiken in Identical Parallel Machine Scheduling (IPMS) Problemen.
Um dieses Lernen effizient zu gestalten, müssen wir bei einem gegebenen Lehrplan und dem aktuellen Lernstatus eines Agenten herausfinden, welches die guten nächsten Aufgaben sind, auf die der Agent trainiert werden soll.Lehrer-Schüler-Algorithmen gehen davon aus, dass die guten nächsten Aufgaben diejenigen sind, bei denen der Agent die schnellsten Fortschritte macht oder abschweift. Daher führen wir einen neuen Algorithmus ein, der geordnete Min-Max-Lehrpläne verwendet und davon ausgeht, dass die guten nächsten Aufgaben diejenigen sind, die erlernt werden können, aber noch nicht erlernt wurden. er übertrifft die Teacher-Student-Algorithmen bei kleinen Lehrplänen und übertrifft sie deutlich bei anspruchsvollen Lehrplänen mit zahlreichen Aufgaben.
Die Bereiche der künstlichen Intelligenz und der Neurowissenschaften haben eine lange Geschichte fruchtbarer bidirektionaler Wechselwirkungen: Einerseits stammen wichtige Anregungen für die Entwicklung künstlicher Intelligenzsysteme aus dem Studium natürlicher Intelligenzsysteme, insbesondere des Neokortex von Säugetieren. Eine zentrale Frage an der Schnittstelle dieser beiden Bereiche betrifft die Prozesse, mit denen der Neokortex lernt, und die Frage, inwieweit sie mit dem Backpropagation-Trainingsalgorithmus von Deep Networks vergleichbar sind. Jüngste Fortschritte in unserem Verständnis der neuronalen, synaptischen und dendritischen Physiologie des Neokortex legen neue Ansätze für das unbeaufsichtigte Repräsentationslernen nahe, vielleicht durch eine neue Klasse von Zielfunktionen, die neben oder anstelle der Backpropagation wirken könnten, wobei solche lokalen Lernregeln eher implizite als explizite Ziele in Bezug auf die Trainingsdaten haben und die Domänenanpassung und Generalisierung erleichtern.  Durch ihre Einbindung in tiefe Netzwerke für das Repräsentationslernen könnten unmarkierte Datensätze besser genutzt werden, um die Dateneffizienz des nachgelagerten überwachten Ausleselernens erheblich zu verbessern und die Anfälligkeit für negative Störungen zu verringern, allerdings auf Kosten eines begrenzteren Anwendungsbereichs.
Bestehende Modelle, die auf LSTMs basieren, benötigen eine große Anzahl von Parametern, um den externen Speicher zu unterstützen, und lassen sich für lange Sequenzeingaben nicht gut verallgemeinern. Speichernetzwerke versuchen, diese Einschränkungen zu beheben, indem sie Informationen in einem externen Speichermodul speichern, müssen aber alle Eingaben im Speicher untersuchen. Im Unterschied zu früheren Ansätzen handelt es sich bei AMN um eine dynamische Netzwerkarchitektur, die eine variable Anzahl von Speicherbänken erzeugt, die nach der Relevanz der Frage gewichtet werden. So kann der Decoder eine variable Anzahl von Speicherbänken auswählen, um eine Antwort mit weniger Bänken zu konstruieren, was einen Laufzeitkompromiss zwischen Genauigkeit und Geschwindigkeit schafft. AMN wird erstens durch einen neuartigen Bank-Controller ermöglicht, der diskrete Entscheidungen mit hoher Genauigkeit trifft, und zweitens durch die Fähigkeiten eines dynamischen Frameworks (wie PyTorch), das dynamische Netzwerkgrößen und effizientes variables Mini-Batching ermöglicht. In unseren Ergebnissen zeigen wir, dass unser Modell lernt, eine unterschiedliche Anzahl von Speicherbänken auf der Grundlage der Aufgabenkomplexität zu konstruieren, und schnellere Inferenzzeiten für Standard-BAbI-Aufgaben und modifizierte BAbI-Aufgaben erreicht.
Wenn ein zweisprachiger Schüler lernt, Wortprobleme in Mathematik zu lösen, erwarten wir, dass der Schüler in der Lage ist, diese Probleme in beiden Sprachen, die er fließend beherrscht, zu lösen, auch wenn der Mathematikunterricht nur in einer Sprache unterrichtet wurde.Allerdings sind die derzeitigen Repräsentationen beim maschinellen Lernen sprachabhängig.In dieser Arbeit stellen wir eine Methode vor, die Sprache vom Problem zu entkoppeln, indem wir sprachunabhängige Repräsentationen lernen und somit das Training eines Modells in einer Sprache und die Anwendung auf eine andere in einer Nullschussweise ermöglichen. Wir lernen diese Repräsentationen, indem wir uns von der Linguistik inspirieren lassen, insbesondere von der Hypothese der universellen Grammatik, und lernen universelle latente Repräsentationen, die sprachunabhängig sind (Chomsky, 2014; Montague, 1970) Wir demonstrieren die Fähigkeiten dieser Repräsentationen, indem wir zeigen, dass die Modelle, die in einer einzigen Sprache mit sprachunabhängigen Repräsentationen trainiert wurden, sehr ähnliche Genauigkeiten in anderen Sprachen erreichen.
Generative Modelle mit sowohl diskreten als auch kontinuierlichen latenten Variablen sind durch die Struktur vieler realer Datensätze hoch motiviert, weisen jedoch Feinheiten bei der Ausbildung auf, die sich oft darin manifestieren, dass die diskrete latente Variable nicht genutzt wird.In diesem Papier zeigen wir, warum solche Modelle mit der traditionellen Log-Likelihood-Maximierung nur schwer trainiert werden können und dass sie mit dem Optimal-Transport-Rahmen von Wasserstein-Autoencodern trainierbar sind. Wir stellen fest, dass unsere diskrete latente Variable vom Modell vollständig genutzt wird, wenn es trainiert wird, ohne dass Änderungen an der Zielfunktion oder eine signifikante Feinabstimmung erforderlich sind, und dass unser Modell vergleichbare Stichproben wie andere Ansätze generiert, während es relativ einfache neuronale Netze verwendet, da die diskrete latente Variable einen Großteil der Beschreibungslast trägt und darüber hinaus eine erhebliche Kontrolle über die Generierung bietet.
Während maschinelle Lernmodelle bei sequentiellen Daten eine mit dem Menschen vergleichbare Leistung erzielen, ist die Nutzung von strukturiertem Wissen immer noch ein schwieriges Problem.räumlich-zeitliche Graphen haben sich als nützliches Werkzeug zur Abstraktion von Interaktionsgraphen erwiesen, und frühere Arbeiten nutzen eine sorgfältig entworfene Feed-Forward-Architektur, um diese Struktur zu erhalten. Das Erlernen einer solchen Interaktionsstruktur ist nicht trivial: Einerseits muss ein Modell die versteckten Beziehungen zwischen verschiedenen Problemfaktoren auf unbeaufsichtigte Weise entdecken; andererseits müssen die gefundenen Beziehungen interpretierbar sein. In diesem Beitrag schlagen wir ein Aufmerksamkeitsmodul vor, das in der Lage ist, eine Graphenunterstruktur in eine Einbettung fester Größe zu projizieren, wobei der Einfluss der Nachbarn auf einen bestimmten Vertex erhalten bleibt.In einer umfassenden Evaluierung, die sowohl mit realen als auch mit Spielzeugaufgaben durchgeführt wurde, hat sich unser Modell als konkurrenzfähig gegenüber starken Basislösungen erwiesen.
Wir stellen NAMSG vor, einen adaptiven Algorithmus erster Ordnung für das Training neuronaler Netze, der rechen- und speichereffizient und einfach zu implementieren ist und die Gradienten an konfigurierbaren entfernten Beobachtungspunkten berechnet, um die Konvergenz durch Anpassung der Schrittgröße für Richtungen mit unterschiedlichen Krümmungen in der stochastischen Umgebung zu beschleunigen und den Aktualisierungsvektor elementweise durch einen nicht-steigenden Vorkonditionierer zu skalieren, um die Vorteile von AMSGRAD zu nutzen. Wir analysieren die Konvergenzeigenschaften sowohl für konvexe als auch für nicht-konvexe Probleme, indem wir den Trainingsprozess als dynamisches System modellieren und eine Strategie zur Auswahl des Beobachtungsfaktors ohne Gittersuche bereitstellen. Es wird eine datenabhängige Bedauernsschranke vorgeschlagen, um die Konvergenz in der konvexen Umgebung zu garantieren, und die Methode kann darüber hinaus eine O(log(T))-Bedauernsschranke für stark konvexe Funktionen erreichen.Experimente zeigen, dass NAMSG in praktischen Problemen gut funktioniert und mit populären adaptiven Methoden wie ADAM, NADAM und AMSGRAD vergleichbar ist.
Die jüngsten Fortschritte im Bereich der generativen adversen Netze, die durch Verbesserungen des Rahmens und die erfolgreiche Anwendung auf verschiedene Probleme ermöglicht wurden, haben zu Erweiterungen in verschiedenen Bereichen geführt.IRGAN versucht, den Rahmen für Information-Retrieval (IR) zu nutzen, eine Aufgabe, die als Modellierung der korrekten bedingten Wahrscheinlichkeitsverteilung p(d|q) über die Dokumente (d) beschrieben werden kann, wenn die Anfrage (q) gegeben ist. Die Arbeit, die IRGAN vorschlägt, behauptet, dass die Optimierung ihrer Minimax-Verlustfunktion zu einem Generator führt, der die Verteilung erlernen kann, aber ihr Aufbau und ihr Basisterminus lenken das Modell weg von einer exakten gegnerischen Formulierung, und diese Arbeit versucht, auf bestimmte Ungenauigkeiten in ihrer Formulierung hinzuweisen.die Analyse ihrer Verlustkurven gibt einen Einblick in mögliche Fehler in den Verlustfunktionen, und eine bessere Leistung kann durch die Verwendung des Co-Trainings wie der Aufbau, den wir vorschlagen, erzielt werden, bei dem zwei Modelle in einer kooperativen statt einer gegnerischen Weise trainiert werden.
Wir schlagen Federated User Representation Learning (FURL) vor, eine einfache, skalierbare, datenschutzfreundliche und ressourceneffiziente Methode, um bestehende neuronale Personalisierungstechniken im Rahmen des Federated Learning (FL) zu nutzen. FURL teilt die Modellparameter in föderierte und private Parameter auf: Private Parameter, wie z.B. private Benutzereinbettungen, werden lokal trainiert, aber im Gegensatz zu föderierten Parametern werden sie nicht auf den Server übertragen oder dort gemittelt. Wir zeigen theoretisch, dass diese Aufteilung der Parameter das Training für die meisten Personalisierungsansätze nicht beeinträchtigt. Die lokale Speicherung von Benutzereinbettungen bewahrt nicht nur die Privatsphäre der Benutzer, sondern verbessert auch die Speicherlokalität der Personalisierung im Vergleich zum Training auf dem Server. Wir evaluieren FURL an zwei Datensätzen und zeigen eine signifikante Verbesserung der Modellqualität mit Leistungssteigerungen von 8 % und 51 % und ungefähr das gleiche Leistungsniveau wie bei zentralisiertem Training mit nur 0 % und 4 % Reduktion, und wir zeigen, dass Benutzereinbettungen, die in FL und der zentralisierten Einstellung gelernt werden, eine sehr ähnliche Struktur haben, was darauf hindeutet, dass FURL durch die gemeinsam genutzten Parameter kollaborativ lernen kann, während die Privatsphäre des Benutzers erhalten bleibt.
Rekurrente Modelle für Sequenzen haben sich in letzter Zeit bei vielen Aufgaben bewährt, insbesondere bei der Sprachmodellierung und der maschinellen Übersetzung.Dennoch bleibt es eine Herausforderung, gute Repräsentationen aus diesen Modellen zu extrahieren.Obwohl Sprache beispielsweise eine klare hierarchische Struktur hat, die von Zeichen über Wörter bis hin zu Sätzen reicht, ist sie in aktuellen Sprachmodellen nicht offensichtlich.Wir schlagen vor, die Repräsentation in Sequenzmodellen zu verbessern, indem wir aktuelle Ansätze mit einem Autoencoder ergänzen, der gezwungen ist, die Sequenz durch einen diskreten latenten Zwischenraum zu komprimieren. Um Gradienten durch diese diskrete Repräsentation zu propagieren, führen wir ein verbessertes semantisches Hashing-Verfahren ein. Wir zeigen, dass dieses Verfahren bei einem neu vorgeschlagenen quantitativen Effizienzmaß gut abschneidet. Wir analysieren auch latente Codes, die von dem Modell erzeugt werden, und zeigen, wie sie Wörtern und Phrasen entsprechen.
 Diese Modelle verwenden jedoch in der Regel den vollständigen euklidischen Raum oder eine begrenzte Teilmenge (wie $[0,1]^l$) als latenten Raum, dessen triviale Geometrie oft zu einfach ist, um die Struktur der Daten sinnvoll widerzuspiegeln.Dieses Papier zielt darauf ab, eine nicht-triviale geometrische Struktur des latenten Raums für eine bessere Datendarstellung zu erforschen. Inspiriert von der Differentialgeometrie schlagen wir einen Textbf{Chart Auto-Encoder (CAE)} vor, der die vielfältige Struktur der Daten mit mehreren Diagrammen und Übergangsfunktionen zwischen ihnen erfasst.CAE übersetzt die mathematische Definition von vielfältig durch Parametrisierung des gesamten Datensatzes als eine Sammlung von überlappenden Diagrammen, die lokale latente Repräsentationen schaffen.Diese Repräsentationen sind eine Verbesserung des latenten Raums mit nur einem Diagramm, der üblicherweise in Auto-Encoding-Modellen verwendet wird, da sie die intrinsische Struktur des vielfältigen Raums reflektieren.  Wir führen Experimente mit synthetischen und realen Daten durch, um die Wirksamkeit des vorgeschlagenen CAE zu demonstrieren.
Wir befassen uns mit dem Problem der Modellierung sequentieller visueller Phänomene: Bei Beispielen eines Phänomens, das in diskrete Zeitschritte unterteilt werden kann, zielen wir darauf ab, eine Eingabe von einem beliebigen Zeitpunkt zu nehmen und diese Eingabe in allen anderen Zeitschritten in der Sequenz zu realisieren, und zwar ohne die Schwierigkeiten, die für die Sammlung von abgeglichenen Daten erforderlich sind. Wir erweitern die Zykluskonsistenz auf \textit{loop consistency} und verringern die Schwierigkeiten, die mit dem Lernen in den daraus resultierenden langen Berechnungsketten verbunden sind.Wir zeigen konkurrenzfähige Ergebnisse im Vergleich zu bestehenden Bild-zu-Bild-Verfahren bei der Modellierung verschiedener Datensätze, einschließlich der Jahreszeiten der Erde und der Alterung menschlicher Gesichter.
Wir schlagen Stochastic Weight Averaging in Parallel (SWAP) vor, einen Algorithmus zur Beschleunigung des DNN-Trainings. Unser Algorithmus verwendet große Mini-Batches, um eine ungefähre Lösung schnell zu berechnen, und verfeinert sie dann durch Mittelung der Gewichte mehrerer Modelle, die unabhängig und parallel berechnet werden. Die resultierenden Modelle verallgemeinern genauso gut wie diejenigen, die mit kleinen Mini-Batches trainiert wurden, werden aber in wesentlich kürzerer Zeit erzeugt. Wir demonstrieren die Reduzierung der Trainingszeit und die gute Verallgemeinerungsleistung der resultierenden Modelle auf den Computer-Vision-Datensätzen CIFAR10, CIFAR100 und ImageNet.
Eine gängige Methode zur Beschleunigung des Trainings großer Faltungsnetze ist das Hinzufügen von Recheneinheiten. Das Training wird dann mit Hilfe von datenparallelem, synchronem stochastischem Gradientenabstieg (SGD) durchgeführt, wobei ein Ministapel auf die Recheneinheiten aufgeteilt wird.Mit zunehmender Anzahl von Knoten wächst die Stapelgröße. Um diese Optimierungsschwierigkeiten zu überwinden, schlagen wir einen neuen Trainingsalgorithmus vor, der auf der schichtweisen adaptiven Raten-Skalierung (LARS) basiert. Mit LARS haben wir AlexNet und ResNet-50 auf eine Stapelgröße von 16K skaliert.
Die Koopman-Operator-Theorie bildet die Grundlage für die Identifizierung der nichtlinearen zu linearen Koordinatentransformationen mit datengesteuerten Methoden.Kürzlich haben Forscher vorgeschlagen, tiefe neuronale Netze als eine aussagekräftigere Klasse von Basisfunktionen für die Berechnung der Koopman-Operatoren zu verwenden.Diese Ansätze gehen jedoch von einem festdimensionalen Zustandsraum aus; sie sind daher nicht auf Szenarien mit einer variablen Anzahl von Objekten anwendbar. In diesem Papier schlagen wir vor, kompositorische Koopman-Operatoren zu erlernen, indem wir graphische neuronale Netze verwenden, um den Zustand in objektzentrierte Einbettungen zu kodieren und eine blockweise lineare Übergangsmatrix zu verwenden, um die gemeinsame Struktur zwischen den Objekten zu regulieren.Die erlernte Dynamik kann sich schnell an neue Umgebungen mit unbekannten physikalischen Parametern anpassen und Steuersignale erzeugen, um ein bestimmtes Ziel zu erreichen.Unsere Experimente zur Manipulation von Seilen und zur Steuerung von Softrobotern zeigen, dass die vorgeschlagene Methode eine bessere Effizienz und Generalisierungsfähigkeit hat als die bestehenden Grundlinien.
Wir leiten eine automatische Differenzierung im Rückwärtsmodus (oder adjungiert) für Lösungen stochastischer Differentialgleichungen (SDEs) ab, die eine zeiteffiziente und speicherkonstante Berechnung pfadweiser Gradienten ermöglicht, ein zeitkontinuierliches Analogon des Reparametrisierungstricks, und konstruieren eine rückwärtsgerichtete SDE, deren Lösung der Gradient ist, und geben Bedingungen an, unter denen numerische Lösungen konvergieren. Außerdem kombinieren wir unseren stochastischen adjungierten Ansatz mit einem stochastischen Variationsinferenzschema für zeitkontinuierliche SDE-Modelle, das es uns ermöglicht, Verteilungen über Funktionen mit Hilfe des stochastischen Gradientenabstiegs zu erlernen.Unser latentes SDE-Modell erreicht eine konkurrenzfähige Leistung im Vergleich zu bestehenden Ansätzen für die Modellierung von Zeitreihen.
Es ist klar, dass die Nutzer ihre Daten und ihre Privatsphäre besitzen und kontrollieren sollten, aber auch die Anbieter von Diensten sind zunehmend an der Gewährleistung des Datenschutzes interessiert, weshalb Nutzer und Anbieter bei der Bewältigung der Herausforderungen des Datenschutzes zusammenarbeiten können und sollten. Wir schlagen einen Rahmen vor, in dem der Nutzer kontrolliert, welche Eigenschaften der Daten er teilen möchte (Nutzen) und welche er geheim halten möchte (Geheimnis), ohne dass der Anbieter des Dienstes notwendigerweise seine bestehenden Algorithmen für maschinelles Lernen ändern muss.Wir analysieren zunächst den Raum der datenschutzfreundlichen Darstellungen und leiten natürliche informationstheoretische Grenzen für den Kompromiss zwischen Nutzen und Privatsphäre bei der Offenlegung einer bereinigten Version der Daten X ab. Wir stellen explizite Lernarchitekturen vor, um datengetrieben datenschützende Repräsentationen zu erlernen, die sich dieser Schranke annähern, und beschreiben wichtige Anwendungsszenarien, in denen die Anbieter von Diensten bereit sind, mit dem Bereinigungsprozess zusammenzuarbeiten, und untersuchen raumerhaltende Transformationen, bei denen der Anbieter von Diensten denselben Algorithmus für ursprüngliche und bereinigte Daten verwenden kann. Wir veranschaulichen diesen Rahmen durch die Implementierung von drei Anwendungsfällen: Subjekt-im-Subjekt, bei dem wir das Problem eines Gesichtsidentitätsdetektors angehen, der nur bei einer zustimmenden Untergruppe von Nutzern funktioniert, eine wichtige Anwendung, zum Beispiel für mobile Geräte, die durch Gesichtserkennung aktiviert werden; Geschlecht-und-Subjekt, bei dem wir die Gesichtsverifizierung beibehalten, während wir das Geschlechtsattribut für Nutzer verbergen, die dies wünschen; und Emotion-und-Geschlecht, bei dem wir unabhängige Variablen verbergen, wie im Fall des Verbergens des Geschlechts bei gleichzeitiger Beibehaltung der Emotionserkennung.
Die Lösung von Aufgaben mit spärlichen Belohnungen ist eine der wichtigsten Herausforderungen beim Reinforcement Learning. Im Single-Agent Setting wurde diese Herausforderung durch die Einführung intrinsischer Belohnungen angegangen, die Agenten motivieren, ungesehene Regionen ihrer Zustandsräume zu erforschen. In diesem Papier schlagen wir einen Ansatz vor, wie man dynamisch zwischen verschiedenen Arten von intrinsischen Belohnungen wählen kann, die nicht nur berücksichtigen, was ein einzelner Agent erforscht hat, sondern alle Agenten, so dass die Agenten ihre Erforschung koordinieren und die extrinsischen Erträge maximieren können. Konkret formulieren wir den Ansatz als eine hierarchische Politik, bei der ein High-Level-Controller aus einer Reihe von Politiken auswählt, die auf verschiedenen Arten von intrinsischen Belohnungen trainiert wurden, und die Low-Level-Controller die Aktionspolitik aller Agenten unter diesen spezifischen Belohnungen erlernen. Wir demonstrieren die Wirksamkeit des vorgeschlagenen Ansatzes in einer Multi-Agenten-Gridworld-Domäne mit spärlichen Belohnungen und zeigen dann, dass unsere Methode auf komplexere Einstellungen skaliert, indem wir sie auf der VizDoom-Plattform evaluieren.
Ein ideales visuelles System muss die vorhandenen visuellen Konzepte zuverlässig erkennen und gleichzeitig mit wenigen Trainingsinstanzen effizient neue Kategorien erlernen. In diesem Papier untersuchen wir das Problem des verallgemeinerten few-shot learning (GFSL) - ein Modell muss während des Einsatzes nicht nur über "tail"-Kategorien mit wenigen Schüssen lernen, sondern gleichzeitig die "head"- und "tail"-Kategorien klassifizieren. Wir schlagen das Classifier Synthesis Learning (CASTLE) vor, einen Lernrahmen, der lernt, wie man kalibrierte few-shot-Klassifikatoren zusätzlich zu den Multi-Klassen-Klassifikatoren der ``head''-Klassen synthetisiert, indem er ein gemeinsames neuronales Wörterbuch nutzt.CASTLE beleuchtet die induktive GFSL durch Optimierung eines sauberen und effektiven GFSL-Lernziels. CASTLE zeigt eine bessere Leistung als bestehende GFSL-Algorithmen und starke Baselines auf MiniImageNet- und TieredImageNet-Datensätzen, und, was noch interessanter ist, es übertrifft frühere State-of-the-Art-Methoden, wenn es auf standardmäßigem few-shot-Lernen basiert.
Die derzeitige Generation von Frameworks stützt sich auf benutzerdefinierte Back-Ends, um Effizienz zu erreichen, was es unpraktisch macht, Modelle auf weniger verbreiteter Hardware zu trainieren, wo keine solchen Back-Ends existieren.Knossos baut auf neueren Arbeiten auf, die die Notwendigkeit für handgeschriebene Bibliotheken vermeiden und stattdessen maschinelle Lernmodelle auf ähnliche Weise kompilieren, wie man andere Arten von Software kompilieren würde.Um den resultierenden Code effizient zu machen, optimiert der Knossos-Compiler direkt den abstrakten Syntaxbaum des Programms. Im Gegensatz zu traditionellen Compilern, die handgeschriebene Optimierungspassagen verwenden, verwenden wir einen Rewriting-Ansatz, der durch den $A^\star$-Suchalgorithmus und eine Lernwertfunktion gesteuert wird, die die potenzielle Kostenreduzierung verschiedener Rewriting-Aktionen für das Programm bewertet.Wir zeigen, dass Knossosos automatisch Optimierungen erlernen kann, die bisherige Compiler von Hand implementieren mussten. Darüber hinaus zeigen wir, dass Knossos im Vergleich zu einem von Hand angepassten Compiler bei einer Reihe von maschinellen Lernprogrammen, einschließlich grundlegender linearer Algebra und Faltungsnetzwerken, eine Wandzeitreduzierung erzielen kann.der Knossos-Compiler hat minimale Abhängigkeiten und kann auf jeder Architektur verwendet werden, die eine \Cpp-Toolchain unterstützt. Da das Kostenmodell, das der vorgeschlagene Algorithmus optimiert, auf eine bestimmte Hardwarearchitektur zugeschnitten werden kann, lässt sich der vorgeschlagene Ansatz potenziell auf eine Vielzahl von Hardware anwenden.
Das Greifen eines Objekts und das präzise Stapeln auf einem anderen ist eine schwierige Aufgabe für die traditionelle Robotersteuerung oder für handwerkliche Ansätze. Hier untersuchen wir das Problem in der Simulation und bieten Techniken an, die darauf abzielen, es durch Deep Reinforcement Learning zu lösen. Unsere Ergebnisse zeigen, dass es durch die umfangreiche Nutzung von Off-Policy-Daten und Replay möglich ist, leistungsstarke Steuerungsstrategien zu finden, und dass es bald möglich sein könnte, erfolgreiche Stapelungsstrategien durch das Sammeln von Interaktionen auf echten Robotern zu trainieren.
In den letzten Jahren hat sich gezeigt, dass Deep Reinforcement Learning in der Lage ist, sequentielle Entscheidungsprozesse mit hochdimensionalen Zustandsräumen zu lösen, wie z.B. in den Atari-Spielen.Viele Reinforcement-Learning-Probleme beinhalten jedoch sowohl hochdimensionale diskrete Aktionsräume als auch hochdimensionale Zustandsräume.In diesem Papier entwickeln wir eine neuartige Policy-Gradienten-Methode für den Fall großer multidimensionaler diskreter Aktionsräume.Wir schlagen zwei Ansätze zur Erstellung parametrisierter Policies vor: LSTM-Parametrisierung und ein modifiziertes MDP (MMDP), das zu einer Feed-Forward Network (FFN)-Parametrisierung führt.Beide Ansätze liefern ausdrucksstarke Modelle, auf die Backpropagation für das Training angewendet werden kann.Wir betrachten dann den Entropie-Bonus, der typischerweise zur Belohnungsfunktion hinzugefügt wird, um die Exploration zu verbessern. Im Fall von hochdimensionalen Aktionsräumen erfordert die Berechnung der Entropie und des Gradienten der Entropie die Aufzählung aller Aktionen im Aktionsraum und die Durchführung von Vorwärts- und Backpropagation für jede Aktion, was rechnerisch undurchführbar sein kann.Wir entwickeln mehrere neuartige unverzerrte Schätzer für den Entropie-Bonus und seinen Gradienten.Schließlich testen wir unsere Algorithmen auf zwei Umgebungen: ein Multi-Hunter-Multi-Hasen-Grid-Spiel und ein Multi-Agenten-Multi-Arm-Bandit Problem.
Wir schlagen eine Technik namens Deep Learning Approximation vor, um aus einem bereits trainierten neuronalen Netzwerkmodell ein schnelleres (und fast genauso genaues) Netzwerk zu erstellen, indem wir die Netzwerkstruktur und die Koeffizienten manipulieren, ohne dass ein erneutes Training oder ein Zugriff auf die Trainingsdaten erforderlich ist. Eine optimale verlustbehaftete Annäherung wird für jede Schicht durch Abwägen des relativen Genauigkeitsverlusts und der FLOP-Reduzierung gewählt. Auf PASCAL VOC 2007 mit dem YOLO-Netzwerk zeigen wir eine durchgängige 2-fache Beschleunigung in einem Netzwerk-Vorwärtsdurchlauf mit einem Rückgang von $5$\% in mAP, der durch Feinabstimmung wiedergewonnen werden kann, wodurch dieses Netzwerk (und andere wie es) in rechenbeschränkten Systemen eingesetzt werden kann.
Asynchrone verteilte Methoden sind ein beliebter Weg, um die Kommunikations- und Synchronisationskosten der groß angelegten Optimierung zu reduzieren, aber trotz ihres Erfolges ist wenig über ihre Konvergenzgarantien im schwierigen Fall allgemeiner nicht-glatter, nicht-konvexer Ziele bekannt, abgesehen von den Fällen, in denen geschlossene Proximaloperatorlösungen verfügbar sind, was umso überraschender ist, als diese Ziele beim Training tiefer neuronaler Netze auftreten. Unsere Analyse gilt für stochastische Subgradientenabstiegsverfahren mit und ohne Blockvariablenpartitionierung sowie mit und ohne Momentum und wird im Kontext eines allgemeinen probabilistischen Modells der asynchronen Planung formuliert, das genau an moderne Hardwareeigenschaften angepasst ist.Wir validieren unsere Analyse experimentell im Zusammenhang mit dem Training tiefer neuronaler Netzarchitekturen.Wir zeigen ihre insgesamt erfolgreiche asymptotische Konvergenz und untersuchen, wie Momentum, Synchronisation und Partitionierung die Leistung beeinflussen.
Stochastische neuronale Netze mit diskreten Zufallsvariablen sind eine wichtige Klasse von Modellen für ihre Ausdruckskraft und Interpretierbarkeit. Da eine direkte Differenzierung und Backpropagation nicht möglich ist, wurden Monte-Carlo-Gradientenschätzverfahren häufig für das Training solcher Modelle eingesetzt. Effiziente stochastische Gradientenschätzer, wie Straight-Through und Gumbel-Softmax, funktionieren gut für flache Modelle mit einer oder zwei stochastischen Schichten, ihre Leistung leidet jedoch mit zunehmender Modellkomplexität.In dieser Arbeit konzentrieren wir uns auf stochastische Netzwerke mit mehreren Schichten von booleschen latenten Variablen.Um solche Netzwerke zu analysieren, verwenden wir den Rahmen der harmonischen Analyse für boolesche Funktionen.  Basierend auf dieser Analyse schlagen wir \emph{FouST} vor, einen einfachen Gradientenschätzungsalgorithmus, der auf drei einfachen Schritten zur Reduzierung der Verzerrung beruht. Nach unserem Kenntnisstand ist FouST der erste Gradientenschätzer, der sehr tiefe stochastische neuronale Netze mit bis zu 80 deterministischen und 11 stochastischen Schichten trainieren kann.
Wir schlagen ein neuartiges generatives adversariales Netz für die Manipulation visueller Attribute (ManiGAN) vor, das in der Lage ist, die visuellen Attribute gegebener Bilder mit Hilfe von Beschreibungen in natürlicher Sprache semantisch zu modifizieren Der Schlüssel zu unserer Methode ist die Entwicklung eines neuartigen Co-Attention-Moduls, das Text- und Bildinformationen kombiniert, anstatt einfach zwei Merkmale entlang der Kanalrichtung zu verketten. Schließlich schlagen wir eine neue Metrik für die Bewertung von Manipulationsergebnissen vor, sowohl in Bezug auf die Erzeugung von textbezogenen Attributen als auch auf die Rekonstruktion von textfremden Inhalten. Umfangreiche Experimente mit Benchmark-Datensätzen zeigen die Vorteile unserer vorgeschlagenen Methode hinsichtlich der Effektivität der Bildmanipulation und der Fähigkeit, qualitativ hochwertige Ergebnisse zu erzeugen.
Um das Problem der Skalierbarkeit zu lösen, schlagen wir ein implizites generatives Lernverfahren vor, das wir SPOT (Scalable Push-forward of Optimal Transport) nennen. Wir approximieren den optimalen Transportplan durch ein Push-forward einer Referenzverteilung und wandeln das optimale Transportproblem in ein Minimax-Problem um. Wir zeigen auch, dass wir die Dichte des optimalen Transportplans mit Hilfe von neuronalen gewöhnlichen Differentialgleichungen wiederherstellen können.Numerische Experimente an synthetischen und realen Datensätzen zeigen, dass SPOT robust ist und ein günstiges Konvergenzverhalten aufweist.SPOT ermöglicht es uns auch, effizient Stichproben aus dem optimalen Transportplan zu ziehen, was nachgelagerten Anwendungen wie der Domänenanpassung zugute kommt.
In dieser Arbeit schlagen wir eine neuartige Formulierung der Planung vor, die diese als probabilistisches Inferenzproblem über zukünftige optimale Trajektorien betrachtet, was uns ermöglicht, Sampling-Methoden zu verwenden und somit die Planung in kontinuierlichen Domänen mit einem festen Rechenbudget anzugehen.   Wir entwerfen einen neuen Algorithmus, Sequential Monte Carlo Planning, indem wir die klassischen Methoden des Sequential Monte Carlo und der Bayes'schen Glättung im Kontext der Steuerung als Inferenz nutzen und zeigen, dass Sequential Monte Carlo Planning multimodale Strategien erfassen kann und schnell kontinuierliche Steuerungsaufgaben erlernen kann.
Wir schlagen eine architektonische Modifikation, die Selbstmodulation, vor und untersuchen sie, um die Leistung von GANs über verschiedene Datensätze, Architekturen, Verluste, Regularisierer und Hyperparametereinstellungen hinweg zu verbessern. In einer groß angelegten empirischen Studie haben wir eine relative Verringerung der FID um 5 % bis 35 % beobachtet, und wenn alle anderen Faktoren gleich sind, führt das Hinzufügen dieser Modifikation zum Generator zu einer verbesserten Leistung in 124/144 (86 %) der untersuchten Einstellungen.
Extreme Klassifizierungsmethoden sind insbesondere für Information Retrieval (IR)-Probleme von größter Bedeutung geworden, da intelligente Algorithmen entwickelt wurden, die für die Herausforderungen der Industrie skalierbar sind. Kürzlich wurde ein neuartiger Ansatz namens MACH (Merged Average Classifiers via Hashing) vorgeschlagen, der die riesigen Label-Vektoren auf eine kleine und überschaubare Count-Min-Sketch-Matrix (CMS) projiziert und dann lernt, diese Matrix vorherzusagen, um die ursprünglichen Vorhersagewahrscheinlichkeiten wiederherzustellen. MACH ist ein einfacher Algorithmus, der in der Praxis außerordentlich gut funktioniert. Trotz dieser Einfachheit von MACH gibt es eine große Lücke im theoretischen Verständnis der Kompromisse mit MACH. In diesem Papier füllen wir diese Lücke. Um dieses Problem zu entschärfen, schlagen wir eine neuartige quadratische Approximation vor, die das Einschluss-Ausschluss-Prinzip verwendet. Unser Schätzer hat einen signifikant niedrigeren Rekonstruktionsfehler als der typische CMS-Schätzer für verschiedene Werte der Anzahl der Klassen K, der Beschriftungssparsamkeit und des Kompressionsverhältnisses.
Neuronale Netze werden üblicherweise als Klassifizierungsmodelle für eine Vielzahl von Aufgaben verwendet, wobei am Ende eines solchen Modells eine gelernte affine Transformation steht, die einen Wert pro Klasse ergibt, der für die Klassifizierung verwendet werden kann. In dieser Arbeit zeigen wir, dass dieser Klassifikator bis zu einer globalen Skalenkonstante mit geringem oder gar keinem Genauigkeitsverlust für die meisten Aufgaben festgelegt werden kann, was Speicher- und Rechenvorteile mit sich bringt, und dass wir durch die Initialisierung des Klassifikators mit einer Hadamard-Matrix auch die Inferenz beschleunigen können.
In diesem Beitrag wird NEMO vorgestellt, ein Ansatz zur unbeaufsichtigten Objekterkennung, der Bewegung - anstelle von Bildbeschriftungen - als Anhaltspunkt für das Erlernen der Objekterkennung verwendet und zur Unterscheidung zwischen der Bewegung des Zielobjekts und anderen Veränderungen im Bild auf Negativbeispiele zurückgreift, die die Szene ohne das Objekt zeigen. Die benötigten Daten können sehr einfach durch die Aufnahme von zwei kurzen Videos gesammelt werden, ein positives, das das Objekt in Bewegung zeigt, und ein negatives, das die Szene ohne das Objekt zeigt. Ohne zusätzliche Form des Vortrainings oder der Überwachung und trotz Verdeckungen, Ablenkungen, Kamerabewegungen und ungünstiger Beleuchtung reichen diese Videos aus, um Objektdetektoren zu erlernen, die auf neue Videos angewandt werden können und sogar auf ungesehene Szenen und Kamerawinkel generalisiert werden können. Die erlernten Objektrepräsentationen sind auch genau genug, um die relevanten Informationen aus Demonstrationen von Manipulationsaufgaben zu erfassen, was sie für das Lernen aus Demonstrationen in der Robotik anwendbar macht. Ein Beispiel für Objekterkennung, die aus 3 Minuten Video gelernt wurde, finden Sie hier: http://y2u.be/u_jyz9_ETz4
Es ist an der Zeit, anspruchsvollere Datensätze einzuführen, um die Entwicklung auf diesem Gebiet in Richtung eines umfassenderen Textverständnisses voranzutreiben.In diesem Papier stellen wir einen neuen Leseverständnis-Datensatz vor, der logisches Denken erfordert (ReClor) und aus standardisierten Hochschulzulassungsprüfungen gewonnen wurde.Wie frühere Studien nahelegen, enthalten von Menschen kommentierte Datensätze in der Regel Verzerrungen, die oft von Modellen ausgenutzt werden, um eine hohe Genauigkeit zu erreichen, ohne den Text wirklich zu verstehen. Empirische Ergebnisse zeigen, dass die State-of-the-Art-Modelle eine herausragende Fähigkeit haben, die im Datensatz enthaltenen Verzerrungen mit hoher Genauigkeit im EASY-Set zu erfassen, dass sie jedoch im HARD-Set mit einer schlechten Leistung nahe der des zufälligen Ratens zu kämpfen haben, was darauf hindeutet, dass weitere Forschung notwendig ist, um die logische Schlussfolgerungsfähigkeit der aktuellen Modelle wesentlich zu verbessern.
Dieses Papier untersucht die Szenarien, unter denen ein Angreifer behaupten kann, dass "Rauschen und Zugriff auf die Softmax-Schicht des Modells alles ist, was man braucht", um die Gewichte eines neuronalen Faltungsnetzes zu stehlen, dessen Architektur bereits bekannt ist.Wir konnten 96% Testgenauigkeit mit dem gestohlenen MNIST-Modell und 82% Genauigkeit mit dem gestohlenen KMNIST-Modell erreichen, das nur mit i.d. Bernoulli-Rauscheingaben gelernt wurde. Wir gehen davon aus, dass die Anfälligkeit der Gewichte für Diebstahl ein Indikator für die Komplexität des Datensatzes ist, und schlagen eine neue Metrik vor, die dasselbe erfasst.Das Ziel dieser Veröffentlichung ist es, nicht nur zu zeigen, wie weit man mit dem Wissen über die Architektur in Bezug auf Modelldiebstahl kommen kann, sondern auch die Aufmerksamkeit auf diese eher idiosynkratischen Aspekte der Lernfähigkeit von CNNs zu lenken, die durch i. Wir verbreiten auch einige erste Ergebnisse, die mit der Verwendung der Ising-Wahrscheinlichkeitsverteilung anstelle der i.i.d.Bernoulli-Verteilung erzielt wurden.
Wir schlagen eine neue Form eines Autocodierungsmodells vor, das die besten Eigenschaften von Variations-Autocodierern (VAE) und generativen adversarischen Netzen (GAN) vereint. Unser Modell optimiert die λ-Jeffreys-Divergenz zwischen der Modellverteilung und der wahren Datenverteilung. Wir zeigen, dass es die besten Eigenschaften der VAE- und GAN-Ziele in sich vereint und aus zwei Teilen besteht. Der zweite Teil ist das eigentliche Ziel des VAE-Modells. Der direkte Weg, den VAE-Verlust zu ersetzen, funktioniert jedoch nicht gut, wenn wir eine explizite Likelihood wie Gauß oder Laplace verwenden, die in hohen Dimensionen nur eine begrenzte Flexibilität aufweisen und für die Modellierung von Bildern im Pixelraum ungeeignet sind. In einer umfangreichen Reihe von Experimenten mit den Datensätzen CIFAR-10 und TinyImagent zeigen wir, dass unser Modell die modernste Generierungs- und Rekonstruktionsqualität erreicht, und demonstrieren, wie wir ein Gleichgewicht zwischen mode-seeking und mode-covering Verhalten unseres Modells herstellen können, indem wir die Gewichtung λ in unserem Ziel anpassen.
Dieser Ansatz stößt auf Schwierigkeiten, wenn die Übertragung nicht gegenseitig vorteilhaft ist, z. B. wenn die Aufgaben hinreichend unterschiedlich sind oder sich im Laufe der Zeit ändern. Hier nutzen wir die Verbindung zwischen gradientenbasiertem Meta-Lernen und hierarchischem Bayes, um eine Mischung aus hierarchischen Bayes'schen Modellen über die Parameter eines beliebigen Funktionsapproximators wie einem neuronalen Netz vorzuschlagen. In Verallgemeinerung des modellagnostischen Meta-Learning-Algorithmus (MAML) präsentieren wir ein stochastisches Erwartungsmaximierungsverfahren zur gemeinsamen Schätzung von Parameterinitialisierungen für den Gradientenabstieg sowie eine latente Zuweisung von Aufgaben zu Initialisierungen, die die Vielfalt der Trainingsaufgaben besser erfasst als die Konsolidierung induktiver Verzerrungen in einem einzigen Satz von Hyperparametern. Unsere Experimente zeigen eine bessere Generalisierung auf dem miniImageNet Standard-Benchmark für 1-Shot-Klassifikation, und wir leiten eine neuartige und skalierbare nicht-parametrische Variante unserer Methode ab, die die Entwicklung einer Aufgabenverteilung im Laufe der Zeit erfasst, wie an einer Reihe von few-shot Regressionsaufgaben gezeigt wurde.
Wir stellen einen neuen Routing-Algorithmus für Kapselnetzwerke vor, bei dem eine Kindkapsel nur aufgrund der Übereinstimmung zwischen dem Zustand des Elternteils und der Stimme des Kindes zu einem Elternteil geleitet wird. Im Gegensatz zu früher vorgeschlagenen Routing-Algorithmen wird die Fähigkeit des Elternteils, das Kind zu rekonstruieren, nicht explizit berücksichtigt, um die Routing-Wahrscheinlichkeiten zu aktualisieren. Der neue Mechanismus entwirft 1) das Routing über eine invertierte Punkt-Produkt-Attention; 2) setzt die Layer-Normalisierung als Normalisierung ein; und 3) ersetzt das sequentielle iterative Routing durch ein gleichzeitiges iteratives Routing. Unser Modell übertrifft nicht nur die Leistung bestehender Kapselnetzwerke, sondern ist auch einem leistungsstarken CNN (ResNet-18) ebenbürtig, wobei weniger als 25% der Parameter verwendet werden.  Bei einer anderen Aufgabe, der Erkennung von Ziffern aus überlagerten Ziffernbildern, schneidet das vorgeschlagene Kapselmodell bei gleicher Anzahl von Schichten und Neuronen pro Schicht besser ab als CNNs.  Wir glauben, dass unsere Arbeit die Möglichkeit eröffnet, Kapselnetzwerke auf komplexe reale Aufgaben anzuwenden.
 Wir stellen Doc2Dial vor, ein End-to-End-Framework zur Generierung von Konversationsdaten auf der Grundlage von Geschäftsdokumenten durch Crowdsourcing, die zum Training von automatisierten Dialogagenten verwendet werden können, die Kundenbetreuungsaufgaben für Unternehmen oder Organisationen übernehmen. Das Ergebnis sind Dialogdaten, die in den gegebenen Dokumenten verankert sind, sowie verschiedene Arten von Anmerkungen, die die Qualität der Daten und die Flexibilität der (neu) zusammengesetzten Dialoge gewährleisten.
Die Erfassung von High-Level-Strukturen in Audiowellenformen ist eine Herausforderung, da sich eine einzige Audiosekunde über Zehntausende von Zeitschritten erstreckt.  Während es schwierig ist, weitreichende Abhängigkeiten direkt im Zeitbereich zu modellieren, zeigen wir, dass sie in zweidimensionalen Zeit-Frequenz-Darstellungen, wie z. B. Spektrogrammen, leichter zu modellieren sind.  Indem wir diesen Darstellungsvorteil in Verbindung mit einem sehr ausdrucksstarken probabilistischen Modell und einem Multiskalen-Generierungsverfahren nutzen, entwickeln wir ein Modell, das in der Lage ist, hochqualitative Audio-Samples zu generieren, die Strukturen auf Zeitskalen erfassen, die von Modellen im Zeitbereich noch nicht erreicht werden.  Wir zeigen, dass unser Modell Abhängigkeiten mit größerer Reichweite erfasst als zeitbasierte Modelle wie WaveNet in einer Reihe von bedingungslosen Generierungsaufgaben, einschließlich der Generierung von Sprache mit einem Sprecher, der Generierung von Sprache mit mehreren Sprechern und der Generierung von Musik.
In diesem Papier zeigen wir, dass moderne CNNs (VGG16, ResNet50 und InceptionResNetV2) ihre Ausgabe drastisch verändern können, wenn ein Bild in der Bildebene um ein paar Pixel verschoben wird, und dass dieses Versagen der Generalisierung auch bei anderen realistischen kleinen Bildtransformationen auftritt.Darüber hinaus sehen wir dieses Versagen der Generalisierung häufiger in moderneren Netzwerken. Wir zeigen, dass diese Fehler mit der Tatsache zusammenhängen, dass die Architektur moderner CNNs das klassische Sampling-Theorem ignoriert, so dass die Generalisierung nicht garantiert ist, und dass die Verzerrungen in den Statistiken häufig verwendeter Bilddatensätze es unwahrscheinlich machen, dass CNNs lernen, gegenüber diesen Transformationen invariant zu sein.
Wir stellen eine Echtzeit-Methode zur Synthese hochkomplexer menschlicher Bewegungen vor, die ein neuartiges Trainingsverfahren verwendet, das wir als autokonditioniertes rekurrentes neuronales Netzwerk (acRNN) bezeichnen.In letzter Zeit haben Forscher versucht, neue Bewegungen mit Hilfe autoregressiver Techniken zu synthetisieren, aber die bestehenden Methoden neigen dazu, nach einigen Sekunden aufgrund einer Ansammlung von Fehlern, die in das Netzwerk zurückgeführt werden, einzufrieren oder zu divergieren.Darüber hinaus haben sich solche Methoden nur für relativ einfache menschliche Bewegungen wie Gehen oder Laufen als zuverlässig erwiesen. Im Gegensatz dazu kann unser Ansatz beliebige Bewegungen mit hochkomplexen Stilen synthetisieren, einschließlich Tänzen oder Kampfsportarten zusätzlich zur Fortbewegung.acRNN ist in der Lage, dies zu erreichen, indem es explizit die Akkumulation von autoregressivem Rauschen während des Trainings berücksichtigt.Unsere Arbeit ist unseres Wissens die erste, die die Fähigkeit demonstriert, über 18.000 kontinuierliche Frames (300 Sekunden) neuer komplexer menschlicher Bewegungen mit verschiedenen Stilen zu erzeugen.
 {Saliency-Methoden versuchen, die Entscheidung eines tiefen Netzes zu erklären, indem sie jedem Merkmal/Pixel in der Eingabe eine Punktzahl zuweisen, wobei diese Zuweisung oft über den Gradienten der Ausgabe in Bezug auf die Eingabe erfolgt. Kürzlich hat \citet{adebayosan} die Gültigkeit vieler dieser Methoden in Frage gestellt, da sie einfache {\em sanity checks} nicht bestehen, bei denen getestet wird, ob sich die Punktzahlen verschieben/verändern, wenn Schichten des trainierten Netzes randomisiert werden oder wenn das Netz unter Verwendung zufälliger Bezeichnungen für die Eingaben neu trainiert wird.% für die Eingaben.  %Überraschenderweise bestanden die getesteten Methoden diese Prüfungen nicht: Die Erklärungen waren relativ unverändert. Wir schlagen eine einfache Lösung für die bestehenden Saliency-Methoden vor, die ihnen hilft, die Plausibilitätsprüfungen zu bestehen, und die wir als {\em competition for pixels} bezeichnen. Dies beinhaltet die Berechnung von Saliency-Karten für alle möglichen Etiketten in der Klassifizierungsaufgabe und die Verwendung eines einfachen Wettbewerbs zwischen ihnen, um weniger relevante Pixel zu identifizieren und aus der Karte zu entfernen.
Klassifizierungssysteme agieren in der Regel isoliert, d.h. sie müssen sich implizit die Merkmale aller Kandidatenklassen merken, um eine Klassifizierung vornehmen zu können, was zu einem erhöhten Speicherbedarf und einer schlechten Stichprobeneffizienz führt. Wir zeigen, dass es möglich ist, ein solches Modell zu trainieren und dass es mit der Basisgenauigkeit übereinstimmen kann, während es gleichzeitig parametereffizienter ist. wir zeigen jedoch, dass es wichtig ist, das richtige Gleichgewicht zwischen Bilderkennung und Verifizierung zu finden, um das Modell in Richtung des gewünschten Verhaltens zu bringen, was darauf hindeutet, dass eine Pipeline aus Erkennung gefolgt von Verifizierung ein vielversprechenderer Ansatz ist, um leistungsfähigere Netzwerke mit einfacheren Architekturen zu entwickeln.
Um den Speicherbedarf und die Laufzeitlatenz zu verringern, wurden Techniken wie das Beschneiden neuronaler Netze und die Binarisierung separat erforscht. Es ist jedoch nicht klar, wie man das Beste aus beiden Welten kombinieren kann, um extrem kleine und effiziente Modelle zu erhalten.  In diesem Beitrag definieren wir zum ersten Mal das Pruning-Problem auf Filterebene für binäre neuronale Netze, das nicht einfach durch die Migration bestehender struktureller Pruning-Methoden für Modelle mit voller Genauigkeit gelöst werden kann.  Ein neuartiger, lernbasierter Ansatz wird vorgeschlagen, um Filter in unserem Haupt-/Tochternetzwerk zu beschneiden, wobei das Hauptnetzwerk für das Lernen repräsentativer Merkmale zur Optimierung der Vorhersageleistung verantwortlich ist und die Tochterkomponente als Filterselektor für das Hauptnetzwerk arbeitet. Zur Vermeidung von Gradientenfehlanpassungen beim Training der Nebenkomponente schlagen wir ein schichtweises und Bottom-up-Schema vor.  Wir bieten auch einen theoretischen und experimentellen Vergleich zwischen unseren lernbasierten und regelbasierten Methoden.  Schließlich demonstrieren wir empirisch die Effektivität unseres Ansatzes für verschiedene binäre Modelle, einschließlich binarizedNIN, VGG-11 und ResNet-18, auf verschiedenen Bildklassifizierungsdatensätzen.  Für das binäre ResNet-18 auf ImageNet verwenden wir 78,6% Filter, können aber einen geringfügig besseren Testfehler 49,87% (50,02%-0,15%) als das ursprüngliche Modell erreichen.
Um dieses Problem anzugehen, entwickeln wir AntMan, das strukturierte Sparsamkeit mit Low-Rank-Zerlegung synergetisch kombiniert, um die Modellberechnung, Größe und Ausführungszeit von RNNs zu reduzieren und gleichzeitig die gewünschte Genauigkeit zu erreichen. Unsere Evaluierung zeigt, dass AntMan eine bis zu 100-fache Reduktion der Rechenzeit bei weniger als 1 Punkt Genauigkeitsverlust für Sprach- und maschinelle Leseverstehensmodelle bietet. Unsere Evaluierung zeigt auch, dass AntMan für ein gegebenes Genauigkeitsziel 5x kleinere Modelle als der Stand der Technik produziert.
Graphenstrukturierte Daten wie soziale Netzwerke, funktionale Gehirnnetzwerke, genregulatorische Netzwerke und Kommunikationsnetzwerke haben das Interesse an der Verallgemeinerung von Deep-Learning-Techniken auf Graphen-Domänen geweckt.In diesem Papier sind wir daran interessiert, neuronale Netze für Graphen mit variabler Länge zu entwerfen, um Lernprobleme wie Vertex-Klassifikation, Graphen-Klassifikation, Graphen-Regression und generative Aufgaben zu lösen. Die meisten existierenden Arbeiten haben sich auf rekurrente neuronale Netze (RNNs) konzentriert, um sinnvolle Repräsentationen von Graphen zu lernen, und in jüngster Zeit wurden neue Faltungsneuronale Netze (ConvNets) eingeführt.In dieser Arbeit wollen wir diese beiden grundlegenden Familien von Architekturen rigoros vergleichen, um Graphenlernaufgaben zu lösen. Wir geben einen Überblick über bestehende Graph-RNN- und ConvNet-Architekturen und schlagen eine natürliche Erweiterung von LSTM und ConvNet auf Graphen beliebiger Größe vor, bevor wir eine Reihe von analytisch kontrollierten Experimenten zu zwei grundlegenden Graphenproblemen, d.h. Subgraph-Matching und Graph-Clustering, durchführen, um die verschiedenen Architekturen zu testen.  Die numerischen Ergebnisse zeigen, dass die vorgeschlagenen Graph ConvNets 3-17% genauer und 1,5-4x schneller sind als Graph RNNs.Graph ConvNets sind auch 36% genauer als variationale (nicht lernende) Techniken.Schließlich verwendet die effektivste Graph ConvNet Architektur gated Kanten und Residualität.Residualität spielt eine wesentliche Rolle, um mehrschichtige Architekturen zu lernen, da sie eine 10%ige Leistungssteigerung bieten.
Komplexe neuronale Netze sind kein neues Konzept, jedoch wurde die Verwendung von reellen Werten aufgrund von Schwierigkeiten beim Training und der Genauigkeit der Ergebnisse oft gegenüber komplexen Werten bevorzugt.In der vorhandenen Literatur wird die Anzahl der verwendeten Parameter nicht berücksichtigt.Wir haben komplexe und reelle neuronale Netze unter Verwendung von fünf Aktivierungsfunktionen verglichen.Wir haben festgestellt, dass beim Vergleich von reellen und komplexen neuronalen Netzen anhand einfacher Klassifizierungsaufgaben komplexe neuronale Netze gleich oder leicht schlechter abschneiden als reelle neuronale Netze. Daher sollten komplexwertige neuronale Netze verwendet werden, wenn die Eingabedaten ebenfalls komplex sind oder sinnvoll auf die komplexe Ebene übertragen werden können, oder wenn die Netzarchitektur die durch die Verwendung komplexer Zahlen definierte Struktur verwendet.
Die Zunahme graphenstrukturierter Daten wie soziale Netzwerke, regulatorische Netzwerke, Zitationsgraphen und funktionale Gehirnnetzwerke in Kombination mit dem durchschlagenden Erfolg von Deep Learning in verschiedenen Anwendungen hat das Interesse an der Verallgemeinerung von Deep-Learning-Modellen auf nicht-euklidische Bereiche geweckt. Der Kernbestandteil unseres Modells ist eine neue Klasse parametrischer rationaler komplexer Funktionen (Cayley-Polynome), die eine effiziente Berechnung von Spektralfiltern auf Graphen ermöglichen, die sich auf interessante Frequenzbänder spezialisieren. Unser Modell generiert reichhaltige Spektralfilter, die im Raum lokalisiert sind, linear mit der Größe der Eingabedaten für spärlich verbundene Graphen skalieren und verschiedene Konstruktionen von Laplacian-Operatoren handhaben können.Umfangreiche experimentelle Ergebnisse zeigen die überlegene Leistung unseres Ansatzes bei der spektralen Bildklassifikation, Community-Detektion, Vertex-Klassifikation und Matrix-Vervollständigung.
Wir stellen FasterSeg vor, ein automatisch entworfenes semantisches Segmentierungsnetzwerk, das nicht nur die modernste Leistung aufweist, sondern auch schneller ist als die aktuellen Methoden. Durch die Verwendung der neuronalen Architektursuche (NAS) wird FasterSeg aus einem neuartigen und breiteren Suchraum entdeckt, der Multi-Auflösungszweige integriert, die sich kürzlich als entscheidend für manuell entworfene Segmentierungsmodelle erwiesen haben. Um das Gleichgewicht zwischen hoher Genauigkeit und geringer Latenz zu verbessern, schlagen wir eine entkoppelte und feinkörnige Latenzregulierung vor, die das von uns beobachtete Phänomen, dass die durchsuchten Netzwerke zu Modellen mit geringer Latenz, aber geringer Genauigkeit "kollabieren", effektiv überwindet. Darüber hinaus erweitern wir FasterSeg nahtlos um ein neues kollaboratives Suchverfahren (Co-Searching), bei dem in einem einzigen Durchlauf gleichzeitig nach einem Lehrer- und einem Schülernetzwerk gesucht wird.Die Lehrer-Schüler-Destillation steigert die Genauigkeit des Schülermodells weiter.Experimente mit populären Segmentierungs-Benchmarks demonstrieren die Kompetenz von FasterSeg.Zum Beispiel kann FasterSeg bei Cityscapes über 30% schneller laufen als der nächste manuell entwickelte Konkurrent, während eine vergleichbare Genauigkeit beibehalten wird.
Wir stellen eine Reihe von acht Aufgaben vor, die diese drei Eigenschaften kombinieren, und zeigen, dass R2D3 einige der Aufgaben lösen kann, bei denen andere State-of-the-Art-Methoden (sowohl mit als auch ohne Demonstrationen) nicht einmal eine einzige erfolgreiche Trajektorie nach zig Milliarden von Erkundungsschritten finden.
Wir untersuchen die erlernte dynamische Landschaft eines rekurrenten neuronalen Netzes, das eine einfache Aufgabe löst, die das Zusammenspiel von zwei Gedächtnismechanismen erfordert: Lang- und Kurzzeitgedächtnis. unsere Ergebnisse zeigen, dass das Langzeitgedächtnis durch asymptotische Attraktoren implementiert wird, während das sequenzielle Abrufen nun zusätzlich durch oszillatorische Dynamik in einem transversalen Unterraum zu den Anziehungsbereichen dieser stabilen stationären Zustände implementiert wird. auf der Grundlage unserer Beobachtungen schlagen wir vor, wie verschiedene Arten von Gedächtnismechanismen in einem einzigen neuronalen Netz koexistieren und zusammenarbeiten können, und diskutieren mögliche Anwendungen in den Bereichen der künstlichen Intelligenz und der Neurowissenschaften.
Das Problem der Exploration beim Reinforcement Learning ist im tabellarischen Fall gut verstanden und viele stichprobeneffiziente Algorithmen sind bekannt. Dennoch ist es oft unklar, wie die Algorithmen in der tabellarischen Umgebung auf Aufgaben mit großen Zustandsräumen erweitert werden können, bei denen eine Verallgemeinerung erforderlich ist.Jüngste vielversprechende Entwicklungen hängen im Allgemeinen von problemspezifischen Dichtemodellen oder handgefertigten Merkmalen ab. In diesem Beitrag stellen wir einen einfachen Ansatz für die Exploration vor, der es uns ermöglicht, theoretisch begründete Algorithmen für den tabellarischen Fall zu entwickeln, der uns aber auch Anhaltspunkte für neue Algorithmen liefert, die in Situationen anwendbar sind, in denen eine Funktionsannäherung erforderlich ist. Während die traditionelle Nachfolgerepräsentation eine Repräsentation ist, die die Generalisierung von Zuständen durch die Ähnlichkeit von Nachfolgezuständen definiert, ist die substochastische Nachfolgerepräsentation auch in der Lage, implizit zu zählen, wie oft jeder Zustand (oder jedes Merkmal) beobachtet wurde.Diese Erweiterung verbindet zwei bisher getrennte Forschungsbereiche. Wir zeigen in traditionellen Tabellendomänen (RiverSwim und SixArms), dass unser Algorithmus empirisch genauso gut abschneidet wie andere stichprobeneffiziente Algorithmen, und beschreiben dann einen tiefen Verstärkungslernalgorithmus, der von diesen Ideen inspiriert ist, und zeigen, dass er die Leistung neuerer pseudo-count-basierter Methoden in schwer zu erforschenden Atari-2600-Spielen erreicht.
Tiefe generative Modellierung mit Flüssen hat Popularität aufgrund der überschaubaren exakten Log-Likelihood-Schätzung mit effizientem Training und Syntheseprozess gewonnen.Allerdings leiden Flussmodelle unter der Herausforderung, einen hochdimensionalen latenten Raum zu haben, der die gleiche Dimension wie der Eingaberaum hat.Eine effektive Lösung für die oben genannte Herausforderung, wie von Dinh et al. (2016) vorgeschlagene Lösung ist eine Multiskalenarchitektur, die auf einer iterativen frühen Faktorisierung eines Teils der Gesamtdimensionen in regelmäßigen Abständen basiert.Frühere Arbeiten zu generativen Flüssen, die eine Multiskalenarchitektur beinhalten, führen die Dimensionsfaktorisierung auf der Grundlage einer statischen Maskierung durch.Wir schlagen eine neuartige Multiskalenarchitektur vor, die eine datenabhängige Faktorisierung durchführt, um zu entscheiden, welche Dimensionen durch mehrere Flussschichten laufen sollten. Um dies zu erleichtern, führen wir eine Heuristik ein, die auf dem Beitrag jeder Dimension zur gesamten Log-Likelihood basiert und die Wichtigkeit der Dimensionen kodiert.Unsere vorgeschlagene Heuristik ist leicht als Teil des Flow-Trainingsprozesses zu erhalten und ermöglicht eine vielseitige Implementierung unserer auf dem Likelihood-Beitrag basierenden Multi-Scale-Architektur für generische Flow-Modelle.Wir präsentieren eine solche Implementierung für den ursprünglichen Flow, der in Dinh et al. (2016) eingeführt wurde, und demonstrieren Verbesserungen in der Log-Likelihood-Score und der Sampling-Qualität auf Standard-Bild-Benchmarks.Wir führen auch Ablationsstudien durch, um die vorgeschlagene Methode mit anderen Optionen für die Dimensionsfaktorisierung zu vergleichen.
Das Verständnis des Informationsflusses in tiefen neuronalen Netzen (DNNs) ist ein herausforderndes Problem, das in den letzten Jahren zunehmend an Aufmerksamkeit gewonnen hat.Während mehrere Methoden vorgeschlagen wurden, um Netzwerkvorhersagen zu erklären, gab es nur wenige Versuche, sie aus einer theoretischen Perspektive zu vergleichen.Darüber hinaus wurde in der Vergangenheit kein erschöpfender empirischer Vergleich durchgeführt.In dieser Arbeit analysieren wir vier gradientenbasierte Attributionsmethoden und beweisen formal Bedingungen der Äquivalenz und Annäherung zwischen ihnen. Schließlich schlagen wir eine neuartige Bewertungsmetrik vor, die wir Sensitivity-n nennen, und testen die gradientenbasierten Attributionsmethoden zusammen mit einer einfachen perturbationsbasierten Attributionsmethode auf mehreren Datensätzen in den Bereichen Bild- und Textklassifikation unter Verwendung verschiedener Netzwerkarchitekturen.
Wir untersuchen SGD und Adam für die Schätzung eines Signals ersten Ranges, das in Matrix- oder Tensorrauschen eingebettet ist. Die extreme Einfachheit des Problemaufbaus ermöglicht es uns, die Auswirkungen verschiedener Faktoren zu isolieren: Signal-Rausch-Verhältnis, Dichte der kritischen Punkte, Stochastizität und Initialisierung.Wir beobachten ein überraschendes Phänomen: Adam scheint in lokalen Minima stecken zu bleiben, sobald polynomiell viele kritische Punkte auftreten (Matrix-Fall), während SGD diesen entkommt; wenn jedoch die Anzahl der kritischen Punkte zu Exponentialen degeneriert (Tensor-Fall), bleiben beide Algorithmen stecken. Die Theorie sagt uns, dass bei festem SNR das Problem für große $d$ unlösbar wird und in unseren Experimenten entgeht SGD dem nicht.Wir zeigen die Vorteile des Warmstarts in diesen Situationen.Wir schließen, dass in dieser Klasse von Problemen der Warmstart nicht durch Stochastik in den Gradienten ersetzt werden kann, um das Anziehungsgebiet zu finden.
Dieses Papier stellt eine Methode zur quantitativen und semantischen Erklärung des Wissens vor, das in einem neuronalen Faltungsnetzwerk (CNN) kodiert ist: Die Analyse der spezifischen Begründung jeder Vorhersage, die von einem CNN gemacht wird, stellt eines der Hauptprobleme beim Verständnis neuronaler Netzwerke dar, ist aber auch von großem praktischen Wert für bestimmte Anwendungen. In dieser Studie schlagen wir vor, Wissen aus dem CNN in ein erklärbares additives Modell zu destillieren, so dass wir das erklärbare Modell verwenden können, um eine quantitative Erklärung für die CNN-Vorhersage zu liefern.Wir analysieren das typische Bias-Interpretationsproblem des erklärbaren Modells und entwickeln vorherige Verluste, um das Lernen des erklärbaren additiven Modells zu leiten.Experimentelle Ergebnisse haben die Wirksamkeit unserer Methode gezeigt.
Die Modelle werden über einen auf Gradientenabstieg basierenden Mechanismus an die jüngste Geschichte angepasst, so dass sie wiederkehrenden sequenziellen Mustern höhere Wahrscheinlichkeiten zuweisen. Die dynamische Bewertung übertrifft in unseren Vergleichen bestehende Anpassungsansätze. Die dynamische Bewertung verbessert die State-of-the-Art-Perplexitäten auf Wortebene in den Datensätzen Penn Treebank und WikiText-2 auf 51,1 bzw. 44,3 und die State-of-the-Art-Kreuzentropien auf Zeichenebene in den Datensätzen text8 und Hutter Prize auf 1,19 Bit/Zeichen bzw. 1,08 Bit/Zeichen.
Wir schlagen einen neuartigen, projektionsbasierten Weg vor, um die bedingte Information in den Diskriminator von GANs zu integrieren, der die Rolle der bedingten Information in dem zugrunde liegenden probabilistischen Modell respektiert. Dieser Ansatz steht im Gegensatz zu den meisten heute verwendeten Frameworks für bedingte GANs, die die bedingte Information durch Verkettung des (eingebetteten) bedingten Vektors mit den Merkmalsvektoren nutzen. Mit dieser Modifikation waren wir in der Lage, die Qualität der klassenbedingten Bilderzeugung auf dem ILSVRC2012 (ImageNet)-Datensatz gegenüber dem aktuellen Stand der Technik deutlich zu verbessern, und wir erreichten dies mit einem einzigen Paar aus einem Diskriminator und einem Generator. Wir waren auch in der Lage, die Anwendung auf die Superauflösung auszudehnen, und es gelang uns, hochdiskriminative Superauflösungsbilder zu erzeugen. Diese neue Struktur ermöglichte auch eine qualitativ hochwertige Kategorietransformation auf der Grundlage einer parametrischen funktionalen Transformation von bedingten Stapelnormalisierungsschichten im Generator.
Die Lerntheorie lehrt uns, dass mehr Daten besser sind, wenn es darum geht, den Generalisierungsfehler von identisch verteilten Trainings- und Testsätzen zu minimieren. Wenn sich jedoch die Verteilung von Trainings- und Testsätzen unterscheidet, kann diese Verteilungsverschiebung einen signifikanten Effekt haben.Mit einer neuartigen Perspektive auf das Funktionstransferlernen sind wir in der Lage, die Leistungsveränderung bei der Übertragung von Trainings- auf Testsätze mit der Wasserstein-Distanz zwischen der eingebetteten Trainings- und Testsatzverteilung zu begrenzen. Wir stellen fest, dass es einen Kompromiss gibt, der die Leistung beeinflusst zwischen der Invarianz einer Funktion gegenüber Änderungen in der Trainings- und Testverteilung und der Größe dieser Verschiebung in der Verteilung.Empirisch über mehrere Datendomänen hinweg untermauern wir diesen Standpunkt, indem wir zeigen, dass die Testleistung stark mit dem Abstand in den Datenverteilungen zwischen Trainings- und Testsatz korreliert.Ergänzend zu dem weit verbreiteten Glauben, dass mehr Daten immer besser sind, unterstreichen unsere Ergebnisse den Nutzen der Auswahl einer Trainingsdatenverteilung, die nahe an der Testdatenverteilung liegt, wenn die gelernte Funktion nicht invariant gegenüber solchen Änderungen ist.
Wir stellen ein neues prozedurales dynamisches System vor, das eine Vielzahl von Formen erzeugen kann, die oft als Kurven erscheinen, aber technisch gesehen sind die Figuren Plots von vielen Punkten.Wir nennen sie Spiroplots und zeigen, wie sich dieses neue System zu anderen Prozeduren oder Prozessen verhält, die Figuren erzeugen.Spiroplots sind ein extrem einfacher Prozess, aber mit einer überraschenden visuellen Vielfalt. Wir zeigen, dass einige Spiroplots einen endlichen Zyklus haben und zur Ausgangssituation zurückkehren, während andere unendlich oft neue Punkte erzeugen. Dieses Papier wird von einer JavaScript-App begleitet, mit der jeder Spiroplots erzeugen kann.
Unüberwachte Bild-zu-Bild-Übersetzung zielt darauf ab, eine Abbildung zwischen mehreren visuellen Domänen zu erlernen, indem ungepaarte Trainingspaare verwendet werden. Jüngste Studien haben bemerkenswerte Erfolge bei der Bild-zu-Bild-Übersetzung für mehrere Domänen gezeigt, aber sie leiden unter zwei Hauptbeschränkungen: Sie sind entweder aus mehreren Zwei-Domänen-Abbildungen aufgebaut, die unabhängig voneinander erlernt werden müssen, und/oder sie erzeugen Ergebnisse mit geringer Diversität, ein Phänomen, das als Modellkollaps bekannt ist.Um diese Beschränkungen zu überwinden, schlagen wir eine Methode mit dem Namen GMM-UNIT vor, die auf einer inhaltlich-attributiven entmischten Darstellung basiert, bei der der Attributraum mit einem GMM angepasst wird. Jede GMM-Komponente repräsentiert eine Domäne, und diese einfache Annahme hat zwei herausragende Vorteile: Erstens wächst die Dimension des Attributraums nicht linear mit der Anzahl der Domänen, wie es in der Literatur der Fall ist, und zweitens ermöglicht die kontinuierliche Kodierung der Domänen eine Interpolation zwischen den Domänen und eine Extrapolation in ungesehene Domänen.
Wir stellen Compositional Attention Networks vor, eine neuartige, vollständig differenzierbare neuronale Netzwerkarchitektur, die explizite und ausdrucksstarke Schlussfolgerungen ermöglicht: Während viele Arten von neuronalen Netzwerken beim Lernen und Verallgemeinern aus großen Datenmengen effektiv sind, bewegt sich dieses Modell weg von monolithischen Black-Box-Architekturen hin zu einem Design, das eine starke Vorbedingung für iterative Schlussfolgerungen bietet und so erklärbares und strukturiertes Lernen sowie Verallgemeinerung aus einer bescheidenen Datenmenge unterstützt.Das Modell baut auf dem großen Erfolg bestehender rekurrenter Zellen wie LSTMs auf: Es sequenziert eine einzelne rekurrente Speicher-, Aufmerksamkeits- und Kontrollzelle (MAC) und legt durch sorgfältiges Design strukturelle Beschränkungen für den Betrieb jeder Zelle und die Interaktionen zwischen ihnen fest, indem es explizite Kontroll- und weiche Aufmerksamkeitsmechanismen in ihre Schnittstellen einbaut. Wir demonstrieren die Stärke und Robustheit des Modells anhand des anspruchsvollen CLEVR-Datensatzes für visuelles Denken und erreichen eine neue, hochmoderne Genauigkeit von 98,9 % und halbieren damit die Fehlerrate des bisher besten Modells.
  Variationale Autokodierer (VAEs) sind darauf ausgelegt, komprimierbare Informationen über einen Datensatz zu erfassen.  Infolgedessen reichen die im latenten Raum gespeicherten Informationen selten aus, um ein bestimmtes Bild zu rekonstruieren.  Um die Art der im latenten Raum gespeicherten Informationen besser zu verstehen, trainieren wir einen GAN-ähnlichen Decoder, der darauf beschränkt ist, Bilder zu erzeugen, die der VAE-Codierer auf dieselbe Region des latenten Raums abbildet, wodurch wir uns die im latenten Raum erfassten Informationen "vorstellen" können.  Wir argumentieren, dass dies notwendig ist, um eine VAE zu einem wirklich generativen Modell zu machen.  Wir verwenden unser GAN zur Visualisierung des latenten Raums einer Standard-VAE und einer $\beta$-VAE.
Das Verständnis der individuellen Variabilität der Gehirnfunktion und ihrer Verbindung mit dem Verhalten ist zu einem der wichtigsten Anliegen der modernen kognitiven Neurowissenschaften geworden.Unsere Arbeit ist durch die Ansicht motiviert, dass generative Modelle ein nützliches Werkzeug zum Verständnis dieser Variabilität bieten.Zu diesem Zweck stellt dieses Manuskript zwei neuartige generative Modelle vor, die auf realen Neuroimaging-Daten trainiert wurden und aufgabenabhängige funktionale Gehirnbilder synthetisieren.Gehirnbilder sind hochdimensionale Tensoren, die strukturierte räumliche Korrelationen aufweisen. Unsere Ergebnisse zeigen, dass die generierten Hirnbilder vielfältig, aber aufgabenabhängig sind. Zusätzlich zur qualitativen Bewertung nutzen wir die generierten synthetischen Hirnvolumen als zusätzliche Trainingsdaten für verbesserte Downstream-FMRI-Klassifikatoren (auch bekannt als Dekodierung oder Gehirnlesen). Unsere Klassifizierungsergebnisse liefern eine quantitative Bewertung der Qualität der generierten Bilder und dienen als zusätzlicher Beitrag zu diesem Manuskript.
Die Übertragung von Repräsentationen von groß angelegten überwachten Aufgaben auf nachgelagerte Aufgaben hat beim maschinellen Lernen sowohl im Bereich des maschinellen Sehens als auch der Verarbeitung natürlicher Sprache (NLP) hervorragende Ergebnisse gezeigt. Das liegt daran, dass NMT-Systeme, sobald sie in einer mehrsprachigen Umgebung trainiert wurden, zwischen mehreren Sprachen übersetzen können und auch in der Lage sind, zum Testzeitpunkt eine Zero-Shot-Übersetzung zwischen ungesehenen Quell-Ziel-Paaren durchzuführen.In diesem Beitrag untersuchen wir zunächst, ob wir die Zero-Shot-Übertragungsfähigkeit von mehrsprachigen NMT-Systemen auf sprachübergreifende NLP-Aufgaben (andere Aufgaben als MÜ, z. B. Sentiment-Klassifizierung und natürliche Sprachinferenz) ausweiten können. Wir zeigen, dass ein einfaches Framework durch die Wiederverwendung des Encoders eines mehrsprachigen NMT-Systems, ein mehrsprachiger Encoder-Klassifikator, eine bemerkenswerte Zero-Shot-Klassifikationsleistung in drei nachgelagerten Benchmark-Aufgaben - Amazon Reviews, Stanford Sentiment Treebank (SST) und Stanford Natural Language Inference (SNLI) - erreicht. Um die zugrundeliegenden Faktoren zu verstehen, die zu diesem Ergebnis beitragen, haben wir eine Reihe von Analysen zu den Auswirkungen des gemeinsamen Vokabulars, des Trainingsdatentyps für NMT-Modelle, der Komplexität des Klassifikators, der Repräsentationsstärke des Kodierers und der Generalisierung des Modells auf die Zero-Shot-Performance durchgeführt. Unsere Ergebnisse liefern starke Belege dafür, dass die Repräsentationen, die von mehrsprachigen NMT-Systemen gelernt werden, weithin über Sprachen und Aufgaben hinweg anwendbar sind, und dass die hohe, sofort einsetzbare Klassifikationsleistung mit der Generalisierungsfähigkeit solcher Systeme korreliert.
Im Gegensatz zu konventionellen Ansätzen, die Evolution oder Reinforcement Learning auf einen diskreten und nicht differenzierbaren Suchraum anwenden, basiert unsere Methode auf einer kontinuierlichen Relaxation der Architekturrepräsentation, was eine effiziente Suche der Architektur mittels Gradientenabstieg ermöglicht. Ausführliche Experimente mit CIFAR-10, ImageNet, Penn Treebank und WikiText-2 zeigen, dass unser Algorithmus bei der Entdeckung von leistungsstarken Faltungsarchitekturen für die Bildklassifikation und rekurrenten Architekturen für die Sprachmodellierung hervorragend abschneidet und dabei um Größenordnungen schneller ist als die modernsten nicht-differenzierbaren Verfahren.
Trotz erheblicher Fortschritte bei der neuronalen Sprachmodellierung bleibt die Frage nach der besten Dekodierungsstrategie für die Texterzeugung aus einem Sprachmodell (z. B. für die Erstellung einer Geschichte) offen.  Die kontraintuitive empirische Beobachtung ist, dass die Verwendung von Likelihood als Trainingsziel zwar zu qualitativ hochwertigen Modellen für ein breites Spektrum von Sprachverstehensaufgaben führt, dass aber maximierungsbasierte Dekodierungsmethoden wie die Balkensuche zu einer Degeneration führen, d. h., dass der ausgegebene Text fade und inkohärent ist oder in sich wiederholenden Schleifen stecken bleibt. Unser Ansatz vermeidet Textdegeneration, indem er den unzuverlässigen Schwanz der Wahrscheinlichkeitsverteilung abschneidet und aus dem dynamischen Kern von Token, der den Großteil der Wahrscheinlichkeitsmasse enthält, Stichproben zieht. Um die aktuellen maximierungsbasierten und stochastischen Dekodierungsmethoden angemessen zu untersuchen, vergleichen wir die Generationen jeder dieser Methoden mit der Verteilung von menschlichem Text entlang verschiedener Achsen wie Wahrscheinlichkeit, Vielfalt und Wiederholung. Unsere Ergebnisse zeigen, dass (1) Maximierung ein ungeeignetes Dekodierungsziel für die Generierung von Texten mit offenem Ende ist, (2) die Wahrscheinlichkeitsverteilungen der besten aktuellen Sprachmodelle einen unzuverlässigen Schwanz haben, der während der Generierung abgeschnitten werden muss, und (3) Nucleus Sampling die beste Dekodierungsstrategie für die Generierung von Langformtexten ist, die sowohl qualitativ hochwertig - gemessen durch menschliche Bewertung - als auch so vielfältig wie von Menschen geschriebene Texte sind.
Bis vor kurzem gab es, inspiriert durch eine Vielzahl von Forschungen zu adversen Beispielen für Computer Vision, ein wachsendes Interesse an der Entwicklung von adversen Angriffen für Natural Language Processing (NLP) Aufgaben, gefolgt von sehr wenigen Arbeiten zu adversen Verteidigungen für NLP. Wir tragen dazu bei, diese Lücke zu schließen, und schlagen eine neuartige Verteidigungsmethode vor, die Synonym-Encoding-Methode (SEM) genannt wird und die einen Encoder vor der Eingabeschicht des Modells einfügt und dann das Modell trainiert, um gegnerische Störungen zu eliminieren. Um SEM besser bewerten zu können, haben wir auch eine starke Angriffsmethode namens Improved Genetic Algorithm (IGA) entwickelt, die die genetische Metaheuristik für Angriffe auf der Basis von Synonymsubstitution einsetzt. Im Vergleich zu bestehenden genetisch basierten Angriffen kann IGA eine höhere Erfolgsrate erzielen und gleichzeitig die Übertragbarkeit der Beispiele beibehalten.
Ein großer Nachteil der Backpropagation über die Zeit (BPTT) ist die Schwierigkeit, langfristige Abhängigkeiten zu erlernen, die sich daraus ergibt, dass die Kreditinformationen in jedem einzelnen Schritt der Vorwärtsberechnung rückwärts übertragen werden müssen, was BPTT sowohl rechnerisch unpraktisch als auch biologisch unplausibel macht.  Dies führt jedoch in der Regel zu verzerrten Schätzungen des Gradienten, bei denen längerfristige Abhängigkeiten ignoriert werden.  Um dieses Problem anzugehen, schlagen wir einen alternativen Algorithmus vor, Sparse Attentive Backtracking, der auch mit den Prinzipien verwandt sein könnte, die von Gehirnen verwendet werden, um langfristige Abhängigkeiten zu erlernen.Sparse Attentive Backtracking lernt einen Aufmerksamkeitsmechanismus über die verborgenen Zustände der Vergangenheit und führt selektiv Backpropagation über Pfade mit hohen Aufmerksamkeitsgewichten durch.  Dies ermöglicht es dem Modell, langfristige Abhängigkeiten zu erlernen, während es nur eine kleine Anzahl von Zeitschritten zurückverfolgt, und zwar nicht nur von der jüngsten Vergangenheit, sondern auch von besuchten relevanten vergangenen Zuständen.   
In dieser Arbeit schlagen wir ein neuartiges adversariales Lernverfahren vor, bei dem zwei getrennte Netzwerke zum Einsatz kommen, d.h, Die Informationen, die von diskriminativen Modellen erfasst werden, ergänzen die in den strukturierten Vorhersagemodellen, aber nur wenige bestehende Forschungen haben die Nutzung solcher Informationen untersucht, um strukturierte Vorhersagemodelle in der Inferenzphase zu verbessern.In dieser Arbeit schlagen wir vor, die Vorhersagen von strukturierten Vorhersagemodellen zu verfeinern, indem wir diskriminative Modelle effektiv in die Vorhersage integrieren.Diskriminative Modelle werden als energiebasierte Modelle behandelt. Ähnlich wie beim adversen Lernen werden diskriminative Modelle trainiert, um Scores zu schätzen, die die Qualität der vorhergesagten Outputs messen, während strukturierte Vorhersagemodelle trainiert werden, um kontrastive Outputs mit maximalen Energie-Scores vorherzusagen.Auf diese Weise wird das Problem des Verschwindens des Gradienten verbessert, und so sind wir in der Lage, die Inferenz durchzuführen, indem wir den aufsteigenden Gradientenrichtungen der diskriminativen Modelle folgen, um die strukturierten Vorhersagemodelle zu verfeinern.Die vorgeschlagene Methode ist in der Lage, eine Reihe von Aufgaben zu bewältigen, z.B. Mehr-Label-Klassifikation und Bildsegmentierung.  Empirische Ergebnisse zu diesen beiden Aufgaben bestätigen die Wirksamkeit unserer Lernmethode.
Wir schlagen RaPP vor, eine neue Methode zur Erkennung von Neuheiten durch die Verwendung von Aktivierungswerten im verborgenen Raum, die von einem tiefen Autoencoder erhalten werden. RaPP vergleicht die Eingabe und ihre Autoencoder-Rekonstruktion nicht nur im Eingaberaum, sondern auch in den verborgenen Räumen. Um die versteckten Raumaktivierungswerte zu aggregieren, schlagen wir zwei Metriken vor, die die Neuheitserkennungsleistung verbessern. Durch umfangreiche Experimente mit verschiedenen Datensätzen validieren wir, dass RaPP die Neuheitserkennungsleistungen von Autoencoder-basierten Ansätzen verbessert. Außerdem zeigen wir, dass RaPP aktuelle Neuheitserkennungsmethoden übertrifft, die auf populären Benchmarks evaluiert wurden.
Das Erlernen von Präferenzen von Nutzern über Planspuren kann eine schwierige Aufgabe sein, da es eine große Anzahl von Merkmalen gibt und die Abfragen, die wir einem einzelnen Nutzer stellen können, begrenzt sind.Darüber hinaus kann die Präferenzfunktion selbst ziemlich kompliziert und nichtlinear sein.Unser Ansatz verwendet merkmalsorientiertes aktives Lernen, um die notwendigen Informationen über die Präferenzen von Planspuren zu sammeln. Wir bewerten die Auswirkungen des aktiven Lernens auf die Anzahl der Spuren, die benötigt werden, um ein Modell zu trainieren, das genau und interpretierbar ist, indem wir das oben genannte Feedforward-Netzwerk mit einem komplexeren neuronalen Netzwerkmodell vergleichen, das LSTMs verwendet und mit einem größeren Datensatz ohne aktives Lernen trainiert wird.
Die Spezifikation von Zielen und Aufgaben für autonome Maschinen, wie z.B. Roboter, ist eine große Herausforderung: Konventionell wurden Belohnungsfunktionen und Zielzustände verwendet, um Ziele zu kommunizieren, aber Menschen können sich gegenseitig Ziele mitteilen, indem sie sie einfach beschreiben oder demonstrieren.Wie können wir Lernalgorithmen entwickeln, die es uns erlauben, Maschinen mitzuteilen, was sie tun sollen? In dieser Arbeit untersuchen wir das Problem der Begründung von Sprachbefehlen als Belohnungsfunktionen unter Verwendung von inversem Verstärkungslernen und argumentieren, dass sprachkonditionierte Belohnungen besser auf neue Umgebungen übertragbar sind als sprachkonditionierte Strategien.Wir schlagen sprachkonditioniertes Belohnungslernen (LC-RL) vor, das Sprachbefehle als Belohnungsfunktion begründet, die durch ein tiefes neuronales Netzwerk repräsentiert wird.Wir zeigen, dass unser Modell Belohnungen lernt, die auf neuartige Aufgaben und Umgebungen in realistischen, hochdimensionalen visuellen Umgebungen mit natürlichen Sprachbefehlen übertragbar sind, während das direkte Lernen einer sprachkonditionierten Strategie zu einer schlechten Leistung führt.
Viele biologische Lernsysteme wie der Pilzkörper, der Hippocampus und das Kleinhirn bestehen aus spärlich verbundenen Neuronennetzwerken. Um ein neues Verständnis solcher Netzwerke zu erlangen, untersuchen wir die Funktionsräume, die durch spärliche Zufallsmerkmale induziert werden, und charakterisieren, welche Funktionen erlernt werden können und welche nicht.Ein Netzwerk mit d Eingängen pro Neuron ist äquivalent zu einem additiven Modell der Ordnung d, wohingegen das Netzwerk mit einer Gradverteilung additive Terme unterschiedlicher Ordnung kombiniert. Wir identifizieren drei spezifische Vorteile der Sparsamkeit: additive Funktionsapproximation ist eine mächtige induktive Verzerrung, die den Fluch der Dimensionalität begrenzt, spärliche Netzwerke sind stabil gegenüber Ausreißerrauschen in den Eingängen, und spärliche Zufallsmerkmale sind skalierbar, so dass selbst einfache Gehirnarchitekturen mächtige Funktionsapproximatoren sein können.Schließlich hoffen wir, dass diese Arbeit dazu beiträgt, Kernel-Theorien von Netzwerken unter Computational Neuroscientists zu popularisieren.
Wir schlagen eine neue Anwendung von Einbettungstechniken für die Suche nach Problemen im Rahmen des adaptiven Tutorsystems vor, um Probleme mit ähnlichen mathematischen Konzepten zu finden: Erstens sind Probleme, die für das Tutoring hilfreich sind, nie genau gleich, was die zugrundeliegenden Konzepte anbelangt; stattdessen mischen gute Probleme Konzepte auf innovative Weise, während sie immer noch Kontinuität in ihren Beziehungen aufweisen; zweitens ist es für Menschen schwierig, eine Ähnlichkeitsbewertung zu bestimmen, die über eine ausreichend große Trainingsmenge hinweg konsistent ist. Wir schlagen einen hierarchischen Problemeinbettungsalgorithmus vor, genannt Prob2Vec, der aus einem Abstraktions- und einem Einbettungsschritt besteht. Prob2Vec erreicht eine Genauigkeit von 96,88\% bei einem Problemähnlichkeitstest, im Gegensatz zu 75\% bei direkter Anwendung modernster Satzeinbettungsmethoden. Es ist überraschend, dass Prob2Vec in der Lage ist, sehr feinkörnige Unterschiede zwischen den Problemen zu unterscheiden, eine Fähigkeit, die Menschen Zeit und Mühe brauchen, um zu erwerben.Darüber hinaus ist das Teilproblem der Konzeptbeschriftung mit unausgewogenen Trainingsdatensatz in seinem eigenen Recht interessant.Es ist ein Multi-Label-Problem, das unter Dimensionalitätsexplosion leidet, die wir Wege vorschlagen, um zu verbessern.Wir schlagen die neuartige negative Pre-Training-Algorithmus, der drastisch reduziert falsch negative und positive Verhältnisse für die Klassifizierung, mit einem unausgewogenen Trainingsdatensatz.
Wir führen ein neues tiefes neuronales Faltungsnetzwerk, CrescendoNet, ein, indem wir einfache Bausteine ohne Restverbindungen stapeln. Jeder Crescendo-Block enthält unabhängige Faltungspfade mit zunehmender Tiefe, wobei die Anzahl der Faltungsschichten und Parameter nur linear in Crescendo-Blöcken erhöht wird. In Experimenten übertrifft CrescendoNet mit nur 15 Schichten fast alle Netzwerke ohne Restverbindungen in den Benchmark-Datensätzen CIFAR10, CIFAR100 und SVHN.Bei ausreichender Datenmenge wie im SVHN-Datensatz kann CrescendoNet mit 15 Schichten und 4,1M Parametern die Leistung von DenseNet-BC mit 250 Schichten und 15.3M Parametern erreichen. Durch die Untersuchung des Verhaltens und der Leistung von Teilnetzen in CrescendoNet stellen wir außerdem fest, dass die hohe Leistung von CrescendoNet von seinem impliziten Ensemble-Verhalten herrührt, das sich von dem von FractalNet unterscheidet, das ebenfalls ein tiefes neuronales Faltungsnetz ohne Restverbindungen ist, und dass die Unabhängigkeit zwischen den Pfaden in CrescendoNet uns erlaubt, ein neues pfadweises Trainingsverfahren einzuführen, das den für das Training benötigten Speicherplatz reduzieren kann.
Gauß-Prozesse sind die führende Klasse von Verteilungen auf Zufallsfunktionen, aber sie leiden unter bekannten Problemen, einschließlich der Schwierigkeit der Skalierung und der Unflexibilität in Bezug auf bestimmte Formeinschränkungen (wie z.B. Nichtnegativität).Hier schlagen wir Deep Random Splines vor, eine flexible Klasse von Zufallsfunktionen, die durch Transformation von Gaußschem Rauschen durch ein tiefes neuronales Netz erhalten werden, dessen Ausgabe die Parameter eines Splines sind. Im Gegensatz zu Gauß'schen Prozessen erlauben Deep Random Splines die einfache Umsetzung von Formbeschränkungen, während sie gleichzeitig den Reichtum und die Überschaubarkeit tiefer generativer Modelle bieten. Wir stellen außerdem ein Beobachtungsmodell für Punktprozessdaten vor, das Deep Random Splines verwendet, um die Intensitätsfunktion jedes Punktprozesses zu modellieren, und wenden es auf neurowissenschaftliche Daten an, um eine niedrigdimensionale Darstellung der Spiking-Aktivität zu erhalten.
Die jüngste Entwicklung der Verarbeitung natürlicher Sprache (NLP) hat große Erfolge erzielt, indem sie große vortrainierte Modelle mit Hunderten von Millionen von Parametern verwendet hat, die jedoch unter der großen Modellgröße und der hohen Latenz leiden, so dass sie nicht direkt auf ressourcenbeschränkten mobilen Geräten eingesetzt werden können. Wie BERT ist MobileBERT aufgabenunabhängig, d.h. es kann durch Feinabstimmung universell auf verschiedene nachgelagerte NLP-Aufgaben angewandt werden. MobileBERT ist eine abgespeckte Version von BERT-LARGE, die um Engpassstrukturen und ein sorgfältig entworfenes Gleichgewicht zwischen Selbstbeobachtung und Feed-Forward-Netzwerken erweitert wurde. Um MobileBERT zu trainieren, verwenden wir ein progressives Schema von unten nach oben, um das intrinsische Wissen eines speziell entworfenen Inverted Bottleneck BERT-LARGE Lehrers zu übertragen.Empirische Studien zeigen, dass MobileBERT 4,3x kleiner und 4,0x schneller als das Original BERT-BASE ist und dabei konkurrenzfähige Ergebnisse bei bekannten NLP Benchmarks erzielt. Bei den GLUE-Aufgaben zur Inferenz natürlicher Sprache erreicht MobileBERT eine Leistungsverschlechterung von 0,6 GLUE-Scores und eine Latenz von 367 ms auf einem Pixel 3-Telefon. Bei der SQuAD v1.1/v2.0-Aufgabe zur Beantwortung von Fragen erreicht MobileBERT einen 90,0/79,2 dev F1-Score, was 1,5/2,1 höher ist als BERT-BASE.
Der importance weighted autoencoder (IWAE) (Burda et al., 2016) ist eine populäre Variations-Inferenz-Methode, die eine engere Evidenz-Schranke (und damit eine geringere Verzerrung) als Standard-Variations-Autoencoder erreicht, indem sie ein Multi-Sample-Ziel optimiert, d.h. ein Ziel, das als Integral über $K > 1$ Monte-Carlo-Stichproben ausgedrückt werden kann.Leider hängt IWAE entscheidend von der Verfügbarkeit von Reparametrisierungen ab, und selbst wenn diese existieren, führt das Multi-Sample-Ziel zu Inferenz-Netzwerk-Gradienten, die zusammenbrechen, wenn $K$ erhöht wird (Rainforth et al, Dieser Zusammenbruch kann nur umgangen werden, indem die hochvariablen Terme der Score-Funktion entfernt werden, entweder durch heuristisches Ignorieren (was den "sticking-the-landing" IWAE (IWAE-STL) Gradienten von Roeder et al. (2017) ergibt) oder durch eine Identität von Tucker et al. (2019) (was den "doppelt-reparametrisierten" IWAE (IWAE-DREG) Gradienten ergibt). In dieser Arbeit argumentieren wir, dass die direkte Optimierung der Vorschlagsverteilung beim Wichtigkeits-Sampling, wie im reweighted wake-sleep (RWS) Algorithmus von Bornschein & Bengio (2015), der Optimierung von IWAE-artigen Multi-Sample-Zielen vorzuziehen ist. Um dieses Argument zu formalisieren, führen wir einen adaptiven Wichtigkeits-Sampling-Rahmen ein, der als adaptives Wichtigkeits-Sampling für das Lernen (AISLE) bezeichnet wird und den RWS-Algorithmus leicht verallgemeinert, und zeigen dann, dass AISLE IWAE-STL und IWAE-DREG (d. h. die IWAE-Gradienten, die einen Zusammenbruch vermeiden) als Spezialfälle zulässt.
Mit zunehmender Größe und Komplexität von Modellen und Datensätzen steigt auch der Bedarf an kommunikationseffizienten Varianten des stochastischen Gradientenabstiegs, die auf Clustern eingesetzt werden können, um die Modellanpassung parallel durchzuführen.Alistarh et al. (2017) beschreiben zwei Varianten des datenparallelen SGD, die Gradienten quantisieren und kodieren, um die Kommunikationskosten zu senken.Für die erste Variante, QSGD, bieten sie starke theoretische Garantien. Für die zweite Variante, die wir QSGDinf nennen, zeigen sie beeindruckende empirische Gewinne für das verteilte Training großer neuronaler Netze. Aufbauend auf ihrer Arbeit schlagen wir ein alternatives Schema für die Quantisierung von Gradienten vor und zeigen, dass es stärkere theoretische Garantien bietet als QSGD und gleichzeitig die empirische Leistung von QSGDinf erreicht.
Das beeindruckende lebenslange Lernen in Tiergehirnen wird in erster Linie durch plastische Veränderungen in der synaptischen Konnektivität ermöglicht, wobei diese Veränderungen nicht passiv sind, sondern aktiv durch Neuromodulation gesteuert werden, die ihrerseits unter der Kontrolle des Gehirns steht.Die daraus resultierenden selbstmodifizierenden Fähigkeiten des Gehirns spielen eine wichtige Rolle beim Lernen und bei der Anpassung und sind eine wichtige Grundlage für biologisches Verstärkungslernen.Wir zeigen hier zum ersten Mal, dass künstliche neuronale Netze mit einer solchen neuromodulierten Plastizität mit Gradientenabstieg trainiert werden können. Wir zeigen, dass neuromodulierte Plastizität die Leistung neuronaler Netze sowohl bei Aufgaben des Verstärkungslernens als auch beim überwachten Lernen verbessert, und dass neuromodulierte plastische LSTMs mit Millionen von Parametern bei einer Benchmark-Aufgabe zur Sprachmodellierung (unter Kontrolle der Anzahl der Parameter) besser abschneiden als Standard-LSTMs.Wir kommen zu dem Schluss, dass die differenzierbare Neuromodulation der Plastizität einen leistungsstarken neuen Rahmen für das Training neuronaler Netze bietet.
Die Algorithmen des Deep Learning stoßen daher auf Schwierigkeiten, wenn sie auf das überwachte Lernen angewandt werden, bei dem nur wenige Daten zur Verfügung stehen.Diese spezielle Aufgabe wird als "few-shot learning" bezeichnet.Um sie anzugehen, schlagen wir einen neuartigen Algorithmus für "fewshotlearning" vor, der diskrete Geometrie verwendet, in dem Sinne, dass die Proben in einer Klasse als ein reduziertes Simplex modelliert werden. Das Volumen des Simplex wird für die Messung der Klassenstreuung verwendet. Beim Testen wird in Kombination mit der Testprobe und den Punkten in der Klasse ein neues Simplex gebildet. Dann kann die Ähnlichkeit zwischen der Testprobe und der Klasse mit dem Verhältnis der Volumina des neuen Simplex zum ursprünglichen Klassensimplex quantifiziert werden. Darüber hinaus präsentieren wir einen Ansatz zur Konstruktion von Simplexen unter Verwendung lokaler Regionen von Merkmalskarten, die von faltbaren neuronalen Netzen erzeugt werden.Experimente mit Omniglot und miniImageNet bestätigen die Effektivität unseres Simplex-Algorithmus beim Lernen mit wenigen Aufnahmen.
Wir zeigen, dass ein Netzwerk komplizierte Sequenzen mit einer belohnungsmodulierten Hebb'schen Lernregel lernen kann, wenn das Netzwerk der Reservoir-Neuronen mit einem zweiten Netzwerk kombiniert wird, das als dynamischer Arbeitsspeicher dient und ein räumlich-zeitliches Backbone-Signal an das Reservoir liefert. In Kombination mit dem Arbeitsspeicher funktioniert das belohnungsmodulierte Hebb'sche Lernen der Ausleseneuronen genauso gut wie das FORCE-Lernen, allerdings mit dem Vorteil einer biologisch plausiblen Interpretation sowohl der Lernregel als auch des Lernparadigmas.
Kürzlich wurde gezeigt, dass Faltungsarchitekturen bei Modellierungsaufgaben mit vielen Sequenzen im Vergleich zum De-facto-Standard der rekurrenten neuronalen Netze (RNNs) konkurrenzfähig sind und aufgrund der inhärenten Parallelität Vorteile bei der Berechnung und Modellierung bieten. In dieser Arbeit schlagen wir stochastische temporale Faltungsnetzwerke (STCNs) vor, eine neuartige Architektur, die die rechnerischen Vorteile temporaler Faltungsnetzwerke (TCNs) mit der Darstellungsstärke und Robustheit stochastischer latenter Räume kombiniert. Insbesondere schlagen wir eine Hierarchie stochastischer latenter Variablen vor, die zeitliche Abhängigkeiten auf verschiedenen Zeitskalen erfasst. die Architektur ist aufgrund der Entkopplung der deterministischen und stochastischen Schichten modular und flexibel. wir zeigen, dass die vorgeschlagene Architektur den Stand der Technik der Log-Likelihoods über mehrere Aufgaben hinweg erreicht. schließlich ist das Modell in der Lage, qualitativ hochwertige synthetische Proben über einen weitreichenden zeitlichen Horizont bei der Modellierung von handgeschriebenem Text vorherzusagen.
Die schwache Kontraktionsabbildung ist eine Selbstabbildung, bei der der Bereich immer eine Teilmenge der Domäne ist, die einen eindeutigen Fixpunkt zulässt.Die Iteration der schwachen Kontraktionsabbildung ist eine Cauchy-Sequenz, die den eindeutigen Fixpunkt liefert.Eine gradientenfreie Optimierungsmethode als Anwendung der schwachen Kontraktionsabbildung wird vorgeschlagen, um globale minimale Konvergenz zu erreichen.Die Optimierungsmethode ist robust gegenüber lokalen Minima und der Position des Anfangspunkts.
In den letzten zehn Jahren haben sich zwei konkurrierende Steuerungsstrategien herausgebildet, um komplexe Steuerungsaufgaben mit hoher Effizienz zu lösen.Modellbasierte Steuerungsalgorithmen, wie die modellprädiktive Steuerung (MPC) und die Trajektorienoptimierung, greifen auf die Gradienten der zugrunde liegenden Systemdynamik zurück, um Steuerungsaufgaben mit hoher Effizienz zu lösen.  Wie alle gradientenbasierten numerischen Optimierungsmethoden reagieren jedoch auch modellbasierte Steuerungsmethoden empfindlich auf Intialisierungen und neigen dazu, in lokalen Minima gefangen zu sein.Deep Reinforcement Learning (DRL) hingegen kann diese Probleme etwas abmildern, indem es den Lösungsraum durch Sampling erkundet - allerdings auf Kosten der Rechenkosten. Wir basieren unseren Algorithmus auf dem Deep Deterministic Policy Gradients (DDPG) Algorithmus und schlagen eine einfache Modifikation vor, die echte Gradienten aus einem differenzierbaren physikalischen Simulator verwendet, um die Konvergenzrate sowohl des Akteurs als auch des Kritikers zu erhöhen.  Wir demonstrieren unseren Algorithmus an sieben 2D-Robotersteuerungsaufgaben, wobei die komplexeste Aufgabe ein differenzierbarer halber Gepard mit harten Kontaktbeschränkungen ist.
Ein Bayes'sches Hypernetzwerk, h, ist ein neuronales Netzwerk, das lernt, eine einfache Rauschverteilung, p(e) = N(0,I), in eine Verteilung q(t) := q(h(e)) über die Parameter t eines anderen neuronalen Netzwerks (das ``primäre Netzwerk) zu transformieren. Im Gegensatz zu den meisten Methoden des Bayes'schen Deep Learning können Bayes'sche Hypernetze ein komplexes multimodales approximatives Posterior mit Korrelationen zwischen den Parametern repräsentieren, während sie gleichzeitig ein kostengünstiges iid-Sampling von q(t) ermöglichen.  In der Praxis bieten Bayes'sche Hypernetze einen besseren Schutz gegen gegnerische Beispiele als Dropout und zeigen auch eine konkurrenzfähige Leistung bei einer Reihe von Aufgaben, die die Modellunsicherheit bewerten, einschließlich Regularisierung, aktives Lernen und Anomalieerkennung.
Dieses Papier stellt eine Methode namens Deep Innovation Protection (DIP) vor, die es ermöglicht, komplexe Weltmodelle für solche 3D-Umgebungen durchgängig zu trainieren. Die Hauptidee hinter dem Ansatz ist die Verwendung von Mehrziel-Optimierung, um den Selektionsdruck auf bestimmte Komponenten in einem Weltmodell zeitlich zu reduzieren, so dass andere Komponenten sich anpassen können.Wir untersuchen die emergenten Repräsentationen dieser entwickelten Netzwerke, die ein Modell der Welt lernen, ohne dass ein spezifischer Vorhersageverlust erforderlich ist.
In diesem Papier schlagen wir den MACER-Algorithmus vor, der robuste Modelle ohne adversariales Training lernt, aber besser abschneidet als alle existierenden beweisbaren l2-Verteidigungen. Neuere Arbeiten zeigen, dass randomisierte Glättung verwendet werden kann, um geglätteten Klassifikatoren einen zertifizierten l2-Radius zu geben, und unser Algorithmus trainiert beweisbar robuste geglättete Klassifikatoren über MAximizing the CErtified Radius (MACER). In unseren Experimenten zeigen wir, dass unsere Methode auf moderne tiefe neuronale Netze in einer Vielzahl von Datensätzen angewendet werden kann, darunter Cifar-10, ImageNet, MNIST und SVHN. Für alle Aufgaben benötigt MACER weniger Trainingszeit als moderne adversarische Trainingsalgorithmen, und die gelernten Modelle erreichen einen größeren durchschnittlichen zertifizierten Radius.
Actor-Critic-Methoden lösen Reinforcement-Learning-Probleme, indem sie eine parametrisierte Politik, die als Actor bekannt ist, in eine Richtung aktualisieren, die eine Schätzung des erwarteten Gewinns, die als Critic bekannt ist, erhöht.Bestehende Actor-Critic-Methoden verwenden jedoch nur Werte oder Gradienten der Critic, um den Politikparameter zu aktualisieren.In diesem Papier schlagen wir eine neuartige Actor-Critic-Methode vor, die Guide Actor-Critic (GAC) genannt wird.GAC lernt zunächst einen Guide Actor, der lokal die Critic maximiert, und aktualisiert dann den Politikparameter auf der Grundlage des Guide Actors durch überwachtes Lernen. Unsere wichtigsten theoretischen Beiträge sind zweierlei: Erstens zeigen wir, dass GAC den Guide Actor aktualisiert, indem es eine Optimierung zweiter Ordnung im Aktionsraum durchführt, wobei die Krümmungsmatrix auf den Hessians des Critics basiert; zweitens zeigen wir, dass die deterministische Policy-Gradienten-Methode ein Spezialfall von GAC ist, wenn die Hessians ignoriert werden; durch Experimente zeigen wir, dass unsere Methode eine vielversprechende Reinforcement Learning-Methode für kontinuierliche Kontrollen ist.
Deep Infomax~ (DIM) ist ein unbeaufsichtigtes Repräsentationslernverfahren, bei dem die gegenseitige Information zwischen den Eingängen und den Ausgängen eines Encoders maximiert wird, während den Ausgängen probabilistische Beschränkungen auferlegt werden.In diesem Papier schlagen wir Supervised Deep InfoMax~ (SDIM) vor, das überwachte probabilistische Beschränkungen für die Encoderausgänge einführt. Im Gegensatz zu anderen Arbeiten, die generative Klassifikatoren mit bedingten generativen Modellen aufbauen, skalieren SDIMs auf komplexen Datensätzen und können eine vergleichbare Leistung wie diskriminative Gegenstücke erreichen.  Mit SDIM können wir \emph{Klassifizierung mit Zurückweisung} durchführen. Anstatt immer ein Klassenlabel zu melden, macht SDIM nur dann Vorhersagen, wenn die größten Logits der Testproben einige vorher gewählte Schwellenwerte überschreiten, andernfalls werden sie als außerhalb der Datenverteilungen liegend betrachtet und zurückgewiesen.  Unsere Experimente zeigen, dass SDIM mit einer Ablehnungspolitik illegale Eingaben, einschließlich Proben, die außerhalb der Verteilung liegen, und gegnerische Beispiele, effektiv ablehnen kann.
Zusammenfassung In dieser Arbeit beschreiben wir eine Reihe von Regeln für den Entwurf und die Initialisierung von gut konditionierten neuronalen Netzen, die von dem Ziel geleitet werden, die diagonalen Blöcke der Hessian zu Beginn des Trainings natürlich auszugleichen. Wir beweisen, dass für ein ReLU-basiertes tiefes mehrschichtiges Perzeptron ein einfaches Initialisierungsschema, das das geometrische Mittel des Fan-In und Fan-Out verwendet, unsere Skalierungsregel erfüllt; für anspruchsvollere Architekturen zeigen wir, wie unser Skalierungsprinzip verwendet werden kann, um Design-Entscheidungen zu treffen, um gut konditionierte neuronale Netze zu erzeugen und so das Rätselraten zu reduzieren.
Wir untersuchen das Problem der Modellextraktion bei der Verarbeitung natürlicher Sprache, bei dem ein Angreifer, der nur Zugriff auf das Modell eines Opfers hat, versucht, eine lokale Kopie dieses Modells zu rekonstruieren, wobei wir davon ausgehen, dass sowohl das Modell des Angreifers als auch das des Opfers ein großes vortrainiertes Sprachmodell wie BERT (Devlin et al, Wir zeigen, dass der Angreifer nicht einmal grammatikalische oder semantisch sinnvolle Abfragen verwenden muss: Wir zeigen, dass zufällige Wortfolgen in Verbindung mit aufgabenspezifischen Heuristiken effektive Abfragen für die Modellextraktion bei einer Vielzahl von NLP-Aufgaben bilden, einschließlich natürlichsprachlicher Inferenz und Fragenbeantwortung. Unsere Arbeit zeigt somit einen Vorteil auf, der erst durch die Verlagerung hin zu Transfer-Learning-Methoden innerhalb der NLP-Gemeinschaft möglich wurde: Für ein Abfrage-Budget von ein paar hundert Dollar kann ein Angreifer ein Modell extrahieren, das nur geringfügig schlechter als das Opfermodell abschneidet.Schließlich untersuchen wir zwei Verteidigungsstrategien gegen die Modellextraktion - Membership-Klassifizierung und API-Wasserzeichen -, die zwar gegen einige Angreifer erfolgreich sind, aber auch von clevereren Angreifern umgangen werden können.
Wir schlagen SEARNN vor, einen neuartigen Trainingsalgorithmus für rekurrente neuronale Netze (RNNs), der vom Ansatz des "Learning to Search" (L2S) zur strukturierten Vorhersage inspiriert ist.RNNs haben sich in strukturierten Vorhersageanwendungen wie der maschinellen Übersetzung oder dem Parsing bewährt und werden üblicherweise mit Hilfe der Maximum-Likelihood-Schätzung (MLE) trainiert. Leider ist dieser Trainingsverlust nicht immer ein geeignetes Surrogat für den Testfehler: Durch die Maximierung der Grundwahrscheinlichkeit wird die Fülle an Informationen, die strukturierte Verluste bieten, nicht ausgenutzt, und es entstehen Diskrepanzen zwischen Training und Vorhersage (z. B. Expositionsverzerrungen), die sich negativ auf die Testleistung auswirken können.SEARNN nutzt stattdessen testähnliche Suchraumexploration, um global-lokale Verluste einzuführen, die näher am Testfehler liegen.Wir zeigen zunächst eine verbesserte Leistung gegenüber MLE bei zwei verschiedenen Aufgaben: Anschließend schlagen wir eine Subsampling-Strategie vor, die es SEARNN ermöglicht, auf ein großes Vokabular zu skalieren, um so die Vorteile unseres Ansatzes bei einer maschinellen Übersetzungsaufgabe zu validieren.
Deep Reinforcement Learning hat zunehmende Fähigkeiten für kontinuierliche Kontrollprobleme gezeigt, einschließlich Agenten, die sich mit Geschick und Beweglichkeit durch ihre Umgebung bewegen können. Ein offenes Problem in diesem Zusammenhang ist die Entwicklung guter Strategien zur Integration oder Zusammenführung von Strategien für mehrere Fähigkeiten, wobei jede einzelne Fähigkeit ein Spezialist für eine bestimmte Fähigkeit und die damit verbundene Zustandsverteilung ist. Wir erweitern Methoden zur Destillation von Richtlinien auf kontinuierliche Aktionen und nutzen diese Technik, um \Experten-Richtlinien zu kombinieren, wie sie im Bereich der simulierten zweibeinigen Fortbewegung in verschiedenen Geländeklassen evaluiert wurden. Die Kombination dieser Methoden ermöglicht es, eine Strategie inkrementell mit neuen Fähigkeiten zu erweitern. Wir vergleichen unsere Methode des progressiven Lernens und der Integration durch Destillation (PLAID) mit drei alternativen Grundmodellen.
Deep Reinforcement Learning (DRL) hat in jüngster Zeit zu vielen Durchbrüchen bei komplexen Steuerungsaufgaben geführt, wie z. B. dem Sieg über den besten menschlichen Spieler im Go-Spiel. Die vom DRL-Agenten getroffenen Entscheidungen sind jedoch nicht erklärbar, was ihre Anwendbarkeit in sicherheitskritischen Umgebungen behindert. Viper, eine kürzlich vorgeschlagene Technik, konstruiert eine Entscheidungsbaum-Politik, indem sie den DRL-Agenten nachahmt.Entscheidungsbäume sind interpretierbar, da jede durchgeführte Aktion auf den Entscheidungsregelpfad zurückverfolgt werden kann, der zu ihr geführt hat.Ein globaler Entscheidungsbaum, der die DRL-Politik annähert, hat jedoch erhebliche Einschränkungen in Bezug auf die Geometrie der Entscheidungsgrenzen. Wir schlagen MoET vor, ein aussagekräftigeres, aber dennoch interpretierbares Modell, das auf einer Mischung von Experten basiert, bestehend aus einer Gating-Funktion, die den Zustandsraum partitioniert, und mehreren Entscheidungsbaumexperten, die sich auf verschiedene Partitionen spezialisieren.Wir schlagen ein Trainingsverfahren vor, um nicht-differenzierbare Entscheidungsbaumexperten zu unterstützen und integrieren es in das Imitationslernverfahren von Viper. Wir evaluieren unseren Algorithmus auf vier OpenAI-Simulationsumgebungen und zeigen, dass die so konstruierte Politik leistungsfähiger ist und den DRL-Agenten besser imitiert, indem sie Fehlprognosen verringert und die Belohnung erhöht.Wir zeigen auch, dass MoET-Politiken für die Verifizierung mit handelsüblichen automatischen Theorembeweisern wie Z3 geeignet sind.
Wir betrachten das Problem der uneingeschränkten Minimierung einer glatten Zielfunktion in $\mathbb{R}^d$ in einer Umgebung, in der nur Funktionsauswertungen möglich sind.Wir schlagen vor und analysieren stochastische Zeroth-order-Methode mit schwerem Ballmomentum. Insbesondere schlagen wir vor, SMTP, eine Momentum-Version der stochastischen Drei-Punkte-Methode (STP) Bergou et al. (2019).Wir zeigen neue Komplexitätsergebnisse für nicht-konvexe, konvexe und stark konvexe Funktionen.Wir testen unsere Methode auf einer Sammlung von Lern- bis kontinuierlichen Steuerungsaufgaben auf verschiedenen MuJoCo Todorov et al. (2012) Umgebungen mit unterschiedlichen Schwierigkeitsgraden und vergleichen sie mit STP, anderen modernen ableitungsfreien Optimierungsalgorithmen und mit Policy-Gradienten-Methoden.SMTP übertrifft STP und alle anderen Methoden, die wir in unseren numerischen Experimenten betrachtet haben, signifikant.Unser zweiter Beitrag ist SMTP mit Wichtigkeitssampling, das wir SMTP_IS nennen.Wir bieten eine Konvergenzanalyse dieser Methode für nicht-konvexe, konvexe und stark konvexe Ziele.
Die Verwendung von Klassenetiketten zur Darstellung der Klassenähnlichkeit ist ein typischer Ansatz für das Training von Deep-Hashing-Systemen für das Retrieval; Proben aus den gleichen oder unterschiedlichen Klassen nehmen binäre 1 oder 0 Ähnlichkeitswerte an.Diese Ähnlichkeit modelliert nicht das gesamte reichhaltige Wissen der semantischen Beziehungen, die zwischen Datenpunkten vorhanden sein können.In dieser Arbeit bauen wir auf der Idee auf, semantische Hierarchien zu verwenden, um Abstandsmetriken zwischen allen verfügbaren Probenetiketten zu bilden; zum Beispiel hat Katze zu Hund einen kleineren Abstand als Katze zu Gitarre. Wir kombinieren diese Art von semantischem Abstand in einer Verlustfunktion, um ähnliche Abstände zwischen den Einbettungen des tiefen neuronalen Netzes zu fördern. Wir führen auch einen empirischen Kullback-Leibler-Divergenz-Verlustterm ein, um die Binarisierung und Einheitlichkeit der Einbettungen zu fördern. Wir testen die resultierende SHREWD-Methode und demonstrieren Verbesserungen in den hierarchischen Suchergebnissen, indem wir kompakte, binäre Hash-Codes anstelle von reellwertigen verwenden, und zeigen, dass wir in einer schwach überwachten Hash-Einstellung in der Lage sind, konkurrenzfähig zu lernen, ohne uns explizit auf Klassenetiketten zu verlassen, sondern stattdessen auf Ähnlichkeiten zwischen den Etiketten.
In diesem Papier schlagen wir einen neuen Ansatz vor, um eine gegebene Oberflächenabbildung durch lokale Verfeinerung zu verbessern. Der Ansatz empfängt eine etablierte Abbildung zwischen zwei Oberflächen und folgt vier Phasen:(i) Inspektion der Abbildung und Erstellung eines sparsamen Satzes von Landmarken in nicht übereinstimmenden Regionen;(ii) Segmentierung mit einem verzerrungsarmen Regionswachstumsprozess, der auf der Abflachung dieser segmentierten Teile basiert;(iii) Optimierung der Deformation der segmentierten Teile, um die Landmarken in der planaren Parametrisierungsdomäne auszurichten; und(iv) Aggregation der Abbildungen von Segmenten zur Aktualisierung der Oberflächenabbildung. Darüber hinaus schlagen wir eine neue Methode vor, um das Netz zu deformieren, um die Bedingungen zu erfüllen (in unserem Fall die Ausrichtung der Orientierungspunkte in Phase (iii)).Wir passen die Kotangensgewichte für die Bedingungen inkrementell an und wenden die Deformation auf eine Weise an, die garantiert, dass das deformierte Netz keine umgedrehten Flächen und eine geringe konforme Verzerrung aufweist. Unser neuer Deformationsansatz, Iterative Least Squares Conformal Mapping (ILSCM), übertrifft andere Deformationsmethoden mit geringer Verzerrung. Der Ansatz ist allgemein, und wir haben ihn durch die Verbesserung der Mappings verschiedener bestehender Surfacemapping-Methoden getestet. Wir haben auch seine Effektivität durch die Bearbeitung der Mappings für eine Vielzahl von 3D-Objekten getestet.
Das Verständnis der bahnbrechenden Leistung von Deep Neural Networks ist eine der größten Herausforderungen für die wissenschaftliche Gemeinschaft today.In dieser Arbeit, weeintroduce eine informationstheoretische Sicht auf das Verhalten von tiefen Netzwerken Optimierungsprozesse und ihre Generalisierungsfähigkeiten.by Studium der InformationPlane, die Ebene der gegenseitigen Informationen zwischen der Eingabe Variable und die gewünschte Label, für jede versteckte layer.Specifically, zeigen wir, dass das Training des Netzes durch einen schnellen Anstieg der gegenseitigen Informationen (MI) zwischen den Schichten und dem Ziel-Label, gefolgt von einer längeren Rückgang der MIbetween die Schichten und die Eingabe Variable gekennzeichnet ist. Darüber hinaus zeigen wir explizit, dass diese beiden grundlegenden informationstheoretischen Größen dem Generalisierungsfehler des Netzes entsprechen, indem wir eine neue Generalisierungsschranke einführen, die in der Darstellungskompression exponentiell ist.Die Analyse konzentriert sich auf typische Muster großer Probleme.Zu diesem Zweck führen wir eine neuartige analytische Schranke für die gegenseitige Information zwischen aufeinanderfolgenden Schichten im Netz ein.Eine wichtige Folge unserer Analyse ist ein superlinearer Anstieg der Trainingszeit mit der Anzahl der nicht entarteten versteckten Schichten, was den rechnerischen Vorteil der versteckten Schichten zeigt.
Wir schlagen und untersuchen eine Methode für das Lernen interpretierbarer Darstellungen für die Aufgabe der Regression.Features sind als Netzwerke von Multi-Typ-Ausdruck Bäume, die von Aktivierungsfunktionen in neuronalen Netzen in zusätzlich zu anderen elementaren functions.Differentiable Features sind über Gradientenabstieg trainiert, und die Leistung der Funktionen in einem linearen Modell wird verwendet, um die Rate der Veränderung zwischen den Teilkomponenten der einzelnen representation.The Suchprozess unterhält ein Archiv von Darstellungen mit Genauigkeit-Komplexität Kompromisse zur Unterstützung bei der Generalisierung und Interpretation. Wir vergleichen verschiedene stochastische Optimierungsansätze innerhalb dieses Rahmens und vergleichen diese Varianten mit 100 Open-Source-Regressionsproblemen im Vergleich zu den modernsten maschinellen Lernansätzen. Unser Hauptergebnis ist, dass dieser Ansatz die höchsten durchschnittlichen Testergebnisse über alle Probleme hinweg erzielt und gleichzeitig Repräsentationen erzeugt, die um Größenordnungen kleiner sind als die nächstbeste Methode (Gradient Boosting).
Die meisten verteilten maschinellen Lernsysteme (ML) speichern eine Kopie der Modellparameter lokal auf jeder Maschine, um die Netzwerkkommunikation zu minimieren. In der Praxis werden diese Kopien des Modells nicht notwendigerweise im Gleichschritt aktualisiert, um die Wartezeit für die Synchronisierung zu reduzieren, und können veralten. Unsere umfangreichen Experimente zeigen die Vielfalt der Auswirkungen von Staleness auf die Konvergenz von ML-Algorithmen und bieten Einblicke in scheinbar widersprüchliche Berichte in der Literatur. Die empirischen Ergebnisse inspirieren auch eine neue Konvergenzanalyse von SGD in der nicht-konvexen Optimierung unter Staleness, die der besten bekannten Konvergenzrate von O(1/\sqrt{T}) entspricht.
Diese Arbeit stellt einen Algorithmus für nichtlineare Offline-Systemidentifikation aus partiellen Beobachtungen vor, d.h. Situationen, in denen der vollständige Zustand des Systems nicht direkt beobachtbar ist. Der vorgestellte Algorithmus, genannt SISL, leitet den vollständigen Zustand des Systems durch nichtlineare Optimierung iterativ ab und aktualisiert dann die Modellparameter. Wir testen unseren Algorithmus an einem simulierten System von gekoppelten Lorenz-Attraktoren und zeigen, dass unser Algorithmus in der Lage ist, hochdimensionale Systeme zu identifizieren, die sich für partikelbasierte Ansätze als unlösbar erweisen.
Es wurden verschiedene Gradientenkompressionsverfahren vorgeschlagen, um die Kommunikationskosten beim verteilten Training großer maschineller Lernmodelle zu verringern, In diesem Papier führen wir eine allgemeine Analyse von vorzeichenbasierten Methoden für nicht-konvexe Optimierung durch, die auf intuitiven Schranken für Erfolgswahrscheinlichkeiten basiert und weder auf speziellen Rauschverteilungen noch auf der Beschränktheit der Varianz stochastischer Gradienten beruht. Durch die Erweiterung der Theorie auf verteilte Einstellungen innerhalb eines Parameterserver-Rahmens stellen wir eine exponentiell schnelle Varianzreduktion in Bezug auf die Anzahl der Knoten sicher, wobei die 1-Bit-Kompression in beiden Richtungen beibehalten wird und wir kleine Mini-Batch-Größen verwenden.Wir validieren unsere theoretischen Ergebnisse experimentell.
Off-Policy-Lernen, die Aufgabe der Bewertung und Verbesserung von Richtlinien unter Verwendung historischer Daten, die von einer Logging-Policy gesammelt wurden, ist wichtig, da die Bewertung von On-Policy in der Regel teuer ist und negative Auswirkungen hat.Eine der größten Herausforderungen des Off-Policy-Lernens ist es, kontrafaktische Schätzer abzuleiten, die ebenfalls eine geringe Varianz und somit einen geringen Generalisierungsfehler aufweisen. Unsere Methode reguliert den Generalisierungsfehler durch Minimierung der Verteilungsdivergenz zwischen der Protokollierungspolitik und der neuen Politik und beseitigt die Notwendigkeit, durch alle Trainingsbeispiele zu iterieren, um die Stichprobenvarianzregulierung in früheren Arbeiten zu berechnen.Mit neuronalen Netzwerkpolitiken zeigten unsere End-to-End-Trainingsalgorithmen unter Verwendung der Variationsdivergenzminimierung eine signifikante Verbesserung gegenüber konventionellen Basisalgorithmen und sind auch mit unseren theoretischen Ergebnissen konsistent.
Wir skizzieren neue Ansätze, um Ideen aus dem Deep Learning in die wellenbasierte Least-Squares-Bildgebung zu integrieren.Das Ziel und der Hauptbeitrag dieser Arbeit ist die Kombination von handgefertigten Einschränkungen mit tiefen Faltungsneuronalen Netzen, um deren bemerkenswerte Leichtigkeit bei der Erzeugung natürlicher Bilder zu nutzen.Die mathematische Grundlage unserer Methode ist die Erwartungsmaximierung, bei der Daten in Stapel aufgeteilt und mit zusätzlichen "latenten" Unbekannten gekoppelt werden. Diese Unbekannten sind Paare von Elementen aus dem ursprünglichen Raum der Unbekannten (aber jetzt an einen bestimmten Datenstapel gekoppelt) und Netzwerkeingaben. in diesem Rahmen kontrolliert das neuronale Netzwerk die Ähnlichkeit zwischen diesen zusätzlichen Parametern und fungiert als "zentrale" Variable. das resultierende Problem läuft auf eine Maximum-Likelihood-Schätzung der Netzwerkparameter hinaus, wenn das erweiterte Datenmodell über die latenten Variablen marginalisiert wird.
Bei der Übersetzung von Fragen in natürlicher Sprache in SQL-Abfragen zur Beantwortung von Fragen aus einer Datenbank haben moderne semantische Parsing-Modelle Schwierigkeiten, auf unbekannte Datenbankschemata zu verallgemeinern.  Die Herausforderung bei der Verallgemeinerung besteht darin, (a) die Datenbankbeziehungen auf eine für den semantischen Parser zugängliche Weise zu kodieren und (b) die Ausrichtung zwischen Datenbankspalten und ihren Erwähnungen in einer gegebenen Abfrage zu modellieren.  Auf dem anspruchsvollen Spider-Datensatz steigert dieses Framework die exakte Treffergenauigkeit auf 53,7 %, verglichen mit 47,4 % für das vorherige State-of-the-Art-Modell ohne BERT-Einbettungen. Darüber hinaus beobachten wir qualitative Verbesserungen im Verständnis des Modells für Schemaverknüpfung und -ausrichtung.
Wie unsere Erfahrung zeigt, können Menschen eine Vielzahl verschiedener Fähigkeiten erlernen und einsetzen, um die Situationen zu bewältigen, mit denen sie täglich konfrontiert werden. Neuronale Netze hingegen haben eine feste Speicherkapazität, die sie daran hindert, mehr als ein paar Sätze von Fähigkeiten zu erlernen, bevor sie beginnen, diese zu vergessen. Um dieses System zu testen, führen wir eine kontinuierliche Lernaufgabe ein, die auf der Sprachmodellierung basiert, bei der das Modell nacheinander mehreren Sprachen und Domänen ausgesetzt wird, ohne dass ein explizites Signal über die Art des Inputs gegeben wird, mit dem es sich gerade befasst.Das vorgeschlagene System weist verbesserte Anpassungsfähigkeiten auf, da es sich nach einem Wechsel der Eingabesprache oder -domäne schneller erholen kann als vergleichbare Basissysteme.
Ein auf einem rekurrenten neuronalen Netz (RNN) basierendes Modell wird eingesetzt, um einen nichtlinearen Operator für die zeitliche Entwicklung einer Wahrscheinlichkeitsdichtefunktion zu erlernen. Wir verwenden eine Softmax-Schicht für eine numerische Diskretisierung einer glatten Wahrscheinlichkeitsdichtefunktion, die ein Funktionsapproximationsproblem in eine Klassifizierungsaufgabe umwandelt. Es werden explizite und implizite Regularisierungsstrategien eingeführt, um eine Glättungsbedingung für die geschätzte Wahrscheinlichkeitsverteilung aufzuerlegen, und es wird ein Monte-Carlo-Verfahren zur Berechnung der zeitlichen Entwicklung der Verteilung für eine mehrstufige Vorhersage vorgestellt.Die Bewertung des vorgeschlagenen Algorithmus auf drei synthetischen und zwei realen Datensätzen zeigt einen Vorteil gegenüber den verglichenen Baselines.
In kognitiven Systemen ist die Rolle des Arbeitsgedächtnisses von entscheidender Bedeutung für das visuelle Denken und die Entscheidungsfindung, und es wurden enorme Fortschritte beim Verständnis der Mechanismen des menschlichen und tierischen Arbeitsgedächtnisses sowie bei der Formulierung verschiedener Rahmen künstlicher neuronaler Netze erzielt.  Beim Menschen ist die Aufgabe des visuellen Arbeitsgedächtnisses (VWM) eine Standardaufgabe, bei der den Versuchspersonen eine Folge von Bildern präsentiert wird, von denen jedes einzelne identifiziert werden muss, ob es bereits gesehen wurde oder nicht. In unserer Arbeit untersuchen wir verschiedene Möglichkeiten zum Erlernen eines Arbeitsgedächtnismodells unter Verwendung rekurrenter neuronaler Netze, die lernen, sich über Zeitschritte hinweg an die eingegebenen Bilder zu erinnern, und trainieren diese neuronalen Netze zur Lösung der Arbeitsgedächtnisaufgabe, indem wir sie mit einer Bildsequenz in überwachten und verstärkenden Lernumgebungen trainieren. Die Einstellung des verstärkenden Lernens ist inspiriert von der in den Neurowissenschaften weit verbreiteten Ansicht, dass das Arbeitsgedächtnis im präfrontalen Kortex durch einen dopaminergen Mechanismus moduliert wird.Wir betrachten die VWM-Aufgabe als eine Umgebung, die den Agenten belohnt, wenn er sich an vergangene Informationen erinnert, und ihn für das Vergessen bestraft.  Wir schätzen die Leistung dieser Modelle bei Bildsequenzen aus einem Standard-Bilddatensatz (CIFAR-100) quantitativ ab und bewerten ihre Fähigkeit, sich zu erinnern und abzurufen, wenn sie zunehmend über Episoden hinweg trainiert werden.Auf der Grundlage unserer Analyse stellen wir fest, dass ein gated rekurrentes neuronales Netzwerkmodell mit Einheiten des Kurzzeitgedächtnisses, das mit Hilfe von Verstärkungslernen trainiert wird, leistungsfähiger und effizienter bei der zeitlichen Konsolidierung der eingegebenen räumlichen Informationen ist. Bei dieser Arbeit handelt es sich um eine erste Analyse im Rahmen unseres endgültigen Ziels, künstliche neuronale Netze zu verwenden, um das Verhalten und die Informationsverarbeitung des Arbeitsgedächtnisses des Gehirns zu modellieren und bildgebende Daten des Gehirns zu verwenden, die von menschlichen Probanden während der kognitiven VWM-Aufgabe erfasst wurden, um verschiedene Gedächtnismechanismen des Gehirns zu verstehen. 
Die Nichtlinearität ist für die Leistung eines tiefen (neuronalen) Netzes (DN) von entscheidender Bedeutung, und bisher gab es kaum Fortschritte beim Verständnis der Menagerie der verfügbaren Nichtlinearitäten, aber in letzter Zeit wurden Fortschritte beim Verständnis der Rolle erzielt, die stückweise affine und konvexe Nichtlinearitäten wie die ReLU- und Absolutwert-Aktivierungsfunktionen und das Max-Pooling spielen. (MASOs) interpretiert werden, die eine elegante Verbindung zur Vektorquantisierung (VQ) und zu $K$-Mittelwerten haben. Während dies ein guter theoretischer Fortschritt ist, basiert der gesamte MASO-Ansatz auf der Anforderung, dass die Nichtlinearitäten stückweise affin und konvex sein müssen, was wichtige Aktivierungsfunktionen wie Sigmoid, hyperbolischer Tangens und Softmax ausschließt. {Diese Arbeit erweitert den MASO-Rahmen auf diese und eine unendlich große Klasse neuer Nichtlinearitäten, indem deterministische MASOs mit probabilistischen Gaussian Mixture Models (GMMs) verbunden werden. Wir zeigen, dass unter einem GMM stückweise affine, konvexe Nichtlinearitäten wie ReLU, Absolutwert und Max-Pooling als Lösungen für bestimmte natürliche ``harte'' VQ-Inferenzprobleme interpretiert werden können, während Sigmoid, hyperbolischer Tangens und Softmax als Lösungen für entsprechende ``weiche'' VQ-Inferenzprobleme interpretiert werden können. Wir erweitern den Rahmen, indem wir die harten und weichen VQ-Optimierungen hybridisieren, um eine $\beta$-VQ-Inferenz zu schaffen, die zwischen harter, weicher und linearer VQ-Inferenz interpoliert.Ein Paradebeispiel für eine $\beta$-VQ-DN-Nonlinearität ist die {\em swish}-Nonlinearität, die in einer Reihe von Computer-Vision-Aufgaben eine Spitzenleistung bietet, aber ad hoc durch Experimente entwickelt wurde.Schließlich validieren wir mit Experimenten eine wichtige Behauptung unserer Theorie, nämlich dass die DN-Leistung durch die Erzwingung von Orthogonalität in ihren linearen Filtern erheblich verbessert werden kann.
Ein wesentlicher Aspekt dieser Herausforderung ist die komplexe Kopplung zwischen Proteinsequenz und 3D-Struktur, und die Aufgabe, ein brauchbares Design zu finden, wird oft als das inverse Proteinfaltungsproblem bezeichnet. Wir entwickeln generative Modelle für Proteinsequenzen, die auf einer graphenstrukturierten Spezifikation des Designziels beruhen, und erfassen die komplexen Abhängigkeiten in Proteinen, indem wir uns auf diejenigen konzentrieren, die in der Sequenz weitreichend, aber im 3D-Raum lokal sind.
Insbesondere zeigen wir, dass ein Vorwärtsdurchlauf durch eine Standard-Dropout-Schicht, gefolgt von einer linearen Schicht und einer nicht-linearen Aktivierung, der Optimierung eines konvexen Ziels mit einer einzigen Iteration einer $\tau$-schönen proximalen stochastischen Gradientenmethode entspricht. Wir führen Experimente mit Standard-Faltungsnetzen durch, die auf die CIFAR-10- und CIFAR-100-Datensätze angewendet werden, und zeigen, dass das Ersetzen eines Blocks von Schichten durch mehrere Iterationen des entsprechenden Lösers, wobei die Schrittgröße über $L$ festgelegt wird, die Klassifizierungsgenauigkeit konsistent verbessert.
Hier stellen wir eine Methode für das Training solcher Netzwerke vor, Learned Step Size Quantization, die die bisher höchste Genauigkeit auf dem ImageNet-Datensatz erreicht, wenn Modelle aus einer Vielzahl von Architekturen mit Gewichten und Aktivierungen verwendet werden, die auf eine Genauigkeit von 2, 3 oder 4 Bits quantisiert sind, und die 3-Bit-Modelle trainieren kann, die die Basisgenauigkeit bei voller Präzision erreichen. Unser Ansatz baut auf bestehenden Methoden zum Erlernen von Gewichten in quantisierten Netzwerken auf, indem wir die Konfiguration des Quantisierers selbst verbessern, insbesondere führen wir ein neuartiges Verfahren zur Schätzung und Skalierung des Verlustgradienten bei der Quantisierungsschrittgröße jeder Gewichts- und Aktivierungsschicht ein, so dass dieser in Verbindung mit anderen Netzwerkparametern erlernt werden kann.dieser Ansatz funktioniert mit verschiedenen Präzisionsniveaus, die für ein bestimmtes System erforderlich sind, und erfordert nur eine einfache Änderung des bestehenden Trainingscodes.
In jüngster Zeit wurden mehrere Studien durchgeführt, die zeigen, dass starke Modelle für das Verstehen natürlicher Sprache (NLU) dazu neigen, sich auf unerwünschte Verzerrungen in Datensätzen zu stützen, ohne die zugrundeliegende Aufgabe zu lernen, was zu Modellen führt, die sich nicht auf Datensätze außerhalb der Domäne verallgemeinern lassen und in realen Szenarien wahrscheinlich schlecht abschneiden werden. Wir führen ein zusätzliches, leichtgewichtiges, reines Bias-Modell ein, das die Verzerrungen der Datensätze lernt und seine Vorhersage verwendet, um den Verlust des Basismodells anzupassen, um die Verzerrungen zu reduzieren.  Wir experimentieren mit großen Datensätzen für die Inferenz natürlicher Sprache und die Verifikation von Fakten und zeigen, dass unsere debias-Modelle die Robustheit in allen Umgebungen signifikant verbessern, einschließlich eines Gewinns von 9,76 Punkten auf dem symmetrischen FEVER-Evaluationsdatensatz, 5,45 Punkten auf dem HANS-Datensatz und 4,78 Punkten auf dem SNLI-Hard-Set.  Diese Datensätze wurden speziell entwickelt, um die Robustheit von Modellen in der Out-of-Domain-Umgebung zu bewerten, in der typische Verzerrungen in den Trainingsdaten im Evaluierungsset nicht vorhanden sind.
Die Rekonstruktion von Röntgen-Computertomographie (CT)-Daten mit wenigen Ansichten ist ein hochgradig ungelöstes Problem, das häufig in Anwendungen eingesetzt wird, die eine niedrige Strahlendosis erfordern, wie z. B. in der klinischen CT, bei schnellen industriellen Scans oder in der Fixed-Gantry-CT. Die vorgeschlagene Methode interpretiert Sinogramm-Daten mit Hilfe von aufmerksamkeitsbasierten tiefen Netzwerken, um das rekonstruierte Bild zu ermitteln. Das vorhergesagte Bild wird dann als Vorwissen im iterativen Algorithmus für die endgültige Rekonstruktion verwendet. Wir demonstrieren die Effektivität des vorgeschlagenen Ansatzes, indem wir Rekonstruktionsexperimente an einem Brust-CT-Datensatz durchführen.
Die populärsten Sequenz-zu-Sequenz-Modelle "generieren und hoffen" typischerweise generische Äußerungen, die in den Gewichten des Modells gespeichert werden können, wenn sie von der/den Eingabe-Äußerung(en) auf die Ausgabe abgebildet werden, anstatt abgerufenes Wissen als Kontext zu verwenden. Die Verwendung von Wissen hat sich bisher als schwierig erwiesen, zum Teil wegen des Fehlens einer überwachten Lern-Benchmark-Aufgabe, die einen wissensbasierten offenen Dialog mit klarer Grundlage zeigt.  Unsere leistungsstärksten Dialogmodelle sind in der Lage, sachkundige Diskussionen zu offenen Themen zu führen, wie sie durch automatische Metriken und menschliche Bewertungen bewertet werden, während unsere neue Benchmark weitere Verbesserungen in dieser wichtigen Forschungsrichtung ermöglicht.
Wir formulieren ein neues Problem an der Schnittstelle von semi-supervised learning und Contextual Bandits, motiviert durch verschiedene Anwendungen, darunter klinische Studien und Dialogsysteme.Wir zeigen, wie Contextual Bandit und Graph Convolutional Networks an die neue Problemformulierung angepasst werden können.Wir nehmen dann das Beste aus beiden Ansätzen, um Multi-GCN Embedded Contextual Bandit zu entwickeln.Unsere Algorithmen sind auf mehreren realen Datensätzen verifiziert.
 Eine Sammlung wissenschaftlicher Arbeiten wird oft von Tags begleitet: Schlüsselwörter, Themen, Konzepte usw., die mit jeder Arbeit verbunden sind.  Manchmal sind diese Tags von Menschen, manchmal von Maschinen generiert.  Wir schlagen ein einfaches Maß für die Konsistenz der Verschlagwortung wissenschaftlicher Arbeiten vor: ob diese Verschlagwortung prädiktiv für die Links im Zitationsgraph ist.  Da die Autoren dazu neigen, Arbeiten zu zitieren, die den Themen ihrer Veröffentlichungen nahe stehen, könnte ein konsistentes Tagging-System Zitate vorhersagen.  Wir stellen einen Algorithmus zur Berechnung der Konsistenz vor und führen Experimente mit von Menschen und Maschinen erzeugten Tags durch.  Wir zeigen, dass die Augmentation, d. h. die Kombination der manuellen Tags mit den maschinell erzeugten, die Konsistenz der Tags verbessern kann.  Darüber hinaus führen wir die Cross-Konsistenz ein, d.h. die Fähigkeit, Zitationsverknüpfungen zwischen Artikeln vorherzusagen, die von verschiedenen Taggern, z.B. manuell und maschinell, getaggt wurden.  Die Kreuzkonsistenz kann zur Bewertung der Tagging-Qualität herangezogen werden, wenn die Menge der gelabelten Daten begrenzt ist.
Jüngste Forschungsarbeiten haben die Anfälligkeit von tiefen neuronalen Netzen, insbesondere von Faltungsneuronalen Netzen (CNNs), bei der Bilderkennung durch die Erstellung von Gegenproben, die sich "leicht" von legitimen Proben unterscheiden, deutlich gemacht. Diese Anfälligkeit deutet darauf hin, dass diese leistungsstarken Modelle empfindlich auf bestimmte Störungen reagieren und diese Störungen nicht herausfiltern können. In dieser Arbeit schlagen wir eine auf Quantisierung basierende Methode vor, die es einem CNN ermöglicht, unerwünschte Störungen effektiv herauszufiltern. Um den Informationsverlust zu kompensieren, der unweigerlich durch die Quantisierung verursacht wird, schlagen wir die Multi-Head-Quantisierung vor, bei der wir Datenpunkte auf verschiedene Unterräume projizieren und die Quantisierung in jedem Unterraum durchführen. Die Ergebnisse, die wir mit den Datensätzen MNIST und Fashion-MNSIT erzielt haben, zeigen, dass das Hinzufügen einer einzigen Q-Schicht zu einem CNN dessen Robustheit gegenüber White-Box- und Black-Box-Angriffen erheblich verbessern kann.
Eine grundlegende Herausforderung bei der Entwicklung solcher Netzwerke ist es, die maximale Sammlung von invarianten und äquivarianten \emph{linearen} Schichten zu finden. Obwohl diese Frage für die ersten drei Beispiele beantwortet ist (zumindest für populäre Transformationen), ist eine vollständige Charakterisierung von invarianten und äquivarianten linearen Schichten für Graphen nicht bekannt. In dieser Arbeit wird eine Charakterisierung aller permutationsinvarianten und äquivarianten linearen Schichten für (Hyper-)Graphdaten gegeben und gezeigt, dass ihre Dimension im Falle von kantenwertigen Graphdaten $2$ bzw. $15$ beträgt. Allgemeiner ausgedrückt: Für Graphdaten, die auf $k$-Tupeln von Knoten definiert sind, ist die Dimension die $k$-te bzw. $2k$-te Bell-Zahl.Orthogonale Basen für die Schichten werden berechnet, einschließlich der Verallgemeinerung auf Multigraphdaten. Aus theoretischer Sicht verallgemeinern und vereinheitlichen unsere Ergebnisse die jüngsten Fortschritte im Bereich des äquivarianten tiefen Lernens, insbesondere zeigen wir, dass unser Modell in der Lage ist, ein beliebiges neuronales Netzwerk mit Nachrichtenübermittlung zu approximieren, und dass die Anwendung dieser neuen linearen Schichten in einem einfachen tiefen neuronalen Netzwerk vergleichbare Ergebnisse wie der Stand der Technik und eine bessere Ausdruckskraft als frühere invariante und äquivariante Basen liefert.
Beim Reinforcement Learning können wir ein Modell zukünftiger Beobachtungen und Belohnungen erlernen und es verwenden, um die nächsten Aktionen des Agenten zu planen. Die gemeinsame Modellierung zukünftiger Beobachtungen kann jedoch rechenintensiv oder sogar unpraktikabel sein, wenn die Beobachtungen hochdimensional sind (z. B. Bilder). Aus diesem Grund haben frühere Arbeiten partielle Modelle in Betracht gezogen, die nur einen Teil der Beobachtung modellieren. In dieser Arbeit zeigen wir, dass partielle Modelle kausal inkorrekt sein können: Sie sind durch die Beobachtungen, die sie nicht modellieren, verwirrt und können daher zu einer falschen Planung führen.Um dieses Problem anzugehen, führen wir eine allgemeine Familie von partiellen Modellen ein, die nachweislich kausal korrekt sind, aber die Notwendigkeit vermeiden, zukünftige Beobachtungen vollständig zu modellieren.
In dieser Arbeit untersuchen wir die Effizienz aktueller lebenslanger Ansätze im Hinblick auf die Komplexität der Stichprobe sowie die Rechen- und Speicherkosten. Zu diesem Zweck führen wir zunächst ein neues und realistischeres Bewertungsprotokoll ein, bei dem die Lernenden jedes Beispiel nur einmal beobachten und die Auswahl der Hyperparameter auf einer kleinen und unzusammenhängenden Menge von Aufgaben erfolgt, die nicht für die eigentliche Lernerfahrung und Bewertung verwendet wird. Zweitens führen wir eine neue Metrik ein, die misst, wie schnell ein Lerner eine neue Fähigkeit erwirbt. Drittens schlagen wir eine verbesserte Version von GEM (Lopez-Paz & Ranzato, 2017) vor, die als A-GEM (A-GEM) bezeichnet wird und dieselbe oder sogar eine bessere Leistung wie GEM aufweist, während sie fast so rechen- und speichereffizient wie EWC (Kirkpatrick et al., Schließlich zeigen wir, dass alle Algorithmen, einschließlich A-GEM, noch schneller lernen können, wenn sie mit Aufgabendeskriptoren versehen werden, die die zu betrachtenden Klassifizierungsaufgaben spezifizieren.
In den letzten Jahren wurde das Training von tiefen neuronalen Netzen weitgehend auf 16-Bit-Präzision umgestellt, was zu erheblichen Leistungs- und Energieeinsparungen geführt hat. Versuche, DNNs mit 8-Bit-Präzision zu trainieren, sind jedoch aufgrund der höheren Anforderungen an die Präzision und den dynamischen Bereich der Backpropagation auf erhebliche Herausforderungen gestoßen.   In dieser Arbeit schlagen wir eine Methode zum Trainieren von tiefen neuronalen Netzen mit 8-Bit-Gleitkommadarstellung für Gewichte, Aktivierungen, Fehler und Gradienten vor.  Wir zeigen, dass die Genauigkeit bei mehreren Datensätzen (imagenet-1K, WMT16) und einer größeren Anzahl von Arbeitslasten (Resnet-18/34/50, GNMT und Transformer) als bisher berichtet, dem Stand der Technik entspricht.   Wir schlagen eine verbesserte Verlustskalierungsmethode vor, um den reduzierten subnormalen Bereich der 8-Bit-Gleitkommazahl zu erweitern, um die Fehlerfortpflanzung zu verbessern, untersuchen die Auswirkungen von Quantisierungsrauschen auf die Generalisierung und schlagen eine stochastische Rundungstechnik vor, um das Gradientenrauschen anzugehen.
Einige überwachen den Lernprozess durch paarweise oder dreifache Ähnlichkeitsbeschränkungen, während andere den Vorteil strukturierter Ähnlichkeitsinformationen zwischen mehreren Datenpunkten nutzen.In dieser Arbeit nähern wir uns dem tiefen metrischen Lernen aus einer neuartigen Perspektive.Wir schlagen die Instanz-Cross-Entropie (ICE) vor, die die Differenz zwischen einer geschätzten Instanz-Level-Matching-Verteilung und ihrer Grundwahrheit misst.ICE hat drei attraktive Haupteigenschaften. Erstens hat ICE, ähnlich wie die kategoriale Kreuzentropie (CCE), eine klare probabilistische Interpretation und nutzt strukturierte semantische Ähnlichkeitsinformationen für die Lernüberwachung. Zweitens ist ICE auf unendlich viele Trainingsdaten skalierbar, da es iterativ auf Mini-Batches lernt und unabhängig von der Größe der Trainingsmenge ist. Drittens ist, motiviert durch unsere Analyse der relativen Gewichtung, eine nahtlose Neugewichtung der Proben integriert, die die Gradienten der Proben neu skaliert, um den Differenzierungsgrad über die Trainingsbeispiele zu kontrollieren, anstatt sie durch Sample Mining abzuschneiden.Zusätzlich zu seiner Einfachheit und Intuitivität zeigen umfangreiche Experimente an drei realen Benchmarks die Überlegenheit von ICE.
Beim modellbasierten Verstärkungslernen wechselt der Agent zwischen Modelllernen und Planung.  Wenn das Modell nicht in der Lage ist, eine vernünftige langfristige Vorhersage zu treffen, würde der Planer Modellfehler ausnutzen, was zu katastrophalen Fehlern führen kann.In diesem Beitrag konzentrieren wir uns auf den Aufbau eines Modells, das Gründe für die langfristige Zukunft liefert, und zeigen, wie dies für eine effiziente Planung und Erkundung genutzt werden kann.Zu diesem Zweck bauen wir ein latent-variables autoregressives Modell, indem wir die neuesten Ideen der Variationsinferenz nutzen. Wir argumentieren, dass das Erzwingen von latenten Variablen, um zukünftige Informationen durch eine Hilfsaufgabe zu tragen, die langfristigen Vorhersagen wesentlich verbessert.Darüber hinaus wird durch die Planung im latenten Raum sichergestellt, dass die Lösung des Planers innerhalb von Regionen liegt, in denen das Modell gültig ist.Eine Explorationsstrategie kann durch die Suche nach unwahrscheinlichen Trajektorien unter dem Modell entwickelt werden.Unsere Methoden erreicht höhere Belohnung schneller im Vergleich zu den Grundlinien auf einer Vielzahl von Aufgaben und Umgebungen in der Nachahmung Lernen und Modell-basierte Verstärkung Lernen Einstellungen.
Obwohl es mehrere Studien zur Erkennung von Anomalien auf der Grundlage von regelbasierten oder auf maschinellem Lernen basierenden Ansätzen für Satellitensysteme gibt, wurde eine tensorbasierte Zerlegungsmethode für die Erkennung von Anomalien noch nicht ausgiebig erforscht.In dieser Arbeit stellen wir ein Integrative Tensor-based Anomaly Detection (ITAD) Framework zur Erkennung von Anomalien in einem Satellitensystem vor. Wir konstruieren Tensoren dritter Ordnung mit Telemetriedaten, die von Korea Multi-Purpose Satellite-2 (KOMPSAT-2) gesammelt wurden, und berechnen den Anomalie-Score unter Verwendung einer der Komponentenmatrizen, die durch die Anwendung der CANDECOMP/PARAFAC-Zerlegung zur Erkennung von Anomalien erhalten wurden.Unser Ergebnis zeigt, dass unser tensorbasierter Ansatz im Vergleich zu anderen existierenden Ansätzen eine höhere Genauigkeit bei der Erkennung von Anomalien und eine Verringerung von Fehlalarmen erreicht.
In dieser Arbeit untersuchen wir die Möglichkeit, eine solche wiederholte Struktur zu nutzen, um das Lernen zu beschleunigen und zu regulieren.Wir beginnen mit dem KL-regulierten Ziel der erwarteten Belohnung, das eine zusätzliche Komponente einführt, eine Standardrichtlinie.Anstatt sich auf eine feste Standardrichtlinie zu verlassen, lernen wir sie aus Daten.Aber entscheidend ist, dass wir die Menge an Informationen, die die Standardrichtlinie erhält, beschränken und sie zwingen, wiederverwendbare Verhaltensweisen zu lernen, die der Richtlinie helfen, schneller zu lernen. Wir stellen empirische Ergebnisse sowohl für diskrete als auch für kontinuierliche Aktionsbereiche vor und zeigen, dass für bestimmte Aufgaben das Erlernen einer Standardrichtlinie neben der Richtlinie das Lernen erheblich beschleunigen und verbessern kann. Sehen Sie sich das Video an, in dem erlernte Experten und Standardrichtlinien für verschiedene kontinuierliche Steuerungsaufgaben demonstriert werden ( https://youtu.be/U2qA3llzus8 ).
Wenn ein Bildklassifikator eine Vorhersage trifft, welche Teile des Bildes sind dann relevant und warum? wir können diese Frage umformulieren und fragen: welche Teile des Bildes, wenn sie vom Klassifikator nicht gesehen würden, würden seine Entscheidung am meisten verändern? um eine Antwort zu finden, müssen wir Bilder ausschließen, die gesehen werden könnten, aber nicht gesehen wurden. Wir optimieren dann, um die Bildregionen zu finden, die die Entscheidung des Klassifizierers nach der Füllung am stärksten verändern. Unser Ansatz steht im Gegensatz zu Ad-hoc-Füllungsansätzen wie Unschärfe oder Rauschen, die Eingaben weit entfernt von der Datenverteilung erzeugen und informative Beziehungen zwischen verschiedenen Teilen des Bildes ignorieren. Unsere Methode erzeugt kompaktere und relevantere Salienzkarten mit weniger Artefakten im Vergleich zu früheren Methoden.
Die Originalität unseres Ansatzes besteht darin, dass VarNet nicht nur in der Lage ist, vordefinierte Attribute zu verarbeiten, sondern auch die relevanten Attribute des Datensatzes selbst zu lernen.  Darüber hinaus verfügt VarNet über eine solide probabilistische Interpretation, die uns eine neuartige Möglichkeit bietet, in den latenten Räumen zu navigieren und zu kontrollieren, wie die Attribute erlernt werden.Wir zeigen experimentell, dass dieses Modell in der Lage ist, interessante Eingabemanipulationen durchzuführen und dass die erlernten Attribute relevant und interpretierbar sind.
Trotz der Besorgnis über die Abhängigkeit von Systemen des maschinellen Lernens von so genannten falschen Mustern in den Trainingsdaten fehlt dem Begriff eine kohärente Bedeutung in den Standardstatistiken, aber die Sprache der Kausalität bietet Klarheit: falsche Assoziationen sind solche, die auf eine gemeinsame Ursache (Confounding) gegenüber direkten oder indirekten Auswirkungen zurückzuführen sind. Wir stellen Methoden und Ressourcen vor, um Modelle zu trainieren, die unempfindlich gegenüber falschen Mustern sind. Bei gegebenen Dokumenten und ihren anfänglichen Bezeichnungen beauftragen wir Menschen damit, jedes Dokument so zu überarbeiten, dass es mit einer kontrafaktischen Zielbezeichnung übereinstimmt, wobei wir verlangen, dass die überarbeiteten Dokumente intern kohärent sind, während wir unnötige Änderungen vermeiden.Interessanterweise versagen Klassifikatoren, die auf Originaldaten trainiert wurden, bei Aufgaben zur Stimmungsanalyse und zur Inferenz natürlicher Sprache bei ihren kontrafaktisch überarbeiteten Gegenstücken und umgekehrt. Klassifikatoren, die auf kombinierten Datensätzen trainiert wurden, schneiden bemerkenswert gut ab und liegen nur knapp hinter denen, die auf eine der beiden Domänen spezialisiert sind.Während Klassifikatoren, die nur auf Original- oder manipulierten Daten trainiert wurden, empfindlich auf falsche Merkmale reagieren (z.B. Erwähnungen des Genres), sind Modelle, die auf den Originaldaten trainiert wurden, empfindlich, Wir werden beide Datensätze veröffentlichen.
Unter den zahlreichen Möglichkeiten, ein Modell des maschinellen Lernens zu interpretieren, ist die Messung der Bedeutung einer Reihe von Merkmalen, die mit einer Vorhersage verknüpft sind, wahrscheinlich eine der intuitivsten Möglichkeiten, ein Modell zu erklären.In dieser Arbeit stellen wir die Verbindung zwischen einer Reihe von Merkmalen und einer Vorhersage mit einem neuen Bewertungskriterium her, der Robustheitsanalyse, die die Mindesttoleranz gegenüber einer Störung durch einen Gegner misst. Durch die Messung des Toleranzniveaus für einen gegnerischen Angriff können wir einen Satz von Merkmalen extrahieren, der die robusteste Unterstützung für eine aktuelle Vorhersage bietet, und wir können auch einen Satz von Merkmalen extrahieren, der die aktuelle Vorhersage mit einer Zielklasse kontrastiert, indem wir einen gezielten gegnerischen Angriff setzen.
Generative adversarial networks (GANs) haben gezeigt, dass sie eine effektive Möglichkeit bieten, komplexe Verteilungen zu modellieren und haben beeindruckende Ergebnisse auf verschiedenen anspruchsvollen tasks.However, typische GANs erfordern vollständig beobachteten Daten während des Trainings.In diesem Papier präsentieren wir eine GAN-basierte Rahmen für das Lernen von komplexen, hochdimensionale unvollständige Daten. Der vorgeschlagene Rahmen lernt einen vollständigen Datengenerator zusammen mit einem Maskengenerator, der die Verteilung der fehlenden Daten modelliert. Wir demonstrieren außerdem, wie fehlende Daten imputiert werden können, indem wir unseren Rahmen mit einem adversarial trainierten Imputer ausstatten.
In diesem Papier schlagen wir eine neue Kompressionsmethode, Inter-Layer Weight Prediction (ILWP) und Quantisierungsmethode, die die vorhergesagten Residuen zwischen den Gewichten in allen Faltungsschichten auf der Grundlage einer Inter-Frame-Vorhersage-Methode in konventionellen Video-Codierung Schemata quantisieren.Darüber hinaus fanden wir ein Phänomen Smoothly Varying Weight Hypothesis (SVWH), die ist, dass die Gewichte in benachbarten Faltungsschichten teilen starke Ähnlichkeit in Formen und Werte, dh , Auf der Grundlage der SVWH schlagen wir eine zweite ILWP- und Quantisierungsmethode vor, die die vorhergesagten Residuen zwischen den Gewichten in benachbarten Faltungsschichten quantisiert. Da die vorhergesagten Gewichtsresiduen dazu neigen, Laplace-Verteilungen mit sehr geringer Varianz zu folgen, kann die Gewichtsquantisierung effektiver angewandt werden, wodurch mehr Nullgewichte erzeugt und das Gewichtskompressionsverhältnis verbessert werden. Darüber hinaus schlagen wir einen neuen schichtübergreifenden Verlust zur Eliminierung von Nicht-Textur-Bits vor, der es uns ermöglicht, nur Textur-Bits effektiver zu speichern, d.h. der vorgeschlagene Verlust reguliert die Gewichte so, dass die kollokierten Gewichte zwischen den beiden benachbarten Schichten die gleichen Werte haben.
Es gibt eine Vielzahl von Techniken, um während des Optimierungsprozesses strukturierte Sparsamkeit in parametrischen Modellen zu induzieren, mit dem Endziel einer ressourceneffizienten Inferenz. Unseres Wissens nach zielt jedoch keine auf eine bestimmte Anzahl von Gleitkommaoperationen (FLOPs) als Teil eines einzigen End-to-End-Optimierungsziels ab, obwohl FLOPs als Teil der Ergebnisse angegeben werden. Darüber hinaus ignoriert ein One-Size-Fits-All-Ansatz realistische Systembeschränkungen, die sich z. B. zwischen einem Grafikprozessor und einem Mobiltelefon erheblich unterscheiden - FLOPs auf dem ersten verursachen weniger Latenz als auf dem zweiten; daher ist es für Praktiker wichtig, eine Zielanzahl von FLOPs während der Modellkomprimierung angeben zu können.In dieser Arbeit erweitern wir eine hochmoderne Technik, um FLOPs direkt als Teil des Optimierungsziels zu integrieren, und zeigen, dass angesichts einer gewünschten FLOPs-Anforderung verschiedene neuronale Netze erfolgreich für die Bildklassifizierung trainiert werden können.
Die unpaarige Bild-zu-Bild-Übersetzung zwischen verschiedenen Domänen hat in den letzten Jahrzehnten bemerkenswerte Erfolge erzielt, wobei sich neuere Studien vor allem auf zwei Herausforderungen konzentrieren: Zum einen ist eine solche Übersetzung von Natur aus multimodal, da die domänenspezifischen Informationen variieren (z.B, Zum einen ist eine solche Übersetzung von Natur aus multimodal, da die domänenspezifischen Informationen variieren (z.B. hat die Domäne der Hauskatze mehrere feinkörnige Unterkategorien), zum anderen sind die bestehenden multimodalen Ansätze bei der Handhabung von mehr als zwei Domänen eingeschränkt, d.h. sie müssen unabhängig voneinander ein Modell für jedes Domänenpaar erstellen.Um diese Probleme zu lösen, schlagen wir die hierarchische Bild-zu-Bild-Übersetzung (HIT) vor, die das multimodale und multidomäne Problem in einer semantischen Hierarchiestruktur formuliert und die Unsicherheiten der Multimodalität besser kontrollieren kann. Insbesondere betrachten wir die domänenspezifischen Variationen als das Ergebnis der Multigranularitätseigenschaft von Domänen, und man kann die Granularität der multimodalen Übersetzung kontrollieren, indem man eine Domäne mit großen Variationen in mehrere Subdomänen unterteilt, die lokale und feinkörnige Variationen erfassen. Um diesen komplizierten Raum zu erlernen, schlagen wir vor, die Inklusionsbeziehung zwischen den Domänen zu nutzen, um die Verteilungen von Eltern und Kindern ineinander zu schachteln.Experimente mit verschiedenen Datensätzen bestätigen die vielversprechenden Ergebnisse und die konkurrenzfähige Leistung gegenüber dem Stand der Technik.
Rekurrente Neuronale Netze (RNNs) sind sehr erfolgreich bei der Lösung von anspruchsvollen Problemen mit sequentiellen Daten.Allerdings ist diese beobachtete Effizienz noch nicht vollständig durch die Theorie erklärt.Es ist bekannt, dass eine bestimmte Klasse von multiplikativen RNNs genießt die Eigenschaft der Tiefe Effizienz --- eine flache Netzwerk von exponentiell großer Breite ist notwendig, um die gleiche Punktzahl Funktion zu realisieren, wie von einem solchen RNN berechnet. In dieser Arbeit versuchen wir, die Lücke zwischen Theorie und Praxis zu verkleinern, indem wir die theoretische Analyse auf RNNs ausweiten, die verschiedene Nichtlinearitäten verwenden, wie z.B. Rectified Linear Unit (ReLU), und zeigen, dass sie ebenfalls von den Eigenschaften der Universalität und Tiefeneffizienz profitieren.
Während sich tiefe neuronale Netze als leistungsfähiges Werkzeug für viele Erkennungs- und Klassifizierungsaufgaben erwiesen haben, sind ihre Stabilitätseigenschaften noch immer nicht gut verstanden.In der Vergangenheit hat sich gezeigt, dass Bildklassifizierer anfällig für sogenannte adversarische Angriffe sind, die durch additive Störung des korrekt klassifizierten Bildes erzeugt werden. In diesem Papier schlagen wir den ADef-Algorithmus vor, um eine andere Art von adversarialem Angriff zu konstruieren, der durch iterative Anwendung kleiner Deformationen auf das Bild entsteht, die durch einen Schritt des Gradientenabstiegs gefunden werden.wir demonstrieren unsere Ergebnisse auf MNIST mit Faltungsneuronalen Netzen und auf ImageNet mit Inception-v3 und ResNet-101.
Adversarial Learning-Methoden wurden für eine Vielzahl von Anwendungen vorgeschlagen, aber das Training von adversarial-Modellen kann notorisch instabil sein. Ein effektives Gleichgewicht zwischen der Leistung des Generators und des Diskriminators ist entscheidend, da ein Diskriminator, der eine sehr hohe Genauigkeit erreicht, relativ uninformative Gradienten erzeugt. In dieser Arbeit schlagen wir ein einfaches und allgemeines Verfahren vor, um den Informationsfluss im Diskriminator durch einen Informationsengpass einzuschränken, indem wir die gegenseitige Information zwischen den Beobachtungen und der internen Darstellung des Diskriminators einschränken. Wir zeigen, dass der von uns vorgeschlagene Variations-Diskriminator-Engpass (VDB) zu signifikanten Verbesserungen in drei verschiedenen Anwendungsbereichen für Algorithmen des kontradiktorischen Lernens führt: Unsere primäre Evaluierung untersucht die Anwendbarkeit des VDB auf das Imitationslernen von dynamischen, kontinuierlichen Kontrollfähigkeiten, wie z.B. Laufen. Wir zeigen, dass unsere Methode solche Fähigkeiten direkt aus rohen Videodemonstrationen erlernen kann und damit die bisherigen adversen Nachahmungslernmethoden deutlich übertrifft.Die VDB kann auch mit adversem inversem Verstärkungslernen kombiniert werden, um einfache Belohnungsfunktionen zu erlernen, die in neue Umgebungen übertragen und neu optimiert werden können.Schließlich zeigen wir, dass die VDB GANs effektiver für die Bilderzeugung trainieren kann und dabei eine Reihe von bisherigen Stabilisierungsmethoden verbessert.
Der Einsatz von maschinellen Lernsystemen in der realen Welt erfordert sowohl eine hohe Genauigkeit bei sauberen Daten als auch Robustheit gegenüber natürlich vorkommenden Fehlern.  In früheren Arbeiten wurde argumentiert, dass es einen inhärenten Kompromiss zwischen Robustheit und Genauigkeit gibt, wie z.B. durch standardmäßige Datenerweiterungstechniken wie Cutout, die die saubere Genauigkeit, aber nicht die Robustheit verbessert, und additives Gaußsches Rauschen, das die Robustheit verbessert, aber die Genauigkeit beeinträchtigt.  Modelle, die mit Patch Gaussian trainiert werden, erreichen den Stand der Technik bei den CIFAR-10 und ImageNet Common Corruptions Benchmarks, während die Genauigkeit auch bei sauberen Daten erhalten bleibt.Wir stellen fest, dass diese Augmentation zu einer geringeren Empfindlichkeit gegenüber hochfrequentem Rauschen führt (ähnlich wie Gaussian), während die Fähigkeit erhalten bleibt, relevante hochfrequente Informationen im Bild zu nutzen (ähnlich wie Cutout).Wir zeigen, dass es in Verbindung mit anderen Regularisierungsmethoden und Datenerweiterungsstrategien wie AutoAugment verwendet werden kann.  Abschließend stellen wir fest, dass die Idee, Störungen auf Patches zu beschränken, auch im Kontext des kontradiktorischen Lernens nützlich sein kann, indem sie Modelle ohne den Genauigkeitsverlust hervorbringt, der beim unkontrollierten kontradiktorischen Training auftritt.
Die Offset-Regression ist eine Standardmethode für die räumliche Lokalisierung bei vielen Sehaufgaben, einschließlich der Schätzung der menschlichen Pose, der Objekterkennung und der Segmentierung von Instanzen. Wenn jedoch eine hohe Lokalisierungsgenauigkeit für eine Aufgabe entscheidend ist, haben faltige neuronale Netze mit Offset-Regression in der Regel Schwierigkeiten, diese zu erreichen.  Ein noch grundlegenderes Problem ist die Multimodalität realer Bilder, die nicht durch ein einziges Modusmodell adäquat approximiert werden kann.  Stattdessen schlagen wir die Verwendung von Mixed-Density-Netzen (MDN) für die Offset-Regression vor, die es dem Modell ermöglichen, verschiedene Modi effizient zu verwalten und zu lernen, die volle bedingte Dichte der Ausgänge anhand der Eingaben vorherzusagen.Bei der 2D-Positionsschätzung von Menschen in freier Wildbahn, die eine genaue Lokalisierung von Körperpunkten erfordert, zeigen wir, dass dies zu einer signifikanten Verbesserung der Lokalisierungsgenauigkeit führt.Unsere Experimente zeigen insbesondere, dass die Variation des Blickpunkts der dominierende multimodale Faktor ist. Durch die sorgfältige Initialisierung der MDN-Parameter treten keine Instabilitäten beim Training auf, was bekanntermaßen ein großes Hindernis für den weitverbreiteten Einsatz von MDN darstellt. Die Methode kann problemlos auf jede Aufgabe mit einer räumlichen Regressionskomponente angewendet werden. Unsere Ergebnisse unterstreichen die multimodale Natur des realen Sehens und die Bedeutung einer expliziten Berücksichtigung von Blickpunktvariationen, zumindest wenn es um räumliche Lokalisierung geht.
Wie Sprache kann Musik als eine Folge von diskreten Symbolen dargestellt werden, die eine hierarchische Syntax bilden, wobei Noten in etwa wie Schriftzeichen und Notenmotive wie Wörter sind.  Im Gegensatz zu Text stützt sich Musik jedoch stark auf Wiederholungen auf mehreren Zeitebenen, um Struktur und Bedeutung aufzubauen. Der Music Transformer hat überzeugende Ergebnisse bei der Erzeugung von Musik mit Struktur gezeigt (Huang et al., 2018).  In diesem Beitrag stellen wir ein Werkzeug zur Visualisierung der Selbstbeobachtung bei mehrstimmiger Musik mit einer interaktiven Pianorolle vor.  Wir verwenden den Musiktransformer sowohl als deskriptives Werkzeug als auch als generatives Modell.  Im ersten Fall analysieren wir damit vorhandene Musik, um zu sehen, ob die resultierende Struktur der Selbstaufmerksamkeit mit der aus der Musiktheorie bekannten musikalischen Struktur übereinstimmt.  Außerdem vergleichen und kontrastieren wir die Aufmerksamkeitsstruktur der regulären Aufmerksamkeit mit der der relativen Aufmerksamkeit (Shaw et al., 2018, Huang et al., 2018) und untersuchen ihre Auswirkungen auf die erzeugte Musik.  Für den JSB-Choralendatensatz ist ein mit relativer Aufmerksamkeit trainiertes Modell beispielsweise konsistenter in der Beachtung aller Stimmen im vorangegangenen Zeitschritt und der Akkorde davor sowie bei Kadenzen zum Beginn einer Phrase, wodurch es einen Bogen erzeugen kann.  Wir hoffen, dass unsere Analysen weitere Beweise für die relative Selbstaufmerksamkeit als leistungsfähige induktive Verzerrung bei der Modellierung von Musik liefern werden.  Wir laden den Leser ein, unsere Videoanimationen der Musikaufmerksamkeit zu erkunden und mit den Visualisierungen unter https://storage.googleapis.com/nips-workshop-visualization/index.html zu interagieren.
Wir untersuchen die statistischen Eigenschaften des Endpunkts des stochastischen Gradientenabstiegs (SGD), indem wir SGD als stochastische Differentialgleichung (SDE) approximieren und seine Boltzmann-Gibbs-Gleichgewichtsverteilung unter der Annahme isotroper Varianz in den Verlustgradienten betrachten... Durch diese Analyse finden wir heraus, dass drei Faktoren - Lernrate, Chargengröße und die Varianz der Verlustgradienten - den Kompromiss zwischen Tiefe und Breite der von SGD gefundenen Minima steuern, wobei breitere Minima durch ein höheres Verhältnis von Lernrate zu Chargengröße begünstigt werden. In der Gleichgewichtsverteilung taucht nur das Verhältnis von Lernrate und Losgröße auf, was bedeutet, dass es bei einer gleichzeitigen Skalierung beider Größen um den gleichen Betrag unveränderlich ist. Wir zeigen experimentell, wie die Lernrate und die Chargengröße die SGD aus zwei Perspektiven beeinflussen: den Endpunkt der SGD und die Dynamik, die zu diesem Punkt führt. Was den Endpunkt betrifft, so deuten die Experimente darauf hin, dass der Endpunkt der SGD bei gleichzeitiger Skalierung der Chargengröße und der Lernrate ähnlich ist und dass ein höheres Verhältnis zu flacheren Minima führt; beide Ergebnisse stimmen mit unserer theoretischen Analyse überein. Wir stellen fest, dass die Dynamik auch bei der gleichen Skalierung von Lernrate und Chargengröße ähnlich zu sein scheint, was wir untersuchen, indem wir zeigen, dass man Chargengröße und Lernrate in einem zyklischen Lernratenplan austauschen kann. Als Nächstes veranschaulichen wir, wie sich Rauschen auf das Erinnerungsvermögen auswirkt, und zeigen, dass ein hoher Rauschpegel zu einer besseren Generalisierung führt. Schließlich stellen wir experimentell fest, dass die Ähnlichkeit bei gleichzeitiger Skalierung von Lernrate und Losgröße zusammenbricht, wenn die Lernrate zu groß oder die Losgröße zu klein wird.
Obwohl Wortanalogie-Probleme zu einem Standardwerkzeug für die Bewertung von Wortvektoren geworden sind, ist nur wenig darüber bekannt, warum Wortvektoren diese Probleme so gut lösen können. In diesem Beitrag versuche ich, unser Verständnis für dieses Thema zu vertiefen, indem ich einen einfachen, aber hochpräzisen generativen Ansatz zur Lösung des Wortanalogie-Problems für den Fall entwickle, dass alle am Problem beteiligten Begriffe Substantive sind. Meine Ergebnisse zeigen die Mehrdeutigkeiten, die mit dem Erlernen der Beziehung zwischen einem Wortpaar verbunden sind, und die Rolle des Trainingsdatensatzes bei der Bestimmung der Beziehung, die am meisten hervorgehoben wird, und sie zeigen, dass die Fähigkeit eines Modells, das Wortanalogieproblem genau zu lösen, nicht unbedingt ein Hinweis auf die Fähigkeit des Modells ist, die Beziehung zwischen einem Wortpaar so zu lernen, wie es ein Mensch tut.
Aktuelle Praktiken für die Feinabstimmung beinhalten typischerweise die Auswahl einer Ad-hoc-Wahl von Hyperparametern und deren Fixierung auf Werte, die normalerweise für das Training von Grund auf verwendet werden.Diese Arbeit untersucht mehrere gängige Praktiken für die Einstellung von Hyperparametern für die Feinabstimmung.Unsere Ergebnisse basieren auf einer umfangreichen empirischen Auswertung für die Feinabstimmung auf verschiedenen Transfer-Learning-Benchmarks.(1) Während frühere Arbeiten die Lernrate und die Batch-Größe gründlich untersucht haben, ist die Dynamik für die Feinabstimmung ein relativ unerforschter Parameter. Wir stellen fest, dass die Wahl des richtigen Wertes für das Momentum entscheidend für die Leistung des Feintunings ist und stellen eine Verbindung zu früheren theoretischen Erkenntnissen her.(2) Optimale Hyper-Parameter für das Feintuning, insbesondere die effektive Lernrate, sind nicht nur datenabhängig, sondern auch empfindlich gegenüber der Ähnlichkeit zwischen Quell- und Zieldomäne. Dies steht im Gegensatz zu den Hyperparametern für das Training von Grund auf.(3) Die referenzbasierte Regularisierung, die die Modelle nahe am Ausgangsmodell hält, ist nicht notwendigerweise auf "unähnliche" Datensätze anwendbar.(4) Unsere Ergebnisse stellen gängige Praktiken der Feinabstimmung in Frage und ermutigen Deep-Learning-Praktiker, die Hyperparameter für die Feinabstimmung zu überdenken.
Transfer-Lernen durch Feinabstimmung eines vortrainierten neuronalen Netzes mit einem extrem großen Datensatz, wie ImageNet, kann das Training erheblich beschleunigen, während die Genauigkeit häufig durch die begrenzte Datensatzgröße der neuen Zielaufgabe eingeschränkt wird.Um das Problem zu lösen, wurden einige Regularisierungsmethoden untersucht, die die Gewichte der äußeren Schicht des Zielnetzes mit dem Ausgangspunkt als Referenzen (SPAR) einschränken.In diesem Papier schlagen wir ein neuartiges regularisiertes Transfer-Lernen-Framework DELTA vor, nämlich DEep Learning Transfer using Feature Map with Attention. Anstatt die Gewichte des neuronalen Netzes zu beschränken, zielt DELTA darauf ab, die Ausgänge der äußeren Schicht des Zielnetzes zu erhalten, und zwar zusätzlich zur Minimierung des empirischen Verlustes, um die Ausgänge der äußeren Schicht von zwei Netzen durch die Beschränkung einer Teilmenge von Merkmalskarten auszurichten, die genau durch die Aufmerksamkeit ausgewählt werden, die in einer überwachten Lernweise gelernt wurde.Wir bewerten DELTA mit dem Stand der Technik Algorithmen, einschließlich L2 und L2-SP.Die experimentellen Ergebnisse zeigen, dass unsere vorgeschlagene Methode diese Grundlinien mit höherer Genauigkeit für neue Aufgaben übertrifft.
In einigen Bereichen sind automatisierte Vorhersagen ohne Begründungen nur begrenzt anwendbar.Kürzlich wurden Fortschritte bei der einaspektigen Stimmungsanalyse für Rezensionen erzielt, bei denen die Mehrdeutigkeit einer Begründung minimal ist.In diesem Zusammenhang besteht eine Begründung oder Maske aus (langen) Wortfolgen aus dem Eingabetext, die für die Vorhersage ausreichen. In unserer Arbeit schlagen wir ein neuronales Modell für die Vorhersage von mehraspektigen Stimmungen für Rezensionen vor und generieren gleichzeitig eine probabilistische mehrdimensionale Maske (eine pro Aspekt) in einer unbeaufsichtigten und Multitasking-Lernweise. unsere Evaluierung zeigt, dass unser Modell auf drei Datensätzen im Bier- und Hotelbereich starke Grundlinien übertrifft und Masken generiert, die starke Merkmalsprädiktoren, sinnvoll und interpretierbar sind.
Die Erzeugung neuronaler Sequenzen wird üblicherweise mit Hilfe von Maximum-Likelihood-Schätzungen (ML) oder Reinforcement Learning (RL) angegangen, die jedoch bekanntermaßen ihre eigenen Schwächen haben: ML weist eine Diskrepanz zwischen Training und Test auf, während RL unter der Ineffizienz von Stichproben leidet. Um diesen Problemen entgegenzuwirken, schlagen wir eine Zielfunktion für die Sequenzgenerierung unter Verwendung der α-Divergenz vor, die zu einer integrierten ML-RL-Methode führt, die die besseren Teile von ML und RL nutzt. Wir zeigen, dass die vorgeschlagene Zielfunktion ML- und RL-Zielfunktionen verallgemeinert, da sie beide als Spezialfälle einschließt (ML entspricht α → 0 und RL α → 1) und dass die Differenz zwischen der RL-Zielfunktion und der vorgeschlagenen Zielfunktion mit zunehmendem α monoton abnimmt.
Capsule Networks haben vielversprechende Ergebnisse in Benchmark-Computer-Vision-Datensätzen wie MNIST, CIFAR und smallNORB gezeigt, aber sie müssen noch an Aufgaben getestet werden, bei denen (1) die erkannten Entitäten von Natur aus komplexere interne Repräsentationen haben und (2) es nur sehr wenige Instanzen pro Klasse gibt, aus denen gelernt werden kann, und (3) bei denen eine punktuelle Klassifizierung nicht geeignet ist. In diesem Papier werden daher Experimente zur Gesichtsverifikation in kontrollierten und unkontrollierten Umgebungen durchgeführt, die sich mit diesen Punkten befassen, wobei wir \textit{Siamese Capsule Networks} einführen, eine neue Variante, die für paarweise Lernaufgaben verwendet werden kann.  Wir stellen fest, dass \textit{Siamese Capsule Networks} in beiden paarweisen Lerndatensätzen eine gute Leistung im Vergleich zu starken Baselines erbringt, wenn es mit einem kontrastiven Verlust mit $\ell_2$-normalisierten, kapselkodierten Pose-Merkmalen trainiert wird, wobei die besten Ergebnisse in der few-shot-Lernumgebung erzielt werden, in der die Bildpaare im Testsatz ungesehene Personen enthalten.
In dieser Arbeit stellen wir eine neue Agentenarchitektur namens Reactor vor, die mehrere algorithmische und architektonische Beiträge kombiniert, um einen Agenten mit höherer Stichproben-Effizienz als Prioritized Dueling DQN (Wang et al., 2016) und Categorical DQN (Bellemare et al., 2017) zu erzeugen und gleichzeitig eine bessere Laufzeitleistung als A3C (Mnih et al., Unser erster Beitrag ist ein neuer Algorithmus zur Policy-Evaluierung namens Distributional Retrace, der mehrstufige Off-Policy-Updates in die Distributional Reinforcement Learning-Umgebung einbringt, wobei derselbe Ansatz verwendet werden kann, um mehrere Klassen von mehrstufigen Policy-Evaluierungsalgorithmen, die für die Erwartungswert-Evaluierung entwickelt wurden, in Distributional-Algorithmen umzuwandeln. Unser letzter algorithmischer Beitrag ist ein neuer priorisierter Wiederholungsalgorithmus für Sequenzen, der die zeitliche Lokalität benachbarter Beobachtungen für eine effizientere Priorisierung der Wiedergabe ausnutzt. anhand der Atari 2600-Benchmarks zeigen wir, dass jede dieser Innovationen sowohl zur Effizienz der Stichprobe als auch zur endgültigen Leistung des Agenten beiträgt. schließlich zeigen wir, dass Reactor nach 200 Millionen Frames und weniger als einem Tag Training die beste Leistung erreicht.
Hierarchische Planung, insbesondere hierarchische Aufgabennetzwerke, wurde als Methode zur Beschreibung von Plänen durch Zerlegung von Aufgaben in Teilaufgaben vorgeschlagen, bis primitive Aufgaben, Aktionen, erhalten werden.Die Planverifizierung nimmt einen vollständigen Plan als Eingabe an, und das Ziel ist es, eine Aufgabe zu finden, die zu diesem Plan zerlegt wird.Bei der Planerkennung ist ein Präfix des Plans gegeben, und das Ziel ist es, eine Aufgabe zu finden, die zu dem (kürzesten) Plan mit dem gegebenen Präfix zerlegt wird.In diesem Papier wird beschrieben, wie man Pläne verifiziert und erkennt, indem man eine gemeinsame Methode verwendet, die aus formalen Grammatiken bekannt ist, nämlich das Parsing.
Die Suche nach neuronalen Architekturen (NAS), die Aufgabe, neuronale Architekturen automatisch zu finden, hat sich in letzter Zeit als vielversprechender Ansatz erwiesen, um bessere Modelle als die von Menschen entworfenen zu finden. Die meisten Erfolgsgeschichten beziehen sich jedoch auf Bildverarbeitungsaufgaben und waren für Text recht begrenzt, mit Ausnahme einer kleinen Sprachmodellierungseinrichtung. Ausgehend von einem Standard-Sequenz-zu-Sequenz-Modell für die Übersetzung führen wir eine umfangreiche Suche über die rekurrenten Zellen und Aufmerksamkeitsähnlichkeitsfunktionen für zwei Übersetzungsaufgaben durch, IWSLT Englisch-Vietnamesisch und WMT Deutsch-Englisch.Wir berichten über die Herausforderungen bei der Durchführung von Zellsuchen und zeigen erste Erfolge bei der Aufmerksamkeitssuche mit Übersetzungsverbesserungen im Vergleich zu starken Basislinien.
Autoencoder bieten einen leistungsfähigen Rahmen für das Lernen komprimierter Repräsentationen, indem sie alle Informationen kodieren, die zur Rekonstruktion eines Datenpunktes in einem latenten Code benötigt werden: Durch die Dekodierung der konvexen Kombination der latenten Codes für zwei Datenpunkte kann der Autoencoder eine Ausgabe erzeugen, die semantisch Merkmale aus den Datenpunkten mischt.In diesem Papier schlagen wir ein Regularisierungsverfahren vor, das interpolierte Ausgaben ermutigt, realistischer zu erscheinen, indem es ein kritisches Netzwerk täuscht, das trainiert wurde, den Mischkoeffizienten aus interpolierten Daten wiederherzustellen. Wir entwickeln dann eine einfache Benchmark-Aufgabe, bei der wir quantitativ messen können, inwieweit verschiedene Auto-Encoder interpolieren können, und zeigen, dass unser Regularisierer die Interpolation in dieser Umgebung dramatisch verbessert. Wir zeigen auch empirisch, dass unser Regularisierer latente Codes erzeugt, die bei nachgelagerten Aufgaben effektiver sind, was auf eine mögliche Verbindung zwischen Interpolationsfähigkeiten und dem Lernen nützlicher Repräsentationen hindeutet.
Wir betrachten das Problem der Generierung plausibler und vielfältiger Videosequenzen, wenn wir nur ein Start- und ein Endbild haben. Diese Aufgabe ist auch als Inbetweening bekannt und gehört zum breiteren Bereich der stochastischen Videogenerierung, die im Allgemeinen mit Hilfe rekurrenter neuronaler Netze (RNN) angegangen wird.In diesem Papier schlagen wir stattdessen ein voll gefaltetes Modell vor, um Videosequenzen direkt im Pixelbereich zu generieren.Wir erhalten zunächst eine latente Videorepräsentation mit Hilfe eines stochastischen Fusionsmechanismus, der lernt, wie man Informationen aus dem Start- und Endbild einbezieht. Unser Modell lernt, eine solche latente Repräsentation zu erzeugen, indem es die zeitliche Auflösung schrittweise erhöht, und dekodiert dann in der räumlich-zeitlichen Domäne unter Verwendung von 3D-Faltungen.Das Modell wird Ende-zu-Ende trainiert, indem es einen Verlust minimiert.Experimente an mehreren weit verbreiteten Benchmark-Datensätzen zeigen, dass es in der Lage ist, aussagekräftige und vielfältige Zwischensequenzen zu erzeugen, sowohl nach quantitativen als auch qualitativen Bewertungen.
Die Ausrichtung von Wissensgraphen aus verschiedenen Quellen oder Sprachen, die darauf abzielt, sowohl die Entität als auch die Beziehung auszurichten, ist für eine Vielzahl von Anwendungen wie Wissensgraphen-Konstruktion und Fragenbeantwortung von entscheidender Bedeutung.Bestehende Methoden der Wissensgraphen-Ausrichtung verlassen sich in der Regel auf eine große Anzahl von ausgerichteten Wissens-Tripletts, um effektive Modelle zu trainieren.Diese ausgerichteten Tripletts sind jedoch möglicherweise nicht verfügbar oder für viele Domänen teuer zu erhalten.Daher untersuchen wir in diesem Papier, wie vollständig unbeaufsichtigte Methoden oder schwach überwachte Methoden entwickelt werden können, d.h., Wir schlagen einen unbeaufsichtigten Rahmen vor, der auf adversarialem Training basiert und in der Lage ist, die Entitäten und Beziehungen in einem Quell-Wissensgraphen auf diejenigen in einem Ziel-Wissensgraphen abzubilden.Dieser Rahmen kann nahtlos in bestehende überwachte Methoden integriert werden, bei denen nur eine begrenzte Anzahl von ausgerichteten Tripletts als Anleitung verwendet wird.Experimente an realen Datensätzen beweisen die Effektivität des von uns vorgeschlagenen Ansatzes sowohl in der schwach überwachten als auch in der unbeaufsichtigten Umgebung.
Die Verteilungshypothese bietet eine weitere Form der nützlichen Selbstüberwachung von benachbarten Sätzen, die in großen, unmarkierten Korpora reichlich vorhanden sind. Motiviert durch die Asymmetrie in den beiden Hemisphären des menschlichen Gehirns sowie die Beobachtung, dass verschiedene Lernarchitekturen dazu neigen, verschiedene Aspekte der Satzbedeutung zu betonen, stellen wir zwei Multiview-Frameworks für das Lernen von Satzrepräsentationen in einer unbeaufsichtigten Weise vor. In beiden Systemen ist die endgültige Repräsentation ein Ensemble aus zwei Ansichten, wobei eine Ansicht den Eingabesatz mit einem rekurrenten neuronalen Netz (RNN) und die andere Ansicht ihn mit einem einfachen linearen Modell kodiert.wir zeigen, dass nach dem Lernen die Vektoren, die von unseren Multiview-Frameworks erzeugt werden, verbesserte Repräsentationen gegenüber ihren Einzelansichten liefern, und dass die Kombination verschiedener Ansichten eine Verbesserung der Repräsentation gegenüber jeder einzelnen Ansicht darstellt und eine solide Übertragbarkeit auf nachgelagerte Standardaufgaben aufweist.
Es gibt unzählige Arten der Segmentierung, und letztlich liegt die "richtige" Segmentierung einer gegebenen Szene im Auge des Beschreibers. Standardansätze erfordern große Mengen an beschrifteten Daten, um nur eine bestimmte Art der Segmentierung zu lernen. Als ersten Schritt zur Entlastung dieser Annotation Last, schlagen wir das Problem der geführten Segmentierung: gegeben unterschiedliche Mengen von Pixel-wise Etiketten, Segment unannotierte Pixel durch die Ausbreitung Aufsicht lokal (innerhalb eines Bildes) und nicht-örtlich (über Bilder).Wir schlagen geführte Netzwerke, die eine latente Aufgabe Darstellung---Anleitung---variablen Mengen und Klassen (Kategorien, Instanzen, etc.) von Pixel Überwachung und optimieren unsere Architektur. Um den Bereich des Lernens mit wenigen und vielen Aufnahmen zu überspannen, untersuchen wir die Führung von nur einem Pixel pro Konzept bis hin zu 1000+ Bildern und vergleichen sie mit einer vollständigen Gradientenoptimierung an beiden Extremen. Um die Verallgemeinerung zu erforschen, analysieren wir die Anleitung als Brücke zwischen verschiedenen Ebenen der Überwachung, um Klassen als die Vereinigung von Instanzen zu segmentieren.
In dieser Arbeit führen wir eine neue Form der latenten Optimierung ein, die vom CS-GAN inspiriert ist, und zeigen, dass sie die Dynamik der Gegenspieler verbessert, indem sie die Interaktionen zwischen dem Diskriminator und dem Generator verbessert.Wir entwickeln eine unterstützende theoretische Analyse aus der Perspektive differenzierbarer Spiele und stochastischer Approximation. Unsere Experimente zeigen, dass die latente Optimierung das GAN-Training signifikant verbessern kann, indem sie die beste Leistung für den ImageNet-Datensatz (128 x 128) erzielt. Unser Modell erreicht einen Inception Score (IS) von 148 und eine Frechet Inception Distance (FID) von 3,4, was einer Verbesserung von 17 % bzw. 32 % bei IS und FID im Vergleich zum BigGAN-Basismodell mit derselben Architektur und Anzahl von Parametern entspricht.
In diesem Beitrag untersuchen wir das Problem der Optimierung eines zweischichtigen künstlichen neuronalen Netzes, das am besten zu einem Trainingsdatensatz passt, wenn die Anzahl der Parameter größer ist als die Anzahl der abgetasteten Punkte. Wir zeigen, dass für eine breite Klasse differenzierbarer Aktivierungsfunktionen (diese Klasse umfasst die meisten nichtlinearen Funktionen und schließt stückweise lineare Funktionen aus), wir haben, dass beliebige optimale Lösungen erster Ordnung globale Optimalität erfüllen, vorausgesetzt, die versteckte Schicht ist nicht-singulär. Wir zeigen im Wesentlichen, dass diese nicht-singuläre Matrix der versteckten Schicht eine "gute" Eigenschaft für diese große Klasse von Aktivierungsfunktionen erfüllt. Die Techniken, die mit dem Nachweis dieses Ergebnisses verbunden sind, inspirieren uns zu einem neuen Algorithmus, bei dem wir zwischen zwei Gradientenschritten der versteckten Schicht einen Schritt des stochastischen Gradientenabstiegs (SGD) der Ausgangsschicht hinzufügen. In diesem neuen algorithmischen Rahmen erweitern wir unser früheres Ergebnis und zeigen, dass die verborgene Schicht für alle endlichen Iterationen die bereits erwähnte "gute" Eigenschaft erfüllt, was den Erfolg der verrauschten Gradientenmethoden teilweise erklärt und das Problem der Datenunabhängigkeit unseres früheren Ergebnisses löst.Beide Ergebnisse lassen sich leicht auf verborgene Schichten erweitern, die durch eine flache Matrix aus einer quadratischen Matrix gegeben sind. Die Ergebnisse sind auch dann anwendbar, wenn das Netz mehr als eine versteckte Schicht hat, vorausgesetzt, dass alle inneren versteckten Schichten beliebig sind, die Nicht-Singularität erfüllen, alle Aktivierungen aus der gegebenen Klasse differenzierbarer Funktionen stammen und die Optimierung nur in Bezug auf die äußerste versteckte Schicht erfolgt, Wir nutzen die Glätteeigenschaften, um eine asymptotische Konvergenz von $O(1/\text{Anzahl der Iterationen})$ zu einer optimalen Lösung erster Ordnung zu garantieren.
Wir führen das Konzept der Kanalaggregation in der ConvNet-Architektur ein, eine neuartige kompakte Darstellung von CNN-Merkmalen, die für die explizite Modellierung der nichtlinearen Kanalcodierung nützlich ist, insbesondere wenn die neue Einheit in tiefe Architekturen zur Handlungserkennung eingebettet ist.Die Kanalaggregation basiert auf den Mehrkanal-Merkmalen von ConvNet und zielt darauf ab, den optischen Konvergenzpfad mit hoher Geschwindigkeit zu finden.Wir nennen unsere vorgeschlagene Faltungsarchitektur "nonlinear channels aggregation networks (NCAN)" und ihre neue Schicht "nonlinear channels aggregation layer (NCAL)". Ein weiterer Beitrag dieser Arbeit ist eine effiziente und effektive Implementierung der NCAL, die eine Beschleunigung um Größenordnungen ermöglicht. Wir bewerten die Leistung der NCAL anhand der Standard-Benchmarks UCF101 und HMDB51, und die experimentellen Ergebnisse zeigen, dass diese Formulierung nicht nur eine schnelle Konvergenz, sondern auch eine stärkere Generalisierungsfähigkeit ohne Leistungseinbußen bietet.
Im Gegensatz zu früheren Methoden, die transferbasierte und scorerbasierte Methoden unter Verwendung des Gradienten oder der Initialisierung eines Surrogat-White-Box-Modells kombinierten, versucht diese neue Methode, eine niedrigdimensionale Einbettung unter Verwendung eines vortrainierten Modells zu erlernen, und führt dann eine effiziente Suche innerhalb des Einbettungsraums durch, um ein unbekanntes Zielnetzwerk anzugreifen.die Methode erzeugt negative Störungen mit hochrangigen semantischen Mustern, die leicht übertragbar sind. Wir zeigen, dass dieser Ansatz die Abfrageeffizienz von Black-Box-Angriffen auf verschiedene Zielnetzwerkarchitekturen erheblich verbessern kann. Wir evaluieren unseren Ansatz auf MNIST, ImageNet und Google Cloud Vision API, was zu einer signifikanten Verringerung der Anzahl der Abfragen führt. Wir greifen auch gegnerisch verteidigte Netzwerke auf CIFAR10 und ImageNet an, wo unsere Methode nicht nur die Anzahl der Abfragen verringert, sondern auch die Erfolgsrate der Angriffe verbessert.
Tiefe neuronale Netze (DNNs) sind vom menschlichen Gehirn inspiriert, und die Verbindung zwischen den beiden wurde in der Literatur umfassend untersucht.  Frühere Arbeiten haben gezeigt, dass DNNs, die durch den Abgleich der neuronalen Antworten aus dem inferioren temporalen (IT) Kortex des Affengehirns trainiert wurden, in der Lage sind, bei der Erkennung von Bildern und Objekten eine Leistung auf menschlichem Niveau zu erzielen, was darauf hindeutet, dass die neuronale Dynamik informatives Wissen liefern kann, das DNNs bei der Bewältigung bestimmter Aufgaben hilft. In diesem Beitrag stellen wir das Konzept einer Neuro-AI-Schnittstelle vor, die darauf abzielt, die neuronalen Reaktionen des Menschen als überwachte Informationen zu nutzen, um KI-Systemen bei der Lösung einer Aufgabe zu helfen, die bei der Verwendung herkömmlicher maschineller Lernstrategien schwierig ist.   
Während die jüngsten Entwicklungen in der Technologie für autonome Fahrzeuge (AV) einen beträchtlichen Fortschritt darstellen, fehlt es an Werkzeugen für rigorose und skalierbare Tests: Tests in der realen Welt, der De-facto-Evaluierungsumgebung, gefährden die Öffentlichkeit und erfordern aufgrund der Seltenheit von Unfällen Milliarden von Kilometern, um die Leistungsansprüche statistisch zu validieren. Wir implementieren einen Simulationsrahmen, mit dem ein komplettes modernes autonomes Fahrsystem getestet werden kann, insbesondere Systeme, die Deep-Learning-Wahrnehmungs- und Steuerungsalgorithmen verwenden. Mit Hilfe von adaptiven Sampling-Methoden zur Beschleunigung der Bewertung der Wahrscheinlichkeit seltener Ereignisse schätzen wir die Wahrscheinlichkeit eines Unfalls unter einer Basisverteilung, die das Standardverkehrsverhalten steuert.
Viele Aufgaben im Bereich des Verstehens natürlicher Sprache erfordern das Erlernen von Beziehungen zwischen zwei Sequenzen für verschiedene Aufgaben, wie z.B. Inferenz, Paraphrasierung und Entailment.Diese oben genannten Aufgaben sind von Natur aus ähnlich, werden aber oft einzeln modelliert.Wissenstransfer kann für eng verwandte Aufgaben effektiv sein, was üblicherweise durch Parametertransfer in neuronalen Netzen durchgeführt wird.Allerdings kann der Transfer aller Parameter, von denen einige für eine Zielaufgabe irrelevant sind, zu suboptimalen Ergebnissen führen und sich negativ auf die Leistung auswirken, was als \textit{negative} Transfer bezeichnet wird. Daher konzentriert sich dieses Papier auf die Übertragbarkeit sowohl von Instanzen als auch von Parametern über Aufgaben zum Verstehen natürlicher Sprache hinweg, indem es eine Ensemble-basierte Transfer-Lernmethode im Kontext des "few-shot"-Lernens vorschlägt. Unser Hauptbeitrag ist eine Methode zur Abschwächung des negativen Transfers über Aufgaben hinweg bei der Verwendung neuronaler Netze, die ein dynamisches Bagging kleiner rekurrenter neuronaler Netze beinhaltet, die auf verschiedenen Teilmengen der Ausgangsaufgabe(n) trainiert wurden. Wir stellen einen einfachen, aber neuartigen Ansatz vor, um diese Netze in eine Zielaufgabe für das Lernen mit wenigen Beispielen einzubinden, indem wir einen abklingenden Parameter verwenden, der entsprechend den Steigungsänderungen einer geglätteten Spline-Fehlerkurve in Teilintervallen während des Trainings ausgewählt wird.Unsere vorgeschlagene Methode zeigt Verbesserungen gegenüber harten und weichen Parameter-Sharing-Transfer-Methoden im Fall des Lernens mit wenigen Beispielen und zeigt eine konkurrenzfähige Leistung im Vergleich zu Modellen, die unter voller Aufsicht auf der Zielaufgabe aus nur wenigen Beispielen trainiert werden.
Viele klinische Metriken können entweder aufgrund ihrer Kosten (z. B. MRT, Ganganalyse) oder weil sie für den Patienten unpraktisch oder schädlich sind (z. B. Biopsie, Röntgen) nicht häufig erhoben werden. Um in solchen Szenarien individuelle Trajektorien des Krankheitsverlaufs zu schätzen, ist es von Vorteil, Ähnlichkeiten zwischen Patienten, d. h. die Kovarianz der Trajektorien, zu nutzen und eine latente Darstellung des Verlaufs zu finden. In dieser Studie entwickeln wir ein maschinelles Lernverfahren namens Coordinatewise-Soft-Impute (CSI) für die Analyse des Krankheitsverlaufs aus spärlichen Beobachtungen bei Vorhandensein von Störereignissen. CSI konvergiert garantiert zum globalen Minimum des entsprechenden Optimierungsproblems.
Ein wichtiger Indikator für die Generalisierung dieser Systeme ist die Qualität der Zero-Shot-Übersetzung, d.h. die Übersetzung zwischen Sprachpaaren, die das System während des Trainings noch nie gesehen hat.Bisher blieb die Zero-Shot-Leistung mehrsprachiger Modelle jedoch weit hinter der Qualität zurück, die mit einem zweistufigen Übersetzungsprozess erreicht werden kann, bei dem eine Zwischensprache (normalerweise Englisch) verwendet wird. Wir schlagen explizite Sprachinvarianzverluste vor, die einen NMT-Encoder zum Erlernen sprachunabhängiger Repräsentationen anleiten. Die von uns vorgeschlagenen Strategien verbessern die Zero-Shot-Übersetzungsleistung beim WMT Englisch-Französisch-Deutsch und bei der gemeinsamen Aufgabe IWSLT 2017 signifikant und erreichen zum ersten Mal die Leistung von Pivot-Ansätzen, während die Leistung in überwachten Richtungen erhalten bleibt.
Wir beweisen die genaue Skalierung bei endlicher Tiefe und Breite für den Mittelwert und die Varianz des neuronalen Tangentenkerns (NTK) in einem zufällig initialisierten ReLU-Netzwerk: Die Standardabweichung ist exponentiell zum Verhältnis von Netzwerktiefe und -breite, so dass der NTK selbst im Grenzfall einer unendlichen Überparametrisierung nicht deterministisch ist, wenn Tiefe und Breite gleichzeitig gegen unendlich tendieren. Darüber hinaus beweisen wir, dass für solche tiefen und breiten Netze die NTK eine nicht-triviale Entwicklung während des Trainings hat, indem wir zeigen, dass der Mittelwert ihrer ersten SGD-Aktualisierung ebenfalls exponentiell im Verhältnis von Netzwerktiefe und -breite ist, was in scharfem Kontrast zu dem Regime steht, in dem die Tiefe fixiert und die Netzbreite sehr groß ist.Unsere Ergebnisse deuten darauf hin, dass im Gegensatz zu relativ flachen und breiten Netzen tiefe und breite ReLU-Netze in der Lage sind, datenabhängige Merkmale auch im sogenannten Lazy-Training-Regime zu lernen.
Die meisten Algorithmen zum Repräsentationslernen und zur Linkvorhersage in relationalen Daten wurden für statische Daten entwickelt, die sich aber in der Regel im Laufe der Zeit verändern, wie z.B. Freundesgraphen in sozialen Netzwerken oder Benutzerinteraktionen mit Artikeln in Empfehlungssystemen.Dies gilt auch für Wissensdatenbanken, die Fakten wie (US, hat Präsident, B. Obama, [2009-2017]) enthalten, die nur zu bestimmten Zeitpunkten gültig sind.Für das Problem der Linkvorhersage unter zeitlichen Einschränkungen, d.h., Für das Problem der Linkvorhersage unter zeitlichen Beschränkungen, d.h. der Beantwortung von Anfragen der Form (US, has president, ?, 2012), schlagen wir eine Lösung vor, die von der kanonischen Zerlegung von Tensoren der Ordnung 4 inspiriert ist, führen neue Regularisierungsschemata ein und stellen eine Erweiterung von ComplEx vor, die eine State-of-the-Art-Performance erreicht.Zusätzlich schlagen wir einen neuen Datensatz für die Vervollständigung von Wissensdatenbanken vor, der aus Wikidata konstruiert wurde und um eine Größenordnung größer ist als frühere Benchmarks.
Der konventionelle Ansatz zur Lösung des Empfehlungsproblems ordnet die einzelnen Dokumentkandidaten gierig nach ihren Vorhersagewerten ein. Diese Methode optimiert jedoch nicht den gesamten Slate und kämpft daher oft damit, Verzerrungen zu erfassen, die durch das Seitenlayout und die Interdependenzen zwischen den Dokumenten verursacht werden.Das Slate-Empfehlungsproblem zielt darauf ab, direkt die optimal geordnete Teilmenge von Dokumenten (d.h. Slates) zu finden, die den Interessen der Benutzer am besten entsprechen.Die Lösung dieses Problems ist aufgrund der kombinatorischen Explosion der Dokumentkandidaten und ihrer Anzeigepositionen auf der Seite schwierig. Daher schlagen wir einen Paradigmenwechsel von der traditionellen Sichtweise der Lösung eines Ranking-Problems zu einem direkten Slate-Generierungsrahmen vor.In diesem Papier führen wir List Conditional Variational Auto-Encoder (ListCVAE) ein, die die gemeinsame Verteilung von Dokumenten auf dem Slate in Abhängigkeit von Benutzerantworten erlernen und direkt vollständige Slates generieren.Experimente mit simulierten und realen Daten zeigen, dass List-CVAE gierige Ranking-Methoden auf verschiedenen Skalen von Dokumentenkorpora konsistent übertrifft.
Neuronale Netze für strukturierte Daten wie Graphen sind in den letzten Jahren ausgiebig untersucht worden.Bis heute hat sich der Großteil der Forschungstätigkeit hauptsächlich auf statische Graphen konzentriert.Die meisten realen Netzwerke sind jedoch dynamisch, da sich ihre Topologie im Laufe der Zeit verändert.Die Vorhersage der Entwicklung dynamischer Graphen ist eine Aufgabe von großer Bedeutung im Bereich des Graph Mining.Trotz ihrer praktischen Bedeutung ist die Aufgabe bisher nicht eingehend untersucht worden, vor allem aufgrund ihrer anspruchsvollen Natur.In diesem Papier schlagen wir ein Modell vor, das die Entwicklung dynamischer Graphen vorhersagt. Wir verwenden ein neuronales Graphen-Netzwerk mit einer rekurrenten Architektur, um die zeitlichen Entwicklungsmuster dynamischer Graphen zu erfassen, und setzen dann ein generatives Modell ein, das die Topologie des Graphen im nächsten Zeitschritt vorhersagt und eine Grapheninstanz konstruiert, die dieser Topologie entspricht.Wir evaluieren das vorgeschlagene Modell an mehreren künstlichen Datensätzen, die der üblichen Netzwerkentwicklungsdynamik folgen, sowie an realen Datensätzen.Die Ergebnisse zeigen die Wirksamkeit des vorgeschlagenen Modells.
Die Ergebnisse zeigen die Effektivität des vorgeschlagenen Modells. Bei der wissensbasierten Beantwortung von Fragen besteht ein grundlegendes Problem darin, die Annahme von beantwortbaren Fragen von einfachen Fragen auf zusammengesetzte Fragen zu erweitern. Unser Modell besteht aus zwei Teilen:(i) einem neuartigen "Learning-to-Decompose"-Agenten, der eine Strategie lernt, um eine zusammengesetzte Frage in einfache Fragen zu zerlegen, und(ii) drei unabhängigen "Simple-Question"-Beantwortern, die die entsprechenden Beziehungen für jede einfache Frage klassifizieren.Experimente zeigen, dass unser Modell komplexe Regeln der Kompositionalität als stochastische Strategie lernt, die einfache neuronale Netze nutzen, um State-of-the-Art-Ergebnisse auf WebQuestions und MetaQA zu erzielen.Wir analysieren den interpretierbaren Zerlegungsprozess sowie die generierten Partitionen.
Auf Energie basierende Modelle geben unmormalisierte log-Wahrscheinlichkeitswerte für Datenproben aus.  Eine solche Schätzung ist für eine Vielzahl von Anwendungsproblemen, wie z.B. Stichprobengenerierung, Entrauschung, Wiederherstellung von Stichproben, Ausreißererkennung, Bayes'sches Reasoning u.v.m., unerlässlich.  Das standardmäßige Maximum-Likelihood-Training ist jedoch aufgrund der Notwendigkeit, die Modellverteilung zu stichprobenartig zu erfassen, sehr rechenaufwändig. Das Score-Matching kann dieses Problem lindern, und das Denoising-Score-Matching (Vincent, 2011) ist eine besonders praktische Variante.  Allerdings sind frühere Versuche gescheitert, Modelle zu erstellen, die eine hochwertige Stichprobensynthese ermöglichen.  Wir glauben, dass dies daran liegt, dass sie das Denoising-Score-Matching nur auf einer einzigen Rauschskala durchgeführt haben.Um diese Einschränkung zu überwinden, lernen wir hier stattdessen eine Energiefunktion auf allen Rauschskalen.   Bei der Abtastung mit der Annealed Langevin-Dynamik und dem einstufigen Entrauschungssprung produzierte unser Modell qualitativ hochwertige Abtastungen, die mit modernsten Techniken wie GANs vergleichbar sind, und wies den Testdaten eine Wahrscheinlichkeit zu, die mit früheren Likelihood-Modellen vergleichbar war.  Unser Modell hat einen neuen Maßstab für die Qualität von Stichproben in Likelihood-basierten Modellen gesetzt.  Darüber hinaus zeigen wir, dass unser Modell die Verteilung der Proben erlernt und sich gut auf eine Bildübermalungsaufgabe verallgemeinern lässt.
Eine eingeschränkte Boltzmann-Maschine (RBM) lernt eine probabilistische Verteilung über ihre Eingabeproben und hat zahlreiche Anwendungen wie Dimensionalitätsreduktion, Klassifizierung und generative Modellierung.Konventionelle RBMs akzeptieren vektorisierte Daten, die potenziell wichtige strukturelle Informationen in der ursprünglichen Tensor (multi-way) input.Matrix-variate und Tensor-variate RBMs, genannt MvRBM und TvRBM, wurden vorgeschlagen, aber sind alle restriktiv durch Konstruktion. In dieser Arbeit wird die Matrix-Produkt-Operator-RBM (MPORBM) vorgestellt, die eine Tensornetzwerk-Verallgemeinerung der Mv/TvRBM nutzt, Eingabeformate sowohl in der sichtbaren als auch in der verborgenen Schicht beibehält und zu einer höheren Ausdruckskraft führt.Ein neuartiger Trainingsalgorithmus, der kontrastive Divergenz und ein alternierendes Optimierungsverfahren integriert, wird ebenfalls entwickelt.
Autonomes Fahren wird immer noch als ein "ungelöstes Problem" betrachtet, da es eine große Variabilität aufweist und viele Prozesse, die mit seiner Entwicklung verbunden sind, wie Fahrzeugkontrolle und Szenenerkennung, offene Fragen bleiben.Obwohl Algorithmen des Verstärkungslernens bemerkenswerte Ergebnisse in Spielen und einigen Robotermanipulationen erzielt haben, wurde diese Technik nicht auf die anspruchsvolleren Anwendungen in der realen Welt wie autonomes Fahren ausgeweitet. In dieser Arbeit schlagen wir einen Deep Reinforcement Learning (RL)-Algorithmus vor, der eine kritische Architektur mit mehrstufigen Rückgaben einbettet, um eine bessere Robustheit der Lernstrategien des Agenten zu erreichen, wenn er in komplexen und instabilen Umgebungen agiert.Das Experiment wird mit dem Carla-Simulator durchgeführt, der anpassbare und realistische Bedingungen für das Fahren in der Stadt bietet.Das entwickelte Deep Actor RL, das von einem Policy-Evaluator-Kritiker geleitet wird, übertrifft die Leistung eines Standard Deep RL-Agenten deutlich.
Eine grundlegende und noch weitgehend unbeantwortete Frage im Zusammenhang mit Generative Adversarial Networks (GANs) ist, ob GANs tatsächlich in der Lage sind, die wichtigsten Eigenschaften der Datensätze, auf denen sie trainiert werden, zu erfassen.Die aktuellen Ansätze zur Untersuchung dieser Frage erfordern erhebliche menschliche Aufsicht, wie z. B. visuelle Inspektion von gesampelten Bildern, und bieten oft nur recht begrenzte Skalierbarkeit.In diesem Papier schlagen wir neue Techniken, die Klassifizierung-basierte Perspektive zu verwenden, um synthetische GAN-Verteilungen und ihre Fähigkeit, genau zu reflektieren die wesentlichen Eigenschaften der Trainingsdaten zu bewerten. Diese Techniken erfordern nur minimale menschliche Aufsicht und kann leicht skaliert und angepasst werden, um eine Vielzahl von State-of-the-Art-GANs auf große, populäre Datensätze zu bewerten.Sie zeigen auch, dass GANs haben erhebliche Probleme bei der Reproduktion der mehr distributional Eigenschaften der Ausbildung dataset.in insbesondere die Vielfalt der synthetischen Daten ist Größenordnungen kleiner als die der ursprünglichen Daten.
Das Ziel des Survival Clustering ist die Zuordnung von Subjekten (z. B., Bestehende Überlebensmethoden setzen das Vorhandensein von eindeutigen Signalen für das Lebensende voraus oder führen sie künstlich ein, indem sie eine vordefinierte Zeitspanne verwenden. In dieser Arbeit verzichten wir auf diese Annahme und führen eine Verlustfunktion ein, die zwischen den empirischen Lebenszeitverteilungen der Cluster unter Verwendung einer modifizierten Kuiper-Statistik differenziert. Wir lernen ein tiefes neuronales Netzwerk durch die Optimierung dieses Verlustes, das ein weiches Clustering von Nutzern in Überlebensgruppen durchführt. Wir wenden unsere Methode auf einen sozialen Netzwerkdatensatz mit über 1 Million Subjekten an und zeigen eine signifikante Verbesserung des C-Index im Vergleich zu Alternativen.
Die Bayes'sche Optimierung (BO) ist eine beliebte Methode zur Abstimmung der Hyperparameter von teuren Black-Box-Funktionen. Trotz ihres Erfolgs konzentriert sich die Standard-BO jeweils auf eine einzelne Aufgabe und ist nicht darauf ausgelegt, Informationen aus verwandten Funktionen zu nutzen, wie z. B. die Abstimmung von Leistungsmetriken desselben Algorithmus über mehrere Datensätze hinweg. In dieser Arbeit stellen wir einen neuartigen Ansatz vor, um Transfer-Lernen über verschiedene Datensätze und Metriken hinweg zu erreichen: Die Hauptidee ist die Regression der Abbildung von Hyperparametern auf Metrik-Quantilen mit einer semiparametrischen Gauß-Copula-Verteilung, die Robustheit gegenüber verschiedenen Skalen oder Ausreißern bietet, die bei verschiedenen Aufgaben auftreten können. Wir stellen zwei Methoden vor, um diese Schätzung zu nutzen: eine Thompson-Sampling-Strategie sowie einen Gauß-Copula-Prozess, der eine solche Quantil-Schätzung als Prior verwendet, und zeigen, dass diese Strategien die Schätzung mehrerer Metriken wie Laufzeit und Genauigkeit kombinieren können, indem sie die Optimierung auf kostengünstigere Hyperparameter für das gleiche Maß an Genauigkeit lenken.
Wir schlagen reine CapsNets (P-CapsNets) ohne Routing-Prozeduren vor, wobei wir insbesondere drei Modifikationen an CapsNets vornehmen.  Erstens entfernen wir Routing-Prozeduren aus CapsNets, basierend auf der Beobachtung, dass die Kopplungskoeffizienten implizit erlernt werden können.Zweitens ersetzen wir die Faltungsschichten in CapsNets, um die Effizienz zu verbessern.Drittens packen wir die Kapseln in Rang-3-Tensoren, um die Effizienz weiter zu verbessern. Das Experiment zeigt, dass P-CapsNets eine bessere Leistung als CapsNets mit verschiedenen Routineverfahren erreichen, indem sie deutlich weniger Parameter auf MNIST&CIFAR10 verwenden. Die hohe Effizienz von P-CapsNets ist sogar mit einigen tiefen Kompressionsmodellen vergleichbar. Zum Beispiel erreichen wir mehr als 99% Genauigkeit auf MNIST, indem wir nur 3888 Parameter verwenden.  Wir visualisieren die Kapseln sowie die zugehörige Korrelationsmatrix, um einen möglichen Weg zur Initialisierung von CapsNets in der Zukunft aufzuzeigen und die Robustheit von P-CapsNets im Vergleich zu CNNs zu untersuchen.
	In jüngster Zeit wurden die statistischen Eigenschaften neuronaler Netze mit großem Erfolg aus der Perspektive der Mean-Field-Theorie untersucht, wobei sehr präzise Vorhersagen über das Verhalten neuronaler Netze und die Testzeitleistung gemacht und verifiziert wurden.	In dieser Arbeit bauen wir auf diesen Arbeiten auf, um zwei Methoden zur Zähmung des Verhaltens zufälliger Residualnetze (mit nur vollständig verbundenen Schichten und ohne Batchnorm) zu untersuchen.	Die erste Methode ist die Breitenvariation (WV), d. h. die Variation der Breiten von Schichten als Funktion der Tiefe.	Wir zeigen, dass der Breitenabfall die Gradientenexplosion reduziert, ohne die mittlere Vorwärtsdynamik des Zufallsnetzes zu beeinträchtigen.	Die zweite Methode ist die Varianzvariation (VV), d. h. die Veränderung der Initialisierungsvarianzen von Gewichten und Vorspannungen über die Tiefe.	Wir zeigen, dass VV bei geeigneter Anwendung die Gradientenexplosion von tanh- und ReLU-Netzen von $\exp(\Theta(\sqrt L))$ bzw. $\exp(\Theta(L))$ auf konstantes $\Theta(1)$ reduzieren kann.	Es wird ein vollständiges Phasendiagramm abgeleitet, das zeigt, wie sich der Varianzzerfall auf verschiedene Dynamiken auswirkt, z. B. auf die von Gradienten- und Aktivierungsnormen.	Insbesondere zeigen wir die Existenz vieler Phasenübergänge, bei denen diese Dynamik zwischen exponentiellem, polynomialem, logarithmischem und sogar konstantem Verhalten wechselt.	Mithilfe der gewonnenen Mean-Field-Theorie sind wir in der Lage, überraschend gut zu verfolgen, wie sich VV zur Initialisierungszeit auf die Trainings- und Testzeitleistung auf MNIST nach einer bestimmten Anzahl von Epochen auswirkt: Die Level-Sets der Test-/Train-Set-Genauigkeiten stimmen mit den Level-Sets der Erwartungen bestimmter Gradientennormen oder der metrischen Expressivität (wie in \cite{yang_meanfield_2017} definiert) überein, einem Maß für die Expansion in einem zufälligen neuronalen Netzwerk.	Basierend auf Erkenntnissen aus früheren Arbeiten in der Deep Mean Field-Theorie und der Informationsgeometrie bieten wir auch eine neue Perspektive auf die Probleme der Gradientenexplosion und des Verschwindens: Sie führen zu einer schlechten Konditionierung der Fisher-Informationsmatrix, was zu Optimierungsproblemen führt.
In diesem Szenario ist das Erlernen eines effektiven Kommunikationsprotokolls von zentraler Bedeutung. Wir schlagen ein Kommunikationsprotokoll vor, das eine zielgerichtete Kommunikation ermöglicht, bei der die Agenten lernen, welche Nachrichten sie senden und an wen sie diese senden sollen. Wir evaluieren unseren Ansatz an mehreren kooperativen Multi-Agenten-Aufgaben mit unterschiedlicher Schwierigkeit und Anzahl von Agenten in einer Vielzahl von Umgebungen, die von 2D-Gitter-Layouts von Formen und simulierten Verkehrsknotenpunkten bis hin zu komplexen 3D-Umgebungen in Innenräumen reichen, und demonstrieren die Vorteile von gezielter und mehrstufiger Kommunikation, und wir zeigen, dass die gezielten Kommunikationsstrategien, die von den Agenten erlernt werden, durchaus interpretierbar und intuitiv sind.
Es ist schwierig für Anfänger in der Kunst des Milchkaffees, durch die Verwendung von zwei Flüssigkeiten mit unterschiedlicher Viskosität, wie z.B. Milchschaum und Sirup, ausgewogene Muster zu erzeugen. In diesem Artikel schlagen wir ein System vor, das Anfängern hilft, ausgewogene Milchkaffees zu machen, indem es den Vorgang der Herstellung von Milchkaffees direkt auf einen Cappuccino projiziert. Die Versuchsergebnisse zeigen den Fortschritt bei der Verwendung unseres Systems.  Wir diskutieren auch über die Ähnlichkeit der Latte Art und der Designvorlagen durch die Verwendung von Hintergrundsubtraktion.
Wir konzentrieren uns auf die zeitliche Selbstüberwachung für GAN-basierte Video-Generierungsaufgaben: Während das kontradiktorische Training erfolgreich generative Modelle für eine Vielzahl von Bereichen hervorbringt, ist die zeitliche Beziehung in den generierten Daten viel weniger erforscht, was für sequenzielle Generierungsaufgaben, z. B. Video-Superresolution und ungepaarte Videoübersetzung, entscheidend ist. Für erstere bevorzugen moderne Methoden oft einfachere Normverluste wie L2 gegenüber dem adversen Training, aber ihre Mittelwertbildung führt leicht zu zeitlich glatten Ergebnissen mit einem unerwünschten Mangel an räumlichen Details.Für ungepaarte Videoübersetzung modifizieren bestehende Ansätze die Generatornetzwerke, um räumlich-zeitliche Zykluskonsistenzen zu bilden. Im Gegensatz dazu konzentrieren wir uns auf die Verbesserung der Lernziele und schlagen einen zeitlich selbstüberwachten Algorithmus vor. für beide Aufgaben zeigen wir, dass zeitlich gegenteiliges Lernen der Schlüssel zum Erreichen zeitlich kohärenter Lösungen ist, ohne räumliche Details zu opfern. wir schlagen auch einen neuartigen Ping-Pong-Verlust vor, um die langfristige zeitliche Konsistenz zu verbessern. Wir schlagen auch eine erste Reihe von Metriken vor, um sowohl die Genauigkeit als auch die wahrnehmbare Qualität der zeitlichen Entwicklung quantitativ zu bewerten, und eine Reihe von Nutzerstudien bestätigt die mit diesen Metriken berechneten Rankings.
Die Internationalisierung von NLP-Modellen für mehrere Sprachen scheitert oft an der Knappheit von gelabelten Trainingsdaten.  Die meisten aktuellen Ansätze konzentrieren sich jedoch auf das Problem der Angleichung von Sprachen und gehen nicht auf die natürliche Domänenabweichung zwischen Sprachen und Kulturen ein.  In diesem Papier befassen wir uns mit der Domänenlücke im Rahmen der halbüberwachten sprachübergreifenden Dokumentenklassifikation, bei der beschriftete Daten in einer Ausgangssprache und nur unbeschriftete Daten in der Zielsprache verfügbar sind.  Wir kombinieren eine hochmoderne unüberwachte Lernmethode, maskiertes Sprachmodellierungstraining, mit einer neueren Methode für halbüberwachtes Lernen, unüberwachte Datenerweiterung (UDA), um gleichzeitig die Sprach- und die Domänenlücke zu schließen.  Wir zeigen, dass die Schließung der Domänenlücke bei sprachübergreifenden Aufgaben entscheidend ist.  Wir verbessern uns im Vergleich zu starken Baselines und erreichen einen neuen Stand der Technik für die sprachenübergreifende Dokumentenklassifikation.
Eine deutliche Gemeinsamkeit zwischen HMMs und RNNs besteht darin, dass beide verborgene Repräsentationen für sequenzielle Daten erlernen. Darüber hinaus wurde festgestellt, dass die Rückwärtsberechnung des Baum-Welch-Algorithmus für HMMs ein Spezialfall des Backpropagation-Algorithmus ist, der für neuronale Netze verwendet wird (Eisner (2016)).  Deuten diese Beobachtungen darauf hin, dass HMMs trotz ihrer vielen scheinbaren Unterschiede ein Spezialfall von RNNs sind?   In diesem Beitrag untersuchen wir eine Reihe von architektonischen Transformationen zwischen HMMs und RNNs, sowohl durch theoretische Ableitungen als auch durch empirische Hybridisierung, um diese Frage zu beantworten. Insbesondere untersuchen wir drei zentrale Designfaktoren - die Annahme der Unabhängigkeit zwischen den verborgenen Zuständen und der Beobachtung, die Platzierung von Softmax und die Verwendung von Nichtlinearität - um ihre empirischen Auswirkungen zu ermitteln.  Wir stellen eine umfassende empirische Studie vor, um Einblicke in das Zusammenspiel zwischen Expressivität und Interpretierbarkeit in Bezug auf Sprachmodellierung und Parts-of-Speech-Induktion zu gewinnen.
Wir stellen einen informationstheoretischen Rahmen für das Verständnis von Kompromissen beim unüberwachten Lernen von tiefen latent-variablen Modellen unter Verwendung von Variationsinferenz vor. Dieser Rahmen betont die Notwendigkeit, latent-variable Modelle entlang zweier Dimensionen zu betrachten: die Fähigkeit, Eingaben zu rekonstruieren (Verzerrung) und die Kommunikationskosten (Rate). Wir leiten die optimale Grenze von generativen Modellen in der zweidimensionalen Rate-Verzerrungs-Ebene ab und zeigen, wie das Standardziel der unteren Evidenzschranke nicht ausreicht, um zwischen Punkten entlang dieser Grenze zu wählen. Durch gezielte Optimierung zum Erlernen von generativen Modellen mit unterschiedlichen Raten sind wir jedoch in der Lage, viele Modelle zu erlernen, die eine ähnliche generative Leistung erzielen können, aber sehr unterschiedliche Kompromisse in Bezug auf die Verwendung der latenten Variablen eingehen.durch Experimente auf MNIST und Omniglot mit einer Vielzahl von Architekturen zeigen wir, wie unser Rahmenwerk Licht auf viele kürzlich vorgeschlagene Erweiterungen der Variations-Autoencoder-Familie wirft.
GNNs folgen einem Nachbarschaftsaggregationsschema, bei dem der Darstellungsvektor eines Knotens durch rekursive Aggregation und Transformation der Darstellungsvektoren seiner Nachbarknoten berechnet wird. Viele GNN-Varianten wurden vorgeschlagen und haben sowohl bei Knoten- als auch bei Graphenklassifizierungsaufgaben Spitzenergebnisse erzielt. Unsere Ergebnisse charakterisieren die Unterscheidungskraft populärer GNN-Varianten, wie Graph Convolutional Networks und GraphSAGE, und zeigen, dass diese nicht in der Lage sind, bestimmte einfache Graphenstrukturen zu unterscheiden. Wir entwickeln dann eine einfache Architektur, die nachweislich die ausdrucksstärkste unter den GNNs ist und genauso leistungsfähig ist wie der Weisfeiler-Lehman-Graphenisomorphismustest.Wir validieren unsere theoretischen Erkenntnisse empirisch anhand einer Reihe von Graphklassifizierungs-Benchmarks und zeigen, dass unser Modell die beste Leistung erreicht.
Wir stellen MTLAB vor, einen neuen Algorithmus für das Lernen mehrerer zusammenhängender Aufgaben mit starken theoretischen Garantien, dessen Kernidee darin besteht, sequentiell über die Daten aller Aufgaben zu lernen, ohne Unterbrechungen oder Neustarts an den Aufgabengrenzen, und aus diesem Prozess durch einen zusätzlichen Online-zu-Batch-Konvertierungsschritt Prädiktoren für einzelne Aufgaben abzuleiten. Im Rahmen des lebenslangen Lernens führt dies zu einer verbesserten Generalisierungsschranke, die mit der Gesamtzahl der Stichproben über alle beobachteten Aufgaben konvergiert, anstatt mit der Anzahl der Beispiele pro Aufgabe oder der Anzahl der Aufgaben unabhängig voneinander.
Jüngste Arbeiten haben die überraschenden sprachenübergreifenden Fähigkeiten des multilingualen BERT (M-BERT) gezeigt - überraschend, da es ohne sprachenübergreifende Zielsetzung und ohne abgeglichene Daten trainiert wird.In dieser Arbeit bieten wir eine umfassende Studie des Beitrags verschiedener Komponenten in M-BERT zu seiner sprachenübergreifenden Fähigkeit.Wir untersuchen die Auswirkungen der linguistischen Eigenschaften der Sprachen, der Architektur des Modells und der Lernziele. Die experimentelle Studie wird im Kontext von drei typologisch unterschiedlichen Sprachen - Spanisch, Hindi und Russisch - und unter Verwendung von zwei konzeptionell unterschiedlichen NLP-Aufgaben, Textentailment und Named-Entity-Recognition, durchgeführt.Eine unserer wichtigsten Schlussfolgerungen ist die Tatsache, dass die lexikalische Überlappung zwischen den Sprachen eine vernachlässigbare Rolle für den sprachenübergreifenden Erfolg spielt, während die Tiefe des Netzwerks ein wichtiger Teil davon ist.
Wir analysieren die Dynamik des Trainings von tiefen ReLU-Netzen und ihre Auswirkungen auf die Generalisierungsfähigkeit: In einem Lehrer-Schüler-Setting haben wir eine neuartige Beziehung zwischen dem Gradienten, den versteckte Schülerknoten erhalten, und den Aktivierungen von Lehrerknoten für tiefe ReLU-Netze entdeckt. Mit dieser Beziehung und der Annahme kleiner überlappender Aktivierungen von Lehrerknoten beweisen wir, dass (1) Schülerknoten, deren Gewichte so initialisiert sind, dass sie nahe an den Lehrerknoten liegen, schneller zu ihnen konvergieren, und (2) in überparametrisierten Regimen und im 2-Schicht-Fall zwar eine kleine Menge glücklicher Knoten zu den Lehrerknoten konvergiert, die Fan-Out-Gewichte anderer Knoten jedoch gegen Null konvergieren. Dieser Rahmen bietet Einblick in mehrere rätselhafte Phänomene im Deep Learning wie Überparametrisierung, implizite Regularisierung, Lotterielose usw. Wir verifizieren unsere Annahme, indem wir zeigen, dass die Mehrheit der BatchNorm-Verzerrungen von vortrainierten VGG11/16-Modellen negativ sind.Experimente an (1) zufälligen tiefen Lehrernetzwerken mit Gauß'schen Eingaben, (2) auf CIFAR-10 vortrainierten Lehrernetzwerken und (3) umfangreichen Ablationsstudien validieren unsere vielfältigen theoretischen Vorhersagen.
Neuere Studien haben gezeigt, dass die Notwendigkeit für parallele Daten Überwachung mit Zeichen-Level-Informationen gemildert werden kann.Während diese Methoden zeigten ermutigende Ergebnisse, sind sie nicht auf Augenhöhe mit ihren überwachten Pendants und sind auf Paare von Sprachen, die ein gemeinsames Alphabet.In dieser Arbeit zeigen wir, dass wir ein zweisprachiges Wörterbuch zwischen zwei Sprachen ohne Verwendung von parallelen Korpora, durch die Ausrichtung einsprachige Wort Einbettung Räume in eine unbeaufsichtigte Weise bauen können. Unsere Experimente zeigen, dass unsere Methode auch für weit entfernte Sprachpaare wie Englisch-Russisch oder Englisch-Chinesisch sehr gut funktioniert, und wir beschreiben schließlich Experimente mit dem ressourcenarmen Sprachpaar Englisch-Esperanto, für das es nur eine begrenzte Menge an parallelen Daten gibt, um die potenziellen Auswirkungen unserer Methode auf die vollständig unüberwachte maschinelle Übersetzung zu zeigen.Unser Code, unsere Einbettungen und Wörterbücher sind öffentlich verfügbar.
Fragen, die das Zählen einer Vielzahl von Objekten in Bildern erfordern, sind nach wie vor eine große Herausforderung bei der Beantwortung visueller Fragen (VQA). Die gängigsten Ansätze für VQA beinhalten entweder die Klassifizierung von Antworten auf der Grundlage von Darstellungen fester Länge sowohl des Bildes als auch der Frage oder die Aufsummierung von Bruchzahlen, die aus jedem Abschnitt des Bildes geschätzt werden.Im Gegensatz dazu behandeln wir das Zählen als einen sequentiellen Entscheidungsprozess und zwingen unser Modell dazu, diskrete Entscheidungen darüber zu treffen, was zu zählen ist. Eine Besonderheit unseres Ansatzes ist die intuitive und interpretierbare Ausgabe, da die diskreten Zählungen automatisch im Bild verankert werden. Darüber hinaus übertrifft unsere Methode den Stand der Technik der Architektur für VQA bei mehreren Metriken, die die Zählung bewerten.
Graphen sind grundlegende Datenstrukturen, die zur Modellierung vieler wichtiger realer Daten benötigt werden, von Wissensgraphen, physikalischen und sozialen Interaktionen bis hin zu Molekülen und Proteinen.In diesem Papier untersuchen wir das Problem des Lernens generativer Modelle von Graphen aus einem Datensatz von Graphen von Interesse.Nach dem Lernen können diese Modelle verwendet werden, um Proben mit ähnlichen Eigenschaften wie die im Datensatz zu erzeugen.  Die Aufgabe, generative Modelle von Graphen zu erlernen, hat jedoch ihre eigenen Herausforderungen, insbesondere die Behandlung von Symmetrien in Graphen und die Anordnung ihrer Elemente während des Generierungsprozesses sind wichtige Fragen.Wir schlagen ein generisches, auf neuronalen Netzen basierendes Modell vor, das in der Lage ist, jeden beliebigen Graphen zu generieren.  Wir untersuchen seine Leistung bei einigen Graphengenerierungsaufgaben im Vergleich zu Basislösungen, die Domänenwissen ausnutzen.  Wir erörtern mögliche Probleme und offene Fragen für solche generativen Modelle in der Zukunft.
Wir verwenden Tree-LSTM als Kompositionsfunktion, die entlang einer Baumstruktur angewendet wird, die von einem vollständig differenzierbaren Parser für natürliche Sprachen gefunden wurde. Unser Modell optimiert gleichzeitig sowohl die Kompositionsfunktion als auch den Parser, wodurch die Notwendigkeit von extern bereitgestellten Parse-Bäumen entfällt, die normalerweise für Tree-LSTM erforderlich sind. Wir zeigen, dass es im Vergleich zu verschiedenen überwachten Tree-LSTM-Architekturen eine bessere Leistung bei einer textuellen Entailment-Aufgabe und einer Reverse-Dictionary-Aufgabe erzielt und wie die Leistung mit einem Aufmerksamkeitsmechanismus verbessert werden kann, der das Parse-Diagramm vollständig ausnutzt, indem er alle möglichen Teilbereiche des Satzes berücksichtigt.
Drei Techniken werden vorgeschlagen und experimentell getestet: abstandsbasierte Regularisierung, Pruning mit verschachteltem Rang und schichtweises bipartites Matching. die ersten beiden Algorithmen werden in der Trainings- bzw. Pruning-Phase verwendet, der dritte in der Phase der Anordnung der Neuronen. Experimente zeigen, dass die abstandsbasierte Regularisierung mit gewichtsbasiertem Pruning mit oder ohne schichtweises bipartites Matching am besten abschneidet, was darauf hindeutet, dass diese Techniken bei der Erstellung neuronaler Netze für die Implementierung in weit verbreiteten spezialisierten Schaltkreisen nützlich sein können.
Bei einem Video und einem Satz besteht das Ziel der schwach überwachten Abfrage von Videomomenten darin, das Videosegment zu finden, das durch den Satz beschrieben wird, ohne während des Trainings Zugriff auf zeitliche Anmerkungen zu haben.  Stattdessen muss ein Modell lernen, das richtige Segment (d.h. den richtigen Moment) zu identifizieren, wenn es nur Video-Satz-Paare zur Verfügung hat.  Um diesen Abgleich zu erleichtern, schlagen wir unser Weakly-supervised Moment Alignment Network (wMAN) vor, das einen mehrstufigen Co-Attention-Mechanismus nutzt, um reichhaltigere multimodale Repräsentationen zu erlernen. Der oben genannte Mechanismus besteht aus einem Frame-By-Word-Interaktionsmodul sowie einem neuartigen Word-Conditioned Visual Graph (WCVG). Unser Ansatz beinhaltet auch eine neuartige Anwendung von Positionskodierungen, die üblicherweise in Transformers verwendet werden, um visuell-semantische Repräsentationen zu erlernen, die kontextuelle Informationen über ihre relativen Positionen in der zeitlichen Abfolge durch iteratives Message-Passing enthalten.Umfassende Experimente auf den DiDeMo- und Charades-STA-Datensätzen demonstrieren die Effektivität unserer erlernten Repräsentationen: Unser kombiniertes wMAN-Modell übertrifft nicht nur den Stand der Technik schwach-überwachten Methode mit einer signifikanten Marge, sondern schneidet auch besser als stark-überwachten Stand der Technik-Methoden auf einigen Metriken.
Bei Aufgaben des maschinellen Lernens kommt es häufig zu Überschneidungen, wenn die Anzahl der Stichproben in der Zieldomäne unzureichend ist, da die Generalisierungsfähigkeit des Klassifizierers unter diesen Umständen schlecht ist.Um dieses Problem zu lösen, nutzt das Transferlernen das Wissen ähnlicher Domänen, um die Robustheit des Lerners zu verbessern. Der Grundgedanke bestehender Algorithmen zum Transferlernen besteht darin, den Unterschied zwischen den Domänen durch die Auswahl von Stichproben oder die Anpassung an die Domäne zu verringern, doch unabhängig davon, welchen Algorithmus zum Transferlernen wir verwenden, besteht der Unterschied immer, und das hybride Training von Quell- und Zieldaten führt zu einer Verringerung der Lernfähigkeit des Lerners in der Zieldomäne. Um dieses Problem zu lösen, haben wir eine zweistufige Transfer-Lern-Architektur vorgeschlagen, die auf Ensemble-Lernen basiert und die vorhandenen Transfer-Lern-Algorithmen verwendet, um die schwachen Lerner in der ersten Stufe zu trainieren, und die Vorhersagen der Zieldaten verwendet, um den endgültigen Lerner in der zweiten Stufe zu trainieren. Unter dieser Architektur können die Färbungsfähigkeit und die Generalisierungsfähigkeit gleichzeitig garantiert werden.Wir haben die vorgeschlagene Methode an öffentlichen Datensätzen evaluiert, was die Wirksamkeit und Robustheit unserer vorgeschlagenen Methode zeigt.
Deep Learning hat bei vielen Aufgaben mit großen Datenmengen und Generalisierung in der Nähe von Trainingsdaten erstaunliche Ergebnisse erzielt.Für viele wichtige Anwendungen in der realen Welt sind diese Anforderungen nicht realisierbar und zusätzliches Vorwissen über die Aufgabendomäne ist erforderlich, um die daraus resultierenden Probleme zu überwinden.Insbesondere das Lernen von physikalischen Modellen für die modellbasierte Steuerung erfordert eine robuste Extrapolation aus wenigen Proben - oft online in Echtzeit gesammelt - und Modellfehler können zu drastischen Schäden des Systems führen. Die direkte Einbeziehung physikalischer Erkenntnisse hat es uns ermöglicht, einen neuartigen Ansatz für das Lernen von tiefen Modellen zu entwickeln, der gut extrapoliert und dabei weniger Stichproben benötigt.Als erstes Beispiel schlagen wir Deep Lagrangian Networks (DeLaN) als tiefe Netzwerkstruktur vor, der die Lagrangesche Mechanik aufgezwungen wurde.DeLaN kann die Bewegungsgleichungen eines mechanischen Systems (d.h. die Systemdynamik) mit einem tiefen Netzwerk effizient lernen, Das daraus resultierende DeLaN-Netzwerk eignet sich sehr gut für die Steuerung von Robotern, wobei die vorgeschlagene Methode nicht nur die Lerngeschwindigkeit früherer Modelllernverfahren übertrifft, sondern auch eine wesentlich bessere und robustere Extrapolation auf neuartige Trajektorien bietet und online in Echtzeit lernt.
Viele anspruchsvolle Vorhersageprobleme, von der molekularen Optimierung bis zur Programmsynthese, beinhalten die Erzeugung komplexer strukturierter Objekte als Outputs, wobei die verfügbaren Trainingsdaten für ein generatives Modell möglicherweise nicht ausreichen, um alle möglichen komplexen Transformationen zu erlernen.Wir zeigen, wie ein einfaches, breit anwendbares, iteratives Zielerweiterungsschema überraschend effektiv sein kann, um das Training und die Verwendung solcher Modelle zu steuern. In jedem Augmentierungsschritt filtern wir die Ausgaben des Modells, um zusätzliche Vorhersageziele für die nächste Trainingsepoche zu erhalten. Unsere Methode ist sowohl in überwachten als auch in halbüberwachten Einstellungen anwendbar. Wir zeigen, dass unser Ansatz sowohl in der molekularen Optimierung als auch in der Programmsynthese signifikante Gewinne gegenüber starken Grundlinien erzielt. Insbesondere übertrifft unser Augmentierungsmodell den bisherigen Stand der Technik in der molekularen Optimierung um mehr als 10% in absoluten Gewinnen.
Die Boltzmann-Verteilung ist ein natürliches Modell für viele Systeme, von Gehirnen bis hin zu Materialien und Biomolekülen, ist aber oft nur von begrenztem Nutzen für die Anpassung von Daten, da Monte-Carlo-Algorithmen nicht in der Lage sind, sie in der verfügbaren Zeit zu simulieren.Diese Lücke zwischen den aussagekräftigen Fähigkeiten und der praktischen Anwendung von energiebasierten Modellen wird durch das Problem der Proteinfaltung veranschaulicht, da Energielandschaften dem heutigen Wissen über die Biophysik von Proteinen zugrunde liegen, Computersimulationen aber nicht in der Lage sind, alle außer den kleinsten Proteinen nach ersten Prinzipien zu falten. In dieser Arbeit versuchen wir, die Lücke zwischen der Ausdruckskraft von Energiefunktionen und den praktischen Möglichkeiten ihrer Simulatoren zu schließen, indem wir eine unrollierte Monte-Carlo-Simulation als Modell für Daten verwenden: Wir kombinieren eine neuronale Energiefunktion mit einem neuartigen und effizienten Simulator, der auf der Langevin-Dynamik basiert, um ein Ende-zu-Ende-differenzierbares Modell der atomaren Proteinstruktur bei gegebener Aminosäuresequenzinformation aufzubauen. Wir stellen Techniken zur Stabilisierung der Backpropagation bei langen Rollouts vor und zeigen, dass das Modell in der Lage ist, multimodale Vorhersagen zu treffen und in einigen Fällen auf unbeobachtete Proteinfaltentypen zu verallgemeinern, wenn es auf einem großen Korpus von Proteinstrukturen trainiert wird.
Um zu verstehen, wie einzelne Tiere lernen, sind standardisierte Hochdurchsatzmethoden für das Verhaltenstraining und Möglichkeiten zur Anpassung des Trainings erforderlich. Während des Trainings mit Hunderten oder Tausenden von Versuchen kann ein Tier seine zugrundeliegende Strategie abrupt ändern, und die Erfassung dieser Änderungen erfordert Echtzeit-Inferenz der latenten Entscheidungsstrategie des Tieres. Um diese Herausforderung zu bewältigen, haben wir eine integrierte Plattform für das automatisierte Training von Tieren und ein iteratives Entscheidungsinferenzmodell entwickelt, das in der Lage ist, die momentane Entscheidungsstrategie abzuleiten und die Wahl des Tieres bei jedem Versuch mit einer Genauigkeit von 80 % vorherzusagen, selbst wenn das Tier eine schlechte Leistung erbringt.Wir haben auch Entscheidungsvorhersagen mit einer Auflösung von einem Versuch mit automatischer Posenschätzung kombiniert, um Bewegungstrajektorien zu bewerten.Die Analyse dieser Merkmale ergab Kategorien von Bewegungstrajektorien, die mit der Entscheidungssicherheit in Verbindung stehen.
Rekurrente neuronale Netze (RNNs) sind ein leistungsfähiges Werkzeug für die Modellierung von sequentiellen Daten. Trotz ihrer weit verbreiteten Verwendung bleibt es schwer zu verstehen, wie RNNs komplexe Probleme lösen.  Trotz ihrer theoretischen Fähigkeit, komplexe, hochdimensionale Berechnungen durchzuführen, stellen wir fest, dass trainierte Netzwerke zu hoch interpretierbaren, niedrigdimensionalen Repräsentationen konvergieren.  Wir identifizieren einen einfachen Mechanismus, die Integration entlang eines approximativen Linien-Attraktors, und stellen fest, dass dieser Mechanismus in allen RNN-Architekturen (einschließlich LSTMs, GRUs und Vanilla RNNs) vorhanden ist.Insgesamt zeigen diese Ergebnisse, dass überraschend universelle und menschlich interpretierbare Berechnungen in einer Reihe von rekurrenten Netzwerken entstehen können.
Parallele Entwicklungen in den Neurowissenschaften und Deep Learning haben zu einem gegenseitig produktiven Austausch geführt, der unser Verständnis von realen und künstlichen neuronalen Netzwerken in sensorischen und kognitiven Systemen vorantreibt.Allerdings ist diese Interaktion zwischen den Feldern in der Studie der motorischen Kontrolle weniger entwickelt.In dieser Arbeit entwickeln wir ein virtuelles Nagetier als Plattform für die geerdete Studie der motorischen Aktivität in künstlichen Modellen der verkörperten Kontrolle.Wir verwenden dann diese Plattform, um die motorische Aktivität in verschiedenen Kontexten zu untersuchen, indem wir ein Modell trainieren, um vier komplexe Aufgaben zu lösen. Mit Methoden, die Neurowissenschaftlern vertraut sind, beschreiben wir die Verhaltensrepräsentationen und Algorithmen, die von verschiedenen Schichten des Netzwerks verwendet werden, indem wir einen neuroethologischen Ansatz verwenden, um die motorische Aktivität in Bezug auf das Verhalten und die Ziele des Nagers zu charakterisieren.Wir stellen fest, dass das Modell zwei Klassen von Repräsentationen verwendet, die jeweils die aufgabenspezifischen Verhaltensstrategien und die aufgabeninvariante Verhaltenskinematik kodieren.Diese Repräsentationen spiegeln sich in der sequenziellen Aktivität und der Populationsdynamik der neuronalen Subpopulationen wider.Insgesamt erleichtert das virtuelle Nagetier eine fundierte Zusammenarbeit zwischen tiefem Verstärkungslernen und motorischen Neurowissenschaften.
Wir stellen Optimal Transport GAN (OT-GAN) vor, eine Variante generativer adversarialer Netze, die eine neue Metrik minimiert, die den Abstand zwischen der Generatorverteilung und der Datenverteilung misst. Diese Metrik, die wir Mini-Batch-Energiedistanz nennen, kombiniert optimalen Transport in primärer Form mit einer Energiedistanz, die in einem adversarial gelernten Merkmalsraum definiert ist, was zu einer hochdiskriminativen Distanzfunktion mit unvoreingenommenen Mini-Batch-Gradienten führt. experimentell zeigen wir, dass OT-GAN sehr stabil ist, wenn es mit großen Mini-Batches trainiert wird, und wir präsentieren State-of-the-Art-Ergebnisse zu mehreren populären Benchmark-Problemen für die Bilderzeugung.
Der Agent sieht Bilder der Umgebung, hört einem virtuellen Lehrer zu und führt Aktionen aus, um Belohnungen zu erhalten. Er lernt interaktiv die Sprache des Lehrers von Grund auf, basierend auf zwei Sprachverwendungsfällen: satzgesteuerte Navigation und Beantwortung von Fragen. Durch die Entflechtung der Spracherfassung von anderen Berechnungsroutinen und die gemeinsame Nutzung einer Konzepterkennungsfunktion zwischen Spracherfassung und Vorhersage interpoliert und extrapoliert der Agent zuverlässig, um Sätze zu interpretieren, die neue Wortkombinationen oder neue Wörter enthalten, die in den Trainingssätzen fehlen. Die neuen Wörter werden aus den Antworten der Sprachvorhersage übernommen. Eine solche Sprachfähigkeit wird auf einer Population von über 1,6 Millionen verschiedenen Sätzen trainiert und evaluiert, die aus 119 Objektwörtern, 8 Farbwörtern, 9 Wörtern mit räumlicher Beziehung und 50 grammatikalischen Wörtern besteht. Das vorgeschlagene Modell übertrifft fünf Vergleichsmethoden für die Interpretation von Sätzen mit Nullschüssen deutlich.
Verstärkungslernalgorithmen sind zwar erfolgreich, neigen aber dazu, sich zu sehr an Trainingsumgebungen anzupassen, was ihre Anwendung in der realen Welt erschwert.In diesem Beitrag wird $\text{W}\text{R}^{2}\text{L}$ vorgeschlagen - ein robuster Verstärkungslernalgorithmus mit einer signifikant robusten Leistung bei niedrig- und hochdimensionalen Steuerungsaufgaben. Unsere Methode formalisiert das robuste Verstärkungslernen als ein neuartiges Min-Max-Spiel mit einer Wasserstein-Beschränkung für einen korrekten und konvergenten Löser, wobei wir neben der Formulierung auch einen effizienten und skalierbaren Löser vorschlagen, der einer neuartigen Optimierungsmethode nullter Ordnung folgt, von der wir glauben, dass sie für die numerische Optimierung im Allgemeinen nützlich sein kann. Wir zeigen empirisch signifikante Verbesserungen im Vergleich zu Standard- und robusten State-of-the-Art-Algorithmen in hochdimensionalen MuJuCo-Umgebungen.
Partiell beobachtbare Markov-Entscheidungsprozesse (POMDPs) sind ein natürliches Modell für Szenarien, in denen man mit unvollständigem Wissen und zufälligen Ereignissen umgehen muss, wie z.B. in der Robotik und der Bewegungsplanung. In unserer Arbeit entwickeln wir eine spielbasierte Abstraktionsmethode, die in der Lage ist, sichere Grenzen und enge Approximationen für wichtige Unterklassen solcher Eigenschaften zu liefern, diskutieren die theoretischen Implikationen und zeigen die Anwendbarkeit unserer Ergebnisse auf einem breiten Spektrum von Benchmarks.
Wir konzentrieren uns auf die topologische Klassifikation der Erreichbarkeit in einer bestimmten Untergruppe von planaren Graphen (Mazes), wobei wir in der Lage sind, die Topologie der Daten zu modellieren und dabei im euklidischen Raum zu bleiben, was ihre Verarbeitung mit Standard-CNN-Architekturen ermöglicht. Wir schlagen eine geeignete Architektur für dieses Problem vor und zeigen, dass sie eine perfekte Lösung für die Klassifikationsaufgabe darstellen kann. Die Form der Kostenfunktion um diese Lösung ist auch abgeleitet und, bemerkenswert, hängt nicht von der Größe des Labyrinths in der großen maze limit.Responsible für dieses Verhalten sind seltene Ereignisse in der Datenmenge, die stark regulieren die Form der Kostenfunktion in der Nähe dieser globalen minimum.We weiter zu identifizieren ein Hindernis für das Lernen in Form von schlecht funktionierenden lokalen minima, in denen das Netzwerk wählt, um einige der Eingänge zu ignorieren.We weiter unterstützen unsere Ansprüche mit Ausbildung Experimente und numerische Analyse der Kostenfunktion auf Netzwerke mit bis zu $128$ Schichten.
Während neuronale Netze trainiert werden können, um einen bestimmten Datensatz auf einen anderen abzubilden, erlernen sie normalerweise keine verallgemeinerte Transformation, die genau über den Trainingsraum hinaus extrapoliert werden kann. z.B. könnte ein generatives adversariales Netz (GAN), das ausschließlich trainiert wurde, um Bilder von Autos von hell nach dunkel zu transformieren, nicht den gleichen Effekt auf Bilder von Pferden haben. Das liegt daran, dass neuronale Netze gut darin sind, innerhalb der Vielfalt der Daten, auf die sie trainiert wurden, neue Stichproben außerhalb der Vielfalt zu erzeugen oder "außerhalb der Stichprobe" zu extrapolieren. Um dieses Problem anzugehen, führen wir eine Technik ein, die als Neuronen-Editing bezeichnet wird und die lernt, wie Neuronen eine Editierung für eine bestimmte Transformation in einem latenten Raum kodieren. Wir verwenden einen Autoencoder, um die Variation innerhalb des Datensatzes in Aktivierungen verschiedener Neuronen zu zerlegen und transformierte Daten zu erzeugen, indem wir eine Editierungstransformation auf diesen Neuronen definieren. Indem wir die Transformation in einem latenten trainierten Raum durchführen, kodieren wir ziemlich komplexe und nichtlineare Transformationen der Daten mit viel einfacheren Verteilungsverschiebungen der Neuronenaktivierungen.Wir präsentieren unsere Technik auf dem Gebiet der Bildübertragung und zwei biologischen Anwendungen: Entfernung von Batch-Artefakten, die unerwünschtes Rauschen darstellen, und Modellierung der Wirkung von Medikamentenbehandlungen, um Synergien zwischen Medikamenten vorherzusagen.
Wir stellen eine Darstellung zur Beschreibung von Übergangsmodellen in komplexen unsicheren Domänen vor, die relationale Regeln verwendet.  Für jede Aktion wählt eine Regel eine Menge relevanter Objekte aus und berechnet eine Verteilung über die Eigenschaften genau dieser Objekte im resultierenden Zustand unter Berücksichtigung ihrer Eigenschaften im vorherigen Zustand.  Ein iterativer Greedy-Algorithmus wird verwendet, um eine Reihe von deiktischen Referenzen zu konstruieren, die bestimmen, welche Objekte in einem bestimmten Zustand relevant sind.   Neuronale Netze mit Vorwärtskopplung werden verwendet, um die Übergangsverteilung für die Eigenschaften der relevanten Objekte zu lernen.  Diese Strategie erweist sich sowohl als vielseitiger als auch als effizienter als das Lernen eines monolithischen Übergangsmodells in einem simulierten Bereich, in dem ein Roboter Stapel von Objekten auf einem unübersichtlichen Tisch schiebt.
Viele großartige Planungsarchitekturen, die später in der Literatur vorgeschlagen wurden, sind von diesem Konstruktionsprinzip inspiriert, bei dem eine rekursive Netzwerkarchitektur angewandt wird, um Backup-Operationen eines Wert-Iterationsalgorithmus zu emulieren.Allerdings können bestehende Rahmenwerke nur auf Domänen mit einer Gitterstruktur, d.h. regelmäßigen Graphen, die in einen bestimmten euklidischen Raum eingebettet sind, lernen und effektiv planen. In diesem Papier schlagen wir ein allgemeines Planungsnetzwerk vor, das wir Graph-based Motion Planning Networks (GrMPN) nennen und das in der Lage ist, toi) auf allgemeinen unregelmäßigen Graphen zu lernen und zu planen und somitii) bestehende Planungsnetzwerkarchitekturen zu Spezialfällen zu machen.Das vorgeschlagene GrMPN-Framework ist invariant gegenüber der Permutation von Aufgabengraphen, d.h. Graph-Isormophismus. Wir demonstrieren die Leistungsfähigkeit der vorgeschlagenen GrMPN-Methode im Vergleich zu anderen Basislösungen in drei Domänen: 2D-Labyrinthe (regulärer Graph), Pfadplanung auf unregelmäßigen Graphen und Bewegungsplanung (ein unregelmäßiger Graph mit Roboterkonfigurationen).
Wir beschreiben Techniken für das Training von qualitativ hochwertigen Bildentrauschungsmodellen, die nur einzelne Instanzen von beschädigten Bildern als Trainingsdaten benötigen. Inspiriert von einer neueren Technik, die die Notwendigkeit einer Überwachung durch Bildpaare durch den Einsatz von Netzwerken mit einem "blinden Fleck" im rezeptiven Feld beseitigt, beheben wir zwei ihrer Mängel: ineffizientes Training und schlechte endgültige Entrauschungsleistung. Dies wird durch eine neuartige Blindspot-Faltungsnetzwerk-Architektur erreicht, die ein effizientes selbstüberwachtes Training ermöglicht, sowie durch die Anwendung der Bayes'schen Verteilungsvorhersage auf Ausgangsfarben, die das selbstüberwachte Modell sowohl in Bezug auf die Qualität als auch auf die Trainingsgeschwindigkeit im Falle von i.i.d. Gauß'schem Rauschen auf eine Stufe mit vollständig überwachten Deep-Learning-Techniken stellen.
Agenten mit Reinforcement Learning (RL) verbessern sich durch Versuch und Irrtum, aber wenn die Belohnung spärlich ist und der Agent keine erfolgreichen Aktionssequenzen entdecken kann, stagniert das Lernen. Dies ist ein bemerkenswertes Problem bei der Ausbildung von Deep-RL-Agenten für webbasierte Aufgaben, wie z. B. das Buchen von Flügen oder das Beantworten von E-Mails, wo ein einziger Fehler die gesamte Aktionssequenz ruinieren kann. Stattdessen schlagen wir vor, die Exploration mit Hilfe von Demonstrationen einzuschränken. Aus jeder Demonstration leiten wir hochrangige "Arbeitsabläufe" ab, die die zulässigen Aktionen in jedem Zeitschritt so einschränken, dass sie denen in der Demonstration ähneln (z.B., "Unsere Explorationspolitik lernt dann, erfolgreiche Workflows zu identifizieren und probiert Aktionen aus, die diese Workflows erfüllen. Workflows entfernen schlechte Explorationsrichtungen und beschleunigen die Fähigkeit des Agenten, Belohnungen zu entdecken. Wir verwenden unseren Ansatz, um eine neuartige neuronale Strategie zu trainieren, die für die semi-strukturierte Natur von Websites entwickelt wurde, und evaluieren eine Reihe von Web-Aufgaben, einschließlich des aktuellen World of Bits Benchmarks.
Das Hauptproblem ist, dass diese Art des Lernens und folglich neuronale Netze, die als tief definiert werden können, ressourcenintensiv sind und spezielle Hardware benötigen, um eine Berechnung in einer angemessenen Zeit durchzuführen. Viele Aufgaben müssen so weit wie möglich in Echtzeit ausgeführt werden, daher müssen viele Komponenten wie Code, Algorithmen, numerische Genauigkeit und Hardware optimiert werden, um sie "effizient und nutzbar" zu machen.All diese Optimierungen können uns helfen, unglaublich genaue und schnelle Lernmodelle zu erstellen.
Zusätzlich zu den Textdaten selbst haben wir oft zusätzliche Kovariaten, die mit einzelnen Dokumenten im Korpus assoziiert sind - z. B. die demografische Herkunft des Autors, Zeitpunkt und Ort der Veröffentlichung usw. -, und wir möchten, dass die Einbettung die Strukturen des gemeinsamen Auftretens in einem großen Textkorpus erfasst. In dieser Arbeit schlagen wir ein neues Tensor-Dekompositionsmodell für Worteinbettungen mit Kovariaten vor: Unser Modell lernt gemeinsam eine \emph{base}-Einbettung für alle Wörter sowie eine gewichtete diagonale Transformation, um zu modellieren, wie jede Kovariate die Basiseinbettung verändert. Um die spezifische Einbettung für einen bestimmten Autor oder Ort zu erhalten, können wir dann einfach die Basiseinbettung mit der Transformationsmatrix multiplizieren, die mit dieser Zeit oder diesem Ort verbunden ist. Unsere Experimente zeigen, dass unser gemeinsames Modell wesentlich bessere Einbettungen für jede Kovariate lernt als der Standardansatz, bei dem eine separate Einbettung für jede Kovariate gelernt wird, indem nur die relevante Teilmenge der Daten verwendet wird. Wir evaluieren die Vorteile unseres Algorithmus empirisch an verschiedenen Datensätzen und zeigen, wie er verwendet werden kann, um viele natürliche Fragen zu den Auswirkungen von Kovariaten zu beantworten.
Obwohl Deep Learning sehr leistungsfähig ist, wird es theoretisch nicht gut verstanden, und insbesondere wurden erst vor kurzem Ergebnisse für die Komplexität des Trainings von tiefen neuronalen Netzen erzielt. In dieser Arbeit zeigen wir, dass große Klassen von tiefen neuronalen Netzen mit verschiedenen Architekturen (z. B. DNNs, CNNs, binäre neuronale Netze und ResNets), Aktivierungsfunktionen (z. B. ReLUs und leaky ReLUs) und Verlustfunktionen (z. B. Hinge Loss, Euklidischer Verlust) in der Lage sind, die Komplexität von tiefen neuronalen Netzen zu berechnen, Hinge-Verlust, Euklidischer Verlust usw.) können mit linearer Programmierung in einer Zeit, die exponentiell zu den Eingabedaten und der Parameterraumdimension und polynomial zur Größe des Datensatzes ist, bis zur annähernden Optimalität mit der gewünschten Zielgenauigkeit trainiert werden; Verbesserungen der Abhängigkeit von der Eingabedimension sind unter der Annahme von $P\neq NP$ bekanntermaßen unwahrscheinlich, und die Verbesserung der Abhängigkeit von der Parameterraumdimension bleibt offen. Unsere Arbeit ist allgemeiner auf empirische Risikominimierungsprobleme anwendbar, was uns erlaubt, verschiedene frühere Ergebnisse zu verallgemeinern und neue Komplexitätsergebnisse für bisher nicht untersuchte Architekturen in der richtigen Lernumgebung zu erhalten.
Der erweiterte Kalman-Filter (EKF) ist ein klassischer Signalverarbeitungsalgorithmus, der eine effiziente approximative Bayes'sche Inferenz in nicht-konjugierten Modellen durchführt, indem er die lokale Messfunktion linearisiert und so die Notwendigkeit vermeidet, unlösbare Integrale zu berechnen, wenn er das Posterior berechnet.In einigen Fällen übertrifft der EKF Methoden, die sich auf Kubatur stützen, um solche Integrale zu lösen, insbesondere bei zeitkritischen realen Problemen.Der Nachteil des EKF ist seine lokale Natur, während moderne Methoden wie Variationsinferenz oder Erwartungsausbreitung (EP) als globale Approximationen betrachtet werden. Wir formulieren Power-EP als nichtlinearen Kalman-Filter und zeigen dann, dass die Linearisierung zu einem global iterierten Algorithmus führt, der beim ersten Durchlauf durch die Daten genau mit dem EKF übereinstimmt und die Linearisierung bei den nachfolgenden Durchläufen iterativ verbessert.Ein zusätzlicher Vorteil ist die Möglichkeit, den Grenzwert zu berechnen, wenn die EP-Power gegen Null tendiert, wodurch die Instabilität des EP-ähnlichen Algorithmus beseitigt wird.Das resultierende Inferenzschema löst nicht-konjugierte zeitliche Gauß-Prozessmodelle in linearer Zeit, $\mathcal{O}(n)$, und in geschlossener Form.
Dieses Papier untersucht die Einfachheit von gelernten neuronalen Netzen unter verschiedenen Bedingungen: gelernt auf realen vs. zufälligen Daten, variierende Größe/Architektur und unter Verwendung einer großen Minibatch-Größe vs. einer kleinen Minibatch-Größe.Der Begriff der Einfachheit, der hier verwendet wird, ist der der Lernfähigkeit, d.h., Während sich die Lernfähigkeit von der Testgenauigkeit unterscheidet (und sogar oft höher ist als diese), deuten die Ergebnisse darauf hin, dass es eine starke Korrelation zwischen kleinen Generalisierungsfehlern und hoher Lernfähigkeit gibt.Diese Arbeit zeigt auch, dass es signifikante qualitative Unterschiede zwischen flachen Netzen und populären tiefen Netzen gibt. Unsere Hoffnung ist, dass eine solche empirische Studie über das Verständnis gelernter neuronaler Netze Licht auf die richtigen Annahmen werfen könnte, die für eine theoretische Studie über tiefes Lernen gemacht werden können.
    Mit der Verbreitung von Modellen für die Verarbeitung natürlicher Sprache (NLP) ist es noch schwieriger, die Unterschiede zwischen den Modellen und ihre relativen Vorzüge zu verstehen. Die bloße Betrachtung von Unterschieden zwischen holistischen Metriken wie Genauigkeit, BLEU oder F1 sagt uns nicht, warum oder wie eine bestimmte Methode besser ist und wie Datensatzverzerrungen die Wahl des Modelldesigns beeinflussen.    In diesem Beitrag stellen wir eine allgemeine Methodik für die {\emph{interpretable}} Evaluierung von NLP-Systemen vor und wählen die Aufgabe der Named-Entity-Recognition (NER) als Fallstudie, die eine Kernaufgabe der Identifizierung von Personen, Orten oder Organisationen in Texten darstellt. Die vorgeschlagene Evaluierungsmethode ermöglicht es uns, die \textit{Modellverzerrungen}, \textit{Datensatzverzerrungen} und die Auswirkungen der \emph{Unterschiede in den Datensätzen} auf das Design der Modelle zu interpretieren und so die Stärken und Schwächen aktueller Ansätze zu identifizieren.ÿIndem wir unser Analysetool zur Verfügung stellen, machen wir es zukünftigen Forschern leicht, ähnliche Analysen durchzuführen und den Fortschritt in diesem Bereich voranzutreiben.
Wir gehen davon aus, dass es ein ungelöstes Problem ist, von den Agenten zu verlangen, sich an die Regeln der menschlichen Sprache zu halten und gleichzeitig den Informationsaustausch zu maximieren, und stellen fest, dass Menschen nicht von einer gemeinsamen Sprache abweichen, weil sie soziale Wesen sind und täglich mit vielen Menschen kommunizieren müssen, und dass es viel einfacher ist, sich an eine gemeinsame Sprache zu halten, selbst um den Preis eines gewissen Effizienzverlustes. Ausgehend von dieser Erkenntnis schlagen wir ein Multi-Agenten-Dialogsystem vor, bei dem jeder Agent mit mehreren Agenten interagiert und von ihnen lernt. Wir zeigen, dass dies zu einem relevanteren und kohärenteren Dialog führt (wie von menschlichen Bewertern beurteilt), ohne dass die Aufgabenleistung darunter leidet (wie von quantitativen Metriken beurteilt).
In diesem Beitrag wird eine einfache und intuitive Erklärung für den Posterior-Kollaps durch die Analyse linearer VAEs und ihre direkte Korrespondenz mit Probabilistic PCA (pPCA) vorgestellt. pPCA zeigt, wie lokale Maxima aus der marginalen log-likelihood von pPCA entstehen können, was zu ähnlichen lokalen Maxima für die Evidence Lower Bound (ELBO) führt. Wir zeigen, dass das Training einer linearen VAE mit Variationsinferenz ein eindeutig identifizierbares globales Maximum ergibt, das den Richtungen der Hauptkomponenten entspricht. Wir liefern empirische Beweise dafür, dass das Vorhandensein lokaler Maxima zu einem Posterior-Kollaps in tiefen, nicht-linearen VAEs führt. Unsere Ergebnisse helfen, eine Vielzahl heuristischer Ansätze in der Literatur zu erklären, die versuchen, die Wirkung des KL-Terms in der ELBO zu verringern, um den Posterior-Kollaps zu reduzieren.
Transformatoren haben bei einer Vielzahl von Aufgaben zur Verarbeitung natürlicher Sprache Spitzenergebnisse erzielt. Trotz der guten Leistung sind Transformatoren bei der Modellierung von langen Sätzen, bei denen die globale Aufmerksamkeitskarte zu verstreut ist, um wertvolle Informationen zu erfassen, immer noch schwach, da in diesem Fall die lokalen/Token-Merkmale, die auch für die Sequenzmodellierung von Bedeutung sind, bis zu einem gewissen Grad ausgelassen werden. Um dieses Problem zu beheben, schlagen wir ein Multi-Skala Aufmerksamkeitsmodell (MUSE) vor, indem wir Aufmerksamkeitsnetzwerke mit Faltungsnetzwerken und positionsweisen Feed-Forward-Netzwerken verketten, um explizit lokale und Token-Merkmale zu erfassen.In Anbetracht der Parametergröße und der Berechnungseffizienz verwenden wir die Feed-Forward-Schicht im ursprünglichen Transformer wieder und nehmen eine leichte dynamische Faltung als Implementierung an. Experimentelle Ergebnisse zeigen, dass das vorgeschlagene Modell erhebliche Leistungsverbesserungen gegenüber Transformer erzielt, insbesondere bei langen Sätzen, und den Stand der Technik von 35,6 auf 36,2 bei der IWSLT 2014 Deutsch-Englisch-Übersetzungsaufgabe, von 30,6 auf 31,3 bei der IWSLT 2015 Englisch-Vietnamesisch-Übersetzungsaufgabe, und auch beim WMT 2014 Englisch-Französisch-Übersetzungsdatensatz erreichen wir mit einem BLEU-Wert von 43,2 den Stand der Technik.
Mehrere bestehende Ansätze verwenden lineare Relaxation-basierte neuronale Netzwerk-Ausgangsgrenzen unter Störung, aber sie können das Training um einen Faktor von Hunderten verlangsamen, abhängig von den zugrunde liegenden Netzwerk-Architekturen.Inzwischen ist Intervall-Bound-Propagation (IBP) basierte Ausbildung effizient und deutlich besser als lineare Relaxation-basierte Methoden auf viele Aufgaben, aber es kann von Stabilitätsproblemen leiden, da die Grenzen sind viel lockerer vor allem zu Beginn der Ausbildung. In diesem Papier schlagen wir eine neue zertifizierte adversarische Trainingsmethode, CROWN-IBP, vor, indem wir die schnellen IBP-Schranken in einem Vorwärts-Bounding-Durchgang und eine enge lineare Relaxationsmethode, CROWN, in einem Rückwärts-Bounding-Durchgang kombinieren.CROWN-IBP ist rechnerisch effizient und übertrifft die IBP-Baselines beim Training nachweislich robuster neuronaler Netzwerke. Wir führen groß angelegte Experimente mit MNIST- und CIFAR-Datensätzen durch und übertreffen alle bisherigen auf linearer Relaxation und Bound-Propagation basierenden zertifizierten Verteidigungsmaßnahmen in Bezug auf L_inf-Robustheit. Insbesondere erreichen wir 7,02% verifizierten Testfehler bei MNIST mit epsilon=0,3 und 66,94% bei CIFAR-10 mit epsilon=8/255.
Das Pruning von Netzwerken ist weit verbreitet, um die hohen Inferenzkosten von tiefen Modellen in ressourcenarmen Umgebungen zu reduzieren.ein typischer Pruning-Algorithmus ist eine dreistufige Pipeline, d.h., Während des Prunings werden nach einem bestimmten Kriterium redundante Gewichte entfernt und wichtige Gewichte beibehalten, um die Genauigkeit bestmöglich zu erhalten. Für alle von uns untersuchten strukturierten Pruning-Algorithmen, die auf dem neuesten Stand der Technik sind, ergibt die Feinabstimmung eines beschnittenen Modells nur eine vergleichbare oder schlechtere Leistung als das Training dieses Modells mit zufällig initialisierten Gewichten.Für Pruning-Algorithmen, die von einer vordefinierten Zielnetzarchitektur ausgehen, kann man die gesamte Pipeline loswerden und das Zielnetz direkt von Grund auf trainieren.Unsere Beobachtungen sind für verschiedene Netzarchitekturen, Datensätze und Aufgaben konsistent, was bedeutet, dass: 1) das Training eines großen, überparametrisierten Modells oft nicht notwendig ist, um ein effizientes Endmodell zu erhalten, 2) gelernte ``wichtige'' Gewichte des großen Modells typischerweise nicht für das kleine beschnittene Modell nützlich sind, 3) die beschnittene Architektur selbst, eher als ein Satz von vererbten ``wichtigen'' Gewichten, für die Effizienz des Endmodells entscheidender ist, was darauf hindeutet, dass in einigen Fällen Pruning als Paradigma für die Architektursuche nützlich sein kann.Unsere Ergebnisse deuten auf die Notwendigkeit sorgfältigerer Grundlagenevaluierungen in der zukünftigen Forschung über strukturierte Pruning-Methoden hin.  Wir vergleichen auch mit der "Lottery Ticket Hypothesis" (Frankle & Carbin 2019) und stellen fest, dass bei optimaler Lernrate die "winning ticket" Initialisierung, wie sie in Frankle & Carbin (2019) verwendet wird, keine Verbesserung gegenüber der zufälligen Initialisierung bringt.
Brushing-Techniken haben eine lange Geschichte mit den ersten interaktiven Auswahl-Tools in den 1990er Jahren erschienen.Seitdem wurden viele zusätzliche Techniken entwickelt, um die Auswahl Genauigkeit, Skalierbarkeit und Flexibilität Probleme zu adressieren.Auswahl ist besonders schwierig in großen Datensätzen, wo viele visuelle Elemente verwirren und Überschneidungen zu schaffen.Dieser Beitrag untersucht eine neuartige Brushing-Technik, die nicht nur auf die tatsächliche Bürsten Position, sondern auch auf die Form der gebürsteten Bereich beruht. Diese Technik umfasst zwei Arten von Vergleichsmetriken, die stückweise Pearson-Korrelation und die auf der Informationsgeometrie basierende Ähnlichkeitsmessung, und wir wenden sie in konkreten Szenarien mit Datensätzen aus der Flugsicherung, Eye-Tracking-Daten und GPS-Trajektorien an.
Generative Adversarial Networks (GANs) können Bilder von überraschender Komplexität und Realismus erzeugen, sind aber im Allgemeinen so strukturiert, dass sie von einer einzigen latenten Quelle abtasten und dabei die explizite räumliche Interaktion zwischen mehreren Objekten, die in einer Szene vorhanden sein könnten, ignorieren. Wir evaluieren unser Modell durch qualitative Experimente und Benutzerevaluationen in Szenarien, in denen entweder gepaarte oder ungepaarte Beispiele für die einzelnen Objektbilder und die gemeinsamen Szenen während des Trainings gegeben sind. Unsere Ergebnisse zeigen, dass das gelernte Modell potenzielle Interaktionen zwischen den beiden Objektdomänen erfasst, die als Eingabe gegeben sind, um neue Instanzen der komponierten Szene zur Testzeit auf angemessene Weise auszugeben.
Da jedoch einzigartige Dateneigenschaften zu unterschiedlichen und leistungsstarken Lernprinzipien inspiriert haben, zielt dieses Papier darauf ab, deren Potenziale zur Abschwächung negativer Eingaben zu erforschen. Insbesondere zeigen unsere Ergebnisse, wie wichtig es ist, die zeitliche Abhängigkeit in Audiodaten zu nutzen, um die Unterscheidungskraft gegenüber negativen Beispielen zu erhöhen. Getestet auf die automatische Spracherkennung (ASR) Aufgaben und drei aktuelle Audio adversarial Angriffe, finden wir, dass (i) Input-Transformation von Bild adversarial Verteidigung entwickelt bietet begrenzte Verbesserung der Robustheit und ist subtil zu fortgeschrittenen Angriffen; (ii) zeitliche Abhängigkeit kann genutzt werden, um diskriminierende Macht gegen Audio adversarial Beispiele zu gewinnen und ist resistent gegen adaptive Angriffe in unseren Experimenten betrachtet.Unsere Ergebnisse zeigen nicht nur vielversprechende Mittel zur Verbesserung der Robustheit von ASR-Systemen, sondern bieten auch neue Einblicke in die Nutzung domänenspezifischen Daten Eigenschaften zu mildern negativen Auswirkungen von adversarial Beispiele.
Um das berüchtigte Phänomen des Modus-Kollapses in generativen adversen Netzwerken (GANs) zu lindern, schlagen wir eine neuartige Trainingsmethode für GANs vor, bei der bestimmte gefälschte Beispiele während des Trainingsprozesses als echte Beispiele betrachtet werden können, um den Gradientenwert zu reduzieren, den der Generator in der Region erhält, in der der Gradient explodiert. Wir zeigen, dass das theoretische Gleichgewicht zwischen den Generatoren und Unterscheidungen in der Praxis nur selten erreicht werden kann und dass dies zu einer unausgewogenen generierten Verteilung führt, die von der Zielverteilung abweicht, wenn gefälschte Datenpunkte mit echten übereinstimmen, was die Nicht-Stabilität von GANs erklärt.Wir beweisen auch, dass durch die Bestrafung der Differenz zwischen den Diskriminator-Ausgängen und die Berücksichtigung bestimmter gefälschter Datenpunkte als echt für benachbarte echte und gefälschte Probenpaare das Gradientenexplodieren gemildert werden kann.Dementsprechend wird eine modifizierte GAN-Trainingsmethode mit einem stabileren Trainingsprozess und einer besseren Generalisierung vorgeschlagen.Experimente auf verschiedenen Datensätzen verifizieren unsere theoretische Analyse.
Wir stellen ein Werkzeug zur interaktiven visuellen Erkundung des latenten Raums (IVELS) für die Modellauswahl vor.  Die Evaluierung generativer Modelle für diskrete Sequenzen aus einem kontinuierlichen latenten Raum ist ein schwieriges Problem, da ihre Optimierung mehrere konkurrierende Zielterme beinhaltet.  Wir stellen eine Modellauswahl-Pipeline vor, um Modelle in aufeinanderfolgenden Phasen komplexer und teurer Metriken zu vergleichen und zu filtern. Wir präsentieren die Pipeline in einem interaktiven visuellen Tool, um die Erkundung der Metriken, die Analyse des gelernten latenten Raums und die Auswahl des besten Modells für eine bestimmte Aufgabe zu ermöglichen.  Wir konzentrieren uns speziell auf die Variations-Auto-Encoder-Familie in einer Fallstudie zur Modellierung von Peptid-Sequenzen, die kurze Sequenzen von Aminosäuren sind. Diese Aufgabe ist besonders interessant, da wir mehrere Attribute modellieren wollen. Wir zeigen, wie ein interaktiver visueller Vergleich bei der Bewertung helfen kann, wie gut ein unbeaufsichtigter Auto-Encoder die interessierenden Attribute in seinem latenten Raum erfasst.
Neuronale Netze, die durch stochastischen Gradientenabstieg (SGD) trainiert werden, gibt es seit mehr als 30 Jahren, aber sie entziehen sich immer noch unserem Verständnis.In diesem Papier wird ein experimenteller Ansatz verfolgt, mit einer Teilungs- und Eroberungsstrategie im Hinterkopf: Wir beginnen damit, zu untersuchen, was in einzelnen Neuronen passiert.Obwohl sie der Kernbaustein tiefer neuronaler Netze sind, ist die Art und Weise, wie sie Informationen über die Eingaben kodieren und wie solche Kodierungen entstehen, immer noch unbekannt.Wir berichten über Experimente, die starke Beweise dafür liefern, dass sich verborgene Neuronen während des Trainings und der Tests wie binäre Klassifizierer verhalten. Während des Trainings zeigt die Analyse der Gradienten, dass ein Neuron zwei Kategorien von Eingaben trennt, die über das gesamte Training hinweg beeindruckend konstant sind, und während des Testens zeigen wir, dass die oben beschriebene unscharfe, binäre Partition die Kerninformationen enthält, die das Netzwerk für seine Vorhersage verwendet.Diese Beobachtungen bringen einige der wichtigsten internen Mechanismen von tiefen neuronalen Netzen ans Licht und haben das Potenzial, die nächsten theoretischen und praktischen Entwicklungen zu leiten.
Obwohl die Verwendung von komplexeren und fortschrittlicheren Klassifizierern dazu beitragen kann, die Genauigkeit von Klassifizierungssystemen zu verbessern, kann dies durch die Analyse von Datensätzen und ihren Merkmalen für ein bestimmtes Problem geschehen.Die Kombination von Merkmalen ist diejenige, die die Qualität der Merkmale verbessern kann.In diesem Papier wird eine Struktur ähnlich dem Feed-Forward Neural Network (FFNN) verwendet, um eine optimierte lineare oder nicht-lineare Kombination von Merkmalen für die Klassifizierung zu erzeugen.Genetischer Algorithmus (GA) wird angewendet, um Gewichte und Verzerrungen zu aktualisieren. Da sich die Art der Datensätze und ihre Merkmale auf die Effektivität der Kombination und des Klassifikationssystems auswirken, werden lineare und nichtlineare Aktivierungsfunktionen (oder Übertragungsfunktionen) verwendet, um ein zuverlässigeres System zu erreichen.Experimente mit verschiedenen UCI-Datensätzen und die Verwendung des Minimal Distance Classifier als einfacher Klassifikator zeigen, dass die vorgeschlagene lineare und nichtlineare intelligente FFNN-basierte Merkmalskombination zuverlässigere und vielversprechendere Ergebnisse liefern kann.Durch die Verwendung einer solchen Merkmalskombination ist es nicht mehr notwendig, einen leistungsfähigeren und komplexeren Klassifikator zu verwenden.
Jüngste Verbesserungen bei Generative Adversarial Networks (GANs) haben es möglich gemacht, realistische Bilder in hoher Auflösung zu erzeugen, die auf natürlichsprachlichen Beschreibungen wie Bildunterschriften basieren. Dies gilt insbesondere für Bilder, die mehrere unterschiedliche Objekte an verschiedenen räumlichen Positionen enthalten sollen.Wir stellen einen neuen Ansatz vor, der es uns erlaubt, die Position von beliebig vielen Objekten innerhalb eines Bildes zu kontrollieren, indem wir sowohl dem Generator als auch dem Diskriminator einen Objektpfad hinzufügen. Der Objektpfad konzentriert sich ausschließlich auf die einzelnen Objekte und wird iterativ an den durch die Boundingboxen festgelegten Stellen angewendet. Der globale Pfad konzentriert sich auf den Bildhintergrund und das allgemeine Bildlayout. Wir führen Experimente mit dem Multi-MNIST-, dem CLEVR- und dem komplexeren MS-COCO-Datensatz durch. Unsere Experimente zeigen, dass wir durch die Verwendung des Objektpfads die Objektpositionen innerhalb von Bildern kontrollieren und komplexe Szenen mit mehreren Objekten an verschiedenen Positionen modellieren können. Wir zeigen außerdem, dass sich der Objektpfad auf die einzelnen Objekte konzentriert und für diese relevante Merkmale lernt, während sich der globale Pfad auf globale Bildmerkmale und den Bildhintergrund konzentriert.
Die Nachfrage nach abstrakten Zusammenfassungen von Dialogen wächst in der realen Welt.z.B. möchten Kundendienstzentren oder Krankenhäuser die Interaktion zwischen Kunden und Arzt und Patient zusammenfassen.jedoch haben nur wenige Forscher die abstrakte Zusammenfassung von Dialogen erforscht, da es an geeigneten Datensätzen mangelt.wir schlagen einen abstrakten Dialogzusammenfassungsdatensatz vor, der auf der MultiWOZ basiert.wenn wir bisherige State-of-the-Art-Methoden der Dokumentenzusammenfassung direkt auf Dialoge anwenden, gibt es zwei wesentliche Nachteile: die informativen Entitäten wie Restaurantnamen sind schwer zu erhalten, und die Inhalte aus verschiedenen Dialogdomänen sind manchmal nicht aufeinander abgestimmt. Um diese beiden Nachteile zu beheben, schlagen wir ein Scaffold Pointer Network (SPNet) vor, das die vorhandenen Annotationen zur Sprecherrolle, zum semantischen Slot und zur Dialogdomäne nutzt und diese semantischen Gerüste für die Dialogzusammenfassung einbezieht.Da ROUGE die beiden genannten Nachteile nicht erfassen kann, schlagen wir auch eine neue Bewertungsmetrik vor, die kritische informative Entitäten im Text berücksichtigt.Auf MultiWOZ übertrifft das von uns vorgeschlagene SPNet die modernsten abstrakten Zusammenfassungsmethoden bei allen automatischen und menschlichen Bewertungsmetriken.
Wissensdatenbanken (KB), sowohl automatisch als auch manuell erstellt, sind oft unvollständig --- viele gültige Fakten können aus der KB durch die Synthese vorhandener Informationen abgeleitet werden.Ein beliebter Ansatz zur Vervollständigung von KB ist die Ableitung neuer Beziehungen durch kombinatorische Schlussfolgerungen über die Informationen, die entlang anderer Pfade gefunden wurden, die ein Paar von Entitäten verbinden. Angesichts der enormen Größe von KBs und der exponentiellen Anzahl von Pfaden haben bisherige pfadbasierte Modelle nur das Problem der Vorhersage einer fehlenden Beziehung zwischen zwei Entitäten oder der Bewertung des Wahrheitsgehalts eines vorgeschlagenen Tripels berücksichtigt. Wir schlagen einen neuen Algorithmus vor, MINERVA, der sich mit der viel schwierigeren und praktischeren Aufgabe befasst, Fragen zu beantworten, bei denen die Beziehung bekannt ist, aber nur eine Entität.Da zufällige Spaziergänge in einer Umgebung mit unbekanntem Ziel und kombinatorisch vielen Pfaden von einem Startknoten aus unpraktisch sind, stellen wir einen neuronalen Verstärkungslernansatz vor, der lernt, wie man durch den Graphen navigiert, der von der Eingabeanfrage abhängt, um prädiktive Pfade zu finden.Bei einer umfassenden Evaluierung mit sieben Wissensdatenbanken haben wir festgestellt, dass MINERVA mit vielen aktuellen State-of-the-Art-Methoden konkurrenzfähig ist.
Es gibt viele Unterschiede zwischen Faltungsnetzwerken und den ventralen visuellen Strömen von Primaten. Standard-Faltungsnetzwerken fehlen beispielsweise rekurrente und laterale Verbindungen, Zelldynamik usw. Ihre Feedforward-Architekturen ähneln jedoch in gewisser Weise dem ventralen Strom und rechtfertigen einen detaillierteren Vergleich. In einer kürzlich durchgeführten Studie wurde festgestellt, dass die Feedforward-Architektur des visuellen Kortex einem Faltungsnetzwerk sehr ähnlich ist, dass sich die resultierende Architektur jedoch in mehrfacher Hinsicht von den weit verbreiteten tiefen Netzwerken unterscheidet.1 In derselben Studie wurde auch festgestellt, dass das Training des ventralen Stroms dieses Netzwerks für die Objekterkennung überraschenderweise zu einer schlechten Leistung führt.2 In dieser Arbeit wird die Leistung dieses Netzwerks genauer untersucht. Ich habe insbesondere eine Reihe von Änderungen an der auf dem ventralen Strom basierenden Architektur vorgenommen, um sie einem DenseNet ähnlicher zu machen, und die Leistung bei jedem Schritt getestet. Ich habe mich für das DenseNet entschieden, weil es einen hohen BrainScore hat und weil es einige Kortex-ähnliche architektonische Merkmale aufweist, wie z. B. große In-Grade und lange Skip-Verbindungen.Die meisten Änderungen (die das Kortex-ähnliche Netzwerk dem DenseNet ähnlicher machten) verbesserten die Leistung. Eine Möglichkeit ist, dass Details der Architektur des ventralen Stroms schlecht für Feedforward-Berechnungen, einfache Verarbeitungseinheiten und/oder Backpropagation geeignet sind, was auf Unterschiede zwischen der Art und Weise, wie leistungsstarke tiefe Netzwerke und das Gehirn die zentrale Objekterkennung angehen, hinweisen könnte.
Beim Verstärkungslernen ist es üblich, einen Agenten für eine bestimmte Zeit mit seiner Umgebung interagieren zu lassen, bevor die Umgebung zurückgesetzt und der Prozess in einer Reihe von Episoden wiederholt wird. Die Aufgabe, die der Agent zu lernen hat, kann entweder darin bestehen, seine Leistung über(i) diese feste Zeitspanne oder(ii) einen unbestimmten Zeitraum zu maximieren, wobei das Zeitlimit nur während des Trainings verwendet wird. Im ersten Fall argumentieren wir, dass die Abbrüche aufgrund von Zeitlimits tatsächlich Teil der Umgebung sind, und schlagen vor, einen Begriff der verbleibenden Zeit als Teil des Agenteninputs aufzunehmen. Wir argumentieren, dass solche Abbrüche nicht als Teil der Umwelt behandelt werden sollten und schlagen eine Methode vor, die speziell für wertbasierte Algorithmen geeignet ist und diese Einsicht berücksichtigt, indem sie am Ende jeder Teilepisode einen Bootstrap durchführt.um die Bedeutung unserer Vorschläge zu veranschaulichen, führen wir mehrere Experimente in einer Reihe von Umgebungen durch, die von einfachen Übergangsgraphen mit wenigen Zuständen bis hin zu komplexen Kontrollaufgaben reichen, einschließlich neuartiger und Standard-Benchmark-Domänen.unsere Ergebnisse zeigen, dass die vorgeschlagenen Methoden die Leistung und Stabilität bestehender Verstärkungslernalgorithmen verbessern.
Obwohl der stochastische Gradientenabstieg (SGD) eine treibende Kraft hinter dem jüngsten Erfolg des Deep Learning ist, ist unser Verständnis seiner Dynamik in einem hochdimensionalen Parameterraum begrenzt.In den letzten Jahren haben einige Forscher die Stochastizität von Minibatch-Gradienten oder das Signal-Rausch-Verhältnis verwendet, um die Lerndynamik von SGD besser zu charakterisieren.Inspiriert von diesen Arbeiten analysieren wir hier SGD aus einer geometrischen Perspektive, indem wir die Stochastizität der Normen und Richtungen der Minibatch-Gradienten untersuchen. Wir schlagen ein Modell der Richtungskonzentration für Minibatch-Gradienten durch von Mises-Fisher (VMF)-Verteilung vor und zeigen, dass die Richtungsgleichförmigkeit von Minibatch-Gradienten im Verlauf von SGD zunimmt. Wir überprüfen unser Ergebnis empirisch mit tiefen Faltungsnetzwerken und beobachten eine höhere Korrelation zwischen der Gradientenstochastizität und der vorgeschlagenen Richtungsgleichförmigkeit als die gegen die Gradientennormstochastizität, was darauf hindeutet, dass die Richtungsstatistik von Minibatch-Gradienten ein wichtiger Faktor hinter SGD ist.
Um die Stabilität zu maximieren, analysieren und entwickeln wir eine rechnerisch effiziente Implementierung der jakobianischen Regularisierung, die die Klassifizierungsmargen neuronaler Netze erhöht. Der stabilisierende Effekt des jakobianischen Regularisierers führt zu einer signifikanten Verbesserung der Robustheit, sowohl gegenüber zufälligen als auch gegenüber ungünstigen Eingangsstörungen, ohne die Generalisierungseigenschaften bei sauberen Daten stark zu beeinträchtigen.
Mit der zunehmenden Nachfrage nach dem Einsatz von Faltungsneuronalen Netzen (CNNs) auf mobilen Plattformen wurde der Sparse-Kernel-Ansatz vorgeschlagen, der mehr Parameter als die Standardfaltung einsparen kann, während die Genauigkeit beibehalten wird, aber trotz des großen Potenzials hat keine frühere Forschung aufgezeigt, wie ein Sparse-Kernel-Design mit einem solchen Potenzial (d.h., In der Zwischenzeit aufgrund der großen Design-Raum ist es auch unmöglich, alle Kombinationen von bestehenden sparse kernels.In diesem Papier, wir sind die ersten auf dem Gebiet zu prüfen, wie eine effektive sparse kernel Design durch die Beseitigung der großen design space.Specifically, präsentieren wir eine sparse kernel Schema zu veranschaulichen, wie man den Raum von drei Aspekten zu reduzieren.First, in Bezug auf die Zusammensetzung entfernen wir Designs, die aus wiederholten Schichten. Zweitens, um Designs mit großer Genauigkeit Verschlechterung zu entfernen, finden wir eine einheitliche Eigenschaft namens~\emph{Informationsfeld} hinter verschiedenen spärlichen Kernel-Designs, die direkt die endgültige Genauigkeit anzeigen könnte.Schließlich entfernen wir Designs in zwei Fällen, in denen eine bessere Parameter-Effizienz erreicht werden könnte.Darüber hinaus bieten wir detaillierte Effizienz-Analyse auf den letzten 4 Designs in unserem Schema.Experimentelle Ergebnisse validieren die Idee unseres Schemas, indem sie zeigen, dass unser Schema in der Lage ist, Designs zu finden, die effizienter bei der Verwendung von Parametern und Berechnung mit ähnlichen oder höheren Genauigkeit sind.
In schwach überwachte temporale Aktion Lokalisierung, haben frühere Arbeiten nicht zu dichten und integralen Regionen für jede gesamte Aktion aufgrund der Überschätzung der auffälligsten Regionen zu lokalisieren.Um dieses Problem zu lindern, schlagen wir eine marginalisierte durchschnittliche Aufmerksamkeits-Netzwerk (MAAN), um die dominante Reaktion der auffälligsten Regionen in einer prinzipiellen way.The MAAN unterdrücken beschäftigt eine neuartige marginalisierte durchschnittliche Aggregation (MAA) Modul und lernt eine Reihe von latenten diskriminierende Wahrscheinlichkeiten in einem Ende-zu-Ende-Mode. MAA nimmt mehrere Teilmengen aus den Videoschnipselmerkmalen gemäß einem Satz latenter Unterscheidungswahrscheinlichkeiten und nimmt die Erwartung über alle gemittelten Teilmengenmerkmale. Theoretisch beweisen wir, dass das MAA-Modul mit erlernten latenten Unterscheidungswahrscheinlichkeiten den Unterschied in den Antworten zwischen den auffälligsten Regionen und den anderen erfolgreich reduziert. Daher ist MAAN in der Lage, bessere Klassenaktivierungssequenzen zu erzeugen und dichte und integrale Aktionsregionen in den Videos zu identifizieren. Darüber hinaus schlagen wir einen schnellen Algorithmus vor, um die Komplexität der Konstruktion von MAA von $O(2^T)$ auf $O(T^2)$ zu reduzieren. Ausführliche Experimente an zwei großen Videodatensätzen zeigen, dass unser MAAN eine überlegene Leistung bei der schwach überwachten zeitlichen Handlungslokalisierung erzielt.
Deep Image Prior (DIP), das eine tiefe Faltungsnetzwerkstruktur (ConvNet) selbst als Bildprior verwendet, hat in der Computer-Vision-Gemeinschaft große Aufmerksamkeit auf sich gezogen.  Es zeigt empirisch die Effektivität der ConvNet-Struktur für verschiedene Anwendungen der Bildwiederherstellung.  Der vorgeschlagene Ansatz unterteilt die Faltung in ``delay-embedding'' und ``transformation (\ie encoder-decoder)'' und schlägt eine einfache, aber essentielle Bild/Tensor-Modellierungsmethode vor, die eng mit dynamischen Systemen und Selbstähnlichkeit verbunden ist. Trotz seiner Einfachheit sind die Bild-/Tensormodellierungs- und Superauflösungsergebnisse von MMES in unseren umfangreichen Experimenten dem DIP sehr ähnlich, ja sogar konkurrenzfähig, und diese Ergebnisse würden uns helfen, das DIP aus der Perspektive des "niedrigdimensionalen Patch-Manifold-Prior" neu zu interpretieren und zu charakterisieren.
Föderiertes Lernen ist ein neuerer Fortschritt beim Schutz der Privatsphäre. In diesem Zusammenhang aggregiert ein vertrauenswürdiger Kurator Parameter, die dezentral von mehreren Klienten optimiert wurden. Das resultierende Modell wird dann an alle Klienten zurückgegeben und konvergiert schließlich zu einem gemeinsamen repräsentativen Modell, ohne dass die Daten explizit geteilt werden müssen. Das Protokoll ist jedoch anfällig für differenzielle Angriffe, die von jeder Partei ausgehen können, die während der föderierten Optimierung einen Beitrag leistet. Bei einem solchen Angriff werden der Beitrag eines Kunden während des Trainings und Informationen über seinen Datensatz durch die Analyse des verteilten Modells aufgedeckt. Das Ziel ist es, die Beiträge der Kunden während des Trainings zu verbergen und dabei einen Kompromiss zwischen dem Verlust der Privatsphäre und der Leistung des Modells zu finden. Empirische Studien deuten darauf hin, dass unser vorgeschlagenes Verfahren bei einer ausreichend großen Anzahl von teilnehmenden Clients die differentielle Privatsphäre auf Client-Ebene aufrechterhalten kann, bei nur geringen Kosten für die Modellleistung.
Der Einsatz von tiefen neuronalen Netzen als natürliche Bildprioritäten zur Lösung inverser Probleme erfordert entweder große Datenmengen, um ausreichend aussagekräftige generative Modelle zu trainieren, oder kann mit untrainierten neuronalen Netzen auch ohne Daten erfolgreich sein, aber nur sehr wenige Arbeiten haben sich mit der Frage beschäftigt, wie man zwischen diesen Regimen ohne und mit großen Datenmengen interpolieren kann. Im Vergleich zu untrainierten neuronalen Netzen, die keine Daten verwenden, zeigen wir, wie man ein neuronales Netz mit einigen wenigen Beispielen vortrainieren kann, um die Rekonstruktionsergebnisse bei der komprimierten Abtastung und bei semantischen Bildwiederherstellungsproblemen wie der Einfärbung zu verbessern. Unser Ansatz führt zu einer verbesserten Rekonstruktion, wenn die Menge der verfügbaren Daten zunimmt, und ist gleichwertig mit vollständig trainierten generativen Modellen, während weniger als 1 % der Daten benötigt werden, um ein generatives Modell zu trainieren.
Wir schlagen Cooperative Training (CoT) vor, um generative Modelle zu trainieren, die eine vertretbare Dichte für diskrete Daten messen. CoT trainiert koordiniert einen Generator G und einen zusätzlichen prädiktiven Mediator M. Das Trainingsziel von M ist es, eine Mischdichte der gelernten Verteilung G und der Zielverteilung P zu schätzen, und das von G ist es, die durch M geschätzte Jensen-Shannon-Divergenz zu minimieren. Dieser Algorithmus mit geringer Varianz ist theoretisch sowohl bei der Generierung von Stichproben als auch bei der Likelihood-Vorhersage überlegen, und wir zeigen theoretisch und empirisch die Überlegenheit von CoT gegenüber den meisten früheren Algorithmen in Bezug auf die generative Qualität und Diversität, die Fähigkeit zur prädiktiven Generalisierung und die Rechenkosten.
Wir stellen eine Methode zur Berechnung einer intrinsischen Belohnung für Neugier vor, die Metriken verwendet, die aus dem Sampling eines latenten Variablenmodells abgeleitet werden, das zur Schätzung der Dynamik verwendet wird. In unseren Experimenten verwendet ein Videospiel-Agent unser Modell, um autonom zu lernen, wie man Atari-Spiele spielt, indem er unsere Neugier-Belohnung in Kombination mit extrinsischen Belohnungen aus dem Spiel verwendet, um eine bessere Leistung bei Spielen mit spärlichen extrinsischen Belohnungen zu erzielen. Wenn Stochastizität in die Umgebung eingeführt wird, zeigt unsere Methode immer noch eine bessere Leistung als die Grundlinie.
Word Embedding ist ein leistungsfähiges Werkzeug in der natürlichen Sprache processing.In diesem Papier betrachten wir das Problem der Einbettung Zusammensetzung \--- gegebenen Vektor Darstellungen von zwei Wörtern, berechnen Sie einen Vektor für die gesamte phrase.We geben ein generatives Modell, das spezifische syntaktische Beziehungen zwischen Wörtern erfassen kann.Under unser Modell, wir beweisen, dass die Korrelationen zwischen drei Wörtern (gemessen durch ihre PMI) bilden einen Tensor, der eine ungefähre niedrigen Rang Tucker decomposition hat. Das Ergebnis der Tucker-Zerlegung liefert sowohl die Worteinbettungen als auch einen Kerntensor, der verwendet werden kann, um bessere Kompositionen der Worteinbettungen zu erzeugen.
Wir schlagen einen hybriden modellbasierten und modellfreien Ansatz vor, LEArning and Planning with Semantics (LEAPS), bestehend aus einer Multi-Target Sub-Policy, die auf visuelle Inputs reagiert, und einem Bayes'schen Modell über semantische Strukturen. Wenn der Agent in einer unbekannten Umgebung platziert wird, plant er mit dem semantischen Modell, um Entscheidungen auf hoher Ebene zu treffen, schlägt das nächste Unterziel für die auszuführende Unterpolitik vor und aktualisiert das semantische Modell auf der Grundlage neuer Beobachtungen. Wir führen Experimente mit visuellen Navigationsaufgaben unter Verwendung von House3D durch, einer 3D-Umgebung, die verschiedene, von Menschen gestaltete Innenraumszenen mit realen Objekten enthält.
In diesem Papier konzentrieren wir uns auf zwei Herausforderungen, die das Versprechen der dünn besetzten Signaldarstellung, -erfassung und -wiederherstellung zunichte machen: Erstens können reale Signale selten als perfekt dünn besetzte Vektoren in einer bekannten Basis beschrieben werden, und die traditionell verwendeten Zufallsmessverfahren sind selten optimal für ihre Erfassung, und zweitens sind die vorhandenen Algorithmen zur Signalwiederherstellung in der Regel nicht schnell genug, um sie auf Echtzeitprobleme anwendbar zu machen. Für die erste Herausforderung stellen wir das Problem der Suche nach informativen Messungen mit Hilfe einer Maximum-Likelihood-Formulierung (ML) dar und zeigen, wie wir ein datengesteuertes Dimensionalitätsreduktionsprotokoll für die Erfassung von Signalen unter Verwendung von Faltungsarchitekturen erstellen können.Für die zweite Herausforderung diskutieren und analysieren wir ein neuartiges Parallelisierungsschema und zeigen, dass es den Signalwiederherstellungsprozess signifikant beschleunigt.Wir demonstrieren die signifikante Verbesserung unserer Methode gegenüber konkurrierenden Methoden durch eine Reihe von Experimenten.
Um effektive Aktionen in komplexen Umgebungen auszuwählen, müssen intelligente Agenten aus vergangenen Erfahrungen verallgemeinern.Weltmodelle können Wissen über die Umgebung darstellen, um eine solche Verallgemeinerung zu erleichtern.Während das Lernen von Weltmodellen aus hochdimensionalen sensorischen Eingaben durch Deep Learning möglich wird, gibt es viele potenzielle Wege, um Verhaltensweisen aus ihnen abzuleiten. Wir stellen Dreamer vor, einen Reinforcement-Learning-Agenten, der Aufgaben mit langem Zeithorizont rein durch latente Vorstellungskraft löst. Wir lernen Verhaltensweisen effizient durch Backpropagating analytischer Gradienten gelernter Zustandswerte durch Trajektorien, die im kompakten Zustandsraum eines gelernten Weltmodells imaginiert werden. Bei 20 anspruchsvollen visuellen Steuerungsaufgaben übertrifft Dreamer bestehende Ansätze in Bezug auf Dateneffizienz, Rechenzeit und endgültige Leistung.
Transfer Reinforcement Learning (RL) zielt darauf ab, die Lerneffizienz eines Agenten zu verbessern, indem Wissen von anderen Quellagenten genutzt wird, die für relevante Aufgaben trainiert wurden.Allerdings bleibt es eine Herausforderung, Wissen zwischen verschiedenen Umgebungsdynamiken zu übertragen, ohne Zugang zu den Quellumgebungen zu haben.In dieser Arbeit erforschen wir eine neue Herausforderung im Transfer-RL, bei der nur ein Satz von Quellpolitiken, die unter unbekannten verschiedenen Dynamiken gesammelt wurden, für das effiziente Lernen einer Zielaufgabe zur Verfügung steht.Um dieses Problem zu lösen, umfasst der vorgeschlagene Ansatz, MULTI-source POLicy AggRegation (MULTIPOLAR), zwei Schlüsseltechniken. Wir lernen, die von den Quellpolitiken bereitgestellten Aktionen adaptiv zu aggregieren, um die Leistung der Zielaufgabe zu maximieren, und lernen gleichzeitig ein Hilfsnetzwerk, das die Residuen um die aggregierten Aktionen herum vorhersagt, was die Aussagekraft der Zielpolitik sicherstellt, selbst wenn einige der Quellpolitiken schlecht abschneiden.Wir haben die Effektivität von MULTIPOLAR durch eine umfangreiche experimentelle Evaluierung in sechs simulierten Umgebungen demonstriert, die von klassischen Steuerungsproblemen bis hin zu anspruchsvollen Robotersimulationen reichen, sowohl in kontinuierlichen als auch in diskreten Aktionsräumen.
Die Algorithmen des Verstärkungslernens stützen sich auf sorgfältig entwickelte Belohnungen aus der Umgebung, die dem Agenten fremd sind. Es ist jedoch schwierig und nicht skalierbar, jede Umgebung mit von Hand entworfenen, dichten Belohnungen zu versehen, so dass es notwendig ist, Belohnungsfunktionen zu entwickeln, die dem Agenten innewohnen. In dieser Arbeit:(a) Wir führen die erste groß angelegte Studie über rein neugiergetriebenes Lernen durch, d.h. {\em ohne jegliche extrinsische Belohnungen}, über $54$ Standard-Benchmark-Umgebungen, einschließlich der Atari-Spielesuite.Unsere Ergebnisse zeigen eine überraschend gute Leistung sowie einen hohen Grad an Übereinstimmung zwischen dem intrinsischen Neugier-Ziel und den von Hand entworfenen extrinsischen Belohnungen vieler Spiele. (b) Wir untersuchen die Auswirkung der Verwendung verschiedener Merkmalsräume für die Berechnung des Vorhersagefehlers und zeigen, dass zufällige Merkmale für viele populäre RL-Spiel-Benchmarks ausreichend sind, aber gelernte Merkmale scheinen besser zu generalisieren (z.B. für neuartige Spielstufen in Super Mario Bros.).(c) Wir demonstrieren die Grenzen der vorhersagebasierten Belohnungen in stochastischen Konstellationen. Spielvideos und Code sind unter https://doubleblindsupplementary.github.io/large-curiosity/.
Diese Arbeit liefert theoretische und empirische Beweise dafür, dass invarianzinduzierende Regularisierer die Vorhersagegenauigkeit für räumliche Transformationen im schlimmsten Fall erhöhen können (räumliche Robustheit).  Anhand dieser nachteilig transformierten Beispiele zeigen wir, dass das Hinzufügen von Regularisierungen zum Standard- oder nachteiligen Training den relativen Fehler für CIFAR10 um 20 % reduziert, ohne die Rechenkosten zu erhöhen.  Darüber hinaus stellen wir für SVHN, das bekanntermaßen eine inhärente Varianz in der Orientierung aufweist, fest, dass robustes Training auch die Standardgenauigkeit auf dem Testset verbessert.
Wir schlagen Ordnungslernen vor, um den Ordnungsgraphen von Klassen zu bestimmen, der Ränge oder Prioritäten darstellt, und eine Objektinstanz in eine der Klassen zu klassifizieren. Zu diesem Zweck entwerfen wir einen paarweisen Komparator, um die Beziehung zwischen zwei Instanzen in einen von drei Fällen zu kategorisieren: eine Instanz ist "größer als", "ähnlich" oder "kleiner als" die andere. Durch den Vergleich einer Eingabeinstanz mit Referenzinstanzen und die Maximierung der Konsistenz zwischen den Vergleichsergebnissen kann die Klasse der Eingabeinstanz zuverlässig geschätzt werden. Wir wenden das Ordnungslernen an, um einen Schätzer für das Gesichtsalter zu entwickeln, der die modernste Leistung bietet.
Wir untersuchen, wie sich die Topologie eines Datensatzes, der aus zwei Komponenten besteht, die zwei Klassen von Objekten in einem binären Klassifizierungsproblem repräsentieren, ändert, wenn er die Schichten eines gut trainierten neuronalen Netzes durchläuft, d.h., Das Ziel ist es, Licht auf zwei bekannte Rätsel in tiefen neuronalen Netzen zu werfen:(i) eine nicht glatte Aktivierungsfunktion wie ReLU übertrifft eine glatte wie hyperbolischer Tangens;(ii) erfolgreiche neuronale Netzarchitekturen sind auf viele Schichten angewiesen, trotz der Tatsache, dass ein flaches Netz in der Lage ist, jede Funktion beliebig gut zu approximieren.Wir haben umfangreiche Experimente zur persistenten Homologie einer Reihe von Punktwolken-Datensätzen durchgeführt.Die Ergebnisse zeigen durchweg das Folgende: (1) Neuronale Netze arbeiten, indem sie die Topologie ändern und einen topologisch komplizierten Datensatz in einen topologisch einfachen umwandeln, während er die Schichten durchläuft. egal wie kompliziert die Topologie des Datensatzes ist, mit dem wir beginnen, wenn er ein gut trainiertes neuronales Netz durchläuft, reduzieren sich die Betti-Zahlen beider Komponenten ausnahmslos auf ihre niedrigsten möglichen Werte: die nullte Betti-Zahl ist eins und alle höheren Betti-Zahlen sind null. Darüber hinaus (2) ist die Reduktion der Betti-Zahlen bei der ReLU-Aktivierung deutlich schneller als bei der hyperbolischen Tangenten-Aktivierung --- was damit zusammenhängt, dass erstere nicht-homomorphe Karten (die die Topologie verändern) definieren, während letztere homöomorphe Karten (die die Topologie erhalten) definieren. Schließlich (3) verarbeiten flache und tiefe Netze denselben Datensatz unterschiedlich --- ein flaches Netz arbeitet hauptsächlich durch Änderung der Geometrie und ändert die Topologie nur in seinen letzten Schichten, ein tiefes Netz verteilt die topologischen Änderungen gleichmäßiger über alle seine Schichten.
Die Konvergenzrate und die endgültige Leistung gängiger Deep-Learning-Modelle haben erheblich von kürzlich vorgeschlagenen Heuristiken wie Lernratenplänen, Wissensdestillation, Überspringen von Verbindungen und Normalisierungsschichten profitiert. Da es keine theoretische Grundlage gibt, können kontrollierte Experimente zur Erklärung der Wirksamkeit dieser Strategien unser Verständnis von Deep-Learning-Landschaften und der Trainingsdynamik verbessern. Stattdessen betrachten wir die empirische Analyse von Heuristiken durch die Linse der kürzlich vorgeschlagenen Methoden für Verlustoberflächen- und Repräsentationsanalyse, d.h. Moduskonnektivität und kanonische Korrelationsanalyse (CCA), und stellen Hypothesen über die Gründe für den Erfolg der Heuristiken auf.Insbesondere untersuchen wir die Wissensdestillation und die Lernratenheuristiken von (Kosinus-)Neustarts und Warmup unter Verwendung von Moduskonnektivität und CCA. Unsere empirische Analyse legt nahe, dass: (a) die häufig angeführten Gründe für den Erfolg von Cosinus Annealing in der Praxis nicht belegt sind; (b) dass der Effekt des Warmups der Lernrate darin besteht, dass die tieferen Schichten keine Trainingsinstabilität erzeugen; und(c) dass das vom Lehrer geteilte latente Wissen hauptsächlich in den tieferen Schichten verteilt ist.
Während die meisten Methoden einen Quantisierungsschritt beinhalten, schlagen wir einen prinzipiellen Bayes'schen Ansatz vor, bei dem wir zunächst eine Verteilung über einen diskreten Gewichtsraum ableiten, aus dem wir anschließend hardwarefreundliche NNs mit niedriger Präzision ableiten. Zu diesem Zweck führen wir einen probabilistischen Vorwärtspass ein, um das schwierige Variationsziel zu approximieren, das es uns ermöglicht, über diskret bewertete Gewichtsverteilungen für NNs mit Vorzeichen-Aktivierungsfunktionen zu optimieren.In unseren Experimenten zeigen wir, dass unser Modell auf mehreren realen Datensätzen den Stand der Technik erreicht.
In den letzten Jahren wurde eine Vielzahl von Graph Neural Networks (GNNs) erfolgreich für das Repräsentationslernen und die Vorhersage auf solchen Graphen angewandt. In vielen Anwendungen ändert sich jedoch der zugrundeliegende Graph im Laufe der Zeit und die existierenden GNNs sind für den Umgang mit solchen dynamischen Graphen unzureichend.In diesem Papier schlagen wir eine neuartige Technik für das Lernen von Einbettungen dynamischer Graphen vor, die auf einem Tensor Algebra Rahmen basiert. Unsere Methode erweitert das populäre Graph Convolutional Network (GCN) für das Lernen von Repräsentationen dynamischer Graphen unter Verwendung der kürzlich vorgeschlagenen Tensor-M-Produkt-Technik.Theoretische Ergebnisse, die die Verbindung zwischen dem vorgeschlagenen Tensor-Ansatz und der spektralen Faltung von Tensoren herstellen, werden entwickelt.Numerische Experimente auf realen Datensätzen demonstrieren die Nützlichkeit der vorgeschlagenen Methode für eine Kantenklassifikationsaufgabe auf dynamischen Graphen.
Unsere Hauptmotivation ist es, einen effizienten Ansatz zur Erzeugung neuartiger stabiler chemischer Verbindungen mit mehreren Elementen vorzuschlagen, die in realen Anwendungen verwendet werden können.Diese Aufgabe kann als kombinatorisches Problem formuliert werden und erfordert viele Stunden menschlicher Experten, um neue Daten zu konstruieren und zu bewerten.Unüberwachte Lernmethoden wie Generative Adversarial Networks (GANs) können effizient zur Erzeugung neuer Daten verwendet werden.  Im Bereich der Materialwissenschaften besteht jedoch die Notwendigkeit, Daten mit höherer Komplexität im Vergleich zu den beobachteten Proben zu synthetisieren, und die modernen domänenübergreifenden GANs können nicht direkt angepasst werden. In diesem Beitrag schlagen wir ein neuartiges GAN namens CrystalGAN vor, das neue chemisch stabile kristallografische Strukturen mit erhöhter Domänenkomplexität generiert. Wir stellen eine originelle Architektur vor, liefern die entsprechenden Verlustfunktionen und zeigen, dass das CrystalGAN sehr vernünftige Daten erzeugt.
In einem hochdimensionalen Regime hat jeder Parameter seine eigene Struktur, wie z.B. Sparsity oder Group Sparsity. In diesem Papier betrachten wir die allgemeine Form der Datenanreicherung, bei der die Daten aus einer festen, aber beliebigen Anzahl von Aufgaben $G$ stammen und jede konvexe Funktion, z.B. die Norm, die Struktur sowohl der gemeinsamen als auch der individuellen Parameter charakterisieren kann. 	Wir schlagen einen Schätzer für das hochdimensionale datenangereicherte Modell vor und untersuchen seine statistischen Eigenschaften.  Wir beschreiben die Stichprobenkomplexität unseres Schätzers und liefern eine nicht-asymptotische Schranke mit hoher Wahrscheinlichkeit für den Schätzfehler aller Parameter unter einer Bedingung, die schwächer ist als der Stand der Technik, und schlagen einen iterativen Schätzalgorithmus mit einer geometrischen Konvergenzrate vor. 	
Autonome Fahrzeuge werden im Stadtverkehr immer häufiger eingesetzt.  Unternehmen werden beginnen, einen Bedarf zu finden, um diese Fahrzeuge intelligente Stadtflottenkoordination zu lehren.  Wir glauben, dass komplexes intelligentes Verhalten von diesen Agenten durch Reinforcement Learning erlernt werden kann. In diesem Papier diskutieren wir unsere Arbeit zur Lösung dieses Systems durch die Anpassung des Deep Q-Learning (DQN) Modells an die Multi-Agenten-Situation.  Unser Ansatz wendet Deep Reinforcement Learning an, indem er Faltungsneuronale Netze mit DQN kombiniert, um den Agenten beizubringen, die Kundennachfrage in einer Umgebung zu erfüllen, die für sie teilweise beobachtbar ist.Wir zeigen auch, wie wir Transfer Learning einsetzen, um den Agenten beizubringen, mehrere Ziele auszubalancieren, wie z.B. zu einer Ladestation zu navigieren, wenn das Energieniveau niedrig ist.Die beiden vorgestellten Evaluierungen zeigen, dass unsere Lösung erfolgreich in der Lage ist, den Agenten Kooperationsstrategien beizubringen und dabei mehrere Ziele auszubalancieren.
In vielen Anwendungen ist der natürliche Begriff der Stabilität geometrisch, wie z.B. in der Computer Vision.Scattering Transformationen konstruieren tiefe Faltungsrepräsentationen, die als stabil gegenüber Verformungen der Eingabedaten zertifiziert sind.Diese Stabilität gegenüber Verformungen kann als Stabilität in Bezug auf Änderungen in der metrischen Struktur des Bereichs interpretiert werden. In dieser Arbeit zeigen wir, dass Scattering-Transformationen mit Hilfe von Diffusions-Wavelets auf nicht-euklidische Domänen verallgemeinert werden können, wobei ein Begriff der Stabilität in Bezug auf metrische Veränderungen in der Domäne, gemessen mit Diffusionskarten, erhalten bleibt.
Wir schlagen eine neuartige Deep-Network-Architektur für lebenslanges Lernen vor, die wir als dynamisch erweiterbares Netzwerk (DEN) bezeichnen, das dynamisch über seine Netzwerkkapazität entscheiden kann, während es auf einer Sequenz von Aufgaben trainiert, um eine kompakte, überlappende Struktur des Wissensaustauschs zwischen den Aufgaben zu erlernen.DEN wird effizient online trainiert, indem es selektives Retraining durchführt, die Netzwerkkapazität bei Ankunft jeder Aufgabe dynamisch mit nur der notwendigen Anzahl von Einheiten erweitert und semantische Drift durch Aufteilung/Duplizierung von Einheiten und deren Zeitstempel effektiv verhindert. Wir validieren DEN auf mehreren öffentlichen Datensätzen in Szenarien des lebenslangen Lernens, auf denen es nicht nur die bestehenden Methoden des lebenslangen Lernens für tiefe Netzwerke deutlich übertrifft, sondern auch das gleiche Leistungsniveau wie das Batch-Modell mit einer wesentlich geringeren Anzahl von Parametern erreicht.
Wir zeigen, dass die Verzerrungen, die dem visuellen Odometrieprozess inhärent sind, getreu erlernt und kompensiert werden können, und dass eine Lernarchitektur, die mit einer probabilistischen Verlustfunktion verbunden ist, gemeinsam eine vollständige Kovarianzmatrix der Restfehler schätzen kann, wodurch ein heteroskedastisches Fehlermodell definiert wird. In Experimenten mit Bildsequenzen vom autonomen Fahren und mit Kameraaufnahmen von Mikro-Luftfahrzeugen wird die Möglichkeit bewertet, gleichzeitig die visuelle Wahrnehmung zu verbessern und einen mit den Ergebnissen verbundenen Fehler zu schätzen.
Die Entwicklung robuster Online-Empfehlungssysteme für Inhalte erfordert das Erlernen komplexer Interaktionen zwischen Benutzerpräferenzen und Inhaltsmerkmalen. Das Feld hat sich in den letzten Jahren schnell von traditionellen mehrarmigen Bandit- und kollaborativen Filtertechniken hin zu neuen Methoden entwickelt, die Deep-Learning-Modelle integrieren, die es ermöglichen, nicht-lineare Interaktionen zwischen Merkmalen zu erfassen.Trotz des Fortschritts stellt die dynamische Natur von Online-Empfehlungen immer noch große Herausforderungen dar, wie z. B. die Suche nach dem empfindlichen Gleichgewicht zwischen Erkundung und Nutzung. In diesem Beitrag stellen wir eine neuartige Methode vor, Deep Density Networks (DDN), die Mess- und Datenunsicherheiten dekonvolviert und Wahrscheinlichkeitsdichten von CTR vorhersagt, wodurch wir eine effizientere Exploration des Merkmalsraums durchführen können.Wir zeigen die Nützlichkeit der Verwendung von DDN online in einem realen Inhaltsempfehlungssystem, das Milliarden von Empfehlungen pro Tag liefert, und präsentieren Online- und Offline-Ergebnisse, um den Nutzen der Verwendung von DDN zu bewerten.
Während es gut dokumentiert ist, dass Befürworter und Verweigerer des Klimawandels in den Vereinigten Staaten im Laufe der Zeit zunehmend polarisiert sind, gab es keine groß angelegte Untersuchung, ob diese Personen dazu neigen, ihre Meinung als Folge von natürlichen externen Ereignissen zu ändern.Auf der Teilpopulation der Twitter-Nutzer untersuchen wir, ob sich die Stimmung zum Klimawandel als Reaktion auf fünf verschiedene Naturkatastrophen in den USA im Jahr 2018 ändert. Zunächst zeigen wir, dass Tweets mit einer Genauigkeit von über 75 % entweder als Befürworter oder als Gegner des Klimawandels klassifiziert werden können, wenn wir unsere Methodik anwenden, um begrenzte gelabelte Daten zu kompensieren; die Ergebnisse sind robust gegenüber verschiedenen Modellen des maschinellen Lernens und liefern Ergebnisse auf geografischer Ebene, die mit früheren Untersuchungen übereinstimmen. Anschließend wenden wir RNNs an, um eine Analyse auf Kohortenebene durchzuführen, die zeigt, dass die Wirbelstürme von 2018 zu einem statistisch signifikanten Anstieg der durchschnittlichen Tweet-Stimmung geführt haben, die den Klimawandel bejaht; dieser Effekt gilt jedoch nicht für die untersuchten Schneestürme und Waldbrände von 2018, was darauf hindeutet, dass die Meinungen der Twitter-Nutzer zum Klimawandel bei dieser Untergruppe von Naturkatastrophen ziemlich tief verwurzelt sind.
Wir untersuchen die Steuerung symmetrischer linearer dynamischer Systeme mit unbekannter Dynamik und einem verborgenen Zustand und formulieren die optimale Steuerung in dieser Umgebung als konvexes Programm, indem wir eine neuere spektrale Filtertechnik zur präzisen Darstellung solcher Systeme auf linearer Basis verwenden. Wir geben den ersten effizienten Algorithmus zum Auffinden des optimalen Kontrollsignals mit einem beliebigen Zeithorizont T an, wobei die Probenkomplexität (Anzahl der Trainingsläufe) nur polynomiell in log(T) und anderen relevanten Parametern ist.
Generative Adversarial Networks (GANs) haben sich zum Goldstandard entwickelt, wenn es um das Lernen generativer Modelle für hochdimensionale Verteilungen geht.Seit ihrer Einführung wurden in der Literatur zahlreiche Variationen von GANs vorgestellt, die sich vor allem auf die Verwendung neuartiger Verlustfunktionen, Optimierungs-/Regularisierungsstrategien und Netzwerkarchitekturen konzentrieren.In diesem Beitrag richten wir unsere Aufmerksamkeit auf den Generator und untersuchen die Verwendung von Polynomen hoher Ordnung als eine alternative Klasse universeller Funktionsapproximatoren. Konkret schlagen wir PolyGAN vor, bei dem wir den Datengenerator durch ein Polynom hoher Ordnung modellieren, dessen unbekannte Parameter auf natürliche Weise durch Tensoren hoher Ordnung dargestellt werden. Wir stellen zwei Tensorzerlegungen vor, die die Anzahl der Parameter erheblich reduzieren, und zeigen, wie sie effizient durch hierarchische neuronale Netze implementiert werden können, die nur lineare/konvolutionäre Blöcke verwenden. Wir zeigen zum ersten Mal, dass ein GAN-Generator mit unserem Ansatz die Datenverteilung ohne Aktivierungsfunktionen approximieren kann. Eine gründliche experimentelle Evaluierung sowohl mit synthetischen als auch realen Daten (Bilder und 3D-Punktwolken) demonstriert die Vorzüge von PolyGAN im Vergleich zum Stand der Technik.
Tiefe neuronale Netze, die auf großen überwachten Datensätzen trainiert werden, haben in den letzten Jahren zu beeindruckenden Ergebnissen geführt, aber da gut annotierte Datensätze unerschwinglich teuer und zeitaufwendig zu sammeln sind, haben neuere Arbeiten die Verwendung größerer, aber verrauschter Datensätze erforscht, die leichter zu erhalten sind.In diesem Papier untersuchen wir das Verhalten von tiefen neuronalen Netzen auf Trainingssätzen mit massiv verrauschten Etiketten.Wir zeigen auf mehreren Datensätzen wie MINST, CIFAR-10 und ImageNet, dass erfolgreiches Lernen auch mit einer im Wesentlichen beliebigen Menge an Rauschen möglich ist. Wir zeigen an mehreren Datensätzen wie MINST, CIFAR-10 oder ImageNet, dass erfolgreiches Lernen selbst bei einer im Wesentlichen beliebigen Menge an Rauschen möglich ist. z.B. finden wir bei MNIST, dass eine Genauigkeit von über 90 Prozent immer noch erreichbar ist, selbst wenn der Datensatz mit 100 verrauschten Beispielen für jedes saubere Beispiel verdünnt wurde.dieses Verhalten gilt für mehrere Muster von Etikettenrauschen, selbst wenn verrauschte Etiketten in Richtung verwirrender Klassen voreingenommen sind.außerdem zeigen wir, wie die erforderliche Datensatzgröße für erfolgreiches Training mit höherem Etikettenrauschen zunimmt.schließlich stellen wir einfache umsetzbare Techniken zur Verbesserung des Lernens bei hohem Etikettenrauschen vor.
In diesem Papier schlagen wir vor, den kürzlich eingeführten modellagnostischen Meta-Lernalgorithmus (MAML, Finn et al., 2017) für die neuronale maschinelle Übersetzung (NMT) mit geringen Ressourcen zu erweitern.Wir betrachten die Übersetzung mit geringen Ressourcen als ein Meta-Lernproblem und lernen, uns an Sprachen mit geringen Ressourcen auf der Grundlage von mehrsprachigen Sprachaufgaben mit hohen Ressourcen anzupassen.Wir verwenden die universelle lexikalische Darstellung (Gu et al, Wir evaluieren die vorgeschlagene Meta-Lernstrategie anhand von achtzehn europäischen Sprachen (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv und Ru) als Ausgangsaufgaben und fünf verschiedenen Sprachen (Ro, Lv, Fi, Tr und Ko) als Zielaufgaben.Wir zeigen, dass der vorgeschlagene Ansatz den mehrsprachigen, auf Transferlernen basierenden Ansatz (Zoph et al, Wir zeigen, dass der vorgeschlagene Ansatz den mehrsprachigen, auf Transferlernen basierenden Ansatz (Zoph et al., 2016) signifikant übertrifft und es uns ermöglicht, ein wettbewerbsfähiges NMT-System mit nur einem Bruchteil der Trainingsbeispiele zu trainieren. Zum Beispiel kann der vorgeschlagene Ansatz einen Wert von 22,04 BLEU im rumänisch-englischen WMT'16 erreichen, indem er nur 16.000 übersetzte Wörter (~600 parallele Sätze) sieht.
In dieser Arbeit wird eine Methode zur aktiven Anomalieerkennung vorgestellt, die auf bestehenden Deep-Learning-Lösungen für die unbeaufsichtigte Anomalieerkennung aufgebaut werden kann: Wir zeigen, dass ein Prior für die Anomalien angenommen werden muss, um Leistungsgarantien für die unbeaufsichtigte Anomalieerkennung zu erhalten. Um dieses Problem zu lösen, stellen wir eine neue Schicht vor, die an jedes Deep-Learning-Modell angehängt werden kann, das für die unbeaufsichtigte Erkennung von Anomalien entwickelt wurde, um es in eine aktive Methode umzuwandeln, und präsentieren Ergebnisse sowohl auf synthetischen als auch auf realen Anomalie-Erkennungsdatensätzen.
Um eine Erklärung für das Verhalten eines Klassifizierers zu liefern, schlagen wir eine Methode vor, die eine Reihe von Beispielen liefert, die die semantischen Unterschiede zwischen den Entscheidungen des Klassifizierers hervorheben, indem wir diese Beispiele durch Interpolationen im latenten Raum erzeugen. Wir führen den Begriff des semantischen stochastischen Pfades ein und formalisieren ihn als einen geeigneten stochastischen Prozess, der im Merkmalsraum durch latente Code-Interpolationen definiert wird, und führen dann das Konzept der semantischen Lagranger ein, um das gewünschte Verhalten des Klassifizierers einzubeziehen, und stellen fest, dass die Lösung des zugehörigen Variationsproblems es ermöglicht, Unterschiede in der Entscheidung des Klassifizierers hervorzuheben.Sehr wichtig ist, dass innerhalb unseres Rahmens der Klassifizierer als Blackbox verwendet wird und nur seine Bewertung erforderlich ist.
In realen Anwendungen ist es schwierig, vollständige Domänenmodelle zu spezifizieren, da die Interaktionen zwischen dem Agenten und seiner Umgebung sehr komplex sein können. Wir schlagen einen Rahmen vor, um eine PPDDL-Darstellung des Modells inkrementell über mehrere Planungsprobleme zu erlernen, wobei nur die Erfahrungen aus dem aktuellen Planungsproblem verwendet werden, was für nicht-stationäre Umgebungen geeignet ist. Wir führen das neuartige Konzept der Verlässlichkeit als intrinsische Motivation für das Verstärkungslernen und als Mittel zum Lernen aus Fehlern ein, um wiederholte Instanzen ähnlicher Fehler zu verhindern.
Auf dem Gebiet des Deep Reinforcement Learning (DRL) hat die Popularität von Maximum Entropy Reinforcement Learning Algorithmen in letzter Zeit stark zugenommen.  In diesem Papier versuchen wir, den primären Beitrag des Entropie-Terms zur Leistung von Maximum-Entropie-Algorithmen zu verstehen. Für den Mujoco-Benchmark zeigen wir, dass der Entropie-Term in Soft Actor Critic (SAC) hauptsächlich die begrenzte Natur der Aktionsräume anspricht. Mit dieser Einsicht schlagen wir ein einfaches Normalisierungsschema vor, das es einem schlanken Algorithmus ohne Entropiemaximierung ermöglicht, die Leistung von SAC zu erreichen. Unsere experimentellen Ergebnisse zeigen, dass die Vorteile der Entropie-Regularisierung in DRL überdacht werden müssen.  Wir zeigen außerdem, dass der vereinfachte Algorithmus mit dem einfachen, ungleichmäßigen Sampling-Schema SAC übertrifft und bei anspruchsvollen kontinuierlichen Steuerungsaufgaben die beste Leistung erzielt.
In jüngster Zeit wird es zu einem beliebten Ansatz für die Beantwortung von offenen Fragen durch die erste Suche Frage-bezogenen Passagen, dann die Anwendung von Leseverständnis-Modelle, um Antworten zu extrahieren.Bestehende Arbeiten in der Regel extrahieren Antworten aus einzelnen Passagen unabhängig, also nicht in vollem Umfang nutzen die mehrere gesuchte Passagen, vor allem für die einige Fragen, die mehrere Beweise, die in verschiedenen Passagen erscheinen kann, beantwortet werden.Die oben genannten Beobachtungen werfen das Problem der Aggregation von Beweisen aus mehreren Passagen. Auf der Grundlage der Antwortkandidaten, die aus dem bestehenden QA-Modell generiert wurden, schlagen wir zwei verschiedene Re-Ranking-Methoden vor, stärkebasierte und abdeckungsbasierte Re-Ranger, die die aggregierten Beweise aus verschiedenen Passagen nutzen, um die richtige Antwort auf die Frage zu finden. Unser Modell erreichte den Stand der Technik bei drei öffentlichen Open-Domain-QA-Datensätzen, Quasar-T, SearchQA und der Open-Domain-Version von TriviaQA, mit einer Verbesserung von etwa 8\% bei den ersten beiden Datensätzen.
Viele große Textsammlungen weisen Graphenstrukturen auf, die entweder dem Inhalt selbst innewohnen oder in den Metadaten der einzelnen Dokumente kodiert sind. Beispiele für Graphen, die aus Dokumentensammlungen extrahiert werden, sind Co-Autoren-Netzwerke, Zitationsnetzwerke oder Named-Entity-Coccurrence-Netzwerke. Wenn es um die Visualisierung dieser großen Korpora geht, wird entweder der textuelle Inhalt oder der Netzwerkgraph verwendet. In dieser Arbeit schlagen wir vor, beides zu integrieren, Text und Graph, um nicht nur die semantische Information, die im Inhalt der Dokumente kodiert ist, zu visualisieren, sondern auch die Beziehungen, die durch die inhärente Netzwerkstruktur ausgedrückt werden. Wir illustrieren die Effektivität unseres Ansatzes mit realen Datensätzen und zeigen, dass wir die Semantik großer Dokumentensammlungen besser erfassen können als andere Visualisierungen, die entweder auf dem Inhalt oder der Netzwerkinformation basieren.
Dies stellt ein ernsthaftes Problem für den Einsatz solcher Technologien dar, da die resultierenden Modelle bei Populationen, die in der Trainingsmenge Minderheiten sind, schlecht abschneiden und letztendlich ein höheres Risiko für sie darstellen können. Wir stellen einen Rahmen vor, der die Bayes'sche Parametersuche nutzt, um den hochdimensionalen Merkmalsraum effizient zu charakterisieren und Leistungsschwächen schneller zu erkennen. Wir wenden unseren Ansatz auf ein Beispielgebiet, die Gesichtserkennung, an und zeigen, dass er dazu beitragen kann, demografische Verzerrungen in kommerziellen Anwendungsprogrammierschnittstellen (APIs) für Gesichter zu erkennen.
Punktwolken sind eine flexible und allgegenwärtige Möglichkeit, 3D-Objekte mit beliebiger Auflösung und Präzision darzustellen. Frühere Arbeiten haben gezeigt, dass die Anpassung von Kodierernetzwerken an die Semantik ihrer Eingabepunktwolken ihre Effektivität im Vergleich zu naiven Feedforward-Alternativen erheblich verbessern kann. Die überwiegende Mehrheit der Arbeiten zu Punktwolken-Dekodierern basiert jedoch immer noch auf vollständig verbundenen Netzwerken, die Formdarstellungen auf eine feste Anzahl von Ausgabepunkten abbilden. Insbesondere untersuchen wir Sample-basierte Punktwolken-Dekoder, die eine Formrepräsentation auf eine Punktmerkmalverteilung abbilden, so dass eine beliebige Anzahl von abgetasteten Merkmalen in einzelne Ausgabepunkte umgewandelt werden kann.Wir entwickeln drei Sample-basierte Dekoder-Architekturen und vergleichen ihre Leistung miteinander und zeigen ihre verbesserte Effektivität gegenüber Feedforward-Architekturen.Darüber hinaus untersuchen wir die gelernten Verteilungen, um Einblicke in die Ausgabetransformation zu gewinnen.Unsere Arbeit ist als erweiterbare Software-Plattform verfügbar, um diese Ergebnisse zu reproduzieren und als Grundlage für zukünftige Arbeiten zu dienen.
Im Gegensatz zu früheren lernbasierten Arbeiten, die ein Training des Optimierers auf demselben zu optimierenden Graphen erfordern, schlagen wir einen Lernansatz vor, der einen Optimierer offline trainiert und dann ohne weiteres Training auf zuvor unbekannte Graphen generalisiert. Wir betrachten zwei Optimierungsaufgaben für Berechnungsgraphen: Minimierung der Laufzeit und Minimierung des maximalen Speicherverbrauchs. Im Vergleich zu einer umfangreichen Reihe von Basisdaten erzielt unser Ansatz signifikante Verbesserungen gegenüber klassischen und anderen lernbasierten Methoden für diese beiden Aufgaben.
Eine der grundlegendsten Vorhersageaufgaben ist die Vorhersage von Ansichten: Wie würde eine bestimmte Szene von einem anderen Standpunkt aus aussehen? Der Mensch ist in dieser Aufgabe überragend. Unsere Fähigkeit, uns vorzustellen und fehlende visuelle Informationen zu ergänzen, ist eng mit der Wahrnehmung gekoppelt: Wir haben das Gefühl, die Welt in drei Dimensionen zu sehen, während in Wirklichkeit nur Informationen von der Vorderseite der Welt auf unsere (2D-)Netzhaut treffen. Wir schlagen inverse grafische Netzwerke vor, die 2,5D-Videoströme, die von einer sich bewegenden Kamera aufgenommen werden, als Eingabe verwenden und stabile 3D-Merkmalskarten der Szene erstellen, indem sie den Inhalt der Szene von der Bewegung der Kamera trennen. Wir schlagen kontrastive Vorhersageverluste vor, die mit der Stochastizität des visuellen Inputs umgehen können und das vorausschauende Lernen auf fotorealistischere Szenen skalieren können, als sie in früheren Arbeiten berücksichtigt wurden. Wir zeigen, dass das vorgeschlagene Modell lernt 3D visuelle Repräsentationen nützlich für (1) semi-supervised Lernen von 3D-Objekt-Detektoren, und (2) unsupervised Lernen von 3D bewegten Objekt-Detektoren, durch die Schätzung der Bewegung der abgeleiteten 3D-Merkmal Karten in Videos von dynamischen scenes.To the best of our knowledge, ist dies die erste Arbeit, die empirisch zeigt, Ansicht Vorhersage zu einem nützlichen und skalierbaren selbst-überwachten Aufgabe vorteilhaft für 3D-Objekt-Erkennung.  
Einige moderne Ansätze trainieren ein Encoder-Decoder-Netzwerk auf gepaarten Text- und Audio-Samples (x_txt, x_aud), indem sie dessen Ausgabe dazu anregen, x_aud zu rekonstruieren.Die synthetisierte Audio-Wellenform soll den verbalen Inhalt von x_txt und den auditiven Stil von x_aud enthalten. Leider ist die Modellierung des Stils in TTS etwas unterbestimmt und das Trainieren von Modellen mit einem Rekonstruktionsverlust allein reicht nicht aus, um Inhalt und Stil von anderen Variationsfaktoren zu entflechten. Wir erreichen dies, indem wir ein paarweises Trainingsverfahren, ein kontradiktorisches Spiel und ein kollaboratives Spiel in einem Trainingsschema kombinieren. Das kontradiktorische Spiel konzentriert die wahre Datenverteilung, und das kollaborative Spiel minimiert den Abstand zwischen realen Proben und generierten Proben sowohl im Originalraum als auch im latenten Raum, wodurch das vorgeschlagene Modell einen hochgradig kontrollierbaren Generator und eine entflochtene Darstellung liefert. Durch die getrennte Modellierung von Stil und Inhalt kann unser Modell Sprache in menschlicher Qualität generieren, die den gewünschten Stilbedingungen entspricht. Unser Modell erzielt Spitzenergebnisse bei verschiedenen Aufgaben, einschließlich Stiltransfer (Austausch von Inhalt und Stil), Emotionsmodellierung und Identitätstransfer (Anpassung der Stimme eines neuen Sprechers).
Empirische Belege deuten darauf hin, dass neuronale Netze mit ReLU-Aktivierungen bei Überparametrisierung besser verallgemeinern, jedoch gibt es derzeit keine theoretische Analyse, die diese Beobachtung erklärt.In dieser Arbeit untersuchen wir eine vereinfachte Lernaufgabe mit überparametrisierten Faltungsnetzen, die empirisch das gleiche qualitative Phänomen aufweist.  Für dieses Setting liefern wir eine theoretische Analyse der Optimierungs- und Generalisierungsleistung des Gradientenabstiegs, insbesondere beweisen wir datenabhängige Schranken für die Probenkomplexität, die zeigen, dass die Überparametrisierung die Generalisierungsleistung des Gradientenabstiegs verbessert.
Wir stellen einen neuen und rigoros formulierten PAC-Bayes Metalernalgorithmus vor, der implizit eine interessierende Modellpriorverteilung erlernt, und erweitern den PAC-Bayes-Rahmen von einer Einzelaufgabeneinstellung auf die Metalerneinstellung mit wenigen Schüssen, um die Generalisierungsfehler bei ungesehenen Aufgaben nach oben zu begrenzen. Wir zeigen, dass die Modelle, die mit dem von uns vorgeschlagenen Meta-Learning-Algorithmus trainiert werden, gut kalibriert und genau sind, mit hochmodernen Kalibrierungs- und Klassifizierungsergebnissen auf dem mini-ImageNet-Benchmark und wettbewerbsfähigen Ergebnissen in einer multimodalen Aufgabenverteilungsregression.
In dem Maße, in dem der Bereich der erklärbaren KI (XAI) und der erklärbaren KI-Planung (XAIP) reift, wird auch die Fähigkeit der Agenten wachsen, Erklärungen zu generieren und zu kuratieren.Wir schlagen einen neuen Bereich der Herausforderung in Form von rebellischen und trügerischen Erklärungen vor.Wir diskutieren, wie diese Erklärungen generiert werden könnten und erörtern dann kurz die Bewertungskriterien.
Wir untersuchen eine Variante von Variations-Autoencodern, bei der eine Überstruktur von diskreten latenten Variablen über den latenten Merkmalen liegt, die im Allgemeinen eine Baumstruktur von mehreren latenten Variablen ist und automatisch aus den Daten gelernt wird. Während bisherige Deep-Learning-Methoden für das Clustering nur eine Partition der Daten erzeugen, erzeugt LTVAE mehrere Partitionen der Daten, die jeweils durch eine latente Supervariable gegeben sind, was wünschenswert ist, da hochdimensionale Daten in der Regel viele verschiedene natürliche Facetten haben und auf mehrere Arten sinnvoll partitioniert werden können.
Beliebte Deep Reinforcement Learning-Ansätze in dieser Richtung beinhalten das Lernen von zielbedingten Strategien oder Wertfunktionen oder Inverse Dynamics Models (IDMs).IDMs bilden den aktuellen Zustand und das gewünschte Ziel eines Agenten auf die erforderlichen Aktionen ab.Wir zeigen, dass der Schlüssel zum Erreichen einer guten Leistung mit IDMs im Lernen der Informationen liegt, die zwischen gleichwertigen Erfahrungen ausgetauscht werden, so dass sie auf unbekannte Szenarien verallgemeinert werden können. Wir entwerfen einen Trainingsprozess, der das Lernen von latenten Repräsentationen anleitet, um diese gemeinsamen Informationen zu kodieren.Mit einer begrenzten Anzahl von Umgebungsinteraktionen ist unser Agent in der Lage, effizient zu beliebigen Punkten im Zielraum zu navigieren.Wir demonstrieren die Effektivität unseres Ansatzes in hochdimensionalen Fortbewegungsumgebungen wie der Mujoco Ameise, dem PyBullet Humanoid und dem PyBullet Minitaur.Wir liefern quantitative und qualitative Ergebnisse, die zeigen, dass unsere Methode konkurrierende Ansätze deutlich übertrifft.
In diesem Papier identifizieren wir zunächst \textit{angle bias}, ein einfaches, aber bemerkenswertes Phänomen, das das Problem des verschwindenden Gradienten in einem mehrschichtigen Perzeptron (MLP) mit sigmoiden Aktivierungsfunktionen verursacht. Wir schlagen dann \textit{linearly constrained weights (LCW)} vor, um die Winkelverzerrung in einem neuronalen Netzwerk zu reduzieren, um das Netzwerk unter der Bedingung zu trainieren, dass die Summe der Elemente jedes Gewichtsvektors Null ist. Interessanterweise kann die Stapelnormalisierung (Ioffe & Szegedy, 2015) als ein Mechanismus zur Korrektur der Winkelverzerrung betrachtet werden, und erste Experimente zeigen, dass LCW dabei hilft, ein MLP mit 100 Schichten effizienter zu trainieren als die Stapelnormalisierung.
Markov Logic Networks (MLNs), die auf elegante Weise logische Regeln und probabilistische grafische Modelle kombinieren, können verwendet werden, um viele Wissensgraphenprobleme anzugehen.Allerdings ist die Inferenz in MLNs rechenintensiv, was die Anwendung von MLNs im industriellen Maßstab sehr schwierig macht.In den letzten Jahren haben sich neuronale Graphen-Netzwerke (GNNs) als effiziente und effektive Werkzeuge für große Graphen-Probleme herauskristallisiert.Nichtsdestotrotz integrieren GNNs nicht explizit vorherige logische Regeln in die Modelle und können viele markierte Beispiele für eine Zielaufgabe erfordern. In diesem Beitrag untersuchen wir die Kombination von MLNs und GNNs und verwenden Graph Neural Networks für die Variationsinferenz in MLNs. Wir schlagen eine GNN-Variante vor, die wir ExpressGNN nennen und die ein gutes Gleichgewicht zwischen der Darstellungsstärke und der Einfachheit des Modells herstellt. unsere umfangreichen Experimente mit verschiedenen Benchmark-Datensätzen zeigen, dass ExpressGNN zu effektiven und effizienten probabilistischen logischen Schlussfolgerungen führt.
Die meisten RL-Strategien weisen jedoch eine gewisse Schwäche auf und können bei hochdimensionalen und instationären Umgebungen rechnerisch schwer zu bewältigen sein.In dieser Arbeit bauen wir eine Meta-Reinforcement-Learning (MRL)-Methode auf, in die ein adaptives neuronales Netzwerk (NN) eingebettet ist, um eine effiziente Iteration der Strategie bei sich ändernden Aufgabenbedingungen zu ermöglichen.Unser Hauptziel ist es, die RL-Anwendung auf die anspruchsvolle Aufgabe des autonomen Fahrens in der Stadt im CARLA-Simulator zu erweitern.
In dieser Arbeit untersuchen wir das Problem des Repräsentationslernens im Kontext des Verstärkungslernens unter Verwendung des Informationsengpasses, um die Effizienz der Lernalgorithmen zu verbessern.wir leiten die optimale bedingte Verteilung der Repräsentation analytisch ab und liefern eine untere Variationsschranke.dann maximieren wir diese untere Schranke mit der Stein variationalen (SV) Gradientenmethode. Unsere experimentellen Ergebnisse zeigen, dass unser Rahmenwerk die Stichprobeneffizienz von Vanilla A2C und PPO erheblich verbessern kann. Schließlich untersuchen wir die Informationsengpass-Perspektive (IB) in Deep RL mit dem Algorithmus namens Mutual Information Neural Estimation (MINE). Wir analysieren auch die Beziehung zwischen MINE und unserer Methode, durch diese Beziehung leiten wir theoretisch einen Algorithmus zur Optimierung unseres IB-Frameworks ab, ohne die untere Grenze zu konstruieren.
 Ein Kernaspekt menschlicher Intelligenz ist die Fähigkeit, neue Aufgaben schnell zu erlernen und flexibel zwischen ihnen zu wechseln.Hier beschreiben wir ein modulares kontinuierliches Verstärkungslernparadigma, das von diesen Fähigkeiten inspiriert ist.Wir stellen zunächst eine visuelle Interaktionsumgebung vor, die es ermöglicht, viele Arten von Aufgaben in einem einzigen Rahmen zu vereinen.Wir beschreiben dann ein Belohnungskartenvorhersageschema, das neue Aufgaben in den sehr großen Zustands- und Aktionsräumen, die eine solche Umgebung erfordert, robust erlernt. Wir untersuchen, wie sich die Eigenschaften der Modularchitektur auf die Effizienz des Aufgabenlernens auswirken, und zeigen, dass ein Modulmotiv, das bestimmte Konstruktionsprinzipien (z. B. frühe Engpässe, polynomiale Nichtlinearitäten niedriger Ordnung und Symmetrie) beinhaltet, Standardmotiven neuronaler Netze deutlich überlegen ist, da es weniger Trainingsbeispiele und weniger Neuronen benötigt, um ein hohes Leistungsniveau zu erreichen.Schließlich stellen wir eine Meta-Controller-Architektur für die Aufgabenumschaltung vor, die auf einem dynamischen neuronalen Abstimmungsschema basiert, das es neuen Modulen ermöglicht, Informationen zu nutzen, die sie aus früheren Aufgaben gelernt haben, um ihre eigene Lerneffizienz erheblich zu verbessern.
Interpretierbarkeit und kleine gelabelte Datensätze sind Schlüsselprobleme bei der praktischen Anwendung von Deep Learning, insbesondere in Bereichen wie der Medizin. Wir lernen dichte Repräsentationen aus großen, unmarkierten Bilddatensätzen und verwenden diese Repräsentationen, um Klassifikatoren aus kleinen, markierten Datensätzen zu lernen und visuelle Erklärungen für die Vorhersagen zu generieren. Mit der Röntgendiagnose der Brust als motivierende Anwendung zeigen wir, dass unsere Methode eine gute Generalisierungsfähigkeit hat, indem wir lernen, unseren Röntgendatensatz zu repräsentieren, während wir einen Klassifikator auf einem separaten Datensatz aus einer anderen Einrichtung trainieren. Für jede Vorhersage generieren wir visuelle Begründungen für positive Klassifizierungen, indem wir eine latente Repräsentation optimieren, um die Wahrscheinlichkeit einer Krankheit zu minimieren, während wir durch ein Ähnlichkeitsmaß im Bildraum eingeschränkt sind.Die Dekodierung der resultierenden latenten Repräsentation erzeugt ein Bild ohne offensichtliche Krankheit.Der Unterschied zwischen dem ursprünglichen und dem veränderten Bild bildet eine interpretierbare visuelle Begründung für die Vorhersage des Algorithmus.Unsere Methode produziert gleichzeitig visuelle Begründungen, die sich positiv mit früheren Techniken vergleichen lassen, und einen Klassifikator, der den aktuellen Stand der Technik übertrifft.
Evolutionäre Strategien (ES) sind eine populäre Familie von Black-Box-Optimierungsalgorithmen der nullten Ordnung, die sich auf Suchverteilungen stützen, um eine große Vielfalt von Zielfunktionen effizient zu optimieren.Dieses Papier untersucht die potenziellen Vorteile der Verwendung hochflexibler Suchverteilungen in ES-Algorithmen im Gegensatz zu Standardverteilungen (typischerweise Gauß). Wir modellieren solche Verteilungen mit Generativen Neuronalen Netzen (GNNs) und stellen einen neuen ES-Algorithmus vor, der ihre Ausdruckskraft nutzt, um die stochastische Suche zu beschleunigen.Da unser Ansatz als Plug-in fungiert, ermöglicht er es, praktisch jeden Standard-ES-Algorithmus mit flexiblen Suchverteilungen zu erweitern.Wir demonstrieren die empirischen Vorteile dieser Methode für eine Vielzahl von Zielfunktionen.
Wir schlagen neue Techniken zur Komprimierung und Beschleunigung von dichten Matrixmultiplikationen vor, wie sie in den vollverknüpften und rekurrenten Schichten neuronaler Netze für die eingebettete kontinuierliche Spracherkennung mit großem Wortschatz (LVCSR) vorkommen, und evaluieren diese. Im Vergleich zum Standard-Training mit niedrigem Rang zeigen wir, dass unsere Methode zu einem guten Kompromiss zwischen Genauigkeit und Anzahl der Parameter führt und zur Beschleunigung des Trainings großer Modelle verwendet werden kann.Zur Beschleunigung ermöglichen wir eine schnellere Inferenz auf ARM-Prozessoren durch neue Open-Source-Kernel, die für kleine Stapelgrößen optimiert sind, was zu einer 3- bis 7-fachen Beschleunigung gegenüber der weit verbreiteten gemmlowp-Bibliothek führt.Über LVCSR hinaus erwarten wir, dass unsere Techniken und Kernel allgemeiner auf eingebettete neuronale Netze mit großen voll verbundenen oder rekurrenten Schichten anwendbar sind.
Das Training aktivierungsquantisierter neuronaler Netze beinhaltet die Minimierung eines stückweise konstanten Trainingsverlustes, dessen Gradient fast überall verschwindet, was für die Standard-Backpropagation oder die Kettenregel unerwünscht ist.Eine empirische Methode zur Umgehung dieses Problems ist die Verwendung eines Straight-Through-Schätzers (STE) (Bengio et al, Da dieser ungewöhnliche "Gradient" mit Sicherheit nicht der Gradient der Verlustfunktion ist, stellt sich die Frage, warum die Suche in seiner negativen Richtung den Trainingsverlust minimiert... In diesem Beitrag liefern wir die theoretische Rechtfertigung des STE-Konzepts, indem wir diese Frage beantworten... Wir betrachten das Problem des Lernens eines Netzes mit zwei linearen Schichten mit binarisierter ReLU-Aktivierung und Gaußschen Eingabedaten. Wir werden den ungewöhnlichen "Gradienten", der durch die STE-modifizierte Kettenregel gegeben ist, als groben Gradienten bezeichnen.Die Wahl der STE ist nicht eindeutig.Wir beweisen, dass, wenn die STE richtig gewählt ist, der erwartete grobe Gradient positiv mit dem Populationsgradienten korreliert (der für das Training nicht zur Verfügung steht), und seine Negation ist eine Abstiegsrichtung zur Minimierung des Populationsverlustes.Wir zeigen ferner, dass der zugehörige Algorithmus für den groben Gradientenabstieg zu einem kritischen Punkt des Populationsverlustminimierungsproblems konvergiert.  Darüber hinaus zeigen wir, dass eine schlechte Wahl von STE zu einer Instabilität des Trainingsalgorithmus in der Nähe bestimmter lokaler Minima führt, was mit CIFAR-10-Experimenten verifiziert wird.
In diesem Beitrag wird GumbelClip vorgestellt, eine Reihe von Modifikationen des Actor-Critic-Algorithmus für das Off-Policy Reinforcement Learning. GumbelClip verwendet die Konzepte des truncated importance sampling zusammen mit additivem Rauschen, um eine Verlustfunktion zu erzeugen, die die Verwendung von Off-Policy Samples ermöglicht. Der modifizierte Algorithmus erreicht eine höhere Konvergenzgeschwindigkeit und Stichprobeneffizienz im Vergleich zu On-Policy-Algorithmen und ist konkurrenzfähig mit bestehenden Off-Policy-Policy-Gradientenmethoden, während er deutlich einfacher zu implementieren ist.
In den letzten Jahren wurden dank der Formulierung von Generative Adversarial Networks (GANs) verschiedene Fortschritte im Bereich der generativen Modelle gemacht.GANs haben sich bei einer Vielzahl von Aufgaben zur Bilderzeugung und Stilübertragung als äußerst leistungsfähig erwiesen.Im Bereich der natürlichen Sprachverarbeitung sind Worteinbettungen wie word2vec und GLoVe die modernsten Methoden zur Anwendung neuronaler Netzmodelle auf Textdaten. In dieser Arbeit wird ein Ansatz zur Texterzeugung vorgestellt, der Skip-Thought-Satzeinbettungen in Verbindung mit GANs auf der Grundlage von Gradientenstraffunktionen und f-Messungen verwendet. Die Ergebnisse der Verwendung von Satzeinbettungen mit GANs zur Erzeugung von Text in Abhängigkeit von Eingabeinformationen sind mit den Ansätzen vergleichbar, bei denen Worteinbettungen verwendet werden.
Autoregressive rekurrente neuronale Decoder, die Sequenzen von Token einzeln und von links nach rechts erzeugen, sind das Arbeitspferd der modernen maschinellen Übersetzung.In dieser Arbeit schlagen wir eine neue Decoder-Architektur vor, die natürliche Sprachsequenzen in beliebiger Reihenfolge erzeugen kann.Neben der Erzeugung von Token aus einem gegebenen Vokabular lernt unser Modell zusätzlich, die optimale Position für jedes produzierte Token auszuwählen. Die vorgeschlagene Decoder-Architektur ist vollständig kompatibel mit dem seq2seq-Framework und kann als Drop-in-Ersatz für jeden klassischen Decoder verwendet werden. Wir demonstrieren die Leistung unseres neuen Decoders bei der maschinellen Übersetzungsaufgabe IWSLT und untersuchen und interpretieren die erlernten Decoder-Muster, indem wir analysieren, wie das Modell neue Positionen für jedes nachfolgende Token auswählt.
Obwohl der lernbasierte Ansatz ungenau ist, sind wir in der Lage, große Muster und Datengraphen in polynomialer Zeit zu zählen, verglichen mit der exponentiellen Zeit des ursprünglichen NP-kompletten Problems.Anders als bei anderen traditionellen Graphenlernproblemen, wie z.B. Knotenklassifikation und Linkvorhersage, erfordert das Zählen von Subgraphenisomorphismen mehr globale Inferenz, um den gesamten Graphen zu überblicken. Um dieses Problem zu lösen, schlagen wir ein dynamisches intermediales Aufmerksamkeits-Speicher-Netzwerk (DIAMNet) vor, das verschiedene Repräsentations-Lernarchitekturen ergänzt und iterativ Muster- und Zieldaten-Graphen besucht, um sich verschiedene Subgraphen-Isomorphismen für die globale Zählung zu merken. Wir entwickeln sowohl kleine Graphen (<= 1.024 Subgraphen-Isomorphismen in jedem) als auch große Graphen (<= 4.096 Subgraphen-Isomorphismen in jedem), um verschiedene Modelle zu evaluieren.Experimentelle Ergebnisse zeigen, dass das lernbasierte Zählen von Subgraphen-Isomorphismen dazu beitragen kann, die Zeitkomplexität mit akzeptabler Genauigkeit zu reduzieren.Unser DIAMNet kann bestehende Repräsentationslernmodelle für dieses globalere Problem weiter verbessern.
In solchen Situationen werden die Agenten in ähnlichen Umgebungen, wie z.B. Simulatoren, trainiert und dann in die ursprüngliche Umgebung übertragen. Die Lücke zwischen den visuellen Beobachtungen der Ausgangs- und der Zielumgebung führt oft dazu, dass der Agent in der Zielumgebung versagt. Wir stellen einen neuen RL-Agenten vor, SADALA (Soft Attention DisentAngled representation Learning Agent).SADALA lernt zunächst eine komprimierte Zustandsrepräsentation und lernt dann gemeinsam, ablenkende Merkmale zu ignorieren und die gestellte Aufgabe zu lösen.SADALAs Trennung von wichtigen und unwichtigen visuellen Merkmalen führt zu einem robusten Domaintransfer.SADALA übertrifft sowohl frühere RL-Ansätze, die auf einer entmischten Repräsentation basieren, als auch Domänen-Randomisierungsansätze in RL-Umgebungen (Visual Cartpole und DeepMind Lab).
Die Robustheitsüberprüfung, die darauf abzielt, das Vorhersageverhalten neuronaler Netze formal zu zertifizieren, ist zu einem wichtigen Werkzeug geworden, um das Verhalten eines gegebenen Modells zu verstehen und Sicherheitsgarantien zu erhalten.Bisherige Methoden sind jedoch in der Regel auf relativ einfache neuronale Netze beschränkt.In diesem Papier betrachten wir das Problem der Robustheitsüberprüfung für Transformers. Wir lösen diese Herausforderungen und entwickeln den ersten Verifikationsalgorithmus für Transformers.Die zertifizierten Robustheitsgrenzen, die von unserer Methode berechnet werden, sind deutlich enger als die von naiver Interval Bound Propagation.Diese Grenzen werfen auch Licht auf die Interpretation von Transformers, da sie konsequent die Bedeutung von Wörtern in der Sentiment-Analyse widerspiegeln.
In den letzten Jahren war Deep Learning in vielen Anwendungen sehr erfolgreich, aber unser theoretisches Verständnis von Deep Learning und damit die Fähigkeit, prinzipielle Verbesserungen zu erzielen, scheint hinterherzuhinken.Ein theoretisches Rätsel betrifft die Fähigkeit von Deep Networks, gute Vorhersagen zu treffen, obwohl sie scheinbar nicht verallgemeinert werden können: Ihre Klassifizierungsgenauigkeit in der Trainingsmenge ist kein Indikator für ihre Leistung in einer Testmenge. Wie ist es möglich, dass die Trainingsleistung unabhängig von der Testleistung ist?erfordern tiefe Netzwerke tatsächlich eine drastisch neue Theorie der Generalisierung?oder gibt es Messungen, die auf den Trainingsdaten basieren, die die Leistung des Netzwerks bei zukünftigen Daten vorhersagen?hier zeigen wir, dass, wenn die Leistung angemessen gemessen wird, die Trainingsleistung tatsächlich die erwartete Leistung vorhersagt, in Übereinstimmung mit der klassischen Theorie des maschinellen Lernens.
Unsere Arbeit baut auf der synergetischen Beziehung zwischen lokaler modellbasierter Steuerung, globalem Wertfunktionslernen und Exploration auf. Wir untersuchen, wie lokale Trajektorienoptimierung mit Approximationsfehlern in der Wertfunktion fertig werden und das Wertfunktionslernen stabilisieren und beschleunigen kann, und umgekehrt untersuchen wir, wie approximative Wertfunktionen dazu beitragen können, den Planungshorizont zu reduzieren und bessere Strategien jenseits lokaler Lösungen zu ermöglichen. Schließlich zeigen wir auch, wie Trajektorienoptimierung verwendet werden kann, um zeitlich koordinierte Exploration in Verbindung mit der Schätzung der Unsicherheit in der Wertfunktionsapproximation durchzuführen.Diese Exploration ist entscheidend für schnelles und stabiles Lernen der Wertfunktion.Die Kombination dieser Komponenten ermöglicht Lösungen für komplexe Steuerungsaufgaben, wie humanoide Fortbewegung und geschickte Manipulation mit der Hand, im Äquivalent von ein paar Minuten Erfahrung in der realen Welt.
Während herkömmliche explizite Regularisierungstechniken wie Dropout, Gewichtsabnahme und Datenerweiterung in diesen neuen Modellen immer noch verwendet werden, wurden die Regularisierungs- und Generalisierungseffekte dieser neuen Strukturen bisher kaum untersucht. Könnten neuere Architekturen wie ResNet und DenseNet nicht nur tiefer sein als ihre Vorgänger, sondern auch von den impliziten Regularisierungseigenschaften ihrer Strukturen profitieren? Durch Experimente zeigen wir, dass bestimmte Architekturen neuronaler Netze zu ihren Generalisierungsfähigkeiten beitragen, insbesondere untersuchen wir den Effekt, den Low-Level-Merkmale auf die Generalisierungsleistung haben, wenn sie in tiefere Schichten in DenseNet, ResNet sowie Netze mit "Skip Connections" eingeführt werden, und wir zeigen, dass diese Low-Level-Darstellungen bei der Generalisierung in verschiedenen Einstellungen helfen, wenn sowohl die Qualität als auch die Quantität der Trainingsdaten verringert wird.
Eine der Herausforderungen bei der Ausbildung generative Modelle wie die variationale Auto-Encoder (VAE) ist die Vermeidung posterior collapse.When der Generator zu viel Kapazität hat, ist es anfällig für latente code.This Problem wird verschärft, wenn der Datensatz klein ist, und die latente Dimension ist hoch.The Wurzel des Problems ist die ELBO Ziel, insbesondere die Kullback-Leibler (KL) Divergenz Begriff in Zielfunktion.This Papier schlägt eine neue Zielfunktion zu ersetzen, die KL Begriff mit einem, dass die maximale mittlere Diskrepanz (MMD) Ziel emuliert. Ein probabilistisches Autoencoder-Modell mit dem Namen $\mu$-VAE wird entworfen und auf MNIST- und MNIST-Mode-Datensätzen trainiert, wobei die neue Zielfunktion verwendet wird, und es wird gezeigt, dass sie Modelle übertrifft, die mit ELBO und $\beta$-VAE trainiert wurden. Das $\mu$-VAE ist weniger anfällig für einen posterioren Kollaps und kann Rekonstruktionen und neue Muster in guter Qualität erzeugen. Die mit dem $\mu$-VAE gelernten latenten Repräsentationen erweisen sich als gut und können für nachgelagerte Aufgaben wie die Klassifizierung verwendet werden.  
Likelihood-basierte generative Modelle sind eine vielversprechende Ressource, um Out-of-Distribution (OOD)-Eingaben zu erkennen, die die Robustheit oder Zuverlässigkeit eines maschinellen Lernsystems beeinträchtigen könnten.Allerdings haben sich Likelihoods, die aus solchen Modellen abgeleitet werden, als problematisch erwiesen, um bestimmte Arten von Eingaben zu erkennen, die sich signifikant von den Trainingsdaten unterscheiden.In diesem Beitrag stellen wir die These auf, dass dieses Problem auf den übermäßigen Einfluss der Eingabekomplexität auf die Likelihoods generativer Modelle zurückzuführen ist. Wir berichten über eine Reihe von Experimenten, die diese Hypothese stützen, und verwenden eine Schätzung der Eingabekomplexität, um einen effizienten und parameterfreien OOD-Score abzuleiten, der als Likelihood-Ratio, ähnlich dem Bayes'schen Modellvergleich, angesehen werden kann.Wir stellen fest, dass ein solcher Score bei einer Vielzahl von Datensätzen, Modellen, Modellgrößen und Komplexitätsschätzungen eine vergleichbare oder sogar bessere Leistung als bestehende OOD-Erkennungsansätze aufweist.
 Mehrere kürzlich vorgeschlagene stochastische Optimierungsmethoden, die erfolgreich beim Training von tiefen Netzwerken eingesetzt wurden, wie RMSProp, Adam, Adadelta, Nadam, basieren auf der Verwendung von Gradientenaktualisierungen, die durch Quadratwurzeln von exponentiellen gleitenden Durchschnitten quadrierter vergangener Gradienten skaliert werden.In vielen Anwendungen, z. B. beim Lernen mit großen Ausgaberäumen, wurde empirisch beobachtet, dass diese Algorithmen nicht zu einer optimalen Lösung (oder einem kritischen Punkt in nicht-konvexen Einstellungen) konvergieren.Wir zeigen, dass eine Ursache für solche Fehler die in den Algorithmen verwendeten exponentiellen gleitenden Durchschnitte sind. Wir geben ein explizites Beispiel für eine einfache konvexe Optimierungsumgebung, in der Adam nicht zur optimalen Lösung konvergiert, und beschreiben die genauen Probleme mit der bisherigen Analyse des Adam-Algorithmus. Unsere Analyse legt nahe, dass die Konvergenzprobleme behoben werden können, indem solche Algorithmen mit einem ``Langzeitgedächtnis'' für vergangene Gradienten ausgestattet werden, und schlagen neue Varianten des Adam-Algorithmus vor, die nicht nur die Konvergenzprobleme beheben, sondern oft auch zu einer verbesserten empirischen Leistung führen.
Gezieltes Clean-Label-Poisoning ist eine Art feindlicher Angriff auf maschinelle Lernsysteme, bei dem der Angreifer einige korrekt beschriftete, minimal gestörte Proben in die Trainingsdaten injiziert und so das eingesetzte Modell dazu veranlasst, eine bestimmte Testprobe während der Inferenz falsch zu klassifizieren.Obwohl für allgemeine Poisoning-Angriffe (die darauf abzielen, die Gesamtgenauigkeit der Tests zu verringern) Verteidigungsmaßnahmen vorgeschlagen wurden, wurde bisher noch keine zuverlässige Verteidigung für Clean-Label-Angriffe demonstriert, trotz der Effektivität der Angriffe und ihrer realistischen Anwendungsfälle.In dieser Arbeit schlagen wir eine Reihe von einfachen, aber hocheffektiven Verteidigungsmaßnahmen gegen diese Angriffe vor. Nach der Reproduktion ihrer Experimente zeigen wir, dass unsere Abwehrmaßnahmen in der Lage sind, über 99% der Vergiftungsbeispiele in beiden Angriffen zu erkennen und sie zu entfernen, ohne die Leistung des Modells zu beeinträchtigen. Unsere einfachen Abwehrmaßnahmen zeigen, dass aktuelle Clean-Label-Poisoning-Angriffsstrategien ausgehebelt werden können, und dienen als starke, aber einfach zu implementierende Basisabwehr, um zukünftige Clean-Label-Poisoning-Angriffe zu testen.
In dieser Arbeit schlagen wir MXGNet vor, ein mehrschichtiges neuronales Graphen-Netzwerk für Diagrammaufgaben mit mehreren Feldern. MXGNet kombiniert drei leistungsstarke Konzepte, nämlich die Darstellung auf Objektebene, neuronale Graphen-Netzwerke und Multiplex-Graphen, um visuelle Argumentationsaufgaben zu lösen. MXGNet extrahiert zunächst Repräsentationen auf Objektebene für jedes Element in allen Feldern der Diagramme und bildet dann einen mehrschichtigen Multiplex-Graphen, der mehrere Beziehungen zwischen Objekten in verschiedenen Diagrammfeldern erfasst. MXGNet fasst die verschiedenen Graphen zusammen, die aus den Diagrammen der Aufgabe extrahiert wurden, und verwendet diese Zusammenfassung, um die wahrscheinlichste Antwort aus den gegebenen Kandidaten auszuwählen. Wir haben MXGNet an zwei Arten von Diagramm-Schlussfolgern getestet, nämlich Diagramm-Syllogismen und Raven Progressive Matrices (RPM). Für eine Euler-Diagramm-Syllogismus-Aufgabe erreicht MXGNet eine State-of-the-Art-Genauigkeit von 99,8%.  Bei PGM und RAVEN, zwei umfassenden Datensätzen für RPM-Schlussfolgerungen, übertrifft MXGNet die State-of-the-Art-Modelle um ein Vielfaches.
Automatische semantische Strukturextraktion ist der Schlüssel zur automatischen Datentransformation von verschiedenen Tabellenstrukturen in ein kanonisches Schema, um Datenanalyse und Wissensentdeckung zu ermöglichen. Allerdings werden sie durch die verschiedenen Tabellenstrukturen und die räumlich korrelierte Semantik auf Zellrastern herausgefordert. Erstens schlagen wir eine Multi-Task-Framework, das Tabelle Region, strukturelle Komponenten und Zelltypen gemeinsam lernt; zweitens nutzen wir die Fortschritte der jüngsten Sprachmodell, um die Semantik in jeder Zelle Wert zu erfassen; drittens bauen wir eine große menschliche beschrifteten Datensatz mit breiter Abdeckung der Tabelle structures.Our Bewertung zeigt, dass unsere vorgeschlagene Multi-Task-Framework ist sehr effektiv, dass die Ergebnisse der Ausbildung jeder Aufgabe getrennt übertrifft.
Der Vergleich dieser Methoden erfordert ein ganzheitliches Mittel der Dialogbewertung.Menschliche Bewertungen gelten als Goldstandard.Da die menschliche Bewertung ineffizient und kostspielig ist, ist ein automatischer Ersatz wünschenswert.In diesem Papier schlagen wir ganzheitliche Bewertungsmetriken vor, die sowohl die Qualität als auch die Vielfalt der Dialoge erfassen. Unsere Metriken bestehen aus (1) GPT-2-basierter Kontextkohärenz zwischen Sätzen in einem Dialog, (2) GPT-2-basierter Geläufigkeit in der Phrasierung und (3) $n$-Gramm-basierter Vielfalt in den Antworten auf erweiterte Abfragen.Die empirische Validität unserer Metriken wird durch eine starke Korrelation mit menschlichen Urteilen demonstriert.
Convolution Neural Network (CNN) hat enormen Erfolg in der Computer-Vision-Aufgaben mit seiner hervorragenden Fähigkeit, die lokale latente features.Recently, hat es ein zunehmendes Interesse an der Erweiterung CNNs auf die allgemeine räumliche domain.Although verschiedene Arten von Graphen Faltung und geometrische Faltung Methoden wurden vorgeschlagen, ihre Verbindungen zu traditionellen 2D-Faltung sind nicht gut verstanden. In diesem Papier zeigen wir, dass die tiefenweise trennbare Faltung ein Weg ist, um die beiden Arten von Faltungsmethoden in einer mathematischen Sichtweise zu vereinen, woraus wir eine neuartige tiefenweise trennbare Graphenfaltung ableiten, die bestehende Graphenfaltungsmethoden als Spezialfälle unserer Formulierung subsumiert.Experimente zeigen, dass der vorgeschlagene Ansatz andere Graphenfaltungsmethoden und geometrische Faltungsmethoden in Benchmark-Datensätzen in verschiedenen Bereichen durchweg übertrifft.
Generierung von musikalischen Audio direkt mit neuronalen Netzen ist bekanntermaßen schwierig, weil es kohärent Modellierung Struktur auf vielen verschiedenen Zeitskalen erfordert.Glücklicherweise ist die meiste Musik auch stark strukturiert und kann als diskrete Note Ereignisse auf Musikinstrumenten gespielt dargestellt werden.Hierin zeigen wir, dass durch die Verwendung von Noten als Zwischen-Darstellung, können wir eine Reihe von Modellen der Lage zu transkribieren, zu komponieren, und die Synthese von Audio-Wellenformen mit kohärenten musikalischen Struktur auf Zeitskalen überspannt sechs Größenordnungen (~ 0. Dieser große Fortschritt im Stand der Technik wird durch die Veröffentlichung des neuen MAESTRO-Datensatzes (MIDI and Audio Edited for Synchronous TRacks and Organization) ermöglicht, der aus über 172 Stunden virtuoser Klavierdarbietungen besteht, die mit einer feinen Ausrichtung (~3 ms) zwischen Notenbeschriftungen und Audiowellenformen aufgezeichnet wurden.Die Netzwerke und der Datensatz stellen zusammen einen vielversprechenden Ansatz zur Schaffung neuer ausdrucksstarker und interpretierbarer neuronaler Modelle der Musik dar.
Die Variationsinferenz auf der Grundlage der Chi-Quadrat-Divergenzminimierung (CHIVI) bietet eine Möglichkeit zur Annäherung an den Posteriorwert eines Modells, während gleichzeitig eine obere Schranke für die marginale Likelihood ermittelt wird. In der Praxis stützt sich CHIVI jedoch auf Monte-Carlo-Schätzungen einer oberen Schranke, die bei bescheidenen Stichprobenumfängen nicht garantiert sind, dass sie echte Schranken für die marginale Likelihood darstellen. Wir zeigen, dass CHIVI viel empfindlicher auf die Initialisierung reagiert als klassisches VI, das auf KL-Minimierung basiert, oft eine sehr große Anzahl von Stichproben (über eine Million) benötigt und möglicherweise keine zuverlässige obere Schranke darstellt.
Die Anwendung von Mixup in der Ausbildung bietet einen effektiven Mechanismus zur Verbesserung der Generalisierungsleistung und der Robustheit des Modells gegenüber nachteiligen Störungen, der das global lineare Verhalten zwischen den Trainingsbeispielen einführt. In früheren Arbeiten haben die Mixup-ausgebildeten Modelle jedoch nur passiv nachteilige Angriffe bei der Inferenz abgewehrt, indem sie die Eingaben direkt klassifiziert haben, wobei die induzierte globale Linearität nicht gut ausgenutzt wurde. Aufgrund der Lokalität der gegnerischen Störungen wäre es nämlich effizienter, die Lokalität aktiv durch die Globalität der Modellvorhersagen zu brechen.Inspiriert durch eine einfache geometrische Intuition entwickeln wir ein Inferenzprinzip, genannt Mixup-Inferenz (MI), für Mixup-trainierte Modelle. Unsere Experimente mit CIFAR-10 und CIFAR-100 zeigen, dass MI die Robustheit von Modellen, die mit Mixup und seinen Varianten trainiert wurden, gegenüber nachteiligen Einflüssen weiter verbessern kann.
Feinabstimmung von Sprachmodellen wie BERT auf domänenspezifischen Korpora hat sich in Domänen wie wissenschaftlichen Abhandlungen und biomedizinischen Texten als wertvoll erwiesen.In diesem Papier zeigen wir, dass die Feinabstimmung von BERT auf juristischen Dokumenten ebenfalls wertvolle Verbesserungen bei NLP-Aufgaben in der juristischen Domäne bietet.Der Nachweis dieses Ergebnisses ist für die Analyse kommerzieller Verträge von Bedeutung, da die Beschaffung großer juristischer Korpora aufgrund ihres vertraulichen Charakters eine Herausforderung darstellt.Daher zeigen wir, dass der Zugang zu großen juristischen Korpora ein Wettbewerbsvorteil für kommerzielle Anwendungen und akademische Forschung zur Analyse von Verträgen ist.
Unser probabilistischer Ansatz modelliert nicht-parallele Daten aus zwei Domänen als ein teilweise beobachtetes paralleles Korpus. Durch die Hypothese einer parallelen latenten Sequenz, die jede beobachtete Sequenz generiert, lernt unser Modell, Sequenzen von einer Domäne in eine andere auf eine völlig unbeaufsichtigte Weise zu transformieren. Im Gegensatz zu traditionellen generativen Sequenzmodellen (z.B. dem HMM) macht unser Modell nur wenige Annahmen über die Daten, die es generiert: es verwendet ein rekurrentes Sprachmodell als Prior und einen Encoder-Decoder als Transduktionsverteilung.Während die Berechnung der marginalen Datenwahrscheinlichkeit in dieser Modellklasse unpraktikabel ist, zeigen wir, dass amortisierte Variationsinferenz ein praktisches Surrogat erlaubt. Darüber hinaus zeigen wir, wie unsere probabilistische Sichtweise einige bekannte nicht-generative Ziele wie Rückübersetzung und adversen Verlust vereinheitlichen kann, indem wir Verbindungen zwischen unserem Variationsziel und anderen neueren unbeaufsichtigten Stiltransfer- und Maschinenübersetzungstechniken herstellen.Schließlich demonstrieren wir die Effektivität unserer Methode bei einer Vielzahl von unbeaufsichtigten Stiltransferaufgaben, einschließlich Stimmungsübertragung, Formalitätstransfer, Wortentschlüsselung, Autorenimitation und verwandter Sprachübersetzung. Bei allen Aufgaben des Stiltransfers erzielt unser Ansatz erhebliche Gewinne gegenüber dem Stand der Technik bei nicht-generativen Grundverfahren, einschließlich der modernsten nicht-überwachten maschinellen Übersetzungstechniken, die unser Ansatz verallgemeinert; außerdem führen wir Experimente mit einer standardmäßigen nicht-überwachten maschinellen Übersetzungsaufgabe durch und stellen fest, dass unser vereinheitlichter Ansatz dem aktuellen Stand der Technik entspricht.
Die derzeitige Praxis des maschinellen Lernens besteht darin, tiefe Netze in einer überparametrisierten Grenze einzusetzen, wobei die nominale Anzahl der Parameter typischerweise die Anzahl der Messungen übersteigt, was der Situation beim Compressed Sensing oder bei der Sparse Regression mit $l_1$ Straftermen ähnelt und einen theoretischen Weg zum Verständnis von Phänomenen bietet, die im Zusammenhang mit tiefen Netzen auftreten. Ein solches Phänomen ist der Erfolg von tiefen Netzen bei der Bereitstellung einer guten Generalisierung in einem interpolierenden Regime mit Null-Trainingsfehler.Traditionelle statistische Praxis fordert eine Regularisierung oder Glättung, um "Overfitting" (schlechte Generalisierungsleistung) zu verhindern.Jüngste Arbeiten zeigen jedoch, dass es Dateninterpolationsverfahren gibt, die statistisch konsistent sind und eine gute Generalisierungsleistung\cite{belkin2018overfitting} (In diesem Zusammenhang wurde vorgeschlagen, dass "klassische" und "moderne" Regime des maschinellen Lernens durch eine Spitze in der Generalisierungsfehlerkurve ("Risiko") getrennt sind, ein Phänomen, das als "Double Descent" bezeichnet wird.\cite{belkin2019reconciling}.Während solche Überanpassungsspitzen existieren und aus schlecht konditionierten Designmatrizen entstehen, stellen wir hier die Interpretation der Überanpassungsspitze als Abgrenzung des Regimes in Frage, in dem gute Generalisierung unter Überparametrisierung auftritt. Wir schlagen ein Modell der Misparamatrized Sparse Regression (MiSpaR) vor und berechnen analytisch die GE-Kurven für $l_2$ und $l_1$ Strafen. Wir zeigen, dass der Overfitting-Peak, der in der Interpolationsgrenze auftritt, von dem Regime der guten Generalisierung getrennt ist. Wir finden ein weiteres interessantes Phänomen: eine zunehmende Überparametrisierung im Anpassungsmodell erhöht die Sparsamkeit, was intuitiv die Leistung der $l_1$ bestraften Regression verbessern sollte. Dennoch kann die $l_1$ bestrafte Regression unter den Bedingungen der Dateninterpolation auch mit einer großen Menge an Überparametrisierung eine gute Generalisierungsleistung zeigen.Diese Ergebnisse bieten einen theoretischen Weg zur Untersuchung inverser Probleme im interpolierenden Regime unter Verwendung von überparametrisierten Anpassungsfunktionen wie z.B. Deep Nets.
Hashing-basierte kollaborative Filterung lernt binäre Vektorrepräsentationen (Hash-Codes) von Benutzern und Objekten, so dass Empfehlungen sehr effizient mit Hilfe der Hamming-Distanz berechnet werden können, die einfach die Summe der unterschiedlichen Bits zwischen zwei Hash-Codes ist.Ein Problem mit Hashing-basierter kollaborativer Filterung unter Verwendung der Hamming-Distanz ist, dass jedes Bit in der Distanzberechnung gleich gewichtet wird, aber in der Praxis könnten einige Bits wichtigere Eigenschaften als andere Bits kodieren, wobei die Wichtigkeit vom Benutzer abhängt. Zu diesem Zweck schlagen wir einen durchgängig trainierbaren, auf variablem Hashing basierenden kollaborativen Filteransatz vor, der das neuartige Konzept der Selbstmaskierung verwendet: Der Hash-Code des Benutzers fungiert als Maske für die Elemente (unter Verwendung der booleschen UND-Verknüpfung), so dass er lernt, die Bits zu kodieren, die für den Benutzer wichtig sind, und nicht die Präferenz des Benutzers für die zugrunde liegende Elementeigenschaft, die die Bits repräsentieren. Dies ermöglicht eine binäre Gewichtung der Wichtigkeit jedes Elements auf Benutzerebene, ohne dass zusätzliche Gewichtungen für jeden Benutzer gespeichert werden müssen. Wir evaluieren unseren Ansatz experimentell gegenüber den modernsten Basislösungen auf 4 Datensätzen und erzielen signifikante Gewinne von bis zu 12 % bei NDCG. Wir stellen auch eine effiziente Implementierung der Selbstmaskierung zur Verfügung, die experimentell einen Laufzeit-Overhead von <4 % im Vergleich zur Standard-Hamming-Distanz ergibt.
Die Bestimmung der geeigneten Batch-Größe für Mini-Batch-Gradientenabstieg ist immer zeitaufwendig, da es oft auf Grid-Suche beruht.Dieses Papier betrachtet eine resizable Mini-Batch-Gradientenabstieg (RMGD) Algorithmus auf der Grundlage eines Multi-Armed Bandit, dass die Leistung äquivalent zu der besten festen Batch-Größe erreicht.In jeder Epoche, die RMGD Stichproben eine Batch-Größe nach einer bestimmten Wahrscheinlichkeitsverteilung proportional zu einer Charge erfolgreich bei der Verringerung der Verlustfunktion.Sampling aus dieser Wahrscheinlichkeit bietet einen Mechanismus für die Erforschung verschiedener Batch-Größe und die Nutzung Batch-Größen mit Geschichte der Erfolg.  Die experimentellen Ergebnisse zeigen, dass die RMGD eine bessere Leistung erzielt als die beste einzelne Chargengröße. Es ist überraschend, dass die RMGD eine bessere Leistung als die Gittersuche erzielt und dass sie diese Leistung in kürzerer Zeit als die Gittersuche erreicht.
Wissensgraphen haben in den letzten Jahren aufgrund ihrer erfolgreichen Anwendung bei zahlreichen Aufgaben zunehmend an Aufmerksamkeit gewonnen.Trotz der rasanten Entwicklung der Wissenskonstruktion leiden Wissensgraphen immer noch unter starker Unvollständigkeit und sind zwangsläufig mit verschiedenen Arten von Fehlern behaftet.Es wurden bereits mehrere Versuche unternommen, Wissensgraphen zu vervollständigen und Rauschen zu erkennen. In diesem Papier haben wir vorgeschlagen, diese beiden Aufgaben mit einem einheitlichen Generative Adversarial Networks (GAN) Framework zu kombinieren, um eine geräuschbewusste Wissensgrapheneinbettung zu erlernen. Umfangreiche Experimente haben gezeigt, dass unser Ansatz den bestehenden State-of-the-Art-Algorithmen sowohl in Bezug auf die Wissensgraphenvervollständigung als auch auf die Fehlererkennung überlegen ist.
Energie-basierte Modelle (EBMs), auch bekannt als nicht-normalisierte Modelle, haben in letzter Zeit Erfolge in kontinuierlichen Räumen erzielt, wurden aber nicht erfolgreich auf die Modellierung von Textsequenzen angewendet.  Während die Verringerung der Energie bei Trainingsmustern einfach ist, ist es schwierig, (negative) Muster zu finden, bei denen die Energie erhöht werden sollte.   Dies liegt zum Teil daran, dass standardmäßige gradientenbasierte Methoden nicht ohne Weiteres anwendbar sind, wenn die Eingabe hochdimensional und diskret ist.  Hier umgehen wir dieses Problem, indem wir Negative mit Hilfe von vortrainierten autoregressiven Sprachmodellen erzeugen.  Die EBM arbeitet dann in den Residuen des Sprachmodells und wird trainiert, um realen Text von Text zu unterscheiden, der von den autoregressiven Modellen generiert wurde.Wir untersuchen die Generalisierungsfähigkeit von Residuen-EBMs, eine Voraussetzung für deren Einsatz in anderen Anwendungen.  Wir analysieren ausgiebig die Generalisierung für die Aufgabe, zu klassifizieren, ob eine Eingabe maschinell oder menschlich generiert wurde, eine natürliche Aufgabe angesichts des Trainingsverlusts und der Art und Weise, wie wir Negative abbauen.Insgesamt stellen wir fest, dass EBMs bemerkenswert gut auf Änderungen in der Architektur der Generatoren, die Negative produzieren, generalisieren können.
Semi-überwachtes Lernen, d.h. gemeinsames Lernen von gelabelten und nicht gelabelten Proben, ist ein aktives Forschungsthema aufgrund seiner Schlüsselrolle bei der Lockerung menschlicher Annotationsbeschränkungen.Im Kontext der Bildklassifikation konzentrieren sich die jüngsten Fortschritte beim Lernen von nicht gelabelten Proben hauptsächlich auf Konsistenzregulierungsmethoden, die invariante Vorhersagen für verschiedene Störungen von nicht gelabelten Proben fördern.Wir schlagen umgekehrt vor, von nicht gelabelten Daten zu lernen, indem wir weiche Pseudo-Label unter Verwendung der Netzwerkvorhersagen erzeugen. Wir zeigen, dass ein naives Pseudo-Labeling aufgrund des so genannten Confirmation Bias zu falschen Pseudo-Labels führt und demonstrieren, dass Mixup Augmentation und die Festlegung einer Mindestanzahl von gelabelten Proben pro Mini-Batch effektive Regularisierungstechniken sind, um diese zu reduzieren.der vorgeschlagene Ansatz erzielt State-of-the-Art-Ergebnisse in CIFAR-10/100 und Mini-ImageNet, obwohl er viel einfacher ist als andere State-of-the-Art-Methoden.diese Ergebnisse zeigen, dass Pseudo-Labeling die Konsistenzregularisierungsmethoden übertreffen kann, während in früheren Arbeiten das Gegenteil angenommen wurde.der Code wird zur Verfügung gestellt.
Modellfreies Reinforcement Learning (RL) hat sich als ein leistungsfähiges, allgemeines Werkzeug zum Erlernen komplexer Verhaltensweisen erwiesen, aber seine Stichprobeneffizienz ist oft unpraktisch groß für die Lösung anspruchsvoller Probleme in der realen Welt, selbst für Algorithmen ohne Strategie wie Q-Learning. Ein einschränkender Faktor im klassischen modellfreien RL ist, dass das Lernsignal nur aus skalaren Belohnungen besteht und viele der reichhaltigen Informationen, die in den Zustandsübergangs-Tupeln enthalten sind, ignoriert werden.Modellbasiertes RL nutzt diese Informationen, indem es ein prädiktives Modell trainiert, erreicht aber aufgrund von Modellverzerrungen oft nicht die gleiche asymptotische Leistung wie modellfreies RL. TDMs kombinieren die Vorteile von modellfreiem und modellbasiertem RL: Sie nutzen die reichhaltigen Informationen in den Zustandsübergängen, um sehr effizient zu lernen, und erreichen dabei eine asymptotische Leistung, die die von direkten modellbasierten RL-Methoden übertrifft. Unsere experimentellen Ergebnisse zeigen, dass TDMs bei einer Reihe von kontinuierlichen Steuerungsaufgaben eine wesentliche Verbesserung der Effizienz im Vergleich zu modernen modellbasierten und modellfreien Methoden bieten.
Wir stellen eine neuronale Architektur vor, um eine amortisierte approximative Bayes'sche Inferenz über latente zufällige Permutationen zweier Objektmengen durchzuführen. Die Methode beinhaltet die Approximation von Permanenzen von Matrizen paarweiser Wahrscheinlichkeiten unter Verwendung neuerer Ideen über Funktionen, die über Mengen definiert sind. Jede abgetastete Permutation wird mit einer Wahrscheinlichkeitsschätzung geliefert, eine Größe, die in MCMC-Ansätzen nicht verfügbar ist.
Maschinell erlernte, groß angelegte Retrievalsysteme erfordern eine große Menge an Trainingsdaten, die die Relevanz von Suchanfragen repräsentieren, wobei die Sammlung von explizitem Feedback der Nutzer sehr kostspielig ist. Durch die Einführung von Hilfsaufgaben, die mit sehr viel umfangreicheren impliziten Benutzer-Feedback-Daten trainiert werden, verbessern wir die Qualität und Auflösung der gelernten Repräsentationen von Anfragen und Artikeln.Die Anwendung dieser gelernten Repräsentationen auf ein industrielles Retrievalsystem hat zu erheblichen Verbesserungen geführt.
Die Fähigkeit zur autonomen Erkundung und Navigation eines physischen Raums ist eine grundlegende Anforderung für praktisch jeden mobilen autonomen Agenten, vom Haushaltsstaubsauger bis hin zu autonomen Fahrzeugen.Traditionelle SLAM-basierte Ansätze zur Erkundung und Navigation konzentrieren sich weitgehend auf die Nutzung der Geometrie der Szene, können aber keine dynamischen Objekte (wie andere Agenten) oder semantische Einschränkungen (wie nasse Böden oder Türöffnungen) modellieren.Lernbasierte RL-Agenten sind eine attraktive Alternative, da sie sowohl semantische als auch geometrische Informationen einbeziehen können, sind aber notorisch ineffiziente Proben, schwer zu verallgemeinern, um neue Einstellungen und sind schwer zu interpretieren. In diesem Beitrag kombinieren wir das Beste aus beiden Welten mit einem modularen Ansatz, der eine räumliche Repräsentation einer Szene erlernt, die so trainiert wird, dass sie in Verbindung mit traditionellen geometrischen Planern effektiv ist: Wir entwerfen einen Agenten, der lernt, eine räumliche Affordanzkarte vorherzusagen, die durch aktives, selbstüberwachtes Sammeln von Erfahrungen aufklärt, welche Teile einer Szene navigierbar sind. Im Gegensatz zu den meisten Simulationsumgebungen, die von einer statischen Welt ausgehen, evaluieren wir unseren Ansatz im VizDoom-Simulator, indem wir große, zufällig generierte Karten mit einer Vielzahl von dynamischen Akteuren und Gefahren verwenden.
