Mixed-Precision-Training (MPT) wird zu einer praktischen Technik, um die Geschwindigkeit und Energieeffizienz des Trainings von tiefen neuronalen Netzen zu verbessern, indem die schnelle Hardware-Unterstützung für IEEE Half-Precision Floating Point, die in bestehenden GPUs verfügbar ist, genutzt wird. MPT wird in der Regel in Kombination mit einer Technik namens Verlustskalierung verwendet, bei der der Verlustwert vor dem Start der Backpropagation hochskaliert wird, um die Auswirkungen eines numerischen Unterlaufs auf das Training zu minimieren.Leider machen bestehende Methoden diesen Verlustskalierungswert zu einem Hyperparameter, der pro Modell eingestellt werden muss, und eine einzelne Skalierung kann nicht an verschiedene Schichten in verschiedenen Trainingsstufen angepasst werden. Wir führen eine auf Verlustskalierung basierende Trainingsmethode ein, die als adaptive Verlustskalierung bezeichnet wird und die MPT einfacher und praktischer macht, indem sie die Notwendigkeit beseitigt, einen modellspezifischen Hyperparameter für die Verlustskalierung einzustellen.Wir erreichen dies, indem wir schichtweise Verlustskalierungswerte einführen, die während des Trainings automatisch berechnet werden, um Unterlauf effektiver als bei bestehenden Methoden zu behandeln.Wir präsentieren experimentelle Ergebnisse zu einer Vielzahl von Netzwerken und Aufgaben, die zeigen, dass unser Ansatz die Zeit bis zur Konvergenz verkürzen und die Genauigkeit verbessern kann, verglichen mit der Verwendung der bestehenden hochmodernen MPT und der Verwendung von Gleitkommazahlen mit einfacher Genauigkeit.
Dies stellt eine Herausforderung für herkömmliche tiefe neuronale Netze dar, die von Natur aus mit strukturierten Ausgaben wie Vektoren, Matrizen oder Tensoren umgehen. Wir präsentieren einen neuartigen Ansatz für das Lernen, um Mengen mit unbekannter Permutation und Kardinalität mit tiefen neuronalen Netzen vorherzusagen. Wir demonstrieren die Gültigkeit dieser neuen Formulierung an zwei relevanten Sehproblemen: Objekterkennung, für die unsere Formulierung die modernsten Detektoren wie Faster R-CNN und YOLO übertrifft, und ein komplexer CAPTCHA-Test, bei dem wir überraschenderweise beobachten, dass unser mengenbasiertes Netzwerk die Fähigkeit erlangt hat, Arithmetik zu imitieren, ohne dass irgendwelche Regeln kodiert werden.
Es gibt jedoch nur wenige systematische Vergleiche zwischen tiefen Netzwerken mit und ohne Foveation und zwischen verschiedenen Downsampling-Methoden mit variabler Auflösung. Hier definieren wir mehrere solcher Methoden und vergleichen ihre Leistung bei der ImageNet-Erkennung mit einem Densenet-121-Netzwerk. Die beste Methode mit variabler Auflösung übertrifft das einheitliche Downsampling leicht.
Wir untersuchen das Konzept des Co-Designs im Zusammenhang mit der Verifikation von neuronalen Netzen und zielen darauf ab, tiefe neuronale Netze zu trainieren, die nicht nur robust gegenüber Störungen sind, sondern deren Robustheit auch leichter verifiziert werden kann.Zu diesem Zweck identifizieren wir zwei Eigenschaften von Netzmodellen - Gewichtsdistanz und sogenannte ReLU-Stabilität -, die die Komplexität der entsprechenden Verifikationsaufgabe erheblich beeinflussen. Wir zeigen, dass bereits die Verbesserung der Gewichtsdistanz allein uns in die Lage versetzt, rechnerisch unlösbare Verifikationsprobleme in überschaubare zu verwandeln, und dass die Verbesserung der ReLU-Stabilität zu einer zusätzlichen Beschleunigung der Verifikationszeiten um das 4-13fache führt.Ein wichtiges Merkmal unserer Methodik ist ihre "Universalität" in dem Sinne, dass sie mit einer breiten Palette von Trainingsverfahren und Verifikationsansätzen verwendet werden kann.
Die Batch-Normalisierung (BatchNorm) hat sich als effektiv erwiesen, um das Training von tiefen neuronalen Netzen zu verbessern und zu beschleunigen, aber es hat sich gezeigt, dass sie auch anfällig für Störungen durch Angreifer ist.In dieser Arbeit wollen wir die Ursache für die Anfälligkeit der BatchNorm durch Angreifer untersuchen. Wir stellen die Hypothese auf, dass die Verwendung unterschiedlicher Normalisierungsstatistiken während des Trainings und der Inferenz (Mini-Batch-Statistiken für das Training und gleitender Durchschnitt dieser Werte bei der Inferenz) die Hauptursache für die Anfälligkeit der BatchNorm-Schicht ist.Wir haben dies durch Experimente mit verschiedenen neuronalen Netzwerkarchitekturen und Datensätzen empirisch bewiesen.Darüber hinaus führen wir die Robuste Normalisierung (RobustNorm) ein und zeigen experimentell, dass sie nicht nur resistent gegen negative Störungen ist, sondern auch die Vorteile der BatchNorm übernimmt.
Trotz einer großen Anzahl von Imputationsmethoden, die vorgeschlagen werden, um diese Probleme anzugehen, ignorieren die meisten der existierenden Methoden korrelierte Merkmale oder zeitliche Dynamik und lassen die Unsicherheit völlig beiseite, insbesondere, da die Schätzungen der fehlenden Werte das Risiko haben, ungenau zu sein, was uns dazu motiviert, die Aufmerksamkeit auf zuverlässige und weniger sichere Informationen zu lenken. In dieser Arbeit schlagen wir ein neuartiges variational-rekurrentes Imputationsnetzwerk (V-RIN) vor, das Imputation und Vorhersagenetzwerk vereint, indem es die korrelierten Merkmale und die zeitliche Dynamik berücksichtigt und die Ungewissheit weiter nutzt, um das Risiko voreingenommener Schätzungen fehlender Werte zu verringern. Insbesondere nutzen wir das tiefe generative Modell, um die fehlenden Werte auf der Grundlage der Verteilung zwischen den Variablen zu schätzen, und ein rekurrentes Imputationsnetzwerk, um die zeitlichen Beziehungen in Verbindung mit der Nutzung der Unsicherheit auszunutzen.
Trotz der State-of-the-Art-Genauigkeit von Deep Neural Networks (DNN) in verschiedenen Klassifizierungsproblemen bleibt ihr Einsatz auf ressourcenbeschränkten Edge-Computing-Geräten aufgrund ihrer großen Größe und Komplexität eine Herausforderung.Mehrere aktuelle Studien haben bemerkenswerte Ergebnisse bei der Reduzierung dieser Komplexität durch Quantisierung von DNN-Modellen berichtet. Diese Studien berücksichtigen jedoch in der Regel weder die Änderungen der Verlustfunktion bei der Quantisierung noch die unterschiedliche Bedeutung der DNN-Modellparameter für die Genauigkeit. In diesem Papier gehen wir auf diese Probleme ein, indem wir eine neue Methode, die sogenannte adaptive Quantisierung, vorschlagen, die ein trainiertes DNN-Modell vereinfacht, indem sie eine eindeutige, optimale Präzision für jeden Netzwerkparameter findet, so dass der Verlustanstieg minimiert wird. Das Optimierungsproblem, das dieser Methode zugrunde liegt, verwendet iterativ den Gradienten der Verlustfunktion, um eine Fehlerspanne für jeden Parameter zu bestimmen, und weist ihm eine entsprechende Präzision zu. Da dieses Problem lineare Funktionen verwendet, ist es rechnerisch günstig und hat, wie wir zeigen werden, eine geschlossene Näherungslösung.
Wir untersuchen das Problem des Lernens permutationsinvarianter Repräsentationen, die Containment-Relationen erfassen können, und schlagen vor, ein Modell für eine neuartige Aufgabe zu trainieren: die Vorhersage der Größe der symmetrischen Differenz zwischen Paaren von Multisets, also Mengen, die mehrere Kopien desselben Objekts enthalten können, mit Motivation aus der Fuzzy-Set-Theorie. Wir modellieren Multiset-Elemente als Vektoren auf dem Standard-Simplex und Multisets als Summen solcher Vektoren, und wir sagen die symmetrische Differenz als l1-Distanz zwischen Multiset-Darstellungen voraus.wir zeigen, dass unsere Darstellungen die Größen symmetrischer Differenzen effektiver vorhersagen als DeepSets-basierte Ansätze mit uneingeschränkten Objektdarstellungen.außerdem zeigen wir, dass das Modell sinnvolle Darstellungen lernt, die Objekte verschiedener Klassen auf verschiedene Standard-Basisvektoren abbilden.
Es ist wichtig, glaubwürdige Trainingsmuster $(x,y)$ zu sammeln, um datenintensive Lernsysteme (z.B. ein Deep-Learning-System) zu entwickeln.In der Literatur gibt es eine Reihe von Studien, die sich mit der Gewinnung von Verteilungsinformationen von selbstinteressierten Agenten befassen, die über eine relevante Information verfügen.  Dies liegt vor allem an der hohen kognitiven Belastung, die für menschliche Agenten erforderlich ist, um diese hochdimensionale Information zu verstehen und zu berichten.Betrachten wir das Beispiel, in dem wir daran interessiert sind, einen Bildklassifikator zu erstellen, indem wir zunächst eine bestimmte Kategorie von hochdimensionalen Bilddaten sammeln.Während die klassischen Ergebnisse für die Erhebung einer komplexen und generativen (und kontinuierlichen) Verteilung $p(x)$ für diese Bilddaten gelten, sind wir daran interessiert, Proben $x_i \sim p(x)$ von Agenten zu erheben. Die Herausforderung besteht darin, eine anreizkompatible Bewertungsfunktion für jede gemeldete Probe zu entwerfen, um wahrheitsgemäße Berichte zu induzieren, anstatt eine willkürliche oder sogar gegnerische. Wir zeigen, dass wir mit einer genauen Schätzung einer bestimmten $f$-Divergenzfunktion in der Lage sind, eine annähernde Anreizkompatibilität bei der Erhebung wahrheitsgemäßer Proben zu erreichen.Wir präsentieren dann einen effizienten Schätzer mit theoretischer Garantie durch die Untersuchung der Variationsformen der $f$-Divergenzfunktion.Unsere Arbeit ergänzt die Literatur zur Informationserhebung durch die Einführung des Problems der \emph{Probenerhebung}.  Wir zeigen auch eine Verbindung zwischen diesem Problem der Stichprobenerhebung und $f$-GAN, und wie diese Verbindung helfen kann, einen Schätzer der Verteilung auf der Grundlage der gesammelten Stichproben zu rekonstruieren.
Die berühmte Sequence-to-Sequence-Lerntechnik (Seq2Seq) und ihre zahlreichen Varianten erzielen bei vielen Aufgaben eine hervorragende Leistung. Viele Aufgaben des maschinellen Lernens haben jedoch Eingaben, die natürlicherweise als Graphen dargestellt werden; bestehende Seq2Seq-Modelle stehen vor einer großen Herausforderung, wenn es darum geht, eine genaue Konvertierung von der Graphenform in die entsprechende Sequenz zu erreichen.Um diese Herausforderung zu bewältigen, stellen wir eine allgemeine neuronale End-to-End-Graph-to-Sequence-Encoder-Decoder-Architektur vor, die einen Eingabegraphen auf eine Sequenz von Vektoren abbildet und eine aufmerksamkeitsbasierte LSTM-Methode verwendet, um die Zielsequenz aus diesen Vektoren zu decodieren. Unsere Methode generiert zunächst die Knoten- und Grapheneinbettungen unter Verwendung eines verbesserten graphenbasierten neuronalen Netzwerks mit einer neuartigen Aggregationsstrategie, um die Kantenrichtungsinformationen in die Knoteneinbettungen einzubeziehen, und führt einen Aufmerksamkeitsmechanismus ein, der die Knoteneinbettungen und die Dekodiersequenz aufeinander abstimmt, um große Graphen besser bewältigen zu können. Experimentelle Ergebnisse zu bAbI-, Shortest Path- und Natural Language Generation-Aufgaben zeigen, dass unser Modell den Stand der Technik erreicht und bestehende neuronale Graphen-Netzwerke, Seq2Seq- und Tree2Seq-Modelle deutlich übertrifft; mit der vorgeschlagenen bi-direktionalen Knoteneinbettungs-Aggregationsstrategie kann das Modell schnell zur optimalen Leistung konvergieren.
Wir befassen uns mit dem Problem des Lernens zur Entdeckung von 3D-Teilen für Objekte in unbekannten Kategorien. Die Fähigkeit, die Geometrie von Teilen zu erlernen und diese Prioritäten auf unbekannte Kategorien zu übertragen, stellt eine fundamentale Herausforderung für datengesteuerte Formsegmentierungsansätze dar.formuliert als kontextuelles Bandit-Problem, schlagen wir einen lernbasierten iterativen Gruppierungsrahmen vor, der eine Gruppierungspolitik erlernt, um schrittweise kleine Teilvorschläge in größere Vorschläge in einer Bottom-up-Methode zusammenzuführen. Der Kern unseres Ansatzes besteht darin, den lokalen Kontext für die Extraktion von Merkmalen auf Teilebene einzuschränken, was die Verallgemeinerbarkeit auf neue Kategorien fördert. An einem kürzlich vorgeschlagenen großen, feinkörnigen 3D-Teiledatensatz, PartNet, zeigen wir, dass unsere Methode das Wissen über Teile, das wir aus drei Trainingskategorien gelernt haben, auf 21 unbekannte Testkategorien übertragen kann, ohne dass wir irgendwelche kommentierten Proben sehen.
Dieses Papier stellt die ballistische Graph neuronale Netzwerk.ballistische Graph neuronale Netzwerk befasst sich mit der Gewichtsverteilung aus einer Transport-Perspektive und hat viele verschiedene Eigenschaften im Vergleich zu den traditionellen Graphen neuronale Netzwerk-Pipeline.die ballistische Graph neuronale Netzwerk erfordert keine Eigenwerte zu berechnen.die Filter propagieren exponentiell schneller ($\sigma^2 \sim T^2$) im Vergleich zu traditionellen Graphen neuronalen Netzwerk ($\sigma^2 \sim T$). Unsere Ergebnisse zeigen, dass durch die Auswahl der Diffusionsgeschwindigkeit, das Netzwerk eine ähnliche Genauigkeit mit weniger Parametern erreichen kann.Wir zeigen auch, dass die gestörten Filter als bessere Darstellungen im Vergleich zu reinen ballistischen diejenigen handeln.Wir bieten eine neue Perspektive der Ausbildung Graph neuronales Netz, durch die Anpassung der Diffusionsrate, die Leistung des neuronalen Netzes verbessert werden kann.
In diesem Papier schlagen wir einen Rahmen für neuronale Ranking-Aufgaben vor, der auf dem Datenprogrammierungsparadigma \citep{Ratner2016} basiert und es uns ermöglicht, mehrere schwache Überwachungssignale aus verschiedenen Quellen zu nutzen.Empirisch betrachten wir zwei Quellen für schwache Überwachungssignale, unbeaufsichtigte Ranking-Funktionen und semantische Merkmalsähnlichkeiten. Wir trainieren ein BERT-basiertes Passagen-Ranking-Modell (das neue State-of-the-Art-Leistungen auf zwei Benchmark-Datensätzen mit voller Überwachung erreicht) in unserem schwachen Überwachungs-Framework.Ohne die Verwendung von Ground-Truth-Trainings-Labels, übertreffen BERT-PR-Modelle die BM25-Baseline mit großem Abstand auf allen drei Datensätzen und schlagen sogar die vorherigen State-of-the-Art-Ergebnisse mit voller Überwachung auf zwei der Datensätze.
Wir untersuchen den Trainingsprozess von Deep Neural Networks (DNNs) aus der Perspektive der Fourier-Analyse und demonstrieren ein sehr universelles Frequenz-Prinzip (F-Prinzip) --- DNNs passen sich oft Zielfunktionen von niedrigen zu hohen Frequenzen an --- auf hochdimensionalen Benchmark-Datensätzen wie MNIST/CIFAR10 und Deep Networks wie VGG16.Dieses F-Prinzip von DNNs steht im Gegensatz zum Lernverhalten der meisten konventionellen iterativen numerischen Verfahren (z.B. Jacobi-Methode), Dieses F-Prinzip von DNNs steht im Gegensatz zum Lernverhalten der meisten konventionellen iterativen numerischen Verfahren (z.B. Jacobi-Methode), die bei verschiedenen wissenschaftlichen Berechnungsproblemen eine schnellere Konvergenz für höhere Frequenzen aufweisen.Mit einer naiven Theorie zeigen wir, dass dieses F-Prinzip aus der Regelmäßigkeit der üblicherweise verwendeten Aktivierungsfunktionen resultiert.Das F-Prinzip impliziert eine implizite Verzerrung, dass DNNs dazu neigen, Trainingsdaten durch eine Funktion mit niedriger Frequenz anzupassen.Dieses Verständnis liefert eine Erklärung für die gute Generalisierung von DNNs auf den meisten realen Datensätzen und die schlechte Generalisierung von DNNs auf Paritätsfunktionen oder randomisierten Datensätzen.
Das Problem der Beschleunigung der Entdeckung von Medikamenten hängt stark von automatischen Werkzeugen zur Optimierung von Vorläufermolekülen ab, um ihnen bessere biochemische Eigenschaften zu verleihen.Unsere Arbeit in diesem Papier erweitert den bisherigen Stand der Technik bei der Übersetzung von Graphen in Graphen für die molekulare Optimierung. Darüber hinaus ist unser Graphendekoder vollständig autoregressiv und verschachtelt jeden Schritt des Hinzufügens einer neuen Substruktur mit dem Prozess der Auflösung ihrer Bindung an das entstehende Molekül.Wir evaluieren unser Modell an mehreren molekularen Optimierungsaufgaben und zeigen, dass unser Modell die bisherigen State-of-the-Art-Baselines deutlich übertrifft.
Obwohl einige Kombinationen von Transformationen niemals auftreten können (z.B. ein aufrechtes Gesicht mit einer horizontalen Nase), berücksichtigen aktuelle äquivariante Architekturen die Menge aller möglichen Transformationen in einer Transformationsgruppe, wenn sie Merkmalsrepräsentationen lernen.Im Gegensatz dazu ist das menschliche visuelle System in der Lage, die Menge der relevanten Transformationen in der Umgebung zu beachten und diese Informationen zu nutzen, um die Objekterkennung zu unterstützen und zu verbessern. Basierend auf dieser Beobachtung modifizieren wir konventionelle äquivariante Merkmalszuordnungen so, dass sie in der Lage sind, auf die Menge der gemeinsam auftretenden Transformationen in den Daten zu achten, und verallgemeinern diesen Begriff, um auf Gruppen zu wirken, die aus mehreren Symmetrien bestehen. Wir zeigen, dass unsere vorgeschlagenen ko-attentiven äquivarianten neuronalen Netze durchweg konventionelle rotationsäquivariante und rotations- & reflexionsäquivariante neuronale Netze auf gedrehten MNIST und CIFAR-10 übertreffen.
In dieser Studie trainieren wir Generative Adversarial Networks (GANs), um Protein-Rückgrate mit fester Länge zu generieren, mit dem Ziel, aus der Verteilung realistischer 3-D-Rückgratfragmente zu sampeln. Wir zeigen, dass Interpolationen im latenten Raum des Generators glatten Verformungen der Ausgangsrückgrate entsprechen und dass Teststrukturen, die der Generator während des Trainings nicht gesehen hat, in seinem Bild existieren. Schließlich führen wir Sequenzdesign, Relaxation und Ab-Initio-Faltung einer Teilmenge der generierten Strukturen durch und zeigen, dass wir in einigen Fällen die generierten Faltungen nach Vorwärtsfaltung wiederherstellen können.
Few-Shot Learning (Lernen mit begrenzten markierten Daten) zielt darauf ab, die Beschränkungen traditioneller maschineller Lernansätze zu überwinden, die Tausende von markierten Beispielen benötigen, um ein effektives Modell zu trainieren, das als Merkmal menschlicher Intelligenz angesehen wird. Die meisten existierenden Arbeiten gehen davon aus, dass sowohl die Trainings- als auch die Testaufgaben aus der gleichen Verteilung gezogen werden und dass eine große Menge an gelabelten Daten in den Trainingsaufgaben verfügbar ist - eine sehr starke Annahme, die den Einsatz von Meta-Learning-Strategien in der realen Welt einschränkt, in der eine große Anzahl von Trainingsaufgaben, die der gleichen Verteilung wie die Testaufgaben folgen, nicht verfügbar ist. In diesem Papier schlagen wir ein neuartiges Meta-Lern-Paradigma vor, bei dem ein "few-shot"-Lernmodell erlernt wird, das gleichzeitig die Domänenverschiebung zwischen den Trainings- und Testaufgaben durch eine gegnerische Domänenanpassung überwindet.Wir demonstrieren die Wirksamkeit der vorgeschlagenen Methode durch umfangreiche Experimente.
Universelle probabilistische Programmiersysteme (PPSs) bieten einen leistungsstarken Rahmen für die Spezifikation von umfangreichen und komplexen probabilistischen Modellen, aber diese Ausdruckskraft hat den Preis, dass sie den Prozess des Ziehens von Schlussfolgerungen aus dem Modell erheblich erschweren, insbesondere wenn die Unterstützung des Modells zwischen den Ausführungen variiert: Divide, Conquer, and Combine (DCC) teilt das Programm in separate, geradlinige Unterprogramme auf, von denen jedes eine feste Unterstützung hat, so dass leistungsfähigere Inferenzalgorithmen lokal ausgeführt werden können, bevor ihre Ergebnisse auf eine prinzipielle Art und Weise rekombiniert werden.Wir zeigen, wie DCC als automatisierte und universelle PPS-Inferenzmaschine implementiert werden kann, und bestätigen empirisch, dass es erhebliche Leistungsverbesserungen gegenüber früheren Ansätzen bieten kann.
Die Erkennung von Gemeinschaften oder der modularen Struktur von realen Netzwerken (z.B. ein soziales Netzwerk oder ein Produktkaufnetzwerk) ist eine wichtige Aufgabe, da die Art und Weise, wie ein Netzwerk funktioniert, oft durch seine Gemeinschaften bestimmt wird.Die traditionellen Ansätze zur Erkennung von Gemeinschaften beinhalten modularitätsbasierte Ansätze, die im Allgemeinen Partitionen auf der Grundlage von Heuristiken konstruieren, die versuchen, das Verhältnis der Kanten innerhalb der Partitionen zu denen zwischen ihnen zu maximieren. Ansätze zur Knoteneinbettung, die jeden Knoten in einem Graphen als alwertigen Vektor darstellen, wandeln das Problem der Erkennung von Gemeinschaften in einem Graphen in das Problem der Clusterung eines Satzes von Vektoren um.Bestehende Ansätze zur Knoteneinbettung basieren in erster Linie darauf, dass von jedem Knoten aus gleichmäßige zufällige Spaziergänge initiiert werden, um einen Kontext eines Knotens zu konstruieren, und dann versucht wird, die Vektordarstellung des Knotens nahe an seinen Kontext zu bringen. Um dieses Problem zu lösen, untersuchen wir zwei verschiedene Arbeitsstränge: Erstens untersuchen wir die Verwendung von voreingenommenen Random Walks (insbesondere auf maximaler Entropie basierende Walks), um eine zentralitätserhaltende Einbettung von Knoten zu erhalten, was unserer Hypothese nach zu effektiveren Clustern im eingebetteten Raum führen kann. Zweitens schlagen wir einen gemeinschaftsstrukturbewussten Knoteneinbettungsansatz vor, bei dem wir modularitätsbasierte Partitionierungsheuristiken in die Zielfunktion der Knoteneinbettung einbeziehen. Wir zeigen, dass unser vorgeschlagener Ansatz für die Gemeinschaftsdetektion eine Reihe von modularitätsbasierten Grundlinien sowie K-means auf einem Standardknoteneinbettungsvektorraum (insbesondere node2vec) auf einer breiten Palette von realen Netzwerken unterschiedlicher Größe und Dichte übertrifft.
Eine Punktwolke ist eine agile 3D-Darstellung, die die Oberflächengeometrie eines Objekts effizient modelliert. Diese oberflächenzentrierten Eigenschaften stellen jedoch auch eine Herausforderung für die Entwicklung von Werkzeugen zur Erkennung und Synthese von Punktwolken dar.Diese Arbeit stellt ein neuartiges autoregressives Modell, PointGrow, vor, das realistische Punktwolkenproben von Grund auf oder bedingt durch gegebene semantische Kontexte generiert.Unser Modell arbeitet rekurrent, wobei jeder Punkt gemäß einer bedingten Verteilung aufgrund seiner zuvor generierten Punkte abgetastet wird. Da die Formen von Punktwolken typischerweise durch weitreichende Abhängigkeiten zwischen den Punkten kodiert werden, erweitern wir unser Modell mit speziellen Modulen zur Selbstbeobachtung, um diese Beziehungen zu erfassen.Umfangreiche Evaluierungen zeigen, dass PointGrow sowohl bei der unbedingten als auch bei der bedingten Generierung von Punktwolken eine zufriedenstellende Leistung in Bezug auf Genauigkeit, Vielfalt und semantische Bewahrung erzielt.Darüber hinaus lernt das bedingte PointGrow eine glatte Mannigfaltigkeit gegebener Bilder, in denen 3D-Forminterpolation und arithmetische Berechnungen durchgeführt werden können.
Verstärkungslernen und evolutionäre Algorithmen können verwendet werden, um ausgeklügelte Steuerungslösungen zu erstellen.Leider kann die Erklärung, wie diese Lösungen funktionieren, aufgrund ihrer "Blackbox"-Natur schwierig sein.Darüber hinaus verhindert die zeitlich ausgedehnte Natur von Steuerungsalgorithmen oft die direkte Anwendung von Erklärbarkeitstechniken, die für standardmäßige überwachte Lernalgorithmen verwendet werden.In diesem Papier wird versucht, die Erklärbarkeit von Blackbox-Steuerungsalgorithmen durch sechs verschiedene Techniken anzugehen:1) Bayes'sche Regellisten,2) Funktionsanalyse,3) Integrierte Einzelzeitschritt-Gradienten,4) Diese Techniken werden an einer einfachen 2D-Domäne getestet, in der ein simulierter Rover versucht, durch Hindernisse zu navigieren, um ein Ziel zu erreichen. Zur Steuerung verwendet dieser Rover eine weiterentwickelte mehrschichtige Wahrnehmung, die ein 8D-Feld von Hindernis- und Zielsensoren auf eine Aktion abbildet, die bestimmt, wohin er im nächsten Zeitschritt gehen soll.
Die Vision-and-Language Navigation (VLN) Aufgabe beinhaltet, dass ein Agent Navigationsanweisungen in fotorealistischen, unbekannten Umgebungen befolgt. Diese anspruchsvolle Aufgabe erfordert, dass der Agent sich bewusst ist, welche Anweisung abgeschlossen wurde, welche Anweisung als nächstes benötigt wird, welchen Weg er einschlagen muss und wie weit er mit seiner Navigation in Richtung des Ziels fortgeschritten ist.In diesem Beitrag stellen wir einen selbstüberwachenden Agenten mit zwei komplementären Komponenten vor: (1) ein visuell-textuelles Co-Grounding-Modul, um die in der Vergangenheit abgeschlossene Anweisung, die für die nächste Aktion erforderliche Anweisung und die nächste Bewegungsrichtung aus den Umgebungsbildern zu lokalisieren, und (2) einen Fortschrittsmonitor, um sicherzustellen, dass die geerdete Anweisung den Navigationsfortschritt korrekt wiedergibt. Wir testen unseren selbstüberwachenden Agenten an einem Standard-Benchmark und analysieren unseren vorgeschlagenen Ansatz durch eine Reihe von Ablationsstudien, die die Beiträge der primären Komponenten verdeutlichen. Mit der von uns vorgeschlagenen Methode setzen wir den neuen Stand der Technik mit einer signifikanten Marge (8% absolute Steigerung der Erfolgsrate auf dem ungesehenen Testset). Der Code ist verfügbar unter https://github.com/chihyaoma/selfmonitoring-agent.
Während gängige Methoden diese Geschichte mit Hilfe eines rekurrenten neuronalen Netzes (RNN) darstellen, schlagen wir in diesem Papier eine alternative Darstellung vor, die auf der Aufzeichnung der vergangenen Ereignisse basiert, die in einer gegebenen Episode beobachtet wurden.Inspiriert durch das menschliche Gedächtnis, beschreiben diese Ereignisse nur wichtige Veränderungen in der Umgebung und werden in unserem Ansatz automatisch durch Selbstüberwachung entdeckt. Wir evaluieren unsere Methode der Geschichtsdarstellung anhand von zwei anspruchsvollen RL-Benchmarks: einige Spiele der Atari-57-Suite und die 3D-Umgebung Obstacle Tower, mit denen wir die Vorteile unserer Lösung gegenüber herkömmlichen RNN-basierten Ansätzen zeigen.
Die unbedingte Generierung von Bildern mit hoher Wiedergabetreue ist seit langem ein Maßstab für die Prüfung der Leistung von Bilddecodern.Autoregressive Bildmodelle sind in der Lage, kleine Bilder bedingungslos zu generieren, aber die Ausweitung dieser Methoden auf große Bilder, bei denen die Wiedergabetreue besser beurteilt werden kann, ist ein offenes Problem geblieben. Zu den größten Herausforderungen gehören die Fähigkeit, den enormen vorherigen Kontext zu kodieren, und die schiere Schwierigkeit, eine Verteilung zu erlernen, die sowohl die globale semantische Kohärenz als auch die Detailgenauigkeit bewahrt.Um die erste Herausforderung anzugehen, schlagen wir das Subscale Pixel Network (SPN) vor, eine bedingte Decoder-Architektur, die ein Bild als eine Sequenz von Bildscheiben gleicher Größe erzeugt. Das SPN erfasst kompakt bildweite räumliche Abhängigkeiten und benötigt nur einen Bruchteil des Speichers und der Berechnungen. Um die letztgenannte Herausforderung zu bewältigen, schlagen wir eine mehrdimensionale Hochskalierung vor, um ein Bild sowohl in der Größe als auch in der Tiefe über Zwischenstufen zu vergrößern, die verschiedenen SPNs entsprechen. Wir evaluieren SPNs auf derunconditional Generation von CelebAHQ der Größe 256 und von ImageNet von der Größe 32 bis 128. Wir erreichen State-of-the-Art-Likelihood-Ergebnisse in mehreren Einstellungen, neue Benchmark-Ergebnisse in bisher unerforschten Einstellungen und sind in der Lage, jede High-Fidelity großen Maßstab Proben auf der Grundlage der beiden Datensätze zu erzeugen.
Die Modellierung und Vorhersage des Verhaltens solcher dynamischen Systeme ist im Allgemeinen nicht einfach, da es schwierig ist, die komplizierten Interaktionen und Entwicklungen ihrer Bestandteile zu verstehen. In diesem Papier wird das relationale Zustandsraummodell (R-SSM) vorgestellt, ein sequentielles hierarchisches latentes Variablenmodell, das graphische neuronale Netze (GNNs) verwendet, um die gemeinsamen Zustandsübergänge mehrerer korrelierter Objekte zu simulieren. Indem GNNs mit SSM zusammenarbeiten, bietet R-SSM eine flexible Möglichkeit, relationale Informationen in die Modellierung der Dynamik mehrerer Objekte einzubeziehen. Wir schlagen außerdem vor, das Modell mit normalisierenden Flüssen zu erweitern, die für vertex-indizierte Zufallsvariablen instanziiert werden, und schlagen zwei zusätzliche kontrastive Ziele vor, um das Lernen zu erleichtern.
Natürliche Sprache ist hierarchisch strukturiert: kleinere Einheiten (z.B. Phrasen) sind in größeren Einheiten (z.B. Sätzen) verschachtelt, Während die Standard-LSTM-Architektur es verschiedenen Neuronen erlaubt, Informationen auf verschiedenen Zeitskalen zu verfolgen, hat sie keine explizite Ausrichtung auf die Modellierung einer Hierarchie von Konstituenten. Ein Vektor von Master-Input und Forget-Gates stellt sicher, dass, wenn ein bestimmtes Neuron aktualisiert wird, alle Neuronen, die ihm in der Reihenfolge folgen, ebenfalls aktualisiert werden. Unsere neuartige rekurrente Architektur, das Ordered Neurons LSTM (ON-LSTM), erzielt gute Leistungen bei vier verschiedenen Aufgaben: Sprachmodellierung, unüberwachtes Parsing, gezielte syntaktische Auswertung und logische Inferenz.
Skip-Verbindungen haben das Training von sehr tiefen Netzen ermöglicht und sind zu einer unverzichtbaren Komponente in einer Vielzahl von neuronalen Architekturen geworden.eine völlig zufriedenstellende Erklärung für ihren Erfolg bleibt schwer fassbar.hier präsentieren wir eine neue Erklärung für die Vorteile von Skip-Verbindungen beim Training von sehr tiefen Netzen. Die Schwierigkeit, tiefe Netze zu trainieren, ist zum Teil auf die Singularitäten zurückzuführen, die durch die Nicht-Identifizierbarkeit des Modells verursacht werden:(i) Überlappungssingularitäten, die durch die Permutationssymmetrie der Knoten in einer bestimmten Schicht verursacht werden,(ii) Eliminierungssingularitäten, die der Eliminierung, d.h. der konsistenten Deaktivierung, von Knoten entsprechen. Diese Singularitäten verursachen degenerierte Mannigfaltigkeiten in der Verlustlandschaft, die das Lernen verlangsamen. Wir argumentieren, dass Skip-Verbindungen diese Singularitäten beseitigen, indem sie die Permutationssymmetrie der Knoten brechen, die Möglichkeit der Knoteneliminierung reduzieren und die Knoten weniger linear abhängig machen. Darüber hinaus bewegen Skip-Verbindungen das Netzwerk bei typischen Initialisierungen weg von den "Geistern" dieser Singularitäten und formen die Landschaft um sie herum, um die Verlangsamung des Lernens zu mildern.diese Hypothesen werden durch Beweise von vereinfachten Modellen sowie von Experimenten mit tiefen Netzwerken, die auf realen Datensätzen trainiert wurden, unterstützt.
Beim Reinforcement Learning haben effektive und funktionale Repräsentationen das Potenzial, den Lernfortschritt enorm zu beschleunigen und anspruchsvollere Probleme zu lösen. Die meisten früheren Arbeiten zum Repräsentationslernen haben sich auf generative Ansätze konzentriert, d.h. auf das Lernen von Repräsentationen, die alle zugrundeliegenden Faktoren der Variation im Beobachtungsraum in einer eher unübersichtlichen oder geordneten Weise erfassen. In dieser Arbeit zielen wir stattdessen darauf ab, funktional auffällige Repräsentationen zu erlernen: Repräsentationen, die nicht notwendigerweise vollständig sind, was die Erfassung aller Variationsfaktoren im Beobachtungsraum angeht, sondern eher darauf abzielen, diejenigen Variationsfaktoren zu erfassen, die für die Entscheidungsfindung wichtig sind - die "handlungsfähig" sind. Wir zeigen, wie diese gelernten Repräsentationen nützlich sein können, um die Exploration für spärliche Belohnungsprobleme zu verbessern, um hierarchisches Verstärkungslernen mit langem Horizont zu ermöglichen und als Zustandsrepräsentation für das Erlernen von Strategien für nachgelagerte Aufgaben.Wir evaluieren unsere Methode auf einer Reihe von simulierten Umgebungen und vergleichen sie mit früheren Methoden für Repräsentationslernen, Exploration und hierarchisches Verstärkungslernen.
Wir untersuchen das Verhalten eines standardmäßigen neuronalen Faltungsnetzes in einer Umgebung, in der Klassifizierungsaufgaben sequentiell eingeführt werden und das Netz neue Aufgaben bewältigen muss, während die Beherrschung der zuvor gelernten Aufgaben erhalten bleibt.  Durch Simulationen mit Sequenzen von 10 zusammenhängenden Aufgaben finden wir Grund zur Zuversicht, dass Netze gut skalieren werden, wenn sie von einer einzigen Fähigkeit zu Domänenexperten werden.Wir haben zwei Schlüsselphänomene beobachtet. Erstens, Vorwärtserleichterung - das beschleunigte Lernen von Aufgabe n+1, nachdem man n vorherige Aufgaben gelernt hat - wächst mit n. Zweitens, Rückwärtsinterferenz - das Vergessen der n vorherigen Aufgaben beim Lernen von Aufgabe n+1 - verringert sich mit n. Vorwärtserleichterung ist das Ziel der Forschung zu Metalearning, und reduzierte Rückwärtsinterferenz ist das Ziel der Forschung zur Verbesserung des katastrophalen Vergessens.
Wir demonstrieren eine Methode, die mit geringem Aufwand aus bestehenden Worteinbettungen aufgabenoptimierte Einbettungen konstruiert, um die Leistung bei einer überwachten Endaufgabe zu steigern. Dadurch werden zusätzliche Beschriftungen oder der Aufbau komplexerer Modellarchitekturen vermieden, indem stattdessen spezialisierte Einbettungen bereitgestellt werden, die besser für die Endaufgabe(n) geeignet sind. Darüber hinaus kann die Methode verwendet werden, um grob abzuschätzen, ob eine bestimmte Art von Endaufgabe(n) aus einem gegebenen unmarkierten Datensatz erlernt werden kann oder darin repräsentiert ist, z.B. unter Verwendung öffentlich verfügbarer Sondierungsaufgaben.Wir evaluieren unsere Methode für verschiedene Worteinbettungs-Sondierungsaufgaben und nach Größe des Einbettungs-Trainingskorpus -- d.h. um ihre Verwendung in reduzierten (Vor-Trainings-Ressourcen) Einstellungen zu untersuchen.
Dieser Prozess wird jedoch oft auf ineffiziente Art und Weise durchgeführt, da künstliche Beispiele durch die Anwendung einer Reihe von Transformationen auf alle Punkte in der Trainingsmenge erstellt werden, was zu einer explosionsartigen Vergrößerung des Datensatzes führen kann, was sowohl die Speicher- und Trainingskosten als auch die Auswahl und Abstimmung der optimalen Transformationen betrifft. In dieser Arbeit zeigen wir, dass es möglich ist, die Anzahl der Datenpunkte, die in die Datenerweiterung einbezogen werden, erheblich zu reduzieren und gleichzeitig die gleichen Genauigkeits- und Invarianzvorteile der Erweiterung des gesamten Datensatzes zu erzielen.
In den letzten Jahren hat die aufregende Arbeit im Bereich der tiefen generativen Modelle Modelle hervorgebracht, die in der Lage sind, neue organische Moleküle vorzuschlagen, indem sie Zeichenketten, Bäume und Graphen generieren, die ihre Struktur darstellen.Während solche Modelle in der Lage sind, Moleküle mit wünschenswerten Eigenschaften zu generieren, ist ihr Nutzen in der Praxis aufgrund der Schwierigkeit, zu wissen, wie diese Moleküle zu synthetisieren sind, begrenzt. Wir schlagen daher ein neues Modell zur Molekülgenerierung vor, das einen realistischeren Prozess widerspiegelt, bei dem die Reaktanten ausgewählt und kombiniert werden, um komplexere Moleküle zu bilden. Erstens zeigen wir, dass ein solches Modell aufgrund der nützlichen induktiven Verzerrungen bei der Modellierung von Reaktionen in der Lage ist, eine große, vielfältige Menge gültiger und einzigartiger Moleküle zu erzeugen.Zweitens bietet die Modellierung von Syntheserouten anstelle von Endmolekülen praktische Vorteile für Chemiker, die nicht nur an neuen Molekülen, sondern auch an Vorschlägen für stabile und sichere Syntheserouten interessiert sind.Drittens demonstrieren wir die Fähigkeiten unseres Modells, auch einstufige Retrosyntheseprobleme zu lösen, indem wir eine Menge von Reaktanten vorhersagen, die ein Zielprodukt erzeugen können.
Tiefe neuronale Netze sind komplexe nichtlineare Modelle, die als prädiktive Analyseinstrumente eingesetzt werden und bei vielen Klassifizierungsaufgaben eine hervorragende Leistung gezeigt haben.  In der jüngeren Vergangenheit gab es mehrere Bemühungen, natürliche Fehler, d. h. falsch klassifizierte Eingaben, zu erkennen, doch diese Mechanismen sind mit einem zusätzlichen Energiebedarf verbunden.  Um dieses Problem zu lösen, stellen wir ein neuartiges post-hoc Framework vor, das natürliche Fehler auf energieeffiziente Weise erkennt.  Wir erreichen dies, indem wir auf relevanten Merkmalen basierende lineare Klassifikatoren pro Klasse anhängen, die als auf relevanten Merkmalen basierende Hilfszellen (RACs) bezeichnet werden.   Die vorgeschlagene Technik nutzt den Konsens zwischen den RACs, die an einigen ausgewählten versteckten Schichten angehängt sind, um die richtig klassifizierten Eingaben von den falsch klassifizierten Eingaben zu unterscheiden. Unsere Ergebnisse zeigen, dass für den CIFAR100-Datensatz, der auf einem VGG16-Netzwerk trainiert wurde, die RACs 46% der falsch klassifizierten Beispiele erkennen können und dabei 12% weniger Energie verbrauchen als das Basisnetzwerk, während 69% der Beispiele richtig klassifiziert werden.
Um vorherzusagen, ob eine Beziehung zwischen Entitäten besteht, werden ihre Einbettungen typischerweise im latenten Raum nach einer beziehungsspezifischen Abbildung verglichen. Während sich die Vorhersage von Verbindungen stetig verbessert hat, bleibt die latente Struktur und damit die Frage, warum solche Modelle semantische Informationen erfassen, unerklärt. Für identifizierbare Beziehungstypen sind wir in der Lage, Eigenschaften vorherzusagen und die relative Leistung führender Wissensgraphen-Darstellungsmethoden zu rechtfertigen, einschließlich ihrer oft übersehenen Fähigkeit, unabhängige Vorhersagen zu machen.
Viele reale Anwendungen beinhalten multivariate, georeferenzierte Zeitreihendaten: An jedem Standort zeichnen mehrere Sensoren entsprechende Messungen auf, z. B. ein Luftqualitätsüberwachungssystem PM2,5, CO usw. Die resultierenden Zeitreihendaten enthalten oft fehlende Werte aufgrund von Geräteausfällen oder Kommunikationsfehlern. Die daraus resultierenden Zeitreihendaten enthalten oft fehlende Werte aufgrund von Geräteausfällen oder Kommunikationsfehlern. Um die fehlenden Werte zu ersetzen, basieren die modernsten Methoden auf rekurrenten neuronalen Netzen (RNN), die jeden Zeitstempel sequentiell verarbeiten und die direkte Modellierung der Beziehung zwischen entfernten Zeitstempeln verhindern. Um die Selbstaufmerksamkeit über verschiedene Dimensionen hinweg (d.h. Zeit, Ort und Sensormessungen) gemeinsam zu erfassen und gleichzeitig die Größe der Aufmerksamkeitskarten vernünftig zu halten, schlagen wir einen neuartigen Ansatz vor, der Cross-Dimensional Self-Attention (CDSA) genannt wird, um jede Dimension sequentiell, aber auftragsunabhängig zu verarbeiten. Anhand von drei realen Datensätzen, darunter unser neu erhobener NYC-Verkehrsdatensatz, zeigen umfangreiche Experimente die Überlegenheit unseres Ansatzes im Vergleich zu State-of-the-Art-Methoden sowohl für Imputations- als auch für Vorhersageaufgaben. 
Diese Arbeit konzentriert sich auf die Verbesserung der Qualität von gescannten Dokumenten, um die OCR-Ausgabe zu verbessern.Wir erstellen eine End-to-End-Pipeline zur Verbesserung von Dokumenten, die in einer Reihe von verrauschten Dokumenten nimmt und produziert saubere ones.Deep neuronales Netzwerk basierte Entrauschung Auto-Encoder trainiert werden, um die OCR-Qualität zu verbessern.We trainieren ein blindes Modell, das auf verschiedenen Ebenen Lärm gescannter Textdokumente funktioniert.Results sind für Unschärfe und Wasserzeichen Rauschen Entfernung von verrauschten gescannten Dokumenten gezeigt.
Ironischerweise basieren viele neue Verteidigungsmaßnahmen auf einer einfachen Beobachtung: Die gegnerischen Eingaben selbst sind nicht robust, und kleine Störungen der angreifenden Eingabe führen oft zur Wiederherstellung der gewünschten Vorhersage. Während die Intuition einigermaßen klar ist, fehlt in der Forschungsliteratur ein detailliertes Verständnis dieses Phänomens. In diesem Beitrag wird eine umfassende experimentelle Analyse darüber vorgestellt, wann und warum die Störungsabwehr funktioniert und welche potenziellen Mechanismen ihre Wirksamkeit (oder Unwirksamkeit) in verschiedenen Situationen erklären könnten.
Es gibt noch keinen Konsens in der Frage, ob adaptive Gradientenmethoden wie Adam einfacher zu verwenden sind als nicht-adaptive Optimierungsmethoden wie SGD.In dieser Arbeit füllen wir das wichtige, aber mehrdeutige Konzept der â€˜Ease-of-useâ€™, indem wir die Abstimmbarkeit eines Optimierers definieren: Wie einfach ist es, gute Hyperparameterkonfigurationen mit automatischer zufälliger Hyperparametersuche zu finden?Wir schlagen ein praktisches und universelles quantitatives Maß für die Abstimmbarkeit von Optimierern vor, das die Grundlage für einen fairen Optimierer-Benchmark bilden kann.  Bei der Evaluierung einer Vielzahl von Optimierern auf einem umfangreichen Satz von Standarddatensätzen und -architekturen haben wir festgestellt, dass Adam für die meisten Probleme am besten abstimmbar ist, insbesondere bei einem geringen Budget für die Hyperparameterabstimmung.
Das Phasenproblem in der Beugungsphysik ist eines der ältesten inversen Probleme in der gesamten Wissenschaft.Die zentrale Schwierigkeit, die jeder Ansatz zur Lösung dieses inversen Problems überwinden muss, ist, dass die Hälfte der Informationen, nämlich die Phase des gebeugten Strahls, immer fehlt.Im Kontext der Elektronenmikroskopie ist das Phasenproblem in der Regel nicht-linear und Lösungen, die von Phase-Retrieval-Techniken bereitgestellt werden, sind bekanntlich schlechte Annäherungen an die Physik der Elektronen, die mit der Materie interagieren.Hier zeigen wir, dass ein halbüberwachtes Lernen Ansatz effektiv das Phasenproblem in der Elektronenmikroskopie / Streuung lösen kann. Insbesondere stellen wir ein neues Deep Neural Network (DNN), Y-net, die gleichzeitig lernt einen Rekonstruktionsalgorithmus über überwachte Ausbildung zusätzlich zu lernen, eine Physik-basierte Regularisierung über unbeaufsichtigte training.We zeigen, dass diese eingeschränkte, semi-supervised Ansatz ist eine Größenordnung mehr Daten-effiziente und genaue als das gleiche Modell in einer rein überwachten fashion.Additionally, die Architektur des Y-net Modell bietet für eine unkomplizierte Bewertung der Konsistenz des Modells Vorhersage während der Inferenz und ist allgemein anwendbar auf die Phase Problem in anderen Einstellungen.
Die meisten Einbettungsmethoden stützen sich auf ein log-bilineares Modell, um das Vorkommen eines Wortes in einem Kontext anderer Wörter vorherzusagen. Hier schlagen wir word2net vor, eine Methode, die ihre lineare Parametrisierung durch neuronale Netze ersetzt. Für jeden Begriff im Vokabular setzt word2net ein neuronales Netz ein, das den Kontext als Input nimmt und eine Wahrscheinlichkeit des Auftretens ausgibt. word2net kann die hierarchische Organisation seiner Wortnetze nutzen, um zusätzliche Metadaten, wie z.B. syntaktische Merkmale, in das Einbettungsmodell einzubeziehen. Wir untersuchen word2net mit zwei Datensätzen, einer Sammlung von Wikipedia-Artikeln und einem Korpus von Reden des US-Senats.Quantitativ haben wir herausgefunden, dass word2net populäre Einbettungsmethoden bei der Vorhersage von herausgehaltenen Wörtern übertrifft und dass die gemeinsame Nutzung von Parametern, die auf dem Wortteil basieren, die Leistung weiter steigert.Qualitativ lernt word2net interpretierbare semantische Repräsentationen und bezieht im Vergleich zu vektorbasierten Methoden syntaktische Informationen besser ein.
Ein wichtiges Ziel in den Neurowissenschaften ist das VerstÃ?ndnis der Gehirnmechanismen kognitiver Funktionen. Ein neuer Ansatz ist die Untersuchung der Dynamik von â€žGehirnzustÃ?ndenâ€œ mit Hilfe der funktionellen Magnetresonanztomographie (fMRI). Bisher wurden in der Literatur GehirnzustÃ?nde typischerweise mit fMRI-Daten von 30 Sekunden oder mehr untersucht, und es ist unklar, inwieweit GehirnzustÃ?nde aus sehr kurzen Zeitreihen zuverlÃ?ssig identifiziert werden kÃ¶nnen. Ausgehend von einem populären Hirngraphen mit Knoten, die durch eine Parzellierung der Großhirnrinde definiert sind, und der angrenzenden Matrix, die aus dem funktionellen Konnektom extrahiert wurde, nimmt GCN eine kurze Serie von fMRI-Volumina als Eingabe, generiert bereichsspezifische Graphdarstellungen auf hoher Ebene und sagt dann den entsprechenden kognitiven Zustand voraus. Wir untersuchten die Leistung dieser GCN "kognitiven Zustand Annotation" in der Human Connectome Project (HCP) Datenbank, die 21 verschiedene experimentelle Bedingungen umfasst sieben großen kognitiven Domänen und hohe zeitliche Auflösung Aufgabe fMRI data.Using ein 10-Sekunden-Fenster, die 21 kognitiven Zustände wurden mit einer ausgezeichneten durchschnittlichen Test Genauigkeit von 89% (Chance Ebene 4. Da die HCP-Aufgabenbatterie so konzipiert wurde, dass sie selektiv ein breites Spektrum spezialisierter funktioneller Netzwerke aktiviert, gehen wir davon aus, dass die GCN-Annotation als Basismodell für andere Transfer-Learning-Anwendungen geeignet ist, z. B. für die Anpassung an neue Aufgabendomänen.
Moderne tiefe neuronale Netze (DNNs) erfordern einen hohen Speicherverbrauch und große Rechenlasten.  Um DNN-Algorithmen effizient auf mobilen Geräten einsetzen zu können, wurde eine Reihe von DNN-Komprimierungsalgorithmen erforscht, darunter auch die Faktorisierungsmethoden, die die Gewichtsmatrix einer DNN-Schicht durch Multiplikation von zwei oder mehreren Matrizen mit niedrigem Rang approximieren. Bisherige Arbeiten induzieren den niedrigen Rang hauptsächlich durch implizite Approximationen oder durch einen kostspieligen Prozess der Singulärwertzerlegung (SVD) bei jedem Trainingsschritt.Der erste Ansatz führt normalerweise zu einem hohen Genauigkeitsverlust, während der zweite verhindert, dass die DNN-Faktorisierung effizient eine hohe Kompressionsrate erreicht. Um die Qualität des Trainings und die Konvergenz zu verbessern, fügen wir den singulären Vektoren eine Orthogonalitätsregulierung hinzu, die die gültige Form der SVD gewährleistet und ein Verschwinden/Explodieren des Gradienten verhindert. Wir zeigen empirisch, dass das SVD-Training den Rang der DNN-Schichten signifikant reduzieren und eine höhere Reduktion der Rechenlast bei gleicher Genauigkeit erreichen kann, nicht nur im Vergleich zu früheren Faktorisierungsmethoden, sondern auch zu den modernsten Filter-Pruning-Methoden.
Die in letzter Zeit zunehmende Popularität von "few-shot"-Lernalgorithmen hat es den Modellen ermöglicht, sich schnell an neue Aufgaben anzupassen, die auf nur wenigen Trainingsbeispielen beruhen.Bisherige Arbeiten zum "few-shot"-Lernen haben sich hauptsächlich auf Klassifizierung und Verstärkungslernen konzentriert. Unser Modell basiert auf der Idee, dass der Freiheitsgrad der unbekannten Funktion erheblich reduziert werden kann, wenn sie als Linearkombination einer Reihe von geeigneten Basisfunktionen dargestellt wird. Wir entwerfen ein Feature Extractor Netzwerk, um die Basisfunktionen für eine Aufgabenverteilung zu kodieren, und einen Weights Generator, um den Gewichtsvektor für eine neue Aufgabe zu generieren, und zeigen, dass unser Modell den aktuellen Stand der Technik bei verschiedenen Regressionsaufgaben übertrifft.
Die meisten Klassifizierungs- und Segmentierungsdatensätze gehen von einem Closed-World-Szenario aus, in dem die Vorhersagen als Verteilung über eine vorgegebene Menge visueller Klassen ausgedrückt werden, was jedoch unvermeidbare und oft unbemerkte Fehler bei Vorhandensein von Eingaben außerhalb der Verteilung (OOD) impliziert. Wir schlagen vor, dieses Problem durch eine diskriminierende Erkennung von OOD-Pixeln in den Eingabedaten anzugehen. Im Gegensatz zu den bisherigen Ansätzen vermeiden wir es, Entscheidungen zu treffen, indem wir nur den Trainingsdatensatz des primären Modells betrachten, das zur Lösung der gewünschten Computer-Vision-Aufgabe trainiert wurde. Stattdessen trainieren wir ein spezielles OOD-Modell, das den primären Trainingsdatensatz von einem viel größeren "Hintergrund"-Datensatz unterscheidet, der die Vielfalt der visuellen Welt annähert.Wir führen unsere Experimente auf hochauflösenden natürlichen Bildern in einem dichten Vorhersage-Setup durch.Wir verwenden mehrere Straßenfahr-Datensätze als unsere Trainingsverteilung, während wir die Hintergrundverteilung mit dem ILSVRC-Datensatz annähern. Die erzielten Ergebnisse zeigen, dass der vorgeschlagene Ansatz erfolgreich ist, Pixel zu identifizieren, die außerhalb der Verteilung liegen, und dabei frühere Arbeiten weit übertrifft.
Netzwerkquantisierung ist eine der Hardware-freundlichsten Techniken, um den Einsatz von neuronalen Faltungsnetzwerken (CNNs) auf mobilen Geräten mit geringem Stromverbrauch zu ermöglichen. Neuere Netzwerkquantisierungstechniken quantisieren jeden Gewichtskern in einer Faltungsschicht unabhängig voneinander, um eine höhere Inferenzgenauigkeit zu erreichen, da die Gewichtskerne in einer Schicht unterschiedliche Varianzen aufweisen und daher unterschiedlich viel Redundanz besitzen. Um die Redundanz effektiv zu reduzieren und die CNN-Inferenzen zu beschleunigen, sollten verschiedene Gewichtungskerne mit unterschiedlichen QBNs quantisiert werden. Bisherige Arbeiten verwenden jedoch nur eine QBN, um jede Faltungsschicht oder das gesamte CNN zu quantisieren, da der Designraum für die Suche nach einer QBN für jeden Gewichtungskern zu groß ist. Die handgefertigte Heuristik der kernelweisen QBN-Suche ist so anspruchsvoll, dass Domänenexperten nur suboptimale Ergebnisse erzielen können. Selbst für DDPG-basierte Agenten mit Deep Reinforcement Learning (DRL) ist es schwierig, eine kernelweise QBN-Konfiguration zu finden, mit der eine angemessene Inferenzgenauigkeit erzielt werden kann. In diesem Papier schlagen wir eine hierarchische DRL-basierte kernelweise Netzwerkquantisierungstechnik, AutoQ, vor, um automatisch ein QBN für jeden Gewichtskern zu suchen und ein anderes QBN für jede Aktivierungsschicht zu wählen. Im Vergleich zu den Modellen, die durch die modernsten DRL-basierten Schemata quantisiert werden, reduzieren dieselben Modelle, die durch AutoQ quantisiert werden, im Durchschnitt die Inferenzlatenz um 54,06% und verringern den Inferenz-Energieverbrauch um 50,69%, während sie die gleiche Inferenzgenauigkeit erreichen.
Während visuelle Analysesysteme mit mehreren Modellen effektiv sein können, wirft ihre zusätzliche Komplexität Bedenken hinsichtlich der Benutzerfreundlichkeit auf, da die Benutzer mit den Parametern mehrerer Modelle interagieren müssen. Darüber hinaus schafft das Aufkommen verschiedener Modellalgorithmen und zugehöriger Hyperparameter einen erschöpfenden Modellraum, aus dem Modelle ausgewählt werden können, was die Navigation in diesem Modellraum kompliziert macht, um das richtige Modell für die Daten und die Aufgabe zu finden. In diesem Beitrag stellen wir Gaggle vor, ein visuelles Multimodell-Analysesystem, das es Benutzern ermöglicht, interaktiv durch den Modellraum zu navigieren und die Benutzerinteraktionen in Schlussfolgerungen umzuwandeln, und das die Arbeit mit mehreren Modellen vereinfacht, indem es automatisch das beste Modell aus dem hochdimensionalen Modellraum findet, um verschiedene Benutzeraufgaben zu unterstützen.Durch eine qualitative Benutzerstudie zeigen wir, wie unser Ansatz den Benutzern hilft, das beste Modell für eine Klassifizierungs- und Ranking-Aufgabe zu finden.Die Ergebnisse der Studie bestätigen, dass Gaggle intuitiv und einfach zu bedienen ist und die interaktive Navigation im Modellraum und die automatische Modellauswahl unterstützt, ohne dass die Benutzer technische Fachkenntnisse benötigen.
Das Problem der chinesischen Textdarstellung behindert jedoch immer noch die Verbesserung der chinesischen Textklassifikation, insbesondere der polyphonen und homophonen Texte in sozialen Medien. Um dieses Problem effektiv zu lösen, schlagen wir eine neue Struktur, den Extraktor, vor, die auf Aufmerksamkeitsmechanismen basiert, und entwerfen neuartige Aufmerksamkeitsnetzwerke, die wir Extraktor-Attentions-Netzwerk (EAN) nennen.Im Gegensatz zu den meisten früheren Arbeiten verwendet EAN eine Kombination aus einem Wort-Encoder und einem Pinyin-Zeichen-Encoder anstelle eines einzelnen Encoders. Darüber hinaus hat EAN im Vergleich zu den hybriden Encoder-Methoden eine komplexere Kombinationsarchitektur und mehr reduzierende Parameterstrukturen, so dass EAN eine große Menge an Informationen nutzen kann, die von Multi-Inputs stammen, und Effizienzprobleme lindert.Das vorgeschlagene Modell erreicht den Stand der Technik Ergebnisse auf 5 großen Datensätzen für chinesische Textklassifikation.
Jüngste Fortschritte beim Lernen aus Demonstrationen (LfD) mit tiefen neuronalen Netzen haben das Erlernen komplexer Roboterfähigkeiten ermöglicht, die hochdimensionale Wahrnehmungen wie Rohbildeingaben beinhalten. In der Praxis ist es jedoch effizienter, wenn ein Lehrer eine Vielzahl von Aufgaben vorführt, ohne dass eine sorgfältige Aufgabenerstellung, -beschriftung und -technik erforderlich ist. Leider können in solchen Fällen herkömmliche Nachahmungslernverfahren die multimodale Natur der Daten nicht abbilden und führen oft zu suboptimalem Verhalten.In diesem Beitrag stellen wir einen LfD-Ansatz zum Erlernen mehrerer Verhaltensweisen aus visuellen Daten vor. Unser Ansatz basiert auf einem stochastischen tiefen neuronalen Netz (SNN), das die zugrundeliegende Absicht in der Demonstration als stochastische Aktivierung im Netz darstellt.Wir stellen einen effizienten Algorithmus für das Training von SNNs vor, und für das Lernen mit Vision-Inputs schlagen wir auch eine Architektur vor, die die Absicht mit einem stochastischen Aufmerksamkeitsmodul verbindet.Wir demonstrieren unsere Methode an realen visuellen Roboter-Objekt-Erreichungsaufgaben und zeigen, dass sie zuverlässig die verschiedenen Verhaltensmodi in den Demonstrationsdaten lernen kann.Videoergebnisse sind unter https://vimeo.com/240212286/fd401241b9 verfügbar.
Die Interpretierbarkeit von neuronalen Netzen hat sich für ihre Anwendungen in der realen Welt im Hinblick auf die Zuverlässigkeit und Vertrauenswürdigkeit entscheidend.Bestehende Erklärung Generation Methoden bieten in der Regel wichtige Funktionen durch die Bewertung ihrer einzelnen Beiträge zu den Modellvorhersage und ignorieren die Wechselwirkungen zwischen den Funktionen, die schließlich eine Tasche von Wörtern Darstellung als Erklärung.In der natürlichen Sprachverarbeitung, diese Art von Erklärungen ist eine Herausforderung für den menschlichen Benutzer, um die Bedeutung einer Erklärung zu verstehen und ziehen Sie die Verbindung zwischen Erklärung und Modellvorhersage, vor allem für lange Texte. Die vorgeschlagene Methode wird mit drei neuronalen Klassifikatoren, LSTM, CNN und BERT, auf zwei Benchmark-Textklassifizierungsdatensätzen evaluiert. Die generierten Erklärungen werden sowohl durch automatische Evaluierungsmessungen als auch durch menschliche Evaluatoren bewertet. Die Experimente zeigen die Effektivität der vorgeschlagenen Methode bei der Bereitstellung von Erklärungen, die sowohl modelltreu als auch für Menschen verständlich sind.
In diesem Papier reduzieren wir diese Kosten, indem wir die Tatsache ausnutzen, dass die Wichtigkeit von Merkmalen, die von Faltungsschichten berechnet werden, in hohem Maße inputabhängig ist, und schlagen Feature Boosting and Suppression (FBS) vor, eine neue Methode zur prädiktiven Verstärkung hervorstechender Fusionskanäle und zum Überspringen unwichtiger Kanäle zur Laufzeit.FBS führt kleine Hilfsverbindungen zu bestehenden Faltungsschichten ein. Im Gegensatz zu Kanalbeschneidungsmethoden, die permanent Kanäle entfernen, bewahrt es die vollständigen Netzwerkstrukturen und beschleunigt die Faltung durch dynamisches Überspringen unwichtiger Eingangs- und Ausgangskanäle.FBS-verstärkte Netzwerke werden mit konventionellem stochastischem Gradientenabstieg trainiert und sind somit für viele moderne CNNs verfügbar. Wir vergleichen FBS mit einer Reihe bestehender Kanalbeschneidungs- und dynamischer AusfÃ?hrungsschemata und demonstrieren groÃŸe Verbesserungen bei der ImageNet-Klassifikation. Experimente zeigen, dass FBS bei VGG-16 und ResNet-18 5Ã- bzw. 2Ã- Einsparungen bei der Rechenleistung ermÃ¶glicht, beide mit weniger als 0,6 % Top-5-Genauigkeitsverlust.
Wir schlagen eine neuartige Methode vor, um die Anzahl der Parameter in den speicherhungrigen vollverknüpften Schichten eines neuronalen Netzes zu reduzieren, indem wir eine vordefinierte Sparsamkeit verwenden, bei der die Mehrheit der Verbindungen vor dem Beginn des Trainings nicht vorhanden ist.Unsere Ergebnisse zeigen, dass faltige neuronale Netze ohne Genauigkeitsverlust bei einer Verbindungsdichte von weniger als 0,5 % für die Klassifizierungsschicht oder weniger als 5 % für das gesamte Netz arbeiten können. Basierend auf unserer Sparsifizierungsmethode führen wir die "Scatter"-Metrik ein, um die Qualität eines bestimmten Verbindungsmusters zu charakterisieren. Als Beweis des Konzepts zeigen wir Ergebnisse für CIFAR, MNIST und einen neuen Datensatz zur Klassifizierung von Morsezeichen, der einige interessante Trends und Grenzen von spärlichen Verbindungsmustern aufzeigt.
Obwohl in den letzten Jahren viele Anstrengungen unternommen wurden, ist es von großer Bedeutung, korrekte und vollständige Evaluierungen der Angriffs- und Verteidigungsalgorithmen durchzuführen. In diesem Papier erstellen wir einen umfassenden, strengen und kohärenten Benchmark, um die Robustheit von Angriffsalgorithmen bei Bildklassifizierungsaufgaben zu bewerten. Nach einem kurzen Überblick über viele repräsentative Angriffs- und Verteidigungsmethoden führen wir groß angelegte Experimente mit zwei Robustheitskurven als fairen Bewertungskriterien durch, um die Leistung dieser Methoden vollständig zu verstehen.
Wir schlagen eine Modifikation traditioneller Künstlicher Neuronaler Netze (ANNs) vor, die ANNs mit neuen, von biologischen Neuronen motivierten Fähigkeiten ausstattet.  Biologische Neuronen arbeiten weit über die lineare Aufsummierung synaptischer Eingaben und die anschließende Umwandlung der integrierten Informationen hinaus.  Ein biologisches Neuron ändert seinen Feuermodus in Abhängigkeit von peripheren Faktoren (z.B. Neuromodulatoren) sowie von intrinsischen Faktoren.  Unsere Modifikation verbindet eine neue Art von ANN-Knoten, die die Funktion biologischer Neuromodulatoren nachahmen und als Modulatoren bezeichnet werden, um andere traditionelle ANN-Knoten in die Lage zu versetzen, ihre Aktivierungssensitivität während der Laufzeit auf der Grundlage ihrer Eingabemuster anzupassen.  Auf diese Weise ermöglichen wir, dass die Steigung der Aktivierungsfunktion kontextabhängig ist.  Diese Modifikation führt zu statistisch signifikanten Verbesserungen im Vergleich zu traditionellen ANN-Knoten im Kontext von Convolutional Neural Networks und Long Short-Term Memory-Netzwerken.
In dieser Arbeit untersuchen wir, wie das groß angelegte Pre-Train-Fine-Tuning das Verhalten eines neuronalen Sprachgenerators verändert. Wir konzentrieren uns auf das Transformator-Encoder-Decoder-Modell für die Aufgabe der Erzeugung von Dialogantworten in einer offenen Domäne und stellen fest, dass das Modell nach dem Standard-Fine-Tuning wichtige Fähigkeiten zur Sprachgenerierung vergisst, die während des groß angelegten Pre-Trainings erworben wurden. Wir demonstrieren das Phänomen des Vergessens durch eine detaillierte Verhaltensanalyse aus der Perspektive der Kontextsensitivität und des Wissenstransfers und schlagen eine intuitive Feinabstimmungsstrategie mit dem Namen "Mix-Review" vor, die den Feinabstimmungsprozess effektiv reguliert und das Problem des Vergessens weitgehend lindert.
Die Kombination von Domänenwissensmodellen mit neuronalen Modellen ist eine Herausforderung.  End-to-End-trainierte neuronale Modelle schneiden oft besser ab (geringerer Mean Square Error) als Modelle mit Domänenwissen oder Kombinationen aus Domäne und Neuronen, und die Kombination ist ineffizient zu trainieren.  In diesem Beitrag zeigen wir, dass wir durch die Kombination von Domänenmodellen mit Modellen des maschinellen Lernens, durch die Verwendung extrapolativer Testsätze und durch den Aufruf von Zielfunktionen für die Dekorrelation Modelle erstellen können, die komplexere Systeme vorhersagen können: Die Modelle sind interpretierbar, extrapolativ, dateneffizient und erfassen vorhersehbares, aber komplexes nicht-stochastisches Verhalten wie nicht modellierte Freiheitsgrade und systemisches Messrauschen.  Wir wenden dieses verbesserte Modellierungsparadigma auf mehrere simulierte Systeme und ein tatsächliches physikalisches System im Zusammenhang mit der Systemidentifikation an.   Es werden verschiedene Möglichkeiten der Zusammenstellung von Domänenmodellen mit neuronalen Modellen für Zeitreihen, Boosting, Bagging und Auto-Encoding für verschiedene Systeme mit unterschiedlicher Komplexität und Nichtlinearität untersucht.  Obwohl diese Arbeit noch vorläufig ist, zeigen wir, dass die Fähigkeit, Modelle zu kombinieren, eine vielversprechende Richtung für die neuronale Modellierung ist.
In dieser Arbeit schlagen wir Scoring-Aggregating-Planning (SAP) vor, ein Framework, das aufgabenagnostische Semantik- und Dynamikprioritäten aus beliebigen Qualitätsinteraktionen sowie die entsprechenden spärlichen Belohnungen erlernen kann und dann für ungesehene Aufgaben unter Null-Schuss-Bedingungen plant. Der Rahmen findet eine neuronale Bewertungsfunktion für lokale regionale Zustands- und Aktionspaare, die aggregiert werden können, um die Qualität einer vollständigen Trajektorie zu approximieren; darüber hinaus kann ein Dynamikmodell, das mit Selbstüberwachung gelernt wird, für die Planung einbezogen werden. Viele frühere Arbeiten, die interaktive Daten für das Policy-Lernen nutzen, benötigen entweder massive On-Policy-Umgebungsinteraktionen oder setzen den Zugang zu Expertendaten voraus, während wir ein ähnliches Ziel mit reinen Off-Policy-unvollkommenen Daten erreichen können.Die Instanziierung unseres Frameworks führt zu einer verallgemeinerbaren Policy für ungesehene Aufgaben.Experimente zeigen, dass die vorgeschlagene Methode Basismethoden in einer Vielzahl von Anwendungen, einschließlich Gridworld, Robotikaufgaben und Videospielen, übertreffen kann.
Der partikelbasierte Inferenzalgorithmus ist eine vielversprechende Methode zur effizienten Generierung von Stichproben für eine schwer fassbare Zielverteilung durch die iterative Aktualisierung einer Menge von Partikeln. Ein bemerkenswertes Beispiel ist der Steinsche Variationsgradientenabstieg (SVGD), der eine deterministische und recheneffiziente Aktualisierung ermöglicht, aber dafür bekannt ist, dass er die Varianz in hohen Dimensionen unterschätzt, wobei der Mechanismus schlecht verstanden wird. Durch den Vergleich der beiden Aktualisierungsregeln identifizieren wir die Quelle der Verzerrung in SVGD als eine Kombination aus hoher Varianz und deterministischer Verzerrung und zeigen empirisch, dass die Beseitigung beider Faktoren zu einer genauen Schätzung der Varianz führt.Darüber hinaus leiten wir für das Lernen eines hochdimensionalen Gaußschen Ziels die konvergierte Varianz für beide Algorithmen analytisch ab und bestätigen, dass nur SVGD unter dem "Fluch der Dimensionalität" leidet.
Wir beschreiben einen Ansatz zum Verständnis der besonderen und kontraintuitiven Generalisierungseigenschaften von tiefen neuronalen Netzen.  Der Ansatz geht über die theoretische Kapazitätskontrolle im schlimmsten Fall hinaus, die in den letzten Jahren im Bereich des maschinellen Lernens beliebt war, und greift alte Ideen aus der statistischen Mechanik neuronaler Netze wieder auf.  Im Rahmen dieses Ansatzes stellen wir ein prototypisches Very Simple Deep Learning (VSDL)-Modell vor, dessen Verhalten von zwei Kontrollparametern gesteuert wird, von denen einer die effektive Datenmenge oder Belastung des Netzes beschreibt (die abnimmt, wenn dem Input Rauschen hinzugefügt wird), und der andere eine effektive Temperaturinterpretation enthält (die zunimmt, wenn Algorithmen frühzeitig gestoppt werden).  Anhand dieses Modells beschreiben wir, wie eine sehr einfache Anwendung von Ideen aus der Theorie der statistischen Mechanik der Verallgemeinerung eine starke qualitative Beschreibung der kürzlich beobachteten empirischen Ergebnisse in Bezug auf die Unfähigkeit tiefer neuronaler Netze, Trainingsdaten nicht zu überfüllen, diskontinuierliches Lernen und scharfe Übergänge in den Verallgemeinerungseigenschaften von Lernalgorithmen usw. liefert.
Berechnungen für die Softmax-Funktion in neuronalen Netzmodellen sind teuer, wenn die Anzahl der Ausgangsklassen groß ist, was sowohl beim Training als auch bei der Inferenz für solche Modelle zu einem erheblichen Problem werden kann.In diesem Papier stellen wir Doubly Sparse Softmax (DS-Softmax), Sparse Mixture of Sparse of Sparse Experts, vor, um die Effizienz der Softmax-Inferenz zu verbessern.Während des Trainings lernt unsere Methode eine zweistufige Klassenhierarchie, indem sie den gesamten Ausgangsklassenraum in mehrere sich teilweise überlappende Experten unterteilt. Jeder Experte ist für eine gelernte Teilmenge des Ausgabeklassenraums verantwortlich und jede Ausgabeklasse gehört nur zu einer kleinen Anzahl dieser Experten.Während der Inferenz findet unsere Methode schnell den wahrscheinlichsten Experten, um Small-Scale-Softmax zu berechnen.Unsere Methode ist lernbasiert und erfordert keine Kenntnis des Ausgabeklassenpartitionsraums a priori.Wir evaluieren unsere Methode empirisch an mehreren realen Aufgaben und zeigen, dass wir ohne Leistungseinbußen signifikante Berechnungsreduzierungen erreichen können.
Ãœberwachte maschinelle Lernmodelle fÃ?r hochwertige Bildverarbeitungsanwendungen wie die Klassifizierung medizinischer Bilder benÃ¶tigen oft groÃŸe DatensÃ?tze, die von DomÃ?nenexperten beschriftet werden, die langsam zu sammeln, teuer zu pflegen und statisch in Bezug auf Ãnderungen in der Datenverteilung sind. In diesem Zusammenhang bewerten wir den Nutzen der BeobachtungsÃ?berwachung, bei der wir passiv gesammelte Signale wie Eye-Tracking- oder â€žBlickâ€œ-Daten nutzen, um die Menge der fÃ?r das Modelltraining benÃ¶tigten handbeschrifteten Daten zu reduzieren. Insbesondere nutzen wir Blickinformationen, um eine visuelle Aufmerksamkeitsschicht direkt zu Ã?berwachen, indem wir die Unstimmigkeit zwischen den rÃ?umlichen Regionen, auf die der menschliche Beschrifter am lÃ?ngsten geschaut hat, und denjenigen, die die Modellausgabe am stÃ?rksten beeinflussen, bestrafen. Wir zeigen, dass die EinschrÃ?nkung des Modells auf diese Weise die Anzahl der beschrifteten Beispiele, die fÃ?r ein bestimmtes Leistungsniveau erforderlich sind, um bis zu 50 % reduzieren kann, und dass Blickinformationen bei schwierigeren Aufgaben am hilfreichsten sind.
Durch die Kombination von nichtlinearen neuronalen Message-Passing-Modellen (z.B. Graph Isomorphism Networks, GraphSAGE, etc.) mit Verlustkorrekturmethoden präsentieren wir einen geräuschtoleranten Ansatz für die Graphenklassifikation.
Durch zahlreiche Fortschritte im Bereich des Lernens von Graphenrepräsentationen hat sich die Leistung bei Aufgaben mit graphenstrukturierten Daten in den letzten Jahren erheblich verbessert - vor allem bei Aufgaben mit Vorhersagen auf Knotenebene. Der Aufbau von Vorhersageaufgaben über ganze Graphen (wie z.B. die Eigenschaftsvorhersage für ein Molekül oder die Nebenwirkungsvorhersage für ein Medikament) erweist sich jedoch als schwieriger, da der Algorithmus Beweise über mehrere strukturell relevante Bereiche des Graphen in einer einzigen Vorhersage kombinieren muss.Die meisten früheren Arbeiten versuchen, diese Eigenschaften auf Graphenebene vorherzusagen, während sie jeweils nur einen Graphen betrachten - was dem Lernenden nicht erlaubt, strukturelle Ähnlichkeiten und Motive über Graphen hinweg direkt zu nutzen. Hier schlagen wir ein Setup vor, bei dem ein neuronales Graphennetzwerk Paare von Graphen auf einmal empfängt, und erweitern es mit einer Co-Attentional-Schicht, die es Knotenrepräsentationen ermöglicht, strukturelle Informationen über sie hinweg auszutauschen.Wir zeigen zunächst, dass ein solches Setup natürliche Vorteile bei einer paarweisen Graphenklassifizierungsaufgabe (Vorhersage von Medikamenten-Interaktionen) bietet, und erweitern es dann auf ein allgemeineres Graphregressions-Setup: Verbesserung von Vorhersagen über QM9, einem Standard-Benchmark für molekulare Vorhersagen.Unser Setup ist flexibel, leistungsfähig und macht keine Annahmen über die zugrundeliegenden Datensatzeigenschaften, abgesehen von der Vorhersage der Existenz mehrerer Trainingsgraphen.
In diesem Papier untersuchen wir Bildbeschriftungen als bedingtes GAN-Training und schlagen sowohl einen kontextbewussten LSTM-Beschrifter als auch einen ko-attentiven Diskriminator vor, der die semantische Übereinstimmung zwischen Bildern und Beschriftungen erzwingt.Wir untersuchen die Durchführbarkeit von zwei diskreten GAN-Trainingsmethoden: Self-critical Sequence Training (SCST) und Gumbel Straight-Through (ST) und zeigen, dass SCST ein stabileres Gradientenverhalten und bessere Ergebnisse als Gumbel ST zeigt.
Wir stellen Newtonian Monte Carlo (NMC) vor, eine Methode zur Verbesserung der Konvergenz von Markov Chain Monte Carlo (MCMC) durch Analyse der Gradienten erster und zweiter Ordnung der Zieldichte, um eine geeignete Vorschlagsdichte an jedem Punkt zu bestimmen.Bestehende gradientenbasierte Methoden erster Ordnung leiden unter dem Problem der Bestimmung einer geeigneten Schrittgröße. NMC ähnelt dem Newton-Raphson-Update in der Optimierung, bei dem der Gradient zweiter Ordnung verwendet wird, um die Schrittgröße in jeder Dimension automatisch zu skalieren. Unser Ziel ist es jedoch nicht, ein Maximum zu finden, sondern eine parametrisierte Dichte, die der lokalen Krümmung der Zieldichte am besten entspricht.  Als weitere Verbesserung gegenüber den Methoden erster Ordnung zeigen wir, dass Zufallsvariablen mit eingeschränkten Unterstützungen nicht transformiert werden müssen, bevor ein Gradientenschritt unternommen wird.NMC passt eingeschränkte Zufallsvariablen direkt an eine Vorschlagsdichte mit der gleichen Unterstützung an, so dass die Krümmung der Zieldichte intakt bleibt. Für statistische Modelle, bei denen der Prior konjugiert zur Wahrscheinlichkeit ist, kann unsere Methode das Posterior in einem Schritt ganz trivial wiederherstellen, aber wir zeigen auch Ergebnisse für ziemlich große, nicht-konjugierte Modelle, bei denen NMC besser abschneidet als adaptive Methoden erster Ordnung wie NUTS oder andere ungenaue skalierbare Inferenzmethoden wie Stochastic Variational Inference oder Bootstrapping.
Neural Tangents ist eine Bibliothek, die für die Erforschung neuronaler Netze mit unendlicher Breite entwickelt wurde und eine High-Level-API für die Spezifikation komplexer und hierarchischer neuronaler Netzarchitekturen bereitstellt. Diese Netze können dann entweder wie üblich mit endlicher Breite oder in ihrer unendlichen Breite trainiert und ausgewertet werden. Netze mit unendlicher Breite können analytisch mit exakter Bayes'scher Inferenz oder mit Gradientenabstieg über den Neural Tangent Kernel trainiert werden. Alle Berechnungen können automatisch auf mehrere Beschleuniger verteilt werden, wobei die Anzahl der Geräte nahezu linear skaliert werden kann. Neural Tangents ist unterhttps://www.github.com/google/neural-tangentsWe verfügbar und bietet auch ein begleitendes interaktives Colab-Notebook unterhttps://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb.
Ein großes Problem auf dem Weg zur künstlichen Intelligenz ist jedoch, dass neuronale Netze nicht in der Lage sind, Proben aus neuartigen Klassenverteilungen genau zu erkennen, weshalb die meisten existierenden Klassifizierungsalgorithmen davon ausgehen, dass alle Klassen vor dem Training bekannt sind. In dieser Arbeit schlagen wir eine Methode für das Training eines neuronalen Netzes vor, die es ihm ermöglicht, Beispiele außerhalb der Verteilung zu erkennen, ohne die Klassifizierungsgenauigkeit bei Testbeispielen aus bekannten Klassen zu beeinträchtigen. Basierend auf der Outlier Exposure (OE)-Technik schlagen wir eine neuartige Verlustfunktion vor, die sowohl bei Bild- als auch bei Textklassifizierungsaufgaben die besten Ergebnisse in der Out-of-Distribution-Detektion mit OE erzielt und sich aufgrund ihrer Konstruktion für das Training aller Klassifizierungsalgorithmen eignet, die auf Maximum-Likelihood-Methoden basieren.
Die genaue Form dieser Repräsentation wird oft als metrische Repräsentation des Raums angesehen, wobei eine interne Repräsentation jedoch nach ihrem Beitrag zur Leistung bei einer bestimmten Aufgabe beurteilt wird und daher zwischen verschiedenen Arten von Navigationsaufgaben variieren kann. Um uns auf die internen Repräsentationen zu konzentrieren, teilen wir das Lernen in eine aufgabenunabhängige Pre-Trainingsphase, die die interne Konnektivität modifiziert, und eine aufgabenspezifische Q-Learning-Phase, die den Output des Netzwerks kontrolliert. Wir zeigen, dass das Vortraining die Attraktorlandschaft der Netze formt, was entweder zu einem kontinuierlichen Attraktor, diskreten Attraktoren oder einem ungeordneten Zustand führt.Diese Strukturen induzieren eine Verzerrung auf die Q-Learning-Phase, was zu einem Leistungsmuster über die Aufgaben hinweg führt, das metrischen und topologischen Regelmäßigkeiten entspricht.Unsere Ergebnisse zeigen, dass in rekurrenten Netzen eine induktive Verzerrung die Form von Attraktorlandschaften annimmt - die durch Vortraining geformt und mit Methoden der dynamischen Systeme analysiert werden können.Außerdem zeigen wir, dass nicht-metrische Repräsentationen für Navigationsaufgaben nützlich sind.  
Die formale Verifikation von Modellen des maschinellen Lernens hat in letzter Zeit viel Aufmerksamkeit erregt, und es wurden bedeutende Fortschritte beim Nachweis einfacher Eigenschaften wie der Robustheit gegenüber kleinen Störungen der Eingangsmerkmale gemacht.In diesem Zusammenhang wurde auch beobachtet, dass die Einbindung der Verifikationsprozedur in das Training es einfacher macht, nachweislich robuste Modelle zu trainieren. In diesem Papier erweitern wir die Anwendbarkeit von verifiziertem Training, indem wir es auf (1) rekurrente neuronale Netzwerkarchitekturen und (2) komplexe Spezifikationen ausdehnen, die über einfache gegnerische Robustheit hinausgehen, insbesondere Spezifikationen, die zeitliche Eigenschaften erfassen, wie die Anforderung, dass ein Roboter regelmäßig eine Ladestation besucht oder dass ein Sprachmodell immer Sätze von begrenzter Länge produziert.Experimente zeigen, dass, während Modelle, die mit Standardtraining trainiert werden, oft gewünschte Spezifikationen verletzen, unsere verifizierte Trainingsmethode Modelle hervorbringt, die sowohl gut funktionieren (in Bezug auf Testfehler oder Belohnung) als auch nachweislich mit den Spezifikationen konsistent sind.
Neuronale Netze (NN) haben bei vielen Aufgaben in den Bereichen Bild, Sprache und Text Spitzenleistungen erbracht, die vor allem auf spezielle Strukturen zurückzuführen sind, die an die jeweiligen Datenmuster angepasst sind, wie z. B. CNN, die räumliche Lokalitäten erfassen, und RNN, die sequentielle Abhängigkeiten modellieren, die im Wesentlichen durch die Nutzung des Vorwissens über die entsprechenden Domänendaten eine gute Leistung erzielen. Da es keine gemeinsamen Muster zwischen diesen verschiedenen tabellarischen Daten gibt, ist es schwierig, spezifische Strukturen zu entwerfen, die für alle geeignet sind.Ohne sorgfältiges Architekturdesign, das auf Domänenwissen basiert, ist es für NN eine ziemliche Herausforderung, eine zufriedenstellende Leistung in diesen tabellarischen Datendomänen zu erreichen.Um die Lücke von NN beim Lernen von tabellarischen Daten zu füllen, schlagen wir eine universelle neuronale Netzwerklösung vor, die wir TabNN nennen, um effektive NN-Architekturen für tabellarische Daten in allen Arten von Aufgaben automatisch abzuleiten.Im Einzelnen folgt das Design von TabNN zwei Prinzipien: \Da GBDT seine Stärke bei der Modellierung tabellarischer Daten empirisch bewiesen hat, verwenden wir GBDT für die Implementierung von TabNN. Umfassende experimentelle Analysen an einer Vielzahl von tabellarischen Datensätzen zeigen, dass TabNN eine viel bessere Leistung als viele Basislösungen erzielen kann.
Wissensdatenbanken (KBs) werden immer größer, spärlicher und probabilistischer.Diese KBs werden typischerweise verwendet, um Abfrage-Inferenzen und Regel-Mining durchzuführen.Aber ihre Wirksamkeit ist nur so hoch wie ihre Vollständigkeit.Effiziente Nutzung unvollständiger KBs bleibt eine große Herausforderung, da die aktuellen KB-Vervollständigung Techniken entweder nicht berücksichtigen die inhärente Unsicherheit mit jedem KB-Tupel verbunden oder nicht auf große KBs skalieren.Probabilistische Regel-Lernen berücksichtigt nicht nur die Wahrscheinlichkeit eines jeden KB-Tupel, sondern auch das Problem der KB-Vervollständigung in einer erklärbaren Weise angegangen. Für jede gegebene probabilistische KB lernt es probabilistische Regeln erster Ordnung aus ihren Relationen, um interessante Muster zu identifizieren. Aber die aktuellen probabilistischen Regel-Lerntechniken führen Grounding durch, um probabilistische Inferenz für die Evaluierung von Kandidatenregeln durchzuführen. Sie skalieren nicht gut auf große KBs, da die Zeitkomplexität der Inferenz mit Grounding exponentiell zur Größe der KB ist.In diesem Papier stellen wir SafeLearner vor - eine skalierbare Lösung für probabilistische KB-Vervollständigung, die probabilistisches Regel-Lernen mit aufgehobener probabilistischer Inferenz durchführt - als schnelleren Ansatz anstelle von Grounding. Wir haben SafeLearner mit dem State-of-the-Art probabilistischen Regel-Lerner ProbFOIL+ und seinem deterministischen Zeitgenossen AMIE+ auf probabilistischen Standard-KBs von NELL (Never-Ending Language Learner) und Yago verglichen und konnten zeigen, dass SafeLearner beim Lernen einfacher Regeln genauso gut skaliert wie AMIE+ und auch deutlich schneller ist als ProbFOIL+.
Jüngste Bemühungen im Bereich Dialogue State Tracking (DST) für aufgabenorientierte Dialoge haben sich in Richtung offenes Vokabular oder generationenbasierte Ansätze entwickelt, bei denen die Modelle Slot-Wert-Kandidaten aus der Dialoghistorie selbst generieren können.Diese Ansätze haben gute Leistungsgewinne gezeigt, insbesondere in komplizierten Dialogdomänen mit dynamischen Slot-Werten: (1) Sie erlauben es den Modellen nicht, explizit Signale über Domänen und Slots hinweg zu lernen, um potenzielle Abhängigkeiten zwischen Textit{(Domäne, Slot)}-Paaren zu erkennen; und (2) bestehende Modelle folgen autoregressiven Ansätzen, die hohe Zeitkosten verursachen, wenn sich der Dialog über mehrere Domänen und mehrere Turns entwickelt. In diesem Papier schlagen wir ein neuartiges Rahmenwerk der nicht-autoregressiven Dialogzustandsverfolgung (NADST) vor, das potenzielle Abhängigkeiten zwischen Domänen und Slots berücksichtigen kann, um die Modelle im Hinblick auf eine bessere Vorhersage der Dialogzustände als vollständigen Satz und nicht als separate Slots zu optimieren. Insbesondere ermöglicht die nicht-autoregressive Natur unserer Methode nicht nur die parallele Dekodierung, um die Latenz von DST für die Echtzeit-Dialog-Antwort-Generierung signifikant zu reduzieren, sondern auch die Erkennung von Abhängigkeiten zwischen Slots auf Token-Ebene zusätzlich zur Slot- und Domänen-Ebene.Unsere empirischen Ergebnisse zeigen, dass unser Modell die modernste gemeinsame Genauigkeit über alle Domänen auf dem MultiWOZ 2.1-Korpus erreicht und die Latenz unseres Modells ist eine Größenordnung niedriger als der bisherige Stand der Technik, wenn sich die Dialoggeschichte über die Zeit erstreckt.
Die 3D-Zoom-Operation ist die positive Verschiebung der Kamera in der Z-Achse, senkrecht zur Bildebene. Im Gegensatz dazu verändert der optische Zoom die Brennweite und der digitale Zoom wird verwendet, um einen bestimmten Bereich eines Bildes auf die ursprüngliche Bildgröße zu vergrößern.In diesem Papier sind wir die ersten, die ein unbeaufsichtigtes 3D-Zoom-Lernproblem formulieren, bei dem Bilder mit einem beliebigen Zoomfaktor aus einem gegebenen Einzelbild erzeugt werden können. Ein unbeaufsichtigter Rahmen ist praktisch, da es eine schwierige Aufgabe ist, einen 3D-Zoom-Datensatz von natürlichen Szenen zu erhalten, da eine spezielle Ausrüstung erforderlich ist, um sicherzustellen, dass die Kamerabewegung auf die Z-Achse beschränkt ist, und die Objekte in den Szenen sich nicht bewegen, wenn sie aufgenommen werden, was den Aufbau eines großen Datensatzes von Außenszenen behindert. Wir stellen einen neuartigen unbeaufsichtigten Rahmen vor, um zu lernen, wie man beliebige 3D-Zoom-Versionen eines Einzelbildes generiert, ohne dass eine 3D-Zoom-Grundwahrheit erforderlich ist. Das Deep 3D-Zoom Net beinhaltet die folgenden Merkmale:(i) Transfer-Lernen von einem vortrainierten Disparitätsschätzungsnetzwerk über einen Rückprojektion-Rekonstruktionsverlust;(ii) eine vollständig konvolutionale Netzwerkarchitektur, die tiefenbildbasiertes Rendering (DIBR) modelliert und hochfrequente Details berücksichtigt, ohne dass die Zwischendisparität geschätzt werden muss; und(iii) die Einbeziehung eines Diskriminatornetzwerks, das als No-Reference-Strafe für unnatürlich gerenderte Bereiche fungiert. Obwohl es keine Grundlage für einen fairen Vergleich unserer Ergebnisse gibt, übertrifft unsere Methode frühere Forschungen zur Synthese neuartiger Ansichten in Bezug auf ein realistisches Erscheinungsbild auf großen Kamera-Basislinien. Wir haben umfangreiche Experimente durchgeführt, um die Wirksamkeit unserer Methode auf den KITTI- und Cityscapes-Datensätzen zu überprüfen.
Das universelle Approximationstheorem, in einer seiner allgemeinsten Versionen, besagt, dass, wenn wir nur kontinuierliche Aktivierungsfunktionen Ïƒ betrachten, dann ist ein standardmäßiges neuronales Feedforward-Netzwerk mit einer versteckten Schicht in der Lage, jede kontinuierliche multivariate Funktion f bis zu einem beliebigen Approximationsschwellenwert Îµ zu approximieren, wenn und nur wenn Ïƒ nicht polynomial ist.In diesem Papier geben wir einen direkten algebraischen Beweis des Theorems. Wenn X in R^n kompakt ist, dann kann ein neuronales Netzwerk mit n Eingabeeinheiten, m Ausgabeeinheiten und einer einzigen versteckten Schicht mit {n+d wählen d} versteckten Einheiten (unabhängig von m und Îµ) jede Polynomfunktion f:X -> R^m, deren Gesamtgrad höchstens d für jede ihrer m Koordinatenfunktionen ist, gleichmäßig approximieren. Für den allgemeinen Fall, dass f eine beliebige kontinuierliche Funktion ist, zeigen wir, dass es N in O(Îµ^{-n}) gibt (unabhängig von m), so dass N versteckte Einheiten ausreichen, um f zu approximieren.Wir zeigen auch, dass diese einheitliche Approximationseigenschaft (UAP) auch unter scheinbar strengen Bedingungen für die Gewichte gilt.Wir heben mehrere Konsequenzen hervor:(i) Für jedes Î' > 0 gilt die UAP immer noch, wenn wir alle Nicht-Bias-Gewichte w in der letzten Schicht auf |w| < Î' beschränken. (ii) Es existiert ein Î">0 (nur abhängig von f und Ïƒ), so dass die UAP immer noch gilt, wenn wir alle Nicht-Bias-Gewichte w in der ersten Schicht so einschränken, dass |w|>Î" erfüllt ist.(iii) Wenn die Nicht-Bias-Gewichte in der ersten Schicht *fest* sind und zufällig aus einem geeigneten Bereich ausgewählt werden, dann gilt die UAP mit Wahrscheinlichkeit 1.
In diesem Papier entwerfen wir einen generischen Rahmen für das Lernen eines robusten Textklassifikationsmodells, das eine mit Standard-Vollmodellen vergleichbare Genauigkeit unter Testzeit-Budgetbeschränkungen erreicht. Wir verfolgen einen anderen Ansatz als bestehende Methoden und lernen, einen großen Anteil unwichtiger Wörter durch einen Selektor mit geringer Komplexität dynamisch zu löschen, so dass der hochkomplexe Klassifikator nur einen kleinen Anteil wichtiger Wörter verarbeiten muss. Darüber hinaus schlagen wir eine neue Datenaggregationsmethode vor, um den Klassifikator zu trainieren, die es ihm ermöglicht, selbst bei fragmentierten Wortfolgen genaue Vorhersagen zu treffen. unsere End-to-End-Methode erreicht eine hochmoderne Leistung, während ihre Rechenkomplexität linear mit dem kleinen Anteil wichtiger Wörter im gesamten Korpus skaliert. außerdem kann ein einzelner Klassifikator für tiefe neuronale Netze, der durch unseren Rahmen trainiert wurde, dynamisch auf verschiedene Budgetebenen zur Inferenzzeit abgestimmt werden.
Differentiable Architektur Suche (DARTS) bot eine schnelle Lösung bei der Suche nach effektiven Netzwerk-Architekturen, aber litt unter großen Speicher und Computing Overheads bei der gemeinsamen Ausbildung ein super-Netz und die Suche nach einer optimalen architecture.In diesem Papier, präsentieren wir einen neuartigen Ansatz, nämlich Partially-Connected DARTS, durch die Probenahme einen kleinen Teil der super-Netz, um die Redundanz bei der Erkundung des Netzwerks Raum zu reduzieren, wodurch eine effizientere Suche ohne die Leistung. Diese Strategie kann unter einer unerwünschten Inkonsistenz bei der Auswahl der Kanten des Supernetzes leiden, die durch das Sampling verschiedener Kanäle verursacht wird. Dank der verringerten Speicherkosten kann PC-DARTS mit einer größeren Stapelgröße trainiert werden, was sowohl eine höhere Geschwindigkeit als auch eine höhere Trainingsstabilität zur Folge hat. Insbesondere erreichen wir eine Fehlerrate von 2,57% auf CIFAR10 innerhalb von nur 0,1 GPU-Tagen für die Architektursuche und eine Top-1-Fehlerrate von 24,2% auf ImageNet (unter der mobilen Einstellung) innerhalb von 3,8 GPU-Tagen für die Suche. Unser Code wurde auf https://www.dropbox.com/sh/on9lg3rpx1r6dkf/AABG5mt0sMHjnEJyoRnLEYW4a?dl=0 zur Verfügung gestellt.
Die Dialogforschung neigt dazu, zwischen Plaudereien und zielgerichteten Aufgaben zu unterscheiden, wobei erstere wohl naturalistischer sind und eine breitere Verwendung von Sprache aufweisen, während letztere klarere Metriken und ein einfacheres Lernsignal haben.Menschen kombinieren mühelos beide und beteiligen sich beispielsweise an Plaudereien mit dem Ziel, Informationen auszutauschen oder eine bestimmte Antwort hervorzurufen.Hier überbrücken wir die Kluft zwischen diesen beiden Bereichen in einer reichhaltigen textbasierten Mehrspieler-Fantasieumgebung, in der Agenten und Menschen sowohl Aktionen als auch Dialoge durchführen. Konkret trainieren wir ein zielorientiertes Modell mit Verstärkungslernen über Selbstspiel gegen ein durch Nachahmung erlerntes Chit-Chat-Modell mit zwei neuen Ansätzen: Die Politik lernt entweder, ein Thema auszuwählen, oder sie lernt, eine Äußerung aus den Top-k-Äußerungen auszuwählen.Wir zeigen, dass beide Modelle eine starke inverse Modellbasis übertreffen und sich auf natürliche Weise mit ihrem Dialogpartner unterhalten können, um Ziele zu erreichen.
Neuere Arbeiten haben gezeigt, dass die Korrekturen der stationären Zustands- oder Zustands-Aktions-Verteilung im Kontext des unendlichen Horizonts für die Bewertung von Off-Policy-Politiken eine wichtige Rolle spielen. Wir schlagen eine geschätzte Mischungspolitik (EMP) vor, eine neue Klasse von teilweise policy-agnostischen Methoden, um diese Größen genau zu schätzen. Mit einer sorgfältigen Analyse zeigen wir, dass EMP zu Schätzungen mit reduzierter Varianz für die Schätzung der stationären Verteilungskorrektur des Zustands führt, während es auch eine nützliche Induktionsverzerrung für die Schätzung der stationären Verteilungskorrektur des Zustands und der Aktion bietet.
Wir führen eine effizientere neuronale Architektur für die amortisierte Inferenz ein, die kontinuierliche und bedingte Normalisierungsflüsse unter Verwendung einer prinzipiellen Strukturwahl kombiniert. Unser Gradientenfluss leitet sein Sparsamkeitsmuster von der minimal getreuen Inversen seines zugrundeliegenden graphischen Modells ab. Wir stellen fest, dass diese Faktorisierung die notwendige Anzahl von Parametern im neuronalen Netzwerk und von adaptiven Integrationsschritten im ODE-Löser reduziert. Indem wir die strukturelle Inversion und die Flusskonstruktion als Kompilationsdurchläufe einer probabilistischen Programmiersprache ausdrücken, demonstrieren wir ihre Anwendbarkeit auf die stochastische Inversion realistischer Modelle wie z.B. Faltungsneuronaler Netze (CNN).
Wir stellen einen Suchalgorithmus für neuronale Architekturen vor, um kompakte Reinforcement Learning (RL)-Politiken zu konstruieren, indem wir ENAS und ES in einer hoch skalierbaren und intuitiven Weise kombinieren: Indem wir den kombinatorischen Suchraum von NAS als die Menge verschiedener Kantenpartitionierungen (Färbungen) in gleichgewichtige Klassen definieren, repräsentieren wir kompakte Architekturen durch effiziente gelernte Kantenpartitionierungen. Für verschiedene RL-Aufgaben gelingt es uns, Färbungen zu erlernen, die zu effektiven Richtlinien führen, die durch nur 17 Gewichtsparameter parametrisiert sind. Dies führt zu einer Kompression von >90 % gegenüber Vanilla-Richtlinien und einer 6-fachen Kompression gegenüber kompakten Richtlinien auf der Basis von Toeplitz-Matrizen, die dem Stand der Technik entsprechen, während gleichzeitig eine gute Belohnung beibehalten wird.Wir glauben, dass unsere Arbeit einer der ersten Versuche ist, einen rigorosen Ansatz für das Training von strukturierten neuronalen Netzarchitekturen für RL-Probleme vorzuschlagen, die insbesondere in der mobilen Robotik mit begrenzten Speicher- und Rechenressourcen von Interesse sind.
Tiefe Ansätze zur Erkennung von Anomalien haben in letzter Zeit vielversprechende Ergebnisse gegenüber flachen Methoden auf großen und komplexen Datensätzen gezeigt.Typischerweise wird die Erkennung von Anomalien als ein unüberwachtes Lernproblem behandelt.In der Praxis kann man jedoch--zusätzlich zu einem großen Satz von unbeschrifteten Proben---Zugang zu einem kleinen Pool von beschrifteten Proben haben, z.B. Semi-überwachte Ansätze zur Erkennung von Anomalien zielen darauf ab, solche gekennzeichneten Proben zu nutzen, aber die meisten vorgeschlagenen Methoden beschränken sich darauf, gekennzeichnete normale Proben einzubeziehen, und nur wenige Methoden nutzen gekennzeichnete Anomalien, wobei die bestehenden tiefen Ansätze domänenspezifisch sind. In dieser Arbeit stellen wir Deep SAD vor, eine durchgängige tiefe Methodik für die allgemeine semi-supervised Anomalie-Erkennung. Unter Verwendung einer informationstheoretischen Perspektive auf die Anomalie-Erkennung leiten wir einen Verlust ab, der durch die Idee motiviert ist, dass die Entropie der latenten Verteilung für normale Daten niedriger sein sollte als die Entropie der anomalen Verteilung. In umfangreichen Experimenten mit MNIST, Fashion-MNIST und CIFAR-10 sowie anderen Benchmark-Datensätzen zur Erkennung von Anomalien zeigen wir, dass unsere Methode gleichwertig oder besser ist als die der flachen, hybriden und tiefen Konkurrenten, und dass sie selbst bei nur wenigen gelabelten Daten erhebliche Leistungsverbesserungen erbringt.
Um tiefe ReLU-Netzwerke zu analysieren, nehmen wir eine Schüler-Lehrer-Einstellung an, in der ein überparametrisiertes Schülernetzwerk von der Ausgabe eines festen Lehrernetzwerks der gleichen Tiefe lernt, mit stochastischem Gradientenabstieg (SGD). Zweitens zeigt die Analyse der verrauschten Wiederherstellung und der Trainingsdynamik in einem zweischichtigen Netzwerk, dass starke Lehrerknoten (mit großen Fan-Out-Gewichten) zuerst gelernt werden und subtile Lehrerknoten bis zum späten Stadium des Trainings nicht gelernt werden, was dazu führt, dass es lange dauern kann, bis diese kritischen Punkte mit kleinem Gradienten erreicht werden: (1) sie ist eine notwendige Bedingung für die Ausrichtung an den kritischen Punkten, und (2) in der Trainingsdynamik hilft sie den Schülerknoten, mehr Lehrerknoten mit weniger Iterationen abzudecken.beides verbessert die Generalisierung.Experimente rechtfertigen unsere Ergebnisse.
Wir untersuchen die Konvergenz des Gradientenabstiegs (GD) und des stochastischen Gradientenabstiegs (SGD) für das Training von $L$-versteckten linearen Restnetzen (ResNets).Wir beweisen, dass für das Training von tiefen Restnetzen mit bestimmten linearen Transformationen an Eingangs- und Ausgangsschichten, die während des gesamten Trainings fixiert sind, sowohl GD als auch SGD mit Null-Initialisierung auf allen versteckten Gewichten zum globalen Minimum des Trainingsverlustes konvergieren können.Darüber hinaus optimieren GD und SGD nachweislich ausreichend tiefe lineare ResNets, wenn sie auf geeignete Gaußsche zufällige lineare Transformationen spezialisiert sind. Verglichen mit dem globalen Konvergenzergebnis von GD für das Training von standardmäßigen tiefen linearen Netzen \citep{du2019width} ist unsere Bedingung für die Breite des neuronalen Netzes um einen Faktor $O(\kappa L)$ schärfer, wobei $\kappa$ die Zustandszahl der Kovarianzmatrix der Trainingsdaten bezeichnet.Darüber hinaus stellen wir zum ersten Mal die globale Konvergenz von SGD für das Training von tiefen linearen ResNets fest und beweisen eine lineare Konvergenzrate, wenn das globale Minimum $0$ ist.
Wir beobachten, dass tiefe neuronale Netze (Deep Neural Networks, DNNs) trainieren, indem sie in den ersten Epochen lernen, flach zu lernende Beispiele korrekt zu klassifizieren, bevor sie die schwierigeren Beispiele lernen. Wir bauen auf dieser Beobachtung auf, um einen Weg zur Partitionierung des Datensatzes in harte und leichte Teilmengen vorzuschlagen, die zur Verbesserung des gesamten Trainingsprozesses verwendet werden können.Übrigens fanden wir auch Hinweise auf eine Teilmenge von faszinierenden Beispielen in allen von uns betrachteten Datensätzen, die flach lernbar, aber nicht tief lernbar waren.Um die Reproduzierbarkeit zu erleichtern, veröffentlichen wir auch unseren Code für diese Arbeit unter https://github.com/karttikeya/Shallow_to_Deep/.
Während viele neuere Arbeiten auf das Lernen von tiefen diskreten latenten Variablenmodellen mit Variationsinferenz abzielen, bleibt diese Einstellung herausfordernd, und es ist oft notwendig, potenziell hochvariante Gradientenschätzer bei der Optimierung der ELBO zu verwenden. Als Alternative schlagen wir vor, ein Nicht-ELBO-Ziel zu optimieren, das von der Bethe-Näherung der freien Energie an die Partitionsfunktion einer MRF abgeleitet ist. Wir evaluieren den vorgeschlagenen Ansatz für das Lernen von neuronalen HMMs hoher Ordnung auf Text und stellen fest, dass er oft andere approximative Inferenzverfahren in Bezug auf die wahre logische Wahrscheinlichkeit übertrifft, während wir gleichzeitig feststellen, dass alle auf approximativer Inferenz basierenden Ansätze für das Lernen von neuronalen HMMs hoher Ordnung, die wir in Betracht ziehen, das Lernen mit exakter Inferenz deutlich unterbieten.
In einem Problem der Erklärungsgenerierung muss ein Agent die Gründe für seine Entscheidungen identifizieren und einem anderen Agenten erklären.Bestehende Arbeiten in diesem Bereich sind meist auf planungsbasierte Systeme beschränkt, die automatisierte Planungsansätze zur Lösung des Problems verwenden.In diesem Papier nähern wir uns diesem Problem aus einer neuen Perspektive, in der wir einen allgemeinen logikbasierten Rahmen für die Erklärungsgenerierung vorschlagen. Insbesondere versuchen wir bei einer Wissensbasis $KB_1$, die eine Formel $\phi$ beinhaltet, und einer zweiten Wissensbasis $KB_2$, die nicht $\phi$ beinhaltet, eine Erklärung $\epsilon$ zu finden, die eine Teilmenge von $KB_1$ ist, so dass die Vereinigung von $KB_2$ und $\epsilon$ $\phi$ beinhaltet. Wir definieren zwei Arten von Erklärungen, modelltheoretische und beweistheoretische Erklärungen, und verwenden Kostenfunktionen, um die Präferenzen zwischen den Erklärungen widerzuspiegeln. Außerdem stellen wir unseren Algorithmus vor, der für die Aussagenlogik implementiert ist und solche Erklärungen berechnet, und evaluieren ihn empirisch in zufälligen Wissensbasen und einer Planungsdomäne.
Neuere theoretische Arbeiten haben gezeigt, dass tiefe neuronale Netze eine bessere Leistung als flache Netze haben, aber ihr Training ist schwieriger, z.B., Dieses Problem kann in der Regel durch die ReLU-Aktivierung (rectified linear unit) gelöst werden, aber wir zeigen hier, dass selbst bei einer solchen Aktivierung tiefe und enge neuronale Netze (NNs) mit hoher Wahrscheinlichkeit zu fehlerhaften mittleren oder mittleren Zuständen der Zielfunktion konvergieren, die vom Verlust abhängen. Wir demonstrieren diesen Zusammenbruch solcher NNs sowohl numerisch als auch theoretisch und liefern Schätzungen der Wahrscheinlichkeit des Zusammenbruchs.Wir konstruieren auch ein Diagramm einer sicheren Region für den Entwurf von NNs, die den Zusammenbruch zu fehlerhaften Zuständen vermeiden.Schließlich untersuchen wir verschiedene Möglichkeiten der Initialisierung und Normalisierung, die das Problem des Zusammenbruchs vermeiden können.Asymmetrische Initialisierungen können die Wahrscheinlichkeit des Zusammenbruchs reduzieren, aber nicht vollständig eliminieren.
Unsere Studie zeigt, dass die Maximierung der Margen durch die Minimierung des Verlustes an der Entscheidungsgrenze bei der "kürzesten erfolgreichen Störung" erreicht werden kann, was eine enge Verbindung zwischen den Verlusten und den Margen aufzeigt.Wir schlagen ein Max-Margin-Adversarial-Training (MMA) vor, um die Margen direkt zu maximieren und damit die Robustheit des Klassifizierers zu erreichen. Anstelle eines adversen Trainings mit einem festen $\epsilon$ bietet MMA eine Verbesserung, indem es eine adaptive Auswahl des "richtigen" $\epsilon$ als Marge individuell für jeden Datenpunkt ermöglicht. Unsere Experimente bestätigen empirisch unsere Theorie und demonstrieren die Wirksamkeit des MMA-Trainings auf den MNIST- und CIFAR10-Datensätzen in Bezug auf die Robustheit von $\ell_\infty$ und $\ell_2$.
Es gibt viele Methoden zur Erkennung von Anomalien, die bei niedrigdimensionalen Problemen gut funktionieren, aber es gibt einen bemerkenswerten Mangel an effektiven Methoden für hochdimensionale Räume, wie z.B. Bilder.Inspiriert von den jüngsten Erfolgen im Deep Learning schlagen wir einen neuartigen Ansatz zur Erkennung von Anomalien unter Verwendung von generativen adversen Netzwerken vor.Bei einer betrachteten Probe basiert unsere Methode auf der Suche nach einer guten Repräsentation dieser Probe im latenten Raum des Generators; wenn eine solche Repräsentation nicht gefunden wird, wird die Probe als anomal angesehen.  Wir erreichen eine Spitzenleistung bei Standard-Benchmark-Datensätzen für Bilder, und eine visuelle Inspektion der anomalsten Proben zeigt, dass unsere Methode tatsächlich Anomalien liefert.
Variational Inference (VI) und Markov Chain Monte Carlo (MCMC) sind Algorithmen zur approximativen Posterior-Inferenz, denen oft komplementäre Stärken nachgesagt werden, wobei VI schnell, aber voreingenommen und MCMC langsamer, aber asymptotisch unvoreingenommen ist. In diesem Beitrag analysieren wir gradientenbasierte MCMC- und VI-Verfahren und finden theoretische und empirische Belege dafür, dass diese Verfahren nicht so unterschiedlich sind, wie man meinen könnte. Insbesondere zeigt eine genaue Untersuchung der Fokker-Planck-Gleichung, die das Langevin-Dynamik (LD) MCMC-Verfahren steuert, dass LD implizit einem Gradientenfluss folgt, der einem Variationsschlussverfahren entspricht, das auf der Optimierung eines nichtparametrischen Normalisierungsflusses basiert. Dieses Ergebnis deutet darauf hin, dass die transiente Verzerrung von LD (aufgrund von zu wenigen Aufwärmschritten) der von VI (aufgrund von zu wenigen Optimierungsschritten) folgen kann, bis hin zu Unterschieden aufgrund der Parametrisierung und der asymptotischen Verzerrung von VI. Empirisch stellen wir fest, dass sich die transienten Verzerrungen dieser Algorithmen (und momentum-beschleunigter Versionen) ähnlich entwickeln. Dies deutet darauf hin, dass Praktiker mit einem begrenzten Zeitbudget mit einem MCMC-Verfahren (auch wenn es noch lange nicht eingebrannt ist) genauere Ergebnisse erzielen können als mit einem VI-Verfahren, solange die Varianz des MCMC-Schätzers beherrscht werden kann (z. B, durch das Ausführen vieler paralleler Ketten).
Graphenfaltungsnetze (Graph Convolutional Networks, GCNs) haben sich in letzter Zeit als recht erfolgreich bei der Modellierung von graphenstrukturierten Daten erwiesen, wobei der Schwerpunkt jedoch auf der Verarbeitung einfacher ungerichteter Graphen lag.Multirelationale Graphen sind eine allgemeinere und weit verbreitete Form von Graphen, bei denen jeder Kante eine Bezeichnung und eine Richtung zugeordnet ist.Die meisten der bestehenden Ansätze zur Verarbeitung solcher Graphen leiden unter einer Überparametrisierung und sind auf das Erlernen von Repräsentationen nur von Knoten beschränkt. In diesem Papier schlagen wir CompGCN vor, ein neuartiges Graph Convolutional Framework, das sowohl Knoten als auch Relationen in einen relationalen Graphen einbettet.CompGCN nutzt eine Vielzahl von Entity-Relations-Composition-Operationen aus Knowledge Graph Embedding Techniken und skaliert mit der Anzahl der Relationen.Es verallgemeinert auch mehrere der bestehenden Multi-Relations-GCN-Methoden.Wir evaluieren unsere vorgeschlagene Methode auf mehrere Aufgaben wie Knoten-Klassifikation, Link-Vorhersage und Graphen-Klassifikation, und erreichen nachweislich überlegene Ergebnisse.Wir stellen den Quellcode von CompGCN zur Verfügung, um reproduzierbare Forschung zu fördern.
In dieser Arbeit schlagen wir eine Quantisierungsstrategie vor, die auf die Transformer-Architektur zugeschnitten ist. Wir evaluieren unsere Methode an den WMT14 EN-FR- und WMT14 EN-DE-Übersetzungsaufgaben und erzielen die besten Quantisierungsergebnisse für den Transformer, ohne dass die BLEU-Werte im Vergleich zur nicht-quantisierten Basislinie sinken.Wir komprimieren den Transformer weiter, indem wir zeigen, dass nach dem Training des Modells ein großer Teil der Knoten im Kodierer ohne BLEU-Verlust entfernt werden kann.
Wir zeigen, dass es möglich ist, diese Einschränkungen zu umgehen, indem wir eine datenabhängige latente generative Repräsentation von Modellparametern lernen und gradientenbasiertes Meta-Lernen in diesem niedrigdimensionalen latenten Raum durchführen. Der daraus resultierende Ansatz, Latent Embedding Optimization (LEO), entkoppelt das gradientenbasierte Adaptionsverfahren vom zugrundeliegenden hochdimensionalen Raum der Modellparameter. unsere Evaluierung zeigt, dass LEO bei den konkurrierenden miniImageNet und tieredImageNet few-shot Klassifikationsaufgaben State-of-the-Art-Leistungen erzielen kann. weitere Analysen zeigen, dass LEO in der Lage ist, die Unsicherheit in den Daten zu erfassen und die Adaption durch Optimierung im latenten Raum effektiver durchführen kann.
Wir stellen einen Ansatz zur Erweiterung modellfreier Deep Reinforcement Learning-Agenten mit einem Mechanismus für relationale Schlussfolgerungen über strukturierte Darstellungen vor, der die Leistung, Lerneffizienz, Verallgemeinerung und Interpretierbarkeit verbessert.Unsere Architektur kodiert ein Bild als eine Menge von Vektoren und wendet ein iteratives Message-Passing-Verfahren an, um relevante Entitäten und Beziehungen in einer Szene zu entdecken und zu schlussfolgern.In sechs von sieben StarCraft II Learning Environment-Minispielen erreichte unser Agent eine State-of-the-Art-Leistung und übertraf in vier Fällen das Niveau eines menschlichen Großmeisters. In einer neuartigen Navigations- und Planungsaufgabe übertraf die Leistung und Lerneffizienz unseres Agenten bei weitem die nicht-relationalen Basiswerte, und er war in der Lage, sich auf komplexere Szenen zu verallgemeinern, als er während des Trainings erfahren hatte. Der Hauptbeitrag dieser Arbeit ist die Einführung von Techniken zur Repräsentation und Schlussfolgerung über Zustände in modellfreien Deep Reinforcement Learning-Agenten mittels relationaler induktiver Verzerrungen. Unsere Experimente zeigen, dass dieser Ansatz Vorteile in Bezug auf Effizienz, Generalisierung und Interpretierbarkeit bietet und so skaliert werden kann, dass er einigen der anspruchsvollsten Testumgebungen der modernen künstlichen Intelligenz gerecht wird.
Die Bildübersetzung zwischen zwei Domänen ist eine Klasse von Problemen, die darauf abzielt, die Abbildung eines Eingangsbildes in der Quelldomäne auf ein Ausgangsbild in der Zieldomäne zu erlernen, und wurde für zahlreiche Anwendungen wie Datenerweiterung, Domänenanpassung und unüberwachtes Training eingesetzt. Wir schränken das Problem mit der Annahme ein, dass das übersetzte Bild dem Originalbild wahrnehmungsmäßig ähnlich sein muss und außerdem aus der neuen Domäne zu stammen scheint, und schlagen ein einfaches, aber effektives Bildübersetzungsmodell vor, das aus einem einzigen Generator besteht, der mit einem Selbstregulierungsterm und einem gegnerischen Term trainiert wird. Daher schlagen wir vor, ein Aufmerksamkeitsmodul hinzuzufügen, das eine Aufmerksamkeitskarte vorhersagt, um den Bildübersetzungsprozess zu leiten. Das Modul lernt, auf wichtige Teile des Bildes zu achten, während alles andere unverändert bleibt, wodurch unerwünschte Artefakte oder Änderungen im Wesentlichen vermieden werden. Ausführliche Experimente und Bewertungen zeigen, dass unser Modell trotz seiner Einfachheit eine deutlich bessere Leistung erzielt als bestehende Bildübersetzungsmethoden.
Der Aufbau von tiefen neuronalen Netzen zur Steuerung von autonomen Agenten, die in Echtzeit mit der physischen Welt interagieren müssen, wie z. B. Roboter oder Kraftfahrzeuge, erfordert eine nahtlose Integration der Zeit in die Architektur eines Netzes. Die meisten künstlichen tiefen neuronalen Netze sind in einen gerichteten Graphen aus verbundenen Modulen oder Schichten unterteilt, und die Schichten selbst bestehen aus elementaren Bausteinen, wie z. B. einzelnen Einheiten. Bei den meisten tiefen neuronalen Netzen werden alle Einheiten einer Schicht synchron und parallel verarbeitet, aber die Schichten selbst werden auf sequenzielle Weise verarbeitet. Im Vergleich zu den standardmäßigen schichtweise-sequentiellen tiefen Netzen zeigen diese neuen schichtweise-parallelen Netze ein grundlegend anderes zeitliches Verhalten und einen anderen Informationsfluss, insbesondere bei Netzen mit Skip- oder rekurrenten Verbindungen. Wir argumentieren, dass schichtweise-parallele tiefe Netze besser für zukünftige Herausforderungen des Designs tiefer neuronaler Netze geeignet sind, wie z.B. große funktionale modularisierte und/oder rekurrente Architekturen sowie Netze, die unterschiedliche Netzkapazitäten in Abhängigkeit von der aktuellen Stimulus- und/oder Aufgabenkomplexität zuweisen.Wir legen grundlegende Eigenschaften dar und diskutieren die wichtigsten Herausforderungen für schichtweise-parallele Netze.Darüber hinaus stellen wir eine Toolbox zur Verfügung, um schichtweise-parallele Netze zu entwerfen, zu trainieren, zu bewerten und online mit ihnen zu interagieren.
In diesem Beitrag wird die Robustheit neuronaler Netze gegenüber Störungen durch Angreifer mit der Lyapunov-Stabilität dynamischer Systeme verknüpft. Aus dieser Sicht ist das Training neuronaler Netze gleichbedeutend mit der Suche nach einer optimalen Steuerung des diskreten dynamischen Systems, was es ermöglicht, Methoden der sukzessiven Approximation, einen Algorithmus zur optimalen Steuerung auf der Grundlage des Pontryaginschen Maximalprinzips, für das Training neuronaler Netze zu verwenden. Diese entkoppelte Trainingsmethode erlaubt es uns, die Optimierung mit Einschränkungen zu versehen, was das tiefe Modell robuster macht. das eingeschränkte Optimierungsproblem kann als semidefinites Programmierproblem formuliert und somit effizient gelöst werden. Experimente zeigen, dass unsere Methode die Robustheit des tiefen Modells gegenüber Gegnern effektiv verbessert.
In diesem Papier schlagen wir eine Methode namens Dimensional reweighting Graph Convolutional Networks (DrGCNs) vor, um das Problem der Varianz zwischen den dimensionalen Informationen in den Knotendarstellungen von GCNs anzugehen.Wir beweisen, dass DrGCNs die Varianz der Knotendarstellungen reduzieren können, indem wir unser Problem mit der Theorie des mittleren Feldes verbinden. Wir überdenken das Problem und entwickeln ein neues Maß K, um den Effekt zu quantifizieren. Dieses Maß gibt uns Hinweise darauf, wann wir dimensionales Neugewichten in GCNs verwenden sollten und wie sehr es helfen kann. Der dimensional reweighting Block ist leichtgewichtig und hochflexibel und kann auf die meisten GCN-Varianten aufgebaut werden.Sorgfältig konzipierte Experimente, einschließlich mehrerer Korrekturen von Duplikaten, Informationslecks und falschen Bezeichnungen der bekannten Knotenklassifizierungs-Benchmark-Datensätze, zeigen die überlegene Leistung von DrGCNs gegenüber den bestehenden State-of-the-Art-Ansätzen.Signifikante Verbesserungen können auch auf einem groß angelegten industriellen Datensatz beobachtet werden.
Da wir uns auf eine bessere Modellierung der Wissensselektion in einem wissensbasierten Multi-Turn-Dialog konzentrieren, schlagen wir ein sequentielles latentes Variablenmodell als ersten Ansatz zu diesem Thema vor. Das Modell mit dem Namen sequentieller Wissenstransformator (SKT) kann die Prior- und Posterior-Verteilung über das Wissen verfolgen; als Ergebnis kann es nicht nur die Mehrdeutigkeit reduzieren, die durch die Vielfalt in der Wissensauswahl der Konversation verursacht wird, sondern auch die Antwortinformationen für die richtige Wahl des Wissens besser nutzen.Unsere experimentellen Ergebnisse zeigen, dass das vorgeschlagene Modell die Genauigkeit der Wissensauswahl und anschließend die Leistung der Äußerungserzeugung verbessert.Wir erreichen die neue State-of-the-Art-Leistung auf Wizard of Wikipedia (Dinan et al., 2019) als einer der umfangreichsten und anspruchsvollsten Benchmarks.Wir validieren die Wirksamkeit unseres Modells gegenüber bestehenden Konversationsmethoden in einem anderen wissensbasierten Dialogdatensatz Holl-E (Moghe et al., 2018).
Meta-Lernen, oder Lernen-zum-Lernen, hat sich als erfolgreiche Strategie bei der Bewältigung von Problemen im überwachten Lernen und Verstärkungslernen, die kleine Datenmengen beinhalten, erwiesen.State-of-the-Art-Lösungen beinhalten das Lernen einer Initialisierung und / oder Lernalgorithmus mit einer Reihe von Trainingsepisoden, so dass der Meta-Lerner schnell zu einer Evaluierungsepisode verallgemeinern kann.Diese Methoden führen gut, aber oft fehlt eine gute Quantifizierung der Unsicherheit, die für reale Anwendungen entscheidend sein kann, wenn Daten fehlen. Wir schlagen eine Meta-Learning-Methode vor, die hierarchische Variationsinferenz über Aufgaben hinweg effizient amortisiert, indem sie eine Prior-Verteilung über die Gewichte des neuronalen Netzes lernt, so dass ein paar Schritte von Bayes durch Backprop ein gutes aufgabenspezifisches approximatives Posterior erzeugen.Wir zeigen, dass unsere Methode gute Unsicherheitsschätzungen bei kontextuellen Bandit- und Wenig-Lern-Benchmarks erzeugt.
  Oft möchten wir repräsentatives Wissen von einem neuronalen Netzwerk auf ein anderes übertragen, z.B. ein großes Netzwerk in ein kleineres destillieren, Wissen von einer sensorischen Modalität auf eine zweite übertragen oder eine Sammlung von Modellen zu einem einzigen Schätzer zusammenfassen.Wissensdestillation, der Standardansatz für diese Probleme, minimiert die KL-Divergenz zwischen den probabilistischen Ausgaben eines Lehrer- und eines Schülernetzwerks. Wir formulieren dieses Ziel als kontrastives Lernen.Experimente zeigen, dass unser neues Ziel die Wissensdestillation bei einer Vielzahl von Wissenstransferaufgaben übertrifft, einschließlich der Komprimierung von Einzelmodellen, der Ensemble-Destillation und des cross-modalen Transfers.Wenn unsere Methode mit der Wissensdestillation kombiniert wird, setzt sie den Stand der Technik bei vielen Transferaufgaben und übertrifft manchmal sogar das Lehrernetzwerk.
Die Entwicklung effektiver, biologisch plausibler Lernregeln für tiefe neuronale Netze ist wichtig, um die Verbindungen zwischen Deep Learning und Neurowissenschaften voranzutreiben.Bislang konnten lokale synaptische Lernregeln, wie sie vom Gehirn verwendet werden, nicht mit der Leistung von Backpropagation in tiefen Netzen mithalten.In dieser Arbeit setzen wir Meta-Lernen ein, um Netze zu entdecken, die mit Feedback-Verbindungen und lokalen, biologisch motivierten Lernregeln lernen.Wichtig ist, dass die Feedback-Verbindungen nicht an die Feedforward-Gewichte gebunden sind, um einen biologisch unplausiblen Gewichtstransport zu vermeiden. Unsere Experimente zeigen, dass die metatrainierten Netzwerke Rückkopplungsverbindungen effektiv nutzen, um eine Online-Kreditvergabe in mehrschichtigen Architekturen durchzuführen, und wir zeigen empirisch, dass dieses Modell einen hochmodernen gradientenbasierten Meta-Lernalgorithmus für kontinuierliches Lernen bei Regressions- und Klassifizierungsbenchmarks übertrifft.Dieser Ansatz stellt einen Schritt in Richtung biologisch plausibler Lernmechanismen dar, die nicht nur mit dem gradientenbasierten Lernen mithalten können, sondern auch dessen Einschränkungen überwinden.
Im visuellen System reagieren Neuronen auf einen Bereich des Inputs, der als ihr klassisches rezeptives Feld (RF) bekannt ist, und können durch Reize in der Umgebung moduliert werden. Diese Interaktionen werden oft durch laterale Verbindungen vermittelt, wodurch extraklassische RFs entstehen. Wir verwenden überwachtes Lernen mittels Backpropagation, um Feedforward-Verbindungen zu erlernen, kombiniert mit einer unbeaufsichtigten Lernregel, um laterale Verbindungen zwischen den Einheiten innerhalb eines faltbaren neuronalen Netzwerks zu erlernen, die es jeder Einheit ermöglichen, Informationen aus ihrer Umgebung zu integrieren und extraklassische rezeptive Felder für die Einheiten in unserem neu vorgeschlagenen Modell (CNNEx) zu erzeugen. Wir zeigen, dass diese Verbindungen das Netzwerk robuster machen und eine bessere Leistung auf verrauschten Versionen der MNIST- und CIFAR-10-Datensätze erzielen. Obwohl sich die Bildstatistiken von MNIST und CIFAR-10 stark unterscheiden, lässt sich dieselbe unüberwachte Lernregel auf beide Datensätze verallgemeinern. Unser Rahmen kann potenziell auf Netzwerke angewendet werden, die für andere Aufgaben trainiert wurden, wobei die erlernten lateralen Verbindungen die Berechnungen unterstützen, die von Feedforward-Verbindungen durchgeführt werden, wenn der Input unzuverlässig ist.
Während natürliche Sprachen reich an grammatikalischen Strukturen sind, ist DL nicht in der Lage, solche Strukturen explizit zu repräsentieren und zu erzwingen. Dieses Papier schlägt eine neue Architektur vor, um diese Lücke zu schließen, indem es Tensor-Produktrepräsentationen (TPR) nutzt, ein strukturiertes neuronal-symbolisches Framework, das in den letzten 20 Jahren in der Kognitionswissenschaft entwickelt wurde, mit dem Ziel, DL mit expliziten Sprachstrukturen und Regeln zu integrieren. Wir nennen es das Tensor Product Generation Network (TPGN) und wenden es auf Bildunterschriften an. Die Schlüsselideen von TPGN sind:1) unbeaufsichtigtes Lernen von Vektoren zur Rollenauflösung von Wörtern über ein TPR-basiertes tiefes neuronales Netzwerk und2) Integration von TPR mit typischen DL-Architekturen, einschließlich Long Short-Term Memory (LSTM)-Modellen. Die Neuartigkeit unseres Ansatzes liegt in seiner Fähigkeit, einen Satz zu generieren und eine teilweise grammatikalische Struktur des Satzes zu extrahieren, indem wir Vektoren zur Rollenentflechtung verwenden, die auf eine unbeaufsichtigte Weise gewonnen werden.
Es ist bekannt, dass Klassifikatoren anfällig für negative Störungen sind.Um sich gegen negative Störungen zu schützen, wurden verschiedene zertifizierte Robustheitsresultate abgeleitet.Allerdings sind bestehende zertifizierte Robustheiten auf Top-1-Vorhersagen beschränkt.In vielen realen Anwendungen sind Top-$k$-Vorhersagen relevanter.In dieser Arbeit zielen wir darauf ab, zertifizierte Robustheit für Top-$k$-Vorhersagen abzuleiten.Insbesondere basiert unsere zertifizierte Robustheit auf randomisierter Glättung, die jeden Klassifikator durch Hinzufügen von Rauschen zu einem Eingabebeispiel in einen neuen Klassifikator verwandelt. Wir leiten eine strenge Robustheit in der $\ell_2$-Norm für Top-$k$-Vorhersagen ab, wenn wir randomisierte Glättung mit Gaußschem Rauschen verwenden. Wir stellen fest, dass die Verallgemeinerung der zertifizierten Robustheit von Top-1- auf Top-$k$-Vorhersagen vor erheblichen technischen Herausforderungen steht. Wir evaluieren unsere Methode auch empirisch auf CIFAR10 und ImageNet. Zum Beispiel kann unsere Methode einen ImageNet-Klassifikator mit einer zertifizierten Top-5-Genauigkeit von 62,8 % erzielen, wenn die $\ell_2$-Normen der gegnerischen Störungen kleiner als 0,5 (=127/255) sind.Unser Code ist öffentlich verfügbar unter: \url{https://github.com/jjy1994/Certify_Topk}.
Diese Methoden haben sich weitgehend auf die Modifizierung der Variationskostenfunktion konzentriert, um dieses Ziel zu erreichen. wir zeigen jedoch, dass Methoden wie beta-VAE die Tendenz der Variationsinferenz zum Underfit vereinfachen, was zu pathologischem Over-Pruning und Über-Orthogonalisierung der gelernten Komponenten führt. in diesem Papier verfolgen wir einen ergänzenden Ansatz: die Modifizierung des probabilistischen Modells, um die Entdeckung strukturierter latenter Variablendarstellungen zu fördern. Insbesondere ist das probabilistische Standardmodell der VAE nicht identifizierbar: Die Wahrscheinlichkeit der Parameter ist bei Rotationen des latenten Raums invariant, d.h. es besteht kein Druck, jeden wahren Variationsfaktor mit einer latenten Variable zu identifizieren. Ausführliche quantitative und qualitative Experimente zeigen, dass der vorgeschlagene Prior den durch modifizierte Kostenfunktionen wie beta-VAE und TCVAE eingeführten Kompromiss zwischen Rekonstruktionsverlust und Entflechtung abschwächt und es ermöglicht, diese Ansätze sowohl hinsichtlich der Entflechtung als auch der Rekonstruktionsqualität gegenüber dem Stand der Technik deutlich zu verbessern.
Die Erklärungen in der Literatur für die offensichtliche Effektivität von Shortcuts sind vielfältig und oft widersprüchlich. Wir stellen die Hypothese auf, dass Shortcuts vor allem deshalb funktionieren, weil sie als lineare Gegenstücke zu nichtlinearen Schichten fungieren. Wir testen diese Hypothese, indem wir verschiedene Variationen des Standard-Residualblocks mit unterschiedlichen Arten von linearen Verbindungen verwenden, um kleine (100k--1.2M Parameter) Bildklassifizierungsnetzwerke zu erstellen.Unsere Experimente zeigen, dass andere Arten von linearen Verbindungen sogar effektiver sein können als die Identitäts-Shortcuts.Unsere Ergebnisse deuten auch darauf hin, dass die beste Art der linearen Verbindung für eine bestimmte Anwendung sowohl von der Netzwerkbreite als auch von der Tiefe abhängen kann.
Adam-typed Optimierer, als eine Klasse von adaptiven Moment-Schätzung Methoden mit dem exponentiellen gleitenden Durchschnitt Schema, wurden erfolgreich in vielen Anwendungen von Deep Learning verwendet.Solche Methoden sind attraktiv für die Fähigkeit auf großen spärlichen Datensätzen.Darüber hinaus sind sie rechnerisch effizient und unempfindlich gegen die Hyper-Parameter-Einstellungen.In diesem Papier stellen wir einen neuen Rahmen für die Anpassung von Adam-typed Methoden, nämlich AdamT. Der neu hinzugefügte Term soll die nicht-horizontalen Bewegungsmuster auf der Kostenoberfläche effizient erfassen und somit schneller konvergieren. Wir zeigen empirisch die Bedeutung der Trendkomponente, wobei AdamT die konventionelle Adam-Methode sowohl in konvexen als auch in nicht-konvexen Einstellungen konstant übertrifft.
Klassische Ansätze, die die Bedeutung von Merkmalen bewerten (z. B. Saliency Maps), erklären nicht, wie und warum eine bestimmte Region eines Bildes für die Vorhersage relevant ist.Wir schlagen eine Methode vor, die das Ergebnis einer Klassifizierungs-Blackbox erklärt, indem sie den semantischen Effekt einer bestimmten Klasse allmählich überhöht. Bei einer Abfrage, die einem Klassifikator eingegeben wird, erzeugt unsere Methode einen progressiven Satz plausibler Variationen dieser Abfrage, die schrittweise die Posterior-Wahrscheinlichkeit von der ursprünglichen Klasse zu ihrer Negation verändern.  Unsere Methode ist modellunabhängig und benötigt nur den Ausgabewert und den Gradienten des Prädiktors in Bezug auf seine Eingabe.
Unsere einflussgesteuerten Erklärungen nähern sich diesem Problem, indem sie in das Netzwerk hineinschauen, um Neuronen mit hohem Einfluss auf die interessierende Eigenschaft mit Hilfe eines axiomatisch begründeten Einflussmaßes zu identifizieren, und dann eine Interpretation für die Konzepte liefern, die diese Neuronen repräsentieren.Wir evaluieren unseren Ansatz durch das Training von faltigen neuronalen Netzwerken auf Pubfig-, ImageNet- und Diabetiker-Retinopathie-Datensätzen.  Unsere Auswertung zeigt, dass einflussorientierte Erklärungen (1) die vom Netzwerk verwendeten Merkmale lokalisieren, (2) Merkmale isolieren, die verwandte Instanzen unterscheiden, (3) dabei helfen, die Essenz dessen zu extrahieren, was das Netzwerk über die Klasse gelernt hat, und (4) bei der Fehlersuche in Fehlklassifikationen helfen.
Herkömmliche Deep-Learning-Systeme benötigen Tausende oder Millionen von Beispielen, um ein Konzept zu erlernen, und können neue Konzepte nicht ohne Weiteres integrieren. Im Gegensatz dazu hat der Mensch die unglaubliche Fähigkeit, in einem einzigen Schritt oder in wenigen Schritten zu lernen. Wir lassen uns davon inspirieren und zeigen eine einfache Technik auf, mit der tiefe rekurrente Netze auf ähnliche Weise ihr Vorwissen nutzen können, um aus wenigen Daten eine nützliche Repräsentation für ein neues Wort zu lernen, was die Systeme zur Verarbeitung natürlicher Sprache wesentlich flexibler machen könnte, da sie kontinuierlich aus neuen Wörtern lernen können.
In der jüngeren Forschung zur Entwicklung neuronaler Netzwerkarchitekturen mit externem Speicher wurde häufig der bAbI-Frage- und Antwortdatensatz verwendet, der eine Reihe anspruchsvoller Aufgaben enthält, die logisches Denken erfordern. Hier haben wir eine klassische assoziative Inferenzaufgabe aus der Literatur der menschlichen Neurowissenschaften verwendet, um die logischen Fähigkeiten bestehender speichererweiterter Architekturen genauer zu untersuchen. Ähnliche Ergebnisse wurden bei einer komplexeren Aufgabe erzielt, bei der es darum ging, den kürzesten Weg zwischen Knoten in einem Pfad zu finden.Wir haben daher eine neuartige Architektur, MEMO, entwickelt, die in der Lage ist, über größere Entfernungen zu denken. Erstens führt sie eine Trennung zwischen Erinnerungen/Fakten, die im externen Speicher gespeichert sind, und den Elementen, aus denen diese Fakten im externen Speicher bestehen, ein.Zweitens nutzt sie einen adaptiven Abrufmechanismus, der eine variable Anzahl von â€˜Memory Hopsâ€™ erlaubt, bevor die Antwort produziert wird.MEMO ist in der Lage, unsere neuartigen Argumentationsaufgaben sowie alle 20 Aufgaben in bAbI zu lösen.
Sie haben sich in Bildklassifizierungsmodellen als erfolgreich erwiesen, sowohl bei der Erzielung besserer Modelle als bisher für eine gegebene Anzahl von Parametern möglich (die Xception-Architektur) als auch bei der beträchtlichen Reduzierung der Anzahl von Parametern, die erforderlich sind, um ein bestimmtes Niveau zu erreichen (die MobileNets-Familie von Architekturen).Kürzlich wurden Faltungs-Sequenz-zu-Sequenz-Netzwerke auf maschinelle Übersetzungsaufgaben mit guten Ergebnissen angewandt.In dieser Arbeit untersuchen wir, wie tiefenweise trennbare Faltungen auf neuronale maschinelle Übersetzung angewendet werden können. Wir stellen eine neue Architektur vor, die von Xception und ByteNet inspiriert ist und SliceNet genannt wird. Sie ermöglicht eine signifikante Reduktion der Parameteranzahl und des Berechnungsaufwands, die notwendig sind, um Ergebnisse wie bei ByteNet zu erzielen, und erzielt bei einer ähnlichen Parameteranzahl bessere Ergebnisse. Wir zeigen nicht nur, dass tiefenweise separierbare Faltungen für die maschinelle Übersetzung gut geeignet sind, sondern untersuchen auch die architektonischen Änderungen, die sie ermöglichen: Wir stellen fest, dass wir dank der tiefenweisen Separierbarkeit die Länge der Faltungsfenster erhöhen können, wodurch die Notwendigkeit der Filterdilatation entfällt.
Die Interpretation des Trainings von generativen adversen Netzwerken (GANs) als annähernde Divergenzminimierung ist theoretisch aufschlussreich, hat die Diskussion beflügelt und zu theoretisch und praktisch interessanten Erweiterungen wie f-GANs und Wasserstein-GANs geführt.Sowohl für klassische GANs als auch für f-GANs gibt es eine ursprüngliche Variante des Trainings und eine "nicht-sättigende" Variante, die eine alternative Form des Generatorgradienten verwendet.Die ursprüngliche Variante ist theoretisch einfacher zu untersuchen, aber für GANs schneidet die alternative Variante in der Praxis besser ab. Das nicht-sättigende Schema wird oft als eine einfache Modifikation angesehen, um Optimierungsprobleme zu lösen, aber wir zeigen, dass das nicht-sättigende Schema für GANs tatsächlich eine umgekehrte KL-ähnliche f-Divergenz optimiert.
Wir stellen eine neuartige Methode zur Umwandlung von Textdaten in abstrakte Bildrepräsentationen vor, die es ermöglicht, bildbasierte Verarbeitungstechniken (z.B. Bildklassifizierungsnetzwerke) auf textbasierte Vergleichsprobleme anzuwenden.Wir wenden die Technik auf die Disambiguierung von Erfindernamen in US-Patenten an.Die Methode beinhaltet die Umwandlung von Text aus jedem paarweisen Vergleich zwischen zwei Erfindernamensdatensätzen in eine 2D-RGB-Bilddarstellung (gestapelt). Anschließend trainieren wir ein neuronales Netzwerk zur Bildklassifizierung, um zwischen solchen paarweisen Vergleichsbildern zu unterscheiden, und verwenden das trainierte Netzwerk, um jedes Datensatzpaar entweder als übereinstimmend (derselbe Erfinder) oder nicht übereinstimmend (verschiedene Erfinder) zu kennzeichnen, wobei wir hochpräzise Ergebnisse erzielen (F1: 99,09 %, Präzision: 99,41 %, Recall: 98,76 %).Unsere neue Text-zu-Bild-Darstellungsmethode könnte potenziell breiter für andere NLP-Vergleichsprobleme verwendet werden, z. B. für die Disambiguierung akademischer Publikationen oder für Probleme, die eine gleichzeitige Klassifizierung von Text und Bildern erfordern.
Wir schlagen einen neuen Algorithmus vor, Difference-Seeking Generative Adversarial Network (DSGAN), der aus dem traditionellen GAN entwickelt wurde.DSGAN berücksichtigt das Szenario, dass die Trainingsmuster der Zielverteilung $p_{t}$ schwer zu sammeln sind.Angenommen, es gibt zwei Verteilungen $p_{\bar{d}}$ und $p_{d}$, so dass die Dichte der Zielverteilung die Unterschiede zwischen den Dichten von $p_{\bar{d}}$ und $p_{d}$ sein kann. Wir zeigen, wie man die Zielverteilung $p_{t}$ nur über Stichproben aus $p_{d}$ und $p_{\bar{d}}$ (relativ einfach zu erhalten) erlernen kann.DSGAN hat die Flexibilität, Stichproben aus verschiedenen Zielverteilungen zu erzeugen (z.B. die Out-of-Distribution).Zwei Schlüsselanwendungen, semi-supervised learning und adversarial training, werden als Beispiele genommen, um die Effektivität von DSGAN zu validieren.Wir liefern auch theoretische Analysen über die Konvergenz von DSGAN.
Kürzlich, Generative Adversarial Network (GAN) und eine Reihe von seinen Varianten wurden weithin verwendet, um die Bild-zu-Bild-Übersetzung Problem zu lösen und erreicht außergewöhnliche Ergebnisse sowohl in einer überwachten und unbeaufsichtigten manner.However, die meisten GAN-basierte Methoden leiden unter dem Ungleichgewicht Problem zwischen dem Generator und Diskriminator in der Praxis.Namentlich, die relative Modell Kapazitäten des Generators und Diskriminator nicht übereinstimmen, was zu Modus Zusammenbruch und / oder verminderte gradients.To tackle dieses Problem, schlagen wir eine GuideGAN auf der Grundlage von Aufmerksamkeit Mechanismus. Genauer gesagt, statten wir den Diskriminator mit einem Aufmerksamkeitsmechanismus aus, so dass er nicht nur die Wahrscheinlichkeit schätzt, dass seine Eingabe real ist, sondern auch eine Aufmerksamkeitskarte erstellt, die die kritischen Merkmale für eine solche Vorhersage hervorhebt.Diese Aufmerksamkeitskarte hilft dann dem Generator, plausiblere und realistischere Bilder zu erzeugen.Wir evaluieren das vorgeschlagene GuideGAN-Framework ausgiebig an einer Reihe von Bildübertragungsaufgaben.Sowohl qualitative Ergebnisse als auch quantitative Vergleiche zeigen die Überlegenheit unseres vorgeschlagenen Ansatzes.
Das Problem der Überprüfung, ob eine Texthypothese auf der Grundlage der gegebenen Evidenz zutrifft, auch bekannt als Faktenüberprüfung, spielt eine wichtige Rolle bei der Untersuchung des Verstehens natürlicher Sprache und der semantischen Repräsentation, wobei sich bestehende Studien hauptsächlich auf den Umgang mit unstrukturierter Evidenz beschränken (z.B., Diese Arbeit zielt speziell darauf ab, die Faktenüberprüfung anhand von halbstrukturierten Daten als Beweismittel zu untersuchen. Zu diesem Zweck konstruieren wir einen großen Datensatz namens TabFact mit 16k Wikipedia-Tabellen als Beweise für 118k von Menschen kommentierte natürlichsprachliche Aussagen, die entweder mit ENTAILED oder REFUTED gekennzeichnet sind.TabFact ist eine Herausforderung, da es sowohl weiche linguistische Argumentation als auch harte symbolische Argumentation beinhaltet.Um diese Herausforderungen zu bewältigen, entwickeln wir zwei verschiedene Modelle: Table-BERT und Latent Program Algorithm (LPA).Table-BERT nutzt das hochmoderne vortrainierte Sprachmodell, um die linearisierten Tabellen und Aussagen in kontinuierliche Vektoren für die Verifizierung zu kodieren.LPA parst die Aussagen in LISP-ähnliche Programme und führt sie gegen die Tabellen aus, um den zurückgegebenen Binärwert für die Verifizierung zu erhalten.Beide Methoden erreichen eine ähnliche Genauigkeit, liegen aber immer noch weit hinter der menschlichen Leistung.Wir führen auch eine umfassende Analyse durch, um große zukünftige Möglichkeiten aufzuzeigen.
In dieser Arbeit wird eine zweistufige neuronale Architektur zum Erlernen und Verfeinern struktureller Korrespondenzen zwischen Graphen vorgestellt: Erstens verwenden wir lokalisierte Knoteneinbettungen, die von einem neuronalen Graphen-Netzwerk berechnet werden, um eine anfängliche Rangfolge weicher Korrespondenzen zwischen Knoten zu erhalten, und zweitens setzen wir synchrone Nachrichtenübermittlungsnetzwerke ein, um die weichen Korrespondenzen iterativ neu zu ordnen und einen übereinstimmenden Konsens in lokalen Nachbarschaften zwischen Graphen zu erreichen. Wir zeigen theoretisch und empirisch, dass unser Nachrichtenübermittlungsschema ein wohlbegründetes Konsensmaß für entsprechende Nachbarschaften berechnet, das dann verwendet wird, um den iterativen Umreihungsprozess zu leiten.unsere rein lokale und sparsame Architektur skaliert gut auf große, reale Eingaben, während sie immer noch in der Lage ist, globale Korrespondenzen konsistent wiederherzustellen.wir demonstrieren die praktische Wirksamkeit unserer Methode an realen Aufgaben aus den Bereichen Computer Vision und Entity Alignment zwischen Wissensgraphen, bei denen wir den aktuellen Stand der Technik verbessern.
Diese Arbeit erweitert den Nachweis der Dichte neuronaler Netze im Raum kontinuierlicher (oder sogar messbarer) Funktionen auf euklidischen Räumen auf Funktionen auf kompakten Mengen von Wahrscheinlichkeitsmaßen und weist damit Parallelen zu einem mehr als zehn Jahre alten Ergebnis zur Mean-Map-Einbettung von Wahrscheinlichkeitsmaßen in reproduzierende Kern-Hilbert-Räume auf.  Das Ergebnis wird dann auf kartesische Produkte ausgedehnt, was zu einem universellen Approximationstheorem für baumstrukturierte Domänen führt, die natürlich in Datenaustauschformaten wie JSON, XML, YAML, AVRO und ProtoBuffer vorkommen, was wichtige praktische Auswirkungen hat, da es die automatische Erstellung einer Architektur neuronaler Netze für die Verarbeitung strukturierter Daten (AutoML-Paradigmen) ermöglicht, wie eine begleitende Bibliothek für das JSON-Format zeigt.
Interaktionen wie doppelte Negation in SÃ?tzen und Szeneninteraktionen in Bildern sind hÃ?ufige Formen komplexer AbhÃ?ngigkeiten, die von modernen maschinellen Lernmodellen erfasst werden. Wir schlagen MahÃ© vor, einen neuartigen Ansatz, der modell-agnostische hierarchische ErklÃ?rungen darÃ?ber liefert, wie leistungsfÃ?hige maschinelle Lernmodelle, z. B. tiefe neuronale Netze, diese Interaktionen entweder abhÃ?ngig vom Kontext der Dateninstanzen oder frei davon erfassen. MahÃ© liefert kontextabhÃ?ngige ErklÃ?rungen durch einen neuartigen lokalen Interpretationsalgorithmus, der effektiv Interaktionen beliebiger Ordnung erfasst, und liefert kontextfreie ErklÃ?rungen durch Verallgemeinerung kontextabhÃ?ngiger Interaktionen zur ErklÃ?rung globalen Verhaltens.â€žExperimentelle Ergebnisse zeigen, dass MahÃ© verbesserte lokale Interaktionsinterpretationen gegenÃ?ber modernsten Methoden erzielt und erfolgreich kontextfreie ErklÃ?rungen von Interaktionen liefert.
Um das Versprechen einer allgegenwÃ?rtigen eingebetteten tiefen Netzwerkinferenz zu verwirklichen, ist es unerlÃ?sslich, die Grenzen der Energie- und FlÃ?cheneffizienz auszuloten.  Zu diesem Zweck bieten Netzwerke mit geringer Genauigkeit ein enormes Potenzial, da sowohl Energie als auch Fläche quadratisch mit der Verringerung der Genauigkeit abnehmen.  Hier demonstrieren wir zum ersten Mal ResNet-18, ResNet-34, ResNet-50, ResNet-152, Inception-v3, densenet-161 und VGG-16bn-Netzwerke im ImageNet-Klassifizierungsbenchmark, die bei einer 8-Bit-Präzision die Genauigkeit der vollpräzisen Basisnetzwerke nach einer Epoche des Finetunings übertreffen und dabei die Verfügbarkeit von vortrainierten Modellen nutzen.Wir demonstrieren auch ResNet-18, ResNet-34 und ResNet-50 4-Bit-Modelle, die die Genauigkeit der vollpräzisen Basisnetzwerke erreichen - die bisher höchsten Werte. Wir stellen fest, dass das Gradientenrauschen aufgrund der Quantisierung während des Trainings mit abnehmender Präzision zunimmt, und suchen nach Möglichkeiten, dieses Rauschen zu überwinden. Die Anzahl der Iterationen, die der stochastische Gradientenabstieg benötigt, um einen bestimmten Trainingsfehler zu erreichen, hängt mit dem Quadrat (a) des Abstands der Ausgangslösung von der endgültigen Lösung und (b) der maximalen Varianz der Gradientenschätzungen zusammen.  In Anlehnung an diese Beobachtung reduzieren wir (a) den Lösungsabstand, indem wir mit vortrainierten fp32-Präzisions-Basisnetzen beginnen und eine Feinabstimmung vornehmen, und (b) bekämpfen das Rauschen, das durch die Quantisierung von Gewichten und Aktivierungen während des Trainings entsteht, indem wir größere Stapel zusammen mit einer angepassten Lernrate (Annealing) verwenden.  Die Sensitivitätsanalyse zeigt, dass diese Techniken in Verbindung mit einer angemessenen Kalibrierung des Aktivierungsfunktionsbereichs eine vielversprechende Heuristik bieten, um Netze mit geringer Genauigkeit zu entdecken, wenn sie in der Nähe von fp32-Präzisions-Basinennetzen existieren.
  Hier stellen wir zwei Methoden vor, die auf der Repräsentativen Ähnlichkeitsanalyse (RSA) und auf Baumkernen (TK) basieren und die es uns ermöglichen, direkt zu quantifizieren, wie stark die in neuronalen Aktivierungsmustern kodierten Informationen mit Informationen übereinstimmen, die durch symbolische Strukturen wie Syntaxbäume dargestellt werden. Wir validieren unsere Methoden zunächst am Beispiel einer einfachen synthetischen Sprache für arithmetische Ausdrücke mit klar definierter Syntax und Semantik und zeigen, dass sie das erwartete Ergebnismuster aufweisen.Wir wenden unsere Methoden dann an, um neuronale Repräsentationen englischer Sätze mit ihren Konstituenten-Parsenbäumen zu korrelieren.
Überwachtes Deep Learning erfordert eine große Menge an Trainingsmustern mit Annotationen (z.B. Labelklassen für Klassifizierungsaufgaben, pixel- oder voxelbasierte Labelkarten für Segmentierungsaufgaben), die teuer und zeitaufwändig zu beschaffen sind.Während des Trainings eines tiefen neuronalen Netzwerks werden die annotierten Muster in das Netzwerk in einer Mini-Batch-Methode eingespeist, wobei sie oft als gleich wichtig angesehen werden. Einige der Proben können jedoch während des Trainings weniger informativ werden, da die Größe des Gradienten für diese Proben zu verschwinden beginnt; in der Zwischenzeit können andere Proben mit höherem Nutzen oder höherer Härte für den Fortgang des Trainingsprozesses wichtiger sein und mehr Ausnutzung erfordern. Um die Herausforderungen der teuren Annotationen und des Verlusts der Informativität der Proben zu bewältigen, schlagen wir hier einen neuartigen Trainingsrahmen vor, der adaptiv informative Proben auswählt, die in den Trainingsprozess eingespeist werden.Die adaptive Auswahl oder das Sampling wird auf der Grundlage einer härtebewussten Strategie im latenten Raum durchgeführt, der durch ein generatives Modell konstruiert wird. Um den vorgeschlagenen Trainingsrahmen zu evaluieren, führen wir Experimente mit drei verschiedenen Datensätzen durch, darunter MNIST und CIFAR-10 für Bildklassifizierungsaufgaben und ein medizinischer Bilddatensatz IVUS für biophysikalische Simulationsaufgaben, und bei allen drei Datensätzen übertrifft der vorgeschlagene Rahmen eine Zufallsstichprobenmethode, was die Effektivität unseres Rahmens zeigt.
Bestehende Methoden für KI-generierte Kunstwerke kämpfen immer noch mit der Generierung qualitativ hochwertiger stilisierter Inhalte, bei denen die Semantik auf hoher Ebene erhalten bleibt, oder mit der Trennung feinkörniger Stile von verschiedenen Künstlern.Wir schlagen ein neuartiges Generative Adversarial Disentanglement Network vor, das zwei komplementäre Faktoren von Variationen entwirren kann, wenn nur einer von ihnen im Allgemeinen etikettiert ist, und komplexe Anime-Illustrationen vollständig in Stil und Inhalt im Besonderen zerlegen kann.Das Training eines solchen Modells ist eine Herausforderung, da für einen Stil verschiedene Inhaltsdaten existieren können, aber nicht andersherum. Unser Ansatz ist in zwei Stufen unterteilt, eine, die ein Eingabebild in einen stilunabhängigen Inhalt kodiert, und eine, die auf einem dualen Bedingungsgenerator basiert. wir zeigen die Fähigkeit, hochrealistische Anime-Porträts mit einem festen Inhalt und einer großen Vielfalt an Stilen von über tausend Künstlern zu generieren und umgekehrt, unter Verwendung eines einzigen End-to-End-Netzwerks und mit Anwendungen im Stil-Transfer. wir zeigen diese einzigartige Fähigkeit sowie eine überlegene Leistung gegenüber dem aktuellen Stand der Technik.
Neuere Forschungen haben gezeigt, dass CNNs oft übermäßig empfindlich auf hochfrequente Texturmuster reagieren. Inspiriert von der Intuition, dass Menschen empfindlicher auf niederfrequente (großflächigere) Muster reagieren, entwickeln wir ein Regularisierungsschema, das große Unterschiede zwischen benachbarten Komponenten innerhalb jedes Faltungskerns bestraft. Wir wenden unsere Regularisierung auf mehrere populäre Trainingsmethoden an und zeigen, dass die Modelle mit den vorgeschlagenen glatten Kerneln eine verbesserte Robustheit gegenüber gegnerischen Angriffen aufweisen.
In dieser Arbeit untersuchen wir das Verhalten der Q-Wert-Schätzungen bei großen Stichproben und charakterisieren die asymptotischen Varianzen in geschlossener Form, was es uns ermöglicht, Vertrauensbereiche für Q-Wert- und optimale Wertfunktionen effizient zu konstruieren und Strategien zu entwickeln, um ihre Schätzfehler zu minimieren, was auch zu einer Strategie für die Strategieexploration führt, die sich auf die Schätzung der relativen Diskrepanzen zwischen den Q-Schätzungen stützt.Numerische Experimente zeigen, dass unsere Explorationsstrategie besser abschneidet als andere Benchmark-Ansätze.
Entailment-Vektoren sind ein prinzipieller Weg, um in einem Vektor zu kodieren, welche Informationen bekannt und welche unbekannt sind.  Sie wurden entwickelt, um Beziehungen zu modellieren, bei denen ein Vektor alle Informationen eines anderen Vektors enthalten sollte, was als Entailment bezeichnet wird.  In dieser Arbeit wird das unüberwachte Lernen von Entailment-Vektoren für die Semantik von Wörtern untersucht.  Unter Verwendung einfacher entailment-basierter Modelle der Wortsemantik in Texten (distributionelle Semantik) induzieren wir entailment-Vektor-Worteinbettungen, die die besten bisherigen Ergebnisse für die Vorhersage von entailment zwischen Wörtern in unbeaufsichtigten und halb-überwachten Experimenten zur Hyponymie übertreffen.
Wir beschreiben ein einfaches Schema, das es einem Agenten ermöglicht, auf unbeaufsichtigte Weise etwas über seine Umgebung zu lernen, indem wir zwei Versionen desselben Agenten, Alice und Bob, gegeneinander antreten lassen: Alice schlägt Bob eine Aufgabe vor, die er erfüllen soll, und Bob versucht, die Aufgabe zu erfüllen.  In dieser Arbeit werden wir uns auf zwei Arten von Umgebungen konzentrieren: (Alice "schlägt" die Aufgabe vor, indem sie eine Reihe von Aktionen ausführt, die Bob dann rückgängig machen bzw. wiederholen muss.  Durch eine geeignete Belohnungsstruktur erstellen Alice und Bob automatisch einen Explorationsplan, der ein unüberwachtes Training des Agenten ermöglicht. Wenn Bob für eine RL-Aufgabe in der Umgebung eingesetzt wird, reduziert dieses unüberwachte Training die Anzahl der zum Lernen erforderlichen überwachten Episoden und führt in einigen Fällen zu einer höheren Belohnung.
Viele reale Datensätze sind als Graphen dargestellt, wie z.B. Zitierlinks, soziale Medien und biologische Interaktionen. Die flüchtige Graphenstruktur macht es nicht trivial, Faltungsneuronale Netze (CNNs) für die Graphdatenverarbeitung einzusetzen. Kürzlich hat Graph Attention Network (GAT) einen vielversprechenden Versuch durch die Kombination von Graph neuronale Netze mit Aufmerksamkeit Mechanismus, um zu erreichen, Massage Passing in Graphen mit beliebigen structures.However, die Aufmerksamkeit in GAT ist vor allem auf der Grundlage der Ähnlichkeit zwischen den Knoten Inhalt berechnet, während die Strukturen des Graphen bleibt weitgehend arbeitslos (außer bei der Maskierung der Aufmerksamkeit aus One-Hop-Nachbarn). In diesem Papier schlagen wir ein `````````````````````````````"ADaptive Structural Fingerprint" (ADSF) Modell vor, um sowohl topologische Details des Graphen als auch inhaltliche Merkmale der Knoten vollständig zu nutzen. Darüber hinaus bietet unser Modell eine nützliche Plattform für verschiedene Unterräume von Knotenmerkmalen und verschiedene Skalen von Graphenstrukturen, um durch das Lernen von Multi-Head-Attention miteinander zu "sprechen", was besonders bei der Verarbeitung komplexer realer Daten nützlich ist.  Bei einer Reihe von Benchmark-Datensätzen wird eine ermutigende Leistung bei der Knotenklassifizierung beobachtet.
Wir formulieren dies als Bayes'sches Verstärkungslernproblem über latente Markov-Entscheidungsprozesse (MDPs). Während Bayes-Optimalität theoretisch der Goldstandard ist, skalieren bestehende Algorithmen nicht gut auf kontinuierliche Zustands- und Aktionsräume. Wir schlagen eine skalierbare Lösung vor, die auf der folgenden Einsicht aufbaut: In Abwesenheit von Unsicherheit ist jedes latente MDP einfacher zu lösen. Unser Algorithmus, Bayesian Residual Policy Optimization (BRPO), importiert die Skalierbarkeit von Policy-Gradienten-Methoden sowie die Initialisierung aus früheren Modellen. BRPO verbessert das Ensemble von Experten signifikant und übertrifft bestehende adaptive RL-Methoden drastisch.
Eines der Geheimnisse des Erfolgs neuronaler Netze ist, dass zufällig initialisierte Methoden erster Ordnung wie Gradientenabstieg einen Trainingsverlust von Null erreichen können, obwohl die Zielfunktion nicht-konvex und nicht-glatt ist.Dieses Papier entmystifiziert dieses überraschende Phänomen für zweischichtige voll verbundene ReLU-aktivierte neuronale Netze. Für ein flaches neuronales Netz mit $m$ versteckten Knoten mit ReLU-Aktivierung und $n$ Trainingsdaten zeigen wir, dass, solange $m$ groß genug ist und keine zwei Eingaben parallel sind, ein zufällig initialisierter Gradientenabstieg mit einer linearen Konvergenzrate für die quadratische Verlustfunktion zu einer global optimalen Lösung konvergiert. Unsere Analyse stützt sich auf die folgende Beobachtung: Überparametrisierung und zufällige Initialisierung schränken gemeinsam jeden Gewichtsvektor so ein, dass er für alle Iterationen nahe an seiner Initialisierung liegt, was uns erlaubt, eine starke konvexitätsähnliche Eigenschaft auszunutzen, um zu zeigen, dass der Gradientenabstieg mit einer globalen linearen Rate zum globalen Optimum konvergiert.Wir glauben, dass diese Einsichten auch bei der Analyse von tiefen Modellen und anderen Methoden erster Ordnung nützlich sind.
FÃ?r viele Anwendungen, insbesondere in den Naturwissenschaften, besteht die Aufgabe darin, versteckte Systemparameter aus einer Reihe von Messungen zu bestimmen.Oft ist der VorwÃ?rtsprozess vom Parameter- zum Messraum wohldefiniert, wÃ?hrend das inverse Problem mehrdeutig ist: mehrere ParametersÃ?tze kÃ¶nnen zur gleichen Messung fÃ?hren.Um diese Mehrdeutigkeit vollstÃ?ndig zu charakterisieren, muss die vollstÃ?ndige posteriore Parameterverteilung, die von einer beobachteten Messung abhÃ?ngig ist, bestimmt werden.Wir argumentieren, dass eine bestimmte Klasse von neuronalen Netzen fÃ?r diese Aufgabe gut geeignet ist â€" so genannte Invertible Neuronale Netze (INNs). Im Gegensatz zu klassischen neuronalen Netzen, die versuchen, das mehrdeutige inverse Problem direkt zu lÃ¶sen, konzentrieren sich INNs auf das Lernen des VorwÃ?rtsprozesses und verwenden zusÃ?tzliche latente Ausgangsvariablen, um die ansonsten verlorenen Informationen zu erfassen. Wir beweisen theoretisch und experimentell anhand von künstlichen Daten und realen Problemen aus der Medizin und Astrophysik, dass INNs ein leistungsstarkes Analysewerkzeug sind, um Multimodalitäten im Parameterraum zu finden, Parameterkorrelationen aufzudecken und nicht wiederherstellbare Parameter zu identifizieren.
Entscheidungen, die von maschinellen Lernsystemen getroffen werden, haben einen zunehmenden Einfluss auf die Welt.Dennoch ist es üblich, dass maschinelle Lernalgorithmen davon ausgehen, dass es keinen solchen Einfluss gibt.Ein Beispiel ist die Verwendung der i.i.d.-Annahme beim Online-Lernen für Anwendungen wie Inhaltsempfehlungen, bei denen die (Auswahl der) angezeigten Inhalte die Wahrnehmungen und Präferenzen der Nutzer verändern oder sie sogar vertreiben können, was zu einer Verschiebung der Verteilung der Nutzer führt.Generell ist es möglich, dass ein Algorithmus die Verteilung seiner eigenen Eingaben verändert.Wir führen den Begriff selbstinduzierte Verteilungsverschiebung (SIDS) ein, um dieses Phänomen zu beschreiben. Unser Ziel ist ähnlich, aber anders: Wir zeigen auf, dass Änderungen am Lernalgorithmus, wie z.B. die Einführung von Meta-Lernen, versteckte Anreize für Verteilungsverschiebungen (HIDS) aufdecken können, und zielen darauf ab, Probleme im Zusammenhang mit versteckten Anreizen zu diagnostizieren und zu verhindern. Wir entwerfen eine einfache Â Umgebung als "Einheitstest" für HIDS sowie eine Umgebung für Inhaltsempfehlungen, die es uns ermöglicht, verschiedene Arten von SIDS zu unterscheiden. Wir zeigen das Potenzial von HIDS auf, unerwartetes oder unerwünschtes Verhalten in diesen Umgebungen zu verursachen, und schlagen eine Strategie zur Abschwächung vor und testen sie.Â 
In Ein-Klassen-Lernaufgaben kann nur der Normalfall mit Daten modelliert werden, wÃ?hrend die Variation aller mÃ¶glichen Anomalien zu groÃŸ ist, um ausreichend durch Stichproben beschrieben zu werden. Aufgrund des Mangels an reprÃ?sentativen Daten kÃ¶nnen die weit verbreiteten diskriminativen AnsÃ?tze solche Lernaufgaben nicht abdecken, und stattdessen werden generative Modelle verwendet, die versuchen, die Eingabedichte der NormalfÃ?lle zu lernen. â€žGenerative Modelle leiden jedoch unter einer groÃŸen EingabedimensionalitÃ?t (wie bei Bildern) und sind typischerweise ineffiziente Lerner. Wir schlagen vor, die Datenverteilung mit einem Multi-Hypothesen-Autoencoder effizienter zu erlernen, wobei das Modell durch einen Diskriminator kritisiert wird, der künstliche Datenmodi, die nicht durch die Daten unterstützt werden, verhindert und die Vielfalt der Hypothesen durchsetzt. Dieser auf Konsistenz basierende Rahmen für die Erkennung von Anomalien (ConAD) ermöglicht die zuverlässige Identifizierung von Stichproben, die außerhalb der Verteilung liegen, und führt bei der Erkennung von Anomalien auf CIFAR-10 zu einer Verbesserung von bis zu 3,9 Prozentpunkten gegenüber den zuvor berichteten Ergebnissen.
Generative Adversarial Networks (GAN) können vielversprechende Leistungen beim Lernen komplexer Datenverteilungen auf verschiedenen Datentypen erzielen. In diesem Papier zeigen wir zunächst, dass eine einfache Erweiterung eines bestehenden GAN-Algorithmus nicht auf Punktwolken anwendbar ist, da die für Diskriminatoren erforderliche Einschränkung für Datensätze undefiniert ist. Wir schlagen eine zweifache Modifikation eines GAN-Algorithmus vor, um Punktwolken generieren zu können (PC-GAN): Erstens kombinieren wir Ideen aus der hierarchischen Bayes'schen Modellierung und impliziten generativen Modellen, indem wir einen hierarchischen und interpretierbaren Sampling-Prozess erlernen; eine Schlüsselkomponente unserer Methode ist, dass wir ein posteriores Inferenznetzwerk für die verborgenen Variablen trainieren. Wir schlagen außerdem ein Sandwiching-Ziel vor, das zu einer engeren Wasserstein-Distanz-Schätzung führt als die üblicherweise verwendete duale Form in WGAN. Wir validieren unsere Behauptungen auf dem ModelNet40-Benchmark-Datensatz und stellen fest, dass PC-GAN, das durch das Sandwiching-Ziel trainiert wird, bessere Ergebnisse auf Testdaten erzielt als bestehende Methoden. Wir führen auch Studien zu verschiedenen Aufgaben durch, darunter Generalisierung auf ungesehenen Punktwolken, latente Rauminterpolation, Klassifizierung und Bild-Punktwolken-Transformation, um die Vielseitigkeit des vorgeschlagenen PC-GAN-Algorithmus zu demonstrieren.
Bestehende Aufmerksamkeitsmechanismen sind meist objektbasiert, d.h. ein Modell wird darauf trainiert, einzelne Objekte in einer Sammlung (dem Speicher) zu beachten, wobei jedes Objekt eine vordefinierte, feste Granularität hat, z.B. ein Zeichen oder ein Wort, Intuitiv kann ein Bereich im Gedächtnis, der aus mehreren Elementen besteht, es wert sein, als Ganzes beachtet zu werden. Wir schlagen die Bereichsaufmerksamkeit vor: eine Möglichkeit, einen Bereich des Gedächtnisses zu beachten, bei der jeder Bereich eine Gruppe von Elementen enthält, die entweder räumlich benachbart sind, wenn das Gedächtnis eine zweidimensionale Struktur hat, wie z. B. Bilder, oder zeitlich benachbart sind, wenn es sich um ein eindimensionales Gedächtnis handelt, wie z. B. Sätze in natürlicher Sprache.Wichtig ist, dass die Größe eines Bereichs, d. h, Die Größe eines Bereichs, d.h. die Anzahl der Elemente in einem Bereich oder der Grad der Aggregation, wird dynamisch durch Lernen bestimmt, das in Abhängigkeit von der erlernten Kohärenz der benachbarten Elemente variieren kann.Indem man dem Modell die Möglichkeit gibt, einen Bereich von Elementen zu beachten, anstatt nur einzelne Elemente, kann ein Modell Informationen mit unterschiedlicher Granularität beachten.Bereichsaufmerksamkeit kann zusammen mit der Mehrkopfaufmerksamkeit funktionieren, um mehrere Bereiche im Gedächtnis zu beachten. Wir evaluieren die Bereichsaufmerksamkeit bei zwei Aufgaben: neuronale maschinelle Übersetzung (sowohl auf Zeichen- als auch auf Token-Ebene) und Bildbeschriftung, und verbessern die starken (State-of-the-Art-) Grundlinien in allen Fällen. Diese Verbesserungen sind mit einer Grundform der Bereichsaufmerksamkeit erreichbar, die parameterfrei ist.
Wir stellen ein Phänomen fest, das wir als *Multi-Modell-Vergessen* bezeichnen und das auftritt, wenn man nacheinander mehrere tiefe Netzwerke mit teilweise gemeinsamen Parametern trainiert; die Leistung der zuvor trainierten Modelle verschlechtert sich, wenn man ein nachfolgendes Modell optimiert, da gemeinsame Parameter überschrieben werden. Um dies zu überwinden, führen wir einen statistisch begründeten Gewichts-Plastizitäts-Verlust ein, der das Lernen der gemeinsamen Parameter eines Modells entsprechend ihrer Bedeutung für die vorherigen Modelle reguliert, und demonstrieren seine Wirksamkeit, wenn zwei Modelle nacheinander trainiert werden und für die neuronale Architektursuche.Das Hinzufügen von Gewichts-Plastizität bei der neuronalen Architektursuche bewahrt die besten Modelle bis zum Ende der Suche und führt zu verbesserten Ergebnissen sowohl bei der Verarbeitung natürlicher Sprache als auch bei Computer-Vision-Aufgaben.
Die Aufdeckung latenter Strukturen in Daten ist ein aktives Forschungsgebiet, das aufregende Technologien wie Variations-Auto-Coder und adversarische Netze eingeführt hat und wesentlich ist, um das maschinelle Lernen in Richtung der unbeaufsichtigten Wissensentdeckung voranzutreiben.eine große Herausforderung ist jedoch das Fehlen geeigneter Benchmarks für eine objektive und quantitative Bewertung gelernter Darstellungen.um dieses Problem anzugehen, stellen wir Morpho-MNIST vor, ein Rahmenwerk, das darauf abzielt, Antworten zu geben: "Wir erweitern den beliebten MNIST-Datensatz um eine morphometrische Analyse, die einen quantitativen Vergleich der trainierten Modelle, die Identifizierung der Rolle latenter Variablen und die Charakterisierung der Stichprobenvielfalt ermöglicht, und schlagen eine Reihe quantifizierbarer Störungen vor, um die Leistung unbeaufsichtigter und überwachter Methoden bei anspruchsvollen Aufgaben wie Ausreißererkennung und Domänenanpassung zu bewerten.
Wie können wir Agenten mit generischen induktiven Vorlieben entwerfen, so dass sie auf konsistente Weise explorieren können, anstatt nur lokale Explorationsschemata wie Epsilon-Greedy zu verwenden? Wir schlagen einen unbeaufsichtigten Reinforcement-Learning-Agenten vor, der ein diskretes Pixel-Gruppierungsmodell lernt, das die räumliche Geometrie der Sensoren und implizit auch der Umgebung bewahrt. Wir verwenden diese Darstellung, um geometrische intrinsische Belohnungsfunktionen abzuleiten, wie z.B. Schwerpunktkoordinaten und Fläche, und lernen Richtlinien, um jede von ihnen mit Off-Policy-Lernen zu kontrollieren. Diese Richtlinien bilden einen Basissatz von Verhaltensweisen (Optionen), die es uns erlauben, auf konsistente Weise zu erforschen und sie in einem hierarchischen Reinforcement-Learning-Setup zu verwenden, um für extrinsisch definierte Belohnungen zu lösen.Wir zeigen, dass unser Ansatz auf eine Vielzahl von Domänen mit konkurrenzfähiger Leistung skalieren kann, einschließlich Navigation in 3D-Umgebungen und Atari-Spiele mit spärlichen Belohnungen.
Während in der Regel solche Probleme sind NP-Hard, aus praktischer Sicht, lokal optimale Lösungen können nützlich sein.in einigen kombinatorischen Problemen kann es jedoch schwer zu definieren, sinnvolle Lösung Nachbarschaften, die große Teile des Suchraums zu verbinden, so behindern Methoden, die diesen Raum direkt zu suchen.wir schlagen vor, solche Fälle durch die Verwendung einer Politik Gradient Algorithmus, der das Problem auf die kontinuierliche Domain transformiert zu umgehen, und ein neues Surrogat Ziel, dass die ehemalige als generische stochastische Optimierer macht zu optimieren. Da wir an Methoden interessiert sind, die erfolgreich lokal optimale Lösungen finden können, verwenden wir das Problem, lokal maximale Cliquen zu finden, als anspruchsvollen experimentellen Benchmark und berichten über Ergebnisse auf einem großen Datensatz von Graphen, der zum Testen von Cliquenfindungsalgorithmen entwickelt wurde. Insbesondere zeigen wir in diesem Benchmark, dass die Festlegung der Verteilung des Surrogats der Schlüssel zur konsistenten Wiederherstellung lokal optimaler Lösungen ist, und dass unser Surrogat-Ziel zu einem Algorithmus führt, der andere von uns getestete Methoden in einer Reihe von Messungen übertrifft.
Deterministische neuronale Netze (NNs) werden zunehmend in sicherheitskritischen Bereichen eingesetzt, in denen kalibrierte, robuste und effiziente Unsicherheitsmaße von entscheidender Bedeutung sind. Während es möglich ist, Regressionsnetze so zu trainieren, dass sie die Parameter einer Wahrscheinlichkeitsverteilung durch Maximierung einer Gaußschen Likelihood-Funktion ausgeben, bleibt dem resultierenden Modell die zugrundeliegende Zuversicht seiner Vorhersagen verborgen.In diesem Papier schlagen wir eine neuartige Methode für das Training deterministischer NNs vor, um nicht nur das gewünschte Ziel zu schätzen, sondern auch die damit verbundenen Beweise zur Unterstützung dieses Ziels. Wir erreichen dies, indem wir Evidenzprioritäten über unsere ursprüngliche Gaußsche Likelihood-Funktion legen und unser NN trainieren, um die Hyperparameter unserer Evidenzverteilung abzuleiten. So schätzt das Modell nicht nur den probabilistischen Mittelwert und die Varianz unseres Ziels, sondern auch die zugrundeliegende Ungewissheit, die mit jedem dieser Parameter verbunden ist. Wir beobachten, dass unsere Methode der evidenzbasierten Regression gut kalibrierte Maße der Ungewissheit auf verschiedenen Benchmarks erlernt, auf komplexe Computer-Vision-Aufgaben skaliert und robust gegenüber nachteiligen Input-Störungen ist.
Die Lotterielos-Hypothese von Frankle & Carbin (2019) besagt, dass es möglich ist, für typisch große neuronale Netze kleine Subnetze zu finden, die schneller trainieren und eine bessere Leistung erbringen als ihre ursprünglichen Gegenstücke.Der vorgeschlagene Algorithmus zur Suche nach solchen Subnetzen (Gewinntickets), Iterative Magnitude Pruning (IMP), findet durchweg Subnetze mit 90-95% weniger Parametern, die tatsächlich schneller und besser trainieren als die überparametrisierten Modelle, aus denen sie extrahiert wurden, was potenzielle Anwendungen für Probleme wie Transferlernen schafft. In diesem Papier schlagen wir einen neuen Algorithmus für die Suche nach Gewinntickets vor, Continuous Sparsification, der während des Trainings kontinuierlich Parameter aus einem Netzwerk entfernt und die Struktur des Subnetzwerks mit gradientenbasierten Methoden lernt, anstatt sich auf Pruning-Strategien zu verlassen. wir zeigen empirisch, dass unsere Methode in der Lage ist, Tickets zu finden, die die durch Iterative Magnitude Pruning gelernten übertreffen, und gleichzeitig eine bis zu 5-mal schnellere Suche bietet, gemessen an der Anzahl der Trainingsepochen.
In den meisten praktischen Situationen und theoretischen Analysen wird davon ausgegangen, dass ein Modell bis zur Konvergenz trainiert werden kann, aber die wachsende Komplexität von Machine-Learning-Datensätzen und -Modellen kann solche Annahmen verletzen.Tatsächlich sind aktuelle Ansätze für die Abstimmung von Hyperparametern und die Suche nach neuronalen Architekturen in der Regel durch praktische Ressourcenbeschränkungen begrenzt.Daher führen wir eine formale Einstellung für die Untersuchung des Trainings unter dem nicht asymptotischen, ressourcenbeschränkten Regime ein, d.h. budgetiertes Training, Wir analysieren das folgende Problem: "Was ist die beste erreichbare Leistung, wenn ein Datensatz, ein Algorithmus und ein festes Ressourcenbudget gegeben sind?" Wir konzentrieren uns auf die Anzahl der Optimierungsiterationen als repräsentative Ressource und zeigen, dass es in einer solchen Umgebung entscheidend ist, den Lernratenplan entsprechend dem gegebenen Budget anzupassen.Unter den budgetbewussten Lernplänen finden wir, dass ein einfacher linearer Zerfall sowohl robust als auch leistungsstark ist. Wir untermauern unsere Behauptung durch umfangreiche Experimente mit State-of-the-Art-Modellen auf ImageNet (Bildklassifikation), Kinetics (Videoklassifikation), MS COCO (Objekterkennung und Instanzsegmentierung) und Cityscapes (semantische Segmentierung).Wir analysieren auch unsere Ergebnisse und stellen fest, dass der Schlüssel zu einem guten Zeitplan die budgetierte Konvergenz ist, ein Phänomen, bei dem der Gradient am Ende jedes erlaubten Budgets verschwindet.Wir überprüfen auch bestehende Ansätze für schnelle Konvergenz und zeigen, dass budgetbewusste Lernzeitpläne solche Ansätze unter (der praktischen, aber wenig erforschten) budgetierten Trainingseinstellung leicht übertreffen.
Wir stellen einen neuen Ansatz zur effizienten Exploration vor, der eine niedrigdimensionale Kodierung der Umgebung nutzt, die mit einer Kombination aus modellbasierten und modellfreien Zielen erlernt wird. Unser Ansatz verwendet intrinsische Belohnungen, die auf einem gewichteten Abstand der nächsten Nachbarn im niedrigdimensionalen Repräsentationsraum basieren, um Neuheit zu messen. Ein Schlüsselelement unseres Ansatzes besteht darin, dass wir zwischen jedem Umgebungsschritt mehrere Gradientenschritte durchführen, um die Modellgenauigkeit zu gewährleisten. Wir testen unseren Ansatz an einer Reihe von Labyrinthaufgaben sowie an einem Kontrollproblem und zeigen, dass unser Explorationsansatz im Vergleich zu starken Basislösungen stichprobeneffizienter ist.
Während sich die bestehende Literatur weitgehend auf die Anfälligkeit von gelernten Modellen konzentriert, zeigen wir ein faszinierendes Phänomen, nämlich dass die Robustheit des gegnerischen Modells im Gegensatz zur reinen Genauigkeit empfindlich auf die Verteilung der Eingabedaten reagiert: Selbst eine semantikerhaltende Transformation der Eingabedatenverteilung kann zu einer signifikant anderen Robustheit des gegnerisch trainierten Modells führen, das sowohl auf der neuen Verteilung trainiert als auch evaluiert wird. Wir zeigen dies, indem wir semantisch identische Varianten für MNIST bzw. CIFAR10 konstruieren und zeigen, dass standardmäßig trainierte Modelle ähnliche saubere Genauigkeiten erreichen, während adversarisch trainierte Modelle signifikant unterschiedliche Robustheitsgenauigkeiten erreichen.Dieses kontraintuitive Phänomen deutet darauf hin, dass die Verteilung der Eingabedaten allein die adversarische Robustheit trainierter neuronaler Netze beeinflussen kann, nicht notwendigerweise die Aufgaben selbst.Abschließend diskutieren wir die praktischen Auswirkungen auf die Bewertung der adversarischen Robustheit und unternehmen erste Versuche, dieses komplexe Phänomen zu verstehen.
Die Ineffizienz von Stichproben ist ein langwieriges Problem beim Reinforcement Learning (RL).  Der Stand der Technik verwendet Aktionswertfunktionen zur Ableitung von Strategien, was in der Regel eine umfangreiche Suche im Zustands-Aktions-Raum und eine instabile Optimierung erfordert. Für ein stichprobeneffizientes RL schlagen wir eine Rangordnungspolitik-Gradienten-Methode (RPG) vor, die den optimalen Rang einer Menge diskreter Aktionen erlernt.  Um das Lernen von Policy-Gradienten-Methoden zu beschleunigen, etablieren wir die Äquivalenz zwischen der Maximierung der unteren Schranke der Rendite und der Nachahmung einer nahezu optimalen Policy, ohne auf irgendwelche Orakel zuzugreifen.Diese Ergebnisse führen zu einem allgemeinen Off-Policy-Learning-Rahmen, der die Optimalität bewahrt, die Varianz reduziert und die Stichproben-Effizienz verbessert.Wir führen umfangreiche Experimente durch, die zeigen, dass RPG bei der Konsolidierung mit dem Off-Policy-Learning-Rahmen die Stichprobenkomplexität im Vergleich zum Stand der Technik erheblich reduziert.
Wir stellen MultiGrain vor, eine neuronale Netzwerkarchitektur, die kompakte Bildeinbettungsvektoren generiert, die mehrere Aufgaben unterschiedlicher Granularität lösen: Klassen-, Instanz- und Kopienerkennung.MultiGrain wird für die Klassifizierung durch Optimierung des Cross-Entropie-Verlusts und für die Instanz-/Kopienerkennung durch Optimierung eines selbstüberwachten Ranking-Verlusts gemeinsam trainiert.Der selbstüberwachte Verlust verwendet nur eine Datenerweiterung und erfordert daher keine zusätzlichen Kennzeichnungen.Bemerkenswerterweise sind die vereinheitlichten Einbettungen nicht nur viel kompakter als die Verwendung mehrerer spezialisierter Einbettungen, sondern sie haben auch die gleiche oder eine bessere Genauigkeit. Wenn ein linearer Klassifikator gefüttert wird, erreicht MultiGrain mit ResNet-50 79,4% Top-1-Genauigkeit auf ImageNet, eine +1,8% absolute Verbesserung gegenüber dem aktuellen Stand der Technik AutoAugment-Methode.Die gleichen Einbettungen auf gleichem Niveau mit dem Stand der Technik Instanz Retrieval mit Bildern von mäßiger Auflösung.Eine Ablation Studie zeigt, dass unser Ansatz profitiert von der Selbst-Überwachung, die Pooling-Methode und die Mini-Batches mit wiederholten Augmentierungen des gleichen Bildes.
 In diesem Papier untersuchen wir die Abbildung der Hyponymie-Relation von Wortordnungen auf Merkmalsvektoren.  Unser Ziel ist es, lexikalisches Wissen so zu modellieren, dass es als Input für generische Machine-Learning-Modelle, wie z.B. Phrase-Entagment-Prädiktoren, verwendet werden kann.  Wir schlagen zwei Modelle vor: Das erste nutzt eine bestehende Zuordnung von Wörtern zu Merkmalsvektoren (Fasttext) und versucht, solche Vektoren als innerhalb oder außerhalb der jeweiligen Klasse zu klassifizieren; das zweite Modell ist vollständig überwacht und verwendet ausschließlich die Wortliste als Grundlage.  Mit dem ersten Modell nähern wir uns dem Stand der Technik an, erreichen ihn aber nicht ganz, während das zweite Modell eine nahezu perfekte Genauigkeit erreicht.
Rekurrente neuronale Netze (RNNs) sind leistungsstarke autoregressive Sequenzmodelle zum Erlernen der in der natürlichen Sprache vorherrschenden Muster.   Die von RNNs erzeugte Sprache weist jedoch oft mehrere degenerierte Merkmale auf, die in der menschlichen Sprache ungewöhnlich sind; obwohl sie fließend ist, kann die RNN-Sprachproduktion übermäßig allgemein, repetitiv und sogar selbstwidersprüchlich sein.  Wir gehen davon aus, dass die von RNN-Sprachmodellen optimierte Zielfunktion, die auf die Gesamt-PerplexitÃ?t eines Textes hinauslÃ?uft, nicht aussagekrÃ?ftig genug ist, um die abstrakten QualitÃ?ten einer guten Generierung, wie z.B. Griceâ€™s Maxims, zu erfassen.â€œ In diesem Papier stellen wir einen allgemeinen Lernrahmen vor, der ein Dekodierungsziel konstruieren kann, das besser fÃ?r die Generierung geeignet ist.â€žAusgehend von einem generativ trainierten RNN-Sprachmodell lernt unser Rahmen, einen wesentlich stÃ?rkeren Generator zu konstruieren, indem er mehrere diskriminativ trainierte Modelle kombiniert, die gemeinsam die BeschrÃ?nkungen der RNN-Generierung angehen kÃ¶nnen.  Die Bewertung durch Menschen zeigt, dass der mit dem resultierenden Generator erzeugte Text mit großem Vorsprung vor dem der Basislinien bevorzugt wird und die Gesamtkohärenz, der Stil und der Informationsgehalt des erzeugten Textes erheblich verbessert werden.
In den letzten Jahren wurden die Effizienz und sogar die Durchführbarkeit traditioneller Lastausgleichsverfahren durch das rasche Wachstum der Cloud-Infrastruktur mit zunehmender Server-Heterogenität und zunehmender Größe der Cloud-Dienste und -Anwendungen in Frage gestellt. In solch vielen heterogenen Systemen mit Software-Lastausgleich verursachen traditionelle Lösungen wie JSQ einen zunehmenden Kommunikations-Overhead, während kommunikationsarme Alternativen wie JSQ(d) und das kürzlich vorgeschlagene JIQ-Schema entweder instabil sind oder eine schlechte Leistung bieten. Wir argumentieren, dass ein besseres kommunikationsarmes Lastausgleichsschema etabliert werden kann, indem jeder Dispatcher eine andere Sicht auf das System hat und JSQ verwendet, anstatt gierig zu versuchen, ein Aushungern auf einer pro-Entscheidungsbasis zu vermeiden. Wir etablieren formal die starke Stabilität jeder Loosely-Shortest-Queue-Politik und stellen eine einfach zu überprüfende hinreichende Bedingung zur Verfügung, um zu verifizieren, dass eine Politik Loosely-Shortest-Queue ist.Wir zeigen weiter, dass der Loosely-Shortest-Queue-Ansatz die Konstruktion von durchsatzoptimalen Politiken mit einem beliebig niedrigen Kommunikationsbudget ermöglicht. Schließlich zeigen wir anhand umfangreicher Simulationen, die homogene, heterogene und stark verzerrte heterogene Systeme in Szenarien mit einem einzigen Dispatcher sowie mit mehreren Dispatchern berücksichtigen, dass die untersuchten Loosely-Shortest-Queue-Beispielpolitiken immer stabil sind, wie es die Theorie vorschreibt, und dass sie eine ansprechende Leistung aufweisen und bekannte kommunikationsarme Politiken wie JSQ(d) und JIQ deutlich übertreffen, während sie ein ähnliches Kommunikationsbudget verwenden.
Wir schlagen ein neuartiges quantitatives Maß zur Vorhersage der Leistung eines Klassifizierers für tiefe neuronale Netze vor, wobei das Maß ausschließlich aus der Graphenstruktur des Netzes abgeleitet wird, und gehen davon aus, dass dieses Maß ein grundlegender erster Schritt bei der Entwicklung einer Methode zur Bewertung neuer Netzarchitekturen ist und die Abhängigkeit von den rechenintensiven Trial-and-Error- oder Brute-Force-Optimierungsprozessen bei der Modellauswahl verringert. Das Maß wurde im Zusammenhang mit mehrschichtigen Perzeptronen (MLPs) abgeleitet, aber die Definitionen erweisen sich auch im Zusammenhang mit tiefen neuronalen Faltungsnetzen (CNN) als nützlich, wo es in der Lage ist, die relative Leistung verschiedener Arten von neuronalen Netzen, wie VGG, ResNet und DenseNet, zu schätzen und zu vergleichen. Unser Maß wird auch verwendet, um die Auswirkungen einiger wichtiger "versteckter" Hyperparameter der DenseNet-Architektur zu untersuchen, wie z. B. die Anzahl der Schichten, die Wachstumsrate und die Dimension der 1x1-Faltung in DenseNet-BC. Letztendlich erleichtert unser Maß die Optimierung des DenseNet-Designs, das im Vergleich zur Basislinie bessere Ergebnisse zeigt.
Es besteht eine große Diskrepanz zwischen den Lernraten, die in der Praxis des maschinellen Lernens in großem Maßstab verwendet werden, und dem, was in der Theorie der stochastischen Approximation als zulässige Lernraten angesehen wird, was durch neuere Ergebnisse, wie z. B. die "Superkonvergenz"-Methoden, die oszillierende Lernraten verwenden, noch mehr betont wird. Eine plausible Erklärung ist, dass nicht-konvexe Trainingsverfahren für neuronale Netze besser geeignet sind, um grundlegend andere Lernratenschemata zu verwenden, wie z.B. die Methode ``Schneiden Sie die Lernrate jede konstante Anzahl von Epochen'' (die eher einem exponentiell abklingenden Lernratenschema ähnelt); man beachte, dass dieses weit verbreitete Schema in krassem Gegensatz zu den polynomialen Abklingschemata steht, die in der stochastischen Approximationsliteratur vorgeschrieben sind, die sich in der Tat als (im schlimmsten Fall) optimal für Klassen von konvexen Optimierungsproblemen erweisen. Der Hauptbeitrag dieser Arbeit zeigt, dass das Bild weitaus nuancierter ist, wobei wir nicht einmal zu nicht-konvexer Optimierung übergehen müssen, um zu zeigen, dass andere Lernratenverfahren weitaus effektiver sein können. Tatsächlich ist selbst für den einfachen Fall der stochastischen linearen Regression mit festem Zeithorizont die durch ein beliebiges polynomiales Zerfallsschema erreichte Rate im Vergleich zur statistischen Minimax-Rate suboptimal (um einen Faktor der Bedingungszahl); im Gegensatz dazu bietet das ``'cut the learning rate every constant number of epochs'' eine exponentielle Verbesserung (die nur logarithmisch von der Bedingungszahl abhängt) im Vergleich zu jedem polynomialen Zerfallsschema.  Schließlich ist es wichtig zu fragen, ob unsere theoretischen Einsichten irgendwie grundlegend an die quadratische Verlustminimierung gebunden sind (wo wir Minimax-Untergrenzen für allgemeinere konvexe Optimierungsprobleme umgangen haben)... Hier vermuten wir, dass neuere Ergebnisse, die die Gradientennorm mit einer nahezu optimalen Rate klein machen, sowohl für konvexe als auch für nicht-konvexe Optimierung, auch weitere Einsichten in die in der Praxis verwendeten Lernratenschemata liefern können.
Wir stellen Value Propagation (VProp) vor, einen Satz parametereffizienter differenzierbarer Planungsmodule, die auf Value Iteration aufbauen. Diese Module können mit Hilfe von Reinforcement Learning erfolgreich trainiert werden, um ungesehene Aufgaben zu lösen, können auf größere Kartengrößen verallgemeinert werden und können lernen, in dynamischen Umgebungen zu navigieren. Wir zeigen, dass die Module das Planen lernen ermöglichen, wenn die Umgebung auch stochastische Elemente enthält, und stellen damit ein kosteneffizientes Lernsystem zur Verfügung, um größeninvariante Low-Level-Planer für eine Vielzahl von interaktiven Navigationsproblemen zu erstellen. Wir evaluieren auf statischen und dynamischen Konfigurationen von MazeBase-Gitterwelten mit zufällig generierten Umgebungen verschiedener Größen und auf einem StarCraft-Navigationsszenario mit komplexerer Dynamik und Pixeln als Eingabe.
Das Erlernen von qualitativ hochwertigen Worteinbettungen ist von großer Bedeutung, um eine bessere Leistung bei vielen nachgelagerten Lernaufgaben zu erzielen: Einerseits werden traditionelle Worteinbettungen auf einem großen Korpus für allgemeine Aufgaben trainiert, die für viele domänenspezifische Aufgaben oft suboptimal sind. Wir stellen fest, dass Domänen nicht isoliert sind und dass ein kleiner Domänenkorpus das gelernte Wissen aus vielen früheren Domänen nutzen kann, um diesen Korpus zu erweitern und qualitativ hochwertige Einbettungen zu erzeugen.In dieser Arbeit formulieren wir das Lernen von Worteinbettungen als einen lebenslangen Lernprozess. Mit Hilfe eines einfachen, aber effektiven Algorithmus und eines Meta-Learners, der in der Lage ist, Wortkontext-Ähnlichkeitsinformationen auf Domänenebene bereitzustellen, kann die vorgeschlagene Methode effektiv neue Domäneneinbettungen generieren, wenn Wissen aus vielen früheren Domänen und ein kleiner neuer Domänenkorpus vorhanden sind. Experimentelle Ergebnisse zeigen, dass die vorgeschlagene Methode effektiv neue Domäneneinbettungen aus einem kleinen Korpus und früheren Domänenkenntnissen lernen kann. Wir zeigen auch, dass allgemeine Einbettungen, die aus einem großen Korpus trainiert wurden, für domänenspezifische Aufgaben suboptimal sind.
Parameter Pruning ist ein vielversprechender Ansatz zur Komprimierung und Beschleunigung von CNNs, indem redundante Modellparameter mit tolerierbaren Leistungseinbußen eliminiert werden. Trotz ihrer Effektivität führen bestehende regularisierungsbasierte Parameter Pruning-Methoden die Gewichte in der Regel mit großen und konstanten Regularisierungsfaktoren gegen Null, was die Tatsache vernachlässigt, dass die Ausdruckskraft von CNNs fragil ist und eine sanftere Art der Regularisierung benötigt, damit sich die Netzwerke während des Pruning anpassen können. Um dieses Problem zu lösen, schlagen wir eine neue regularisierungsbasierte Pruning-Methode (IncReg genannt) vor, die verschiedenen Gewichtsgruppen auf der Grundlage ihrer relativen Wichtigkeit inkrementell unterschiedliche Regularisierungsfaktoren zuweist, deren Effektivität auf populären CNNs im Vergleich zu State-of-the-Art-Methoden nachgewiesen wurde.
Momentumbasierte stochastische Gradientenmethoden wie Heavy Ball (HB) und Nesterovs Accelerated Gradient Descent (NAG) werden in der Praxis häufig für das Training von tiefen Netzen und anderen überwachten Lernmodellen verwendet, da sie oft signifikante Verbesserungen gegenüber dem stochastischen Gradientenabstieg (SGD) bieten.Streng genommen haben schnelle Gradientenmethoden nachweisbare Verbesserungen gegenüber dem Gradientenabstieg nur für den deterministischen Fall, wo die Gradienten exakt sind. Im stochastischen Fall ist die populäre Erklärung für ihre breite Anwendbarkeit, dass diese schnellen Gradientenmethoden, wenn sie im stochastischen Fall angewandt werden, teilweise ihre exakten Gradienten-Gegenstücke nachahmen, was zu einem gewissen praktischen Gewinn führt.Diese Arbeit bietet einen Kontrapunkt zu dieser Überzeugung, indem sie beweist, dass es einfache Problemfälle gibt, in denen diese Methoden SGD trotz der besten Einstellung ihrer Parameter nicht übertreffen können. Diese Ergebnisse deuten darauf hin (zusammen mit empirischen Beweisen), dass die praktischen Leistungsgewinne von HB oder NAG ein Nebenprodukt des Minibatching sind. Darüber hinaus bietet diese Arbeit eine praktikable (und nachweisbare) Alternative, die bei der gleichen Menge von Problemfällen die Leistung von HB, NAG und SGD deutlich verbessert. Dieser Algorithmus, Accelerated Stochastic Gradient Descent (ASGD) genannt, ist ein einfach zu implementierender stochastischer Algorithmus, der auf einer relativ weniger populären Variante der Nesterov-Beschleunigung basiert. Umfangreiche empirische Ergebnisse in dieser Arbeit zeigen, dass ASGD Leistungsgewinne gegenüber HB, NAG und SGD hat.
Bei der Überbelegungsplanung (OSP) geht es darum, Pläne zu finden, die den Nutzwert ihres Endzustands maximieren und gleichzeitig innerhalb einer bestimmten Kostengrenze bleiben.  Wir machen uns diese Umformulierung zunutze, um zu zeigen, dass OSP-Probleme mit dem A*-Suchalgorithmus optimal gelöst werden können, im Gegensatz zu früheren Ansätzen, die Abwandlungen der Branch-and-Bound-Suche verwendet haben, wodurch viele leistungsstarke Techniken, die für die klassische Planung entwickelt wurden, auf OSP-Probleme angewendet werden können. Wir führen auch neuartige bound-sensitive Heuristiken ein, die in der Lage sind, über die primären Kosten einer Lösung nachzudenken und dabei sekundäre Kostenfunktionen und Grenzen zu berücksichtigen, um im Vergleich zu Heuristiken, die diese Grenzen nicht berücksichtigen, eine bessere Orientierung zu bieten.Wir implementieren zwei solcher bound-sensitiven Varianten bestehender klassischer Planungsheuristiken und zeigen experimentell, dass die resultierende Suche wesentlich informierter ist als vergleichbare Heuristiken, die keine Grenzen berücksichtigen.
Bisherige Arbeiten über gegnerisch robuste neuronale Netze erfordern große Trainingsmengen und rechenintensive Trainingsverfahren.  Andererseits sind "few-shot"-Lernmethoden sehr anfällig für ungünstige Beispiele.  Das Ziel unserer Arbeit ist es, Netze zu entwickeln, die sowohl gute Leistungen bei "few-shot"-Aufgaben erbringen als auch gleichzeitig robust gegenüber gegnerischen Beispielen sind.  Wir adaptieren adversariales Training für das Meta-Lernen, wir passen robuste architektonische Merkmale an kleine Netzwerke für das Meta-Lernen an, wir testen Pre-Processing-Verteidigungen als Alternative zu adversarialem Training für das Meta-Lernen, und wir untersuchen die Vorteile von robustem Meta-Lernen gegenüber robustem Transfer-Lernen für "few-shot" Aufgaben.  Diese Arbeit bietet eine gründliche Analyse von widerstandsfähigen Methoden im Kontext des Meta-Lernens, und wir legen den Grundstein für künftige Arbeiten zu Verteidigungsmaßnahmen für Aufgaben mit wenigen Schüssen.
Viele unserer Kernannahmen über die Funktionsweise neuronaler Netze bleiben empirisch ungetestet: Eine gängige Annahme ist, dass neuronale Faltungsnetze stabil gegenüber kleinen Translationen und Deformationen sein müssen, um Bilderkennungsaufgaben zu lösen: Sind unsere Intuitionen in Bezug auf die Deformationsstabilität überhaupt richtig?Ist sie wichtig?Ist Pooling für die Deformationsinvarianz notwendig?Wenn nicht, wie wird die Deformationsinvarianz ohne Pooling erreicht?In dieser Arbeit testen wir diese Fragen rigoros und stellen fest, dass die Deformationsstabilität in Faltungsnetzen nuancierter ist, als es zunächst scheint: (1) Deformationsinvarianz ist keine binäre Eigenschaft, sondern verschiedene Aufgaben erfordern unterschiedliche Grade an Deformationsstabilität auf verschiedenen Schichten.(2) Deformationsstabilität ist keine feste Eigenschaft eines Netzes und wird im Laufe des Trainings stark angepasst, hauptsächlich durch die Glätte der Faltungsfilter.(3) Verschachtelte Pooling-Schichten sind weder notwendig noch ausreichend, um die optimale Form der Deformationsstabilität für die Klassifikation natürlicher Bilder zu erreichen. (4) Pooling verleiht bei der Initialisierung eine zu hohe Deformationsstabilität für die Bildklassifizierung, und während des Trainings müssen die Netzwerke lernen, dieser induktiven Verzerrung entgegenzuwirken. Zusammengenommen bieten diese Ergebnisse neue Einblicke in die Rolle des verschachtelten Poolings und der Deformationsinvarianz in CNNs und zeigen, wie wichtig es ist, selbst unsere grundlegendsten Annahmen über die Funktionsweise neuronaler Netze rigoros empirisch zu testen.
Um dieses Problem zu überwinden, stellen wir eine einfache und effektive Methode vor: Self-Ensemble Label Filtering (SELF), um schrittweise die falschen Etiketten während des Trainings herauszufiltern. Unsere Methode verbessert die Leistung der Aufgabe, indem sie schrittweise die Überwachung nur von den potenziell nicht verrauschten (sauberen) Etiketten zulässt und das Lernen auf den gefilterten verrauschten Etiketten stoppt.Für die Filterung bilden wir laufende Durchschnitte von Vorhersagen über den gesamten Trainingsdatensatz unter Verwendung der Netzwerkausgabe in verschiedenen Trainingsepochen. Wir zeigen, dass diese Ensemble-Schätzungen eine genauere Identifizierung von inkonsistenten Vorhersagen während des gesamten Trainings liefern als die einzelnen Schätzungen des Netzwerks in der letzten Trainingsepoche.Während gefilterte Proben vollständig aus dem überwachten Trainingsverlust entfernt werden, nutzen wir sie dynamisch über halbüberwachtes Lernen im unbeaufsichtigten Verlust. Wir demonstrieren den positiven Effekt eines solchen Ansatzes auf verschiedene Bildklassifizierungsaufgaben unter symmetrischem und asymmetrischem Label-Rauschen und bei unterschiedlichen Rauschverhältnissen. Er übertrifft alle bisherigen Arbeiten zum rauschbewussten Lernen über verschiedene Datensätze hinweg und kann auf eine Vielzahl von Netzwerkarchitekturen angewendet werden.
Lange Trainingszeiten von tiefen neuronalen Netzen sind ein Engpass in der Forschung zum maschinellen Lernen. Das Haupthindernis für ein schnelles Training ist das quadratische Wachstum der Speicher- und Rechenanforderungen von dichten und faltigen Schichten in Bezug auf ihre Informationsbandbreite. Kürzlich wurde vorgeschlagen, spärliche Netze a priori zu trainieren, um den Schichten eine hohe Informationsbandbreite zu ermöglichen und gleichzeitig den Speicher- und Rechenbedarf niedrig zu halten.Allerdings ist die Wahl der spärlichen Topologie, die in diesen Netzen verwendet werden sollte, unklar.In dieser Arbeit liefern wir eine theoretische Grundlage für die Wahl der schichtinternen Topologie. Um diese Unterschiede zu erklären, entwickeln wir eine datenfreie Heuristik, die eine Topologie unabhängig vom Datensatz, auf dem das Netzwerk trainiert wird, bewerten kann. Anschließend leiten wir eine Reihe von Anforderungen an eine gute Topologie ab und kommen zu einer einzigen Topologie, die alle diese Anforderungen erfüllt.
Die Erkundung des Modellentwurfsraums wird oft von einem menschlichen Experten durchgeführt und mit einer Kombination aus Gittersuche und Suchheuristiken über einen großen Raum möglicher Optionen optimiert. neuronale Architektursuche (NAS) ist ein Ansatz des Verstärkungslernens, der vorgeschlagen wurde, um den Architekturentwurf zu automatisieren. NAS wurde erfolgreich angewandt, um neuronale Netze zu generieren, die mit den besten von Menschen entworfenen Architekturen konkurrieren können. Allerdings erfordert NAS das Sampling, die Konstruktion und das Training von Hunderten bis Tausenden von Modellen, um gut funktionierende Architekturen zu erhalten. Dieses Verfahren muss für jede neue Aufgabe von Grund auf neu ausgeführt werden.Bei der Anwendung von NAS auf eine große Anzahl von Aufgaben fehlt derzeit eine Möglichkeit, verallgemeinerbares Wissen über Aufgaben hinweg zu übertragen. Unser Ziel ist es, einen verallgemeinerbaren Rahmen zu erlernen, der die Modellkonstruktion auf erfolgreiche Modellsuchen für zuvor gesehene Aufgaben konditioniert und so die Suche nach neuen Aufgaben erheblich beschleunigt.Wir zeigen, dass MNMS eine automatisierte Architektursuche für mehrere Aufgaben gleichzeitig durchführen kann, während es weiterhin gut funktionierende, spezialisierte Modelle für jede Aufgabe lernt. Durch die Nutzung von Wissen aus früheren Suchvorgängen stellen wir fest, dass vortrainierte MNMS-Modelle von einer besseren Position im Suchraum aus starten und die Suchzeit für unbekannte Aufgaben reduzieren, während sie immer noch Modelle entdecken, die besser sind als die veröffentlichten, von Menschen entworfenen Modelle.
Diese Arbeit untersucht das Problem der Modellierung nichtlinearer visueller Prozesse durch die Nutzung tiefer generativer Architekturen für das Lernen linearer, Gaußscher Modelle beobachteter Sequenzen.Wir schlagen einen gemeinsamen Lernrahmen vor, der ein multivariates autoregressives Modell und tiefe generative Faltungsnetzwerke kombiniert. Nach der Rechtfertigung der theoretischen Annahmen der Inearisierung schlagen wir eine Architektur vor, die es Variational Autoencodern und Generative Adversarial Networks ermöglicht, gleichzeitig die nichtlineare Beobachtung sowie das lineare Zustandsübergangsmodell aus einer Sequenz beobachteter Frames zu lernen, und demonstrieren unseren Ansatz an konzeptionellen Spielzeugbeispielen und dynamischen Texturen.
Partielle Differentialgleichungen (PDEs) spielen eine wichtige Rolle in vielen Disziplinen wie angewandte Mathematik, Physik, Chemie, Materialwissenschaften, Informatik, etc.PDEs sind in der Regel auf der Grundlage von physikalischen Gesetzen oder empirischen Beobachtungen abgeleitet.Allerdings sind die Gleichungen für viele komplexe Systeme in modernen Anwendungen noch nicht vollständig bekannt.Mit der rasanten Entwicklung von Sensoren, Rechenleistung und Datenspeicherung in den letzten zehn Jahren können riesige Mengen von Daten leicht gesammelt und effizient gespeichert werden. Inspiriert durch die neueste Entwicklung von neuronalen Netzwerken im Bereich des Deep Learning, schlagen wir ein neues Feed-Forward Deep Network, genannt PDE-Net, vor, um zwei Ziele gleichzeitig zu erreichen: die genaue Vorhersage der Dynamik komplexer Systeme und die Aufdeckung der zugrundeliegenden versteckten PDE-Modelle.Die Grundidee des vorgeschlagenen PDE-Net ist das Erlernen von Differentialoperatoren durch das Erlernen von Faltungskernen (Filtern) und die Anwendung von neuronalen Netzwerken oder anderen Methoden des maschinellen Lernens, um die unbekannten nichtlinearen Antworten zu approximieren. Im Vergleich zu bestehenden Ansätzen, die entweder davon ausgehen, dass die Form der nichtlinearen Antwort bekannt ist oder bestimmte Finite-Differenzen-Approximationen von Differentialoperatoren festlegen, bietet unser Ansatz die größte Flexibilität, da er sowohl Differentialoperatoren als auch die nichtlinearen Antworten erlernt.Ein besonderes Merkmal des vorgeschlagenen PDE-Netzes ist, dass alle Filter ordnungsgemäß eingeschränkt sind, was es uns ermöglicht, die maßgeblichen PDE-Modelle leicht zu identifizieren und gleichzeitig die Ausdrucks- und Vorhersagekraft des Netzes zu erhalten. Wir diskutieren auch die Beziehungen des PDE-Netzes mit einigen bestehenden Netzwerken in der Computer Vision wie Network-In-Network (NIN) und Residual Neural Network (ResNet).Numerische Experimente zeigen, dass das PDE-Netz das Potenzial hat, die versteckte PDE der beobachteten Dynamik aufzudecken und das dynamische Verhalten für eine relativ lange Zeit vorherzusagen, sogar in einer verrauschten Umgebung.
Jeder Trainingsschritt für einen Variations-Autoencoder (VAE) erfordert eine Stichprobe aus dem approximativen Posterior. Daher wählen wir in der Regel einfache (z. B. faktorisierte) approximative Posterioren, bei denen die Stichprobe eine effiziente Berechnung ist, die die GPU-Parallelität vollständig ausnutzt.  Solche einfachen approximativen Posterioren sind jedoch oft unzureichend, da sie statistische Abhängigkeiten im Posterior eliminieren.  Der natürlichste Ansatz zur Modellierung diskreter Abhängigkeiten ist eine autoregressive Verteilung, aber das Sampling aus solchen Verteilungen ist von Natur aus sequenziell und daher langsam.  Wir entwickeln ein schnelles, paralleles Stichprobenverfahren für autoregressive Verteilungen, das auf Festkomma-Iterationen basiert und eine effiziente und genaue Variationsinferenz in diskreten Zustandsraummodellen ermöglicht.  Um die Variationsschranke zu optimieren, haben wir zwei Möglichkeiten zur Bewertung von Wahrscheinlichkeiten in Betracht gezogen: Einfügen der entspannten Stichproben direkt in die pmf für die diskrete Verteilung oder Umwandlung in kontinuierliche logistische latente Variablen und Interpretation der K-Schritt-Fixpunkt-Iterationen als Normalisierungsfluss.  Wir fanden heraus, dass die Konvertierung in kontinuierliche latente Variablen einen beträchtlichen zusätzlichen Spielraum für Unstimmigkeiten zwischen den wahren und den approximativen Posterioren bietet, was zu verzerrten Schlussfolgerungen führt.  Wir haben unseren Ansatz an einem neurowissenschaftlichen Problem getestet, bei dem es darum ging, aus verrauschten Kalzium-Bildgebungsdaten auf diskrete Spiking-Aktivitäten zu schließen, und stellten fest, dass er in einer um eine Größenordnung kürzeren Zeit genaue Konnektivitätsschätzungen liefert.
Tiefe neuronale Netze (DNNs) hatten großen Erfolg bei NLP-Aufgaben wie Sprachmodellierung, maschineller Übersetzung und bestimmten Aufgaben zur Beantwortung von Fragen (QA), jedoch ist der Erfolg bei wissensintensiveren Aufgaben wie QA aus einem großen Korpus begrenzt.Bestehende durchgängige tiefe QA-Modelle (Miller et al., 2016; Weston et al, (Miller et al., 2016; Weston et al., 2014) müssen den gesamten Text lesen, nachdem sie die Frage beobachtet haben, und daher ist ihre Komplexität bei der Beantwortung einer Frage linear zur Textgröße.Dies ist für praktische Aufgaben wie QA aus Wikipedia, einem Roman oder dem Web unerschwinglich.Wir schlagen vor, dieses Skalierbarkeitsproblem zu lösen, indem wir symbolische Bedeutungsrepräsentationen verwenden, die mit einer Komplexität, die unabhängig von der Textgröße ist, effizient indiziert und abgerufen werden können. Genauer gesagt verwenden wir Sequenz-zu-Sequenz-Modelle, um Wissen symbolisch zu kodieren und Programme zu generieren, die Fragen aus dem kodierten Wissen beantworten.Wir wenden unseren Ansatz, genannt N-Gram Machine (NGM), auf die bAbI-Aufgaben (Weston et al., Unsere Experimente zeigen, dass die NGM diese beiden Aufgaben erfolgreich, genau und effizient lÃ¶sen kann. Im Gegensatz zu voll differenzierbaren GedÃ?chtnismodellen werden die ZeitkomplexitÃ?t und die BeantwortungsqualitÃ?t der NGM nicht durch die LÃ?nge der Geschichten beeinflusst. Das gesamte System von NGM wird mit REINFORCE (Williams, 1992) trainiert. Um eine hohe Varianz in der Gradientenschätzung zu vermeiden, die typisch für diskrete latente Variablenmodelle ist, verwenden wir Balkensuche anstelle von Sampling. Um den exponentiell großen Suchraum zu bewältigen, verwenden wir ein stabilisiertes Auto-Encoding-Ziel und ein Verfahren zur Strukturverfeinerung, um den Suchraum iterativ zu reduzieren und zu verfeinern.
Wir schlagen vor, ein Meta-Lernziel zu verwenden, das die Geschwindigkeit der Übertragung auf eine modifizierte Verteilung maximiert, um zu lernen, wie das erworbene Wissen modularisiert werden kann, und konzentrieren uns insbesondere auf die Frage, wie eine gemeinsame Verteilung in geeignete Konditionale zerlegt werden kann, die mit den Kausalrichtungen übereinstimmen. Wir erklären, wann dies funktionieren kann, indem wir die Annahme verwenden, dass die Änderungen in den Verteilungen lokalisiert sind (z.B. auf eines der Marginalen, z.B. aufgrund eines Eingriffs in eine der Variablen) und beweisen, dass unter dieser Annahme von lokalisierten Änderungen in kausalen Mechanismen der korrekte kausale Graph dazu neigt, nur einige seiner Parameter mit einem Gradienten ungleich Null zu haben, d.h. die angepasst werden müssen. Wir argumentieren und beobachten experimentell, dass dies zu einer schnelleren Anpassung führt, und nutzen diese Eigenschaft, um einen Meta-Lern-Surrogat-Score zu definieren, der zusätzlich zu einer kontinuierlichen Parametrisierung von Graphen korrekte Kausalgraphen begünstigt. Schließlich betrachten wir, motiviert durch die Sichtweise von KI-Agenten (z. B. eines Roboters, der seine Umgebung autonom erkundet), wie dasselbe Ziel die kausalen Variablen selbst entdecken kann, als eine Transformation von beobachteten Variablen auf niedriger Ebene ohne kausale Bedeutung.Experimente im Fall von zwei Variablen validieren die vorgeschlagenen Ideen und theoretischen Ergebnisse.
Kontinuierliches Lernen ist ein langjähriges Ziel der künstlichen Intelligenz, wird aber oft durch katastrophales Vergessen behindert, das neuronale Netze daran hindert, Aufgaben sequentiell zu lernen.Frühere Methoden des kontinuierlichen Lernens haben gezeigt, wie man das katastrophale Vergessen abmildern und neue Aufgaben lernen kann, während die Leistung der vorherigen Aufgaben erhalten bleibt. Wir analysieren das katastrophale Vergessen aus der Perspektive der Veränderung der Klassifizierungswahrscheinlichkeit und schlagen ein einfaches L1-Minimierungskriterium vor, das an verschiedene Anwendungsfälle angepasst werden kann.Wir untersuchen außerdem zwei Möglichkeiten, das Vergessen zu minimieren, wie es durch dieses Kriterium quantifiziert wird, und schlagen Strategien vor, um eine feinere Kontrolle über das Vergessen zu erreichen.Schließlich evaluieren wir unsere Strategien an drei Datensätzen mit unterschiedlichem Schwierigkeitsgrad und zeigen Verbesserungen gegenüber bisher bekannten L2-Strategien zur Abschwächung des katastrophalen Vergessens.
Wir schlagen einen Ansatz zur Konstruktion realistischer 3D-Gesichtsmorphologiemodelle (3DMM) vor, der einen intuitiven Arbeitsablauf für die Bearbeitung von Gesichtsattributen ermöglicht.Aktuelle Gesichtsmodellierungsmethoden, die 3DMM verwenden, leiden unter dem Mangel an lokaler Kontrolle.Wir erstellen daher ein 3DMM, indem wir lokale teilbasierte 3DMM für die Augen, die Nase, den Mund, die Ohren und die Gesichtsmaskenregionen kombinieren. Unser lokaler PCA-basierter Ansatz verwendet eine neuartige Methode zur Auswahl der besten Eigenvektoren aus der lokalen 3DMM, um sicherzustellen, dass die kombinierte 3DMM aussagekräftig ist und gleichzeitig eine genaue Rekonstruktion ermöglicht.Die Bearbeitungssteuerungen, die wir dem Benutzer zur Verfügung stellen, sind intuitiv, da sie aus anthropometrischen Messungen in der Literatur stammen. Aus einer großen Menge möglicher anthropometrischer Messungen filtern wir diejenigen heraus, die angesichts des Gesichtsdatensatzes eine sinnvolle generative Kraft haben. Wir binden die Messungen durch Mapping-Matrizen, die aus unserem Datensatz von Gesichtsscans abgeleitet sind, an die teilbasierte 3DMM. Unser teilbasiertes 3DMM ist kompakt und dennoch genau, und im Vergleich zu anderen 3DMM-Methoden bietet es einen neuen Kompromiss zwischen lokaler und globaler Kontrolle.Wir haben unseren Ansatz an einem Datensatz von 135 Scans getestet, die zur Ableitung des 3DMM verwendet wurden, sowie an 19 Scans, die zur Validierung dienten.Die Ergebnisse zeigen, dass unser teilbasiertes 3DMM hervorragende generative Eigenschaften hat und dem Benutzer eine intuitive lokale Kontrolle ermöglicht.
Wir überprüfen acht Klassifizierungsalgorithmen des maschinellen Lernens zur Analyse von elektroenzephalographischen (EEG) Signalen, um EEG-Muster zu unterscheiden, die mit fünf grundlegenden Bildungsaufgaben verbunden sind. Während frühere EEG-Experimente mehrere Klassifikatoren in denselben Experimenten verwendeten oder verschiedene Algorithmen auf Datensätzen aus verschiedenen Experimenten prüften, konzentriert sich unser Ansatz auf die Prüfung von acht Klassifikator-Kategorien auf demselben Datensatz, einschließlich linearer Klassifikatoren, nichtlinearer Bayes'scher Klassifikatoren, nächstgelegener Nachbar-Klassifikatoren, Ensemble-Methoden, adaptiver Klassifikatoren, Tensor-Klassifikatoren, Transfer-Lernen und Deep Learning.Außerdem beabsichtigen wir, einen Ansatz zu finden, der problemlos auf den aktuellen Mainstream-Personalcomputern und Smartphones laufen kann.  Die empirische Evaluierung hat gezeigt, dass Random Forest und LSTM (Long Short-Term Memory) anderen Ansätzen überlegen sind.Wir haben einen Datensatz verwendet, bei dem Benutzer fünf häufig ausgeführte lernbezogene Aufgaben, einschließlich Lesen, Schreiben und Tippen, durchgeführt haben.Die Ergebnisse haben gezeigt, dass diese beiden besten Algorithmen verschiedene Benutzer mit einem Genauigkeitszuwachs von 5 % bis 9 % korrekt klassifizieren konnten, wobei jede Aufgabe unabhängig verwendet wurde. Diese Arbeit legt nahe, dass Random Forest ein empfohlener Ansatz (schnell und genau) für aktuelle Mainstream-Hardware sein könnte, während LSTM das Potenzial hat, die erste Wahl zu sein, wenn die Mainstream-Computer und Smartphones mehr Daten in kürzerer Zeit verarbeiten können.
In diesem Beitrag untersuchen wir die Entstehung von Kommunikation in der Verhandlungsumgebung, einem semi-kooperativen Modell der Agenteninteraktion. Wir führen zwei Kommunikationsprotokolle ein - eines, das in der Semantik des Spiels begründet ist, und eines, das a priori nicht begründet ist.  Wir zeigen, dass eigennützige Agenten den geerdeten Kommunikationskanal nutzen können, um fair zu verhandeln, aber nicht in der Lage sind, den ungeerdeten, billigen Gesprächskanal effektiv zu nutzen, um dasselbe zu tun.  Wir untersuchen auch das Kommunikationsverhalten in einer Umgebung, in der ein Agent mit Agenten in einer Gemeinschaft mit unterschiedlichem Grad an Prosozialität interagiert, und zeigen, wie die Identifizierbarkeit von Agenten die Verhandlung unterstützen kann.
Das Ziel von few-shot learning ist es, einen Klassifikator zu erlernen, der auch bei einer begrenzten Anzahl von Trainingsinstanzen pro Klasse gut verallgemeinert. Die kürzlich eingeführten meta-learning Ansätze gehen dieses Problem an, indem sie einen generischen Klassifikator über eine große Anzahl von Multiklassen-Klassifikationsaufgaben erlernen und das Modell auf eine neue Aufgabe verallgemeinern. In diesem Papier schlagen wir Transductive Propagation Network (TPN) vor, ein neuartiges Meta-Learning-Framework für transduktive Inferenz, das den gesamten Testsatz auf einmal klassifiziert, um das Problem der geringen Datenmenge zu lindern.Insbesondere schlagen wir vor, zu lernen, Etiketten von beschrifteten Instanzen auf unbeschriftete Testinstanzen zu übertragen, indem wir ein Graphenkonstruktionsmodul lernen, das die vielfältige Struktur in den Daten ausnutzt.TPN lernt sowohl die Parameter der Merkmalseinbettung als auch die Graphenkonstruktion in einer End-to-End-Weise.  Wir validieren TPN auf mehreren Benchmark-Datensätzen, auf denen es die bestehenden "few-shot"-Lernansätze weitgehend übertrifft und die besten Ergebnisse erzielt.
Wir beschreiben den Einsatz eines automatisierten Planungssystems für den Entwurf von Beobachtungsrichtlinien und für die Planung des Betriebs des ECOSystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) der NASA (National Aeronautics and Space Administration). Wir beschreiben die Anpassung des Planungssystems Compressed Large-scale Activity Scheduler and Planner (CLASP) an das ECOSTRESS-Planungsproblem, wobei wir mehrere Anwendungsfälle für die automatisierte Planung und mehrere Herausforderungen für die Planungstechnologie hervorheben: die Handhabung langfristiger Kampagnen mit sich ändernden Informationen, die Herausforderungen des Massenspeicher-Ringpuffers und die Ungewissheit des Orbits.Das beschriebene Planungssystem wurde für den Betrieb des ECOSTRESS-Instruments seit dem nominalen Betriebsbeginn im Juli 2018 verwendet und wird voraussichtlich bis zum Ende der Mission im Sommer 2019 betrieben.
Adversarial examples are modified samples that preserve original image structures but deviate classifiers.Researchers have put efforts in developing methods for generating adversarial examples and finding out origins.Past research put much attention on decision boundary changes caused by these methods.This paper, in contrast, discusses the origin of adversarial examples from a more underlying knowledge representation point of view. Während Menschen sowohl Prototypen als auch Transformationen von Objekten lernen und klassifizieren können, speichern neuronale Netze das gelernte Wissen auf eine eher hybride Art und Weise, indem sie alle Prototypen und Transformationen zu einer Gesamtverteilung zusammenfassen.Die hybride Speicherung kann zu geringeren Abständen zwischen verschiedenen Klassen führen, so dass kleine Änderungen den Klassifikator in die Irre führen können. Experimente zeigen, dass die einfache Imitation von Verteilungen aus einem Trainingsset ohne Kenntnis des Klassifizierers immer noch zu offensichtlichen Auswirkungen auf die Klassifizierungsergebnisse von tiefen Netzwerken führen kann, was auch bedeutet, dass negative Beispiele in mehr Formen als nur kleine Störungen auftreten können. Der erste Weg besteht darin, die Kodierung der Daten zu ändern, die an den Trainingsschritt gesendet werden.Trainingsdaten, die prototypischer sind, können helfen, robusteres und genaueres strukturelles Wissen zu erfassen.Der zweite Weg erfordert die Konstruktion von Lernrahmen mit verbesserten Repräsentationen.
Anders als das beliebte Deep Q-Network (DQN) Lernen, Alternating Q-learning (AltQ) passt nicht vollständig eine Ziel-Q-Funktion bei jeder Iteration, und ist allgemein bekannt, instabil und ineffizient.Limited Anwendungen von AltQ meist auf wesentliche Änderung der Algorithmus-Architektur, um seine Leistung zu verbessern.Obwohl Adam scheint eine natürliche Lösung, seine Leistung in AltQ wurde selten zuvor untersucht.In diesem Papier, bieten wir zunächst eine solide Exploration auf, wie gut AltQ führt mit Adam. Die vorgeschlagenen Algorithmen werden an einer Reihe von Atari 2600-Spielen getestet und zeigen eine bessere Leistung als die DQN-Lernmethode. Die Konvergenzrate der leicht modifizierten Version der vorgeschlagenen Algorithmen wird durch die lineare Funktionsannäherung charakterisiert.
Auf der Suche nach genaueren Vorhersagemodellen passen wir Kapselnetzwerke für das Problem des Diagnoselernens an.Wir schlagen auch spektrale Kapselnetzwerke vor, eine neue Variante von Kapselnetzwerken, die schneller konvergieren als Kapselnetzwerke mit EM-Routing.Spektrale Kapselnetzwerke bestehen aus räumlichen Koinzidenzfiltern, die Entitäten auf der Grundlage der Ausrichtung von extrahierten Merkmalen auf einem eindimensionalen linearen Unterraum erkennen.Experimente an einem öffentlichen Benchmark-Datensatz des Diagnoselernens zeigen nicht nur den Erfolg von Kapselnetzwerken bei dieser Aufgabe, sondern bestätigen auch die schnellere Konvergenz der spektralen Kapselnetzwerke.
Eine der groÃŸen Herausforderungen bei Anwendungen des maschinellen Lernens besteht darin, dass sich die Trainingsdaten von den realen Daten, mit denen der Algorithmus konfrontiert wird, unterscheiden kÃ¶nnen. Bei der Sprachmodellierung kann sich die Sprache der Benutzer (z. B. in privaten Nachrichten) innerhalb eines Jahres Ã?ndern und sich vÃ¶llig von dem unterscheiden, was wir in Ã¶ffentlich verfÃ?gbaren Daten beobachten.â€œ Gleichzeitig kÃ¶nnen Ã¶ffentliche Daten verwendet werden, um allgemeines Wissen zu erhalten (z. B. ein allgemeines Modell der englischen Sprache). â€žWir untersuchen AnsÃ?tze fÃ?r die verteilte Feinabstimmung eines allgemeinen Modells auf privaten Benutzerdaten mit den zusÃ?tzlichen Anforderungen der Aufrechterhaltung der QualitÃ?t der allgemeinen Daten und der Minimierung der Kommunikationskosten. Wir schlagen eine neuartige Technik vor, die die VorhersagequalitÃ?t fÃ?r die Sprache der Benutzer im Vergleich zu einem allgemeinen Modell signifikant verbessert und Gradientenkompressionsmethoden in Bezug auf die Kommunikationseffizienz Ã?bertrifft.â€œ Das vorgeschlagene Verfahren ist schnell und fÃ?hrt zu einer Reduzierung der KomplexitÃ?t um fast 70 % und einer Verbesserung der Einsparungsrate von 8,7 Prozentpunkten bei informellen englischen Texten.â€œ SchlieÃŸlich schlagen wir einen experimentellen Rahmen fÃ?r die Bewertung der differentiellen PrivatsphÃ?re des verteilten Trainings von Sprachmodellen vor und zeigen, dass unser Ansatz gute PrivatsphÃ?rengarantien bietet.
Wir schlagen vor, dass approximative Bayes'sche Algorithmen ein neues, direkt vom Verlust abgeleitetes Kriterium optimieren sollten, um ihr approximatives Posterior zu berechnen, das wir als Pseudo-Posterior bezeichnen. Im Gegensatz zur Standard-Variationsinferenz, die eine untere Schranke für die log marginale Wahrscheinlichkeit optimiert, können die neuen Algorithmen analysiert werden, um Verlustgarantien für die Vorhersagen mit dem Pseudo-Posterior zu geben.
In diesem Papier schlagen wir eine neue Regularisierungsmethode, RotationOut, für neuronale Netze vor. Anders als Dropout, das jedes Neuron/jeden Kanal unabhängig behandelt, betrachtet RotationOut seine Eingabeschicht als einen ganzen Vektor und führt eine Regularisierung durch zufällige Rotation des Vektors ein. RotationOut kann mit einer kleinen Modifikation auch in Faltungsschichten und rekurrenten Schichten verwendet werden. Wir verwenden außerdem eine Methode zur Rauschanalyse, um den Unterschied zwischen RotationOut und Dropout bei der Co-Adaptationsreduktion zu interpretieren. Mit dieser Methode zeigen wir auch, wie man RotationOut/Dropout zusammen mit Batch Normalization verwenden kann. Ausführliche Experimente in Seh- und Sprachaufgaben werden durchgeführt, um die Effektivität der vorgeschlagenen Methode zu zeigen. Codes werden verfügbar sein.
Die Formulierung des Problems des Reinforcement Learning (RL) im Rahmen der probabilistischen Inferenz bietet nicht nur eine neue Perspektive auf RL, sondern führt auch zu praktischen Algorithmen, die robuster und einfacher zu trainieren sind.Während diese Verbindung zwischen RL und probabilistischer Inferenz in der Ein-Agenten-Umgebung ausgiebig untersucht wurde, ist sie im Multi-Agenten-Setup noch nicht vollständig verstanden worden. In diesem Papier stellen wir das Problem des Multi-Agenten-Verstärkungslernens als das Problem der Durchführung von Inferenz in einem bestimmten grafischen Modell dar. Wir modellieren die Umgebung, wie sie von jedem der Agenten gesehen wird, mit separaten, aber verwandten Markov-Entscheidungsprozessen. Wir leiten einen praktischen Off-Policy Maximum-Entropy Actor-Critic Algorithmus ab, den wir Multi-Agent Soft Actor-Critic (MA-SAC) nennen, um eine approximative Inferenz in dem vorgeschlagenen Modell unter Verwendung von Variationsinferenz durchzuführen.MA-SAC kann sowohl im kooperativen als auch im kompetitiven Umfeld eingesetzt werden. Während MA-SAC ein resultierender Multi-Agenten-RL-Algorithmus ist, der aus dem vorgeschlagenen probabilistischen Rahmen abgeleitet werden kann, bietet unsere Arbeit eine einheitliche Sicht auf Maximum-Entropie-Algorithmen im Multi-Agenten-Umfeld.
In dieser Arbeit schlagen wir NeuralSort vor, eine allgemeine kontinuierliche Entspannung der Ausgabe des Sortieroperators von Permutationsmatrizen auf die Menge der unimodalen zeilenstochastischen Matrizen, bei denen jede Zeile die Summe eins ergibt und ein eindeutiges argmax hat, was eine geradlinige Optimierung jedes Berechnungsgraphen mit einer Sortieroperation ermöglicht. Darüber hinaus nutzen wir diese Entspannung, um eine gradientenbasierte stochastische Optimierung über den kombinatorisch großen Raum der Permutationen zu ermöglichen, indem wir einen reparametrisierten Gradientenschätzer für die Plackett-Luce-Familie von Verteilungen über Permutationen ableiten.3 Wir demonstrieren die Nützlichkeit unseres Rahmens anhand von drei Aufgaben, die das Erlernen semantischer Ordnungen von hochdimensionalen Objekten erfordern, einschließlich einer vollständig differenzierbaren, parametrisierten Erweiterung des k-nearest neighbors-Algorithmus
Die Übertragung von Wissen über Aufgaben hinweg zur Verbesserung der Dateneffizienz ist eine der offenen Schlüsselherausforderungen im Bereich der globalen Optimierungsalgorithmen. Die verfügbaren Algorithmen sind typischerweise als universelle Optimierer konzipiert und daher oft suboptimal für spezifische Aufgaben. Wir schlagen eine neuartige Transfer-Learning-Methode vor, um maßgeschneiderte Optimierer innerhalb des etablierten Rahmens der Bayes'schen Optimierung zu erhalten, die es unserem Algorithmus ermöglicht, die bewährten Generalisierungsfähigkeiten von Gauß-Prozessen zu nutzen. Mit Hilfe von Verstärkungslernen zum Meta-Training einer Erfassungsfunktion (AF) auf einer Reihe von verwandten Aufgaben lernt die vorgeschlagene Methode, implizite strukturelle Informationen zu extrahieren und sie für eine verbesserte Dateneffizienz zu nutzen.Wir präsentieren Experimente auf einer Sim-zu-Real-Transferaufgabe sowie auf mehreren simulierten Funktionen und zwei Hyperparameter-Suchproblemen. Die Ergebnisse zeigen, dass unser Algorithmus (1) automatisch strukturelle Eigenschaften von Zielfunktionen aus verfügbaren Quellaufgaben oder Simulationen identifiziert, (2) sowohl bei knappen als auch bei reichlich vorhandenen Quelldaten gute Leistungen erbringt und (3) auf das Leistungsniveau von allgemeinen AFs zurückfällt, wenn keine Struktur vorhanden ist.
Wir untersuchen die Entwicklung interner Repräsentationen während des Trainings von tiefen neuronalen Netzen (DNN) mit dem Ziel, den Kompressionsaspekt der Theorie des Informationsengpasses zu entmystifizieren, die besagt, dass das Training von DNN eine schnelle Anpassungsphase umfasst, gefolgt von einer langsameren Kompressionsphase, in der die gegenseitige Information I(X;T) zwischen der Eingabe X und den internen Repräsentationen T abnimmt. Diese Arbeit erklärt die Diskrepanz zwischen Theorie und Experimenten und klärt, was in diesen früheren Arbeiten tatsächlich gemessen wurde. Zu diesem Zweck führen wir einen (verrauschten) DNN-Rahmen ein, für den I(X;T) eine sinnvolle Größe ist, die von den Parametern des Netzwerks abhängt. Wir entwickeln dann einen rigorosen Schätzer für I(X;T) in verrauschten DNNs und beobachten die Kompression in verschiedenen Modellen. Indem wir I(X;T) im verrauschten DNN mit einem informationstheoretischen Kommunikationsproblem in Verbindung bringen, zeigen wir, dass die Kompression durch die fortschreitende Clusterung der verborgenen Repräsentationen von Eingaben derselben Klasse angetrieben wird. Schließlich kehren wir zum Schätzer von I(X;T) zurück, der in früheren Arbeiten verwendet wurde, und zeigen, dass er zwar nicht die wahre (leere) gegenseitige Information erfasst, aber als Maß für die Clusterbildung dient, was die bisherigen Beobachtungen der Kompression klärt und die geometrische Clusterbildung der verborgenen Repräsentationen als das wahre Phänomen von Interesse isoliert.
Eine zentrale Herausforderung beim Multi-Agenten-Verstärkungslernen ist die Induktion von Koordination zwischen den Agenten eines Teams. In dieser Arbeit untersuchen wir, wie man die Koordination zwischen den Agenten durch Regularisierung von Richtlinien fördern kann, und diskutieren zwei mögliche Wege, die auf der Modellierung zwischen den Agenten bzw. auf der synchronisierten Auswahl von Teilrichtlinien basieren. Um einen fairen Vergleich zu gewährleisten, stützen wir uns auf eine gründliche Hyper-Parameter-Auswahl- und Trainingsmethodik, die ein festes Hyper-Parameter-Suchbudget für jeden Algorithmus und jede Umgebung erlaubt.Wir bewerten folglich sowohl die Hyper-Parameter-Sensitivität, die Stichproben-Effizienz als auch die asymptotische Leistung jeder Lernmethode.Unsere Experimente zeigen, dass die vorgeschlagenen Methoden zu signifikanten Verbesserungen bei kooperativen Problemen führen.Wir analysieren außerdem die Auswirkungen der vorgeschlagenen Regularisierungen auf die von den Agenten erlernten Verhaltensweisen.
Die zentrale Herausforderung beim multimodalen Lernen besteht darin, gemeinsame Repräsentationen abzuleiten, die Informationen aus diesen Modalitäten verarbeiten und zueinander in Beziehung setzen können, wobei bestehende Arbeiten gemeinsame Repräsentationen unter Verwendung mehrerer Modalitäten als Input erlernen und empfindlich auf verrauschte oder fehlende Modalitäten zur Testzeit reagieren können. Mit dem jüngsten Erfolg von Sequenz-zu-Sequenz-Modellen in der maschinellen Übersetzung bietet sich die Gelegenheit, neue Wege zum Erlernen gemeinsamer Repräsentationen zu erforschen, die nicht alle Eingabemodalitäten zum Testzeitpunkt erfordern.In diesem Beitrag schlagen wir eine Methode zum Erlernen robuster gemeinsamer Repräsentationen durch Übersetzung zwischen Modalitäten vor. Unsere Methode basiert auf der Einsicht, dass die Übersetzung von einer Quell- in eine Zielmodalität eine Methode zum Erlernen gemeinsamer Repräsentationen darstellt, die nur die Quellmodalität als Eingabe verwendet.Wir ergänzen die Übersetzungen der Modalitäten mit einem Zykluskonsistenzverlust, um sicherzustellen, dass unsere gemeinsamen Repräsentationen maximale Informationen aus allen Modalitäten enthalten. Sobald unser Übersetzungsmodell mit gepaarten multimodalen Daten trainiert ist, benötigen wir nur noch Daten aus der Quellmodalität zur Testzeit für die Vorhersage, was sicherstellt, dass unser Modell robust gegenüber Störungen oder fehlenden Zielmodalitäten bleibt.Wir trainieren unser Modell mit einem gekoppelten Übersetzungs-Vorhersage-Ziel und es erreicht neue State-of-the-Art-Ergebnisse auf multimodalen Sentiment-Analyse-Datensätzen: CMU-MOSI, ICT-MMMO und YouTube.Zusätzliche Experimente zeigen, dass unser Modell zunehmend diskriminierende gemeinsame Repräsentationen mit mehr Eingabemodalitäten erlernt, während es robust gegenüber Störungen aller anderen Modalitäten bleibt.
Wir untersuchen die Unterschiede zwischen den Eigenwerten der Hessian des neuronalen Netzwerks, die über den empirischen Datensatz ausgewertet werden, der empirischen Hessian, und den Eigenwerten der Hessian unter der datengenerierenden Verteilung, die wir als True Hessian bezeichnen. Unter milden Annahmen verwenden wir die Theorie der Zufallsmatrix, um zu zeigen, dass die Wahre Hessian Eigenwerte mit kleinerem Absolutwert als die Empirische Hessian hat. Wir unterstützen diese Ergebnisse für verschiedene SGD-Schemata auf einem 110-Schicht-ResNet und VGG-16. Zur Durchführung dieser Experimente schlagen wir einen Rahmen für die spektrale Visualisierung vor, der auf GPU-beschleunigter stochastischer Lanczos-Quadratur basiert.
Basierend auf den vielversprechenden Ergebnissen von neuronalen Graphen-Netzwerken auf hochstrukturierten Daten, entwickeln wir einen Rahmen, um bestehende Sequenz-Encoder mit einer Graphen-Komponente zu erweitern, die über Beziehungen über lange Distanzen in schwach strukturierten Daten wie Text schließen kann.In einer umfangreichen Auswertung zeigen wir, dass die resultierenden hybriden Sequenz-Graphen-Modelle sowohl reine Sequenz-Modelle als auch reine Graphen-Modelle auf einer Reihe von Zusammenfassungsaufgaben übertreffen.
In der probabilistischen Klassifikation zeigt ein diskriminatives Modell, das auf einer Gauß'schen Mischung basiert, eine flexible Anpassungsfähigkeit, aber es ist schwierig, die Anzahl der Komponenten zu bestimmen.Wir schlagen einen spärlichen Klassifikator vor, der auf einem diskriminativen Gauß'schen Mischungsmodell (GMM) basiert und spärliche diskriminative Gauß'sche Mischung (SDGM) genannt wird.Im SDGM wird ein GMM-basiertes diskriminatives Modell durch spärliches Bayes'sches Lernen trainiert. Die SDGM kann in neuronale Netze (NNs) wie z.B. Faltungs-NNs eingebettet werden und kann durchgängig trainiert werden.Experimentelle Ergebnisse zeigen, dass die vorgeschlagene Methode eine Überanpassung verhindert, indem sie Sparsamkeit erreicht.Darüber hinaus haben wir gezeigt, dass die vorgeschlagene Methode eine vollständig verbundene Schicht mit der Softmax-Funktion in bestimmten Fällen übertrifft, wenn sie als letzte Schicht eines tiefen NNs verwendet wird.
Wir haben kürzlich beobachtet, dass Faltungsfilter, die mit Hilfe von vorberechneten Grassmann'schen Unterraum-Packing-Codebüchern aus dem Handel am weitesten voneinander entfernt initialisiert wurden, in vielen Datensätzen überraschend gut abschneiden. Mit diesem kurzen Beitrag möchten wir einige erste Ergebnisse in dieser Hinsicht verbreiten, in der Hoffnung, dass wir die Neugier der Deep-Learning-Gemeinschaft anregen, die klassischen Grassmann'schen Unterraum-Packing-Ergebnisse als Quelle für neue Ideen für effizientere Initialisierungsstrategien zu betrachten.
Adversarische Anpassungsmodelle, die in Merkmalsräumen angewandt werden, entdecken domäneninvariante Repräsentationen, sind aber schwer zu visualisieren und versagen manchmal bei der Erfassung von Verschiebungen auf Pixel- und niedriger Domänenebene. Jüngste Arbeiten haben gezeigt, dass generative adversarische Netzwerke in Kombination mit Zyklus-Konsistenz-Bedingungen überraschend effektiv bei der Abbildung von Bildern zwischen Domänen sind, auch ohne die Verwendung von ausgerichteten Bildpaaren.Wir schlagen eine neuartige diskriminativ trainierte Cycle-Consistent Adversarial Domain Adaptation model.CyCADA passt Darstellungen sowohl auf der Pixel-Ebene und Feature-Ebene, erzwingt Zyklus-Konsistenz, während die Nutzung einer Aufgabe Verlust, und erfordert keine ausgerichteten Paare.  Unser Modell kann in einer Vielzahl von visuellen Erkennungs- und Vorhersageumgebungen eingesetzt werden. Wir zeigen neue State-of-the-Art-Ergebnisse für mehrere Anpassungsaufgaben, einschließlich der Klassifizierung von Ziffern und der semantischen Segmentierung von Straßenszenen, die die Übertragung von synthetischen auf reale Domänen demonstrieren.
Stemming ist der Prozess der Entfernung von Affixen (d.h. Präfixe, Infixe und Suffixe), die die Genauigkeit und Leistung von Information Retrieval-Systemen verbessern.Dieses Papier präsentiert die Reduktion von amharischen Wörtern auf entsprechende Stamm, wo mit der Absicht, dass es semantische Informationen bewahrt. Der Prozess des Entfernens solcher Affixe (Präfixe, Infixe und Suffixe) von einem Wort auf seine Grundform wird als Stemming bezeichnet. Während es für dominante Sprachen wie Englisch viele Stemming-Programme gibt, fehlt es bei unterversorgten Sprachen wie Amharisch an einer derartigen Unterstützung durch leistungsfähige Werkzeuge. In diesem Papier, entwerfen wir eine leichte Amharic stemmer auf der Grundlage der Regeln, die ein Amharic Wort erhält und dann findet es eine Übereinstimmung mit dem Anfang eines Wortes, um die möglichen Präfixe und die Endung mit den möglichen Suffixe und schließlich prüft es, ob es infix.the Endergebnis ist der Stamm, wenn es keine Präfix, Infix oder / und Suffix, sonst bleibt es in einem der früheren Staaten. Die Leistung des generierten Stammes wird anhand von manuell annotierten amharischen Wörtern bewertet, wobei das Ergebnis mit dem aktuellen Stand der Technik verglichen wird und eine Steigerung der Korrektheit des Stammes um 7% zeigt.
Wir untersuchen das Vorhandensein eines solchen Raums in tiefen neuronalen Netzen, indem wir das Aktivierungsprofil der Neuronen der verborgenen Schicht aufzeichnen. Obwohl Platzzellen und konzeptzellenähnliche Eigenschaften gefunden werden, sind gitterzellenähnliche Feuerungsmuster nicht vorhanden, was auf einen Mangel an Pfadintegration oder Merkmalstransformation in trainierten Netzwerken hinweist.Insgesamt stellen wir eine plausible Unzulänglichkeit in den aktuellen Deep-Learning-Praktiken vor, die tiefe Netzwerke daran hindern, analoges Denken und Gedächtnisabfragen durchzuführen.
Wir entwickeln eine umfassende Beschreibung des aktiven Inferenzrahmens, wie er von Friston (2010) vorgeschlagen wurde, aus der Perspektive des maschinellen Lernens, und schlagen eine Skizze einer kognitiven Architektur vor, die auf der Grundlage biologischer Inspiration und der Prinzipien der Selbstkodierung Wege zur Umsetzung schätzungsorientierter Kontrollstrategien aufzeigt.  Obwohl die Optimierung der zukünftigen posterioren Entropie über die Aktionsmenge ausreicht, um eine lokal optimale Aktionsauswahl zu erreichen, erweist sich die Offline-Berechnung unter Verwendung klassenspezifischer Salienzkarten als besser, da sie Verarbeitungskosten durch Sakkadenpfade und Vorverarbeitung einspart, mit einem vernachlässigbaren Effekt auf die Erkennungs-/Kompressionsraten.
Um dieses Problem zu umgehen und auf Graphen zu lernen, ist die Darstellung von Graphenmerkmalen erforderlich. Die Hauptschwierigkeiten bei der Merkmalsextraktion liegen in der Abwägung zwischen Aussagekraft, Konsistenz und Effizienz, d. h. Während modernste Methoden die Ausdruckskraft mit leistungsstarken neuronalen Netzen verbessern, schlagen wir vor, die natürlichen spektralen Eigenschaften von Graphen zu nutzen, um ein einfaches Graphenmerkmal zu untersuchen: das Graph-Laplacian-Spektrum (GLS). Wir analysieren die Darstellungskraft dieses Objekts, das sowohl Isomorphie-Invarianz, Ausdruckskraft als auch Deformationskonsistenz erfüllt, und schlagen insbesondere eine theoretische Analyse auf der Grundlage von Graphenstörungen vor, um zu verstehen, welche Art von Vergleich zwischen Graphen wir beim Vergleich von GLS durchführen. Um dies zu tun, leiten wir Grenzen für den Abstand zwischen GLS ab, die mit der Divergenz zur Isomorphie zusammenhängen, einer rechenaufwändigen Standardgraphen-Divergenz, und testen GLS als Graphenrepräsentation durch Konsistenztests und Klassifizierungsaufgaben.
Adversarial Training, eine Methode zum Erlernen von robusten tiefen Netzwerken, ist in der Regel angenommen, dass teurer als herkömmliche Ausbildung aufgrund der Notwendigkeit der Konstruktion von adversarial Beispiele über eine erste Ordnung Methode wie projiziert Gradient dezent (PGD) sein.  In diesem Papier machen wir die überraschende Entdeckung, dass es möglich ist, empirisch robuste Modelle mit einem viel schwächeren und billigeren Gegner zu trainieren, ein Ansatz, der bisher als ineffektiv galt, wodurch die Methode in der Praxis nicht teurer ist als Standardtraining.  Konkret zeigen wir, dass das adversarische Training mit der Fast-Gradient-Sign-Methode (FGSM) in Kombination mit einer zufälligen Initialisierung genauso effektiv ist wie das PGD-basierte Training, aber deutlich geringere Kosten verursacht.  Darüber hinaus zeigen wir, dass das adversarische Training mit FGSM durch den Einsatz von Standardtechniken für das effiziente Training von tiefen Netzwerken weiter beschleunigt werden kann. So konnten wir einen robusten CIFAR10-Klassifikator mit einer robusten Genauigkeit von 45% bei epsilon=8/255 in 6 Minuten und einen robusten ImageNet-Klassifikator mit einer robusten Genauigkeit von 43% bei epsilon=2/255 in 12 Stunden erlernen, im Vergleich zu früheren Arbeiten, die auf ``freiem'' adversarischem Training basierten und 10 bzw. 50 Stunden benötigten, um die gleichen Schwellenwerte zu erreichen.
Auf der Suche nach spärlichen und effizienten neuronalen Netzwerkmodellen haben viele frühere Arbeiten untersucht, wie man L1- oder L0-Regularisierer erzwingen kann, um die Spärlichkeit der Gewichte während des Trainings zu fördern.Der L0-Regularisierer misst die Spärlichkeit der Parameter direkt und ist invariant gegenüber der Skalierung der Parameterwerte.Aber er kann keine nützlichen Gradienten liefern und erfordert daher komplexe Optimierungstechniken.Der L1-Regularisierer ist fast überall differenzierbar und kann leicht mit Gradientenabstieg optimiert werden.Aber er ist nicht skaleninvariant und verursacht die gleiche Schrumpfungsrate für alle Parameter, was bei der Erhöhung der Spärlichkeit ineffizient ist. Inspiriert durch das Hoyer-Maß (das Verhältnis zwischen L1- und L2-Normen), das in traditionellen Compressed-Sensing-Problemen verwendet wird, stellen wir DeepHoyer vor, eine Reihe von sparitätsinduzierenden Regularisierern, die sowohl fast überall differenzierbar als auch skaleninvariant sind.Unsere Experimente zeigen, dass die Erzwingung von DeepHoyer-Regularisierern sogar spärlichere neuronale Netzmodelle als frühere Arbeiten erzeugen kann, bei gleichem Genauigkeitsniveau.Wir zeigen auch, dass DeepHoyer sowohl auf elementweises als auch auf strukturelles Pruning angewendet werden kann.
Selbstüberwachung, bei der eine Zielaufgabe ohne externe Überwachung verbessert wird, wurde hauptsächlich in Umgebungen erforscht, die die Verfügbarkeit zusätzlicher Daten voraussetzen.In vielen Fällen, insbesondere im Gesundheitswesen, hat man jedoch möglicherweise keinen Zugang zu zusätzlichen Daten (gelabelt oder anderweitig).In solchen Umgebungen stellen wir die Hypothese auf, dass Selbstüberwachung, die ausschließlich auf der Struktur der vorliegenden Daten basiert, hilfreich sein kann.Wir erforschen einen neuartigen Selbstüberwachungsrahmen für Zeitreihendaten, in dem mehrere Hilfsaufgaben (z. B. Prognosen) einbezogen werden, um die Gesamtgenauigkeit zu verbessern, Wir nennen diesen Ansatz begrenzte Selbst-Überwachung, da wir uns nur auf die vorliegenden Daten beschränken.Wir demonstrieren den Nutzen der begrenzten Selbst-Überwachung an drei Klassifizierungsaufgaben auf Sequenzebene, von denen sich zwei auf reale klinische Daten beziehen und eine synthetische Daten verwendet.Innerhalb dieses Rahmens führen wir neue Formen der Selbst-Überwachung ein und demonstrieren ihren Nutzen bei der Verbesserung der Leistung bei der Zielaufgabe. Unsere Ergebnisse zeigen, dass begrenzte Selbstüberwachung zu einer konsistenten Verbesserung gegenüber einer überwachten Basislinie in einer Reihe von Bereichen führt. Insbesondere für die Aufgabe der Identifizierung von Vorhofflimmern aus kleinen Mengen von Elektrokardiogrammdaten beobachten wir eine fast 13%ige Verbesserung der Fläche unter der Receiver Operating Characteristics Curve (AUC-ROC) im Vergleich zur Basislinie (AUC-ROC=0,55 vs. AUC-ROC=0,62).Begrenzte Selbstüberwachung, die auf sequenzielle Daten angewandt wird, kann beim Erlernen von Zwischendarstellungen helfen und ist daher besonders in Umgebungen anwendbar, in denen die Datensammlung schwierig ist.
Sind neuronale Netze für einfache Funktionen voreingenommen?Hilft Tiefe immer beim Erlernen komplexerer Merkmale?Ist das Training der letzten Schicht eines Netzes genauso gut wie das Training aller Schichten?Diese Fragen scheinen auf den ersten Blick nichts miteinander zu tun zu haben, aber in dieser Arbeit behandeln wir sie alle gemeinsam aus der spektralen Perspektive. Wir untersuchen die Spektren des *Conjugate Kernel, CK* (auch *Neural Network-Gaussian Process Kernel* genannt) und des *Neural Tangent Kernel, NTK*. Grob gesagt sagen uns der CK und der NTK, "wie ein Netz bei der Initialisierung aussieht" und "wie ein Netz während und nach dem Training aussieht". "Durch die Analyse der Eigenwerte gewinnen wir neue Einsichten in die eingangs gestellten Fragen, und wir überprüfen diese Einsichten durch umfangreiche Experimente mit neuronalen Netzen. Wir sind davon überzeugt, dass die von uns entwickelten Berechnungswerkzeuge für die Analyse der Spektren von CK und NTK eine solide Grundlage für künftige Studien über tiefe neuronale Netze bilden. github.com/jxVmnLgedVwv6mNcGCBy/NNspectra stellt den Code dafür und für die Generierung der Diagramme in diesem Papier als Open Source bereit.
Um zu kommunizieren, Hypothesen zu begründen und Daten zu analysieren, beziehen sich Neurowissenschaftler oft auf Unterteilungen des Gehirns. Hier betrachten wir Atlanten, die zur Parzellierung des Gehirns bei der Untersuchung von Hirnfunktionen verwendet werden, und diskutieren die Bedeutung und Gültigkeit dieser Parzellierungen, sowohl aus konzeptioneller Sicht als auch durch die Durchführung verschiedener analytischer Aufgaben auf populären funktionellen Gehirnparzellierungen.
Hochdimensionale, spärliche Belohnungsaufgaben stellen große Herausforderungen für Reinforcement Learning Agenten dar.  In dieser Arbeit verwenden wir Nachahmungslernen, um zwei dieser Herausforderungen zu bewältigen: Wie lernt man eine nützliche Repräsentation der Welt, z.B. aus Pixeln, und wie kann man angesichts der Seltenheit eines Belohnungssignals effizient erforschen? wir zeigen, dass die gegnerische Nachahmung sogar in diesem hochdimensionalen Beobachtungsraum gut funktionieren kann. überraschenderweise kann der Gegner selbst, der als gelernte Belohnungsfunktion fungiert, winzig klein sein, mit nur 128 Parametern, und kann leicht mit der einfachsten GAN-Formulierung trainiert werden. Unser Ansatz beseitigt die Beschränkungen, die in den meisten zeitgenössischen Imitationsansätzen vorhanden sind: Er erfordert keine Aktionen des Demonstrators (nur Video), keine speziellen Anfangsbedingungen oder Warmstarts und keine explizite Verfolgung einer einzelnen Demo. Der vorgeschlagene Agent kann eine anspruchsvolle Robotermanipulationsaufgabe des Blockstapelns nur mit Hilfe von Videodemonstrationen und spärlicher Belohnung lösen, bei der die nicht imitierenden Agenten nicht vollständig lernen können.  Darüber hinaus lernt unser Agent viel schneller als konkurrierende Ansätze, die auf handgefertigten, gestaffelten dichten Belohnungsfunktionen beruhen, und auch besser als die Standard-GAIL-Baselines.Schließlich entwickeln wir einen neuen gegnerischen Zielerkenner, der es dem Agenten in einigen Fällen ermöglicht, das Stapeln ohne jegliche Aufgabenbelohnung zu lernen, rein durch Nachahmung.
Eine Strategie basiert auf der statistischen Analyse und dem Vergleich von rohen Pixelwerten und daraus extrahierten Merkmalen, die andere Strategie lernt formale Spezifikationen aus den realen Daten und zeigt, dass gefälschte Proben die Spezifikationen der realen Daten verletzen. Wir zeigen, dass gefälschte Proben, die mit GANs erzeugt wurden, eine universelle Signatur haben, die zur Identifizierung gefälschter Proben verwendet werden kann.
Bemühungen, die numerische Präzision von Berechnungen im Deep Learning Training zu reduzieren, haben zu Systemen geführt, die Gewichte und Aktivierungen aggressiv quantisieren, aber dennoch große hochpräzise Akkumulatoren für Partialsummen in inneren Produktoperationen verwenden, um die Qualität der Konvergenz zu bewahren.Das Fehlen eines Rahmens, um die Präzisionsanforderungen von Partialsummenakkumulationen zu analysieren, führt zu konservativen Designentscheidungen, die eine Obergrenze für die Reduzierung der Komplexität von Multiplikationsakkumulationseinheiten festlegen. Wir stellen einen statistischen Ansatz vor, um die Auswirkungen einer reduzierten Akkumulationspräzision auf das Deep-Learning-Training zu analysieren, indem wir feststellen, dass eine schlechte Wahl der Akkumulationspräzision zu einem Informationsverlust führt, der sich als Verringerung der Varianz in einem Ensemble von Partialsummen manifestiert, und leiten eine Reihe von Gleichungen ab, die diese Varianz mit der Länge der Akkumulation und der Mindestanzahl der für die Akkumulation benötigten Bits in Beziehung setzen: CIFAR-10 ResNet 32, ImageNet ResNet 18 und ImageNet AlexNet.in jedem Fall, mit Akkumulation Präzision in Übereinstimmung mit unseren vorgeschlagenen Gleichungen, die Netze erfolgreich konvergieren, um die einfache Präzision Gleitkomma-Basislinie.wir zeigen auch, dass die Verringerung der Akkumulation Präzision weiter verschlechtert die Qualität des trainierten Netzwerks, was beweist, dass unsere Gleichungen engen Grenzen.insgesamt diese Analyse ermöglicht eine präzise Anpassung der Berechnung Hardware auf die Anwendung, was Bereichs-und Power-optimale Systeme.
Unüberwachte Domänenanpassung ist ein vielversprechender Weg, um die Leistung von tiefen neuronalen Netzen auf einer Zieldomäne zu verbessern, indem nur Bezeichnungen aus einer Quelldomäne verwendet werden, aber die beiden vorherrschenden Methoden, das Lernen der Domänendiskrepanz und das halbüberwachte Lernen, sind nicht ohne weiteres anwendbar, wenn Quell- und Zieldomäne keinen gemeinsamen Bezeichnungsraum haben. Dieses Papier befasst sich mit dem obigen Szenario, indem ein Repräsentationsraum erlernt wird, der sowohl in der (beschrifteten) Quell- als auch in der (unbeschrifteten) Zieldomäne diskriminierend wirkt, während die Repräsentationen für die beiden Domänen gut voneinander getrennt bleiben.Inspiriert durch eine theoretische Analyse formulieren wir zunächst die disjunkte Klassifizierungsaufgabe, bei der die Quell- und Zieldomänen nicht überlappenden Klassenbeschriftungen entsprechen, zu einer Verifikationsaufgabe um. Um sowohl innerhalb als auch zwischen den Domänen zu verifizieren, schlagen wir ein Feature Transfer Network (FTN) vor, um den Ziel-Merkmalsraum vom ursprünglichen Quellraum zu trennen, während wir ihn an einem transformierten Quellraum ausrichten, und präsentieren einen nicht-parametrischen Multi-Klassen-Entropie-Minimierungsverlust, um die Unterscheidungskraft von FTNs auf der Zieldomäne weiter zu erhöhen. In Experimenten veranschaulichen wir zunächst, wie FTN in einer kontrollierten Umgebung der Anpassung von MNIST-M an MNIST mit disjunkten Ziffernklassen zwischen den beiden Domänen funktioniert, und demonstrieren dann die Wirksamkeit von FTNs durch State-of-the-Art-Leistungen auf einem ethnienübergreifenden Gesichtserkennungsproblem.
Um ein NN mit spezifischen Strukturen zu fördern, berücksichtigen wir explizit die nicht-glatte Regularisierung (z.B. L1-Norm) und Einschränkungen (z.B. Intervallbeschränkung). Dies wird als eingeschränktes nicht-glattes nicht-konvexes Optimierungsproblem formuliert, und wir schlagen einen konvergenten stochastischen Gradientenabstiegsalgorithmus des proximalen Typs (Prox-SGD) vor. Wir zeigen, dass unter richtig gewählten Lernraten, Momentum schließlich ähnelt der unbekannten realen gradient und ist somit entscheidend bei der Analyse der convergence.We etablieren, dass mit der Wahrscheinlichkeit 1, jeder Grenzpunkt der Sequenz, die von der vorgeschlagenen Prox-SGD ist ein stationärer Punkt.Then die Prox-SGD ist zugeschnitten auf die Ausbildung eines spärlichen neuronalen Netzes und ein binäres neuronales Netz, und die theoretische Analyse ist auch durch umfangreiche numerische Tests unterstützt.
Der Verlust einiger weniger Neuronen in einem Gehirn fÃ?hrt selten zu einem sichtbaren Funktionsverlust, aber es ist unklar, was â€žwenigeâ€œ in diesem Zusammenhang bedeutet: Wie viele zufÃ?llige NeuronenausfÃ?lle sind notwendig, um einen sichtbaren Funktionsverlust zu verursachen? Wir untersuchen die Fehlertoleranz neuronaler Netze, die kleinen zufÃ?lligen AusfÃ?llen von Neuronen/Gewichten in einer probabilistischen Umgebung ausgesetzt sind, und geben nachweisbare Garantien fÃ?r die Robustheit des Netzes gegenÃ?ber diesen AusfÃ?llen. Unser Hauptbeitrag ist eine Schranke für den Fehler in der Ausgabe eines Netzes unter kleinen zufälligen Bernoulli-Abstürzen, die durch die Verwendung einer Taylor-Erweiterung in der kontinuierlichen Grenze bewiesen wird, in der nahe beieinander liegende Neuronen in einer Schicht ähnlich sind.Der Fehlermodus, den wir in unserem Modell annehmen, ist charakteristisch für neuromorphe Hardware, eine vielversprechende Technologie zur Beschleunigung künstlicher neuronaler Netze, sowie für biologische Netze. Wir zeigen, dass unsere theoretischen Grenzen verwendet werden können, um die Fehlertoleranz verschiedener Architekturen zu vergleichen und einen Regularizer zu entwerfen, der die Fehlertoleranz einer bestimmten Architektur verbessert, und entwerfen einen Algorithmus, der Fehlertoleranz mit einer angemessenen Anzahl von Neuronen erreicht.
In der Robotik haben wir enorme Fortschritte bei der Nutzung der visuellen und taktilen Wahrnehmung gesehen, aber wir haben oft einen Schlüsselsinn ignoriert: den Klang.Dies ist vor allem auf den Mangel an Daten zurückzuführen, die das Zusammenspiel von Aktion und Klang erfassen.In dieser Arbeit führen wir die erste groß angelegte Studie über die Wechselwirkungen zwischen Klang und Roboteraktion durch. Um dies zu tun, erstellen wir den größten verfügbaren Sound-Action-Vision-Datensatz mit 15.000 Interaktionen auf 60 Objekten mit unserer Roboterplattform Tilt-Bot.Durch das Kippen von Objekten und deren Aufprall auf die Wände eines Robotertabletts sammeln wir reichhaltige Vier-Kanal-Audio-Informationen.Mit diesen Daten erforschen wir die Synergien zwischen Sound und Aktion und präsentieren drei wichtige Erkenntnisse.Erstens, Sound ist ein Indikator für feinkörnige Objektklasseninformationen, z.B., Zweitens enthält Klang auch Informationen über die kausalen Auswirkungen einer Aktion, d.h. anhand des erzeugten Klangs können wir vorhersagen, welche Aktion auf das Objekt angewendet wurde.Schließlich sind Objektrepräsentationen, die aus Audioeinbettungen abgeleitet werden, ein Hinweis auf implizite physikalische Eigenschaften.Wir zeigen, dass bei zuvor ungesehenen Objekten Audioeinbettungen, die durch Interaktionen erzeugt werden, Vorwärtsmodelle 24% besser vorhersagen können als passive visuelle Einbettungen.
Hierarchische Label-Strukturen sind in vielen Aufgaben des maschinellen Lernens weit verbreitet, von solchen mit expliziten Label-Hierarchien wie der Bildklassifikation bis hin zu solchen mit latenten Label-Hierarchien wie der semantischen Segmentierung.Leider verwenden State-of-the-Art-Methoden oft Cross-Entropie-Verluste, die explizit die Unabhängigkeit zwischen den Klassenlabels voraussetzen.Motiviert durch die Tatsache, dass Klassenmitglieder aus derselben Hierarchie einander ähnlich sein müssen, entwerfen wir ein neues Trainingsdiagramm namens Hierarchical Complement Objective Training (HCOT). Wir führen unsere Methode sowohl bei der Bildklassifikation als auch bei der semantischen Segmentierung durch, und die Ergebnisse zeigen, dass HCOT die State-of-the-Art-Modelle im CIFAR100-, Imagenet- und PASCAL-Kontext übertrifft. Unsere Experimente zeigen auch, dass HCOT bei Aufgaben mit latenten Hierarchien von Bezeichnungen angewandt werden kann, was ein häufiges Merkmal bei vielen maschinellen Lernaufgaben ist.
Um die Effizienz von NAS zu verbessern, haben frühere Ansätze die Methode der gemeinsamen Nutzung von Gewichten angewandt, um alle Modelle zu zwingen, den gleichen Satz von Gewichten zu teilen.  In diesem Beitrag analysieren wir die bestehenden One-Shot-NAS-Ansätze mit Gewichtsteilung aus Bayesscher Sicht und identifizieren das Posterior-Fading-Problem, das die Effektivität geteilter Gewichte beeinträchtigt. Um dieses Problem zu lindern, stellen wir einen praktischen Ansatz vor, um das Parameter-Posterior in Richtung seiner wahren Verteilung zu lenken, und führen während der Suche eine harte Latenzbeschränkung ein, um die gewünschte Latenz zu erreichen. In unserem kleinen Suchraum erreicht unser Modell PC-NAS-S eine Top-1-Genauigkeit von 76,8%, 2,1% höher als MobileNetV2 (1,4x) mit der gleichen Latenzzeit. Wenn es auf unseren großen Suchraum übertragen wird, erreicht PC-NAS-L eine Top-1-Genauigkeit von 78,1% innerhalb von 11 ms. Die entdeckte Architektur lässt sich auch gut auf andere Computer-Vision-Anwendungen übertragen, wie z.B. die Erkennung von Objekten und die Wiedererkennung von Personen.
In diesem Papier behaupten wir, dass eine solche Überanpassung vermieden werden kann, indem wir die Ausbildung eines tiefen neuronalen Netzes "früh stoppen", bevor die verrauschten Etiketten ernsthaft gespeichert werden, und dann die Ausbildung des früh gestoppten Netzes unter Verwendung eines "maximal sicheren Satzes" fortsetzen, der eine Sammlung von mit ziemlicher Sicherheit wahrheitsgetreu beschrifteten Proben in jeder Epoche seit dem frühen Stopp-Punkt beibehält. Indem wir alle Komponenten zusammenfügen, realisiert unsere neuartige zweistufige Trainingsmethode, Prestopping genannt, ein rauschfreies Training unter jeder Art von Etikettenrauschen für den praktischen Einsatz. Ausführliche Experimente mit vier Bild-Benchmark-Datensätzen bestätigen, dass unsere Methode vier State-of-the-Art-Methoden in Bezug auf den Testfehler um 0,4 bis 8,2 Prozentpunkte bei Vorhandensein von realem Rauschen deutlich übertrifft.
Neuere Arbeiten zeigen, dass kontinuierliche Kommunikation ein effizientes Training mit Backpropagation in Multiagentenszenarien ermÃ¶glicht, aber auf vollkooperative Aufgaben beschrÃ?nkt ist. In diesem Papier prÃ?sentieren wir ein individualisiertes kontrolliertes kontinuierliches Kommunikationsmodell (IC3Net), das eine bessere Trainingseffizienz als ein einfaches kontinuierliches Kommunikationsmodell aufweist und neben kooperativen Einstellungen auch auf semi-kooperative und kompetitive Einstellungen angewendet werden kann. IC3Net steuert die kontinuierliche Kommunikation mit einem Gating-Mechanismus und verwendet individualisierte Belohnungen für jeden Agenten, um eine bessere Leistung und Skalierbarkeit zu erreichen und gleichzeitig Probleme bei der Kreditvergabe zu beheben.Mit einer Vielzahl von Aufgaben, einschließlich StarCraft BroodWars erforschen und Kampfszenarien, zeigen wir, dass unser Netzwerk eine bessere Leistung und Konvergenzraten als die Grundlinien liefert, wenn die Skala zunimmt.Unsere Ergebnisse vermitteln, dass IC3Net-Agenten lernen, wann sie auf der Grundlage des Szenarios und der Rentabilität kommunizieren.
Neuronale Sequenz-zu-Sequenz-Modelle sind eine kürzlich vorgeschlagene Familie von Ansätzen, die in der abstrakten Zusammenfassung von Textdokumenten verwendet werden und nützlich sind, um verdichtete Versionen von Ausgangstext-Erzählungen zu produzieren, ohne auf die Verwendung von Wörtern aus dem Originaltext beschränkt zu sein.Trotz der Fortschritte in der abstrakten Zusammenfassung bleibt die benutzerdefinierte Generierung von Zusammenfassungen (z.B. nach den Präferenzen eines Benutzers) unerforscht. In diesem Beitrag stellen wir CATS vor, ein abstraktes neuronales Zusammenfassungsmodell, das Inhalte in einer Sequenz-zu-Sequenz-Methode zusammenfasst, aber auch einen neuen Mechanismus einführt, um die zugrundeliegende latente Themenverteilung der erstellten Zusammenfassungen zu kontrollieren.Unsere experimentellen Ergebnisse auf dem bekannten CNN/DailyMail-Datensatz zeigen, dass unser Modell die beste Leistung erreicht.
Wir schlagen ein Software-Framework vor, das auf den Ideen des Lern-Kompressions-Algorithmus basiert und es ermöglicht, jedes neuronale Netz durch verschiedene Kompressionsmechanismen zu komprimieren (Pruning, Quantisierung, Low-Rank, etc.) Das Lernen des neuronalen Netzes (durch SGD) ist von der Kompression seiner Parameter (durch eine Signalkompressionsfunktion) entkoppelt, so dass das Framework leicht erweitert werden kann, um verschiedene Kombinationen von neuronalem Netz und Kompressionstyp zu behandeln. Unser Toolkit ist in Python und Pytorch geschrieben und wir planen, es bis zum Workshop zur Verfügung zu stellen und es schließlich für Beiträge aus der Community zu öffnen.
Zu diesem Zweck schlagen wir einen multimodalen Lernrahmen vor, der die Inferenz- und die Generierungsphase miteinander verbindet: Zunächst werden die Inferenznetzwerke trainiert, um die Identität des Sprechers zwischen den beiden verschiedenen Modalitäten abzugleichen, dann arbeiten die vortrainierten Inferenznetzwerke mit dem Generierungsnetzwerk zusammen, indem sie bedingte Informationen über die Stimme liefern.
Wir stellen ein einfaches neuronales Modell vor, das bei Vorliegen einer Formel und einer Eigenschaft versucht, die Frage zu beantworten, ob die Formel die gegebene Eigenschaft hat, z. B. ob eine propositionale Formel immer wahr ist.Die Struktur der Formel wird von einem vorwärtsgerichteten neuronalen Netz erfasst, das rekursiv für die gegebene Formel in einer Top-Down-Methode aufgebaut wird.Die Ergebnisse dieses Netzes werden dann von zwei rekurrenten neuronalen Netzen verarbeitet.Einer der interessanten Aspekte unseres Modells ist, wie propositionale Atome behandelt werden.Das Modell ist z. B. unempfindlich gegenüber ihren Namen, es ist nur wichtig, ob sie gleich oder verschieden sind.
Trotz signifikanter Fortschritte auf dem Gebiet des tiefen Reinforcement Learning (RL) sind die heutigen Algorithmen immer noch nicht in der Lage, über eine Reihe unterschiedlicher Aufgaben wie Atari 2600-Spiele konsistent Richtlinien auf menschlichem Niveau zu erlernen.Wir identifizieren drei zentrale Herausforderungen, die jeder Algorithmus meistern muss, um bei allen Spielen gut abzuschneiden: die Verarbeitung unterschiedlicher Belohnungsverteilungen, das Denken über lange Zeithorizonte und die effiziente Erforschung.  In diesem Papier schlagen wir einen Algorithmus vor, der jede dieser Herausforderungen adressiert und in der Lage ist, Richtlinien auf menschlichem Niveau für fast alle Atari-Spiele zu erlernen.Ein neuer transformierter Bellman-Operator erlaubt es unserem Algorithmus, Belohnungen unterschiedlicher Dichte und Größenordnung zu verarbeiten; ein zusätzlicher zeitlicher Konsistenzverlust erlaubt es uns, stabil mit einem Diskontierungsfaktor von 0. 999 (anstelle von 0,99) zu trainieren, was den effektiven Planungshorizont um eine Größenordnung verlängert; und wir erleichtern das Explorationsproblem durch die Verwendung menschlicher Demonstrationen, die den Agenten in Richtung lohnender Zustände leiten.Bei Tests mit 42 Atari-Spielen übertrifft unser Algorithmus die Leistung eines durchschnittlichen Menschen bei 40 Spielen unter Verwendung eines gemeinsamen Satzes von Hyperparametern.
Das Wissen, das Menschen über ein Problem besitzen, geht oft weit über einen Satz von Trainingsdaten und Output-Labels hinaus.Während der Erfolg von Deep Learning meist auf überwachtem Training beruht, können wichtige Eigenschaften nicht effizient aus End-to-End-Annotationen allein abgeleitet werden, zum Beispiel kausale Beziehungen oder domänenspezifische Invarianten.Wir stellen eine allgemeine Technik vor, um überwachtes Training mit Vorwissen zu ergänzen, das als Beziehungen zwischen Trainingsinstanzen ausgedrückt wird. Wir illustrieren die Methode an der Aufgabe der visuellen Beantwortung von Fragen, um verschiedene Hilfsannotationen zu nutzen, einschließlich Äquivalenz- und logische Entailment-Relationen zwischen Fragen.Bestehende Methoden zur Nutzung dieser Annotationen, einschließlich Hilfsverlusten und Datenerweiterung, können die strikte Einbeziehung dieser Relationen in das Modell nicht garantieren, da sie eine sorgfältige Abwägung mit dem End-to-End-Ziel erfordern. Unsere Methode verwendet diese Relationen, um den Einbettungsraum des Modells zu formen, und behandelt sie als strenge Einschränkungen für die gelernten Repräsentationen. Das resultierende Modell kodiert Relationen, die sich besser über Instanzen hinweg verallgemeinern lassen. Im Kontext der VQA bringt dieser Ansatz signifikante Verbesserungen in Bezug auf Genauigkeit und Robustheit, insbesondere gegenüber der üblichen Praxis, die Einschränkungen als weiche Regularisierer einzubeziehen. Wir zeigen außerdem, dass die Einbeziehung dieser Art von Vorwissen in unsere Methode zu konsistenten Verbesserungen führt, unabhängig von der Menge der verwendeten überwachten Daten, und demonstrieren damit den Wert eines zusätzlichen Trainingssignals, das ansonsten nur schwer aus End-to-End-Annotationen extrahiert werden kann.
KÃ?nstliche neuronale Netze haben in den letzten Jahren viele Bereiche der Informatik revolutioniert, da sie LÃ¶sungen fÃ?r eine Reihe bisher ungelÃ¶ster Probleme bieten.â€œ Andererseits existieren fÃ?r viele Probleme klassische Algorithmen, die typischerweise die Genauigkeit und StabilitÃ?t neuronaler Netze Ã?bertreffen.â€œ Um diese beiden Konzepte zu kombinieren, stellen wir eine neue Art von neuronalen Netzen vor â€" algorithmische neuronale Netze (AlgoNets).â€œ Diese Netze integrieren glatte Versionen klassischer Algorithmen in die Topologie neuronaler Netze.â€œ Unser neuartiges rekonstruktives adverses Netz (RAN) ermÃ¶glicht die LÃ¶sung inverser Probleme ohne oder mit nur schwacher Ãœberwachung.
In dieser Arbeit konzentrieren wir uns auf die schwach überwachte Lokalisierung (weakly supervised localization, WSL), bei der ein Modell trainiert wird, um ein Bild zu klassifizieren und Regionen von Interesse auf Pixelebene zu lokalisieren, wobei nur globale Bildannotationen verwendet werden.Typische konvolutionale Aufmerksamkeitskarten führen zu einer hohen Anzahl falsch positiver Regionen. Um dieses Problem zu lindern, schlagen wir eine neue Deep-Learning-Methode für WSL vor, die aus einem Lokalisierer und einem Klassifikator besteht, wobei der Lokalisierer darauf beschränkt ist, relevante und irrelevante Regionen mithilfe der bedingten Entropie (CE) zu bestimmen, um falsch positive Regionen zu reduzieren. Experimentelle Ergebnisse auf einem öffentlichen medizinischen Datensatz und zwei natürlichen Datensätzen, die den Dice-Index verwenden, zeigen, dass unser Vorschlag im Vergleich zu den aktuellen WSL-Methoden signifikante Verbesserungen in Bezug auf die Klassifizierung auf Bildebene und die Lokalisierung auf Pixelebene (niedrige Falsch-Positiv-Werte) bei gleichzeitiger Robustheit gegenüber Überanpassung bieten kann.Eine öffentliche, reproduzierbare PyTorch-Implementierung wird bereitgestellt.
Modellbasiertes Verstärkungslernen hat sich empirisch als erfolgreiche Strategie zur Verbesserung der Sample-Effizienz erwiesen.Dyna-Architektur, als eine elegante modellbasierte Architektur, die Lernen und Planung integriert, bietet enorme Flexibilität bei der Verwendung eines Modells.Eine der wichtigsten Komponenten in Dyna heißt Suchsteuerung, die sich auf den Prozess der Erzeugung von Zuständen oder Zustands-Aktions-Paaren bezieht, aus denen wir das Modell abfragen, um simulierte Erfahrungen zu sammeln.Suchsteuerung ist entscheidend für die Verbesserung der Lerneffizienz.In dieser Arbeit schlagen wir eine einfache und neuartige Suchsteuerungsstrategie vor, indem wir eine Hochfrequenzregion auf der Wertfunktion suchen. Unsere Hauptintuition basiert auf dem Shannon-Sampling-Theorem aus der Signalverarbeitung, das besagt, dass ein hochfrequentes Signal mehr Samples benötigt, um rekonstruiert zu werden.Wir zeigen empirisch, dass eine hochfrequente Funktion schwieriger zu approximieren ist.Dies legt eine Such-Kontroll-Strategie nahe: Wir sollten Zustände im hochfrequenten Bereich der Wertfunktion verwenden, um das Modell abzufragen, um mehr Samples zu erhalten. Wir entwickeln eine einfache Strategie zur lokalen Messung der Häufigkeit einer Funktion durch die Gradientennorm und liefern eine theoretische Begründung für diesen Ansatz, wenden unsere Strategie zur Suchkontrolle in Dyna an und führen Experimente durch, um ihre Eigenschaften und ihre Effektivität in Benchmark-Domänen zu zeigen.
Die vorgeschlagene Architektur, die wir als Distributed Recurrent Autoencoder for Scalable Image Compression (DRASIC) bezeichnen, ist in der Lage, verteilte Encoder und einen gemeinsamen Decoder auf korrelierten Datenquellen zu trainieren. Bei 10 verteilten Quellen erreicht unser verteiltes System eine bemerkenswerte Leistung innerhalb von 2 dB Peak-Signal-Rausch-Verhältnis (PSNR) gegenüber einem einzelnen Codec, der mit allen Datenquellen trainiert wurde. Wir experimentieren mit verteilten Quellen mit verschiedenen Korrelationen und zeigen, wie unsere Methode gut mit dem Slepian-Wolf-Theorem in Distributed Source Coding (DSC) übereinstimmt.Unsere Methode ist auch robust gegenüber dem Fehlen von kodierten Daten aus einer Reihe von verteilten Quellen.Darüber hinaus ist es skalierbar in dem Sinne, dass Codes können gleichzeitig auf mehr als eine Kompression Qualitätsstufe dekodiert werden.Nach unserem Wissen ist dies die erste datengesteuerte DSC Rahmen für allgemeine verteilte Code-Design mit Deep Learning.
Netzwerke mit langem Kurzzeitgedächtnis (LSTMs) wurden eingeführt, um verschwindende Gradienten in einfachen rekurrenten neuronalen Netzwerken (S-RNNs) zu bekämpfen, indem sie mit additiven rekurrenten Verbindungen, die durch Gates gesteuert werden, erweitert wurden.Wir präsentieren eine alternative Sichtweise, um den Erfolg von LSTMs zu erklären: die Gates selbst sind leistungsstarke rekurrente Modelle, die mehr Repräsentationskraft bieten, als bisher geschätzt wurde. Wir tun dies, indem wir zeigen, dass die Gatter der LSTMs von den eingebetteten S-RNNs entkoppelt werden können, wodurch eine eingeschränkte Klasse von RNNs entsteht, bei denen die Hauptrekursion eine elementweise gewichtete Summe von kontextunabhängigen Funktionen der Eingänge berechnet.Experimente mit einer Reihe von anspruchsvollen NLP-Problemen zeigen, dass die vereinfachten gatterbasierten Modelle wesentlich besser als S-RNNs und oft genauso gut wie die ursprünglichen LSTMs funktionieren, was stark darauf hindeutet, dass die Gatter in der Praxis viel mehr tun als nur verschwindende Gradienten zu mildern.
Von Algorithmen des maschinellen Lernens zur Charakterisierung, Überwachung und Beeinflussung der menschlichen Gesundheit (ML4H) wird erwartet, dass sie sicher und zuverlässig arbeiten, wenn sie in großem Maßstab und möglicherweise außerhalb strenger menschlicher Aufsicht eingesetzt werden. Diese Anforderung rechtfertigt eine strengere Beachtung von Fragen der Reproduzierbarkeit als in anderen Bereichen des maschinellen Lernens.In dieser Arbeit führen wir eine systematische Bewertung von über 100 kürzlich veröffentlichten ML4H-Forschungsarbeiten entlang mehrerer von uns identifizierter Dimensionen im Zusammenhang mit der Reproduzierbarkeit durch.Wir stellen fest, dass der Bereich ML4H im Vergleich zu etablierteren Bereichen des maschinellen Lernens schlecht abschneidet, insbesondere was die Zugänglichkeit von Daten und Code betrifft.  Abschließend schlagen wir Empfehlungen für Datenanbieter, akademische Verlage und die ML4H-Forschungsgemeinschaft vor, um reproduzierbare Forschung in Zukunft zu fördern.
Wir schlagen eine Lösung für die Bewertung von mathematischen Ausdrücken vor, aber anstatt ein einzelnes End-to-End-Modell zu entwerfen, schlagen wir eine Architektur im Stil von Legosteinen vor: Statt ein komplexes neuronales End-to-End-Netz zu trainieren, können viele kleine Netze unabhängig voneinander trainiert werden, von denen jedes eine bestimmte Operation ausführt und als einzelner Legostein fungiert; schwierigere oder komplexere Aufgaben können dann mit einer Kombination dieser kleineren Netze gelöst werden. Wir zeigen dann, dass verschiedene Operationen können einfach durch die Wiederverwendung dieser kleineren networks.As ein Beispiel, das wir wiederverwenden diese kleineren Netze zu entwickeln, größer und eine komplexere Netzwerk zu lösen n-stellige Multiplikation, n-stellige Division und Kreuzprodukt.This Bottom-up-Strategie nicht nur die Wiederverwendbarkeit, wir zeigen auch, dass es für Berechnungen mit n-Zahlen zu verallgemeinern und wir zeigen Ergebnisse für bis zu 7-stellige Zahlen.Unlike bestehenden Methoden, unsere Lösung auch für positive als auch negative Zahlen verallgemeinert.
Wir argumentieren, dass der Generator gleichzeitig auch die Wahrscheinlichkeit verringern sollte, dass die echten Daten echt sind, weil1) dies das a priori Wissen berücksichtigen würde, dass die Hälfte der Daten in der Mini-Batch gefälscht ist,2) dies mit Divergenzminimierung beobachtet werden würde, und3) in optimalen Einstellungen SGANs äquivalent zu Integral Probability Metric (IPM) GANs wären. Wir zeigen, dass diese Eigenschaft durch die Verwendung eines relativistischen Diskriminators herbeigeführt werden kann, der die Wahrscheinlichkeit schätzt, dass die gegebenen realen Daten realistischer sind als eine zufällige Stichprobe von gefälschten Daten. Wir verallgemeinern beide Ansätze auf Nicht-Standard-GAN-Verlustfunktionen und bezeichnen sie als Relativistische GANs (RGANs) bzw. Relativistische Durchschnitts-GANs (RaGANs) Wir zeigen, dass IPM-basierte GANs eine Teilmenge von RGANs sind, die die Identitätsfunktion verwenden. Empirisch stellen wir fest, dass1) RGANs und RaGANs deutlich stabiler sind und qualitativ hochwertigere Datenproben erzeugen als ihre nichtrelativistischen Gegenstücke,2) Standard-RagANs mit Gradientenstrafe Daten von besserer Qualität als WGAN-GP erzeugen und dabei nur ein einziges Diskriminator-Update pro Generator-Update benötigen (was die Zeit, die zum Erreichen des Standes der Technik benötigt wird, um 400% reduziert), und3) RaGANs in der Lage sind, plausible hochauflösende Bilder (256x256) aus einer sehr kleinen Probe (N=2011) zu erzeugen, während GAN und LSGAN dies nicht können; diese Bilder sind von deutlich besserer Qualität als die von WGAN-GP und SGAN mit spektraler Normalisierung erzeugten Bilder. Der Code ist frei verfügbar auf https://github.com/AlexiaJM/RelativisticGAN.
Einige der erfolgreichsten Anwendungen von Deep Reinforcement Learning in anspruchsvollen Bereichen der diskreten und kontinuierlichen Steuerung haben Policy-Gradienten-Methoden in der On-Policy-Einstellung verwendet. Policy-Gradienten können jedoch unter einer großen Varianz leiden, die die Leistung einschränken kann, und erfordern in der Praxis eine sorgfältig abgestimmte Entropie-Regularisierung, um einen Policy-Kollaps zu verhindern.Als Alternative zu Policy-Gradienten-Algorithmen stellen wir V-MPO vor, eine On-Policy-Anpassung der Maximum a Posteriori Policy Optimization (MPO), die eine Policy-Iteration auf der Grundlage einer gelernten Zustandswertfunktion durchführt. Wir zeigen, dass V-MPO sowohl für die Atari-57- als auch für die DMLab-30-Benchmark-Suiten in der Multi-Task-Einstellung die bisher berichteten Ergebnisse übertrifft, und zwar zuverlässig ohne Wichtigkeitsgewichtung, Entropie-Regularisierung oder populationsbasierte Abstimmung von Hyperparametern auf den einzelnen DMLab- und Atari-Ebenen. V-MPO ist auch auf Probleme mit hochdimensionalen, kontinuierlichen Aktionsräumen anwendbar, was wir im Zusammenhang mit dem Erlernen der Steuerung von simulierten Humanoiden mit 22 Freiheitsgraden aus vollständigen Zustandsbeobachtungen und 56 Freiheitsgraden aus Pixelbeobachtungen sowie beispielhaften OpenAI Gym-Aufgaben demonstrieren, bei denen V-MPO wesentlich höhere asymptotische Punktzahlen als bisher berichtet erzielt.
Es gibt zahlreiche Arbeiten, die sich mit neuronalen Netzen befassen, die allgemeine Berechnungen nachahmen, aber diese Netze sind nicht in der Lage, auf Datenverteilungen zu verallgemeinern, die außerhalb ihres Trainingssets liegen.Wir untersuchen dieses Problem durch die Brille grundlegender Informatikprobleme: Sortierung und Graphenverarbeitung. Wir modifizieren den Maskierungsmechanismus eines Transformators, um ihm zu ermöglichen, rudimentäre Funktionen mit starker Generalisierung zu implementieren, und zeigen, dass er durch Überwachung lernt, die grundlegenden Unterprogramme, die diese Algorithmen umfassen, mit nahezu perfekter Genauigkeit numerisch zu berechnen, und dass er dieses Maß an Genauigkeit beibehält, während er auf ungesehene Daten und lange Sequenzen außerhalb der Trainingsverteilung generalisiert.
Meta-Learning ist eine vielversprechende Strategie für das Lernen, um effizient innerhalb neuer Aufgaben zu lernen, unter Verwendung von Daten, die aus einer Verteilung von Aufgaben gesammelt wurden.die Meta-Learning-Literatur hat sich jedoch bisher auf die Aufgabe segmentiert Einstellung konzentriert, wo zur Trainingszeit, Offline-Daten wird davon ausgegangen, dass nach der zugrunde liegenden Aufgabe aufgeteilt werden, und zur Testzeit, die Algorithmen sind optimiert, um in einer einzigen Aufgabe zu lernen.in dieser Arbeit ermöglichen wir die Anwendung von generischen Meta-Learning-Algorithmen, um Einstellungen, bei denen diese Aufgabe Segmentierung ist nicht verfügbar, wie kontinuierliche Online-Lernen mit einer zeitlich veränderlichen Aufgabe. Wir stellen Meta-Learning über Online-Changepoint-Analyse (MOCA) vor, einen Ansatz, der einen Meta-Learning-Algorithmus mit einem differenzierbaren Bayes'schen Schema zur Erkennung von Changepoints erweitert, das sowohl das Training als auch das Testen direkt auf Zeitreihendaten ermöglicht, ohne diese in diskrete Aufgaben zu segmentieren.
Menschen mit hochfrequentem Hörverlust sind auf Hörgeräte angewiesen, die Frequenzabsenkungsalgorithmen verwenden, die einen Teil der Töne aus dem Hochfrequenzband in das niedrigere Frequenzband verschieben, wo die Töne für die Betroffenen besser wahrnehmbar werden.Frikative Phoneme haben einen wichtigen Teil ihres Inhalts in hohen Frequenzbändern konzentriert.Es ist wichtig, dass der Frequenzabsenkungsalgorithmus genau für die Dauer eines frikativen Phonems aktiviert wird und zu allen anderen Zeiten ausgeschaltet bleibt. In diesem Beitrag stellen wir einen auf Deep Learning basierenden Algorithmus zur Erkennung von frikativen Phonemen vor, der eine Erkennungsverzögerung von Null aufweist und die höchste Erkennungsgenauigkeit für frikative Phoneme auf dem TIMIT Speech Corpus erreicht.alle berichteten Ergebnisse sind reproduzierbar und werden mit einem einfach zu verwendenden Code geliefert, der als Grundlage für zukünftige Forschung dienen könnte.
Sequenz-zu-Sequenz-Modelle mit weicher Aufmerksamkeit wurden erfolgreich auf eine Vielzahl von Problemen angewandt, aber ihr Dekodierungsprozess verursacht quadratische Zeit- und Raumkosten und ist für die Echtzeit-Sequenztransduktion ungeeignet.Um diese Probleme zu lösen, schlagen wir Monotonic Chunkwise Attention (MoChA) vor, die die Eingabesequenz adaptiv in kleine Teile aufteilt, über die weiche Aufmerksamkeit berechnet wird. Wir zeigen, dass Modelle, die MoChA verwenden, effizient mit Standard-Backpropagation trainiert werden können und gleichzeitig eine Online-Dekodierung in linearer Zeit zum Testzeitpunkt ermöglichen. Bei der Anwendung auf die Online-Spracherkennung erzielen wir Ergebnisse auf dem neuesten Stand der Technik und erreichen die Leistung eines Modells, das einen Offline-Mechanismus mit weicher Aufmerksamkeit verwendet.
Wir stellen einen Rahmen für die automatische Anordnung von Bildfeldern vor, der eine eingehende Analyse der Beziehung zwischen dem Datensatz und der Erlernbarkeit einer Klassifizierungsaufgabe unter Verwendung eines Faltungsneuronalen Netzwerks ermöglicht.Ein Bildfeld ist eine Gruppe von Pixeln, die sich in einem zusammenhängenden Bereich befinden, der in der Probe enthalten ist.Unsere vorläufigen experimentellen Ergebnisse zeigen, dass ein informiertes intelligentes Mischen von Feldern auf der Ebene der Probe das Training beschleunigen kann, indem wichtige Merkmale in frühen Phasen des Trainings aufgedeckt werden.Darüber hinaus führen wir systematische Experimente durch und weisen nach, dass die Verallgemeinerungsfähigkeiten von CNN nicht mit menschlich erkennbaren Merkmalen in den Trainingsproben korrelieren. Wir haben den Rahmen nicht nur genutzt, um zu zeigen, dass die räumliche Lokalisierung von Merkmalen innerhalb von Proben nicht mit der Generalisierung korreliert, sondern auch, um die Konvergenz zu beschleunigen und gleichzeitig eine ähnliche Generalisierungsleistung zu erzielen. Unter Verwendung mehrerer Netzwerkarchitekturen und Datensätze zeigen wir, dass die Anordnung von Bildregionen unter Verwendung des gegenseitigen Informationsmaßes zwischen benachbarten Patches CNNs ermöglicht, in einem Drittel der Gesamtschritte zu konvergieren, die erforderlich sind, um das gleiche Netzwerk ohne Patch-Anordnung zu trainieren.
Eine Methode zur Überwindung dieses Problems ist die Domänen-Randomisierung, bei der zu Beginn jeder Trainingsepisode einige Parameter der Umgebung randomisiert werden, so dass der Agent vielen möglichen Variationen ausgesetzt ist.Die Domänen-Randomisierung ist jedoch sehr ineffizient und kann zu Strategien mit hoher Varianz über Domänen hinweg führen. In dieser Arbeit formalisieren wir das Problem der Domänenrandomisierung und zeigen, dass die Minimierung der Lipschitz-Konstante der Politik in Bezug auf die Randomisierungsparameter zu einer geringen Varianz in den gelernten Politiken führt. Wir schlagen eine Methode vor, bei der der Agent nur auf einer Variation der Umgebung trainiert werden muss und seine gelernten Zustandsrepräsentationen während des Trainings reguliert werden, um diese Konstante zu minimieren.
Behauptungen aus den Bereichen Netzwerk-Neurowissenschaft und Connectomics legen nahe, dass topologische Modelle des Gehirns, die komplexe Netzwerke beinhalten, von besonderem Nutzen und Interesse sind.Der Bereich der tiefen neuronalen Netze hat die Inspiration durch diese Behauptungen meist außen vor gelassen.In diesem Papier schlagen wir drei Architekturen vor und verwenden jede von ihnen, um die Schnittmenge von Netzwerk-Neurowissenschaft und Deep Learning zu erforschen, in einem Versuch, die Lücke zwischen den beiden Bereichen zu überbrücken. Mit Hilfe der Lehren aus der Netzwerk-Neurowissenschaft und Connectomics zeigen wir Verbesserungen gegenüber der ResNet-Architektur, wir zeigen einen möglichen Zusammenhang zwischen frühem Training und den spektralen Eigenschaften des Netzwerks, und wir zeigen die Trainierbarkeit eines DNN, das auf dem neuronalen Netzwerk von C. Elegans basiert.
Die Erstellung einer genauen, aktuellen und vollständigen Wissensdatenbank bleibt trotz erheblicher Anstrengungen im Bereich der automatischen Wissensdatenbankerstellung eine große Herausforderung.  In dieser Arbeit stellen wir Alexandria vor - ein System für die unbeaufsichtigte, hochpräzise Konstruktion von Wissensdatenbanken. Alexandria verwendet ein probabilistisches Programm, um einen Prozess der Umwandlung von Fakten aus der Wissensdatenbank in unstrukturierten Text zu definieren.  Die Verwendung eines probabilistischen Programms ermöglicht es, die Unsicherheit im Text auf die abgerufenen Fakten zu übertragen, was die Genauigkeit erhöht und die Zusammenführung von Fakten aus verschiedenen Quellen erleichtert.Da Alexandria keine gelabelten Trainingsdaten benötigt, können Wissensdatenbanken mit einem Minimum an manuellen Eingaben erstellt werden.Wir demonstrieren dies, indem wir eine hochpräzise (typischerweise 97%+) Wissensdatenbank für Personen aus einem einzigen Seed-Faktor erstellen.
Aufbauend auf den jüngsten Fortschritten, schlagen wir eine neue tiefe komplex-bewertete Methode für die Signalsuche und -extraktion im Frequenzbereich vor. als Fallstudie führen wir eine Audio-Quellentrennung im Fourier-Bereich durch. unsere neue Methode macht sich das Faltungs-Theorem zunutze, das besagt, dass die Fourier-Transformation zweier gefalteter Signale das elementweise Produkt ihrer Fourier-Transformationen ist. Unsere neue Methode basiert auf einer komplexwertigen Version der Feature-Wise Linear Modulation (FiLM) und dient als Grundpfeiler unserer vorgeschlagenen Methode zur Signalextraktion.Wir führen auch einen neuen und expliziten Amplituden- und Phasen-bewussten Verlust ein, der skalen- und zeitinvariant ist und die komplexwertigen Komponenten des Spektrogramms berücksichtigt.Anhand des Wall Street Journal-Datensatzes verglichen wir unseren phasenbewussten Verlust mit mehreren anderen, die sowohl im Zeit- als auch im Frequenzbereich arbeiten, und demonstrieren die Effektivität unserer vorgeschlagenen Methode zur Signalextraktion und des vorgeschlagenen Verlusts.
Wir schlagen eine GNN-Implementierung vor, die Bewegungsabläufe aus beobachteten Schwarmtrajektoriendaten vorhersagt und imitiert. Die Fähigkeit des Netzwerks, die Interaktionsdynamik in Schwärmen zu erfassen, wird durch Transferlernen demonstriert. Schließlich diskutieren wir die inhärente Verfügbarkeit und die Herausforderungen bei der Skalierbarkeit von GNN und schlagen eine Methode vor, um sie durch schichtweises Abstimmen und Mischen von Daten durch Auffüllen zu verbessern.
Einbettungsschichten werden üblicherweise verwendet, um diskrete Symbole in kontinuierliche Einbettungsvektoren abzubilden, die ihre semantischen Bedeutungen widerspiegeln. Trotz ihrer Effektivität steigt die Anzahl der Parameter in einer Einbettungsschicht linear mit der Anzahl der Symbole und stellt eine kritische Herausforderung für Speicher- und Aufbewahrungsbeschränkungen dar.In dieser Arbeit schlagen wir einen generischen und durchgängig lernfähigen Kompressionsrahmen vor, der als differenzierbare Produktquantisierung (DPQ) bezeichnet wird. Wir stellen zwei Instanziierungen von DPQ vor, die verschiedene Approximationstechniken nutzen, um Differenzierbarkeit im End-to-End-Lernen zu ermöglichen. Unsere Methode kann ohne weiteres als Drop-in-Alternative für jede bestehende Einbettungsschicht dienen. Empirisch bietet DPQ signifikante Kompressionsraten (14-238x) bei vernachlässigbaren oder keinen Leistungskosten auf 10 Datensätzen über drei verschiedene Sprachaufgaben.
Für mehrwertige Funktionen - z. B. wenn die bedingte Verteilung auf die Ziele angesichts der Eingaben multimodal ist - sind standardmäßige Regressionsansätze nicht immer wünschenswert, da sie den bedingten Mittelwert liefern.Modale Regressionsansätze zielen stattdessen darauf ab, den bedingten Modus zu finden, sind aber auf nichtparametrische Ansätze beschränkt. Solche Ansätze können schwer zu skalieren sein und erschweren die Nutzung parametrischer Funktionsannäherungen, wie z.B. neuronaler Netze, die komplexe Beziehungen zwischen Eingaben und Zielen erlernen können.In dieser Arbeit schlagen wir einen parametrischen modalen Regressionsalgorithmus vor, indem wir das implizite Funktionstheorem verwenden, um ein Ziel für das Lernen einer gemeinsamen parametrisierten Funktion über Eingaben und Ziele zu entwickeln. Wir demonstrieren empirisch an mehreren synthetischen Problemen, dass unsere Methode(i) mehrwertige Funktionen lernen und die bedingten Modi erzeugen kann,(ii) gut auf hochdimensionale Eingaben skaliert und(iii) sogar noch effektiver für bestimmte unimodale Probleme ist, insbesondere für Hochfrequenzdaten, bei denen die gemeinsame Funktion über Eingaben und Ziele die komplexe Beziehung zwischen ihnen besser erfassen kann.
Deep Reinforcement Learning Algorithmen benötigen große Mengen an Erfahrung, um eine individuelle Aufgabe zu erlernen. Während im Prinzip Meta-Reinforcement Learning (Meta-RL) Algorithmen es Agenten ermöglichen, neue Fähigkeiten aus kleinen Mengen an Erfahrung zu erlernen, schließen mehrere große Herausforderungen ihre Praxistauglichkeit aus. Ihnen fehlen auch Mechanismen, um bei der Anpassung an neue Aufgaben auf die Unsicherheit der Aufgabe zu schließen, was ihre Effektivität bei spärlichen Belohnungsproblemen einschränkt.In diesem Papier gehen wir diese Herausforderungen an, indem wir einen Meta-RL-Algorithmus entwickeln, der Aufgabeninferenz und Kontrolle entkoppelt. In unserem Ansatz führen wir online probabilistische Filterung von latenten Aufgabenvariablen durch, um aus kleinen Erfahrungsmengen abzuleiten, wie eine neue Aufgabe zu lösen ist.Diese probabilistische Interpretation ermöglicht posteriores Sampling für strukturierte und effiziente Exploration.Wir zeigen, wie diese Aufgabenvariablen mit Off-Policy-RL-Algorithmen integriert werden können, um sowohl Meta-Training als auch Adaptionseffizienz zu erreichen.Unsere Methode übertrifft frühere Algorithmen in der Sampling-Effizienz um das 20-100fache sowie in der asymptotischen Leistung bei mehreren Meta-RL-Benchmarks.
Wissensdatenbanken, riesige Sammlungen von Fakten (RDF-Triples) zu verschiedenen Themen, unterstützen wichtige moderne Anwendungen, aber die bestehenden Wissensdatenbanken enthalten nur sehr wenige Daten im Vergleich zu der Fülle an Informationen im Web, da der Industriestandard bei der Erstellung und Erweiterung von Wissensdatenbanken unter einem ernsthaften Engpass leidet: Sie sind auf Domänenexperten angewiesen, um geeignete Webquellen zur Datenextraktion zu identifizieren. Die Bemühungen, die Wissensextraktion vollständig zu automatisieren, haben diesen Standard nicht verbessert: Diese automatisierten Systeme sind zwar in der Lage, viel mehr Daten und aus einem breiteren Spektrum von Quellen abzurufen, aber sie leiden unter einer sehr geringen Genauigkeit und Wiederauffindbarkeit, so dass diese groß angelegten Extraktionen ungenutzt bleiben. In diesem Beitrag stellen wir MIDAS vor, ein System, das die Ergebnisse automatisierter Wissensextraktionspipelines nutzt, um den Engpass in industriellen Prozessen der Wissenserstellung und -erweiterung zu beheben.MIDAS automatisiert den Vorschlag qualitativ hochwertiger Webquellen und beschreibt, was im Hinblick auf die Erweiterung einer bestehenden Wissensbasis zu extrahieren ist.Wir leisten drei wichtige Beiträge. Erstens führen wir ein neuartiges Konzept, Web-Source-Slices, um den Inhalt einer Web-Quelle zu beschreiben.Zweitens definieren wir eine Gewinn-Funktion, um den Wert eines Web-Source-Slice in Bezug auf die Erweiterung einer bestehenden Wissensbasis zu quantifizieren.Drittens entwickeln wir effektive und hoch skalierbare Algorithmen, um High-Profit-Web-Source-Slices abzuleiten.Wir zeigen, dass MIDAS produziert High-Profit-Ergebnisse und übertrifft die Grundlinien deutlich auf beiden realen Wort und synthetische Datensätze.
Wir untersuchen das Problem der Vorhersage von Übereinstimmungen, bei dem versucht wird, die Wahrscheinlichkeit abzuschätzen, mit der eine Gruppe von M Elementen gegenüber einer anderen bevorzugt wird, und zwar auf der Grundlage von Teilgruppen-Vergleichsdaten.In der Praxis ergeben sich Herausforderungen, da die vorhandenen hochmodernen Algorithmen auf bestimmte statistische Modelle zugeschnitten sind und wir in verschiedenen Szenarien unterschiedliche beste Algorithmen haben. Dies erfordert einen einheitlichen Ansatz, der universell auf eine breite Palette von Szenarien angewendet werden kann und eine gleichbleibend hohe Leistung erzielt.zu diesem Zweck integrieren wir Deep-Learning-Architekturen, um die wichtigsten strukturellen Merkmale zu reflektieren, die die meisten State-of-the-Art-Algorithmen, von denen einige in bestimmten Einstellungen optimal sind, gemeinsam haben. Dies ermöglicht es uns, verborgene Modelle abzuleiten, die einem gegebenen Datensatz zugrunde liegen und die Interaktionen innerhalb einer Gruppe sowie statistische Vergleichsmuster steuern, und somit den besten Algorithmus zu entwickeln, der auf den jeweiligen Datensatz zugeschnitten ist.Durch umfangreiche Experimente mit synthetischen und realen Datensätzen bewerten wir unseren Rahmen im Vergleich zu den modernsten Algorithmen. Es stellt sich heraus, dass unser Rahmenwerk über alle Datensätze hinweg konsistent zu den besten Leistungen in Bezug auf den Kreuzentropieverlust und die Vorhersagegenauigkeit führt, während die State-of-the-Art-Algorithmen unter inkonsistenten Leistungen über verschiedene Datensätze hinweg leiden.Darüber hinaus zeigen wir, dass es leicht erweitert werden kann, um zufriedenstellende Leistungen bei Rangaggregationsaufgaben zu erreichen, was darauf hindeutet, dass es auch für andere Aufgaben anpassbar ist.
Rekurrente Neuronale Netze (RNNs) wurden entwickelt, um sequentielle Daten zu verarbeiten, leiden aber unter verschwindenden oder explodierenden Gradienten.  Jüngste Arbeiten über Unitary Recurrent Neural Networks (uRNNs) wurden verwendet, um dieses Problem zu lösen und in einigen Fällen die Fähigkeiten von Long Short-Term Memory-Netzwerken (LSTMs) zu übertreffen.  Wir schlagen ein einfacheres und neuartiges Aktualisierungsschema vor, um orthogonale rekurrente Gewichtsmatrizen zu erhalten, ohne komplexwertige Matrizen zu verwenden, und zwar durch Parametrisierung mit einer schräg-symmetrischen Matrix unter Verwendung der Cayley-Transformation, die nicht in der Lage ist, Matrizen mit negativen Eigenwerten darzustellen.  In mehreren Experimenten erzielt das vorgeschlagene skalierte Cayley orthogonale rekurrente neuronale Netzwerk (scoRNN) bessere Ergebnisse mit weniger trainierbaren Parametern als andere unitäre RNNs.
Diese scheinbar sehr unterschiedlichen Aufgaben werden in der Regel durch speziell entworfene Architekturen gelöst. In diesem Papier stellen wir die einfache Einsicht, dass eine große Vielfalt von Aufgaben in einem einzigen einheitlichen Format dargestellt werden kann, das aus Beschriftungsspannen und Beziehungen zwischen Spannen besteht, so dass ein einziges aufgabenunabhängiges Modell für verschiedene Aufgaben verwendet werden kann. Wir führen umfangreiche Experimente durch, um diese Einsicht an 10 unterschiedlichen Aufgaben zu testen, die so breit gefächert sind wie Dependency Parsing (Syntax), semantische Rollenbeschriftung (Semantik), Beziehungsextraktion (Informationsgehalt), aspektbasierte Stimmungsanalyse (Sentiment) und viele andere, und erreichen dabei eine vergleichbare Leistung wie spezialisierte Modelle auf dem neuesten Stand der Technik.Wir demonstrieren außerdem die Vorteile des Multi-Task-Lernens.Wir konvertieren diese Datensätze in ein einheitliches Format, um einen Benchmark zu erstellen, der eine ganzheitliche Testumgebung für die Evaluierung zukünftiger Modelle für die verallgemeinerte natürliche Sprachanalyse bietet.
Mit der Verwendung von GPs als Bausteine für immer anspruchsvollere Bayesian Deep Learning-Modelle ist die Beseitigung dieser Hindernisse ein notwendiger Schritt, um Ergebnisse in großem Maßstab zu erzielen.Wir präsentieren eine Variationsannäherung für eine breite Palette von GP-Modellen, die keine Matrixinversion bei jedem Optimierungsschritt erfordert. Wir beweisen, dass unsere Schranke die gleichen Garantien bietet wie frühere Variationsapproximationen. Wir demonstrieren experimentell einige vorteilhafte Eigenschaften der Schranke, obwohl signifikante Verbesserungen der Wanduhrzeit zukünftige Verbesserungen in der Optimierung und Implementierung erfordern.
Es hat sich gezeigt, dass die Verwendung von geometrischen Räumen mit Krümmung ungleich Null anstelle von einfachen euklidischen Räumen mit Krümmung Null die Leistung bei einer Reihe von Aufgaben des maschinellen Lernens zum Erlernen von Repräsentationen verbessert.Jüngste Arbeiten haben diese Geometrien genutzt, um latente Variablenmodelle wie Variationale Autoencoder (VAEs) in sphärischen und hyperbolischen Räumen mit konstanter Krümmung zu erlernen. ~Wir entwickeln einen Variations-Autoencoder mit gemischter Krümmung, einen effizienten Weg, um einen VAE zu trainieren, dessen latenter Raum ein Produkt aus Riemannschen Mannigfaltigkeiten mit konstanter Krümmung ist, wobei die Krümmung pro Komponente gelernt werden kann, was den euklidischen VAE auf gekrümmte latente Räume verallgemeinert, da das Modell im Wesentlichen auf den euklidischen VAE reduziert wird, wenn die Krümmungen aller Komponenten des latenten Raums auf 0 gehen.
Wir stellen uns die molekulare Optimierung als ein Übersetzungsproblem vor, bei dem das Ziel darin besteht, eine Eingabeverbindung auf eine Zielverbindung mit verbesserten biochemischen Eigenschaften abzubilden. bemerkenswerterweise stellen wir fest, dass sich die Eigenschaften der Moleküle mit jedem Schritt verbessern, wenn die erzeugten Moleküle iterativ in den Übersetzer zurückgeführt werden. wir zeigen, dass dieses Ergebnis unabhängig von der Wahl des Übersetzungsmodells ist, was diesen Algorithmus zu einer "Black Box" macht. Wir nennen diese Methode Black Box Recursive Translation (BBRT), eine neue Inferenzmethode für die Optimierung von Moleküleigenschaften.diese einfache, leistungsstarke Technik arbeitet streng auf die Eingänge und Ausgänge eines Übersetzungsmodells.wir erhalten neue State-of-the-Art-Ergebnisse für Moleküleigenschaften Optimierungsaufgaben mit unseren einfachen Drop-in-Ersatz mit bekannten Sequenz und Graph-basierte Modelle.unsere Methode bietet eine deutliche Leistungssteigerung im Vergleich zu seinen nicht-rekursiven Kollegen mit nur einem einfachen "``for"-Schleife.Darüber hinaus ist BBRT hoch interpretierbar, so dass Benutzer, um die Entwicklung der neu entdeckten Verbindungen von bekannten Ausgangspunkten abbilden.
Deep Neural Networks (DNNs) werden aufgrund ihrer überragenden Leistung zunehmend in Cloud-Servern und autonomen Agenten eingesetzt, wobei die DNNs je nach Anwendung entweder in einer White-Box-Einstellung (die Interna des Modells sind öffentlich bekannt) oder in einer Black-Box-Einstellung (nur die Modellausgaben sind bekannt) genutzt werden. Wir schlagen BlackMarks vor, das erste End-to-End-Multi-Bit-Wasserzeichen-Framework, das im Blackbox-Szenario anwendbar ist.BlackMarks nimmt das vortrainierte, unmarkierte Modell und die binäre Signatur des Eigentümers als Eingaben.Die Ausgabe ist das entsprechende markierte Modell mit spezifischen Schlüsseln, die später zum Auslösen des eingebetteten Wasserzeichens verwendet werden können. Dazu entwirft BlackMarks zunÃ?chst ein modellabhÃ?ngiges Kodierungsschema, das alle mÃ¶glichen Klassen in der Aufgabe auf Bit â€˜0â€™ und Bit â€˜1â€™ abbildet.â€œ Ausgehend von der Signatur des EigentÃ?mers (einer binÃ?ren Zeichenkette) wird ein Satz von SchlÃ?sselbild- und Label-Paaren mit Hilfe gezielter Angriffe entworfen.â€œ Das Wasserzeichen (WM) wird dann in der Verteilung der Ausgangsaktivierungen des DNN durch Feinabstimmung des Modells mit einem WM-spezifischen regulierten Verlust kodiert. Um das WM zu extrahieren, fragt BlackMarks das Modell mit den WM-Schlüsselbildern ab und dekodiert die Signatur des Eigentümers aus den entsprechenden Vorhersagen unter Verwendung des entworfenen Kodierungsschemas.Wir führen eine umfassende Bewertung der Leistung von BlackMarks auf MNIST-, CIFAR-10- und ImageNet-Datensätzen durch und bestätigen seine Effektivität und Robustheit.BlackMarks bewahrt die Funktionalität des ursprünglichen DNN und verursacht einen vernachlässigbaren WM-Einbettungs-Overhead von nur 2,054 %.
Die äußere Minimierung versucht, einen robusten Klassifikator zu erlernen, während die innere Maximierung versucht, gegnerische Proben zu generieren. Leider ist ein solches Minmax-Problem aufgrund des Fehlens einer konvex-konkaven Struktur sehr schwer zu lösen.Diese Arbeit schlägt eine neue gegnerische Trainingsmethode vor, die auf einem allgemeinen Learning-to-Learn-Rahmen basiert. Anstatt die bestehenden Hand-Design-Algorithmen für das innere Problem anzuwenden, lernen wir einen Optimierer, der als konvolutionelles neuronales Netz parametrisiert ist, und lernen gleichzeitig einen robusten Klassifikator, um den durch den gelernten Optimierer generierten Angriff abzuwehren. Aus der Perspektive des generativen Lernens kann die von uns vorgeschlagene Methode als das Erlernen eines tiefen generativen Modells zur Generierung von gegnerischen Mustern betrachtet werden, das sich an die robuste Klassifizierung anpasst. Unsere Experimente zeigen, dass die von uns vorgeschlagene Methode die bestehenden Trainingsmethoden für gegnerische Muster in den Datensätzen CIFAR-10 und CIFAR-100 deutlich übertrifft.
Es basiert auf der einfachen Idee, Komponenten des zukünftigen Zustands, die vorhersehbar sind, von denen zu trennen, die von Natur aus unvorhersehbar sind, und die unvorhersehbaren Komponenten in eine niedrigdimensionale Variable zu kodieren, die in das Vorwärtsmodell eingespeist wird. Wir evaluieren es im Kontext der Videovorhersage auf mehreren Datensätzen und zeigen, dass es in der Lage ist, konsistent verschiedene Vorhersagen zu generieren, ohne dass eine alternierende Minimierung über einen latenten Raum oder ein adversariales Training erforderlich ist.
Die Durchführung von Reinforcement-Learning-Experimenten kann ein komplexer und zeitaufwändiger Prozess sein: Eine vollständige experimentelle Pipeline besteht in der Regel aus der Simulation einer Umgebung, der Implementierung eines oder mehrerer Lernalgorithmen, einer Vielzahl zusätzlicher Komponenten zur Erleichterung des Zusammenspiels zwischen Agent und Umgebung sowie der erforderlichen Analyse, Aufzeichnung und Protokollierung. Angesichts dieser Komplexität wird in diesem Beitrag simple_rl vorgestellt, eine neue Open-Source-Bibliothek für die Durchführung von Reinforcement-Learning-Experimenten in Python 2 und 3 mit dem Schwerpunkt auf Einfachheit. simple_rl soll nahtlose, reproduzierbare Methoden für die Durchführung von Reinforcement-Learning-Experimenten unterstützen.Dieser Beitrag gibt einen Überblick über die zentrale Design-Philosophie des Pakets, wie es sich von bestehenden Bibliotheken unterscheidet, und stellt seine zentralen Funktionen vor.
Wasserstein GAN (WGAN) ist ein Modell, das die Wasserstein-Distanz zwischen einer Datenverteilung und einer Stichprobenverteilung minimiert.Neuere Studien haben vorgeschlagen, den Trainingsprozess für WGAN zu stabilisieren und die Lipschitz-Beschränkung zu implementieren.In dieser Studie beweisen wir die lokale Stabilität der Optimierung der einfachen Gradientenstrafe $\mu$-WGAN (SGP $\mu$-WGAN) unter geeigneten Annahmen bezüglich des Gleichgewichts und des Strafmaßes $\mu$. Basierend auf dieser Analyse behaupten wir, dass die Bestrafung der Datenvielfalt oder der Probenvielfalt der Schlüssel zur Regularisierung der ursprünglichen WGAN mit einer Gradientenstrafe ist.Experimentelle Ergebnisse, die mit unintuitiven Strafmaßen erhalten wurden, die unsere Annahmen erfüllen, werden ebenfalls bereitgestellt, um unsere theoretischen Ergebnisse zu unterstützen.
Wir stellen die Random Partition Relaxation (RPR) vor, eine Methode zur starken Quantisierung der Parameter von faltbaren neuronalen Netzen auf binäre (+1/-1) und ternäre (+1/0/-1) Werte. Ausgehend von einem vortrainierten Modell quantisieren wir zunächst die Gewichte und entspannen dann zufällige Partitionen von ihnen auf ihre kontinuierlichen Werte für ein erneutes Training, bevor wir sie erneut quantisieren und zu einer anderen Gewichtspartition für die weitere Anpassung wechseln.  Wir evaluieren empirisch die Leistung von RPR mit ResNet-18, ResNet-50 und GoogLeNet bei der ImageNet-Klassifizierungsaufgabe für binäre und ternäre Gewichtsnetzwerke und zeigen Genauigkeiten, die über den Stand der Technik für binäres und ternäres GoogLeNet hinausgehen, sowie eine konkurrenzfähige Leistung für ResNet-18 und ResNet-50 unter Verwendung einer SGD-basierten Trainingsmethode, die leicht in bestehende Frameworks integriert werden kann.
Hierarchische rekurrente neuronale Netze (HRNNs) gelten als vielversprechender Ansatz, da langfristige Abhängigkeiten durch Abkürzungen in der Hierarchie aufgelöst werden, aber der Speicherbedarf von Truncated Backpropagation Through Time (TBPTT) verhindert nach wie vor das Training auf sehr langen Sequenzen. In diesem Beitrag zeigen wir empirisch, dass in (tiefen) HRNNs die Rückpropagierung von Gradienten von höheren zu niedrigeren Ebenen durch lokal berechenbare Verluste ersetzt werden kann, ohne die Lernfähigkeit des Netzes zu beeinträchtigen, und zwar über einen breiten Bereich von Aufgaben.
In einem typischen Deep-Learning-Ansatz für eine Computer-Vision-Aufgabe werden Convolutional Neural Networks (CNNs) verwendet, um Merkmale auf verschiedenen Abstraktionsebenen aus einem Bild zu extrahieren und eine hochdimensionale Eingabe durch eine Reihe von Transformationen in einen niedrigdimensionalen Entscheidungsraum zu komprimieren.In diesem Papier untersuchen wir, wie eine Klasse von Eingabebildern im Laufe dieser Transformationen komprimiert wird.Insbesondere verwenden wir die Singulärwertzerlegung, um die relevanten Variationen im Merkmalsraum zu analysieren. Wir zeigen, dass die effektive Dimension einer Klasse über verschiedene Datensätze und Architekturen hinweg zunimmt, bevor sie weiter in das Netzwerk hinein abnimmt, was auf eine Art anfänglicher Aufhellungstransformation hindeutet, und dass die Abnahmerate der effektiven Dimension tiefer im Netzwerk mit der Trainingsleistung des Modells übereinstimmt.
Deep-Learning-Methoden haben eine hohe Leistung bei der Erkennung von Geräuschen erreicht. Die Entscheidung, wie die Trainingsdaten eingegeben werden, ist wichtig für eine weitere Leistungsverbesserung.Wir schlagen eine neue Lernmethode für die tiefe Geräuscherkennung vor: Unsere Strategie besteht darin, einen diskriminativen Merkmalsraum zu lernen, indem wir die Klänge zwischen den Klassen als Klänge zwischen den Klassen erkennen.Wir erzeugen Klänge zwischen den Klassen, indem wir zwei Klänge, die zu verschiedenen Klassen gehören, mit einem zufälligen Verhältnis mischen.Wir geben dann den gemischten Klang in das Modell ein und trainieren das Modell, um das Mischungsverhältnis auszugeben.Die Vorteile des BC-Lernens beschränken sich nicht nur auf die Erhöhung der Variation der Trainingsdaten; BC-Lernen führt zu einer Vergrößerung des Fisherâ€™s-Kriteriums im Merkmalsraum und einer Regularisierung der Positionsbeziehung zwischen den Merkmalsverteilungen der Klassen. Die experimentellen Ergebnisse zeigen, dass BC-Lernen die Leistung auf verschiedenen Tonerkennungsnetzwerken, Datensätzen und Datenerweiterungsschemata verbessert, wobei sich BC-Lernen immer als vorteilhaft erweist. Darüber hinaus konstruieren wir ein neues tiefes Tonerkennungsnetzwerk (EnvNet-v2) und trainieren es mit BC-Lernen.
Die räumlich-zeitliche Vorhersage hat sich zu einer immer wichtigeren Aufgabe im Bereich des maschinellen Lernens und der Statistik entwickelt, da sie eine Vielzahl von Anwendungen wie Klimamodellierung, Verkehrsvorhersage, Video-Caching-Vorhersage usw. bietet. Obwohl zahlreiche Studien durchgeführt wurden, gehen die meisten Arbeiten davon aus, dass die Daten aus verschiedenen Quellen oder an verschiedenen Orten gleich zuverlässig sind. In diesem Beitrag schlagen wir eine neuartige Lösung vor, die automatisch auf die Datenqualität verschiedener Quellen durch lokale Variationen von räumlich-zeitlichen Signalen ohne explizite Kennzeichnungen schließen kann, und integrieren die Schätzung der Datenqualität mit Graph-Faltungsnetzen, um deren effiziente Strukturen zu nutzen.
Die menschliche Wahrnehmung von 3D-Formen geht über deren Rekonstruktion als Punktesatz oder Komposition geometrischer Primitive hinaus: Wir verstehen auch mühelos die Struktur von Formen auf höherer Ebene, wie z.B. die Wiederholung und reflektierende Symmetrie von Objektteilen. Im Gegensatz dazu konzentrieren sich die jüngsten Fortschritte in der 3D-Formerfassung mehr auf die Geometrie auf niedriger Ebene, aber weniger auf diese Beziehungen auf höherer Ebene. Da es für reale Formen keine Annotationen von Formprogrammen gibt, entwickeln wir neuronale Module, die nicht nur lernen, 3D-Formprogramme aus rohen, unannotierten Formen abzuleiten, sondern auch diese Programme zur Formrekonstruktion auszuführen. Nach anfänglichem Bootstrapping lernt unser differenzierbares End-to-End-Modell 3D-Formprogramme, indem es Formen auf selbstüberwachte Weise rekonstruiert. Experimente zeigen, dass unser Modell 3D-Formprogramme für hochkomplexe Formen aus verschiedenen Kategorien genau ableitet und ausführt. Es kann auch mit einem Bild-zu-Form-Modul integriert werden, um 3D-Formprogramme direkt aus einem RGB-Bild abzuleiten, was zu 3D-Formrekonstruktionen führt, die sowohl genauer als auch physikalisch plausibler sind.
Deep Reinforcement Learning (Deep RL) hat dank seiner ermutigenden Leistung bei einer Vielzahl von Steuerungsaufgaben immer mehr Aufmerksamkeit erhalten, doch herkömmliche Regularisierungstechniken beim Training neuronaler Netze (z.B., In dieser Arbeit präsentieren wir die erste umfassende Studie über Regularisierungstechniken mit mehreren Optimierungsalgorithmen für kontinuierliche Steuerungsaufgaben. Interessanterweise stellen wir fest, dass konventionelle Regularisierungstechniken für Policy-Netzwerke oft eine große Verbesserung der Aufgabenleistung bringen können, und die Verbesserung ist typischerweise signifikanter, wenn die Aufgabe schwieriger ist. Wir vergleichen auch mit der weit verbreiteten Entropie-Regularisierung und stellen fest, dass die $L_2$-Regularisierung im Allgemeinen besser ist.Unsere Ergebnisse sind außerdem robust gegenüber der Wahl der Trainings-Hyperparameter.Wir untersuchen auch die Auswirkungen der Regularisierung verschiedener Komponenten und stellen fest, dass nur die Regularisierung des Policy-Netzwerks in der Regel ausreichend ist.Wir hoffen, dass unsere Studie eine Anleitung für zukünftige Praktiken bei der Regularisierung von Policy-Optimierungsalgorithmen bietet.
Wir stellen FigureQA vor, ein visuelles Reasoning-Korpus mit über einer Million Frage-Antwort-Paaren, die auf über 100.000 Bildern basieren. Die Bilder sind synthetische, wissenschaftliche Abbildungen aus fünf Klassen: Liniendiagramme, Punkt-Linien-Diagramme, vertikale und horizontale Balkendiagramme und Kreisdiagramme. Wir formulieren unsere Schlussfolgerungsaufgabe, indem wir Fragen aus 15 Vorlagen generieren; die Fragen betreffen verschiedene Beziehungen zwischen Plot-Elementen und untersuchen Merkmale wie das Maximum, das Minimum, die Fläche unter der Kurve, die Glätte und die Überschneidung.Um solche Fragen zu lösen, muss man sich oft auf mehrere Plot-Elemente beziehen und Informationen synthetisieren, die räumlich über eine Abbildung verteilt sind. Um das Training von maschinellen Lernsystemen zu erleichtern, enthält der Korpus auch Nebendaten, die zur Formulierung von Hilfszielen verwendet werden können.Insbesondere stellen wir die numerischen Daten zur Verfügung, die zur Generierung jeder Figur verwendet werden, sowie Bounding-Box-Annotationen für alle Plot-Elemente.Wir untersuchen die vorgeschlagene visuelle Schlussfolgerung Aufgabe durch das Training mehrerer Modelle, einschließlich der kürzlich vorgeschlagenen Relation Network als starke Basis.Vorläufige Ergebnisse deuten darauf hin, dass die Aufgabe eine erhebliche maschinelles Lernen Herausforderung.Wir sehen FigureQA als einen ersten Schritt in Richtung der Entwicklung von Modellen, die intuitiv Muster aus visuellen Darstellungen von Daten erkennen können.
Ich unterscheide zwischen Prozesserklärungen, die die detaillierten Entscheidungen während der heuristischen Suche ansprechen, und Präferenzerklärungen, die die Reihenfolge der Alternativen unabhängig davon klären, wie sie generiert wurden.Ich stelle auch Hypothesen auf, welche Arten von Benutzern welche Arten von Erklärungen zu schätzen wissen.Darüber hinaus diskutiere ich drei Facetten der mehrstufigen Entscheidungsfindung - konzeptionelle Inferenz, Plangenerierung und Planausführung -, in denen Erklärungen entstehen können.Ich betrachte auch alternative Wege, um Fragen an Agenten zu stellen und sie ihre Antworten geben zu lassen.
Generatives Deep Learning hat eine neue Welle von Super-Resolution (SR)-Algorithmen ausgelöst, die Einzelbilder mit beeindruckenden ästhetischen Ergebnissen verbessern, wenn auch mit imaginären Details.Multi-Frame-Super-Resolution (MFSR) bietet einen fundierteren Ansatz für das ungelöste Problem, indem es auf mehreren niedrig aufgelösten Ansichten basiert. Dies ist wichtig für die Satellitenüberwachung des menschlichen Einflusses auf den Planeten - von der Abholzung bis hin zu Menschenrechtsverletzungen -, die von zuverlässigen Bildern abhängt. Zu diesem Zweck stellen wir HighRes-net vor, den ersten Deep-Learning-Ansatz für MFSR, der seine Teilaufgaben in einer End-to-End-Methode erlernt: (i) Co-Registrierung, (ii) Fusion, (iii) Up-Sampling und (iv) Registrierung-at-the-loss. Wir lernen einen globalen Fusionsoperator, der rekursiv auf eine beliebige Anzahl von niedrig aufgelösten Paaren angewandt wird.Wir führen einen Registrierungsverlust ein, indem wir lernen, die SR-Ausgabe durch ShiftNet an einer Bodenwahrheit auszurichten.Wir zeigen, dass wir durch das Lernen tiefer Repräsentationen mehrerer Ansichten niedrig aufgelöste Signale superauflösen und Erdbeobachtungsdaten in großem Maßstab verbessern können.Unser Ansatz hat kürzlich den MFSR-Wettbewerb der Europäischen Weltraumorganisation für reale Satellitenbilder gewonnen.
Ansätze, die eine eng gekoppelte exakte verteilte Mittelwertbildung auf der Basis von AllReduce verwenden, reagieren empfindlich auf langsame Knoten und Kommunikation mit hoher Latenz.In dieser Arbeit zeigen wir die Anwendbarkeit von Stochastic Gradient Push (SGP) für verteiltes Training.SGP verwendet einen Gossip-Algorithmus namens PushSum für die annähernde verteilte Mittelwertbildung, der eine viel lockerere Kommunikation ermöglicht, was in Szenarien mit hoher Latenz oder hoher Variabilität von Vorteil sein kann. Wir beweisen, dass SGP zu einem stationären Punkt von glatten, nicht-konvexen Zielfunktionen konvergiert und validieren das Potenzial von SGP empirisch. Bei der Verwendung von 32 Knoten mit 8 GPUs pro Knoten zum Trainieren von ResNet-50 auf ImageNet, bei dem die Knoten über 10Gbps Ethernet kommunizieren, schließt SGP 90 Epochen in etwa 1,5 Stunden ab, während AllReduce SGD über 5 Stunden benötigt, und die Top-1-Validierungsgenauigkeit von SGP bleibt innerhalb von 1,2% der mit AllReduce SGD erzielten Genauigkeit.
In diesem Papier, erweitern wir die Persona-basierte Sequenz-zu-Sequenz (Seq2Seq) neuronales Netz Gesprächsmodell zu einem Multi-Turn-Dialog-Szenario durch die Änderung der state-of-the-art hredGAN Architektur gleichzeitig erfassen Äußerung Attribute wie Sprecher Identität, Dialog Thema, Sprecher Gefühle usw. Das vorgeschlagene System, phredGAN hat eine Persona-basierte HRED-Generator (PHRED) und eine bedingte discriminator.we auch zwei Ansätze zu erreichen, die bedingte discriminator: (1) $phredGAN_a$, ein System, das die Attributrepräsentation als zusätzliche Eingabe in einen traditionellen adversen Diskriminator weitergibt, und (2) $phredGAN_d$, ein duales Diskriminatorsystem, das zusätzlich zum adversen Diskriminator kollaborativ das/die Attribut(e) vorhersagt, die die Eingabeäußerung generiert haben. Um die überlegene Leistung von phredGAN gegenüber dem Persona-SeqSeq-Modell zu demonstrieren, experimentieren wir mit zwei Gesprächsdatensätzen, dem Ubuntu Dialogue Corpus (UDC) und TV-Serientranskripten von The Big Bang Theory und Friends. Wir untersuchen auch die Kompromisse bei der Verwendung beider Varianten von $phredGAN$ auf Datensätzen mit vielen, aber schwachen Attributmodalitäten (wie bei Big Bang Theory und Friends) und solchen mit wenigen, aber starken Attributmodalitäten (Kunden-Agenten-Interaktionen im Ubuntu-Datensatz).
Um die Eigenschaften biologischer Systeme zu simulieren, fügen wir die Kosten hinzu, die lange Verbindungen und die Nähe von Neuronen in einem zweidimensionalen Raum bestrafen. Unsere Experimente zeigen, dass sich die Neuronen in dem Fall, in dem das Netzwerk zwei verschiedene Aufgaben ausführt, auf natürliche Weise in Cluster aufteilen, wobei jeder Cluster für die Verarbeitung einer anderen Aufgabe zuständig ist.
Der Transformator ist zu einem zentralen Modell fÃ?r viele NLP-Aufgaben geworden, von der Ãœbersetzung Ã?ber die Sprachmodellierung bis hin zum ReprÃ?sentationslernen.â€œ Sein Erfolg zeigt die EffektivitÃ?t der gestapelten Aufmerksamkeit als Ersatz fÃ?r die Rekursion fÃ?r viele Aufgaben.â€œ Theoretisch bietet die Aufmerksamkeit auch mehr Einblicke in die internen Entscheidungen des Modells; in der Praxis wird es jedoch schnell fast so vollstÃ?ndig vernetzt wie rekurrente Modelle.â€œ In dieser Arbeit schlagen wir eine alternative Transformator-Architektur vor, den diskreten Transformator, mit dem Ziel, die internen Modellentscheidungen besser zu separieren. Das Modell verwendet harte Aufmerksamkeit, um sicherzustellen, dass jeder Schritt nur von einem festen Kontext abhÃ?ngt.zusÃ?tzlich verwendet das Modell einen separaten â€žsyntaktischenâ€œ Controller, um die Netzwerkstruktur von der Entscheidungsfindung zu trennen.schlieÃŸlich zeigen wir, dass dieser Ansatz mit direkter Regularisierung weiter sparsam gemacht werden kann.empirisch ist dieser Ansatz in der Lage, das gleiche Leistungsniveau auf mehreren DatensÃ?tzen beizubehalten, wÃ?hrend er Argumentationsentscheidungen Ã?ber die Daten diskretisiert.
Wir bauen auf der PredNet-Implementierung von Lotter, Kreiman und Cox (2016) auf, um zu untersuchen, ob prädiktive Kodierungsrepräsentationen nützlich sind, um die Hirnaktivität im visuellen Kortex vorherzusagen.Wir verwenden die Repräsentationsähnlichkeitsanalyse (RSA), um PredNet-Repräsentationen mit funktioneller Magnetresonanztomographie (fMRI) und Magnetoenzephalographie (MEG) aus dem Algonauts-Projekt (Cichy et al, Im Gegensatz zu früheren Erkenntnissen in der Literatur (Khaligh-Razavi & Kriegeskorte, 2014) berichten wir über empirische Daten, die darauf hindeuten, dass unbeaufsichtigte Modelle, die zur Vorhersage von Videobildern ohne weitere Feinabstimmung trainiert wurden, überwachte Bildklassifizierungsgrundlagen in Bezug auf die Korrelation mit räumlichen (fMRI) und zeitlichen (MEG) Daten übertreffen können.
Solches Wissen wird oft durch die Verfügbarkeit von verwandten Daten aus ähnlichen Bereichen und Aufgaben wie der aktuellen Aufgabe integriert. Idealerweise möchte man sowohl die Daten für die aktuelle Aufgabe als auch für frühere verwandte Aufgaben zur Selbstorganisation des Lernsystems nutzen, so dass Gemeinsamkeiten und Unterschiede zwischen den Aufgaben datengesteuert gelernt werden. Wir entwickeln einen Rahmen für das gleichzeitige Lernen mehrerer Aufgaben, der auf der gemeinsamen Nutzung von Merkmalen beruht, die allen Aufgaben gemeinsam sind, was durch die Verwendung eines modularen tiefen neuronalen Feedforward-Netzwerks erreicht wird, das aus gemeinsamen Zweigen besteht, die sich mit den gemeinsamen Merkmalen aller Aufgaben befassen, und aus privaten Zweigen, die die spezifischen, einzigartigen Aspekte jeder Aufgabe lernen.Sobald eine geeignete Architektur für die Gewichtsteilung festgelegt wurde, erfolgt das Lernen durch Standardalgorithmen für Feedforward-Netzwerke, z. B, Die Methode befasst sich mit Meta-Lernen (wie Domänenanpassung, Transfer- und Multi-Task-Lernen) in einer einheitlichen Art und Weise und kann leicht mit Daten umgehen, die aus verschiedenen Arten von Quellen stammen.Numerische Experimente demonstrieren die Effektivität des Lernens in Domänenanpassungs- und Transfer-Lernkonfigurationen und liefern Beweise für die flexiblen und aufgabenorientierten Repräsentationen, die im Netzwerk entstehen.
Tiefe neuronale Netze und Entscheidungsbäume arbeiten mit weitgehend getrennten Paradigmen; typischerweise führen erstere das Repräsentationslernen mit vorgegebenen Architekturen durch, während letztere durch das Lernen von Hierarchien über vorgegebene Merkmale mit datengesteuerten Architekturen gekennzeichnet sind.Wir vereinen die beiden über adaptive neuronale Bäume (ANTs), ein Modell, das das Repräsentationslernen in Kanten, Routing-Funktionen und Blattknoten eines Entscheidungsbaums integriert, zusammen mit einem Backpropagation-basierten Trainingsalgorithmus, der die Architektur adaptiv aus primitiven Modulen (z.B. Faltungsschichten) wachsen lässt, Wir demonstrieren dies anhand von Klassifizierungs- und Regressionsaufgaben, indem wir eine Genauigkeit von über 99 % und 90 % bei den MNIST- und CIFAR-10-Datensätzen erreichen und Standard-Neuronale Netze, Random Forests und Gradient-Boosted-Bäume beim SARCOS-Datensatz übertreffen.Darüber hinaus passt die ANT-Optimierung die Architektur natürlich an die Größe und Komplexität der Trainingsdaten an.
Während sich Systeme zur Verarbeitung natürlicher Sprache oft auf eine einzige Sprache konzentrieren, hat das mehrsprachige Transferlernen das Potenzial, die Leistung zu verbessern, insbesondere bei Sprachen mit geringen Ressourcen. Wir stellen XLDA vor, eine Methode zur sprachübergreifenden Datenerweiterung, bei der ein Segment des Eingabetextes durch die Übersetzung in eine andere Sprache ersetzt wird. XLDA verbessert die Leistung aller 14 getesteten Sprachen des XNLI-Benchmarks (Cross-Lingual Natural Language Inference) mit Verbesserungen von bis zu 4,8. XLDA steht im Gegensatz zu einem naiveren Ansatz, der Beispiele in verschiedenen Sprachen so zusammenfasst, dass jedes Beispiel nur in einer Sprache vorliegt, und schneidet dabei deutlich besser ab.Bei der SQuAD-Aufgabe zur Beantwortung von Fragen zeigt sich, dass XLDA eine Leistungssteigerung von 1,0 auf dem englischen Evaluierungsset bietet.Umfassende Experimente deuten darauf hin, dass die meisten Sprachen als sprachenübergreifende Augmentoren effektiv sind, dass XLDA gegenüber einem breiten Spektrum an Übersetzungsqualität robust ist und dass XLDA für zufällig initialisierte Modelle sogar effektiver ist als für vortrainierte Modelle.
Das Training von konditionalen generativen Modellen mit latenter Variable ist eine Herausforderung in Szenarien, in denen das Konditionierungssignal sehr stark ist und der Decoder aussagekräftig genug ist, um eine plausible Ausgabe zu erzeugen, die nur die Bedingung berücksichtigt; das generative Modell neigt dazu, die latente Variable zu ignorieren und leidet unter einem posterioren Kollaps. Wir finden und zeigen empirisch, dass einer der Hauptgründe für den Posterior-Kollaps in der Art und Weise liegt, wie generative Modelle konditioniert werden, d.h. durch die Verkettung der latenten Variable und der Bedingung. Um dieses Problem zu entschärfen, schlagen wir vor, die latenten Variablen explizit von der Bedingung abhängig zu machen, indem wir die Konditionierung und das Sampling der latenten Variablen vereinheitlichen und sie so koppeln, dass das Modell die Wurzel der Variationen nicht verwirft. Um dies zu erreichen, entwickeln wir eine bedingte Variations-Autoencoder-Architektur, die nicht nur eine Verteilung der latenten Variablen, sondern auch der Bedingung erlernt, wobei letztere als Prior für die erstere fungiert. Unsere Experimente zu den anspruchsvollen Aufgaben der bedingten Vorhersage menschlicher Bewegungen und der Beschriftung von Bildern zeigen die Effektivität unseres Ansatzes bei der Vermeidung von Posterior-Kollaps. Die Videoergebnisse unseres Ansatzes werden anonymisiert unter http://bit.ly/iclr2020 zur Verfügung gestellt.
Wir schlagen eine Studie über die Stabilität verschiedener "few-shot"-Lernalgorithmen vor, die Variationen in den Hyperparametern und Optimierungsschemata unterworfen sind, während wir den zufälligen Seed kontrollieren.  Wir schlagen eine Methodik vor, um statistische Unterschiede in der Modellleistung bei mehreren Replikationen zu testen, wobei wir versuchen, die Ergebnisse von drei bekannten Arbeiten zu reproduzieren: Matching Nets, Prototypical Networks und TADAM.Wir analysieren den miniImagenet-Datensatz mit der Standard-Klassifizierungsaufgabe in der 5-Wege, 5-Schuss-Lerneinstellung zur Testzeit.Wir stellen fest, dass die ausgewählten Implementierungen Stabilität über zufällige Seeds und Wiederholungen aufweisen.
In solchen hierarchischen Strukturen löst ein übergeordneter Controller Aufgaben, indem er iterativ Ziele kommuniziert, die eine untergeordnete Policy erreichen soll. Dementsprechend ist die Wahl der Repräsentation - die Abbildung des Beobachtungsraumes auf den Zielraum - entscheidend.Um dieses Problem zu untersuchen, entwickeln wir einen Begriff der Suboptimalität einer Repräsentation, definiert als erwartete Belohnung der optimalen hierarchischen Policy unter Verwendung dieser Repräsentation. Wir leiten Ausdrücke ab, die die Suboptimalität einschränken, und zeigen, wie diese Ausdrücke in Repräsentationslernziele übersetzt werden können, die in der Praxis optimiert werden können.Ergebnisse zu einer Reihe schwieriger kontinuierlicher Kontrollaufgaben zeigen, dass unser Ansatz zum Repräsentationslernen qualitativ bessere Repräsentationen sowie quantitativ bessere hierarchische Politiken im Vergleich zu bestehenden Methoden liefert.
Heuristische Suchforschung befasst sich häufig mit der Suche nach Algorithmen für die Offline-Planung, die darauf abzielen, die Anzahl der expandierten Knoten oder die Planungszeit zu minimieren.Bei der Online-Planung wurden bereits Algorithmen für die Echtzeitsuche oder die termingenaue Suche in Betracht gezogen.In dieser Arbeit interessieren wir uns jedoch für das Problem der situierten zeitlichen Planung, bei der der Plan eines Agenten von exogenen Ereignissen in der Außenwelt abhängen kann, so dass es wichtig wird, den Zeitablauf während des Planungsprozesses zu berücksichtigen.  In früheren Arbeiten zur situierten zeitlichen Planung wurden sowohl einfache Beschneidungsstrategien als auch komplexe Schemata für eine vereinfachte Version des zugehörigen Metareasoning-Problems vorgeschlagen. In diesem Beitrag schlagen wir eine einfache Metareasoning-Technik vor, die als grobes Giersch-Schema bezeichnet wird und in einem situierten temporalen Planer angewendet werden kann Unsere empirische Auswertung zeigt, dass das grobe Giersch-Schema die heuristische Standardsuche auf der Grundlage von Cost-to-Go-Schätzungen übertrifft.
Neuronale Netze sind anfällig für kleine negative Störungen, und die bestehende Literatur konzentriert sich weitgehend auf das Verständnis und die Abschwächung der Anfälligkeit von gelernten Modellen.In diesem Papier zeigen wir ein faszinierendes Phänomen über die beliebteste robuste Trainingsmethode in der Literatur, das negative Training: Selbst eine semantikerhaltende Transformation der Eingabedatenverteilung kann zu einer signifikant anderen Robustheit des adversarisch trainierten Modells führen, das sowohl auf der neuen Verteilung trainiert als auch evaluiert wird.Unsere Entdeckung einer solchen Sensitivität auf die Datenverteilung basiert auf einer Studie, die das Verhalten der sauberen Genauigkeit und der robusten Genauigkeit des Bayes-Klassifikators entflechtet.Empirische Untersuchungen bestätigen unsere Entdeckung weiter. Wir konstruieren semantisch identische Varianten für MNIST bzw. CIFAR10 und zeigen, dass standardmäßig trainierte Modelle vergleichbare saubere Genauigkeiten erreichen, während adversarisch trainierte Modelle signifikant unterschiedliche Robustheitsgenauigkeiten erzielen.Dieses kontraintuitive Phänomen deutet darauf hin, dass die Verteilung der Eingabedaten allein die adversarische Robustheit trainierter neuronaler Netze beeinflussen kann und nicht unbedingt die Aufgaben selbst.Abschließend diskutieren wir die praktischen Implikationen für die Bewertung adversarischer Robustheit und unternehmen erste Versuche, dieses komplexe Phänomen zu verstehen.
 Viele Aufgaben in der natürlichen Sprachverarbeitung beinhalten den Vergleich von zwei Sätzen, um einen Begriff der Relevanz, der Folgerung oder der Ähnlichkeit zu berechnen.Typischerweise wird dieser Vergleich entweder auf der Wortebene oder auf der Satzebene durchgeführt, ohne dass versucht wird, die inhärente Struktur des Satzes zu nutzen.Wenn die Satzstruktur für den Vergleich verwendet wird, wird sie während eines nicht differenzierbaren Vorverarbeitungsschritts erhalten, was zur Ausbreitung von Fehlern führt.Wir stellen ein Modell der strukturierten Ausrichtungen zwischen Sätzen vor und zeigen, wie zwei Sätze durch den Abgleich ihrer latenten Strukturen verglichen werden können. Mit Hilfe eines strukturierten Aufmerksamkeitsmechanismus gleicht unser Modell mögliche Abschnitte im ersten Satz mit möglichen Abschnitten im zweiten Satz ab, indem es gleichzeitig die Baumstruktur jedes Satzes entdeckt und einen Vergleich durchführt, und zwar in einem Modell, das vollständig differenzierbar ist und nur auf das Vergleichsziel trainiert wird.Wir evaluieren dieses Modell an zwei Satzvergleichsaufgaben: dem Stanford-Datensatz für natürliche Sprache und dem TREC-QA-Datensatz.Wir stellen fest, dass der Vergleich von Abschnitten zu einer besseren Leistung führt als der individuelle Vergleich von Wörtern, und dass die gelernten Bäume mit den tatsächlichen linguistischen Strukturen übereinstimmen.
In diesem Papier schlagen wir einen informationsmaximierenden Autoencoder (InfoAE) vor, bei dem der Kodierer eine leistungsfähige entwirrte Repräsentation durch Maximierung der gegenseitigen Information zwischen der Repräsentation und den gegebenen Informationen in einer unbeaufsichtigten Art und Weise erlernt. Wir haben unser Modell auf dem MNIST-Datensatz evaluiert und erreichten eine Testgenauigkeit von etwa 98,9 %, während wir ein vollständig unbeaufsichtigtes Training verwendeten.
Effektives Training von neuronalen Netzen erfordert viele Daten.In der datenarmen Zeit sind die Parameter unterbestimmt, und die gelernten Netze verallgemeinern schlecht.DataAugmentation (Krizhevsky et al., Das Modell, das auf bildkonditionalen generativen adversen Netzen basiert, nimmt Daten aus einer Quelldomäne und lernt, jedes beliebige Datenelement zu nehmen und zu verallgemeinern, um andere Datenelemente innerhalb der Klasse zu generieren. Da dieser generative Prozess nicht von den Klassen selbst abhängt, kann er auf neue, ungesehene Datenklassen angewendet werden. Wir zeigen, dass ein Data Augmentation Generative Adversarial Network (DAGAN) Standard-Vanilla-Klassifikatoren gut ergänzt.Wir zeigen auch, dass ein DAGAN wenige Lernsysteme wie Matching Networks verbessern kann.Wir demonstrieren diese Ansätze auf Omniglot, auf EMNIST, nachdem das DAGAN auf Omniglot gelernt wurde, und aufVGG-Gesichtsdaten. In unseren Experimenten können wir eine Steigerung der Genauigkeit um mehr als 13% in den Experimenten mit geringen Datenmengen in Omniglot (von 69% auf 82%), EMNIST (73,9% auf 76%) und VGG-Face (4,5% auf 12%) feststellen; in Matching Networks für Omniglot beobachten wir eine Steigerung von 0,5% (von 96,9% auf 97,4%) und eine Steigerung von 1,8% inEMNIST (von 59,5% auf 61,3%).
Die Beantwortung von Fragen zu Daten kann ein Verständnis dafür erfordern, welche Teile einer Eingabe X die Antwort Y beeinflussen. Ein solches Verständnis kann durch das Testen von Beziehungen zwischen Variablen durch ein maschinelles Lernmodell aufgebaut werden. Wir formalisieren eine Klasse geeigneter Teststatistiken, die garantiert ein Merkmal auswählen, wenn es Informationen über die Antwort liefert, selbst wenn der Rest der Merkmale bekannt ist. Wir zeigen, dass f-Divergenzen eine breite Klasse geeigneter Teststatistiken bieten. In der Klasse der f-Divergenzen liefert die KL-Divergenz eine einfach zu berechnende geeignete Teststatistik, die sich auf den AMI bezieht.  Wir zeigen, dass Schätzer aus demselben AMI-Test auch verwendet werden können, um wichtige Merkmale in einer bestimmten Instanz zu finden. Wir zeigen anhand eines Beispiels, dass perfekte Vorhersagemodelle für die Auswahl von Merkmalen auf Instanzebene unzureichend sind. Wir evaluieren unsere Methode in mehreren Simulationsexperimenten, in einem Genomdatensatz, einem klinischen Datensatz für die Wiederaufnahme von Patienten in Krankenhäusern und in einer Untergruppe von Klassen in ImageNet. Unsere Methode übertrifft mehrere Baselines in verschiedenen simulierten Datensätzen, ist in der Lage, biologisch bedeutsame Gene zu identifizieren, kann die wichtigsten Prädiktoren für ein Krankenhausrückkehrereignis auswählen und ist in der Lage, Unterscheidungsmerkmale in einer Bildklassifizierungsaufgabe zu identifizieren.
Diese Beschriftungen stammen jedoch häufig von verrauschten Crowdsourcing-Plattformen wie Amazon Mechanical Turk. Praktiker sammeln in der Regel mehrere Beschriftungen pro Beispiel und fassen die Ergebnisse zusammen, um das Rauschen zu mindern (das klassische Crowdsourcing-Problem).Bei einem festen Beschriftungsbudget und unbegrenzten unbeschrifteten Daten geht eine redundante Beschriftung auf Kosten weniger beschrifteter Beispiele.Dies wirft zwei grundlegende Fragen auf: (1) Wie können wir am besten von verrauschten Arbeitern lernen?(2) Wie sollten wir unser Beschriftungsbudget zuweisen, um die Leistung eines Klassifizierers zu maximieren?Wir schlagen einen neuen Algorithmus für die gemeinsame Modellierung von Beschriftungen und Arbeiterqualität aus verrauschten Crowdsourcing-Daten vor.Die alternierende Minimierung erfolgt in Runden, wobei die Arbeiterqualität aus der Nichtübereinstimmung mit dem aktuellen Modell geschätzt und das Modell dann durch Optimierung einer Verlustfunktion aktualisiert wird, die die aktuelle Schätzung der Arbeiterqualität berücksichtigt. Im Gegensatz zu früheren Ansätzen kann unser Algorithmus auch mit nur einer Annotation pro Beispiel die Qualität der Arbeiter einschätzen. wir legen eine Generalisierungsfehlergrenze für Modelle fest, die mit unserem Algorithmus gelernt wurden, und stellen theoretisch fest, dass es besser ist, viele Beispiele einmal zu etikettieren (im Vergleich zu einer geringeren Anzahl), wenn die Qualität der Arbeiter einen Schwellenwert überschreitet. die Experimente, die sowohl mit ImageNet (mit simulierten verrauschten Arbeitern) als auch mit MS-COCO (unter Verwendung echter, von der Masse gesammelter Etiketten) durchgeführt wurden, bestätigen die Vorteile unseres Algorithmus.
Neuronale Netze machen Fehler und der Grund, warum ein Fehler gemacht wird, bleibt oft ein Rätsel.Neuronale Netze werden daher oft als Blackbox betrachtet.Es wäre nützlich, eine Methode zu haben, die dem Benutzer eine intuitive Erklärung dafür gibt, warum ein Bild falsch klassifiziert wird.In diesem Beitrag entwickeln wir eine Methode, um die Fehler eines Klassifizierungsmodells zu erklären, indem wir visuell zeigen, was zu einem Bild hinzugefügt werden muss, damit es richtig klassifiziert wird. Unsere Arbeit kombiniert die Bereiche der adversen Beispiele, der generativen Modellierung und einer Korrekturtechnik, die auf der Differenzzielpropagation basiert, um eine Technik zu schaffen, die Erklärungen dafür liefert, warum ein Bild falsch klassifiziert wurde. in diesem Papier erklären wir unsere Methode und demonstrieren sie an MNIST und CelebA. dieser Ansatz könnte dabei helfen, neuronale Netzwerke für einen Benutzer zu entmystifizieren.
Im Kontext des Multi-Task-Lernens werden häufig neuronale Netze mit verzweigten Architekturen eingesetzt, um die anstehenden Aufgaben gemeinsam zu bewältigen: Solche verzweigten Netze beginnen typischerweise mit einer Reihe gemeinsamer Schichten, nach denen sich die verschiedenen Aufgaben in eine eigene Schichtenfolge verzweigen. Da die Anzahl der möglichen Netzwerkkonfigurationen kombinatorisch groß ist, wird es verständlicherweise schwierig zu entscheiden, welche Schichten gemeinsam genutzt werden und wo sie sich verzweigen sollen. frühere Arbeiten haben sich entweder auf Ad-hoc-Methoden verlassen, um den Grad der gemeinsamen Nutzung von Schichten zu bestimmen, was suboptimal ist, oder sie haben Suchtechniken für neuronale Architekturen verwendet, um das Netzwerkdesign festzulegen, was sehr teuer ist. Bei einem bestimmten Budget, d.h. einer bestimmten Anzahl von lernbaren Parametern, generiert der vorgeschlagene Ansatz Architekturen, bei denen flache Schichten aufgabenunabhängig sind, während tiefere Schichten allmählich aufgabenspezifischer werden. Eine umfangreiche experimentelle Analyse zahlreicher, unterschiedlicher Multitasking-Datensätze zeigt, dass unsere Methode für ein bestimmtes Budget durchgängig Netzwerke mit der höchsten Leistung hervorbringt, während sie für eine bestimmte Leistungsschwelle die geringste Anzahl von lernbaren Parametern benötigt.
Wir stellen eine Methode vor, um den Gewichtstensor in einer Faltungsschicht mit Hilfe von diagonalen Matrizen, diskreten Kosinustransformationen (DCTs) und Permutationen auszudrücken, die mit Hilfe von stochastischen Gradientenmethoden optimiert werden kann. ein Netzwerk, das aus solchen strukturierten effizienten Faltungsschichten (SECL) besteht, übertrifft bestehende Low-Rank-Netzwerke und zeigt eine konkurrenzfähige Recheneffizienz.
Die Entfärbung blinder Dokumente ist eine grundlegende Aufgabe im Bereich der Dokumentenverarbeitung und -restaurierung, die in optischen Zeichenerkennungssystemen, in der Forensik usw. eine breite Anwendung findet. Wir stellen SVDocNet vor, ein durchgängig trainierbares, auf einem U-Netz basierendes räumliches rekurrentes neuronales Netz (RNN) zur blinden Entschärfung von Dokumenten, bei dem die Gewichte der RNNs durch verschiedene Faltungsneuronale Netze (CNNs) bestimmt werden.
Im Gegensatz zu den monolithischen Deep-Learning-Architekturen, die heute für das Computersehen verwendet werden, verarbeitet der visuelle Kortex Retina-Bilder über zwei funktionell unterschiedliche, aber miteinander verbundene Netzwerke: den ventralen Pfad für die Verarbeitung objektbezogener Informationen und den dorsalen Pfad für die Verarbeitung von Bewegungen und Transformationen. Inspiriert von dieser kortikalen Arbeitsteilung und den Eigenschaften der magno- und parvozellulären Systeme erforschen wir einen unbeaufsichtigten Ansatz zum Merkmalslernen, der gemeinsam Objektmerkmale und ihre Transformationen aus natürlichen Videos erlernt.Wir schlagen ein neues faltbares bilineares Sparse-Coding-Modell vor, das (1) unabhängige Merkmalstransformationen ermöglicht und (2) in der Lage ist, große Bilder zu verarbeiten. Unsere Ergebnisse zeigen, dass unser Modell Gruppen von Merkmalen und deren Transformationen direkt aus natürlichen Videos in einer völlig unbeaufsichtigten Weise lernen kann.Die gelernten "dynamischen Filter" weisen bestimmte Äquivarianzeigenschaften auf, ähneln kortikalen räumlich-zeitlichen Filtern und erfassen die Statistik der Übergänge zwischen Videobildern.Unser Modell kann als einer der ersten Ansätze angesehen werden, die das unbeaufsichtigte Lernen von primären "Kapseln" (vorgeschlagen von Hinton und Kollegen für das überwachte Lernen) demonstrieren und hat starke Verbindungen zum Lie-Gruppen-Ansatz der visuellen Wahrnehmung.
 Herkömmliche Verfahren zur Erkennung von Out-of-Distribution (OOD), die auf Variations-Autoencoder oder Random Network Distillation (RND) basieren, sind dafür bekannt, dass sie den OOD-Daten eine geringere Unsicherheit zuweisen als die Zielverteilung. Basierend auf dieser Beobachtung konstruieren wir einen neuartigen RND-basierten OOD-Detektor, SVD-RND, der unscharfe Bilder während des Trainings verwendet. unser Detektor ist einfach, effizient in der Testzeit und übertrifft die Basis-OOD-Detektoren in verschiedenen Domänen. weitere Ergebnisse zeigen, dass SVD-RND eine bessere Zielverteilungsrepräsentation lernt als die Basislinien. schließlich erreicht SVD-RND in Kombination mit geometrischer Transformation eine nahezu perfekte Detektionsgenauigkeit in der CelebA-Domäne.
Der prominenteste Algorithmus in dieser Forschungsrichtung ist LARS, der durch die Verwendung von schichtweise adaptiven Lernraten ResNet auf ImageNet in wenigen Minuten trainiert. Allerdings schneidet LARS bei Aufmerksamkeitsmodellen wie BERT schlecht ab, was darauf hindeutet, dass seine Leistungsgewinne nicht über alle Aufgaben hinweg konsistent sind.In diesem Papier untersuchen wir zunächst eine prinzipielle schichtweise Anpassungsstrategie zur Beschleunigung des Trainings von tiefen neuronalen Netzen unter Verwendung großer Mini-Batches. Unter Verwendung dieser Strategie entwickeln wir eine neue schichtweise adaptive Optimierungstechnik für große Batches mit dem Namen LAMB; anschließend bieten wir eine Konvergenzanalyse von LAMB und LARS an, die die Konvergenz zu einem stationären Punkt in allgemeinen nicht-konvexen Einstellungen zeigt. Unsere empirischen Ergebnisse zeigen die überlegene Leistung von LAMB bei verschiedenen Aufgaben wie BERT- und ResNet-50-Training mit sehr geringem Hyperparameter-Tuning; insbesondere beim BERT-Training ermöglicht unser Optimierer die Verwendung sehr großer Batch-Größen von 32868 ohne Leistungseinbußen. Durch Erhöhen der Stapelgröße auf die Speichergrenze eines TPUv3-Pods kann die BERT-Trainingszeit von 3 Tagen auf nur 76 Minuten reduziert werden (Tabelle 1).
Modellagnostisches Meta-Lernen (MAML) ist als leistungsstarke Meta-Lernmethode bekannt, aber MAML ist dafür berüchtigt, dass es wegen der Existenz von zwei Lernraten schwer zu trainieren ist. Daher leiten wir in diesem Papier die Bedingungen ab, die die innere Lernrate $\alpha$ und die Meta-Lernrate $\beta$ erfüllen müssen, damit MAML mit einigen Vereinfachungen zu Minima konvergiert. Wir stellen fest, dass die obere Schranke von $\beta$ von $\alpha$ abhängt, im Gegensatz zum Fall der Verwendung der normalen Gradientenabstiegsmethode. Dieses Ergebnis wird durch Experimente mit verschiedenen "few-shot" Aufgaben und Architekturen verifiziert; insbesondere führen wir Sinusoid-Regression und Klassifizierung von Omniglot- und MiniImagenet-Datensätzen mit einem mehrschichtigen Perceptron und einem Faltungsneuronalen Netzwerk durch. Auf der Grundlage dieser Ergebnisse stellen wir einen Leitfaden für die Bestimmung der Lernraten vor: Zunächst wird nach dem größtmöglichen $\alpha$ gesucht; anschließend wird $\beta$ auf der Grundlage des gewählten Wertes von $\alpha$ abgestimmt.
Wir stellen einen neuronalen Rahmen für das Lernen von Assoziationen zwischen zusammenhängenden Wortgruppen vor, wie sie in Subjekt-Verb-Objekt (SVO)-Strukturen vorkommen: Unser Modell induziert einen gemeinsamen funktionsspezifischen Wortvektorraum, in dem Vektoren von z.B. plausiblen SVO-Zusammensetzungen nahe beieinander liegen. Das Modell behält die Information über die Wortgruppenzugehörigkeit auch im gemeinsamen Raum bei und kann daher effektiv auf eine Reihe von Aufgaben angewendet werden, die auf die SVO-Struktur schließen lassen, Die Ergebnisse zeigen, dass die Kombinationen von Repräsentationen, die mit unserem aufgabenunabhängigen Modell erlernt werden, aufgabenspezifische Architekturen aus früheren Arbeiten übertreffen und gleichzeitig die Anzahl der Parameter um bis zu 95 % reduzieren.Der vorgeschlagene Rahmen ist vielseitig und verspricht, das Erlernen funktionsspezifischer Repräsentationen über die SVO-Strukturen hinaus zu unterstützen.
Die Herstellung von Halbleitern beinhaltet Ätzen Prozess, um ausgewählte Bereiche von Wafern zu entfernen.Allerdings ist die Messung der geätzten Struktur in Mikrografie stark auf zeitaufwendige manuelle Routinen.Traditionelle Bildverarbeitung erfordert in der Regel auf eine große Anzahl von kommentierten Daten und die Leistung ist immer noch schlecht.Wir behandeln diese Herausforderung als Segmentierung Problem und verwenden Deep-Learning-Ansatz, um Masken von Objekten in geätzten Struktur der Wafer zu erkennen.Dann verwenden wir einfache Bildverarbeitung, um die automatische Messung auf die Objekte durchzuführen. Wir laden 10 SEM-Bilder (Rasterelektronenmikroskop) von 4 Typen aus dem Internet herunter, auf deren Grundlage wir unsere Experimente durchführen. Unsere auf Deep Learning basierende Methode zeigt eine Überlegenheit gegenüber dem Bildverarbeitungsansatz mit einer mittleren Genauigkeit von über 96 % für die Messungen, verglichen mit der Grundwahrheit.Soweit wir wissen, ist es das erste Mal, dass Deep Learning in der Halbleiterindustrie für automatische Messungen angewendet wurde.
Wir stellen drei Methoden vor, um gültige zeitliche Platzierungsintervalle für eine Aktivität in einem zeitlich begründeten Plan zu bestimmen, wenn solche Einschränkungen vorliegen. Wir stellen die Algorithmen Max Duration und Probe vor, die solide, aber unvollständig sind, und den Algorithmus Linear, der solide und vollständig für den Ressourcenverbrauch mit linearer Rate ist. Wir wenden diese Techniken auf das Problem der Planung von Aufwachen für einen planetarischen Rover an, bei dem die Aufwachdauer durch bestehende Aktivitäten beeinflusst wird. Wir zeigen, wie der Probe-Algorithmus mit dem linearen Algorithmus konkurriert, wenn ein vorteilhafter Problemraum und wohldefinierte Heuristiken vorliegen. Wir zeigen, dass der Probe-Algorithmus und der Linear-Algorithmus den Max-Duration-Algorithmus empirisch übertreffen und stellen die Laufzeitunterschiede zwischen den drei Algorithmen empirisch dar.Der Probe-Algorithmus wird derzeit für den Einsatz im Onboard-Scheduler des nächsten NASA-Planetensuchers, dem Mars 2020-Rover, entwickelt.
Eine Frage, die sich stellt, ist, ob die Verwendung des euklidischen Raums für latente Variablenmodelle zu einer entwirrten Darstellung führen kann, wenn die zugrundeliegenden generierenden Faktoren eine bestimmte geometrische Struktur haben, z. B. die Bilder eines Autos aus verschiedenen Winkeln. Die Einreichungen, die für die erste Stufe der NeurIPS2019 Disentanglement Challenge eingereicht wurden, bestehen aus einem Diffusion Variational Autoencoder ($\Delta$VAE) mit einem hypersphärischen latenten Raum, der z.B. periodische wahre Faktoren wiederherstellen kann.Das Training des $\Delta$VAE wird durch die Einbeziehung einer modifizierten Version des Evidence Lower Bound (ELBO) zur Anpassung der Kodierungskapazität des posterioren Approximats verbessert.
In dieser Arbeit stellen wir eine vereinheitlichende Sichtweise vor und schlagen eine Open-Set-Methode vor, um die derzeitigen Generalisierungsannahmen zu lockern.Darüber hinaus erweitern wir die Anwendbarkeit von transformationsbasierten Methoden auf Nicht-Bilddaten, indem wir zufällige affine Transformationen verwenden.Unsere Methode zeigt, dass sie die höchste Genauigkeit erzielt und auf eine Vielzahl von Datentypen anwendbar ist.Die starke Leistung unserer Methode wird ausführlich an mehreren Datensätzen aus verschiedenen Bereichen validiert.
Während das Training auf solchen Korpora das Modell ermutigt, weitreichende Abhängigkeiten im Text zu verstehen, kann es auch dazu führen, dass die Modelle die in den Korpora vorhandenen sozialen Verzerrungen internalisieren. In dieser Arbeit geht es darum, die Verzerrungen von Sprachmodellen zu quantifizieren und zu reduzieren: Anhand eines konditionierten Kontextes (z.B. einer Schreibaufforderung) und eines Sprachmodells wird analysiert, ob (und wie) die Stimmung des generierten Textes durch Änderungen der Werte sensibler Attribute (z.B. Ländernamen, Berufe, Namen) beeinflusst wird. Wir quantifizieren diese Verzerrungen, indem wir individuelle und gruppenbezogene Fairness-Metriken aus der Literatur zum fairen maschinellen Lernen adaptieren. Ausführliche Auswertungen an zwei verschiedenen Korpora (Nachrichtenartikel und Wikipedia) zeigen, dass moderne Transformer-basierte Sprachmodelle Verzerrungen aufweisen, die aus Daten gelernt wurden. Wir schlagen Methoden zur Regularisierung der Einbettungsähnlichkeit und der Gefühlsähnlichkeit vor, die sowohl die individuellen als auch die gruppenspezifischen Fairness-Metriken verbessern, ohne die Komplexität und die semantische Ähnlichkeit zu beeinträchtigen - ein positiver Schritt in Richtung Entwicklung und Einsatz von faireren Sprachmodellen für reale Anwendungen.
Die Themenmodellierung von Textdokumenten ist eine der wichtigsten Aufgaben beim Repräsentationslernen. In dieser Arbeit schlagen wir iTM-VAE vor, ein nichtparametrisches Bayes'sches (BNP) Themenmodell mit variablen Autokodierern. Einerseits hat iTM-VAE als BNP-Themenmodell potentiell unendlich viele Themen und kann die Anzahl der Themen automatisch an die Daten anpassen, andererseits wird die Inferenz von iTM-VAE, anders als bei anderen BNP-Themenmodellen, durch neuronale Netze modelliert, die eine hohe Darstellungskapazität haben und in einer einfachen Feed-Forward-Weise berechnet werden können. In diesem Papier werden auch zwei Varianten von iTM-VAE vorgeschlagen, wobei iTM-VAE-Prod den generativen Prozess in der Art von Expertenprodukten modelliert, um eine bessere Leistung zu erzielen, und iTM-VAE-G einen Prior über den Konzentrationsparameter legt, so dass das Modell einen geeigneten Konzentrationsparameter automatisch an die Daten anpassen kann.Experimentelle Ergebnisse auf den Datensätzen 20News und Reuters RCV1-V2 zeigen, dass die vorgeschlagenen Modelle den Stand der Technik in Bezug auf Perplexität, Themenkohärenz und Dokument-Retrieval-Aufgaben übertreffen.Darüber hinaus wird die Fähigkeit der Anpassung des Konzentrationsparameters an die Daten auch durch Experimente bestätigt.
Knowledge Distillation (KD) ist eine weit verbreitete Technik in der jüngsten Deep-Learning-Forschung, um kleine und einfache Modelle, deren Leistung ist auf einer Stufe mit ihren großen und komplexen counterparts.Standard Knowledge Distillation neigt dazu, zeitaufwendig sein, weil der Ausbildung Zeit ausgegeben, um eine Lehrer-Modell, das dann bieten würde Leitlinien für die Schüler model.It könnte möglich sein, um die Zeit durch die Ausbildung eines Lehrers Modell on the fly zu verkürzen, aber es ist nicht trivial, eine solche hohe Kapazität Lehrer, die Qualität Leitlinien für die Schüler-Modelle auf diese Weise geben. In diesem Rahmen schlagen wir eine einfache und effektive Implementierung vor, die wir als "Distillation by Utilizing Peer Samples" (DUPS) in einer Generation bezeichnen.Wir verifizieren unseren Algorithmus in zahlreichen Experimenten.Verglichen mit dem Standardtraining auf modernen Architekturen erreicht DUPS eine durchschnittliche Verbesserung von 1%-2% bei verschiedenen Aufgaben mit nahezu null zusätzlichen Kosten.Im Vergleich zu einigen typischen Methoden der Wissensdestillation, die viel zeitaufwändiger sind, erreichen wir mit DUPS eine vergleichbare oder sogar bessere Leistung.
Wir entwickeln einen Metalearning-Ansatz fÃ?r das Lernen hierarchisch strukturierter Policies, der die Stichprobeneffizienz bei ungesehenen Aufgaben durch die Verwendung gemeinsam genutzter Primitive verbessert â€" Policies, die fÃ?r eine groÃŸe Anzahl von Zeitschritten ausgefÃ?hrt werden.â€œ Speziell werden eine Reihe von Primitiven innerhalb einer Verteilung von Aufgaben gemeinsam genutzt und durch aufgabenspezifische Policies umgeschaltet.â€žWir stellen eine konkrete Metrik zur Messung der StÃ?rke solcher Hierarchien bereit, die zu einem Optimierungsproblem fÃ?r das schnelle Erreichen einer hohen Belohnung bei ungesehenen Aufgaben fÃ?hrt. Wir stellen dann einen Algorithmus vor, um dieses Problem durchgängig zu lösen, indem wir wiederholt neue Aufgaben abtasten und aufgabenspezifische Richtlinien zurücksetzen.Wir entdecken erfolgreich sinnvolle motorische Primitive für die gerichtete Bewegung von vierbeinigen Robotern, allein durch die Interaktion mit Verteilungen von Labyrinthen.Wir demonstrieren auch die Übertragbarkeit der Primitive, um Hindernisparcours mit langer Zeitskala und geringer Belohnung zu lösen, und wir ermöglichen es humanoiden 3D-Robotern, mit derselben Richtlinie robust zu gehen und zu kriechen.
Bestehende Ansätze erfordern entweder komplexe Inferenzen oder verwenden rekurrente neuronale Netze, die sich nur schwer parallelisieren lassen. Wir gehen einen anderen Weg und nutzen die jüngsten Fortschritte in der Sprachmodellierung, um ein Einbettungsmodell auf der Basis von Faltungsneuronalen Netzen zu entwickeln, mit dem wir tiefere Architekturen trainieren können, die vollständig parallelisierbar sind. Durch das Stapeln von Schichten wird das rezeptive Feld vergrößert, so dass jede aufeinanderfolgende Schicht zunehmend größere semantische Abhängigkeiten innerhalb des Dokuments modellieren kann.Empirisch zeigen wir überlegene Ergebnisse bei zwei öffentlich zugänglichen Benchmarks.
Wir beweisen Grenzen für den Generalisierungsfehler von Faltungsnetzen, die sich auf den Trainingsverlust, die Anzahl der Parameter, die Lipschitz-Konstante des Verlustes und den Abstand der Gewichte zu den Anfangsgewichten beziehen. Sie sind unabhängig von der Anzahl der Pixel in der Eingabe und der Höhe und Breite der versteckten Merkmalskarten. Wir präsentieren Experimente mit CIFAR-10, zusammen mit variierenden Parametern eines tiefen Faltungsnetzes, und vergleichen unsere Grenzen mit praktischen Generalisierungslücken.
Die MobileNets-Familie neuronaler Netze hat in den letzten Jahren enorme Fortschritte bei der Entwicklung und Organisation ressourceneffizienter Architekturen gemacht. Neue Anwendungen mit strengen Echtzeitanforderungen in stark eingeschränkten Geräten erfordern eine weitere Komprimierung der MobileNets-ähnlichen, bereits recheneffizienten Netze. Modellquantisierung ist eine weit verbreitete Technik zur Komprimierung und Beschleunigung der Inferenz neuronaler Netze, und frühere Arbeiten haben MobileNets auf 4 âˆ' 6 Bits quantisiert, wenn auch mit einem bescheidenen bis signifikanten Rückgang der Genauigkeit. Während die Quantisierung auf Sub-Byte-Werte (d.h. Präzision â‰¤ 8 Bits) wertvoll war, ist eine noch weitergehende Quantisierung von MobileNets auf binäre oder ternäre Werte notwendig, um signifikante Energieeinsparungen und möglicherweise Laufzeitbeschleunigungen auf spezialisierter Hardware, wie ASICs und FPGAs, zu realisieren. Ausgehend von der Beobachtung, dass Faltungsfilter in jeder Schicht eines tiefen neuronalen Netzes unterschiedlich auf ternäre Quantisierung reagieren können, schlagen wir eine neuartige Quantisierungsmethode vor, die hybride Filterbänke pro Schicht generiert, die aus Filtern mit voller Genauigkeit und ternären Gewichten für MobileNets bestehen Die schichtweisen hybriden Filterbänke kombinieren im Wesentlichen die Stärken von Filtern mit voller Genauigkeit und ternären Gewichten, um eine kompakte, energieeffiziente Architektur für MobileNets abzuleiten. Mit dieser vorgeschlagenen Quantisierungsmethode haben wir einen wesentlichen Teil der Gewichtsfilter von MobileNets auf ternäre Werte quantisiert, was zu einer Energieeinsparung von 27,98 % und einer Verringerung der Modellgröße um 51,07 % führt, während eine vergleichbare Genauigkeit und keine Verschlechterung des Durchsatzes auf spezialisierter Hardware im Vergleich zu den Grundlinien-MobileNets mit voller Genauigkeit erreicht wird.
Aufgrund des Mangels an geeigneten Datensätzen haben frühere Forschungen Deep Learning nur bei kontrolliertem synthetischem Rauschen untersucht, während Rauschen in der realen Welt noch nie systematisch in einer kontrollierten Umgebung untersucht wurde.Zu diesem Zweck wird in diesem Papier ein Benchmark von verrauschten Etiketten bei 10 kontrollierten Rauschpegeln erstellt.Da Rauschen in der realen Welt einzigartige Eigenschaften besitzt, führen wir eine groß angelegte Studie über eine Vielzahl von Rauschpegeln und -typen, Architekturen, Methoden und Trainingseinstellungen durch, um den Unterschied zu verstehen.Unsere Studie zeigt, dass: (1) Deep Neural Networks (DNNs) verallgemeinern viel besser bei realem Lärm.(2) DNNs lernen möglicherweise nicht zuerst Muster bei realem Lärm.(3) Wenn die Netzwerke fein abgestimmt sind, verallgemeinern ImageNet-Architekturen gut bei verrauschten Daten. (4) Rauschen in der realen Welt scheint weniger schädlich zu sein, aber es ist schwieriger für robuste DNN-Methoden, sich zu verbessern.(5) Robuste Lernmethoden, die bei synthetischem Rauschen gut funktionieren, funktionieren möglicherweise nicht so gut bei Rauschen in der realen Welt und umgekehrt.Wir hoffen, dass unser Benchmark sowie unsere Ergebnisse die Deep-Learning-Forschung bei verrauschten Daten erleichtern werden.
Das Design von RNA-Molekülen hat in jüngster Zeit in der Medizin, der synthetischen Biologie, der Biotechnologie und der Bioinformatik großes Interesse geweckt, da sich gezeigt hat, dass viele funktionale RNA-Moleküle an regulatorischen Prozessen der Transkription, Epigenetik und Translation beteiligt sind. Da die Funktion einer RNA von ihren strukturellen Eigenschaften abhängt, besteht das RNA-Design-Problem darin, eine RNA-Sequenz zu finden, die den gegebenen strukturellen Bedingungen entspricht. Hier schlagen wir einen neuen Algorithmus für das RNA-Design-Problem vor, der LEARNA genannt wird. Durch Meta-Lernen über 65000 verschiedene RNA-Design-Aufgaben für eine Stunde auf 20 CPU-Kernen konstruiert unsere Erweiterung Meta-LEARNA eine RNA-Design-Policy, die sofort angewendet werden kann, um neuartige RNA-Design-Aufgaben zu lösen.Methodisch optimieren wir, unserer Meinung nach zum ersten Mal, gemeinsam über einen großen Raum von Architekturen für das Policy-Netzwerk, die Hyperparameter des Trainingsverfahrens und die Formulierung des Entscheidungsprozesses. Umfassende empirische Ergebnisse auf zwei weit verbreiteten RNA-Design-Benchmarks sowie einem dritten, den wir einführen, zeigen, dass unser Ansatz eine neue State-of-the-Art-Performance auf dem erstgenannten erreicht und gleichzeitig um Größenordnungen schneller ist, wenn es darum geht, die vorherige State-of-the-Art-Performance zu erreichen.In einer Ablationsstudie analysieren wir die Bedeutung der verschiedenen Komponenten unserer Methode.
Pruning ist eine beliebte Technik zur Komprimierung eines neuronalen Netzes: ein großes, vortrainiertes Netz wird fein abgestimmt, während Verbindungen sukzessive entfernt werden.der Wert des Pruning hat sich jedoch weitgehend einer Prüfung entzogen.in dieser erweiterten Zusammenfassung untersuchen wir Restnetze, die durch Fisher-Pruning erhalten wurden, und machen zwei interessante Beobachtungen.erstens, wenn die Zeit begrenzt ist, ist es besser, ein einfaches, kleineres Netz von Grund auf zu trainieren als ein großes Netz zu beschneiden. Zweitens sind es die durch das Pruning erhaltenen Architekturen --- nicht die gelernten Gewichte ---, die sich als wertvoll erweisen. Solche Architekturen sind leistungsfähig, wenn sie von Grund auf neu trainiert werden. Außerdem sind diese Architekturen ohne weiteres Pruning leicht zu approximieren: Wir können einmal prunen und erhalten eine Familie neuer, skalierbarer Netzwerkarchitekturen für unterschiedliche Speicheranforderungen.
Das heißt, dass menschliche Leser, die dieselben Daten betrachten, zu legitimen, aber völlig unterschiedlichen Schlussfolgerungen kommen können, die auf ihren persönlichen Erfahrungen beruhen. Im Rahmen des maschinellen Lernens wird das Feedback mehrerer menschlicher Kommentatoren jedoch oft auf eine einzige ``Grundwahrheit'' reduziert, wodurch die wahren, potenziell reichhaltigen und vielfältigen Interpretationen der Daten, die im gesamten sozialen Spektrum gefunden werden, verborgen bleiben. Wir schlagen vor, Label-Verteilungen nicht nur über Individuen, sondern auch über Datenelemente zu aggregieren, um die Kosten für Menschen in der Schleife zu maximieren. Wir testen verschiedene Aggregationsansätze an hochmodernen Deep-Learning-Modellen. Unsere Ergebnisse deuten darauf hin, dass sorgfältige Label-Aggregationsmethoden die Anzahl der Stichproben, die für repräsentative Verteilungen benötigt werden, stark reduzieren können.
Jüngste Fortschritte bei Deep-Learning-Techniken wie Convolutional Neural Networks (CNN) und Generative Adversarial Networks (GAN) haben einen Durchbruch beim semantischen Bild-Inpainting, der Rekonstruktion fehlender Pixel in gegebenen Bildern, erzielt. Obwohl Deep-Learning-Modelle viel effektiver als herkömmliche Ansätze sind, benötigen sie große Datensätze und große Rechenressourcen für das Training, und die Qualität des Inpaintings variiert erheblich, wenn die Trainingsdaten in Größe und Vielfalt variieren. Um diese Probleme anzugehen, stellen wir in diesem Papier eine Inpainting-Strategie von \textit{Comparative Sample Augmentation} vor, die die Qualität des Trainingssatzes verbessert, indem irrelevante Bilder herausgefiltert und zusätzliche Bilder mit Informationen über die umgebenden Regionen der zu malenden Bilder konstruiert werden.Experimente an mehreren Datensätzen zeigen, dass unsere Methode die Anwendbarkeit von Deep-Inpainting-Modellen auf Trainingssätze mit unterschiedlichen Größen erweitert, während die Inpainting-Qualität gemessen an qualitativen und quantitativen Metriken für eine große Klasse von Deep-Modellen beibehalten wird, ohne dass eine modellspezifische Berücksichtigung erforderlich ist.
Im Gegensatz zu anderen generativen Modellen wird die Datenverteilung über ein Spiel zwischen einem Generator (dem generativen Modell) und einem Diskriminator (einem Lehrer, der ein Trainingssignal bereitstellt) gelernt, die jeweils ihre eigenen Kosten minimieren.GANs sind so konzipiert, dass sie ein Nash-Gleichgewicht erreichen, bei dem jeder Spieler seine Kosten nicht reduzieren kann, ohne die Parameter der anderen Spieler zu verändern. Ein hilfreicher Ansatz fÃ?r die Theorie der GANs besteht darin zu zeigen, dass eine Divergenz zwischen der Trainingsverteilung und der Modellverteilung ihren Minimalwert im Gleichgewicht erreicht. Mehrere neuere Forschungsrichtungen sind von der Idee motiviert, dass diese Divergenz der primÃ?re Leitfaden fÃ?r den Lernprozess ist und dass jeder Lernschritt die Divergenz verringern sollte. Wir zeigen, dass diese Sichtweise zu restriktiv ist: Während des GAN-Trainings liefert der Diskriminator Lernsignale in Situationen, in denen die Gradienten der Divergenzen zwischen den Verteilungen nicht von Nutzen sind, und liefern empirische Gegenbeispiele für die Sichtweise des GAN-Trainings als Divergenzminimierung. Wir zeigen auch, dass Gradientenstrafen, die aus der Perspektive der Divergenzminimierung motiviert sind, ebenso hilfreich sind, wenn sie in anderen Kontexten angewendet werden, in denen die Perspektive der Divergenzminimierung nicht vorhersagt, dass sie hilfreich wären.
Die Messung der gegenseitigen Information (Mutual Information, MI) zwischen hochdimensionalen, kontinuierlichen Zufallsvariablen aus beobachteten Stichproben hat ein breites theoretisches und praktisches Anwendungsspektrum. Jüngste Arbeiten haben genaue MI-Schätzer durch beweisbar verzerrungsarme Approximationen und enge Variationsschranken entwickelt, die von einem reichlichen Angebot an Stichproben ausgehen, aber eine unrealistische Anzahl von Stichproben erfordern, um die statistische Signifikanz der Schätzung zu garantieren. In dieser Arbeit konzentrieren wir uns auf die Verbesserung der Dateneffizienz und schlagen einen dateneffizienten MINE-Schätzer (DEMINE) vor, der ein enges unteres vertrauenswürdiges Intervall von MI unter begrenzten Daten bereitstellen kann, indem wir der unteren Schranke von MINE eine Kreuzvalidierung hinzufügen (Belghazi et al., 2018).Die Suche nach Hyperparametern wird eingesetzt und ein neuartiger Meta-Learning-Ansatz mit Aufgabenerweiterung wird entwickelt, um die Robustheit gegenüber Hyperparametern zu erhöhen, die Überanpassung zu reduzieren und die Genauigkeit zu verbessern.Mit verbesserter Dateneffizienz ermöglicht unser DEMINE-Schätzer statistische Tests der Abhängigkeit bei praktischen Datensatzgrößen.Wir demonstrieren die Wirksamkeit von DEMINE auf synthetischen Benchmarks und einem realen fMRI-Datensatz, mit Anwendung der Inter-Subjekt-Korrelationsanalyse.
Sprache und Vision sind als zwei verschiedene modale in der aktuellen Arbeit für Bild captioning verarbeitet.Allerdings zeigt die jüngste Arbeit auf Super Characters Methode die Wirksamkeit der zweidimensionalen Wort Einbettung, die Text-Klassifizierung Problem in Bild-Klassifizierung Problem konvertiert. In diesem Papier schlagen wir die SuperCaptioning-Methode vor, die die Idee der zweidimensionalen Worteinbettung aus der Super Characters-Methode aufgreift und die Informationen der Sprache und des Sehens zusammen in einem einzigen CNN-Modell verarbeitet.Die experimentellen Ergebnisse auf Flickr30k-Daten zeigen, dass die vorgeschlagene Methode qualitativ hochwertige Bildunterschriften liefert.Eine interaktive Demo ist bereit, auf dem Workshop zu zeigen.
Die Bestimmung der optimalen Reihenfolge, in der Datenbeispiele Deep Neural Networks während des Trainings präsentiert werden, ist ein nicht-triviales Problem, aber die Wahl einer nicht-trivialen Planungsmethode kann die Konvergenz drastisch verbessern. In diesem Papier schlagen wir ein Self-Paced Learning (SPL)-fused Deep Metric Learning (DML)-Framework vor, das wir Learning Embeddings for Adaptive Pace (LEAP) nennen.Unsere Methode parametrisiert Mini-Batches dynamisch basierend auf der \textit{easiness} und \textit{true diversity} der Probe innerhalb eines markanten Merkmalsrepräsentationsraums.In LEAP trainieren wir ein \textit{embedding} Convolutional Neural Network (CNN), um einen aussagekräftigen Repräsentationsraum durch adaptive Dichtediskriminierung unter Verwendung des Magnet Loss zu erlernen. \textit{student} CNN-Klassifikator wählt dynamisch Proben aus, um einen Mini-Batch zu bilden, der auf der \textit{Einfachheit} von Kreuzentropieverlusten und der \textit{echten Vielfalt} von Beispielen aus dem Repräsentationsraum basiert, der durch das \textit{embedding} Wir evaluieren LEAP unter Verwendung von Deep-CNN-Architekturen für die Aufgabe der überwachten Bildklassifikation auf MNIST, FashionMNIST, CIFAR-10, CIFAR-100 und SVHN und zeigen, dass das LEAP-Framework schneller konvergiert, was die Anzahl der Mini-Batch-Updates angeht, die erforderlich sind, um eine vergleichbare oder bessere Testleistung auf jedem der Datensätze zu erzielen.
Konventionelles tiefes Verstärkungslernen bestimmt typischerweise eine geeignete primitive Aktion in jedem Zeitschritt, was einen enormen Zeit- und Arbeitsaufwand für das Erlernen einer effektiven Strategie erfordert, insbesondere in großen und komplexen Umgebungen.Um dieses Problem grundlegend zu lösen, integrieren wir Makroaktionen, die als Sequenzen von primitiven Aktionen definiert sind, in den primitiven Aktionsraum, um einen erweiterten Aktionsraum zu bilden.Das Problem besteht darin, eine geeignete Makroaktion zu finden, um den primitiven Aktionsraum zu erweitern.  Das Problem besteht darin, eine geeignete Makroaktion zu finden, um den primitiven Aktionsraum zu erweitern. Der Agent, der einen geeigneten erweiterten Aktionsraum verwendet, ist in der Lage, zu einem weiter entfernten Zustand zu springen und somit den Explorationsprozess zu beschleunigen und den Lernprozess zu vereinfachen.In früheren Forschungen wurden Makroaktionen entwickelt, indem die am häufigsten verwendeten Aktionssequenzen analysiert oder frühere Aktionen wiederholt wurden. Allerdings werden die am häufigsten verwendeten Aktionssequenzen aus einer vergangenen Politik extrahiert, was das ursprüngliche Verhalten dieser Politik nur verstärken kann.Andererseits kann die Wiederholung von Aktionen die Vielfalt der Verhaltensweisen des Agenten einschränken.Stattdessen schlagen wir vor, Makro-Aktionen durch einen genetischen Algorithmus zu konstruieren, der die Abhängigkeit der Makro-Aktionsableitung von den vergangenen Politiken des Agenten eliminiert.  Unser Ansatz fügt dem primitiven Aktionsraum einmalig eine Makroaktion hinzu und evaluiert, ob der erweiterte Aktionsraum zu einer vielversprechenden Leistung führt oder nicht.   Wir führen umfangreiche Experimente durch und zeigen, dass die konstruierten Makro-Aktionen in der Lage sind, den Lernprozess für eine Vielzahl von Deep Reinforcement Learning-Methoden zu beschleunigen. Unsere experimentellen Ergebnisse zeigen auch, dass die Makro-Aktionen, die von unserem Ansatz vorgeschlagen werden, zwischen Deep Reinforcement Learning-Methoden und ähnlichen Umgebungen übertragbar sind.
Ein Schlüsselproblem in den Neurowissenschaften und den Biowissenschaften im Allgemeinen ist, dass der Datenerzeugungsprozess oft am besten als eine Hierarchie dynamischer Systeme betrachtet werden kann. Ein Beispiel hierfür sind In-vivo-Kalzium-Bildgebungsdaten, bei denen die beobachteten Kalziumtransienten durch eine Kombination elektrochemischer Kinetik angetrieben werden, bei der die Häufigkeit dieser Transienten durch hypothetische Trajektorien um Mannigfaltigkeiten bestimmt wird. Ein kürzlich vorgestellter Ansatz, der sequentielle Variations-Autocodierer verwendet, hat gezeigt, dass es möglich ist, die latente dynamische Struktur des Erreichungsverhaltens aus Spiking-Daten zu lernen, die als Poisson-Prozess modelliert sind. Hier erweitern wir diesen Ansatz, indem wir eine Ladder-Methode verwenden, um die Spiking-Ereignisse, die die Kalziumtransienten antreiben, zusammen mit dem tieferen latenten dynamischen System abzuleiten.
Trotz des jüngsten Erfolgs der neuronalen maschinellen Übersetzung (NMT) in Standard-Benchmarks stellt der Mangel an großen parallelen Korpora ein großes praktisches Problem für viele Sprachpaare dar.Es gab mehrere Vorschläge, dieses Problem zu lindern, z.B. mit Triangulation und halb-überwachten Lerntechniken, aber sie erfordern immer noch ein starkes sprachübergreifendes Signal.In dieser Arbeit beseitigen wir vollständig die Notwendigkeit paralleler Daten und schlagen eine neuartige Methode vor, um ein NMT-System auf völlig unbeaufsichtigte Weise zu trainieren, indem wir uns nur auf einsprachige Korpora verlassen. Unser Modell baut auf den jüngsten Arbeiten zu unbeaufsichtigten Embedding-Mappings auf und besteht aus einem leicht modifizierten attentionalen Encoder-Decoder-Modell, das allein auf monolingualen Korpora trainiert werden kann, indem eine Kombination aus Denoising und Rückübersetzung verwendet wird. Trotz der Einfachheit des Ansatzes erreicht unser System 15,56 bzw. 10,21 BLEU-Punkte in den WMT 2014-Übersetzungen Französisch-Englisch und Deutsch-Englisch. Das Modell kann auch von kleinen Parallelkorpora profitieren und erreicht 21,81 bzw. 15,24 Punkte, wenn es mit 100.000 parallelen Sätzen kombiniert wird.Unsere Implementierung wird als Open-Source-Projekt veröffentlicht.
Die Schlüsselidee besteht darin, sowohl den Generator als auch den Diskriminator progressiv wachsen zu lassen: Ausgehend von einer niedrigen Auflösung fügen wir neue Schichten hinzu, die mit fortschreitendem Training immer feinere Details modellieren, Wir schlagen auch einen einfachen Weg vor, um die Variation in den generierten Bildern zu erhöhen und erreichen einen Rekordwert von 8,80 in der unüberwachten CIFAR10.Darüber hinaus beschreiben wir mehrere Implementierungsdetails, die wichtig sind, um einen ungesunden Wettbewerb zwischen dem Generator und dem Diskriminator zu verhindern.Schließlich schlagen wir eine neue Metrik für die Bewertung von GAN-Ergebnissen vor, sowohl in Bezug auf die Bildqualität als auch auf die Variation.Als zusätzlichen Beitrag konstruieren wir eine qualitativ hochwertigere Version des CelebA-Datensatzes.
Die Entwicklung einer Faltung für ein sphärisches neuronales Netz erfordert einen heiklen Kompromiss zwischen Effizienz und Rotationsäquivarianz.DeepSphere, eine Methode, die auf einer Graphendarstellung der diskretisierten Sphäre basiert, stellt ein kontrollierbares Gleichgewicht zwischen diesen beiden Desideraten her.Dieser Beitrag ist zweifach. Erstens untersuchen wir sowohl theoretisch als auch empirisch, wie die Äquivarianz durch den zugrundeliegenden Graphen in Bezug auf die Anzahl der Pixel und Nachbarn beeinflusst wird.Zweitens evaluieren wir DeepSphere auf relevanten Problemen.Experimente zeigen State-of-the-Art-Leistung und demonstriert die Effizienz und Flexibilität dieser Formulierung.Vielleicht überraschend, Vergleich mit früheren Arbeiten legt nahe, dass anisotrope Filter ein unnötiger Preis zu zahlen sein könnte.
Der Begriff des stationären Gleichgewichtsensembles hat in der statistischen Mechanik eine zentrale Rolle gespielt, und auch beim maschinellen Lernen dient das Training als verallgemeinerte Äquilibrierung, die die Wahrscheinlichkeitsverteilung der Modellparameter in Richtung Stationarität treibt.Hier leiten wir stationäre Fluktuations-Dissipations-Beziehungen ab, die messbare Größen und Hyperparameter im stochastischen Gradientenabstiegsalgorithmus miteinander verbinden. Diese Relationen gelten exakt für jeden stationären Zustand und können insbesondere dazu verwendet werden, den Trainingsplan adaptiv festzulegen.Wir können die Relationen außerdem dazu verwenden, um effizient Informationen zu extrahieren, die zu einer Verlustfunktionslandschaft gehören, wie z.B. die Größen ihrer Hessian und Anharmonizität.Unsere Behauptungen werden empirisch verifiziert.
Rekurrente neuronale Netze (RNNs) sind schwierig auf Sequenzverarbeitungsaufgaben zu trainieren, nicht nur, weil das Eingangsrauschen durch Rückkopplung verstärkt werden kann, sondern auch, weil jede Ungenauigkeit in den Gewichten ähnliche Konsequenzen wie das Eingangsrauschen hat.Wir beschreiben eine Methode zur Denoisierung des verborgenen Zustands während des Trainings, um robustere Darstellungen zu erreichen und dadurch die Generalisierungsleistung zu verbessern.Attraktor-Dynamiken werden in den verborgenen Zustand integriert, um Darstellungen bei jedem Schritt einer Sequenz zu "säubern".Die Attraktor-Dynamiken werden durch einen zusätzlichen Denoisierungsverlust trainiert, um zuvor erfahrene verborgene Zustände aus verrauschten Versionen dieser Zustände wiederherzustellen. Dieses zustandsentfremdete rekurrente neuronale Netz (SDRNN) führt mehrere Schritte der internen Verarbeitung für jeden externen Sequenzschritt durch. Bei einer Reihe von Aufgaben zeigen wir, dass das SDRNN ein allgemeines RNN sowie eine Variante des SDRNN mit Attraktor-Dynamik auf dem verborgenen Zustand, aber ohne den Hilfsverlust, übertrifft. Wir argumentieren, dass Attraktor-Dynamik - und entsprechende Konnektivitätsbeschränkungen - eine wesentliche Komponente des Deep-Learning-Arsenals sind und nicht nur für rekurrente Netze, sondern auch für die Verbesserung tiefer Feedforward-Netze und den Intertask-Transfer herangezogen werden sollten.
Wir betrachten Verstärkungslernen in input-gesteuerten Umgebungen, in denen ein exogener, stochastischer Input-Prozess die Dynamik des Systems beeinflusst.Input-Prozesse treten in vielen Anwendungen auf, einschließlich Warteschlangensystemen, Robotik-Steuerung mit Störungen und Objektverfolgung.Da die Zustandsdynamik und die Belohnungen vom Input-Prozess abhängen, bietet der Zustand allein nur begrenzte Informationen für die erwarteten zukünftigen Erträge.Daher leiden Policy-Gradient-Methoden mit standardmäßigen zustandsabhängigen Grundlinien unter einer hohen Varianz während des Trainings. Anschließend schlagen wir einen Meta-Lernansatz vor, um die Komplexität des Lernens einer Basislinie zu überwinden, die von einer langen Sequenz von Eingaben abhängt. Unsere experimentellen Ergebnisse zeigen, dass eingabeabhängige Basislinien in Umgebungen wie Warteschlangensystemen, Computernetzwerken und MuJoCo-Roboter-Lokomotion durchweg die Trainingsstabilität verbessern und zu besseren endgültigen Richtlinien führen.
Die von den Klassifizierungsnetzwerken gelernten Parameter vernachlässigen jedoch in der Regel die stilistischen Informationen der Eingabe zugunsten von Informationen, die ausschließlich für die Klassifizierung relevant sind. Die generative Kapazität unseres Netzwerks zeigt, dass die Kombination von Neuronen mit Stilgedächtnis und Klassifizierungsneuronen gute Rekonstruktionen der Eingaben liefert, wenn die Klassifizierung korrekt ist.
Routing-Modelle, eine Form der bedingten Berechnung, bei der Beispiele durch eine Teilmenge von Komponenten in einem größeren Netzwerk geleitet werden, haben in den letzten Arbeiten vielversprechende Ergebnisse gezeigt.Überraschenderweise haben Routing-Modelle bisher wichtige Eigenschaften vermissen lassen, wie z.B. architektonische Vielfalt und eine große Anzahl von Routing-Entscheidungen.Sowohl die architektonische Vielfalt als auch die Routing-Tiefe können die Darstellungskraft eines Routing-Netzwerks erhöhen.In dieser Arbeit befassen wir uns mit diesen beiden Mängeln. In unseren Experimenten stellen wir fest, dass das Hinzufügen von architektonischer Diversität zu Routing-Modellen die Leistung erheblich verbessert und die Fehlerraten einer starken Basislinie um 35% auf einem Omniglot-Setup reduziert. Wenn wir jedoch die Routing-Tiefe erhöhen, stellen wir fest, dass moderne Routing-Techniken mit der Optimierung kämpfen.
Obwohl die Verwendung von Deep-Learning-Techniken vorgeschlagen wurde, wurde die Verwendung durch die Tatsache eingeschränkt, dass die Trainingsdaten mit Hilfe von PDE-Lösern gewonnen werden, wodurch die Verwendung auf Bereiche beschränkt wurde, in denen der PDE-Löser anwendbar war, aber nicht darüber hinaus. Wir stellen Methoden für das Training auf kleinen Domänen vor, während wir die trainierten Modelle auf größeren Domänen anwenden, wobei Konsistenzbedingungen sicherstellen, dass die Lösungen auch an den Grenzen der kleinen Domänen physikalisch sinnvoll sind.
Wir befassen uns mit dem Problem des Limit-Cycling-Verhaltens beim Training von Generative Adversarial Networks und schlagen die Verwendung von Optimistic Mirror Decent (OMD) für das Training von Wasserstein-GANs vor. Jüngste theoretische Ergebnisse haben gezeigt, dass Optimistic Mirror Decent (OMD) im Kontext von Nullsummenspielen schnellere Regret-Raten erzielen kann.  Wir zeigen, dass im Falle von bi-linearen Nullsummenspielen die letzte Iteration der OMD-Dynamik zu einem Gleichgewicht konvergiert, im Gegensatz zur GD-Dynamik, die zwangsläufig zyklisch ist. Wir zeigen auch den großen qualitativen Unterschied zwischen GD- und OMD-Dynamik anhand von Spielzeugbeispielen, selbst wenn GD mit vielen Anpassungen modifiziert wird, die in der jüngsten Literatur vorgeschlagen wurden, wie z.B. Gradientenstrafe oder Momentum. Wir wenden das OMD-WGAN-Training auf ein bioinformatisches Problem der Generierung von DNA-Sequenzen an und stellen fest, dass mit OMD trainierte Modelle durchweg eine geringere KL-Divergenz in Bezug auf die wahre zugrundeliegende Verteilung erreichen als Modelle, die mit GD-Varianten trainiert wurden.Schließlich stellen wir einen neuen Algorithmus, Optimistic Adam, vor, der eine optimistische Variante von Adam ist.Wir wenden ihn auf das WGAN-Training auf CIFAR10 an und stellen eine verbesserte Leistung in Bezug auf die Inception Score im Vergleich zu Adam fest.
Matrixfaktorisierung ist die grundlegende Idee, um die Repräsentationen von Nutzern und Elementen durch die Zerlegung der gegebenen Interaktionsmatrix abzuleiten. Bestehende Ansätze, die auf Matrixfaktorisierung basieren, haben jedoch die Einschränkung, dass die Interaktion zwischen der Einbettung von Nutzern und der Einbettung von Elementen nur schwach erzwungen wird, indem der gegebene individuelle Bewertungswert angepasst wird, wodurch potenziell nützliche Informationen verloren gehen können. Trotz der Einfachheit des von uns vorgeschlagenen Ansatzes zeigen umfangreiche Experimente an vier öffentlichen impliziten Feedback-Datensätzen, dass unser Ansatz den State-of-the-Art-Ansätzen überlegen ist. Darüber hinaus zeigt die Ablationsstudie, dass durch die Verwendung von Multi-Hot-Codierung zur Anreicherung der Benutzereinbettung und der Elementeinbettung für die verallgemeinerte Matrixfaktorisierung eine bessere Leistung, schnellere Konvergenz und ein geringerer Trainingsverlust erreicht werden kann.
Es basiert auf einem hierarchischen generativen Modell, das aus zwei Ebenen besteht: Auf der ersten Ebene lernt ein Modell Repräsentationen, um beobachtete Daten zu generieren; auf der zweiten Ebene kodieren Repräsentationszustände die Dynamik der unteren Ebene; das Modell ist als Bayes'sches Netzwerk mit Schaltvariablen konzipiert, die auf der höheren Ebene repräsentiert werden und Übergangsmodelle erzeugen. Das Verfahren erkundet aktiv den latenten Raum, geleitet von seinem Wissen und der Unsicherheit darüber, indem es die latenten Variablen aus Vorhersagefehlersignalen aktualisiert, die in den latenten Raum rückproportioniert werden.Es werden also keine Encoder- oder Inferenzmodelle verwendet, da die Generatoren auch als deren inverse Transformationen dienen.Das Verfahren wird in zwei Szenarien evaluiert, mit statischen Bildern und mit Videos.Die Ergebnisse zeigen, dass die Anpassung über die Zeit zu einer besseren Leistung führt als bei ähnlichen Architekturen ohne zeitliche Abhängigkeiten, z.B. variationale Autokoder, Bei Videos zeigt sich, dass das System die Dynamik der Daten in Zuständen extrahiert, die in hohem Maße mit der Grundwahrheit der beobachteten Handlungen korrelieren.
Während Rectified Linear Unit (ReLU) ist die erfolgreichste Aktivierungsfunktion, seine Derivate haben gezeigt, überlegene Leistung auf Benchmark-Datensätze.In dieser Arbeit, erforschen wir die Polynome als Aktivierungsfunktionen (Ordnung â‰¥ 2), die kontinuierliche reelle Funktion innerhalb eines bestimmten Intervalls annähern kann.Leveraging diese Eigenschaft, die wichtigste Idee ist, die Nichtlinearität zu lernen, zu akzeptieren, dass die daraus resultierende Funktion möglicherweise nicht monoton sein. Um dieses Problem in den Griff zu bekommen, führen wir eine dynamische Eingangsskalierung, eine Ausgangsskalierung und eine niedrigere Lernrate für die Polynomgewichte ein, um die abrupten Schwankungen der Polynome zwischen den Gewichtsaktualisierungen zu kontrollieren. In Experimenten mit drei öffentlichen Datensätzen stimmt unsere vorgeschlagene Methode mit der Leistung früherer Aktivierungsfunktionen überein und bietet so einen Einblick in die Nichtlinearitätspräferenz eines Netzwerks.
Wir stellen CBF, eine Explorationsmethode, die in Abwesenheit von Belohnungen oder Ende der Episode Signal funktioniert.CBF basiert auf intrinsische Belohnung aus dem Fehler eines Dynamikmodells im Feature Space abgeleitet.Es wurde von (Pathak et al., 2017) inspiriert, ist einfach zu implementieren, und kann Ergebnisse wie vier Ebenen von Super Mario Bros passieren, navigieren VizDoom Labyrinthe und zwei Ebenen von SpaceInvaders passieren.Wir untersuchten die Wirkung der Kombination der Methode mit mehreren Hilfsaufgaben, aber finden inkonsistente Verbesserungen gegenüber der CBF-Basislinie.
Dieses Papier befasst sich mit der Robustheit von VAEs gegenüber Angriffen und zeigt, dass konventionelle VAEs bei Angriffen brüchig sind, aber dass kürzlich eingeführte Methoden zur Entflechtung wie Î²-TCVAE (Chen et al., 2018) die Robustheit verbessern, wie durch eine Vielzahl von zuvor vorgeschlagenen adversarischen Angriffen (Tabacof et al. (2016); Gondim-Ribeiro et al. (2018); Kos et al.(2018)) demonstriert wurde.Dies motivierte uns, Seatbelt-VAE zu entwickeln, eine neue hierarchische entwirrte VAE, die so konzipiert ist, dass sie deutlich robuster gegenüber adversarischen Angriffen ist als bestehende Ansätze, während sie qualitativ hochwertige Rekonstruktionen beibehält.
Der Backpropagation-Algorithmus ist aufgrund seiner empirischen Ergebnisse der De-facto-Standard für die Kreditzuweisung in künstlichen neuronalen Netzen. Seit seiner Entwicklung sind Varianten des Backpropagation-Algorithmus entstanden, insbesondere Varianten, die Funktionsänderungen in den Backpropagation-Gleichungen nutzen, um ihren spezifischen Anforderungen gerecht zu werden.Feedback Alignment ist ein solches Beispiel, das die Gewichtstranspositionsmatrix in den Backpropagation-Gleichungen durch eine Zufallsmatrix ersetzt, um einen biologisch plausibleren Kreditzuweisungsalgorithmus zu finden. In dieser Arbeit zeigen wir, dass Funktionsänderungen in der Backpropagation-Prozedur gleichbedeutend sind mit dem Hinzufügen einer impliziten Lernrate zu einem künstlichen neuronalen Netzwerk. Darüber hinaus lernen wir Aktivierungsfunktionsableitungen in den Backpropagation-Gleichungen, um eine frühe Konvergenz in diesen künstlichen neuronalen Netzwerken zu demonstrieren. Unsere Arbeit zeigt konkurrenzfähige Leistungen mit früher Konvergenz bei MNIST und CIFAR10 auf ausreichend großen tiefen neuronalen Netzwerkarchitekturen.
Unüberwachter Textstiltransfer ist die Aufgabe, einen Text eines bestimmten Stils in einen Zielstil umzuschreiben, ohne ein paralleles Korpus von Sätzen im Quell- und Zielstil für das Training zu verwenden. Stiltransfersysteme werden nach ihrer Fähigkeit bewertet, Sätze zu generieren, die1) den Zielstil besitzen,2) flüssig und natürlich klingen und3) die nicht-stilistischen Teile (Inhalt) des Quellsatzes bewahren. Wir trainieren ein auf Reinforcement Learning (RL) basierendes unüberwachtes Stiltransfersystem, das Belohnungen für die oben genannten Maßnahmen einbezieht, und beschreiben neuartige Methoden zur Gestaltung von Belohnungen für dieselben.Unser Ansatz versucht nicht, Stil und Inhalt zu trennen, und nutzt die Leistung von massiv vortrainierten Sprachmodellen sowie des Transformers.Unser System übertrifft bestehende State-of-the-Art-Systeme auf der Grundlage menschlicher und automatischer Bewertungen des Zielstils, der Flüssigkeit und der Bewahrung des Inhalts sowie des Gesamterfolgs des Stiltransfers auf einer Vielzahl von Datensätzen erheblich.
Trotz des Erfolges von Generative Adversarial Networks (GANs) in der Bildsynthese ist noch nicht ausreichend bekannt, was die Netzwerke innerhalb der tiefen generativen Repräsentationen gelernt haben und wie fotorealistische Bilder aus zufälligem Rauschen zusammengesetzt werden können. In dieser Arbeit zeigen wir, dass sich aus den generativen Repräsentationen eine hochstrukturierte semantische Hierarchie als Variationsfaktor für die Synthese von Szenen ergibt. indem wir die schichtweisen Repräsentationen mit einem breiten Satz visueller Konzepte auf verschiedenen Abstraktionsebenen untersuchen, sind wir in der Lage, die Kausalität zwischen den Aktivierungen und der im Ausgangsbild auftretenden Semantik zu quantifizieren. Die qualitativen und quantitativen Ergebnisse deuten darauf hin, dass die generativen Repräsentationen, die von GANs erlernt werden, darauf spezialisiert sind, verschiedene hierarchische Semantiken zu synthetisieren: die frühen Schichten neigen dazu, das räumliche Layout und die Konfiguration zu bestimmen, die mittleren Schichten kontrollieren die kategorischen Objekte, und die späteren Schichten schließlich geben die Szenenattribute sowie das Farbschema wieder.die Identifizierung eines solchen Satzes von manipulierbaren latenten Semantiken erleichtert die semantische Szenenmanipulation.
Variationale Autocodierer (VAEs), die über SMILES-Strings und graph-basierte Repräsentationen von Molekülen definiert sind, versprechen, die Optimierung molekularer Eigenschaften zu verbessern und damit die Pharma- und Materialindustrie zu revolutionieren, werden jedoch durch die nicht-eindeutige Natur von SMILES-Strings und die Rechenkosten von Graph-Faltungen behindert. Um Nachrichten effizient entlang aller Pfade durch den molekularen Graphen zu übermitteln, kodieren wir mehrere SMILES-Strings eines einzelnen Moleküls mit einer Reihe von gestapelten rekurrenten neuronalen Netzen, harmonisieren die verborgenen Repräsentationen jedes Atoms zwischen den SMILES-Repräsentationen und verwenden Aufmerksamkeitspooling, um eine endgültige latente Repräsentation mit fester Länge aufzubauen. Durch die anschließende Dekodierung in eine disjunkte Menge von SMILES-Strings des Moleküls erlernt unsere All-SMILES-VAE eine nahezu bijektive Abbildung zwischen Molekülen und latenten Repräsentationen in der Nähe des hochwahrscheinlichen Massenunterraums des Priors. Unsere von SMILES abgeleiteten, aber auf Molekülen basierenden latenten Repräsentationen übertreffen den Stand der Technik in einer Vielzahl von voll- und halbüberwachten Eigenschaftsregressions- und Moleküleigenschaftsoptimierungsaufgaben erheblich.
Wir schlagen eine einfache, aber hocheffektive Methode vor, die das Problem des Modus-Kollapses im Conditional Generative Adversarial Network (cGAN) angeht, Obwohl konditionale Verteilungen in der Praxis multimodal sind (d.h. viele Modi haben), neigen die meisten cGAN-Ansätze dazu, eine übermäßig vereinfachte Verteilung zu erlernen, bei der eine Eingabe immer auf eine einzige Ausgabe abgebildet wird, unabhängig von Variationen im latenten Code.Um dieses Problem anzugehen, schlagen wir vor, den Generator explizit zu regularisieren, um verschiedene Ausgaben in Abhängigkeit von latenten Codes zu erzeugen.Die vorgeschlagene Regularisierung ist einfach, allgemein und kann leicht in die meisten konditionalen GAN-Ziele integriert werden. Wir demonstrieren die Wirksamkeit unserer Methode auf drei bedingte Generation Aufgaben: Bild-zu-Bild-Übersetzung, Bild inpainting, und zukünftige Video prediction.We zeigen, dass einfache Zugabe von unseren Regularisierung zu bestehenden Modellen führt zu überraschend vielfältigen Generationen, wesentlich besser als die bisherigen Ansätze für multimodale bedingte Generation speziell in jeder einzelnen Aufgabe.
Der Transformator ist ein hochmodernes neuronales Ãœbersetzungsmodell, das Aufmerksamkeit nutzt, um lexikalische ReprÃ€sentationen iterativ mit Informationen aus dem umgebenden Kontext zu verfeinern. Lexikalische Merkmale werden in die erste Schicht eingespeist und durch ein tiefes Netzwerk versteckter Schichten propagiert. Wir argumentieren, dass die Notwendigkeit, lexikalische Merkmale in jeder Schicht zu reprÃ€sentieren und zu propagieren, die KapazitÃ€t des Modells zum Lernen und ReprÃ€sentieren anderer fÃŒr die Aufgabe relevanter Informationen einschrÃ€nkt.â€œ Um diesen Engpass zu mildern, fÃŒhren wir gated shortcut-Verbindungen zwischen der Einbettungsschicht und jeder nachfolgenden Schicht innerhalb des Encoders und Decoders ein. Wir zeigen, dass die vorgeschlagene Modifikation zu konsistenten Verbesserungen bei Standard-WMT-Übersetzungsaufgaben führt und die Menge an lexikalischen Informationen, die entlang der versteckten Schichten weitergegeben werden, reduziert.Wir evaluieren außerdem verschiedene Möglichkeiten, lexikalische Verbindungen in die Transformer-Architektur zu integrieren und präsentieren Ablationsexperimente, die die Auswirkungen der vorgeschlagenen Verknüpfungen auf das Modellverhalten untersuchen.
Die Schätzung von Wahrscheinlichkeitsdichten ist ein klassisches und gut untersuchtes Problem, aber Standard-Dichte-Schätzungsmethoden waren in der Vergangenheit nicht in der Lage, komplexe und hochdimensionale Bildverteilungen zu modellieren.  Neuere generative Modelle nutzen die Leistungsfähigkeit neuronaler Netze, um implizit Wahrscheinlichkeitsmodelle über komplexe Bilder zu lernen und darzustellen.  Wir beschreiben Methoden, um explizite Wahrscheinlichkeitsdichteschätzungen aus GANs zu extrahieren, und untersuchen die Eigenschaften dieser Bilddichtefunktionen.  Wir führen Experimente zur Überprüfung der Richtigkeit durch, um nachzuweisen, dass diese Wahrscheinlichkeiten angemessen sind.  Wir zeigen jedoch auch, dass Dichtefunktionen natürlicher Bilder schwer zu interpretieren und daher nur begrenzt einsetzbar sind.  Wir untersuchen die Gründe für diesen Mangel an Interpretierbarkeit und schlagen vor, dass wir eine bessere Interpretierbarkeit erreichen können, indem wir Dichteabschätzungen auf latenten Repräsentationen von Bildern durchführen.  
Das Design der regulären Faltung basiert auf dem rezeptiven Feld (RF), in dem die Informationen innerhalb einer bestimmten Region verarbeitet werden. Bei der Betrachtung des RF der regulären Faltung werden die Ausgaben der Neuronen in den unteren Schichten mit kleinerem RF gebündelt, um Neuronen in den höheren Schichten mit größerem RF zu erzeugen. Infolgedessen sind die Neuronen in den höheren Schichten in der Lage, den globalen Kontext zu erfassen, obwohl die Neuronen in den unteren Schichten nur die lokalen Informationen sehen.In den unteren Schichten des biologischen Gehirns verändern jedoch die Informationen außerhalb des RF die Eigenschaften der Neuronen.In dieser Arbeit erweitern wir die reguläre Faltung und schlagen eine räumlich gemischte Faltung (ss-Faltung) vor. Wir führen Experimente mit den Datensätzen CIFAR-10 und ImageNet-1k durch und zeigen, dass die ss-Faltung die Klassifikationsleistung verschiedener CNNs verbessert.
Wir schlagen einen Rahmen vor, um die Verteilung von sequentiellen Daten zu modellieren, die von einer Gruppe von Entitäten stammen, die in einem Graphen mit einer bekannten Topologie verbunden sind.Die Methode basiert auf einer Mischung von gemeinsamen Hidden Markov Modellen (HMMs), die trainiert werden, um das Wissen über die Graphenstruktur auszunutzen, und zwar so, dass die erhaltenen Mischungen dazu neigen, spärlich zu sein.Experimente in verschiedenen Anwendungsdomänen zeigen die Effektivität und Vielseitigkeit der Methode.
Um in Multi-Agenten-Szenen hohe Gewinne zu erzielen, ist es manchmal notwendig, andere Agenten zu verstehen und entsprechende optimale Entscheidungen zu treffen.Wir können diese Aufgaben lösen, indem wir zunächst Modelle für andere Agenten erstellen und dann die optimale Politik mit diesen Modellen finden.Um ein genaues Modell zu erhalten, sind viele Beobachtungen erforderlich, und dies kann stichprobenineffizient sein.Darüber hinaus können das gelernte Modell und die Politik zu sehr an die aktuellen Agenten angepasst werden und nicht verallgemeinert werden, wenn die anderen Agenten durch neue Agenten ersetzt werden. In vielen praktischen Situationen kann jeder Agent, mit dem wir konfrontiert sind, als eine Stichprobe aus einer Population mit einer festen, aber unbekannten Verteilung betrachtet werden, so dass wir die Aufgabe gegen einige spezifische Agenten als eine Aufgabe behandeln können, die aus einer Aufgabenverteilung entnommen wurde.Wir wenden die Methode des Meta-Lernens an, um Modelle zu erstellen und Strategien zu erlernen, so dass wir uns, wenn neue Agenten auftauchen, effizient an sie anpassen können.Experimente mit Grid-Spielen zeigen, dass unsere Methode schnell hohe Belohnungen erzielen kann.
Wir charakterisieren die singulären Werte der linearen Transformation, die mit einer Standard-2D-Mehrkanal-Faltungsschicht verbunden sind, und ermöglichen so ihre effiziente Berechnung.  Diese Charakterisierung führt auch zu einem Algorithmus für die Projektion einer Faltungsschicht auf eine Operator-Norm-Kugel. Wir zeigen, dass dies ein effektiver Regularisierer ist; zum Beispiel verbessert er den Testfehler eines tiefen Residualnetzes mit Batch-Normalisierung auf CIFAR-10 von 6,2% auf 5,3%.
Eine Bayes-optimale Strategie, die dies optimal tut, konditioniert ihre Aktionen nicht nur auf den Zustand der Umgebung, sondern auch auf die Ungewissheit des Agenten über die Umgebung.Die Berechnung einer Bayes-optimalen Strategie ist jedoch für alle außer für die kleinsten Aufgaben schwierig. In diesem Beitrag stellen wir variational Bayes-Adaptive Deep RL (variBAD) vor, eine Methode zum Meta-Lernen, um approximative Inferenz in einer unbekannten Umgebung durchzuführen und die Unsicherheit der Aufgabe direkt während der Aktionsauswahl zu berücksichtigen. variBAD führt strukturierte Online-Exploration als Funktion der Unsicherheit der Aufgabe in einer Grid-World-Domäne durch. variBAD wird auch auf MuJoCo-Domänen evaluiert, die häufig in Meta-RL verwendet werden, und zeigt, dass es während des Trainings einen höheren Ertrag erzielt als bestehende Methoden.
Während tiefe neuronale Netze in der klassischen Umgebung einen durchschlagenden Erfolg erzielt haben, ist bekannt, dass sie das in früheren Lernepisoden erworbene Wissen vergessen, wenn die Beispiele, die in der aktuellen Lernepisode vorkommen, sich drastisch von denen unterscheiden, die in früheren Episoden vorkamen. In diesem Papier schlagen wir ein neues Modell vor, das sowohl die Ausdruckskraft von tiefen neuronalen Netzen nutzen kann als auch resistent gegen das Vergessen ist, wenn neue Kategorien eingeführt werden.
Biomedizinische Wissensdatenbanken sind von entscheidender Bedeutung in der modernen datengesteuerten biomedizinischen Wissenschaften, aber auto-mated biomedizinischen Wissensbasis Bau bleibt challenging.in diesem Papier, betrachten wir das Problem der Krankheit Entität Normalisierung, eine wesentliche Aufgabe bei der Konstruktion einer biomedizinischen Wissensbasis.  Wir stellen NormCo vor, ein tiefes Kohärenzmodell, das sowohl die Semantik einer Entitätserwähnung als auch die thematische Kohärenz der Erwähnungen innerhalb eines einzelnen Dokuments berücksichtigt. NormCo modelliert Entitätserwähnungen mit Hilfe eines einfachen semantischen Modells, das Phrasenrepräsentationen aus Worteinbettungen zusammensetzt, und behandelt Kohärenz als eine Sequenz von Krankheitsbegriffen und Ko-Erwähnungen mit Hilfe eines RNN, anstatt die gemeinsame Wahrscheinlichkeit aller Begriffe in einem Dokument zu modellieren, was NP-harte Inferenz erfordert.  Um das Problem der spärlichen Daten zu überwinden, haben wir fernüberwachte Daten und synthetische Daten verwendet, die mit Prioritäten aus dem BioASQ-Datensatz generiert wurden.  Unsere experimentellen Ergebnisse zeigen, dassNormCo bei zwei Krankheitsnormalisierungs-Korpora in Bezug auf (1) Vorhersagequalität und (2) Effizienz besser abschneidet als modernste Basismethoden und in Bezug auf Genauigkeit und F1-Score bei getaggten Dokumenten mindestens genauso leistungsfähig ist.
Wir untersuchen die Rolle der multiplikativen Interaktion als vereinheitlichenden Rahmen, um eine Reihe klassischer und moderner neuronaler Netzwerkarchitekturen zu beschreiben, wie z.B. Gating, Aufmerksamkeitsschichten, Hypernetze und dynamische Faltungen u.a. Multiplikative Interaktionsschichten als primitive Operationen sind in der Literatur seit langem etabliert, werden aber oft nicht hervorgehoben und daher unterschätzt. Wir beginnen damit, dass wir zeigen, dass solche Schichten die darstellbaren Funktionsklassen neuronaler Netze stark bereichern, und wir vermuten, dass multiplikative Interaktionen eine besonders starke induktive Verzerrung bieten, wenn mehrere Informationsströme fusioniert werden oder wenn bedingte Berechnungen erforderlich sind. Schließlich untermauern wir unsere Behauptungen und demonstrieren das Potenzial multiplikativer Interaktionen, indem wir sie in groß angelegten, komplexen RL- und Sequenzmodellierungsaufgaben anwenden, bei denen wir dank ihrer Verwendung Ergebnisse auf dem neuesten Stand der Technik erzielen können, und liefern damit neue Beweise dafür, dass multiplikative Interaktionen beim Entwurf neuer neuronaler Netzarchitekturen eine wichtigere Rolle spielen.
Die Entwicklung bedingter generativer Modelle für die Text-Video-Synthese ist ein äußerst anspruchsvolles, aber wichtiges Forschungsthema im Bereich des maschinellen Lernens. In dieser Arbeit gehen wir dieses Problem an, indem wir ein Text-Filter-konditionierendes generatives adversarisches Netzwerk (TFGAN) einführen, ein GAN-Modell mit einem neuartigen Konditionierungsschema, das zur Verbesserung der Text-Video-Assoziationen beiträgt. Mit einer Kombination aus dieser Konditionierung Schema und eine tiefe GAN-Architektur, TFGAN generiert fotorealistische Videos von Text auf sehr anspruchsvolle realen video datasets.Additionally konstruieren wir eine Benchmark synthetischen Datensatz von bewegten Formen systematisch unsere Konditionierung scheme.Extensive Experimente zeigen, dass TFGAN deutlich übertrifft die bestehenden Ansätze, und kann auch Videos von neuartigen Kategorien nicht während der Ausbildung gesehen zu generieren.
Überparametrisierung ist heutzutage bei der Ausbildung neuronaler Netze allgegenwärtig, um sowohl die Optimierung bei der Suche nach globalen Optima als auch die Verallgemeinerung bei der Verringerung des Vorhersagefehlers zu fördern.komprimierende Netze sind jedoch in vielen realen Anwendungen erwünscht und die direkte Ausbildung kleiner Netze kann in lokalen Optima gefangen sein. Anstatt überparametrisierte Modelle zu beschneiden oder zu destillieren, schlagen wir in diesem Papier einen neuen Ansatz vor, der auf \emph{differentiellen Einschlüssen von inversen Skalenräumen} basiert und eine Familie von einfachen bis komplexen Modellen erzeugt, indem er Gradientenabstieg und Spiegelabstieg verbindet, um die strukturelle Spärlichkeit des Modells zu erkunden. Es verfügt über eine einfache Diskretisierung, die so genannte Split Linearized Bregman Iteration (SplitLBI), deren globale Konvergenzanalyse beim Deep Learning feststellt, dass die algorithmischen Iterationen von beliebigen Initialisierungen aus zu einem kritischen Punkt empirischer Risiken konvergieren. Experimentelle Beweise zeigen, dass SplitLBI beim Training in großem Maßstab auf dem ImageNet-2012-Datensatz usw. eine Spitzenleistung erzielen kann, während es mit \emph{frühzeitigem Stoppen} eine effektive Subnetz-Architektur mit vergleichbarer Testgenauigkeit wie dichte Modelle nach der Umschulung enthüllt, anstatt gut trainierte Modelle zu stutzen.
In diesem Beitrag untersuchen wir den erlernten iterativen Schrumpfungsschwellenwert-Algorithmus (LISTA) zur Lösung von Sparse-Coding-Problemen.  Um dieses Problem zu lösen, wird ein Gated-Mechanismus eingeführt, der einer theoretischen Analyse zugänglich ist.Das spezifische Design der Gates wird durch Konvergenzanalysen des Mechanismus inspiriert, so dass seine Effektivität formal garantiert werden kann.Zusätzlich zu den Gates für die Verstärkung führen wir Gates für das Überschwingen ein, um eine unzureichende Schrittgröße in LISTA zu kompensieren.Umfangreiche empirische Ergebnisse bestätigen unsere theoretischen Erkenntnisse und verifizieren die Effektivität unserer Methode.
Das Lernen von hierarchischen Repräsentationen für die Bildklassifikation hat eine beeindruckende Reihe von Erfolgen erlebt, die zum Teil auf die Verfügbarkeit von großen gelabelten Daten für das Training zurückzuführen sind, andererseits wurden die trainierten Klassifikatoren traditionell auf einer Handvoll Testbilder evaluiert, die als extrem spärlich im Raum aller natürlichen Bilder verteilt gelten. Es ist daher fraglich, ob die jüngsten Leistungsverbesserungen auf den exzessiv wiederverwendeten Testsätzen auf reale natürliche Bilder mit viel reichhaltigeren Inhaltsvariationen verallgemeinert werden können.Darüber hinaus zeigen Studien über adversariales Lernen, dass es mühelos möglich ist, adversarische Beispiele zu konstruieren, die fast alle Bildklassifikatoren täuschen, was den relativen Leistungsvergleich bestehender Modelle noch komplizierter macht. In dieser Arbeit wird ein effizienter Rahmen für den Vergleich von Bildklassifikatoren vorgestellt, den wir MAximum Discrepancy (MAD)-Wettbewerb nennen: Anstatt Bildklassifikatoren auf festen Testsätzen zu vergleichen, ziehen wir adaptiv einen Testsatz aus einem beliebig großen Korpus unbeschrifteter Bilder, um die Diskrepanzen zwischen den Klassifikatoren zu maximieren, gemessen an der Distanz über die WordNet-Hierarchie. Wir berichten über die MAD-Wettbewerbsergebnisse von elf ImageNet-Klassifikatoren und weisen darauf hin, dass der Rahmen leicht erweiterbar und kostengünstig ist, um weitere Klassifikatoren in den Wettbewerb aufzunehmen.
Die Robustheit neuronaler Netze wurde vor kurzem durch negative Beispiele hervorgehoben, d.h., In diesem Papier entwerfen wir eine neue CNN-Architektur, die von sich aus eine gute Robustheit aufweist. Wir führen eine einfache, aber leistungsstarke Technik, Random Mask, ein, um bestehende CNN-Strukturen zu verändern. Wir zeigen, dass ein CNN mit Random Mask die beste Leistung gegen Blackbox-Angriffe erzielt, ohne dass ein Training mit Gegenspielern erforderlich ist.Als Nächstes untersuchen wir die Gegenbeispiele, die ein CNN mit Random Mask "täuschen".Überraschenderweise stellen wir fest, dass diese Gegenbeispiele oft auch Menschen "täuschen".Dies wirft grundlegende Fragen auf, wie man Gegenbeispiele und Robustheit richtig definiert.
Ãœberwachte Deep-Learning-Methoden erfordern sauber beschriftete, groÃŸe DatensÃ?tze, aber das Sammeln solcher Daten ist schwierig und manchmal unmÃ¶glich.â€œ Es gibt zwei populÃ?re Rahmenwerke, um dieses Problem zu lindern: halb-Ãœberwachtes Lernen und robustes Lernen bei Beschriftungsrauschen.â€œ Obwohl diese Rahmenwerke die BeschrÃ?nkung des Ã?berwachten Lernens lockern, werden sie unabhÃ?ngig voneinander untersucht.â€œ Daher bleibt das Trainingsschema, das geeignet ist, wenn nur kleine sauber beschriftete Daten verfÃ?gbar sind, unbekannt. In dieser Studie betrachten wir das Lernen aus bi-qualitativen Daten als eine Verallgemeinerung dieser Studien, in denen ein kleiner Teil der Daten sauber beschriftet ist und der Rest verfälscht ist.In diesem Rahmen vergleichen wir aktuelle Algorithmen für halb-überwachtes und robustes Lernen.Die Ergebnisse deuten darauf hin, dass halb-überwachtes Lernen das robuste Lernen mit verrauschten Beschriftungen übertrifft.Wir schlagen auch eine Trainingsstrategie für das Mischen von Verwechslungstechniken vor, um aus solchen bi-qualitativen Daten effektiv zu lernen.
Hierarchical Sparse Coding (HSC) ist ein leistungsfähiges Modell zur effizienten Darstellung von mehrdimensionalen, strukturierten Daten wie z.B. Bildern. Die einfachste Lösung für dieses rechenintensive Problem besteht darin, es in unabhängige schichtweise Teilprobleme zu zerlegen. In dieser Studie wird ein neues Modell mit der Bezeichnung Sparse Deep Predictive Coding (SDPC) eingeführt, um die Auswirkungen dieser Rückkopplungsverbindung zwischen den Schichten zu bewerten, und zwar im Vergleich zu einem hierarchischen Lasso-Netz (Hi-La), das aus einer Folge von Lasso-Schichten besteht. Ein zweischichtiges SDPC- und ein Hi-La-Netz werden auf drei verschiedenen Datenbanken und mit unterschiedlichen Sparsity-Parametern auf jeder Schicht trainiert.1 Wir zeigen, dass der von SDPC erzeugte Gesamtvorhersagefehler dank des Feedback-Mechanismus geringer ist, da er den Vorhersagefehler zwischen den Schichten überträgt. Zweitens zeigen wir, dass die Inferenzphase des SDPC schneller konvergiert als beim Hi-La-Modell, und drittens, dass das SDPC auch den Lernprozess beschleunigt, und schließlich zeigt die qualitative Analyse der Wörterbücher beider Modelle, unterstützt durch ihre Aktivierungswahrscheinlichkeit, dass die SDPC-Merkmale generischer und informativer sind.
Die Erklärung eines Deep-Learning-Modells kann den Nutzern helfen, sein Verhalten zu verstehen, und den Forschern ermöglichen, seine Mängel zu erkennen.Jüngste Arbeiten haben sich hauptsächlich auf die Erklärung von Modellen für Aufgaben wie Bildklassifikation oder visuelle Fragebeantwortung konzentriert.  In diesem Papier stellen wir einen Erklärungsansatz für Bildähnlichkeitsmodelle vor, bei dem die Ausgabe eines Modells eine Punktzahl ist, die die Ähnlichkeit von zwei Eingaben misst, anstatt eine Klassifizierung.  Wir schlagen eine Erklärungsmethode vor, die eine Salienzkarte, die wichtige Bildregionen identifiziert, mit einem Attribut verbindet, das die Übereinstimmung am besten erklärt.  Wir stellen fest, dass unsere Erklärungen zusätzliche Informationen liefern, die typischerweise nicht von Saliency Maps allein erfasst werden, und auch die Leistung bei der klassischen Aufgabe der Attributerkennung verbessern können.Die Verallgemeinerungsfähigkeit unseres Ansatzes wird anhand von zwei Datensätzen aus verschiedenen Bereichen, Polyvore Outfits und Animals with Attributes 2, demonstriert.
Es hat sich gezeigt, dass Negativbeispiele ein effektiver Weg sind, um die Robustheit von neuronalen Sequenz-zu-Sequenz-Modellen (seq2seq) zu bewerten, indem man Störungen auf die Eingabe eines Modells anwendet, die zu einer großen Leistungsverschlechterung führen, wobei diese Störungen nur dann auf eine Schwäche des Modells hinweisen, wenn sie die Semantik der Eingabe nicht so verändern, dass sich die erwartete Ausgabe ändert. Am Beispiel der maschinellen Übersetzung (MT) schlagen wir einen neuen Evaluierungsrahmen für gegnerische Angriffe auf seq2seq-Modelle vor, der die Bedeutungserhaltung berücksichtigt, und zeigen, dass bestehende Methoden die Bedeutung im Allgemeinen nicht bewahren.auf der Grundlage dieser Erkenntnisse schlagen wir neue Einschränkungen für Angriffe auf wortbasierte MT-Systeme vor und zeigen anhand menschlicher und automatischer Evaluierung, dass sie semantisch ähnlichere gegnerische Eingaben erzeugen.außerdem zeigen wir, dass die Durchführung von gegnerischem Training mit bedeutungserhaltenden Angriffen dem Modell in Bezug auf gegnerische Robustheit zugute kommt, ohne die Testleistung zu beeinträchtigen.
Wir stellen eine neue Normalisierungstechnik vor, die die schnellen Konvergenzeigenschaften der Batch-Normalisierung aufweist, indem sie eine Transformation der Ebenengewichte anstelle der Ebenenausgaben verwendet. die vorgeschlagene Technik hält den Beitrag der positiven und negativen Gewichte zur Ebenenausgabe im Gleichgewicht. wir validieren unsere Methode auf einer Reihe von Standard-Benchmarks einschließlich CIFAR-10/100, SVHN und ILSVRC 2012 ImageNet.
Wir stellen einen Rahmen für den Aufbau von unbeaufsichtigten Repräsentationen von Entitäten und ihren Zusammensetzungen vor, bei dem jede Entität als Wahrscheinlichkeitsverteilung und nicht als Vektor mit fester Länge betrachtet wird, wobei diese Verteilung über die Kontexte unterstützt wird, die mit der Entität zusammen auftreten und in einen geeigneten niedrigdimensionalen Raum eingebettet sind. Wir erläutern, wie die Methode für die Gewinnung von unüberwachten Repräsentationen von Text angewendet werden kann und illustrieren die Leistung sowohl quantitativ als auch qualitativ bei Aufgaben wie der Messung von Satzähnlichkeit und Wortverknüpfung, bei denen wir empirisch signifikante Gewinne beobachten (z.B. 4,1% relative Verbesserung gegenüber Sent2), Zu den wichtigsten Vorteilen des vorgeschlagenen Ansatzes gehören:(a) die Erfassung von Unsicherheit und Polysemie durch die Modellierung der Entitäten als Verteilungen,(b) die Nutzung der zugrundeliegenden Geometrie der jeweiligen Aufgabe (mit den Grundkosten),(c) die gleichzeitige Bereitstellung von Interpretierbarkeit mit dem Begriff des optimalen Transports zwischen Kontexten und(d) die einfache Anwendbarkeit zusätzlich zu den bestehenden Punkt-Einbettungsmethoden. Im Wesentlichen kann der Rahmen für jedes unüberwachte oder überwachte Problem (auf Text oder anderen Modalitäten) nützlich sein; und erfordert nur eine Co-Occurrence-Struktur, die vielen Problemen inhärent ist. Der Code, sowie vorgefertigte Histogramme, sind unter https://github.com/context-mover verfügbar.
    In den letzten Jahren hat das Phänomen der adversen Beispiele - böswillig konstruierte Eingaben, die trainierte Modelle des maschinellen Lernens täuschen - die Aufmerksamkeit der Forschungsgemeinschaft auf sich gezogen, insbesondere wenn sich der Gegner darauf beschränkt, kleine Änderungen an einer korrekt bearbeiteten Eingabe vorzunehmen. In dieser Arbeit zeigen wir, dass es sich hierbei um zwei Erscheinungsformen desselben Phänomens handelt, und stellen diesen Zusammenhang auf verschiedene Weise her: Erstens stellen wir fest, dass gegnerische Beispiele auf denselben Entfernungsskalen existieren, die wir von einem linearen Modell mit derselben Leistung auf beschädigten Bildern erwarten würden. Schließlich präsentieren wir eine modellunabhängige obere Schranke für den Abstand eines beschädigten Bildes zu seinem nächsten Fehler bei gegebener Testleistung und zeigen, dass wir in der Praxis bereits nahe an die Schranke herankommen, so dass eine weitere Verbesserung der Robustheit für die beschädigte Bildverteilung eine deutliche Reduzierung des Testfehlers erfordert. All dies deutet darauf hin, dass die Verbesserung der Robustheit des Gegners Hand in Hand mit der Verbesserung der Leistung bei allgemeineren und realistischeren Bildverfälschungen gehen sollte, was zu einer rechnerisch nachvollziehbaren Bewertungsmetrik für die Verteidigung führt: Testfehler in verrauschten Bildverteilungen.
Die jüngsten Entwicklungen im Bereich der Repräsentationen natürlicher Sprache wurden von großen und teuren Modellen begleitet, die durch selbstüberwachtes Vortraining große Mengen an allgemeinem Text nutzen.Aufgrund der Kosten für die Anwendung solcher Modelle auf nachgelagerte Aufgaben wurden mehrere Modellkomprimierungstechniken auf vortrainierten Sprachrepräsentationen vorgeschlagen (Sun et al, In diesem Beitrag zeigen wir zunächst, dass das Vortraining auch im Kontext kleinerer Architekturen wichtig ist und dass die Feinabstimmung von vortrainierten kompakten Modellen mit den aufwändigeren Methoden konkurrieren kann, die in konkurrierenden Arbeiten vorgeschlagen wurden.Ausgehend von vortrainierten kompakten Modellen untersuchen wir dann die Übertragung von Aufgabenwissen aus großen feinabgestimmten Modellen durch Standard-Wissensdestillation. Der daraus resultierende einfache, aber effektive und allgemeine Algorithmus, Pre-trained Distillation, bringt weitere Verbesserungen mit sich.Durch umfangreiche Experimente erforschen wir generell die Interaktion zwischen Pre-Training und Destillation unter zwei Variablen, die bisher nur wenig untersucht wurden: Modellgröße und Eigenschaften der unbeschrifteten Aufgabendaten.Eine überraschende Beobachtung ist, dass sie eine zusammengesetzte Wirkung haben, selbst wenn sie nacheinander auf die gleichen Daten angewendet werden.Um die zukünftige Forschung zu beschleunigen, werden wir unsere 24 vortrainierten Miniatur-BERT-Modelle öffentlich zugänglich machen.
In dieser Arbeit untersuchen wir die verlustbehaftete Kompression von tiefen neuronalen Netzen (DNNs) durch Gewichtsquantisierung und verlustfreie Quellcodierung für einen speichereffizienten Einsatz. Insbesondere untersuchen wir die universelle randomisierte Gitterquantisierung von DNNs, die DNN-Gewichte durch gleichmäßiges zufälliges Dithering vor der Gitterquantisierung randomisiert und bei jeder Quelle nahezu optimal arbeiten kann, ohne auf die Kenntnis ihrer Wahrscheinlichkeitsverteilung angewiesen zu sein. Unsere experimentellen Ergebnisse zeigen, dass das vorgeschlagene universelle DNN-Kompressionsschema das 32-schichtige ResNet (trainiert auf CIFAR-10) und das AlexNet (trainiert auf ImageNet) mit Kompressionsverhältnissen von $47.1$ bzw. $42.5$ komprimiert.
Was würde von Variational Autoencoder (VAE) gelernt werden und was beeinflusst die Entflechtung von VAE?Dieses Papier versucht, vorläufig VAE's intrinsische Dimension, realen Faktor, Entflechtung und Indikator Fragen theoretisch in der idealistischen Situation und Umsetzung Problem praktisch durch Lärm Modellierung Perspektive im realistischen Fall.  In Bezug auf die intrinsische Dimension lernt die idealistische VAE aufgrund der Informationserhaltung nur die intrinsische Faktordimension; außerdem fördert die durch die Eigenschaft der gegenseitigen Informationstrennung angeregte Einschränkung, die durch den Gaußschen Vorrang des VAE-Ziels induziert wird, die Informationssparsamkeit in der Dimension; in Bezug auf das Problem der Entflechtung wird anschließend, inspiriert durch das Theorem der Informationserhaltung, die Klärung der Entflechtung in diesem Papier vorgenommen; in Bezug auf das Problem des realen Faktors lernt die idealistische VAE aufgrund der Faktoräquivalenz möglicherweise jede beliebige Faktormenge in der Äquivalenzklasse.  Zur Frage der Indikatoren wird das Verhalten der aktuellen Entflechtungsmetrik erörtert, und anschließend werden mehrere Leistungsindikatoren in Bezug auf die Entflechtung und den erzeugenden Einfluss erhoben, um die Leistung des VAE-Modells zu bewerten und die verwendeten Faktoren zu überwachen.
Gewichtsabnahme ist einer der Standardtricks im Werkzeugkasten neuronaler Netze, aber die Gründe für seinen Regularisierungseffekt sind schlecht verstanden, und neuere Ergebnisse haben Zweifel an der traditionellen Interpretation in Bezug auf die $L_2$ Regularisierung aufkommen lassen.Es hat sich gezeigt, dass buchstäbliche Gewichtsabnahme die $L_2$ Regularisierung für Optimierer übertrifft, bei denen sie sich unterscheiden. Wir untersuchen empirisch den Gewichtsverfall für drei Optimierungsalgorithmen (SGD, Adam und K-FAC) und eine Vielzahl von Netzwerkarchitekturen und identifizieren drei verschiedene Mechanismen, durch die der Gewichtsverfall einen Regularisierungseffekt ausübt, abhängig von dem jeweiligen Optimierungsalgorithmus und der Architektur: (1) Erhöhung der effektiven Lernrate, (2) annähernde Regularisierung der Input-Output-Jacob-Norm und (3) Reduzierung des effektiven Dämpfungskoeffizienten für die Optimierung zweiter Ordnung. Unsere Ergebnisse geben Aufschluss darüber, wie die Regularisierung von neuronalen Netzen verbessert werden kann.
Der Datensatz enthält 40 Log-Mel-Band-Energien, die aus $100$ verschiedenen synthetischen Schallereignis-Tracks extrahiert wurden, mit additivem Rauschen aus neun verschiedenen akustischen Szenen (aus Innen-, Außen- und Fahrzeugumgebungen), gemischt bei sechs verschiedenen Geräusch-Rausch-Verhältnissen (von -12 bis -27 dB mit einem Schritt von -3 dB), und insgesamt 5400 (9 * 100 * 6) Schalldateien mit einer Gesamtlänge von 30 564 Minuten. Wir stellen den Datensatz in seiner jetzigen Form, den Code zur Neuerstellung des Datensatzes und zum Remixen der Klangereignis-Tracks und der akustischen Szenen mit unterschiedlichen SNRs sowie eine Basismethode zur Verfügung, die die Anpassungsleistung mit dem vorgeschlagenen Datensatz testet und einige erste Ergebnisse liefert.
Durch die Neudefinition der Kosten mit Hilfe verallgemeinerter Funktionen aus der nicht-extensiven statistischen Mechanik erhöhen wir die Obergrenze bisheriger Schätzer und ermöglichen die Kontrolle des Bias-Varianz-Kompromisses.Variationsbasierte Schätzer übertreffen bisherige Methoden vor allem in hochdimensionalen Szenarien mit hoher Abhängigkeit, wie sie in maschinellen Lernsystemen vorkommen. Unser Ansatz, der von der nicht-extensiven statistischen Mechanik inspiriert ist, verwendet verschiedene Verallgemeinerungen für den Logarithmus und die Exponentialfunktion in der Partitionsfunktion, was es dem Schätzer ermöglicht, Änderungen in der gegenseitigen Information über einen größeren Bereich von Dimensionen und Korrelationen der Eingangsvariablen zu erfassen, während frühere Schätzer sie sättigen.
Wasserstein GANs (WGANs), eine der erfolgreichsten Varianten von GANs, erfordern die Lösung eines Minmax-Problems bis zur globalen Optimalität, werden aber in der Praxis erfolgreich mit stochastischem Gradientenabstieg trainiert.In diesem Papier zeigen wir, dass, wenn der Generator ein einschichtiges Netzwerk ist, stochastischer Gradientenabstieg zu einer globalen Lösung in polynomialer Zeit und Probenkomplexität konvergiert.
Es hat sich gezeigt, dass Klassifizierer wie tiefe neuronale Netze bei Problemen mit hochdimensionalem Eingaberaum anfällig für nachteilige Störungen sind. Während das nachteilige Training die Robustheit von Klassifizierern gegen solche nachteiligen Störungen verbessert, bleiben Klassifizierer bei einem nicht zu vernachlässigenden Teil der Eingaben anfällig für diese Störungen. Wir argumentieren, dass es zwei verschiedene Arten von Störungen gibt: Gemeinsame Störungen, die einen Klassifikator auf vielen Eingaben täuschen, und singuläre Störungen, die den Klassifikator nur auf einem kleinen Teil der Daten täuschen.Wir stellen fest, dass adversariales Training die Robustheit von Klassifikatoren gegen gemeinsame Störungen erhöht. Darüber hinaus ist es besonders effektiv bei der Beseitigung von universellen Störungen, die als eine extreme Form von gemeinsamen Störungen angesehen werden können.Leider erhöht adversariales Training nicht durchgängig die Robustheit gegen singuläre Störungen auf ungesehene Eingaben. Wir stellen jedoch fest, dass gegnerisches Training die Robustheit der verbleibenden Störungen gegenüber Bildtransformationen wie Kontrast- und Helligkeitsänderungen oder Gauß'scher Unschärfe verringert und somit erfolgreiche Angriffe auf den Klassifikator in der physischen Welt unwahrscheinlicher macht.Schließlich zeigen wir, dass selbst singuläre Störungen leicht erkannt werden können und daher verallgemeinerbare Muster aufweisen müssen, auch wenn die Störungen für bestimmte Eingaben spezifisch sind.
Wir befassen uns mit dem anspruchsvollen Problem des effizienten Einsatzes von Deep-Learning-Modellen, bei dem das Ziel darin besteht, neuronale Netzwerkarchitekturen zu entwerfen, die an verschiedene Hardwareplattformen angepasst werden können. Die meisten traditionellen Ansätze entwerfen entweder manuell oder verwenden die neuronale Architektursuche (NAS), um ein spezialisiertes neuronales Netzwerk zu finden und es von Grund auf für jeden Fall zu trainieren, was rechenintensiv und nicht skalierbar ist. Zu diesem Zweck schlagen wir vor, ein einmaliges Netzwerk (OFA) zu trainieren, das verschiedene architektonische Einstellungen (Tiefe, Breite, Kernelgröße und Auflösung) unterstützt. In einem gegebenen Einsatzszenario können wir dann schnell ein spezialisiertes Subnetzwerk erhalten, indem wir aus dem OFA-Netzwerk ohne zusätzliches Training auswählen. Um Interferenzen zwischen vielen Teilnetzen während des Trainings zu vermeiden, schlagen wir außerdem einen neuartigen progressiven Schrumpfungsalgorithmus vor, der eine überraschend große Anzahl von Teilnetzen ($> 10^{19}$) gleichzeitig trainieren kann. Umfangreiche Experimente auf verschiedenen Hardwareplattformen (CPU, GPU, mCPU, mGPU, FPGA-Beschleuniger) zeigen, dass OFA die SOTA NAS-Methoden konsequent übertrifft (bis zu 4. 0% ImageNet top1 Genauigkeit über MobileNetV3) bei gleichzeitiger Reduzierung von Größenordnungen GPU Stunden und $CO_2$ Emission.Insbesondere erreicht OFA eine neue SOTA 80,0% ImageNet top1 Genauigkeit unter der mobilen Einstellung ($<$600M FLOPs).Code und pre-trained Modelle sind auf https://github.com/mit-han-lab/once-for-all veröffentlicht.
Ein tiefes generatives Modell ist eine leistungsfähige Methode zum Erlernen einer Datenverteilung, die in zahlreichen Szenarien enorme Erfolge erzielt hat.Allerdings ist es für ein einzelnes generatives Modell nicht trivial, die Verteilungen komplexer Daten wie Bilder mit komplizierten Strukturen getreu zu erfassen.In diesem Papier schlagen wir einen neuartigen Ansatz des kaskadierten Boostings für das Boosten generativer Modelle vor, bei dem Metamodelle (d.h. schwache Lerner) kaskadiert werden, Wir leiten eine dekomponierbare Variationsuntergrenze des geboosteten Modells ab, die es erlaubt, jedes Meta-Modell separat und gierig zu trainieren.Wir können die Lernfähigkeit der generativen Modelle weiter verbessern, indem wir unser kaskadiertes Boosting-Framework mit dem multiplikativen Boosting-Framework kombinieren.
Kontextualisierte Repräsentationsmodelle wie ELMo (Peters et al., 2018a) und BERT (Devlin et al., Aufbauend auf den jüngsten Arbeiten zum Sondieren auf Token-Ebene führen wir ein neuartiges Aufgabendesign für das Sondieren auf Randebene ein und konstruieren eine breite Palette von Teilsatzaufgaben, die von der traditionellen strukturierten NLP-Pipeline abgeleitet sind. Wir untersuchen die kontextuellen Repräsentationen auf Wortebene von vier aktuellen Modellen und untersuchen, wie sie die Satzstruktur über eine Reihe von syntaktischen, semantischen, lokalen und weitreichenden Phänomenen kodieren. Wir stellen fest, dass bestehende Modelle, die auf Sprachmodellierung und Übersetzung trainiert wurden, starke Repräsentationen für syntaktische Phänomene erzeugen, aber nur vergleichsweise kleine Verbesserungen bei semantischen Aufgaben gegenüber einer nicht-kontextuellen Basislinie bieten.
Deep Reinforcement Learning hat sich in anspruchsvollen Spielen wie Atari, Go usw. bewährt. Die Entscheidungsfindung in der realen Welt erfordert jedoch oft Schlussfolgerungen mit partiellen Informationen, die aus komplexen visuellen Beobachtungen gewonnen werden. In diesem Papier wird Discriminative Particle Filter Reinforcement Learning (DPFRL) vorgestellt, ein neues Reinforcement Learning Framework für partielle und komplexe Beobachtungen. DPFRL kodiert einen differenzierbaren Partikelfilter mit gelernten Übergangs- und Beobachtungsmodellen in einem neuronalen Netzwerk, das Schlussfolgerungen mit partiellen Beobachtungen über mehrere Zeitschritte ermöglicht. Während ein Standardpartikelfilter auf einem generativen Beobachtungsmodell beruht, lernt DPFRL ein diskriminativ parametrisiertes Modell, das direkt für die Entscheidungsfindung trainiert wird. Wir zeigen, dass die diskriminative Parametrisierung zu einer signifikant verbesserten Leistung führt, insbesondere für Aufgaben mit komplexen visuellen Beobachtungen, da sie die Schwierigkeit umgeht, Beobachtungen explizit zu modellieren. In den meisten Fällen übertrifft DPFRL die modernsten POMDP RL-Modelle in Flickering Atari Games, einem bestehenden POMDP RL-Benchmark, und in Natural Flickering Atari Games, einem neuen, anspruchsvolleren POMDP RL-Benchmark, den wir einführen, und wir zeigen außerdem, dass DPFRL bei der visuellen Navigation mit realen Daten gut abschneidet.
Die Erweiterung von Modellen mit latenten Hilfsvariablen ist eine bekannte Technik zur Erhöhung der Modellexpressivität. Bachman & Precup (2015); Naesseth et al. (2018); Cremer et al. (2017); Domke & Sheldon (2018) zeigen, dass Importance Weighted Autoencoders (IWAE) (Burda et al, 2015) als eine Erweiterung der Variationsfamilie mit latenten Hilfsvariablen angesehen werden können, und wir zeigen, dass diese Sichtweise viele der jüngsten Entwicklungen im Bereich der Variationsschranken umfasst (Maddisonet al., 2017; Naesseth et al., 2018; Le et al., 2017; Yin & Zhou, 2018; Molchanovet al, Wir entwickeln ein generatives Modell analog zur IWAE-Schranke und zeigen empirisch, dass es den kürzlich vorgeschlagenen Learned Accept/Reject Sampling-Algorithmus (Bauer & Mnih, 2018) übertrifft, während es wesentlich einfacher zu implementieren ist.Darüber hinaus zeigen wir, dass dieses generative Verfahren neue Erkenntnisse über die Rangfolge von Noise Contrastive Estimation (Jozefowicz et al., 2016 ; Ma & Collins, 2018) und Contrastive Predictive Coding (Oord et al., 2018).
Stochastic Gradient Descent oder SGD ist der beliebteste Optimierungsalgorithmus für große Probleme.SGD schätzt den Gradienten durch gleichmäßiges Sampling mit Stichprobengröße eins.Es gab mehrere andere Arbeiten, die eine schnellere epochenweise Konvergenz durch die Verwendung von gewichtetem ungleichmäßigem Sampling für bessere Gradientenschätzungen vorschlagen.Leider sind die Kosten pro Iteration für die Aufrechterhaltung dieser adaptiven Verteilung für die Gradientenschätzung höher als die Berechnung des vollständigen Gradienten.Als Ergebnis führt der falsche Eindruck einer schnelleren Konvergenz in Iterationen zu einer langsameren Konvergenz in der Zeit, die wir als eine Huhn-und-Ei-Schleife bezeichnen. In diesem Papier, brechen wir diese Barriere durch die Bereitstellung der ersten Demonstration eines Sampling-Schema, das zu überlegenen Gradienten Schätzung führt, während die Probenahme Kosten pro Iteration ähnlich wie die der einheitlichen sampling.such ein Algorithmus ist möglich aufgrund der Probenahme Sicht der Locality Sensitive Hashing (LSH), die vor kurzem ans Licht kam.als Folge der überlegenen und schnellen Schätzung, reduzieren wir die Laufzeit aller bestehenden Gradientenabstieg Algorithmen.wir demonstrieren die Vorteile unserer Vorschlag auf beiden SGD und AdaGrad.
In den letzten Jahren haben wir bedeutende Fortschritte bei der Identifizierung von Berechnungsprinzipien gemacht, die der neuronalen Funktion zugrunde liegen, und obwohl sie noch nicht vollständig sind, haben wir genügend Beweise dafür, dass eine Synthese dieser Ideen zu einem Verständnis darüber führen könnte, wie neuronale Berechnungen aus einer Kombination von angeborener Dynamik und Plastizität entstehen, und die möglicherweise verwendet werden könnten, um neue KI-Technologien mit einzigartigen Fähigkeiten zu konstruieren.Ich diskutiere die relevanten Prinzipien, die Vorteile, die sie für die Berechnung haben, und wie sie der KI zugute kommen können.Die Grenzen der derzeitigen KI sind allgemein anerkannt, aber weniger Menschen sind sich bewusst, dass wir genug über das Gehirn wissen, um sofort neue KI-Formulierungen anzubieten.
Wir schlagen die Fragebeantwortung als allgemeines Paradigma vor, um die Repräsentationen, die solche Agenten entwickeln, zu entschlüsseln und zu verstehen, und wenden unsere Methode auf zwei neuere Ansätze zur prädiktiven Modellierung an - handlungsbedingte CPC (Guo et al., 2018) und SimCore (Gregor et al., 2019), Nachdem wir Agenten mit diesen prädiktiven Zielen in einer visuell reichhaltigen 3D-Umgebung mit einer Auswahl an Objekten, Farben, Formen und räumlichen Konfigurationen trainiert haben, untersuchen wir ihre internen Zustandsrepräsentationen mit einer Vielzahl synthetischer (englischer) Fragen, ohne dass Gradienten vom Frage-Antwort-Decoder in den Agenten rückgekoppelt werden.Die Leistung verschiedener Agenten, wenn sie auf diese Weise untersucht werden, zeigt, dass sie lernen, detaillierte und scheinbar zusammengesetzte Informationen über Objekte, Eigenschaften und räumliche Beziehungen aus ihrer physischen Umgebung zu kodieren. Unser Ansatz ist intuitiv, d.h. Menschen können die Antworten des Modells im Gegensatz zur Inspektion kontinuierlicher Vektoren leicht interpretieren, und modellunabhängig, d.h. auf jeden Modellierungsansatz anwendbar.Durch die Offenlegung des impliziten Wissens über Objekte, Mengen, Eigenschaften und Beziehungen, das Agenten während des Lernens erwerben, kann das fragenbedingte Sondieren von Agenten den Entwurf und die Entwicklung stärkerer prädiktiver Lernziele anregen.
In den meisten realen Szenarien sind die Trainingsdatensätze stark klassenungleichgewichtig, so dass tiefe neuronale Netze unter der Generalisierung auf ein ausgewogenes Testkriterium leiden. In diesem Papier erforschen wir eine neuartige und dennoch einfache Möglichkeit, dieses Problem durch die Synthese von weniger häufigen Klassen mit gegnerischen Beispielen anderer Klassen zu lindern. Unsere experimentellen Ergebnisse auf verschiedenen Arten von klassensymmetrischen Datensätzen in der Bildklassifikation und der Verarbeitung natürlicher Sprache zeigen, dass die vorgeschlagene Methode nicht nur die Generalisierung von Minderheitsklassen im Vergleich zu anderen Re-Sampling- oder Re-Gewichtungsverfahren deutlich verbessert, sondern auch andere Methoden auf dem Stand der Technik für die klassensymmetrische Klassifikation übertrifft.
Ein synthetisches aktives System aus Mikrotubuli-Polymeren, die von Proteinmotoren angetrieben werden, bildet spontan eine flüssigkristalline nematische Phase, wobei die von den Proteinmotoren erzeugte Dehnungsspannung eine kontinuierliche Knickung und Faltung der Mikrotubuli auslöst, wodurch bewegliche topologische Defekte und turbulente Flüssigkeitsströmungen entstehen. Die Messung der Defektdynamik kann grundlegende Erkenntnisse über aktive nematische Materialien liefern, zu denen auch bakterielle Filme und tierische Zellen gehören. Den derzeitigen Methoden zur Defektdetektion mangelt es an Robustheit und Präzision, und sie erfordern eine Feinabstimmung für Datensätze mit unterschiedlicher visueller Qualität.  In dieser Studie haben wir Deep Learning angewendet, um einen Defekt-Detektor zu trainieren, um automatisch Mikroskopie-Videos der Mikrotubuli aktiven nematischen zu analysieren.  Die experimentellen Ergebnisse zeigen, dass unsere Methode robust und genau ist und die Menge der zu verarbeitenden Videodaten signifikant erhöhen wird.
In dieser Arbeit untersuchen wir Lokalität und Kompositionalität im Zusammenhang mit Lernrepräsentationen für Zero Shot Learning (ZSL). Die Ergebnisse unseres Experiments zeigen, dass Lokalität, d.h. kleine Teile der Eingabe, und Kompositionalität, d.h. die Frage, wie gut die gelernten Repräsentationen als Funktion eines kleineren Vokabulars ausgedrückt werden können, beide eng mit der Generalisierung verbunden sind und motivieren den Fokus auf lokalere Modelle in zukünftigen Forschungsrichtungen für das Lernen von Repräsentationen.
Bei dem Versuch, den Ursprung von Fehlern zu erklären, haben sich frühere Studien typischerweise auf die Tatsache konzentriert, dass neuronale Netze mit hochdimensionalen Daten arbeiten, dass sie überangepasst sind oder dass sie zu linear sind. hier zeigen wir, dass Verteilungen von Logit-Differenzen eine universelle funktionale Form haben. diese funktionale Form ist unabhängig von der Architektur, dem Datensatz und dem Trainingsprotokoll und ändert sich auch nicht während des Trainings. Wir zeigen, dass diese Universalität für eine breite Palette von Datensätzen (MNIST, CIFAR10, ImageNet und Zufallsdaten), Modellen (einschließlich modernster tiefer Netzwerke, linearer Modelle, gegnerisch trainierter Netzwerke und Netzwerke, die auf zufällig gemischten Etiketten trainiert wurden) und Angriffen (FGSM, step l.l.) gilt, Schließlich untersuchen wir die Auswirkungen von Netzwerkarchitekturen auf die Empfindlichkeit von Angreifern, indem wir die Suche nach neuronalen Architekturen mit Verstärkungslernen nutzen, um für CIFAR10 widerstandsfähige Architekturen zu finden, die im Vergleich zu früheren Versuchen robuster gegenüber White- und Blackbox-Angriffen sind.
    Reinforcement Learning (RL) hat in den letzten Jahren zu einem immer komplexer aussehenden Verhalten geführt, aber diese Komplexität kann irreführend sein und verbirgt eine Überanpassung Wir finden, dass visuelle Repräsentationen eine nützliche Metrik der Komplexität sein können und sowohl gut mit der objektiven Optimierung korrelieren als auch die Belohnungsoptimierung kausal beeinflussen. Wir schlagen dann vor, neugierig Repräsentation Lernen (CRL), die es uns ermöglicht, bessere visuelle Repräsentation Lernalgorithmen zu verwenden, um entsprechend zu erhöhen visuelle Repräsentation in der Politik durch eine intrinsische Ziel auf beiden simulierten Umgebungen und Übertragung auf reale images.Finally, zeigen wir bessere visuelle Repräsentationen durch CRL induziert ermöglicht es uns, eine bessere Leistung auf Atari ohne Belohnung als andere Neugier Ziele zu erhalten.
In diesem Beitrag wird die Aufgabe der semantischen Instanzvervollständigung vorgestellt: Aus einem unvollständigen RGB-D-Scan einer Szene sollen die einzelnen Objektinstanzen, aus denen die Szene besteht, erkannt und ihre vollständige Objektgeometrie abgeleitet werden, was eine semantisch sinnvolle Zerlegung einer gescannten Szene in einzelne, vollständige 3D-Objekte ermöglicht, einschließlich versteckter und unbeobachteter Objektteile. Dies eröffnet neue Möglichkeiten für die Interaktion mit Objekten in einer Szene, z.B. für virtuelle oder robotische Agenten. Um diese Aufgabe anzugehen, schlagen wir 3D-SIC vor, einen neuen datengesteuerten Ansatz, der gemeinsam Objektinstanzen erkennt und deren vollständige Geometrie vorhersagt. Die Kernidee von 3D-SIC ist eine neuartige End-to-End-Architektur für neuronale 3D-Netzwerke, die das gemeinsame Erlernen von Farb- und Geometriemerkmalen nutzt. Die vollständig evolutionäre Natur unseres 3D-Netzwerks ermöglicht eine effiziente Inferenz der semantischen Instanzvervollständigung für 3D-Scans im Maßstab großer Innenräume in einem einzigen Vorwärtsdurchlauf. In einer Evaluierungsserie haben wir sowohl reale als auch synthetische Scan-Benchmark-Daten ausgewertet, bei denen wir State-of-the-Art-Ansätze um über 15 in mAP@0.5 auf ScanNet und über 18 in mAP@0.5 auf SUNCG übertreffen.
Style-Transfer bezieht sich in der Regel auf die Aufgabe der Anwendung von Farb- und Texturinformationen aus einem bestimmten Style-Bild auf ein gegebenes Content-Bild unter Beibehaltung der Struktur des letzteren. Hier gehen wir das allgemeinere Problem des semantischen Style-Transfers an: Bei zwei ungepaarten Sammlungen von Bildern zielen wir darauf ab, eine Zuordnung zwischen dem Corpus-Level-Stil jeder Sammlung zu lernen, während semantische Inhalte, die in den beiden Domänen gemeinsam sind, erhalten bleiben. Wir stellen XGAN ("Cross-GAN") vor, einen dualen adversarischen Autoencoder, der eine gemeinsame Repräsentation des gemeinsamen semantischen Inhalts der Domäne auf unüberwachte Weise erfasst, während er gleichzeitig die Übersetzungen der Bilder von der Domäne in die Domäne in beide Richtungen lernt.  Wir nutzen Ideen aus der Literatur zur Domänenanpassung und definieren einen semantischen Konsistenzverlust, der das Modell dazu anregt, die Semantik im erlernten Einbettungsraum zu bewahren.Wir berichten über vielversprechende qualitative Ergebnisse für die Aufgabe der Übersetzung von Gesichtern in Cartoons.Der Cartoon-Datensatz, den wir zu diesem Zweck gesammelt haben, wird auch als neuer Benchmark für den semantischen Stiltransfer veröffentlicht.
Das Training neuronaler Netze auf großen Datensätzen kann durch die Verteilung der Arbeitslast auf ein Netzwerk von Maschinen beschleunigt werden. Da die Datensätze immer größer werden, werden Netzwerke mit Hunderten oder Tausenden von Maschinen wirtschaftlich sinnvoll. Wir erforschen einen besonders einfachen Algorithmus für robustes, kommunikationseffizientes Lernen - signSGD. Die Arbeiter übermitteln nur das Vorzeichen ihres Gradientenvektors an einen Server, und die Gesamtaktualisierung wird durch eine Mehrheitsabstimmung entschieden. Dieser Algorithmus benötigt 32-mal weniger Kommunikation pro Iteration als vollpräzises, verteiltes SGD. Unter natürlichen Bedingungen, die durch Experimente verifiziert wurden, beweisen wir, dass signSGD in den großen und Mini-Batch-Einstellungen konvergiert, wobei die Konvergenz für ein Parameterregime von Adam als Nebenprodukt etabliert wird.Aggregation von Vorzeichengradienten durch Mehrheitsabstimmung bedeutet, dass kein einzelner Arbeiter zu viel Macht hat.Wir beweisen, dass im Gegensatz zu SGD, Mehrheitsabstimmung robust ist, wenn bis zu 50% der Arbeiter sich adversarisch verhalten. Auf der praktischen Seite haben wir unser verteiltes Trainingssystem in Pytorch aufgebaut. Beim Benchmarking mit der State of the Art Collective Communications Library (NCCL) führte unser Framework - mit dem Parameter-Server, der vollständig auf einer Maschine untergebracht ist - zu einer 25%igen Reduzierung der Zeit für das Training von resnet50 auf Imagenet, wenn 15 AWS p3.2xlarge-Maschinen verwendet wurden.
Die Erstellung von Profilen zellulärer Phänotypen aus mikroskopischer Bildgebung kann aussagekräftige biologische Informationen liefern, die sich aus verschiedenen Faktoren ergeben, die auf die Zellen einwirken. Eine motivierende Anwendung ist die Entwicklung von Medikamenten: morphologische Zellmerkmale können aus Bildern erfasst werden, anhand derer Ähnlichkeiten zwischen verschiedenen Medikamenten in unterschiedlichen Dosierungen quantifiziert werden können. Ein wichtiges bekanntes Problem bei solchen Methoden ist die Trennung relevanter biologischer Signale von störenden Variationen. z.B. neigen die Einbettungsvektoren dazu, für Zellen, die in der gleichen Woche kultiviert und abgebildet wurden, stärker korreliert zu sein als für Zellen aus einer anderen Woche, obwohl in beiden Fällen die gleichen Wirkstoffe eingesetzt wurden. Ein idealer Satz von Bildeinbettungen sollte nur die relevanten biologischen Informationen (z. B. Arzneimittelwirkungen) enthalten. Wir entwickeln einen allgemeinen Rahmen für die Anpassung der Bildeinbettungen, um domänenspezifische Informationen zu "vergessen", während relevante biologische Informationen erhalten bleiben. Um dies zu erreichen, minimieren wir eine Verlustfunktion, die auf den Abständen zwischen den Randverteilungen (z. B. dem Wasserstein-Abstand) der Einbettungen über die Domänen hinweg für jede replizierte Behandlung basiert.Für den vorgestellten Datensatz ist die replizierte Behandlung die negative Kontrolle.Wir stellen fest, dass für unsere transformierten Einbettungen (1) die zugrunde liegende geometrische Struktur nicht nur erhalten bleibt, sondern die Einbettungen auch ein verbessertes biologisches Signal tragen (2) weniger domänenspezifische Informationen vorhanden sind.
In diesem Papier wird ein Mutual Information Neural Estimator (MINE) vorgestellt, der sowohl in der Dimensionalität als auch in der Stichprobengröße linear skalierbar ist.MINE ist back-propable und wir beweisen, dass es stark konsistent ist.Wir illustrieren eine Handvoll Anwendungen, in denen MINE erfolgreich angewandt wird, um die Eigenschaft von generativen Modellen sowohl in unbeaufsichtigten als auch in überwachten Einstellungen zu verbessern.Wir wenden unseren Rahmen an, um den Informationsengpass zu schätzen, und wenden ihn in Aufgaben an, die mit überwachten Klassifizierungsproblemen zusammenhängen.Unsere Ergebnisse zeigen eine erhebliche zusätzliche Flexibilität und Verbesserung in diesen Einstellungen.
 Methoden des Reinforcement Learning haben in letzter Zeit beeindruckende Ergebnisse bei einer Vielzahl von Steuerungsproblemen erzielt, benötigen aber insbesondere bei komplexen Eingaben immer noch eine große Menge an Trainingsdaten, um zu einer sinnvollen Lösung zu konvergieren.Diese Einschränkung verbietet weitgehend ihre Verwendung für komplexe Eingabesituationen wie Videosignale, und es ist immer noch unmöglich, sie für eine Reihe von komplexen Problemen in einer realen Umgebung zu verwenden, einschließlich vieler solcher für videobasierte Steuerung.Überwachtes Lernen hingegen ist in der Lage, auf einer relativ kleinen Anzahl von Proben zu lernen, berücksichtigt aber keine belohnungsbasierten Steuerungsstrategien und ist nicht in der Lage, unabhängige Steuerungsstrategien zu liefern.  Wir verwenden das Videospiel SpeedDreams/TORCS, um zu zeigen, dass unser Ansatz viel weniger Stichproben benötigt (Hunderttausende im Vergleich zu Millionen oder Zehnmillionen) als die modernsten Verstärkungslernverfahren für ähnliche Daten, und dass er gleichzeitig sowohl die überwachten als auch die Verstärkungslernverfahren in Bezug auf die Qualität übertrifft.
Ein typisches Experiment zur Untersuchung kognitiver Funktionen besteht darin, Tiere zu trainieren, Aufgaben auszufÃ?hren, wÃ?hrend der Forscher die elektrische AktivitÃ?t der Neuronen der Tiere aufzeichnet.â€œ Das Haupthindernis bei der Verwendung dieser Art von elektrophysiologischen Experimenten zur Aufdeckung der Schaltkreismechanismen, die komplexen Verhaltensweisen zugrunde liegen, ist unser unvollstÃ?ndiger Zugang zu den relevanten Schaltkreisen im Gehirn.â€œ Ein vielversprechender Ansatz ist die Modellierung neuronaler Schaltkreise mit Hilfe eines kÃ?nstlichen neuronalen Netzes (ANN), das einen vollstÃ?ndigen Zugang zu den fÃ?r ein Verhalten verantwortlichen â€žneuronalen Schaltkreisenâ€œ bietet. In diesem Papier schlagen wir ein biologisch plausibles Actor-Critic with Episodic Memory (B-ACEM) Framework vor, um einen präfrontalen Kortex-Basalganglien-Hippocampus (PFC-BG) Schaltkreis zu modellieren, der verifiziert wurde, um die Verhaltensbefunde einer bekannten Wahrnehmungsentscheidungsaufgabe zu erfassen, Dieser B-ACEM-Rahmen verbindet neuronale Berechnungen mit Verhaltensweisen, anhand derer wir erforschen können, wie das episodische Gedächtnis berücksichtigt werden sollte, um zukünftige Entscheidungen zu steuern.Experimente werden mit verschiedenen Einstellungen des episodischen Gedächtnisses durchgeführt und die Ergebnisse zeigen, dass alle Muster des episodischen Gedächtnisses das Lernen beschleunigen können.Insbesondere werden auffällige Ereignisse priorisiert, um Belohnungsinformationen zu verbreiten und Entscheidungen zu leiten.Unser B-ACEM-Rahmen und die darauf aufbauenden Experimente geben Inspirationen sowohl für Entwürfe für Standard-Entscheidungsmodelle in biologischen Systemen als auch für ein biologisch plausibleres ANN.
Das Verständnis der Darstellungsleistung von tiefen neuronalen Netzen (DNNs) und wie ihre strukturellen Eigenschaften (z.B., In einer bahnbrechenden Arbeit hat Telgarsky die Vorteile der Tiefe hervorgehoben, indem er eine Familie von Funktionen (basierend auf einfachen Dreieckswellen) vorstellte, für die DNNs einen Klassifikationsfehler von Null erreichen, während flache Netzwerke mit weniger als exponentiell vielen Knoten einen konstanten Fehler aufweisen. Obwohl Telgarskys Arbeit die Grenzen von flachen neuronalen Netzen aufzeigt, informiert sie uns nicht darÃ¼ber, warum diese Funktionen schwierig darzustellen sind, und tatsÃ¤chlich erklÃ¤rt er es als eine quÃ€lende offene Frage, die Funktionen zu charakterisieren, die nicht durch kleinere Tiefen gut angenÃ€hert werden kÃ¶nnen. In dieser Arbeit zeigen wir eine neue Verbindung zwischen der AusdrucksfÃ?higkeit von DNNs und Sharkovskyâ€™s Theorem aus dynamischen Systemen auf, die es uns ermÃ¶glicht, die Tiefen-Breiten-AbwÃ?gungen von ReLU-Netzen zur Darstellung von Funktionen zu charakterisieren, die auf dem Vorhandensein eines verallgemeinerten Begriffs von Fixpunkten basieren, der als periodische Punkte bezeichnet wird (ein Fixpunkt ist ein Punkt der Periode 1). Motiviert durch unsere Beobachtung, dass die in Telgarskys Arbeit verwendeten Dreieckswellen Punkte der Periode 3 enthalten â€" eine Periode, die insofern besonders ist, als sie chaotisches Verhalten impliziert, das auf dem gefeierten Ergebnis von Li-Yorke beruht â€" gehen wir dazu über, allgemeine untere Schranken fÃ?r die Breite anzugeben, die fÃ?r die Darstellung periodischer Funktionen in AbhÃ?ngigkeit von der Tiefe benÃ¶tigt wird.â€œ Technisch gesehen basiert der Kern unseres Ansatzes auf einer Eigenwertanalyse der mit solchen Funktionen verbundenen dynamischen Systeme.
Wir untersuchen die Low-Bit-Quantisierung, um die Rechenkosten von Deep Neural Network (DNN) basiertem Keyword Spotting (KWS) zu reduzieren und schlagen Ansätze vor, um die Quantisierungsbits weiter zu reduzieren, indem wir die Quantisierung in das Training des Keyword Spotting Modells integrieren. Unsere experimentellen Ergebnisse auf großen Datensatz zeigen, dass Quantisierung-bewusste Ausbildung kann die Leistung Modelle quantisiert, um niedrigere Bits Darstellungen wiederherzustellen.Durch die Kombination von Quantisierung-bewusste Ausbildung und Gewicht Matrix Faktorisierung, sind wir in der Lage, deutlich zu reduzieren Modell Größe und Berechnung für kleine Fußabdruck Keyword-Spotting, während die Leistung.
Single-cell RNA-sequencing (scRNA-seq) ist ein leistungsfähiges Werkzeug für die Analyse biologischer Systeme, aber aufgrund von biologischem und technischem Rauschen stellt die Quantifizierung der Auswirkungen mehrerer experimenteller Bedingungen eine analytische Herausforderung dar.Um diese Herausforderung zu überwinden, haben wir MELD entwickelt: Manifold Enhancement of Latent Dimensions.MELD nutzt Werkzeuge aus der Graphsignalverarbeitung, um eine latente Dimension innerhalb der Daten zu erlernen, die die Prototypizität jedes Datenpunkts in Bezug auf experimentelle oder Kontrollbedingungen bewertet.Wir nennen diese Dimension das Enhanced Experimental Signal (EES). MELD lernt das EES, indem es die verrauschte kategoriale experimentelle Kennzeichnung im Frequenzbereich des Graphen filtert, um ein glattes Signal mit kontinuierlichen Werten wiederherzustellen. Diese Methode kann verwendet werden, um Signaturgene zu identifizieren, die zwischen den Bedingungen variieren, und um festzustellen, welche Zelltypen von einer gegebenen Störung am stärksten betroffen sind.Wir demonstrieren die Vorteile der MELD-Analyse in zwei biologischen Datensätzen, einschließlich der T-Zell-Aktivierung als Reaktion auf mit Antikörpern beschichtete Kügelchen und der Behandlung menschlicher Inselzellen der Bauchspeicheldrüse mit Interferon gamma.
Gaußsche Prozesse (GPs) sowie deren nichtlineare Erweiterungen bieten einen flexiblen Rahmen, um Benutzermodelle in Verbindung mit approximativer Bayes'scher Inferenz zu erlernen, wobei die resultierenden Modelle jedoch im Allgemeinen nicht interpretierbar sind. Wir schlagen Entscheidungsregel-GPs (DRGPs) vor, die GPs in einem transformierten Raum anwenden, der durch Entscheidungsregeln definiert ist, die für Praktiker unmittelbar interpretierbar sind.Wir illustrieren dieses Modellierungswerkzeug an einer realen Anwendung und zeigen, dass strukturelle Variationsinferenztechniken mit DRGPs verwendet werden können.Wir stellen fest, dass DRGPs die direkte Verwendung von GPs in Bezug auf die Leistung außerhalb der Stichprobe übertreffen.
Während die Bayes'sche Optimierung (BO) große Erfolge bei der Optimierung teurer Black-Box-Funktionen erzielt hat, insbesondere bei der Abstimmung von Hyperparametern neuronaler Netze, haben Methoden wie die Zufallssuche (Li et al., 2016) und Multi-Fidelity-BO (z.B. Klein et al. (2017)), die billige Approximationen ausnutzen, z.B. Training auf kleineren Trainingsdaten oder mit weniger Iterationen, können Standard-BO-Ansätze, die nur Full-Fidelity-Beobachtungen verwenden, übertreffen. In diesem Beitrag schlagen wir einen neuartigen Bayes'schen Optimierungsalgorithmus vor, die Continuous-Fidelity Knowledge Gradient (cfKG)-Methode, die verwendet werden kann, wenn die Fidelity durch eine oder mehrere kontinuierliche Einstellungen wie die Größe der Trainingsdaten und die Anzahl der Trainingsiterationen kontrolliert wird. cfKG charakterisiert den Wert der Informationen, die durch das Abtasten eines Punktes mit einer bestimmten Treue gewonnen werden, wobei der Punkt und die Treue mit dem größten Wert pro Kosteneinheit gewählt werden. cfKG kann darüber hinaus in Anlehnung an Wu et al. (2017) auf Einstellungen verallgemeinert werden, bei denen Ableitungen im Optimierungsprozess verfügbar sind, z. B. beim groß angelegten Kernel-Lernen. Numerische Experimente zeigen, dass cfKG bei der Optimierung synthetischer Funktionen, bei der Abstimmung von Faltungsneuronalen Netzen (CNNs) auf CIFAR-10 und SVHN sowie beim groß angelegten Kernel-Lernen besser abschneidet als aktuelle Algorithmen.
Neuronale Netze, die nur für die Optimierung der Trainingsgenauigkeit trainiert werden, können oft durch ungünstige Beispiele getäuscht werden - leicht gestörte Eingaben, die mit hohem Vertrauen falsch klassifiziert werden. Bei einer repräsentativen Aufgabe, die darin besteht, minimale nachteilige Verzerrungen zu finden, ist unser Verifizierer um zwei bis drei Größenordnungen schneller als der Stand der Technik, und zwar durch enge Formulierungen für Nichtlinearitäten und einen neuartigen Presolve-Algorithmus, der alle verfügbaren Informationen nutzt. Die Beschleunigung der Berechnungen erlaubt es uns, Eigenschaften von Faltungsnetzwerken und Residualnetzwerken mit über 100.000 ReLUs zu verifizieren --- mehrere Größenordnungen mehr als Netzwerke, die zuvor von einem vollständigen Verifizierer verifiziert wurden. Insbesondere bestimmen wir zum ersten Mal die exakte Genauigkeit eines MNIST-Klassifizierers gegenüber Störungen mit begrenzter l-âˆž-Norm Îµ=0. FÃ?r diesen Klassifikator finden wir fÃ?r 4,38% der Proben ein negatives Beispiel und fÃ?r den Rest ein Robustheitszertifikat fÃ?r normgebundene StÃ¶rungen.â€œ FÃ?r alle betrachteten robusten Trainingsverfahren und Netzwerkarchitekturen und sowohl fÃ?r die MNIST- als auch fÃ?r die CIFAR-10-DatensÃ?tze sind wir in der Lage, mehr Proben zu zertifizieren als der Stand der Technik und mehr negative Beispiele zu finden als ein starker Angriff erster Ordnung.
Die meisten State-of-the-Art Deep-Learning-Modelle sind jedoch entweder nicht in der Lage, eine Unsicherheitsschätzung zu erhalten, oder sie müssen erheblich modifiziert werden (z. B. durch die Formulierung einer angemessenen Bayes'schen Behandlung), um sie zu erhalten.Keine der bisherigen Methoden ist in der Lage, ein beliebiges Modell von der Stange zu nehmen und eine Unsicherheitsschätzung zu generieren, ohne es neu zu trainieren oder umzugestalten.Um diese Lücke zu schließen, führen wir die erste systematische Erforschung der trainingsfreien Unsicherheitsschätzung durch. Wir schlagen drei einfache und skalierbare Methoden vor, um die Varianz des Outputs eines trainierten Netzwerks unter tolerierbaren Störungen zu analysieren: infer-transformation, infer-noise und infer-dropout. Sie arbeiten ausschließlich während der Inferenz, ohne die Notwendigkeit, das Modell neu zu trainieren, neu zu entwerfen oder fein abzustimmen, wie es typischerweise von anderen State-of-the-Art-Methoden zur Unsicherheitsschätzung verlangt wird. Überraschenderweise liefern unsere Methoden auch ohne die Einbeziehung solcher Störungen in das Training eine vergleichbare oder sogar bessere Unsicherheitsschätzung im Vergleich zu anderen trainingsabhängigen State-of-the-Art-Methoden, und nicht zuletzt zeigen wir, dass die Unsicherheit unserer vorgeschlagenen Methoden zur Verbesserung des Trainings neuronaler Netze genutzt werden kann.
In diesem Papier stellen wir lernfähige Operationen höherer Ordnung als eine generische Familie von Bausteinen für die Erfassung von Korrelationen höherer Ordnung aus dem hochdimensionalen Eingangsvideoraum vor. wir beweisen, dass mehrere erfolgreiche Architekturen für visuelle Klassifizierungsaufgaben zur Familie der neuronalen Netze höherer Ordnung gehören, deren theoretische und experimentelle Analyse zeigt, dass ihr zugrunde liegender Mechanismus höherer Ordnung ist.  Bei der Videoerkennung können unsere Modelle höherer Ordnung selbst bei ausschließlicher Verwendung von RGB ohne Feinabstimmung mit anderen Videodatensätzen gleichwertige oder bessere Ergebnisse erzielen als die bestehenden State-of-the-Art-Methoden in den beiden Datensätzen Something-Something (V1 und V2) und Charades.
Um die Regularisierung der Konsistenz zu verstehen, untersuchen wir konzeptionell, wie die Verlustgeometrie mit den Trainingsverfahren interagiert: Der Konsistenzverlust verbessert die Generalisierungsleistung im Vergleich zum reinen Training unter Aufsicht dramatisch; wir zeigen jedoch, dass SGD Schwierigkeiten hat, auf dem Konsistenzverlust zu konvergieren und weiterhin große Schritte zu machen, die zu Änderungen in den Vorhersagen auf den Testdaten führen. Motiviert durch diese Beobachtungen schlagen wir vor, konsistenzbasierte Methoden mit Stochastic Weight Averaging (SWA) zu trainieren, einem neueren Ansatz, der die Gewichte entlang der Trajektorie von SGD mit einem modifizierten Lernratenplan mittelt.Wir schlagen auch fast-SWA vor, das die Konvergenz weiter beschleunigt, indem es mehrere Punkte innerhalb jedes Zyklus eines zyklischen Lernratenplans mittelt. Mit der Mittelwertbildung erreichen wir die besten bekannten semi-supervised Ergebnisse auf CIFAR-10 und CIFAR-100, über viele verschiedene Mengen von gelabelten Trainingsdaten. z.B. erreichen wir 5,0% Fehler auf CIFAR-10 mit nur 4000 Labels, verglichen mit dem vorherigen besten Ergebnis in der Literatur von 6,3%.
In diesem Papier finden wir, dass durch die Entwicklung einer neuartigen Verlustfunktion mit dem Namen "Tracking Loss", Convolutional Neural Network (CNN) basierte Objektdetektoren erfolgreich in gut funktionierende visuelle Tracker umgewandelt werden können, ohne dass zusätzliche Rechenkosten anfallen.Diese Eigenschaft ist vorteilhaft für visuelles Tracking, bei dem kommentierte Videosequenzen für das Training immer fehlen, da reichhaltige Merkmale, die von Detektoren aus Standbildern gelernt wurden, von dynamischen Trackern genutzt werden können. Der Tracking-Verlust erreicht diese Eigenschaft, indem er die interne Struktur der Merkmalskarten innerhalb des Erkennungsnetzwerks ausnutzt und verschiedene Merkmalspunkte diskriminierend behandelt. Wir schlagen auch eine Methode zur Komprimierung des Netzwerks vor, um die Verfolgungsgeschwindigkeit ohne Leistungseinbußen zu beschleunigen, was auch beweist, dass der Tracking-Verlust selbst bei einer drastischen Komprimierung des Netzwerks sehr effektiv bleibt. Die Evaluierungsergebnisse zeigen, dass unsere Tracker (einschließlich des Ensemble-Trackers und zwei Baseline-Trackern) alle State-of-the-Art-Methoden auf der VOT 2016 Challenge in Bezug auf Expected Average Overlap (EAO) und Robustheit übertreffen.Wir werden den Code öffentlich zugänglich machen.
Wir untersuchen das Problem der semantischen Code-Reparatur, die im weitesten Sinne als automatisches Reparieren von nicht-syntaktischen Fehlern im Quellcode definiert werden kann.Die meisten bisherigen Arbeiten im Bereich der semantischen Code-Reparatur setzten den Zugang zu Unit-Tests voraus, anhand derer Reparaturkandidaten validiert werden können. Im Gegensatz dazu ist es hier das Ziel, ein starkes statistisches Modell zu entwickeln, um sowohl die Fehlerpositionen als auch die genauen Fehlerbehebungen genau vorherzusagen, ohne Zugang zu Informationen über das beabsichtigte korrekte Verhalten des Programms. Unser Rahmenwerk verfolgt einen zweistufigen Ansatz, bei dem zunächst eine große Menge von Reparaturkandidaten durch regelbasierte Prozessoren generiert wird und diese Kandidaten dann durch ein statistisches Modell unter Verwendung einer neuartigen neuronalen Netzwerkarchitektur bewertet werden, die wir als Share, Specialize, and Compete bezeichnen. Konkret erzeugt die Architektur (1) eine gemeinsame Kodierung des Quellcodes mit einem RNN über den abstrakten Syntaxbaum, (2) bewertet jeden Reparaturkandidaten mit spezialisierten Netzwerkmodulen und (3) normalisiert dann diese Bewertungen zusammen, so dass sie in einem vergleichbaren Wahrscheinlichkeitsraum miteinander konkurrieren können.Wir evaluieren unser Modell auf einem realen Testsatz, der von GitHub gesammelt wurde und vier gängige Kategorien von Fehlern enthält.Unser Modell ist in der Lage, die exakte korrekte Reparatur in 41% der Fälle mit einer einzigen Vermutung vorherzusagen, verglichen mit einer Genauigkeit von 13% für ein Aufmerksamkeits-Sequenz-zu-Sequenz-Modell.
Kürzlich wurde darauf hingewiesen, dass tiefe Netzwerke mit einem Dilemma zwischen Genauigkeit (bei sauberen natürlichen Bildern) und Robustheit (bei nachteilig gestörten Bildern) konfrontiert sind (Tsipras et al., 2019), das in der inhärent höheren Komplexität der Proben (Schmidt et al, In Anbetracht dessen scheint bei einer Klassifizierungsaufgabe die Erhöhung der Modellkapazität dazu beizutragen, eine Win-Win-Situation zwischen Genauigkeit und Robustheit zu schaffen, allerdings auf Kosten der Modellgröße und der Latenzzeit, was eine Herausforderung für Anwendungen mit eingeschränkten Ressourcen darstellt. Ist es möglich, Modellgenauigkeit, Robustheit und Effizienz gemeinsam zu gestalten, um die dreifachen Vorteile zu erreichen? In diesem Beitrag werden Multi-Exit-Netzwerke untersucht, die mit input-adaptiver effizienter Inferenz verbunden sind, und es wird gezeigt, dass sie vielversprechend sind, um einen "Sweet Point" bei der gemeinsamen Optimierung von Modellgenauigkeit, Robustheit und Effizienz zu erreichen. Die von uns vorgeschlagene Lösung, die wir Robust Dynamic Inference Networks (RDI-Nets) nennen, erlaubt es, dass jede Eingabe (entweder sauber oder adversarisch) adaptiv eine der mehreren Ausgabeschichten (frühe Verzweigungen oder die endgültige) wählt, um ihre Vorhersage auszugeben.Diese Multiverlust-Adaptivität fügt adversarischen Angriffen und Verteidigungen neue Variationen und Flexibilität hinzu, die wir systematisch untersuchen. Wir zeigen experimentell, dass durch die Ausstattung bestehender Backbones mit einer solchen robusten adaptiven Inferenz die resultierenden RDI-Netze eine bessere Genauigkeit und Robustheit erreichen können, und zwar mit über 30 % Rechenzeitersparnis im Vergleich zu den verteidigten Originalmodellen.
In einem Versuch, die Repräsentationen von tiefen Faltungsnetzen zu verstehen, die auf Sprachaufgaben trainiert wurden, zeigen wir, dass einzelne Einheiten selektiv auf spezifische Morpheme, Wörter und Phrasen reagieren, anstatt auf willkürliche und uninterpretierbare Muster. Um dieses faszinierende Phänomen quantitativ zu analysieren, schlagen wir eine Methode zum Konzeptabgleich vor, die darauf basiert, wie die Einheiten auf replizierten Text reagieren, und führen Analysen mit verschiedenen Architekturen auf mehreren Datensätzen für Klassifizierungs- und Übersetzungsaufgaben durch.
Wir untersuchen das Problem des Aufbaus von Modellen, die unabhängige Faktoren der Variation entwirren.Solche Modelle kodieren Merkmale, die effizient für die Klassifizierung und die Übertragung von Attributen zwischen verschiedenen Bildern in der Bildsynthese verwendet werden können.Als Daten verwenden wir eine schwach beschriftete Trainingsmenge, in der Beschriftungen angeben, welcher einzelne Faktor zwischen zwei Datenproben geändert hat, obwohl der relative Wert der Änderung unbekannt ist.Diese Beschriftung ist von besonderem Interesse, da sie ohne Annotationskosten leicht verfügbar sein kann.Wir führen ein Autoencoder-Modell ein und trainieren es durch Beschränkungen auf Bildpaare und Triplets. Wir zeigen theoretisch und experimentell die Rolle der Merkmalsdimensionalität und des adversen Trainings und beweisen formal die Existenz der Referenzmehrdeutigkeit, die bei der Entflechtung von Daten mit schwacher Beschriftung inhärent ist.Der numerische Wert eines Faktors hat in verschiedenen Referenzrahmen unterschiedliche Bedeutung.Wenn die Referenz von anderen Faktoren abhängt, wird die Übertragung dieses Faktors mehrdeutig.Wir zeigen experimentell, dass das vorgeschlagene Modell erfolgreich Attribute auf mehrere Datensätze übertragen kann, zeigen aber auch Fälle, in denen die Referenzmehrdeutigkeit auftritt.
In Information Retrieval, Lernen zu Rang konstruiert eine maschinenbasierte Ranking-Modell, das eine Abfrage gegeben, sortiert die Suchergebnisse durch den Grad der Relevanz oder Bedeutung für die query.neuronale Netze wurden erfolgreich auf dieses Problem angewandt, und in diesem Papier, schlagen wir eine Aufmerksamkeit-basierte tiefe neuronale Netz, das besser umfasst verschiedene Einbettungen der Abfragen und Suchergebnisse mit einer Aufmerksamkeit-basierten Mechanismus. Die Einbettungen werden mit Faltungsneuronalen Netzen oder dem word2vec-Modell trainiert. Wir demonstrieren die Leistung dieses Modells anhand von Datensätzen für Bildabfragen und Textabfragen.
Die Computational Neuroscience zielt darauf ab, zuverlässige Modelle der neuronalen Aktivität in vivo zu erstellen und diese als abstrakte Berechnungen zu interpretieren. Jüngste Arbeiten haben gezeigt, dass die funktionelle Vielfalt von Neuronen auf relativ wenige Zelltypen beschränkt sein kann; andere Arbeiten haben gezeigt, dass die Einbeziehung von Einschränkungen in künstliche neuronale Netze (ANNs) deren Fähigkeit zur Nachahmung neuronaler Daten verbessern kann. In dieser Arbeit wird ein Algorithmus entwickelt, der Aufzeichnungen neuronaler Aktivität als Input nimmt und Cluster von Neuronen nach Zelltyp und Modelle neuronaler Aktivität, die durch diese Cluster eingeschränkt sind, liefert. die resultierenden Modelle sind sowohl vorhersehbarer als auch interpretierbarer und enthüllen die Beiträge funktionaler Zelltypen zur neuronalen Berechnung und informieren letztendlich das Design zukünftiger ANNs.
Graph Neural Networks (GNNs) sind ein leistungsfähiges Repräsentationswerkzeug für die Lösung von Problemen mit graphenstrukturierten Eingaben. In fast allen bisherigen Fällen wurden sie jedoch zur direkten Gewinnung einer endgültigen Lösung aus rohen Eingaben eingesetzt, ohne explizite Anleitung zur Strukturierung ihrer Problemlösung. Hier konzentrieren wir uns stattdessen auf das Lernen im Raum der Algorithmen: Wir trainieren mehrere hochmoderne GNN-Architekturen, um einzelne Schritte klassischer Graphenalgorithmen zu imitieren, und zwar sowohl parallel (breadth-first search, Bellman-Ford) als auch sequentiell (Prim's algorithm). Da Graphenalgorithmen in der Regel auf diskreten Entscheidungen innerhalb von Nachbarschaften beruhen, stellen wir die Hypothese auf, dass maximierungsbasierte neuronale Netze mit Nachrichtenübermittlung für solche Ziele am besten geeignet sind, und validieren diese Behauptung empirisch.Wir zeigen auch, wie das Lernen im Raum der Algorithmen neue Möglichkeiten für einen positiven Transfer zwischen Aufgaben bieten kann - wir zeigen, wie das Lernen eines Algorithmus für den kürzesten Weg erheblich verbessert werden kann, wenn gleichzeitig ein Erreichbarkeitsalgorithmus gelernt wird.
Prospektion ist ein wichtiger Teil davon, wie Menschen kommen mit neuen Aufgabe plant, hat aber nicht in der Tiefe in robotics.Predicting mehrere Aufgabe-Ebene ist ein anspruchsvolles Problem, das sowohl Aufgabe Semantik und kontinuierliche Variabilität über den Zustand der world.Idealerweise würden wir die Fähigkeit des maschinellen Lernens zu nutzen, um große Daten für das Lernen der Semantik einer Aufgabe zu kombinieren, während die Verwendung von Techniken aus Aufgabe Planung zuverlässig auf neue environment.In dieser Arbeit, schlagen wir eine Methode für das Lernen eines Modells kodiert genau eine solche Darstellung für die Aufgabenplanung. Wir lernen ein neuronales Netz, das die k wahrscheinlichsten Ergebnisse von Aktionen auf hoher Ebene aus einer gegebenen Welt kodiert.Unser Ansatz schafft verständliche Aufgabenpläne, die es uns ermöglichen, Veränderungen in der Umgebung viele Zeitschritte in die Zukunft vorherzusagen.Wir demonstrieren diesen Ansatz durch Anwendung auf eine Stapelaufgabe in einer unübersichtlichen Umgebung, in der der Roboter zwischen verschiedenen farbigen Blöcken wählen muss, während er Hindernissen ausweicht, um eine Aufgabe zu erfüllen.Wir zeigen auch Ergebnisse auf einer einfachen Navigationsaufgabe.Unser Algorithmus erzeugt realistische Bild- und Posenvorhersagen an mehreren Punkten in einer bestimmten Aufgabe.
Während die Theorie adaptiver Gradientenmethoden für Minimierungsprobleme gut verstanden wird, bleiben die zugrunde liegenden Faktoren, die ihren empirischen Erfolg bei Min-Max-Problemen wie GANs antreiben, unklar.In diesem Beitrag versuchen wir, diese Lücke sowohl aus theoretischer als auch aus empirischer Sicht zu schließen. Zunächst analysieren wir eine in~\citep{daskalakis2017training} vorgeschlagene Variante des Optimistic Stochastic Gradient (OSG) zur Lösung einer Klasse von nicht-konvexen, nicht-konkaven Min-Max-Problemen und ermitteln $O(\epsilon^{-4})$-Komplexität für das Finden eines stationären Punktes erster Ordnung $\epsilon$, wobei der Algorithmus nur ein stochastisches Orakel erster Ordnung benötigt, während er die modernste Iterationskomplexität genießt, die durch die stochastische Extragradientenmethode von~\citep{iusem2017extragradient} erreicht wird. Dann schlagen wir eine adaptive Variante von OSG namens Optimistic Adagrad (OAdagrad) vor und zeigen eine \emph{verbesserte} adaptive Komplexität $\widetilde{O}\left(\epsilon^{-\frac{2}{1-\alpha}}\right)$~\footnote{Hier $\widetilde{O}(\cdot)$ komprimiert einen logarithmischen Faktor von $\epsilon$. }, wobei $\alpha$ die Wachstumsrate des kumulativen stochastischen Gradienten charakterisiert und $0\leq \alpha\leq 1/2$. Soweit wir wissen, ist dies die erste Arbeit, die adaptive Komplexität in nicht-konvexer, nicht-konkaver Min-Max-Optimierung etabliert.Empirisch zeigen unsere Experimente, dass adaptive Gradientenalgorithmen ihre nicht-adaptiven Gegenstücke im GAN-Training tatsächlich übertreffen.Darüber hinaus kann diese Beobachtung durch die langsame Wachstumsrate des kumulativen stochastischen Gradienten, wie empirisch beobachtet, erklärt werden.
Wir betrachten das Problem des unbeaufsichtigten Lernens eines niedrigdimensionalen, interpretierbaren, latenten Zustands eines Videos, das ein sich bewegendes Objekt enthält. Das Problem des Destillierens von Dynamik aus Pixeln wurde ausgiebig durch die Linse von graphischen/Zustandsraummodellen betrachtet, die Markov-Strukturen für billige Berechnungen und strukturierte graphische Modellprioritäten für die Durchsetzung von Interpretierbarkeit auf latenten Repräsentationen ausnutzen. Wir machen einen Schritt in Richtung der Erweiterung dieser Ansätze, indem wir die Markov-Struktur weglassen; stattdessen verwenden wir den kürzlich vorgeschlagenen Gaußschen Prozessprior-Variations-Autoencoder für das Lernen von anspruchsvollen latenten Trajektorien. Wir beschreiben das Modell und führen Experimente auf einem synthetischen Datensatz durch und sehen, dass das Modell zuverlässig glatte Dynamiken rekonstruiert, die U-Turns und Schleifen aufweisen. Wir beobachten auch, dass dieses Modell ohne Beta-Tannealing oder Freeze-Tau der Trainingsparameter trainiert werden kann.
Träume und unsere Fähigkeit, sich an sie zu erinnern, gehören zu den rätselhaftesten Fragen in der Schlafforschung, insbesondere die vermeintlichen Unterschiede in der Dynamik der Gehirnnetzwerke zwischen Personen mit hoher und niedriger Traumerinnerungsrate sind immer noch schlecht verstanden.in dieser Studie haben wir diese Frage als Klassifizierungsproblem angegangen, indem wir tiefe Faltungsnetzwerke (CNN) auf Schlaf-EEG-Aufzeichnungen angewendet haben, um vorherzusagen, ob die Probanden zur Gruppe mit hoher oder niedriger Traumerinnerung (HDR bzw. LDR) gehören. Wir haben auch den Merkmalsraum visualisiert, um die Subjektspezifität der gelernten Merkmale zu untersuchen und so sicherzustellen, dass das Netzwerk Unterschiede auf Populationsebene erfasst. Es ist nicht nur die erste Studie, die Deep Learning auf Schlaf-EEG anwendet, um HDR und LDR zu klassifizieren, sondern auch die erste Studie, in der wir mit Hilfe von geführter Backpropagation die am stärksten diskriminierenden Merkmale in jedem Schlafstadium visualisieren konnten.die Bedeutung dieser Ergebnisse und zukünftige Richtungen werden diskutiert.
Wir formulieren das Problem des vernetzten MARL (NMARL) als einen räumlich-zeitlichen Markov-Entscheidungsprozess und führen einen räumlichen Diskontfaktor ein, um das Training jedes lokalen Agenten zu stabilisieren. Basierend auf Experimenten in realistischen NMARL-Szenarien der adaptiven Verkehrssignalsteuerung und der kooperativen adaptiven Geschwindigkeitsregelung verbessert ein geeigneter räumlicher Diskontfaktor effektiv die Lernkurven von nicht-kommunikativen MARL-Algorithmen, während NeurComm die bestehenden Kommunikationsprotokolle sowohl in der Lerneffizienz als auch in der Steuerungsleistung übertrifft.
Neuere Arbeiten zur Entwicklung dieser Klasse von Methoden haben immer umfangreichere Parametrisierungen des approximativen Posters erforscht, in der Hoffnung, die Leistung zu verbessern. Im Gegensatz dazu teilen wir hier ein kurioses experimentelles Ergebnis, das nahelegt, die Variationsverteilung stattdessen auf eine kompaktere Parametrisierung zu beschränken. Für eine Reihe von tiefen Bayes'schen neuronalen Netzen, die mit Gauß'scher Mittelwertfeld-Variationsinferenz trainiert wurden, stellen wir fest, dass die Posterior-Standardabweichungen nach der Konvergenz durchgängig eine starke Low-Rank-Struktur aufweisen, was bedeutet, dass wir durch Zerlegung dieser Variationsparameter in eine Low-Rank-Faktorisierung unsere Variationsapproximation kompakter machen können, ohne die Leistung der Modelle zu verringern.
Training überwacht neuronale Netze für Aspekt-Extraktion ist nicht möglich, wenn Grundwahrheit Aspekt Etiketten nicht verfügbar sind, während die unbeaufsichtigte neuronale Themen-Modelle nicht auf die besonderen Aspekte von Interesse zu erfassen.In dieser Arbeit schlagen wir eine schwach überwachten Ansatz für die Ausbildung neuronaler Netze für Aspekt-Extraktion in Fällen, in denen nur eine kleine Menge von Seed-Wörter, dh, Erstens zeigen wir, dass derzeitige schwach überwachte Netze nicht in der Lage sind, die Vorhersagekraft der verfügbaren Startwörter zu nutzen, indem wir sie mit einem einfachen Bag-of-Words-Klassifikator vergleichen.  Zweitens schlagen wir einen Destillationsansatz für die Aspektextraktion vor, bei dem die Saatwörter vom Bag-of-Words-Klassifikator (Lehrer) berücksichtigt und zu den Parametern eines neuronalen Netzes (Schüler) destilliert werden. 3. zeigen wir, dass die Regularisierung den Schüler dazu ermutigt, Nicht-Saatwörter für die Klassifizierung zu berücksichtigen, und dass der Schüler infolgedessen besser abschneidet als der Lehrer, der nur die Saatwörter berücksichtigt. Schließlich zeigen wir empirisch, dass unser vorgeschlagener Destillationsansatz frühere schwach überwachte Ansätze zur Aspekt-Extraktion in sechs Domänen von Amazon-Produktrezensionen übertrifft (um bis zu 34,4 % im F1-Score).
Es wird angenommen, dass diese Fähigkeit im Gehirn durch Berechnungen entsteht, die durch Bottom-up-, horizontale und Top-down-Verbindungen zwischen Neuronen implementiert werden. Die relativen Beiträge dieser Verbindungen zur wahrnehmungsbezogenen Gruppierung sind jedoch kaum bekannt. Wir befassen uns mit dieser Frage, indem wir systematisch neuronale Netzwerkarchitekturen mit Kombinationen dieser Verbindungen bei zwei synthetischen visuellen Aufgaben evaluieren, die Low-Level-"Gestalt"- bzw. High-Level-Objekthinweise für die wahrnehmungsbezogene Gruppierung betonen. Wir zeigen, dass die Erhöhung des Schwierigkeitsgrades einer der beiden Aufgaben das Lernen von Netzwerken belastet, die sich ausschließlich auf die Bottom-up-Verarbeitung verlassen.Horizontale Verbindungen lösen diese Einschränkung bei Aufgaben mit Gestalt-Hinweisen, indem sie die inkrementelle räumliche Ausbreitung von Aktivitäten unterstützen, während Top-down-Verbindungen das Lernen bei Aufgaben mit Objekt-Hinweisen auf hoher Ebene retten, indem sie grobe Vorhersagen über die Position des Zielobjekts verändern.Unsere Ergebnisse trennen die rechnerischen Rollen von Bottom-up-, horizontaler und Top-down-Konnektivität und zeigen, wie ein Modell, das alle diese Interaktionen berücksichtigt, flexibler lernen kann, Wahrnehmungsgruppen zu bilden.
Generative adversarische Netze haben in den letzten Jahren eine rasante Entwicklung erlebt und zu bemerkenswerten Verbesserungen bei der generativen Modellierung von Bildern geführt. Ihre Anwendung im Audiobereich hat jedoch nur begrenzte Aufmerksamkeit erhalten, und autoregressive Modelle wie WaveNet sind nach wie vor der Stand der Technik bei der generativen Modellierung von Audiosignalen wie menschlicher Sprache. Um diesen Mangel zu beheben, stellen wir GAN-TTS vor, ein Generatives Adversariales Netzwerk für Text-to-Speech. Unsere Architektur besteht aus einem bedingten Feed-Forward-Generator, der rohe Sprachaudiosignale erzeugt, und einem Ensemble von Diskriminatoren, die mit Zufallsfenstern unterschiedlicher Größe arbeiten. Die Diskriminatoren analysieren die Audiosignale sowohl im Hinblick auf ihren allgemeinen Realismus als auch darauf, wie gut die Audiosignale der zu sprechenden Äußerung entsprechen.  Um die Leistung von GAN-TTS zu messen, verwenden wir sowohl subjektive menschliche Bewertungen (MOS - Mean Opinion Score) als auch neuartige quantitative Metriken (FrÃ©chet DeepSpeech Distance und Kernel DeepSpeech Distance), die unserer Meinung nach gut mit MOS korrelieren. Wir zeigen, dass GAN-TTS in der Lage ist, hochrealistische Sprache mit einer Natürlichkeit zu erzeugen, die mit den State-of-the-Art-Modellen vergleichbar ist, und im Gegensatz zu autoregressiven Modellen ist es dank eines effizienten Feed-Forward-Generators hochgradig parallelisierbar. HÃ¶ren Sie sich GAN-TTS an und lesen Sie diese Zusammenfassung auf http://tiny.cc/gantts.
Im Gegensatz zu bestehenden Arbeiten verwendet unser PiT-Rahmenwerk die spärlichen Strafen, um Netzwerke zu trainieren und so die Wichtigkeit von Gewichten und Filtern zu bewerten. Unsere PiT-Algorithmen können das Netzwerk direkt beschneiden, ohne dass eine Feinabstimmung erforderlich ist, und die beschnittenen Netzwerke können immer noch eine vergleichbare Leistung wie die ursprünglichen Netzwerke erzielen. Insbesondere führen wir die (Gruppe) Lasso-Typ Penalty (L-P /GL-P), und (Gruppe) Split LBI Penalty (S-P / GS-P), um die Netze zu regularisieren, und ein Pruning-Strategie vorgeschlagen wird in Hilfe beschneiden das network.We führen die umfangreichen Experimente auf MNIST, Cifar-10, und miniImageNet.The Ergebnisse bestätigen die Wirksamkeit unserer vorgeschlagenen Methoden.Remarkably, auf MNIST-Datensatz, unsere PiT Rahmen kann 17,5% Parameter Größe von LeNet-5, die die 98,47% Erkennungsgenauigkeit erreicht speichern.
Wir stellen zunächst die Unsupervised Continual Learning (UCL) Problem: Lernen salient Darstellungen aus einem nicht-stationären Strom von unbeschrifteten Daten, in denen die Anzahl der Objektklassen variiert mit der Zeit.Angesichts der begrenzten beschrifteten Daten kurz vor Inferenz, können diese Darstellungen auch mit bestimmten Objekttypen zugeordnet werden, um die Klassifizierung durchzuführen.Um das UCL-Problem zu lösen, schlagen wir eine Architektur, die ein einzelnes Modul, genannt Self-Taught Associative Memory (STAM), die lose Modelle die Funktion einer kortikalen Spalte im Säugetiergehirn beinhaltet. Hierarchien von STAM-Modulen lernen auf der Grundlage einer Kombination aus Hebb'schem Lernen, Online-Clustering, Erkennung neuartiger Muster und Vergessen von Ausreißern sowie Top-Down-Vorhersagen. Wir veranschaulichen die Funktionsweise von STAMs im Zusammenhang mit dem kontinuierlichen Erlernen handgeschriebener Ziffern mit nur 3-12 markierten Beispielen pro Klasse.STAMs weisen einen vielversprechenden Weg zur Lösung des UCL-Problems ohne katastrophales Vergessen.
Trotz dieses Fortschritts ist die potenzielle Leistung von voll komplexen Zwischenberechnungen und Darstellungen noch nicht für viele anspruchsvolle Lernprobleme erforscht worden.Aufbauend auf den jüngsten Fortschritten, schlagen wir einen neuartigen Mechanismus für die Extraktion von Signalen im Frequenzbereich.Als Fallstudie führen wir Audio-Quelle Trennung in der Fourier-Domäne.Unsere Extraktion Mechanismus könnte als eine lokale Ensembling-Methode, die eine komplex-bewertete Faltung Version von Feature-Wise Linear Modulation (FiLM) und eine Signal-Mittelung Betrieb kombiniert betrachtet werden. Außerdem führen wir einen neuen expliziten Amplituden- und Phasenverlust ein, der skalen- und zeitinvariant ist und die komplexwertigen Komponenten des Spektrogramms berücksichtigt. Anhand des Wall Street Journal-Datensatzes vergleichen wir unseren Phasenverlust mit mehreren anderen, die sowohl im Zeit- als auch im Frequenzbereich arbeiten, und demonstrieren die Wirksamkeit unserer vorgeschlagenen Signalextraktionsmethode und des vorgeschlagenen Verlusts. Wenn wir im komplexwertigen Frequenzbereich arbeiten, übertrifft unser tiefes komplexwertiges Netzwerk seine reellwertigen Gegenstücke erheblich, sogar mit der Hälfte der Tiefe und einem Drittel der Parameter.Unser vorgeschlagener Mechanismus verbessert die Leistung von tiefen komplexwertigen Netzwerken erheblich und wir demonstrieren die Nützlichkeit seiner regulierenden Wirkung.
Es ist eine Herausforderung, ein Objekt in zwei orthogonale Räume von Inhalt und Stil zu entwirren, da jeder die visuelle Beobachtung in einer anderen und unvorhersehbaren Weise beeinflussen kann.Es ist selten, dass man Zugang zu einer großen Anzahl von Daten hat, um die Einflüsse zu trennen.In diesem Papier stellen wir einen neuartigen Rahmen vor, um diese entwirrte Darstellung in einer völlig unbeaufsichtigten Weise zu lernen.Wir gehen dieses Problem in einem Zwei-Zweig-Autoencoder-Rahmen an.Für den strukturellen Inhaltszweig projizieren wir den latenten Faktor in einen weich strukturierten Punkttensor und beschränken ihn mit Verlusten, die aus dem Vorwissen abgeleitet sind. Die beiden Zweige bilden einen effektiven Rahmen, der die Repräsentation von Inhalt und Stil eines Objekts ohne menschliche Annotation entwirren kann. Wir evaluieren unseren Ansatz an vier Bilddatensätzen, an denen wir die überlegene Qualität der Entwirrung und der visuellen Analogie sowohl in synthetisierten als auch in realen Daten demonstrieren. Wir sind in der Lage, fotorealistische Bilder mit einer Auflösung von 256x256 zu erzeugen, die in Inhalt und Stil klar entwirrt sind.
Wir entwickeln den Y-Learner für die Schätzung heterogener Behandlungseffekte in experimentellen und Beobachtungsstudien.Der Y-Learner ist so konzipiert, dass er die Fähigkeiten neuronaler Netze zur Optimierung mehrerer Ziele und zur kontinuierlichen Aktualisierung nutzt, was eine bessere Zusammenführung der zugrunde liegenden Merkmalsinformationen zwischen Behandlungs- und Kontrollgruppen ermöglicht.Wir evaluieren den Y-Learner an drei Testproblemen: (1) Ein Satz von sechs simulierten Daten-Benchmarks aus der Literatur.(2) Ein reales, groß angelegtes Experiment zur Wählerüberzeugung.(3) Eine Aufgabe aus der Literatur, die künstlich erzeugte Behandlungseffekte auf MNIST-Didgits schätzt.Der Y-learner erreicht bei zwei der drei Aufgaben den Stand der Technik.Bei der MNIST-Aufgabe erzielt er die zweitbesten Ergebnisse.
Mit der rasanten Verbreitung von IoT-Geräten wird unser Cyberspace heute von Milliarden von kostengünstigen Computerknoten beherrscht, die eine noch nie dagewesene Heterogenität unserer Computersysteme darstellen. Die dynamische Analyse, einer der effektivsten Ansätze zum Auffinden von Softwarefehlern, ist aufgrund des Fehlens eines generischen Emulators, der in der Lage ist, verschiedene, zuvor nicht gesehene Firmware auszuführen, gelähmt. In den letzten Jahren wurden wir Zeugen verheerender Sicherheitsverletzungen, die auf IoT-Geräte abzielten. Diese Sicherheitsbedenken haben die weitere Entwicklung der IoT-Technologie erheblich behindert. In dieser Arbeit stellen wir Laelaps vor, einen Geräteemulator, der speziell dafür entwickelt wurde, verschiedene Software auf kostengünstigen IoT-Geräten auszuführen. Wir kodieren in unserem Emulator keine spezifischen Informationen über ein Gerät, stattdessen leitet Laelaps das erwartete Verhalten der Firmware durch symbolische, ausführungsunterstützte Peripherie-Emulation ab und generiert geeignete Eingaben, um die konkrete Ausführung im laufenden Betrieb zu steuern. Um die Fähigkeiten von Laelaps zu demonstrieren, haben wir zwei populäre dynamische Analysetechniken - Fuzzing-Tests und dynamische symbolische Ausführung - auf unserem Emulator eingesetzt und erfolgreich sowohl selbst injizierte als auch reale Schwachstellen identifiziert.
Tiefe neuronale Modelle wie Faltungsnetze und rekurrente Netze erzielen phänomenale Ergebnisse bei räumlichen Daten wie Bildern und Texten, aber bei tabellarischen Daten ist die Methode der Wahl nach wie vor das Gradient-Boosting von Entscheidungsbäumen (GBDT).Um diese Lücke zu schließen, schlagen wir \emph{deep neural forests} (DNF) vor, eine neuartige Architektur, die sowohl Elemente von Entscheidungsbäumen als auch dichte Restverbindungen kombiniert. Wir stellen die Ergebnisse einer umfangreichen empirischen Studie vor, in der wir die Leistung von GBDTs, DNFs und (tiefen) vollständig verbundenen Netzwerken untersuchen. Diese Ergebnisse zeigen, dass DNFs vergleichbare Ergebnisse wie GBDTs auf tabellarischen Daten erzielen und die Tür zu einer durchgängigen neuronalen Modellierung von multimodalen Daten öffnen.
State-of-the-Art-Optimierer, wie AdaGrad, RMSProp und Adam, reduzieren diese Arbeit, indem sie eine individuelle Lernrate für jede Variable adaptiv abstimmen.Kürzlich haben Forscher ein erneutes Interesse an einfacheren Methoden wie Momentum SGD gezeigt, da sie bessere Ergebnisse liefern können.Motiviert durch diesen Trend, fragen wir: können einfache adaptive Methoden, die auf SGD basieren, genauso gut oder besser abschneiden?Wir überarbeiten den Momentum SGD-Algorithmus und zeigen, dass die manuelle Abstimmung einer einzelnen Lernrate und Momentum ihn mit Adam konkurrenzfähig macht. Basierend auf diesen Erkenntnissen entwickeln wir YellowFin, einen automatischen Tuner für Momentum und Lernrate in SGD. YellowFin verwendet optional eine negative Rückkopplungsschleife, um die Dynamik des Momentums in asynchronen Einstellungen zu kompensieren. Wir zeigen empirisch, dass YellowFin in weniger Iterationen konvergieren kann als Adam auf ResNets und LSTMs für Bilderkennung, Sprachmodellierung und Constituency Parsing, mit einem Speedup von bis zu $3.28$x in synchronen und bis zu $2.69$x in asynchronen Einstellungen.
Robustheit und Sicherheit von maschinellen Lernsystemen (ML) sind eng miteinander verknüpft, da ein nicht robustes ML-System (Klassifikatoren, Regressoren usw.) durch eine Vielzahl von Exploits angegriffen werden kann.Mit dem Aufkommen skalierbarer Deep-Learning-Methoden wurde viel Wert auf die Robustheit von überwachten, unüberwachten und verstärkenden Lernalgorithmen gelegt. Hier untersuchen wir die Robustheit des latenten Raums eines Deep Variational Autoencoders (dVAE), eines unbeaufsichtigten generativen Frameworks, um zu zeigen, dass es in der Tat möglich ist, den latenten Raum zu stören, die Klassenvorhersagen umzudrehen und die Klassifizierungswahrscheinlichkeit vor und nach einem Angriff annähernd gleich zu halten, was bedeutet, dass ein Agent, der sich die Ausgaben eines Decoders ansieht, einen Angriff nicht bemerkt.
Graphenbasiertes Dependency Parsing besteht aus zwei Schritten: Erstens erzeugt ein Encoder eine Merkmalsrepräsentation für jede Parsing-Substruktur des Eingabesatzes, die dann zur Berechnung einer Punktzahl für die Substruktur verwendet wird; und zweitens findet ein Decoder} den Parse-Baum, dessen Substrukturen die größte Gesamtpunktzahl haben. In den letzten Jahren wurden leistungsstarke neuronale Techniken in den Kodierungsschritt eingeführt, die die Parsing-Genauigkeit erheblich steigern, während fortgeschrittene Dekodierungstechniken, insbesondere die Dekodierung höherer Ordnung, immer weniger genutzt werden. Es wird allgemein angenommen, dass kontextualisierte Merkmale, die von neuronalen Kodierern erzeugt werden, dazu beitragen können, Dekodierungsinformationen höherer Ordnung zu erfassen und somit die Notwendigkeit eines Dekodierers höherer Ordnung zu verringern. In dieser Arbeit evaluieren wir empirisch die Kombinationen verschiedener neuronaler und nicht-neuronaler Encoder mit Decodern erster und zweiter Ordnung und liefern eine umfassende Analyse über die Effektivität dieser Kombinationen mit unterschiedlichen Trainingsdatengrößen. Wir stellen fest, dass erstens bei großen Trainingsdaten ein starker neuronaler Encoder mit Dekodierung erster Ordnung ausreicht, um eine hohe Parsing-Genauigkeit zu erreichen, und nur geringfügig hinter der Kombination aus neuronaler Enkodierung und Dekodierung zweiter Ordnung zurückbleibt; zweitens bei kleinen Trainingsdaten ein nicht-neuronaler Encoder mit einem Decoder zweiter Ordnung die anderen Kombinationen in den meisten Fällen übertrifft.   
In der Praxis ist es jedoch schwierig, die Verteilung der Meta-Trainingsaufgaben zu definieren, die zum Trainieren von Meta-Lernern verwendet werden. Wenn die Aufgaben zu klein sind, sind sie zu ähnlich, als dass ein Modell sinnvoll verallgemeinern könnte, und wenn sie zu groß sind, wird die Verallgemeinerung unglaublich schwierig. Wir argumentieren, dass beide Probleme durch die Einführung eines Lehrermodells gemildert werden können, das die Abfolge der Aufgaben steuert, mit denen ein Meta-Lerner trainiert wird.Dieses Lehrermodell hat den Anreiz, den Meta-Lerner mit einfachen Aufgaben zu beginnen und dann den Schwierigkeitsgrad der Aufgaben als Reaktion auf die Fortschritte des Schülers zu erhöhen.Während dieser Ansatz bereits bei der Erstellung von Lehrplänen untersucht wurde, besteht unser Hauptbeitrag darin, ihn auf das Meta-Lernen auszuweiten.
Die Verwendung von Wissen höherer Ordnung zur Reduktion von Trainingsdaten ist ein beliebtes Forschungsthema geworden, aber die Fähigkeit der verfügbaren Methoden, effektive Entscheidungsgrenzen zu ziehen, ist immer noch begrenzt: Wenn die Trainingsmenge klein ist, werden neuronale Netze zu bestimmten Bezeichnungen voreingenommen.Auf der Grundlage dieser Beobachtung betrachten wir die Einschränkung der Ausgabe-Wahrscheinlichkeitsverteilung als Domänenwissen höherer Ordnung.Wir entwerfen einen neuartigen Algorithmus, der gemeinsam die Ausgabe-Wahrscheinlichkeitsverteilung auf einem geclusterten Einbettungsraum optimiert, damit neuronale Netze effektive Entscheidungsgrenzen ziehen.  Wir verwenden Experimente, um empirisch zu beweisen, dass unser Modell zu einer höheren Genauigkeit konvergieren kann als andere state-of-art semi-supervised Lernmodelle mit weniger hochwertigen gelabelten Trainingsbeispielen.
Wir führen eine neue Art von tiefer kontextualisierter Wortrepräsentation ein, die sowohl (1) komplexe Merkmale der Wortverwendung (z.B. Syntax und Semantik) modelliert, als auch (2) wie diese Verwendungen in verschiedenen sprachlichen Kontexten variieren (d.h. um Polysemie zu modellieren).  Unsere Wortvektoren sind gelernte Funktionen der internen Zustände eines tiefen bidirektionalen Sprachmodells (biLM), das auf einem großen Textkorpus vortrainiert wurde. wir zeigen, dass diese Repräsentationen leicht zu bestehenden Modellen hinzugefügt werden können und den Stand der Technik bei sechs anspruchsvollen NLP-Problemen, einschließlich Fragebeantwortung, textuellem Entailment und Stimmungsanalyse, erheblich verbessern.  Wir stellen auch eine Analyse vor, die zeigt, dass die Offenlegung der tiefen Interna des vortrainierten Netzwerks entscheidend ist und es nachgelagerten Modellen ermöglicht, verschiedene Arten von Semi-Supervisionssignalen zu mischen.
Diese Arbeit befasst sich mit dem seit langem bestehenden Problem der robusten Ereignislokalisierung in Anwesenheit von zeitlich falsch ausgerichteten Beschriftungen in den Trainingsdaten.Wir schlagen eine neuartige, vielseitige Verlustfunktion vor, die eine Reihe von Trainingsregimen verallgemeinert, von der standardmäßigen, vollständig überwachten Kreuzentropie bis hin zum zählbasierten, schwach überwachten Lernen.Im Gegensatz zu klassischen Modellen, die gezwungen sind, sich während des Trainings streng an die Annotationen anzupassen, entspannt unser weicher Lokalisierungslernansatz stattdessen die Abhängigkeit von der genauen Position der Beschriftungen. Im Gegensatz zu klassischen Modellen, die sich während des Trainings streng an die exakte Position der Beschriftungen anpassen müssen, ist unser Soft-Localization-Learning-Ansatz nicht auf die exakte Position der Beschriftungen angewiesen.
Die treibende Kraft hinter tiefen Netzwerken ist ihre Fähigkeit, umfangreiche Funktionsklassen kompakt darzustellen. Der primäre Begriff für die formale Betrachtung dieses Phänomens ist die expressive Effizienz, die sich auf eine Situation bezieht, in der ein Netzwerk unvorstellbar groß werden muss, um Funktionen eines anderen zu replizieren. In diesem Papier untersuchen wir die expressive Effizienz, die durch Konnektivität hervorgerufen wird, motiviert durch die Beobachtung, dass moderne Netzwerke ihre Schichten auf ausgeklügelte Weise miteinander verbinden. Durch die Einführung und Analyse des Konzepts der gemischten Tensor-Dekompositionen beweisen wir, dass die Verbindung von dilatierten Faltungsnetzen zu einer ausdrucksstarken Effizienz führen kann, insbesondere zeigen wir, dass bereits eine einzige Verbindung zwischen Zwischenschichten zu einer fast quadratischen Lücke führen kann, was in großen Umgebungen typischerweise den Unterschied zwischen einem praktikablen und einem unpraktikablen Modell ausmacht. Die empirische Auswertung zeigt, dass die expressive Effizienz der Konnektivität, ähnlich wie die der Tiefe, zu einem Gewinn an Genauigkeit führt, was uns zu der Überzeugung führt, dass die expressive Effizienz eine Schlüsselrolle bei der Entwicklung neuer Werkzeuge für das Design von tiefen Netzwerken spielen kann.
Der MNIST-Datensatz wurde als die Drosophila des maschinellen Lernens bezeichnet und diente als Prüfstand für viele Lerntheorien. Der NotMNIST-Datensatz und der FashionMNIST-Datensatz wurden mit dem MNIST-Datensatz als Referenz erstellt. In dieser Arbeit nutzen wir diese MNIST-ähnlichen Datensätze für das Multi-Task-Lernen. Die gelernten Parameter werden dann als Anfangsparameter verwendet, um disjunkte Klassifizierungsnetzwerke neu zu trainieren.Das Basis-Erkennungsmodell sind neuronale Netze mit vollständiger Faltung.Ohne Multi-Task-Lernen sind die Erkennungsgenauigkeiten für MNIST, NotMNIST und FashionMNIST 99,56\%, 97. 22\% bzw. 94,32\%. Mit Multi-Task-Learning zum Vortraining der Netze liegt die Erkennungsgenauigkeit bei 99,70\%, 97,46\% bzw. 95,25\%. Die Ergebnisse bestätigen erneut, dass Multi-Task-Learning auch bei Daten mit unterschiedlichen Genres zu erheblichen Verbesserungen führt.
Jüngste Arbeiten haben gezeigt, dass neuronale Netze anfällig für ungünstige Beispiele sind, d. h., Um dieses Problem anzugehen, untersuchen wir die Widerstandsfähigkeit neuronaler Netze durch die Linse der robusten Optimierung. Dieser Ansatz bietet uns eine breite und vereinheitlichende Sicht auf viele frühere Arbeiten zu diesem Thema. Seine prinzipielle Natur ermöglicht es uns auch, Methoden für das Training und den Angriff auf neuronale Netze zu identifizieren, die zuverlässig und in gewissem Sinne universell sind. Diese Methoden ermöglichen es uns, Netze zu trainieren, die eine deutlich verbesserte Resistenz gegen eine Vielzahl von Angriffen aufweisen, und sie schlagen Robustheit gegen einen Gegner erster Ordnung als natürliche Sicherheitsgarantie vor.Wir glauben, dass Robustheit gegen solche wohldefinierten Klassen von Angreifern ein wichtiger Schritt auf dem Weg zu vollständig resistenten Deep Learning Modellen ist.
In den letzten Jahren gab es einen rasanten Anstieg der Klassifizierungsmethoden für graphenstrukturierte Daten, und sowohl bei Graphenkernen als auch bei neuronalen Netzen war eine der impliziten Annahmen erfolgreicher State-of-the-Art-Modelle, dass die Einbeziehung von Graphenisomorphismusmerkmalen in die Architektur zu einer besseren empirischen Leistung führt. Dies verhindert einen fairen Wettbewerb zwischen den Algorithmen und wirft die Frage nach der Gültigkeit der erzielten Ergebnisse auf.54 Datensätze, die zuvor häufig für Graphenaufgaben verwendet wurden, werden auf das Vorhandensein von Isomorphismusverzerrungen hin analysiert, und es werden Empfehlungen für Praktiker im Bereich des maschinellen Lernens gegeben, um ihre Modelle richtig einzurichten.
Nachahmungslernen, gefolgt von Algorithmen des Verstärkungslernens, ist ein vielversprechendes Paradigma, um komplexe Steuerungsaufgaben stichprobenartig effizient zu lösen. Allerdings leidet das Lernen aus Demonstrationen oft unter dem Problem der Kovariatenverschiebung, was zu kaskadierenden Fehlern der gelernten Strategie führt. Wir entwerfen einen Algorithmus Value Iteration with Negative Sampling (VINS), der praktisch solche Wertfunktionen mit konservativer Extrapolation lernt.Wir zeigen, dass VINS Fehler der Behavioral-Cloning-Politik auf simulierten Robotik-Benchmark-Aufgaben korrigieren kann.Wir schlagen auch den Algorithmus der Verwendung von VINS vor, um einen Reinforcement-Learning-Algorithmus zu initialisieren, von dem gezeigt wird, dass er frühere Arbeiten in Bezug auf die Stichprobeneffizienz übertrifft.
Ein strukturiertes Verständnis unserer Welt in Form von Objekten, Beziehungen und Hierarchien ist eine wichtige Komponente der menschlichen Kognition.Lernen wie eine strukturierte Welt Modell von rohen sensorischen Daten bleibt eine Herausforderung.Als ein Schritt in Richtung dieses Ziels, führen wir Contrastively-trained Structured World Models (C-SWMs).C-SWMs nutzen einen kontrastiven Ansatz für die Darstellung Lernen in Umgebungen mit kompositorischen Struktur.Wir strukturieren jeden Zustand Einbettung als eine Reihe von Objekt-Darstellungen und deren Beziehungen, modelliert durch einen Graphen neuronales Netz. Wir evaluieren C-SWMs in kompositorischen Umgebungen mit mehreren interagierenden Objekten, die unabhängig von einem Agenten manipuliert werden können, in einfachen Atari-Spielen und in einer physikalischen Simulation mit mehreren Objekten. Unsere Experimente zeigen, dass C-SWMs die Einschränkungen von Modellen, die auf Pixelrekonstruktion basieren, überwinden und typische Vertreter dieser Modellklasse in stark strukturierten Umgebungen übertreffen können, während sie interpretierbare objektbasierte Repräsentationen lernen.
Neuronale maschinelle Übersetzungsmodelle (NMT) lernen Repräsentationen, die umfangreiche linguistische Informationen enthalten, wobei nicht klar ist, ob diese Informationen vollständig verteilt sind oder ob ein Teil davon einzelnen Neuronen zugeordnet werden kann.Wir entwickeln unbeaufsichtigte Methoden zur Entdeckung wichtiger Neuronen in NMT-Modellen. Wir zeigen experimentell, dass die Übersetzungsqualität von den entdeckten Neuronen abhängt, und stellen fest, dass viele von ihnen häufige linguistische Phänomene erfassen.Schließlich zeigen wir, wie man NMT-Übersetzungen auf vorhersehbare Weise kontrollieren kann, indem man die Aktivierungen einzelner Neuronen modifiziert.
Berechnungen für die Softmax-Funktion in neuronalen Netzmodellen sind teuer, wenn die Anzahl der Ausgangsklassen groß ist, was sowohl beim Training als auch bei der Inferenz für solche Modelle zu einem erheblichen Problem werden kann.In diesem Papier stellen wir Doubly Sparse Softmax (DS-Softmax), Sparse Mixture of Sparse of Sparse Experts, vor, um die Effizienz der Softmax-Inferenz zu verbessern.Während des Trainings lernt unsere Methode eine zweistufige Klassenhierarchie, indem sie den gesamten Ausgangsklassenraum in mehrere sich teilweise überlappende Experten unterteilt. Während des Trainings lernt unsere Methode eine zweistufige Klassenhierarchie, indem sie den gesamten Klassenraum in mehrere, sich teilweise überschneidende Experten unterteilt. Jeder Experte ist für eine gelernte Teilmenge des Klassenraums verantwortlich und jede Klasse gehört nur zu einer kleinen Anzahl dieser Experten.Während der Inferenz findet unsere Methode schnell den wahrscheinlichsten Experten, um Softmax auf kleiner Ebene zu berechnen.
Unsere Arbeit liefert empirische Beweise dafür, dass die Schichtrotation, d.h. die Entwicklung des Kosinusabstandes zwischen dem Gewichtsvektor jeder Schicht und ihrer Initialisierung während des Trainings, ein beeindruckend konsistenter Indikator für die Generalisierungsleistung ist. Im Vergleich zu früher untersuchten Generalisierungsindikatoren zeigen wir, dass die Schichtrotation den zusätzlichen Vorteil hat, dass sie leicht überwacht und kontrolliert werden kann und ein netzwerkunabhängiges Optimum aufweist: Die Trainingsverfahren, bei denen die Gewichte aller Schichten einen Kosinusabstand von 1 zu ihrer Initialisierung erreichen, sind anderen Konfigurationen durchweg überlegen - mit einer Testgenauigkeit von bis zu 20 %.Schließlich deuten unsere Ergebnisse auch darauf hin, dass die Untersuchung der Schichtrotation einen einheitlichen Rahmen zur Erklärung der Auswirkungen von Gewichtsabnahme und adaptiven Gradientenmethoden auf die Generalisierung bieten kann.
Modelle von Code können verteilte Repräsentationen der Syntax und Semantik eines Programms erlernen, um viele nicht-triviale Eigenschaften eines Programms vorherzusagen.Aktuelle State-of-the-Art-Modelle nutzen hochstrukturierte Repräsentationen von Programmen, wie Bäume, Graphen und Pfade darin (z.B. Datenflussrelationen), die präzise und reichlich für Code verfügbar sind.Dies bietet eine starke induktive Ausrichtung auf semantisch sinnvolle Relationen, was zu verallgemeinerbaren Repräsentationen führt als klassische sequenzbasierte Modelle. Leider verlassen sich diese Modelle in erster Linie auf Graphen-basiertes Message-Passing, um Beziehungen im Code darzustellen, was sie aufgrund der hohen Kosten der Message-Passing-Schritte de facto lokal macht, ganz im Gegensatz zu modernen, globalen sequenzbasierten Modellen wie dem Transformer.In dieser Arbeit überbrücken wir diese Kluft zwischen globalen und strukturierten Modellen, indem wir zwei neue hybride Modellfamilien einführen, die sowohl global sind als auch eine strukturelle Ausrichtung aufweisen: Graph Sandwiches, die traditionelle (gated) Graph Message-Passing-Schichten in sequenzielle Message-Passing-Schichten einhüllen; und Graph Relational Embedding Attention Transformers (kurz GREAT), die traditionelle Transformers mit relationalen Informationen aus Graphkanten-Typen verzahnen. Ausgehend von einem graphenbasierten Modell, das den bisherigen Stand der Technik für diese Aufgabe bereits um 20 % verbessert, zeigen wir, dass die von uns vorgeschlagenen hybriden Modelle eine zusätzliche Verbesserung von 10-15 % bewirken, während sie gleichzeitig schneller und mit weniger Parametern trainieren.
Rekurrente neuronale Netze (RNNs) eignen sich besonders gut für die Modellierung langfristiger Abhängigkeiten in sequentiellen Daten, sind aber bekanntermaßen schwer zu trainieren, da der zeitlich rückproportionierte Fehler entweder verschwindet oder mit exponentieller Rate explodiert. Während eine Reihe von Arbeiten versuchen, diesen Effekt durch Gated Recurrent Units, Skip-Connections, parametrische Einschränkungen und Design-Entscheidungen abzuschwächen, schlagen wir ein neuartiges inkrementelles RNN (iRNN) vor, bei dem die versteckten Zustandsvektoren inkrementelle Änderungen verfolgen und so die Zustandsvektor-Inkremente von Rosenblatts (1962) zeitkontinuierlichen RNNs approximieren. Wir zeigen, dass unsere Methode rechnerisch effizient ist und den Overhead vieler bestehender Methoden überwindet, die versuchen, das RNN-Training zu verbessern, während sie keine Leistungseinbußen erleidet. Wir demonstrieren den Nutzen unseres Ansatzes mit umfangreichen Experimenten und zeigen eine konkurrenzfähige Leistung gegenüber Standard-LSTMs bei LTD- und anderen Nicht-LTD-Aufgaben.
Jüngste empirische Ergebnisse zu überparametrisierten tiefen Netzen zeichnen sich durch ein auffälliges Fehlen der klassischen U-förmigen Testfehlerkurve aus: Der Testfehler nimmt in breiteren Netzen immer weiter ab.Forscher arbeiten aktiv an der Überbrückung dieser Diskrepanz, indem sie bessere Komplexitätsmaße vorschlagen.Stattdessen messen wir direkt die Vorhersageverzerrung und -varianz für vier Klassifizierungs- und Regressionsaufgaben auf modernen tiefen Netzen.Wir stellen fest, dass sowohl die Verzerrung als auch die Varianz mit zunehmender Anzahl von Parametern abnehmen können. Um die Rolle der Optimierung besser zu verstehen, zerlegen wir die Gesamtvarianz in die Varianz aufgrund des Samplings der Trainingsmenge und die Varianz aufgrund der Initialisierung.Die Varianz aufgrund der Initialisierung ist im Regime der Unterparametrisierung signifikant.Im Regime der Überparametrisierung ist die Gesamtvarianz viel geringer und wird von der Varianz aufgrund des Samplings dominiert.Wir bieten eine theoretische Analyse in einer vereinfachten Umgebung, die mit unseren empirischen Ergebnissen übereinstimmt.
In diesem Papier schlagen wir einen datenschutzfreundlichen Deep-Learning-Rahmen mit einem lernfähigen Obfuscator für die Bildklassifizierung vor. Unser Rahmen besteht aus drei Modulen: lernfähiger Obfuscator, Klassifikator und Rekonstruktor. Der lernfähige Obfuscator wird verwendet, um die sensiblen Informationen in den Bildern zu entfernen und die Merkmalskarten aus ihnen zu extrahieren. Der Rekonstrukteur spielt die Rolle eines Angreifers, der versucht, das Bild aus den vom Obfuscator extrahierten Feature-Maps wiederherzustellen. Um die Privatsphäre der Benutzer in Bildern bestmöglich zu schützen, entwickeln wir eine gegnerische Trainingsmethodik für unser Framework, um den Obfuscator zu optimieren.
Bitcoin ist ein virtuelles MÃ?nzsystem, das es den Nutzern ermÃ¶glicht, praktisch frei von einer zentralen, vertrauenswÃ?rdigen AutoritÃ?t zu handeln.â€œ Alle Transaktionen auf der Bitcoin-Blockchain sind Ã¶ffentlich zugÃ?nglich, doch da Bitcoin hauptsÃ?chlich aus SicherheitsgrÃ?nden entwickelt wurde, ermÃ¶glicht seine ursprÃ?ngliche Struktur keine direkte Analyse von Adresstransaktionen. Wir schlagen ein rechnerisch effizientes Modell vor, um Bitcoin-Blockchain-Adressen zu analysieren und ihre Verwendung mit bestehenden Algorithmen des maschinellen Lernens zu ermöglichen. Wir vergleichen unseren Ansatz mit Multi Level Sequence Learners (MLSLs), einem der leistungsfähigsten Modelle für Bitcoin-Adressdaten.
Trotz bemerkenswerter empirischer Erfolge ist die Trainingsdynamik von generativen adversen Netzwerken (GAN), die die Lösung eines Minimax-Spiels unter Verwendung stochastischer Gradienten beinhaltet, immer noch schlecht verstanden.In dieser Arbeit analysieren wir die Konvergenz der letzten Stufe des simultanen Gradientenabstiegs (simGD) und seiner Varianten unter der Annahme der konvexen Konkavität, geleitet von einer zeitkontinuierlichen Analyse mit Differentialgleichungen. Erstens zeigen wir, dass simGD, so wie es ist, mit stochastischen Subgradienten unter strenger Konvexität in der primären Variable konvergiert.Zweitens verallgemeinern wir optimistisches simGD, um eine von der Lernrate getrennte Optimismusrate unterzubringen und zeigen seine Konvergenz mit vollen Gradienten.Schließlich stellen wir verankertes simGD vor, eine neue Methode, und zeigen Konvergenz mit stochastischen Subgradienten.
Kleine Raumfahrzeuge verfügen heute über präzise Lageregelungssysteme, die es ihnen ermöglichen, in drei Freiheitsgraden zu schwenken und innerhalb kurzer Zeit Bilder aufzunehmen. In Kombination mit geeigneter Software kann diese Agilität die Reaktionsgeschwindigkeit, die Wiederholungszeit und die Abdeckung erheblich steigern. Der Algorithmus ist verallgemeinerbar für kleine, steuerbare Raumfahrzeuge, Steuerungskapazitäten, Sensorspezifikationen, Bildgebungsanforderungen und Regionen von Interesse. In diesem Artikel modifizieren wir den Algorithmus so, dass er an Bord von kleinen Raumfahrzeugen ausgeführt werden kann, so dass die Konstellation zeitabhängige Entscheidungen zum Schwenken und zur Aufnahme von Bildern autonom, ohne Bodenkontrolle, treffen kann.Wir haben ein Kommunikationsmodul auf der Grundlage von Delay/Disruption Tolerant Networking (DTN) für das Datenmanagement an Bord und das Routing zwischen den Satelliten entwickelt, das in Verbindung mit den anderen Modulen den Zeitplan für die agile Kommunikation und Steuerung optimiert. Anschließend wenden wir diesen vorläufigen Rahmen auf repräsentative Konstellationen an, um gezielte Messungen von episodischen Niederschlagsereignissen und anschließenden städtischen Überschwemmungen zu simulieren. Die Befehls- und Steuerungseffizienz unseres agilen Algorithmus wird mit nicht-agilen (11,3-fache Verbesserung) und nicht-DTN (21% Verbesserung) Konstellationen verglichen.
Importance Sampling (IS) ist ein Standard-Monte-Carlo-Werkzeug (MC) zur Berechnung von Informationen über Zufallsvariablen wie Momente oder Quantile mit unbekannten Verteilungen.  IS ist asymptotisch konsistent, wenn die Anzahl der MC-Stichproben und damit der Deltas (Partikel), die die Dichteschätzung parametrisieren, gegen unendlich geht, aber die Beibehaltung unendlich vieler Partikel ist schwierig. Um dies auf {online Art und Weise} zu erreichen, approximieren wir das Wichtigkeits-Sampling auf zwei Arten.  Erstens ersetzen wir die Deltas durch Kernel, was zu Kernel-Dichte-Schätzungen (KDEs) führt.  Wir charakterisieren die asymptotische Verzerrung dieses Schemas, die durch einen Kompressionsparameter und eine Kernelbandbreite bestimmt wird, was zu einem abstimmbaren Kompromiss zwischen Konsistenz und Speicher führt.In Experimenten beobachten wir einen vorteilhaften Kompromiss zwischen Speicher und Genauigkeit, der zum ersten Mal nahezu konsistente Kompressionen beliebiger Posteriorverteilungen ermöglicht.
Wir untersuchen die folgenden drei grundlegenden Probleme der Ridge-Regression: (1) Wie sieht die Struktur des Schätzers aus?(2) Wie kann man die Kreuzvalidierung korrekt nutzen, um den Regularisierungsparameter zu wählen? und (3) Wie kann man die Berechnung beschleunigen, ohne zu viel Genauigkeit zu verlieren? Wir betrachten die drei Probleme in einem einheitlichen linearen Modell mit großen Daten und geben eine präzise Darstellung der Ridge-Regression als kovarianzmatrixabhängige Linearkombination des wahren Parameters und des Rauschens. Wir untersuchen die Verzerrung der $K$-fachen Kreuzvalidierung für die Wahl des Regularisierungsparameters und schlagen eine einfache Verzerrungskorrektur vor.Wir analysieren die Genauigkeit der primären und dualen Skizze für die Ridge-Regression und zeigen, dass sie überraschend genau sind.Unsere Ergebnisse werden durch Simulationen und durch die Analyse empirischer Daten illustriert.
Trotz signifikanter empirischer Gewinne gibt es einen Mangel an theoretischen Analysen, um ihre Effektivität zu verstehen.In diesem Papier gehen wir dieses Problem an, indem wir die Landschaft der Population und der empirischen Verlustfunktionen von aufmerksamkeitsbasierten neuronalen Netzen untersuchen.Unsere Ergebnisse zeigen, dass unter milden Annahmen jedes lokale Minimum eines zweischichtigen globalen Aufmerksamkeitsmodells einen niedrigen Vorhersagefehler hat und Aufmerksamkeitsmodelle eine geringere Probenkomplexität benötigen als Modelle, die keine Aufmerksamkeit verwenden. Wir erweitern dann unsere Analysen auf das populäre Selbstaufmerksamkeitsmodell und beweisen, dass sie konsistente Vorhersagen mit einer aussagekräftigeren Klasse von Funktionen liefern.Darüber hinaus bieten unsere theoretischen Ergebnisse mehrere Richtlinien für die Gestaltung von Aufmerksamkeitsmechanismen.Unsere Erkenntnisse werden mit zufriedenstellenden experimentellen Ergebnissen auf MNIST und IMDB Bewertungen dataset validiert.
Jüngste Fortschritte in der tiefen Lerntechniken hat die Nützlichkeit der tiefen neuronalen Netzen bei der Extraktion von Merkmalen erforderlich, um die Aufgabe bei hand.However, diese Funktionen gelernt sind insbesondere nur für die erste task.This ist aufgrund der Tatsache, dass die Funktionen gelernt sind sehr aufgabenspezifische und nicht erfassen die meisten allgemeinen und Aufgabe agnostic Features der input.In der Tat die Art und Weise Menschen gesehen werden, um zu lernen, ist durch Entflechtung Features, die Aufgabe agnostic. Da diese latenten Merkmale als kontinuierliche und/oder diskrete Variablen dargestellt werden können, deutet dies darauf hin, dass wir VAE mit einer Mischung aus kontinuierlichen und diskreten Variablen für den latenten Raum verwenden, indem wir unsere Experimente mit einer modifizierten Version von Joint-Vae durchführen, um die entwirrten Merkmale zu lernen.
Um die Funktionsweise neuronaler Netze zu verbessern, ist es von entscheidender Bedeutung, ihren Lernprozess zu verstehen: Die Theorie des Informationsengpasses beim Deep Learning besagt, dass neuronale Netze eine gute Generalisierung erreichen, indem sie ihre Darstellungen komprimieren und Informationen, die für die Aufgabe nicht relevant sind, außer Acht lassen. Im Gegensatz dazu erreichten Netzwerke mit nicht-sättigenden Aktivierungsfunktionen ein vergleichbares Leistungsniveau, zeigten aber keine Kompression. In dieser Arbeit haben wir robustere Verfahren zur Schätzung der gegenseitigen Information entwickelt, die sich an die verborgene Aktivität neuronaler Netzwerke anpassen und empfindlichere Messungen der Aktivierungen aller Funktionen, insbesondere unbeschränkter Funktionen, liefern. Mit zwei verbesserten Methoden der Schätzung, erstens, zeigen wir, dass die Sättigung der Aktivierungsfunktion nicht für die Kompression erforderlich ist, und die Höhe der Kompression variiert zwischen verschiedenen Aktivierungsfunktionen.Wir finden auch, dass es eine große Menge an Variation in der Kompression zwischen verschiedenen Netzwerk-Initialisierungen.Zweitens sehen wir, dass L2-Regularisierung führt zu einer deutlich erhöhten Kompression, während die Verhinderung overfitting.Schließlich zeigen wir, dass nur die Kompression der letzten Schicht ist positiv korreliert mit Generalisierung.
In dieser Arbeit befassen wir uns mit dem Problem des musikalischen Timbre-Transfers, bei dem das Ziel darin besteht, die Klangfarbe eines Instrumentes so zu verändern, dass sie zu einem anderen Instrument passt, wobei andere musikalische Inhalte wie Tonhöhe, Rhythmus und Lautstärke erhalten bleiben.Im Prinzip könnte man bildbasierte Stiltransfertechniken auf eine Zeit-Frequenz-Darstellung eines Audiosignals anwenden, aber dies hängt davon ab, dass man eine Darstellung hat, die eine unabhängige Manipulation der Klangfarbe sowie eine qualitativ hochwertige Wellenformerzeugung ermöglicht. Wir stellen TimbreTron vor, eine Methode fÃ?r die musikalische KlangfarbenÃ?bertragung, die die StilÃ?bertragung im â€žBildbereichâ€œ auf eine Zeit-Frequenz-Darstellung des Audiosignals anwendet und anschlieÃŸend eine hochwertige Wellenform unter Verwendung eines bedingten WaveNet-Synthesizers erzeugt. Wir zeigen, dass die Constant Q Transform (CQT) ReprÃ?sentation aufgrund ihrer annÃ?hernden TonhÃ¶henÃ?quivarianz besonders gut fÃ?r Faltungsarchitekturen geeignet ist.â€œ Basierend auf menschlichen Wahrnehmungsbewertungen bestÃ?tigten wir, dass TimbreTron die Klangfarbe erkennbar Ã?bertrÃ?gt, wÃ?hrend ansonsten der musikalische Inhalt erhalten bleibt, sowohl fÃ?r monophone als auch fÃ?r polyphone Samples.â€œ Wir haben ein begleitendes Demovideo erstellt: https://www.cs.toronto.edu/~huang/TimbreTron/index.html, das Sie sich unbedingt ansehen sollten, bevor Sie den Artikel lesen.
Neuromorphe Hardware neigt dazu, die Konnektivität von tiefen Netzwerken zu begrenzen, aber auch generische Hardware- und Software-Implementierungen des tiefen Lernens laufen effizienter für spärliche Netzwerke.Es gibt mehrere Methoden, um Verbindungen eines neuronalen Netzwerks zu beschneiden, nachdem es ohne Konnektivitätsbeschränkungen trainiert wurde.Wir stellen einen Algorithmus, DEEP R, vor, der es uns ermöglicht, direkt ein spärlich verbundenes neuronales Netzwerk zu trainieren. Wir zeigen, dass DEEP R verwendet werden kann, um sehr spärliche Feedforward- und rekurrente neuronale Netze auf Standard-Benchmark-Aufgaben mit nur einem geringen Leistungsverlust zu trainieren.DEEP R basiert auf einer strengen theoretischen Grundlage, die die Neuverdrahtung als stochastisches Sampling von Netzwerkkonfigurationen aus einem Posterior betrachtet.
Der Erfolg von Deep Learning hat zu immer größeren Modellen geführt, um immer komplexere Aufgaben zu bewältigen; trainierte Modelle können Millionen von Parametern enthalten. Diese großen Modelle sind rechen- und speicherintensiv, was es zu einer Herausforderung macht, sie mit minimalen Latenz-, Durchsatz- und Speicheranforderungen einzusetzen.Einige Modellkompressionsmethoden wurden erfolgreich auf Bildklassifizierungs- und -erkennungs- oder Sprachmodelle angewandt, aber es gab nur sehr wenige Arbeiten zur Kompression von generativen adversen Netzwerken (GANs), die komplexe Aufgaben erfüllen. In diesem Papier zeigen wir, dass ein Standardmodell Kompressionstechnik, Gewicht Pruning, kann nicht auf GANs mit bestehenden Methoden angewandt werden.Wir entwickeln dann eine selbstüberwachte Kompressionstechnik, die den trainierten Diskriminator verwendet, um die Ausbildung eines komprimierten generator.We zeigen, dass dieser Rahmen hat eine überzeugende Leistung zu hohen Grad der Sparsamkeit, verallgemeinert gut auf neue Aufgaben und Modelle, und ermöglicht aussagekräftige Vergleiche zwischen verschiedenen Granularität Pruning.
Verteiltes Training in großem Maßstab erfordert eine beträchtliche Kommunikationsbandbreite für den Gradientenaustausch, was die Skalierbarkeit von Multi-Node-Training einschränkt und eine teure Netzwerkinfrastruktur mit hoher Bandbreite erfordert. In diesem Beitrag stellen wir fest, dass 99,9 % des Gradientenaustauschs bei verteiltem SGD redundant ist, und schlagen eine tiefe Gradientenkompression (DGC) vor, um die Kommunikationsbandbreite erheblich zu reduzieren.Um die Genauigkeit während der Kompression zu erhalten, verwendet DGC vier Methoden: Momentum-Korrektur, lokales Gradienten-Clipping, Maskierung des Momentum-Faktors und Aufwärmtraining. Wir haben Deep Gradient Compression auf Bildklassifikation, Spracherkennung und Sprachmodellierung mit mehreren Datensätzen angewandt, darunter Cifar10, ImageNet, Penn Treebank und Librispeech Corpus. in diesen Szenarien erreicht Deep Gradient Compression ein Gradientenkompressionsverhältnis von 270x bis 600x ohne Genauigkeitsverlust, wobei die Gradientengröße von ResNet-50 von 97MB auf 0. Deep Gradient Compression ermöglicht verteiltes Training in großem Maßstab auf kostengünstigem 1Gbps Ethernet und erleichtert verteiltes Training auf mobilen Geräten.
Die meisten Arbeiten konzentrieren sich darauf, entweder eine Eins-zu-Eins-Zuordnung auf unüberwachte Weise oder eine Viele-zu-Viele-Zuordnung auf überwachte Weise zu erlernen. Eine praktischere Einstellung ist jedoch die Viele-zu-Viele-Zuordnung auf unüberwachte Weise, die aufgrund der fehlenden Überwachung und der komplexen inner- und bereichsübergreifenden Variationen schwieriger ist. Um diese Probleme zu lösen, schlagen wir das Exemplar Guided & Semantically Consistent Image-to-Image Translation (EGSC-IT) Netzwerk vor, das den Übersetzungsprozess an ein Beispielbild in der Zieldomäne koppelt. Unter der Anleitung eines Beispiels aus der Zieldomäne wenden wir die adaptive Instanznormalisierung auf die gemeinsame Inhaltskomponente an, wodurch wir die Stilinformationen der Zieldomäne auf die Quelldomäne übertragen können. Um semantische Inkonsistenzen während der Übersetzung zu vermeiden, die aufgrund der großen inner- und überdomänenlichen Variationen auftreten, führen wir das Konzept der Merkmalsmasken ein, die eine grobe semantische Orientierung bieten, ohne dass semantische Etiketten verwendet werden müssen.Experimentelle Ergebnisse auf verschiedenen Datensätzen zeigen, dass EGSC-IT nicht nur das Quellbild in verschiedene Instanzen der Zieldomäne übersetzt, sondern auch die semantische Konsistenz während des Prozesses bewahrt.
Tiefe neuronale Netze können aussagekräftige Repräsentationen von Daten erlernen, die jedoch schwer zu interpretieren sind. Zum Beispiel ist die Visualisierung einer latenten Schicht in der Regel nur für maximal drei Dimensionen möglich.Neuronale Netze sind in der Lage, viel höher dimensionale Repräsentationen zu erlernen und zu nutzen, die jedoch nicht visuell interpretierbar sind, da die Knoten innerhalb einer Schicht beliebig angeordnet sind. Hier nutzen wir die Fähigkeit des menschlichen Beobachters, Muster in strukturierten Darstellungen zu erkennen, um höhere Dimensionen zu visualisieren. Dazu schlagen wir eine Klasse von Regularisierungen vor, die wir \textit{Graph Spectral Regularizations} nennen und die latenten Schichten eine Graphenstruktur aufzwingen. Dies wird erreicht, indem Aktivierungen als Signale auf einem vordefinierten Graphen behandelt werden und diese Aktivierungen mit Graphenfiltern wie Tiefpass- und Wavelet-ähnlichen Filtern eingeschränkt werden. Zunächst zeigen wir anhand eines synthetischen Beispiels, dass die graph-strukturierte Schicht topologische Merkmale der Daten aufdecken kann, und dann, dass eine glättende Regularisierung eine semantisch konsistente Ordnung der Knoten erzwingen kann, wenn sie auf Kapselnetze angewendet wird. Des Weiteren zeigen wir, dass die graph-strukturierte Schicht unter Verwendung von wavelet-ähnlichen, räumlich lokalisierten Filtern lokalisierte rezeptive Felder für eine verbesserte Interpretation von Bildern und biomedizinischen Daten bilden kann, d.h. die Zuordnung zwischen latenter Schicht, Neuronen und dem Ausgaberaum wird durch die Lokalisierung der Aktivierungen deutlich.
Der vorherrschende parametrische Ansatz basiert auf lokal normalisierten Modellen, die ein Wort nach dem anderen vorhersagen. Obwohl diese bemerkenswert gut funktionieren, sind sie aufgrund der gierigen Natur des Generierungsprozesses mit einer Verzerrung durch die Exposition behaftet. In dieser Arbeit untersuchen wir nicht-normalisierte energiebasierte Modelle (EBMs), die nicht auf Token-, sondern auf Sequenzebene arbeiten. Da das EBM auf Sequenzebene arbeitet, können wir außerdem vortrainierte bidirektionale kontextuelle Repräsentationen wie BERT und RoBERTa nutzen. Unsere Experimente an zwei großen Sprachmodellierungsdatensätzen zeigen, dass residuale EBMs im Vergleich zu lokal normalisierten Basismodellen eine geringere Perplexität aufweisen und dass die Generierung mittels Wichtigkeitsabtastung sehr effizient und qualitativ hochwertiger ist als die Basismodelle.
Wir untersuchen die Robustheit von Bilderkennungsmodellen, die mit zwei vom menschlichen Sehen inspirierten Merkmalen ausgestattet sind, einem expliziten episodischen Gedächtnis und einer Formverzerrung, auf der ImageNet-Skala: Wie in früheren Arbeiten berichtet, zeigen wir, dass ein explizites episodisches Gedächtnis die Robustheit von Bilderkennungsmodellen gegenüber Störungen durch kleine Normen bei einigen Bedrohungsmodellen verbessert. Das Erlernen von robusteren Merkmalen während des Trainings scheint für die Robustheit in diesem zweiten Sinne notwendig zu sein.Wir zeigen, dass Merkmale, die von einem Modell abgeleitet wurden, das zum Erlernen globaler, formbezogener Repräsentationen angeregt wurde (Geirhos et al, Wir zeigen, dass Merkmale, die aus einem Modell stammen, das zum Erlernen globaler, formorientierter Repräsentationen angeregt wurde (Geirhos et al., 2019), nicht nur die Robustheit gegenüber natürlichen Störungen verbessern, sondern in Verbindung mit einem episodischen Gedächtnis auch zusätzliche Robustheit gegenüber gegnerischen Störungen bieten.Schließlich befassen wir uns mit drei wichtigen Design-Entscheidungen für das episodische Gedächtnis: Speichergröße, Dimensionalität der Erinnerungen und die Abrufmethode.Wir zeigen, dass es für ein kompakteres episodisches Gedächtnis besser ist, die Anzahl der Erinnerungen durch Clustering zu reduzieren, anstatt ihre Dimensionalität zu verringern.
Zentral für den Erfolg von G-CNNs ist die Aufhebung von Merkmalskarten auf höherdimensionale, entwirrte Repräsentationen, in denen Datencharakteristika effektiv erlernt werden, geometrische Datenerweiterungen obsolet werden und vorhersagbares Verhalten unter geometrischen Transformationen (Äquivarianz) über die Gruppentheorie garantiert wird. Derzeit sind die praktischen Implementierungen von G-CNNs jedoch entweder auf diskrete Gruppen (die das Gitter intakt lassen) oder auf kontinuierliche, kompakte Gruppen wie Rotationen (die die Verwendung der Fourier-Theorie ermöglichen) beschränkt.In diesem Papier heben wir diese Einschränkungen auf und schlagen einen modularen Rahmen für den Entwurf und die Implementierung von G-CNNs für beliebige Lie-Gruppen vor. In unserem Ansatz wird die differentielle Struktur von Lie-Gruppen genutzt, um Faltungskerne in einer generischen Basis von B-Splines zu expandieren, die auf der Lie-Algebra definiert ist. Dies führt zu einem flexiblen Rahmen, der lokalisierte, unregelmäßige und deformierbare Faltungen in G-CNNs mittels lokalisierter, spärlicher und uneinheitlicher B-Spline-Expansionen ermöglicht. Die Auswirkungen und das Potenzial unseres Ansatzes werden an zwei Benchmark-Datensätzen untersucht: Krebserkennung in histopathologischen Präparaten (PCam-Datensatz), bei der Rotationsäquivarianz eine Schlüsselrolle spielt, und Lokalisierung von Gesichtsmerkmalen (CelebA-Datensatz), bei der Skalenäquivarianz wichtig ist. In beiden Fällen übertreffen G-CNN-Architekturen ihre klassischen 2D-Gegenstücke, und der Mehrwert von atroiden und lokalisierten Gruppenfaltungen wird im Detail untersucht.
 Obwohl es alternative Pooling-Methoden gibt (z.B. max, lp norm, stochastisch), ist die Mittelwertbildung immer noch das dominierende globale Pooling-Schema in populären Modellen.Da feinkörnige Erkennung das Erlernen von subtilen, diskriminierenden Merkmalen erfordert, betrachten wir die Frage: Ist durchschnittliches Pooling die optimale Strategie?Wir fragen zuerst: ``Gibt es einen Unterschied zwischen Merkmalen, die durch globales durchschnittliches und maximales Pooling gelernt werden?'' Visualisierung und quantitative Analyse zeigen, dass Max-Pooling das Erlernen von Merkmalen verschiedener räumlicher Skalen fördert, und wir fragen dann: "Gibt es eine einzige globale Pooling-Variante, die sich am besten für feinkörnige Erkennung eignet? Eine gründliche Auswertung von neun repräsentativen Pooling-Algorithmen zeigt, dass das Max-Pooling das Average-Pooling über alle Modelle, Datensätze und Bildauflösungen hinweg übertrifft, und zwar durch eine Verringerung der Generalisierungslücke, und dass die Leistung des generalisierten Poolings fast monoton ansteigt, wenn es von Average zu Max wechselt.Abschließend stellen wir die Frage, wie zwei heterogene Pooling-Verfahren am besten kombiniert werden können. Wir finden, dass die post-globale Batch-Normalisierung zu einer schnelleren Konvergenz beiträgt und die Leistung des Modells beständig verbessert.
Obwohl die jüngste Forschung hat gezeigt, Vorteile der selbst-überwachten Lernens (SSL) auf große unbeschriftete Datensätze, seinen Nutzen auf kleine Datensätze ist unbekannt.Wir finden, dass SSL reduziert die relative Fehlerquote von wenigen Schuss Meta-Lernern von 4%-27%, auch wenn die Datensätze sind klein und nur die Verwendung von Bildern innerhalb der Datensätze.Die Verbesserungen sind größer, wenn die Ausbildung Satz ist kleiner oder die Aufgabe ist anspruchsvoller. Basierend auf dieser Analyse stellen wir eine Technik vor, die automatisch Bilder für SSL aus einem großen, generischen Pool von unbeschrifteten Bildern für einen gegebenen Datensatz auswählt und dabei einen Domänen-Klassifikator verwendet, der weitere Verbesserungen liefert.Wir präsentieren Ergebnisse unter Verwendung mehrerer Meta-Learner und selbstüberwachter Aufgaben über Datensätze mit unterschiedlichen Graden von Domänenverschiebungen und Etikettengrößen, um die Effektivität von SSL für das Lernen mit wenigen Bildern zu charakterisieren.
Die Abstraktion von Markov-Entscheidungsprozessen ist ein nützliches Werkzeug zur Lösung komplexer Probleme, da sie unwichtige Aspekte einer Umgebung ignorieren kann, was den Prozess des Lernens einer optimalen Strategie vereinfacht. Wir demonstrieren die Fähigkeit unseres Algorithmus, Abstraktionen aus gesammelten Erfahrungen zu lernen und zeigen, wie die Abstraktionen wiederverwendet werden können, um die Erkundung neuer Aufgaben, denen der Agent begegnet, zu leiten.Unsere neuartige Aufgabentransfermethode schlägt eine auf einem tiefen Q-Netz basierende Grundlinie.
Eine Reihe neuerer Methoden zum Verständnis neuronaler Netze haben sich auf die Quantifizierung der Rolle einzelner Merkmale konzentriert.  Eine solche Methode, NetDissect, identifiziert interpretierbare Merkmale eines Modells unter Verwendung des Broden-Datensatzes visueller semantischer Bezeichnungen (Farben, Materialien, Texturen, Objekte und Szenen).  Da in letzter Zeit eine Reihe von Datensätzen zur Handlungserkennung entstanden sind, schlagen wir vor, den Broden-Datensatz um Handlungen zu erweitern, um gelernte Handlungsmodelle besser analysieren zu können.  Wir beschreiben den Annotationsprozess, die Ergebnisse der Interpretation von Handlungserkennungsmodellen auf dem erweiterten Broden-Datensatz und untersuchen interpretierbare Merkmalspfade, die uns helfen, die konzeptionelle Hierarchie zu verstehen, die zur Klassifizierung einer Handlung verwendet wird.
Die automatische Generierung von Melodien für Popmusik ist ein langjähriges Bestreben von KI-Forschern und Musikern.Allerdings hat sich das Erlernen der Generierung wohlklingender Melodien aufgrund einer Reihe von Faktoren als äußerst schwierig erwiesen.Die Darstellung der multivariaten Eigenschaften von Noten ist eine der größten Herausforderungen.Es ist auch schwierig, im zulässigen Spektrum der musikalischen Vielfalt zu bleiben, außerhalb dessen es als reines Zufallsspiel ohne Hörgenuss wahrgenommen werden würde.Die Beachtung der konventionellen Struktur von Popmusik stellt eine weitere Herausforderung dar. In dieser Arbeit schlagen wir vor, jede Note und ihre Eigenschaften als eindeutiges â€˜Wortâ€™ darzustellen, um die Wahrscheinlichkeit von Fehlanpassungen zwischen den Eigenschaften zu verringern und die Komplexität des Lernens zu reduzieren.â€œ Wir setzen auch Regularisierungsrichtlinien für den Bereich der Noten durch und ermutigen so die generierte Melodie, nahe an dem zu bleiben, was Menschen als einfach zu folgen empfinden würden. Die experimentellen Ergebnisse zeigen, dass unser Modell hörenswerte und angenehme Lieder generieren kann, die von menschlichen Liedern nicht mehr zu unterscheiden sind als frühere Modelle.
Tiefe ist eine Schlüsselkomponente von Deep Neural Networks (DNNs), jedoch ist die Gestaltung der Tiefe heuristisch und erfordert viele menschliche Bemühungen.Wir schlagen AutoGrow vor, um die Entdeckung der Tiefe in DNNs zu automatisieren: ausgehend von einer flachen Seed-Architektur, wächst AutoGrow neue Schichten, wenn das Wachstum die Genauigkeit verbessert; andernfalls stoppt das Wachstum und entdeckt so die Tiefe.Wir schlagen robuste Wachstums- und Stoppstrategien vor, um auf verschiedene Netzwerkarchitekturen und Datensätze zu verallgemeinern. Unsere Experimente zeigen, dass durch die Anwendung der gleichen Politik auf verschiedene Netzwerk-Architekturen, AutoGrow kann immer entdecken, in der Nähe von optimalen Tiefe auf verschiedenen Datensätzen von MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 und ImageNet.Zum Beispiel, in Bezug auf die Genauigkeit-Computation Trade-off, AutoGrow entdeckt eine bessere Tiefe Kombination in ResNets als menschliche Experten.Unsere AutoGrow ist effizient.Es entdeckt Tiefe in ähnlicher Zeit der Ausbildung eines einzelnen DNN.
In Anbetracht der Bedeutung der Fernerkundung hat die Gemeinschaft des Repräsentationslernens diesem Bereich erstaunlich wenig Aufmerksamkeit geschenkt.um dies zu ändern und die Innovation in diesem Bereich zu beschleunigen, bieten wir einen vereinfachten Zugang zu 5 verschiedenen Fernerkundungsdatensätzen in standardisierter Form.wir erforschen speziell das Repräsentationslernen in der Domäne und gehen der Frage nach, welche Eigenschaften ein Datensatz haben sollte, um eine gute Quelle für das Repräsentationslernen in der Fernerkundung zu sein.die etablierten Baselines erreichen die modernste Leistung auf diesen Datensätzen. 
Generative seq2seq-Dialogsysteme werden so trainiert, dass sie das nächste Wort in bereits stattgefundenen Dialogen vorhersagen. Sie können aus großen, unmarkierten Gesprächsdatensätzen lernen, ein tiefes Verständnis des Gesprächskontextes aufbauen und eine Vielzahl von Antworten generieren. Diese Flexibilität geht auf Kosten der Kontrolle. Anstatt Antworten Wort für Wort zu generieren, trainieren wir einen Klassifikator, der aus einer vordefinierten Liste vollständiger Antworten auswählt. Der Klassifikator wird auf (Gesprächskontext, Antwortklasse) Paaren trainiert, wobei jede Antwortklasse eine geräuschvoll markierte Gruppe austauschbarer Antworten ist. Experten können diese Beispielantworten im Laufe der Zeit bearbeiten und verbessern, ohne den Klassifikator neu zu trainieren oder alte Trainingsdaten ungültig zu machen.Die menschliche Auswertung von 775 ungesehenen Arzt-Patienten-Gesprächen zeigt, dass dieser Kompromiss die Antworten verbessert.Nur 12 % der Antworten unseres diskriminativen Ansatzes sind schlechter als die Antwort des Arztes im gleichen Gesprächskontext, verglichen mit 18 % für das generative Modell.Ein diskriminatives Modell, das ohne manuelle Kennzeichnung der Antwortklassen trainiert wird, erreicht die gleiche Leistung wie das generative Modell.
Es gibt eine zuvor identifizierte Äquivalenz zwischen breiten voll verbundenen neuronalen Netzen (FCNs) und Gauß'schen Prozessen (GPs). Diese Äquivalenz ermöglicht es zum Beispiel, Vorhersagen für die Testmenge zu berechnen, die sich aus einem voll Bayes'schen, unendlich breiten trainierten FCN ergeben hätten, ohne das FCN jemals zu instanziieren, sondern indem stattdessen der entsprechende GP ausgewertet wird. In dieser Arbeit leiten wir eine analoge Äquivalenz für mehrschichtige neuronale Faltungsnetze (CNNs) sowohl mit als auch ohne Pooling-Schichten ab und erzielen die besten Ergebnisse auf CIFAR10 für GPs ohne trainierbare Kerne. Wir führen auch eine Monte-Carlo-Methode ein, um die GP zu schätzen, die einer bestimmten neuronalen Netzarchitektur entspricht, selbst in Fällen, in denen die analytische Form zu viele Terme hat, um rechnerisch machbar zu sein. Überraschenderweise sind die GPs, die CNNs mit und ohne Gewichtsteilung entsprechen, identisch, wenn keine Pooling-Schichten vorhanden sind, so dass die Übersetzungsäquivarianz, die in CNNs mit endlichen Kanälen, die mit stochastischem Gradientenabstieg (SGD) trainiert werden, vorteilhaft ist, bei der Bayes'schen Behandlung der unendlichen Kanalgrenze garantiert keine Rolle spielt - ein qualitativer Unterschied zwischen den beiden Regimen, der im FCN-Fall nicht vorhanden ist. Wir bestätigen experimentell, dass sich die Leistung von SGD-trainierten endlichen CNNs in einigen Szenarien der Leistung der entsprechenden GPs annähert, wenn die Anzahl der Kanäle zunimmt. Bei sorgfältiger Abstimmung können SGD-trainierte CNNs ihre entsprechenden GPs deutlich übertreffen, was auf Vorteile des SGD-Trainings im Vergleich zur vollständig Bayesianischen Parameterschätzung hindeutet.
Die Bayes'sche Inferenz verspricht, die Leistung von tiefen neuronalen Netzen zu erden und zu verbessern, da sie robust gegenüber Überanpassung ist, das Trainingsverfahren und den Raum der Hyperparameter vereinfacht und ein kalibriertes Maß für die Unsicherheit bietet, das die Entscheidungsfindung, die Erkundung von Agenten und die Vorhersagegerechtigkeit verbessern kann. Trotz der theoretischen Vorteile der Bayes'schen Inferenz und der Ähnlichkeit zwischen MCMC- und Optimierungsmethoden ist die Leistung von Sampling-Methoden bisher hinter der von Optimierungsmethoden für große Deep-Learning-Aufgaben zurückgeblieben. Wir wollen diese Lücke schließen und stellen ATMC vor, einen adaptiven Noise-MCMC-Algorithmus, der das Posterior eines neuronalen Netzwerks schätzt und sampeln kann.ATMC passt die Menge an Momentum und Rauschen, die auf jede Parameteraktualisierung angewendet wird, dynamisch an, um die Verwendung stochastischer Gradienten zu kompensieren. Wir verwenden eine ResNet-Architektur ohne Batch-Normalisierung, um ATMC auf dem Cifar10-Benchmark und dem groß angelegten ImageNet-Benchmark zu testen, und zeigen, dass ATMC trotz fehlender Batch-Normalisierung eine starke Optimierungs-Baseline sowohl in Bezug auf die Klassifizierungsgenauigkeit als auch auf die Test-Log-Likelihood übertrifft.Wir zeigen, dass ATMC intrinsisch robust gegenüber Overfitting auf den Trainingsdaten ist und dass ATMC im Vergleich zur Optimierungs-Baseline ein besser kalibriertes Maß für die Unsicherheit liefert.
GANs können nun mehr und mehr realistische Gesichtsbilder erzeugen, die Menschen leicht täuschen können.  Im Gegensatz dazu kann ein gewöhnliches Faltungsneuronales Netzwerk (CNN), z. B. ResNet-18, eine Genauigkeit von mehr als 99,9 % bei der Unterscheidung von gefälschten/echten Gesichtern erreichen, wenn die Trainings- und Testgesichter aus derselben Quelle stammen.In dieser Arbeit haben wir sowohl menschliche Studien als auch CNN-Experimente durchgeführt, die uns zu zwei wichtigen Erkenntnissen geführt haben.Eine Erkenntnis ist, dass sich die Texturen von gefälschten Gesichtern erheblich von echten unterscheiden.CNNs können lokale Bildtexturinformationen zur Erkennung von gefälschten/echten Gesichtern erfassen, während solche Hinweise von Menschen leicht übersehen werden. Basierend auf den oben genannten Erkenntnissen schlagen wir eine neuartige Architektur vor, die als Gram-Net bezeichnet wird und die â€žGram-Blockâ€œ in mehreren semantischen Ebenen beinhaltet, um globale Bildtexturrepräsentationen zu extrahieren.Experimentelle Ergebnisse zeigen, dass unser Gram-Net besser abschneidet als existierende Ansätze zur Erkennung gefälschter Gesichter.  Insbesondere ist unser Gram-Net robuster gegen Bildbearbeitung, z.B. Downsampling, JPEG-Kompression, Unschärfe und Rauschen.  Noch wichtiger ist, dass unser Gram-Netz deutlich besser verallgemeinert, wenn es darum geht, gefälschte Gesichter aus GAN-Modellen zu erkennen, die in der Trainingsphase nicht gesehen wurden.
Im Gegensatz zur Kullback-Leibler-Divergenz, die ausschließlich die Veränderung der Wahrscheinlichkeit misst, spiegelt die Wasserstein-Metrik die zugrundeliegende Geometrie zwischen den Ergebnissen wider, deren Wert unter anderem bei der ordinalen Regression und der generativen Modellierung sowie in jüngster Zeit beim Reinforcement Learning nachgewiesen wurde. In diesem Papier beschreiben wir drei natürliche Eigenschaften von Wahrscheinlichkeitsdivergenzen, die unserer Meinung nach die Anforderungen des maschinellen Lernens widerspiegeln: Summeninvarianz, Skalensensitivität und unverzerrte Stichprobengradienten. Die Wasserstein-Metrik besitzt die ersten beiden Eigenschaften, aber im Gegensatz zur Kullback-Leibler-Divergenz nicht die dritte. Wir zeigen empirische Beweise, die darauf hindeuten, dass dies in der Praxis ein ernsthaftes Problem ist.Wir nutzen Erkenntnisse aus der probabilistischen Vorhersage und schlagen eine Alternative zur Wasserstein-Metrik vor, die CramÃ©r-Distanz.Wir zeigen, dass die CramÃ©r-Distanz alle drei gewÃ?nschten Eigenschaften besitzt und das Beste aus der Wasserstein- und der Kullback-Leibler-Divergenz kombiniert. Um die praktische Relevanz der CramÃ©r-Distanz zu veranschaulichen, entwerfen wir einen neuen Algorithmus, das CramÃ©r Generative Adversarial Network (GAN), und zeigen, dass es eine Reihe von erwÃ?nschten Eigenschaften gegenÃ?ber dem verwandten Wasserstein-GAN besitzt.
Wir Menschen haben ein angeborenes VerstÃ?ndnis fÃ?r den asymmetrischen Verlauf der Zeit, das wir nutzen, um unsere Umwelt effizient und sicher wahrzunehmen und zu manipulieren. Davon inspiriert gehen wir das Problem des Erlernens eines Zeitpfeils in einem Markov-(Entscheidungs-)Prozess an und zeigen, wie ein erlernter Zeitpfeil wichtige Informationen Ã?ber die Umwelt erfassen kann, die wiederum genutzt werden kÃ¶nnen, um die Erreichbarkeit zu messen, Nebenwirkungen zu erkennen und ein intrinsisches Belohnungssignal zu erhalten. Unsere empirischen Ergebnisse umfassen eine Auswahl von diskreten und kontinuierlichen Umgebungen und zeigen für eine Klasse von stochastischen Prozessen, dass der erlernte Zeitpfeil recht gut mit dem bekannten Begriff des Zeitpfeils von Jordan, Kinderlehrer und Otto (1998) übereinstimmt.
Wir formulieren den stochastischen Gradientenabstieg (SGD) als ein neuartiges faktorisiertes Bayes'sches Filterproblem, bei dem jeder Parameter separat abgeleitet wird, bedingt durch den korrespondierenden backpropagierten Gradienten.  Die Ableitung in diesem Rahmen führt natürlich zu BRMSprop und BAdam: Bayesianische Varianten von RMSprop und Adam.  Bemerkenswert ist, dass der Bayes'sche Ansatz viele Eigenschaften der modernsten adaptiven SGD-Methoden wiederherstellt, darunter die Root-Mean-Square-Normalisierung, die Nesterov-Beschleunigung und AdamW.  Somit liefert der Bayes'sche Ansatz eine Erklärung für die empirische Wirksamkeit der modernen adaptiven SGD-Algorithmen.  Ein empirischer Vergleich von BRMSprop und BAdam mit naivem RMSprop und Adam auf MNIST zeigt, dass Bayes'sche Methoden das Potenzial haben, Testverluste und Klassifikationsfehler erheblich zu reduzieren.
Datenerweiterung (DA) ist weit verbreitet, um die Generalisierung beim Training tiefer neuronaler Netze zu verbessern. In letzter Zeit wurde die von Menschen entworfene Datenerweiterung allmählich durch eine automatisch gelernte Erweiterungspolitik ersetzt.Durch das Finden der besten Politik im gut entworfenen Suchraum der Datenerweiterung kann AutoAugment (Cubuk et al., In diesem Papier entwickeln wir eine adversarische Methode, um zu einer rechnerisch erschwinglichen Lösung namens Adversarial AutoAugment zu gelangen, die gleichzeitig zielbezogene Objekt- und Augmentierungspolitik-Suchverluste optimieren kann.Das Augmentierungspolitik-Netzwerk versucht, den Trainingsverlust eines Zielnetzwerks durch die Erzeugung von adversarischen Augmentierungspolitiken zu erhöhen, während das Zielnetzwerk robustere Merkmale aus schwierigeren Beispielen lernen kann, um die Generalisierung zu verbessern. Im Gegensatz zu früheren Arbeiten verwenden wir die Berechnungen beim Training des Zielnetzes für die Bewertung der Richtlinien wieder und verzichten auf das erneute Training des Zielnetzes, was im Vergleich zu AutoAugment zu einer 12-fachen Reduzierung der Rechenkosten und einer 11-fachen Verkürzung des Zeitaufwands im ImageNet führt. Wir zeigen experimentelle Ergebnisse unseres Ansatzes auf CIFAR-10/CIFAR-100 und ImageNet und demonstrieren signifikante Leistungsverbesserungen gegenüber dem Stand der Technik: Auf CIFAR-10 erreichen wir einen Top-1-Testfehler von 1,36%, was das derzeit leistungsfähigste Einzelmodell ist, und auf ImageNet erreichen wir eine führende Leistung von Top-1-Genauigkeit 79,40% auf ResNet-50 und 80,00% auf ResNet-50-D ohne zusätzliche Daten.
In dieser Studie konzentrieren wir uns auf Meta-Learning-Algorithmen erster Ordnung, die darauf abzielen, eine Parameterinitialisierung eines Netzwerks zu erlernen, das sich schnell an neue Konzepte anpassen kann, wenn nur wenige Beispiele gegeben sind.Wir untersuchen zwei Ansätze, um die Generalisierung und die Lerngeschwindigkeit solcher Algorithmen zu verbessern, und erweitern dabei insbesondere den Reptile (Nichol et al, Wir stellen eine neuartige Regularisierungstechnik namens Meta-Schritt-Gradient Pruning vor und untersuchen auch die Auswirkungen der Erhöhung der Tiefe der Netzwerkarchitekturen im Meta-Lernen erster Ordnung.Wir präsentieren eine empirische Bewertung beider Ansätze, bei denen wir Benchmark-Klassifizierungsergebnisse für Bilder mit wenigen Aufnahmen mit 10 Mal weniger Iterationen unter Verwendung des Mini-ImageNet-Datensatzes erreichen und mit der Verwendung von tieferen Netzwerken Genauigkeiten erreichen, die die aktuellen Benchmarks für die Klassifizierung von Bildern mit wenigen Aufnahmen unter Verwendung des Omniglot-Datensatzes übertreffen.
In diesem Papier schlagen wir die Verwendung von In-Training-Matrix-Faktorisierung vor, um die Modellgröße für neuronale maschinelle Übersetzung zu reduzieren. Durch die Verwendung von In-Training-Matrix-Faktorisierung können Parametermatrizen in Produkte kleinerer Matrizen zerlegt werden, wodurch große maschinelle Übersetzungsarchitekturen komprimiert werden können, indem die Anzahl der erlernbaren Parameter erheblich reduziert wird. Wir wenden die In-Training-Matrixfaktorisierung auf verschiedene Schichten von neuronalen Standardarchitekturen an und zeigen, dass die In-Training-Matrixfaktorisierung in der Lage ist, fast 50 % der erlernbaren Parameter zu reduzieren, ohne dass damit ein Verlust an BLEU-Score verbunden ist.
Wir erforschen fünf experimentelle Methoden, die von früheren Arbeiten zur Evaluierung von vortrainierten Satzrepräsentationsmodellen inspiriert sind.Wir verwenden ein einzelnes linguistisches Phänomen, die Lizenzierung von negativen Polaritätselementen (NPIs), als Fallstudie für unsere Experimente.NPIs wie 'any' sind nur dann grammatikalisch, wenn sie in einer Lizenzierungsumgebung wie der Negation auftreten ('Sue hat keine Katzen' vs. '*Sue hat keine Katzen'). Wir stellen einen künstlich erzeugten Datensatz vor, der die Schlüsselmerkmale der NPI-Lizenzierung für die Experimente manipuliert.Wir stellen fest, dass BERT signifikantes Wissen über diese Merkmale hat, aber sein Erfolg variiert stark zwischen den verschiedenen experimentellen Methoden.Wir schließen daraus, dass eine Vielzahl von Methoden notwendig ist, um alle relevanten Aspekte des grammatikalischen Wissens eines Modells in einer bestimmten Domäne aufzudecken.
Das visuelle System von Primaten baut robuste, vielseitige Repräsentationen der Außenwelt auf, um verschiedene nachgelagerte kortikale Prozesse zu unterstützen. Solche Repräsentationen müssen invariant gegenüber sensorischen Inkonsistenzen sein, die durch dynamisch variierende Beleuchtung, lokale Texturverzerrungen usw. verursacht werden. Ein wichtiges architektonisches Merkmal, das solche Umweltunregelmäßigkeiten bekämpft, sind â€˜langreichweitige horizontale Verbindungenâ€™, die die Wahrnehmung der globalen Form von Objekten unterstützen. In dieser Arbeit untersuchen wir die EinfÃ?hrung solcher horizontalen Verbindungen in standardmÃ?ÃŸige tiefe Faltungsnetzwerke; wir prÃ?sentieren V1Net - eine neuartige rekurrente Faltungseinheit, die lineare und nichtlineare horizontale inhibitorische und exzitatorische Verbindungen modelliert, die von der visuellen kortikalen KonnektivitÃ?t von Primaten inspiriert sind.â€žWir stellen die Texturized Challenge vor â€" eine neue Benchmark zur Bewertung der Objekterkennungsleistung unter Wahrnehmungsrauschen â€" die wir verwenden, um V1Net gegen eine Reihe von sorgfÃ?ltig ausgewÃ?hlten Kontrollmodellen mit/ohne rekurrente Verarbeitung zu bewerten. Darüber hinaus stellen wir Ergebnisse einer Ablationsstudie von V1Net vor, die den Nutzen verschiedener neuronal inspirierter horizontaler Verbindungen für moderne KI-Systeme bei der Erkennung von Objektgrenzen in natürlichen Bildern demonstrieren. Wir präsentieren auch die Entstehung mehrerer biologisch plausibler horizontaler Konnektivitätsmuster, nämlich Center-On-Surround-Off-, Assoziationsfelder- und Border-Ownership-Konnektivitätsmuster in einem V1Net-Modell, das für die Erkennung von Grenzen in natürlichen Bildern aus dem Berkeley Segmentation Dataset 500 (BSDS500) trainiert wurde. Unsere Ergebnisse deuten auf eine erhöhte Ähnlichkeit der Repräsentation zwischen dem V1Net und biologischen visuellen Systemen hin und unterstreichen die Bedeutung neuronal inspirierter rekurrenter kontextueller Verarbeitungsprinzipien für das Erlernen visueller Repräsentationen, die robust gegenüber Wahrnehmungsrauschen sind und den Stand der Technik im Bereich des Computersehens voranbringen.
Im Gegensatz dazu versagen neuronale Netzwerkmodelle für die Modellierung natürlicher Sprache, wenn eine solche kompositorische Generalisierung erforderlich ist.Der Hauptbeitrag dieser Arbeit ist die Hypothese, dass Sprachkompositionalität eine Form von Gruppenäquivarianz ist. Auf der Grundlage dieser Hypothese schlagen wir eine Reihe von Werkzeugen für die Konstruktion von äquivarianten Sequenz-zu-Sequenz-Modellen vor. In einer Reihe von Experimenten zu den SCAN-Aufgaben analysieren wir das Verhalten bestehender Modelle unter dem Blickwinkel der Äquivarianz und zeigen, dass unsere äquivariante Architektur in der Lage ist, die Art von kompositioneller Generalisierung zu erreichen, die für das menschliche Sprachverständnis erforderlich ist.
Variationsinferenz (VI) ist ein beliebter Ansatz für die approximative Bayes'sche Inferenz, der besonders vielversprechend für stark parametrisierte Modelle wie tiefe neuronale Netze ist.  In dieser Arbeit schlagen wir eine Methode für das Training hochflexibler Variationsverteilungen vor, indem wir mit einer groben Approximation beginnen und diese iterativ verfeinern.Jeder Verfeinerungsschritt macht billige, lokale Anpassungen und erfordert nur die Optimierung einfacher Variationsfamilien.Wir zeigen theoretisch, dass unsere Methode immer eine Schranke für die Approximation (die untere Evidenzschranke) verbessert und beobachten dies empirisch bei einer Vielzahl von Benchmark-Aufgaben.  In Experimenten übertrifft unsere Methode durchweg aktuelle Variationsinferenzmethoden für Deep Learning in Bezug auf die Log-Likelihood und die ELBO.  Wir sehen, dass die Gewinne bei größeren Modellen noch weiter verstärkt werden, indem sie Standard-VI und tiefe Ensembles auf Restnetzwerken bei CIFAR10 deutlich übertreffen.
Ohne die ungleichmäßige Verteilung von Informationen in den beschädigten Bildern zu berücksichtigen, sind bisherige Methoden durch lokale Faltungsoperationen und die Gleichbehandlung von räumlichen und kanalweisen Merkmalen eingeschränkt. Um dieses Problem zu lösen, entwerfen wir lokale und nicht-lokale Aufmerksamkeitsblöcke, um Merkmale zu extrahieren, die die weitreichenden Abhängigkeiten zwischen Pixeln erfassen und den schwierigen Teilen mehr Aufmerksamkeit schenken.Insbesondere entwerfen wir einen Stammzweig und einen (nicht-)lokalen Maskenzweig in jedem (nicht-)lokalen Aufmerksamkeitsblock.Der Stammzweig wird verwendet, um hierarchische Merkmale zu extrahieren.Lokale und nicht-lokale Maskenzweige zielen darauf ab, diese hierarchischen Merkmale mit gemischter Aufmerksamkeit adaptiv neu zu skalieren. Die lokale Maske Zweig konzentriert sich auf mehr lokale Strukturen mit Faltungsoperationen, während nicht-lokale Aufmerksamkeit berücksichtigt mehr über die Langstrecken-Abhängigkeiten in der gesamten Feature-Map.Darüber hinaus schlagen wir residuale lokale und nicht-lokale Aufmerksamkeit Lernen, um die sehr tiefe Netzwerk zu trainieren, die weitere Verbesserung der Darstellung Fähigkeit des network.Our vorgeschlagene Methode kann für verschiedene Anwendungen der Bildwiederherstellung, wie z. B. Bild-Entrauschung, Demosaicing, Kompression Artefakte Reduktion und Super-Auflösung verallgemeinert werden.Experimente zeigen, dass unsere Methode erhält vergleichbare oder bessere Ergebnisse im Vergleich zu den kürzlich führenden Methoden quantitativ und visuell.
Die meisten Ansätze zum Erlernen von Handlungsplanungsmodellen beruhen auf einer großen Anzahl von Trainingsbeispielen oder Planbeobachtungen.In diesem Papier verfolgen wir einen anderen Ansatz, der auf deduktivem Lernen aus domänenspezifischem Wissen basiert, insbesondere aus logischen Formeln, die Einschränkungen über die möglichen Zustände einer gegebenen Domäne spezifizieren.Die minimale Eingangsbeobachtbarkeit, die unser Ansatz benötigt, ist ein einzelnes Beispiel, das aus einem vollständigen Ausgangszustand und einem partiellen Zielzustand besteht.Wir werden zeigen, dass die Ausnutzung von spezifischem Domänenwissen es ermöglicht, den Raum möglicher Handlungsmodelle einzuschränken sowie partielle Beobachtungen zu vervollständigen, die sich beide als hilfreich erweisen, um qualitativ hochwertige Handlungsmodelle zu lernen.
Wir veröffentlichen den größten öffentlichen EKG-Datensatz mit kontinuierlichen Rohsignalen für das Repräsentationslernen, der mehr als 11.000 Patienten und 2 Milliarden gelabelte Schläge enthält. Unser Ziel ist es, halbüberwachte EKG-Modelle zu erstellen sowie unbekannte Untertypen von Arrhythmien und anomale EKG-Signalereignisse zu entdecken.  Wir stellen eine Reihe von Basiswerten für verschiedene Merkmalsextraktoren zur Verfügung, auf denen aufgebaut werden kann.  Darüber hinaus führen wir qualitative Auswertungen der Ergebnisse von PCA-Einbettungen durch, bei denen wir eine gewisse Clusterung bekannter Subtypen feststellen, was auf das Potenzial des Repräsentationslernens für die Entdeckung von Arrhythmie-Subtypen hinweist.
Als grundlegender Baustein von Convolutional Neural Networks (CNNs) ist die Faltungsschicht darauf ausgelegt, lokale Muster zu extrahieren, und es mangelt ihr an der Fähigkeit, globale Zusammenhänge zu modellieren.Viele Anstrengungen wurden in letzter Zeit unternommen, um CNNs mit der Fähigkeit zur globalen Modellierung zu ergänzen, insbesondere durch eine Reihe von Arbeiten zur Interaktion globaler Merkmale. Die neurowissenschaftliche Forschung zeigt jedoch, dass neben den Einflüssen, die die Eingänge zu unseren Neuronen verändern, die Fähigkeit der Neuronen, ihre Funktionen dynamisch entsprechend dem Kontext zu verändern, für Wahrnehmungsaufgaben wesentlich ist, was bei den meisten CNNs übersehen wurde. Aus diesem Grund schlagen wir eine neuartige Context-Gated Convolution (CGC) vor, die explizit die Gewichte der Faltungsschichten adaptiv unter der Führung des globalen Kontextes modifiziert, so dass der modulierte Faltungs-Kernel unserer vorgeschlagenen CGC besser repräsentative lokale Muster extrahieren und diskriminierende Merkmale komponieren kann; darüber hinaus ist die von uns vorgeschlagene CGC leichtgewichtig, für moderne CNN-Architekturen geeignet und verbessert die Leistung von CNNs in umfangreichen Experimenten zur Bildklassifikation, Handlungserkennung und maschinellen Übersetzung.
Wir analysieren den Kompromiss zwischen Quantisierungsrauschen und Clipping-Verzerrung in Netzwerken mit geringer Präzision, identifizieren die Statistiken verschiedener Tensoren und leiten exakte Ausdrücke für die Verschlechterung des mittleren quadratischen Fehlers aufgrund von Clipping ab, und zeigen durch Optimierung dieser Ausdrücke deutliche Verbesserungen gegenüber Standard-Quantisierungsschemata, die normalerweise Clipping vermeiden. Zum Beispiel, nur durch die Wahl der genauen Clipping-Werte, mehr als 40\% Genauigkeitsverbesserung wird für die Quantisierung von VGG-16 auf 4-Bit Präzision erhalten.Unsere Ergebnisse haben viele Anwendungen für die Quantisierung von neuronalen Netzen sowohl beim Training und Inferenz Zeit. 
Die Batch-Normalisierung (BN) ist eine der am häufigsten verwendeten Techniken im Bereich des Deep Learning, aber ihre Leistung kann bei unzureichender Batch-Größe stark abnehmen, was die Verwendung von BN bei vielen Computer-Vision-Aufgaben wie Erkennung oder Segmentierung einschränkt, bei denen die Batch-Größe aufgrund des eingeschränkten Speicherverbrauchs in der Regel klein ist. Daher wurden viele modifizierte Normalisierungstechniken vorgeschlagen, die entweder die Leistung der BN nicht vollständig wiederherstellen oder zusätzliche nichtlineare Operationen in die Inferenzprozedur einführen und den Speicherverbrauch enorm erhöhen. Basierend auf unserer Analyse, schlagen wir eine neuartige Normalisierungsmethode vor, genannt Moving Average Batch Normalization (MABN).MABN kann die Leistung von Vanilla BN in kleinen Batch-Fällen vollständig wiederherstellen, ohne zusätzliche nichtlineare Operationen in der Inferenzprozedur einzuführen.Wir beweisen die Vorteile von MABN sowohl durch theoretische Analyse als auch durch Experimente.Unsere Experimente demonstrieren die Effektivität von MABN in mehreren Computer-Vision-Aufgaben, einschließlich ImageNet und COCO.Der Code wurde in https://github.com/megvii-model/MABN veröffentlicht.
Wir präsentieren einen einfachen Beweis für den Nutzen von Tiefe in mehrschichtigen Feedforward-Netzwerken mit gleichgerichteter Aktivierung (``Tiefentrennung''), insbesondere stellen wir eine Folge von Klassifizierungsproblemen f_i vor, so dass(a) für jedes Netzwerk mit fester Tiefe ein Index m gefunden werden kann, so dass Probleme mit Index > m eine exponentielle Netzwerkbreite benötigen, um die Funktion f_m vollständig zu repräsentieren; und(b) für jedes Problem f_m in der Familie präsentieren wir ein konkretes neuronales Netzwerk mit linearer Tiefe und begrenzter Breite, das es vollständig repräsentiert. Es gibt zwar mehrere frühere Arbeiten, die ähnliche Ergebnisse zeigen, aber unser Beweis verwendet wesentlich einfachere Werkzeuge und Techniken und sollte für Studenten der Informatik und Personen mit ähnlichem Hintergrund zugänglich sein.
Die reichhaltigen und zugänglichen beschrifteten Daten haben den revolutionären Erfolg von Deep Learning ermöglicht, doch bleibt eine massive Überwachung für viele reale Anwendungen ein Luxus, was das Interesse an beschriftungsarmen Techniken wie dem "few-shot learning" (FSL) erhöht.Ein intuitiv durchführbarer Ansatz für FSL ist die Datenerweiterung durch die Synthese zusätzlicher Trainingsmuster. In diesem Papier schlagen wir ein neuartiges FSL-Modell vor, das $\textrm{D}^2$GAN genannt wird und das vielfältige und unterscheidende Merkmale auf der Basis von Generative Adversarial Networks (GAN) synthetisiert.$\textrm{D}^2$GAN sichert die Unterscheidbarkeit der synthetisierten Merkmale, indem es sie dazu zwingt, eine hohe Korrelation mit realen Merkmalen derselben Klassen und eine niedrige Korrelation mit denen anderer Klassen zu haben.  Basierend auf der Beobachtung, dass Rauschvektoren, die im Raum der latenten Codes näher beieinander liegen, eher in denselben Modus kollabieren, wenn sie auf den Merkmalsraum abgebildet werden, enthält $\textrm{D}^2$GAN einen neuartigen Anti-Collapse-Regularisierungsterm, der die Merkmalsvielfalt fördert, indem er das Verhältnis der logarithmischen Ähnlichkeit zweier synthetisierter Merkmale und der logarithmischen Ähnlichkeit der sie erzeugenden latenten Codes bestraft. Experimente an drei gängigen Benchmark-Datensätzen verifizieren die Effektivität von $\textrm{D}^2$GAN im Vergleich zum Stand der Technik.
Der Mangel an klaren mathematischen Modellen, die die Struktur realer Datensätze erfassen, ist ein Haupthindernis für das detaillierte theoretische Verständnis tiefer neuronaler Netze. Hier demonstrieren wir zunächst die Wirkung strukturierter Datensätze durch den experimentellen Vergleich der Dynamik und der Leistung von zwei Layernetzwerken, die auf zwei verschiedenen Datensätzen trainiert wurden:(i) einem unstrukturierten synthetischen Datensatz mit zufälligen i.i.d.-Eingaben und(ii) einem einfachen kanonischen Datensatz wie MNIST-Bilder. Unsere Analyse zeigt zwei Phänomene in Bezug auf die Dynamik der Netze und ihre Fähigkeit zur Generalisierung, die nur beim Training auf strukturierten Datensätzen auftreten.Zweitens führen wir ein generatives Modell für Datensätze ein, bei denen hochdimensionale Eingaben auf einer niedrigdimensionalen Mannigfaltigkeit liegen und Labels haben, die nur von ihrer Position innerhalb dieser Mannigfaltigkeit abhängen.Wir nennen es das*versteckte Mannigfaltigkeitsmodell* und wir zeigen experimentell, dass das Training der Netze auf Datensätzen, die aus diesem Modell stammen, beide Phänomene reproduziert, die beim Training auf MNIST auftreten.
In diesem Papier untersuchen wir tiefe diagonale zirkulierende neuronale Netze, d.h. tiefe neuronale Netze, bei denen die Gewichtsmatrizen das Produkt aus diagonalen und zirkulierenden Matrizen sind, und führen neben einer theoretischen Analyse ihrer Ausdruckskraft auch prinzipielle Techniken für das Training dieser Modelle ein: Wir entwickeln ein Initialisierungsschema und schlagen eine intelligente Verwendung von Nichtlinearitätsfunktionen vor, um tiefe diagonale zirkulierende Netze zu trainieren. Wir führen eine gründliche experimentelle Studie durch, um die Leistung von tiefen diagonalen zirkulanten Netzwerken mit dem Stand der Technik Modelle auf der Grundlage von strukturierten Matrizen und mit dichten Modellen zu vergleichen.Wir zeigen, dass unsere Modelle eine bessere Genauigkeit als andere strukturierte Ansätze erreichen, während 2x weniger Gewichte als der nächstbeste Ansatz erforderlich.Schließlich trainieren wir tiefe diagonale zirkulante Netzwerke, um eine kompakte und genaue Modelle auf einer realen Welt Video-Klassifikation Datensatz mit über 3,8 Millionen Trainingsbeispiele zu bauen.
Die Interpretierbarkeit hat sich weitgehend auf lokale Erklärungen konzentriert, d.h. auf die Erklärung, warum ein Modell eine bestimmte Vorhersage für eine Probe gemacht hat. Diese Erklärungen sind aufgrund ihrer Einfachheit und lokalen Treue ansprechend, aber sie liefern keine Informationen über das allgemeine Verhalten des Modells. Durch sorgfältige Experimente zeigen wir qualitativ und quantitativ, dass globale additive Erklärungen in der Lage sind, das Modellverhalten zu beschreiben und Einblicke in Modelle wie neuronale Netze zu geben. Eine Visualisierung unseres Ansatzes, angewandt auf ein neuronales Netz, während es trainiert wird, ist unter https://youtu.be/ErQYwNqzEdc verfügbar.
Ein großer Teil des jüngsten Erfolgs in der Verarbeitung natürlicher Sprache (NLP) wurde durch verteilte Vektordarstellungen von Wörtern erzielt, die auf große Mengen von Text in einer nicht überwachten Weise trainiert wurden.Diese Darstellungen werden typischerweise als allgemeine Merkmale für Wörter in einer Reihe von NLP-Problemen verwendet. Jüngste Arbeiten haben sowohl unüberwachte als auch überwachte Lerntechniken mit unterschiedlichen Trainingszielen untersucht, um allgemeine Satzrepräsentationen mit fester Länge zu erlernen.In dieser Arbeit stellen wir ein einfaches, effektives Multitasking-Lernverfahren für Satzrepräsentationen vor, das die induktiven Verzerrungen verschiedener Trainingsziele in einem einzigen Modell kombiniert. Wir trainieren dieses Modell auf mehreren Datenquellen mit mehreren Trainingszielen auf über 100 Millionen Sätzen.Umfangreiche Experimente zeigen, dass die gemeinsame Nutzung eines einzigen rekurrenten Satz Kodierer über schwach verwandte Aufgaben führt zu konsistenten Verbesserungen gegenüber früheren Methoden.Wir präsentieren erhebliche Verbesserungen im Zusammenhang mit Transfer-Lernen und Low-Ressource-Einstellungen mit unseren gelernten Allzweck-Repräsentationen.
In einer Zeit, in der neuronale Netze zunehmend in sensiblen Anwendungen eingesetzt werden, hat sich algorithmische Voreingenommenheit als ein Thema mit moralischen Implikationen herauskristallisiert.Während es unzählige Möglichkeiten gibt, wie ein System durch Voreingenommenheit beeinträchtigt werden kann, ist die systematische Isolierung und Bewertung bestehender Systeme in Bezug auf solche Szenarien nicht trivial, d.h., Zu diesem Zweck wird in diesem Papier die erste systematische Studie zum Benchmarking neuronaler Modelle auf dem neuesten Stand der Technik in Bezug auf voreingenommene Szenarien vorgeschlagen.konkret postulieren wir, dass das Problem der voreingenommenen Annotatoren durch neuronale Modelle angenähert werden kann, d.h., Alles in allem bietet unser Rahmenwerk einen neuen Weg für die prinzipielle Quantifizierung und Evaluierung von Modellen gegen voreingenommene Datensätze. Folglich stellen wir fest, dass moderne NLP-Modelle (z.B. BERT, RoBERTa, XLNET) leicht durch voreingenommene Daten beeinträchtigt werden.
In diesem Szenario gehen wir davon aus, dass der Benutzer a priori eine Teilmenge der Themen kennt, die das Modell lernen soll, und in der Lage ist, einige Beispieldokumente für diese Themen bereitzustellen.      Wir erweitern NTMs auf die schwach semi-überwachte Einstellung, indem wir informative Prioren im Trainingsziel verwenden. Nach der Analyse der Auswirkungen informativer Prioren schlagen wir eine einfache Modifikation des NVDM-Modells vor, die ein logit-normales Posterior verwendet, von dem wir zeigen, dass es im Vergleich zu anderen NTM-Modellen eine bessere Anpassung an die vom Benutzer gewünschten Themen erreicht.
Die Analyse tiefer neuronaler Netze (DNNs) mit Hilfe der Theorie der Informationsebene (IP) hat in letzter Zeit große Aufmerksamkeit erregt, da sie u. a. einen Einblick in ihre Generalisierungsfähigkeit bietet. Beispielsweise erfordern versteckte Schichten mit vielen Neuronen MI-Schätzer mit Robustheit gegenüber der hohen Dimensionalität, die mit solchen Schichten verbunden ist.MI-Schätzer sollten auch in der Lage sein, auf natürliche Weise mit Faltungsschichten umzugehen, während sie gleichzeitig rechnerisch vertretbar sind, um auf große Netzwerke skalieren zu können.Keine der bestehenden IP-Methoden war bisher in der Lage, wirklich tiefe Faltungsneuronale Netze (CNNs) zu untersuchen, wie z. B. das In dieser Arbeit schlagen wir eine IP-Analyse unter Verwendung der neuen matrixbasierten R\'enyi-Entropie in Verbindung mit Tensor-Kerneln über Faltungsschichten vor, wobei die Leistungsfähigkeit von Kernel-Methoden zur Darstellung von Eigenschaften der Wahrscheinlichkeitsverteilung unabhängig von der Dimensionalität der Daten genutzt wird. Die erzielten Ergebnisse werfen ein neues Licht auf die bisherige Literatur über kleine DNNs, allerdings unter Verwendung eines völlig neuen Ansatzes, der es uns ermöglicht, die erste umfassende IP-Analyse zeitgenössischer großer DNNs und CNNs durchzuführen, die verschiedenen Trainingsphasen zu untersuchen und neue Einblicke in die Trainingsdynamik großer neuronaler Netze zu geben.
Die Entwicklung von Agenten, die lernen können, Anweisungen in natürlicher Sprache zu befolgen, ist eine aufstrebende Forschungsrichtung, die zwar zugänglich und flexibel ist, aber selbst für Menschen manchmal mehrdeutig sein kann. Wir entwickeln dann einen modularen Rahmen, der lernt, eine von einem Programm spezifizierte Aufgabe auszufÃ?hren â€" da verschiedene UmstÃ?nde zu unterschiedlichen MÃ¶glichkeiten fÃ?r die ErfÃ?llung der Aufgabe fÃ?hren, kann unser Rahmen erkennen, unter welchen UmstÃ?nden er sich gerade befindet, und entsprechend eine Multitasking-Politik anweisen, jede Teilaufgabe der Gesamtaufgabe zu erfÃ?llen. Experimentelle Ergebnisse auf einer 2D Minecraft-Umgebung zeigen nicht nur, dass das vorgeschlagene Framework lernt, Programmanweisungen zuverlässig auszuführen und eine Null-Schuss-Generalisierung auf komplexere Anweisungen erreicht, sondern verifizieren auch die Effizienz des vorgeschlagenen Modulationsmechanismus für das Lernen der Multitasking-Politik.Wir führen auch eine Analyse durch, die verschiedene Modelle vergleicht, die von Programmen und natürlichsprachlichen Anweisungen in einer End-to-End-Mode lernen.
Wir analysieren die Konvergenz des (stochastischen) Gradientenabstiegsalgorithmus für das Lernen eines Faltungsfilters mit einer Rectified Linear Unit (ReLU) Aktivierungsfunktion, wobei unsere Analyse nicht von einer bestimmten Form der Eingabeverteilung abhängt und unsere Beweise nur die Definition von ReLU verwenden, im Gegensatz zu früheren Arbeiten, die auf eine Standard-Gauß-Eingabe beschränkt sind. Wir zeigen, dass der (stochastische) Gradientenabstieg mit zufälliger Initialisierung den Faltungsfilter in polynomialer Zeit erlernen kann und die Konvergenzrate von der Glattheit der Eingangsverteilung und der Nähe der Patches abhängt.Nach unserem Wissen ist dies die erste Wiederherstellungsgarantie von gradientenbasierten Algorithmen für Faltungsfilter auf nicht-Gauß'schen Eingangsverteilungen.Unsere Theorie rechtfertigt auch die zweistufige Lernratenstrategie in tiefen neuronalen Netzen.Obwohl unser Schwerpunkt auf der Theorie liegt, stellen wir auch Experimente vor, die unsere theoretischen Ergebnisse rechtfertigen.
Tiefe neuronale Netze (DNNs) werden aufgrund ihrer hohen Genauigkeit häufig in realen kognitiven Anwendungen eingesetzt. Die Robustheit von DNN-Modellen wurde jedoch in letzter Zeit durch gegnerische Angriffe in Frage gestellt, bei denen kleine Störungen in den Eingabeproben zu einer Fehlklassifizierung führen können. Moderne Verteidigungsalgorithmen, wie z.B. adversariales Training oder robuste Optimierung, verbessern die Widerstandsfähigkeit von DNNs gegen adversarische Angriffe, sind aber mit hohen Rechenkosten verbunden. Darüber hinaus sind diese Ansätze in der Regel nur für die Verteidigung gegen eine oder wenige bekannte Angriffstechniken ausgelegt, so dass die Wirksamkeit der Verteidigung gegen andere Arten von Angriffsmethoden, insbesondere solche, die noch nicht entdeckt oder erforscht wurden, nicht garantiert werden kann. Diese Arbeit zielt auf einen allgemeinen Ansatz zur Verbesserung der Robustheit von DNN-Modellen unter gegnerischen Angriffen.Insbesondere schlagen wir Bamboo - die erste Daten Augmentation Methode zur Verbesserung der allgemeinen Robustheit von DNN ohne Hypothese über die angreifenden algorithms.Bamboo erweitert die Ausbildung Datensatz mit einer kleinen Menge von Daten gleichmäßig auf einem festen Radius Ball um jede Ausbildung data.and daher effektiv erhöhen den Abstand zwischen natürlichen Datenpunkte und Entscheidung boundary. Unsere Experimente zeigen, dass Bamboo die allgemeine Robustheit gegen beliebige Arten von Angriffen und Rauschen erheblich verbessert und bessere Ergebnisse im Vergleich zu früheren adversen Trainingsmethoden, robusten Optimierungsmethoden und anderen Methoden zur Datenerweiterung mit der gleichen Menge an Datenpunkten erzielt.
Die Fähigkeit, realistische Muster neuronaler Aktivität zu synthetisieren, ist für die Untersuchung der neuronalen Informationsverarbeitung von entscheidender Bedeutung: Hier haben wir das Generative Adversarial Networks (GANs) Framework verwendet, um die konzertierte Aktivität einer Population von Neuronen zu simulieren. Wir haben die Wasserstein-GAN-Variante angepasst, um die Erzeugung von uneingeschränkten Aktivitätsmustern neuronaler Populationen zu erleichtern und gleichzeitig von der gemeinsamen Nutzung von Parametern im zeitlichen Bereich zu profitieren, und wir zeigen, dass unser vorgeschlagenes GAN, das wir als Spike-GAN bezeichnet haben, Spike Trains erzeugt, die genau mit den Statistiken erster und zweiter Ordnung von Datensätzen mit Dutzenden von Neuronen übereinstimmen und auch deren Statistiken höherer Ordnung gut annähern. Wir haben Spike-GAN auf einen realen Datensatz angewandt, der von der Salamander-Retina aufgenommen wurde, und gezeigt, dass es genauso gut funktioniert wie State-of-the-Art-Ansätze, die auf der maximalen Entropie und dem dichotomisierten Gauß-Rahmen beruhen, wobei Spike-GAN nicht erfordert, die Statistiken, die vom Modell angepasst werden sollen, a priori zu spezifizieren, und daher eine flexiblere Methode als diese alternativen Ansätze darstellt. Spike-GAN bietet eine leistungsstarke, einfach anzuwendende Technik zur Erzeugung realistischer neuronaler Spike-Aktivitäten und zur Beschreibung der wichtigsten Merkmale der groß angelegten neuronalen Populationsaufzeichnungen, die in den modernen Systemneurowissenschaften untersucht werden.
Tiefe latente Variablenmodelle sind aufgrund der skalierbaren Lernalgorithmen, die von Kingma & Welling (2013) und Rezende et al. (2014) eingeführt wurden, zu einer beliebten Modellwahl geworden, da diese Ansätze eine untere Variationsschranke für die unlösbare log Likelihood der beobachteten Daten maximieren. (2015) haben eine Variationsschranke für mehrere Stichproben, IWAE, eingeführt, die mindestens so eng ist wie die Standard-Variationsschranke und mit zunehmender Stichprobenzahl immer enger wird.Widersinnigerweise schneidet der typische Gradientenschätzer des Inferenznetzwerks für die IWAE-Schranke mit zunehmender Stichprobenzahl schlecht ab (Rainforth et al. 2018, Le et al. 2018).Roeder et a. (2017) schlagen einen verbesserten Gradientenschätzer vor, können aber nicht zeigen, dass er unverzerrt ist. Wir zeigen, dass er tatsächlich verzerrt ist und dass die Verzerrung mit einer zweiten Anwendung des Reparametrisierungstricks effizient geschätzt werden kann.Der doppelt reparametrisierte Gradientenschätzer (DReG) leidet nicht, wenn die Anzahl der Stichproben steigt, wodurch die zuvor aufgeworfenen Probleme gelöst werden.Die gleiche Idee kann verwendet werden, um viele kürzlich eingeführte Trainingstechniken für latente Variablenmodelle zu verbessern. Insbesondere zeigen wir, dass dieser Schätzer die Varianz des IWAE-Gradienten, des regewichteten Wake-Sleep-Updates (RWS) (Bornschein & Bengio 2014) und des Jackknife-Variationsinferenz-Gradienten (JVI) (Nowozin 2018) reduziert und dass dieser rechnerisch effiziente Drop-in-Schätzer zu einer verbesserten Leistung für alle drei Ziele bei verschiedenen Modellierungsaufgaben führt.
Die Optimierung nullter Ordnung ist der Prozess der Minimierung eines Ziels $f(x)$ mit Orakelzugriff auf Auswertungen bei adaptiv gewählten Eingaben $x$.In diesem Beitrag stellen wir zwei einfache, aber leistungsfähige GradientLess Descent (GLD)-Algorithmen vor, die nicht auf einer zugrunde liegenden Gradientenschätzung beruhen und numerisch stabil sind. Wir analysieren unseren Algorithmus aus einer neuartigen geometrischen Perspektive und zeigen, dass wir für {\it einer beliebigen monotonen Transformation} eines glatten und stark konvexen Ziels mit latenter Dimension $k \ge n$ eine neuartige Analyse präsentieren, die Konvergenz innerhalb einer $\epsilon$-Kugel des Optimums in $O(kQ\log(n)\log(R/\epsilon))$ Evaluationen zeigt, wobei die Eingabedimension $n$, $R$ der Durchmesser des Eingaberaums und $Q$ die Bedingungszahl ist. Unsere Raten sind die ersten ihrer Art, die sowohl1) polylogarithmisch von der Dimensionalität abhängen als auch2) invariant unter monotonen Transformationen sind.Wir nutzen außerdem unsere geometrische Perspektive, um zu zeigen, dass unsere Analyse optimal ist.Sowohl die monotone Invarianz als auch die Fähigkeit, eine niedrige latente Dimensionalität zu nutzen, sind der Schlüssel zum empirischen Erfolg unserer Algorithmen, wie an synthetischen und MuJoCo-Benchmarks gezeigt wurde.
Viele Prozesse können prägnant als eine Abfolge von Ereignissen dargestellt werden, die von einem Anfangszustand zu einem Endzustand führen. Angesichts roher Zutaten und eines fertigen Kuchens kann ein erfahrener Koch das Rezept erahnen. Aufbauend auf dieser Intuition schlagen wir eine neue Klasse von visuellen generativen Modellen vor: zielkonditionierte Prädiktoren (GCP).Frühere Arbeiten zur Videogenerierung konzentrieren sich weitgehend auf Vorhersagemodelle, die nur Frames vom Anfang des Videos beobachten.GCP behandelt Videos stattdessen als Start-Ziel-Transformationen und erleichtert die Videogenerierung durch die Konditionierung auf den informativeren Kontext, der durch die ersten und letzten Frames bereitgestellt wird. Bestehende Vorwärtsvorhersageansätze synthetisieren nicht nur bessere und längere Videos, wenn sie so modifiziert werden, dass sie zielkonditioniert sind, sondern GCP-Modelle können auch Strukturen nutzen, die nicht linear in der Zeit sind, um hierarchische Vorhersagen zu erreichen. Zu diesem Zweck untersuchen wir sowohl autoregressive GCP-Modelle als auch neuartige baumstrukturierte GCP-Modelle, die Frames rekursiv generieren und das Video iterativ in immer feinere, durch Unterziele abgegrenzte Segmente aufteilen. In Experimenten mit simulierten und realen Datensätzen erzeugen unsere GCP-Methoden qualitativ hochwertige Sequenzen über lange Zeiträume.  Baumstrukturierte GCPs sind auch wesentlich einfacher zu parallelisieren als autoregressive GCPs, was das Training und die Inferenz sehr effizient macht und es dem Modell ermöglicht, auf Sequenzen zu trainieren, die Tausende von Frames lang sind. Videos finden Sie auf der ergänzenden Website: https://sites.google.com/view/video-gcp
Jüngste Fortschritte in der Computertechnologie und im Sensordesign haben es einfacher gemacht, Längsschnitt- oder Zeitreihendaten von Patienten zu sammeln, was zu einer gigantischen Menge an verfügbaren medizinischen Daten geführt hat.Die meisten medizinischen Zeitreihen sind nicht kommentiert, oder selbst wenn die Kommentare verfügbar sind, könnten sie subjektiv und anfällig für menschliche Fehler sein. Frühere Arbeiten haben Techniken zur Verarbeitung natürlicher Sprache entwickelt, um Konzeptannotationen und/oder klinische Erzählungen aus Arztnotizen zu extrahieren, doch diese Ansätze sind langsam und nutzen nicht die begleitenden medizinischen Zeitreihendaten, Wir schlagen Relational Multi-Instance Learning (RMIL) vor - ein tiefes Multi-Instance Learning Framework, das auf rekurrenten neuronalen Netzen basiert, die Pooling-Funktionen und Aufmerksamkeitsmechanismen für die Konzeptannotation verwenden.
Die Einbettungsschichten, die Eingabewörter in reale Vektoren umwandeln, sind die Schlüsselkomponenten von tiefen neuronalen Netzen, die in der Verarbeitung natürlicher Sprache verwendet werden. Wenn das Vokabular jedoch groß ist, können die entsprechenden Gewichtsmatrizen enorm sein, was ihren Einsatz in einer Umgebung mit begrenzten Ressourcen ausschließt.Wir stellen eine neuartige Methode zur Parametrisierung von Einbettungsschichten vor, die auf der Tensor-Train (TT)-Zerlegung basiert und eine erhebliche Komprimierung des Modells auf Kosten eines vernachlässigbaren Rückgangs oder sogar eines leichten Leistungsgewinns ermöglicht.  Wir evaluieren unsere Methode anhand einer Vielzahl von Benchmarks im Bereich der Verarbeitung natürlicher Sprache und analysieren den Kompromiss zwischen Leistung und Kompressionsraten für eine Vielzahl von Architekturen, von MLPs über LSTMs bis hin zu Transformers.
Wir stellen fest, dass gängige Implementierungen adaptiver Gradientenalgorithmen, wie Adam, den potenziellen Nutzen der Gewichtsabnahme-Regularisierung einschränken, da die Gewichte nicht multiplikativ abnehmen (wie es bei einer Standardabnahme der Gewichte zu erwarten wäre), sondern um einen additiven konstanten Faktor. Wir liefern empirische Belege dafür, dass die von uns vorgeschlagene Modifikation(i) die optimale Wahl des Gewichtsdecay-Faktors von der Einstellung der Lernrate sowohl für Standard-SGD als auch für Adam entkoppelt und(ii) die Generalisierungsleistung von Adam erheblich verbessert, so dass es mit SGD auf Bildklassifizierungsdatensätzen konkurrieren kann (auf denen es zuvor typischerweise von letzterem übertroffen wurde). Wir zeigen auch, dass längere Optimierungsläufe kleinere Werte für den Gewichtsverfall erfordern, um optimale Ergebnisse zu erzielen, und führen eine normalisierte Variante des Gewichtsverfalls ein, um diese Abhängigkeit zu verringern. Schließlich schlagen wir eine Version von Adam mit warmen Neustarts (AdamWR) vor, die eine starke Leistung zu jeder Zeit aufweist und gleichzeitig Ergebnisse auf dem neuesten Stand der Technik bei CIFAR-10 und ImageNet32x32 erzielt. Unser Quellcode wird nach dem Review-Prozess verfügbar sein.
Lebenslanges Lernen ist das Problem des Lernens von mehreren aufeinanderfolgenden Aufgaben in einer sequentiellen Art und Weise, wo das Wissen aus früheren Aufgaben gewonnen wird beibehalten und für zukünftige learning.it ist von wesentlicher Bedeutung für die Entwicklung von intelligenten Maschinen, die zu ihrer Umgebung anpassen können.in dieser Arbeit konzentrieren wir uns auf ein lebenslanges Lernen Ansatz zur generativen Modellierung, wo wir kontinuierlich neu beobachtete Streaming-Verteilungen in unsere gelernten model.we tun dies durch einen Schüler-Lehrer-Architektur, die uns zu lernen und zu bewahren alle Verteilungen gesehen, so weit, ohne die Notwendigkeit, die Vergangenheit Daten noch die Vergangenheit Modelle. Durch die Einführung eines neuartigen modellübergreifenden Regularisierers nutzt das Schülermodell die vom Lehrer gelernten Informationen, die als Zusammenfassung aller bisher gesehenen Daten fungieren. Der Regularisierer hat den zusätzlichen Vorteil, dass er den Effekt der katastrophalen Interferenz reduziert, der auftritt, wenn wir über Streaming-Daten lernen.
Dreidimensionale geometrische Daten bieten eine hervorragende Domäne für das Studium der Darstellung Lernen und generative modeling.in diesem Papier, schauen wir auf geometrische Daten als Punktwolken dargestellt.wir führen eine tiefe Autoencoder (AE) Netzwerk mit ausgezeichneten Rekonstruktion Qualität und Generalisierung ability.the gelernt Darstellungen übertreffen den Stand der Technik in 3D-Erkennung Aufgaben und ermöglichen grundlegende Formbearbeitung Anwendungen über einfache algebraische Manipulationen, wie semantische Teil Bearbeitung, Form Analogien und Form Interpolation. Wir führen auch eine gründliche Untersuchung verschiedener generativer Modelle durch, darunter GANs, die auf den rohen Punktwolken operieren, deutlich verbesserte GANs, die im festen latenten Raum unserer AEs trainiert wurden, und Gaußsche Mischmodelle (GMM).Interessanterweise erzeugen GMMs, die im latenten Raum unserer AEs trainiert wurden, Proben mit der besten Genauigkeit und Vielfalt.Um unsere quantitative Bewertung generativer Modelle durchzuführen, schlagen wir einfache Maße für Genauigkeit und Vielfalt vor, die auf einer optimalen Übereinstimmung zwischen den Punktwolken basieren.
Trotz der bemerkenswerten Leistung von tiefen neuronalen Netzen (DNNs) auf verschiedene Aufgaben, sind sie anfällig für Störungen, die es schwierig macht, sie in der realen Welt sicherheitskritischen Anwendungen einzusetzen.in diesem Papier, wir zielen darauf ab, robuste Netzwerke durch Sparsifying DNN latente Funktionen empfindlich auf Störungen zu erhalten. Insbesondere definieren wir die Verwundbarkeit im latenten Merkmalsraum und schlagen dann einen Bayes'schen Rahmen vor, um Merkmale auf der Grundlage ihres Beitrags zum ursprünglichen und zum gegnerischen Verlust zu priorisieren bzw. zu beschneiden, und schlagen vor, die Verwundbarkeit der Merkmale während des Trainings zu regulieren, um die Robustheit weiter zu verbessern. Während eine solche Netzwerk-Sparsamkeit in der Literatur vor allem im Hinblick auf die Recheneffizienz und den Regularisierungseffekt von DNNs untersucht wurde, bestätigen wir, dass es auch nützlich ist, einen Verteidigungsmechanismus durch quantitative Bewertung und qualitative Analyse zu entwickeln. Wir validieren unsere Methode, \emph{Adversarial Neural Pruning (ANP)} auf mehreren Benchmark-Datensätzen, was zu einer Verbesserung der Testgenauigkeit und zu einer Robustheit auf dem neuesten Stand der Technik führt.ANP geht auch das praktische Problem an, spärliche und robuste Netzwerke zur gleichen Zeit zu erhalten, was entscheidend sein könnte, um die Robustheit von leichtgewichtigen Netzwerken zu gewährleisten, die auf rechen- und speicherbegrenzten Geräten eingesetzt werden.
Bei der Erkennung von Anomalien (anomaly detection, AD) wird versucht, anhand eines Datensatzes normaler Stichproben festzustellen, ob ein Testmuster anormal ist.   Ein neuer und vielversprechender Ansatz für AD beruht auf tiefen generativen Modellen, wie Variations-Auto-Codern (VAEs), für das unbeaufsichtigte Lernen der normalen Datenverteilung.In semi-supervised AD (SSAD) enthalten die Daten auch eine kleine Stichprobe von markierten Anomalien.In dieser Arbeit schlagen wir zwei Variationsmethoden für das Training von VAEs für SSAD vor. Die intuitive Idee beider Methoden ist es, den Kodierer zu trainieren, um zwischen latenten Vektoren für normale und Ausreißerdaten zu â€˜trennenâ€™. Wir zeigen, dass diese Idee aus prinzipiellen probabilistischen Formulierungen des Problems abgeleitet werden kann, und schlagen einfache und effektive Algorithmen vor.  Unsere Methoden können auf verschiedene Datentypen angewandt werden, wie wir an SSAD-Datensätzen von Naturbildern bis hin zu Astronomie und Medizin demonstrieren, und können mit jeder VAE-Modellarchitektur kombiniert werden.Im Vergleich zu State-of-the-Art-SSAD-Methoden, die nicht spezifisch für bestimmte Datentypen sind, erzielen wir deutliche Verbesserungen bei der Ausreißererkennung.
DIH ist eine Eigenschaft jedes Trainingsmusters und wird als laufender Mittelwert der momentanen Härte des Musters berechnet, die über den Trainingsverlauf gemessen wird. Wir verwenden DIH, um zu bewerten, wie gut ein Modell das Wissen über jedes Trainingsmuster im Laufe der Zeit beibehält. Wir stellen fest, dass für tiefe neuronale Netze (DNNs) die DIH eines Musters in relativ frühen Trainingsphasen seine DIH in späteren Phasen widerspiegelt. Konkret werden in jeder Epoche nur Proben mit hohem DIH trainiert (da sie historisch schwer sind), während Proben mit niedrigem DIH sicher ignoriert werden können.DIH wird in jeder Epoche nur für die ausgewählten Proben aktualisiert, so dass keine zusätzlichen Berechnungen erforderlich sind. Da sich das Modell auf die historisch schwierigeren Stichproben konzentriert, sind die resultierenden Modelle auch genauer.Das oben beschriebene Verfahren kann, wenn es als Algorithmus formuliert wird, als eine Form des Curriculum-Lernens angesehen werden, daher nennen wir unseren Rahmen DIH-Curriculum-Lernen (oder DIHCL).Die Vorteile von DIHCL im Vergleich zu anderen Curriculum-Lernansätzen sind: (1) DIHCL erfordert keine zusätzlichen Inferenzschritte über die nicht von DIHCL ausgewählten Daten in jeder Epoche, (2) die dynamische Instanzhärte ist im Vergleich zur statischen Instanzhärte (z.B., (2) ist die dynamische Instanzhärte im Vergleich zur statischen Instanzhärte (z.B. momentaner Verlust) stabiler, da sie Informationen über die gesamte Trainingsgeschichte bis zum aktuellen Zeitpunkt integriert.Unter bestimmten mathematischen Annahmen formulieren wir das Problem von DIHCL als das Finden eines Curriculums, das eine Multi-Set-Funktion $f(\cdot)$ maximiert, und leiten eine Näherungsschranke für ein DIH-produziertes Curriculum relativ zum optimalen Curriculum ab. Empirisch gesehen übertreffen DIHCL-trainierte DNNs die zufällige Mini-Batch-SGD und andere kürzlich entwickelte Curriculum-Lernmethoden in Bezug auf Effizienz, Konvergenz im Frühstadium und endgültige Leistung erheblich. Dies wird durch das Training mehrerer hochmoderner DNNs auf 11 modernen Datensätzen gezeigt.
Dieses Papier untersucht viele unmittelbare Verbindungen zwischen adaptiver Steuerung und maschinellem Lernen, sowohl durch gemeinsame Update-Gesetze als auch gemeinsame Konzepte.adaptive Steuerung als ein Feld hat sich auf mathematische Strenge und garantierte Konvergenz konzentriert.die raschen Fortschritte im maschinellen Lernen auf der anderen Seite haben eine Fülle von neuen Techniken und Probleme für das Lernen gebracht.dieses Papier erhellt viele der zahlreichen gemeinsamen Verbindungen zwischen den beiden Bereichen, so dass die Ergebnisse aus beiden können zusammen genutzt werden, um neue Probleme zu lösen.insbesondere ein spezifisches Problem im Zusammenhang mit Lernen höherer Ordnung wird durch Erkenntnisse aus diesen Überschneidungen erhalten gelöst.
Die rekurrente Faltung (RC) teilt sich dieselben Faltungskerne und rollt sie mehrfach aus, was ursprünglich zur Modellierung von Zeit-Raum-Signalen vorgeschlagen wurde.Wir schlagen vor, dass RC als eine Modellkompressionsstrategie für tiefe faltige neuronale Netze betrachtet werden kann.RC reduziert die Redundanz über Schichten hinweg und ist komplementär zu den meisten existierenden Modellkompressionsansätzen.Allerdings kann die Leistung eines RC-Netzes nicht mit der Leistung des entsprechenden Standardnetzes mithalten, d.h. mit der gleichen Tiefe, aber unabhängigen Faltungskernen.  In diesem Papier schlagen wir eine einfache Variante vor, die RC-Netze verbessert: Die Batch-Normalisierungsschichten eines RC-Moduls werden unabhängig (nicht gemeinsam) für verschiedene Abrollschritte gelernt.Wir geben Einblicke, warum dies funktioniert.Experimente an CIFAR zeigen, dass das Abrollen einer Faltungsschicht in mehreren Schritten die Leistung verbessern kann und somit indirekt eine Rolle bei der Modellkompression spielt.
Die visuelle Welt ist groß und vielfältig, aber ihre Variationen teilen sich in strukturierte und unstrukturierte Faktoren auf.strukturierte Faktoren, wie Maßstab und Orientierung, lassen klare Theorien und effizientes Repräsentationsdesign zu.unstrukturierte Faktoren, wie z.B. was eine Katze wie eine Katze aussehen lässt, sind zu kompliziert, um sie analytisch zu modellieren, und erfordern daher das Lernen von Freiformrepräsentationen. Unsere Experimente zur dynamischen Struktur, bei denen die strukturierten Filter mit der Eingabe variieren, entsprechen der Genauigkeit der dynamischen Inferenz mit mehr Freiheitsgraden und verbessern gleichzeitig die Effizienz.(Die vollständige Ausgabe finden Sie unter https://arxiv.org/abs/1904.11487.)
Es ist allgemein bekannt, dass gut konzipierte Störungen dazu führen können, dass moderne maschinelle Lernklassifizierer ein Bild falsch beschriften, und zwar mit ausreichend kleinen Störungen, die für das menschliche Auge nicht wahrnehmbar sind. Um dies zu erreichen, schlagen wir einen Algorithmus namens LabelFool vor, der ein Ziel-Label identifiziert, das dem Ground-Truth-Label ähnlich ist, und eine Störung des Bildes für dieses Ziel-Label findet.Wir finden zunächst das Ziel-Label für ein Eingabebild durch ein Wahrscheinlichkeitsmodell und bewegen dann die Eingabe im Merkmalsraum in Richtung des Ziel-Labels.Subjektive Studien auf ImageNet zeigen, dass unser Angriff im Label-Raum für menschliche Beobachter viel weniger erkennbar ist, während objektive experimentelle Ergebnisse auf ImageNet zeigen, dass wir eine ähnliche Leistung im Bildraum sowie Angriffsraten wie die modernsten Angriffsalgorithmen beibehalten.
Für diese Studie wird eine Sammlung von Boden Trittschall Datensatz mit einem einzigen Mikrofon aufgezeichnet.Geräuscharten / Positionen sind auf der Grundlage eines Berichts von der Floor Management Center unter Korea Environmental Corporation ausgewählt.mit einem Faltungsneuronale Netze basierte Klassifikator, der Trittschall-Signale in log-skalierte Mel-Spektrogramme konvertiert werden in Lärm-Typen oder Positionen klassifiziert.auch unser Modell auf einem Standard-Umgebungsgeräusche Datensatz ESC-50 zu zeigen, Erweiterbarkeit auf Umgebungsgeräusche Klassifizierung bewertet.
Aufzeichnungen von neuronalen Schaltkreisen im Gehirn zeigen eine außerordentliche dynamische Vielfalt und hohe Variabilität, während Techniken zur Dimensionalitätsreduktion im Allgemeinen niedrigdimensionale Strukturen aufdecken, die diesen Dynamiken zugrunde liegen.Was bestimmt die Dimensionalität der Aktivität in neuronalen Schaltkreisen? Wir stellen fest, dass RNNs in Abhängigkeit von der Dynamik des ursprünglichen Netzwerks lernen, die Dimensionalität in einer Weise zu erhöhen und zu reduzieren, die den Anforderungen der Aufgabe entspricht. Diese Erkenntnisse werfen ein Licht auf die grundlegenden dynamischen Mechanismen, durch die neuronale Netzwerke Aufgaben mit robusten Repräsentationen lösen, die sich auf neue Fälle verallgemeinern lassen.
Die Domänenanpassung befasst sich mit dem häufigen Problem, dass die Zielverteilung, die unsere Testdaten generiert, von der Quell- (Trainings-) Verteilung abweicht. Während ohne Annahmen eine Domänenanpassung nicht möglich ist, ermöglichen strenge Bedingungen, z. B. Kovariaten- oder Label-Shift, prinzipielle Algorithmen.Kürzlich vorgeschlagene domänenadversarische Ansätze bestehen aus der Angleichung von Quell- und Zielkodierungen, wobei dieser Ansatz häufig mit der Minimierung von zwei (von drei) Termen in einer theoretischen Schranke für den Zielfehler begründet wird. Wir schlagen eine asymmetrisch-relaxierte Verteilungsanpassung vor, einen neuen Ansatz, der einige Beschränkungen von Standard-Domain-Adversarial-Algorithmen überwindet. Darüber hinaus charakterisieren wir präzise Annahmen, unter denen unser Algorithmus theoretisch prinzipientreu ist, und demonstrieren empirische Vorteile sowohl auf synthetischen als auch realen Datensätzen.
Um zu verhindern, dass Sequenz-zu-Sequenz-Modelle (seq2seq) zu Sprachmodellen degenerieren, und um den zu generierenden langen Text besser kontrollieren zu können, schlagen wir einen hierarchischen Generierungsansatz vor, der zunächst eine Skizze von mittlerer Länge auf der Grundlage der Zusammenfassung generiert und dann den Artikel durch Anreicherung der generierten Skizze vervollständigt. Um die Diskrepanz zwischen der ``Orkelskizze'', die während des Trainings verwendet wird, und der verrauschten Skizze, die während der Inferenz generiert wird, zu mildern, schlagen wir ein gemeinsames End-to-End-Training vor, das auf Multi-Agenten-Verstärkungslernen basiert. Für die Evaluierung verwenden wir Korpora mit Textzusammenfassungen, indem wir deren Eingaben und Ausgaben umkehren, und führen eine neuartige Evaluierungsmethode ein, die ein Zusammenfassungssystem einsetzt, um den generierten Artikel zusammenzufassen und seine Übereinstimmung mit der ursprünglichen Eingabezusammenfassung zu testen.Experimente zeigen, dass der von uns vorgeschlagene hierarchische Generierungsansatz einen kohärenten und relevanten Artikel auf der Grundlage der gegebenen Zusammenfassung generieren kann, was zu signifikanten Verbesserungen gegenüber herkömmlichen seq2seq-Modellen führt.
Beim Training eines tiefen neuronalen Netzes für die überwachte Bildklassifizierung kann man grob zwischen zwei Arten von latenten Merkmalen von Bildern unterscheiden, die die Klassifizierung der Klasse Y vorantreiben werden. (2016) können wir die Merkmale grob in die folgenden Klassen einteilen:(i) "Kern"- oder "bedingungslos invariante" Merkmale X^ci, deren Verteilung P(X^ci | Y) sich über Domänen hinweg nicht wesentlich ändert, und(ii) "Stil"- oder "orthogonale" Merkmale X^orth, deren Verteilung P(X^orth | Y) sich über Domänen hinweg wesentlich ändern kann. Wir versuchen, uns gegen zukÃ?nftige gegnerische DomÃ?nenverschiebungen zu schÃ?tzen, indem wir idealerweise nur die â€žbedingt invariantenâ€œ Merkmale fÃ?r die Klassifizierung verwenden.â€œ Im Gegensatz zu frÃ?heren Arbeiten gehen wir davon aus, dass die DomÃ?ne selbst nicht beobachtet wird und daher eine latente Variable ist, so dass wir die VerteilungsÃ?nderung der Merkmale Ã?ber verschiedene DomÃ?nen nicht direkt sehen kÃ¶nnen. Wir gehen jedoch davon aus, dass wir manchmal eine so genannte Identifikator- oder ID-Variable beobachten können, z.B. wenn wir wissen, dass zwei Bilder dieselbe Person zeigen, wobei sich ID auf die Identität der Person bezieht.Bei der Datenerweiterung erzeugen wir mehrere Bilder aus demselben Originalbild, wobei sich ID auf das entsprechende Originalbild bezieht. Wir bieten einen kausalen Rahmen für das Problem, indem wir die ID-Variable zum Modell von Gong et al. (2016) hinzufügen. Wenn zwei oder mehr Stichproben dieselbe Klasse und Kennung (Y, ID)=(y,i) haben, dann behandeln wir diese Stichproben als kontrafaktisch unter verschiedenen Stilinterventionen auf den orthogonalen oder Stilmerkmalen.Mit diesem Ansatz der Gruppierung nach ID regulieren wir das Netzwerk, um eine nahezu konstante Ausgabe über Stichproben hinweg zu liefern, die dieselbe ID haben, indem wir mit einem geeigneten Graph-Laplacian bestrafen. Es wird gezeigt, dass dies die Leistung in Bereichen, in denen sich die Bildqualität, die Helligkeit, die Farbe und komplexere Veränderungen wie Bewegung und Körperhaltung ändern, erheblich verbessert und dass es Verbindungen zu Fragen der Interpretierbarkeit, Fairness und des Transferlernens gibt.
Gradienten-basierte Meta-Learning-Algorithmen benötigen mehrere Schritte des Gradientenabstiegs, um sich an neu eintreffende Aufgaben anzupassen - ein Prozess, der mit zunehmender Anzahl von Stichproben immer kostspieliger wird.  Darüber hinaus leiden die Gradientenaktualisierungen unter verschiedenen Rauschquellen, was zu einer verminderten Leistung führt.   In dieser Arbeit schlagen wir einen Meta-Lernalgorithmus vor, der mit der GradiEnt Component COrrections, kurz GECCO-Zelle, ausgestattet ist und eine multiplikative korrigierende Low-Rank-Matrix erzeugt, die (nach Vektorisierung) die geschätzten Gradienten korrigiert. GECCO enthält ein einfaches decoderähnliches Netzwerk mit lernbaren Parametern, ein Aufmerksamkeitsmodul und einen so genannten Kontexteingabeparameter. Der Kontextparameter von GECCO wird aktualisiert, um einen Low-Rank-Korrekturterm für die Netzgradienten zu erzeugen.   Infolgedessen sind für das Meta-Lernen nur wenige Aktualisierungen der Gradienten erforderlich, um eine neue Aufgabe zu absorbieren (oft reicht eine einzige Aktualisierung in einem Szenario mit wenigen Aufnahmen aus). WÃ?hrend frÃ?here AnsÃ?tze dieses Problem durch VerÃ?nderung der Lernraten, Faktorisierung von Netzwerkparametern oder direktes Lernen von Merkmalskorrekturen aus Merkmalen und/oder Gradienten angehen, ist GECCO ein Generator-Ã?hnliches GerÃ?t, das elementweise Gradientenkorrekturen durchfÃ?hrt, ohne die Merkmale und/oder Gradienten direkt â€˜beobachtenâ€™ zu mÃ?ssen.  Wir zeigen, dass unser GECCO (i) das Lernen beschleunigt, (ii) robuste Korrekturen der durch Rauschen korrumpierten Gradienten durchfÃ?hrt und (iii) zu bemerkenswerten Verbesserungen gegenÃ?ber bestehenden gradientenbasierten Meta-Lernalgorithmen fÃ?hrt.
Diskriminative Modelle zur Beantwortung von Fragen können sich zu sehr an oberflächliche Verzerrungen in Datensätzen anpassen, da ihre Verlustfunktion gesättigt ist, wenn ein Hinweis die Antwort wahrscheinlich macht.  Wir fÃ?hren generative Modelle der gemeinsamen Verteilung von Fragen und Antworten ein, die darauf trainiert sind, die gesamte Frage zu erklÃ?ren und nicht nur zu beantworten.â€œ Unser Modell fÃ?r die Beantwortung von Fragen (QA) wird implementiert, indem ein Prior Ã?ber die Antworten und ein konditionales Sprachmodell erlernt wird, um die Frage angesichts der Antwort zu generieren â€" dies ermÃ¶glicht skalierbares und interpretierbares Many-Hop-Reasoning, wÃ?hrend die Frage Wort fÃ?r Wort generiert wird.  Unser Modell erreicht eine wettbewerbsfÃ?hige Leistung mit spezialisierten diskriminativen Modellen bei den SQUAD- und CLEVR-Benchmarks, was darauf hindeutet, dass es sich um eine allgemeinere Architektur fÃ?r SprachverstÃ?ndnis und Schlussfolgerungen handelt als frÃ?here Arbeiten.â€œ Das Modell verbessert die Generalisierung sowohl von voreingenommenen Trainingsdaten als auch von gegnerischen Testdaten erheblich und erreicht einen neuen Spitzenwert bei ADVERSARIAL SQUAD.
Wir schlagen die Aktivierungsfunktion Displaced Rectifier Linear Unit (DReLU) vor, indem wir vermuten, dass die Erweiterung der Identitätsfunktion von ReLU auf den dritten Quadranten die Kompatibilität mit der Batch-Normalisierung verbessert. Darüber hinaus haben wir mit statistischen Tests die Auswirkungen der Verwendung verschiedener Aktivierungsfunktionen (ReLU, LReLU, PReLU, ELU und DReLU) auf die Lerngeschwindigkeit und die Testgenauigkeit von standardisierten VGG- und Residual Networks-Modellen verglichen. Die Ergebnisse zeigen, dass DReLU das Lernen in allen Modellen und Datensätzen beschleunigt und dass die statistisch signifikanten Leistungsbewertungen (p<0,05) zeigen, dass DReLU die Testgenauigkeit von ReLU in allen Szenarien verbessert. Darüber hinaus zeigte DReLU in allen Experimenten eine bessere Testgenauigkeit als jede andere getestete Aktivierungsfunktion, mit einer Ausnahme, in der es die zweitbeste Leistung zeigte.
Die explizite Kodierung der Skaleninformation in die von einem neuronalen Faltungsnetzwerk (CNN) gelernte Repräsentation ist für viele Sehaufgaben von Vorteil, insbesondere wenn es sich um multiskalige Eingangssignale handelt.wir untersuchen in dieser Arbeit eine skalenäquivariante CNN-Architektur mit gemeinsamen Faltungen über den Raum und die Skalierungsgruppe, die sich als ausreichend und notwendig erweist, um skalenäquivariante Repräsentationen zu erreichen. Ein weiterer Vorteil der abgeschnittenen Filterexpansion ist die verbesserte Deformationsrobustheit der äquivarianten Repräsentation.Numerische Experimente zeigen, dass das vorgeschlagene skalenäquivariante neuronale Netzwerk mit zerlegten Faltungsfiltern (ScDCFNet) eine signifikant verbesserte Leistung in der Multiskalen-Bildklassifikation und eine bessere Interpretierbarkeit als reguläre CNNs bei einer reduzierten Modellgröße erreicht.
In diesem Beitrag diagnostizieren wir tiefe neuronale Netze für die 3D-Punktwolkenverarbeitung, um den Nutzen verschiedener Netzarchitekturen zu untersuchen. Wir stellen eine Reihe von Hypothesen über die Auswirkungen spezifischer Netzarchitekturen auf die Darstellungskapazität von DNNs auf.Um die Hypothesen zu beweisen, entwerfen wir fünf Metriken, um verschiedene Arten von DNNs aus den folgenden Perspektiven zu diagnostizieren: Informationsverwerfung, Informationskonzentration, Rotationsrobustheit, gegnerische Robustheit und Nachbarschaftsinkonsistenz. Wir führen vergleichende Studien auf der Grundlage dieser Metriken durch, um die Hypothesen zu verifizieren, die neue Erkenntnisse über die Architektur neuronaler Netze liefern können.Experimente haben die Wirksamkeit unserer Methode bewiesen.
In dieser Arbeit konstruieren wir flexible gemeinsame Verteilungen aus niedrigdimensionalen bedingten semi-impliziten Verteilungen, wobei die explizite Definition der Struktur der Annäherung es ermöglicht, die untere Variationsschranke enger zu machen, was zu genaueren Schlussfolgerungen führt.
Es hat sich gezeigt, dass das Nachahmungslernen von menschlichen Expertendemonstrationen bei anspruchsvollen Verstärkungslernproblemen mit spärlichen Umweltbelohnungen sehr hilfreich ist, aber es ist sehr schwierig, ähnliche Erfolge zu erzielen, ohne sich auf Expertendemonstrationen zu verlassen. Neuere Arbeiten zum Selbstimitationslernen haben gezeigt, dass die Nachahmung der eigenen guten Erfahrungen des Agenten indirekt die Erkundung in einigen Umgebungen vorantreiben könnte, aber diese Methoden führen oft zu suboptimalem und kurzsichtigem Verhalten.Um dieses Problem anzugehen, argumentieren wir, dass die Erkundung in verschiedene Richtungen durch die Nachahmung verschiedener Trajektorien, anstatt sich auf begrenzte gute Trajektorien zu konzentrieren, für schwierige Erkundungsaufgaben wünschenswert ist. Wir schlagen eine neue Methode zum Erlernen einer trajektorienabhängigen Strategie vor, um verschiedene Trajektorien aus den eigenen vergangenen Erfahrungen des Agenten zu imitieren, und zeigen, dass eine solche Selbstimitation hilft, kurzsichtiges Verhalten zu vermeiden und die Chance zu erhöhen, eine global optimale Lösung für schwierige Erkundungsaufgaben zu finden, insbesondere wenn es irreführende Belohnungen gibt. Unsere Methode übertrifft bestehende Selbstimitations-Lern- und zählbasierte Explorationsmethoden bei verschiedenen schwierigen Explorationsaufgaben mit lokalen Optima signifikant. Insbesondere bei Montezumas Revenge haben wir eine State-of-the-Art-Punktzahl von mehr als 20.000 Punkten erreicht, ohne Expertendemonstrationen zu verwenden oder auf beliebige Zustände zurückzusetzen.
Wir stellen eine Methode vor, mit der neuronale Netze mit großer Kapazität mit deutlich verbesserter Genauigkeit und geringeren dynamischen Rechenkosten trainiert werden können, indem die Deep-Learning-Architektur auf einer feinkörnigen Ebene gesteuert wird: Einzelne Faltungsmaps werden abhängig von den Merkmalen im Netz ein- und ausgeschaltet. Um dies zu erreichen, führen wir eine neue Residual-Block-Architektur ein, die Faltungs-Kanäle in einer feinkörnigen Art und Weise gattert.Wir führen auch ein allgemein anwendbares Tool Batch-Shaping ein, das die marginalen aggregierten Posterioren von Merkmalen in einem neuronalen Netzwerk an eine vorher festgelegte Prior-Verteilung anpasst.Wir verwenden diese neuartige Technik, um Gatter zu erzwingen, die stärker von den Daten abhängig sind. Unsere Ergebnisse zeigen, dass unsere Methode große Architekturen bedingt verschlanken kann, so dass die durchschnittlichen Rechenkosten für die Daten auf dem Niveau einer kleineren Architektur liegen, jedoch mit höherer Genauigkeit. Insbesondere bei ImageNet erreichen unsere ResNet50 und ResNet34 gated networks bei ähnlicher Komplexität eine Top-1-Genauigkeit von 74,60 % bzw. 72,55 % im Vergleich zu 69,76 % des ResNet18-Basismodells. Wir zeigen auch, dass die resultierenden Netzwerke automatisch lernen, mehr Merkmale für schwierige Beispiele und weniger Merkmale für einfache Beispiele zu verwenden.
Um die Lücke zwischen Deep Learning und symbolischer KI zu schließen, stellen wir eine neuartige End-to-End-Architektur für neuronale Netze vor, die lernt, propositionale Repräsentationen mit einer explizit relationalen Struktur aus rohen Pixeldaten zu bilden.um die Architektur zu evaluieren und zu analysieren, stellen wir eine Familie von einfachen visuellen relationalen Argumentationsaufgaben unterschiedlicher Komplexität vor. Wir zeigen, dass die vorgeschlagene Architektur, wenn sie auf einem Curriculum solcher Aufgaben vortrainiert ist, lernt, wiederverwendbare Repräsentationen zu generieren, die im Vergleich zu einer Reihe von Basisarchitekturen das anschließende Lernen auf zuvor ungesehenen Aufgaben besser erleichtern.Die Arbeitsweise eines erfolgreich trainierten Modells wird visualisiert, um die Funktionsweise der Architektur zu beleuchten.
In natÃ?rlicher Sprache Inferenz, die Semantik einiger WÃ¶rter nicht auf die Inferenz.Solche Informationen werden als oberflÃ?chlich und fÃ?hrt overfitting.How kÃ¶nnen wir darstellen und verwerfen solche oberflÃ?chlichen Informationen?In diesem Papier, verwenden wir erster Ordnung Logik (FOL) - eine klassische Technik aus der Bedeutung ReprÃ?sentation Sprache â€" zu erklÃ?ren, welche Informationen sind oberflÃ?chlich fÃ?r ein gegebenes Satzpaar.Diese ErklÃ?rung auch vor, zwei induktive Verzerrungen entsprechend ihrer Eigenschaften.Wir schlugen vor, ein neuronales Netzwerk-basierten Ansatz, der die beiden induktiven Verzerrungen.Wir erhalten erhebliche Verbesserungen Ã?ber umfangreiche Experimente.
Wir schlagen einen Ansatz für die Ausbildung von maschinellen Lernmodellen, die fair in dem Sinne, dass ihre Leistung ist invariant unter bestimmten Störungen auf die features.for example, die Leistung eines Lebenslauf-Screening-System sollte invariant unter Änderungen an den Namen des Antragstellers.we formalisieren diese intuitive Vorstellung von Fairness, indem sie mit dem ursprünglichen Begriff der individuellen Fairness, die von Dwork et al und zeigen, dass der vorgeschlagene Ansatz erreicht diese Vorstellung von Fairness.we auch zeigen, die Wirksamkeit des Ansatzes auf zwei maschinelles Lernen Aufgaben, die anfällig für Geschlecht und Rasse biases sind.
In diesem Papier schlagen wir ein Seed-Augment-Train/Transfer (SAT)-Framework vor, das ein Verfahren zur Erzeugung eines synthetischen Seed-Bilddatensatzes für Sprachen mit verschiedenen Zahlensystemen unter Verwendung frei verfügbarer, offener Schriftdateidatensätze enthält. Dieser Seed-Bilddatensatz wird dann erweitert, um einen rein synthetischen Trainingsdatensatz zu erstellen, der wiederum verwendet wird, um ein tiefes neuronales Netzwerk zu trainieren und auf einem gehaltenen realen handgeschriebenen Zifferndatensatz zu testen, der fünf indische Schriften, Kannada, Tamil, Gujarati, Malayalam und Devanagari umfasst. Wir zeigen die Wirksamkeit dieses Ansatzes sowohl in qualitativer Hinsicht, indem wir ein Boundary-seeking GAN (BGAN) trainieren, das realistische Ziffernbilder in den fünf Sprachen erzeugt, als auch in qualitativer Hinsicht, indem wir ein CNN, das auf den synthetischen Daten trainiert wurde, an den realen Datensätzen testen.
Ein besonders erfolgreicher Algorithmus ist Model Agnostic Meta-Learning (MAML), eine Methode, die aus zwei Optimierungsschleifen besteht, wobei die äußere Schleife eine Meta-Initialisierung findet, von der die innere Schleife effizient neue Aufgaben lernen kann. Trotz der Beliebtheit von MAML bleibt eine grundsätzliche Frage offen: Ist die Effektivität von MAML darauf zurückzuführen, dass die Meta-Initialisierung für schnelles Lernen (große, effiziente Änderungen in den Repräsentationen) vorbereitet ist, oder auf die Wiederverwendung von Merkmalen, da die Meta-Initialisierung bereits qualitativ hochwertige Merkmale enthält?Wir untersuchen diese Frage anhand von Ablationsstudien und Analysen der latenten Repräsentationen und finden heraus, dass die Wiederverwendung von Merkmalen der dominierende Faktor ist. Dies führt zum ANIL-Algorithmus (Almost No Inner Loop), einer Vereinfachung von MAML, bei der wir die innere Schleife für alle bis auf den (aufgabenspezifischen) Kopf des zugrunde liegenden neuronalen Netzes entfernen.ANIL entspricht der Leistung von MAML bei Benchmark-Bildern mit wenigen Aufnahmen und RL und bietet rechnerische Verbesserungen gegenüber MAML. Wir untersuchen weiter die genauen Beiträge des Kopfes und des Körpers des Netzwerks und zeigen, dass die Leistung bei den Testaufgaben vollständig von der Qualität der gelernten Merkmale bestimmt wird und wir sogar den Kopf des Netzwerks (den NIL-Algorithmus) entfernen können.Wir schließen mit einer Diskussion der Frage des schnellen Lernens gegenüber der Wiederverwendung von Merkmalen für Meta-Lernalgorithmen im Allgemeinen.
Mit wachsendem Interesse an immer komplexeren Modellen besteht ein Bedarf an Techniken, die helfen, den gesamten Trainingsaufwand zu reduzieren.Während inkrementelles Training erhebliche Zeit und Kosten sparen kann, indem ein bestehendes Modell auf einer kleinen Teilmenge von Daten trainiert wird, gibt es nur wenige Arbeiten, die sich mit der Frage befassen, wann inkrementelles Training eine adäquate Modellleistung im Vergleich zu vollständigem Neutraining liefert. Wir stellen einen methodenunabhängigen Algorithmus zur Verfügung, um zu entscheiden, wann inkrementelles und wann volles Training erforderlich ist, und nennen diese Einstellung von nicht-deterministischem vollem oder inkrementellem Training ``Mixed Setting Training''. Bei der Evaluierung von Slot-Filling-Aufgaben haben wir festgestellt, dass dieser Algorithmus einen begrenzten Fehler liefert, katastrophales Vergessen vermeidet und zu einer signifikanten Beschleunigung im Vergleich zu einer Politik des vollen Trainings führt.
Neuronale Netze haben sich bei vielen logischen Aufgaben bewährt, die erfahrungsgemäß spezielle Netzstrukturen erfordern, z.B., Theoretisch gibt es nur ein begrenztes Verständnis darüber, warum und wann eine Netzwerkstruktur besser generalisiert als andere, gleich ausdrucksstarke Strukturen. Wir entwickeln einen Rahmen, um zu charakterisieren, welche Schlussfolgerungsaufgaben ein Netzwerk gut lernen kann, indem wir untersuchen, wie gut seine Struktur mit der algorithmischen Struktur des relevanten Schlussfolgerungsprozesses übereinstimmt. Dieser Rahmen erklärt den empirischen Erfolg populärer Argumentationsmodelle und zeigt ihre Grenzen auf.Wir vereinen scheinbar unterschiedliche Argumentationsaufgaben, wie intuitive Physik, visuelle Fragebeantwortung und kürzeste Pfade, durch die Linse eines leistungsstarken algorithmischen Paradigmas, der dynamischen Programmierung (DP).Wir zeigen, dass GNNs DP lernen können und somit diese Aufgaben lösen.Bei mehreren Argumentationsaufgaben stimmt unsere Theorie mit empirischen Ergebnissen überein.
Die Untersuchung spezifischer Zell-Zell-Interaktionen hat das Potenzial, nicht nur das Verständnis der Tumorentstehung zu erweitern, sondern auch das klinische Management von Patientenreaktionen auf Krebsimmuntherapien zu steuern. Ein neues bildgebendes Verfahren zur Erforschung von Zell-Zell-Interaktionen, das Multiplex-Ionenstrahl-Imaging per Time-of-Flight (MIBI-TOF), ermöglicht die Quantifizierung von Zellen anhand von 36 verschiedenen Proteinmarkern bei subzellulären Auflösungen in situ in Form von hochauflösenden Multiplex-Bildern.1 Um die MIBI-Bilder zu untersuchen, schlagen wir ein GAN für Multiplex-Daten mit protein-spezifischer Aufmerksamkeit vor. Durch die Konditionierung der Bilderzeugung auf Zelltypen, Größen und Nachbarschaften durch semantische Segmentierungskarten sind wir in der Lage zu beobachten, wie diese Faktoren Zell-Zell-Interaktionen gleichzeitig in verschiedenen Proteinkanälen beeinflussen. Unser Modell, das Zell-Zell-Interaktions-GAN (CCIGAN), übertrifft bestehende Bildsynthesemethoden in allen konventionellen Maßstäben und schneidet bei biologisch motivierten Metriken deutlich besser ab als diese.
Modelle des maschinellen Lernens für die Beantwortung von Fragen (QA), bei denen der Lernende angesichts einer Frage und einer Passage einen Bereich in der Passage als Antwort auswählen muss, sind bekanntermaßen brüchig. Ein vielversprechender neuer Ansatz für die QS zerlegt die Aufgabe in zwei Stufen: (i) Auswahl relevanter Sätze aus dem Text und (ii) Auswahl eines Bereichs aus diesen Sätzen, wenn der Satzselektor den fehlerhaften Satz ausschließt, ist der nachgeschaltete Bereichsselektor robust. Während neuere Arbeiten die potentielle Robustheit der zweistufigen QA angedeutet haben, wurden diese Methoden unseres Wissens noch nie explizit mit adversarialem Training kombiniert. Dieses Papier bietet eine gründliche empirische Untersuchung der adversarialer Robustheit und zeigt, dass obwohl der zweistufige Ansatz hinter der einstufigen Spannenauswahl zurückbleibt, adversariales Training seine Leistung signifikant verbessert, was zu einer Verbesserung von über 22 Punkten im F1-Score gegenüber dem adversarial trainierten einstufigen Modell führt.
Das Ziel dieser Studie ist die Einführung eines formalen Rahmens für die Analyse und Synthese von Fahrerassistenzsystemen. Sie wendet formale Methoden auf die Verifikation eines stochastischen menschlichen Fahrermodells an, das mit Hilfe der kognitiven Architektur ACT-R erstellt wurde, und stärkt dann die Sicherheit in teilautonomen Fahrzeugen durch den Entwurf von beweisbar korrekten Fahrerassistenzsystemen. Zu den Hauptbeiträgen gehören die Integration probabilistischer ACT-R-Modelle in die formale Analyse teilautonomer Systeme und eine Abstraktionstechnik, die eine endliche Repräsentation eines großdimensionalen, kontinuierlichen Systems in Form eines Markov-Modells ermöglicht.Die Wirksamkeit der Methode wird in mehreren Fallstudien unter verschiedenen Bedingungen illustriert.
Im Gegensatz zum älteren Schriftsystem des 19. Jahrhunderts enthält die moderne hawaiianische Rechtschreibung Zeichen für lange Vokale und Glottalstopps, die etwa ein Drittel der Phoneme im Hawaiianischen ausmachen, so dass ihre Einbeziehung einen großen Unterschied für das Leseverständnis und die Aussprache bedeutet.Allerdings ist die Transliteration zwischen älteren und neueren Texten eine mühsame Aufgabe, wenn sie manuell durchgeführt wird.Wir stellen zwei verwandte Methoden vor, um dieses Transliterationsproblem automatisch zu lösen, da es nicht genügend Daten gab, um ein End-to-End Deep Learning-Modell zu trainieren. Wir stellen zwei verwandte Methoden vor, um das Transliterationsproblem automatisch zu lösen, wenn nicht genügend Daten vorhanden sind, um ein rekurrentes neuronales Netzwerk (RNN) zu trainieren. Eine Methode wird durchgängig mit Finite-State-Transducern (FST) implementiert, die andere ist ein hybrider Deep-Learning-Ansatz, der eine FST mit einem rekurrenten neuronalen Netzwerk (RNN) kombiniert.
In vielen realen Situationen muss ein Lernmodell eine "few-shot"-Klassifikation durchführen, d.h. es muss lernen, Beispiele aus unbekannten Klassen zu klassifizieren, indem es nur einige wenige markierte Beispiele pro Klasse verwendet, und es sollte, um sicher eingesetzt werden zu können, in der Lage sein, "out-of-distribution"-Eingaben zu erkennen, d.h. Beispiele, die keiner der Klassen angehören. In dieser Arbeit schlagen wir Aufgaben für die Erkennung von Out-of-Distribution in der few-shot Umgebung vor und erstellen Benchmark-Datensätze, die auf vier populären few-shot Klassifizierungsdatensätzen basieren.  Zusammenfassend lässt sich sagen, dass wir mit Hilfe von Standardmetriken auf neuen Benchmark-Datensätzen grundlegende Ergebnisse zur Erkennung von Out-of-Distribution erzielen und mit den von uns vorgeschlagenen Methoden bessere Ergebnisse erzielen.
Während moderne generative Modelle in der Lage sind, visuell ansprechende Bilder mit hoher Wiedergabetreue zu synthetisieren, bleibt die erfolgreiche Generierung von Beispielen, die für Erkennungsaufgaben nützlich sind, ein schwer zu fassendes Ziel.Zu diesem Zweck ist unsere wichtigste Erkenntnis, dass die Beispiele synthetisiert werden sollten, um die Entscheidungsgrenzen des Klassifizierers wiederherzustellen, die aus einer großen Menge von realen Beispielen gelernt wurden.Konkret behandeln wir einen Klassifizierer, der auf synthetischen Beispielen trainiert wurde, als "Schüler" und einen Klassifizierer, der auf realen Beispielen trainiert wurde, als "Lehrer". Um die potenzielle Kluft zwischen Schüler- und Lehrerklassifikatoren zu verringern, schlagen wir außerdem vor, das Wissen schrittweise zu destillieren, indem entweder der Lehrer allmählich gestärkt oder der Schüler geschwächt wird. Wir demonstrieren die Verwendung unseres modellagnostischen Destillationsansatzes, um mit Datenknappheit umzugehen, indem wir die Lernleistung bei den miniImageNet- und ImageNet1K-Benchmarks mit wenigen Aufnahmen deutlich verbessern.
Tiefe neuronale Netze bieten für viele interessante Anwendungen die beste Leistung, sind aber bekanntermaßen anfällig für negative Beispiele, die durch die Anwendung kleiner, aber bösartiger Störungen auf die ursprünglichen Eingaben gebildet werden, und können zudem modellübergreifend übertragen werden: Negative Beispiele, die für ein bestimmtes Modell generiert werden, führen oft andere, nicht gesehene Modelle in die Irre, was der Gegner für Angriffe auf die eingesetzten Blackbox-Systeme nutzen kann. In dieser Arbeit zeigen wir, dass die Störung in zwei Komponenten zerlegt werden kann: eine modellspezifische und eine datenabhängige, wobei die letztere hauptsächlich zur Übertragbarkeit beiträgt. Motiviert durch dieses Verständnis schlagen wir vor, die Störungsbeispiele unter Verwendung des rauschreduzierten Gradienten (NRG) zu erstellen, der die datenabhängige Komponente annähert. Experimente mit verschiedenen Klassifizierungsmodellen, die auf ImageNet trainiert wurden, zeigen, dass der neue Ansatz die Übertragbarkeit drastisch verbessert und dass Modelle mit geringer Kapazität eine stärkere Angriffsfähigkeit haben als Modelle mit hoher Kapazität, vorausgesetzt, sie haben eine vergleichbare Testleistung.  Diese Erkenntnisse führen zu einer prinzipiellen Art und Weise, gegnerische Beispiele mit hohen Erfolgsquoten zu konstruieren, und könnten uns eine Anleitung für die Entwicklung effektiver Verteidigungsansätze gegen Blackbox-Angriffe liefern.
Wir stellen den iterativen Zwei-Pass-Dekompositionsfluss vor, um bestehende Faltungsneuronale Netze (CNNs) zu beschleunigen.  Der vorgeschlagene Rangauswahlalgorithmus kann effektiv die richtigen Ränge der Zielfaltungsschichten für die Annäherung mit niedrigem Rang bestimmen. Unsere CP-Zerlegung in zwei Durchgängen hilft, das Instabilitätsproblem zu vermeiden. Der iterative Fluss macht die Zerlegung der tieferen Netze systematisch. Die Versuchsergebnisse zeigen, dass VGG16 mit einer 6,2-fachen gemessenen Beschleunigung beschleunigt werden kann, während der Genauigkeitsabfall nur 1,2 % beträgt.
Wir stellen LiPopt vor, ein polynomiales Optimierungsverfahren zur Berechnung von immer engeren oberen Schranken für die Lipschitz-Konstante von neuronalen Netzen. Die zugrundeliegenden Optimierungsprobleme laufen entweder auf lineare (LP) oder semidefinite (SDP) Programmierung hinaus. Wir zeigen, wie man die spärliche Konnektivität eines Netzes nutzen kann, um die Komplexität der Berechnung erheblich zu reduzieren. Wir führen Experimente an Netzwerken mit zufälligen Gewichten sowie an Netzwerken, die auf MNIST trainiert wurden, durch und zeigen, dass unser Ansatz im speziellen Fall der $\ell_\infty$-Lipschitz-Konstante bessere Schätzungen liefert als andere in der Literatur verfügbare Grundlinien.
Wir schlagen einen einfachen, aber effektiven Weg für die few-shot-Klassifikation vor, bei dem sich eine Aufgabenverteilung über mehrere Domänen erstreckt, einschließlich zuvor ungesehener Domänen während des Meta-Trainings. Die Schlüsselidee besteht darin, einen Pool von Einbettungsmodellen aufzubauen, die ihre eigenen metrischen Räume haben, und zu lernen, das beste Modell für eine bestimmte Aufgabe durch Multi-Domain-Meta-Learning auszuwählen. Dies vereinfacht die aufgabenspezifische Anpassung über eine komplexe Aufgabenverteilung als ein einfaches Auswahlproblem, anstatt das Modell mit einer Reihe von Parametern zum Zeitpunkt des Metatests zu modifizieren.Inspiriert von gängigen Multitasking-Lerntechniken lassen wir alle Modelle im Pool ein Basisnetzwerk teilen und fügen jedem Modell einen separaten Modulator hinzu, um das Basisnetzwerk auf seine eigene Weise zu verfeinern.Diese Architektur ermöglicht es dem Pool, die Repräsentationsvielfalt beizubehalten und jedes Modell hat auch eine domäneninvariante Repräsentation. Die Experimente zeigen, dass unser Auswahlschema andere Klassifizierungsalgorithmen mit wenigen Schüssen übertrifft, wenn die Zielaufgaben aus vielen verschiedenen Domänen stammen könnten, und dass die Aggregation der Ergebnisse aller konstituierenden Modelle für Aufgaben aus unbekannten Domänen effektiv ist, was die Wirksamkeit unseres Rahmens zeigt.
Noch im Jahr 2019, viele gescannte Dokumente kommen in Unternehmen in nicht-digitalen Format.Text aus der realen Welt Dokumente extrahiert werden, ist oft eingebettet in reichhaltige Formatierung, wie tabellarische Strukturen oder Formulare mit Fill-in-the-Blank-Boxen oder Unterstreichungen, deren Tinte oft berührt oder sogar durch die Tinte des Textes selbst.Solche Tinte Artefakte können stark mit der Leistung der Erkennungsalgorithmen oder andere nachgelagerte Verarbeitung Aufgaben stören.In dieser Arbeit schlagen wir DeepErase, eine neuronale Präprozessor, um Tinte Artefakte aus Text-Bilder zu löschen. Neben einer hohen Segmentierungsgenauigkeit zeigen wir, dass unsere bereinigten Bilder die Erkennungsgenauigkeit gängiger OCR-Software wie Tesseract 4.0 erheblich verbessern. Wir testen DeepErase an nicht mehr erhältlichen Datensätzen (NIST SDB) gescannter IRS-Steuererklärungsformulare und erzielen zweistellige Verbesserungen der Erkennungsgenauigkeit sowohl für gedruckten als auch für handgeschriebenen Text gegenüber der Basislinie.
Aktuelle Ansätze, die sich auf das Training von Ersatzmodellen, Gradientenschätzung oder genetische Algorithmen stützen, erfordern oft eine übermäßige Anzahl von Abfragen und sind daher nicht für reale Systeme geeignet, bei denen die maximale Abfragezahl aus Kostengründen begrenzt ist. Wir schlagen einen abfrageeffizienten Black-Box-Angriff vor, der Bayes'sche Optimierung in Kombination mit Bayes'scher Modellauswahl verwendet, um die Störung des Gegners und den optimalen Grad der Dimensionsreduktion des Suchraums zu optimieren. Wir zeigen empirisch, dass unsere Methode vergleichbare Erfolgsraten mit 2-5 mal weniger Abfragen im Vergleich zu früheren Black-Box-Angriffen auf dem Stand der Technik erreichen kann.
Das Erlernen multimodaler Repräsentationen ist ein grundlegend komplexes Forschungsproblem, das durch das Vorhandensein mehrerer heterogener Informationsquellen bedingt ist. Obwohl das Vorhandensein mehrerer Modalitäten zusätzliche wertvolle Informationen liefert, gibt es zwei zentrale Herausforderungen, die beim Lernen aus multimodalen Daten zu bewältigen sind:1) Modelle müssen die komplexen intra-modalen und cross-modalen Interaktionen für die Vorhersage erlernen und2) Modelle müssen robust gegenüber unerwarteten fehlenden oder verrauschten Modalitäten während des Tests sein. In dieser Arbeit schlagen wir vor, ein gemeinsames generativ-diskriminatives Ziel für multimodale Daten und Labels zu optimieren. Wir stellen ein Modell vor, das Repräsentationen in zwei Gruppen unabhängiger Faktoren faktorisiert: multimodale diskriminative und modalitätsspezifische generative Faktoren. Modalitätsspezifische generative Faktoren sind für jede Modalität einzigartig und enthalten die Informationen, die für die Generierung von Daten erforderlich sind. Experimentelle Ergebnisse zeigen, dass unser Modell in der Lage ist, aussagekräftige multimodale Repräsentationen zu erlernen, die auf sechs multimodalen Datensätzen State-of-the-Art oder konkurrenzfähige Leistungen erzielen. Unser Modell zeigt flexible generative Fähigkeiten durch Konditionierung auf unabhängige Faktoren und kann fehlende Modalitäten rekonstruieren, ohne die Leistung signifikant zu beeinträchtigen.Schließlich interpretieren wir unsere faktorisierten Repräsentationen, um die Wechselwirkungen zu verstehen, die multimodales Lernen beeinflussen.
Die erfolgreiche Anwendung flexibler, allgemeiner Lernalgorithmen auf reale Robotikanwendungen wird oft durch ihre geringe Dateneffizienz eingeschränkt. Um diese Herausforderung zu bewältigen, fördern Domänen mit mehr als einer dominanten Aufgabe von Interesse die gemeinsame Nutzung von Informationen über Aufgaben hinweg, um die erforderliche Experimentierzeit zu begrenzen.Zu diesem Zweck untersuchen wir kompositionelle induktive Verzerrungen in Form von hierarchischen Richtlinien als Mechanismus für den Wissenstransfer über Aufgaben hinweg beim Verstärkungslernen (RL).Wir zeigen, dass diese Art von Hierarchie einen positiven Transfer ermöglicht und gleichzeitig negative Interferenzen abschwächt. Wir entwerfen einen RL-Algorithmus, der stabiles und schnelles Lernen von strukturierten Policies und die effektive Wiederverwendung von Verhaltenskomponenten und Übergangsdaten über Aufgaben hinweg in einer Off-Policy-Umgebung ermöglicht. Schließlich evaluieren wir unseren Algorithmus in simulierten Umgebungen sowie in physischen Roboterexperimenten und zeigen erhebliche Verbesserungen in der Daten-Effizienz gegenüber konkurrierenden Baselines.
In diesem Papier untersuchen wir die Darstellungsleistung von tiefen neuronalen Netzen (DNN), die zur Familie der stückweise-linearen (PWL) Funktionen gehören, basierend auf PWL-Aktivierungseinheiten wie Gleichrichter oder maxout.We untersuchen die Komplexität solcher Netze durch die Untersuchung der Anzahl der linearen Regionen der PWL-Funktion.Typischerweise kann eine PWL-Funktion von einem DNN als eine große Familie von linearen Funktionen gesehen werden, die auf Millionen von solchen Regionen wirken.We direkt auf die Arbeit von MontÂ'ufar et al bauen. (2014), MontÂ'ufar (2017) und Raghu et al. (2017) auf, indem wir die oberen und unteren Grenzen für die Anzahl der linearen Regionen für gleichgerichtete und Maxout-Netzwerke verfeinern.Zusätzlich zum Erreichen engerer Grenzen entwickeln wir auch eine neuartige Methode zur exakten Aufzählung oder Zählung der Anzahl der linearen Regionen mit einer gemischt-ganzzahligen linearen Formulierung, die den Eingaberaum auf die Ausgabe abbildet.Wir nutzen diese neue Fähigkeit, um zu visualisieren, wie sich die Anzahl der linearen Regionen beim Training von DNNs ändert.  
Faltungsneuronale Netze merken sich einen Teil ihrer Trainingsdaten, weshalb Strategien wie DatenvergrÃ¶ÃŸerung und Drop-Out eingesetzt werden, um eine Ãœberanpassung zu verhindern. Dieses Papier befasst sich mit der verwandten Frage der â€žMitgliedschaftsinferenzâ€œ, bei der das Ziel darin besteht, festzustellen, ob ein Bild wÃ?hrend des Trainings verwendet wurde.Wir betrachten Mitgliedschaftstests entweder Ã?ber Ensembles von Proben oder Ã?ber einzelne Proben. Dann stellen wir einen neuen Ansatz vor, um die ZugehÃ¶rigkeit abzuleiten, wenn einige der obersten Schichten nicht verfÃ?gbar sind oder feinabgestimmt wurden, und zeigen, dass untere Schichten immer noch Informationen Ã?ber die Trainingsmuster enthalten. Um unsere Ergebnisse zu untermauern, fÃ?hren wir groÃŸangelegte Experimente mit Imagenet und Teilmengen von YFCC-100M mit modernen Architekturen wie VGG und Resnet durch.
Während Generative Adversarial Networks (GANs) empirisch beeindruckende Ergebnisse beim Erlernen komplexer Verteilungen in der realen Welt erzielt haben, haben neuere Arbeiten gezeigt, dass sie unter mangelnder Diversität oder Mode-Kollaps leiden. Die theoretische Arbeit von Arora et al. (2017a) deutet auf ein Dilemma bei den statistischen Eigenschaften von GANs hin: Starke Diskriminatoren verursachen Overfitting, während schwache Diskriminatoren keinen Mode-Kollaps erkennen können. Im Gegensatz dazu zeigen wir in diesem Beitrag, dass GANs prinzipiell Verteilungen in Wasserstein-Distanz (oder KL-Divergenz in vielen Fällen) mit polynomialer Stichprobenkomplexität lernen können, wenn die Diskriminatorklasse eine starke Unterscheidungskraft gegenüber der bestimmten Generatorklasse hat (anstatt gegenüber allen möglichen Generatoren). Für verschiedene Generatorklassen wie Gauß-Mischung, Exponentialfamilien und invertierbare und injektive neuronale Netzgeneratoren entwerfen wir entsprechende Diskriminatoren (bei denen es sich oft um neuronale Netze bestimmter Architekturen handelt), so dass die durch die Diskriminatoren induzierte Integrale Wahrscheinlichkeitsmetrik (IPM) nachweislich den Wasserstein-Abstand und/oder die KL-Divergenz approximieren kann. Unsere vorläufigen Experimente zeigen, dass auf synthetischen Datensätzen die Test-IPM gut mit der KL-Divergenz oder der Wasserstein-Distanz korreliert ist, was darauf hindeutet, dass der Mangel an Vielfalt in GANs durch die Suboptimalität der Optimierung und nicht durch statistische Ineffizienz verursacht werden kann.
Wir zeigen, wie die Hyperparameter des stochastischen Gradientenabstiegs die Kovarianz der Gradienten (K) und die Hessian des Trainingsverlustes (H) entlang dieser Trajektorie beeinflussen. auf der Grundlage eines theoretischen Modells sagen wir voraus, dass die Verwendung einer hohen Lernrate oder einer kleinen Batchgröße in der frühen Phase des Trainings SGD zu Regionen des Parameterraums mit (1) reduzierter Spektralnorm von K und (2) verbesserter Konditionierung von K und H führt. Wir zeigen, dass der Punkt auf der Trajektorie, nach dem diese Effekte anhalten, den wir als Break-Even-Punkt bezeichnen, früh während des Trainings erreicht wird. Wir demonstrieren diese Effekte empirisch für eine Reihe von tiefen neuronalen Netzen, die auf mehrere verschiedene Aufgaben angewandt werden. Schließlich wenden wir unsere Analyse auf Netze mit Batch-Normalisierungsschichten (BN) an und stellen fest, dass es notwendig ist, eine hohe Lernrate zu verwenden, um Verlustglättungseffekte zu erzielen, die zuvor allein der BN zugeschrieben wurden.
Graph Convolution Network (GCN) wurde als eines der effektivsten Graphenmodelle für halbüberwachtes Lernen anerkannt, aber es extrahiert lediglich die Nachbarschaftsinformationen erster oder weniger Ordnung durch Informationsfortpflanzung, was bei tieferen Strukturen zu einem Leistungsabfall führt.Bestehende Ansätze, die sich mit den Nachbarn höherer Ordnung befassen, neigen dazu, die Leistung der Adjazenzmatrix zu nutzen.In diesem Papier gehen wir von der scheinbar trivialen Bedingung aus, dass die Nachbarschaftsinformationen höherer Ordnung denen der Nachbarn erster Ordnung ähneln. Dementsprechend stellen wir einen unbeaufsichtigten Ansatz vor, um solche Ähnlichkeiten zu beschreiben und die Gewichtsmatrizen der Nachbarn höherer Ordnung automatisch durch Lasso zu lernen, das den Merkmalsverlust zwischen den Nachbarn erster Ordnung und höherer Ordnung minimiert, auf dessen Grundlage wir den neuen Faltungsfilter für GCN formulieren, um die besseren Knotenrepräsentationen zu lernen.
Die Leistung von tiefen neuronalen Netzen wird oft auf ihre automatisierte, aufgabenbezogene Merkmalskonstruktion zurückgeführt. Es bleibt jedoch eine offene Frage, warum dies zu Lösungen mit guter Generalisierung führt, selbst in Fällen, in denen die Anzahl der Parameter größer ist als die Anzahl der Stichproben.Bereits in den 90er Jahren beobachteten Hochreiter und Schmidhuber, dass die Flachheit der Verlustfläche um ein lokales Minimum mit einem geringen Generalisierungsfehler korreliert. Kürzlich wurde jedoch gezeigt, dass die bestehenden Maße für die Flachheit theoretisch nicht mit der Generalisierung in Verbindung gebracht werden können: Wenn ein Netz ReLU-Aktivierungen verwendet, kann die Netzfunktion reparametrisiert werden, ohne dass sich die Ausgabe so ändert, dass die Flachheit fast beliebig verändert wird.In diesem Beitrag wird eine natürliche Modifikation der bestehenden Flachheitsmaße vorgeschlagen, die zu einer Invarianz gegenüber Reparametrisierung führt. Das vorgeschlagene Maß impliziert eine Robustheit des Netzes gegenüber Änderungen im Input und in den versteckten Schichten.Die Verbindung dieser Merkmalsrobustheit mit der Generalisierung führt zu einer verallgemeinerten Definition der Repräsentativität von Daten.Damit kann der Generalisierungsfehler eines auf repräsentativen Daten trainierten Modells durch seine Merkmalsrobustheit begrenzt werden, die von unserem neuartigen Flatness-Maß abhängt.
Bayes'sche Methoden wurden erfolgreich angewandt, um Gewichte von neuronalen Netzen zu sparsifizieren und Struktureinheiten aus den Netzen zu entfernen, z.B. Neuronen.Wir wenden diesen Ansatz für gated rekurrente Architekturen an und entwickeln ihn weiter.Insbesondere schlagen wir vor, zusätzlich zur Sparsifizierung einzelner Gewichte und Neuronen die Voraktivierungen von Gattern und den Informationsfluss in LSTM zu sparsifizieren.Dadurch werden einige Gatter und Komponenten des Informationsflusses konstant, der Vorwärtsdurchlauf wird beschleunigt und die Kompression verbessert.Außerdem ist die resultierende Struktur der Gattersparsamkeit interpretierbar und hängt von der Aufgabe ab.
Die Verbesserung der Genauigkeit numerischer Methoden bleibt eine zentrale Herausforderung in vielen Disziplinen und ist besonders wichtig für nichtlineare Simulationsprobleme.Ein repräsentatives Beispiel für solche Probleme ist die Strömung, die gründlich untersucht wurde, um zu effizienten Simulationen komplexer Strömungsphänomene zu gelangen.In diesem Papier wird ein datengesteuerter Ansatz vorgestellt, der lernt, um die Genauigkeit numerischer Löser zu verbessern.Die vorgeschlagene Methode nutzt ein fortschrittliches numerisches Schema mit einer feinen Simulationsauflösung, um Referenzdaten zu erfassen. Die vorgeschlagene Methode verwendet ein fortschrittliches numerisches Schema mit einer feinen Simulationsauflösung zur Erfassung von Referenzdaten und setzt dann ein neuronales Netzwerk ein, das eine Korrektur ableitet, um ein grobes und damit schnell zu erzielendes Ergebnis näher an die Referenzdaten heranzuführen.Wir geben Einblicke in das gezielte Lernproblem mit verschiedenen Lernansätzen: vollständig überwachte Lernmethoden mit einer naiven und einer optimierten Datenerfassung sowie eine unbeaufsichtigte Lernmethode mit einem differenzierbaren Navier-Stokes-Löser.Während unser Ansatz sehr allgemein und auf beliebige Modelle partieller Differentialgleichungen anwendbar ist, heben wir speziell die Genauigkeitsgewinne für Strömungssimulationen hervor.
Obwohl es technisch machbar ist, Daten für die Analyse in einer Weise zu vereinen, die ein schnelles, lernendes Gesundheitssystem unterstützt, schränken Datenschutzbedenken und regulatorische Hindernisse die Zentralisierung von Daten ein. Maschinelles Lernen kann in einer föderierten Art und Weise auf Patientendatensätzen mit demselben Satz von Variablen durchgeführt werden, die jedoch über die verschiedenen Versorgungseinrichtungen hinweg getrennt sind. Wir bezeichnen Methoden, die das Training von maschinellen Lernmodellen auf Daten ermÃ¶glichen, die durch zwei oder mehr Grade getrennt sind, als â€žkonfÃ¶deriertes maschinelles Lernenâ€œ. Wir haben ein konfÃ¶deriertes maschinelles Lernmodell entwickelt und evaluiert, um das Risiko von UnfÃ?llen bei Ã?lteren Menschen zu stratifizieren.
Bestehende neuronale Netze sind anfällig für "gegnerische Beispiele", die durch das Hinzufügen böswilliger kleiner Störungen in den Eingaben erzeugt werden, um eine Fehlklassifizierung durch die Netze zu bewirken. Im Gegensatz zum einstufigen Training führt das mehrstufige Training zu den besten Ergebnissen bei MNIST und CIFAR10, benötigt jedoch einen enormen Zeitaufwand. Daher schlagen wir eine Methode vor, die Stochastic Quantized Activation (SQA), die das Problem der Überanpassung beim einstufigen adversen Training löst und schnell eine Robustheit erreicht, die mit der des mehrstufigen Trainings vergleichbar ist.SQA schwächt die adversen Effekte ab, indem es den Aktivierungsfunktionen eine zufällige Selektivität verleiht und es dem Netzwerk ermöglicht, Robustheit mit nur einstufigem Training zu lernen. Während des gesamten Experiments zeigt unsere Methode den Stand der Technik Robustheit gegen eine der stärksten White-Box-Angriffe als PGD Ausbildung, aber mit viel weniger computational cost.Finally, visualisieren wir den Lernprozess des Netzes mit SQA auf starke Gegner zu behandeln, die sich von bestehenden Methoden ist.
Wir haben einen offenen Datensatz, das Allen Brain Observatory, verwendet, um die Verteilung der Reaktionen auf wiederholte natürliche Filmvorführungen zu quantifizieren. Ein großer Teil der Reaktionen lässt sich am besten durch lognormale Verteilungen oder Gaußsche Mischungen mit zwei Komponenten beschreiben, die denen von Einheiten in tiefen neuronalen Netzen mit Dropout ähneln. Anhand eines separaten Satzes elektrophysiologischer Aufzeichnungen konstruierten wir ein Populationskopplungsmodell als Kontrolle für zustandsabhängige Aktivitätsfluktuationen und stellten fest, dass die Modellresiduen ebenfalls nicht-gaußsche Verteilungen aufweisen.Wir analysierten dann Antworten über Versuche aus mehreren Abschnitten verschiedener Filmclips und beobachteten, dass sich das Rauschen im Kortex besser an die Variationen des Reizes innerhalb des Clips als an die Variationen außerhalb des Clips anpasst.Wir argumentieren, dass Rauschen für die Generalisierung nützlich ist, wenn es sich entlang der Repräsentationen verschiedener Exemplare in der Klasse bewegt, ähnlich wie die Struktur des kortikalen Rauschens.
Die meisten der existierenden Arbeiten befassen sich mit dem Closed-Set-Szenario und gehen davon aus, dass die Quell- und die Zieldomäne genau die gleichen Kategorien haben, Die Ausweitung der Domänenanpassung von einer geschlossenen Domäne auf eine solche offene Domäne ist nicht trivial, da nicht erwartet wird, dass die Zielproben in der unbekannten Klasse mit der Quelle übereinstimmen.In diesem Papier gehen wir dieses Problem an, indem wir die hochmoderne Domänenanpassungstechnik Self-Ensembling um kategorieagnostische Cluster in der Zieldomäne erweitern. Konkret stellen wir Self-Ensembling with Category-agnostic Clusters (SE-CC) --- eine neuartige Architektur vor, die die Domänenanpassung mit der zusätzlichen Anleitung von kategorie-agnostischen Clustern steuert, die spezifisch für die Zieldomäne sind.Diese Clustering-Informationen liefern domänenspezifische visuelle Hinweise, die die Verallgemeinerung von Self-Ensembling sowohl für Closed-Set- als auch Open-Set-Szenarien erleichtern. Ein Clustering-Zweig wird genutzt, um sicherzustellen, dass die gelernte Repräsentation die zugrunde liegende Struktur beibehält, indem die geschätzte Zuweisungsverteilung über die Cluster mit der inhärenten Clusterverteilung für jede Zielprobe abgeglichen wird. Darüber hinaus verbessert SE-CC die gelernte Repräsentation durch Maximierung der gegenseitigen Information. Es wurden umfangreiche Experimente mit Office- und VisDA-Datensätzen sowohl für die Open-Set- als auch für die Closed-Set-Domain-Anpassung durchgeführt, und im Vergleich zu den State-of-the-Art-Ansätzen wurden hervorragende Ergebnisse erzielt.
Wir stellen Spectral Inference Networks vor, ein Framework zum Lernen von Eigenfunktionen linearer Operatoren durch stochastische Optimierung. Spectral Inference Networks verallgemeinern die Slow Feature Analysis auf generische symmetrische Operatoren und sind eng verwandt mit Variational Monte Carlo-Methoden aus der Computerphysik, so dass sie ein leistungsfähiges Werkzeug für das unbeaufsichtigte Lernen von Repräsentationen aus Video- oder graph-strukturierten Daten sein können. Wir stellen das Training von Spektralen Inferenznetzwerken als ein zweistufiges Optimierungsproblem dar, das das Online-Lernen mehrerer Eigenfunktionen ermöglicht. Wir zeigen Ergebnisse des Trainings von Spektralen Inferenznetzwerken für Probleme in der Quantenmechanik und des Merkmalslernens für Videos auf synthetischen Datensätzen. Unsere Ergebnisse zeigen, dass Spektrale Inferenznetzwerke Eigenfunktionen von linearen Operatoren genau wiederherstellen und interpretierbare Darstellungen aus Videos auf völlig unbeaufsichtigte Weise entdecken können.
Die Tensor-Train-Faktorisierung (TTF) ist ein effizienter Weg, um große Gewichtsmatrizen von voll verbundenen Schichten und rekurrenten Schichten in rekurrenten neuronalen Netzen (RNNs) zu komprimieren. Allerdings müssen hohe Tensor-Train-Ränge für alle Kerntensoren von Parametern elementweise festgelegt werden, was zu einer unnötigen Redundanz von Modellparametern führt.Diese Arbeit wendet Riemannian stochastic gradient descent (RSGD) an, um Kerntensoren von Parametern in der Riemannschen Mannigfaltigkeit zu trainieren, bevor Vektoren mit niedrigeren Tensor-Train-Rängen für Parameter gefunden werden. Der Beitrag stellt zunächst den RSGD-Algorithmus mit einer Konvergenzanalyse vor und testet ihn dann an fortgeschrittenen Tensor-Train-RNNs wie bi-direktionalen GRU/LSTM und Encoder-Decoder-RNNs mit einem Tensor-Train-Attention-Modell.Die Experimente zur Ziffernerkennung und maschinellen Übersetzung zeigen die Effektivität des RSGD-Algorithmus für Tensor-Train-RNNs.
Wir schlagen einen neuen Algorithmus vor - Projection Based ConstrainedPolicy Optimization (PCPO), eine iterative Methode zur Optimierung von Strategien in einem zweistufigen Prozess - der erste Schritt führt eine uneingeschränkte Aktualisierung durch, während der zweite Schritt die Verletzung der Einschränkung durch Projektion der Strategie zurück auf die Einschränkungsmenge ausgleicht. Wir analysieren PCPO theoretisch und liefern eine untere Schranke für die Belohnungsverbesserung sowie eine obere Schranke für die Constraint-Verletzung für jedes Policy-Update. Außerdem charakterisieren wir die Konvergenz von PCPO mit Projektion auf der Grundlage von zwei verschiedenen Metriken - L2-Norm und Kullback-Leibler-Divergenz. Unsere empirischen Ergebnisse über mehrere Steuerungsaufgaben zeigen, dass unser Algorithmus eine bessere Leistung erzielt, im Durchschnitt mehr als 3,5-mal weniger Constraint-Verletzung und etwa 15 % höhere Belohnung im Vergleich zu den modernsten Methoden.
Wir führen diese Anfälligkeit auf die Einschränkungen zurück, die der aktivierungsbasierten Repräsentation innewohnen. Um die gelernten Informationen aus der aktivierungsbasierten Repräsentation zu ergänzen, schlagen wir die Verwendung einer gradientenbasierten Repräsentation vor, die sich explizit auf fehlende Informationen fokussiert.Zusätzlich schlagen wir eine Richtungsbeschränkung für die Gradienten als Zielsetzung während des Trainings vor, um die Charakterisierung fehlender Informationen zu verbessern. Wir zeigen, dass die gradientenbasierte Repräsentation die aktivierungsbasierte Repräsentation um 0,093 in den CIFAR-10- und 0,361 in den CURE-TSR-Datensätzen in Bezug auf den über alle Klassen gemittelten AUROC übertrifft. Außerdem schlagen wir einen Algorithmus zur Erkennung von Anomalien vor, der die gradientenbasierte Repräsentation verwendet (GradCon), und validieren seine Leistung in drei Benchmarking-Datensätzen. Die vorgeschlagene Methode übertrifft die Mehrheit der State-of-the-Art-Algorithmen in CIFAR-10-, MNIST- und fMNIST-Datensätzen mit einem durchschnittlichen AUROC von 0,664, 0,973 bzw. 0,934.
Medizinische Bilder kÃ¶nnen verschiedene Arten von Artefakten mit unterschiedlichen Mustern und Mischungen enthalten, die von vielen Faktoren abhÃ?ngen, wie z.B. Scaneinstellungen, Maschinenzustand, Patientencharakteristika, Umgebung usw. Bestehende Deep-Learning-Methoden zur Artefaktreduktion sind jedoch durch ihr Trainingsset mit bestimmten vorgegebenen Artefakttypen und -mustern eingeschrÃ?nkt und haben daher nur eine begrenzte klinische Anwendbarkeit. In diesem Papier stellen wir ein "Zero-Shot"-Framework fÃ?r die Artefaktreduktion in medizinischen Bildern (ZSAR) vor, das die LeistungsfÃ?higkeit von Deep Learning nutzt, ohne jedoch allgemeine vortrainierte Netzwerke oder eine saubere Bildreferenz zu verwenden. Wir nutzen die niedrige interne visuelle Entropie eines Bildes und trainieren ein leichtgewichtiges bildspezifisches Artefaktreduktionsnetzwerk, um Artefakte in einem Bild zur Testzeit zu reduzieren. Wir verwenden Computertomographie (CT) und Magnetresonanztomographie (MRT) als Vehikel, um zu zeigen, dass ZSAR Artefakte sowohl qualitativ als auch quantitativ besser als der Stand der Technik reduzieren kann, während es eine kürzere Ausführungszeit benötigt.Soweit wir wissen, ist dies das erste Deep-Learning-Framework, das Artefakte in medizinischen Bildern reduziert, ohne eine vorherige Trainingsmenge zu verwenden.
Attributionsmethoden bieten Einblicke in die Entscheidungsfindung von maschinellen Lernmodellen wie künstlichen neuronalen Netzen, die für eine gegebene Eingabeprobe jeder einzelnen Eingabevariablen, z. B. den Pixeln eines Bildes, einen Relevanzwert zuweisen. In dieser Arbeit passen wir das Konzept des Informationsengpasses für die Attribution an, indem wir Rauschen zu zwischengeschalteten Merkmalskarten hinzufügen, um den Informationsfluss einzuschränken und zu quantifizieren (in Bits), wie viele Informationen Bildregionen liefern. Wir vergleichen unsere Methode mit zehn Baselines unter Verwendung von drei verschiedenen Metriken auf VGG-16 und ResNet-50 und stellen fest, dass unsere Methode alle Baselines in fünf von sechs Einstellungen übertrifft. Die informationstheoretische Grundlage der Methode bietet einen absoluten Referenzrahmen für Attributionswerte (Bits) und eine Garantie, dass Regionen, die nahe Null bewertet werden, für die Entscheidung des Netzwerks nicht notwendig sind.
Rekurrente neuronale Netze (RNNs) werden in modernen Modellen in Bereichen wie Spracherkennung, maschinelle Übersetzung und Sprachmodellierung verwendet.Sparsity ist eine Technik zur Verringerung der Rechen- und Speicheranforderungen von Deep-Learning-Modellen.Sparse RNNs lassen sich leichter auf Geräten und High-End-Server-Prozessoren einsetzen. Obwohl Sparse-Operationen im Vergleich zu ihren dichten Gegenstücken weniger Rechenleistung und Speicher benötigen, ist der Geschwindigkeitszuwachs, der durch die Verwendung von Sparse-Operationen beobachtet wird, auf verschiedenen Hardware-Plattformen geringer als erwartet.Um dieses Problem anzugehen, untersuchen wir zwei verschiedene Ansätze, um Block-Sparsity in RNNs zu induzieren: Pruning von Gewichtsblöcken in einer Schicht und die Verwendung von Gruppen-Lasso-Regularisierung mit Pruning, um Gewichtsblöcke mit Nullen zu erzeugen. Mit diesen Techniken können wir blocksparse RNNs mit einer Spärlichkeit von 80 % bis 90 % mit einem geringen Genauigkeitsverlust erstellen. Diese Technik ermöglicht es uns, die Modellgröße um etwa das 10-fache zu reduzieren. Zusätzlich können wir ein größeres dichtes Netzwerk beschneiden, um diesen Genauigkeitsverlust auszugleichen, während wir eine hohe Blockspärlichkeit beibehalten und die Gesamtparameteranzahl reduzieren. Unsere Technik funktioniert mit einer Vielzahl von Blockgrößen bis zu 32x32. Blocksparse RNNs eliminieren Overheads im Zusammenhang mit der Datenspeicherung und unregelmäßigen Speicherzugriffen, während sie die Hardwareeffizienz im Vergleich zu unstrukturierter Spärlichkeit erhöhen.
Value Iteration Netzwerke sind eine Annäherung an den Wert Iteration (VI) Algorithmus mit Faltung neuronale Netze implementiert, um VI voll differenzierbar zu machen.In dieser Arbeit untersuchen wir diese Netze im Zusammenhang mit Roboter-Bewegungsplanung, mit einem Schwerpunkt auf Anwendungen auf Planeten Rover.Die wichtigste Aufgabe in Lernen-basierte Bewegungsplanung ist es, eine Transformation von Geländebeobachtungen zu einer geeigneten Navigation Belohnungsfunktion zu lernen.Um mit komplexen Geländebeobachtungen und Politik Lernen zu behandeln, schlagen wir einen Wert Iteration Rekursion, die so genannte weiche Wert Iteration Netzwerk (SVIN). SVIN wurde entwickelt, um effektivere Trainingsgradienten durch das Wertiterationen-Netzwerk zu erzeugen. Es basiert auf einem weichen Strategiemodell, bei dem die Strategie mit einer Wahrscheinlichkeitsverteilung über alle möglichen Aktionen dargestellt wird, anstatt einer deterministischen Strategie, die nur die beste Aktion liefert.
Transformer-Netzwerke haben zu wichtigen Fortschritten in der Sprachmodellierung und der maschinellen Übersetzung geführt. Diese Modelle enthalten zwei aufeinanderfolgende Module, eine Feed-Forward-Schicht und eine Self-Attention-Schicht. Letztere ermöglicht es dem Netzwerk, langfristige Abhängigkeiten zu erfassen, und wird oft als Schlüssel zum Erfolg von Transformers angesehen.aufbauend auf dieser Intuition schlagen wir ein neues Modell vor, das ausschließlich aus Aufmerksamkeitsschichten besteht. Dank dieser Vektoren können wir die Feed-Forward-Schicht entfernen, ohne die Leistung eines Transformers zu beeinträchtigen. Unsere Evaluierung zeigt die Vorteile unseres Modells bei Standard-Benchmarks zur Modellierung von Zeichen und Wörtern.
Diese Arbeit betrachtet neuronale Netze als datenerzeugende Systeme und wendet Techniken zur Erkennung anomaler Muster auf diese Daten an, um zu erkennen, wann ein Netz eine Gruppe anomaler Eingaben verarbeitet.  Die Erkennung von Anomalien ist eine kritische Komponente für mehrere Probleme des maschinellen Lernens, einschließlich der Erkennung von schädlichem Rauschen, das den Eingaben hinzugefügt wurde.Im weiteren Sinne ist diese Arbeit ein Schritt in Richtung der Fähigkeit neuronaler Netze, Gruppen von Proben außerhalb der Verteilung zu erkennen.  Diese Arbeit führt ``Subset Scanning-Methoden aus dem Bereich der Erkennung anomaler Muster in die Aufgabe der Erkennung anomaler Eingaben für neuronale Netze ein.  Subset Scanning erlaubt es uns, die Frage zu beantworten: "Welche Teilmenge von Eingaben hat größere Aktivierungen als erwartet an welcher Teilmenge von Knoten?"  Wenn wir das Problem der Erkennung von Fehlern auf diese Weise angehen, können wir systematische Muster im Aktivierungsraum identifizieren, die sich über mehrere fehlerhaft verrauschte Bilder erstrecken.  Solche Bilder sind ``gemeinsam seltsam''.  Unter Ausnutzung dieses gemeinsamen anomalen Musters zeigen wir eine erhöhte Erkennungsleistung, wenn der Anteil der verrauschten Bilder in einem Testsatz zunimmt.   Die Ergebnisse der Erkennungsleistung und -genauigkeit werden für gezieltes negatives Rauschen, das zu CIFAR-10-Bildern hinzugefügt wurde, auf einem 20-Schichten-ResNet unter Verwendung des Angriffs mit der Basic Iterative Method bereitgestellt.
In dieser Arbeit führen wir eine gründliche Untersuchung stabiler rekurrenter Modelle durch. Theoretisch beweisen wir, dass stabile rekurrente neuronale Netze gut durch Feed-Forward-Netze für den Zweck der Inferenz und des Trainings durch Gradientenabstieg approximiert werden. Zusammengenommen werfen diese Ergebnisse ein Licht auf die effektive Leistung von rekurrenten Netzen und legen nahe, dass ein Großteil des Sequenzlernens im stabilen Regime stattfindet oder stattfinden kann, und unsere Ergebnisse helfen zu erklären, warum es Praktikern in vielen Fällen gelingt, rekurrente Modelle durch Feed-Forward-Modelle zu ersetzen.
Gewichtsteilung spielt eine wichtige Rolle für den Erfolg vieler tiefer neuronaler Netze, indem sie die Speichereffizienz erhöht und nützliche induktive Vorannahmen über das Problem in das Netz einbezieht.Aber das Verständnis, wie Gewichtsteilung im Allgemeinen effektiv genutzt werden kann, ist ein Thema, das noch nicht umfassend untersucht wurde.Chen et al. (2015) schlugen HashedNets vor, die ein mehrschichtiges Perzeptron mit einer Hashtabelle als Methode zur Komprimierung neuronaler Netze ergänzen. Wir verallgemeinern diese Methode zu einem Rahmenwerk (ArbNets), das eine effiziente beliebige Gewichtsverteilung ermöglicht, und verwenden es, um die Rolle der Gewichtsverteilung in neuronalen Netzen zu untersuchen.Wir zeigen, dass gängige neuronale Netze als ArbNets mit verschiedenen Hash-Funktionen ausgedrückt werden können.Wir stellen auch zwei neuartige Hash-Funktionen vor, den Dirichlet-Hash und den Neighborhood-Hash, und verwenden sie, um experimentell zu zeigen, dass eine ausgewogene und deterministische Gewichtsverteilung die Leistung eines neuronalen Netzes verbessert.
Wir stellen Neuronale Markov-Logische Netze (NMLNs) vor, ein statistisches relationales Lernsystem, das Ideen aus der Markov-Logik entlehnt: Wie Markov-Logische Netze (MLNs) sind NMLNs ein Modell der Exponentialfamilie zur Modellierung von Verteilungen über mögliche Welten, aber im Gegensatz zu MLNs stützen sie sich nicht auf explizit spezifizierte logische Regeln erster Ordnung, sondern lernen eine implizite Repräsentation solcher Regeln als neuronales Netz, das als potenzielle Funktion auf Fragmenten der relationalen Struktur agiert. Interessanterweise kann jedes MLN als NMLN dargestellt werden.Ähnlich wie die kürzlich vorgeschlagenen neuronalen Theorembeweiser (NTPs) (Rocktaschel at al. 2017) können NMLNs Einbettungen von Konstanten nutzen, aber im Gegensatz zu NTPs funktionieren NMLNs auch in Abwesenheit von Konstanten.Dies ist extrem wichtig für die Vorhersage in anderen Umgebungen als der transduktiven.Wir zeigen das Potenzial von NMLNs auf Wissensbasis Vervollständigung Aufgaben und auf die Erzeugung von molekularen (Graph) Daten.
Mit Hilfe von neuronalen Variations-Bayes-Netzen entwickeln wir einen Algorithmus, der in der Lage ist, Wissen aus mehreren verschiedenen Aufgaben in einem Prior zu akkumulieren, was zu einem reichhaltigen Prior führt, der in der Lage ist, in wenigen Schritten auf neue Aufgaben zu lernen, wobei das Posterior über die Mean-Field-Approximation hinausgeht und eine gute Unsicherheit über die durchgeführten Experimente liefert. Experimente mit Mini-Imagenet erreichen den Stand der Technik mit einer Genauigkeit von 74,5% bei 5 Shot Learning. Schließlich stellen wir zwei neue Benchmarks zur Verfügung, die jeweils einen Fehlermodus bestehender Meta-Learning-Algorithmen wie MAML und prototypischer Netzwerke zeigen.
Um robuste, umgebungsübergreifende Beschreibungen von Sequenzen zu lernen, führen wir disentangled state space models (DSSM) ein. Im latenten Raum von DSSM wird die umgebungsinvariante Zustandsdynamik explizit von der umgebungsspezifischen Information, die diese Dynamik steuert, entkoppelt. Wir zeigen empirisch, dass eine solche Trennung robuste Vorhersage, Sequenzmanipulation und Umgebungscharakterisierung ermöglicht.Wir schlagen auch ein unbeaufsichtigtes VAE-basiertes Trainingsverfahren vor, um DSSM als Bayes'sche Filter zu erlernen.In unseren Experimenten demonstrieren wir modernste Leistung bei der kontrollierten Erzeugung und Vorhersage von Prellball-Videosequenzen unter verschiedenen Gravitationseinflüssen.
Wir schlagen einen metrischen Lerner vor, der eine Bregman-Divergenz lernt, indem er die zugrundeliegende konvexe Funktion lernt. Bregman-Divergenzen sind ein guter Kandidat für diesen Rahmen, da sie die einzige Klasse von Divergenzen mit der Eigenschaft sind, dass der beste Repräsentant einer Menge von Punkten durch ihren Mittelwert gegeben ist. Wir schlagen eine flexible Erweiterung der prototypischen Netzwerke vor, um ein gemeinsames Lernen der Einbettung und der Divergenz zu ermöglichen und gleichzeitig die Recheneffizienz zu erhalten. unsere vorläufigen Ergebnisse sind vergleichbar mit den früheren Arbeiten an den Omniglot- und Mini-ImageNet-Datensätzen, zwei Standard-Benchmarks für One-Shot- und Little-Shot-Lernen. wir argumentieren, dass unser Modell auch für andere Aufgaben verwendet werden kann, die metrisches Lernen oder Aufgaben beinhalten, die annähernde Konvexität erfordern, wie z.B. strukturierte Vorhersage und Datenvervollständigung.
Motiviert durch die Flexibilität biologischer neuronaler Netze, deren Konnektivitätsstruktur sich während ihrer Lebenszeit signifikant verändert, führen wir das Unrestricted Recursive Network (URN) ein und zeigen, dass es eine ähnliche Flexibilität während des Trainings mittels Gradientenabstiegs aufweisen kann.Wir zeigen empirisch, dass viele der verschiedenen neuronalen Netzstrukturen, die heute in der Praxis verwendet werden (einschließlich vollständig verbundener, lokal verbundener und restlicher Netze unterschiedlicher Tiefe und Breite), dynamisch aus demselben URN entstehen können. Diese unterschiedlichen Strukturen können durch Gradientenabstieg auf einer einzigen allgemeinen Verlustfunktion abgeleitet werden, wobei die Struktur der Daten und die relativen Stärken der verschiedenen Regulatoren die Struktur des entstehenden Netzes bestimmen.Wir zeigen, dass sich diese Verlustfunktion und die Regulatoren auf natürliche Weise ergeben, wenn man die Symmetrien des Netzes sowie die geometrischen Eigenschaften der Eingabedaten berücksichtigt.
Wir stellen CROSSGRAD vor, eine Methode zur Verwendung von Multidomänen-Trainingsdaten, um einen Klassifikator zu lernen, der sich auf neue Domänen verallgemeinert.CROSSGRAD benötigt keine Anpassungsphase über beschriftete oder unbeschriftete Daten oder Domänenmerkmale in der neuen Domäne.Die meisten existierenden Methoden zur Domänenanpassung versuchen, Domänensignale mit Hilfe von Techniken wie domänenfeindlichem Training zu löschen. Im Gegensatz dazu steht es CROSSGRAD frei, Domänensignale für die Vorhersage von Bezeichnungen zu verwenden, wenn es eine Überanpassung auf Trainingsdomänen verhindern kann.Wir konzeptualisieren die Aufgabe in einem Bayes'schen Rahmen, in dem ein Sampling-Schritt als Datenerweiterung implementiert wird, basierend auf domänengesteuerten Störungen der Eingangsinstanzen. CROSSGRAD trainiert gemeinsam ein Label und einen Domain-Klassifikator auf Beispielen, die durch Verlustgradienten der jeweils anderen Ziele gestÃ¶rt sind. Dies ermÃ¶glicht es uns, Eingaben direkt zu stÃ¶ren, ohne Domain-Signale zu trennen und neu zu mischen, wÃ?hrend verschiedene Verteilungsannahmen getroffen werden. Die empirische Auswertung von drei verschiedenen Anwendungen, bei denen diese Einstellung natürlich ist, zeigt, dass (1) domänengeleitete Störung im Vergleich zu allgemeinen Instanzstörungsmethoden eine durchgängig bessere Generalisierung auf ungesehene Domänen bietet und (2) Datenerweiterung eine stabilere und genauere Methode als domänenfeindliches Training ist.
Wir präsentieren sketch-rnn, ein rekurrentes neuronales Netzwerk, das in der Lage ist, Strich-basierte Zeichnungen von gewöhnlichen Objekten zu konstruieren.Das Modell wird auf einem Datensatz von von Menschen gezeichneten Bildern trainiert, die viele verschiedene Klassen repräsentieren.Wir skizzieren einen Rahmen für bedingte und unbedingte Skizzengenerierung und beschreiben neue robuste Trainingsmethoden für die Erzeugung von kohärenten Skizzenzeichnungen in einem Vektorformat.
Wilson et al. (2017) haben gezeigt, dass der stochastische Gradient bei geeigneter Gestaltung der Schrittweite besser generalisiert als ADAM (Kingma & Ba, 2014), 2018) überprüfen wir diese Behauptungen, um zu sehen, ob solche Methoden die Lücke zwischen den populärsten Optimierern schließen.Als Nebenprodukt analysieren wir den wahren Nutzen dieser Hypergradienten-Methoden im Vergleich zu klassischeren Zeitplänen, wie z.B. dem Fixed Decay von Wilson et al. (2017).Insbesondere stellen wir fest, dass sie von marginaler Hilfe sind, da ihre Leistung signifikant variiert, wenn sie ihre Hyperparameter abstimmen.Da Robustheit eine kritische Qualität eines Optimierers ist, bieten wir eine Sensitivitätsanalyse dieser gradientenbasierten Optimierer an, um zu bewerten, wie herausfordernd ihre Abstimmung ist.
Trotz einer ständig wachsenden Literatur über Reinforcement Learning Algorithmen und Anwendungen, ist viel weniger über ihre statistische Inferenz bekannt.in diesem Papier untersuchen wir die großen Stichproben Verhalten der Q-Wert Schätzungen mit geschlossener Form Charakterisierungen der asymptotischen Varianzen.dies ermöglicht es uns, effizient zu konstruieren Vertrauen Regionen für Q-Wert und optimalen Wert Funktionen, und zu entwickeln, um ihre Schätzung Fehler zu minimieren.dies führt auch zu einer Politik Exploration Strategie, die sich auf die Schätzung der relativen Diskrepanzen zwischen den Q-Schätzungen.numerische Experimente zeigen überlegene Leistungen unserer Exploration Strategie als andere Benchmark-Ansätze.
Wir führen eine völlig unüberwachte einseitige Bild-zu-Bild-Übersetzung zwischen einer Quelldomäne $X$ und einer Zieldomäne $Y$ durch, wobei wir die zugrundeliegende gemeinsame Semantik (z.B. Klasse, Größe, Form, etc.) beibehalten. Insbesondere interessieren wir uns für einen schwierigeren Fall als die typischerweise in der Literatur behandelten Fälle, in denen die Quelle und das Ziel so weit voneinander entfernt sind, dass rekonstruktionsartige oder pixelweise Ansätze scheitern. Wir argumentieren, dass die Übertragung (d.h. die Übersetzung) der besagten relevanten Informationen sowohl das Verwerfen quellbereichsspezifischer Informationen als auch die Einbeziehung zielbereichsspezifischer Informationen beinhalten sollte, wobei letztere mit einer verrauschten Prioritätsverteilung modelliert werden. Um den degenerierten Fall zu vermeiden, in dem die generierten Stichproben nur durch die vorherige Verteilung erklärt werden, schlagen wir vor, eine Schätzung der gegenseitigen Information zwischen der generierten Stichprobe und der Stichprobe aus der vorherigen Verteilung zu minimieren.Wir entdecken, dass die architektonischen Entscheidungen ein wichtiger Faktor sind, der berücksichtigt werden muss, um die gemeinsame Semantik zwischen $X$ und $Y$ zu erhalten. Wir zeigen State-of-the-Art-Ergebnisse bei der MNIST-zu-SVHN-Aufgabe für unüberwachte Bild-zu-Bild-Übersetzung.
Die Identifizierung von markanten Punkten in Bildern ist eine entscheidende Komponente für visuelle Odometrie, Structure-from-Motion- oder SLAM-Algorithmen, und in jüngster Zeit haben mehrere gelernte Keypoint-Methoden überzeugende Leistungen bei anspruchsvollen Benchmarks gezeigt.  Wir stellen IO-Net (d.h. InlierOutlierNet) vor, eine neuartige Proxy-Aufgabe für die Selbstüberwachung der Keypoint-Erkennung, -Beschreibung und -Matching. Indem wir das Sampling von Inlier-Outlier-Sets aus Punkt-Paar-Korrespondenzen im Rahmen des Keypoint-Learnings vollständig differenzierbar machen, zeigen wir, dass wir in der Lage sind, gleichzeitig die Keypoint-Beschreibung selbst zu überwachen und das Keypoint-Matching zu verbessern.Zweitens stellen wir KeyPointNet vor, eine Keypoint-Netzwerk-Architektur, die besonders für die robuste Keypoint-Erkennung und -Beschreibung geeignet ist. Wir entwerfen das Netzwerk, um lokale Keypoint-Aggregation zu ermöglichen, um Artefakte aufgrund von räumlichen Diskretisierungen, die üblicherweise für diese Aufgabe verwendet werden, zu vermeiden, und wir verbessern die feinkörnige Keypoint-Deskriptor-Leistung, indem wir die Vorteile effizienter Sub-Pixel-Faltungen nutzen, um die Deskriptor-Merkmalskarten auf eine höhere Betriebsauflösung hochzusampeln.Durch umfangreiche Experimente und ablative Analysen zeigen wir, dass die vorgeschlagene selbstüberwachte Keypoint-Lernmethode die Qualität des Merkmalsabgleichs und der Homographieschätzung bei anspruchsvollen Benchmarks gegenüber dem Stand der Technik erheblich verbessert.
Wir untersuchen die Rolle der intrinsischen Motivation als ein Explorationsbias für das Verstärkungslernen in synergetischen Aufgaben mit geringer Belohnung, d.h. Aufgaben, bei denen mehrere Agenten zusammenarbeiten müssen, um ein Ziel zu erreichen, das sie einzeln nicht erreichen könnten.Unsere Schlüsselidee ist, dass ein gutes Leitprinzip für die intrinsische Motivation in synergetischen Aufgaben darin besteht, Handlungen auszuführen, die die Welt auf eine Art und Weise beeinflussen, die nicht erreicht würde, wenn die Agenten alleine handeln würden. Wir untersuchen zwei Varianten dieser Idee, eine auf der Grundlage des tatsächlichen Zustands und eine andere auf der Grundlage eines Dynamikmodells, das gleichzeitig mit der Strategie trainiert wird. Während die erste Variante einfacher ist, hat die zweite den Vorteil, dass sie analytisch in Bezug auf die durchgeführte Aktion differenzierbar ist. Wir validieren unseren Ansatz in bimanuellen Robotermanipulationsaufgaben mit spärlichen Belohnungen; wir stellen fest, dass unser Ansatz effizienteres Lernen ermöglicht als sowohl1) das Training mit nur der spärlichen Belohnung als auch2) die Verwendung der typischen überraschungsbasierten Formulierung der intrinsischen Motivation, die nicht zu synergistischem Verhalten neigt.Videos sind auf der Projektwebseite verfügbar: https://sites.google.com/view/iclr2020-synergistic.
Eine allgemeine graphenstrukturierte neuronale Netzwerkarchitektur arbeitet auf Graphen durch zwei Kernkomponenten: (In diesem Papier stellen wir den Policy Message Passing Algorithmus vor, der eine probabilistische Perspektive einnimmt und die gesamte Informationsaggregation als stochastische sequentielle Prozesse neu formuliert.Der Algorithmus arbeitet auf einem viel größeren Suchraum, nutzt die Argumentationsgeschichte, um Schlussfolgerungen zu ziehen, und ist robust gegenüber verrauschten Kanten.Wir wenden unseren Algorithmus auf mehrere komplexe Graphen-Schlussfolgerungen und Vorhersageaufgaben an und zeigen, dass unser Algorithmus die modernsten graphenstrukturierten Modelle durchweg deutlich übertrifft.
Tiefe Multitasking-Netzwerke, bei denen ein neuronales Netzwerk mehrere Vorhersage-Outputs erzeugt, sind besser skalierbar und oft besser reguliert als ihre Single-Tasking-Gegenstücke, was zu Geschwindigkeits- und Leistungssteigerungen führen kann. Wir zeigen, dass GradNorm für verschiedene Netzwerkarchitekturen, sowohl für Regressions- als auch für Klassifikationsaufgaben und sowohl auf synthetischen als auch auf realen Datensätzen die Genauigkeit verbessert und die Überanpassung im Vergleich zu einzelnen Netzwerken, statischen Baselines und anderen adaptiven Multitask-Verlustausgleichstechniken reduziert. GradNorm erreicht oder übertrifft auch die Leistung von Methoden der erschöpfenden Gittersuche, obwohl nur ein einziger Asymmetrie-Hyperparameter $\alpha$ verwendet wird. So kann das, was früher ein langwieriger Suchprozess war, der für jede hinzugefügte Aufgabe exponentiell mehr Rechenaufwand erforderte, jetzt innerhalb von wenigen Trainingsläufen erreicht werden, unabhängig von der Anzahl der Aufgaben.Letztlich hoffen wir zu zeigen, dass die Gradientenmanipulation uns eine große Kontrolle über die Trainingsdynamik von Multitask-Netzwerken ermöglicht und einer der Schlüssel zur Erschließung des Potenzials des Multitask-Lernens sein kann.
Bei der Bildsegmentierung geht es darum, Pixel zu gruppieren, die zum selben Objekt oder zur selben Region gehören. Im Zentrum der Bildsegmentierung steht das Problem, zu bestimmen, ob ein Pixel innerhalb oder außerhalb einer Region liegt, was wir als "Insideness"-Problem bezeichnen. Viele Varianten von Deep Neural Networks (DNNs) schneiden bei Segmentierungs-Benchmarks hervorragend ab, aber in Bezug auf Insideness sind sie nicht gut visualisiert oder verstanden worden: Welche Repräsentationen verwenden DNNs, um die weitreichenden Beziehungen der Insideness zu adressieren?Wie beeinflusst die Wahl der Architektur das Lernen dieser Repräsentationen?In diesem Papier nehmen wir den reduktionistischen Ansatz, indem wir DNNs analysieren, die das Insideness-Problem isoliert lösen, d.h. das Innere geschlossener (Jordan-)Kurven bestimmen.Wir zeigen analytisch, dass modernste Feed-Forward- und rekurrente Architekturen Lösungen des Insideness-Problems für jede gegebene Kurve implementieren können. Unsere Ergebnisse unterstreichen die Notwendigkeit neuer Trainingsstrategien, die das Lernen in geeignete Phasen aufteilen und zu einer allgemeinen Klasse von Lösungen führen, die für DNNs notwendig sind, um Insideness zu verstehen.
Wir befassen uns mit dem herausfordernden Problem des tiefen Repräsentationslernens - der effizienten Anpassung eines vortrainierten tiefen Netzwerks an verschiedene Aufgaben - und schlagen vor, gradientenbasierte Merkmale zu erforschen, d.h. die Gradienten der Modellparameter in Bezug auf einen aufgabenspezifischen Verlust bei einer Eingabeprobe. Wir zeigen, dass unser Modell eine lokale lineare Annäherung an ein zugrundeliegendes tiefes Modell bietet, und diskutieren wichtige theoretische Einsichten.Darüber hinaus stellen wir einen effizienten Algorithmus für das Training und die Inferenz unseres Modells vor, ohne die tatsächlichen Gradienten zu berechnen.Unsere Methode wird über eine Reihe von Repräsentationslernaufgaben auf mehreren Datensätzen und unter Verwendung verschiedener Netzwerkarchitekturen evaluiert.Wir zeigen starke Ergebnisse in allen Einstellungen.Und unsere Ergebnisse sind gut mit unseren theoretischen Einsichten abgestimmt.
Die Wiederherstellung von 3D-Geometrie, Form, Albedo und Beleuchtung aus einem einzigen Bild hat breite Anwendungen in vielen Bereichen, die auch ein typisches schlecht gestelltes Problem.Um die Mehrdeutigkeit zu beseitigen, Gesicht Vorwissen wie lineare 3D morphable Modelle (3DMM) aus begrenzten Scan-Daten gelernt werden oft auf den Rekonstruktionsprozess übernommen. Neuere Methoden zielen darauf ab, ein nichtlineares parametrisches Modell mit Hilfe von Faltungsneuronalen Netzen (CNN) zu erlernen, um die Gesichtsform und -textur direkt zu regressieren. Allerdings wurden die Modelle nur auf einem Datensatz trainiert, der aus einem linearen 3DMM erzeugt wurde. In diesem Papier trainieren wir unser Modell mit adversarialem Verlust in einer halb-überwachten Weise auf hybriden Stapeln von unbeschrifteten und beschrifteten Gesichtsbildern, um den Wert von großen Mengen von unbeschrifteten Gesichtsbildern aus unbeschränkten Fotosammlungen zu nutzen. Ein neuartiger Zentrumsverlust wird eingeführt, um sicherzustellen, dass verschiedene Gesichtsbilder derselben Person dieselbe Identitätsform und Albedo haben. Außerdem werden in dem von uns vorgeschlagenen Modell Identitäts-, Ausdrucks-, Posen- und Beleuchtungsrepräsentationen getrennt, was die Gesamtleistung der Rekonstruktion verbessert und Gesichtsbearbeitungsanwendungen erleichtert, z. B. die Übertragung von Ausdrücken, Umfassende Experimente zeigen, dass unser Modell eine qualitativ hochwertige Rekonstruktion im Vergleich zu State-of-the-Art-Methoden und ist robust gegenüber verschiedenen Ausdruck, Pose, und Lichtverhältnissen.
Dieses Papier stellt ConceptFlow vor, das Commonsense-Wissensgraphen nutzt, um solche Gesprächsflüsse explizit zu modellieren und so eine bessere Gesprächsreaktion zu generieren.ConceptFlow begründet die Gesprächseingaben auf den latenten Konzeptraum und stellt den potenziellen Gesprächsfluss als Konzeptfluss entlang der Commonsense-Beziehungen dar. Das Konzept wird durch einen Graph-Attention-Mechanismus geleitet, der die Möglichkeit modelliert, dass sich die Konversation in Richtung verschiedener Konzepte entwickelt.Die Konversationsantwort wird dann unter Verwendung der Kodierungen sowohl der Äußerungstexte als auch der Konzeptflüsse dekodiert, wobei die gelernte Konversationsstruktur in den Konzeptraum integriert wird.Unsere Experimente mit Reddit-Konversationen zeigen den Vorteil von ConceptFlow gegenüber früheren Commonsense-bewussten Dialogmodellen und fein abgestimmten GPT-2-Modellen, während viel weniger Parameter verwendet werden, aber mit expliziter Modellierung von Konversationsstrukturen.
Biologische neuronale Netze unterliegen homöostatischen und ressourcenbezogenen Beschränkungen, die die zulässigen Konfigurationen der Verbindungsgewichte einschränken. Wenn eine Beschränkung eng ist, definiert sie einen sehr kleinen Lösungsraum, und die Größe dieser Beschränkungsräume bestimmt ihre potenzielle Überlappung mit den Lösungen für Rechenaufgaben. Wir untersuchen die Geometrie der Lösungsräume für Beschränkungen des gesamten synaptischen Gewichts der Neuronen und der individuellen synaptischen Gewichte und charakterisieren die Verbindungsgrade (Anzahl der Partner), die die Größe dieser Lösungsräume maximieren. Wir stellen die Hypothese auf, dass die Größe der Lösungsräume von Beschränkungen als Kostenfunktion für die Entwicklung neuronaler Schaltkreise dienen könnte, und entwickeln analytische Näherungen und Grenzen für den Modellbeweis der maximalen Entropiegradverteilungen unter diesen Kostenfunktionen, die wir an einem veröffentlichten elektronenmikroskopischen Konnektom eines assoziativen Lernzentrums im Fliegenhirn testen und Hinweise auf eine Entwicklungsprogression in der Schaltkreisstruktur finden.
In dieser Vorarbeit untersuchen wir die Generalisierungseigenschaften von unendlichen Ensembles unendlich breiter neuronaler Netze.  Erstaunlich ist, dass diese Modellfamilie für viele informationstheoretische Größen nachvollziehbare Berechnungen zulässt.  Wir berichten über analytische und empirische Untersuchungen auf der Suche nach Signalen, die mit Generalisierung korrelieren.
Das Lernen mehrsprachiger Repräsentationen von Texten hat sich als erfolgreiche Methode für viele sprachübergreifende Transfer-Lernaufgaben erwiesen: (1) Alignment, bei dem verschiedene unabhängig voneinander trainierte einsprachige Repräsentationen auf einen gemeinsamen Raum abgebildet werden, und (2) gemeinsames Training, bei dem einheitliche mehrsprachige Repräsentationen unter Verwendung von ein- und zweisprachigen Zielen gemeinsam erlernt werden.In diesem Beitrag führen wir zunächst direkte Vergleiche von Repräsentationen durch, die mit diesen beiden Methoden für verschiedene sprachübergreifende Aufgaben erlernt wurden.Unsere empirischen Ergebnisse offenbaren eine Reihe von Vor- und Nachteilen für beide Methoden und zeigen, dass die relative Leistung von Alignment gegenüber gemeinsamem Training aufgabenabhängig ist. Ausgehend von dieser Analyse schlagen wir ein einfaches und neuartiges Rahmenwerk vor, das diese beiden sich bisher gegenseitig ausschließenden Ansätze kombiniert. umfangreiche Experimente an verschiedenen Aufgaben zeigen, dass unser vorgeschlagenes Rahmenwerk die Einschränkungen beider Ansätze mildert und bestehende Methoden beim MUSE Benchmark für zweisprachige Lexikoninduktion (BLI) übertrifft. wir zeigen außerdem, dass unser vorgeschlagenes Rahmenwerk auf kontextualisierte Repräsentationen verallgemeinert werden kann und beim CoNLL Benchmark für sprachenübergreifende NER Ergebnisse auf dem neuesten Stand der Technik erzielt.
Die große Anzahl von Gewichten in tiefen neuronalen Netzen erschwert den Einsatz der Modelle in Umgebungen mit geringem Speicherplatz, wie z. B. Mobiltelefone, IOT-Edge-Geräte sowie "Inferencing as a Service"-Umgebungen in der Cloud. In früheren Arbeiten wurde die Verringerung der Größe der Modelle durch Kompressionstechniken wie Weight Pruning, Filter Pruning usw. oder durch Low-Rank-Zerlegung der Faltungsschichten in Betracht gezogen. Wir zeigen, dass unser Ansatz eine bis zu 57% höhere Modellkompression im Vergleich zu Tucker-Dekomposition oder Filter Pruning allein bei ähnlicher Genauigkeit für GoogleNet erreicht und außerdem die Flops um bis zu 48% reduziert, wodurch die Inferenz schneller wird.
Wir überprüfen die Grenzen von BLEU und ROUGE - den beliebtesten Metriken, die zur Bewertung von Referenzzusammenfassungen gegenüber Hypothesenzusammenfassungen verwendet werden - und stellen JAUNE vor: eine Reihe von Kriterien dafür, wie sich eine gute Metrik verhalten sollte, und schlagen konkrete Möglichkeiten vor, aktuelle transformatorbasierte Sprachmodelle zur Bewertung von Referenzzusammenfassungen gegenüber Hypothesenzusammenfassungen zu verwenden.
Viele Standard-GNN-Varianten propagieren Informationen entlang der Kanten eines Graphen, indem sie ``Nachrichten'' berechnen, die nur auf der Repräsentation der Quelle jeder Kante basieren. In GNN-FiLM wird die Repräsentation des Zielknotens einer Kante zusätzlich verwendet, um eine Transformation zu berechnen, die auf alle eingehenden Nachrichten angewandt werden kann, was eine merkmalsweise Modulation der weitergegebenen Informationen ermöglicht. Die Ergebnisse von Experimenten, in denen verschiedene GNN-Architekturen mit drei Aufgaben aus der Literatur verglichen wurden, basieren auf Re-Implementierungen von Basismethoden. Hyperparameter für alle Methoden wurden durch extensive Suche gefunden, was zu etwas überraschenden Ergebnissen führte: Die Unterschiede zwischen den Basismodellen sind geringer als in der Literatur angegeben.
Für die gleichzeitige Behandlung der attributiven Netzwerkeinbettung und des Clustering schlagen wir ein neues Modell vor, das sowohl Inhalts- als auch Strukturinformationen ausnutzt, indem es sich deren gleichzeitige Verwendung zunutze macht.Das vorgeschlagene Modell beruht auf der Annäherung der entspannten kontinuierlichen Einbettungslösung durch die wahre diskrete Clustering-Lösung. Experimentelle Ergebnisse zeigen, dass der vorgeschlagene Algorithmus in Bezug auf Clustering und Einbettung besser abschneidet als die aktuellen Algorithmen, einschließlich Deep-Learning-Methoden, die für ähnliche Aufgaben für zugeschriebene Netzwerkdatensätze mit unterschiedlichen Eigenschaften eingesetzt werden.
Wir schlagen eine gelernte, bildgesteuerte Rendering-Technik vor, die die Vorteile von bildbasiertem Rendering und GAN-basierter Bildsynthese kombiniert, um fotorealistische Re-Renderings von rekonstruierten Objekten für Virtual- und Augmented-Reality-Anwendungen zu erzeugen (z.B., Ein zentraler Bestandteil unserer Arbeit ist die Behandlung von sichtabhängigen Effekten, d.h. wir trainieren direkt ein objektspezifisches tiefes neuronales Netz, um das sichtabhängige Erscheinungsbild eines Objekts zu synthetisieren. Als Eingabedaten verwenden wir ein RGB-Video des Objekts. Dieses Video wird verwendet, um eine Proxy-Geometrie des Objekts über Multi-View-Stereo zu rekonstruieren. Basierend auf diesem 3D-Proxy kann das Erscheinungsbild einer aufgenommenen Ansicht in eine neue Zielansicht gewarpt werden, wie im klassischen bildbasierten Rendering. Dieses Warping geht von diffusen Oberflächen aus, im Falle von sichtabhängigen Effekten, wie z.B. Glanzlichtern, führt es zu Artefakten. Basierend auf diesen Schätzungen sind wir in der Lage, beobachtete Bilder in diffuse Bilder zu konvertieren, die dann in andere Ansichten projiziert werden können, wobei unsere Pipeline die neuen sichtabhängigen Effekte in die Zielansicht einfügt. Um mehrere reprojizierte Bilder zu einer endgültigen Ausgabe zusammenzusetzen, erlernen wir ein Kompositionsnetzwerk, das fotorealistische Ergebnisse ausgibt. Bei diesem bildgesteuerten Ansatz muss das Netzwerk keine Kapazität für das "Erinnern" an das Aussehen von Objekten bereitstellen, sondern lernt, wie es das Aussehen der aufgenommenen Bilder kombiniert.
Wir evaluieren die Verteilungslernfähigkeiten von generativen adversen Netzwerken, indem wir sie an synthetischen Datensätzen testen, die allgemeine Verteilungen von Punkten im $R^n$-Raum und Bilder mit Polygonen verschiedener Formen und Größen enthalten. Wir stellen fest, dass GANs im Großen und Ganzen nicht in der Lage sind, Punktdatensätze, die eine diskontinuierliche Unterstützung oder scharfe Biegungen mit Rauschen enthalten, originalgetreu nachzubilden, und dass GANs bei Bilddatensätzen nicht zu lernen scheinen, die Anzahl von Objekten der gleichen Art in einem Bild zu zählen.
Die vorgeschlagene Methode, die so genannte Schrittgrößenoptimierung (SSO), formuliert die Schrittgrößenanpassung als Optimierungsproblem, das die Verlustfunktion in Bezug auf die Schrittgröße für die gegebenen Modellparameter und Gradienten minimiert, und optimiert die Schrittgröße auf der Grundlage der Methode der alternierenden Richtung der Multiplikatoren (ADMM). SSO erfordert keine Informationen zweiter Ordnung oder probabilistische Modelle für die Anpassung der Schrittgröße, so dass es effizient und einfach zu implementieren ist.Darüber hinaus führen wir auch stochastische SSO für stochastische Lernumgebungen ein.In den Experimenten haben wir SSO in Vanilla SGD und Adam integriert, und sie übertrafen die modernsten adaptiven Gradientenmethoden, einschließlich RMSProp, Adam, L4-Adam und AdaBound auf umfangreichen Benchmark-Datensätzen.
Trotz der Tatsache, dass generative Modelle in der Praxis äußerst erfolgreich sind, beginnt die Theorie, die diesem Phänomen zugrunde liegt, erst allmählich mit der Praxis gleichzuziehen.In dieser Arbeit befassen wir uns mit der Frage der Universalität generativer Modelle: Stimmt es, dass neuronale Netze jede beliebige Datenvielfalt beliebig gut approximieren können? Wir geben eine positive Antwort auf diese Frage und zeigen, dass man unter milden Annahmen über die Aktivierungsfunktion immer ein neuronales Netzwerk mit Vorwärtskopplung finden kann, das den latenten Raum auf eine Menge abbildet, die sich innerhalb der spezifizierten Hausdorff-Distanz von der gewünschten Datenvielfalt befindet.Wir beweisen auch ähnliche Theoreme für den Fall von generativen Mehrklassenmodellen und generativen Zyklusmodellen, die darauf trainiert sind, Proben von einer Vielfältigkeit auf eine andere abzubilden und umgekehrt.
Modellbasiertes Reinforcement Learning (RL) gilt als vielversprechender Ansatz, um die Komplexität von Stichproben zu reduzieren, die modellfreies RL behindert. Allerdings ist das theoretische Verständnis solcher Methoden bisher eher begrenzt. Der Meta-Algorithmus baut iterativ eine untere Schranke der erwarteten Belohnung auf, die auf dem geschätzten dynamischen Modell und den Beispieltrajektorien basiert, und maximiert dann die untere Schranke gemeinsam über die Politik und das Modell. Die Instanziierung unseres Rahmens mit Vereinfachung ergibt eine Variante des modellbasierten RL-Algorithmus Stochastic Lower Bounds Optimization (SLBO). Experimente zeigen, dass SLBO die modernste Leistung erreicht, wenn nur 1 Mio. oder weniger Stichproben für eine Reihe von kontinuierlichen Kontroll-Benchmark-Aufgaben zulässig sind.
Wir zeigen, dass die Standard-Destillation zwar nicht ausreicht, um ein komprimiertes U-Netz zuverlässig zu trainieren, dass aber die Einführung anderer Regularisierungsmethoden wie Batch-Normalisierung und Klassen-Neugewichtung in die Wissensdestillation den Trainingsprozess erheblich verbessert, so dass wir ein U-Netz um mehr als das 1000-fache komprimieren können, d. h. auf 0,1 % seiner ursprünglichen Parameteranzahl, bei einer vernachlässigbaren Leistungsabnahme.
Das Lernen neuronaler Netze mit Gradientenabstieg Ã?ber eine lange Abfolge von Aufgaben ist problematisch, da die Feinabstimmung auf neue Aufgaben die Netzgewichte Ã?berschreibt, die fÃ?r frÃ?here Aufgaben wichtig sind, was zu einer schlechten Leistung bei alten Aufgaben fÃ?hrt â€" ein PhÃ?nomen, das als katastrophales Vergessen bezeichnet wird.  WÃ?hrend frÃ?here AnsÃ?tze Aufgabenwiederholung und wachsende Netzwerke verwenden, die beide die Skalierbarkeit der Aufgabensequenz begrenzen, bauen orthogonale AnsÃ?tze auf Regularisierung.  Basierend auf der Fisher-Informationsmatrix (FIM) werden Änderungen an Parametern, die für alte Aufgaben relevant sind, bestraft, was dazu führt, dass die Aufgabe auf die verfügbare Restkapazität des Netzwerks abgebildet wird, was die Berechnung der Hessian um einen Modus herum erfordert, was das Lernen überschaubar macht.In diesem Papier führen wir Hessian-freie Krümmungsschätzungen als alternative Methode zur Berechnung der Hessian ein.  Im Gegensatz zu früheren Arbeiten nutzen wir die Tatsache aus, dass die meisten Regionen in der Verlustfläche flach sind und berechnen daher nur ein Hessian-Vektorprodukt um die Fläche, die für die aktuelle Aufgabe relevant ist.unsere Experimente zeigen, dass wir bei einer Vielzahl bekannter Aufgabensequenzen entweder deutlich besser abschneiden oder mit früheren Arbeiten gleichauf sind.
In jüngster Zeit hat sich das Interesse an der Verbesserung der Leistung von einfachen Modellen aus mehreren Gründen, wie z. B. Interpretierbarkeit, robustes Lernen aus kleinen Daten, Einsatz in speicherbeschränkten Umgebungen sowie Umweltüberlegungen.In diesem Papier schlagen wir eine neuartige Methode SRatio, die Informationen aus leistungsstarken komplexen Modellen (dh. Unsere Methode nutzt auch die pro Stichprobe geschätzte Härte des einfachen Modells, was bei früheren Arbeiten, die in erster Linie die Konfidenz/Vorhersagen des komplexen Modells berücksichtigen, nicht der Fall ist, und ist daher konzeptionell neu. Darüber hinaus verallgemeinern und formalisieren wir das Konzept des Anhängens von Sonden an Zwischenschichten eines neuronalen Netzes, das eine der Hauptideen in früheren Arbeiten war, auf andere häufig verwendete Klassifikatoren und integrieren es in unsere Methode. Der Nutzen dieser Beiträge zeigt sich in den Experimenten, in denen wir bei 6 UCI-Datensätzen und CIFAR-10 in der Mehrheit der Fälle (16 von 27) die Konkurrenten übertreffen und in den übrigen Fällen die beste Leistung erzielen. Wir führen auch weitere Experimente durch, um Behauptungen zu validieren und intuitiv zu verstehen, warum unsere Methode funktioniert.Theoretisch motivieren wir unseren Ansatz, indem wir zeigen, dass der gewichtete Verlust, der durch einfache Modelle mit unserer Gewichtung minimiert wird, den Verlust des komplexen Modells nach oben begrenzt.
Wir schlagen eine prinzipielle Methode für das Kernel-Lernen vor, die auf einer Fourier-analytischen Charakterisierung von translations- oder rotationsinvarianten Kerneln beruht und eine Sequenz von Merkmalskarten erzeugt, die den SVM-Rand iterativ verfeinern, indem wir strenge Garantien für Optimalität und Verallgemeinerung geben, indem wir unseren Algorithmus als Online-Gleichgewichtsfindungsdynamik in einem bestimmten Min-Max-Spiel mit zwei Spielern interpretieren.
Wir zeigen, wie dies nativ in der probabilistischen Programmierung implementiert werden kann.Durch die Berücksichtigung der Struktur der kontrafaktischen Abfrage kann der Inferenzprozess signifikant optimiert werden.Wir betrachten auch Design-Entscheidungen, um weitere Optimierungen zu ermöglichen.Wir stellen MultiVerse vor, eine probabilistische Programmier-Prototyp-Engine für approximative kausale Schlussfolgerungen.Wir liefern experimentelle Ergebnisse und vergleichen mit Pyro, einem bestehenden probabilistischen Programmier-Framework mit einigen kausalen Reasoning-Tools.
Wir betrachten das Problem der Darstellung des kollektiven Verhaltens großer Populationen und der Vorhersage der Entwicklung einer Populationsverteilung über einen diskreten Zustandsraum. Ein zeitdiskretes Mean-Field-Game (MFG) wird als interpretierbares Modell motiviert, das auf der Spieltheorie basiert, um den Gesamteffekt individueller Aktionen zu verstehen und die zeitliche Entwicklung von Populationsverteilungen vorherzusagen. Wir erreichen eine Synthese von MFG und Markov-Entscheidungsprozessen (MDP), indem wir zeigen, dass ein spezielles MFG auf ein MDP reduzierbar ist. Dies ermöglicht es uns, den Anwendungsbereich der Mean-Field-Game-Theorie zu erweitern und MFG-Modelle großer realer Systeme durch tiefes inverses Reinforcement Learning abzuleiten. Unsere Methode lernt sowohl die Belohnungsfunktion als auch die Vorwärtsdynamik eines MFG aus realen Daten, und wir berichten über den ersten empirischen Test eines Mean-Field-Game-Modells einer realen Population sozialer Medien.
Wir untersuchen das Problem der Ausbildung sequentieller generativer Modelle zur Erfassung von koordiniertem Multi-Agenten-Trajektorienverhalten, wie z.B. offensives Basketballspiel.  Bei der Modellierung solcher Situationen ist es oft von Vorteil, hierarchische Modelle zu entwerfen, die die langfristige Koordination mit Hilfe von Zwischenvariablen erfassen können.  Darüber hinaus sollten diese Zwischenvariablen interessante High-Level-Verhaltenssemantiken in einer interpretierbaren und manipulierbaren Weise erfassen.wir präsentieren einen hierarchischen Rahmen, der solche sequentiellen generativen Modelle effektiv lernen kann.  Zusätzlich zu synthetischen Einstellungen zeigen wir, wie unser Rahmenwerk eingesetzt werden kann, um komplexe Interaktionen zwischen Basketballspielern effektiv zu modellieren und realistische Multi-Agenten-Trajektorien des Basketballspiels über lange Zeiträume zu generieren.Wir validieren unseren Ansatz mit Hilfe von quantitativen und qualitativen Auswertungen, einschließlich eines Benutzerstudienvergleichs mit professionellen Sportanalysten.
Viele automatisierte Methoden des maschinellen Lernens, wie z.B. die Optimierung von Hyperparametern und neuronaler Architektur, sind rechenintensiv, da sie das Training vieler verschiedener Modellkonfigurationen erfordern. Im Gegensatz zu bestehenden Verfahren betrachten wir diese Aufgabe als Ranking- und Transfer-Learning-Problem und zeigen qualitativ, dass unser Modell durch die Optimierung eines paarweisen Ranking-Verlustes und die Nutzung von Lernkurven aus anderen Datensätzen in der Lage ist, Lernkurven effektiv zu ranken, ohne dass viele oder sehr lange Lernkurven beobachtet werden müssen.Wir zeigen weiter, dass unsere Methode verwendet werden kann, um die Suche nach einer neuronalen Architektur um einen Faktor von bis zu 100 zu beschleunigen, ohne dass es zu einer signifikanten Leistungsverschlechterung der gefundenen Architektur kommt.In weiteren Experimenten analysieren wir die Qualität des Rankings, den Einfluss verschiedener Modellkomponenten sowie das Vorhersageverhalten des Modells.
Neuronale Netze, die durch stochastischen Gradientenabstieg trainiert werden, verschlechtern sich oft bei alten Aufgaben, wenn sie nacheinander auf neue Aufgaben mit unterschiedlichen Datenverteilungen trainiert werden. Dieses Phänomen, das als katastrophales Vergessen bezeichnet wird, gilt als eine große Hürde für das Lernen mit nicht-stationären Daten oder Sequenzen von neuen Aufgaben und verhindert, dass Netzwerke kontinuierlich Wissen und Fähigkeiten ansammeln.Wir untersuchen dieses Problem im Kontext des Verstärkungslernens in einer Umgebung, in der ein Agent Aufgaben in einer Sequenz ausgesetzt ist.Im Gegensatz zu den meisten anderen Arbeiten geben wir dem Modell keine expliziten Hinweise auf Aufgabengrenzen, was der allgemeinste Umstand für einen lernenden Agenten ist, der kontinuierlicher Erfahrung ausgesetzt ist. Während in letzter Zeit verschiedene Methoden vorgeschlagen wurden, um dem katastrophalen Vergessen entgegenzuwirken, erforschen wir eine unkomplizierte, allgemeine und scheinbar übersehene Lösung - die Verwendung von Erfahrungswiederholungspuffern für alle vergangenen Ereignisse - mit einer Mischung aus On- und Off-Policy-Lernen, die das Klonen von Verhalten nutzt. Wir zeigen, dass diese Strategie neue Aufgaben immer noch schnell erlernen kann und gleichzeitig das katastrophale Vergessen sowohl in Atari- als auch in DMLab-Domänen erheblich reduzieren kann und sogar die Leistung von Methoden erreicht, die Aufgabenidentitäten erfordern.
Wir stellen eine Methode vor, die lernt, zeitliche Informationen aus einem gelernten Dynamikmodell mit mehrdeutigen visuellen Informationen aus einem gelernten Sichtmodell im Kontext interagierender Agenten zu integrieren. Unsere Methode basiert auf einem graph-strukturierten rekurrenten neuronalen Netz, das Ende-zu-Ende trainiert wird, um den aktuellen Zustand der (teilweise beobachteten) Welt abzuleiten sowie zukünftige Zustände vorherzusagen.Wir zeigen, dass unsere Methode verschiedene Basisdaten auf zwei Sportdatensätzen übertrifft, einen, der auf realen Basketball-Trajektorien basiert, und einen, der von einer Fußballspiel-Engine erzeugt wurde.
In diesem Papier untersuchen wir das Problem des Lernens der Gewichte eines tiefen neuronalen Faltungsnetzwerks.Wir betrachten ein Netzwerk, in dem Faltungen über nicht überlappende Patches mit einem einzigen Kernel in jeder Schicht durchgeführt werden.Wir entwickeln einen Algorithmus für das gleichzeitige Lernen aller Kernel aus den Trainingsdaten.Unser Ansatz, Deep Tensor Decomposition (DeepTD) genannt, basiert auf einer Rang-1-Tensor-Dekomposition. Wir untersuchen DeepTD theoretisch unter einem realisierbaren Modell für die Trainingsdaten, bei dem die Eingaben i.i.d. aus einer Gauß-Verteilung gewählt werden und die Beschriftungen gemäß gepflanzter Faltungskerne generiert werden.Wir zeigen, dass DeepTD dateneffizient ist und nachweislich funktioniert, sobald die Stichprobengröße die Gesamtzahl der Faltungsgewichte im Netzwerk übersteigt.Unsere numerischen Experimente zeigen die Effektivität von DeepTD und verifizieren unsere theoretischen Erkenntnisse.
Techniken zum Pruning neuronaler Netze können die Anzahl der Parameter trainierter Netze um mehr als 90 % reduzieren, was den Speicherbedarf senkt und die Rechenleistung der Inferenz verbessert, ohne die Genauigkeit zu beeinträchtigen.die heutige Erfahrung zeigt jedoch, dass die durch Pruning erzeugten spärlichen Architekturen von Anfang an schwer zu trainieren sind, was die Trainingsleistung ebenfalls verbessern würde. Auf der Grundlage dieser Ergebnisse formulieren wir die "Lotterielos-Hypothese": dichte, zufällig initialisierte Feed-Forward-Netzwerke enthalten Teilnetzwerke ("Gewinnlose"), die - wenn sie isoliert trainiert werden - eine mit dem ursprünglichen Netzwerk vergleichbare Testgenauigkeit in einer ähnlichen Anzahl von Iterationen erreichen. Wir stellen einen Algorithmus zur Identifizierung von Gewinnertickets und eine Reihe von Experimenten vor, die die Hypothese der Gewinnertickets und die Bedeutung dieser zufälligen Initialisierungen untermauern. Wir finden durchgängig Gewinnertickets, die weniger als 10-20% der Größe mehrerer voll verbundener und faltungsbasierter Feed-Forward-Architekturen für MNIST und CIFAR10 betragen.Über diese Größe hinaus lernen die Gewinnertickets, die wir finden, schneller als das ursprüngliche Netzwerk und erreichen eine höhere Testgenauigkeit.
Wir untersuchen die Schwierigkeiten beim Training spärlicher neuronaler Netze und machen neue Beobachtungen über die Optimierungsdynamik und die Energielandschaft innerhalb des spärlichen Regimes.Jüngste Arbeiten von \citep{Gale2019, Liu2018} haben gezeigt, dass spärliche ResNet-50-Architekturen, die auf dem ImageNet-2012-Datensatz trainiert wurden, zu Lösungen konvergieren, die deutlich schlechter sind als die, die durch Pruning gefunden wurden.Wir zeigen, dass es trotz des Versagens der Optimierer einen linearen Pfad mit einem monoton abnehmenden Ziel von der Initialisierung bis zur ``guten'' Lösung gibt. Darüber hinaus scheitern unsere Versuche, einen abnehmenden Zielpfad von den ``schlechten'' Lösungen zu den ``guten'' Lösungen im dünnbesetzten Unterraum zu finden; wenn wir jedoch zulassen, dass der Pfad den dichten Unterraum durchquert, finden wir durchweg einen Pfad zwischen zwei Lösungen, was darauf hindeutet, dass das Durchqueren zusätzlicher Dimensionen erforderlich sein kann, um stationäre Punkte im dünnbesetzten Unterraum zu umgehen.
Das Training neuronaler Netze hängt von der Struktur der zugrundeliegenden Verlustlandschaft ab, d.h. von lokalen Minima, Sattelpunkten, flachen Plateaus und Verlustbarrieren. In Bezug auf die Struktur der Landschaft untersuchen wir die Permutationssymmetrie der Neuronen in jeder Schicht eines tiefen neuronalen Netzes, die nicht nur zu mehreren äquivalenten globalen Minima der Verlustfunktion führt, sondern auch zu kritischen Punkten zwischen den Partnerminima. In einem Netzwerk aus $d-1$ versteckten Schichten mit $n_k$ Neuronen in den Schichten $k = 1, \ldots, d$ konstruieren wir kontinuierliche Pfade zwischen äquivalenten globalen Minima, die durch einen "Permutationspunkt" führen, an dem die Eingangs- und Ausgangsgewichtsvektoren zweier Neuronen in derselben versteckten Schicht $k$ kollidieren und sich austauschen. Wir zeigen, dass solche Permutationspunkte kritische Punkte sind, die in hochdimensionalen Unterräumen mit gleichem Verlust liegen und zur globalen Flachheit der Landschaft beitragen, und dass ein Permutationspunkt für den Austausch von Neuronen $i$ und $j$ in ein flaches hochdimensionales Plateau übergeht, das alle $n_k!$Permutationen von Neuronen in einer gegebenen Schicht $k$ mit demselben Verlustwert ermöglicht. Darüber hinaus führen wir Permutationspunkte höherer Ordnung ein, indem wir die hierarchische Struktur in den Verlustlandschaften neuronaler Netze ausnutzen, und stellen fest, dass die Anzahl der Permutationspunkte $K$-ter Ordnung viel größer ist als die (ohnehin schon riesige) Anzahl äquivalenter globaler Minima - zumindest um einen Polynomialfaktor der Ordnung $K$. In zwei Aufgaben demonstrieren wir numerisch mit unserer Pfadfindungsmethode, dass kontinuierliche Pfade zwischen Partnerminima existieren: erstens in einem Spielzeugnetz mit einer einzelnen versteckten Schicht in einer Funktionsapproximationsaufgabe und zweitens in einem mehrschichtigen Netz in der MNIST-Aufgabe. Unser geometrischer Ansatz liefert eine untere Schranke für die Anzahl der kritischen Punkte, die durch Gewichtsraum-Symmetrien erzeugt werden, und bietet eine einfache intuitive Verbindung zwischen früheren theoretischen Ergebnissen und numerischen Beobachtungen.
Das Training von stochastischen neuronalen Netzmodellen mit binären ($\pm1$) Gewichten und Aktivierungen über kontinuierliche Surrogatnetze wird untersucht: Wir leiten mit Hilfe der Mean-Field-Theorie eine Reihe von skalaren Gleichungen ab, die beschreiben, wie sich Eingangssignale durch Surrogatnetze ausbreiten. Die Gleichungen zeigen, dass die Netze je nach Wahl des Surrogatmodells einen Übergang von Ordnung zu Chaos und das Vorhandensein von Tiefenskalen, die die maximale trainierbare Tiefe begrenzen, aufweisen können oder nicht. Die Theorie wird auf eine Reihe von binären Neuronen und Gewichtsdesigns angewandt, wie z.B. verschiedene Neuronenrauschmodelle, was die Kategorisierung von Algorithmen hinsichtlich ihres Verhaltens bei der Initialisierung ermöglicht. Darüber hinaus wird theoretisch vorhergesagt und numerisch bestätigt, dass gängige Gewichtsinitialisierungsschemata, die in standardmäßigen kontinuierlichen Netzen verwendet werden, bei Anwendung auf die Mittelwerte der stochastischen binären Gewichte zu einer schlechten Trainingsleistung führen, und es wird gezeigt, dass die Mittelwerte der stochastischen binären Gewichte entgegen der üblichen Intuition nahe bei $\pm 1$ initialisiert werden sollten, damit tiefere Netze trainierbar sind.
Semantisches Dependenz-Parsing, das darauf abzielt, reichhaltige bi-lexikalische Beziehungen zu finden, erlaubt es, dass Wörter mehrere Dependenzköpfe haben, was zu graph-strukturierten Darstellungen führt.Wir schlagen einen Ansatz zum semi-supervised Lernen von semantischen Dependenz-Parsern vor, der auf dem CRF-Autoencoder-Framework basiert.Unser Encoder ist ein diskriminativer neuronaler semantischer Dependenz-Parser, der den latenten Parse-Graphen des Eingabesatzes vorhersagt. Unser Decoder ist ein generatives neuronales Modell, das den Eingabesatz auf der Grundlage des latenten Parse-Graphen rekonstruiert. Unser Modell ist arc-faktorisiert und daher sind sowohl das Parsen als auch das Lernen überschaubar.
In dieser Arbeit beschreiben wir eine neue Methode, DeFINE, zum effizienten Erlernen von tiefen Repräsentationen auf Wortebene.Unsere Architektur verwendet eine hierarchische Struktur mit neuartigen Skip-Verbindungen, die die Verwendung von niedrigdimensionalen Eingabe- und Ausgabeschichten ermöglicht, wodurch die Gesamtparameter und die Trainingszeit reduziert werden, während eine ähnliche oder bessere Leistung im Vergleich zu bestehenden Methoden erzielt wird.DeFINE kann leicht in neue oder bestehende Sequenzmodelle integriert werden. Im Vergleich zu State-of-the-Art-Methoden mit adaptiven Eingabedarstellungen führt diese Technik zu einer Verringerung der Perplexität um 6 % bis 20 %.Auf WikiText-103 reduziert DeFINE die Gesamtparameter von Transformer-XL um die Hälfte bei minimaler Auswirkung auf die Leistung.Auf der Penn Treebank verbessert DeFINE AWD-LSTM um 4 Punkte bei einer 17 %igen Verringerung der Parameter und erreicht damit eine vergleichbare Leistung wie State-of-the-Art-Methoden mit weniger Parametern.Bei der maschinellen Übersetzung verbessert DeFINE ein Transformer-Modell um 2 % bei gleichzeitiger Reduzierung der Gesamtparameter um 26 %.
In diesem Papier präsentieren wir eine Reproduktion des Papiers von Bertinetto et al. [2019] "Meta-learning with differentiable closed-form solvers" als Teil der ICLR 2019 Reproducibility Challenge.Durch die erfolgreiche Reproduktion des wichtigsten Teils des Papiers erreichen wir eine Leistung, die mit dem Original-Papier auf zwei Benchmarks für mehrere Einstellungen vergleichbar oder überlegen ist.Wir evaluieren neue Baseline-Ergebnisse, unter Verwendung eines neuen Datensatzes, der im Papier vorgestellt wurde.Wir geben jedoch auch mehrere Anmerkungen und Empfehlungen zur Reproduzierbarkeit und Vergleichbarkeit.  Nachdem wir die Autoren auf unsere Arbeit zur Reproduzierbarkeit aufmerksam gemacht haben, haben sie das Originalpapier, auf dem diese Arbeit basiert, aktualisiert und auch den Code verÃ¶ffentlicht.â€œ Unsere BeitrÃ?ge bestehen hauptsÃ?chlich darin, die wichtigsten Ergebnisse des Originalpapiers zu reproduzieren, einen Einblick in die Reproduzierbarkeit zu geben und eine erste Open-Source-Implementierung bereitzustellen.
In letzter Zeit sind alternative Techniken aufgetaucht, um spärliche Netzwerke direkt zu trainieren, ohne vorher ein großes, dichtes Modell trainieren zu müssen, und so einen kleinen Speicherbedarf sowohl beim Training als auch bei der Inferenz zu erreichen. Diese Techniken basieren auf einer dynamischen Neuzuweisung von Parametern, die nicht Null sind, während des Trainings und führen somit eine Suche nach dem optimalen Teilnetzwerk während der Trainingszeit durch. Wir untersuchen eine der neuesten dieser Techniken und führen zusätzliche Experimente durch, um ihr Verhalten bei der Ausbildung von spärlichen tiefen Faltungsnetzen zu erhellen.Dynamische Parameterumverteilung konvergiert früh während des Trainings zu einem hoch trainierbaren Teilnetz.Wir zeigen, dass weder die Struktur noch die Initialisierung des entdeckten Hochleistungs-Teilnetzes ausreicht, um seine gute Leistung zu erklären. Wir zeigen, dass weder die Struktur noch die Initialisierung des entdeckten Hochleistungs-Teilnetzes ausreichen, um seine gute Leistung zu erklären, sondern dass vielmehr die Dynamik der Parameter-Neuzuordnung für den Lernerfolg verantwortlich ist: Die dynamische Parameter-Neuzuordnung verbessert also die Trainierbarkeit tiefer Faltungsnetze und spielt eine ähnliche Rolle wie die Überparametrisierung, ohne die Speicher- und Rechenkosten der letzteren zu verursachen.
In diesem Papier präsentieren wir einen Vorstoß in drei Richtungen der visuellen Entwicklung unter Verwendung überwachter und halbüberwachter Techniken: Die erste ist eine Implementierung der halbüberwachten Objekterkennung und -detektion unter Verwendung der Prinzipien der Soft At- tention und der Generative Adversarial Networks (GANs). Die zweite und die dritte sind überwachte Netzwerke, die grundlegende Konzepte der räumlichen Lokalität bzw. Quantität unter Verwendung von Convolutional Neural Networks (CNNs) erlernen.Die drei Ansätze zusammen basieren auf dem Ansatz des Experiential Robot Learning, der in einer früheren Veröffentlichung vorgestellt wurde.Obwohl die Ergebnisse noch nicht reif für die Umsetzung sind, glauben wir, dass sie ein Sprungbrett für die autonome Entwicklung von Roboter-Vitalmodulen darstellen.
Die Charakterisierung der Repräsentationen, die in den Zwischenschichten von tiefen Netzen gelernt werden, kann einen wertvollen Einblick in die Natur einer Aufgabe geben und die Entwicklung von maßgeschneiderten Lernstrategien anleiten.Hier untersuchen wir auf Faltungsneuronalen Netzen basierende akustische Modelle im Kontext der automatischen Spracherkennung.In Anlehnung an eine von Yosinski et al. [2014] vorgeschlagene Methode messen wir die Übertragbarkeit jeder Schicht zwischen Deutsch und Englisch, um ihre Sprachspezifität zu bewerten.Wir beobachten drei verschiedene Regionen der Übertragbarkeit: (1) die ersten beiden Schichten sind vollständig zwischen den Sprachen übertragbar, (2) die Schichten 2â€"8 sind ebenfalls hochgradig übertragbar, aber wir finden Hinweise auf eine gewisse Sprachspezifität, (3) die nachfolgenden voll verknüpften Schichten sind eher sprachspezifisch, können aber erfolgreich auf die Zielsprache abgestimmt werden.Um den Effekt des Einfrierens von Gewichten weiter zu untersuchen, haben wir Folgeexperimente mit Freeze-Training durchgeführt [Raghu et al., 2017].Unsere Ergebnisse stimmen mit der Beobachtung überein, dass CCNs während des Trainings "von unten nach oben" konvergieren, und zeigen den Vorteil von Freeze-Training, insbesondere für Transferlernen.
Methoden mit Policy-Gradienten erzielen oft eine bessere Leistung, wenn die Änderung der Policy auf eine kleine Kullback-Leibler-Divergenz begrenzt ist.Wir leiten Policy-Gradienten ab, bei denen die Änderung der Policy auf eine kleine Wasserstein-Distanz (oder Vertrauensregion) begrenzt ist.Dies geschieht in den diskreten und kontinuierlichen Multi-Armed-Bandit-Einstellungen mit Entropie-Regularisierung. Wir zeigen, dass im Grenzbereich kleiner Schritte in Bezug auf die Wasserstein-Distanz $W_2$ die Politikdynamik durch die Wärmegleichung bestimmt wird, die dem Jordan-Kinderlehrer-Otto-Ergebnis folgt, d.h. dass die Politik Diffusion und Advektion erfährt und sich in der Nähe von Aktionen mit hoher Belohnung konzentriert, was dazu beiträgt, die Art der Konvergenz im Wahrscheinlichkeits-Matching-Setup zu klären und empirische Praktiken wie Gaußsche Politikprioritäten und additives Gradientenrauschen zu rechtfertigen.
Trotz ihrer herausragenden Leistung bei Klassifizierungsaufgaben sind die aus der Softmax-Überwachung abgeleiteten Merkmale in einigen Szenarien, in denen euklidische Abstände in Merkmalsräumen gelten, in der Regel suboptimal.Um dieses Problem anzugehen, schlagen wir einen neuen Verlust vor, der als isotroper Verlust bezeichnet wird, in dem Sinne, dass die Gesamtverteilung der Datenpunkte reguliert wird, um sich der isotropen Normalverteilung anzunähern. Kombiniert mit der Vanilla Softmax, formalisieren wir ein neuartiges Kriterium, die isotrope Softmax, oder kurz Isomax, für das überwachte Lernen von tiefen neuronalen Netzen.Durch die Isomax, die Intra-Klasse Merkmale werden durch die isotrope Verlust bestraft, während Inter-Klasse Abstände sind gut durch die ursprüngliche Softmax loss.Moreover, die Isomax Verlust erfordert keine zusätzlichen Änderungen an das Netzwerk, Mini-Batches oder den Trainingsprozess.Extensive Experimente auf Klassifizierung und Clustering durchgeführt werden, um die Überlegenheit und Robustheit der Isomax Verlust zu demonstrieren.
Kürzlich schlugen Jin et al. (2018) einen Q-Learning-Algorithmus mit UCB-Explorationspolitik vor und bewiesen, dass er eine nahezu optimale Bedauernsschranke für episodische MDP mit endlichem Horizont hat.In diesem Papier passen wir Q-Learning mit UCB-Explorationsbonus an MDP mit unendlichem Horizont und diskontierten Belohnungen an, ohne auf ein generatives Modell zuzugreifen. Wir zeigen, dass die \textit{Stichprobenkomplexität der Exploration} unseres Algorithmus durch $\tilde{O}({\frac{SA}{\epsilon^2(1-\gamma)^7}})$ begrenzt ist. Dies verbessert das bisher beste bekannte Ergebnis von $\tilde{O}({\frac{SA}{\epsilon^4(1-\gamma)^8}})$ in dieser Umgebung, das durch verzögertes Q-Learning (Strehlet al, 2006), und entspricht der unteren Schranke in Bezug auf $\epsilon$ sowie $S$ und $A$ bis zu logarithmischen Faktoren.
Backpropagation ist die Grundlage für die heutigen künstlichen neuronalen Netze (ANNs).Trotz umfangreicher Forschung ist jedoch unklar, ob das Gehirn diesen Algorithmus implementiert.Unter Neurowissenschaftlern werden Algorithmen des verstärkenden Lernens (Reinforcement Learning, RL) oft als realistische Alternative angesehen: Neuronen können zufällig Änderungen einführen und unspezifische Rückkopplungssignale verwenden, um ihre Auswirkungen auf die Kosten zu beobachten und so ihren Gradienten anzunähern.Die Konvergenzrate eines solchen Lernens skaliert jedoch schlecht mit der Anzahl der beteiligten Neuronen.Hier schlagen wir einen hybriden Lernansatz vor. Wir weisen nach, dass unser Ansatz für bestimmte Klassen von Netzwerken zum wahren Gradienten konvergiert. Sowohl in Feedforward- als auch in Faltungsnetzwerken zeigen wir empirisch, dass unser Ansatz lernt, den Gradienten anzunähern, und die Leistung des gradientenbasierten Lernens erreichen kann.
Dieses Papier schlägt vor und zeigt ein überraschendes Muster in der Ausbildung von neuronalen Netzen: Es gibt eine 1:1-Beziehung zwischen den Werten eines beliebigen Paares von Verlusten (wie Kreuzentropie, mittlerer quadratischer Fehler, 0/1-Fehler usw.), die für ein Modell ausgewertet werden, das an (einem beliebigen Punkt) eines Trainingslaufs entsteht.Dieses Muster ist universell in dem Sinne, dass diese 1:1-Beziehung über Architekturen (wie VGG, Resnet, Densenet usw.), Algorithmen (SGD und SGD mit Momentum) und Trainingsverlustfunktionen (Kreuzentropie und mittlerer quadratischer Fehler) identisch ist.
