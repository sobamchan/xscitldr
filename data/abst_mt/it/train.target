A causa del successo dell'apprendimento profondo per risolvere una varietà di compiti impegnativi di apprendimento automatico, c'è un crescente interesse nella comprensione delle funzioni di perdita per l'addestramento delle reti neurali da un aspetto teorico.In particolare, le proprietà dei punti critici e il paesaggio intorno ad essi sono importanti per determinare le prestazioni di convergenza degli algoritmi di ottimizzazione.In questo articolo, forniamo una caratterizzazione necessaria e sufficiente delle forme analitiche per i punti critici (così come i minimizzatori globali) delle funzioni di perdita quadrate per reti neurali lineari. Mostriamo che le forme analitiche dei punti critici caratterizzano i valori delle funzioni di perdita corrispondenti così come le condizioni necessarie e sufficienti per raggiungere il minimo globale.Inoltre, sfruttiamo le forme analitiche dei punti critici per caratterizzare le proprietà del paesaggio per le funzioni di perdita delle reti neurali lineari e delle reti ReLU poco profonde.Una conclusione particolare è che: Mentre la funzione di perdita delle reti lineari non ha un minimo locale spurio, la funzione di perdita delle reti non lineari a uno strato nascosto con funzione di attivazione ReLU ha un minimo locale che non è minimo globale.
L'algoritmo di backpropagation (BP) è spesso ritenuto biologicamente implausibile nel cervello.Uno dei motivi principali è che BP richiede matrici di peso simmetriche nei percorsi di feedforward e feedback.Per affrontare questo "problema di trasporto del peso" (Grossberg, 1987), due algoritmi biologicamente plausibili, proposti da Liao et al. (2016) e Lillicrap et al. (2016), rilassano i requisiti di simmetria dei pesi di BP e dimostrano capacità di apprendimento paragonabili a quelle di BP su piccoli set di dati.Tuttavia, un recente studio di Bartunov et al. (2018) trova che sebbene l'allineamento di feedback (FA) e alcune varianti di target-propagation (TP) eseguano bene su MNIST e CIFAR, eseguono significativamente peggio di BP su ImageNet.Qui, valutiamo ulteriormente l'algoritmo sign-symmetry (SS) (Liao et al, 2016), che differisce sia da BP che da FA in quanto i pesi di feedback e feedforward non condividono le grandezze ma condividono i segni.Abbiamo esaminato le prestazioni di allineamento sign-symmetry e feedback su ImageNet e MS COCO dataset utilizzando diverse architetture di rete (ResNet-18 e AlexNet per ImageNet; RetinaNet per MS COCO). Sorprendentemente, le reti addestrate con la simmetria dei segni possono raggiungere prestazioni di classificazione che si avvicinano a quelle delle reti addestrate con BP.Questi risultati completano lo studio di Bartunov et al. (2018) e stabiliscono un nuovo punto di riferimento per futuri algoritmi di apprendimento biologicamente plausibili su set di dati più difficili e architetture più complesse.
Introduciamo il Transformer 2-simplicial, un'estensione del Transformer che include una forma di attenzione a più dimensioni generalizzando l'attenzione dot-product, e utilizza questa attenzione per aggiornare le rappresentazioni di entità con prodotti tensoriali di vettori di valore.Mostriamo che questa architettura è un utile bias induttivo per il ragionamento logico nel contesto del deep reinforcement learning.
Presentiamo Tensor-Train RNN (TT-RNN), una nuova famiglia di architetture di sequenze neurali per la previsione multivariata in ambienti con dinamiche non lineari.La previsione a lungo termine in tali sistemi è molto impegnativa, poiché esistono dipendenze temporali a lungo termine, correlazioni di ordine superiore e sensibilità alla propagazione degli errori.La nostra architettura ricorrente tensoriale proposta affronta questi problemi imparando le dinamiche non lineari direttamente usando momenti di ordine superiore e funzioni di transizione di stato di ordine superiore. Inoltre, decomponiamo la struttura di ordine superiore utilizzando la decomposizione tensor-train (TT) per ridurre il numero di parametri preservando le prestazioni del modello.Stabiliamo teoricamente le proprietà di approssimazione di Tensor-Train RNNs per input di sequenza generali, e tali garanzie non sono disponibili per RNNs usuali.Dimostriamo anche significativi miglioramenti di previsione a lungo termine rispetto alle architetture RNN e LSTM generali su una serie di ambienti simulati con dinamiche non lineari, così come sul clima del mondo reale e sui dati di traffico.
Gli sforzi recenti sulla combinazione dei modelli profondi con i modelli grafici probabilistici sono promettenti nel fornire i modelli flessibili che sono inoltre facili da interpretare.Proponiamo un algoritmo di messaggio-passaggio variazionale per l'inferenza variazionale in tali modelli.Facciamo tre contributions.First, proponiamo le reti strutturate di inferenza che incorporano la struttura del modello grafico nella rete di inferenza degli auto-encoders variazionali (VAE). In secondo luogo, stabiliamo le condizioni sotto le quali tali reti di inferenza consentono un'inferenza ammortizzata veloce simile a VAE.Infine, deriviamo un algoritmo di passaggio dei messaggi variazionale per eseguire un'inferenza efficiente a gradiente naturale mantenendo l'efficienza dell'inferenza ammortizzata.consentendo simultaneamente l'inferenza strutturata, ammortizzata e a gradiente naturale per modelli strutturati profondi, il nostro metodo semplifica e generalizza i metodi esistenti.
Le moderne reti neurali profonde hanno una grande quantità di pesi, che le rendono difficili da distribuire su dispositivi con vincoli di calcolo come i telefoni cellulari. Un approccio comune per ridurre le dimensioni del modello e il costo computazionale è quello di utilizzare la fattorizzazione a basso rango per approssimare una matrice di pesi. In questo lavoro, proponiamo di utilizzare una miscela di più fattorizzazioni di basso rango per modellare una grande matrice di peso, e i coefficienti della miscela sono calcolati dinamicamente a seconda del suo input.dimostriamo l'efficacia dell'approccio proposto sia sulla modellazione del linguaggio che sui compiti di classificazione delle immagini.gli esperimenti mostrano che il nostro metodo non solo migliora l'efficienza di calcolo, ma mantiene anche (a volte supera) la sua precisione rispetto alle controparti di rango completo.
La formazione per l'apprendimento profondo accede a grandi quantità di dati ad alta velocità, ponendo sfide per i set di dati recuperati su reti e dispositivi di archiviazione comuni. Introduciamo un modo per ridurre dinamicamente l'overhead del recupero e del trasporto dei dati di formazione con un metodo che chiamiamo Progressive Compressed Records (PCRs).PCRs si discosta dai formati precedenti sfruttando la compressione progressiva per dividere ogni esempio di formazione in più esempi di fedeltà sempre più alta, senza aggiungere alla dimensione totale dei dati. Dimostriamo che i modelli possono essere addestrati su rappresentazioni aggressivamente compresse dei dati di addestramento e mantenere un'elevata accuratezza, e che i PCR possono consentire un aumento di velocità medio di 2 volte rispetto ai formati di base che utilizzano la compressione JPEG: ImageNet, HAM10000, Stanford Cars e CelebA-HQ.
È fondamentale e impegnativo addestrare reti neurali profonde (DNN) robuste e accurate quando esistono esempi semanticamente anomali.Sebbene siano stati fatti grandi progressi, c'è ancora una domanda di ricerca cruciale che non è stata ancora esplorata a fondo: In questo lavoro, studiamo questa domanda e proponiamo il gradient rescaling (GR) per risolverla.GR modifica la grandezza del gradiente del vettore logit per enfatizzare i punti di dati di formazione relativamente più facili quando il rumore diventa più grave, che funziona come regolarizzazione esplicita dell'enfasi per migliorare le prestazioni di generalizzazione delle DNN. Oltre alla regolarizzazione, colleghiamo GR alla ponderazione degli esempi e alla progettazione di funzioni di perdita robuste. dimostriamo empiricamente che GR è altamente resistente alle anomalie e supera lo stato dell'arte con un ampio margine, ad es, aumentando del 7% su CIFAR100 con il 40% di etichette rumorose, ed è anche significativamente superiore ai regolarizzatori standard in entrambe le impostazioni pulite e anomale. Inoltre, presentiamo studi completi di ablazione per esplorare il comportamento di GR in diversi casi, che è informativo per l'applicazione di GR in scenari del mondo reale.
Nella maggior parte delle applicazioni, i modelli GAN hanno due aspetti in comune: da un lato, l'addestramento dei GAN comporta la risoluzione di un impegnativo problema di ottimizzazione del punto di sella, interpretato come un gioco contraddittorio tra un generatore e un discriminatore; dall'altro, il generatore e il discriminatore sono parametrizzati in termini di reti neurali convoluzionali profonde. L'obiettivo di questo articolo è quello di distinguere il contributo di questi due fattori al successo delle GANs.In particolare, introduciamo Generative Latent Optimization (GLO), un framework per addestrare generatori convoluzionali profondi senza usare discriminatori, evitando così l'instabilità dei problemi di ottimizzazione avversaria. Attraverso una varietà di esperimenti, dimostriamo che GLO gode di molte delle proprietà desiderabili di GANs: apprendimento da grandi dati, sintetizzando campioni visivamente attraenti, interpolando in modo significativo tra i campioni, ed eseguendo l'aritmetica lineare con i vettori di rumore.
In questo articolo, proponiamo un nuovo tipo di kernel, il kernel della foresta casuale, per migliorare le prestazioni empiriche di MMD GAN.Diverso dalle foreste comuni con instradamenti deterministici, una variante di instradamento probabilistico è usata nel nostro innovato kernel della foresta casuale, che è possibile fondere con i quadri CNN.Il nostro kernel della foresta casuale proposto ha i seguenti vantaggi: Dal punto di vista della foresta casuale, l'output del discriminatore GAN può essere visto come input di caratteristiche per la foresta, dove ogni albero ha accesso solo a una frazione delle caratteristiche, e quindi l'intera foresta beneficia dell'apprendimento d'insieme.Nell'aspetto del metodo del kernel, il kernel della foresta casuale ha dimostrato di essere caratteristico, e quindi adatto alla struttura MMD. Inoltre, essendo un kernel asimmetrico, il nostro kernel random-forest è molto più flessibile, in termini di catturare le differenze tra le distribuzioni.Condividendo i vantaggi di CNN, metodo kernel, e l'apprendimento d'insieme, il nostro kernel random-forest basato MMD GAN ottiene prestazioni empiriche desiderabili su CIFAR-10, CelebA e LSUN camera da letto set di dati.Inoltre, per amore di completezza, abbiamo anche presentato analisi teoriche complete per sostenere i nostri risultati sperimentali.
L'apprendimento di rinforzo in un'impostazione attore-critico si basa su stime accurate del valore del critico.Tuttavia, la combinazione di approssimazione della funzione, l'apprendimento della differenza temporale (TD) e l'addestramento off-policy può portare a una sovrastima della funzione di valore.Una soluzione è quella di utilizzare Clipped Double Q-learning (CDQ), che è usato nell'algoritmo TD3 e calcola il minimo di due critici nel TD-target. Mostriamo che CDQ induce un bias di sottostima e proponiamo un nuovo algoritmo che tiene conto di questo utilizzando una media ponderata dell'obiettivo da CDQ e l'obiettivo proveniente da un singolo critico.Il parametro di ponderazione viene regolato durante la formazione in modo tale che le stime del valore corrispondano al rendimento effettivo scontato sugli episodi più recenti e da bilanciare la sovrastima e la sottostima.Empiricamente, otteniamo stime di valore più accurate e dimostriamo risultati allo stato dell'arte su diversi compiti di palestra OpenAI.
Introduciamo un quadro sistematico per quantificare la robustezza dei classificatori alle perturbazioni naturali delle immagini che si trovano nei video. Come parte di questo quadro, costruiamo ImageNet-Vid-Robust, un set di dati revisionato da esperti umani di 22.668 immagini raggruppate in 1.145 set di immagini percettivamente simili derivate da fotogrammi nel set di dati ImageNet Video Object Detection. valutiamo una gamma diversificata di classificatori addestrati su ImageNet, compresi i modelli addestrati per la robustezza, e mostriamo un calo mediano della precisione di classificazione del 16%. Inoltre, valutiamo i modelli Faster R-CNN e R-FCN per il rilevamento, e mostriamo che le perturbazioni naturali inducono sia errori di classificazione che di localizzazione, portando a un calo mediano del mAP di rilevamento di 14 punti.La nostra analisi mostra che le perturbazioni naturali nel mondo reale sono fortemente problematiche per le CNN attuali, ponendo una sfida significativa al loro impiego in ambienti critici per la sicurezza che richiedono previsioni affidabili e a bassa latenza.
I dati tabellari strutturati sono la forma di dati più comunemente usata nell'industria secondo un sondaggio Kaggle ML e DS.Gradient Boosting Trees, Support Vector Machine, Random Forest, e Logistic Regression sono tipicamente usati per compiti di classificazione su dati tabellari.Il recente lavoro del metodo Super Characters usando l'incorporazione di parole bidimensionali ha raggiunto risultati all'avanguardia in compiti di classificazione del testo, mostrando la promessa di questo nuovo approccio. In questo articolo, proponiamo il metodo SuperTML, che prende in prestito l'idea del metodo Super Characters e l'incorporazione bidimensionale per affrontare il problema della classificazione su dati tabulari.Per ogni input di dati tabulari, le caratteristiche sono prima proiettate in un'incorporazione bidimensionale come un'immagine, e poi questa immagine è alimentata in modelli ImageNet CNN per la classificazione.I risultati sperimentali hanno dimostrato che il metodo SuperTML proposto ha raggiunto risultati all'avanguardia su set di dati sia grandi che piccoli.
Imparare rappresentazioni ricche dall'apprendimento predittivo senza etichette è stata una sfida di lunga data nel campo dell'apprendimento automatico.Il pre-addestramento generativo non ha finora avuto successo come i metodi contrastivi nella modellazione delle rappresentazioni delle immagini grezze.In questo articolo, proponiamo un'architettura neurale per l'apprendimento auto-supervisionato della rappresentazione sulle immagini grezze chiamata PatchFormer che impara a modellare le dipendenze spaziali tra le patch in un'immagine grezza.Il nostro metodo impara a modellare la distribuzione di probabilità condizionata delle patch mancanti dato il contesto delle patch circostanti. Valutiamo l'utilità delle rappresentazioni apprese mettendo a punto il modello pre-addestrato su compiti di classificazione a basso data-regime. In particolare, valutiamo il nostro modello sulla classificazione semi-supervisionata di ImageNet, che è diventata recentemente un popolare punto di riferimento per i metodi di apprendimento semi-supervisionati e auto-supervisionati. Il nostro modello è in grado di raggiungere il 30,3% e il 65,5% di accuratezze top-1 quando addestrato utilizzando solo l'1% e il 10% delle etichette su ImageNet, mostrando la promessa di metodi di pre-addestramento generativo.
I metodi di regolarizzazione adattiva pre-moltiplicano una direzione di discesa con una matrice di precondizionamento.A causa del gran numero di parametri dei problemi di apprendimento automatico, i metodi di precondizionamento a matrice intera sono proibitivamente costosi.Mostriamo come modificare la regolarizzazione adattiva a matrice intera per renderla pratica ed efficace. Il nucleo del nostro algoritmo, chiamato GGT, consiste in un efficiente calcolo inverso delle radici quadrate delle matrici di basso rango. I nostri esperimenti preliminari sottolineano un miglior tasso di convergenza di GGT attraverso una varietà di compiti sintetici e benchmark standard di apprendimento profondo.
I sistemi di dialogo richiedono una grande quantità di competenze diverse ma complementari per assistere, informare e intrattenere gli esseri umani, prenotazione di ristoranti, prenotazione di biglietti del treno) di sistemi di dialogo orientati all'obiettivo possono essere visti come diverse competenze, e lo stesso vale per le capacità ordinarie di chat dei sistemi di dialogo chit-chat.In questo articolo, proponiamo di imparare un sistema di dialogo che parametrizza indipendentemente le diverse competenze di dialogo, e impara a selezionare e combinare ciascuna di esse attraverso Attention over Parameters (AoP).I risultati sperimentali mostrano che questo approccio raggiunge prestazioni competitive su un dataset combinato di MultiWOZ (Budzianowski et al, 2018), In-Car Assistant (Eric et al.,2017), e Persona-Chat (Zhang et al., 2018).Infine, dimostriamo che ogni abilità di dialogo è effettivamente appresa e può essere combinata con altre abilità per produrre risposte selettive.
La distillazione del modello mira a distillare la conoscenza di un modello complesso in uno più semplice.In questo articolo, consideriamo una formulazione alternativa chiamata distillazione del dataset: manteniamo il modello fisso e invece tentiamo di distillare la conoscenza da un grande dataset di allenamento in uno piccolo. L'idea è quella di sintetizzare un piccolo numero di punti dati che non hanno bisogno di provenire dalla corretta distribuzione dei dati, ma che, quando vengono dati all'algoritmo di apprendimento come dati di addestramento, approssimano il modello addestrato sui dati originali.Per esempio, dimostriamo che è possibile comprimere 60.000 immagini di addestramento MNIST in sole 10 immagini sintetiche distillate (una per classe) e ottenere prestazioni vicine a quelle originali, data una inizializzazione fissa della rete.Valutiamo il nostro metodo in varie impostazioni di inizializzazione.  Gli esperimenti su più dataset, MNIST, CIFAR10, PASCAL-VOC, e CUB-200, dimostrano il vantaggio del nostro approccio rispetto ai metodi alternativi.  Infine, includiamo un'applicazione del mondo reale della distillazione dei dati all'impostazione di apprendimento continuo: dimostriamo che la memorizzazione di immagini distillate come memoria episodica di compiti precedenti può alleviare la dimenticanza più efficacemente delle immagini reali.
Mettiamo in relazione il gioco minimax delle reti generative avversarie (GAN) con la ricerca dei punti di sella della funzione Lagrangiana per un problema di ottimizzazione convessa, dove le uscite del discriminatore e la distribuzione delle uscite del generatore giocano rispettivamente il ruolo di variabili primarie e duali. Questa formulazione mostra la connessione tra il processo standard di formazione GAN e i metodi primari-duali subgradienti per l'ottimizzazione convessa. Un esempio giocattolo mostra che il metodo proposto è in grado di risolvere il collasso della modalità, che in questo caso non può essere evitato dalla GAN standard o dalla GAN di Wasserstein. Gli esperimenti su entrambi i dati sintetici della miscela gaussiana e sui set di dati di immagini del mondo reale dimostrano le prestazioni del metodo proposto sulla generazione di campioni diversi.
Specificare le funzioni di ricompensa è difficile, il che motiva l'area dell'inferenza della ricompensa: imparare le ricompense dal comportamento umano.L'assunzione di partenza nell'area è che il comportamento umano sia ottimale data la funzione di ricompensa desiderata, ma in realtà le persone hanno molte forme diverse di irrazionalità, dal rumore alla miopia all'avversione al rischio e oltre. Questo fatto sembra essere strettamente dannoso per l'inferenza della ricompensa: è già difficile dedurre la ricompensa dal comportamento razionale, e il rumore e le distorsioni sistematiche rendono le azioni meno dirette di una relazione con la ricompensa.La nostra intuizione in questo lavoro è che, contrariamente alle aspettative, l'irrazionalità può effettivamente aiutare piuttosto che ostacolare l'inferenza della ricompensa. Per alcuni tipi e quantità di irrazionalità, l'esperto produce ora politiche più varie rispetto al comportamento razionale, che aiutano a disambiguare tra diversi parametri di ricompensa - quelli che altrimenti corrispondono allo stesso comportamento razionale. Iniziamo coprendo lo spazio delle irrazionalità come deviazioni dall'aggiornamento di Bellman, simuliamo il comportamento degli esperti e misuriamo l'accuratezza dell'inferenza per contrastare i diversi tipi e studiare i guadagni e le perdite. Forniamo un'analisi basata sull'informazione reciproca dei nostri risultati e concludiamo discutendo la necessità di modellare accuratamente l'irrazionalità, così come in che misura potremmo aspettarci (o essere in grado di addestrare) le persone reali a mostrare irrazionalità utili quando insegniamo le ricompense agli studenti.
I modelli di Natural Language Processing mancano di un approccio unificato al test di robustezza. In questo articolo introduciamo WildNLP - una struttura per testare la stabilità dei modelli in un ambiente naturale in cui si verificano corruzioni del testo come errori di tastiera o errori di ortografia.Confrontiamo la robustezza dei modelli di 4 popolari compiti NLP: Q&A, NLI, NER e Sentiment Analysis testando le loro prestazioni su aspetti introdotti nel framework. In particolare, ci concentriamo su un confronto tra le recenti rappresentazioni di testo allo stato dell'arte e le word embeddings non contestualizzate. Al fine di migliorare la robustezza, eseguiamo l'addestramento avversario su aspetti selezionati e controlliamo la sua trasferibilità al miglioramento dei modelli con vari tipi di corruzione. Troviamo che l'alta performance dei modelli non assicura una sufficiente robustezza, anche se le moderne tecniche di incorporazione aiutano a migliorarla.
L'addestramento di modelli generativi come Generative Adversarial Network (GAN) è impegnativo per i dati rumorosi. Un nuovo algoritmo di apprendimento curricolare relativo al clustering è proposto per affrontare questo problema in questo documento.La costruzione curricolare si basa sulla centralità dei cluster sottostanti nei punti dati.  Per rendere il nostro algoritmo scalabile a dati su larga scala, viene ideato il set attivo, nel senso che ogni round di formazione procede solo su un sottoinsieme attivo contenente una piccola frazione di dati già addestrati e i dati incrementali di minore centralità. Inoltre, l'analisi geometrica è presentata per interpretare la necessità del curriculum di cluster per i modelli generativi. Gli esperimenti su dati di gatti e volti umani convalidano che il nostro algoritmo è in grado di apprendere i modelli generativi ottimali (ad esempio ProGAN) rispetto alle metriche di qualità specificate per i dati rumorosi.Una scoperta interessante è che il curriculum di cluster ottimale è strettamente legato al punto critico del processo di percolazione geometrica formulato nel documento.
Gli attacchi backdoor mirano a manipolare un sottoinsieme di dati di addestramento iniettando trigger avversari tali che i modelli di apprendimento automatico addestrati sul dataset manomesso faranno arbitrariamente (mirati) previsioni errate sul testset con lo stesso trigger incorporato.Mentre l'apprendimento federato (FL) è in grado di aggregare le informazioni fornite da diverse parti per la formazione di un modello migliore, la sua metodologia di apprendimento distribuito e la distribuzione dei dati intrinsecamente eterogenea tra le parti può portare nuove vulnerabilità. Oltre ai recenti attacchi backdoor centralizzati su FL dove ogni parte incorpora lo stesso trigger globale durante l'allenamento, proponiamo l'attacco backdoor distribuito (DBA) --- un nuovo quadro di valutazione delle minacce sviluppato sfruttando appieno la natura distribuita di FL.DBA scompone un modello di trigger globale in modelli locali separati e li incorpora nel set di allenamento di diverse parti avversarie rispettivamente. Rispetto alle backdoor centralizzate standard, dimostriamo che DBA è sostanzialmente più persistente e furtivo contro FL su insiemi di dati diversi come la finanza e i dati di immagine.conduciamo ampi esperimenti per dimostrare che il tasso di successo dell'attacco di DBA è significativamente superiore alle backdoor centralizzate sotto diverse impostazioni.inoltre, troviamo che gli attacchi distribuiti sono effettivamente più insidiosi, come DBA può eludere due algoritmi FL robusti allo stato dell'arte contro le backdoor centralizzate. Per esplorare ulteriormente le proprietà di DBA, testiamo le prestazioni dell'attacco variando diversi fattori di innesco, comprese le variazioni locali di innesco (dimensione, distanza e posizione), il fattore di scala in FL, la distribuzione dei dati e il rapporto e l'intervallo di veleno.
Le reti di grafi hanno recentemente attirato un considerevole interesse, e in particolare nel contesto dell'apprendimento semi-supervisionato.Questi metodi lavorano tipicamente generando rappresentazioni di nodi che si propagano in un dato grafo pesato.Qui sosteniamo che per l'apprendimento semi-supervisionato, è più naturale considerare invece la propagazione delle etichette nel grafo.A tal fine, proponiamo una versione neurale differenziabile del classico algoritmo Label Propagation (LP). Questa formulazione può essere utilizzata per l'apprendimento dei pesi dei bordi, a differenza di altri metodi in cui i pesi sono impostati euristicamente.Partendo da uno strato che implementa una singola iterazione di LP, procediamo aggiungendo diversi importanti passi non lineari che migliorano significativamente il meccanismo di propagazione delle etichette.Gli esperimenti in due impostazioni distinte dimostrano l'utilità del nostro approccio.
La ricerca di architetture neurali (NAS) ha fatto rapidi progressi nell'incomputervision, dove nuovi risultati allo stato dell'arte sono stati raggiunti in una serie di compiti con architetture di reti neurali (NN) ricercate automaticamente, mentre la NAS non ha fatto progressi comparabili nella comprensione del linguaggio naturale (NLU). Corrispondendo alla meta-architettura encoder-aggregatore dei tipici modelli di reti neurali per compiti NLU (Gong et al. 2018), abbiamo ridefinito lo spazio di ricerca, dividendolo in due parti: spazio di ricerca encoder e spazio di ricerca aggregatore.Lo spazio di ricerca encoder contiene operazioni di base come convoluzioni, RNN, attenzione multitesta e le sue varianti sparse, trasformatori a stella. Il nostro algoritmo di ricerca viene poi riempito tramite DARTS, un framework di ricerca per architetture neurali differenziabili, riducendo progressivamente lo spazio di ricerca ogni poche epoche, il che riduce ulteriormente il tempo di ricerca e i costi delle risorse. Gli esperimenti su cinque set di dati di riferimento mostrano che le nuove reti neurali che generiamo possono raggiungere prestazioni paragonabili ai modelli allo stato dell'arte che non coinvolgono il pre-training del modello linguistico.
I metodi di Network embedding (NE) mirano ad apprendere rappresentazioni bidimensionali dei nodi della rete come vettori, tipicamente nello spazio euclideo; queste rappresentazioni sono poi utilizzate per una varietà di compiti di predizione a valle; la predizione dei collegamenti è una delle scelte più popolari per valutare le prestazioni dei metodi NE. Tuttavia, la complessità della predizione dei link richiede una pipeline di valutazione accuratamente progettata per fornire risultati coerenti, riproducibili e comparabili.Noi sosteniamo che questo non è stato sufficientemente considerato nei lavori recenti.L'obiettivo principale di questo articolo è quello di superare le difficoltà associate alle pipeline di valutazione e alla riproducibilità dei risultati. Introduciamo EvalNE, una struttura di valutazione per valutare e confrontare in modo trasparente le prestazioni dei metodi NE sulla predizione dei link.EvalNE fornisce automazione e astrazione per compiti come la sintonizzazione degli iperparametri, la validazione del modello, il campionamento dei bordi, il calcolo delle embeddings dei bordi e la validazione del modello. Il framework integra procedure efficienti per il campionamento di edge e non-edge e può essere utilizzato per valutare facilmente qualsiasi metodo di embedding off-the-shelf.Il framework è liberamente disponibile come toolbox Python.Infine, per dimostrare l'utilità di EvalNE nella pratica, conduciamo uno studio empirico in cui cerchiamo di replicare e analizzare sezioni sperimentali di diversi articoli influenti.
I modelli di apprendimento profondo possono essere efficientemente ottimizzati tramite la discesa del gradiente stocastico, ma ci sono poche prove teoriche a sostegno di questo.Una domanda chiave nell'ottimizzazione è capire quando il paesaggio di ottimizzazione di una rete neurale è suscettibile di ottimizzazione basata sul gradiente.Ci concentriamo su una semplice rete neurale a due strati ReLU con due unità nascoste, e dimostriamo che tutti i minimizzatori locali sono globali.Questo combinato con il recente lavoro di Lee et al. (2017); Lee et al. (2016) mostrano che la discesa del gradiente converge al minimizzatore globale.
Il dropout è una tecnica semplice ma efficace per migliorare le prestazioni di generalizzazione e prevenire l'overfitting nelle reti neurali profonde (DNNs).In questo articolo, discutiamo tre nuove osservazioni sul dropout per comprendere meglio la generalizzazione delle DNNs con attivazioni di unità lineare rettificata (ReLU): 1) il dropout è una tecnica di smoothing che incoraggia ogni modello lineare locale di una DNN ad essere addestrato su punti dati provenienti da regioni vicine; 2) un tasso di dropout costante può risultare in tassi effettivi di disattivazione neurale che sono significativamente diversi per strati con diverse frazioni di neuroni attivati; e 3) il fattore di ridimensionamento del dropout provoca un'incongruenza tra la normalizzazione durante le condizioni di allenamento e di test quando viene utilizzata anche la normalizzazione batch.  Quanto sopra porta a tre semplici ma non banali miglioramenti al dropout che risultano nel nostro metodo proposto "Jumpout". "Jumpout campiona il tasso di abbandono usando una distribuzione monotona decrescente (come la parte destra di una gaussiana troncata), così il modello lineare locale in ogni punto dei dati è addestrato, con alta probabilità, a funzionare meglio per i punti dei dati provenienti da regioni vicine che da regioni più lontane. Invece di sintonizzare un tasso di abbandono per ogni strato e applicarlo a tutti i campioni, jumpout inoltre normalizza in modo adattivo il tasso di abbandono ad ogni strato e ad ogni campione/batch di allenamento, in modo che il tasso di abbandono effettivo applicato ai neuroni attivati sia mantenuto lo stesso. Inoltre, riscaliamo le uscite di jumpout per un migliore trade-off che mantiene sia la varianza che la media dei neuroni più coerenti tra le fasi di allenamento e di test, il che mitiga l'incompatibilità tra dropout e normalizzazione dei lotti.Rispetto al dropout originale, jumpout mostra prestazioni notevolmente migliorate su CIFAR10, CIFAR100, Fashion- MNIST, STL10, SVHN, ImageNet-1k, ecc, pur introducendo costi di memoria e calcolo aggiuntivi trascurabili.
Le preoccupazioni circa l'interpretabilità, le risorse computazionali e i priori induttivi di principio hanno motivato gli sforzi per progettare modelli neurali sparsi per i compiti di NLP. Se la sparsità è importante per NLP, i modelli neurali ben addestrati potrebbero diventare naturalmente approssimativamente sparsi? Usando la norma Taxi-Euclidea per misurare la sparsità, troviamo che le parole di input frequenti sono associate ad attivazioni concentrate o rade, mentre le parole di destinazione frequenti sono associate ad attivazioni disperse, ma a gradienti concentrati.Troviamo che i gradienti associati alle parole funzione sono più concentrati dei gradienti delle parole contenuto, anche controllando la frequenza delle parole.
L'integrazione di una Knowledge Base (KB) in un agente di dialogo neurale è una delle sfide chiave nell'IA conversazionale.Le reti di memoria hanno dimostrato di essere efficaci per codificare le informazioni della KB in una memoria esterna per generare così risposte più fluenti e informate.Sfortunatamente, tale memoria si riempie di rappresentazioni latenti durante l'allenamento, quindi la strategia più comune è quella di sovrascrivere le vecchie voci di memoria in modo casuale. In questo articolo, mettiamo in discussione questo approccio e forniamo prove sperimentali che dimostrano che le reti di memoria convenzionali generano molti vettori latenti ridondanti con conseguente overfitting e la necessità di memorie più grandi.Introduciamo il dropout della memoria come una tecnica automatica che incoraggia la diversità nello spazio latente attraverso1) Invecchiamento delle memorie ridondanti per aumentare la loro probabilità di essere sovrascritte durante l'addestramento2) Campionamento di nuove memorie che riassumono la conoscenza acquisita dalle memorie ridondanti. Questa tecnica ci permette di incorporare le Knowledge Bases per ottenere una generazione di dialogo allo stato dell'arte nel dataset Stanford Multi-Turn Dialogue.Considerando la stessa architettura, il suo utilizzo fornisce un miglioramento di +2,2 punti BLEU per la generazione automatica di risposte e un aumento di +8,1% nel riconoscimento di entità nominate.
Su-Boyd-Candes (2014) ha fatto una connessione tra il metodo di Nesterov e un'equazione differenziale ordinaria (ODE).  Mostriamo che se un termine di smorzamento Hessiano viene aggiunto all'ODE di Su-Boyd-Candes (2014), allora il metodo di Nesterov si presenta come una discretizzazione diretta dell'ODE modificata. Analogamente, nel caso fortemente convesso, un termine di smorzamento Hessiano viene aggiunto all'ODE di Polyak, che viene poi discretizzato per produrre il metodo di Nesterov per funzioni fortemente convesse.   Nonostante il termine Hessiano, entrambe le ODE del secondo ordine possono essere rappresentate come sistemi del primo ordine. L'analisi di Liapunov stabilita viene utilizzata per recuperare i tassi accelerati di convergenza sia in tempo continuo che discreto.  Inoltre, l'analisi di Liapunov può essere estesa al caso dei gradienti stocastici che permette di considerare il caso a gradiente completo come un caso speciale del caso stocastico.  Il risultato è un approccio unificato all'accelerazione convessa sia in tempo continuo che discreto e in entrambi i casi stocastico e a gradiente completo. 
Proponiamo di imparare a trasferire l'apprendimento (L2TL) per migliorare l'apprendimento di trasferimento su un set di dati di destinazione attraverso l'estrazione giudiziosa di informazioni da un set di dati di origine. L2TL considera l'ottimizzazione congiunta di pesi ampiamente condivisi tra i modelli per i compiti di origine e di destinazione, e impiega pesi adattivi per la scalatura delle perdite costituenti. L'adattamento dei pesi si basa sull'apprendimento di rinforzo, guidato con una metrica di performance sul set di validazione di destinazione.Dimostriamo lo stato dell'arte delle prestazioni di L2TL dato modelli fissi, superando costantemente le linee di base fine-tuning su vari set di dati.Nei regimi di set di dati di destinazione su piccola scala e significativo mismatch di etichetta tra la fonte e set di dati di destinazione, L2TL supera il lavoro precedente da un margine ancora più grande.
In molti scenari parzialmente osservabili, gli agenti di Reinforcement Learning (RL) devono fare affidamento sulla memoria a lungo termine per apprendere una politica ottimale, dimostrando che l'utilizzo di tecniche di NLP e di apprendimento supervisionato fallisce nei compiti di RL a causa della stocasticità dell'ambiente e dell'esplorazione. Utilizzando le nostre intuizioni sui limiti dei metodi di memoria tradizionali in RL, proponiamo AMRL, una classe di modelli che possono apprendere politiche migliori con una maggiore efficienza del campione e sono resistenti agli input rumorosi. In particolare, i nostri modelli utilizzano un modulo di memoria standard per riassumere il contesto a breve termine, e poi aggregano tutti gli stati precedenti dal modello standard senza rispettare l'ordine.Mostriamo che questo fornisce vantaggi sia in termini di decadimento del gradiente che di rapporto segnale-rumore nel tempo.Valutando in Minecraft e ambienti labirinto che testano la memoria a lungo termine, troviamo che il nostro modello migliora il rendimento medio del 19% rispetto a una linea di base che ha lo stesso numero di parametri e del 9% rispetto a una linea di base più forte che ha molti più parametri.
L'ottimizzazione su collettore è stata ampiamente utilizzata nell'apprendimento automatico, per gestire i problemi di ottimizzazione con vincoli.La maggior parte dei lavori precedenti si concentra sul caso con un singolo collettore.Tuttavia, in pratica è abbastanza comune che il problema di ottimizzazione coinvolga più di un vincolo, (ogni vincolo corrispondente a un collettore).Non è chiaro in generale come ottimizzare su più collettori in modo efficace e dimostrabile soprattutto quando l'intersezione di più collettori non è un collettore o non può essere facilmente calcolato. Proponiamo un quadro unificato di algoritmi per gestire l'ottimizzazione su collettori multipli.In particolare, integriamo le informazioni da collettori multipli e ci muoviamo lungo una direzione d'insieme vedendo le informazioni da ogni collettore come una deriva e aggiungendole insieme.Dimostriamo le proprietà di convergenza degli algoritmi proposti.Applichiamo anche gli algoritmi nella rete neurale di formazione con livelli di normalizzazione batch e otteniamo risultati empirici preferibili.
In questo articolo, traiamo ispirazione dal recente successo dei modelli sequence-to-sequence per problemi di previsione strutturati per sviluppare politiche su spazi discretizzati. centrale per questo metodo è la realizzazione che funzioni complesse su spazi ad alta dimensione possono essere modellate da reti neurali che prevedono una dimensione alla volta. In particolare, mostriamo come i valori Q e le politiche su spazi continui possono essere modellati utilizzando un modello di predizione di passo successivo su dimensioni discretizzate. Con questa parametrizzazione, è possibile sia sfruttare la struttura compositiva degli spazi d'azione durante l'apprendimento, sia calcolare i massimi sugli spazi d'azione (approssimativamente). Su un semplice compito di esempio dimostriamo empiricamente che il nostro metodo può eseguire la ricerca globale, che aggira efficacemente i problemi di ottimizzazione locale che affliggono DDPG.Applichiamo la tecnica ai metodi off-policy (Q-learning) e dimostriamo che il nostro metodo può raggiungere lo stato dell'arte per i metodi off-policy su diversi compiti di controllo continuo.
L'apprendimento di rinforzo basato sul modello (MBRL) mira ad apprendere un modello dinamico per ridurre il numero di interazioni con gli ambienti del mondo reale.Tuttavia, a causa dell'errore di stima, i rollout nel modello appreso, specialmente quelli di lungo orizzonte, non riescono a corrispondere a quelli negli ambienti del mondo reale.Questo mismatching ha seriamente colpito la complessità del campione di MBRL.Il fenomeno può essere attribuito al fatto che i lavori precedenti impiegano l'apprendimento supervisionato per apprendere i modelli di transizione a un passo, che ha difficoltà intrinseche a garantire la corrispondenza delle distribuzioni dai rollout a più passi. Sulla base di questa affermazione, proponiamo di imparare il modello sintetizzato abbinando le distribuzioni dei rollout multi-step campionati dal modello sintetizzato e quelli reali tramite WGAN.Mostriamo teoricamente che abbinare i due può minimizzare la differenza di ricompense cumulative tra la transizione reale e quella appresa.I nostri esperimenti mostrano anche che il metodo di imitazione del modello proposto supera lo stato dell'arte in termini di complessità del campione e rendimento medio.
La normalizzazione dei lotti (BN) e le sue varianti hanno visto l'adozione diffusa nella comunità dell'apprendimento profondo perché migliorano l'addestramento delle reti neurali profonde.Le discussioni sul perché questa normalizzazione funzioni così bene rimangono incerte.  Noi rendiamo esplicita la relazione tra i minimi quadrati ordinari e le derivate parziali calcolate durante il back-propagating attraverso BN.Rifondiamo il back-propagation di BN come un adattamento ai minimi quadrati, che azzera e decorrela le derivate parziali dalle attivazioni normalizzate.Questa visione, che noi chiamiamo {\em gradient-least-squares}, è una descrizione estensibile e aritmeticamente accurata di BN.Per esplorare ulteriormente questa prospettiva, motiviamo, interpretiamo e valutiamo due aggiustamenti a BN.
La normalizzazione dei lotti (BN) è diventata una pietra miliare dell'apprendimento profondo in diverse architetture, sembrando aiutare l'ottimizzazione e la generalizzazione. Mentre l'idea ha un senso intuitivo, l'analisi teorica della sua efficacia è stata carente.Qui viene fornito un supporto teorico per una delle sue proprietà congetturate, cioè la capacità di permettere alla discesa del gradiente di avere successo con meno sintonizzazione dei tassi di apprendimento, pesi di ogni strato con BN) a una costante (diciamo, 0.3), la discesa del gradiente si avvicina ancora a un punto stazionario (cioè, una soluzione dove il gradiente è zero) nel tasso di T^{-1/2} in T iterazioni, corrispondendo asintoticamente al miglior limite per la discesa del gradiente con tassi di apprendimento ben sintonizzati.Un risultato simile con tasso di convergenza T^{-1/4} è mostrato anche per la discesa del gradiente stocastica.
I modelli generativi delle immagini naturali hanno progredito verso campioni di alta fedeltà grazie al forte sfruttamento della scala. Noi tentiamo di portare questo successo nel campo della modellazione video dimostrando che le grandi reti generative avversarie addestrate sul complesso dataset Kinetics-600 sono in grado di produrre campioni video di complessità e fedeltà sostanzialmente superiori rispetto ai lavori precedenti.  Il nostro modello proposto, Dual Video Discriminator GAN (DVD-GAN), si adatta a video più lunghi e di più alta risoluzione sfruttando una decomposizione computazionalmente efficiente del suo discriminatore. Valutiamo i compiti correlati di sintesi e previsione video, e raggiungiamo un nuovo stato dell'arte della Fréchet Inception Distance per la previsione per Kinetics-600, così come lo stato dell'arte dell'Inception Score per la sintesi sul dataset UCF-101, oltre a stabilire una forte base per la sintesi su Kinetics-600.
La comprensione del linguaggio procedurale richiede di anticipare gli effetti causali delle azioni, anche quando non sono esplicitamente dichiarati. In questo lavoro, introduciamo le reti di processo neurale per comprendere il testo procedurale attraverso la simulazione (neurale) delle dinamiche delle azioni.   Il nostro modello integra le architetture di memoria esistenti con il tracciamento dinamico delle entità modellando esplicitamente le azioni come trasformatori di stato. Il modello aggiorna gli stati delle entità eseguendo gli operatori di azione appresi. I risultati empirici dimostrano che il nostro modello proposto può ragionare sugli effetti causali non dichiarati delle azioni, permettendo di fornire informazioni contestuali più accurate per comprendere e generare testo procedurale, il tutto offrendo rappresentazioni interne più interpretabili rispetto alle alternative esistenti.
C'è stata una tendenza recente nell'addestramento delle reti neurali per sostituire le strutture di dati che sono state create a mano, con l'obiettivo di un'esecuzione più veloce, una migliore precisione o una maggiore compressione.  In questa impostazione, una struttura dati neurale viene istanziata addestrando una rete su molte epoche dei suoi input fino alla convergenza.In molte applicazioni questa costosa inizializzazione non è pratica, per esempio gli algoritmi di streaming --- dove gli input sono effimeri e possono essere ispezionati solo un piccolo numero di volte.  In questo articolo esploriamo l'apprendimento dell'appartenenza approssimativa agli insiemi su un flusso di dati in una sola volta attraverso il meta-apprendimento. Proponiamo una nuova architettura di memoria, il Filtro di Bloom Neurale, che mostriamo essere più compressivo dei Filtri di Bloom e di diverse reti neurali con memoria aumentata esistenti in scenari di dati distorti o insiemi strutturati.
Sfruttiamo le recenti intuizioni dell'ottimizzazione del secondo ordine per le reti neurali per costruire un'approssimazione di Kronecker fattorizzata Laplace al posteriore sui pesi di una rete addestrata. La nostra approssimazione non richiede alcuna modifica della procedura di addestramento, permettendo ai professionisti di stimare l'incertezza dei loro modelli attualmente utilizzati in produzione senza doverli ri-addestrare. Dimostriamo che il nostro metodo fattorizzato Kronecker porta a migliori stime di incertezza su dati fuori distribuzione ed è più robusto a semplici attacchi avversari.Il nostro approccio richiede solo il calcolo di due matrici quadrate di fattori di curvatura per ogni strato.La loro dimensione è uguale al rispettivo quadrato delle dimensioni di input e output dello strato, rendendo il metodo efficiente sia computazionalmente che in termini di utilizzo della memoria.Illustriamo la sua scalabilità applicandolo a un'architettura di rete convoluzionale all'avanguardia.
In questo articolo, spieghiamo su un semplice modello a blocchi l'impatto della regolarizzazione del grafico completo, per cui una costante viene aggiunta a tutte le voci della matrice di adiacenza. In particolare, mostriamo che la regolarizzazione costringe l'incorporazione spettrale a concentrarsi sui blocchi più grandi, rendendo la rappresentazione meno sensibile al rumore o agli outlier.Illustriamo questi risultati su entrambi i dati sintetici e reali, mostrando come la regolarizzazione migliora i punteggi standard di clustering.
Il problema del bias di esposizione si riferisce alla discrepanza di formazione-inferenza causata dalla forzatura dell'insegnante nella stima di massima verosimiglianza (MLE) di formazione per i modelli linguistici auto-regressivi della rete neurale (LM).È stato considerato come un problema centrale per la formazione del modello di generazione del linguaggio naturale (NLG).Anche se molti algoritmi sono stati proposti per evitare la forzatura dell'insegnante e quindi per alleviare il bias di esposizione, c'è poco lavoro che mostra quanto sia grave il problema del bias di esposizione. In questo lavoro, per prima cosa identifichiamo la capacità di auto-recupero di LM addestrati con MLE, il che mette in dubbio la serietà dell'exposure bias, quindi sviluppiamo una definizione precisa e quantificabile per l'exposure bias. Tuttavia, secondo le nostre misurazioni in esperimenti controllati, c'è solo circa il 3% di guadagno di prestazioni quando la discrepanza tra addestramento e inferenza viene completamente rimossa.
La capacità degli algoritmi di evolvere o imparare protocolli di comunicazione (compositivi) è stata tradizionalmente studiata nella letteratura sull'evoluzione del linguaggio attraverso l'uso di compiti di comunicazione emergenti. Estendiamo il lavoro precedente, in cui gli agenti sono stati addestrati in ambienti simbolici, sviluppando agenti che sono in grado di imparare da dati pixel grezzi, una rappresentazione di input più impegnativa e realistica. Troviamo che il grado di struttura trovato nei dati di input influenza la natura dei protocolli emersi, e quindi corroborare l'ipotesi che il linguaggio compositivo strutturato ha maggiori probabilità di emergere quando gli agenti percepiscono il mondo come strutturato.  
Per la comprensione di documenti generici, informazioni come le dimensioni dei caratteri, la disposizione delle colonne e in generale il posizionamento delle parole possono portare informazioni semantiche che sono cruciali per risolvere un compito di intelligenza del documento a valle. Il nostro nuovo BERTgrid, che è basato su Chargrid di Katti et al. (2018), rappresenta un documento come una griglia di vettori di incorporamento contestualizzati di pezzi di parole, rendendo così la sua struttura spaziale e semantica accessibile alla rete neurale di elaborazione. Usiamo BERTgrid in combinazione con una rete completamente convoluzionale su un compito di segmentazione semantica dell'istanza per l'estrazione dei campi dalle fatture, dimostrando le sue prestazioni sull'estrazione dei campi delle voci di riga e dell'intestazione del documento.
Le politiche profonde di apprendimento per rinforzo (RL) sono note per essere vulnerabili alle perturbazioni avversarie alle loro osservazioni, simili agli esempi avversari per i classificatori.Tuttavia, un attaccante non è solitamente in grado di modificare direttamente le osservazioni di un altro agente.Questo potrebbe portare a chiedersi: è possibile attaccare un agente RL semplicemente scegliendo una politica avversaria che agisce in un ambiente multi-agente in modo da creare osservazioni naturali che siano avversarie? Dimostriamo l'esistenza di politiche avversarie in giochi a somma zero tra robot umanoidi simulati con osservazioni propriocettive, contro vittime allo stato dell'arte addestrate attraverso l'auto-gioco per essere robuste agli avversari. Le politiche avversarie vincono in modo affidabile contro le vittime, ma generano un comportamento apparentemente casuale e scoordinato. Troviamo che queste politiche hanno più successo in ambienti ad alta densità, e inducono attivazioni sostanzialmente diverse nella rete delle politiche delle vittime rispetto a quando la vittima gioca contro un avversario normale.
I metodi GloVe e Skip-gram word embedding imparano i vettori di parole decomponendo una matrice denocciolata di co-occorrenze di parole in un prodotto di matrici a basso rango.In questo lavoro, proponiamo un algoritmo iterativo per il calcolo dei vettori di parole basato sulla modellazione delle matrici di co-occorrenze di parole con Generalized Low Rank Models. Il nostro algoritmo generalizza sia Skip-gram che GloVe e dà origine ad altri metodi di incorporazione basati sulla matrice di co-occorrenza specificata, sulla distribuzione delle co-occorrenze e sul numero di iterazioni dell'algoritmo iterativo. Per esempio, usando una distribuzione Tweedie con un'iterazione si ottiene GloVe e usando una distribuzione Multinomiale con modalità di convergenza completa si ottiene Skip-gram. I risultati sperimentali dimostrano che le iterazioni multiple del nostro algoritmo migliorano i risultati rispetto al metodo GloVe sul compito di somiglianza analogica delle parole di Google.
I modelli deterministici sono approssimazioni della realtà che sono spesso più facili da costruire e interpretare rispetto alle alternative stocastiche.  Sfortunatamente, poiché la natura è capricciosa, i dati osservativi non possono mai essere completamente spiegati da modelli deterministici nella pratica.  L'osservazione ed il rumore di processo devono essere aggiunti per adattare i modelli deterministici per comportarsi stocasticamente, tali che siano capaci di spiegare e di estrapolare dai dati rumorosi. L'aggiunta del rumore di processo ai simulatori deterministici puÃ² indurre un guasto nel simulatore con conseguente nessun valore di ritorno per determinati input -- una proprietÃ che descriviamo come ``brittle.''We esamina ed indirizza la computazione sprecata che sorge da questi guasti e l'effetto di tali guasti sulle mansioni di inferenza a valle. Mostriamo che l'esecuzione dell'inferenza in questo spazio può essere vista come un campionamento di rifiuto, e addestriamo un flusso di normalizzazione condizionale come una proposta sui valori di rumore tale che c'è una bassa probabilità che il simulatore vada in crash, aumentando l'efficienza computazionale e la fedeltà di inferenza per un budget di campione fisso quando viene usato come proposta in un algoritmo di inferenza approssimato.
La maggior parte degli approcci attuali imparano ad affrontare questo compito in un modo end-to-end con reti neurali, senza mantenere una rappresentazione esplicita del processo di ragionamento. Noi proponiamo un metodo per estrarre una catena discreta di ragionamento sul testo, che consiste in una serie di frasi che portano alla risposta. In seguito alimentiamo le catene estratte a un modello di AQ basato su BERT per fare la predizione della risposta finale.Criticamente, non ci basiamo su catene annotate in oro o ``fatti di supporto'': al momento dell'addestramento, deriviamo catene di ragionamento pseudogold usando l'euristica basata sul riconoscimento delle entità nominate e la risoluzione delle coreferenze.Né ci basiamo su queste annotazioni al momento del test, poiché il nostro modello impara a estrarre catene dal solo testo grezzo.  Testiamo il nostro approccio su due grandi dataset di risposta alle domande multi-hop proposti di recente: La nostra analisi mostra le proprietà delle catene che sono cruciali per le alte prestazioni: in particolare, la modellazione dell'estrazione in modo sequenziale è importante, così come la gestione di ogni frase candidata in un modo consapevole del contesto.Inoltre, la valutazione umana mostra che le nostre catene estratte permettono agli umani di dare risposte con alta fiducia, indicando che queste sono una forte astrazione intermedia per questo compito.
Normalizzare la costante (chiamata anche funzione di partizione, evidenza bayesiana o verosimiglianza marginale) è uno degli obiettivi centrali dell'inferenza bayesiana, ma la maggior parte dei metodi esistenti sono sia costosi che imprecisi. Applichiamo un nuovo approccio Normalizing Flow (NF) per ottenere uno stimatore di densità analitico da questi campioni, seguito da Optimal Bridge Sampling (OBS) per ottenere la costante di normalizzazione. Confrontiamo il nostro metodo, che chiamiamo Gaussianized Bridge Sampling (GBS), con i metodi esistenti come Nested Sampling (NS) e Annealed Importance Sampling (AIS) su diversi esempi, mostrando che il nostro metodo è sia significativamente più veloce e sostanzialmente più accurato di questi metodi, e viene fornito con una stima affidabile degli errori.
Presentiamo uno studio empirico su larga scala della dimenticanza catastrofica (CF) nei moderni modelli Deep Neural Network (DNN) che eseguono l'apprendimento sequenziale (o: incrementale).Viene proposto un nuovo protocollo sperimentale che tiene conto dei vincoli tipici incontrati negli scenari di applicazione. Poiché l'indagine è empirica, valutiamo il comportamento di CF sul numero finora più grande di dataset di classificazione visiva, da ognuno dei quali costruiamo un numero rappresentativo di Sequential Learning Tasks (SLTs) in stretto allineamento con i lavori precedenti su CF.I nostri risultati indicano chiaramente che non esiste un modello che eviti CF per tutti i dataset investigati e SLTs in condizioni di applicazione.Concludiamo con una discussione di potenziali soluzioni e workaround a CF, in particolare per i modelli EWC e IMM.
Federated Learning (FL) si riferisce all'apprendimento di un modello globale di alta qualità basato sull'archiviazione decentralizzata dei dati, senza mai copiare i dati grezzi.Uno scenario naturale si presenta con i dati creati sui telefoni cellulari dall'attività dei loro utenti.Data la tipica eterogeneità dei dati in tali situazioni, è naturale chiedersi come il modello globale possa essere personalizzato per ogni dispositivo, individualmente. In questo lavoro, sottolineiamo che l'impostazione del Model Agnostic Meta Learning (MAML), in cui si ottimizza per un adattamento veloce, basato sul gradiente e su pochi colpi a una distribuzione eterogenea di compiti, ha una serie di somiglianze con l'obiettivo della personalizzazione per FL.Presentiamo FL come una fonte naturale di applicazioni pratiche per gli algoritmi MAML, e facciamo le seguenti osservazioni.1 ) Il popolare algoritmo FL, Federated Averaging, può essere interpretato come un algoritmo di meta apprendimento.2) Un attento fine-tuning può produrre un modello globale con una maggiore accuratezza, che è allo stesso tempo più facile da personalizzare.Tuttavia, ottimizzando solo per l'accuratezza del modello globale si ottiene un risultato di personalizzazione più debole.3) Un modello addestrato utilizzando un metodo standard di ottimizzazione del datacenter è molto più difficile da personalizzare, rispetto a uno addestrato utilizzando Federated Averaging, sostenendo la prima affermazione.Questi risultati sollevano nuove questioni per FL, MAML, e più ampia ricerca ML.
La memorizzazione dei dati nelle reti neurali profonde è diventata un argomento di notevole interesse per la ricerca. In questo articolo, colleghiamo la memorizzazione delle immagini negli autocodificatori convoluzionali profondi al downsampling attraverso la convoluzione strided.  Per analizzare questo meccanismo in un contesto più semplice, addestriamo autocodificatori convoluzionali lineari e mostriamo che le combinazioni lineari dei dati di addestramento sono memorizzate come autovettori nell'operatore lineare corrispondente alla rete quando viene utilizzato il downsampling.  D'altra parte, le reti senza downsampling non memorizzano i dati di allenamento.  Forniamo ulteriori prove che lo stesso effetto si verifica nelle reti non lineari.  Inoltre, il downsampling nelle reti non lineari fa sì che il modello non memorizzi solo combinazioni lineari di immagini, ma anche singole immagini di allenamento.  Poiché i componenti convoluzionali autoencoder sono elementi costitutivi delle reti convoluzionali profonde, prevediamo che i nostri risultati faranno luce sull'importante fenomeno della memorizzazione nelle reti profonde iper-parametrizzate.  
L'apprendimento per rinforzo fornisce una struttura potente e generale per il processo decisionale e il controllo, ma la sua applicazione pratica è spesso ostacolata dalla necessità di un'ampia ingegnerizzazione delle caratteristiche e delle ricompense. I metodi di apprendimento per rinforzo profondo possono rimuovere la necessità di un'ingegnerizzazione esplicita delle caratteristiche della politica o del valore, ma richiedono ancora una funzione di ricompensa specificata manualmente. In questo lavoro, proponiamo AIRL, un algoritmo di apprendimento di rinforzo inverso pratico e scalabile basato su una formulazione di apprendimento della ricompensa avversaria che è competitiva con gli algoritmi di apprendimento per imitazione diretta. Inoltre, dimostriamo che AIRL è in grado di recuperare funzioni di ricompensa portatili che sono robuste ai cambiamenti nella dinamica, consentendoci di imparare le politiche anche in condizioni di variazione significativa dell'ambiente durante l'addestramento.
Consideriamo due domande al cuore dell'apprendimento automatico: come possiamo prevedere se un minimo generalizzerà al set di test, e perché la discesa del gradiente stocastico trova i minimi che generalizzano bene? Il nostro lavoro risponde a \citet{zhang2016comprensione}, che ha dimostrato che le reti neurali profonde possono facilmente memorizzare dati di allenamento etichettati a caso, nonostante generalizzino bene su etichette reali degli stessi input.Mostriamo che lo stesso fenomeno si verifica in piccoli modelli lineari.Queste osservazioni sono spiegate dalla prova bayesiana, che penalizza i minimi netti ma è invariante alla parametrizzazione del modello. Dimostriamo anche che, quando si tiene fisso il tasso di apprendimento, c'è una dimensione ottimale dei lotti che massimizza l'accuratezza del test set, e proponiamo che il rumore introdotto da piccoli mini lotti spinge i parametri verso i minimi la cui evidenza è grande. Interpretando la discesa del gradiente stocastico come un'equazione differenziale stocastica, identifichiamo la "scala del rumore" $g = \epsilon (\frac{N}{B} - 1) \approx \epsilon N/B$, dove $ \epsilon$ è il tasso di apprendimento, $N$ la dimensione del set di allenamento e $B$ la dimensione del batch.Di conseguenza la dimensione ottimale del batch è proporzionale sia al tasso di apprendimento sia alla dimensione del set di allenamento, $B_{opt} \propto \epsilon N$.Verifichiamo queste previsioni empiricamente.
Nel campo industriale, l'annichilimento dei positroni non è influenzato da un ambiente complesso, e la penetrazione dei fotoni gamma è forte, quindi il rilevamento non distruttivo delle parti industriali può essere realizzato.A causa della scarsa qualità dell'immagine causata dalla dispersione dei fotoni gamma, l'attenuazione e il breve tempo di campionamento nel processo dei positroni, proponiamo l'idea di combinare l'apprendimento profondo per generare immagini di positroni con buona qualità e dettagli chiari da reti avversarie. La struttura dell'articolo è la seguente: in primo luogo, codifichiamo per ottenere i vettori nascosti delle immagini CT mediche basate sul transfer Learning, e usiamo PCA per estrarre le caratteristiche dell'immagine di positroni.In secondo luogo, costruiamo una memoria dell'immagine di positroni basata sul meccanismo di attenzione come un intero input per le reti avversarie che usa le variabili nascoste mediche come una query.Infine, addestriamo l'intero modello congiuntamente e aggiorniamo i parametri di input fino alla convergenza.Gli esperimenti hanno dimostrato la possibilità di generare immagini di positroni rare per test industriali non distruttivi usando reti contromisure, e sono stati ottenuti buoni risultati di imaging.
Rivisitiamo il Recurrent Attention Model (RAM, Mnih et al. (2014)), una rete neurale ricorrente per l'attenzione visiva, da una prospettiva di campionamento attivo delle informazioni. Prendiamo in prestito idee dalla ricerca delle neuroscienze sul ruolo del campionamento attivo delle informazioni nel contesto dell'attenzione visiva e dello sguardo (Gottlieb, 2018), dove l'autore ha suggerito tre tipi di motivi per le strategie di campionamento attivo delle informazioni.Troviamo che il modello RAM originale implementa solo uno di loro. Identifichiamo tre debolezze chiave della RAM originale e forniamo una soluzione semplice aggiungendo due termini extra sulla funzione obiettivo.La RAM modificata1) raggiunge una convergenza più veloce,2) consente un processo decisionale dinamico per campione senza perdita di precisione, e3) generalizza molto meglio su una sequenza più lunga di scorci per cui non è addestrata, rispetto alla RAM originale. 
Graph Neural Networks (GNNs) per compiti di predizione come la classificazione dei nodi o la predizione dei bordi hanno ricevuto un'attenzione crescente nel recente apprendimento automatico da dati strutturati graficamente.Tuttavia, una grande quantità di grafici etichettati è difficile da ottenere, che limitano significativamente il vero successo di GNNs.Although l'apprendimento attivo è stato ampiamente studiato per affrontare le questioni label-sparse con altri tipi di dati come testo, immagini, ecc, come renderlo efficace sui grafici è una questione aperta per la ricerca.  In questo articolo, presentiamo l'indagine sull'apprendimento attivo con GNNs per compiti di classificazione dei nodi.  In particolare, proponiamo un nuovo metodo, che utilizza la propagazione delle caratteristiche dei nodi seguita dal clustering K-Medoids dei nodi per la selezione delle istanze nell'apprendimento attivo.Con un'analisi dei limiti teorici giustifichiamo la scelta del design del nostro approccio.Nei nostri esperimenti su quattro dataset di riferimento, il metodo proposto supera altri metodi rappresentativi di base in modo coerente e significativo.
Continuous Normalizing Flows (CNFs) sono emersi come promettenti modelli generativi profondi per una vasta gamma di compiti grazie alla loro invertibilità e stima esatta della verosimiglianza.Tuttavia, condizionare CNFs su segnali di interesse per la generazione di immagini condizionali e compiti predittivi a valle è inefficiente a causa del codice latente ad alta densità generato dal modello, che deve essere della stessa dimensione dei dati di input. In questo articolo, proponiamo InfoCNF, un efficiente CNF condizionale che partiziona lo spazio latente in un codice supervisionato specifico della classe e un codice non supervisionato condiviso tra tutte le classi per un uso efficiente delle informazioni etichettate. Poiché la strategia di partizionamento aumenta (leggermente) il numero di valutazioni di funzione (NFEs), InfoCNF impiega anche reti di gating per imparare le tolleranze di errore dei suoi solutori di equazioni differenziali ordinarie (ODE) per una migliore velocità e prestazioni.Mostriamo empiricamente che InfoCNF migliora l'accuratezza del test rispetto alla linea di base, mentre produce punteggi di verosimiglianza comparabili e riduce le NFEs su CIFAR10.Inoltre, applicando la stessa strategia di partizionamento in InfoCNF su dati di serie temporali aiuta a migliorare le prestazioni di estrapolazione.
Un obiettivo centrale dell'apprendimento non supervisionato è quello di acquisire rappresentazioni dai dati non etichettati o dall'esperienza che possono essere utilizzati per un apprendimento più efficace dei compiti a valle da quantità modeste di dati etichettati.Molti lavori precedenti di apprendimento non supervisionato mirano a farlo sviluppando obiettivi proxy basati su ricostruzione, disentanglement, previsione, e altre metriche. Al contrario, noi sviluppiamo un metodo di meta-apprendimento non supervisionato che ottimizza esplicitamente la capacità di imparare una varietà di compiti da piccole quantità di dati. Per fare ciò, costruiamo compiti da dati non etichettati in modo automatico ed eseguiamo il meta-apprendimento sui compiti costruiti. I nostri esperimenti su quattro serie di dati di immagini indicano che il nostro approccio di meta-apprendimento non supervisionato acquisisce un algoritmo di apprendimento senza alcun dato etichettato che è applicabile a una vasta gamma di compiti di classificazione a valle, migliorando l'incorporazione appresa da quattro metodi di apprendimento non supervisionato precedenti.
Il trasferimento di dominio è un ramo eccitante e impegnativo dell'apprendimento automatico perché i modelli devono imparare a trasferire agevolmente tra i domini, preservando le variazioni locali e catturando molti aspetti della variazione senza etichette. Tuttavia, la maggior parte delle applicazioni di successo fino ad oggi richiedono che i due domini siano strettamente correlati (es. immagine-immagine, video-video), utilizzando reti simili o condivise per trasformare le proprietà specifiche del dominio come texture, colorazione e forme delle linee. Qui, dimostriamo che è possibile trasferire attraverso le modalità (es. immagine-audio) astraendo prima i dati con modelli generativi latenti e poi imparando le trasformazioni tra spazi latenti. Scopriamo che un semplice autoencoder variazionale è in grado di imparare uno spazio latente condiviso per fare da ponte tra due modelli generativi in modo non supervisionato, e anche tra diversi tipi di modelli (es. autoencoder variazionale e una rete generativa avversaria). Possiamo inoltre imporre l'allineamento semantico desiderato degli attributi con un classificatore lineare nello spazio latente condiviso. L'autocodificatore variazionale proposto permette di preservare sia la località che l'allineamento semantico attraverso il processo di trasferimento, come mostrato nelle valutazioni qualitative e quantitative. Infine, la struttura gerarchica disaccoppia il costo dell'addestramento dei modelli generativi di base e degli allineamenti semantici, permettendo una riqualificazione computazionalmente efficiente ed efficiente dei dati delle funzioni di mappatura personalizzate.
Proponiamo l'Adversarial Inductive Transfer Learning (AITL), un metodo per affrontare le discrepanze negli spazi di input e output tra i domini di origine e di destinazione. La nostra applicazione motivante è la farmacogenomica, dove l'obiettivo è quello di prevedere la risposta ai farmaci nei pazienti utilizzando le loro informazioni genomiche. La sfida è che i dati clinici (cioè i pazienti) con risultati di risposta ai farmaci è molto limitata, creando la necessità di apprendimento di trasferimento per colmare il divario tra grandi set di dati di farmacogenomica pre-clinica (ad es. Le discrepanze esistono tra1) i dati genomici dei set di dati pre-clinici e clinici (lo spazio di input), e2) le diverse misure della risposta al farmaco (lo spazio di output).Al meglio della nostra conoscenza, AITL è il primo metodo di apprendimento induttivo contraddittorio di trasferimento per affrontare entrambe le discrepanze di input e output.I risultati sperimentali indicano che AITL supera lo stato-of-the-art farmacogenomica e di apprendimento di trasferimento di base e può guidare oncologia precisione più accuratamente.
Il riconoscimento di entità nominate (NER) e l'estrazione di relazioni (RE) sono due compiti importanti nell'estrazione e nel recupero delle informazioni (IE & IR).Il lavoro recente ha dimostrato che è vantaggioso imparare questi compiti congiuntamente, che evita la propagazione dell'errore inerente ai sistemi basati su pipeline e migliora le prestazioni.Tuttavia, i modelli congiunti allo stato dell'arte si basano tipicamente su strumenti esterni di elaborazione del linguaggio naturale (NLP), come i parser delle dipendenze, limitando la loro utilità ai domini (ad esempio le notizie) dove questi strumenti funzionano bene. I pochi modelli neurali end-to-end che sono stati proposti sono addestrati quasi completamente da zero.In questo articolo, proponiamo un modello neurale end-to-end per l'estrazione congiunta di entità e delle loro relazioni che non si basa su strumenti NLP esterni e che integra un grande modello linguistico pre-addestrato.Poiché la maggior parte dei parametri del nostro modello sono pre-addestrati e si evita la ricorrenza per l'autoattenzione, il nostro modello è veloce da addestrare.Su 5 serie di dati in 3 domini, il nostro modello corrisponde o supera le prestazioni dello stato dell'arte, a volte con un ampio margine.
In primo luogo, dimostriamo che un semplice adattamento della backpropagation troncata nel tempo può produrre stime di incertezza di buona qualità e una regolarizzazione superiore con un piccolo costo computazionale in più durante l'addestramento, riducendo anche la quantità di parametri dell'80%. In secondo luogo, dimostriamo come un nuovo tipo di approssimazione posteriore produca ulteriori miglioramenti alle prestazioni delle RNN bayesiane. Dimostriamo come questa tecnica non sia esclusiva delle reti neurali ricorrenti e possa essere applicata più ampiamente per addestrare le reti neurali bayesiane.Dimostriamo anche empiricamente come le RNN bayesiane siano superiori alle RNN tradizionali su un benchmark di modellazione linguistica e su un compito di sottotitolazione delle immagini, oltre a mostrare come ciascuno di questi metodi migliori il nostro modello rispetto a una varietà di altri schemi per addestrarli.Introduciamo anche un nuovo benchmark per studiare l'incertezza dei modelli linguistici in modo che i metodi futuri possano essere facilmente confrontati.
Con il passare del tempo i veicoli autonomi senza equipaggio (UAV), in particolare i droni volanti autonomi, hanno attirato molta attenzione nel campo dell'intelligenza artificiale, poiché la tecnologia elettronica sta diventando più piccola, più economica e più efficiente. Dal monitoraggio delle inondazioni, discernendo la diffusione delle alghe nei corpi idrici per rilevare le tracce della foresta, il nostro lavoro è principalmente focalizzato sui flyingdrones autonomi dove stabiliamo un caso di studio verso l'efficienza, la robustezza e la precisione degli UAV dove abbiamo mostrato i nostri risultati ben supportati attraverso gli esperimenti. Forniamo i dettagli dell'architettura software e hardware utilizzata nello studio.Wefurther discutere circa i nostri algoritmi di implementazione e presentare esperimenti che forniscono un confronto tra tre diversi algoritmi state-of-the-art e cioèTrailNet, InceptionResnet e MobileNet in termini di precisione, robustezza, consumo di energia e tempo di inferenza.Nel nostro studio, abbiamo dimostrato che MobileNet haprodotto risultati migliori con requisito computazionale molto meno e consumo di energia.Abbiamo anche riportato le sfide che abbiamo affrontato durante il nostro lavoro, nonché una breve discussione sul nostro lavoro futuro per migliorare le caratteristiche di sicurezza e prestazioni.
La musica si basa molto sulla ripetizione per costruire la struttura e il significato.  L'autoreferenzialità si verifica su più scale temporali, dai motivi alle frasi al riutilizzo di intere sezioni di musica, come nei pezzi con struttura ABA.  Il Transformer (Vaswani et al., 2017), un modello di sequenza basato sull'auto-attenzione, ha ottenuto risultati convincenti in molti compiti di generazione che richiedono il mantenimento della coerenza a lungo raggio.  Gli approcci esistenti per rappresentare le informazioni posizionali relative nel trasformatore modulano l'attenzione in base alla distanza a coppie (Shaw et al., 2018).  Questo è impraticabile per lunghe sequenze come le composizioni musicali, poiché la loro complessità di memoria è quadratica nella lunghezza della sequenza.  Proponiamo un algoritmo che riduce i requisiti di memoria intermedia a lineare nella lunghezza della sequenza.Questo ci permette di dimostrare che un Transformer con il nostro meccanismo di attenzione relativa modificato può generare composizioni lunghe un minuto (migliaia di passi) con struttura convincente, generare continuazioni che elaborano coerentemente un dato motivo, e in una configurazione seq2seq generare accompagnamenti condizionati su melodie.   Valutiamo il Transformer con il nostro meccanismo di attenzione relativa su due dataset, JSB Chorales e Piano-e-competition, e otteniamo risultati all'avanguardia su quest'ultimo.
I problemi decisionali sequenziali per le applicazioni del mondo reale spesso devono essere risolti in tempo reale, richiedendo algoritmi che si comportino bene con un budget computazionale limitato. I lookahead basati sulla larghezza hanno mostrato prestazioni allo stato dell'arte nei classici problemi di pianificazione così come nei giochi Atari con budget limitati. Analizziamo il motivo per cui gli algoritmi basati sulla larghezza hanno scarse prestazioni nei problemi SSP, e superiamo queste insidie proponendo un metodo per stimare i costi di percorrenza. Formalizziamo i lookahead basati sulla larghezza come un'istanza dell'algoritmo di rollout, diamo una definizione di larghezza per i problemi SSP e spieghiamo la sua complessità campionaria.I nostri risultati sperimentali su una varietà di benchmark SSP mostrano che l'algoritmo supera altri algoritmi di rollout all'avanguardia come UCT e RTDP.
Le reti neurali profonde (DNN) sono note per le eccellenti prestazioni in compiti supervisionati come la classificazione.Le reti neurali convoluzionali (CNN), in particolare, possono imparare caratteristiche efficaci e costruire rappresentazioni di alto livello che possono essere utilizzate per la classificazione, ma anche per l'interrogazione e la ricerca del vicino più vicino.Tuttavia, le CNN hanno anche dimostrato di soffrire di un calo delle prestazioni quando la distribuzione dei dati cambia dai dati di allenamento a quelli di test. In questo articolo analizziamo le rappresentazioni interne delle CNN e osserviamo che le rappresentazioni dei dati non visti in ogni classe, si diffondono di più (con una varianza più alta) nello spazio di incorporazione della CNN rispetto alle rappresentazioni dei dati di allenamento.Più importante, questa differenza è più estrema se i dati non visti provengono da una distribuzione spostata.Sulla base di questa osservazione, valutiamo oggettivamente il grado di varianza della rappresentazione in ogni classe applicando la decomposizione dell'autovalore sulla covarianza entro la classe delle rappresentazioni interne delle CNN e osserviamo lo stesso comportamento. Questo può essere problematico in quanto varianze più grandi potrebbero portare a una classificazione errata se il campione attraversa il confine decisionale della sua classe. Applichiamo la classificazione nearest neighbor sulle rappresentazioni e dimostriamo empiricamente che le embeddings con l'alta varianza hanno effettivamente prestazioni di classificazione KNN significativamente peggiori, anche se questo non potrebbe essere previsto dai loro risultati di classificazione end-to-end. Per affrontare questo problema, proponiamo Deep Within-Class Covariance Analysis (DWCCA), uno strato di rete neurale profonda che riduce significativamente la covarianza within-class della rappresentazione di una DNN, migliorando le prestazioni sui dati di test non visti di una distribuzione spostata. Valutiamo empiricamente DWCCA su due set di dati per la classificazione di scene acustiche (DCASE2016 e DCASE2017) e dimostriamo che non solo DWCCA migliora significativamente la rappresentazione interna della rete, ma aumenta anche l'accuratezza della classificazione end-to-end, specialmente quando il set di test presenta un leggero spostamento della distribuzione, aggiungendo DWCCA a una rete neurale VGG, otteniamo circa 6 punti percentuali di miglioramento nel caso di un errore di distribuzione.
I modelli generativi hanno dimostrato di essere uno strumento eccezionale per rappresentare distribuzioni di probabilità ad alta densità e generare immagini dall'aspetto realistico. Una caratteristica fondamentale dei modelli generativi è la loro capacità di produrre output multimodali. Tuttavia, durante l'addestramento, sono spesso suscettibili di collasso della modalità, il che significa che il modello è limitato nella mappatura del rumore in ingresso solo a poche modalità della vera distribuzione dei dati. In questo articolo, ci ispiriamo al Determinantal Point Process (DPP) per ideare un modello generativo che allevia il collasso delle modalità e produce campioni di qualità superiore. Il DPP è un'elegante misura probabilistica utilizzata per modellare le correlazioni negative all'interno di un sottoinsieme e quindi quantificare la sua diversità. Usiamo il kernel DPP per modellare la diversità nei dati reali così come nei dati sintetici.Poi, ideiamo un termine di penalità di generazione che incoraggia il generatore a sintetizzare dati con una diversità simile ai dati reali.In contrasto con i precedenti modelli generativi allo stato dell'arte che tendono a usare parametri addestrabili aggiuntivi o paradigmi di formazione complessi, il nostro metodo non cambia lo schema di formazione originale. Incorporato in un training adversariale e in un autocodificatore variazionale, il nostro approccio generativo DPP mostra una resistenza consistente al mode-collapse su un'ampia varietà di dati sintetici e dataset di immagini naturali tra cui MNIST, CIFAR10 e CelebA, mentre supera i metodi allo stato dell'arte per efficienza dei dati, tempo di convergenza e qualità della generazione.Il nostro codice sarà reso disponibile al pubblico.
Nonostante il lavoro esistente sulla garanzia della generalizzazione delle reti neurali in termini di misure di complessità sensibili alla scala, come norme, margine e nitidezza, queste misure di complessità non offrono una spiegazione del perché le reti neurali generalizzano meglio con la sovra-parametrizzazione. Il nostro limite di capacità è correlato al comportamento dell'errore di test con l'aumento delle dimensioni della rete (entro l'intervallo riportato negli esperimenti), e potrebbe in parte spiegare il miglioramento della generalizzazione con l'over-parametrizzazione.Presentiamo inoltre un limite inferiore corrispondente per la complessità di Rademacher che migliora rispetto ai precedenti limiti inferiori di capacità per le reti neurali.
Introduciamo tre blocchi generici di elaborazione delle nuvole di punti che migliorano sia l'accuratezza che il consumo di memoria di più reti all'avanguardia, permettendo così di progettare reti più profonde e accurate. I nuovi blocchi di elaborazione che facilitano il flusso di informazioni efficienti sono un blocco di operazione di tipo convoluzione per gli insiemi di punti che fonde le informazioni del vicinato in un modo efficiente dal punto di vista della memoria; un blocco di elaborazione multirisoluzione delle nuvole di punti; e un blocco crosslink che condivide in modo efficiente le informazioni tra rami di elaborazione a bassa e alta risoluzione. Combinando questi blocchi, progettiamo architetture significativamente più ampie e profonde.Valutiamo ampiamente le architetture proposte su più benchmark di segmentazione dei punti (ShapeNetPart, ScanNet, PartNet) e riportiamo miglioramenti sistematici sia in termini di precisione che di consumo di memoria utilizzando i nostri moduli generici insieme a più architetture recenti (PointNet++, DGCNN, SpiderCNN, PointCNN).Segnaliamo un aumento del 9,7% di IoU sul dataset PartNet, che è il più complesso, mentre diminuiamo l'impronta di memoria del 57%.
I modelli di riconoscimento vocale end-to-end acustico-parola hanno recentemente guadagnato popolarità perché sono facili da addestrare, si adattano bene a grandi quantità di dati di formazione e non richiedono un lessico. Inoltre, i modelli di parola possono anche essere più facili da integrare con compiti a valle come la comprensione del linguaggio parlato, perché l'inferenza (ricerca) è molto semplificata rispetto a fonemi, caratteri o qualsiasi altro tipo di unità sub-parola. Su una serie di 16 compiti standard di valutazione delle frasi, le nostre incorporazioni mostrano prestazioni competitive rispetto a un modello word2vec addestrato sulle trascrizioni del discorso.Inoltre, valutiamo queste incorporazioni su un compito di comprensione del linguaggio parlato e osserviamo che le nostre incorporazioni corrispondono alle prestazioni delle incorporazioni basate sul testo in una pipeline che prima esegue il riconoscimento vocale e poi costruisce le incorporazioni di parole dalle trascrizioni.
La stima non supervisionata della profondità monoculare ha fatto grandi progressi dopo il coinvolgimento del deeplearning.l'addestramento con immagini stereo binoculari è considerato una buona opzione in quanto i dati possono essere facilmente ottenuti.tuttavia, la profondità o i risultati di previsione della disparità mostrano scarse prestazioni per i confini dell'oggetto.la ragione principale è legata alla gestione delle aree di occlusione durante l'addestramento. Sfruttando la proprietà delle mappe di disparità, generiamo una maschera di occlusione per bloccare la back-propagation delle aree di occlusione durante il warping dell'immagine, inoltre progettiamo nuove reti con immagini stereo capovolte per indurre le reti a imparare i confini occlusi, dimostrando che il nostro metodo raggiunge confini più chiari e migliori risultati di valutazione su KITTIdriving dataset e Virtual KITTI dataset.
La classificazione dei grafi è attualmente dominata dai kernel di grafi, che, sebbene potenti, soffrono di alcune limitazioni significative. le reti neurali convoluzionali (CNN) offrono un'alternativa molto attraente. tuttavia, elaborare i grafi con le CNN non è banale. per affrontare questa sfida, recentemente sono state proposte molte estensioni sofisticate delle CNN. In questa carta, invertiamo il problema: piuttosto che proporre ancora un altro modello di CNN del grafico, introduciamo un senso novello rappresentare i grafici come strutture immagine-come multicanale che permette che siano trattate da CNNs 2D vanilla. malgrado la relativa semplicitÃ , il nostro metodo dimostra molto competitivo ai kernel del grafico dello stato-de-arte ed ai CNNs del grafico e li supera da un ampio margine su alcuni insiemi di dati. Ã¨ inoltre preferibile ai kernel del grafico in termini di complessitÃ di tempo. codice e dati sono pubblicamente disponibili.
L'attributo chiave che guida il successo senza precedenti delle moderne Reti Neurali Ricorrenti (RNN) su compiti di apprendimento che coinvolgono dati sequenziali, è la loro capacità sempre migliore di modellare intricate dipendenze temporali a lungo termine.Tuttavia, manca una misura ben stabilita della capacità di memoria a lungo termine delle RNN, e quindi la comprensione formale della loro capacità di correlare dati nel tempo è limitata. Sebbene l'efficienza della profondità nelle reti convoluzionali sia ormai consolidata, non è sufficiente per spiegare il successo delle RNN profonde su input di varia lunghezza, e nasce la necessità di affrontare la loro "potenza espressiva delle serie temporali". In questo articolo, analizziamo l'effetto della profondità sulla capacità delle reti ricorrenti di esprimere correlazioni che vanno su lunghe scale temporali. Essenzialmente, questa misura riflette la distanza della funzione realizzata dalla rete ricorrente da una funzione che non modella alcuna interazione tra l'inizio e la fine della sequenza di input.Dimostriamo che le reti ricorrenti profonde supportano i gradi di separazione Start-End che sono esponenzialmente più alti di quelli supportati dalle loro controparti superficiali.Inoltre, dimostriamo che la capacità delle reti ricorrenti profonde di correlare parti diverse della sequenza di input aumenta esponenzialmente con l'estensione della sequenza di input, mentre quella delle reti ricorrenti superficiali di vaniglia non si adatta affatto alla lunghezza della sequenza. Così, stabiliamo che la profondità porta un vantaggio schiacciante nella capacità delle reti ricorrenti di modellare le dipendenze a lungo termine, e forniamo un esempio di quantificazione di questo attributo chiave che può essere facilmente esteso ad altre architetture RNN di interesse, ad esempio le varianti di reti LSTM.Otteniamo i nostri risultati considerando una classe di reti ricorrenti denominate Circuiti Aritmetici Ricorrenti (RAC), che fondono lo stato nascosto con l'ingresso attraverso l'operazione di integrazione moltiplicativa.
Esplorare olisticamente le rappresentazioni percettive e neurali alla base della comunicazione animale è stato tradizionalmente molto difficile a causa della complessità del segnale sottostante. Presentiamo qui una nuova serie di tecniche per proiettare interi repertori comunicativi in spazi a bassa dimensione che possono essere sistematicamente campionati, esplorando la relazione tra rappresentazioni percettive, rappresentazioni neurali e spazi rappresentazionali latenti appresi da algoritmi di apprendimento automatico. Mostriamo questo metodo in un esperimento in corso che studia il mantenimento sequenziale e temporale del contesto nelle rappresentazioni neurali e percettive delle sillabe degli uccelli canori. Discutiamo inoltre come lo studio dei meccanismi neurali alla base del mantenimento del contenuto informativo a lungo raggio presente nel canto degli uccelli possa informare ed essere informato dalla modellazione automatica della sequenza.
Il principio del collo di bottiglia dell'informazione (Shwartz-Ziv & Tishby, 2017) suggerisce che l'addestramento basato su SGD di reti neurali profonde si traduce in strati nascosti ottimamente compressi, da una prospettiva teorica dell'informazione.Tuttavia, questa affermazione è stata stabilita su dati giocattolo.L'obiettivo del lavoro che presentiamo qui è quello di testare queste affermazioni in un ambiente realistico utilizzando un'architettura convoluzionale più grande e profonda, un modello ResNet. Abbiamo addestrato i modelli PixelCNN++ come decodificatori a rappresentazione inversa per misurare l'informazione reciproca tra gli strati nascosti di una ResNet e i dati dell'immagine in ingresso, quando addestrati per (1) la classificazione e (2) l'autocodifica.Troviamo che avvengono due fasi di apprendimento per entrambi i regimi di addestramento, e che la compressione avviene, anche per un autoencoder.Campionare le immagini condizionando le attivazioni degli strati nascosti offre una visualizzazione intuitiva per capire cosa una ResNets impara a dimenticare.
Studiamo il problema dell'adattamento sicuro: dato un modello addestrato su una varietà di esperienze passate per qualche compito, questo modello può imparare ad eseguire quel compito in una nuova situazione evitando il fallimento catastrofico? Questo problema si presenta frequentemente in scenari di apprendimento per rinforzo nel mondo reale, come un veicolo che si adatta a guidare in una nuova città, o un drone robotico che adatta una politica addestrata solo in simulazione. Mentre imparare senza fallimenti catastrofici è eccezionalmente difficile, l'esperienza precedente può permetterci di imparare modelli che rendono questo molto più facile. Questi modelli potrebbero non trasferirsi direttamente a nuove impostazioni, ma possono permettere un adattamento cauto che è sostanzialmente più sicuro dell'adattamento na\"{i}vo così come l'apprendimento da zero. Basandoci su questa intuizione, proponiamo l'adattamento al dominio avverso al rischio (RADA).RADA funziona in due fasi: prima allena agenti RL basati su modelli probabilistici in una popolazione di domini di origine per acquisire esperienza e catturare l'incertezza epistemica sulle dinamiche dell'ambiente. Poi, quando viene lasciato in un nuovo ambiente, impiega una politica di esplorazione pessimistica, selezionando le azioni che hanno la migliore performance nel caso peggiore, come previsto dal modello probabilistico. Mostriamo che questa semplice politica di maximin accelera l'adattamento del dominio in un ambiente di guida critico per la sicurezza con veicoli di dimensioni variabili.
Proponiamo il Neuro-Symbolic Concept Learner (NS-CL), un modello che impara concetti visivi, parole e parsing semantico delle frasi senza supervisione esplicita su nessuno di essi; invece, il nostro modello impara semplicemente guardando le immagini e leggendo domande e risposte accoppiate. Per collegare l'apprendimento di due moduli, usiamo un modulo di ragionamento neuro-simbolico che esegue questi programmi sulla rappresentazione latente della scena.Analogamente all'apprendimento dei concetti umani, il modulo di percezione impara concetti visivi basati sulla descrizione linguistica dell'oggetto a cui ci si riferisce. Nel frattempo, i concetti visivi appresi facilitano l'apprendimento di nuove parole e il parsing di nuove frasi.Usiamo l'apprendimento del curriculum per guidare la ricerca sul grande spazio compositivo delle immagini e del linguaggio.Estesi esperimenti dimostrano l'accuratezza e l'efficienza del nostro modello sull'apprendimento dei concetti visivi, sulle rappresentazioni delle parole e sul parsing semantico delle frasi.Inoltre, il nostro metodo permette una facile generalizzazione a nuovi attributi di oggetti, composizioni, concetti linguistici, scene e domande, e persino nuovi domini di programma.Esso permette anche applicazioni che includono la risposta visiva alle domande e il recupero bidirezionale di immagini e testi.
L'inferenza bayesiana offre un modo teoricamente fondato e generale per addestrare le reti neurali e può potenzialmente dare un'incertezza calibrata. Tuttavia, è impegnativo specificare una priorità significativa e trattabile sui parametri di rete e affrontare le correlazioni di peso nel posteriore. A tal fine, questo articolo introduce due innovazioni: (i) un modello gerarchico basato su un processo gaussiano per i parametri di rete basato su embedding di unità introdotti di recente che possono codificare in modo flessibile le strutture dei pesi, e (ii) variabili contestuali dipendenti dall'input per il priore del peso che possono fornire modi convenienti per regolarizzare lo spazio delle funzioni modellate dalla rete attraverso l'uso di kernel. Mostriamo che questi modelli forniscono stime di incertezza desiderabili in tempo di test, dimostriamo casi di modellazione di bias induttivi per le reti neurali con i kernel e dimostriamo prestazioni predittive competitive su un benchmark di apprendimento attivo.
Eseguiamo un'indagine approfondita sull'idoneità dei modelli di auto-attenzione per la traduzione automatica neurale a livello di carattere.Testiamo il modello standard di trasformatore, così come una nuova variante in cui il blocco codificatore combina le informazioni dai caratteri vicini usando la convoluzione.Eseguiamo ampi esperimenti sui dataset WMT e UN, testando sia la traduzione bilingue che multilingue in inglese usando fino a tre lingue di input (francese, spagnolo e cinese).La nostra variante di trasformatore supera costantemente il trasformatore standard a livello di carattere e converge più velocemente mentre apprende allineamenti più robusti a livello di carattere.
Il campo della diagnostica medica contiene una ricchezza di sfide che assomigliano molto ai classici problemi di apprendimento automatico; i vincoli pratici, tuttavia, complicano la traduzione di questi punti finali ingenuamente in architetture classiche.Molti compiti in radiologia, per esempio, sono in gran parte problemi di classificazione multi-label in cui le immagini mediche sono interpretate per indicare più patologie presenti o sospette.Le impostazioni cliniche guidano la necessità di alta precisione contemporaneamente attraverso una moltitudine di risultati patologici e limitano notevolmente l'utilità degli strumenti che considerano solo un sottoinsieme. Questo problema Ã¨ esacerbato da una scarsitÃ generale dei dati di addestramento e massimizza la necessitÃ di estrarre le caratteristiche clinicamente pertinenti dai campioni disponibili -- idealmente senza l'uso dei modelli pre-addestrati che possono portare avanti i pregiudizi indesiderabili dai compiti tangenzialmente relativi. Presentiamo e valutiamo una soluzione parziale a questi vincoli utilizzando LSTMs per sfruttare le interdipendenze tra le etichette di destinazione nella previsione di 14 modelli patologici dalle radiografie del torace e stabiliamo lo stato dell'arte dei risultati sul più grande set di dati di radiografie del torace pubblicamente disponibile dal NIH senza pre-addestramento.Inoltre, proponiamo e discutiamo metriche di valutazione alternative e la loro rilevanza nella pratica clinica.
Semmelhack et al. (2014) hanno raggiunto un'elevata precisione di classificazione nel distinguere gli attacchi di nuoto di zebrafish usando una Support Vector Machine (SVM).Le reti neurali convoluzionali (CNN) hanno raggiunto prestazioni superiori in vari compiti di riconoscimento delle immagini rispetto alle SVM, ma queste potenti reti rimangono una scatola nera.Raggiungere una migliore trasparenza aiuta a costruire la fiducia nelle loro classificazioni e rende le caratteristiche apprese interpretabili dagli esperti.Usando una tecnica recentemente sviluppata chiamata Deep Taylor Decomposition, abbiamo generato heatmaps per evidenziare le regioni di input di alta rilevanza per le previsioni. Abbiamo scoperto che la nostra CNN fa previsioni analizzando la stabilità del tronco della coda, che differisce notevolmente dalle caratteristiche estratte manualmente utilizzate da Semmelhack et al. (2014).Abbiamo inoltre scoperto che la rete ha prestato attenzione agli artefatti sperimentali.Rimuovere questi artefatti ha garantito la validità delle previsioni.Dopo la correzione, la nostra migliore CNN batte la SVM del 6,12%, ottenendo un'accuratezza di classificazione del 96,32%.Il nostro lavoro dimostra quindi l'utilità della spiegabilità AI per le CNN.
Quando si comunica, gli esseri umani si basano su rappresentazioni linguistiche internamente coerenti, cioè, come oratori, ci aspettiamo che gli ascoltatori si comportino nello stesso modo in cui ci comportiamo noi quando ascoltiamo.Questo lavoro propone diversi metodi per incoraggiare tale coerenza interna negli agenti di dialogo in un ambiente di comunicazione emergente. Consideriamo due ipotesi sull'effetto dei vincoli di coerenza interna: 1) che migliorano la capacità degli agenti di riferirsi a referenti non visti, e 2) che migliorano la capacità degli agenti di generalizzare attraverso i ruoli comunicativi (ad esempio, eseguendo come un altoparlante nonostante sia stato addestrato solo come ascoltatore).
Le reti neurali (NN) sono in grado di eseguire compiti che si basano sulla struttura compositiva anche se mancano di meccanismi ovvi per rappresentare questa struttura. Per analizzare le rappresentazioni interne che consentono tale successo, proponiamo ROLE, una tecnica che rileva se queste rappresentazioni codificano implicitamente la struttura simbolica. ROLE impara ad approssimare le rappresentazioni di un codificatore di destinazione E imparando una struttura simbolica costituente e un'inclusione di tale struttura nello spazio vettoriale rappresentazionale di E. I costituenti della struttura simbolica approssimata sono definiti da posizioni strutturali - ruoli - che possono essere riempiti da simboli, Analizziamo poi una rete seq2seq addestrata per eseguire un compito compositivo più complesso (SCAN), dove non è disponibile uno schema di ruoli di verità di base. Per questo modello, ROLE scopre con successo una struttura simbolica interpretabile che il modello usa implicitamente per eseguire il compito SCAN, fornendo un resoconto completo del legame tra le rappresentazioni e il comportamento di un tipo di modello notoriamente difficile da interpretare. Verifichiamo l'importanza causale della struttura simbolica scoperta mostrando che, quando manipoliamo sistematicamente le incorporazioni nascoste basate su questa struttura simbolica, anche l'output del modello viene modificato nel modo previsto dalla nostra analisi. Infine, usiamo ROLE per esplorare se i modelli popolari di incorporamento delle frasi catturano la struttura compositiva e troviamo prove che non lo fanno; concludiamo discutendo come le intuizioni di ROLE possono essere usate per imprimere nuove tendenze induttive che miglioreranno le capacità di composizione di tali modelli.
Le rappresentazioni neurali variano drasticamente nelle prime fasi dell'elaborazione visiva: all'uscita della retina, i campi recettivi delle cellule gangliari (RF) mostrano una chiara struttura antagonista centro-superiore, mentre nella corteccia visiva primaria (V1), i tipici RF sono fortemente sintonizzati su un preciso orientamento. Qui, utilizzando una rete neurale convoluzionale profonda addestrata al riconoscimento delle immagini come modello del sistema visivo, mostriamo che tali differenze di rappresentazione possono emergere come conseguenza diretta di diversi vincoli di risorse neurali sulle reti retiniche e corticali, e per la prima volta troviamo un unico modello da cui entrambe le geometrie emergono spontaneamente nelle fasi appropriate dell'elaborazione visiva. Il vincolo chiave è un numero ridotto di neuroni all'uscita della retina, coerente con l'anatomia del nervo ottico come stretto collo di bottiglia.In secondo luogo, troviamo che, per semplici reti corticali a valle, le rappresentazioni visive all'uscita della retina emergono come rilevatori di caratteristiche non lineari e con perdita, mentre emergono come codificatori lineari e fedeli della scena visiva per reti corticali più complesse.Questo risultato prevede che le retine di piccoli vertebrati (ad es. Queste previsioni potrebbero riconciliare i due punti di vista apparentemente incompatibili della retina come estrattori di caratteristiche o codificatori efficienti di scene naturali, suggerendo che tutti i vertebrati si trovano su uno spettro tra questi due obiettivi, a seconda del grado di risorse neurali assegnate al loro sistema visivo.
Anche se non è stato ancora dimostrato, l'evidenza empirica suggerisce che la generalizzazione del modello è legata alle proprietà locali degli ottimali che possono essere descritti attraverso l'Hessiano. In particolare, dimostriamo che la capacità di generalizzazione del modello è legata all'Hessiana, ai termini di "morbidezza" di ordine superiore caratterizzati dalla costante Lipschitz dell'Hessiana, e alle scale dei parametri.Guidati dalla dimostrazione, proponiamo una metrica per valutare la capacità di generalizzazione del modello, così come un algoritmo che ottimizza il modello perturbato di conseguenza.
L'apprendimento non supervisionato riguarda la cattura delle dipendenze tra le variabili ed è guidato dal contrasto tra le configurazioni probabili e improbabili di queste variabili, spesso attraverso un modello generativo che campiona solo quelle probabili o con una funzione energetica (log-densità non normalizzata) che è bassa per quelle probabili e alta per quelle improbabili. Mentre il critico (o discriminatore) nelle reti generative avversarie (GAN) impara a separare i dati e i campioni del generatore, l'introduzione di un regolatore di massimizzazione dell'entropia sul generatore può trasformare l'interpretazione del critico in una funzione di energia, che separa la distribuzione di addestramento da tutto il resto, e quindi può essere usata per compiti come il rilevamento di anomalie o novità. Questo articolo è motivato dalla vecchia idea di campionare nello spazio latente piuttosto che nello spazio dei dati, perché l'esecuzione di una catena di Markov Monte-Carlo (MCMC) nello spazio latente è stata trovata più facile e più efficiente, e perché un generatore simile a GAN può convertire i campioni dello spazio latente in campioni dello spazio dei dati.A questo scopo, mostriamo come una catena di Markov può essere eseguita nello spazio latente i cui campioni possono essere mappati nello spazio dei dati, producendo campioni migliori. Questi campioni sono anche usati per il gradiente di fase negativo richiesto per stimare il gradiente di log-likelihood della funzione di energia dello spazio dati.Per massimizzare l'entropia all'uscita del generatore, approfittiamo degli stimatori neurali di mutua informazione recentemente introdotti.Troviamo che oltre a produrre una funzione di punteggio utile per il rilevamento delle anomalie, l'approccio risultante produce campioni nitidi (come le GAN) mentre copre bene le modalità, portando ad alti punteggi Inception e Fréchet.
Questo recente successo nel trasferimento di stile dell'immagine ha sollevato la questione se metodi simili possono essere sfruttati per alterare lo "stile" dell'audio musicale. In questo lavoro, tentiamo una sintesi di alta qualità audio transferand texture su lunga scala temporale nel dominio del tempo che cattura elementi armonici, ritmici e timbrici relativi allo stile musicale, utilizzando esempi che possono avere diverse lunghezze e chiavi musicali. Dimostriamo la capacità di utilizzare reti neurali convoluzionali inizializzate in modo casuale per trasferire questi aspetti dello stile musicale da un pezzo a un altro utilizzando 3 rappresentazioni diverse dell'audio: la log-magnitudine della Short TimeFourier Transform (STFT), lo spettrogramma di Mel e lo spettrogramma Constant-Q Transform.Proponiamo di utilizzare queste rappresentazioni come un modo di generare e modificare le caratteristiche percettivamente significative del contenuto audio musicale. Dimostriamo i difetti e i vantaggi di ciascuna rappresentazione rispetto alle altre progettando attentamente strutture di reti neurali che completano la natura dell'audio musicale. Infine, mostriamo che gli esempi di trasferimento di "stile" più convincenti fanno uso di un insieme di rappresentazioni per aiutare a catturare le diverse caratteristiche desiderate dei segnali audio.
Per comunicare con nuovi partner in nuovi contesti, gli esseri umani formano rapidamente nuove convenzioni linguistiche. I recenti modelli linguistici addestrati con reti neurali profonde sono in grado di comprendere e produrre le convenzioni esistenti presenti nei loro dati di allenamento, ma non sono in grado di adattare in modo flessibile e interattivo tali convenzioni al volo come fanno gli umani. Introduciamo un compito di riferimento ripetuto come punto di riferimento per i modelli di adattamento nella comunicazione e proponiamo un quadro di apprendimento continuo regolarizzato che permette a un agente artificiale inizializzato con un modello linguistico generico di comprendere in modo più accurato ed efficiente il proprio partner nel corso del tempo.valutiamo questo quadro attraverso simulazioni su COCO ed esperimenti di gioco di riferimento in tempo reale con partner umani.
I modelli tradizionali di previsione degli insiemi possono lottare con insiemi di dati semplici a causa di un problema che chiamiamo il problema della responsabilità.Introduciamo un metodo di raggruppamento per insiemi di vettori di caratteristiche basato sull'ordinamento delle caratteristiche tra gli elementi dell'insieme.Questo può essere usato per costruire un auto-encoder permutazione-equivariante che evita questo problema di responsabilità. Su un dataset giocattolo di poligoni e su una versione set di MNIST, mostriamo che un tale auto-encoder produce ricostruzioni e rappresentazioni considerevolmente migliori.Sostituendo la funzione di pooling nei codificatori set esistenti con FSPool si migliora l'accuratezza e la velocità di convergenza su una varietà di dataset.
Presentiamo un metodo per l'apprendimento della politica per navigare in ambienti interni.Adottiamo un approccio gerarchico della politica, dove due agenti sono addestrati a lavorare in coesione l'uno con l'altro per eseguire un complesso compito di navigazione.Un agente Planner opera a un livello superiore e propone sotto-obiettivi per un agente Executor.The Executor riporta un riassunto di incorporamento al Planner come informazione laterale aggiuntiva alla fine della sua serie di operazioni per la prossima proposta sub-goal del Planner. L'obiettivo finale è generato dall'ambiente ed esposto al pianificatore che poi decide quale serie di sotto-obiettivi proporre all'esecutore.Mostriamo che questa configurazione pianificatore-esecutore aumenta drasticamente l'efficienza del campione del nostro metodo rispetto ai tradizionali approcci ad agente singolo, mitigando efficacemente la difficoltà che accompagna lunghe serie di azioni con un segnale di ricompensa sparso.Sul difficile ambiente Habitat che richiede la navigazione di vari ambienti interni realistici, dimostriamo che il nostro approccio offre un miglioramento significativo rispetto ai lavori precedenti per la navigazione.
I metodi di salienza mirano a spiegare le previsioni delle reti neurali profonde.Questi metodi mancano di affidabilità quando la spiegazione è sensibile a fattori che non contribuiscono alla previsione del modello.Usiamo un semplice e comune passo di pre-elaborazione ---aggiungendo uno spostamento medio ai dati di input--- per mostrare che una trasformazione senza effetto sul modello può causare numerosi metodi per attribuire in modo errato. Definiamo l'invarianza dell'input come il requisito che un metodo di salienza rispecchia la sensibilità del modello rispetto alle trasformazioni dell'input. Mostriamo, attraverso diversi esempi, che i metodi di salienza che non soddisfano una proprietà di invarianza dell'input sono inaffidabili e possono portare ad attribuzione fuorviante e imprecisa.
I grandi modelli Transformer raggiungono abitualmente risultati allo stato dell'arte su un certo numero di compiti, ma l'addestramento di questi modelli può essere proibitivamente costoso, specialmente su lunghe sequenze. Introduciamo due tecniche per migliorare l'efficienza dei Transformer. Per uno, sostituiamo l'attenzione dot-product con una che usa l'hashing sensibile alla località, cambiando la sua complessità da O(L^2) a O(L), dove L è la lunghezza della sequenza. Inoltre, usiamo strati residui reversibili invece dei residui standard, il che permette di memorizzare le attivazioni solo una volta nel processo di addestramento invece di N volte, dove N è il numero di strati. Il modello risultante, il Reformer, si comporta alla pari con i modelli Transformer pur essendo molto più efficiente in termini di memoria e molto più veloce su sequenze lunghe.
Ottenere le politiche che possono generalizzare ai nuovi ambienti nell'apprendimento di rinforzo è challenging.In questo lavoro, dimostriamo che la comprensione di lingua via un discente di politica della lettura è un veicolo promettente per la generalizzazione ai nuovi ambienti.We propone un problema fondato di apprendimento di politica, leggere per combattere i mostri (RTFM), in cui l'agente deve ragionare insieme sopra un obiettivo di lingua, dinamiche pertinenti descritte in un documento e le osservazioni dell'ambiente. Generiamo proceduralmente le dinamiche dell'ambiente e le corrispondenti descrizioni linguistiche delle dinamiche, in modo tale che gli agenti debbano leggere per capire le nuove dinamiche dell'ambiente invece di memorizzare qualsiasi informazione particolare.Inoltre, proponiamo txt2π, un modello che cattura le interazioni a tre vie tra l'obiettivo, il documento e le osservazioni. Su RTFM, txt2π si generalizza a nuovi ambienti con dinamiche non viste durante l'addestramento tramite la lettura.Inoltre, il nostro modello supera le baseline come FiLM e CNN condizionate dalla lingua su RTFM.Attraverso l'apprendimento curricolare, txt2π produce politiche che eccellono su compiti RTFM complessi che richiedono diversi passaggi di ragionamento e coreferenza.
Una questione aperta nella comunità del Deep Learning è perché le reti neurali addestrate con Gradient Descent generalizzano bene su dataset reali anche se sono in grado di adattare dati casuali.Proponiamo un approccio per rispondere a questa domanda basato su un'ipotesi sulla dinamica della discesa del gradiente che chiamiamo Coherent Gradients: I gradienti di esempi simili sono simili e quindi il gradiente complessivo è più forte in certe direzioni in cui questi si rinforzano a vicenda.Quindi le modifiche ai parametri di rete durante l'addestramento sono orientate verso quelle che (localmente) beneficiano simultaneamente di molti esempi quando esiste tale somiglianza.Sosteniamo questa ipotesi con argomenti euristici ed esperimenti perturbativi e delineiamo come questo possa spiegare diverse osservazioni empiriche comuni sul Deep Learning.Inoltre, la nostra analisi non è solo descrittiva, ma prescrittiva.Essa suggerisce una modifica naturale al gradient descent che può ridurre notevolmente l'overfitting.
 I recenti progressi nell'apprendimento profondo hanno mostrato risultati promettenti in molti compiti di visione di basso livello, tuttavia, risolvere la sintesi della vista basata su una singola immagine è ancora un problema aperto. In particolare, la generazione di nuove immagini a viste di telecamere parallele data una singola immagine di input è di grande interesse, in quanto consente la visualizzazione 3D dello scenario di input 2D. Proponiamo una nuova architettura di rete per eseguire la sintesi della vista stereoscopica in posizioni arbitrarie della telecamera lungo l'asse X, o Deep 3D Pan, con kernel adattivi a forma di "t" dotati di dilatazioni adattive globali e locali. La nostra architettura di rete proposta, la monster-net, è concepita con un nuovo kernel adattivo a forma di "t" con dilatazione adattativa globale e locale, che può incorporare in modo efficiente lo spostamento globale della telecamera e gestire le geometrie 3D locali dei pixel dell'immagine di destinazione per la sintesi di viste panoramiche 3D dall'aspetto naturale quando viene data un'immagine di input 2-D. Per dimostrare l'efficacia del nostro metodo, sono stati eseguiti esperimenti su KITTI, CityScapes e sul nostro set di dati VXXLXX_STEREO in interni, e il nostro monster-net supera significativamente il metodo allo stato dell'arte, SOTA, con un ampio margine in tutte le metriche di RMSE, PSNR e SSIM. Il nostro monster-net proposto è in grado di ricostruire strutture d'immagine più affidabili in immagini sintetizzate con geometria coerente; inoltre, le informazioni di disparità che possono essere estratte dal kernel a forma di "t" sono molto più affidabili di quelle del SOTA per il compito di stima della profondità monoculare non supervisionata, confermando l'efficacia del nostro metodo.
Sfortunatamente, la memoria della GPU come risorsa hardware è sempre limitata, il che limita la risoluzione dell'immagine, la dimensione del batch e il tasso di apprendimento che potrebbero essere utilizzati per migliorare le prestazioni della DNN. In questo articolo, proponiamo un nuovo approccio di formazione, chiamato Re-forwarding, che riduce sostanzialmente l'utilizzo della memoria nella formazione. Il costo totale della memoria diventa la somma di (1) il costo della memoria nel sottoinsieme di vertici e (2) il costo massimo della memoria tra i ri-avanzamenti locali. Il Re-forwarding scambia il tempo di formazione con la memoria e non compromette le prestazioni nei test.Proponiamo teorie e algoritmi che raggiungono le soluzioni di memoria ottimali per DNN con grafici di calcolo lineari o arbitrari.Gli esperimenti mostrano che il Re-forwarding riduce fino all'80% della memoria di formazione su DNN popolari come Alexnet, VGG, ResNet, Densenet e Inception net.
La compressione è un passo fondamentale per distribuire grandi reti neurali su piattaforme con risorse limitate.Come una tecnica di compressione popolare, la quantizzazione vincola il numero di valori di peso distinti e quindi riduce il numero di bit necessari per rappresentare e memorizzare ogni peso.In questo articolo, studiamo il potere di rappresentazione delle reti neurali quantizzate. In primo luogo, dimostriamo l'approssimabilità universale delle reti ReLU quantizzate su un'ampia classe di funzioni, quindi forniamo dei limiti superiori sul numero di pesi e sulla dimensione della memoria per un dato limite di errore di approssimazione e la larghezza di bit dei pesi per strutture indipendenti dalla funzione e dipendenti dalla funzione. I nostri risultati rivelano che, per raggiungere un limite di errore di approssimazione di $ $epsilon$, il numero di pesi necessari per una rete quantizzata non è più di $mathcal{O}\sinistra(\log^5(1/epsilon)\destra)$ volte quello di una rete non quantizzata. Questo overhead è di ordine molto più basso del limite inferiore del numero di pesi necessari per il limite di errore, sostenendo il successo empirico di varie tecniche di quantizzazione. Per quanto ne sappiamo, questo è il primo studio approfondito sui limiti di complessità delle reti neurali quantizzate.
L'apprendimento per rinforzo (RL) con metodi basati sul valore (per esempio, Q-learning) ha mostrato successo in una varietà di domini come i giochi e i sistemi di raccomandazione (RS). Quando lo spazio di azione è finito, questi algoritmi trovano implicitamente una politica imparando la funzione di valore ottimale, che sono spesso molto efficienti. Tuttavia, una delle maggiori sfide nell'estendere l'apprendimento di Q per affrontare problemi di RL ad azione continua è che ottenere il backup ottimale di Bellman richiede la risoluzione di un problema di massimizzazione dell'azione continua (max-Q).Mentre è comune limitare la parametrizzazione della funzione Q per essere concava nelle azioni per semplificare il problema max-Q, una tale restrizione potrebbe portare ad una degradazione delle prestazioni.In alternativa, quando la funzione Q è parametrizzata con una generica rete neurale feed-forward (NN), il problema max-Q può essere NP-hard. In questo lavoro, proponiamo il metodo CAQL che minimizza il residuo di Bellman usando Q-learning con uno dei diversi ottimizzatori di azione plug-and-play.In particolare, sfruttando i progressi delle teorie di ottimizzazione in NN profonda, mostriamo che il problema max-Q può essere risolto in modo ottimale con la programmazione mista-integrale (MIP) - quando la funzione Q ha sufficiente potere di rappresentazione, questa ottimizzazione basata su MIP induce politiche migliori ed è più robusta delle controparti, per esempio, Per accelerare l'addestramento di CAQL, sviluppiamo tre tecniche, vale a dire (i) tolleranza dinamica, (ii) filtraggio duale, e (iii) clustering.Per accelerare l'inferenza di CAQL, introduciamo la funzione di azione che apprende simultaneamente la politica ottimale.Per dimostrare l'efficienza di CAQL lo confrontiamo con lo stato dell'arte degli algoritmi RL su problemi di controllo continuo di riferimento che hanno diversi gradi di vincoli di azione e dimostriamo che CAQL supera significativamente i metodi basati sulla politica in ambienti fortemente vincolati.
Generative Adversarial Networks (GANs) hanno dimostrato di essere una struttura potente per imparare a disegnare campioni da distribuzioni complesse.Tuttavia, i GANs sono anche notoriamente difficili da addestrare, con collasso di modalità e oscillazioni un problema comune.Noi ipotizziamo che questo sia almeno in parte dovuto all'evoluzione della distribuzione del generatore e alla tendenza alla dimenticanza catastrofica delle reti neurali, che porta il discriminatore a perdere la capacità di ricordare campioni sintetizzati da istanziazioni precedenti del generatore. Riconoscendo questo, i nostri contributi sono duplici: in primo luogo, mostriamo che l'addestramento GAN costituisce un punto di riferimento più interessante e realistico per la valutazione dei metodi di apprendimento continuo rispetto ad alcuni dei set di dati più canonici; in secondo luogo, proponiamo di sfruttare le tecniche di apprendimento continuo per aumentare il discriminatore, preservando la sua capacità di riconoscere i campioni precedenti del generatore; dimostriamo che i metodi risultanti aggiungono solo una piccola quantità di calcoli, comportano modifiche minime al modello e producono prestazioni generali migliori sulle attività di generazione di immagini e testo esaminate.
Molti problemi con dati di formazione etichettati su larga scala sono stati risolti in modo impressionante dal deep learning.Tuttavia, Unseen Class Categorization (UCC) con informazioni minime fornite sulle classi di destinazione è l'impostazione più comunemente incontrata nell'industria, che rimane un problema di ricerca impegnativo nel machine learning.Approcci precedenti a UCC o non riescono a generare un potente estrattore di caratteristiche discriminative o non riescono a imparare un classificatore flessibile che può essere facilmente adattato alle classi non viste.In questo articolo, proponiamo di affrontare questi problemi attraverso la riparametrizzazione della rete, \textit{i. e.}, riparametrizzando i pesi apprendibili di una rete in funzione di altre variabili, con cui disaccoppiamo la parte di estrazione delle caratteristiche e la parte di classificazione di un modello di classificazione profonda per soddisfare l'impostazione speciale di UCC, garantendo sia una forte discriminabilità che un'eccellente adattabilità.
Le proteine sono molecole onnipresenti la cui funzione nei processi biologici è determinata dalla loro struttura 3D. L'identificazione sperimentale della struttura di una proteina può richiedere molto tempo, essere proibitiva e non sempre possibile. In alternativa, il ripiegamento delle proteine può essere modellato utilizzando metodi computazionali, che tuttavia non sono garantiti per produrre sempre risultati ottimali.GraphQA è un metodo basato sui grafi per stimare la qualità dei modelli proteici, che possiede proprietà favorevoli come l'apprendimento della rappresentazione, la modellazione esplicita della struttura sia sequenziale che 3D, l'invarianza geometrica e l'efficienza computazionale. In questo lavoro, dimostriamo miglioramenti significativi dello stato dell'arte per entrambi gli approcci di apprendimento manuale e di rappresentazione, oltre a valutare attentamente i singoli contributi di GraphQA.
Studiamo il problema dell'addestramento di modelli di apprendimento automatico in modo incrementale usando l'apprendimento attivo con accesso a oracoli imperfetti o rumorosi. Consideriamo specificamente l'impostazione dell'apprendimento attivo in batch, in cui vengono selezionati più campioni invece di un singolo campione come nelle impostazioni classiche, in modo da ridurre il sovraccarico di formazione. Il nostro approccio è un ponte tra la casualità uniforme e il campionamento d'importanza basato sul punteggio dei cluster quando si seleziona un lotto di nuovi campioni. Gli esperimenti sui set di dati di classificazione delle immagini di benchmark (MNIST, SVHN e CIFAR10) mostrano un miglioramento rispetto alle strategie di apprendimento attivo esistenti, introducendo uno strato extra di denoising alle reti profonde per rendere l'apprendimento attivo robusto ai rumori delle etichette e mostrando miglioramenti significativi.
Il trasferimento di stile artistico è il problema di sintetizzare un'immagine con un contenuto simile a una data immagine e uno stile simile a un'altra.Anche se le recenti reti neurali feed-forward possono generare immagini stilizzate in tempo reale, questi modelli producono una singola stilizzazione data una coppia di immagini stile/contenuto, e l'utente non ha il controllo sull'output sintetizzato. Inoltre, il trasferimento dello stile dipende dagli iper-parametri del modello che variano per le diverse immagini di input, quindi, se l'output stilizzato non è attraente per l'utente, deve provare più modelli o riaddestrarne uno con diversi iper-parametri per ottenere una stilizzazione preferita. In questo articolo, affrontiamo questi problemi proponendo un nuovo metodo che permette la regolazione degli iper-parametri cruciali, dopo l'addestramento e in tempo reale, attraverso una serie di parametri regolabili manualmente che permettono all'utente di modificare gli output sintetizzati dalla stessa coppia di immagini stile/contenuto, alla ricerca dell'immagine stilizzata preferita. I nostri esperimenti quantitativi e qualitativi indicano come la regolazione di questi parametri sia paragonabile al riaddestramento del modello con iper-parametri diversi e dimostrano come questi parametri possano essere randomizzati per generare risultati diversi ma ancora molto simili in stile e contenuto.
Tuttavia, questo pone ai progettisti dell'ambiente l'onere di progettare funzioni di ricompensa condizionate dalla lingua che potrebbero non essere facilmente o comodamente implementate man mano che la complessità dell'ambiente e della lingua aumenta.  Man mano che i modelli di ricompensa migliorano, imparano a ricompensare accuratamente gli agenti per il completamento dei compiti per le configurazioni dell'ambiente - e per le istruzioni - non presenti tra i dati degli esperti.Questa struttura separa efficacemente la rappresentazione di ciò che le istruzioni richiedono da come possono essere eseguite.In un semplice mondo a griglia, permette a un agente di imparare una serie di comandi che richiedono l'interazione con i blocchi e la comprensione delle relazioni spaziali e delle disposizioni astratte non specificate.Mostriamo inoltre che il metodo permette al nostro agente di adattarsi ai cambiamenti dell'ambiente senza richiedere nuovi esempi esperti.
Presentiamo Multitask Soft Option Learning (MSOL), una struttura gerarchica multi-task basata su Planning-as-Inference.MSOL estende il concetto di Opzioni, utilizzando posteriors variazionali separati per ogni compito, regolarizzati da un priore condiviso.Le soft-options apprese sono temporalmente estese, permettendo ad una politica master di livello superiore di allenarsi più velocemente su nuovi compiti prendendo decisioni con frequenza inferiore. Inoltre, MSOL permette un fine-tuning delle soft-opzioni per i nuovi compiti senza disimparare il comportamento precedentemente utile, ed evita i problemi con i minimi locali nell'addestramento multitask.dimostriamo empiricamente che MSOL supera significativamente sia le linee di base gerarchiche che quelle piatte di transfer-learning in ambienti multi-task impegnativi.
Proponiamo un algoritmo, guided variational autoencoder (Guided-VAE), che è in grado di imparare un modello generativo controllabile eseguendo la rappresentazione latente disentanglement learning.The obiettivo di apprendimento è raggiunto fornendo un segnale alla codifica latente/embedding in VAE senza cambiare la sua architettura principale, quindi mantenendo le proprietà desiderabili del VAE.We design unsupervised e una strategia supervisionata in Guided-VAE e osservare una migliore modellazione e capacità di controllo rispetto al VAE vanilla. Nella strategia non supervisionata, guidiamo l'apprendimento del VAE introducendo un decodificatore leggero che impara la trasformazione geometrica latente e le componenti principali; nella strategia supervisionata, usiamo un meccanismo di eccitazione e inibizione avversaria per incoraggiare il disentanglement delle variabili latenti.Guided-VAE gode della sua trasparenza e semplicità per il compito generale di apprendimento della rappresentazione, così come l'apprendimento del disentanglement.Su una serie di esperimenti per l'apprendimento della rappresentazione, sono stati osservati una migliore sintesi/campionamento, un migliore disentanglement per la classificazione, e ridotti errori di classificazione nel meta apprendimento.
I modelli linguistici neurali (NLM) sono generativi, e modellano la distribuzione delle frasi grammaticali.Addestrati su enormi corpus, gli NLM stanno spingendo il limite della precisione di modellazione.Inoltre, sono stati anche applicati a compiti di apprendimento supervisionato che decodificano il testo, ad es, Rivalutando la lista n-migliore, NLM può selezionare il candidato grammaticalmente più corretto tra la lista, e ridurre significativamente il tasso di errore parola/carattere.Tuttavia, la natura generativa di NLM potrebbe non garantire una discriminazione tra frasi "buone" e "cattive" (in un senso specifico del compito), con conseguente performance subottimale. Questo lavoro propone un approccio per adattare un NLM generativo a uno discriminativo; a differenza dell'obiettivo di massima verosimiglianza comunemente usato, il metodo proposto mira ad ampliare il margine tra le frasi "buone" e "cattive"; è addestrato end-to-end e può essere ampiamente applicato a compiti che comportano il re-scoring del testo decodificato; guadagni significativi sono osservati sia in compiti di ASR che di traduzione automatica statistica (SMT).
Convenzionalmente, le reti neurali convoluzionali (CNN) elaborano immagini diverse con lo stesso set di filtri.Tuttavia, le variazioni nelle immagini pongono una sfida a questa moda.In questo documento, proponiamo di generare filtri specifici per gli strati convoluzionali nel passaggio in avanti.Poiché i filtri sono generati on-the-fly, il modello diventa più flessibile e può adattarsi meglio ai dati di formazione rispetto alle CNN tradizionali.Per ottenere caratteristiche specifiche del campione, estraiamo le mappe di caratteristiche intermedie da un autoencoder. Poiché i filtri sono di solito molto dimensionali, proponiamo di imparare un insieme di coefficienti invece di un insieme di filtri.Questi coefficienti sono usati per combinare linearmente i filtri di base da un archivio di filtri per generare i filtri finali per una CNN.Il metodo proposto è valutato su MNIST, MTFL e CIFAR10 datasets.I risultati degli esperimenti dimostrano che l'accuratezza di classificazione del modello di base può essere migliorata utilizzando il metodo proposto di generazione dei filtri.
Proponiamo una nuova rete neurale anytime che permette la valutazione parziale da sottoreti con diverse larghezze e profondità.Rispetto alle reti anytime convenzionali solo con la controllabilità della profondità, la maggiore diversità architettonica porta a un maggiore utilizzo delle risorse e al conseguente miglioramento delle prestazioni sotto vari e dinamici budget di risorse.Evidenziamo le caratteristiche architettoniche per rendere il nostro schema fattibile oltre che efficiente, e mostriamo la sua efficacia nei compiti di classificazione delle immagini.
Questo compito generativo può essere inquadrato come un problema sequenza-sequenza utilizzando le rappresentazioni SMILES delle molecole. costruendo sulla cima della popolare architettura Transformer, proponiamo due nuovi metodi di pre-formazione che costruiscono compiti ausiliari rilevanti (reazioni plausibili) per il nostro problema. Inoltre, incorporiamo un modello discreto di variabile latente nell'architettura per incoraggiare il modello a produrre una serie diversificata di previsioni alternative. Sul sottoinsieme di 50k di esempi di reazione dalla letteratura brevettuale degli Stati Uniti (USPTO-50k), il nostro modello migliora notevolmente le prestazioni rispetto alla linea di base, generando anche previsioni che sono più diverse.
La libreria Disentanglement-PyTorch è stata sviluppata per facilitare la ricerca, l'implementazione e il test di nuovi algoritmi variazionali. In questa libreria modulare, le architetture neurali, la dimensionalità dello spazio latente e gli algoritmi di allenamento sono completamente disaccoppiati, consentendo esperimenti indipendenti e coerenti tra metodi variazionali. La libreria, finora, include implementazioni dei seguenti algoritmi non supervisionati VAE, Beta-VAE, Factor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE, e Beta-TCVAE, così come approcci condizionali come CVAE e IFCVAE. La libreria è compatibile con la Disentanglement Challenge di NeurIPS 2019, ospitata su AICrowd ed è stata utilizzata per competere nella prima e nella seconda fase della sfida, dove si è classificata tra i migliori pochi partecipanti.
I metodi di regione di fiducia, come TRPO, sono spesso utilizzati per stabilizzare gli algoritmi di ottimizzazione della politica nell'apprendimento per rinforzo (RL).Mentre le attuali strategie di regione di fiducia sono efficaci per il controllo continuo, richiedono tipicamente una grande quantità di interazione on-policy con l'ambiente.Per affrontare questo problema, proponiamo un metodo di regione di fiducia off-policy, Trust-PCL, che sfrutta un'osservazione che la politica ottimale e i valori di stato di un obiettivo di massima ricompensa con un regolatore di entropia relativa soddisfano un insieme di consistenze pathwise multi-step lungo qualsiasi percorso. L'introduzione della regolarizzazione dell'entropia relativa permette a Trust-PCL di mantenere la stabilità dell'ottimizzazione mentre sfrutta i dati off-policy per migliorare l'efficienza del campione. Quando viene valutato su una serie di compiti di controllo continuo, Trust-PCL migliora significativamente la qualità della soluzione e l'efficienza del campione di TRPO.
L'apprendimento profondo di rinforzo ha raggiunto molti successi recenti, ma la nostra comprensione dei suoi punti di forza e dei suoi limiti è ostacolata dalla mancanza di ambienti ricchi in cui possiamo caratterizzare completamente il comportamento ottimale e, di conseguenza, diagnosticare le singole azioni rispetto a tale caratterizzazione. Qui consideriamo una famiglia di giochi combinatori, derivanti dal lavoro di Erdos, Selfridge e Spencer, e proponiamo il loro uso come ambienti per valutare e confrontare diversi approcci all'apprendimento di rinforzo. Questi giochi hanno una serie di caratteristiche interessanti: sono impegnativi per gli attuali approcci di apprendimento, ma formano (i) un ambiente a bassa dimensione, semplicemente parametrizzato dove (ii) c'è una soluzione lineare in forma chiusa per il comportamento ottimale da qualsiasi stato, e (iii) la difficoltà del gioco può essere regolata cambiando i parametri ambientali in un modo interpretabile. Usiamo questi giochi Erdos-Selfridge-Spencer non solo per confrontare diversi algoritmi, ma anche per confrontare approcci basati sull'apprendimento supervisionato e di rinforzo, per analizzare il potere degli approcci multi-agente nel migliorare le prestazioni, e per valutare la generalizzazione ad ambienti esterni al set di allenamento.
Diverse metodologie per stimare l'incertezza del modello sono state proposte, ma queste metodologie vincolano il modo in cui la rete neurale è addestrata o costruita. Presentiamo Outlier Detection In Neural networks (ODIN), un metodo senza presupposti per rilevare le osservazioni outlier durante la previsione, basato su principi ampiamente utilizzati nel monitoraggio dei processi produttivi. Usando un'approssimazione lineare del manifold dello strato nascosto, aggiungiamo il rilevamento degli outlier in tempo di predizione ai modelli dopo l'addestramento senza alterare l'architettura o l'addestramento. Dimostriamo che ODIN rileva efficacemente gli outlier durante la predizione su Fashion-MNIST, ImageNet-synsets e riconoscimento dei comandi vocali.
Questo lavoro introduce una semplice rete per la produzione di embeddings di parole consapevoli dei caratteri, che vengono combinati per produrre un vettore di embedding per ogni parola. Le rappresentazioni di parole apprese si dimostrano molto rade e facilitano il miglioramento dei risultati nei compiti di modellazione del linguaggio, nonostante l'uso di un numero nettamente inferiore di parametri, e senza la necessità di applicare il dropout.Un esperimento finale suggerisce che la condivisione del peso contribuisce alla sparsità, aumenta le prestazioni e previene l'overfitting.
Le reti neurali con pesi e attivazioni a bassa precisione offrono notevoli vantaggi in termini di efficienza rispetto ai loro equivalenti a piena precisione. I due vantaggi più frequentemente discussi della quantizzazione sono un ridotto consumo di memoria e un passaggio in avanti più veloce se implementato con efficienti operazioni bitwise. Ci concentriamo sul caso a bassissima precisione in cui i pesi e le attivazioni sono entrambi quantizzati a $\pm$1, e notiamo che quantificare stocasticamente i pesi in un solo strato può ridurre nettamente l'impatto degli attacchi iterativi. Osserviamo che le reti neurali binarie non scalate mostrano un effetto simile alla procedura originale di distillazione difensiva che ha portato al mascheramento dei gradienti, e una falsa nozione di sicurezza, che affrontiamo conducendo sia esperimenti black-box che white-box con modelli binari che non mascherano artificialmente i gradienti.
Unsupervised bilingual dictionary induction (UBDI) è utile per la traduzione automatica non supervisionata e per il trasferimento interlinguistico di modelli in lingue a bassa risorsa.Un approccio a UBDI è quello di allineare gli spazi vettoriali delle parole in diverse lingue utilizzando reti generative avversarie (GAN) con generatori lineari, raggiungendo prestazioni all'avanguardia per diverse coppie di lingue.Per alcune coppie, tuttavia, l'induzione basata su GAN è instabile o non riesce completamente ad allineare gli spazi vettoriali. Mostriamo che l'instabilità dipende dalla forma e dalla densità degli insiemi vettoriali, ma non dal rumore; è il risultato di ottimizzazioni locali, ma né l'iperparametrizzazione né la modifica della dimensione del batch o del tasso di apprendimento riducono costantemente l'instabilità; tuttavia, possiamo stabilizzare GAN-based UBDI attraverso la selezione del modello best-of-N, basata su un criterio di arresto non supervisionato.
A causa dell'ubiquità del software, il rilevamento delle vulnerabilità del software (SVD) è diventato un problema importante nell'industria del software e nel campo della sicurezza informatica.Uno dei problemi più cruciali in SVD è far fronte alla scarsità di vulnerabilità etichettate nei progetti che richiedono la laboriosa etichettatura manuale del codice da parte degli esperti di sicurezza del software. Un possibile modo per affrontare il problema è quello di impiegare l'adattamento al dominio profondo, che recentemente ha visto un enorme successo nel trasferire l'apprendimento da fonti di dati con etichetta strutturale a fonti di dati senza etichetta.L'idea generale è quella di mappare entrambi i dati di origine e di destinazione in uno spazio delle caratteristiche congiunte e chiudere il divario di discrepanza di quei dati in questo spazio delle caratteristiche congiunte. Generative adversarial network (GAN) è una tecnica che tenta di colmare il divario di discrepanza ed emerge anche come un blocco di costruzione per sviluppare approcci di adattamento del dominio profondo con prestazioni allo stato dell'arte.Tuttavia, gli approcci di adattamento del dominio profondo che utilizzano il principio GAN per chiudere il divario di discrepanza sono soggetti al problema del collasso della modalità che influisce negativamente sulle prestazioni predittive. Il nostro obiettivo in questo articolo è quello di proporre una rete di adattamento del dominio del codice profondo con doppio generatore e discriminatore (Dual-GD-DDAN) per affrontare il problema dell'apprendimento di trasferimento da progetti software etichettati a progetti software non etichettati nel contesto di SVD, al fine di risolvere il problema del collasso della modalità affrontato negli approcci precedenti.I risultati sperimentali su progetti software del mondo reale mostrano che il nostro metodo proposto supera le prestazioni di base dello stato dell'arte con un ampio margine.
L'apprendimento delle rappresentazioni è uno dei fondamenti del Deep Learning e ha permesso importanti miglioramenti in diversi compiti di Machine Learning, come la traduzione automatica neurale, la risposta alle domande e il riconoscimento vocale.Lavori recenti hanno proposto nuovi metodi per l'apprendimento delle rappresentazioni per i nodi e i bordi nei grafi.Molti di questi metodi sono basati sull'algoritmo SkipGram, e di solito elaborano un gran numero di vicini multi-hop per produrre il contesto dal quale vengono apprese le rappresentazioni dei nodi. In questo articolo, proponiamo un metodo efficace ed efficiente per la generazione di embeddings dei nodi nei grafi che impiega un numero ristretto di permutazioni sul vicinato immediato di un nodo come contesto per generare la sua rappresentazione, quindi rappresentazioni ego-centriche. Presentiamo una valutazione approfondita che mostra che il nostro metodo supera i metodi allo stato dell'arte in sei diversi set di dati relativi ai problemi di previsione dei collegamenti e classificazione dei nodi, essendo da uno a tre ordini di grandezza più veloce delle linee base quando si generano embeddings dei nodi per grafi molto grandi.
Le reti neurali ricorrenti ortogonali affrontano il problema del gradiente evanescente parametrizzando le connessioni ricorrenti utilizzando una matrice ortogonale. Questa classe di modelli è particolarmente efficace per risolvere compiti che richiedono la memorizzazione di lunghe sequenze. Mostriamo come un'architettura ricorrente recentemente proposta, la Rete di Memoria Lineare, composta da uno strato feedforward non lineare e da una ricorrenza lineare separata, può essere utilizzata per risolvere compiti di memorizzazione difficili.Proponiamo uno schema di inizializzazione che imposta i pesi di un'architettura ricorrente per approssimare un autoencoder lineare delle sequenze di input, che può essere trovato con una soluzione in forma chiusa.Lo schema di inizializzazione può essere facilmente adattato a qualsiasi architettura ricorrente.    Sosteniamo che questo approccio è superiore a un'inizializzazione ortogonale casuale grazie all'autocodificatore, che permette la memorizzazione di lunghe sequenze anche prima dell'addestramento.L'analisi empirica mostra che il nostro approccio raggiunge risultati competitivi contro modelli ortogonali alternativi, e il LSTM, su MNIST sequenziale, MNIST permutato e TIMIT.
Questo articolo migliora la linea di ricerca che formula il riconoscimento delle entità nominate (NER) come un problema di etichettatura delle sequenze. Usiamo i cosiddetti codificatori black-box con memoria a breve termine lunga (LSTM) per ottenere risultati all'avanguardia, fornendo al contempo una comprensione approfondita di ciò che il modello auto-regressivo impara con un meccanismo di auto-attenzione parallelo.In particolare, disaccoppiamo il problema di etichettatura delle sequenze di NER in chunking delle entità, ad es, Barack_B Obama_E was_O elected_O, e digitazione di entità, ad esempio, Barack_PERSON Obama_PERSON was_NONE elected_NONE, e analizziamo come il modello impara a, o ha difficoltà a, catturare i modelli di testo per ciascuno dei subtasks.The insights we gain then lead us to explore a more sophisticated deep cross-Bi-LSTM encoder, which proves better at capturing global interactions given both empirical results and a theoretical justification.
I metodi esistenti per l'apprendimento di KGE possono essere visti come un processo a due fasi in cui (a) le entità e le relazioni nel grafo della conoscenza sono rappresentate utilizzando alcune strutture algebriche lineari (embeddings), e (b) viene definita una funzione di punteggio che valuta la forza di una relazione che esiste tra due entità utilizzando la relazione corrispondente e le embeddings delle entità. Sfortunatamente, le precedenti proposte per le funzioni di punteggio nel primo passo sono state motivate euristicamente, e non è chiaro come le funzioni di punteggio nei KGE si relazionino al processo di generazione del sottostante grafo di conoscenza.Per affrontare questo problema, proponiamo un account generativo del compito di apprendimento dei KGE.In particolare, dato un grafo di conoscenza rappresentato da un insieme di triple relazionali (h, R, t), dove la relazione semantica R tiene tra le due entità h (testa) e t (coda), estendiamo il modello random walk (Arora et al, Deriviamo una relazione teorica tra la probabilità congiunta p(h, R, t) e le embeddings di h, R e t. Inoltre, mostriamo che la minimizzazione della perdita marginale, un obiettivo popolare usato da molti lavori precedenti in KGE, segue naturalmente dalla massimizzazione del rapporto di log-likelihood sotto le probabilità stimate dai KGE secondo la nostra relazione teorica. Proponiamo un obiettivo di apprendimento motivato dall'analisi teorica per imparare i KGE da un dato grafo di conoscenza. I KGE appresi dal nostro metodo proposto ottengono prestazioni allo stato dell'arte sui set di dati di riferimento FB15K237 e WN18RR, fornendo prove empiriche a sostegno della teoria.
Attualmente le uniche tecniche per condividere la governance di un modello di apprendimento profondo sono la crittografia omomorfa e la computazione sicura multiparty.Purtroppo, nessuna di queste tecniche è applicabile all'addestramento di grandi reti neurali a causa dei loro grandi sovraccarichi computazionali e di comunicazione.Come tecnica scalabile per la governance condivisa del modello, proponiamo la divisione del modello di apprendimento profondo tra più parti.Questo articolo indaga empiricamente la garanzia di sicurezza di questa tecnica, che viene introdotta come il problema del completamento del modello:  Dato l'intero set di dati di allenamento o un simulatore di ambiente, e un sottoinsieme dei parametri di un modello di deep learning addestrato, quanto allenamento è necessario per recuperare le prestazioni originali del modello?  Definiamo una metrica per valutare la durezza del problema del completamento del modello e la studiamo empiricamente sia nell'apprendimento supervisionato su ImageNet che nell'apprendimento per rinforzo su Atari e DeepMind Lab. I nostri esperimenti mostrano che (1) il problema del completamento del modello è più difficile nell'apprendimento per rinforzo che nell'apprendimento supervisionato a causa della non disponibilità delle traiettorie dell'agente addestrato, e (2) la sua durezza non dipende principalmente dal numero di parametri della parte mancante, ma piuttosto dal loro tipo e posizione.  I nostri risultati suggeriscono che la divisione del modello potrebbe essere una tecnica fattibile per la governance condivisa del modello in alcune impostazioni in cui l'addestramento è molto costoso.
Questo articolo propone l'adattamento variazionale del dominio, un framework unificato, scalabile e semplice per l'apprendimento di più distribuzioni attraverso l'inferenza variazionale.A differenza dei metodi esistenti sul trasferimento del dominio attraverso modelli generativi profondi, come StarGAN (Choi et al., 2017) e UFDN (Liu et al, 2018), l'adattamento variazionale del dominio ha tre vantaggi.In primo luogo, i campioni del target non sono richiesti.Invece, il quadro richiede una fonte nota come priore $p(x)$ e discriminatori binari, $p(\mathcal{D}_i|x)$, che discriminano il dominio target $\mathcal{D}_i$ dagli altri. Di conseguenza, la struttura considera un obiettivo come un posteriore che può essere formulato esplicitamente attraverso l'inferenza bayesiana, $p(x|\mathcal{D}_i) \propto p(\mathcal{D}_i|x)p(x)$, come esposto da un ulteriore modello proposto di dual variational autoencoder (DualVAE). Così come VAE codifica un campione $x$ come una modalità su uno spazio latente: $\mu(x) \in \mathcal{Z}$, DualVAE codifica un dominio $\mathcal{D}_i$ come una modalità sul doppio spazio latente $\mu^*(\mathcal{D}_i) \in \mathcal{Z}^$, chiamato domain embedding. Riformula il posteriore con un paring naturale $\langolo, \rangolo: \mathcal{Z} \volte \mathcal{Z}^* \freccia destra \reale$, che può essere espanso a domini infiniti non numerabili come i domini continui così come l'interpolazione.In terzo luogo, DualVAE converge velocemente senza una sofisticata ricerca automatica/manuale degli iperparametri in confronto ai GAN, poiché richiede solo un parametro aggiuntivo a VAE. Attraverso l'esperimento numerico, dimostriamo i tre benefici con il compito di generazione di immagini multi-dominio su CelebA con fino a 60 domini, e mostra che DualVAE registra la performance all'avanguardia superando StarGAN e UFDN.
L'idea chiave è quella di modellare l'addestramento come una procedura che include sia il verificatore che l'avversario: in ogni iterazione, il verificatore mira a certificare la rete utilizzando il rilassamento convesso, mentre l'avversario cerca di trovare input all'interno del rilassamento convesso che facciano fallire la verifica. Mostriamo sperimentalmente che questo metodo di formazione è promettente e raggiunge il meglio dei due mondi - produce un modello con una precisione allo stato dell'arte (74,8%) e una robustezza certificata (55,9%) sul difficile set di dati CIFAR-10 con una perturbazione di 2/255 L-infinito.Questo è un miglioramento significativo rispetto ai migliori risultati attualmente noti di 68,3% di precisione e 53,9% di robustezza certificata, ottenuti utilizzando una rete 5 volte più grande del nostro lavoro.
I compiti di apprendimento su codice sorgente (cioè linguaggi formali) sono stati presi in considerazione di recente, ma la maggior parte dei lavori ha cercato di trasferire i metodi del linguaggio naturale e non ha sfruttato le opportunità uniche offerte dalla sintassi nota del codice, per esempio, le dipendenze a lungo raggio indotte dall'uso della stessa variabile o funzione in posizioni distanti non sono spesso considerate. Noi proponiamo di usare i grafi per rappresentare sia la struttura sintattica che semantica del codice e di usare metodi di deep learning basati sui grafi per imparare a ragionare sulle strutture dei programmi.In questo lavoro, presentiamo come costruire i grafi dal codice sorgente e come scalare l'allenamento delle reti neurali a grafi così grandi.Valutiamo il nostro metodo su due compiti: VarNaming, in cui una rete tenta di predire il nome di una variabile dato il suo utilizzo, e VarMisuse, in cui la rete impara a ragionare sulla selezione della variabile corretta che dovrebbe essere usata in una data posizione del programma.Il nostro confronto con i metodi che usano rappresentazioni di programma meno strutturate mostra i vantaggi della modellazione della struttura nota, e suggerisce che i nostri modelli imparano a dedurre nomi significativi e a risolvere il compito VarMisuse in molti casi.Inoltre, i nostri test hanno mostrato che VarMisuse identifica un certo numero di bug in progetti open-source maturi.
L'overfitting è un problema onnipresente nell'addestramento delle reti neurali e di solito viene mitigato utilizzando un set di dati holdout.Qui sfidiamo questa logica e studiamo i criteri per l'overfitting senza utilizzare un set di dati holdout.In particolare, addestriamo un modello per un numero fisso di epoche più volte con frazioni variabili di etichette randomizzate e per una gamma di forze di regolarizzazione. Un modello correttamente addestrato non dovrebbe essere in grado di raggiungere un'accuratezza maggiore della frazione di punti dati correttamente etichettati, altrimenti il modello si adatta troppo. Introduciamo due criteri per rilevare l'overfitting e uno per rilevare l'underfitting.Analizziamo l'arresto anticipato, il fattore di regolarizzazione e la profondità della rete.Nelle applicazioni critiche per la sicurezza siamo interessati ai modelli e alle impostazioni dei parametri che funzionano bene e non sono suscettibili di overfittare.I metodi di questo articolo permettono di caratterizzare e identificare tali modelli.
I processi decisionali di Markov parzialmente osservabili (POMDPs) sono una struttura ampiamente utilizzata per modellare il processo decisionale con incertezza sull'ambiente e sotto esito stocastico.Nei modelli POMDP convenzionali, le osservazioni che l'agente riceve provengono da una distribuzione nota fissa.Tuttavia, in una varietà di scenari del mondo reale l'agente ha un ruolo attivo nella sua percezione selezionando quali osservazioni ricevere. A causa della natura combinatoria di tale processo di selezione, è computazionalmente intrattabile integrare la decisione di percezione con la decisione di pianificazione.Per prevenire tale espansione dello spazio di azione, proponiamo una strategia greedy per la selezione delle osservazioni che mira a minimizzare l'incertezza di stato. Sviluppiamo un nuovo algoritmo di iterazione del valore basato sul punto che incorpora la strategia greedy per ottenere una riduzione quasi ottimale dell'incertezza per i punti di credenza campionati.Questo a sua volta permette al solutore di approssimare efficientemente il sottospazio raggiungibile del simplex di credenza essenzialmente separando i calcoli relativi alla percezione dalla pianificazione.Infine, implementiamo il solutore proposto e dimostriamo le sue prestazioni e il vantaggio computazionale in una gamma di scenari robotici dove il robot esegue simultaneamente la percezione attiva e la pianificazione.
Le reti neurali profonde, in particolare le reti neurali convoluzionali, sono diventate strumenti molto efficaci per comprimere le immagini e risolvere problemi inversi tra cui denoising, inpainting, e la ricostruzione da poche e rumorose misure.Questo successo può essere attribuito in parte alla loro capacità di rappresentare e generare bene le immagini naturali.Contrariamente agli strumenti classici come le wavelet, le reti neurali profonde che generano immagini hanno un gran numero di parametri---tipicamente un multiplo della loro dimensione di uscita---e hanno bisogno di essere addestrate su grandi set di dati. In questo articolo, proponiamo un modello di immagine semplice non addestrato, chiamato deep decoder, che è una rete neurale profonda in grado di generare immagini naturali da pochissimi parametri di peso. Il deep decoder ha un'architettura semplice senza convoluzioni e un numero di parametri di peso inferiore alla dimensionalità dell'output. Inoltre, la sottoparametrizzazione fornisce una barriera all'overfitting, permettendo al decoder profondo di avere prestazioni allo stato dell'arte per il denoising.Il decoder profondo è semplice nel senso che ogni strato ha una struttura identica che consiste in una sola unità di upsampling, combinazione lineare pixel-wise di canali, attivazione ReLU, e normalizzazione channelwise.Questa semplicità rende la rete suscettibile di analisi teorica, e fa luce sugli aspetti delle reti neurali che permettono loro di formare rappresentazioni efficaci del segnale.
In questo articolo esaminiamo la famiglia di funzioni rappresentabili dalle reti neurali profonde (DNN) con unità lineari rettificate (ReLU) e forniamo un algoritmo per addestrare una DNN ReLU con uno strato nascosto fino all'ottimalità globale con un tempo di esecuzione polinomiale nella dimensione dei dati, ma esponenziale nella dimensione dell'input. Inoltre, miglioriamo i limiti inferiori noti sulla dimensione (da esponenziale a super-esponenziale) per approssimare una funzione ReLU deep net con una ReLU net meno profonda. I nostri teoremi di gap sono validi per famiglie di funzioni ``hard'' parametrizzate in modo uniforme, contrariamente alle famiglie discrete e numerabili note in letteratura.  Un esempio di conseguenza dei nostri teoremi di gap è il seguente: per ogni numero naturale $k$ esiste una funzione rappresentabile da una ReLU DNN con $k^2$ strati nascosti e dimensione totale $k^3$, tale che ogni ReLU DNN con al massimo $k$ strati nascosti richiede almeno $\frac12k^{k+1}-1$ nodi totali. Infine, per la famiglia di DNN da $R^n a \R$ con attivazioni ReLU, mostriamo un nuovo limite inferiore sul numero di pezzi affini, che è più grande delle costruzioni precedenti in certi regimi dell'architettura della rete e, in particolare, il nostro limite inferiore è dimostrato da una costruzione esplicita di una famiglia di funzioni parametrizzate in modo liscio che raggiungono questa scala.
Il recente successo delle reti profonde nell'apprendimento automatico e nell'IA, tuttavia, ha ispirato una serie di proposte per capire come il cervello potrebbe imparare attraverso strati multipli, e quindi come potrebbe implementare o approssimare BP. Finora, nessuna di queste proposte è stata rigorosamente valutata su compiti in cui l'apprendimento profondo guidato da BP si è rivelato critico, o in architetture più strutturate di semplici reti completamente connesse. Qui presentiamo i primi risultati sullo scaling up di un modello biologicamente motivato di apprendimento profondo a set di dati che richiedono reti profonde con architetture appropriate per ottenere buone prestazioni.  Per CIFAR-10 dimostriamo che il nostro algoritmo, una variante semplice e senza trasporto di pesi della difference target-propagation (DTP) modificata per rimuovere la backpropagation dal penultimo strato, è competitivo con BP nell'addestramento di reti profonde con campi recettivi localmente definiti che hanno pesi slegati.  Per ImageNet troviamo che sia DTP che il nostro algoritmo hanno prestazioni significativamente peggiori di BP, aprendo la questione se siano necessarie architetture o algoritmi diversi per scalare questi approcci. I nostri risultati e i dettagli dell'implementazione aiutano a stabilire delle linee di base per gli schemi di apprendimento profondo biologicamente motivati in futuro.
Le reti neurali profonde (DNN) di solito contengono milioni, forse miliardi, di parametri/pesi, rendendo sia l'immagazzinamento che il calcolo molto costosi, il che ha motivato un ampio corpo di lavoro per ridurre la complessità della rete neurale utilizzando regolatori che inducono la sparsità.  Un altro approccio ben noto per il controllo della complessità delle DNN è la condivisione dei parametri, dove alcuni gruppi di pesi sono costretti a condividere un valore comune. Alcune forme di condivisione dei pesi sono cablate per esprimere certe variazioni, un esempio notevole è la shift-invariance degli strati convoluzionali, ma ci possono essere altri gruppi di pesi che possono essere legati insieme durante il processo di apprendimento, riducendo ulteriormente la complessità della rete. In questo articolo, adottiamo un regolatore che induce la sparsità recentemente proposto, chiamato GrOWL (group ordered weighted l1), che incoraggia la sparsità e, simulta- mente, impara quali gruppi di parametri dovrebbero condividere un valore comune.GrOWL si è dimostrato efficace nella regressione lineare, essendo in grado di identificare e affrontare covariate fortemente correlate, l1 a.k.a.Lasso), GrOWL non solo elimina i neuroni non importanti impostando tutti i pesi corrispondenti a zero, ma identifica anche esplicitamente i neuroni fortemente correlati legando i pesi corrispondenti a un valore comune.Questa capacità di GrOWL motiva la seguente procedura in due fasi: (i) usare la regolarizzazione GrOWL nel processo di addestramento per identificare simultaneamente i neuroni significativi e i gruppi di parametri che dovrebbero essere legati insieme; (ii) riaddestrare la rete, applicando la struttura che è stata svelata nella fase precedente, cioè, mantenendo solo i neuroni significativi e applicando la struttura di legatura appresa.Valutiamo l'approccio proposto su diversi set di dati di riferimento, mostrando che può comprimere notevolmente la rete con una perdita leggera o addirittura nulla sulle prestazioni di generalizzazione.
Weight-sharing - l'ottimizzazione simultanea di più reti neurali utilizzando gli stessi parametri - è emerso come un componente chiave dello stato dell'arte della ricerca di architetture neurali. Tuttavia, il suo successo è scarsamente compreso e spesso trovato sorprendente. Noi sosteniamo che, piuttosto che essere solo un trucco di ottimizzazione, il peso-sharing approccio è indotto dal rilassamento di uno spazio di ipotesi strutturato, e introduce nuove sfide algoritmiche e teoriche, nonché applicazioni oltre la ricerca di architetture neurali. Algoritmicamente, mostriamo come la geometria dell'ERM per la condivisione dei pesi richieda maggiore attenzione nella progettazione di metodi di minimizzazione basati sul gradiente e applichiamo strumenti di ottimizzazione non convessa non euclidea per fornire algoritmi generici che si adattano alla struttura sottostante. Successivamente, utilizzando la configurazione del kernel e la selezione delle caratteristiche NLP come casi di studio, dimostriamo come la condivisione del peso si applica alla generalizzazione della ricerca dell'architettura di NAS e ottimizza efficacemente l'obiettivo bilivello risultante.
La compressione senza perdite è un'applicazione di questi modelli che, nonostante abbia il potenziale per essere molto utile, deve ancora essere implementata in modo pratico. Presentiamo "Bit Back with ANS" (BB-ANS), uno schema per eseguire la compressione senza perdite con modelli di variabili latenti a un tasso quasi ottimale. Dimostriamo questo schema usandolo per comprimere il dataset MNIST con un modello variational auto-encoder (VAE), ottenendo tassi di compressione superiori ai metodi standard con solo un semplice VAE.Dato che lo schema è altamente suscettibile di parallelizzazione, concludiamo che con un modello generativo di qualità sufficientemente alta questo schema potrebbe essere usato per ottenere miglioramenti sostanziali nel tasso di compressione con un tempo di esecuzione accettabile.Rendiamo la nostra implementazione disponibile open source a https://github.com/bits-back/bits-back .
La sintonizzazione degli iperparametri è probabilmente l'ingrediente più importante per ottenere prestazioni allo stato dell'arte nelle reti profonde.  Ci concentriamo sugli iperparametri che sono legati all'algoritmo di ottimizzazione, ad esempio i tassi di apprendimento, che hanno un grande impatto sulla velocità di formazione e l'accuratezza risultante.Tipicamente, i programmi fissi di apprendimento sono impiegati durante la formazione.Proponiamo Hyperdyn un metodo dinamico di ottimizzazione degli iperparametri che seleziona nuovi tassi di apprendimento al volo alla fine di ogni epoca.Il nostro quadro explore-exploit combina l'ottimizzazione bayesiana (BO) con una strategia di rifiuto, basata su un semplice test probabilistico di attesa e osservazione.   Otteniamo risultati di accuratezza allo stato dell'arte sui set di dati CIFAR e Imagenet, ma con una formazione significativamente più veloce, se confrontata con le migliori reti sintonizzate manualmente.
La risposta a domande basate su testo multi-hop è una sfida attuale nella comprensione automatica. In questo documento proponiamo una nuova architettura, chiamata Latent Question Reformulation Network (LQR-net), una rete attenta, multi-hop e parallela, progettata per compiti di risposta alle domande che richiedono capacità di ragionamento. LQR-net è composta da un'associazione di moduli di lettura del testo e di moduli di riformulazione, il cui scopo è quello di produrre una rappresentazione del documento consapevole delle domande. Da questa rappresentazione del documento, il modulo di riformulazione estrae gli elementi essenziali per calcolare una rappresentazione aggiornata della domanda.Questa domanda aggiornata viene poi passata all'hop successivo.Valutiamo la nostra architettura sul dataset di domande-risposta \hotpotqa progettato per valutare le capacità di ragionamento multi-hop.Il nostro modello raggiunge risultati competitivi nella classifica pubblica e supera i migliori modelli attuali \textit{published} in termini di Exact Match (EM) e punteggio $F_1$.Infine, dimostriamo che un'analisi delle riformulazioni sequenziali può fornire percorsi di ragionamento interpretabili.
Proponiamo un metodo per calcolare automaticamente l'importanza delle caratteristiche ad ogni osservazione nelle serie temporali, simulando traiettorie controfattuali date le osservazioni precedenti.Definiamo l'importanza di ogni osservazione come il cambiamento nell'output del modello causato dalla sostituzione dell'osservazione con una generata.Il nostro metodo può essere applicato a modelli di serie temporali arbitrariamente complessi.Confrontiamo l'importanza delle caratteristiche generate con metodi esistenti come le analisi di sensibilità, l'occlusione delle caratteristiche, e altre basi di spiegazione per mostrare che il nostro approccio genera spiegazioni più precise ed è meno sensibile al rumore nei segnali di input.
Questo documento affronta l'adattamento di dominio non supervisionato, l'impostazione in cui i dati di formazione etichettati sono disponibili su un dominio di origine, ma l'obiettivo è quello di avere buone prestazioni su un dominio di destinazione con solo dati non etichettati.Come gran parte del lavoro precedente, cerchiamo di allineare le rappresentazioni apprese dei domini di origine e di destinazione, preservando la discriminabilità.Il modo in cui realizziamo l'allineamento è quello di imparare ad eseguire ausiliari compiti auto-supervisionati su entrambi i domini contemporaneamente.  Ogni compito auto-supervisionato avvicina i due domini lungo la direzione pertinente a quel compito.L'addestramento di questo congiuntamente al classificatore del compito principale sul dominio di origine è dimostrato per generalizzare con successo al dominio di destinazione non etichettato.  L'obiettivo presentato è semplice da implementare e facile da ottimizzare.Raggiungiamo risultati allo stato dell'arte su quattro dei sette benchmark standard, e risultati competitivi sull'adattamento della segmentazione.Dimostriamo anche che il nostro metodo si compone bene con un altro popolare metodo di adattamento a livello di pixel.
Introduciamo algoritmi semplici ed efficienti per il calcolo di un MinHash di una distribuzione di probabilità, adatti sia per dati sparsi che densi, con tempi di esecuzione equivalenti allo stato dell'arte per entrambi i casi.La probabilità di collisione di questi algoritmi è una nuova misura della somiglianza dei vettori positivi che studiamo in dettaglio. Descriviamo il senso in cui questa probabilità di collisione è ottimale per qualsiasi Locality Sensitive Hash basato sul campionamento.Sosteniamo che questa misura di somiglianza è più utile per le distribuzioni di probabilità rispetto alla somiglianza perseguita da altri algoritmi per il MinHash pesato, ed è la naturale generalizzazione dell'indice Jaccard.
Tra i modelli esistenti, le reti neurali a grafo (GNN) sono uno degli approcci più efficaci per il ragionamento relazionale multi-hop; infatti, il ragionamento relazionale multi-hop è indispensabile in molti compiti di elaborazione del linguaggio naturale come l'estrazione delle relazioni. In questo articolo, proponiamo di generare i parametri delle reti neurali a grafo (GP-GNN) in base alle frasi del linguaggio naturale, il che permette alle GNN di elaborare il ragionamento relazionale su input di testo non strutturati. I risultati sperimentali su un set di dati commentati dall'uomo e su due set di dati supervisionati a distanza dimostrano che il nostro modello raggiunge miglioramenti significativi rispetto alle linee di base, inoltre eseguiamo un'analisi qualitativa per dimostrare che il nostro modello potrebbe scoprire relazioni più accurate attraverso il ragionamento relazionale multi-hop.
I metodi Off-Policy Actor-Critic (Off-PAC) hanno dimostrato di avere successo in una varietà di compiti di controllo continuo.Normalmente, la funzione azione-valore del critico viene aggiornata usando la differenza temporale, e il critico a sua volta fornisce una perdita per l'attore che lo allena a intraprendere azioni con un ritorno atteso più alto.In questo articolo, introduciamo un nuovo e flessibile meta-critico che osserva il processo di apprendimento e meta-apprende una perdita aggiuntiva per l'attore che accelera e migliora l'apprendimento actor-critic. Rispetto al critico vaniglia, la rete meta-critica è esplicitamente addestrata per accelerare il processo di apprendimento; e rispetto agli algoritmi di meta-apprendimento esistenti, il meta-critico è rapidamente appreso online per un singolo compito, piuttosto che lentamente su una famiglia di compiti.Crucialmente, la nostra struttura meta-critica è progettata per gli apprendisti basati su off-policy, che attualmente forniscono lo stato dell'arte dell'efficienza del campione di apprendimento di rinforzo.dimostriamo che l'apprendimento meta-critico online porta a miglioramenti in una varietà di ambienti di controllo continuo quando combinato con metodi contemporanei Off-PAC DDPG, TD3 e lo stato dell'arte SAC.
Le reti neurali moderne sono altamente iperparametrizzate, con la capacità di sovraraffinare sostanzialmente ai dati di addestramento. Tuttavia, queste reti spesso generalizzano bene in pratica. È stato anche osservato che le reti addestrate possono spesso essere ``compresse a rappresentazioni molto più piccole. Lo scopo di questo articolo è di collegare queste due osservazioni empiriche. In particolare, forniamo le prime garanzie di generalizzazione non vuote per architetture realistiche applicate al problema di classificazione ImageNet. Inoltre, dimostriamo che la compressibilità dei modelli che tendono a sovrafittare è limitata. I risultati empirici mostrano che un aumento del sovrafittamento aumenta il numero di bit richiesti per descrivere una rete addestrata.
Gli esempi avversari possono essere definiti come gli input ad un modello che inducono un errore - dove l'uscita del modello Ã¨ differente da quella di un oracolo, forse nei modi sorprendenti o maliziosi. I modelli originali degli attacchi avversari sono studiati soprattutto nel contesto delle mansioni di classificazione e di visione del calcolatore. Mentre parecchi attacchi sono stati proposti nelle regolazioni di elaborazione del linguaggio naturale (NLP), variano spesso nella definizione dei parametri di un attacco e di che cosa un attacco riuscito osserverebbe come. Definiamo la nozione di guadagno avversario: basata sulla teoria del controllo, è una misura del cambiamento nell'output di un sistema rispetto alla perturbazione dell'input (causata dal cosiddetto avversario) presentata al discente. Questa definizione, come dimostriamo, può essere usata in diversi spazi di caratteristiche e condizioni di distanza per determinare l'efficacia dell'attacco o della difesa attraverso diversi collettori intuitivi. Questa nozione di guadagno avversario non solo fornisce un modo utile per valutare gli avversari e le difese, ma può agire come un blocco di costruzione per il lavoro futuro sulla robustezza sotto gli avversari grazie alla sua natura radicata nella teoria della stabilità e dei manifold.
Proponiamo una Warped Residual Network (WarpNet) che utilizza un operatore di curvatura parallelizzabile per la propagazione in avanti e all'indietro a strati distanti che si allena più velocemente della rete neurale residua originale.Applichiamo una teoria della perturbazione sulle reti residue e disaccoppiamo le interazioni tra le unità residue.L'operatore di curvatura risultante è un'approssimazione del primo ordine dell'output su più strati.La teoria della perturbazione del primo ordine esibisce proprietà come la lunghezza binomiale dei percorsi e la scalatura esponenziale del gradiente trovata sperimentalmente da Veit et al (2016). Dimostriamo attraverso un ampio studio delle prestazioni che la rete proposta raggiunge prestazioni predittive paragonabili alla rete residua originale con lo stesso numero di parametri, pur ottenendo una significativa accelerazione del tempo totale di formazione.Poiché WarpNet esegue il parallelismo del modello nella formazione della rete residua in cui i pesi sono distribuiti su diverse GPU, offre un'accelerazione e la capacità di formare reti più grandi rispetto alle reti residue originali.
Una pletora di metodi che tentano di spiegare le previsioni dei modelli black-box sono stati proposti dalla comunità di Explainable Artificial Intelligence (XAI), ma la misurazione della qualità delle spiegazioni generate è in gran parte inesplorata, rendendo i confronti quantitativi non banali. In questo lavoro, proponiamo una serie di metriche sfaccettate che ci permettono di confrontare oggettivamente gli spiegatori sulla base della correttezza, della coerenza e della fiducia delle spiegazioni generate. Queste metriche sono computazionalmente poco costose, non richiedono il riaddestramento del modello e possono essere utilizzate in diverse modalità di dati. Le valutiamo su spiegatori comuni come Grad-CAM, SmoothGrad, LIME e Integrated Gradients. I nostri esperimenti mostrano che le metriche proposte riflettono le osservazioni qualitative riportate in lavori precedenti.
Le reti neurali sono note per produrre risultati inaspettati su input che sono lontani dalla distribuzione di addestramento.Un approccio per affrontare questo problema è quello di rilevare i campioni su cui la rete addestrata non può rispondere in modo affidabile.ODIN è un metodo recentemente proposto per il rilevamento di out-of-distribution che non modifica la rete addestrata e raggiunge buone prestazioni per vari compiti di classificazione delle immagini.In questo articolo adattiamo ODIN per la classificazione delle frasi e i compiti di tagging delle parole.Mostriamo che i punteggi prodotti da ODIN possono essere utilizzati come una misura di fiducia per le previsioni sia su dataset in-distribuzione che out-of-distribuzione.
Alcuni lavori recenti hanno mostrato la separazione tra la potenza espressiva delle reti neurali depth-2 e depth-3. Questi risultati di separazione sono mostrati costruendo funzioni e distribuzioni di input, in modo che la funzione sia ben approssimabile da una rete neurale depth-3 di dimensioni polinomiali, ma non può essere ben approssimata sotto la distribuzione di input scelta da qualsiasi rete neurale depth-2 di dimensioni polinomiali. Questi risultati non sono robusti e richiedono funzioni accuratamente scelte così come le distribuzioni di input. Mostriamo una simile separazione tra il potere espressivo delle reti neurali sigmoidali depth-2 e depth-3 su un'ampia classe di distribuzioni di input, a patto che i pesi siano polinomialmente delimitati.Facendo questo, mostriamo anche che le reti neurali sigmoidali depth-2 con piccola larghezza e piccoli pesi possono essere ben approssimate da polinomi multivariati di basso grado.
Nell'apprendimento di rinforzo (RL), dove il grafico ponderato può essere interpretato come il processo di transizione di stato indotto da una politica di comportamento che agisce sull'ambiente, l'approssimazione degli autovettori del Laplaciano fornisce un approccio promettente all'apprendimento della rappresentazione dello stato:  In questo articolo, presentiamo un metodo completamente generale e scalabile per approssimare gli autovettori del Laplaciano in un contesto di RL senza modello. Valutiamo sistematicamente il nostro approccio e dimostriamo empiricamente che si generalizza oltre l'impostazione tabellare a stato finito. Anche nelle impostazioni tabulari a stati finiti, la sua capacità di approssimare gli autovettori supera le proposte precedenti. Infine, mostriamo i potenziali benefici dell'utilizzo di una rappresentazione del Laplaciano appresa con il nostro metodo in compiti di RL con obiettivi, fornendo la prova che la nostra tecnica può essere utilizzata per migliorare significativamente le prestazioni di un agente RL.
Il nostro lavoro offre un nuovo metodo per la traduzione del dominio dalle mappe semantiche delle etichette e dalle immagini della mappa dei bordi di simulazione della Computer Graphic (CG) alle immagini foto-realistiche, addestrando una Generative Adversarial Network (GAN) in modo condizionale per generare una versione foto-realistica di una data scena CG. Le architetture esistenti delle GAN mancano ancora delle capacità di fotorealismo necessarie per addestrare le DNN per compiti di computervision, noi affrontiamo questo problema incorporando mappe di bordo e addestrandole in modalità anadversariale.
Le reti neurali profonde sono ampiamente utilizzate in vari domini, ma la proibitiva complessità computazionale impedisce la loro distribuzione su dispositivi mobili.Sono stati proposti numerosi algoritmi di compressione del modello, tuttavia, è spesso difficile e richiede tempo scegliere gli iper-parametri appropriati per ottenere un modello compresso efficiente.In questo articolo, proponiamo un framework automatizzato per la compressione e l'accelerazione del modello, ovvero PocketFlow. Si tratta di un toolkit facile da usare che integra una serie di algoritmi di compressione del modello e incorpora un modulo di ottimizzazione degli iperparametri per cercare automaticamente la combinazione ottimale di iperparametri.Inoltre, il modello compresso può essere convertito nel formato TensorFlow Lite e facilmente distribuito su dispositivi mobili per accelerare l'inferenza.PocketFlow è ora open-source e pubblicamente disponibile su https://github.com/Tencent/PocketFlow.
I modelli generativi forniscono un modo per modellare la struttura in distribuzioni complesse e hanno dimostrato di essere utili per molti compiti di interesse pratico. Tuttavia, le tecniche attuali per la formazione di modelli generativi richiedono l'accesso a campioni completamente osservati. In molte impostazioni, è costoso o addirittura impossibile ottenere campioni completamente osservati, ma è economico ottenere osservazioni parziali e rumorose. Dimostriamo che la vera distribuzione sottostante può essere provatamente recuperata anche in presenza di perdita di informazioni per campione per una classe di modelli di misurazione.Sulla base di questo, proponiamo un nuovo metodo di addestramento delle reti generative avversarie (GAN) che chiamiamo AmbientGAN.Su tre set di dati di riferimento, e per vari modelli di misurazione, dimostriamo sostanziali miglioramenti qualitativi e quantitativi.I modelli generativi addestrati con il nostro metodo possono ottenere $2$-$4$x punteggi di inizio più alti rispetto alle linee base.
La teoria delle matrici casuali (RMT) viene applicata per analizzare le matrici dei pesi delle reti neurali profonde (DNN), includendo sia la qualità della produzione, modelli pre-addestrati come AlexNet e Inception, sia modelli più piccoli addestrati da zero, come LeNet5 e un mini-AlexNet.  I risultati empirici e teorici indicano chiaramente che la densità spettrale empirica (ESD) delle matrici di strato DNN mostra le firme dei modelli statistici tradizionalmente regolarizzati, anche in assenza di forme esogene di regolarizzazione, come i vincoli Dropout o Weight Norm.  Sulla base dei recenti risultati in RMT, in particolare la sua estensione alle classi di universalità delle matrici a coda pesante, sviluppiamo una teoria per identificare 5+1 fasi di addestramento, corrispondenti a quantità crescenti di auto-regolarizzazione implicita.  Per le DNN più piccole e/o più vecchie, questa auto-regolarizzazione implicita è come la tradizionale regolarizzazione di Tikhonov, nel senso che c'è una "scala di grandezza" che separa il segnale dal rumore.  Per le DNN allo stato dell'arte, tuttavia, identifichiamo una nuova forma di auto-regolarizzazione a coda pesante, simile all'auto-organizzazione vista nella fisica statistica dei sistemi disordinati.  Questa auto-regolarizzazione implicita può dipendere fortemente dalle molte manopole del processo di formazione.  Sfruttando i fenomeni di gap di generalizzazione, dimostriamo che possiamo far sì che un piccolo modello esibisca tutte le 5+1 fasi di addestramento semplicemente cambiando la dimensione del batch.
Introduciamo un meccanismo di attenzione per migliorare l'estrazione delle caratteristiche per l'apprendimento attivo profondo (AL) nell'impostazione semi-supervisionata.Il meccanismo di attenzione proposto si basa su metodi recenti per spiegare visivamente le previsioni fatte da DNNs.Applichiamo l'attenzione basata sulla spiegazione proposta alla classificazione MNIST e SVHN.Gli esperimenti condotti mostrano miglioramenti di precisione per i set di dati originali e di classe-imbalanced con lo stesso numero di esempi di formazione e una convergenza a coda lunga più veloce rispetto ai metodi basati sull'incertezza.
Applichiamo forme canoniche di complessi di gradiente (codici a barre) per esplorare le superfici di perdita delle reti neurali e presentiamo un algoritmo per il calcolo dei codici a barre dei minimi della funzione obiettivo.  I nostri esperimenti confermano due osservazioni principali: (1) i codici a barre dei minimi si trovano in una piccola parte inferiore della gamma di valori della funzione obiettivo e (2) l'aumento della profondità della rete neurale fa scendere i codici a barre dei minimi.
I nuovi tipi di hardware di calcolo in fase di sviluppo e che stanno entrando sul mercato mantengono la promessa di rivoluzionare l'apprendimento profondo in un modo profondo come le GPU.Tuttavia, le strutture software esistenti e gli algoritmi di addestramento per l'apprendimento profondo devono ancora evolversi per sfruttare appieno la capacità della nuova ondata di silicio.In particolare, i modelli che sfruttano l'input strutturato attraverso un flusso di controllo complesso e dipendente dall'istanza sono difficili da accelerare utilizzando gli algoritmi esistenti e l'hardware che in genere si basa sul minibatching. Presentiamo un algoritmo di formazione asincrona modello-parallelo (AMP) che è specificamente motivato dalla formazione su reti di dispositivi interconnessi.Attraverso un'implementazione su CPU multi-core, dimostriamo che la formazione AMP converge alla stessa precisione degli algoritmi di formazione sincroni convenzionali in un numero simile di epoche, ma utilizza l'hardware disponibile in modo più efficiente, anche per piccole dimensioni minibatch, con conseguente tempi di formazione complessiva più breve.Il nostro quadro apre la porta per scalare una nuova classe di modelli di apprendimento profondo che non può essere efficientemente addestrato oggi.
Il segnale di ricompensa globale assegna la stessa ricompensa globale a tutti gli agenti senza distinguere i loro contributi, mentre il segnale di ricompensa locale fornisce diverse ricompense locali a ciascun agente basato esclusivamente sul comportamento individuale. Entrambi i due approcci di assegnazione della ricompensa hanno alcuni difetti: il primo potrebbe incoraggiare agenti pigri, mentre il secondo potrebbe produrre agenti egoisti. In primo luogo, mostriamo che i due segnali di ricompensa di cui sopra sono inclini a produrre politiche subottimali.Poi, ispirati da alcune osservazioni e considerazioni, progettiamo alcuni segnali di ricompensa misti, che sono fuori mercato per imparare politiche migliori.Infine, trasformiamo i segnali di ricompensa misti nelle controparti adattive, che ottengono i migliori risultati nei nostri esperimenti.Altri segnali di ricompensa sono anche discussi in questo articolo.Poiché la progettazione della ricompensa è un problema molto fondamentale in RL e specialmente in MARL, speriamo che i ricercatori MARL possano ripensare i premi utilizzati nei loro sistemi.
Recenti progressi hanno illustrato che spesso è possibile imparare a risolvere problemi inversi lineari nell'imaging usando dati di addestramento che possono superare le soluzioni regolarizzate più tradizionali dei minimi quadrati.Lungo queste linee, presentiamo alcune estensioni della rete di Neumann, un'architettura appresa end-to-end introdotta recentemente ispirata da un'espansione in serie troncata di Neumann della mappa della soluzione di un problema regolarizzato dei minimi quadrati. Qui riassumiamo l'approccio della rete di Neumann, e mostriamo che ha una forma compatibile con la funzione di ricostruzione ottimale per un dato problema inverso. Studiamo anche un'estensione della rete di Neumann che incorpora un approccio di regolarizzazione più efficiente basato su patch.
Il dialogo end-to-end orientato al compito è impegnativo poiché le basi di conoscenza sono solitamente grandi, dinamiche e difficili da incorporare in un quadro di apprendimento.Proponiamo le reti global-to-local memory pointer (GLMP) per affrontare questo problema.Nel nostro modello, un codificatore di memoria globale e un decodificatore di memoria locale sono proposti per condividere la conoscenza esterna.Il codificatore codifica la storia del dialogo, modifica la rappresentazione contestuale globale e genera un puntatore di memoria globale.Il decodificatore genera prima una risposta di schizzo con slot non riempiti. Successivamente, passa il puntatore di memoria globale per filtrare la conoscenza esterna per le informazioni pertinenti, quindi istanzia gli slot tramite i puntatori di memoria locale.Mostriamo empiricamente che il nostro modello può migliorare l'accuratezza della copia e mitigare il problema comune fuori dal vocabolario.Di conseguenza, GLMP è in grado di migliorare rispetto ai precedenti modelli all'avanguardia sia nel set di dati simulati bAbI Dialogue sia nel set di dati umano-umano Stanford Multi-domain Dialogue sulla valutazione automatica e umana.
Il fenomeno della scacchiera è uno dei ben noti artefatti visivi nel campo della computer vision.Le origini e le soluzioni degli artefatti della scacchiera nello spazio dei pixel sono state studiate per molto tempo, ma i loro effetti sullo spazio del gradiente sono stati raramente studiati.In questo articolo, rivisitiamo gli artefatti della scacchiera nello spazio del gradiente che risultano essere il punto debole di un'architettura di rete.Esploriamo la proprietà image-agnostic degli artefatti della scacchiera del gradiente e proponiamo un metodo di difesa semplice ma efficace utilizzando gli artefatti. Introduciamo il nostro modulo di difesa, soprannominato Artificial Checkerboard Enhancer (ACE), che induce attacchi avversari su pixel designati.Questo permette al modello di deviare gli attacchi spostando solo un singolo pixel nell'immagine con un notevole tasso di difesa.Forniamo ampi esperimenti per sostenere l'efficacia del nostro lavoro per vari scenari di attacco utilizzando metodi di attacco all'avanguardia.Inoltre, dimostriamo che ACE è anche applicabile a set di dati su larga scala tra cui il dataset ImageNet e può essere facilmente trasferito a varie reti preformate.
Le formulazioni Min-max hanno attirato grande attenzione nella comunità ML a causa dell'aumento dei modelli generativi profondi e dei metodi avversari, e la comprensione delle dinamiche degli algoritmi di gradiente (stocastico) per la risoluzione di tali formulazioni è stata una grande sfida. Come primo passo, ci limitiamo ai giochi bilineari a somma zero e forniamo un'analisi sistematica dei più diffusi aggiornamenti del gradiente, sia per le versioni simultanee che per quelle alternate; forniamo condizioni esatte per la loro convergenza e troviamo la configurazione ottimale dei parametri e i tassi di convergenza; in particolare, i nostri risultati offrono la prova formale che gli aggiornamenti alternati convergono "meglio" di quelli simultanei.
La maggior parte degli approcci nell'apprendimento generalizzato a zero colpi si basano sulla mappatura cross-modale tra uno spazio di caratteristiche dell'immagine e uno spazio di incorporamento di classe o sulla generazione di caratteristiche artificiali dell'immagine; tuttavia, l'apprendimento di un incorporamento cross-modale condiviso allineando gli spazi latenti degli autocodificatori specifici della modalità ha dimostrato di essere promettente nell'apprendimento (generalizzato) a zero colpi. Seguendo la stessa direzione, facciamo anche un ulteriore passo avanti nella generazione di caratteristiche artificiali e proponiamo un modello in cui uno spazio latente condiviso di caratteristiche dell'immagine e incorporazioni di classe viene appreso da autoencoder variazionali allineati, allo scopo di generare caratteristiche latenti per addestrare un classificatore softmax. Valutiamo le nostre caratteristiche latenti apprese su set di dati di riferimento convenzionali e stabiliamo un nuovo stato dell'arte sull'apprendimento generalizzato a zero scatti e su quello a pochi scatti.
Intuitivamente, la classificazione delle immagini dovrebbe trarre vantaggio dall'uso delle informazioni spaziali, ma un lavoro recente suggerisce che questo potrebbe essere sopravvalutato nelle CNN standard. In questo articolo, stiamo spingendo la busta e miriamo a indagare ulteriormente la dipendenza e la necessità delle informazioni spaziali. Proponiamo e analizziamo tre metodi, cioè Shuffle Conv, GAP+FC e 1x1 Conv, che distruggono le informazioni spaziali durante entrambe le fasi di allenamento e di test. Valutiamo ampiamente questi metodi su diversi set di dati di riconoscimento degli oggetti (CIFAR100, Small-ImageNet, ImageNet) con una vasta gamma di architetture CNN (VGG16, ResNet50, ResNet152, MobileNet, SqueezeNet). È interessante notare che osserviamo costantemente che le informazioni spaziali possono essere completamente eliminate da un numero significativo di strati senza o solo con piccoli cali di prestazioni.
Il nostro primo metodo, Unlabeled Disentangling GAN (UD-GAN, non supervisionato), decompone il rumore latente generando coppie di immagini simili/dissimili e apprende una metrica di distanza su queste coppie con reti siamesi e una perdita contrastiva. Il nostro secondo metodo (UD-GAN-G, debolmente supervisionato) modifica UD-GAN con funzioni di guida definite dall'utente, che limitano le informazioni che entrano nelle reti siamesi.Questo vincolo aiuta UD-GAN-G a concentrarsi sulle variazioni semantiche desiderate nei dati.Mostriamo che entrambi i nostri metodi superano gli approcci non supervisionati esistenti nelle metriche quantitative che misurano la precisione semantica delle rappresentazioni apprese.Inoltre, illustriamo che le semplici funzioni di guida che usiamo in UD-GAN-G ci permettono di catturare direttamente le variazioni desiderate nei dati.
Presentiamo Predicted Variables, un approccio per rendere il machine learning (ML) un cittadino di prima classe nei linguaggi di programmazione.C'è una crescente divisione negli approcci alla costruzione di sistemi: usando esperti umani (ad esempio la programmazione) da un lato, e usando il comportamento appreso dai dati (ad esempio ML) dall'altro.PVars mira a rendere l'uso del ML nella programmazione più facile, ibridando i due.Sfruttiamo il concetto esistente di variabili e creiamo un nuovo tipo, una variabile predetta.PVars sono simili alle variabili native con una distinzione importante: Descriviamo le PVars e la loro interfaccia, come possono essere usate nella programmazione e dimostriamo la fattibilità del nostro approccio su tre problemi algoritmici: ricerca binaria, QuickSort e cache.Mostriamo sperimentalmente che le PVars sono in grado di migliorare rispetto alle euristiche comunemente usate e portano ad una performance migliore degli algoritmi originali.A differenza dei lavori precedenti che applicano il ML ai problemi algoritmici, le PVars hanno il vantaggio di poter essere usate all'interno dei framework esistenti e non richiedono la sostituzione della conoscenza del dominio esistente. La nostra implementazione di PVars attualmente si basa su metodi standard di Reinforcement Learning (RL).Per imparare più velocemente, i PVars usano la funzione euristica, che stanno sostituendo, come una funzione iniziale.Dimostriamo che i PVars prendono rapidamente il comportamento della funzione iniziale e poi migliorano le prestazioni oltre questo senza mai eseguire sostanzialmente peggio - permettendo un'implementazione sicura in applicazioni critiche.
Molte ricerche recenti sono state dedicate alla predizione e alla generazione di video, ma per lo più per orizzonti temporali su breve scala.Il metodo di predizione video gerarchico di Villegas et al. (2017) è un esempio di un metodo allo stato dell'arte per la predizione video a lungo termine.  Tuttavia, il loro metodo ha un'applicabilità limitata nelle impostazioni pratiche in quanto richiede una posa di verità a terra (ad esempio, le pose delle articolazioni di un umano) al momento della formazione.   Questo articolo presenta un modello di predizione video gerarchico a lungo termine che non ha tale restrizione e dimostra che la rete impara la propria struttura di livello superiore (ad esempio, variabili nascoste equivalenti alla posa) che funziona meglio nei casi in cui la posa di verità a terra non cattura completamente tutte le informazioni necessarie per prevedere il fotogramma successivo.   Questo metodo dà risultati più nitidi di altri metodi di predizione video che non richiedono una posa di verità a terra, e la sua efficienza è dimostrata sui dataset Humans 3.6M e Robot Pushing.
Combinare le informazioni provenienti da diverse modalità sensoriali per eseguire azioni dirette all'obiettivo è un aspetto chiave dell'intelligenza umana. In particolare, gli agenti umani sono molto facilmente in grado di tradurre il compito comunicato in un dominio sensoriale (ad esempio la visione) in una rappresentazione che consente loro di completare questo compito quando possono solo percepire il loro ambiente utilizzando una modalità sensoriale separata (ad esempio il tatto).Al fine di costruire agenti con capacità simili, in questo lavoro consideriamo il problema di un recupero di un oggetto target da un cassetto. L'agente riceve un'immagine di un oggetto precedentemente non visto ed esplora gli oggetti nel cassetto usando solo il rilevamento tattile per recuperare l'oggetto che è stato mostrato nell'immagine senza ricevere alcun feedback visivo.Il successo in questo compito richiede una stretta integrazione del rilevamento visivo e tattile.Presentiamo un metodo per eseguire questo compito in un ambiente simulato usando una mano antropomorfa.Speriamo che la ricerca futura nella direzione della combinazione dei segnali sensoriali per agire trovi il recupero dell'oggetto da un cassetto come un utile problema di riferimento
Gli schemi di hashing sensibili alla localizzazione come \simhash forniscono rappresentazioni compatte di multiset da cui la somiglianza può essere stimata.Tuttavia, in alcune applicazioni, abbiamo bisogno di stimare la somiglianza di set che cambiano dinamicamente.  In questo caso, abbiamo bisogno che la rappresentazione sia un omomorfismo in modo che l'hash di unioni e differenze di insiemi possa essere calcolato direttamente dagli hash degli operandi.  Proponiamo due rappresentazioni che hanno questa proprietà per la somiglianza del coseno (un'estensione di \simhash e proiezioni casuali che conservano l'angolo), e facciamo progressi sostanziali su una terza rappresentazione per la somiglianza di Jaccard (un'estensione di \minhash). Impieghiamo questi hash per comprimere le statistiche sufficienti di un modello di coreferenza del campo casuale condizionato (CRF) e studiamo come questa compressione influenzi la nostra capacità di calcolare le somiglianze quando le entità sono divise e unite durante l'inferenza. \Studiamo questi hashish in un modello di coreferenza gerarchico in un campo casuale condizionato (CRF) per calcolare la somiglianza delle entità mentre vengono fuse e divise durante l'inferenza. Forniamo anche una nuova analisi statistica di \simhash per aiutare a giustificarlo come uno stimatore all'interno di un CRF, mostrando che la distorsione e la varianza si riducono rapidamente con il numero di bit. Su un problema di coreferenza d'autore, troviamo che il nostro schema \simhash permette di scalare l'algoritmo di coreferenza gerarchica di un ordine di grandezza senza degradare le sue prestazioni statistiche o l'accuratezza di coreferenza del modello, a patto di impiegare almeno 128 o 256 bit.  Le proiezioni casuali che preservano l'angolo migliorano ulteriormente la qualità della coreferenza, permettendo potenzialmente di utilizzare anche meno dimensioni.
Le analisi recenti hanno mostrato che gli stimatori ingenui kNN dell'informazione reciproca hanno serie limitazioni statistiche che motivano metodi più raffinati. In questo articolo dimostriamo che le serie limitazioni statistiche sono inerenti a qualsiasi metodo di misurazione. Più specificamente, dimostriamo che qualsiasi limite inferiore ad alta confidenza senza distribuzione sull'informazione reciproca non può essere più grande di $O(\ln N)$ dove $N$ è la dimensione del campione di dati. Analizziamo anche il limite inferiore di Donsker-Varadhan sulla divergenza KL in particolare e mostriamo che, quando semplici considerazioni statistiche sono prese in considerazione, questo limite non può mai produrre un valore di alta confidenza più grande di $\ln N$.Mentre grandi limiti inferiori di alta confidenza sono impossibili, in pratica si possono usare stimatori senza garanzie formali. Osserviamo che, sebbene l'entropia incrociata sia solo un limite superiore dell'entropia, le stime dell'entropia incrociata convergono alla vera entropia incrociata al tasso di $1/\sqrt{N}$.
In questo articolo, proponiamo una struttura di rete neurale chiamata rete gerarchica di neuroni (NHN), che si evolve oltre la gerarchia in strati, e si concentra sulla gerarchia dei neuroni.Osserviamo la ridondanza di massa nei pesi di entrambe le architetture artigianali e cercate a caso.Ispirati dallo sviluppo del cervello umano, potiamo i neuroni a bassa sensibilità nel modello e aggiungiamo nuovi neuroni al grafico, e la relazione tra i singoli neuroni sono enfatizzati e l'esistenza di strati indeboliti. Proponiamo un processo per scoprire il miglior modello di base tramite la ricerca casuale dell'architettura, e scoprire le migliori posizioni e connessioni dei neuroni aggiunti tramite la ricerca evolutiva. I risultati degli esperimenti mostrano che l'NHN raggiunge un'accuratezza di test più alta su Cifar-10 rispetto alle architetture artigianali e casuali all'avanguardia, mentre richiede molti meno parametri e meno tempo di ricerca.
La simulazione è uno strumento utile in situazioni in cui i dati di formazione per i modelli di apprendimento automatico sono costosi da annotare o addirittura difficili da acquisire. In questo lavoro, proponiamo un metodo basato sull'apprendimento di rinforzo per regolare automaticamente i parametri di qualsiasi simulatore (non differenziabile), controllando così la distribuzione dei dati sintetizzati al fine di massimizzare la precisione di un modello addestrato su quei dati. In contrasto con l'arte precedente che crea a mano questi parametri di simulazione o regola solo parti dei parametri disponibili, il nostro approccio controlla completamente il simulatore con l'obiettivo effettivo di massimizzare la precisione, piuttosto che imitare la distribuzione dei dati reali o generare casualmente un grande volume di dati. Troviamo che il nostro approccio (i) converge rapidamente ai parametri di simulazione ottimali in esperimenti controllati e (ii) può effettivamente scoprire buoni set di parametri per un simulatore di rendering delle immagini in applicazioni reali di visione artificiale.
Modellare le relazioni statistiche al di là della media condizionale è cruciale in molte impostazioni.La stima della densità condizionale (CDE) mira ad apprendere la densità di probabilità condizionale completa dai dati.Anche se altamente espressivi, i modelli CDE basati su reti neurali possono soffrire di un grave over-fitting quando addestrati con l'obiettivo di massima verosimiglianza.A causa della struttura intrinseca di tali modelli, i classici approcci di regolarizzazione nello spazio dei parametri sono resi inefficaci. Per affrontare questo problema, sviluppiamo un metodo di regolarizzazione di rumore modello-agnostico per CDE che aggiunge perturbazioni casuali ai dati durante l'addestramento.Dimostriamo che l'approccio proposto corrisponde a una regolarizzazione di morbidezza e dimostriamo la sua consistenza asintotica.Nei nostri esperimenti, la regolarizzazione di rumore supera significativamente e costantemente altri metodi di regolarizzazione attraverso sette set di dati e tre modelli CDE.L'efficacia della regolarizzazione di rumore fa la rete neurale basata CDE il metodo preferibile sopra i precedenti approcci non e semi-parametrici, anche quando i dati di formazione sono scarsi.
Una tecnica promettente per l'apprendimento non supervisionato è il quadro di Variational Auto-encoders (VAEs).Tuttavia, le rappresentazioni non supervisionate apprese da VAEs sono significativamente superate da quelle apprese dalla supervisione per il riconoscimento.La nostra ipotesi è che per imparare rappresentazioni utili per il riconoscimento il modello deve essere incoraggiato ad apprendere i modelli ripetitivi e coerenti nei dati. Traendo ispirazione dal lavoro di scoperta della rappresentazione di medio livello, proponiamo PatchVAE, che ragiona sulle immagini a livello di patch.Il nostro contributo chiave è una formulazione del collo di bottiglia in una struttura VAE che incoraggia le rappresentazioni di stile di medio livello.I nostri esperimenti dimostrano che le rappresentazioni apprese dal nostro metodo eseguono molto meglio sui compiti di riconoscimento rispetto a quelle apprese da VAE vaniglia.
I gradienti che svaniscono ed esplodono sono due dei principali ostacoli nell'addestramento delle reti neurali profonde, specialmente nel catturare le dipendenze a lungo raggio nelle reti neurali ricorrenti (RNNs).In questo articolo, presentiamo una parametrizzazione efficiente della matrice di transizione di una RNN che ci permette di stabilizzare i gradienti che si presentano nel suo addestramento. In particolare, parametrizziamo la matrice di transizione attraverso la sua decomposizione dei valori singolari (SVD), che ci permette di tracciare e controllare esplicitamente i suoi valori singolari. Raggiungiamo l'efficienza utilizzando strumenti che sono comuni nell'algebra lineare numerica, in particolare i riflettori di Householder per rappresentare le matrici ortogonali che sorgono nella SVD. Controllando esplicitamente i valori singolari, il nostro metodo svdRNN proposto ci permette di risolvere facilmente il problema del gradiente che esplode e osserviamo che risolve empiricamente il problema del gradiente che svanisce in larga misura.Notiamo che la parametrizzazione SVD può essere usata per qualsiasi matrice di peso rettangolare, quindi può essere facilmente estesa a qualsiasi rete neurale profonda, come un perceptron multistrato. Teoricamente, dimostriamo che la nostra parametrizzazione non perde alcun potere espressivo, e mostriamo come potenzialmente rende il processo di ottimizzazione più facile. I nostri ampi risultati sperimentali dimostrano anche che il quadro proposto converge più velocemente, e ha una buona generalizzazione, specialmente quando la profondità è grande. 
Per trasferire lo stile di un'immagine arbitraria a un'immagine di contenuto, questi metodi hanno usato una rete feed-forward con un trasformatore di caratteristiche a scala più bassa o una cascata di reti con un trasformatore di caratteristiche a scala corrispondente. Tuttavia, i loro approcci non hanno considerato né lo stile multi-scala nel loro trasformatore di caratteristiche a scala singola né la dipendenza tra le statistiche delle caratteristiche trasformate attraverso le reti a cascata. Per superare questa limitazione del trasferimento parziale dello stile, proponiamo un metodo di trasferimento totale dello stile che trasferisce le statistiche delle caratteristiche multi-scala attraverso un singolo processo feed-forward. In primo luogo, il nostro metodo trasforma le mappe di caratteristiche multi-scala di un'immagine di contenuto in quelle di un'immagine di stile di destinazione considerando sia le correlazioni inter-canale in ogni singola mappa di caratteristiche in scala sia le correlazioni inter-scala tra le mappe di caratteristiche multi-scala. In secondo luogo, ogni mappa di caratteristiche trasformata viene inserita nel livello di decodifica della scala corrispondente utilizzando la connessione a skip. Infine, le mappe di caratteristiche multi-scala collegate a skip sono decodificate in un'immagine stilizzata attraverso la nostra rete di decodifica addestrata.
Una sfida per gli agenti di dialogo è riconoscere i sentimenti nel partner di conversazione e rispondere di conseguenza, un'abilità comunicativa chiave che è banale per gli esseri umani. La ricerca in quest'area è resa difficile dalla scarsità di set di dati adatti disponibili pubblicamente sia per le emozioni che per i dialoghi. I nostri esperimenti indicano che i modelli di dialogo che utilizzano il nostro set di dati sono percepiti come più empatici dai valutatori umani, migliorando anche altre metriche (ad esempio la rilevanza percepita delle risposte, i punteggi BLEU), rispetto ai modelli semplicemente addestrati sui dati di conversazione su larga scala di Internet.Presentiamo anche confronti empirici di diversi modi per migliorare le prestazioni di un dato modello sfruttando modelli o set di dati esistenti senza richiedere un lungo ri-addestramento del modello completo.
La causalità di Granger è un criterio ampiamente utilizzato per l'analisi delle interazioni nelle reti su larga scala. Poiché la maggior parte delle interazioni fisiche sono intrinsecamente non lineari, consideriamo il problema di dedurre l'esistenza della causalità di Granger a coppie tra processi stocastici non linearmente interagenti dalle loro misure di serie temporali. Il nostro approccio proposto si basa sulla modellazione delle non linearità incorporate nelle misurazioni utilizzando un modello di predizione delle serie temporali basato su Unità Ricorrenti Statistiche (SRU), in base al quale la topologia della rete delle relazioni causali di Granger è direttamente deducibile da una stima strutturata dei parametri interni delle reti SRU addestrate per prevedere le misure delle serie temporali dei processi. Proponiamo una variante della SRU, chiamata economy-SRU, che, per progettazione, ha un numero considerevolmente inferiore di parametri addestrabili, e quindi meno incline all'overfitting. La economy-SRU calcola uno schizzo bidimensionale del suo stato nascosto ad alta densità sotto forma di proiezioni casuali per generare il feedback per la sua elaborazione ricorrente. Inoltre, i parametri di peso interni dell'economia-SRU sono strategicamente regolarizzati in un modo group-wise per facilitare la rete proposta nell'estrazione di caratteristiche predittive significative che sono altamente localizzate nel tempo per imitare gli eventi causali del mondo reale. Vasti esperimenti sono condotti per dimostrare che il modello proposto di previsione delle serie temporali basato sull'economia-SRU supera i modelli di serie temporali basati su MLP, LSTM e attenzione-gated CNN considerati precedentemente per dedurre la causalità di Granger.
Le reti convoluzionali a grafo (GCNs) sono potenti reti neurali profonde per i dati strutturati a grafo. Tuttavia, GCN calcola la rappresentazione dei nodi in modo ricorsivo dai loro vicini, facendo crescere esponenzialmente la dimensione del campo ricettivo con il numero di strati.  I tentativi precedenti di ridurre la dimensione del campo ricettivo tramite il sottocampionamento dei vicini non hanno alcuna garanzia di convergenza, e la loro dimensione del campo ricettivo per nodo è ancora nell'ordine delle centinaia.In questo articolo, sviluppiamo una strategia di pre-elaborazione e due algoritmi basati sulle varianti di controllo per ridurre ulteriormente la dimensione del campo ricettivo. I nostri algoritmi sono garantiti per convergere all'optimum locale di GCN indipendentemente dalla dimensione del campionamento dei vicini.I risultati empirici mostrano che i nostri algoritmi hanno una velocità di convergenza simile per epoca con l'algoritmo esatto anche usando solo due vicini per nodo.Il consumo di tempo del nostro algoritmo sul dataset Reddit è solo un quinto dei precedenti algoritmi di campionamento dei vicini.
I pesi e le attivazioni intere a bassa larghezza di bit sono molto importanti per un'inferenza efficiente, specialmente per quanto riguarda il consumo di energia più basso.Proponiamo di applicare i metodi Monte Carlo e il campionamento di importanza per sparsificare e quantizzare le reti neurali pre-addestrate senza alcun retraining.Otteniamo rappresentazioni intere sparse e a bassa larghezza di bit che approssimano i pesi e le attivazioni a piena precisione. Il nostro approccio, chiamato Monte Carlo Quantization (MCQ), è lineare sia nel tempo che nello spazio, mentre le reti rade quantizzate risultanti mostrano una perdita minima di accuratezza rispetto alle reti originali a piena precisione. Il nostro metodo supera o raggiunge risultati competitivi con i metodi che richiedono ulteriore formazione su una varietà di compiti difficili.
A differenza del framework Variational Autoencoder, IMAE parte da un codificatore stocastico che cerca di mappare ogni dato di input in una rappresentazione ibrida discreta e continua con l'obiettivo di massimizzare l'informazione reciproca tra i dati e le loro rappresentazioni.Un decodificatore è incluso per approssimare la distribuzione posteriore dei dati date le loro rappresentazioni, dove un'approssimazione di alta fedeltà può essere raggiunta sfruttando le rappresentazioni informative.     Mostriamo che l'obiettivo proposto è teoricamente valido e fornisce un quadro di principio per comprendere i compromessi riguardanti l'informatività di ogni fattore di rappresentazione, il disinserimento delle rappresentazioni e la qualità della decodifica.
La maggior parte delle tecniche di regolarizzazione sono concettualizzate e implementate nello spazio dei parametri, ma è anche possibile regolarizzare nello spazio delle funzioni. Qui proponiamo di misurare le reti in uno spazio di Hilbert $L^2$ e di testare una regola di apprendimento che regolarizza la distanza che una rete può percorrere nello spazio $L^2$ ad ogni aggiornamento.  Questo approccio si ispira al lento movimento della discesa del gradiente attraverso lo spazio dei parametri e al gradiente naturale, che può essere derivato da un termine di regolarizzazione al cambio di funzione. La regola di apprendimento risultante, che chiamiamo Hilbert-constrained gradient descent (HCGD), è quindi strettamente legata al gradiente naturale ma regolarizza una metrica diversa e più calcolabile sullo spazio delle funzioni.
La ricerca su SGD è risorta di recente nell'apprendimento automatico per l'ottimizzazione di funzioni di perdita convesse e per l'addestramento di reti neurali profonde non convesse.La teoria presuppone che si possa facilmente calcolare uno stimatore imparziale del gradiente, il che è solitamente il caso a causa della natura media del campione della minimizzazione empirica del rischio.Esistono, tuttavia, molti scenari (ad es, grafici) in cui uno stimatore imparziale può essere costoso da calcolare come il gradiente completo perché gli esempi di formazione sono interconnessi.Recentemente, Chen et al. (2018) hanno proposto di utilizzare uno stimatore del gradiente coerente come alternativa economica.Incoraggiati dal successo empirico, mostriamo, in un ambiente generale, che gli stimatori coerenti risultano nello stesso comportamento di convergenza di quelli imparziali. La nostra analisi copre obiettivi fortemente convessi, convessi e non convessi.Verifichiamo i risultati con esperimenti illustrativi su dati sintetici e reali.Questo lavoro apre diverse nuove direzioni di ricerca, tra cui lo sviluppo di aggiornamenti SGD più efficienti con stimatori coerenti e la progettazione di algoritmi di formazione efficienti per grafici su larga scala.
Consideriamo il problema della stima dell'incertezza nel contesto della classificazione neurale profonda (non bayesiana).In questo contesto, tutti i metodi conosciuti si basano sull'estrazione di segnali di incertezza da una rete addestrata ottimizzata per risolvere il problema di classificazione a portata di mano.Dimostriamo che tali tecniche tendono a introdurre stime distorte per le istanze le cui previsioni dovrebbero essere altamente sicure.Sosteniamo che questa carenza è un artefatto della dinamica di addestramento con ottimizzatori simili a SGD, e ha alcune proprietà simili all'overfitting. Sulla base di questa osservazione, sviluppiamo un algoritmo di stima dell'incertezza che stima selettivamente l'incertezza dei punti ad alta confidenza, utilizzando le prime istantanee del modello addestrato, prima che le loro stime siano jitterate (e molto prima che siano pronte per la classificazione effettiva).
I modelli addestrati da tali set di dati soffrono di una precisione di classificazione incoerente, che limita l'applicabilità dei sistemi di analisi del volto ai gruppi di razza non bianca. Per mitigare il problema della distorsione della razza in questi set di dati, abbiamo costruito un nuovo set di dati di immagini del volto contenente 108.501 immagini che è equilibrato sulla razza: Le immagini sono state raccolte dal dataset YFCC-100M di Flickr ed etichettate con gruppi di razza, sesso ed età. Le valutazioni sono state effettuate su dataset di attributi facciali esistenti e su nuovi dataset di immagini per misurare le prestazioni di generalizzazione. Troviamo che il modello addestrato dal nostro set di dati è sostanzialmente più accurato sui nuovi set di dati e l'accuratezza è coerente tra i gruppi di razza e sesso. Confrontiamo anche diverse API commerciali di computer vision e riportiamo la loro accuratezza bilanciata tra i gruppi di sesso, razza ed età.
Nonostante tali progressi, una comprensione di livello superiore della visione e delle immagini non deriva dalla modellazione esaustiva di un oggetto, ma piuttosto dall'identificazione di attributi di livello superiore che riassumono al meglio gli aspetti di un oggetto.   In questo lavoro tentiamo di modellare il processo di disegno dei caratteri costruendo modelli generativi sequenziali di grafica vettoriale.  Questo modello ha il vantaggio di fornire una rappresentazione invariante in scala per le immagini la cui rappresentazione latente può essere sistematicamente manipolata e sfruttata per eseguire la propagazione dello stile. Dimostriamo questi risultati su un grande set di dati di caratteri ed evidenziamo come un tale modello catturi le dipendenze statistiche e la ricchezza di questo set di dati.
Cosa possiamo imparare sull'organizzazione funzionale dei microcircuiti corticali dalle registrazioni su larga scala dell'attività neurale?  Per ottenere un modello esplicito e interpretabile delle connessioni funzionali dipendenti dal tempo tra i neuroni e per stabilire la dinamica del flusso di informazioni corticali, sviluppiamo una "inferenza relazionale neurale dinamica" (dNRI). Studiamo sia dati di spiking neurali sintetici che del mondo reale e dimostriamo che il metodo sviluppato è in grado di scoprire le relazioni dinamiche tra i neuroni in modo più affidabile rispetto alle basi esistenti.
DeePa è un framework di deep learning che esplora il parallelismo in tutte le dimensioni parallelizzabili per accelerare il processo di formazione delle reti neurali convoluzionali.DeePa ottimizza il parallelismo alla granularità di ogni singolo strato della rete.Presentiamo un algoritmo basato sull'eliminazione che trova una configurazione di parallelismo ottimale per ogni strato.La nostra valutazione mostra che DeePa raggiunge fino a 6,5× di accelerazione rispetto allo stato dell'arte dei framework di deep learning e riduce i trasferimenti di dati fino a 23×.
Si può sostituire ogni neurone in qualsiasi rete neurale con una macchina kernel e ottenere una controparte alimentata da macchine kernel.La nuova rete eredita la potenza espressiva e l'architettura dell'originale ma funziona in modo più intuitivo poiché ogni nodo gode della semplice interpretazione come un iperpiano (in uno spazio di Hilbert kernel riproducente).Inoltre, utilizzando il perceptron multistrato kernel come esempio, dimostriamo che in classificazione, una rappresentazione ottimale che minimizza il rischio della rete può essere caratterizzata per ogni strato nascosto. Questo risultato elimina la necessità di backpropagation nell'apprendimento del modello e può essere generalizzato a qualsiasi rete feedforward kernel.Inoltre, a differenza della backpropagation, che trasforma i modelli in scatole nere, la rappresentazione nascosta ottimale gode di un'interpretazione geometrica intuitiva, rendendo le dinamiche di apprendimento in una rete kernel profonda semplici da comprendere.Vengono forniti risultati empirici per convalidare la nostra teoria.
Molte nozioni di equità possono essere espresse come vincoli lineari, e l'obiettivo vincolato risultante è spesso ottimizzato trasformando il problema nella sua duale lagrangiana con penalità lineari additive.Nelle impostazioni non convesse, il problema risultante può essere difficile da risolvere poiché la lagrangiana non è garantita per avere un equilibrio deterministico a punto di sella.  In questo articolo, proponiamo di modificare le penalità lineari in penalità del secondo ordine, e sosteniamo che questo si traduce in una procedura di addestramento più pratica in ambienti non convessi e con grandi dati. Per esempio, l'uso di penalità del secondo ordine permette di addestrare l'obiettivo penalizzato con un valore fisso del coefficiente di penalità, evitando così l'instabilità e la potenziale mancanza di convergenza associata ai giochi min-max a due giocatori. In secondo luogo, deriviamo un metodo per calcolare in modo efficiente i gradienti associati alle penalità del secondo ordine in impostazioni stocastiche mini-batch. Il nostro algoritmo risultante si comporta bene empiricamente, imparando un classificatore adeguatamente equo su una serie di benchmark standard.
I metodi di addestramento per le reti profonde sono principalmente varianti della discesa del gradiente stocastico.  Le tecniche che utilizzano informazioni di secondo ordine (approssimate) sono raramente utilizzate a causa del costo computazionale e del rumore associato a questi approcci in contesti di apprendimento profondo.  Tuttavia, in questo articolo, mostriamo come le reti profonde feedforward mostrano una struttura derivata di basso rango.  Questa struttura a basso rango rende possibile l'uso di informazioni del secondo ordine senza bisogno di approssimazioni e senza incorrere in un costo computazionale significativamente maggiore rispetto alla discesa del gradiente.  Per dimostrare questa capacità, implementiamo Cubic Regularization (CR) su una rete profonda feedforward con discesa del gradiente stocastico e due delle sue varianti.  Lì, usiamo CR per calcolare i tassi di apprendimento su una base per iterazione mentre ci addestriamo sui dataset MNIST e CIFAR-10.  CR ha dimostrato un particolare successo nel fuggire dalle regioni di plateau della funzione obiettivo.  Abbiamo anche scoperto che questo approccio richiede meno informazioni specifiche del problema (ad esempio un tasso di apprendimento iniziale ottimale) rispetto ad altri metodi del primo ordine per poter funzionare bene.
Il principio di formazione del caso peggiore che minimizza la massima perdita avversaria, noto anche come formazione avversaria (AT), ha dimostrato di essere un approccio all'avanguardia per migliorare la robustezza avversaria contro le perturbazioni di input delimitate da una palla di norme. In particolare, dato un insieme di fonti di rischio (domini), la minimizzazione della perdita massima indotta dall'insieme del dominio può essere riformulata come un problema generale di min-max che è diverso dall'AT. Esempi di questa formulazione generale includono l'attacco di insiemi di modelli, l'ideazione di perturbazioni universali sotto input multipli o trasformazioni di dati, e l'AT generalizzato su diversi tipi di modelli di attacco.  Mostriamo anche che i pesi di dominio autoregolati appresi dal nostro metodo forniscono un mezzo per spiegare il livello di difficoltà dell'attacco e della difesa su domini multipli.
La maggior parte dei modelli di apprendimento profondo si basano su rappresentazioni espressive ad alta densità per ottenere buone prestazioni in compiti come la classificazione. Tuttavia, l'alta dimensionalità di queste rappresentazioni le rende difficili da interpretare e inclini all'over-fitting. Noi proponiamo un quadro di riduzione delle dimensioni semplice, intuitivo e scalabile che tiene conto dell'interpretazione probabilistica morbida dei modelli profondi standard per la classificazione. Quando si applica la nostra struttura alla visualizzazione, le nostre rappresentazioni riflettono più accuratamente le distanze inter-classe rispetto alle tecniche di visualizzazione standard come t-SNE.Mostriamo sperimentalmente che la nostra struttura migliora le prestazioni di generalizzazione alle categorie non viste nell'apprendimento zero-shot.Forniamo anche una garanzia di errore campione finito al limite superiore per il metodo.
Gli algoritmi di esplorazione degli obiettivi intrinsecamente motivati permettono alle macchine di scoprire repertori di politiche che producono una diversità di effetti in ambienti complessi.Questi algoritmi di esplorazione hanno dimostrato di permettere ai robot del mondo reale di acquisire competenze come l'uso degli strumenti in spazi di stato e d'azione continui ad alta densità.Tuttavia, finora hanno assunto che gli obiettivi auto-generati siano campionati in uno spazio di caratteristiche specificamente progettato, limitando la loro autonomia.In questo lavoro, proponiamo un approccio che utilizza algoritmi di apprendimento di rappresentazione profonda per imparare uno spazio di obiettivi adeguato. Si tratta di un approccio evolutivo in 2 fasi: in primo luogo, in una fase di apprendimento percettivo, gli algoritmi di apprendimento profondo utilizzano le osservazioni passive dei sensori grezzi dei cambiamenti del mondo per imparare uno spazio latente corrispondente; poi l'esplorazione degli obiettivi avviene in una seconda fase campionando gli obiettivi in questo spazio latente.
Una delle principali sfide dei metodi di deep learning è la scelta di un'appropriata strategia di training.In particolare, è stato dimostrato che passi aggiuntivi, come il pre-training non supervisionato, migliorano notevolmente le prestazioni delle strutture profonde.In questo articolo, proponiamo un passo extra di training, chiamato post-training, che ottimizza solo l'ultimo strato della rete. Mostriamo che questa procedura può essere analizzata nel contesto della teoria dei kernel, con i primi strati che calcolano un'incorporazione dei dati e l'ultimo strato un modello statistico per risolvere il compito basato su questa incorporazione.Questo passo fa sì che l'incorporazione, o rappresentazione, dei dati sia usata nel miglior modo possibile per il compito considerato.Questa idea viene poi testata su più architetture con vari set di dati, mostrando che fornisce costantemente un aumento delle prestazioni.
I modelli di elaborazione del linguaggio naturale (NLP) spesso richiedono un numero massiccio di parametri per le embeddature delle parole, con conseguente grande memoria o impronta di memoria.L'implementazione di modelli NLP neurali su dispositivi mobili richiede la compressione delle embeddature delle parole senza sacrifici significativi nelle prestazioni.A questo scopo, proponiamo di costruire le embeddature con pochi vettori base.Per ogni parola, la composizione dei vettori base è determinata da un codice hash.Per massimizzare il tasso di compressione, adottiamo l'approccio multi-codebook quantization invece dello schema di codifica binario. Ogni codice è composto da più numeri discreti, come (3, 2, 1, 8), dove il valore di ogni componente è limitato a un intervallo fisso.Proponiamo di imparare direttamente i codici discreti in una rete neurale end-to-end applicando il trucco Gumbel-softmax.Gli esperimenti mostrano che il tasso di compressione raggiunge il 98% in un compito di analisi del sentimento e 94% ~ 99% in compiti di traduzione automatica senza perdita di prestazioni. In entrambi i compiti, il metodo proposto può migliorare le prestazioni del modello abbassando leggermente il tasso di compressione.Rispetto ad altri approcci come la segmentazione a livello di carattere, il metodo proposto è indipendente dalla lingua e non richiede modifiche all'architettura della rete.
L'uso di input più grandi e più complessi nell'apprendimento profondo ingrandisce la difficoltà di distinguere tra esempi anomali e in-distribuzione. Allo stesso tempo, diverse immagini e dati di testo sono disponibili in quantità enormi. Proponiamo di sfruttare questi dati per migliorare il rilevamento delle anomalie profonde allenando i rilevatori di anomalie contro un set di dati ausiliario di outlier, un approccio che chiamiamo Outlier Exposure (OE). In ampi esperimenti sull'elaborazione del linguaggio naturale e su compiti di visione su piccola e grande scala, troviamo che l'esposizione di outlier migliora significativamente le prestazioni di rilevamento. osserviamo anche che i modelli generativi all'avanguardia addestrati su CIFAR-10 possono assegnare maggiori probabilità alle immagini SVHN rispetto alle immagini CIFAR-10; usiamo OE per mitigare questo problema. analizziamo anche la flessibilità e la robustezza di Outlier Exposure e identifichiamo le caratteristiche del dataset ausiliario che migliorano le prestazioni.
Per esempio, per ingannare il discriminatore, una rete neurale generativa (GAN) addestrata esclusivamente a trasformare immagini di *uomini* dai capelli neri in *uomini* dai capelli biondi dovrebbe cambiare le caratteristiche legate al genere e il colore dei capelli quando riceve in input immagini di *donne* dai capelli neri. Questo è problematico, poiché spesso è possibile ottenere *una* coppia di distribuzioni (sorgente, destinazione) ma poi avere una seconda distribuzione sorgente in cui la distribuzione di destinazione è sconosciuta.La sfida computazionale è che i modelli generativi sono bravi a generare all'interno del collettore dei dati su cui sono addestrati.Tuttavia, generare nuovi campioni fuori dal collettore o estrapolare "fuori dal campione" è un problema molto più difficile che è stato studiato meno bene. Per affrontare questo problema, introduciamo una tecnica chiamata *neuron editing* che impara come i neuroni codificano una modifica per una particolare trasformazione in uno spazio latente.Usiamo un autoencoder per decomporre la variazione all'interno del dataset in attivazioni di diversi neuroni e generare dati trasformati definendo una trasformazione di editing su quei neuroni. Eseguendo la trasformazione in uno spazio latente addestrato, codifichiamo trasformazioni abbastanza complesse e non lineari ai dati con spostamenti di distribuzione molto più semplici alle attivazioni dei neuroni.La nostra tecnica è generale e funziona su un'ampia varietà di domini di dati e applicazioni.La dimostriamo prima sulle trasformazioni delle immagini e poi passiamo alle nostre due principali applicazioni biologiche: la rimozione di artefatti di batch che rappresentano rumore indesiderato e la modellazione dell'effetto dei trattamenti farmacologici per prevedere la sinergia tra farmaci.
In questo articolo, proponiamo di combinare l'imitazione e l'apprendimento di rinforzo attraverso l'idea di modellare la ricompensa usando un oracolo. Studiamo l'efficacia dell'oracolo quasi ottimale cost-to-go sull'orizzonte di pianificazione e dimostriamo che l'oracolo cost-to-go accorcia l'orizzonte di pianificazione del discente in funzione della sua accuratezza: un oracolo globalmente ottimale può accorciare l'orizzonte di pianificazione a uno, portando a un processo decisionale di Markov greedy a un passo che è molto più facile da ottimizzare, mentre un oracolo che è lontano dall'ottimalità richiede una pianificazione su un orizzonte più lungo per raggiungere prestazioni quasi ottimali. Quindi la nostra nuova intuizione colma il divario e interpola tra l'apprendimento per imitazione e l'apprendimento per rinforzo. Motivati dalle intuizioni di cui sopra, proponiamo Truncated HORizon Policy Search (THOR), un metodo che si concentra sulla ricerca di politiche che massimizzano la ricompensa totale rimodellata su un orizzonte di pianificazione finito quando l'oracolo è sub-ottimale. dimostriamo sperimentalmente che un'implementazione basata sul gradiente di THOR può raggiungere prestazioni superiori rispetto alle linee di base RL e IL anche quando l'oracolo è sub-ottimale.
Recentemente, le Reti Generative Adversariali (GANs) sono emerse come un'alternativa popolare per la modellazione di complesse distribuzioni ad alta dimensione.La maggior parte dei lavori esistenti presuppone implicitamente che i campioni puliti dalla distribuzione di destinazione siano facilmente disponibili.Tuttavia, in molte applicazioni, questo presupposto è violato. In questo articolo, consideriamo l'impostazione di osservazione in cui i campioni di una distribuzione di destinazione sono dati dalla sovrapposizione di due componenti strutturate, e sfruttiamo le GAN per l'apprendimento della struttura delle componenti.Proponiamo un nuovo quadro, demixing-GAN, che impara la distribuzione di due componenti allo stesso tempo.Attraverso ampi esperimenti numerici, dimostriamo che il quadro proposto può generare campioni puliti da distribuzioni sconosciute, che inoltre possono essere utilizzati nella demiscelazione delle immagini di prova non viste.
I flussi normalizzanti (NF) sono una classe di modelli generativi basati sulla verosimiglianza che hanno recentemente guadagnato popolarità e si basano sull'idea di trasformare una densità semplice in quella dei dati. Cerchiamo di capire meglio questa classe di modelli e come si confrontano con le tecniche precedentemente proposte per la modellazione generativa e l'apprendimento non supervisionato della rappresentazione. Utilizzando il nostro modello unificato, esaminiamo sistematicamente lo spazio del modello tra flussi, autocodificatori variazionali e autocodificatori denoising, in una serie di esperimenti preliminari sulle cifre scritte a mano MNIST. Gli esperimenti fanno luce sulle ipotesi di modellazione implicite in questi modelli e suggeriscono molteplici nuove direzioni per la ricerca futura in questo spazio.
Indaghiamo i metodi per apprendere in modo efficiente diverse strategie nell'apprendimento di rinforzo per un problema generativo di previsione strutturata: la riformulazione della query.Nel quadro proposto, un agente consiste in più sub-agenti specializzati e un meta-agente che impara ad aggregare le risposte dei sub-agenti per produrre una risposta finale. I sub-agenti sono addestrati su partizioni disgiunte dei dati di formazione, mentre il meta-agente è addestrato sull'intero set di formazione.Il nostro metodo rende l'apprendimento più veloce, perché è altamente parallelizzabile, e ha migliori prestazioni di generalizzazione rispetto alle linee di base forti, come un insieme di agenti addestrati sui dati completi.Valutiamo sui compiti di recupero dei documenti e risposta alle domande. Questo suggerisce che gli approcci multi-agente e gerarchici potrebbero giocare un ruolo importante nei compiti di previsione strutturati di questo tipo. Tuttavia, troviamo anche che non è ovvio come caratterizzare la diversità in questo contesto, e un primo tentativo basato sul clustering non ha prodotto buoni risultati. Inoltre, l'apprendimento di rinforzo per il compito di riformulazione è difficile nei regimi ad alte prestazioni.
Studiamo problemi di apprendimento di rinforzo ad azione continua in cui è cruciale che l'agente interagisca con l'ambiente solo attraverso politiche sicure, cioè politiche che mantengono l'agente in situazioni desiderabili, sia durante la formazione che alla convergenza. I nostri algoritmi possono utilizzare qualsiasi metodo standard di gradiente della politica (PG), come il gradiente deterministico profondo della politica (DDPG) o l'ottimizzazione prossimale della politica (PPO), per addestrare una politica di rete neurale, garantendo nel contempo la quasi soddisfazione dei vincoli per ogni aggiornamento della politica proiettando il parametro della politica o l'azione selezionata sull'insieme delle soluzioni fattibili indotte dai vincoli di Lyapunov linearizzati dipendenti dallo stato. Rispetto agli algoritmi di PG vincolati esistenti, i nostri sono più efficienti dal punto di vista dei dati in quanto sono in grado di utilizzare sia i dati on-policy che quelli off-policy. Inoltre, il nostro algoritmo di proiezione dell'azione spesso porta ad aggiornamenti meno conservativi della politica e permette un'integrazione naturale in una pipeline di formazione di PG end-to-end. Valutiamo i nostri algoritmi e li confrontiamo con lo stato dell'arte su diversi compiti simulati (MuJoCo), così come un problema reale di evitamento degli ostacoli dei robot, dimostrando la loro efficacia in termini di bilanciamento delle prestazioni e soddisfazione dei vincoli.
Le reti generative sono note per essere difficili da valutare.I recenti lavori sui modelli generativi, specialmente sulle reti generative avversarie, producono bei campioni di varie categorie di immagini.Ma la validazione della loro qualità dipende molto dal metodo usato.Un buon generatore dovrebbe generare dati che contengono informazioni significative e varie e che si adattano alla distribuzione di un set di dati. Il nostro approccio si basa sull'addestramento di un classificatore con una miscela di campioni reali e generati: addestriamo un modello generativo su un set di formazione etichettato, poi usiamo questo modello generativo per campionare nuovi punti di dati che mescoliamo con i dati di formazione originali. Questa miscela di dati reali e generati viene quindi utilizzata per addestrare un classificatore che viene poi testato su un dato set di dati di prova etichettati.Confrontiamo questo risultato con il punteggio dello stesso classificatore addestrato sui dati di formazione reali mescolati con rumore.Calcolando l'accuratezza del classificatore con diversi rapporti di campioni da entrambe le distribuzioni (reali e generati) siamo in grado di valutare se il generatore si adatta con successo ed è in grado di generalizzare la distribuzione del dataset.I nostri esperimenti confrontano il risultato di diversi generatori dal quadro VAE e GAN su MNIST e fashion MNIST dataset.
Proponiamo Automating Science Journalism (ASJ), il processo di produzione di un comunicato stampa da un articolo scientifico, come un nuovo compito che può servire come un nuovo punto di riferimento per la sintesi neurale astrattiva.ASJ è un compito impegnativo in quanto richiede lunghi testi sorgente da riassumere in lunghi testi di destinazione, mentre anche la parafrasi di concetti scientifici complessi per essere compresi dal pubblico generale.Per questo scopo, introduciamo un set di dati specializzato per ASJ che contiene articoli scientifici e i loro comunicati stampa da Science Daily. Mentre lo stato dell'arte dei modelli sequence-to-sequence (seq2seq) potrebbe facilmente generare comunicati stampa convincenti per ASJ, questi sono generalmente non fattuali e si discostano dalla fonte.Per affrontare questo problema, miglioriamo la generazione di seq2seq attraverso il transfer learning co-addestramento con nuovi obiettivi:(i) abstract scientifici delle fonti e(ii) comunicati stampa partizionati. La nostra valutazione quantitativa e qualitativa mostra notevoli miglioramenti rispetto a una linea di base forte, suggerendo che il quadro proposto potrebbe migliorare la sintesi seq2seq oltre ASJ.
Un approccio alternativo per garantire che l'agente generi un comportamento interpretabile sarebbe quello di progettare l'ambiente dell'agente in modo tale che i comportamenti non interpretabili siano proibitivamente costosi o non disponibili per l'agente.Fino ad oggi, c'è stato un lavoro sotto l'ombrello della progettazione del riconoscimento degli obiettivi o dei piani che ha esplorato questa nozione di riprogettazione dell'ambiente in alcune istanze specifiche di comportamento interpretabile. In particolare, ci concentriamo su tre tipi specifici di comportamenti interpretabili - spiegabilità, leggibilità e prevedibilità - e presentiamo una struttura generale per il problema della progettazione dell'ambiente che può essere istanziata per realizzare ciascuno dei tre comportamenti interpretabili.
I modelli di inferenza, che sostituiscono una procedura di inferenza basata sull'ottimizzazione con un modello appreso, sono stati fondamentali nell'avanzamento dell'apprendimento profondo bayesiano, l'esempio più notevole essendo autocodificatori variazionali (VAEs).In questo articolo, proponiamo modelli di inferenza iterativi, che imparano come ottimizzare un limite inferiore variazionale attraverso la codifica ripetuta dei gradienti. Il nostro approccio generalizza VAEs in determinate condizioni, e vedendo VAEs nel contesto dell'inferenza iterativa, forniamo ulteriore comprensione in parecchi risultati empirici recenti. dimostriamo le capacità di ottimizzazione di inferenza dei modelli iterativi di inferenza, esploriamo gli aspetti unici di questi modelli e mostriamo che superano i modelli standard di inferenza sui set di dati tipici del benchmark.
Nelle reti neurali artificiali addestrate con la discesa del gradiente, i pesi usati per l'elaborazione degli stimoli sono anche usati durante i passaggi a ritroso per calcolare i gradienti. Per il cervello reale per approssimare i gradienti, le informazioni sul gradiente dovrebbero essere propagate separatamente, in modo che un set di pesi sinaptici sia usato per l'elaborazione e un altro set sia usato per i passaggi a ritroso. Questo produce il cosiddetto "problema del trasporto dei pesi" per i modelli biologici di apprendimento, dove i pesi all'indietro utilizzati per calcolare i gradienti devono rispecchiare i pesi in avanti utilizzati per elaborare gli stimoli. Questo problema del trasporto dei pesi è stato considerato così difficile che le proposte popolari per l'apprendimento biologico assumono che i pesi all'indietro siano semplicemente casuali, come nell'algoritmo di allineamento di feedback. Tuttavia, tali pesi casuali non sembrano funzionare bene per reti di grandi dimensioni. Qui mostriamo come la discontinuità introdotta in un sistema di spike può portare a una soluzione a questo problema. L'algoritmo risultante è un caso speciale di uno stimatore utilizzato per l'inferenza causale in econometria, il disegno di discontinuità di regressione. Man mano che i pesi all'indietro diventano corretti, questo migliora le prestazioni di apprendimento rispetto all'allineamento di feedback su compiti come Fashion-MNIST e CIFAR-10. I nostri risultati dimostrano che una semplice regola di apprendimento in una rete di spie può permettere ai neuroni di produrre le giuste connessioni all'indietro e quindi risolvere il problema del trasporto dei pesi.
I metodi di inferenza variazionale (VI) e specialmente gli autocodificatori variazionali (VAE) specificano modelli generativi scalabili che godono di una connessione intuitiva con l'apprendimento manifold --- con molti priori predefiniti la coppia posterior/likelihood $q(z|x)$/$p(x|z)$ può essere vista come un omeomorfismo approssimativo (e il suo inverso) tra il manifold di dati e uno spazio euclideo latente. Tuttavia, queste approssimazioni sono ben documentate per diventare degenerate durante l'addestramento. A meno che il priore soggettivo sia scelto con cura, le topologie delle distribuzioni del priore e dei dati spesso non corrispondono. Al contrario, le mappe di diffusione (DM) \textit{inferiscono automaticamente la topologia dei dati e godono di una connessione rigorosa con l'apprendimento di manifold, ma non scalano facilmente o forniscono l'omeomorfismo inverso.In questo articolo, proponiamo una misura di principio per riconoscere il mismatch tra i dati e le distribuzioni latenti e un metodo che combina i vantaggi dell'inferenza variazionale e delle mappe di diffusione per imparare un modello generativo omeomorfo. La misura, la proprietà bi-Lipschitz locale, è una condizione sufficiente per un omeomorfismo ed è facile da calcolare e interpretare. (VDAE), è un nuovo algoritmo generativo che prima infonde la topologia della distribuzione dei dati, poi modella un cammino casuale di diffusione sui dati. Per ottenere un calcolo efficiente nei VDAE, usiamo versioni stocastiche sia dell'inferenza variazionale che dell'ottimizzazione dell'apprendimento del manifold. Dimostriamo risultati teorici di approssimazione per la dipendenza dalle dimensioni di VDAEs, e che il campionamento localmente isotropo nello spazio latente risulta in una passeggiata casuale sul manifold ricostruito.Infine, dimostriamo l'utilità del nostro metodo su vari set di dati reali e sintetici, e dimostriamo che esibisce prestazioni superiori ad altri modelli generativi.
Mentre l'apprendimento profondo e i sistemi profondi di apprendimento di rinforzo hanno dimostrato i risultati impressionanti nei domini quali la classificazione di immagine, il gioco e il controllo robotico, l'efficienza di dati rimane una sfida importante, specialmente poichÃ¨ questi algoritmi imparano i diversi compiti da zero. l'apprendimento del multi-task Ã¨ emerso come metodo promettente per la condivisione della struttura attraverso i compiti multipli per permettere l'apprendimento piÃ¹ efficiente. tuttavia, la regolazione del multi-task presenta una serie di sfide di ottimizzazione, rendente lo difficile realizzare i grandi guadagni di efficienza confrontati ai compiti imparanti indipendentemente. Motivati dall'intuizione che l'interferenza del gradiente causa sfide di ottimizzazione, sviluppiamo un approccio semplice e generale per evitare l'interferenza tra i gradienti di compiti diversi, alterando i gradienti attraverso una tecnica che chiamiamo "chirurgia del gradiente". Proponiamo una forma di chirurgia del gradiente che proietta il gradiente di un compito sul piano normale del gradiente di qualsiasi altro compito che ha un gradiente in conflitto.  Inoltre, può essere efficacemente combinato con le architetture multi-task proposte in precedenza per migliorare le prestazioni in modo modello-agnostico.
In questo articolo proponiamo di vedere il tasso di accettazione dell'algoritmo di Metropolis-Hastings come un obiettivo universale per l'apprendimento del campionamento dalla distribuzione di destinazione - dato sia come un insieme di campioni che sotto forma di densità non normalizzata. Questo punto di vista unifica gli obiettivi di approcci come Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), inferenza variazionale. Per rivelare la connessione deriviamo il limite inferiore del tasso di accettazione e lo trattiamo come l'obiettivo per l'apprendimento di campionatori espliciti e impliciti.La forma del limite inferiore permette un'ottimizzazione del gradiente doppiamente stocastica nel caso in cui la distribuzione target fattorizzi (cioè su punti dati).Validiamo empiricamente il nostro approccio sull'inferenza bayesiana per reti neurali e modelli generativi per immagini.
Il nostro metodo estende il modello BERT per sequenze di testo al caso di sequenze di vettori di caratteristiche con valore reale, sostituendo la perdita softmax con la stima contrastiva del rumore (NCE). Mostriamo anche come imparare rappresentazioni da sequenze di caratteristiche visive e sequenze di parole derivate dall'ASR (automatic speech recognition), e dimostriamo che tale formazione cross-modale (quando possibile) aiuta ancora di più.
Presentiamo un'architettura dinamica generica che impiega un meccanismo di biforcazione differenziabile specifico del problema per sfruttare le informazioni logiche discrete sulla struttura dei dati del problema. Adattiamo e applichiamo il nostro modello a CLEVR Visual Question Answering, dando origine all'architettura DDRprog; rispetto agli approcci precedenti, il nostro modello raggiunge un'accuratezza superiore nella metà delle epoche con cinque volte meno parametri apprendibili. Il nostro modello modella direttamente la logica della domanda sottostante usando un controller ricorrente che prevede ed esegue congiuntamente moduli neurali funzionali; biforca esplicitamente i sottoprocessi per gestire la ramificazione logica. Mentre FiLM e altri modelli competitivi sono architetture statiche con meno supervisione, noi sosteniamo che l'inclusione delle etichette del programma consente l'apprendimento di operazioni logiche di livello superiore - la nostra architettura raggiunge prestazioni particolarmente elevate su domande che richiedono il conteggio e il confronto tra interi. Dimostriamo ulteriormente la generalità del nostro approccio anche se DDRstack - un'applicazione del nostro metodo alla valutazione delle espressioni in notazione polacca inversa in cui l'inclusione di un presupposto dello stack permette al nostro approccio di generalizzarsi alle espressioni lunghe, superando significativamente un LSTM con dieci volte più parametri apprendibili.
Proponiamo il Support-guided Adversarial Imitation Learning (SAIL), una struttura generica di apprendimento per imitazione che unifica la stima del supporto della politica esperta con la famiglia di algoritmi Adversarial Imitation Learning (AIL).SAIL affronta due importanti sfide di AIL, tra cui la distorsione implicita della ricompensa e la potenziale instabilità della formazione. In una valutazione estesa, dimostriamo che il metodo proposto gestisce efficacemente la distorsione della ricompensa e raggiunge migliori prestazioni e stabilità di formazione rispetto ad altri metodi di base su una vasta gamma di compiti di controllo di riferimento.
Consideriamo il compito di predizione del collegamento di pochi colpi, in cui l'obiettivo Ã¨ di predire i bordi mancanti attraverso i grafici multipli usando soltanto un piccolo campione dei bordi conosciuti. Mostriamo che i metodi correnti di predizione del collegamento sono generalmente mal equipaggiati per trattare questo compito---perchÃ© non possono trasferire efficacemente la conoscenza fra i grafici in una regolazione del multi-grafo e non possono imparare efficacemente dai dati molto sparsi. Per affrontare questa sfida, introduciamo una nuova struttura di meta-apprendimento basata sul gradiente, Meta-Graph, che sfrutta i gradienti di ordine superiore insieme a una funzione di firma del grafico appresa che genera condizionatamente un'inizializzazione della rete neurale del grafico. utilizzando un nuovo set di benchmark di previsione dei collegamenti con pochi colpi, dimostriamo che Meta-Graph consente non solo un adattamento veloce ma anche una migliore convergenza finale e può imparare efficacemente utilizzando solo un piccolo campione di bordi veri.
Le reti neurali generative mappano una distribuzione standard, possibilmente ad una distribuzione complessa ad alta densità, che rappresenta il mondo reale dei dati. Tuttavia, una determinata distribuzione di input così come una specifica architettura delle reti neurali possono imporre limiti nel catturare la diversità nello spazio di destinazione ad alta dimensione. Per risolvere questa difficoltà, proponiamo un quadro di formazione che produce avidamente una serie di reti generative avversarie che catturano incrementalmente la diversità dello spazio di destinazione. Mostriamo teoricamente ed empiricamente che il nostro algoritmo di formazione converge alla distribuzione teoricamente ottimale, la proiezione della distribuzione reale sul guscio convesso dello spazio di distribuzione della rete.
I priori generativi sono diventati molto efficaci nella risoluzione di problemi inversi tra cui il denoising, l'inpainting e la ricostruzione da poche e rumorose misure.Con un modello generativo possiamo rappresentare un'immagine con un codice latente di dimensioni molto più basse.Nel contesto del compressive sensing, se l'immagine sconosciuta appartiene all'intervallo di una rete generativa preaddestrata, allora possiamo recuperare l'immagine stimando il sottostante codice latente compatto dalle misure disponibili.Tuttavia, studi recenti hanno rivelato che anche reti neurali profonde non addestrate possono funzionare come un priore per recuperare immagini naturali. Questi approcci aggiornano i pesi della rete mantenendo i codici latenti fissi per ricostruire l'immagine di destinazione dalle misure date.In questo articolo, ottimizziamo i pesi della rete e i codici latenti per usare una rete generativa non addestrata come priore per il problema del video compressive sensing. Dimostriamo che ottimizzando il codice latente, possiamo ottenere una rappresentazione concisa dei fotogrammi che mantiene la somiglianza strutturale dei fotogrammi del video, inoltre applichiamo un vincolo di basso rango ai codici latenti per rappresentare le sequenze video in uno spazio latente di dimensione ancora più bassa, dimostrando empiricamente che i nostri metodi proposti forniscono una precisione migliore o comparabile e una bassa complessità computazionale rispetto ai metodi esistenti.
La potatura basata sulla magnitudine è uno dei metodi più semplici per la potatura delle reti neurali.Nonostante la sua semplicità, la potatura basata sulla magnitudine e le sue varianti hanno dimostrato prestazioni notevoli per la potatura delle architetture moderne. Basandoci sull'osservazione che la potatura basata sulla magnitudine minimizza effettivamente la distorsione di Frobenius di un operatore lineare corrispondente a un singolo strato, sviluppiamo un semplice metodo di potatura, chiamato lookahead pruning, estendendo l'ottimizzazione a singolo strato a un'ottimizzazione multistrato. I nostri risultati sperimentali dimostrano che il metodo proposto supera costantemente la potatura basata sulla magnitudine su varie reti tra cui VGG e ResNet, in particolare nel regime di alta dispersione.
La letteratura recente ha dimostrato risultati promettenti sull'addestramento delle reti generative avversarie impiegando un insieme di discriminatori, in contrasto con il gioco tradizionale che coinvolge un generatore contro un singolo avversario.Quei metodi eseguono un'ottimizzazione mono-obiettivo su qualche semplice consolidamento delle perdite, per esempio una media.In questo lavoro, rivisitiamo l'approccio a discriminatori multipli inquadrando la minimizzazione simultanea delle perdite fornite da diversi modelli come un problema di ottimizzazione multi-obiettivo. Inoltre, sosteniamo che i metodi precedentemente proposti e la massimizzazione dell'ipervolume possono essere visti come variazioni della discesa a gradiente multiplo in cui il calcolo della direzione di aggiornamento può essere fatto in modo efficiente. I nostri risultati indicano che la massimizzazione dell'ipervolume presenta un compromesso migliore tra qualità e diversità del campione e costo computazionale rispetto ai metodi precedenti.
La progettazione dello spazio di ricerca è un problema critico per gli algoritmi di ricerca dell'architettura neurale (NAS).Proponiamo uno spazio di ricerca a grana fine composto da blocchi atomici, un'unità di ricerca minima molto più piccola di quelle utilizzate nei recenti algoritmi NAS.Questo spazio di ricerca facilita la selezione diretta dei numeri di canale e delle dimensioni del kernel nelle convoluzioni. Inoltre, proponiamo un algoritmo di ricerca dell'architettura consapevole delle risorse che seleziona dinamicamente i blocchi atomici durante l'addestramento.L'algoritmo è ulteriormente accelerato da una tecnica di restringimento dinamico della rete.Invece di un paradigma di ricerca e addestramento in due fasi, il nostro metodo può cercare e addestrare simultaneamente l'architettura di destinazione in un modo end-to-end. Il nostro metodo raggiunge prestazioni allo stato dell'arte sotto diverse configurazioni FLOPS su ImageNet con un costo di ricerca trascurabile.Apriamo il nostro intero codice base a: https://github.com/meijieru/AtomNAS.
Introduciamo Lyceum, un ecosistema computazionale ad alte prestazioni per l'apprendimento dei robot.   Lyceum è costruito sopra il linguaggio di programmazione Julia e il simulatore fisico MuJoCo, combinando la facilità d'uso di un linguaggio di programmazione di alto livello con le prestazioni del C nativo. Lyceum è fino a 10-20 volte più veloce rispetto ad altre astrazioni popolari come Gym di OpenAI e sdm-control di Deep-Mind.  Questo riduce sostanzialmente il tempo di addestramento per vari algoritmi di re-inforcement learning; ed è anche abbastanza veloce da supportare il controllo predittivo del modello in tempo reale con simulatori di fisica.   Lyceum ha un'API diretta e supporta il calcolo parallelo su più core o macchine. Il codice base, i tutorial e i video dimostrativi possono essere trovati su: https://sites.google.com/view/lyceum-anon.
C'è un forte incentivo a sviluppare tecniche di apprendimento versatili che possano trasferire la conoscenza della separabilità delle classi da un dominio di origine etichettato a un dominio di destinazione non etichettato in presenza di un domain-shift.Gli attuali approcci di adattamento del dominio (DA) non sono attrezzati per gli scenari pratici di DA a causa della loro dipendenza dalla conoscenza della relazione sorgente-target label-set (ad esempio Closed-set, Open-set o Partial DA). Inoltre, quasi tutti i lavori precedenti di DA non supervisionata richiedono la coesistenza di campioni di origine e di destinazione anche durante la distribuzione, rendendoli inadatti all'adattamento incrementale e in tempo reale. Privi di tali presupposti poco pratici, proponiamo un nuovo processo di apprendimento in due fasi: inizialmente, nella fase di approvvigionamento, l'obiettivo è quello di attrezzare il modello per la futura distribuzione senza fonte, assumendo nessuna conoscenza preliminare del prossimo category-gap e domain-shift. Per raggiungere questo obiettivo, miglioriamo la capacità del modello di rifiutare i campioni di distribuzione out-of-source sfruttando i dati di origine disponibili, in un nuovo quadro generativo classificatore.Successivamente, nella fase di spiegamento, l'obiettivo è quello di progettare un algoritmo di adattamento unificato in grado di operare attraverso una vasta gamma di category-gap, senza accesso ai campioni di origine precedentemente visti. Per raggiungere questo obiettivo, in contrasto con l'uso di complessi regimi di addestramento avversari, definiamo un obiettivo di adattamento semplice ma efficace senza fonte, utilizzando un nuovo meccanismo di pesatura a livello di istanza, chiamato Source Similarity Metric (SSM). Una valutazione approfondita mostra l'usabilità pratica del quadro di apprendimento proposto con prestazioni DA superiori anche rispetto agli approcci dipendenti dalla fonte.
Una delle sfide di lunga data nell'Intelligenza Artificiale per l'apprendimento del comportamento diretto all'obiettivo è quella di costruire un singolo agente che possa risolvere più compiti. I recenti progressi nell'apprendimento multi-task per problemi sequenziali diretti all'obiettivo sono stati sotto forma di apprendimento basato sulla distillazione, in cui una rete di studenti impara da più reti di esperti specifici dell'attività imitando le politiche specifiche dell'attività delle reti di esperti. Mentre tali approcci offrono una soluzione promettente al problema dell'apprendimento multi-task, essi richiedono la supervisione di grandi reti di esperti che richiedono ampi dati e tempo di calcolo per l'addestramento. In questo lavoro, proponiamo un efficiente quadro di apprendimento multi-task che risolve molteplici compiti orientati all'obiettivo in una configurazione on-line senza la necessità della supervisione di esperti. Il nostro lavoro utilizza i principi dell'apprendimento attivo per realizzare l'apprendimento multi-task campionando i compiti più difficili rispetto a quelli più facili.Proponiamo tre modelli distinti sotto il nostro quadro di campionamento attivo.Un metodo adattivo con prestazioni multi-tasking estremamente competitive.Un meta-apprendista basato su UCB che lancia il problema di scegliere il prossimo compito su cui allenarsi come un problema di bandito multi-armato. Un metodo di meta-apprendimento che presenta il problema della scelta del prossimo compito come un problema completo di Reinforcement Learning e utilizza metodi actor-critic per ottimizzare direttamente le prestazioni multi-tasking.Dimostriamo i risultati nel dominio Atari 2600 su sette istanze multi-tasking: tre istanze da 6 compiti, un'istanza da 8 compiti, due istanze da 12 compiti e un'istanza da 21 compiti.
Numerosi dataset di comprensione della lettura automatica (MRC) spesso coinvolgono l'annotazione manuale, richiedendo un enorme sforzo umano, e quindi la dimensione del dataset rimane significativamente più piccola rispetto alla dimensione dei dati disponibili per l'apprendimento non supervisionato. Questa tecnica mostra prestazioni migliori rispetto ad altre tecniche generali di pre-addestramento come la modellazione del linguaggio, perché le caratteristiche dei dati generati sono simili a quelle dei dati MRC a valle.Tuttavia, è difficile avere dati sintetici di alta qualità paragonabili ai set di dati MRC annotati dall'uomo. Per affrontare questo problema, proponiamo Answer-containing Sentence Generation (ASGen), un nuovo metodo di pre-addestramento per la generazione di dati sintetici che coinvolge due tecniche avanzate, (1) determinare dinamicamente K risposte e (2) pre-addestrare il generatore di domande sul compito di generazione di frasi contenenti risposte. Valutiamo la capacità di generazione di domande del nostro metodo confrontando il punteggio BLEU con i metodi esistenti e testiamo il nostro metodo mettendo a punto il modello MRC sui dati MRC a valle dopo l'addestramento sui dati sintetici. I risultati sperimentali mostrano che il nostro approccio supera i metodi di generazione esistenti e aumenta le prestazioni dei modelli MRC all'avanguardia su una serie di dataset MRC come SQuAD-v1.1, SQuAD-v2.0, KorQuAD e QUASAR-T senza alcuna modifica architettonica al modello MRC originale.
Le reti neurali profonde (DNN) dominano l'attuale ricerca nell'apprendimento automatico.Grazie alla massiccia parallelizzazione delle GPU l'addestramento DNN non è più un collo di bottiglia, e i grandi modelli con molti parametri e un elevato sforzo computazionale guidano le comuni tabelle di benchmark.Al contrario, i dispositivi embedded hanno una capacità molto limitata.Di conseguenza, sia la dimensione del modello che il tempo di inferenza devono essere significativamente ridotti se le DNN devono raggiungere prestazioni adeguate sui dispositivi embedded.Proponiamo un approccio di quantizzazione morbida per addestrare le DNN che possono essere valutate usando la pura aritmetica a virgola fissa. Sfruttando il meccanismo bit-shift, deriviamo vincoli di quantizzazione in virgola fissa per tutti i componenti importanti, tra cui la normalizzazione batch e ReLU.Rispetto all'aritmetica in virgola mobile, i calcoli in virgola fissa riducono significativamente lo sforzo computazionale mentre le rappresentazioni a basso bit diminuiscono immediatamente i costi di memoria.Valutiamo il nostro approccio con diverse architetture su set di dati di benchmark comuni e lo confrontiamo con approcci di quantizzazione recenti.Raggiungiamo nuove prestazioni allo stato dell'arte utilizzando modelli in virgola fissa a 4 bit con un tasso di errore del 4,98% su CIFAR-10.
	Presentiamo un nuovo approccio e una nuova architettura, denominata WSNet, per l'apprendimento di reti neurali profonde compatte ed efficienti.Gli approcci esistenti convenzionalmente imparano i parametri completi del modello in modo indipendente e poi li comprimono attraverso un'elaborazione ad hoc come la potatura del modello o la fattorizzazione del filtro.In alternativa, WSNet propone di imparare i parametri del modello campionando da un insieme compatto di parametri imparabili, che naturalmente impone la condivisione dei parametri durante il processo di apprendimento.Dimostriamo che tale nuovo approccio di campionamento dei pesi (e WSNet indotta) promuove favorevolmente sia i pesi che la condivisione dei calcoli. Impiegando questo metodo, possiamo imparare in modo più efficiente reti molto più piccole con prestazioni competitive rispetto alle reti di base con lo stesso numero di filtri di convoluzione. Combinati con la quantizzazione dei pesi, i modelli risultanti sono fino a 180 volte più piccoli e teoricamente fino a 16 volte più veloci delle linee di base consolidate, senza un notevole calo delle prestazioni.
Recenti lavori nel campo dell'adversarial machine learning hanno iniziato a concentrarsi sulla percezione visiva nella guida autonoma e hanno studiato gli Adversarial Examples (AEs) per i modelli di rilevamento degli oggetti.Tuttavia, in tale pipeline di percezione visiva gli oggetti rilevati devono anche essere tracciati, in un processo chiamato Multiple Object Tracking (MOT), per costruire le traiettorie mobili degli ostacoli circostanti. Poiché il MOT è progettato per essere robusto contro gli errori nel rilevamento degli oggetti, pone una sfida generale alle tecniche di attacco esistenti che mirano ciecamente al rilevamento degli oggetti: scopriamo che è necessario un tasso di successo superiore al 98% perché possano effettivamente influenzare i risultati del tracking, un requisito che nessuna tecnica di attacco esistente può soddisfare. In questo articolo, siamo i primi a studiare gli attacchi adversarial machine learning contro la completa pipeline di percezione visiva nella guida autonoma, e scopriamo una nuova tecnica di attacco, il tracker hijacking, che può efficacemente ingannare il MOT utilizzando gli AE sul rilevamento degli oggetti. Usando la nostra tecnica, gli attacchi AE riusciti su un solo frame possono spostare un oggetto esistente dentro o fuori la traiettoria di un veicolo autonomo per causare potenziali pericoli per la sicurezza. Eseguiamo una valutazione usando il dataset Berkeley Deep Drive e scopriamo che in media quando vengono attaccati 3 frame, il nostro attacco può avere un tasso di successo quasi del 100% mentre gli attacchi che mirano ciecamente al rilevamento degli oggetti hanno solo fino al 25%.
L'apprendimento auto-supervisionato (SlfSL), che mira ad apprendere rappresentazioni di caratteristiche attraverso compiti pretestuosi ingegnosamente progettati senza annotazione umana, ha ottenuto progressi convincenti negli ultimi anni.Molto recentemente, SlfSL è stato anche identificato come una soluzione promettente per l'apprendimento semi-supervisionato (SemSL) in quanto offre un nuovo paradigma per utilizzare i dati senza etichetta. Questo lavoro esplora ulteriormente questa direzione proponendo una nuova struttura per accoppiare senza soluzione di continuità SlfSL con SemSL.La nostra intuizione è che l'obiettivo di predizione in SemSL può essere modellato come il fattore latente nel predittore per l'obiettivo SlfSL.Marginalizzando sul fattore latente deriva naturalmente una nuova formulazione che sposa gli obiettivi di predizione di questi due processi di apprendimento. Implementando questa struttura attraverso un approccio SlfSL semplice ma efficace - predizione dell'angolo di rotazione, creiamo un nuovo approccio SemSL chiamato Predizione condizionale dell'angolo di rotazione (CRAP).In particolare, CRAP è caratterizzato dall'adozione di un modulo che predice l'angolo di rotazione dell'immagine \textbf{condizionato sulla classe dell'immagine candidata}. Attraverso la valutazione sperimentale, dimostriamo che CRAP raggiunge prestazioni superiori rispetto agli altri modi esistenti di combinare SlfSL e SemSL.Inoltre, la struttura SemSL proposta è altamente estendibile.Aumentando CRAP con una semplice tecnica SemSL e una modifica del compito di predizione dell'angolo di rotazione, il nostro metodo ha già raggiunto lo stato dell'arte delle prestazioni SemSL.
La classificazione è un compito centrale nel machine learning e nell'information retrieval.In questo compito, è particolarmente importante presentare all'utente una lista di elementi che sia attraente nel suo insieme.Questo a sua volta richiede di prendere in considerazione le interazioni tra gli elementi, poiché intuitivamente, mettere un elemento sulla lista influenza la decisione di quali altri elementi dovrebbero essere scelti accanto ad esso.In questo lavoro, proponiamo un modello sequenza-sequenza per la classificazione chiamato seq2slate. La natura ricorrente del modello permette di catturare direttamente le dipendenze complesse tra gli elementi in modo flessibile e scalabile. Mostriamo come apprendere il modello end-to-end da una supervisione debole sotto forma di dati click-through facilmente ottenibili. dimostriamo inoltre l'utilità del nostro approccio in esperimenti su benchmark standard di ranking e in un sistema di raccomandazione del mondo reale.
Mentre i metodi basati sulla quantità di moto, insieme alla discesa stocastica del gradiente, sono ampiamente utilizzati quando si addestrano i modelli di apprendimento automatico, c'è poca comprensione teorica sull'errore di generalizzazione di tali metodi.In pratica, il parametro della quantità di moto è spesso scelto in modo euristico con poca guida teorica. In questo lavoro, usiamo la struttura della stabilità algoritmica per fornire un limite superiore all'errore di generalizzazione per la classe di funzioni di perdita fortemente convesse, sotto ipotesi tecniche lievi. Il nostro limite decade a zero inversamente con la dimensione dell'insieme di allenamento, e aumenta con l'aumento del parametro di slancio.
L'apprendimento per imitazione dalle dimostrazioni di solito si basa sull'apprendimento di una politica dalle traiettorie degli stati e delle azioni ottimali.Tuttavia, nelle dimostrazioni degli esperti della vita reale, spesso le informazioni sulle azioni mancano e sono disponibili solo le traiettorie degli stati.Presentiamo un metodo di apprendimento per imitazione basato sul modello che può imparare azioni ottimali specifiche dell'ambiente solo dalle traiettorie degli stati degli esperti.Il nostro metodo proposto inizia con un algoritmo di apprendimento di rinforzo senza modello con un segnale di ricompensa euristico per campionare le dinamiche dell'ambiente, che viene poi utilizzato per allenare la probabilità di transizione tra stati. Le valutazioni sperimentali mostrano che il nostro metodo proposto raggiunge con successo prestazioni simili ai metodi tradizionali di apprendimento per imitazione basati sulle traiettorie (stato, azione) anche in assenza di informazioni sull'azione, con molte meno iterazioni rispetto ai metodi convenzionali di apprendimento per rinforzo senza modello.
Una recente ricerca ha proposto l'ipotesi del biglietto della lotteria, suggerendo che per una rete neurale profonda, esistono sottoreti addestrabili che eseguono ugualmente o meglio del modello originale con passi di addestramento commisurati.Mentre questa scoperta è perspicace, trovare sottoreti adeguate richiede un addestramento iterativo e la potatura. L'alto costo sostenuto limita le applicazioni dell'ipotesi del biglietto della lotteria.Mostriamo che esiste un sottoinsieme delle suddette sottoreti che convergono significativamente più velocemente durante il processo di formazione e quindi possono mitigare il problema dei costi.Conduciamo ampi esperimenti per dimostrare che tali sottoreti esistono costantemente attraverso varie strutture di modelli per un'impostazione restrittiva di iperparametri (ad es, attentamente selezionato il tasso di apprendimento, il rapporto di sfrondamento e la capacità del modello).  Come applicazione pratica dei nostri risultati, dimostriamo che tali sottoreti possono aiutare a ridurre il tempo totale di addestramento avversario, un approccio standard per migliorare la robustezza, fino al 49% su CIFAR-10 per raggiungere lo stato dell'arte della robustezza.
Le rappresentazioni disentangled, dove i fattori generativi dei dati di livello superiore si riflettono in dimensioni latenti disgiunte, offrono diversi vantaggi come la facilità di derivare rappresentazioni invarianti, la trasferibilità ad altri compiti, l'interpretabilità, ecc. Consideriamo il problema dell'apprendimento non supervisionato delle rappresentazioni disentangled da un grande pool di osservazioni non etichettate, e proponiamo un approccio basato sull'inferenza variazionale per dedurre i fattori latenti disentangled. Introduciamo un regolatore sull'aspettativa del posteriore approssimativo sui dati osservati che incoraggia il disentanglement.Proponiamo anche una nuova metrica di disentanglement che è meglio allineata con il disentanglement qualitativo osservato nell'output del decoder.Osserviamo empiricamente un miglioramento significativo rispetto ai metodi esistenti sia in termini di disentanglement che di likelihood dei dati (qualità della ricostruzione). 
Nei sistemi multiagente (MAS), ogni agente prende decisioni individuali ma tutti contribuiscono globalmente all'evoluzione del sistema.L'apprendimento nei MAS è difficile poiché la selezione delle azioni di ogni agente deve avvenire in presenza di altri agenti in co-apprendimento.Inoltre, la stocasticità ambientale e le incertezze aumentano esponenzialmente con l'aumento del numero di agenti.Lavori precedenti prendono in prestito vari meccanismi di coordinamento multiagente nell'architettura di apprendimento profondo per facilitare il coordinamento multiagente.Tuttavia, nessuno di loro considera esplicitamente la semantica dell'azione tra agenti che azioni diverse hanno diverse influenze su altri agenti. In questo documento, proponiamo una nuova architettura di rete, chiamata Action Semantics Network (ASN), che rappresenta esplicitamente tale semantica dell'azione tra gli agenti.ASN caratterizza l'influenza delle diverse azioni sugli altri agenti usando reti neurali basate sulla semantica dell'azione tra di loro.ASN può essere facilmente combinata con gli algoritmi esistenti di deep reinforcement learning (DRL) per aumentare le loro prestazioni.I risultati sperimentali sulla micromanagement di StarCraft II e Neural MMO mostrano che ASN migliora significativamente le prestazioni degli approcci DRL allo stato dell'arte rispetto a diverse architetture di rete.
Le reti neurali spike sono state studiate sia come modelli biologicamente plausibili di calcolo neurale, sia come un tipo di rete neurale potenzialmente più efficiente.Mentre le reti neurali spike convoluzionali hanno dimostrato di raggiungere prestazioni vicine allo stato dell'arte, solo una soluzione è stata proposta per convertire le reti neurali ricorrenti gated, finora. Le reti neurali ricorrenti sotto forma di reti di celle di memoria gating sono state centrali nelle soluzioni state-of-the-art in domini problematici che coinvolgono il riconoscimento o la generazione di sequenze.Qui, progettiamo una cella LSTM gated analogica in cui i suoi neuroni possono essere sostituiti da efficienti neuroni spiking stocastici.Questi neuroni spiking adattivi implementano una forma adattiva di codifica sigma-delta per convertire i valori di attivazione analogici calcolati internamente in spike-train. Per tali neuroni, approssimiamo la funzione di attivazione effettiva, che assomiglia a una sigmoide.Mostriamo come i neuroni analogici con tali funzioni di attivazione possono essere usati per creare una cella LSTM analogica; le reti di queste cellule possono poi essere addestrate con backpropagation standard. Addestriamo queste reti LSTM su una versione rumorosa e senza rumore del compito originale di predizione delle sequenze di Hochreiter & Schmidhuber (1997), e anche su una versione rumorosa e senza rumore di un classico compito di apprendimento di rinforzo della memoria di lavoro, il T-Maze. Sostituendo i neuroni analogici con corrispondenti neuroni a spirale adattivi, dimostriamo poi che quasi tutti gli equivalenti di rete neurale a spirale risultanti calcolano correttamente i compiti originali.
Le CNN hanno un grande successo nel riconoscere le azioni umane nei video, anche se con un grande costo di calcolo, che è significativamente più alto nel caso di azioni a lungo raggio, dove un video può durare fino a pochi minuti, in media.L'obiettivo di questo articolo è di ridurre il costo computazionale di queste CNN, senza sacrificare le loro prestazioni. Proponiamo VideoEpitoma, un'architettura di rete neurale che comprende due moduli: un selettore di timestamp e un classificatore di video.Dato un video a lungo raggio di migliaia di timestep, il selettore impara a scegliere solo pochi ma più rappresentativi timestep per il video.Questo selettore risiede sopra una CNN leggera come MobileNet e utilizza un nuovo modulo di gating per prendere una decisione binaria: considerare o scartare un video timestep. Questa decisione è condizionata sia dalla caratteristica a livello di timestep che dal consenso a livello di video.Un modello CNN pesante come I3D prende i fotogrammi selezionati come input ed esegue la classificazione video.Utilizzando classificatori video off-the-shelf, VideoEpitoma riduce la computazione fino al 50% senza compromettere la precisione. Inoltre, dimostriamo che se addestrato end-to-end, il selettore impara a fare scelte migliori a beneficio del classificatore, nonostante il selettore e il classificatore risiedano su due CNN diverse.Infine, riportiamo risultati allo stato dell'arte su due dataset per il riconoscimento di azioni a lungo raggio: In particolare, eguagliamo l'accuratezza di I3D usando meno della metà del calcolo.
L'annotazione umana per il parsing sintattico è costosa, e grandi risorse sono disponibili solo per una frazione di lingue. Una domanda che ci poniamo è se si può sfruttare l'abbondanza di testi non etichettati per migliorare i parser sintattici, oltre a usare semplicemente i testi per ottenere caratteristiche lessicali più generalizzabili (cioè oltre le embeddings delle parole). Poiché l'inferenza esatta è intrattabile, introduciamo un rilassamento differenziabile per ottenere campioni approssimativi e calcolare i gradienti rispetto ai parametri del parser. Il nostro metodo (Differentiable Perturb-and-Parse) si basa sulla programmazione dinamica differenziabile sui punteggi dei bordi stocasticamente perturbati.
DeConvNet, Guided BackProp, LRP, sono stati inventati per comprendere meglio le reti neurali profonde.Mostriamo che questi metodi non producono la spiegazione teoricamente corretta per un modello lineare.Eppure sono utilizzati su reti multistrato con milioni di parametri.Questo è motivo di preoccupazione poiché i modelli lineari sono semplici reti neurali. Noi sosteniamo che i metodi di spiegazione per le reti neurali dovrebbero funzionare in modo affidabile nel limite della semplicità, i modelli lineari. Sulla base della nostra analisi dei modelli lineari proponiamo una generalizzazione che produce due tecniche di spiegazione (PatternNet e PatternAttribution) che sono teoricamente valide per i modelli lineari e producono spiegazioni migliori per le reti profonde.
Le reti neurali a grafo hanno mostrato risultati promettenti nella rappresentazione e nell'analisi di diversi dati strutturati a grafo, come le reti sociali, citazionali e di interazione proteica. Gli approcci esistenti soffrono comunemente del problema dell'oversmoothing, indipendentemente dal fatto che le politiche siano basate sui bordi o sui nodi per l'aggregazione dei vicini. Per affrontare questi problemi, proponiamo un nuovo modello di rete neurale a grafo che considera sia le relazioni di vicinato basate sui bordi che le caratteristiche delle entità basate sui nodi, cioè Graph Entities with Step Mixture via random walk (GESM). GESM impiega una miscela di vari passi attraverso il random walk per alleviare il problema dell'oversmoothing e l'attenzione ad usare esplicitamente le informazioni sui nodi.Questi due meccanismi permettono un'aggregazione ponderata del vicinato che considera le proprietà delle entità e delle relazioni.Con esperimenti intensivi, dimostriamo che il GESM proposto raggiunge lo stato dell'arte o prestazioni comparabili su quattro dataset di grafici di riferimento che comprendono compiti di apprendimento induttivo e trasduttivo.Inoltre, dimostriamo empiricamente l'importanza di considerare le informazioni globali.Il codice sorgente sarà disponibile al pubblico nel prossimo futuro.
L'inseguimento delle basi è un'ottimizzazione di rilevamento compresso in cui il l1-norm è minimizzato soggetto a vincoli di errore del modello.Qui usiamo una rete neurale profonda al posto della l1-regolarizzazione.Usando le statistiche di rumore note, impariamo congiuntamente il priore e ricostruiamo le immagini senza accesso ai dati di verità a terra.Durante la formazione, usiamo la minimizzazione alternata attraverso una rete iterativa srotolata e risolviamo congiuntamente i pesi della rete neurale e le ricostruzioni delle immagini del set di allenamento. Confrontiamo le prestazioni di ricostruzione tra metodi non supervisionati e supervisionati (cioè con la verità a terra) e ipotizziamo che questa tecnica possa essere usata per imparare la ricostruzione quando i dati di verità a terra non sono disponibili, come nella risonanza magnetica dinamica ad alta risoluzione.
Gli approcci di apprendimento profondo di solito richiedono una grande quantità di dati etichettati per generalizzare.Tuttavia, gli esseri umani possono imparare un nuovo concetto solo da pochi campioni.Una delle capacità umane ad alta cognizione è quella di imparare diversi concetti allo stesso tempo.In questo documento, affrontiamo il compito di classificare più oggetti vedendo solo pochi campioni da ogni categoria. Al meglio delle conoscenze degli autori, non esiste un dataset appositamente progettato per la classificazione multiclasse a pochi colpi.Progettiamo un compito di classificazione mutli-oggetto a poche classi e un ambiente per creare facilmente dataset controllabili per questo compito.Dimostriamo che il dataset proposto è solido usando un metodo che è un'estensione delle reti prototipiche.
Presentiamo un nuovo approccio alla definizione di una funzione di perdita di sequenza per addestrare un sintetizzatore utilizzando un codificatore-decodificatore secondario come funzione di perdita, alleviando una lacuna dell'addestramento a livello di parola per gli output di sequenza.La tecnica si basa sull'intuizione che se un riassunto è buono, dovrebbe contenere le informazioni più essenziali dell'articolo originale, e quindi dovrebbe essere esso stesso una buona sequenza di input, al posto dell'originale, da cui può essere generato un riassunto. Presentiamo risultati sperimentali in cui applichiamo questa funzione di perdita aggiuntiva a un riassumitore generale astrattivo su un set di dati di riassunto di notizie. Il risultato è un miglioramento nella metrica ROUGE e un miglioramento particolarmente grande nelle valutazioni umane, suggerendo un miglioramento delle prestazioni che è competitivo con i modelli specializzati allo stato dell'arte.
I metodi di traduzione video-to-video non riescono a produrre video tradotti che siano realistici dal punto di vista del frame, che conservino le informazioni semantiche e che siano coerenti a livello video. In questo lavoro, proponiamo un nuovo modello di traduzione video-to-video non supervisionato che decompone lo stile e il contenuto, utilizza una struttura specializzata di encoder-decoder e propaga le informazioni inter-frame attraverso unità RNN (Recurrent Neural Network) bidirezionali. Inoltre, cambiando i fotogrammi di input e i codici di stile incorporati nella nostra traduzione, proponiamo una perdita di interpolazione video, che cattura le informazioni temporali all'interno della sequenza per addestrare i nostri blocchi di costruzione in modo auto-supervisionato.Il nostro modello può produrre video tradotti foto-realistici e coerenti dal punto di vista spazio-temporale in modo multimodale.I risultati sperimentali soggettivi e oggettivi validano la superiorità del nostro modello rispetto ai metodi esistenti.
Per creare un sistema trasparente, un utente deve essere in grado di interrogarlo per ottenere spiegazioni sui suoi risultati. Noi sosteniamo che un principio chiave di base per questo è l'uso della causalità all'interno di un modello di pianificazione, e che le strutture di argomentazione forniscono una rappresentazione intuitiva di tale causalità.In questo articolo, discutiamo come l'argomentazione può aiutare ad estrarre le causalità in piani e modelli, e come possono creare spiegazioni da essi.
Le nuvole di punti, come forma di rappresentazione lagrangiana, consentono applicazioni potenti e flessibili in un gran numero di discipline computazionali. Identifichiamo una serie di problemi intrinseci con questi approcci: senza la conoscenza della dimensione temporale, le soluzioni dedotte possono esibire un forte sfarfallio, e soluzioni semplici per sopprimere questo sfarfallio possono risultare in minimi locali indesiderati che si manifestano come strutture ad alone.Proponiamo una nuova funzione di perdita temporale che tiene conto delle derivate superiori del tempo delle posizioni dei punti, e incoraggia il mingling, vale a dire, Combiniamo queste tecniche in un metodo di super-risoluzione con un approccio di troncamento per adattare in modo flessibile la dimensione delle posizioni generate.Mostriamo che il nostro metodo funziona per grandi insiemi di punti deformanti da diverse fonti per dimostrare la flessibilità del nostro approccio.
Studiamo il problema della generazione di esempi avversari in un'impostazione black-box in cui è disponibile solo l'accesso loss-oracle a un modello.Introduciamo un quadro che unifica concettualmente gran parte del lavoro esistente sugli attacchi black-box, e dimostriamo che gli attuali metodi state-of-the-art sono ottimali in un senso naturale.Nonostante questa ottimalità, mostriamo come migliorare gli attacchi black-box portando un nuovo elemento nel problema: priori gradiente. Diamo un algoritmo basato sull'ottimizzazione bandit che ci permette di integrare senza soluzione di continuità tali priori, e identifichiamo esplicitamente e incorporiamo due esempi. I metodi risultanti utilizzano da due a quattro volte meno query e falliscono da due a cinque volte meno dell'attuale stato dell'arte. Il codice per riprodurre il nostro lavoro è disponibile su https://git.io/fAjOJ.
Lo scopo del presente lavoro è quello di migliorare l'efficienza delle reti neurali di grandi dimensioni che operano su dati audio attraverso l'apprendimento multitask con compiti auto-supervisionati su dati non etichettati. A tal fine, abbiamo addestrato un estrattore di caratteristiche audio end-to-end basato su WaveNet che alimenta reti neurali semplici, ma versatili e specifiche per i compiti. Dimostriamo che, in uno scenario con limitati dati di formazione etichettati, è possibile migliorare significativamente le prestazioni di un compito di classificazione supervisionato addestrandolo simultaneamente con questi compiti aggiuntivi auto-supervisionati.Mostriamo che è possibile migliorare le prestazioni su un compito di classificazione di diversi eventi sonori di quasi il 6% quando addestrato congiuntamente con fino a tre distinti compiti auto-supervisionati.Questo miglioramento scala con il numero di compiti ausiliari aggiuntivi così come la quantità di dati non supervisionati.Mostriamo anche che incorporare l'aumento dei dati nella nostra impostazione multitask porta a guadagni ancora maggiori nelle prestazioni.
Introduciamo un set di dati per la generazione sequenziale di testo procedurale (how-to) da immagini nel dominio della cucina. Il set di dati consiste di 16.441 ricette di cucina con 160.479 foto associate a diversi passi, e abbiamo impostato una linea di base motivata dal modello più performante in termini di valutazione umana per il compito Visual Story Telling (ViST): (1) Scaffolding Structure in Decoder (SSiD) (2) Scaffolding Structure in Loss (SSiL).Questi modelli mostrano un miglioramento nella valutazione empirica e umana.Il nostro modello più performante (SSiL) raggiunge un punteggio METEOR di 0.31, che è un miglioramento di 0.6 rispetto al modello di base. Abbiamo anche condotto una valutazione umana delle ricette generate a terra, che rivela che il 61% ha trovato che il nostro modello proposto (SSiL) è migliore del modello di base in termini di ricette complessive, e il 72,5% ha preferito il nostro modello in termini di coerenza e struttura.
I modelli impliciti, che permettono la generazione di campioni ma non la valutazione puntuale delle probabilità, sono onnipresenti nei problemi del mondo reale affrontati dall'apprendimento automatico e sono un argomento caldo della ricerca attuale. Alcuni esempi includono i simulatori di dati che sono ampiamente utilizzati nella ricerca ingegneristica e scientifica, le reti generative avversarie (GAN) per la sintesi delle immagini e le tecniche di inferenza approssimativa che si basano su distribuzioni implicite. La maggior parte degli approcci esistenti per l'apprendimento di modelli impliciti si basa sull'approssimazione della distribuzione intrattabile o sull'obiettivo di ottimizzazione basato sul gradiente, che rischia di produrre aggiornamenti imprecisi e quindi modelli scadenti. Questo articolo allevia la necessità di tali approssimazioni proponendo lo stimatore del gradiente di Stein, che stima direttamente la funzione di punteggio della distribuzione implicitamente definita.
L'iniezione di rumore è uno strumento fondamentale per l'aumento dei dati, ma non esiste ancora una procedura ampiamente accettata per incorporarlo in strutture di apprendimento. Questo studio analizza gli effetti dell'aggiunta o dell'applicazione di diversi modelli di rumore di varie grandezze alle architetture di reti neurali convoluzionali (CNN). I modelli di rumore che sono distribuiti con diverse funzioni di densità sono dati livelli di grandezza comuni attraverso la metrica di Similarità Strutturale (SSIM) al fine di creare un terreno appropriato per il confronto. I risultati di base sono conformi con la maggior parte delle nozioni comuni nell'apprendimento automatico, e introduce anche alcune nuove euristiche e raccomandazioni sull'iniezione di rumore.I nuovi approcci forniranno una migliore comprensione delle procedure di apprendimento ottimale per la classificazione delle immagini.
Lo stato dell'arte dei metodi Unsupervised Domain Adaptation (UDA) impara le caratteristiche trasferibili minimizzando la discrepanza della distribuzione delle caratteristiche tra il dominio di origine e quello di destinazione.A differenza di questi metodi che non modellano esplicitamente le distribuzioni delle caratteristiche, in questo articolo, esploriamo la modellazione esplicita della distribuzione delle caratteristiche per UDA. In particolare, proponiamo Distribution Matching Prototypical Network (DMPN) per modellare le caratteristiche profonde di ogni dominio come distribuzioni di miscele gaussiane.Con la modellazione esplicita della distribuzione delle caratteristiche, possiamo facilmente misurare la discrepanza tra i due domini.In DMPN, proponiamo due nuove perdite di discrepanza di dominio con interpretazioni probabilistiche. La prima minimizza le distanze tra le corrispondenti componenti gaussiane medie dei dati di origine e di destinazione; la seconda minimizza la probabilità pseudo negativa del log di generare le caratteristiche di destinazione dalla distribuzione delle caratteristiche di origine; per imparare entrambe le caratteristiche discriminanti e invarianti del dominio, DMPN viene addestrato minimizzando la perdita di classificazione sui dati di origine etichettati e le perdite di discrepanza del dominio insieme. Il nostro approccio produce un ampio margine nel compito di trasferimento dell'immagine Digits rispetto agli approcci più avanzati; inoltre, DMPN ottiene un'accuratezza media dell'81,4% sul set di dati VisDA 2017; l'analisi di sensibilità dell'iperparametro mostra che il nostro approccio è robusto rispetto ai cambiamenti dell'iperparametro.
Imparare in modo efficiente a risolvere compiti in ambienti complessi è una sfida chiave per gli agenti di apprendimento per rinforzo (RL). Proponiamo di decomporre un ambiente complesso usando un grafo del mondo agnostico, un'astrazione che accelera l'apprendimento consentendo agli agenti di concentrare l'esplorazione su un sottospazio dell'ambiente. I nodi di un grafo del mondo sono importanti stati waypoint e i bordi rappresentano attraversamenti fattibili tra loro.  La nostra struttura ha due fasi di apprendimento: 1) identificare i nodi e i bordi del grafo del mondo addestrando un auto-encoder variazionale binario (VAE) sui dati della traiettoria e 2) un quadro RL gerarchico che sfrutta la conoscenza strutturale e di connettività dal grafo del mondo appreso per orientare l'esplorazione verso i waypoint e le regioni rilevanti per il compito. Mostriamo che il nostro approccio accelera significativamente la RL su una serie di compiti impegnativi del mondo a griglia 2D: rispetto alle linee di base, l'integrazione del grafo del mondo raddoppia i premi ottenuti sui compiti più semplici, ad esempio MultiGoal, e riesce a risolvere compiti più impegnativi, ad esempio Door-Key, dove le linee di base falliscono.
Introduciamo la nozione di firme di proprietà, una rappresentazione per programmi e specifiche di programma destinate al consumo da parte di algoritmi di apprendimento automatico. Data una funzione con input di tipo τ_in e output di tipo τ_out, una proprietà è una funzione di tipo: (τ_in, τ_out) → Bool che (informalmente) descrive qualche semplice proprietà della funzione in esame.Per esempio, se τ_in e τ_out sono entrambe liste dello stesso tipo, una proprietà potrebbe chiedere "la lista in ingresso ha la stessa lunghezza della lista in uscita?".Se abbiamo una lista di tali proprietà, possiamo valutarle tutte per la nostra funzione per ottenere una lista di output che chiameremo firma della proprietà. Discutiamo diverse potenziali applicazioni delle firme di proprietà e mostriamo sperimentalmente che possono essere usate per migliorare un sintetizzatore di base in modo che emetta il doppio dei programmi in meno di un decimo del tempo.
Costruire agenti artificialmente intelligenti che ottengono buoni risultati in queste situazioni è importante perché molte interazioni del mondo reale includono una tensione tra interessi egoistici e il benessere degli altri. Mostriamo come modificare i moderni metodi di apprendimento per rinforzo per costruire agenti che agiscono in modi che sono semplici da capire, piacevoli (iniziano cooperando), provocabili (cercano di evitare di essere sfruttati) e indulgenti (cercano di tornare alla cooperazione reciproca). Mostriamo sia teoricamente che sperimentalmente che tali agenti possono mantenere la cooperazione nei dilemmi sociali di Markov. La nostra costruzione non richiede metodi di addestramento oltre a una modifica dell'auto-gioco, quindi se un ambiente è tale che buone strategie possono essere costruite nel caso a somma zero (ad esempio Atari) allora possiamo costruire agenti che risolvono i dilemmi sociali in questo ambiente.
Il trucco della riparametrizzazione è diventato uno degli strumenti più utili nel campo dell'inferenza variazionale. Tuttavia, il trucco della riparametrizzazione è basato sulla trasformazione di standardizzazione che limita il campo di applicazione di questo metodo alle distribuzioni che hanno funzioni di distribuzione cumulativa inversa trattabili o che sono esprimibili come trasformazioni deterministiche di tali distribuzioni.In questo articolo, abbiamo generalizzato il trucco della riparametrizzazione permettendo una trasformazione generale.A differenza di altri lavori simili, sviluppiamo il modello di gradiente generalizzato basato sulla trasformazione in modo formale e rigoroso. Scopriamo che il modello proposto è un caso speciale della varianza di controllo che indica che il modello proposto può combinare i vantaggi del CV e della riparametrizzazione generalizzata.Sulla base del modello di gradiente proposto, proponiamo un nuovo stimatore di gradiente basato su polinomi che ha una prestazione teorica migliore del trucco di riparametrizzazione in determinate condizioni e può essere applicato a una classe più grande di distribuzioni variazionali.Negli studi dei dati sintetici e reali, mostriamo che il nostro stimatore di gradiente proposto ha una varianza di gradiente significativamente più bassa che altri metodi dell'avanguardia permettendo così una procedura di inferenza più veloce.
La diagnosi dei guasti in un moderno sistema di comunicazione è tradizionalmente ritenuta difficile, o addirittura impraticabile per un approccio di apprendimento automatico puramente basato sui dati, poiché si tratta di un sistema umano di conoscenza intensiva.Alcuni flussi di pacchetti grezzi etichettati estratti dall'archivio dei guasti possono difficilmente essere sufficienti per dedurre la logica intricata dei protocolli sottostanti.In questo articolo, integriamo questi campioni limitati con due fonti di dati inesauribili: i record non etichettati sondati da un sistema in servizio, e i dati etichettati simulati in un ambiente di emulazione. Per trasferire la loro conoscenza intrinseca al dominio di destinazione, costruiamo un grafico di flusso di informazioni diretto, i cui nodi sono componenti di rete neurale costituiti da due generatori, tre discriminatori e un classificatore, e il cui ogni percorso in avanti rappresenta una coppia di obiettivi di ottimizzazione avversaria, in accordo con le richieste di apprendimento semi-supervisionale e di trasferimento. La rete a più teste può essere addestrata in un approccio alternativo, ad ogni iterazione del quale selezioniamo un obiettivo per aggiornare i pesi lungo il percorso a monte, e rinfrescare il residuo in modo stratificato a tutte le uscite a valle.I risultati effettivi mostrano che può raggiungere una precisione comparabile sulla classificazione dei flussi di Transmission Control Protocol (TCP) senza caratteristiche esperte deliberate.La soluzione ha sollevato gli ingegneri operativi da lavori massicci di comprensione e mantenimento delle regole, e ha fornito una soluzione rapida indipendente dai protocolli specifici.
Il nostro lavoro affronta due problemi importanti con le reti neurali ricorrenti: (Il primo aumenta la complessità campionaria dell'apprendimento e il tempo di addestramento, il secondo causa il problema del gradiente che svanisce ed esplode. Presentiamo un modello flessibile di rete neurale ricorrente chiamato Kronecker Recurrent Units (KRU). KRU raggiunge l'efficienza dei parametri nelle RNN attraverso una matrice ricorrente fattorizzata Kronecker, che supera il malcondizionamento della matrice ricorrente imponendo vincoli unitari morbidi sui fattori. I nostri risultati sperimentali su sette set di dati standard rivelano che KRU può ridurre il numero di parametri di tre ordini di grandezza nella matrice dei pesi ricorrenti rispetto ai modelli ricorrenti esistenti, senza compromettere le prestazioni statistiche. Questi risultati mostrano in particolare che mentre ci sono vantaggi nell'avere uno spazio ricorrente di alta dimensione, la capacità della parte ricorrente del modello può essere drasticamente ridotta.
Questo articolo studia il fenomeno indesiderato dell'eccessiva sensibilità delle rappresentazioni apprese dalle reti profonde ai cambiamenti semanticamente irrilevanti nei dati, identificando una causa di questa carenza nel classico obiettivo dell'Auto-encoder Variazionale (VAE), l'evidence lower bound (ELBO). Mostriamo che l'ELBO non riesce a controllare il comportamento del codificatore fuori dal supporto della distribuzione empirica dei dati e questo comportamento del VAE può portare a errori estremi nella rappresentazione appresa.Questo è un ostacolo chiave nell'uso efficace delle rappresentazioni per un apprendimento e un trasferimento efficiente dei dati.Per affrontare questo problema, proponiamo di aumentare i dati con specifiche che impongono l'insensibilità della rappresentazione rispetto alle famiglie di trasformazioni. Per incorporare queste specifiche, proponiamo un metodo di regolarizzazione che si basa su un meccanismo di selezione che crea un punto di dati fittizio perturbando esplicitamente un punto di dati vero osservato.Per alcune scelte di parametri, la nostra formulazione porta naturalmente alla minimizzazione della distanza Wasserstein regolarizzata dall'entropia tra le rappresentazioni. Illustriamo il nostro approccio su set di dati standard e dimostriamo sperimentalmente che miglioramenti significativi nell'accuratezza avversaria a valle possono essere raggiunti imparando rappresentazioni robuste completamente in modo non supervisionato, senza un riferimento a un particolare compito a valle e senza una costosa procedura di formazione avversaria supervisionata. 
Proponiamo un nuovo approccio basato sul punteggio per l'apprendimento di un grafo aciclico diretto (DAG) da dati osservazionali, adattiamo una formulazione di ottimizzazione continua vincolata proposta di recente per consentire relazioni non lineari tra le variabili utilizzando reti neurali, questa estensione permette di modellare interazioni complesse e allo stesso tempo è più globale nella sua ricerca rispetto ad altri approcci greedy. Oltre a confrontare il nostro metodo con i metodi di ottimizzazione continui esistenti, forniamo i confronti empirici mancanti ai metodi di ricerca greedy non lineari. Su entrambi i set di dati sintetici e del mondo reale, questo nuovo metodo supera i metodi continui attuali sulla maggior parte delle attività mentre è competitivo con i metodi di ricerca greedy esistenti su metriche importanti per l'inferenza causale.
Studiamo il problema della progettazione di algoritmi provatamente ottimali di rumore avversario che inducono l'errore di classificazione nelle impostazioni in cui un discente aggrega le decisioni di più classificatori. Data la dimostrata vulnerabilità dei modelli allo stato dell'arte agli esempi avversari, gli sforzi recenti nel campo dell'apprendimento automatico robusto si sono concentrati sull'uso di classificatori di insieme come un modo per aumentare la robustezza dei singoli modelli. In questo articolo, progettiamo attacchi provatamente ottimali contro un insieme di classificatori e dimostriamo come questo problema possa essere inquadrato come trovare strategie all'equilibrio in un gioco a somma zero per due giocatori tra un discente e un avversario e, di conseguenza, illustriamo la necessità di randomizzazione negli attacchi avversari. La principale sfida tecnica che consideriamo è la progettazione dei migliori oracoli di risposta che possono essere implementati in un quadro di Multiplicative Weight Updates per trovare strategie di equilibrio nel gioco a somma zero. Sviluppiamo una serie di algoritmi scalabili di generazione del rumore per reti neurali profonde, e dimostriamo che supera lo stato dell'arte degli attacchi su vari compiti di classificazione delle immagini. Anche se in genere non ci sono garanzie per l'apprendimento profondo, dimostriamo che questo è un approccio ben studiato in quanto è dimostrabilmente ottimale per i classificatori lineari. L'intuizione principale è una caratterizzazione geometrica dello spazio decisionale che riduce il problema della progettazione dei migliori oracoli di risposta alla minimizzazione di una funzione quadratica su un insieme di polipi convessi.
I sistemi multiagente in cui gli agenti interagiscono tra di loro e con un ambiente stocastico possono essere formalizzati come giochi stocastici. Studiamo una sottoclasse di questi giochi, chiamata Markov potential games (MPGs), che appare spesso in applicazioni economiche e ingegneristiche quando gli agenti condividono alcune risorse comuni. Consideriamo MPGs con variabili di stato-azione continue, vincoli accoppiati e ricompense non convesse. Le analisi precedenti hanno seguito un approccio variazionale che è valido solo per casi molto semplici (ricompense convesse, dinamiche invertibili e nessun vincolo accoppiato); oppure hanno considerato dinamiche deterministiche e fornito analisi a circuito aperto (OL), studiando strategie che consistono in sequenze di azioni predefinite, che non sono ottimali per ambienti stocastici. Noi presentiamo un'analisi a ciclo chiuso (CL) per gli MPG e consideriamo politiche parametriche che dipendono dallo stato corrente e in cui gli agenti si adattano alle transizioni stocastiche.Forniamo condizioni facilmente verificabili, sufficienti e necessarie perché un gioco stocastico sia un MPG, anche per funzioni parametriche complesse (ad es, Questo è utile in quanto la risoluzione di un OCP - che è un problema a singolo obiettivo - è solitamente molto più semplice della risoluzione dell'insieme originale di OCP accoppiati che formano il gioco - che è un problema di controllo multiobiettivo. Questo è un notevole miglioramento rispetto al precedente approccio standard per l'analisi CL dei MPG, che non dà alcuna soluzione approssimativa se nessun NE appartiene alla famiglia parametrica scelta, e che è pratico solo per forme parametriche semplici. Illustriamo i contributi teorici con un esempio applicando il nostro approccio a un gioco non cooperativo di ingegneria delle comunicazioni, quindi risolviamo il gioco con un algoritmo di apprendimento di rinforzo profondo che impara le politiche che approssima strettamente un NE variazionale esatto del gioco.
Facciamo la seguente osservazione sorprendente: i modelli VAE completamente convoluzionali addestrati su 32x32 ImageNet possono generalizzare bene, non solo a 64x64 ma anche a fotografie molto più grandi, senza modifiche al modello. Usiamo questa proprietà, applicando modelli completamente convoluzionali alla compressione senza perdita, dimostrando un metodo per scalare l'algoritmo 'Bits-Back with ANS' basato su VAE per la compressione senza perdita a grandi fotografie a colori, e raggiungendo lo stato dell'arte per la compressione di immagini ImageNet a grandezza naturale.
    Lo stato dell'arte dei modelli di computer vision ha dimostrato di essere vulnerabile a piccole perturbazioni avversarie dell'input. In altre parole, la maggior parte delle immagini nella distribuzione dei dati sono sia correttamente classificate dal modello che sono molto vicine a un'immagine visivamente simile non classificata. Come primo passo verso l'esplorazione di questa ipotesi, studiamo un semplice dataset sintetico di classificazione tra due sfere concentriche ad alta dimensione. In particolare, dimostriamo che qualsiasi modello che sbagli a classificare una piccola frazione costante di una sfera sarà vulnerabile a perturbazioni avversarie di dimensioni $O(1/qrt{d})$. Sorprendentemente, quando addestriamo diverse architetture su questo set di dati, tutti i loro set di errori si avvicinano naturalmente a questo limite teorico. Come risultato della teoria, la vulnerabilità delle reti neurali a piccole perturbazioni avversarie è una conseguenza logica della quantità di errore di prova osservata.Speriamo che la nostra analisi teorica di questo caso molto semplice indichi la strada da seguire per esplorare come la geometria di complessi set di dati del mondo reale porti ad esempi avversativi.
È stato stabilito che diversi comportamenti che coprono il sottospazio controllabile di un processo decisionale di Markov possono essere addestrati premiando una politica per essere distinguibile da altre politiche.Tuttavia, una limitazione di questa formulazione è la difficoltà di generalizzare oltre l'insieme finito di comportamenti esplicitamente appresi, come può essere necessario in compiti successivi.Le caratteristiche di successione forniscono una soluzione attraente a questo problema di generalizzazione, ma richiedono la definizione della funzione di ricompensa come lineare in qualche spazio di caratteristiche fondato.In questo documento, mostriamo che queste due tecniche possono essere combinate, e che ogni metodo risolve la limitazione principale dell'altro. Per fare questo introduciamo Variational Intrinsic Successor FeatuRes (VISR), un nuovo algoritmo che impara caratteristiche controllabili che possono essere sfruttate per fornire una migliore generalizzazione e una rapida inferenza dei compiti attraverso il framework delle caratteristiche del successore. Convalidiamo empiricamente VISR sull'intera suite Atari, in una nuova configurazione in cui le ricompense sono esposte solo brevemente dopo una lunga fase non supervisionata.raggiungendo prestazioni di livello umano su 12 giochi e battendo tutte le linee di base, crediamo che VISR rappresenti un passo verso agenti che imparano rapidamente da un feedback limitato.
La previsione di output strutturati come la segmentazione semantica si basa su costose annotazioni per pixel per imparare forti modelli supervisionati come le reti neurali convoluzionali, ma questi modelli addestrati su un dominio di dati possono non generalizzarsi bene ad altri domini non dotati di annotazioni per la messa a punto del modello. Per evitare il laborioso processo di annotazione, sviluppiamo un metodo di adattamento del dominio per adattare i dati di origine al dominio di destinazione non etichettato.A tal fine, proponiamo di imparare rappresentazioni discriminanti delle caratteristiche delle patch basate sugli istogrammi delle etichette nel dominio di origine, attraverso la costruzione di uno spazio disentangled. Con tali rappresentazioni come guida, usiamo poi uno schema di apprendimento avversario per spingere le rappresentazioni delle caratteristiche nelle patch di destinazione alle distribuzioni più vicine in quelle di origine.Inoltre, dimostriamo che il nostro quadro può integrare un processo di allineamento globale con l'allineamento proposto a livello di patch e raggiungere prestazioni allo stato dell'arte sulla segmentazione semantica.Estesi studi di ablazione ed esperimenti sono condotti su numerosi set di dati di riferimento con varie impostazioni, come gli scenari da sintetico a reale e cross-city.
Molti algoritmi di apprendimento automatico sono vulnerabili a perturbazioni quasi impercettibili dei loro input.Finora non era chiaro quanto rischio comportino le perturbazioni avversarie per la sicurezza delle applicazioni di apprendimento automatico nel mondo reale perché la maggior parte dei metodi utilizzati per generare tali perturbazioni si basano su informazioni dettagliate del modello (attacchi basati sul gradiente) o su punteggi di fiducia come le probabilità di classe (attacchi basati sul punteggio), nessuno dei quali è disponibile nella maggior parte degli scenari del mondo reale. In molti di questi casi si deve attualmente ripiegare su attacchi basati sul trasferimento che si basano su modelli sostitutivi ingombranti, hanno bisogno di accedere ai dati di formazione e possono essere difesi contro.Qui sottolineiamo l'importanza degli attacchi che si basano esclusivamente sulla decisione finale del modello.Tali attacchi basati sulla decisione sono (1) applicabili ai modelli black-box del mondo reale come le auto autonome, (2) richiedono meno conoscenze e sono più facili da applicare rispetto agli attacchi basati sul trasferimento e (3) sono più robusti alle difese semplici rispetto agli attacchi basati sul gradiente o sul punteggio. Gli attacchi precedenti in questa categoria erano limitati a semplici modelli o semplici set di dati. qui introduciamo l'attacco Boundary, un attacco basato sulla decisione che parte da una grande perturbazione avversaria e poi cerca di ridurre la perturbazione pur rimanendo avversaria. l'attacco è concettualmente semplice, non richiede quasi nessuna regolazione degli iperparametri, non si basa su modelli sostitutivi ed è competitivo con i migliori attacchi basati sul gradiente in compiti standard di computer vision come ImageNet. Applichiamo l'attacco a due algoritmi black-box di Clarifai.com.Il Boundary Attack in particolare e la classe degli attacchi basati sulla decisione in generale aprono nuove strade per studiare la robustezza dei modelli di apprendimento automatico e sollevano nuove questioni riguardanti la sicurezza dei sistemi di apprendimento automatico distribuiti.Un'implementazione dell'attacco è disponibile come parte di Foolbox (https://github.com/bethgelab/foolbox).
Dimostriamo che è possibile addestrare grandi modelli linguistici ricorrenti con garanzie di privacy differenziale a livello utente con solo un costo trascurabile nell'accuratezza predittiva.  Il nostro lavoro si basa su recenti progressi nell'addestramento di reti profonde su dati suddivisi per utente e sulla contabilità della privacy per la discesa del gradiente stocastico. In particolare, aggiungiamo la protezione della privacy a livello utente all'algoritmo di mediazione federata, che effettua aggiornamenti a grandi passi dai dati a livello utente. Il nostro lavoro dimostra che, dato un set di dati con un numero sufficientemente grande di utenti (un requisito facilmente soddisfatto anche da piccoli set di dati su scala internet), il raggiungimento della privacy differenziale avviene al costo di un aumento dei calcoli, piuttosto che in una diminuzione dell'utilità come nella maggior parte dei lavori precedenti.Troviamo che i nostri modelli linguistici LSTM privati sono quantitativamente e qualitativamente simili ai modelli non annualizzati quando addestrati su un grande set di dati.
Le reti neurali convoluzionali (CNN) sono comunemente addestrate utilizzando una dimensione fissa dell'immagine spaziale predeterminata per un dato modello. Anche se addestrate su immagini di una dimensione specifica, è ben stabilito che le CNN possono essere utilizzate per valutare una vasta gamma di dimensioni dell'immagine al momento del test, regolando la dimensione delle mappe di caratteristiche intermedie. In questo lavoro, descriviamo e valutiamo un nuovo regime di addestramento a dimensioni miste che mescola diverse dimensioni delle immagini al momento dell'addestramento, dimostrando che i modelli addestrati con il nostro metodo sono più resistenti ai cambiamenti di dimensione dell'immagine e generalizzano bene anche su immagini piccole, il che consente un'inferenza più rapida utilizzando immagini più piccole al momento del test. 43% di accuratezza top-1 utilizzando ResNet50 con una dimensione dell'immagine di 160, che corrisponde all'accuratezza del modello di base con 2 volte meno calcoli.Inoltre, per una data dimensione dell'immagine utilizzata al momento del test, dimostriamo che questo metodo può essere sfruttato sia per accelerare l'allenamento che l'accuratezza del test finale.Per esempio, siamo in grado di raggiungere un'accuratezza del 79,27% con un modello valutato a una dimensione spaziale di 288 per un miglioramento relativo del 14% rispetto alla linea di base.
Proponiamo una tecnica semplice per incoraggiare le RNN generative a pianificare in anticipo: addestriamo una rete ricorrente ``indietro'' per generare una data sequenza in ordine inverso, e incoraggiamo gli stati del modello in avanti a prevedere gli stati cotemporali del modello all'indietro. Ipotizziamo che il nostro approccio faciliti la modellazione delle dipendenze a lungo termine forzando implicitamente gli stati forward a contenere informazioni sul futuro a lungo termine (contenute negli stati backward). Mostriamo empiricamente che il nostro approccio raggiunge il 9% di miglioramento relativo per un compito di riconoscimento vocale, e raggiunge un miglioramento significativo su un compito di generazione di didascalie COCO.
I modelli generativi profondi cercano di recuperare il processo con cui sono stati generati i dati osservati, e possono essere utilizzati per sintetizzare nuovi campioni o per estrarre successivamente delle rappresentazioni. Gli approcci di successo nel dominio delle immagini sono guidati da diversi bias induttivi fondamentali, ma un bias per tenere conto del modo compositivo con cui gli esseri umani strutturano una scena visiva in termini di oggetti è stato spesso trascurato. Questo fornisce un modo per imparare in modo efficiente un modello generativo più accurato delle immagini del mondo reale, e serve come un primo passo verso l'apprendimento delle rappresentazioni degli oggetti corrispondenti.Valutiamo il nostro approccio su diversi set di dati di immagini multi-oggetto, e troviamo che il generatore impara a identificare e a separare le informazioni corrispondenti ai diversi oggetti a livello rappresentazionale.Uno studio umano rivela che il modello generativo risultante è migliore nel generare immagini che sono più fedeli alla distribuzione di riferimento.
La letteratura attuale nell'apprendimento automatico sostiene che gli agenti non allineati e auto-interessati non imparano a usare un canale di comunicazione emergente.Introduciamo un nuovo gioco mittente-ricevente per studiare la comunicazione emergente per questo spettro di scenari parzialmente competitivi e mettiamo particolare cura nella valutazione.Troviamo che la comunicazione può effettivamente emergere in scenari parzialmente competitivi, e scopriamo tre cose che sono legate al suo miglioramento. Primo, che la comunicazione egoistica è proporzionale alla cooperazione, e si verifica naturalmente per situazioni che sono più cooperative che competitive.Secondo, che la stabilità e le prestazioni sono migliorate utilizzando LOLA (Foerster et al, 2018), soprattutto in scenari più competitivi.E terzo, che i protocolli discreti si prestano meglio all'apprendimento della comunicazione cooperativa rispetto a quelli continui.
Le reti neurali profonde (DNN) hanno tipicamente abbastanza capacità di adattarsi a dati casuali con la forza bruta anche quando vengono imposte regolarizzazioni convenzionali dipendenti dai dati che si concentrano sulla geometria delle caratteristiche.Scopriamo che la ragione di questo è l'incoerenza tra la geometria forzata e la perdita di entropia trasversale softmax standard. Per risolvere questo problema, proponiamo una nuova struttura per la regolarizzazione DNN dipendente dai dati, la Geometrically-Regularized-Self-Validating neural Networks (GRSVNet). Durante l'addestramento, la geometria imposta su un lotto di caratteristiche è simultaneamente validata su un lotto separato usando una perdita di validazione coerente con la geometria. Studiamo un caso particolare di GRSVNet, la Orthogonal-Low-rank Embedding (OLE)-GRSVNet, che è in grado di produrre caratteristiche altamente discriminanti che risiedono in sottospazi ortogonali a basso rango.esperimenti numerici dimostrano che OLE-GRSVNet supera le DNN con regolarizzazione convenzionale quando addestrato su dati reali. Ancora più importante, a differenza delle DNN convenzionali, OLE-GRSVNet si rifiuta di memorizzare dati casuali o etichette casuali, suggerendo che impara solo modelli intrinseci riducendo la capacità di memorizzazione della DNN di base.
Il riconoscimento vocale automatico end-to-end (ASR) trascrive comunemente i segnali audio in sequenze di caratteri, mentre le sue prestazioni sono valutate misurando il tasso di errore di parola (WER).Questo suggerisce che prevedere direttamente le sequenze di parole può essere utile invece.Tuttavia, la formazione con supervisione a livello di parola può essere più difficile a causa della scarsità di esempi per classe di etichetta.In questo articolo analizziamo un modello ASR end-to-end che combina una rappresentazione di parole e caratteri in una struttura di apprendimento multi-task (MTL). Mostriamo che migliora il WER e studiamo come il modello a livello di parola possa beneficiare della supervisione a livello di carattere analizzando empiricamente la preferenza induttiva imparata di ogni componente del modello. Troviamo che aggiungendo la supervisione a livello di carattere, il modello MTL interpola tra il riconoscimento di parole più frequenti (preferite dal modello a livello di parola) e parole più brevi (preferite dal modello a livello di carattere).
Discretizzare i vettori in virgola mobile è un passo fondamentale dei moderni metodi di indicizzazione. Le tecniche più moderne imparano i parametri dei quantizzatori sui dati di allenamento per ottenere prestazioni ottimali, adattando così i quantizzatori ai dati. In questo lavoro, proponiamo di invertire questo paradigma e di adattare i dati al quantizzatore: addestriamo una rete neurale i cui ultimi strati formano un quantizzatore fisso senza parametri, come i punti predefiniti di una sfera.Come obiettivo delegato, progettiamo e addestriamo una rete neurale che favorisce l'uniformità nello spazio latente sferico, conservando la struttura del vicinato dopo la mappatura.  A questo scopo, proponiamo un nuovo regolatore derivato dallo stimatore di entropia differenziale di Kozachenko-Leonenko e lo combiniamo con una perdita di tripletta locality-aware. Gli esperimenti mostrano che il nostro approccio end-to-end supera la maggior parte dei metodi di quantizzazione appresi ed è competitivo con lo stato dell'arte su benchmark ampiamente adottati. Inoltre, dimostriamo che l'addestramento senza la fase di quantizzazione non comporta quasi nessuna differenza di accuratezza, ma produce un catalizzatore generico che può essere applicato con qualsiasi tecnica di quantizzazione successiva.
L'apprendimento della differenza temporale (TD) è un algoritmo popolare per la valutazione della politica nell'apprendimento di rinforzo, ma la vaniglia TD può sostanzialmente soffrire della varianza di ottimizzazione inerente.Un algoritmo TD ridotto alla varianza (VRTD) è stato proposto da Korda e La (2015), che applica la tecnica di riduzione della varianza direttamente all'apprendimento TD online con campioni markoviani. In questo lavoro, in primo luogo evidenziamo gli errori tecnici nell'analisi di VRTD in Korda e La (2015), e poi forniamo un'analisi matematicamente solida della convergenza non asintotica di VRTD e delle sue prestazioni di riduzione della varianza. Mostriamo che VRTD è garantito per convergere ad un quartiere della soluzione a punto fisso di TD ad un tasso di convergenza lineare.Inoltre, l'errore di varianza (sia per il campionamento i.i.d. che per quello markoviano) e l'errore di bias (per il campionamento markoviano) di VRTD sono significativamente ridotti dalla dimensione del batch di riduzione della varianza rispetto a quelli di vanilla TD.
Affrontiamo l'adattamento non supervisionato del dominio tenendo conto del fatto che i diversi domini possono aver bisogno di essere elaborati in modo diverso per arrivare a una rappresentazione comune delle caratteristiche efficace per il riconoscimento. A tal fine, introduciamo un quadro di apprendimento profondo in cui ogni dominio è sottoposto a una diversa sequenza di operazioni, consentendo ad alcuni domini, probabilmente più complessi, di passare attraverso più calcoli di altri. Questo contrasta con le tecniche di adattamento dei domini allo stato dell'arte che costringono tutti i domini ad essere elaborati con la stessa serie di operazioni, anche quando si utilizzano architetture multi-stream i cui parametri non sono condivisi.Come evidenziato dai nostri esperimenti, la maggiore flessibilità del nostro metodo si traduce in una maggiore precisione.Inoltre, ci permette di gestire qualsiasi numero di domini contemporaneamente.
L'uso pratico degli agenti di apprendimento di rinforzo è spesso ostacolato dalla durata del tempo di formazione.Per accelerare la formazione, i professionisti spesso si rivolgono alle architetture distribuite di apprendimento di rinforzo per parallelizzare e accelerare il processo di formazione. Tuttavia, i metodi moderni per l'apprendimento di rinforzo scalabile (RL) spesso fanno un compromesso tra il throughput dei campioni da cui un agente RL può imparare (sample throughput) e la qualità dell'apprendimento da ogni campione (efficienza del campione).In queste architetture RL scalabili, come si aumenta il throughput del campione (cioè aumentando la parallelizzazione in IMPALA (Espeholt et al, 2018)), l'efficienza campionaria scende significativamente.Per affrontare questo, proponiamo un nuovo algoritmo di apprendimento di rinforzo distribuito, IMPACT.IMPACT estende PPO con tre modifiche: una rete target per stabilizzare l'obiettivo surrogato, un buffer circolare, e il campionamento di importanza troncata.In ambienti discreti action-space, dimostriamo che IMPACT raggiunge una ricompensa più alta e, contemporaneamente, raggiunge fino al 30% di riduzione del wall-time di formazione rispetto a quello di IMPALA.Per ambienti di controllo continuo, IMPACT si allena più velocemente degli agenti scalabili esistenti, pur conservando l'efficienza campionaria di PPO sincrono.
In questo articolo, mostriamo che un semplice schema di colorazione può migliorare, sia teoricamente che empiricamente, la potenza espressiva delle reti neurali a passaggio di messaggi (MPNNs).Più specificamente, introduciamo una rete neurale a grafo chiamata Colored Local Iterative Procedure (CLIP) che usa i colori per disambiguare gli attributi dei nodi identici, e mostriamo che questa rappresentazione è un approssimatore universale di funzioni continue su grafi con attributi dei nodi. Il nostro metodo si basa sulla separabilità, una caratteristica topologica chiave che permette di estendere reti neurali ben scelte in rappresentazioni universali. Infine, mostriamo sperimentalmente che CLIP è in grado di catturare caratteristiche strutturali che le MPNN tradizionali non riescono a distinguere, mentre è all'avanguardia su dataset di classificazione di grafi di riferimento.
In questo articolo presentiamo un metodo per la generazione algoritmica di melodie usando una rete generativa avversaria senza componenti ricorrenti.La generazione di musica è stata fatta con successo usando reti neurali ricorrenti, dove il modello impara informazioni sulla sequenza che possono aiutare a creare melodie dal suono autentico.  Qui, usiamo l'architettura DCGAN con convoluzioni dilatate e torri per catturare le informazioni sequenziali come informazioni di immagine spaziale, e imparare le dipendenze a lungo raggio nelle forme di melodia a lunghezza fissa come il reel tradizionale irlandese.
I sistemi di traduzione automatica neurale (NMT) hanno raggiunto lo stato dell'arte nella traduzione del testo e sono ampiamente diffusi.  Eppure poco è compreso su come questi sistemi funzionano o si rompono.  Qui mostriamo che i sistemi NMT sono suscettibili di produrre traduzioni altamente patologiche che sono completamente slegate dal materiale di origine, che noi chiamiamo allucinazioni.  Tali traduzioni patologiche sono problematiche perché disturbano profondamente la fiducia dell'utente e sono facili da trovare.  Descriviamo un metodo per generare allucinazioni e mostriamo che molte varianti comuni dell'architettura NMT sono suscettibili ad esse.Studiamo una varietà di approcci per ridurre la frequenza delle allucinazioni, incluso l'aumento dei dati, i sistemi dinamici e le tecniche di regolarizzazione e mostriamo che l'aumento dei dati riduce significativamente la frequenza delle allucinazioni.Infine, analizziamo le reti che producono allucinazioni e mostriamo le firme delle allucinazioni nella matrice di attenzione e nelle misure di stabilità del decoder.
In questo lavoro, identifichiamo due problemi degli attuali metodi esplicativi: in primo luogo, mostriamo che due prospettive prevalenti sulle spiegazioni - feature-additivity e feature-selection - portano a spiegazioni istanze fondamentalmente diverse. Il secondo problema è che gli attuali metodi esplicativi post-hoc sono stati convalidati solo su modelli semplici, come la regressione lineare, e, quando vengono applicati alle reti neurali del mondo reale, gli esplicativi sono comunemente valutati sotto l'ipotesi che i modelli appresi si comportino ragionevolmente. Tuttavia, le reti neurali spesso si basano su correlazioni irragionevoli, anche quando producono decisioni corrette.Introduciamo un quadro di verifica per i metodi esplicativi sotto la prospettiva della feature-selection.Il nostro quadro si basa su un'architettura di rete neurale non banale addestrata su un compito del mondo reale, e per la quale siamo in grado di fornire garanzie sul suo funzionamento interno.Convalidiamo l'efficacia della nostra valutazione mostrando le modalità di fallimento degli spiegatori attuali.Puntiamo affinché questo quadro fornisca una valutazione pubblicamente disponibile,1 off-the-shelf quando è necessaria la prospettiva della feature-selection sulle spiegazioni.
La pianificazione in uno spazio ad alta densità rimane un problema impegnativo, anche con i recenti progressi negli algoritmi e nella potenza di calcolo. Ci ispiriamo alla copia di efferenza e alla teoria della reafferenza sensoriale delle neuroscienze.  Il nostro obiettivo è quello di permettere agli agenti di formare modelli mentali dei loro ambienti per la pianificazione.  Il cervelletto è emulato con una rete predittiva a due flussi, completamente connessa, che riceve come input l'efferenza e le caratteristiche dello stato corrente. Basandoci sulle intuizioni acquisite dai metodi di distillazione della conoscenza, scegliamo come caratteristiche gli output di una rete pre-addestrata, ottenendo una rappresentazione compressa dello stato corrente.  La rappresentazione è scelta in modo tale da permettere una ricerca veloce usando i classici algoritmi di ricerca a grafo. Mostriamo l'efficacia del nostro approccio su un compito di corrispondenza del punto di vista usando un algoritmo modificato di ricerca best-first.
L'evidenza sperimentale indica che i modelli semplici superano le reti profonde complesse su molti compiti di somiglianza non supervisionati.Introducendo il concetto di uno spazio di rappresentazione ottimale, forniamo una semplice risoluzione teorica a questo apparente paradosso.Inoltre, presentiamo una procedura diretta che, senza alcuna riqualificazione o modifica architettonica, permette ai modelli ricorrenti profondi di eseguire altrettanto bene (e talvolta meglio) rispetto ai modelli poco profondi. Per convalidare la nostra analisi, conduciamo una serie di valutazioni empiriche coerenti e introduciamo diversi nuovi modelli di incorporazione delle frasi nel processo.Anche se questo lavoro è presentato nel contesto dell'elaborazione del linguaggio naturale, le intuizioni sono facilmente applicabili ad altri domini che si basano su rappresentazioni distribuite per compiti di trasferimento.
Il test cloze è ampiamente adottato negli esami di lingua per valutare le competenze linguistiche degli studenti. In questo articolo, proponiamo il primo set di dati su larga scala di test cloze progettato dall'uomo, CLOTH, in cui le domande sono state utilizzate negli esami di lingua delle scuole medie e superiori. Con gli spazi vuoti mancanti accuratamente creati dagli insegnanti e le scelte dei candidati appositamente progettate per confondere, CLOTH richiede una comprensione più profonda del linguaggio e una maggiore capacità di attenzione rispetto ai precedenti set di dati cloze generati automaticamente. Indaghiamo la fonte del divario di prestazioni, riconduciamo le carenze del modello ad alcune proprietà distinte di CLOTH e identifichiamo la limitata capacità di comprensione di un contesto a lungo termine come il collo di bottiglia chiave. Inoltre, troviamo che i dati progettati dall'uomo portano a un divario maggiore tra le prestazioni del modello e le prestazioni umane rispetto ai dati generati automaticamente.
Mentre le proprietà di risposta dei neuroni nelle reti neurali artificiali sono simili a quelle del cervello, le architetture di rete sono spesso costrette ad essere diverse. Qui ci chiediamo se una rete neurale può recuperare sia le rappresentazioni neurali che, se l'architettura non è vincolata e ottimizzata, anche le proprietà anatomiche dei circuiti neurali. Lo dimostriamo in un sistema in cui la connettività e l'organizzazione funzionale sono state caratterizzate, cioè il circuito di direzione della testa del roditore e del moscerino della frutta. Abbiamo addestrato reti neurali ricorrenti (RNN) per stimare la direzione della testa attraverso l'integrazione della velocità angolare. Abbiamo scoperto che le due classi distinte di neuroni osservati nel sistema di direzione della testa, i neuroni ad anello e i neuroni Shifter, sono emersi naturalmente nelle reti neurali artificiali come risultato della formazione. Inoltre, l'analisi della connettività e la neurofisiologia in-silico hanno rivelato somiglianze strutturali e meccanicistiche tra le reti artificiali e il sistema di direzione della testa. Nel complesso, i nostri risultati mostrano che l'ottimizzazione delle RNN in un compito guidato dall'obiettivo può ricapitolare la struttura e la funzione dei circuiti biologici, suggerendo che le reti neurali artificiali possono essere utilizzate per studiare il cervello sia a livello di attività neurale che di organizzazione anatomica.
Le reti neurali convoluzionali (CNN) sono computazionalmente intensive, il che limita la loro applicazione su dispositivi mobili.La loro energia è dominata dal numero di moltiplicazioni necessarie per eseguire le convoluzioni.L'algoritmo di filtraggio minimo di Winograd (Lavin, 2015) e il pruning della rete (Han et al, 2015) possono ridurre il numero di operazioni, ma questi due metodi non possono essere combinati direttamente - applicando la trasformazione di Winograd si riempie la sparsità sia nei pesi che nelle attivazioni.Proponiamo due modifiche alle CNN basate su Winograd per permettere a questi metodi di sfruttare la sparsità.Primo, spostiamo l'operazione ReLU nel dominio Winograd per aumentare la sparsità delle attivazioni trasformate. Per i modelli sui dataset CIFAR-10, CIFAR-100 e ImageNet, il nostro metodo riduce il numero di moltiplicazioni rispettivamente di 10.4x, 6.8x e 10.8x con una perdita di accuratezza inferiore allo 0.1%, superando le precedenti baseline di 2.0x-3.0x.Mostriamo anche che spostare ReLU nel dominio Winograd permette una potatura più aggressiva.
In questo articolo presentiamo un nuovo algoritmo di ottimizzazione chiamato Advanced Neuroevolution, il cui scopo è quello di addestrare le reti neurali profonde, ed eventualmente agire come un'alternativa a Stochastic Gradient Descent (SGD) e le sue varianti, se necessario. Abbiamo valutato il nostro algoritmo sul dataset MNIST, così come su diversi problemi di ottimizzazione globale come la funzione Ackley, e troviamo che l'algoritmo funziona relativamente bene in entrambi i casi, superando altri algoritmi di ottimizzazione globale come Particle Swarm Optimization (PSO) e Evolution Strategies (ES).
Sfortunatamente, a causa del gran numero di pesi, tutti gli esempi in un mini-batch condividono tipicamente la stessa perturbazione del peso, limitando così l'effetto di riduzione della varianza dei grandi mini-batch. Introduciamo flipout, un metodo efficiente per decorrelare i gradienti all'interno di un mini-batch campionando implicitamente perturbazioni del peso pseudo-indipendenti per ogni esempio. Empiricamente, flipout raggiunge l'ideale riduzione della varianza lineare per reti completamente connesse, reti convoluzionali e RNNs.Troviamo significative accelerazioni nell'addestramento di reti neurali con perturbazioni gaussiane moltiplicative.Mostriamo che flipout è efficace nel regolarizzare LSTMs, e supera i metodi precedenti. Flipout ci permette anche di vettorializzare le strategie di evoluzione: nei nostri esperimenti, una singola GPU con flipout può gestire lo stesso throughput di almeno 40 core della CPU utilizzando i metodi esistenti, equivalente a una riduzione dei costi di un fattore di 4 su Amazon Web Services.
I modelli generativi profondi come Variational AutoEncoder (VAE) e Generative Adversarial Network (GAN) giocano un ruolo sempre più importante nell'apprendimento automatico e nella computer vision, ma ci sono due problemi fondamentali che ostacolano le loro applicazioni nel mondo reale: la difficoltà di condurre l'inferenza variazionale in VAE e l'assenza funzionale di codifica dei campioni del mondo reale in GAN. In questo articolo, proponiamo un nuovo algoritmo chiamato Latently Invertible Autoencoder (LIA) per affrontare i due problemi di cui sopra in un unico quadro.Una rete invertibile e la sua mappatura inversa sono simmetricamente incorporati nello spazio latente di VAE. Così il codificatore parziale prima trasforma l'input in vettori di caratteristiche e poi la distribuzione di questi vettori di caratteristiche è rimodellata per adattarsi a un priore dalla rete invertibile.Il decodificatore procede nell'ordine inverso delle mappature composte del codificatore. Uno schema di addestramento a due stadi senza stocasticità è progettato per addestrare LIA attraverso l'apprendimento avversario, nel senso che il decoder di LIA viene prima addestrato come una GAN standard con la rete invertibile e poi il codificatore parziale viene appreso da un autoencoder staccando la rete invertibile da LIA.  Gli esperimenti condotti sul dataset FFHQ face e su tre dataset LSUN convalidano l'efficacia di LIA per l'inferenza e la generazione.
I programmi neurali sono politiche altamente accurate e strutturate che eseguono compiti algoritmici controllando il comportamento di un meccanismo di calcolo.Nonostante il potenziale per aumentare l'interpretabilità e la componibilità del comportamento degli agenti artificiali, rimane difficile apprendere da dimostrazioni di reti neurali che rappresentano programmi per computer.Le sfide principali che distinguono i domini algoritmici da altri domini di apprendimento per imitazione sono la necessità di alta precisione, il coinvolgimento di strutture specifiche di dati, e l'osservabilità estremamente limitata.Per affrontare queste sfide, proponiamo di modellare i programmi come Parametrized Hierarchical Procedures (PHPs). Una PHP è una sequenza di operazioni condizionali, che utilizza un contatore di programma insieme all'osservazione per scegliere tra intraprendere un'azione elementare, invocare un'altra PHP come sottoprocedura e tornare al chiamante. Sviluppiamo un algoritmo per l'addestramento di PHP da un insieme di dimostrazioni di supervisione, solo alcune delle quali sono annotate con la struttura interna delle chiamate, e lo applichiamo per un efficiente addestramento level-wise di PHP multilivello.Mostriamo in due benchmark, NanoCraft e l'aggiunta a mano lunga, che i PHP possono imparare programmi neurali in modo più accurato da piccole quantità di dimostrazioni sia annotate che non annotate.
Il Deep Reinforcement Learning è riuscito a raggiungere risultati all'avanguardia nell'apprendimento delle politiche di controllo direttamente dai pixel grezzi, ma nonostante il suo notevole successo, non riesce a generalizzare, una componente fondamentale richiesta in un sistema di intelligenza artificiale stabile. Utilizzando il gioco Atari Breakout, dimostriamo la difficoltà di un agente addestrato ad adattarsi a semplici modifiche nell'immagine grezza, a cui un umano potrebbe adattarsi banalmente. Nell'apprendimento di trasferimento, l'obiettivo è quello di utilizzare la conoscenza acquisita dal compito di origine per rendere l'addestramento del compito di destinazione più veloce e migliore.Mostriamo che l'utilizzo di varie forme di fine-tuning, un metodo comune per l'apprendimento di trasferimento, non è efficace per adattarsi a tali piccoli cambiamenti visivi.Infatti, è spesso più facile ri-addestrare l'agente da zero che mettere a punto un agente addestrato. Suggeriamo che in alcuni casi l'apprendimento di trasferimento può essere migliorato aggiungendo un componente dedicato il cui obiettivo è quello di imparare a mappare visivamente tra il dominio conosciuto e quello nuovo.Concretamente, usiamo le Reti Generative Adversariali Non Allineate (GAN) per creare una funzione di mappatura per tradurre le immagini nel compito target in immagini corrispondenti nel compito sorgente. Queste funzioni di mappatura ci permettono di trasformare tra varie varianti del gioco Breakout, così come tra diversi livelli di un gioco Nintendo, Road Fighter.Mostriamo che l'apprendimento di questa mappatura è sostanzialmente più efficiente del ri-addestramento.Una visualizzazione di un agente addestrato che gioca a Breakout e Road Fighter, con e senza il trasferimento GAN, può essere vista in \url{https://streamable.com/msgtm} e \url{https://streamable.com/5e2ka}.
Un luogo comune nella comunità dell'apprendimento automatico è che l'uso di metodi a gradiente adattivo nuoce alla generalizzazione. Riesaminiamo questa convinzione sia teoricamente che sperimentalmente, alla luce delle intuizioni e delle tendenze degli ultimi anni.rivisitiamo alcuni esperimenti precedenti spesso citati e i conti teorici in modo più approfondito, e forniamo una nuova serie di esperimenti su larga scala, impostazioni all'avanguardia. Concludiamo che, con una corretta messa a punto, il miglioramento delle prestazioni di addestramento degli ottimizzatori adattativi non comporta in generale una penalità di overfitting, specialmente nell'apprendimento profondo contemporaneo.Infine, sintetizziamo una ``guida per l'utente'' agli ottimizzatori adattativi, comprese alcune modifiche proposte ad AdaGrad per mitigare alcune delle sue carenze empiriche.
La capacità di generalizzare rapidamente da poche osservazioni è cruciale per i sistemi intelligenti.In questo articolo introduciamo APL, un algoritmo che approssima le distribuzioni di probabilità ricordando le osservazioni più sorprendenti che ha incontrato.Queste osservazioni passate sono richiamate da un modulo di memoria esterna ed elaborate da una rete di decodifica che può combinare le informazioni da diversi slot di memoria per generalizzare oltre il richiamo diretto.Mostriamo che questo algoritmo può funzionare bene come lo stato dell'arte su benchmark di classificazione a pochi scatti con un minore ingombro di memoria.  Inoltre, la sua compressione della memoria gli permette di scalare a migliaia di etichette sconosciute.  Infine, introduciamo un compito di ragionamento di meta-apprendimento che è più impegnativo della classificazione diretta. In questa impostazione, APL è in grado di generalizzare con meno di un esempio per classe attraverso il ragionamento deduttivo.
I recenti progressi nelle reti neurali ricorrenti (RNN) hanno mostrato molte promesse in molte applicazioni nell'elaborazione del linguaggio naturale.Per la maggior parte di questi compiti, come l'analisi del sentimento delle recensioni dei clienti, un modello di rete neurale ricorrente analizza l'intera recensione prima di formare una decisione.Noi sosteniamo che leggere l'intero input non è sempre necessario nella pratica, poiché molte recensioni sono spesso facili da classificare, cioè, In questo articolo, presentiamo un approccio di lettura veloce per la classificazione del testo.Ispirato da diverse tecniche di lettura umana ben note, il nostro approccio implementa un agente ricorrente intelligente che valuta l'importanza del frammento corrente per decidere se fare una previsione, o saltare alcuni testi, o rileggere una parte della frase. Il nostro agente usa un modulo RNN per codificare le informazioni dal passato e dai token attuali, e applica un modulo di policy per formare le decisioni. Con un algoritmo di formazione end-to-end basato sul gradiente di policy, addestriamo e testiamo il nostro agente su diversi set di dati di classificazione del testo e otteniamo sia una maggiore efficienza che una migliore accuratezza rispetto agli approcci precedenti. 
La soluzione ottimale di Bayes per l'esplorazione è intrattabile per ambienti complessi, e mentre diversi metodi di esplorazione sono stati proposti come approssimazioni, non è ancora chiaro quale obiettivo sottostante sia ottimizzato dai metodi di esplorazione esistenti, o come possano essere modificati per incorporare la conoscenza preliminare sul compito. Inoltre, non è chiaro come acquisire una singola strategia di esplorazione che sia utile per risolvere più compiti a valle. Noi affrontiamo queste carenze imparando una singola politica di esplorazione che può risolvere rapidamente una serie di compiti a valle in un ambiente multi-task, ammortizzando il costo dell'apprendimento dell'esplorazione. Rifondiamo l'esplorazione come un problema di State Marginal Matching (SMM), dove miriamo a imparare una politica per la quale la distribuzione marginale dello stato corrisponde a una data distribuzione di stato di destinazione, che può incorporare la conoscenza precedente sul compito. ottimizziamo l'obiettivo riducendolo a un gioco a somma zero a due giocatori tra un modello di densità di stato e una politica parametrica. La nostra analisi teorica di questo approccio suggerisce che i metodi di esplorazione precedenti non imparano una politica che fa la corrispondenza della distribuzione, ma acquisiscono un buffer di replay che esegue la corrispondenza della distribuzione, un'osservazione che spiega potenzialmente il successo di questi metodi precedenti nelle impostazioni a compito singolo.
L'efficacia delle reti neurali convoluzionali deriva in gran parte dalla loro capacità di sfruttare l'invarianza di traduzione che è inerente a molti problemi di apprendimento.Recentemente, è stato dimostrato che le CNN possono sfruttare altre invarianze, come l'invarianza di rotazione, utilizzando convoluzioni di gruppo invece di convoluzioni planari. Tuttavia, per ragioni di prestazioni e facilità di implementazione, è stato necessario limitare la convoluzione di gruppo alle trasformazioni che possono essere applicate ai filtri senza interpolazione.Quindi, per le immagini con pixel quadrati, solo le traslazioni intere, le rotazioni per multipli di 90 gradi e le riflessioni sono ammissibili.Mentre la piastrellatura quadrata fornisce una simmetria rotazionale di 4 volte, una piastrellatura esagonale del piano ha una simmetria rotazionale di 6 volte. In questo articolo mostriamo come si possa implementare in modo efficiente la convoluzione planare e la convoluzione di gruppo su reticoli esagonali, riutilizzando le routine di convoluzione altamente ottimizzate esistenti. Troviamo che, a causa della ridotta anisotropia dei filtri esagonali, l'HexaConv planare fornisce una migliore accuratezza rispetto alla convoluzione planare con filtri quadrati, dato un budget fisso di parametri. Inoltre, troviamo che il maggior grado di simmetria della griglia esagonale aumenta l'efficacia delle convoluzioni di gruppo, consentendo una maggiore condivisione dei parametri. Mostriamo che il nostro metodo supera significativamente le CNN convenzionali sul dataset di classificazione delle scene aeree AID, superando anche i modelli pre-addestrati ImageNet.
Le reti neurali convoluzionali profonde (CNN) hanno ripetutamente dimostrato di funzionare bene nei compiti di classificazione delle immagini, riconoscendo con successo una vasta gamma di oggetti quando viene data una quantità sufficiente di dati di allenamento. I metodi per la localizzazione degli oggetti, tuttavia, hanno ancora bisogno di un miglioramento sostanziale. In generale, questi approcci sono dispendiosi in termini di tempo, richiedendo molti calcoli di classificazione.In questo documento, offriamo un approccio fondamentalmente diverso alla localizzazione degli oggetti riconosciuti nelle immagini.Il nostro metodo si basa sull'idea che una CNN profonda in grado di riconoscere un oggetto deve contenere implicitamente la conoscenza della posizione dell'oggetto nei suoi pesi di connessione. Forniamo un metodo semplice per interpretare i pesi del classificatore nel contesto delle singole immagini classificate. Questo metodo comporta il calcolo della derivata dei modelli di attivazione generati dalla rete, come l'attivazione delle unità di etichettatura di classe in uscita, rispetto a ciascun pixel inserito, eseguendo un'analisi di sensibilità che identifica i pixel che, in senso locale, hanno la maggiore influenza sulle rappresentazioni interne e sul riconoscimento degli oggetti. Queste derivate possono essere calcolate in modo efficiente utilizzando un singolo passaggio all'indietro attraverso il classificatore CNN profondo, producendo una mappa di sensibilità dell'immagine. dimostriamo che una semplice mappatura lineare può essere appresa dalle mappe di sensibilità alle coordinate del bounding box, localizzando l'oggetto riconosciuto. i nostri risultati sperimentali, utilizzando set di dati del mondo reale per i quali sono note informazioni di localizzazione della verità di base, rivelano una precisione competitiva dalla nostra tecnica veloce.
Da un lato, una rete a traliccio è una rete convoluzionaria temporale con una struttura speciale, caratterizzata da legami di peso attraverso la profondità e l'iniezione diretta dell'input negli strati profondi. Dall'altro lato, mostriamo che le reti ricorrenti troncate sono equivalenti alle reti a traliccio con una speciale struttura di sparsità nelle loro matrici di peso. Gli esperimenti dimostrano che le reti a traliccio superano l'attuale stato dell'arte dei metodi su una varietà di benchmark impegnativi, inclusi i compiti di modellazione linguistica a livello di parola e a livello di carattere, e gli stress test progettati per valutare la conservazione della memoria a lungo termine.Il codice è disponibile all'indirizzo https://github.com/locuslab/trellisnet .
Proponiamo un framework end-to-end per l'addestramento di modelli specifici del dominio (DSM) per ottenere sia un'alta accuratezza che un'efficienza computazionale per i compiti di rilevamento degli oggetti.I DSM sono addestrati con la distillazione e si concentrano sul raggiungimento di un'alta accuratezza in un dominio limitato (ad esempio la vista fissa di un incrocio).Sosteniamo che i DSM possono catturare bene le caratteristiche essenziali anche con un modello di piccole dimensioni, permettendo una maggiore accuratezza ed efficienza rispetto alle tecniche tradizionali.  Inoltre, miglioriamo l'efficienza dell'addestramento riducendo la dimensione del set di dati eliminando le immagini facili da classificare dal set di addestramento.Per il dominio limitato, abbiamo osservato che i DSM compatti superano significativamente l'accuratezza dei modelli addestrati COCO della stessa dimensione.Addestrandosi su un set di dati compatto, dimostriamo che con un calo di precisione di solo il 3,6%, il tempo di addestramento può essere ridotto del 93%.
Confrontiamo l'apprendimento di rinforzo senza modello con gli approcci basati sul modello attraverso la lente della potenza espressiva delle reti neurali per le politiche, le funzioni $Q$ e le dinamiche.  Mostriamo, teoricamente ed empiricamente, che anche per uno spazio di stato continuo unidimensionale, ci sono molti MDP le cui funzioni $Q$ ottimali e le politiche sono molto più complesse delle dinamiche, e ipotizziamo che anche molti MDP del mondo reale abbiano una proprietà simile. Per questi MDP, la pianificazione basata sul modello è un algoritmo favorevole, perché le politiche risultanti possono approssimare la politica ottimale molto meglio di una parametrizzazione della rete neurale, e l'ottimizzazione della politica senza modello o basata sul modello si basa sulla parametrizzazione della politica. Motivati dalla teoria, applichiamo un semplice pianificatore di bootstrapping basato sul modello a più passi (BOOTS) per fare il bootstrap di una funzione $Q$ debole in una politica più forte. I risultati empirici mostrano che l'applicazione di BOOTS in cima agli algoritmi di ottimizzazione della politica basati sul modello o senza modello al momento del test migliora le prestazioni sui compiti di benchmark MuJoCo.
Il ragionamento fisico di buon senso è un ingrediente essenziale per qualsiasi agente intelligente che operi nel mondo reale, per esempio, può essere usato per simulare l'ambiente o per dedurre lo stato di parti del mondo che non sono attualmente osservate. Per corrispondere alle condizioni del mondo reale, questa conoscenza causale deve essere appresa senza accesso a dati supervisionati. Incorpora la conoscenza precedente sulla natura compositiva della percezione umana per fattorizzare le interazioni tra coppie di oggetti e imparare in modo efficiente. Su video di palline che rimbalzano mostriamo le capacità di modellazione superiori del nostro metodo rispetto ad altri approcci neurali non supervisionati che non incorporano tale conoscenza precedente. dimostriamo la sua capacità di gestire l'occlusione e dimostriamo che può estrapolare la conoscenza appresa a scene con diverso numero di oggetti.
L'idea che le reti neurali possano esibire un pregiudizio verso la semplicità ha una lunga storia: il pregiudizio della semplicità fornisce un modo per quantificare questa intuizione.  Prevede, per un'ampia classe di mappe input-output che possono descrivere molti sistemi nella scienza e nell'ingegneria, che le uscite semplici hanno esponenzialmente più probabilità di verificarsi su un campionamento casuale uniforme degli input rispetto alle uscite complesse.  Questo comportamento di bias della semplicità è stato osservato per sistemi che vanno dalla sequenza RNA alla mappa della struttura secondaria, ai sistemi di equazioni differenziali accoppiate, ai modelli di crescita delle piante.   Le reti neurali profonde possono essere viste come una mappatura dallo spazio dei parametri (i pesi) allo spazio delle funzioni (come gli input vengono trasformati in output dalla rete).  Mostriamo che questa mappa parametro-funzione obbedisce alle condizioni necessarie per la polarizzazione della semplicità, e mostriamo numericamente che è enormemente polarizzata verso funzioni con bassa complessità descrittiva.  Dimostriamo anche una relazione probabilità-ranco di Zipf come power-law.   Un bias verso la semplicità può aiutare a spiegare perché le reti neurali generalizzano così bene.
L'apprendimento per imitazione (IL) è un approccio attraente per imparare un comportamento autonomo desiderabile.Tuttavia, dirigere IL per raggiungere obiettivi arbitrari è difficile.Al contrario, gli algoritmi basati sulla pianificazione usano modelli dinamici e funzioni di ricompensa per raggiungere gli obiettivi.Tuttavia, le funzioni di ricompensa che evocano un comportamento desiderabile sono spesso difficili da specificare.In questo articolo, proponiamo "Modelli Imitativi" per combinare i benefici di IL e della pianificazione diretta agli obiettivi. I modelli imitativi sono modelli predittivi probabilistici di comportamenti desiderabili in grado di pianificare traiettorie interpretabili come quelle degli esperti per raggiungere obiettivi specificati; deriviamo famiglie di obiettivi flessibili, tra cui regioni di obiettivi vincolati, set di obiettivi non vincolati e obiettivi basati sull'energia; dimostriamo che il nostro metodo può utilizzare questi obiettivi per dirigere con successo il comportamento; il nostro metodo supera sostanzialmente sei approcci IL e un approccio basato sulla pianificazione in un compito dinamico di guida autonoma simulata, e viene appreso in modo efficiente da dimostrazioni di esperti senza raccolta dati online.  Mostriamo anche che il nostro approccio è robusto agli obiettivi mal specificati, come gli obiettivi sul lato sbagliato della strada.
Tuttavia, imparare quali informazioni comunicate sono utili per il processo decisionale di ciascun agente rimane un compito impegnativo. Per affrontare questo problema, introduciamo una struttura completamente differenziabile per la comunicazione e il ragionamento, consentendo agli agenti di risolvere compiti cooperativi in ambienti parzialmente osservabili. La struttura è progettata per facilitare il ragionamento esplicito tra gli agenti, attraverso una nuova rete di attenzione basata sulla memoria che può imparare selettivamente dai suoi ricordi passati. Il modello comunica attraverso una serie di fasi di ragionamento che decompongono le intenzioni di ogni agente in rappresentazioni apprese che vengono utilizzate in primo luogo per calcolare la rilevanza delle informazioni comunicate, e in secondo luogo per estrarre informazioni dalle memorie date le nuove informazioni ricevute. Interagendo selettivamente con le nuove informazioni, il modello apprende efficacemente un protocollo di comunicazione direttamente, in un modo end-to-end.dimostriamo empiricamente la forza del nostro modello in compiti cooperativi multi-agente, dove la comunicazione inter-agente e il ragionamento sulle informazioni precedenti migliora sostanzialmente le prestazioni rispetto alle linee base.
In questo articolo, introduciamo Symplectic ODE-Net (SymODEN), una struttura di apprendimento profondo che può dedurre la dinamica di un sistema fisico dalle traiettorie di stato osservate.Per ottenere una migliore generalizzazione con un minor numero di campioni di allenamento, SymODEN incorpora un appropriato bias induttivo progettando il grafico di calcolo associato in un modo informato dalla fisica.In particolare, applichiamo la dinamica hamiltoniana con controllo per imparare la dinamica sottostante in un modo trasparente che può poi essere sfruttato per trarre intuizioni su aspetti fisici rilevanti del sistema, come la massa e l'energia potenziale. Inoltre, proponiamo una parametrizzazione che può applicare questo formalismo hamiltoniano anche quando i dati delle coordinate generalizzate sono incorporati in uno spazio ad alta densità o possiamo accedere solo ai dati di velocità invece che al momento generalizzato. Questo quadro, offrendo modelli interpretabili e fisicamente coerenti per i sistemi fisici, apre nuove possibilità per sintetizzare strategie di controllo basate sul modello.
L'apprendimento federato, in cui un modello globale è addestrato dalla media iterativa dei parametri degli aggiornamenti calcolati localmente, è un approccio promettente per l'addestramento distribuito delle reti profonde; fornisce un'elevata efficienza di comunicazione e privacy-preservability, che permette di adattarsi bene in ambienti di dati decentralizzati, ad esempio, gli ecosistemi mobile-cloud.Tuttavia, nonostante i vantaggi, i metodi basati sull'apprendimento federato hanno ancora una sfida nel trattare con i dati di formazione non-IID dei dispositivi locali (cioè, A questo proposito, studiamo gli effetti di una varietà di condizioni iperparametriche negli ambienti non-IID, per rispondere a importanti preoccupazioni nelle implementazioni pratiche: (i) in primo luogo indaghiamo la divergenza dei parametri degli aggiornamenti locali per spiegare la degradazione delle prestazioni dai dati non-IID.L'origine della divergenza dei parametri è anche trovata sia empiricamente che teoricamente. (ii) Poi rivisitiamo gli effetti degli ottimizzatori, della profondità/larghezza della rete e delle tecniche di regolarizzazione; le nostre osservazioni mostrano che i ben noti vantaggi delle strategie di ottimizzazione degli iperparametri potrebbero piuttosto produrre rendimenti decrescenti con i dati non-IID.(iii) Infine forniamo le ragioni dei casi di fallimento in modo categorizzato, principalmente basate sulle metriche della divergenza dei parametri.
I modelli di apprendimento profondo sono noti per essere vulnerabili agli esempi avversari.Un attacco avversario pratico dovrebbe richiedere il minor numero possibile di conoscenze dei modelli attaccati T. Gli attuali attacchi sostitutivi hanno bisogno di modelli pre-addestrati per generare esempi avversari e i loro tassi di successo dell'attacco dipendono fortemente dalla trasferibilità degli esempi avversari. Gli attuali attacchi basati sul punteggio e sulla decisione richiedono molte interrogazioni per il T. In questo studio, proponiamo un nuovo attacco di imitazione avversaria, che produce una replica del T tramite un gioco a due giocatori come le reti generative avversarie (GAN). L'obiettivo del modello generativo G è quello di generare esempi che portino D a restituire output diversi da T. L'obiettivo del modello discriminativo D è quello di produrre le stesse etichette con T sotto gli stessi input, quindi gli esempi avversari generati da D sono utilizzati per ingannare T. Rispetto agli attuali attacchi sostitutivi, l'attacco di imitazione può utilizzare meno dati di formazione per produrre una replica di T e migliorare la trasferibilità degli esempi avversari.Gli esperimenti dimostrano che il nostro attacco di imitazione richiede meno dati di formazione rispetto agli attacchi sostitutivi black-box, ma raggiunge una percentuale di successo dell'attacco white-box su dati non visti e senza query.
I metodi Stochastic Gradient Descent (SGD) che utilizzano lotti selezionati casualmente sono ampiamente utilizzati per addestrare modelli di reti neurali (NN). L'esplorazione del design per trovare il miglior NN per una particolare attività spesso richiede un ampio addestramento con diversi modelli su un grande set di dati, che è molto costoso dal punto di vista computazionale. Il metodo più diretto per accelerare questo calcolo è quello di distribuire il lotto di SGD su più processori.  Le soluzioni esistenti per l'addestramento di grande lotto o non funzionano o richiedono la sintonizzazione massiccia di iper-parametro.Per affrontare questo problema, proponiamo un metodo di addestramento di grande lotto novello che combina i risultati recenti nell'addestramento avversario (per regolarizzare contro i ``minimi acuti'') e l'ottimizzazione di secondo ordine (per usare le informazioni di curvatura per cambiare la dimensione del lotto adaptively durante l'addestramento).valutiamo estesamente il nostro metodo su Cifar-10/100, SVHN, TinyImageNet e sui set di dati di ImageNet, usando NNs multiplo, compreso le reti residue cosÃ¬ come le reti compresse quale SqueezeNext.  Il nostro nuovo approccio supera le prestazioni delle soluzioni esistenti sia in termini di accuratezza che di numero di iterazioni SGD (rispettivamente fino all'1\% e $3\times$). Sottolineiamo che questo risultato è ottenuto senza alcun iper-parametro aggiuntivo per adattare il nostro metodo a qualsiasi di questi esperimenti.
Ispirati dalle scoperte neurofisiologiche delle cellule di navigazione nel cervello dei mammiferi, introduciamo la prima architettura di rete neurale profonda per la modellazione della memoria spaziale egocentrica (ESM), che impara a stimare la posa dell'agente e costruisce progressivamente mappe globali 2D top-down da viste egocentriche in un ambiente spazialmente esteso. Durante l'esplorazione, il nostro modello di rete ESM proposto aggiorna la credenza della mappa globale sulla base di osservazioni locali utilizzando una rete neurale ricorrente, inoltre aumenta la mappatura locale con una nuova memoria esterna per codificare e memorizzare le rappresentazioni latenti dei luoghi visitati sulla base delle loro posizioni corrispondenti nella coordinata egocentrica, il che consente agli agenti di eseguire la chiusura del ciclo e la correzione della mappatura. Questo lavoro contribuisce nei seguenti aspetti: in primo luogo, la nostra rete ESM proposta fornisce un'accurata mappabilità che è di vitale importanza per gli agenti incarnati per navigare verso le posizioni dell'obiettivo. Negli esperimenti, dimostriamo le funzionalità della rete ESM in passeggiate casuali in complicati labirinti 3D, confrontando con diverse linee base competitive e algoritmi di localizzazione e mappatura simultanea (SLAM) all'avanguardia. In secondo luogo, ipotizziamo fedelmente la funzionalità e il meccanismo di lavoro delle cellule di navigazione nel cervello. L'analisi completa dei nostri modelli evidenzia il ruolo essenziale dei singoli moduli nella nostra architettura proposta e dimostra l'efficienza delle comunicazioni tra questi moduli.
Dimostriamo che se la solita perdita di formazione è aumentata da un termine di regolarizzazione Lipschitz, allora le reti generalizzano.  Proviamo la generalizzazione stabilendo prima un risultato di convergenza più forte, insieme a un tasso di convergenza.   Un secondo risultato risolve una questione posta in Zhang et al. (2016): come può un modello distinguere tra il caso di etichette pulite e quello di etichette randomizzate?  La nostra risposta è che la regolarizzazione Lipschitz utilizzando la costante Lipschitz dei dati puliti fa questa distinzione.  In questo caso, il modello impara una funzione diversa che noi ipotizziamo correttamente non riesca ad imparare le etichette sporche.  
Per una distribuzione veloce ed efficiente dal punto di vista energetico delle reti neurali profonde addestrate su hardware embedded con risorse limitate, ogni parametro di peso appreso dovrebbe idealmente essere rappresentato e memorizzato utilizzando un singolo bit.  Il nostro approccio semplifica i metodi esistenti che binarizzano i pesi applicando la funzione di segno nella formazione; applichiamo fattori di scala per ogni strato con valori costanti non appresi pari alle deviazioni standard specifiche dello strato utilizzate per l'inizializzazione. Per CIFAR-10, CIFAR-100 e ImageNet, e modelli con 1-bit-per-weight che richiedono meno di 10 MB di memoria dei parametri, otteniamo tassi di errore del 3,9%, 18,5% e 26,0% / 8,5% (Top-1 / Top-5) rispettivamente.Abbiamo anche considerato MNIST, SVHN e ImageNet32, ottenendo risultati di test 1-bit-per-weight dello 0. 27%, 1.9%, e 41.3% / 19.1% rispettivamente.Per CIFAR, i nostri tassi di errore dimezzano i valori precedentemente riportati, e sono entro circa l'1% dei nostri tassi di errore per la stessa rete con pesi di piena precisione.Per le reti che sovrafittano, mostriamo anche miglioramenti significativi nel tasso di errore non imparando la scala di normalizzazione batch e i parametri di offset.Questo vale sia per la precisione completa che per le reti 1-bit-per-weight. Utilizzando un programma di apprendimento a caldo, abbiamo scoperto che l'addestramento per le reti 1-bit-per-weight è altrettanto veloce delle reti a precisione completa, con una migliore accuratezza rispetto ai programmi standard, e ha raggiunto circa il 98%-99% delle prestazioni di picco in appena 62 epoche di addestramento per CIFAR-10/100.Per il codice di addestramento completo e i modelli addestrati in MATLAB, Keras e PyTorch vedere https://github.com/McDonnell-Lab/1-bit-per-weight/ .
Questo articolo presenta un sistema per la visualizzazione immersiva di spazi non euclidei utilizzando il ray-tracing in tempo reale, sfruttando le capacità della nuova generazione di GPU basate sull'architettura Turing di NVIDIA, al fine di sviluppare nuovi metodi per l'esplorazione intuitiva di paesaggi con geometria e topologia non banali nella realtà virtuale.
Proponiamo il set autoencoder, un modello per l'apprendimento non supervisionato di rappresentazioni per insiemi di elementi. È strettamente correlato ai modelli sequenza-sequenza, che apprendono rappresentazioni latenti di dimensioni fisse per le sequenze, e sono stati applicati a una serie di impegnativi compiti di sequenza supervisionati come la traduzione automatica, nonché all'apprendimento non supervisionato di rappresentazioni per le sequenze. A differenza delle sequenze, gli insiemi sono invarianti alle permutazioni e l'autocodificatore proposto considera questo fatto, sia per quanto riguarda l'input che l'output del modello. Sul lato dell'output, usiamo un algoritmo di matrimonio stabile per allineare le previsioni alle etichette nella fase di apprendimento.Addestriamo il modello su set di dati sintetici di nuvole di punti e dimostriamo che le rappresentazioni apprese cambiano senza problemi con le traduzioni negli input, preservano le distanze negli input, e che la dimensione dell'insieme è rappresentata direttamente. Applichiamo il modello a compiti supervisionati sulle nuvole di punti usando la rappresentazione latente a dimensione fissa.Per una serie di difficili problemi di classificazione, i risultati sono migliori di quelli di un modello che non considera l'invarianza di permutazione.Specialmente per piccoli insiemi di formazione, il modello consapevole del set beneficia del preallenamento non supervisionato.
Gli algoritmi di apprendimento di rinforzo profondo possono imparare abilità comportamentali complesse, ma l'applicazione nel mondo reale di questi metodi richiede una notevole quantità di esperienza da raccogliere da parte dell'agente.Nelle impostazioni pratiche, come la robotica, questo comporta tentare ripetutamente un compito, resettando l'ambiente tra ogni tentativo.Tuttavia, non tutti i compiti sono facilmente o automaticamente reversibili.In pratica, questo processo di apprendimento richiede un notevole intervento umano. In questo lavoro, proponiamo un metodo autonomo per l'apprendimento di rinforzo sicuro ed efficiente che impara simultaneamente una politica in avanti e all'indietro, con la politica all'indietro che resetta l'ambiente per un tentativo successivo. Imparando una funzione di valore per la politica all'indietro, possiamo determinare automaticamente quando la politica in avanti sta per entrare in uno stato non reversibile, fornendo interruzioni di sicurezza consapevoli dell'incertezza.I nostri esperimenti illustrano che l'uso corretto della politica all'indietro può ridurre notevolmente il numero di reset manuali necessari per imparare un compito e può ridurre il numero di azioni non sicure che portano a stati non reversibili.
È stato sostenuto che gli attuali modelli di apprendimento automatico non hanno commonsense, e quindi devono essere hard-coded con conoscenze precedenti (Marcus, 2018).Qui mostriamo prove sorprendenti che i modelli linguistici possono già imparare a catturare alcune conoscenze di senso comune.La nostra osservazione chiave è che un modello linguistico può calcolare la probabilità di qualsiasi dichiarazione, e questa probabilità può essere utilizzata per valutare la veridicità di quella dichiarazione.  Sul Winograd Schema Challenge (Levesque et al., 2011), i modelli linguistici hanno un'accuratezza superiore dell'11% rispetto ai precedenti metodi supervisionati allo stato dell'arte.I modelli linguistici possono anche essere messi a punto per il compito di Mining Commonsense Knowledge su ConceptNet per ottenere un punteggio F1 di 0,912 e 0,824, superando i migliori risultati precedenti (Jastrzebskiet al., 2018).  Ulteriori analisi dimostrano che i modelli linguistici possono scoprire caratteristiche uniche dei contesti Winograd Schema che decidono le risposte corrette senza supervisione esplicita.
In molti scenari di apprendimento del mondo reale, le caratteristiche sono acquisibili solo ad un costo vincolato sotto un budget.In questo documento, proponiamo un nuovo approccio per l'acquisizione di caratteristiche sensibili ai costi al tempo di predizione.Il metodo suggerito acquisisce le caratteristiche in modo incrementale sulla base di una funzione di valore della caratteristica consapevole del contesto.Formuliamo il problema nel paradigma di apprendimento di rinforzo, e introduciamo una funzione di ricompensa basata sull'utilità di ogni caratteristica. Inoltre, suggeriamo di condividere le rappresentazioni tra il predittore di classe e le reti di stima della funzione di valore. L'approccio suggerito è completamente online ed è facilmente applicabile alle configurazioni di apprendimento del flusso. La soluzione è valutata su tre diversi set di dati tra cui il noto set di dati MNIST come benchmark e due set di dati sensibili ai costi: Secondo i risultati, il metodo proposto è in grado di acquisire efficientemente le caratteristiche e fare previsioni accurate.
Questo articolo rivisita il problema della modellazione di sequenze utilizzando architetture convoluzionali. Sebbene sia le architetture convoluzionali che quelle ricorrenti abbiano una lunga storia nella previsione delle sequenze, l'attuale mentalità "predefinita" in gran parte della comunità di apprendimento profondo è che la modellazione generica delle sequenze sia meglio gestita utilizzando reti ricorrenti. L'obiettivo di questo articolo è quello di mettere in discussione questo presupposto. In particolare, consideriamo una semplice rete di convoluzione temporale generica (TCN), che adotta le caratteristiche delle moderne architetture ConvNet come le dilatazioni e le connessioni residue. Mostriamo che su una varietà di compiti di modellazione della sequenza, compresi molti frequentemente utilizzati come benchmark per valutare le reti ricorrenti, la TCN supera i metodi RNN di base (LSTMs, GRUs, e RNNs vaniglia) e a volte anche approcci altamente specializzati. Mostriamo inoltre che il potenziale vantaggio della "memoria infinita" che le RNN hanno sulle TCN è in gran parte assente nella pratica: Le TCN mostrano infatti dimensioni effettive della storia più lunghe rispetto alle loro controparti ricorrenti.  Nel complesso, sosteniamo che potrebbe essere il momento di (ri)considerare le reti convettive come l'architettura predefinita per la modellazione di sequenze.
Le reti neurali profonde funzionano bene nell'approssimazione di funzioni complicate quando vengono fornite di dati e addestrate con metodi di discesa del gradiente; allo stesso tempo, c'è una grande quantità di funzioni esistenti che risolvono programmaticamente diversi compiti in modo preciso, eliminando la necessità di addestramento. In molti casi, è possibile decomporre un compito in una serie di funzioni, di cui per alcune si può preferire l'uso di una rete neurale per imparare la funzionalità, mentre per altre il metodo preferito sarebbe quello di utilizzare funzioni black-box esistenti.Proponiamo un metodo per l'addestramento end-to-end di una rete neurale di base che integra le chiamate a funzioni black-box esistenti. Lo facciamo approssimando la funzionalità black-box con una rete neurale differenziabile in modo da guidare la rete di base per conformarsi all'interfaccia della funzione black-box durante il processo di ottimizzazione end-to-end. Al momento dell'inferenza, sostituiamo lo stimatore differenziabile con la sua controparte esterna black-box non differenziabile in modo che l'output della rete di base corrisponda agli argomenti di input della funzione black-box. Usando questo paradigma ``Estimate and Replace'', addestriamo una rete neurale, end to end, per calcolare l'input alla funzione black-box eliminando la necessità di etichette intermedie. Mostriamo che sfruttando la precisa funzione black-box esistente durante l'inferenza, il modello integrato generalizza meglio di un modello completamente differenziabile, e impara in modo più efficiente rispetto ai metodi basati su RL.
Questo articolo propone un nuovo algoritmo in stile actor-critic chiamato Dual Actor-Critic o Dual-AC.  E' derivato in modo principesco dalla forma duale lagrangiana dell'equazione di ottimalità di Bellman, che può essere vista come un gioco a due giocatori tra l'attore e una funzione di tipo critico, che viene chiamata doppio critico.  Rispetto ai suoi parenti attori-critici, Dual-AC ha la proprietà desiderata che l'attore e il doppio critico sono aggiornati in modo cooperativo per ottimizzare la stessa funzione obiettivo, fornendo un modo più trasparente per l'apprendimento del critico che è direttamente correlato alla funzione obiettivo dell'attore.Forniamo quindi un algoritmo concreto che può risolvere efficacemente il problema di ottimizzazione minimax, utilizzando tecniche di bootstrapping multi-step, regolarizzazione del percorso e algoritmo di ascesa duale stocastico.dimostriamo che l'algoritmo proposto raggiunge lo stato dell'arte delle prestazioni su diversi benchmark.
L'addestramento di un modello per eseguire un compito richiede tipicamente una grande quantità di dati dai domini in cui il compito sarà applicato.Tuttavia, è spesso il caso che i dati sono abbondanti in alcuni domini ma scarsi in altri.L'adattamento al dominio si occupa della sfida di adattare un modello addestrato da un dominio di origine ricco di dati per eseguire bene in un dominio di destinazione povero di dati.In generale, questo richiede l'apprendimento di mappature plausibili tra domini. CycleGAN è una struttura potente che impara efficientemente a mappare gli input da un dominio ad un altro usando l'addestramento avversario e un vincolo di ciclo-consistenza. Tuttavia, il metodo convenzionale di imporre la ciclo-consistenza tramite ricostruzione può essere eccessivamente restrittivo nei casi in cui uno o più domini hanno dati di addestramento limitati. Questo modello specifico dell'attività rilassa il vincolo di coerenza del ciclo e completa il ruolo del discriminatore durante l'addestramento, servendo come fonte di informazione aumentata per l'apprendimento della mappatura. Esploriamo l'adattamento nei domini vocali e visivi a bassa risorsa in un'impostazione supervisionata. Il nostro approccio migliora le prestazioni assolute del riconoscimento vocale del 2% per i parlanti di sesso femminile nel set di dati TIMIT, dove la maggior parte dei campioni di formazione sono da voci maschili.Nell'adattamento al dominio visivo a bassa risorsa, i risultati mostrano che il nostro approccio migliora le prestazioni assolute del 14% e del 4% quando si adatta SVHN a MNIST e viceversa, rispettivamente, che supera i metodi di adattamento al dominio non supervisionato che richiedono un dominio di destinazione non etichettato ad alta risorsa. 
Il successo degli algoritmi popolari per l'apprendimento di rinforzo profondo, come policy-gradients e Q-learning, si basa molto sulla disponibilità di un segnale di ricompensa informativo in ogni fase del processo decisionale sequenziale. Quando le ricompense sono solo scarsamente disponibili durante un episodio, o un feedback gratificante viene fornito solo dopo la fine dell'episodio, questi algoritmi eseguono in modo sub-ottimale a causa della difficoltà di assegnazione dei crediti. In alternativa, i metodi di ottimizzazione della politica basati sulla traiettoria, come il metodo dell'entropia incrociata e le strategie di evoluzione, non richiedono ricompense per-timestep, ma sono stati trovati a soffrire di un'elevata complessità del campione completando rinunciando alla natura temporale del problema.Migliorare l'efficienza degli algoritmi RL nei problemi del mondo reale con ricompense sparse o episodiche è quindi un bisogno pressante. In questo lavoro, introduciamo un algoritmo di apprendimento di auto-imitazione che sfrutta e sfrutta bene nelle impostazioni di ricompensa sparse ed episodiche.Vediamo ogni politica come una distribuzione di visita di stato-azione e formuliamo l'ottimizzazione della politica come un problema di minimizzazione della divergenza.Mostriamo che con la divergenza di Jensen-Shannon, questo problema di minimizzazione della divergenza può essere ridotto in un algoritmo di politica-gradiente con ricompense modellate apprese dai replay di esperienza. I risultati sperimentali indicano che il nostro algoritmo funziona paragonabile agli algoritmi esistenti in ambienti con ricompense dense, e significativamente meglio in ambienti con ricompense rade ed episodiche.Discutiamo poi i limiti dell'apprendimento dell'auto-imitazione, e proponiamo di risolverli usando la discesa del gradiente di politica variazionale di Stein con il kernel di Jensen-Shannon per imparare più politiche diverse.Dimostriamo la sua efficacia su una variante impegnativa di compiti di locomozione MuJoCo a controllo continuo.
Studiamo i meccanismi precisi che permettono agli autoencoder di codificare e decodificare una forma geometrica semplice, il disco.In questo ambiente accuratamente controllato, siamo in grado di descrivere la forma specifica della soluzione ottimale al problema di minimizzazione della fase di formazione.Mostriamo che l'autoencoder approssima effettivamente questa soluzione durante la formazione.In secondo luogo, identifichiamo un chiaro fallimento nella capacità di generalizzazione dell'autoencoder, cioè la sua incapacità di interpolare i dati. Infine, esploriamo diversi schemi di regolarizzazione per risolvere il problema della generalizzazione.Data la grande attenzione che è stata data recentemente alla capacità generativa delle reti neurali, crediamo che studiare in profondità semplici casi geometrici faccia luce sul processo di generazione e possa fornire un setup sperimentale con requisiti minimi per architetture più complesse. 
Presentiamo una semplice idea che permette di registrare un oratore in una data lingua e sintetizzare la sua voce in altre lingue che potrebbe anche non conoscere.Queste tecniche aprono una vasta gamma di potenziali applicazioni come la comunicazione interlinguistica, l'apprendimento delle lingue o il doppiaggio automatico dei video.Chiamiamo questo problema generale sintesi vocale condizionata dall'oratore multilingue e presentiamo una linea di base semplice ma forte per esso.L'architettura del nostro modello è simile al modello encoder-decoder Char2Wav o Tacotron. La differenza principale è che, invece di condizionare su caratteri o fonemi che sono specifici di una data lingua, noi condizioniamo su una rappresentazione fonetica condivisa che è universale a tutte le lingue.Questa rappresentazione fonetica di testo cross-lingua permette di sintetizzare il discorso in qualsiasi lingua preservando le caratteristiche vocali dell'oratore originale.Inoltre, mostriamo che la regolazione fine dei pesi del nostro modello ci permette di estendere i nostri risultati agli oratori al di fuori del dataset di formazione.
L'obiettivo dell'apprendimento per imitazione (IL) è quello di consentire a un discente di imitare il comportamento di un esperto dato dimostrazioni di esperti.Recentemente, l'apprendimento generativo di imitazione avversaria (GAIL) ha mostrato progressi significativi su IL per compiti continui complessi.Tuttavia, GAIL e le sue estensioni richiedono un gran numero di interazioni ambientali durante la formazione. Negli ambienti del mondo reale, più un metodo IL richiede che l'allievo interagisca con l'ambiente per una migliore imitazione, più tempo di formazione richiede, e più danni provoca agli ambienti e all'allievo stesso.Crediamo che gli algoritmi IL potrebbero essere più applicabili ai problemi del mondo reale se il numero di interazioni potesse essere ridotto. Il nostro algoritmo si compone principalmente di tre modifiche ai metodi esistenti di apprendimento per imitazione avversaria (AIL): (a) adozione dell'algoritmo off-policy actor-critic (Off-PAC) per ottimizzare la politica del discente, (b) stima del valore stato-azione usando campioni off-policy senza imparare funzioni di ricompensa, e (c) rappresentazione della funzione di politica stocastica in modo che le sue uscite siano limitate. I risultati sperimentali mostrano che il nostro algoritmo raggiunge risultati competitivi con GAIL riducendo significativamente le interazioni con l'ambiente.
L'approccio dominante al "trasferimento di stile" non supervisionato nel testo si basa sull'idea di imparare una rappresentazione latente, che è indipendente dagli attributi che specificano il suo "stile".In questo articolo, mostriamo che questa condizione non è necessaria e non è sempre soddisfatta nella pratica, anche con l'addestramento avversario del dominio che mira esplicitamente ad apprendere tali rappresentazioni dissociate. Proponiamo quindi un nuovo modello che controlla diversi fattori di variazione nei dati testuali in cui questa condizione sul disentanglement è sostituita da un meccanismo più semplice basato sulla back-translation.Il nostro metodo permette il controllo su più attributi, come il genere, il sentiment, il tipo di prodotto, ecc, I nostri esperimenti dimostrano che il modello completamente impigliato produce generazioni migliori, anche quando testato su nuovi e più impegnativi benchmark che comprendono recensioni con più frasi e più attributi.
Le RNN Vanilla con attivazione ReLU hanno una struttura semplice che è adatta all'analisi e all'interpretazione sistematica dei sistemi dinamici, ma soffrono del problema dei gradienti che esplodono o svaniscono. Tentativi recenti di mantenere questa semplicità alleviando il problema dei gradienti si basano su schemi di inizializzazione adeguati o su vincoli di ortogonalità/unitari sulla matrice di ricorsività della RNN, il che, tuttavia, comporta limiti alla sua potenza espressiva per quanto riguarda i fenomeni dei sistemi dinamici come il caos o la multi-stabilità. Qui, invece, suggeriamo uno schema di regolarizzazione che spinge parte del sottospazio latente della RNN verso una configurazione di attrazione lineare che permette una lunga memoria a breve termine e scale temporali arbitrariamente lente. Mostriamo che il nostro approccio eccelle su una serie di benchmark come il MNIST sequenziale o problemi di moltiplicazione, e permette la ricostruzione di sistemi dinamici che ospitano scale temporali molto diverse.
Nel campo delle Generative Adversarial Networks (GANs), come progettare una strategia di addestramento stabile rimane un problema aperto. Le GANs di Wasserstein hanno ampiamente promosso la stabilità rispetto alle GANs originali introducendo la distanza di Wasserstein, ma rimangono ancora instabili e sono soggette a una varietà di modalità di fallimento. In questo articolo, presentiamo un quadro generale chiamato Wasserstein-Bounded GAN (WBGAN), che migliora una grande famiglia di approcci basati su WGAN aggiungendo semplicemente un vincolo upper-bound al termine Wasserstein.Inoltre, dimostriamo che WBGAN può ragionevolmente misurare la differenza di distribuzioni che quasi non hanno intersezione.Gli esperimenti dimostrano che WBGAN può stabilizzare e accelerare la convergenza nei processi di formazione di una serie di varianti basate su WGAN.
I modelli generativi come Variational Auto Encoders (VAEs) e Generative Adversarial Networks (GANs) sono tipicamente addestrati per una distribuzione a priori fissa nello spazio latente, come uniforme o gaussiana.Dopo aver ottenuto un modello addestrato, si può campionare il generatore in varie forme per l'esplorazione e la comprensione, come l'interpolazione tra due campioni, il campionamento nelle vicinanze di un campione o esplorare le differenze tra una coppia di campioni applicata a un terzo campione. In questo articolo, mostriamo che le operazioni di spazio latente usate finora in letteratura inducono un mismatch di distribuzione tra gli output risultanti e la distribuzione a priori su cui il modello è stato addestrato. Per risolvere questo problema, proponiamo di usare mappe di trasporto corrispondenti alla distribuzione per garantire che tali operazioni di spazio latente preservino la distribuzione a priori, modificando minimamente l'operazione originale. I nostri risultati sperimentali confermano che le operazioni proposte danno campioni di qualità superiore rispetto alle operazioni originali.
Le incorporazioni neurali dei programmi hanno mostrato recentemente molta promessa per una varietà di compiti di analisi dei programmi, tra cui la sintesi dei programmi, la riparazione dei programmi, il completamento del codice e la localizzazione dei guasti.Tuttavia, la maggior parte delle incorporazioni dei programmi esistenti sono basate sulle caratteristiche sintattiche dei programmi, come le sequenze di token o gli alberi sintattici astratti. A differenza delle immagini e del testo, un programma ha una semantica ben definita che può essere difficile da catturare considerando solo la sua sintassi (cioè programmi sintatticamente simili possono esibire un comportamento run-time molto diverso), il che rende le integrazioni di programma basate sulla sintassi fondamentalmente limitate. La nostra intuizione chiave è che gli stati del programma espressi come tuple sequenziali di valori di variabili dal vivo non solo catturano la semantica del programma in modo più preciso, ma offrono anche un ﬁt più naturale per le Reti Neurali Ricorrenti da modellare.Valutiamo diverse incorporazioni sintattiche e semantiche del programma sul compito di classificare i tipi di errori che gli studenti fanno nei loro invii a una classe di programmazione introduttiva e sulla piattaforma educativa CodeHunt. I nostri risultati di valutazione mostrano che l'incorporazione semantica del programma supera significativamente l'incorporazione sintattica del programma basata su sequenze di token e alberi sintattici astratti. Inoltre, aumentiamo un sistema di riparazione del programma basato sulla ricerca con previsioni fatte dalla nostra incorporazione semantica e dimostriamo un'efficienza di ricerca significativamente migliorata.
In questo articolo, proponiamo una generalizzazione dell'algoritmo BN, la normalizzazione a lotti decrescenti (DBN), dove aggiorniamo i parametri BN in un modo di media mobile decrescente.La normalizzazione a lotti (BN) è molto efficace nell'accelerare la convergenza della fase di addestramento di una rete neurale che è diventata una pratica comune. La nostra proposta di algoritmo DBN mantiene la struttura generale dell'algoritmo BN originale mentre introduce un aggiornamento medio ponderato per alcuni parametri allenabili. Forniamo un'analisi della convergenza dell'algoritmo DBN che converge ad un punto stazionario rispetto ai parametri addestrabili.La nostra analisi può essere facilmente generalizzata per l'algoritmo BN originale impostando alcuni parametri a costante.Per quanto ne sanno gli autori, questa analisi è la prima del suo genere per la convergenza con Batch Normalization introdotto.Analizziamo un modello a due strati con funzione di attivazione arbitraria. La sfida principale dell'analisi è il fatto che alcuni parametri sono aggiornati dal gradiente mentre altri non lo sono. L'analisi della convergenza si applica a qualsiasi funzione di attivazione che soddisfi le nostre ipotesi comuni.Per l'analisi, mostriamo anche le condizioni sufficienti e necessarie per gli stepsizes e i pesi decrescenti per garantire la convergenza. Negli esperimenti numerici, usiamo modelli più complessi con più strati e attivazione ReLU.Osserviamo che DBN supera l'algoritmo BN originale su Imagenet, MNIST, NI e CIFAR-10 dataset con modelli FNN e CNN ragionevolmente complessi.
I modelli generativi come Variational Auto Encoders (VAEs) e Generative Adversarial Networks (GANs) sono tipicamente addestrati per una distribuzione prioritaria fissa nello spazio latente, come uniforme o gaussiana. Dopo aver ottenuto un modello addestrato, si può campionare il generatore in varie forme per l'esplorazione e la comprensione, come l'interpolazione tra due campioni, il campionamento nelle vicinanze di un campione o l'esplorazione delle differenze tra una coppia di campioni applicata a un terzo campione.Tuttavia, le operazioni dello spazio latente comunemente usate in letteratura finora inducono un mismatch di distribuzione tra le uscite risultanti e la distribuzione prioritaria su cui il modello è stato addestrato. I lavori precedenti hanno tentato di ridurre questo mismatch con modifiche euristiche alle operazioni o cambiando la distribuzione latente e ri-addestrando i modelli.In questo articolo, proponiamo una struttura per modificare le operazioni dello spazio latente in modo che il mismatch della distribuzione sia completamente eliminato.Il nostro approccio si basa su mappe di trasporto ottimali, che adattano le operazioni dello spazio latente in modo che corrispondano completamente alla distribuzione precedente, mentre modificano minimamente l'operazione originale.Le nostre operazioni abbinate sono facilmente ottenute per le operazioni e le distribuzioni comunemente usate e non richiedono alcuna regolazione della procedura di formazione.
Il problema di costruire un agente di conversazione coerente e non monotono con un discorso e una copertura adeguati è ancora un'area di ricerca aperta.Le architetture attuali si occupano solo delle informazioni semantiche e contestuali per una data query e non riescono a tenere completamente conto della conoscenza sintattica ed esterna che sono cruciali per generare risposte in un sistema di chit-chat. Per superare questo problema, proponiamo un'architettura di deep learning end to end multi-stream che impara embeddings unificati per coppie di domande-risposte sfruttando le informazioni contestuali dalle reti di memoria e le informazioni sintattiche incorporando le reti di convoluzione del grafico (GCN) sul loro parse di dipendenza. Un flusso di questa rete utilizza anche l'apprendimento di trasferimento pre-addestrando un trasformatore bidirezionale per estrarre la rappresentazione semantica per ogni frase di input e incorpora la conoscenza esterna attraverso il vicinato delle entità da una base di conoscenza (KB).Noi valutiamo questi embeddings sul compito di predizione della frase successiva e migliorano significativamente le tecniche esistenti.Inoltre, usiamo AMUSED per rappresentare la domanda e le risposte insieme al suo contesto per sviluppare un agente di conversazione basato sul recupero che è stato convalidato da linguisti esperti per avere un impegno completo con gli esseri umani.
Esaminiamo le tecniche per combinare le politiche generalizzate con gli algoritmi di ricerca per sfruttare i punti di forza e superare le debolezze di ciascuno quando si risolvono problemi di pianificazione probabilistica.L'Action Schema Network (ASNet) è un recente contributo alla pianificazione che utilizza l'apprendimento profondo e le reti neurali per imparare politiche generalizzate per problemi di pianificazione probabilistica.Le ASNet sono adatte a problemi in cui la conoscenza locale dell'ambiente può essere sfruttata per migliorare le prestazioni, ma possono non riuscire a generalizzare a problemi su cui non sono state addestrate. Monte-Carlo Tree Search (MCTS) è un algoritmo di ricerca nello spazio degli stati per il processo decisionale ottimale che esegue simulazioni per costruire incrementalmente un albero di ricerca e stimare i valori di ogni stato. Sebbene MCTS possa raggiungere risultati all'avanguardia quando è accoppiato con la conoscenza specifica del dominio, senza questa conoscenza, MCTS richiede un gran numero di simulazioni per ottenere stime affidabili nell'albero di ricerca. Combinando ASNets con MCTS, siamo in grado di migliorare la capacità di un ASNet di generalizzare oltre la distribuzione dei problemi su cui è stato addestrato, così come migliorare la navigazione dello spazio di ricerca da MCTS.
Le moderne reti neurali profonde possono raggiungere un'elevata accuratezza quando la distribuzione di addestramento e la distribuzione di test sono distribuite in modo identico, ma questo presupposto è spesso violato nella pratica; quando le distribuzioni di addestramento e di test non corrispondono, l'accuratezza può crollare. Proponiamo AugMix, una tecnica di elaborazione dei dati che è semplice da implementare, aggiunge un overhead computazionale limitato e aiuta i modelli a resistere a corruzioni impreviste. AugMix migliora significativamente la robustezza e le misure di incertezza su benchmark impegnativi di classificazione delle immagini, chiudendo il divario tra i metodi precedenti e le migliori prestazioni possibili in alcuni casi di oltre la metà.
La diteggiatura automatica del pianoforte è un compito difficile che i computer possono imparare usando i dati. Poiché la raccolta dei dati è difficile e costosa, proponiamo di automatizzare questo processo estraendo automaticamente le diteggiature dai video pubblici e dai file MIDI, usando tecniche di computer-vision. Eseguendo questo processo su 90 video si ottiene il più grande set di dati per la diteggiatura del pianoforte con più di 150K note. Mostriamo che eseguendo un modello precedentemente proposto per la diteggiatura automatica del pianoforte sul nostro set di dati e poi mettendolo a punto su dati di diteggiatura del pianoforte etichettati manualmente, otteniamo risultati all'avanguardia. Oltre al metodo di estrazione della diteggiatura, introduciamo anche un nuovo metodo per trasferire modelli di computer-vision ad apprendimento profondo per lavorare su dati fuori dal dominio, mettendolo a punto sull'aumento fuori dal dominio proposto da una Generative Adversarial Network (GAN).Per dimostrazione, rilasciamo anonimamente una visualizzazione dell'output del nostro processo per un singolo video su https://youtu.be/Gfs1UWQhr5Q
L'adattamento al dominio si riferisce al problema di sfruttare i dati etichettati in un dominio di origine per imparare un modello accurato in un dominio di destinazione in cui le etichette sono scarse o non disponibili.Un approccio recente per trovare una rappresentazione comune dei due domini è tramite l'addestramento avversario del dominio (Ganin & Lempitsky, 2015), che tenta di indurre un estrattore di caratteristiche che corrisponde alle distribuzioni delle caratteristiche di origine e di destinazione in qualche spazio delle caratteristiche. Tuttavia, l'addestramento avversario del dominio affronta due limitazioni critiche: 1) se la funzione di estrazione delle caratteristiche ha un'alta capacità, allora la corrispondenza della distribuzione delle caratteristiche è un vincolo debole, 2) nell'adattamento al dominio non conservativo (dove nessun classificatore singolo può funzionare bene sia nel dominio di origine che in quello di destinazione), l'addestramento del modello per fare bene nel dominio di origine danneggia le prestazioni nel dominio di destinazione, Proponiamo due modelli nuovi e correlati: 1) il modello Virtual Adversarial Domain Adaptation (VADA), che combina l'addestramento avversario del dominio con un termine di penalità che punisce la violazione del presupposto del cluster; 2) il modello Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T), che prende il modello VADA come inizializzazione e impiega gradienti naturali per minimizzare ulteriormente la violazione del presupposto del cluster. Ampi risultati empirici dimostrano che la combinazione di questi due modelli migliora significativamente le prestazioni allo stato dell'arte sui benchmark di adattamento del dominio di riconoscimento di cifre, segnali stradali e Wi-Fi.
In questo articolo, proponiamo Continuous Graph Flow, un metodo generativo basato sul flusso continuo che mira a modellare distribuzioni complesse di dati strutturati a grafo.  Una volta appreso, il modello può essere applicato a un grafico arbitrario, definendo una densità di probabilità sulle variabili casuali rappresentate dal grafico, ed è formulato come un sistema di equazioni differenziali ordinarie con funzioni condivise e riutilizzabili che operano sui grafi.  Questa classe di modelli offre diversi vantaggi: una rappresentazione flessibile che può generalizzarsi a dimensioni di dati variabili; capacità di modellare le dipendenze in distribuzioni di dati complesse; reversibilità ed efficienza della memoria; e calcolo esatto ed efficiente della probabilità dei dati.Dimostriamo l'efficacia del nostro modello su una serie diversificata di compiti di generazione in diversi domini: generazione di grafici, generazione di puzzle di immagini e generazione di layout da grafici di scene.
In questo lavoro, studiamo la teoria del collo di bottiglia informativo (IB) dell'apprendimento profondo, che fa tre affermazioni specifiche: in primo luogo, che le reti profonde subiscono due fasi distinte che consistono in una fase iniziale di adattamento e una successiva fase di compressione; in secondo luogo, che la fase di compressione è causalmente collegata alle eccellenti prestazioni di generalizzazione delle reti profonde; e in terzo luogo, che la fase di compressione avviene a causa del comportamento simile alla diffusione della discesa del gradiente stocastico. Attraverso una combinazione di risultati analitici e di simulazione, dimostriamo che la traiettoria del piano di informazione è prevalentemente una funzione della nonlinearità neurale impiegata: le nonlinearità saturanti a doppia faccia come tanh producono una fase di compressione quando le attivazioni neurali entrano nel regime di saturazione, ma le funzioni di attivazione lineare e le nonlinearità saturanti a singola faccia come la ReLU ampiamente utilizzata in effetti non lo fanno. Inoltre, troviamo che non c'è un'evidente connessione causale tra compressione e generalizzazione: le reti che non comprimono sono ancora capaci di generalizzare, e viceversa. Successivamente, dimostriamo che la fase di compressione, quando esiste, non deriva dalla stocasticità nell'addestramento dimostrando che possiamo replicare i risultati di IB usando la discesa del gradiente batch completa piuttosto che la discesa del gradiente stocastica. Infine, dimostriamo che quando un dominio di input consiste in un sottoinsieme di informazioni rilevanti e irrilevanti per il compito, le rappresentazioni nascoste comprimono le informazioni irrilevanti per il compito, anche se le informazioni complessive sull'input possono aumentare monotonicamente con il tempo di formazione, e che questa compressione avviene contemporaneamente al processo di adattamento piuttosto che durante un successivo periodo di compressione.
Negli ultimi quattro anni, le reti neurali si sono dimostrate vulnerabili alle immagini avversarie: perturbazioni mirate ma impercettibili dell'immagine portano a previsioni drasticamente diverse. Per la maggior parte delle architetture di rete attuali, dimostriamo che il L1-norm di questi gradienti cresce come la radice quadrata della dimensione dell'input.Queste reti diventano quindi sempre più vulnerabili con la crescita della dimensione dell'immagine.Le nostre prove si basano sulla distribuzione dei pesi della rete all'inizializzazione, ma ampi esperimenti confermano che le nostre conclusioni tengono ancora dopo la formazione abituale.
Le prestazioni effettive delle reti neurali dipendono in modo critico dalla sintonizzazione efficace degli iperparametri di ottimizzazione, in particolare i tassi di apprendimento (e la loro programmazione). Presentiamo l'Ottimizzazione prossimale ammortizzata (APO), che prende la prospettiva che ogni passo di ottimizzazione dovrebbe approssimativamente minimizzare un obiettivo prossimale (simile a quelli utilizzati per motivare il gradiente naturale e l'ottimizzazione della politica della regione di fiducia).Gli iperparametri di ottimizzazione sono adattati per minimizzare al meglio l'obiettivo prossimale dopo un aggiornamento del peso. Mostriamo che una versione idealizzata di APO (dove un oracolo minimizza esattamente l'obiettivo prossimale) raggiunge la convergenza globale al punto stazionario e localmente la convergenza di secondo ordine all'optimum globale per le reti neurali. Sperimentiamo l'uso di APO per adattare una varietà di iperparametri di ottimizzazione online durante l'addestramento, inclusi i tassi di apprendimento (possibilmente specifici del livello), i coefficienti di smorzamento e gli esponenti di varianza del gradiente. Per una varietà di architetture di rete e algoritmi di ottimizzazione (inclusi SGD, RMSprop e K-FAC), dimostriamo che con una regolazione minima, APO è competitivo con ottimizzatori accuratamente sintonizzati.
I vettori di parole dense hanno dimostrato il loro valore in molti compiti di NLP a valle negli ultimi anni.Tuttavia, le dimensioni di tali embeddings non sono facilmente interpretabili.Delle d-dimensioni in un vettore di parole, non saremmo in grado di capire cosa significano i valori alti o bassi.Approcci precedenti che affrontano questo problema si sono concentrati principalmente sull'addestramento di embeddings di parole sparse/non negative vincolate, o sul post-processing di embeddings di parole standard pre-addestrate. Utilizziamo un nuovo metodo di analisi degli autovettori ispirato alla teoria delle matrici casuali e dimostriamo che i gruppi semanticamente coerenti non si formano solo nello spazio delle righe, ma anche in quello delle colonne, il che ci permette di considerare le singole dimensioni dei vettori di parole come caratteristiche semantiche interpretabili dall'uomo.
Le reti neurali nel cervello e nei chip neuromorfici conferiscono ai sistemi la capacità di eseguire molteplici compiti cognitivi. Tuttavia, entrambi i tipi di reti sperimentano una vasta gamma di perturbazioni fisiche, che vanno dai danni ai bordi della rete alla completa cancellazione dei nodi, che alla fine potrebbero portare al fallimento della rete. Una questione critica è capire come le proprietà computazionali delle reti neurali cambiano in risposta al danno ai nodi e se esistono strategie per riparare queste reti al fine di compensare la degradazione delle prestazioni. Qui, studiamo le caratteristiche di risposta ai danni di due classi di reti neurali, vale a dire i perceptron multistrato (MLP) e le reti neurali convoluzionali (CNN) addestrate per classificare le immagini da MNIST e CIFAR-10 dataset rispettivamente. Proponiamo anche un nuovo quadro per scoprire strategie di riparazione efficienti per salvare le reti neurali danneggiate. Il quadro prevede la definizione di danni e operatori di riparazione per attraversare dinamicamente il paesaggio di perdita delle reti neurali, con l'obiettivo di mappare le sue caratteristiche geometriche salienti. Identifichiamo anche che uno schema di recupero dinamico, in cui le reti sono costantemente danneggiate e riparate, produce un gruppo di reti resilienti ai danni in quanto può essere rapidamente salvato.In generale, il nostro lavoro dimostra che possiamo progettare reti tolleranti ai guasti applicando la riqualificazione on-line in modo coerente durante i danni per applicazioni in tempo reale in biologia e apprendimento automatico.
La generazione automatica di domande dai paragrafi è un problema importante e impegnativo, in particolare a causa del lungo contesto dei paragrafi. In questo articolo, proponiamo e studiamo due modelli gerarchici per il compito di generare domande dai paragrafi. In particolare, proponiamo (a) un nuovo modello gerarchico BiLSTM con attenzione selettiva e (b) una nuova architettura gerarchica Transformer, entrambi i quali imparano rappresentazioni gerarchiche dei paragrafi. Mentre l'introduzione del meccanismo di attenzione avvantaggia il modello BiLSTM gerarchico, il trasformatore gerarchico, con la sua attenzione intrinseca e i meccanismi di codifica posizionale, si comporta meglio del modello trasformatore piatto. Abbiamo condotto una valutazione empirica sui dataset SQuAD e MS MARCO, ampiamente utilizzati, utilizzando metriche standard. I risultati dimostrano l'efficacia complessiva dei modelli gerarchici rispetto alle loro controparti piatte. Qualitativamente, i nostri modelli gerarchici sono in grado di generare domande fluenti e rilevanti.
Molti modelli appresi dispiegati sono scatole nere: dato l'input, restituisce l'output.Le informazioni interne sul modello, come l'architettura, la procedura di ottimizzazione, o i dati di allenamento, non sono divulgate esplicitamente perché potrebbero contenere informazioni proprietarie o rendere il sistema più vulnerabile.Questo lavoro mostra che tali attributi delle reti neurali possono essere esposti da una sequenza di query.Questo ha molteplici implicazioni. Da un lato, il nostro lavoro espone la vulnerabilità delle reti neurali black-box a diversi tipi di attacchi - mostriamo che le informazioni interne rivelate aiutano a generare esempi avversari più efficaci contro il modello black box.Dall'altro lato, questa tecnica può essere usata per una migliore protezione dei contenuti privati dai modelli di riconoscimento automatico usando esempi avversari.Il nostro articolo suggerisce che è effettivamente difficile tracciare una linea tra modelli white box e black box.
L'apprendimento di rinforzo inverso (IRL) è usato per dedurre la funzione di ricompensa dalle azioni di un esperto che esegue un processo decisionale di Markov (MDP).Un nuovo approccio che usa l'inferenza variazionale per imparare la funzione di ricompensa è proposto in questa ricerca.Usando questa tecnica, la distribuzione posteriore intrattabile della variabile latente continua (la funzione di ricompensa in questo caso) è approssimata analiticamente per apparire il più vicino possibile alla credenza precedente mentre cerca di ricostruire lo stato futuro condizionato dallo stato e dall'azione attuali. La funzione di ricompensa è derivata utilizzando un noto modello generativo profondo noto come Conditional Variational Auto-encoder (CVAE) con funzione di perdita di Wasserstein, quindi indicato come Conditional Wasserstein Auto-encoder-IRL (CWAE-IRL), che può essere analizzato come una combinazione di inferenza a ritroso e in avanti. Questo può quindi formare un'alternativa efficiente ai precedenti approcci all'IRL pur non avendo alcuna conoscenza della dinamica del sistema dell'agente.I risultati sperimentali su benchmark standard come objectworld e pendulum mostrano che l'algoritmo proposto può imparare efficacemente la funzione di ricompensa latente in ambienti complessi e ad alta densità.
Il problema del commesso viaggiatore (TSP) è un ben noto problema di ottimizzazione combinatoria con una varietà di applicazioni della vita reale. Affrontiamo il TSP incorporando la metodologia di apprendimento automatico e sfruttando la strategia di ricerca del vicinato variabile. Più precisamente, il processo di ricerca è considerato come un processo decisionale di Markov (MDP), dove una ricerca locale a 2 opzioni è usata per cercare all'interno di un piccolo vicinato, mentre un metodo di ricerca ad albero Monte Carlo (MCTS) (che itera attraverso la simulazione, la selezione e le fasi di back-propagation), è usato per campionare una serie di azioni mirate in un vicinato più ampio. Questo nuovo paradigma si distingue chiaramente dai paradigmi esistenti basati sull'apprendimento automatico (ML) per la risoluzione del TSP, che utilizza un modello ML end-to-end, o semplicemente applica tecniche tradizionali dopo ML per l'ottimizzazione post. Gli esperimenti basati su due set di dati pubblici mostrano che il nostro approccio domina chiaramente tutti gli algoritmi TSP esistenti basati sull'apprendimento in termini di prestazioni, dimostrando il suo alto potenziale sul TSP, e soprattutto, essendo un quadro generale senza regole complicate fatte a mano, può essere facilmente esteso a molti altri problemi di ottimizzazione combinatoria.
Nonostante questi progressi, tuttavia, gli approcci allo stato dell'arte sono ancora largamente incapaci di catturare strutture globali complesse nei dati. Per esempio, le immagini degli edifici contengono tipicamente modelli spaziali come finestre che si ripetono a intervalli regolari; i metodi generativi allo stato dell'arte non possono facilmente riprodurre queste strutture, Inoltre, proponiamo una struttura per l'apprendimento di questi modelli sfruttando la sintesi dei programmi per generare dati di addestramento.Su entrambi i dati sintetici e del mondo reale, dimostriamo che il nostro approccio è sostanzialmente migliore dello stato dell'arte sia nella generazione che nel completamento di immagini che contengono una struttura globale.
L'apprendimento delle politiche di controllo nei compiti robotici richiede un gran numero di interazioni a causa di piccoli tassi di apprendimento, limiti sugli aggiornamenti o vincoli sconosciuti.Al contrario gli umani possono dedurre soluzioni protettive e sicure dopo un singolo fallimento o un'osservazione inaspettata. Al fine di raggiungere prestazioni simili, abbiamo sviluppato un algoritmo gerarchico di ottimizzazione bayesiana che replica l'inferenza cognitiva e il processo di memorizzazione per evitare i fallimenti nei compiti di controllo motorio.Un processo gaussiano implementa la modellazione e il campionamento della funzione di acquisizione.Questo permette un apprendimento rapido con grandi tassi di apprendimento, mentre una fase di replay mentale assicura che le regioni di politica che hanno portato ai fallimenti siano inibite durante il processo di campionamento.    Le caratteristiche del metodo di ottimizzazione gerarchica bayesiana sono valutate in un compito di bilanciamento posturale umanoide simulato e fisiologico.Confrontiamo quantitativamente le prestazioni di apprendimento umano con il nostro approccio di apprendimento valutando le deviazioni del centro di massa durante la formazione. I nostri risultati mostrano che possiamo riprodurre l'apprendimento efficiente dei soggetti umani in compiti di controllo posturale che fornisce un modello testabile per futuri compiti di controllo motorio fisiologico. In questi compiti di controllo posturale, il nostro metodo supera l'ottimizzazione bayesiana standard nel numero di interazioni per risolvere il compito, nelle richieste computazionali e nella frequenza dei fallimenti osservati.
L'apprendimento di rinforzo profondo ha raggiunto un grande successo in molti compiti di apprendimento di rinforzo precedentemente difficili, ma studi recenti dimostrano che gli agenti RL profondi sono anche inevitabilmente suscettibili di perturbazioni avversarie, simili alle reti neurali profonde nei compiti di classificazione.lavori precedenti si concentrano per lo più su attacchi avversari senza modello e agenti con azioni discrete. In questo lavoro, studiamo il problema degli agenti di controllo continuo nella RL profonda con attacchi avversari e proponiamo il primo algoritmo a due fasi basato sulla dinamica del modello appreso.esperimenti estesi su vari domini MuJoCo (Cartpole, Fish, Walker, Humanoid) dimostrano che il nostro quadro proposto è molto più efficace ed efficiente rispetto alle linee di base degli attacchi senza modello nel degradare le prestazioni degli agenti e nel guidare gli agenti verso stati non sicuri.
Una delle ipotesi principali per la sorprendente generalizzazione delle reti neurali è che la dinamica della discesa del gradiente inclina il modello verso soluzioni semplici, cercando attraverso lo spazio delle soluzioni in un ordine incrementale di complessità. Definiamo formalmente la nozione di dinamica di apprendimento incrementale e deriviamo le condizioni sulla profondità e l'inizializzazione per cui questo fenomeno si verifica nei modelli lineari profondi. Il nostro principale contributo teorico è un risultato di separazione dinamica della profondità, dimostrando che mentre i modelli poco profondi possono esibire dinamiche di apprendimento incrementale, essi richiedono che l'inizializzazione sia esponenzialmente piccola perché queste dinamiche si presentino.Tuttavia, una volta che il modello diventa più profondo, la dipendenza diventa polinomiale e l'apprendimento incrementale può sorgere in impostazioni più naturali.Completiamo i nostri risultati teorici sperimentando con il rilevamento di matrici profonde, reti neurali quadratiche e con la classificazione binaria usando reti lineari diagonali e convoluzionali, mostrando che tutti questi modelli esibiscono apprendimento incrementale.
La modellazione generativa di dati ad alta dimensione come le immagini è un problema notoriamente difficile e mal definito, in particolare, come valutare un modello generativo appreso non è chiaro. In questo articolo, sosteniamo che l'apprendimento *adversariale*, sperimentato con le reti generative avversarie (GAN), fornisce una struttura interessante per definire implicitamente perdite di compito più significative per compiti non supervisionati, come la generazione di immagini "visivamente realistiche". Mettendo in relazione le GAN e la predizione strutturata nel quadro della teoria della decisione statistica, mettiamo in luce i collegamenti tra i recenti progressi nella teoria della predizione strutturata e la scelta della divergenza nelle GANs. Sosteniamo che le intuizioni sulle nozioni di perdite "difficili" e "facili" da imparare possono essere analogamente estese alle divergenze avversarie. Discutiamo anche le interessanti proprietà delle divergenze avversarie parametriche per la modellazione generativa, ed eseguiamo esperimenti per mostrare l'importanza di scegliere una divergenza che rifletta il compito finale.
La riproducibilità e la replicabilità sperimentale sono argomenti critici nell'apprendimento automatico.Gli autori hanno spesso sollevato preoccupazioni per la loro mancanza nelle pubblicazioni scientifiche per migliorare la qualità del campo.Recentemente, il campo di apprendimento della rappresentazione del grafico ha attirato l'attenzione di un'ampia comunità di ricerca, che ha portato a un grande flusso di lavori.Come tale, diversi modelli di reti neurali grafiche sono stati sviluppati per affrontare efficacemente la classificazione del grafico.Tuttavia, le procedure sperimentali spesso mancano di rigore e sono difficilmente riproducibili. Per contrastare questa tendenza preoccupante, abbiamo eseguito più di 47000 esperimenti in un quadro controllato e uniforme per rivalutare cinque modelli popolari su nove benchmark comuni. Inoltre, confrontando GNNs con baseline struttura-agnostiche, forniamo prove convincenti che, su alcuni set di dati, le informazioni strutturali non sono state ancora sfruttate. Crediamo che questo lavoro possa contribuire allo sviluppo del campo dell'apprendimento dei grafi, fornendo una base molto necessaria per valutazioni rigorose dei modelli di classificazione dei grafi.
L'aumento dei dati (DA) è fondamentale contro l'overfitting nelle grandi reti neurali convoluzionali, specialmente con un set di dati limitato.Nelle immagini, il DA è solitamente basato su trasformazioni euristiche, come le trasformazioni geometriche o di colore.Invece di usare trasformazioni predefinite, il nostro lavoro impara l'aumento dei dati direttamente dai dati di formazione imparando a trasformare le immagini con un'architettura encoder-decoder combinata con una rete di trasformazione spaziale. Le immagini trasformate appartengono ancora alla stessa classe, ma sono nuovi campioni più complessi per il classificatore. I nostri esperimenti dimostrano che il nostro approccio è migliore dei precedenti metodi generativi di aumento dei dati, e paragonabile ai metodi di trasformazione predefiniti quando si allena un classificatore di immagini.
Consideriamo il problema della compressione dell'informazione da dati ad alta dimensione.Dove molti studi considerano il problema della compressione da trasformazioni non invertibili, noi sottolineiamo l'importanza della compressione invertibile.Introduciamo una nuova classe di codificatori automatici basati sulla verosimiglianza con architettura pseudo bijettiva, che chiamiamo Pseudo Invertible Encoders.Forniamo la spiegazione teorica dei loro principi.Valutiamo Gaussian Pseudo Invertible Encoder su MNIST, dove il nostro modello supera WAE e VAE in nitidezza delle immagini generate.
Sfruttiamo uno schema di inversione recentemente derivato per reti neurali profonde arbitrarie per sviluppare un nuovo quadro di apprendimento semi-supervisionato che si applica a una vasta gamma di sistemi e problemi.  L'approccio raggiunge gli attuali metodi allo stato dell'arte su MNIST e fornisce prestazioni ragionevoli su SVHN e CIFAR10.Attraverso il metodo introdotto, le reti residue sono per la prima volta applicate a compiti semi-supervisionati.Gli esperimenti con segnali unidimensionali evidenziano la generalità del metodo.Importante, il nostro approccio è semplice, efficiente e non richiede alcun cambiamento nell'architettura della rete profonda.
L'apprendimento profondo è diventato uno strumento ampiamente utilizzato in molti problemi computazionali e di classificazione. Tuttavia l'ottenimento e l'etichettatura dei dati, necessari per ottenere risultati validi, è spesso costoso o addirittura non possibile. In questo articolo tre diversi approcci algoritmici per affrontare l'accesso limitato ai dati sono valutati e confrontati tra loro. Mostriamo gli svantaggi e i benefici di ogni metodo. Un approccio di successo, specialmente in compiti di apprendimento di uno o pochi colpi, è l'uso di dati esterni durante il compito di classificazione. Un altro approccio di successo, che raggiunge lo stato dell'arte nei benchmark di apprendimento semi-supervisionato (SSL), è la regolarizzazione della consistenza, in particolare il virtual adversarial training (VAT) ha mostrato forti risultati e sarà studiato in questo articolo. Lo scopo della regolarizzazione della consistenza è quello di forzare la rete a non cambiare l'output, quando l'input o la rete stessa è perturbata.Le reti generative avversarie (GAN) hanno anche mostrato forti risultati empirici. In molti approcci l'architettura GAN viene utilizzata per creare dati aggiuntivi e quindi per aumentare la capacità di generalizzazione della rete di classificazione. L'uso di dati non etichettati è studiato sia per le GAN che per l'IVA. 
Questo articolo introduce una nuova struttura neurale chiamata FusionNet, che estende gli approcci di attenzione esistenti da tre punti di vista: primo, propone un nuovo concetto di "Storia della parola" per caratterizzare le informazioni di attenzione dal più basso incorporamento a livello di parola fino alla più alta rappresentazione a livello semantico; secondo, identifica una funzione di punteggio di attenzione che utilizza meglio il concetto di "storia della parola"; terzo, propone un meccanismo di attenzione multilivello completamente consapevole per catturare le informazioni complete in un testo (come una domanda) e sfruttarle nella sua controparte (come il contesto o il passaggio) strato per strato. Applichiamo FusionNet allo Stanford Question Answering Dataset (SQuAD) e raggiunge la prima posizione sia per il modello singolo che per quello ensemble nella classifica ufficiale SQuAD al momento della scrittura (4 ottobre 2017). Nel frattempo, verifichiamo la generalizzazione di FusionNet con due dataset SQuAD avversari e stabilisce il nuovo state-of-the-art su entrambi i dataset: su AddSent, FusionNet aumenta la migliore metrica F1 dal 46,6% al 51,4%; su AddOneSent, FusionNet aumenta la migliore metrica F1 dal 56,0% al 60,7%.
Proponiamo un nuovo modello generativo gerarchico con una semplice struttura markoviana e un modello di inferenza corrispondente. Sia il modello generativo che quello di inferenza sono addestrati usando il paradigma dell'apprendimento avversario. Dimostriamo che la struttura gerarchica supporta l'apprendimento di rappresentazioni progressivamente più astratte e fornisce ricostruzioni semanticamente significative con diversi livelli di fedeltà, inoltre dimostriamo che minimizzare la divergenza di Jensen-Shanon tra la rete generativa e quella di inferenza è sufficiente per minimizzare l'errore di ricostruzione. La scoperta della struttura latente gerarchica semanticamente significativa risultante è esemplificata sul set di dati CelebA.  Mostriamo che le caratteristiche apprese dal nostro modello in modo non supervisionato superano le migliori caratteristiche artigianali. Inoltre, le caratteristiche estratte rimangono competitive se paragonate a diversi approcci recenti supervisionati su un compito di predizione degli attributi su CelebA. Infine, sfruttiamo la rete di inferenza del modello per ottenere prestazioni all'avanguardia su una variante semi-supervisionata del compito di classificazione delle cifre MNIST.
Le leggi di conservazione sono considerate leggi fondamentali della natura e hanno un'ampia applicazione in molti campi, tra cui la fisica, la chimica, la biologia, la geologia e l'ingegneria. La soluzione delle equazioni differenziali associate alle leggi di conservazione è un ramo importante della matematica computazionale. In questo articolo, siamo i primi a esplorare la possibilità e il vantaggio di risolvere le leggi di conservazione non lineari utilizzando l'apprendimento profondo di rinforzo. Come prova di concetto, ci concentriamo sulle leggi di conservazione scalari 1-dimensionali. Utilizziamo il meccanismo dell'apprendimento profondo di rinforzo per addestrare una rete di policy che può decidere come le soluzioni numeriche dovrebbero essere approssimate in modo sequenziale e adattivo spazio-temporale. Mostreremo che il problema di risolvere le leggi di conservazione può essere naturalmente visto come un processo decisionale sequenziale e gli schemi numerici appresi in questo modo possono facilmente imporre la precisione a lungo termine. Inoltre, la rete di policy appresa è attentamente progettata per determinare una buona approssimazione discreta locale basata sullo stato attuale della soluzione, che essenzialmente rende il metodo proposto un approccio di meta-apprendimento. In altre parole, il metodo proposto è in grado di imparare come discretizzare per una data situazione imitando gli esperti umani. Infine, forniremo dettagli su come la rete di policy è addestrata, come si comporta rispetto ad alcuni solutori numerici all'avanguardia come gli schemi WENO, e quanto bene generalizza.
Presentiamo un'architettura di rendering neurale che aiuta gli autocodificatori variazionali (VAE) ad imparare rappresentazioni dissociate. Invece della rete deconvoluzionale tipicamente usata nel decoder dei VAE, noi piastrelliamo (trasmettiamo) il vettore latente nello spazio, concateniamo canali fissi di "coordinate" X e Y, e applichiamo una rete completamente convoluzionale con stride 1x1. Questo fornisce un'architettura precedente per dissociare le caratteristiche posizionali da quelle non posizionali nello spazio latente, senza tuttavia fornire alcuna supervisione esplicita a questo effetto. Mostriamo che questa architettura, che chiamiamo decodificatore Spatial Broadcast, migliora la separazione, l'accuratezza della ricostruzione e la generalizzazione alle regioni tenute fuori nello spazio dei dati.  Mostriamo che lo Spatial Broadcast Decoder è complementare alle tecniche di disentangling allo stato dell'arte (SOTA) e quando è incorporato migliora le loro prestazioni.
Simile agli esseri umani e agli animali, le reti neurali artificiali profonde mostrano periodi critici durante i quali un deficit temporaneo dello stimolo può compromettere lo sviluppo di un'abilità.L'entità del danno dipende dall'insorgenza e dalla lunghezza della finestra del deficit, come nei modelli animali, e dalle dimensioni della rete neurale.I deficit che non influenzano le statistiche di basso livello, come il flipping verticale delle immagini, non hanno effetti duraturi sulle prestazioni e possono essere superati con ulteriore formazione.  Per comprendere meglio questo fenomeno, usiamo l'informazione di Fisher dei pesi per misurare la connettività effettiva tra gli strati di una rete durante l'allenamento.  Controintuitivamente, l'informazione aumenta rapidamente nelle prime fasi dell'addestramento, e poi diminuisce, impedendo la ridistribuzione delle risorse informative in un fenomeno a cui ci riferiamo come una perdita di "Plasticità informativa".  La nostra analisi suggerisce che le prime poche epoche sono critiche per la creazione di connessioni forti che sono ottimali rispetto alla distribuzione dei dati di input. Una volta che tali connessioni forti sono create, non sembrano cambiare durante l'addestramento supplementare. Questi risultati suggeriscono che il transitorio di apprendimento iniziale, sotto-scrutinato rispetto al comportamento asintotico, gioca un ruolo chiave nel determinare il risultato del processo di formazione. Infine, i periodi critici non sono limitati ai sistemi biologici, ma possono emergere naturalmente nei sistemi di apprendimento, sia biologici che artificiali, a causa di vincoli fondamentali derivanti dalle dinamiche di apprendimento e dall'elaborazione delle informazioni.
Proponiamo un metodo per imparare incrementalmente uno spazio di incorporamento sul dominio delle architetture di rete, per consentire l'attenta selezione delle architetture da valutare durante la ricerca di architetture compresse.Data una rete di insegnanti, cerchiamo un'architettura di rete compressa utilizzando l'ottimizzazione bayesiana (BO) con una funzione kernel definita sul nostro spazio di incorporamento proposto per selezionare le architetture da valutare.Dimostriamo che il nostro algoritmo di ricerca può superare significativamente vari metodi di base, come la ricerca casuale e il reinforcement learning (Ashok et al., 2018).Le architetture compresse trovate dal nostro metodo sono anche migliori dello stato dell'arte dell'architettura compatta progettata manualmente ShuffleNet (Zhang et al., 2018).Dimostriamo anche che lo spazio di incorporazione appreso può essere trasferito a nuove impostazioni per la ricerca di architetture, come una rete di insegnanti più grande o una rete di insegnanti in una famiglia di architetture diversa, senza alcun allenamento.
Il problema del Reinforcement Learning (RL) può essere risolto in due modi diversi - l'approccio basato sulla funzione di valore e l'approccio basato sull'ottimizzazione della politica - per arrivare alla fine a una politica ottimale per l'ambiente dato.Una delle recenti scoperte nel reinforcement learning è l'uso di reti neurali profonde come approssimatori di funzione per approssimare la funzione di valore o q-funzione in uno schema di apprendimento di rinforzo.Questo ha portato a risultati con agenti che imparano automaticamente come giocare a giochi come alpha-go mostrando prestazioni migliori dell'uomo. Deep Q-learning networks (DQN) e Deep Deterministic Policy Gradient (DDPG) sono due di questi metodi che hanno mostrato risultati all'avanguardia negli ultimi tempi.Tra le molte varianti di RL, una classe importante di problemi è quella in cui gli spazi di stato e di azione sono continui --- robot autonomi, veicoli autonomi, controllo ottimale sono tutti esempi di tali problemi che possono prestarsi naturalmente ad algoritmi basati sul rinforzo, e hanno spazi di stato e di azione continui. In questo articolo, adattiamo e combiniamo approcci come DQN e DDPG in modi nuovi per superare i risultati precedenti per problemi con spazi di stato e d'azione continui.  Crediamo che questi risultati siano una preziosa aggiunta al corpo di risultati in rapida crescita sul Reinforcement Learning, soprattutto per i problemi di stato continuo e spazio d'azione.
Il compito di Natural Language Inference (NLI) richiede che un agente determini la relazione logica tra una premessa in linguaggio naturale e un'ipotesi in linguaggio naturale. Introduciamo Interactive Inference Network (IIN), una nuova classe di architetture di rete neurale che è in grado di raggiungere una comprensione di alto livello della coppia di frasi estraendo gerarchicamente le caratteristiche semantiche dallo spazio di interazione. Mostriamo che un tensore di interazione (peso di attenzione) contiene informazioni semantiche per risolvere l'inferenza del linguaggio naturale, e un tensore di interazione più denso contiene informazioni semantiche più ricche. Un'istanza di tale architettura, Densely Interactive Inference Network (DIIN), dimostra lo stato dell'arte delle prestazioni su copora NLI su larga scala e su corpus NLI su larga scala simili. È degno di nota che DIIN raggiunge una riduzione degli errori superiore al 20% sul difficile set di dati Multi-Genre NLI (MultiNLI) rispetto al più forte sistema pubblicato.
I processi puntuali determinanti (DPP) forniscono un modo elegante e versatile per campionare insiemi di elementi che bilanciano la qualità puntuale con la diversità dell'insieme di elementi selezionati, e per questo motivo hanno guadagnato importanza in molte applicazioni di apprendimento automatico che si basano sulla selezione di sottoinsiemi. Tuttavia, il campionamento da un DPP su un ground set di dimensione N è un'operazione costosa, che richiede in generale un costo di preprocessing O(N^3) e un costo di campionamento O(Nk^3) per sottoinsiemi di dimensione k. Ci avviciniamo a questo problema introducendo DppNets: modelli generativi profondi che producono campioni simili a DPP per ground set arbitrari.  Sviluppiamo un meccanismo di attenzione inibitoria basato su reti di trasformatori che cattura una nozione di dissimilarità tra vettori di caratteristiche.  Mostriamo teoricamente che una tale approssimazione è sensata in quanto mantiene le garanzie di inibizione o dissimilarità che rende DPP così potente e unico.  Empiricamente, dimostriamo che i campioni del nostro modello ricevono un'alta probabilità sotto la più costosa alternativa DPP.
Questo articolo introduce un'architettura di rete per risolvere il problema della struttura dal movimento (SfM) attraverso l'aggiustamento del bundle delle caratteristiche-metriche (BA), che fa esplicitamente rispettare i vincoli della geometria multi-vista sotto forma di errore delle caratteristiche-metriche. La rete genera prima diverse mappe di profondità di base in base all'immagine di input, e ottimizza la profondità finale come una combinazione lineare di queste mappe di profondità di base tramite il generatore di mappe di profondità di base è anche appreso tramite l'addestramento end-to-end. L'intero sistema combina bene la conoscenza del dominio (cioè i vincoli geometrici hard-coded multi-view) e l'apprendimento profondo (cioè l'apprendimento delle caratteristiche e delle mappe di profondità di base) per affrontare il difficile problema SfM denso.
L'apprendimento delle differenze temporali con l'approssimazione delle funzioni è noto per essere instabile.Lavori precedenti come \citet{sutton2009fast} e \citet{sutton2009convergent} hanno presentato obiettivi alternativi che sono stabili per minimizzare.Tuttavia, in pratica, l'apprendimento TD con reti neurali richiede vari trucchi come l'utilizzo di una rete target che si aggiorna lentamente \citep{mnih2015human}. Questo vincolo può essere applicato ai gradienti di qualsiasi obiettivo TD, e può essere facilmente applicato all'approssimazione di funzioni non lineari. Convalidiamo questo aggiornamento applicando la nostra tecnica al Q-learning profondo, e all'addestramento senza una rete target.
Proponiamo DuoRC, un nuovo set di dati per la comprensione della lettura (RC) che motiva diverse nuove sfide per gli approcci neurali nella comprensione del linguaggio oltre a quelle offerte dai set di dati RC esistenti.DuoRC contiene 186.089 coppie uniche domanda-risposta create da una collezione di 7680 coppie di trame di film dove ogni coppia nella collezione riflette due versioni dello stesso film - una da Wikipedia e l'altra da IMDb - scritte da due autori diversi. Abbiamo chiesto ai lavoratori in crowdsourcing di creare domande da una versione della trama e a un diverso gruppo di lavoratori di estrarre o sintetizzare le risposte corrispondenti dall'altra versione.Questa caratteristica unica di DuoRC in cui le domande e le risposte sono create da diverse versioni di un documento che narra la stessa storia sottostante, assicura, per progettazione, che ci sia pochissima sovrapposizione lessicale tra le domande create da una versione e i segmenti contenenti la risposta nell'altra versione.Inoltre, poiché le due versioni hanno diverso livello di dettaglio della trama, stile di narrazione, lessico, ecc, Rispondere alle domande della seconda versione richiede una comprensione più profonda del linguaggio e l'incorporazione di conoscenze di base non disponibili nel testo dato.Inoltre, lo stile narrativo dei passaggi derivanti dalle trame dei film (in contrasto con i tipici passaggi descrittivi nei set di dati esistenti) mostra la necessità di eseguire un ragionamento complesso sugli eventi attraverso più frasi. In effetti, osserviamo che i modelli neurali RC allo stato dell'arte, che hanno raggiunto prestazioni vicine a quelle umane sul set di dati SQuAD, anche quando sono accoppiati con tecniche NLP tradizionali per affrontare le sfide presentate in DuoRC, mostrano prestazioni molto scarse (punteggio F1 del 37,42% su DuoRC contro l'86% sul set di dati SQuAD). Questo apre diverse interessanti strade di ricerca in cui DuoRC potrebbe integrare altri set di dati in stile Reading Comprehension per esplorare nuovi approcci neurali per studiare la comprensione del linguaggio.
Consideriamo il problema della predizione strutturata debolmente supervisionata (SP) con apprendimento di rinforzo (RL) - per esempio, data una tabella di database e una domanda, eseguire una sequenza di azioni di calcolo sulla tabella, che genera una risposta e riceve una ricompensa binaria successo-fallimento.   Questa linea di ricerca ha avuto successo sfruttando la RL per ottimizzare direttamente le metriche desiderate dei compiti SP - per esempio, la precisione nella risposta alle domande o il punteggio BLEU nella traduzione automatica.  Tuttavia, diversamente dalle comuni impostazioni di RL, la dinamica dell'ambiente è deterministica in SP, il che non è stato pienamente utilizzato dai metodi di RL senza modello che vengono solitamente applicati. Poiché i modelli SP hanno solitamente pieno accesso alla dinamica dell'ambiente, proponiamo di applicare metodi RL basati sul modello, che si basano sulla pianificazione come componente primaria del modello. Dimostriamo l'efficacia della RL basata sulla pianificazione con un Neural Program Planner (NPP), che, dato un insieme di programmi candidati da una politica di ricerca preaddestrata, decide quale programma è il più promettente considerando tutte le informazioni generate dall'esecuzione di questi programmi.Valutiamo NPP sulla sintesi di programmi debolmente supervisionati dal linguaggio naturale (parsing semantico) impilando un modulo di pianificazione basato su politiche di ricerca preaddestrate.Sul benchmark WIKITABLEQUESTIONS, NPP raggiunge un nuovo state-of-the-art del 47,2% di precisione.
Per affrontare questo costo, sono stati proposti diversi schemi di quantizzazione, ma la maggior parte di queste tecniche si è concentrata sulla quantizzazione dei pesi, che sono relativamente più piccoli rispetto alle attivazioni. Questo articolo propone un nuovo schema di quantizzazione per le attivazioni durante l'addestramento, che permette alle reti neurali di lavorare bene con pesi e attivazioni di precisione ultra bassa senza alcuna degradazione significativa della precisione.  Questa tecnica, PArameterized Clipping acTi-vation (PACT), utilizza un parametro di clipping di attivazione α che è ottimizzato durante l'addestramento per trovare la giusta scala di quantizzazione. Mostriamo, per la prima volta, che sia i pesi che le attivazioni possono essere quantizzati a 4-bit di precisione, pur raggiungendo un'accuratezza paragonabile alle reti a precisione completa su una gamma di modelli e set di dati popolari. Mostriamo anche che lo sfruttamento di queste unità di calcolo a precisione ridotta nell'hardware può consentire un miglioramento super-lineare nelle prestazioni di inferenza grazie a una significativa riduzione dell'area dei motori di calcolo dell'acceleratore, unita alla capacità di mantenere il modello quantizzato e i dati di attivazione nelle memorie on-chip.
La potatura dei parametri delle reti neurali è spesso vista come un mezzo per comprimere i modelli, ma la potatura è stata anche motivata dal desiderio di prevenire l'overfitting. Questa motivazione è particolarmente rilevante data l'osservazione forse sorprendente che un'ampia varietà di approcci di potatura aumenta la precisione del test nonostante le riduzioni talvolta massicce del numero di parametri. Per comprendere meglio questo fenomeno, analizziamo il comportamento della potatura nel corso dell'addestramento, trovando che l'effetto della potatura sulla generalizzazione si basa più sull'instabilità che genera (definita come le cadute nell'accuratezza del test immediatamente dopo la potatura) che sulla dimensione finale del modello potato. Dimostriamo che anche la potatura di parametri non importanti può portare a tale instabilità, e mostriamo somiglianze tra la potatura e la regolarizzazione iniettando rumore, suggerendo un meccanismo per i miglioramenti di generalizzazione basati sulla potatura che è compatibile con la forte generalizzazione recentemente osservata nelle reti iper-parametrizzate.
Le reti neurali addestrate con backpropagation, l'algoritmo standard dell'apprendimento profondo che utilizza il trasporto dei pesi, sono facilmente ingannate dagli attacchi avversari esistenti basati sul gradiente. Questa classe di attacchi si basa su alcune piccole perturbazioni degli input per far sì che le reti li classifichino in modo errato. Testate su MNIST, le reti neurali profonde addestrate senza trasporto di pesi (1) hanno un'accuratezza avversaria del 98% rispetto allo 0,03% per le reti neurali addestrate con backpropagation e (2) generano esempi avversari non trasferibili.Tuttavia, questo divario diminuisce su CIFAR-10 ma è ancora significativo soprattutto per piccole perturbazioni di grandezza inferiore a 1 ⁄ 2.
Le reazioni chimiche possono essere descritte come la ridistribuzione graduale degli elettroni nelle molecole.Come tali, le reazioni sono spesso rappresentate usando diagrammi "a freccia" che mostrano questo movimento come una sequenza di frecce.Proponiamo un modello di predizione del percorso degli elettroni (ELECTRO) per imparare queste sequenze direttamente dai dati grezzi di reazione. Invece di predire le molecole di prodotto direttamente dalle molecole reagenti in un colpo solo, l'apprendimento di un modello di movimento degli elettroni ha i vantaggi di (a) essere facile da interpretare per i chimici, (b) incorporare i vincoli della chimica, come i conteggi equilibrati degli atomi prima e dopo la reazione, e (c) codificare naturalmente la sparsità delle reazioni chimiche, che di solito coinvolgono cambiamenti solo in un piccolo numero di atomi nei reagenti. Progettiamo un metodo per estrarre percorsi di reazione approssimativi da qualsiasi set di dati di stringhe SMILES di reazione mappate con gli atomi. Il nostro modello raggiunge prestazioni eccellenti su un importante sottoinsieme del set di dati di reazione USPTO, confrontandosi favorevolmente con le linee di base più forti.
Quando i modelli di apprendimento automatico sono utilizzati per decisioni ad alta posta in gioco, dovrebbero predire accuratamente, equamente e responsabilmente. Per soddisfare questi tre requisiti, un modello deve essere in grado di produrre un'opzione di rifiuto (cioè dire "``I Don't Know") quando non è qualificato per fare una previsione. Mostriamo che imparare a differire generalizza il quadro di apprendimento del rifiuto in due modi: considerando l'effetto di altri agenti nel processo decisionale, e consentendo l'ottimizzazione di obiettivi complessi.Proponiamo un algoritmo di apprendimento che tiene conto di potenziali pregiudizi tenuti da decision-makerslater in una pipeline.Experiments su dataset del mondo reale dimostrano che learningto defer può rendere un modello non solo più preciso, ma anche meno biased.Even quando operato da utenti altamente biased, mostriamo chedeferring modelli possono ancora migliorare notevolmente la correttezza dell'intera pipeline.
Mentre molti pianificatori HTN possono effettuare chiamate a processi esterni (ad esempio a un'interfaccia del simulatore) durante il processo di decomposizione, questo è un processo computazionalmente costoso, quindi le implementazioni dei pianificatori spesso utilizzano tali chiamate in modo ad-hoc utilizzando una conoscenza del dominio molto specializzata per limitare il numero di chiamate. Al contrario, i pochi pianificatori classici che sono in grado di usare chiamate esterne (spesso chiamate allegati semantici) durante la pianificazione lo fanno in modi molto più limitati, generando un numero fisso di operatori di terra al momento del grounding del problema. In questo articolo sviluppiamo la nozione di allegati semantici per la pianificazione HTN utilizzando semi-croutine, permettendo a tali predicati definiti proceduralmente di collegare il processo di pianificazione a unificazioni personalizzate al di fuori del pianificatore. Il pianificatore risultante può quindi utilizzare tali co-routine come parte del suo meccanismo di backtracking per cercare attraverso dimensioni parallele dello spazio di stato (ad esempio attraverso variabili numeriche).
La letteratura recente suggerisce che i vettori di parole mediati seguiti da una semplice post-elaborazione superano molti metodi di apprendimento profondo su compiti di somiglianza semantica testuale; inoltre, quando i vettori di parole mediati sono addestrati in modo supervisionato su grandi corpora di parafrasi, raggiungono risultati all'avanguardia su benchmark STS standard. Ispirati da queste intuizioni, spingiamo i limiti delle incorporazioni di parole ancora più in là e proponiamo una nuova rappresentazione fuzzy bag-of-words (FBoW) per il testo che contiene tutte le parole nel vocabolario simultaneamente ma con diversi gradi di appartenenza, che sono derivati dalle somiglianze tra i vettori di parole. Mostriamo che i vettori di parole max-pooled sono solo un caso speciale di fuzzy BoW e dovrebbero essere confrontati tramite l'indice fuzzy Jaccard piuttosto che la somiglianza del coseno. Infine, proponiamo DynaMax, una misura di somiglianza completamente non supervisionata e non parametrica che estrae dinamicamente e max-pooling di buone caratteristiche a seconda della coppia di frasi. Questo metodo è efficiente e facile da implementare, ma supera le attuali linee di base sui compiti STS con un ampio margine ed è persino competitivo con i vettori di parole supervisionati addestrati per ottimizzare direttamente la somiglianza coseno.
Lo stato dell'arte dei risultati nell'apprendimento per imitazione è attualmente detenuto da metodi avversari che stimano iterativamente la divergenza tra le politiche degli studenti e quelle degli esperti e poi minimizzano questa divergenza per portare la politica di imitazione più vicina al comportamento dell'esperto.Tecniche analoghe per l'apprendimento per imitazione dalle sole osservazioni (senza etichette di azioni degli esperti), tuttavia, non hanno goduto degli stessi successi onnipresenti. Il lavoro recente nei metodi avversari per i modelli generativi ha dimostrato che la misura utilizzata per giudicare la discrepanza tra i campioni reali e sintetici è una scelta di progettazione algoritmica, e che scelte diverse possono portare a differenze significative nelle prestazioni del modello. Scelte che includono la distanza di Wasserstein e varie $f$-divergenze sono già state esplorate nella letteratura sulle reti avversarie, mentre più recentemente quest'ultima classe è stata studiata per l'apprendimento per imitazione. Sfortunatamente, troviamo che in pratica questa struttura esistente di apprendimento per imitazione per l'utilizzo delle $f$-divergenze soffre di instabilità numeriche derivanti dalla combinazione di approssimazione delle funzioni e apprendimento per rinforzo a gradiente. In questo lavoro, alleviamo queste sfide e offriamo una riparametrizzazione dell'apprendimento dell'imitazione avversaria come minimizzazione della $f$-divergenza prima di estendere ulteriormente il quadro per gestire il problema dell'imitazione dalle sole osservazioni. Empiricamente, dimostriamo che le nostre scelte di design per accoppiare l'apprendimento dell'imitazione e le $f$-divergenze sono fondamentali per recuperare politiche di imitazione di successo. Inoltre, troviamo che con la scelta appropriata di $f$-divergenza, possiamo ottenere algoritmi di imitazione dall'osservazione che superano gli approcci di base e corrispondono più da vicino alle prestazioni degli esperti in compiti di controllo continuo con spazi di osservazione bidimensionali.Con osservazioni ad alta densità, osserviamo ancora un divario significativo con e senza etichette di azione, offrendo una strada interessante per il lavoro futuro.
Le fluttuazioni momentanee nell'attenzione (accuratezza percettiva) sono correlate con le fluttuazioni dell'attività neurale nelle aree visive dei primati, ma il legame tra queste fluttuazioni neurali momentanee e lo stato di attenzione non è ancora stato dimostrato nel cervello umano, e noi indaghiamo questo legame usando un'interfaccia cognitiva in tempo reale basata sui potenziali evocati visivamente allo stato stazionario (SSVEPs): potenziali EEG occipitali evocati da stimoli che lampeggiano ritmicamente. Seguendo le fluttuazioni momentanee nella potenza SSVEP, in tempo reale, abbiamo presentato degli stimoli legati al tempo in cui questa potenza ha raggiunto soglie (predeterminate) alte o basse. Abbiamo osservato un aumento significativo nell'accuratezza della discriminazione (d') quando gli stimoli sono stati attivati durante epoche di alta (contro bassa) potenza SSVEP, nella posizione indicata per l'attenzione.
Studi recenti hanno dimostrato la vulnerabilità delle reti neurali convoluzionali profonde contro gli esempi avversari.Ispirati dall'osservazione che la dimensione intrinseca dei dati di immagine è molto più piccola della sua dimensione dello spazio dei pixel e la vulnerabilità delle reti neurali cresce con la dimensione dell'input, proponiamo di incorporare immagini di input ad alta dimensione in uno spazio a bassa dimensione per eseguire la classificazione. Tuttavia, proiettare arbitrariamente le immagini di input in uno spazio bidimensionale senza regolarizzazione non migliorerà la robustezza delle reti neurali profonde.Proponiamo un nuovo quadro, Embedding Regularized Classifier (ER-Classifier), che migliora la robustezza avversaria del classificatore attraverso la regolarizzazione embedding.I risultati sperimentali su diversi set di dati di riferimento mostrano che, il nostro quadro proposto raggiunge lo stato dell'arte delle prestazioni contro i forti metodi di attacco avversariale.
Il nostro metodo misura le somiglianze tra le attività utilizzando la matrice delle prestazioni di trasferimento tra attività, anche se questa matrice ci fornisce informazioni critiche per quanto riguarda le somiglianze tra le attività, le coppie di attività incerte, vale a dire quelle con prestazioni estremamente asimmetriche, Per superare queste limitazioni, proponiamo un nuovo algoritmo di clustering per stimare la matrice di somiglianza basato sulla teoria del completamento della matrice. L'algoritmo proposto può lavorare su matrici di somiglianza parzialmente osservate basate solo su coppie di compiti campionati con punteggi affidabili, garantendo la sua efficienza e robustezza. La nostra analisi teorica mostra che, sotto ipotesi blande, la matrice ricostruita corrisponde perfettamente alla matrice di somiglianza "vera" sottostante con una probabilità schiacciante.La partizione finale delle attività viene calcolata applicando un efficiente algoritmo di clustering spettrale alla matrice recuperata.I nostri risultati mostrano che il nuovo metodo di clustering delle attività può scoprire cluster di attività che avvantaggiano sia l'apprendimento multi-task che le impostazioni di apprendimento a pochi colpi per la classificazione del sentimento e le attività di classificazione dell'intento del dialogo.
La somiglianza tra i metodi utilizzati nello studio della fisica dei corpi quantistici e nell'apprendimento automatico ha attirato una notevole attenzione. In particolare, le reti di tensori (TN) e le architetture di apprendimento profondo presentano notevoli somiglianze nella misura in cui le TN possono essere utilizzate per l'apprendimento automatico.I risultati precedenti hanno utilizzato TN unidimensionali nel riconoscimento delle immagini, mostrando una scalabilità limitata e una richiesta di dimensioni di legame elevate.In questo lavoro, addestriamo TN gerarchiche bidimensionali per risolvere problemi di riconoscimento delle immagini, utilizzando un algoritmo di formazione derivato dal multipartite entanglement renormalization ansatz (MERA). Questo approccio supera i problemi di scalabilità e implica nuove connessioni matematiche tra la fisica quantistica a molti corpi, la teoria dell'informazione quantistica e l'apprendimento automatico. mantenendo il TN unitario nella fase di addestramento, possono essere definiti gli stati TN, che codificano in modo ottimale ogni classe delle immagini in uno stato quantistico a molti corpi. Studiamo le caratteristiche quantistiche degli stati TN, tra cui l'entanglement quantistico e la fedeltà.Suggeriamo che queste quantità potrebbero essere nuove proprietà che caratterizzano le classi di immagini, così come i compiti di apprendimento automatico.Il nostro lavoro potrebbe essere ulteriormente applicato per identificare possibili proprietà quantistiche di alcuni metodi di intelligenza artificiale.
Una varietà di tali compiti coinvolge sistemi fisici continui, che possono essere descritti da equazioni differenziali parziali (PDE) con molti gradi di libertà. I metodi esistenti che mirano a controllare la dinamica di tali sistemi sono tipicamente limitati a tempi relativamente brevi o a un piccolo numero di parametri di interazione. Entrambe le fasi sono addestrate end-to-end utilizzando un risolutore di PDE differenziabili. dimostriamo che il nostro metodo sviluppa con successo una comprensione di sistemi fisici complessi e impara a controllarli per compiti che coinvolgono PDE come le equazioni di Navier-Stokes incomprimibili.
La capacità delle reti profonde iperparametrizzate di generalizzare bene è stata collegata al fatto che la discesa stocastica del gradiente (SGD) trova soluzioni che si trovano in minimi piatti e ampi nella perdita di addestramento - minimi in cui l'uscita della rete è resiliente al piccolo rumore casuale aggiunto ai suoi parametri. Finora questa osservazione è stata usata per fornire garanzie di generalizzazione solo per reti neurali i cui parametri sono o \textit{stocastico} o \textit{compresso}. In questo lavoro, presentiamo un quadro generale PAC-Bayesiano che sfrutta questa osservazione per fornire un limite alla rete originale appresa -- una rete che è deterministica e non compressa.  Ciò che ci permette di fare questo è una novità chiave nel nostro approccio: la nostra struttura ci permette di mostrare che se sui dati di allenamento, le interazioni tra le matrici di peso soddisfano certe condizioni che implicano un'ampia perdita minima di allenamento, queste stesse condizioni si generalizzano alle interazioni tra le matrici sui dati di test, implicando così un'ampia perdita minima di test. Applichiamo quindi il nostro quadro generale in una configurazione in cui assumiamo che i valori di pre-attivazione della rete non siano troppo piccoli (anche se lo assumiamo solo sui dati di allenamento). In questa configurazione, forniamo una garanzia di generalizzazione per la rete originale (deterministica, non compressa), che non scala con il prodotto delle norme spettrali delle matrici di peso - una garanzia che non sarebbe stata possibile con approcci precedenti.
L'addestramento delle reti neurali per essere certifiably robusto è critico per assicurare la loro sicurezza contro gli attacchi avversari.Tuttavia, è attualmente molto difficile addestrare una rete neurale che è sia accurata che certifiably robusta.In questo lavoro facciamo un passo verso l'indirizzo di questa sfida. Dimostriamo che per ogni funzione continua $f$, esiste una rete $n$ tale che:(i) $n$ approssima $f$ in modo arbitrariamente vicino, e(ii) la semplice propagazione dell'intervallo di una regione $B$ attraverso $n$ produce un risultato che è arbitrariamente vicino all'output ottimale di $f$ su $B$.Il nostro risultato può essere visto come un teorema di approssimazione universale per reti ReLU certificate per l'intervallo.Per quanto ne sappiamo, questo è il primo lavoro a dimostrare l'esistenza di reti accurate certificate per l'intervallo.
In questo articolo, proponiamo un quadro efficiente per accelerare le reti neurali convoluzionali.Utilizziamo due tipi di metodi di accelerazione: la potatura e i suggerimenti.La potatura può ridurre le dimensioni del modello rimuovendo i canali degli strati.I suggerimenti possono migliorare le prestazioni del modello dello studente trasferendo la conoscenza dal modello dell'insegnante.Dimostriamo che la potatura e i suggerimenti sono complementari tra loro.Da un lato, i suggerimenti possono beneficiare della potatura mantenendo rappresentazioni di caratteristiche simili. D'altra parte, il modello potato dalle reti degli insegnanti è una buona inizializzazione per il modello degli studenti, il che aumenta la trasferibilità tra due reti.Il nostro approccio esegue la fase di potatura e la fase dei suggerimenti in modo iterativo per migliorare ulteriormente le prestazioni.Inoltre, proponiamo un algoritmo per ricostruire i parametri dello strato dei suggerimenti e rendere il modello potato più adatto ai suggerimenti.Gli esperimenti sono stati condotti su vari compiti tra cui la classificazione e la stima della posa.I risultati su CIFAR-10, ImageNet e COCO dimostrano la generalizzazione e la superiorità della nostra struttura.
Per i tipici problemi di predizione delle sequenze, come la generazione della lingua, la stima della massima verosimiglianza (MLE) è stata comunemente adottata in quanto incoraggia la sequenza predetta più coerente con la sequenza ground-truth ad avere la massima probabilità di verificarsi. Tuttavia, MLE si concentra sulla corrispondenza una volta per tutte tra la sequenza prevista e il gold-standard, trattando di conseguenza tutte le previsioni errate come ugualmente errate; in questo documento ci riferiamo a questo inconveniente come ignoranza negativa della diversità, trattando tutte le previsioni errate come uguali e sminuendo ingiustamente la sfumatura della struttura dettagliata di queste sequenze in termini di token. Per contrastare questo, aumentiamo la perdita MLE introducendo un ulteriore termine di divergenza di Kullback-Leibler derivato dal confronto tra un priore gaussiano dipendente dai dati e la previsione dettagliata dell'addestramento. L'obiettivo proposto per il priore gaussiano dipendente dai dati (D2GPo) è definito su un ordine topologico precedente di token ed è diverso dal priore gaussiano indipendente dai dati (regolarizzazione L2) comunemente adottato per lisciare l'addestramento di MLE. I risultati sperimentali mostrano che il metodo proposto fa un uso efficace di un priore più dettagliato nei dati e ha migliorato le prestazioni nei tipici compiti di generazione del linguaggio, tra cui la traduzione automatica supervisionata e non supervisionata, il riassunto del testo, la narrazione e le didascalie delle immagini.
Proponiamo un approccio di classificazione interattiva per le query in linguaggio naturale: invece di classificare solo la query in linguaggio naturale, chiediamo all'utente informazioni aggiuntive usando una sequenza di domande binarie e a scelta multipla. Ad ogni turno, usiamo un controller di policy per decidere se presentare una domanda o fornire all'utente la risposta finale, e selezionare la migliore domanda da porre massimizzando il guadagno informativo del sistema. La nostra formulazione permette di avviare il sistema senza alcun dato di interazione, affidandosi invece a compiti di an-notazione non interattivi di crowdsourcing.La nostra valutazione mostra che l'interazione aiuta il sistema ad aumentare la sua precisione e a gestire le domande ambigue, mentre il nostro approccio bilancia efficacemente il numero di domande e la precisione finale.
Le reti neurali convoluzionali (CNN) hanno raggiunto lo stato dell'arte nel riconoscimento e nella rappresentazione di audio, immagini, video e volumi 3D; cioè, domini in cui l'input può essere caratterizzato da una struttura a grafo regolare. Tuttavia, generalizzare le CNN a domini irregolari come le mesh 3D è impegnativo, inoltre i dati di allenamento per le mesh 3D sono spesso limitati. In questo lavoro, generalizziamo gli autocodificatori convoluzionali alle superfici delle mesh, eseguiamo la decomposizione spettrale delle mesh e applichiamo le convoluzioni direttamente nello spazio delle frequenze. Inoltre, usiamo il max pooling e introduciamo l'upsampling all'interno della rete per rappresentare le mesh in uno spazio a bassa dimensionalità.Costruiamo un dataset complesso di 20.466 mesh ad alta risoluzione con espressioni facciali estreme e lo codifichiamo usando il nostro Convolutional Mesh Autoencoder.Nonostante i dati di allenamento limitati, il nostro metodo supera i modelli PCA all'avanguardia dei volti con un errore inferiore del 50%, mentre utilizza il 75% di parametri in meno.
Il calcolo delle distanze tra gli esempi è al centro di molti algoritmi di apprendimento per le serie temporali. Di conseguenza, una grande quantità di lavoro è stata dedicata alla progettazione di efficaci misure di distanza per le serie temporali. Il nostro approccio è quello di riformulare il compito come un problema di apprendimento della rappresentazione - piuttosto che progettare una funzione di distanza elaborata, usiamo una CNN per imparare un incorporamento tale che la distanza euclidea è efficace.
La corteccia prefrontale (PFC) è una parte del cervello che è responsabile del repertorio dei comportamenti. Ispirandosi alla funzionalità e alla connettività della PFC, così come al processo di formazione del comportamento umano, proponiamo una nuova architettura modulare di reti neurali con un modulo comportamentale (BM) e una corrispondente strategia di formazione end-to-end.  Questo approccio permette l'apprendimento efficiente dei comportamenti e la rappresentazione delle preferenze; questa proprietà è particolarmente utile per la modellazione dell'utente (come per gli agenti di dialogo) e i compiti di raccomandazione, poiché permette di imparare rappresentazioni personalizzate dei diversi stati dell'utente.  Nell'esperimento con i videogiochi, i risultati mostrano che il metodo proposto permette la separazione degli obiettivi e dei comportamenti del compito principale tra diversi BMs. Gli esperimenti mostrano anche l'estendibilità della rete attraverso l'apprendimento indipendente di nuovi modelli di comportamento.
Una componente importante di overfitting nell'apprendimento di rinforzo senza modello (RL) coinvolge il caso in cui l'agente può erroneamente correlare la ricompensa con alcune caratteristiche spurie dalle osservazioni generate dal processo decisionale di Markov (MDP).Forniamo un quadro generale per analizzare questo scenario, che usiamo per progettare più benchmark sintetici dalla sola modifica dello spazio di osservazione di un MDP. Quando un agente si adatta a diversi spazi di osservazione anche se la dinamica MDP sottostante è fissa, definiamo questo overfitting osservazionale. I nostri esperimenti espongono proprietà intriganti soprattutto per quanto riguarda la regolarizzazione implicita, e corroborano anche i risultati di lavori precedenti nella generalizzazione RL e nell'apprendimento supervisionato (SL).
Le reti neurali ricorrenti standard sono limitate dalla loro struttura e non riescono a utilizzare in modo efficiente le informazioni sintattiche; d'altra parte, le reti ricorsive strutturate ad albero di solito richiedono una supervisione strutturale aggiuntiva al costo dell'annotazione di esperti umani. In questo articolo, proponiamo un nuovo modello neurale del linguaggio, chiamato Parsing-Reading-Predict Networks (PRPN), che può indurre simultaneamente la struttura sintattica da frasi non annotate e sfruttare la struttura dedotta per imparare un modello linguistico migliore. Nel nostro modello, il gradiente può essere direttamente retro-propagato dalla perdita del modello linguistico nella rete di parsing neurale.
L'apprendimento incorporato non supervisionato mira ad estrarre buone rappresentazioni dai dati senza l'uso di etichette commentate dall'uomo.Tali tecniche sono apparentemente alla ribalta a causa delle sfide nella raccolta di etichette su larga scala necessarie per l'apprendimento supervisionato.Questo articolo propone un approccio completo, chiamato Super-AND, che si basa sul modello Anchor Neighbourhood Discovery. Come risultato, il nostro modello supera gli approcci esistenti in vari set di dati di riferimento e raggiunge una precisione dell'89,2% in CIFAR-10 con la rete dorsale Resnet18, un guadagno del 2,9% rispetto allo stato dell'arte.
L'analisi dei vetrini istopatologici è un passo critico per molte diagnosi, e in particolare in oncologia, dove definisce il gold standard. Nel caso dell'analisi istopatologica digitale, i patologi altamente qualificati devono esaminare vaste immagini di vetrini interi di estrema risoluzione digitale (100.000^2 pixel) su più livelli di zoom per individuare regioni anomale di cellule, o in alcuni casi singole cellule, su milioni. L'applicazione del deep learning a questo problema è ostacolata non solo dalle piccole dimensioni dei campioni, dato che i tipici set di dati contengono solo poche centinaia di campioni, ma anche dalla generazione di annotazioni localizzate ground-truth per la formazione di modelli di classificazione e segmentazione interpretabili. Anche senza annotazioni a livello di pixel, siamo in grado di dimostrare prestazioni paragonabili ai modelli addestrati con annotazioni forti sulla sfida di rilevamento delle metastasi linfonodali Camelyon-16. Realizziamo questo attraverso l'uso di reti convoluzionali profonde pre-addestrate, l'incorporazione di caratteristiche, così come l'apprendimento tramite istanze superiori e prove negative, una tecnica di apprendimento di istanze multiple daatp il campo della segmentazione semantica e del rilevamento di oggetti.
I problemi di previsione/classificazione massicciamente multietichetta sorgono in ambienti come l'assistenza sanitaria o la biologia, dove è utile fare previsioni molto precise. Una sfida con i problemi massicciamente multietichetta è che spesso c'è una distribuzione di frequenza a coda lunga per le etichette, con conseguenti pochi esempi positivi per le etichette rare. Noi proponiamo una soluzione a questo problema modificando lo strato di uscita di una rete neurale per creare una rete bayesiana di sigmoidi che sfrutta le relazioni ontologiche tra le etichette per aiutare a condividere informazioni tra le etichette rare e quelle più comuni.  Applichiamo questo metodo ai due compiti massicciamente multietichetta della predizione delle malattie (codici ICD-9) e della predizione delle funzioni proteiche (termini della Gene Ontology) e otteniamo miglioramenti significativi in AUROC per etichetta e precisione media.
I Generative Adversarial Networks hanno reso possibile la generazione di dati in vari casi d'uso, ma nel caso di distribuzioni complesse e altamente dimensionali può essere difficile addestrarli, a causa di problemi di convergenza e la comparsa di collassi di modalità. Le GAN Sliced Wasserstein e specialmente l'applicazione della distanza Max-Sliced Wasserstein hanno reso possibile approssimare la distanza di Wasserstein durante l'addestramento in un modo efficiente e stabile e hanno contribuito ad alleviare i problemi di convergenza di queste architetture. Questo metodo trasforma l'assegnazione dei campioni e il calcolo della distanza nell'ordinamento della proiezione monodimensionale dei campioni, che risulta in un'approssimazione sufficiente della distanza Wasserstein ad alta dimensione. In questo articolo dimostreremo che l'approssimazione della distanza di Wasserstein attraverso l'ordinamento dei campioni non è sempre l'approccio ottimale e l'assegnazione avida dei campioni reali e falsi può risultare in una convergenza più veloce e in una migliore approssimazione della distribuzione originale.
L'apprendimento automatico permanente si concentra sull'adattamento a nuovi compiti senza dimenticare i vecchi compiti, mentre l'apprendimento a pochi colpi si sforza di imparare un singolo compito dato una piccola quantità di dati.Queste due diverse aree di ricerca sono cruciali per l'intelligenza artificiale generale, tuttavia, i loro studi esistenti hanno in qualche modo assunto alcune impostazioni poco pratiche durante l'addestramento dei modelli.Per l'apprendimento permanente, la natura (o la quantità) dei compiti in arrivo durante il tempo di inferenza si assume che sia nota al momento dell'addestramento.Come per l'apprendimento a pochi colpi, è comunemente assunto che un gran numero di compiti sia disponibile durante l'addestramento. Ispirandoci a come funziona il cervello umano, proponiamo un nuovo modello, chiamato Slow Thinking to Learn (STL), che fa previsioni sofisticate (e leggermente più lente) considerando iterativamente le interazioni tra i compiti attuali e quelli visti in precedenza al momento dell'addestramento.Avendo condotto degli esperimenti, i risultati dimostrano empiricamente l'efficacia di STL per impostazioni di apprendimento più realistiche per tutta la vita e few-shot.
Gli hypernetwork sono meta reti neurali che generano pesi per una rete neurale principale in un modo differenziabile end-to-end.Nonostante le ampie applicazioni che vanno dal multi-task learning al deep learning bayesiano, il problema dell'ottimizzazione degli hypernetwork non è stato studiato fino ad oggi. Osserviamo che i metodi classici di inizializzazione dei pesi come Glorot & Bengio (2010) e He et al. (2015), quando applicati direttamente su una hypernet, non riescono a produrre pesi per la mainnet nella scala corretta.Sviluppiamo tecniche di principio per l'inizializzazione dei pesi nelle hypernet, e dimostriamo che portano a pesi mainnet più stabili, perdita di formazione inferiore, e convergenza più veloce.
Per la modellazione congiunta bidirezionale dell'immagine e del testo, sviluppiamo la rete adversariale generativa randomizzata variazionale etero-encoder (VHE) (GAN), un modello generativo profondo versatile che integra un decodificatore probabilistico del testo, un codificatore probabilistico dell'immagine e una GAN in un quadro coerente di apprendimento multimodale end-to-end. Inseriamo tre moduli di serie, tra cui un modello di argomento profondo, un codificatore di immagini strutturato in scala e StackGAN++, in VHE-GAN, che già raggiunge prestazioni competitive. Questo motiva ulteriormente lo sviluppo di VHE-raster-scan-GAN che genera immagini fotorealistiche non solo in un modo multi-scala a bassa-alta risoluzione, ma anche in un modo gerarchico-semantico grossolano-fine. Catturando e mettendo in relazione concetti gerarchici semantici e visivi con un addestramento end-to-end, VHE-raster-scan-GAN raggiunge prestazioni allo stato dell'arte in un'ampia varietà di compiti di apprendimento e generazione di immagini e testi multimodali.
Gli attuali pianificatori classici hanno molto successo nel trovare piani (non ottimali), anche per istanze di pianificazione di grandi dimensioni.Per fare ciò, la maggior parte dei pianificatori si basa su una fase di preprocessing che calcola una rappresentazione grounded del compito.Ogni volta che il compito grounded è troppo grande per essere generato (cioè, Per affrontare questo problema, introduciamo un approccio di grounding parziale che fonda solo una proiezione del compito, quando il grounding completo non è fattibile.Proponiamo un meccanismo di guida che, per un dato dominio, identifica le parti di un compito che sono rilevanti per trovare un piano utilizzando metodi di apprendimento automatico off-the-shelf.La nostra valutazione empirica attesta che l'approccio è capace di risolvere istanze di pianificazione che sono troppo grandi per essere completamente grounded.
In questo articolo, introduciamo un nuovo metodo per interpretare le reti neurali ricorrenti (RNNs), in particolare le reti a memoria lunga a breve termine (LSTMs) a livello cellulare.Proponiamo una pipeline sistematica per interpretare le singole dinamiche di stato nascoste all'interno della rete utilizzando metodi di caratterizzazione delle risposte.Il contributo classificato delle singole cellule all'output della rete viene calcolato analizzando un insieme di metriche interpretabili delle loro risposte disaccoppiate a gradino e sinusoidali. Come risultato, il nostro metodo è in grado di identificare in modo univoco i neuroni con dinamiche perspicaci, quantificare le relazioni tra le proprietà dinamiche e l'accuratezza della prova attraverso l'analisi dell'ablazione e interpretare l'impatto della capacità della rete sulla distribuzione dinamica di una rete.Infine, dimostriamo la generalizzabilità e la scalabilità del nostro metodo valutando una serie di diversi set di dati sequenziali di riferimento.
La corteccia Entorhinal (CE) del cervello dei mammiferi contiene una ricca serie di correlati spaziali, comprese le cellule di griglia che codificano lo spazio utilizzando modelli tessellating.Tuttavia, i meccanismi e il significato funzionale di queste rappresentazioni spaziali rimangono in gran parte misteriosi.Come un nuovo modo per capire queste rappresentazioni neurali, abbiamo addestrato reti neurali ricorrenti (RNNs) per eseguire compiti di navigazione in arene 2D basato su input di velocità. Tutti questi diversi tipi funzionali di neuroni sono stati osservati sperimentalmente e l'ordine di comparsa delle cellule a griglia e di quelle di confine è anche coerente con le osservazioni degli studi sullo sviluppo. Insieme, i nostri risultati suggeriscono che le cellule a griglia, le cellule di confine e altre come quelle osservate nella CE possono essere una soluzione naturale per rappresentare lo spazio in modo efficiente, date le connessioni ricorrenti predominanti nei circuiti neurali.
Mentre i modelli end-to-end che generano direttamente la forma d'onda sono all'avanguardia in molti problemi di sintesi audio, i migliori modelli di separazione delle sorgenti multi-strumento generano maschere sullo spettro di magnitudine e raggiungono prestazioni molto al di sopra degli attuali modelli end-to-end, waveform-to-waveform. Presentiamo un'analisi approfondita di una nuova architettura, che chiameremo Demucs, basata su un autocodificatore convoluzionale (trasposto), con un LSTM bidirezionale al livello del collo di bottiglia e connessioni a salto come in U-Networks (Ronneberger et al, Rispetto al modello wave-to-wave state-of-the-art, Wave-U-Net (Stoller et al., 2018), le caratteristiche principali del nostro approccio oltre al bi-LSTM sono l'uso di strati di convoluzione trans-posati invece di blocchi di upsampling-convoluzione, l'uso di unità lineari gated, la crescita esponenziale del numero di canali con profondità e una nuova attenta inizializzazione dei pesi.  I risultati sul set di dati MusDB mostrano che la nostra architettura raggiunge un rapporto segnale-distorsione (SDR) di quasi 2,2 punti superiore al miglior concorrente waveform-to-waveform (da 3,2 a 5,4 SDR).Questo fa sì che il nostro modello corrisponda alle prestazioni dello stato dell'arte su questo set di dati, colmando il divario di prestazioni tra modelli che operano sullo spettrogramma e approcci end-to-end.
Anche se impegnativa, la valutazione dei profili strategici in grandi reti di apprendisti connessi è cruciale per abilitare la prossima ondata di applicazioni di apprendimento automatico.Recentemente, $\alfa$-Rank, un algoritmo evolutivo, è stato proposto come una soluzione per classificare i profili strategici congiunti in sistemi multi-agente. In questo articolo, dimostriamo formalmente che tale affermazione non è fondata: infatti, dimostriamo che $alfa-Rank presenta una complessità esponenziale nel numero di agenti, ostacolando la sua applicazione oltre un piccolo numero finito di profili congiunti. Rendendoci conto di tale limitazione, contribuiamo proponendo un protocollo di valutazione scalabile che chiamiamo $\alfa^{alfa}$-Rank.Il nostro metodo combina la dinamica evolutiva con l'ottimizzazione stocastica e i doppi oracoli per una classifica scalabile con complessità lineare (in numero di agenti) di tempo e memoria. I nostri contributi ci permettono, per la prima volta, di condurre esperimenti di valutazione su larga scala di sistemi multi-agente, dove mostriamo risultati di successo su grandi profili di strategie congiunte con dimensioni dell'ordine di $\mathcal{O}(2^{25})$ (cioè, circa 33 milioni di strategie) - un'impostazione non valutabile con le tecniche attuali.
Le reti neurali profonde sono note per essere affamate di annotazioni. Numerosi sforzi sono stati dedicati a ridurre il costo di annotazione quando si impara con le reti profonde. Due direzioni prominenti includono l'apprendimento con etichette rumorose e l'apprendimento semi-supervisionato sfruttando i dati non etichettati.In questo lavoro, proponiamo DivideMix, un nuovo quadro per l'apprendimento con etichette rumorose sfruttando tecniche di apprendimento semi-supervisionato. In particolare, DivideMix modella la distribuzione delle perdite per campione con un modello misto per dividere dinamicamente i dati di formazione in un set etichettato con campioni puliti e un set non etichettato con campioni rumorosi, e allena il modello su entrambi i dati etichettati e non etichettati in modo semi-supervisionato. Durante la fase di addestramento semi-supervisionato, miglioriamo la strategia MixMatch eseguendo il co-refinement delle etichette e il co-guessing delle etichette sui campioni etichettati e non etichettati, rispettivamente. Gli esperimenti su più set di dati di riferimento dimostrano miglioramenti sostanziali rispetto ai metodi all'avanguardia. Il codice è disponibile su https://github.com/LiJunnan1992/DivideMix .
Presentiamo un nuovo algoritmo per addestrare una rete neurale robusta contro gli attacchi avversari. Il nostro algoritmo è motivato dalle seguenti due idee.In primo luogo, anche se un lavoro recente ha dimostrato che fondere la casualità può migliorare la robustezza delle reti neurali (Liu 2017), abbiamo notato che aggiungere il rumore alla cieca a tutti gli strati non è il modo ottimale per incorporare la casualità. Invece, modelliamo la casualità nel quadro della Rete Neurale Bayesiana (BNN) per imparare formalmente la distribuzione posteriore dei modelli in modo scalabile.In secondo luogo, formuliamo il problema mini-max in BNN per imparare la migliore distribuzione del modello sotto attacchi avversari, portando a una rete neurale bayesiana addestrata in modo avversario. I risultati degli esperimenti dimostrano che l'algoritmo proposto raggiunge prestazioni allo stato dell'arte sotto forti attacchi.Su CIFAR-10 con rete VGG, il nostro modello porta al 14% di miglioramento della precisione rispetto all'addestramento avversario (Madry 2017) e all'auto-assemblaggio casuale (Liu, 2017) sotto attacco PGD con distorsione 0,035, e il divario diventa ancora maggiore su un sottoinsieme di ImageNet.
L'apprendimento federato distribuisce l'addestramento del modello tra una moltitudine di agenti, che, guidati da preoccupazioni di privacy, eseguono l'addestramento utilizzando i loro dati locali ma condividono solo gli aggiornamenti dei parametri del modello, per l'aggregazione iterativa al server. In questo lavoro, esploriamo la minaccia di attacchi di avvelenamento del modello sull'apprendimento federato avviati da un singolo agente maligno non collusivo, dove l'obiettivo avversario è quello di indurre il modello a classificare erroneamente un insieme di input scelti con alta confidenza.esploriamo una serie di strategie per effettuare questo attacco, a partire dal semplice aumento dell'aggiornamento dell'agente maligno per superare gli effetti degli aggiornamenti degli altri agenti. Per aumentare la furtività dell'attacco, proponiamo una strategia di minimizzazione alternata, che ottimizza alternativamente la perdita di addestramento e l'obiettivo avversario.Seguiamo usando la stima dei parametri per gli aggiornamenti degli agenti benigni per migliorare il successo dell'attacco.Infine, usiamo una serie di tecniche di interpretabilità per generare spiegazioni visive delle decisioni del modello sia per i modelli benigni che per quelli maligni e mostriamo che le spiegazioni sono quasi indistinguibili visivamente. I nostri risultati indicano che anche un avversario altamente vincolato può effettuare attacchi di avvelenamento del modello mantenendo contemporaneamente la furtività, evidenziando così la vulnerabilità dell'impostazione di apprendimento federato e la necessità di sviluppare strategie di difesa efficaci.
Nonostante i rapidi progressi nel riconoscimento vocale, i modelli attuali rimangono fragili alle perturbazioni superficiali dei loro input. Piccole quantità di rumore possono distruggere le prestazioni di un modello altrimenti all'avanguardia. Per irrigidire i modelli contro il rumore di fondo, i professionisti spesso eseguono l'aumento dei dati, aggiungendo esempi artificialmente rumorosi al set di allenamento, portando l'etichetta originale. In questo articolo, ipotizziamo che un esempio pulito e le sue controparti superficialmente perturbate non dovrebbero semplicemente mappare alla stessa classe--- dovrebbero mappare alla stessa rappresentazione.Proponiamo un invariant-representation-learning (IRL): Ad ogni iterazione di addestramento, per ogni esempio di addestramento, campioniamo una controparte rumorosa, quindi applichiamo un termine di penalità per forzare le rappresentazioni corrispondenti ad ogni strato (al di sopra di qualche strato scelto). 3% vs 6.5%) e `altro' (11.0% vs 18.1%) set di test;(ii) su diverse impostazioni di rumore fuori dal dominio (diverse da quelle viste durante l'addestramento), i benefici di IRL sono ancora più pronunciati.Attente ablazioni confermano che i nostri risultati non sono semplicemente dovuti alla contrazione delle attivazioni negli strati scelti.
Questo modello organizza la convoluzione convenzionale e la convoluzione rada in modo tale che i modelli armonici globali catturati dalla convoluzione rada siano composti da un numero sufficiente di modelli locali catturati dagli strati della convoluzione convenzionale.Quando viene addestrato sul set di dati MAPS, il modello armonico supera tutti i sistemi esistenti di rilevamento del tono addestrati sullo stesso set di dati. Più impressionante, quando addestrato su MAPS con un semplice aumento dei dati, il modello armonico con uno strato LSTM in cima supera un aggiornato, più complesso sistema di rilevamento della tonalità addestrato sul set di dati MAESTRO a cui viene applicato un complicato aumento dei dati e la cui divisione di formazione è un ordine di grandezza più grande della divisione di formazione di MAPS.Il modello armonico ha dimostrato il potenziale per essere utilizzato per sistemi avanzati di trascrizione musicale automatica (AMT).
L'apprendimento della rappresentazione invariante al dominio è un approccio dominante per la generalizzazione del dominio. Tuttavia, i metodi precedenti basati sull'invarianza del dominio hanno trascurato la dipendenza sottostante delle classi dai domini, che è responsabile del trade-off tra la precisione della classificazione e l'invarianza. Questo studio propone un nuovo metodo {em adversarial feature learning under accuracy constraint (AFLAC)}, che massimizza l'invarianza di dominio entro un intervallo che non interferisce con l'accuratezza.Le convalide empiriche mostrano che le prestazioni di AFLAC sono superiori a quelle dei metodi di base, sostenendo l'importanza di considerare la dipendenza e l'efficacia del metodo proposto per superare il problema.
I recenti progressi nei modelli generativi profondi hanno portato a notevoli progressi nella sintesi di immagini di alta qualità. Dopo il successo della loro applicazione nell'elaborazione delle immagini e nell'apprendimento delle rappresentazioni, un importante passo successivo è quello di considerare i video.Imparare modelli generativi di video è un compito molto più difficile, richiedendo un modello per catturare le dinamiche temporali di una scena, oltre alla presentazione visiva degli oggetti. Mentre i recenti modelli generativi di video hanno avuto un certo successo, il progresso attuale è ostacolato dalla mancanza di metriche qualitative che considerano la qualità visiva, la coerenza temporale e la diversità dei campioni.Per questo motivo proponiamo Fréchet Video Distance (FVD), una nuova metrica per i modelli generativi di video basata su FID.Contribuiamo uno studio umano su larga scala, che conferma che FVD si correla bene con il giudizio umano qualitativo dei video generati.
Nonostante i progressi nell'apprendimento profondo, le reti neurali artificiali non imparano allo stesso modo degli esseri umani.Oggi, le reti neurali possono imparare compiti multipli quando sono addestrate su di essi congiuntamente, ma non possono mantenere le prestazioni sui compiti appresi quando i compiti sono presentati uno alla volta - questo fenomeno chiamato dimenticanza catastrofica è una sfida fondamentale da superare prima che le reti neurali possano imparare continuamente dai dati in arrivo.In questo lavoro, ci ispiriamo alla memoria umana per sviluppare un'architettura capace di imparare continuamente dai compiti in arrivo in sequenza, evitando la dimenticanza catastrofica. In particolare, il nostro modello consiste in un'architettura a doppia memoria per emulare i sistemi di apprendimento complementari (ippocampo e neocorteccia) nel cervello umano e mantiene una memoria a lungo termine consolidata attraverso il replay generativo delle esperienze passate. (i) sosteniamo la nostra affermazione che il replay dovrebbe essere generativo, (ii) dimostriamo i benefici del replay generativo e della doppia memoria tramite esperimenti, e (iii) dimostriamo una migliore conservazione delle prestazioni anche per piccoli modelli con bassa capacità.
La maggior parte delle ricerche sull'apprendimento permanente si applica alle immagini o ai giochi, ma non al linguaggio.Presentiamo LAMOL, un metodo semplice ma efficace per l'apprendimento permanente del linguaggio (LLL) basato sulla modellazione del linguaggio.LAMOL riproduce pseudo-campioni di compiti precedenti mentre non richiede memoria extra o capacità del modello.In particolare, LAMOL è un modello di linguaggio che impara contemporaneamente a risolvere i compiti e a generare campioni di allenamento. Quando il modello viene addestrato per un nuovo compito, genera pseudocampioni di compiti precedenti per l'addestramento insieme ai dati per il nuovo compito.I risultati mostrano che LAMOL impedisce la dimenticanza catastrofica senza alcun segno di intransigenza e può eseguire cinque compiti linguistici molto diversi in sequenza con un solo modello. Nel complesso, LAMOL supera i metodi precedenti con un margine considerevole ed è solo il 2-3% peggiore del multitasking, che di solito è considerato il limite superiore di LLL.Il codice sorgente è disponibile su https://github.com/jojotenya/LAMOL.
I modelli di elaborazione del linguaggio naturale di apprendimento profondo spesso usano embeddings di parole vettoriali, come word2vec o GloVe, per rappresentare le parole. Una sequenza discreta di parole può essere integrata molto più facilmente con gli strati neurali a valle se è rappresentata come una sequenza di vettori continui; inoltre, le relazioni semantiche tra le parole, apprese da un corpus di testo, possono essere codificate nelle configurazioni relative dei vettori di embedding. Tuttavia, la memorizzazione e l'accesso ai vettori di incorporamento per tutte le parole in un dizionario richiede una grande quantità di spazio e può macchiare i sistemi con una memoria GPU limitata. Qui, abbiamo usato approcci ispirati al quantum computing per proporre due metodi correlati, word2ket e word2ketXS, per memorizzare la matrice di incorporamento delle parole durante l'addestramento e l'inferenza in un modo altamente efficiente.Il nostro approccio raggiunge una riduzione di cento volte o più nello spazio richiesto per memorizzare le incorporazioni con quasi nessun calo relativo di precisione in compiti pratici di elaborazione del linguaggio naturale.
Uno degli aspetti distintivi del linguaggio umano è la sua composizionalità, che ci permette di descrivere ambienti complessi con un vocabolario limitato.In precedenza, è stato dimostrato che gli agenti delle reti neurali possono imparare a comunicare in un linguaggio altamente strutturato, possibilmente compositivo, basato su input disgiunti (ad esempio, caratteristiche ingegnerizzate a mano).Gli esseri umani, tuttavia, non imparano a comunicare sulla base di caratteristiche ben sintetizzate.In questo lavoro, addestriamo gli agenti neurali a sviluppare contemporaneamente la percezione visiva dai pixel grezzi dell'immagine e impariamo a comunicare con una sequenza di simboli discreti. Gli agenti giocano un gioco di descrizione dell'immagine in cui l'immagine contiene fattori come i colori e le forme.Addestriamo gli agenti usando la tecnica obverter in cui un agente introspetta per generare messaggi che massimizzano la propria comprensione.Attraverso l'analisi qualitativa, la visualizzazione e un test zero-shot, dimostriamo che gli agenti possono sviluppare, dai pixel grezzi dell'immagine, un linguaggio con proprietà compositive, data una pressione adeguata dall'ambiente.
La capacità di prevedere un insieme di probabili ma diversi comportamenti futuri di un agente (ad esempio, le traiettorie future di un pedone) è essenziale per i sistemi di percezione critici per la sicurezza (ad es, In particolare, un insieme di possibili comportamenti futuri generati dal sistema deve essere diversificato per tenere conto di tutti i possibili risultati al fine di prendere le necessarie precauzioni di sicurezza. Non è sufficiente mantenere un insieme dei risultati futuri più probabili perché l'insieme può contenere solo perturbazioni di un singolo risultato dominante (modalità principale). Mentre i modelli generativi come i variational autoencoders (VAEs) hanno dimostrato di essere un potente strumento per l'apprendimento di una distribuzione sulle traiettorie future, i campioni estratti a caso dal modello di verosimiglianza implicito appreso possono non essere diversi - il modello di verosimiglianza è derivato dalla distribuzione dei dati di addestramento e i campioni si concentreranno intorno alla modalità principale dei dati. In questo lavoro, proponiamo di imparare una funzione di campionamento della diversità (DSF) che genera un insieme vario ma probabile di traiettorie future. La DSF mappa le caratteristiche del contesto di previsione in un insieme di codici latenti che possono essere decodificati da un modello generativo (ad es, Per imparare i parametri del DSF, la diversità dei campioni di traiettoria è valutata da una perdita di diversità basata su un processo determinantico (DPP). Il nostro metodo è una nuova applicazione di DPP per ottimizzare un insieme di elementi (traiettorie previste) in uno spazio continuo. Dimostriamo la diversità delle traiettorie prodotte dal nostro approccio sia su dati di traiettoria bidimensionali 2D che su dati di movimento umano ad alta densità.
C'è una crescente evidenza che il preaddestramento può essere prezioso per i modelli di comprensione linguistica delle reti neurali, ma non abbiamo ancora una chiara comprensione di come la scelta dell'obiettivo di preaddestramento influenzi il tipo di informazioni linguistiche che i modelli imparano.Con questo in mente, confrontiamo quattro obiettivi - modellazione linguistica, traduzione, skip-thought e autocodifica - sulla loro capacità di indurre informazioni sintattiche e part-of-speech, mantenendo costante il genere e la quantità di dati di addestramento. Troviamo che le rappresentazioni dei modelli linguistici ottengono costantemente le migliori prestazioni sui nostri compiti di predizione sintattica ausiliaria, anche quando vengono addestrati su quantità relativamente piccole di dati, il che suggerisce che la modellazione linguistica può essere il miglior compito di preformazione ricco di dati per le applicazioni di apprendimento del trasferimento che richiedono informazioni sintattiche.Troviamo anche che un modello congelato e inizializzato in modo casuale può ottenere prestazioni sorprendenti sui nostri compiti ausiliari, ma che questo effetto scompare quando la quantità di dati di allenamento per i compiti ausiliari viene ridotta.
Consideriamo il problema della generazione di configurazioni che soddisfano i vincoli fisici per la progettazione ottimale di nano-modelli di materiali, dove più proprietà (e spesso in conflitto) devono essere soddisfatte simultaneamente.  Si consideri, per esempio, il compromesso tra resistenza termica, conducibilità elettrica e stabilità meccanica necessario per progettare un modello nano-poroso con efficienza termoelettrica ottimale.  A tal fine, sfruttiamo la struttura di regolarizzazione posteriore e mostriamo che questo problema di soddisfazione dei vincoli può essere formulato come campionamento di una distribuzione di Gibbs.  Le sfide principali provengono dalla natura black-box di questi vincoli fisici, poiché sono ottenuti tramite la risoluzione di PDEs altamente non lineari.Per superare queste difficoltà, introduciamo la dinamica di Langevin vincolata basata su surrogati per il campionamento black-box.Esploriamo due approcci surrogati.Il primo approccio sfrutta l'approssimazione di ordine zero dei gradienti nel campionamento di Langevin e ci riferiamo ad esso come Zero-Order Langevin.In pratica, questo approccio può essere proibitivo poiché abbiamo ancora bisogno di interrogare spesso i solutori PDE costosi. Il secondo approccio approssima i gradienti nella dinamica di Langevin con reti neurali profonde, permettendoci un'efficiente strategia di campionamento usando il modello surrogato. Dimostriamo la convergenza di questi due approcci quando la distribuzione target è log-concava e liscia. Mostriamo l'efficacia di entrambi gli approcci nella progettazione di configurazioni ottimali di materiali nano-porosi, dove l'obiettivo è produrre modelli nano-pattern con bassa conducibilità termica e ragionevole stabilità meccanica.
C'è un crescente interesse per le incorporazioni ispirate alla geometria per l'apprendimento di gerarchie, ordini parziali e strutture reticolari, con applicazioni naturali ai dati relazionali transitivi come i grafi di entailment. Un lavoro recente ha esteso queste idee oltre le gerarchie deterministiche a modelli calibrati probabilisticamente, che consentono l'apprendimento da una supervisione incerta e l'inferenza di soft-inclusioni tra i concetti, pur mantenendo il bias induttivo geometrico dei modelli di incorporamento gerarchici. Ci basiamo sul modello Box Lattice di Vilnis et al. (2018), che ha mostrato risultati promettenti nella modellazione di soft-inclusioni attraverso una gerarchia sovrapposta di insiemi, parametrizzati come iperrettangoli ad alta dimensione (scatole).Tuttavia, i bordi duri delle scatole presentano difficoltà per l'ottimizzazione standard basata sul gradiente; quel lavoro ha impiegato una speciale funzione surrogata per il caso disgiunto, ma troviamo che questo metodo sia fragile.  In questo lavoro, presentiamo un nuovo modello di embedding gerarchico, ispirato da un rilassamento di embeddings di scatole in funzioni di densità parametrizzate usando convoluzioni gaussiane sulle scatole. Il nostro approccio fornisce un surrogato alternativo alla misura originale del reticolo che migliora la robustezza dell'ottimizzazione nel caso disgiunto, mentre conserva anche le proprietà desiderabili rispetto al reticolo originale. Dimostriamo l'aumento o la corrispondenza delle prestazioni su WordNet hypernymy prediction, Flickr caption entailment, e un dataset di paniere di mercato basato su MovieLens. Mostriamo miglioramenti particolarmente marcati nel caso di dati sparsi, dove molte probabilità condizionali dovrebbero essere basse, e quindi le caselle dovrebbero essere quasi disgiunte.
Presentiamo un approccio di aumento dei dati debolmente supervisionato per migliorare il Named Entity Recognition (NER) in un dominio impegnativo: l'estrazione di entità biomediche (ad esempio, le proteine) dalla letteratura scientifica.In primo luogo, addestriamo un modello NER neurale (NNER) su un piccolo seme di esempi completamente etichettati.In secondo luogo, usiamo un set di riferimento di nomi di entità (ad es, Infine, addestriamo nuovamente il nostro modello NNER in modo iterativo sul set di addestramento aumentato, includendo il seme, gli esempi del set di riferimento e gli esempi debolmente etichettati, il che si traduce in etichette raffinate.Mostriamo empiricamente che questo processo di bootstrapping aumentato migliora significativamente le prestazioni NER, e discutiamo i fattori che influenzano l'efficacia dell'approccio.
Mentre la disponibilità di dati per l'addestramento di modelli di apprendimento automatico è in costante aumento, spesso è molto più facile raccogliere vettori di caratteristiche che ottenere le etichette corrispondenti. Uno degli approcci per affrontare questo problema è quello di utilizzare l'apprendimento semi-supervisionato, che sfrutta non solo i campioni etichettati, ma anche i vettori di caratteristiche non etichettati. L'algoritmo utilizza i recenti progressi nella simulazione hamiltoniana basata su campioni quantistici per estendere l'algoritmo esistente Quantum LS-SVM per gestire il termine semi-supervisionato nella perdita, mantenendo la stessa accelerazione quantistica del Quantum LS-SVM.
Le reti neurali profonde sono diventate lo stato dell'arte dei modelli in numerosi compiti di apprendimento automatico; tuttavia, manca ancora una guida generale alla progettazione dell'architettura di rete. Nel nostro lavoro, colleghiamo la progettazione di reti neurali profonde con le equazioni differenziali numeriche e dimostriamo che molte reti efficaci, come ResNet, PolyNet, FractalNet e RevNet, possono essere interpretate come diverse discretizzazioni numeriche di equazioni differenziali. Possiamo approfittare della ricca conoscenza dell'analisi numerica per guidarci nella progettazione di reti profonde nuove e potenzialmente più efficaci.Come esempio, proponiamo un'architettura lineare multi-step (LM-architecture) che si ispira al metodo lineare multi-step che risolve le equazioni differenziali ordinarie.La LM-architecture è una struttura efficace che può essere utilizzata su qualsiasi rete ResNet-like.In particolare, dimostriamo che LM-ResNet e LM-ResNeXt (cioè le reti ottenute applicando il metodo LM-ResNeXt). In particolare, sia su CIFAR che su ImageNet, LM-ResNet/LM-ResNeXt possono comprimere significativamente (>50%) le reti originali mantenendo una performance simile, il che può essere spiegato matematicamente usando il concetto di equazione modificata dall'analisi numerica. Ultimo ma non meno importante, stabiliamo anche una connessione tra il controllo stocastico e l'iniezione di rumore nel processo di formazione che aiuta a migliorare la generalizzazione delle reti.Inoltre, mettendo in relazione la strategia di formazione stocastica con il sistema dinamico stocastico, possiamo facilmente applicare la formazione stocastica alle reti con l'architettura LM.Come esempio, abbiamo introdotto la profondità stocastica a LM-ResNet e ottenere un miglioramento significativo rispetto alla LM-ResNet originale su CIFAR10.
Trasformare una schermata dell'interfaccia grafica creata da un designer in codice informatico è un compito tipico condotto da uno sviluppatore al fine di costruire software personalizzato, siti web e applicazioni mobili.In questo documento, dimostriamo che i metodi di deep learning possono essere sfruttati per addestrare un modello end-to-end per generare automaticamente il codice da una singola immagine di input con oltre il 77% di precisione per tre diverse piattaforme (cioè iOS, Android e tecnologie web-based).
Compiti di computer vision come la classificazione delle immagini, il recupero delle immagini e l'apprendimento di pochi scatti sono attualmente dominati da incorporazioni euclidee e sferiche, in modo che le decisioni finali sull'appartenenza alla classe o sul grado di somiglianza siano prese utilizzando iperpiani lineari, distanze euclidee, o distanze geodetiche sferiche (somiglianza coseno).In questo lavoro, dimostriamo che in molti scenari pratici le incorporazioni iperboliche forniscono una migliore alternativa.
Poiché la cognizione umana non è ottimizzata per lavorare bene in spazi ad alta dimensione, queste aree potrebbero beneficiare di rappresentazioni bidimensionali interpretabili. Tuttavia, la maggior parte degli algoritmi di apprendimento delle rappresentazioni per i dati delle serie temporali sono difficili da interpretare, a causa di mappature non intuitive dalle caratteristiche dei dati alle proprietà salienti della rappresentazione e alla non linearità nel tempo. Per affrontare questo problema, proponiamo una nuova struttura di apprendimento della rappresentazione basata sulle idee della riduzione della dimensionalità discreta interpretabile e della modellazione generativa profonda. Questa struttura ci permette di imparare rappresentazioni discrete delle serie temporali, che danno origine a incorporazioni lisce e interpretabili con prestazioni di clustering superiori. Introduciamo un nuovo modo per superare la non-differenziabilità nell'apprendimento delle rappresentazioni discrete e presentiamo una versione basata sul gradiente del tradizionale algoritmo della mappa auto-organizzante che è più performante dell'originale. Inoltre, per consentire un'interpretazione probabilistica del nostro metodo, integriamo un modello di Markov nello spazio di rappresentazione, che scopre la struttura di transizione temporale, migliora ulteriormente le prestazioni di clustering e fornisce ulteriori approfondimenti esplicativi, nonché una rappresentazione naturale dell'incertezza. Valutiamo il nostro modello in termini di prestazioni di clustering e interpretabilità su dati statici (Fashion-)MNIST, una serie temporale di immagini (Fashion-)MNIST interpolate linearmente, un sistema caotico attrattore di Lorenz con due macrostati, così come su una difficile applicazione di serie temporali mediche del mondo reale sul set di dati eICU.
Proponiamo Significance-Offset Convolutional Neural Network, un'architettura di rete convoluzionale profonda per la regressione di serie temporali asincrone multivariate.  Il modello è ispirato ai modelli autoregressivi (AR) standard e ai meccanismi di gating utilizzati nelle reti neurali ricorrenti.  L'architettura è stata progettata per applicazioni su serie temporali asincrone ed è stata valutata su tali dataset: un dataset proprietario di hedge fund di oltre 2 milioni di quotazioni per un indice di derivati di credito, una serie autoregressiva rumorosa generata artificialmente e un dataset di consumi elettrici domestici.   L'architettura proposta raggiunge risultati promettenti rispetto alle reti neurali convoluzionali e ricorrenti. Il codice per gli esperimenti numerici e l'implementazione dell'architettura saranno condivisi online per rendere la ricerca riproducibile.
MixUp è uno schema di aumento dei dati in cui le coppie di campioni di formazione e le loro etichette corrispondenti sono mescolate usando i coefficienti lineari.Senza la miscelazione dell'etichetta, MixUp diventa uno schema più convenzionale: i campioni di input sono spostati ma le loro etichette originali sono mantenute.Poiché i campioni sono spostati preferenzialmente nella direzione di altre classi - che sono tipicamente raggruppate nello spazio di input - ci riferiamo a questo metodo come addestramento adversarial direzionale, o DAT.We mostrano che sotto due condizioni miti, MixUp converge asintoticamente ad un sottoinsieme di DAT. Definiamo MixUp slegato (UMixUp), un sottoinsieme di MixUp in cui le etichette di addestramento sono mescolate con coefficienti lineari diversi da quelli dei loro campioni corrispondenti e dimostriamo che sotto le stesse condizioni blande, MixUp slegato converge all'intera classe di schemi DAT. Motivati dalla comprensione che UMixUp è sia una generalizzazione di MixUp che una forma di formazione avversaria, sperimentiamo con diversi set di dati e funzioni di perdita per dimostrare che UMixUp fornisce prestazioni migliori rispetto a MixUp.In breve, presentiamo una nuova interpretazione di MixUp come appartenente a una classe altamente analoga alla formazione avversaria, e su questa base introduciamo una semplice generalizzazione che supera MixUp.
Il riconoscimento dei piani mira a cercare i piani di destinazione che spieghino al meglio le azioni osservate sulla base di librerie di piani e/o modelli di dominio. I recenti progressi nel riconoscimento delle attività visive hanno il potenziale per abilitare applicazioni come la videosorveglianza automatizzata.Approcci efficaci per tali problemi richiederebbero la capacità di riconoscere i piani degli agenti dalle informazioni video.Gli algoritmi di riconoscimento dei piani tradizionali si basano sull'accesso a modelli di dominio di pianificazione dettagliati.Una recente direzione promettente comporta l'apprendimento di modelli di dominio approssimati (o poco profondi) direttamente dalle sequenze di attività osservate. Tuttavia, i risultati dell'inferenza visiva sono spesso rumorosi e incerti, tipicamente rappresentati come una distribuzione di azioni possibili. In questo lavoro, sviluppiamo un quadro di riconoscimento visivo del piano che riconosce i piani con un modello di dominio approssimativo appreso da dati visivi incerti.
Consideriamo il compito di rispondere a domande complesse a più livelli usando un corpus come una base di conoscenza virtuale (KB).In particolare, descriviamo un modulo neurale, DrKIT, che attraversa i dati testuali come una KB virtuale, seguendo dolcemente percorsi di relazioni tra menzioni di entità nel corpus.Ad ogni passo l'operazione utilizza una combinazione di indici TFIDF a matrice sparsa e ricerca del prodotto interno massimo (MIPS) su un indice speciale di rappresentazioni contestuali. Questo modulo è differenziabile, quindi l'intero sistema può essere addestrato completamente end-to-end usando metodi basati sul gradiente, partendo da input in linguaggio naturale.Descriviamo anche uno schema di preaddestramento per il codificatore di citazione dell'indice generando esempi negativi difficili usando basi di conoscenza esistenti.Mostriamo che DrKIT migliora l'accuratezza di 9 punti sulle domande a 3 punti nel set di dati MetaQA, riducendo il divario tra lo stato dell'arte basato sul testo e quello basato sulla KB del 70%.DrKIT è anche molto efficiente, elaborando fino a 10 volte più query al secondo rispetto agli attuali sistemi QA all'avanguardia.
Nonostante il loro grande successo, gli algoritmi di fattorizzazione tradizionali in genere non supportano le funzioni (ad es, D'altra parte, i metodi neurali permettono grandi insiemi di caratteristiche, ma sono spesso progettati per un'applicazione specifica. Noi proponiamo nuovi metodi di fattorizzazione profonda che permettono una rappresentazione efficiente e flessibile delle caratteristiche. Per esempio, permettiamo di descrivere gli elementi con un linguaggio naturale con una complessità lineare alla dimensione del vocabolario - questo permette la predizione per gli elementi non visti ed evita il problema dell'avvio a freddo.Mostriamo che la nostra architettura può generalizzare alcune architetture neurali monofunzionali precedentemente pubblicate.I nostri esperimenti suggeriscono tempi di addestramento e precisione migliori rispetto ai metodi superficiali.
La Realtà Aumentata (AR) può assistere in compiti fisici come l'assemblaggio di oggetti attraverso l'uso di "istruzioni situate".Queste istruzioni possono essere sotto forma di video, immagini, testo o animazioni guida, dove il mezzo più utile tra questi è altamente dipendente sia dall'utente che dalla natura del compito.Il nostro lavoro supporta l'authoring di tutorial AR per compiti di assemblaggio con poco overhead oltre alla semplice esecuzione del compito stesso. Il sistema presentato, AuthAR, riduce il tempo e lo sforzo richiesto per costruire tutorial interattivi AR generando automaticamente i componenti chiave del tutorial AR mentre l'autore sta assemblando i pezzi fisici. Inoltre, il sistema guida gli autori attraverso il processo di aggiunta di video, immagini, testo e animazioni al tutorial. Questo approccio simultaneo di assemblaggio e generazione di tutorial permette la creazione di tutorial portatili che si adattano alle preferenze di diversi utenti finali.
Il monitoraggio dei pazienti in terapia intensiva è un compito impegnativo e ad alto costo.Quindi, prevedere le condizioni dei pazienti durante il loro soggiorno in terapia intensiva può aiutare a fornire una migliore assistenza acuta e pianificare le risorse dell'ospedale.C'è stato un continuo progresso nella ricerca sull'apprendimento automatico per la gestione della terapia intensiva, e la maggior parte di questo lavoro si è concentrato sull'utilizzo di segnali di serie temporali registrati da strumenti di terapia intensiva. Nel nostro lavoro, mostriamo che l'aggiunta di note cliniche come un'altra modalità migliora le prestazioni del modello per tre compiti di riferimento: previsione della mortalità in ospedale, modellazione dello scompenso e previsione della durata del soggiorno che giocano un ruolo importante nella gestione dell'ICU.Mentre i dati di serie temporale sono misurati a intervalli regolari, le note mediche sono registrate in tempi irregolari, rendendo impegnativo modellarle insieme.Proponiamo un metodo per modellarle insieme, ottenendo un notevole miglioramento nei compiti di riferimento rispetto al modello di serie temporale di base.
I lavori esistenti nel profondo Multi-Agent Reinforcement Learning (MARL) si concentrano principalmente sul coordinamento di agenti cooperativi per completare alcuni compiti congiuntamente.Tuttavia, in molti casi del mondo reale, gli agenti sono auto-interessati come i dipendenti in una società e i club in un campionato.Pertanto, il leader, cioè Le difficoltà principali della coordinazione costosa sono che) il leader deve considerare l'effetto a lungo termine e prevedere i comportamenti dei seguaci quando assegna i bonus eii) le complesse interazioni tra i seguaci rendono il processo di formazione difficile da convergere, specialmente quando la politica del leader cambia nel tempo.In questo lavoro, affrontiamo questo problema attraverso un approccio RL profondo basato sugli eventi. (1) Modelliamo il processo decisionale del leader come un processo decisionale semi-Markov e proponiamo un nuovo gradiente di politica basato su eventi multi-agente per imparare la politica a lungo termine del leader.(2) Sfruttiamo lo schema di coerenza leader-follower per progettare un modulo follower-aware e un modulo di attenzione specifico del follower per prevedere i comportamenti dei follower e dare una risposta accurata ai loro comportamenti. (3) Proponiamo un algoritmo di gradiente basato sull'astrazione dell'azione per ridurre lo spazio decisionale dei seguaci e quindi accelerare il processo di formazione dei seguaci. Gli esperimenti nelle raccolte di risorse, nella navigazione e nel gioco predatore-preda rivelano che il nostro approccio supera nettamente i metodi all'avanguardia.
Un lavoro recente ha studiato l'emergere del linguaggio tra gli agenti di apprendimento di rinforzo profondo che devono collaborare per risolvere un compito.Di particolare interesse sono i fattori che fanno sì che il linguaggio sia compositivo---cioè, I linguisti evolutivi hanno scoperto che, oltre ai priori strutturali come quelli già studiati nell'apprendimento profondo, le dinamiche di trasmissione del linguaggio da una generazione all'altra contribuiscono significativamente all'emergere della composizionalità. In questo articolo, introduciamo queste dinamiche evolutive culturali nell'emergere del linguaggio sostituendo periodicamente gli agenti in una popolazione per creare un gap di conoscenza, inducendo implicitamente la trasmissione culturale del linguaggio.
Sulla base della nostra osservazione che esiste un calo drammatico per i valori singolari degli strati completamente connessi o di una singola mappa di caratteristiche dello strato convoluzionale, e che la dimensione del vettore di caratteristiche concatenato è quasi uguale alla somma della dimensione su ogni mappa di caratteristiche, proponiamo un approccio basato sulla decomposizione dei valori singolari (SVD) per stimare la dimensione dei manifold profondi per una tipica rete neurale convoluzionale VGG19. Scegliamo tre categorie da ImageNet, vale a dire gatto persiano, nave container e vulcano, e determiniamo la dimensione locale dei collettori profondi degli strati profondi attraverso lo spazio tangente di un'immagine target. Attraverso diversi metodi di aumento, abbiamo scoperto che il metodo del rumore gaussiano è più vicino alla dimensione intrinseca, poiché aggiungendo rumore casuale a un'immagine ci stiamo muovendo in una dimensione arbitraria, e quando il rango della matrice delle caratteristiche delle immagini aumentate non aumenta siamo molto vicini alla dimensione locale del collettore. I nostri risultati mostrano che le dimensioni delle diverse categorie sono vicine l'una all'altra e diminuiscono rapidamente lungo gli strati convoluzionali e gli strati completamente connessi.Inoltre, mostriamo che le dimensioni diminuiscono rapidamente all'interno dello strato Conv5.Il nostro lavoro fornisce nuove intuizioni per la struttura intrinseca delle reti neurali profonde e aiuta a svelare l'organizzazione interna della scatola nera delle reti neurali profonde.
I grandi trasformatori pre-addestrati come il BERT sono stati tremendamente efficaci per molti compiti NLP. Tuttavia, l'inferenza in questi modelli di grande capacità è proibitivamente lenta e costosa. I trasformatori sono essenzialmente una pila di strati di auto-attenzione che codificano ogni posizione di input utilizzando l'intera sequenza di input come contesto. Tuttavia, troviamo che potrebbe non essere necessario applicare questa costosa auto-attenzione a livello di sequenza su tutti gli strati. Sulla base di questa osservazione, proponiamo una decomposizione in un trasformatore pre-addestrato che permette agli strati inferiori di elaborare segmenti dell'input in modo indipendente consentendo il parallelismo e il caching. Mostriamo che la perdita di informazioni dovuta a questa decomposizione può essere recuperata negli strati superiori con una supervisione ausiliaria durante il fine-tuning.  Valutiamo la de-composizione con modelli BERT pre-addestrati su cinque diversi compiti a coppie di input nella risposta alle domande, nella somiglianza delle frasi e nell'inferenza del linguaggio naturale.  I risultati mostrano che la decomposizione permette un'inferenza più veloce (fino a 4x), una significativa riduzione della memoria (fino al 70%) pur mantenendo la maggior parte (fino al 99%) delle prestazioni originali. Rilasceremo il codice a< url anonimizzato>.
L'esplorazione durante l'apprendimento delle rappresentazioni è una delle principali sfide che il DeepReinforcement Learning (DRL) deve affrontare oggi. Poiché la rappresentazione appresa dipende dai dati osservati, la strategia di esplorazione ha un ruolo cruciale. Il popolare algoritmo DQN ha migliorato significativamente le capacità degli algoritmi di ReinforcementLearning (RL) di apprendere rappresentazioni di stato dai dati grezzi, tuttavia, utilizza una strategia di esplorazione ingenua che è statisticamente inefficiente.L'algoritmo RandomizedLeast Squares Value Iteration (RLSVI) (Osband et al, 2016), d'altra parte, esplora e generalizza in modo efficiente attraverso funzioni di valore parametrizzate linearmente.Tuttavia, si basa su una rappresentazione di stato progettata a mano che richiede un lavoro di ingegneria preliminare per ogni ambiente.In questo articolo, proponiamo un adattamento DeepLearning per RLSVI.Piuttosto che usare una rappresentazione di stato progettata a mano, usiamo una rappresentazione di stato che viene appresa direttamente dai dati da un agenteDQN. Poiché la rappresentazione viene ottimizzata durante il processo di apprendimento, una componente chiave per il metodo proposto è un meccanismo di likelihood matching, che si adatta alle rappresentazioni mutevoli. Dimostriamo l'importanza delle varie proprietà del nostro algoritmo su un problema giocattolo e dimostriamo che il nostro metodo supera DQN in cinque benchmark Atari, raggiungendo risultati competitivi con l'algoritmo Rainbow.
La complessità delle reti neurali su larga scala può portare a una scarsa comprensione dei loro dettagli interni. Mostriamo che questa opacità fornisce un'opportunità agli avversari di incorporare funzionalità non volute nella rete sotto forma di attacchi Trojan horse. Il nostro attacco è flessibile, facile da eseguire e difficile da rilevare. dimostriamo teoricamente che il rilevamento della rete dannosa è computazionalmente impossibile e dimostriamo empiricamente che la rete di trasporto non compromette il suo travestimento. il nostro attacco espone un'importante lacuna precedentemente sconosciuta che svela una nuova direzione nella sicurezza dell'apprendimento automatico.
In questo articolo, introduciamo Random Path Generative Adversarial Network (RPGAN) --- uno schema alternativo di GANs che può servire come strumento per l'analisi dei modelli generativi.Mentre lo spazio latente di una tipica GAN consiste di vettori di input, campionati casualmente dalla distribuzione gaussiana standard, lo spazio latente di RPGAN consiste di percorsi casuali in una rete generatrice. Come dimostriamo, questo design permette di associare diversi strati del generatore con diverse regioni dello spazio latente, fornendo la loro naturale interpretabilità.Con esperimenti su benchmark standard, dimostriamo che RPGAN rivela diverse intuizioni interessanti sui ruoli che i diversi strati svolgono nel processo di generazione dell'immagine.A parte l'interpretabilità, il modello RPGAN fornisce anche una qualità di generazione competitiva e permette un efficiente apprendimento incrementale su nuovi dati.
Le reti neurali artificiali profonde possono raggiungere una differenza estremamente piccola tra le accuratezze di addestramento e di prova su insiemi di addestramento e di prova distribuiti in modo identico, che è una misura standard di generalizzazione.Tuttavia, gli insiemi di addestramento e di prova possono non essere sufficientemente rappresentativi del set di campioni empirico, che consiste in campioni di input del mondo reale. Quando i campioni sono tratti da un sottoinsieme sottorappresentato o non rappresentato durante l'inferenza, il divario tra l'addestramento e le accuratezze di inferenza può essere significativo.Per affrontare questo problema, riformuliamo prima un algoritmo di classificazione come una procedura per la ricerca di un codice sorgente che mappa le caratteristiche di input alle classi.Poi deriviamo una condizione necessaria e sufficiente per la generalizzazione utilizzando una metrica di somiglianza cognitiva universale, cioè la distanza di informazione, basata sulla complessità Kolmogorov. Usando questa condizione, formuliamo un problema di ottimizzazione per imparare una funzione di classificazione più generale.Per raggiungere questo scopo, estendiamo le caratteristiche di input concatenando le codifiche di loro, e poi addestriamo il classificatore sulle caratteristiche estese.Come illustrazione di questa idea, ci concentriamo sulla classificazione delle immagini, dove usiamo i codici di canale sulle caratteristiche di input come un modo sistematico per migliorare il grado in cui i set di allenamento e test sono rappresentativi del set di campioni empirico. Per mostrare i nostri risultati teorici, considerando che le caratteristiche di input corrotte o perturbate appartengono al set di campioni empirico, ma tipicamente non ai set di allenamento e test, dimostriamo attraverso ampi esperimenti sistematici che, come risultato dell'apprendimento di una funzione di classificazione più generale, un modello addestrato sulle caratteristiche di input codificate è significativamente più robusto alle comuni corruzioni, ad es, rumore gaussiano e di colpo, così come alle perturbazioni avversarie, ad esempio quelle trovate tramite la discesa del gradiente proiettato, rispetto al modello addestrato sulle caratteristiche di input non codificate.
Molti approcci alla scoperta causale sono limitati dalla loro incapacità di discriminare tra i grafi equivalenti di Markov dati solo i dati osservazionali. formuliamo la scoperta causale come un problema di selezione del modello bayesiano basato sulla verosimiglianza marginale. adottiamo una parametrizzazione basata sulla nozione di indipendenza dei meccanismi causali che rende i grafi equivalenti di Markov distinguibili. completiamo questo con un approccio empirico bayesiano per impostare i priori in modo che il grafico causale effettivo sottostante sia assegnato una verosimiglianza marginale superiore rispetto alle sue alternative. L'adozione di un approccio bayesiano permette anche di modellare direttamente le variabili di confondimento non osservate, per le quali forniamo un algoritmo variazionale per approssimare la verosimiglianza marginale, poiché questa caratteristica desiderabile rende il calcolo della verosimiglianza marginale intrattabile. Crediamo che l'approccio bayesiano alla scoperta causale permetta di utilizzare la ricca metodologia dell'inferenza bayesiana in vari aspetti difficili di questo problema e fornisca un quadro unificante alla ricerca sulla scoperta causale. Dimostriamo risultati promettenti in esperimenti condotti su dati reali, supportando il nostro approccio di modellazione e la nostra metodologia di inferenza.
  L'obiettivo del rilevamento compresso è quello di imparare un segnale strutturato $x$ da un numero limitato di misurazioni lineari rumorose $y\approx Ax$.  Nel rilevamento compresso tradizionale, la ``struttura'' è rappresentata dalla sparsità in qualche base nota.  Ispirato dal successo del deep learning nella modellazione delle immagini, un lavoro recente, a partire da~\cite{BDJP17}, ha invece considerato la struttura come proveniente da un modello generativo $G: \R^k \R^n$.  Presentiamo due risultati che stabiliscono la difficoltà di quest'ultimo compito, mostrando che i limiti esistenti sono stretti.  In primo luogo, forniamo un limite inferiore che coincide con l'upper bound di ~\cite{BDJP17} per il compressed sensing da modelli generativi $L$-Lipschitz $G$.  In particolare, esiste una tale funzione che richiede approssimativamente $\Omega(k \log L)$ misure lineari perché il recupero sparso sia possibile.  Questo vale anche per l'obiettivo più rilassato del recupero non uniforme.  In secondo luogo, mostriamo che i modelli generativi generalizzano la sparsità come rappresentazione della struttura.  In particolare, costruiamo una rete neurale basata su ReLU $G: \R^{2k} \a \R^n$ con $O(1)$ strati e $O(kn)$ attivazioni per strato, tale che l'intervallo di $G$ contiene tutti i vettori $k$-sparsi.
Noi ipotizziamo che i sistemi neurali end-to-end di image captioning funzionino apparentemente bene perché sfruttano e apprendono la 'somiglianza distributiva' in uno spazio di caratteristiche multimodali, mappando un'immagine di test con immagini di allenamento simili in questo spazio e generando una didascalia dallo stesso spazio. Per convalidare la nostra ipotesi, ci concentriamo sul lato 'immagine' della didascalia dell'immagine, variando la rappresentazione dell'immagine in ingresso ma mantenendo costante il modello di generazione del testo RNN di una CNN-RNN, proponendo un vettore sparse bag-of-objects come rappresentazione interpretabile per studiare la nostra ipotesi di somiglianza distributiva. Abbiamo scoperto che i modelli di image captioning (i) sono in grado di separare la struttura da rappresentazioni di input rumorose; (ii) non subiscono praticamente alcuna perdita significativa di prestazioni quando una rappresentazione ad alta dimensione viene compressa in uno spazio dimensionale inferiore; (iii) raggruppano le immagini con informazioni visive e linguistiche simili; (iv) dipendono fortemente da set di test con una distribuzione simile a quella del set di allenamento; (v) generano ripetutamente le stesse didascalie abbinando le immagini e "recuperando" una didascalia nello spazio visivo-testuale congiunto. Concludiamo che, indipendentemente dalla rappresentazione dell'immagine, i sistemi di image captioning sembrano abbinare le immagini e generare didascalie in un sottospazio semantico congiunto immagine-testo appreso.
Presentiamo un approccio di parsing sequence-to-action per il linguaggio naturale al compito SQL che riempie incrementalmente le fessure di una query SQL con azioni fattibili da un inventario predefinito.Per rendere conto del fatto che tipicamente ci sono più query SQL corrette con la stessa semantica o molto simili, traiamo ispirazione dalle tecniche di parsing sintattico e proponiamo di addestrare i nostri modelli sequence-to-action con oracoli non deterministici. Valutiamo i nostri modelli sul set di dati WikiSQL e raggiungiamo un'accuratezza di esecuzione dell'83,7% sul set di test, un miglioramento assoluto del 2,1% rispetto ai modelli addestrati con i tradizionali oracoli statici che assumono una singola query SQL corretta.Quando viene ulteriormente combinato con la strategia di decodifica guidata dall'esecuzione, il nostro modello stabilisce un nuovo stato dell'arte con un'accuratezza di esecuzione dell'87,1%.
Proponiamo la ricerca efficiente dell'architettura neurale (ENAS), un approccio più veloce e meno costoso alla progettazione automatica del modello rispetto ai metodi precedenti. In ENAS, un controllore impara a scoprire le architetture della rete neurale cercando un percorso ottimale all'interno di un modello più grande. Nel frattempo il modello corrispondente al percorso selezionato viene addestrato per minimizzare la perdita di entropia incrociata.Sul dataset Penn Treebank, ENAS può scoprire una nuova architettura che raggiunge una perplessità di test di 57,8, che è all'avanguardia tra i metodi di progettazione automatica del modello su Penn Treebank.Sul dataset CIFAR-10, ENAS può progettare nuove architetture che raggiungono un errore di test del 2,89%, vicino al 2,65% raggiunto da NAS standard (Zoph et al, Soprattutto, i nostri esperimenti dimostrano che ENAS è più di 10 volte più veloce e 100 volte meno esigente in termini di risorse di NAS.
Al giorno d'oggi, le reti neurali profonde (DNN) sono diventate lo strumento principale per i compiti di apprendimento automatico all'interno di una vasta gamma di domini, tra cui visione, NLP e discorso.Nel frattempo, in un caso importante di dati tabulari eterogenei, il vantaggio delle DNN rispetto alle controparti superficiali rimane discutibile. In particolare, non ci sono prove sufficienti che il macchinario di apprendimento profondo permetta di costruire metodi che superino i gradient boosting decision trees (GBDT), che sono spesso la scelta migliore per i problemi tabulari.In questo articolo, introduciamo Neural Oblivious Decision Ensembles (NODE), una nuova architettura di apprendimento profondo, progettata per lavorare con qualsiasi dato tabulare. In poche parole, l'architettura NODE proposta generalizza gli insiemi di alberi decisionali obliqui, ma beneficia sia dell'ottimizzazione basata sul gradiente end-to-end che della potenza dell'apprendimento della rappresentazione gerarchica multistrato.Con un ampio confronto sperimentale con i principali pacchetti GBDT su un gran numero di set di dati tabulari, dimostriamo il vantaggio dell'architettura NODE proposta, che supera i concorrenti sulla maggior parte dei compiti.Apriamo l'implementazione PyTorch di NODE e crediamo che diventerà un framework universale per l'apprendimento automatico su dati tabulari.
La re-identificazione delle persone (re-ID) mira a identificare le immagini delle stesse persone attraverso diverse telecamere.Tuttavia, le diversità di dominio tra i diversi set di dati pongono una sfida evidente per adattare il modello re-ID addestrato su un set di dati a un altro.I metodi di adattamento di dominio non supervisionato allo stato dell'arte per la re-ID delle persone hanno trasferito la conoscenza appresa dal dominio di origine ottimizzando con pseudo etichette create da algoritmi di clustering sul dominio di destinazione.Anche se hanno raggiunto prestazioni all'avanguardia, l'inevitabile rumore delle etichette causato dalla procedura di clustering è stato ignorato. Al fine di mitigare gli effetti delle pseudo etichette rumorose, proponiamo di raffinare dolcemente le pseudo etichette nel dominio di destinazione proponendo una struttura non supervisionata, Mutual Mean-Teaching (MMT), per imparare caratteristiche migliori dal dominio di destinazione attraverso pseudo etichette dure raffinate off-line e pseudo etichette morbide raffinate on-line in un modo alternativo di formazione.  Inoltre, la pratica comune è quella di adottare sia la perdita di classificazione che la perdita di tripletta congiuntamente per ottenere prestazioni ottimali nei modelli di identificazione delle persone, ma la perdita di tripletta convenzionale non può funzionare con etichette raffinate in modo morbido. Per risolvere questo problema, viene proposto un nuovo softmax-triplet loss per supportare l'apprendimento con etichette pseudo-triplet morbide per raggiungere le prestazioni ottimali di adattamento al dominio.Il quadro MMT proposto raggiunge notevoli miglioramenti del 14,4%, 18,2%, 13,1% e 16,4% mAP su Market-to-Duke, Duke-to-Market, Market-to-MSMT e Duke-to-MSMT compiti di adattamento al dominio non supervisionato.
Presentiamo il primo verificatore end-to-end di classificatori audio. Rispetto ai metodi esistenti, il nostro approccio permette di analizzare sia l'intera fase di elaborazione audio che le architetture di reti neurali ricorrenti (ad esempio, LSTM), Mostriamo che il verificatore è in grado di scalare fino a reti di grandi dimensioni, mentre calcola limiti significativamente più stretti rispetto ai metodi esistenti per i comuni benchmark di classificazione audio: sull'impegnativo set di dati Google Speech Commands certifichiamo il 95% in più di ingressi rispetto all'approssimazione dell'intervallo (unico metodo scalabile precedente), per una perturbazione di -90dB.
Dal momento che le reti neurali profonde sono iper-parametrizzate, possono memorizzare esempi rumorosi. Affrontiamo tale problema di memorizzazione in presenza di rumore di annotazione. Dal fatto che le reti neurali profonde non possono generalizzare i quartieri delle caratteristiche acquisite tramite memorizzazione, ipotizziamo che gli esempi rumorosi non incorrono costantemente in piccole perdite sulla rete sotto una certa perturbazione. Su questa base, proponiamo un nuovo metodo di formazione chiamato Learning with Ensemble Consensus (LEC) che previene l'overfitting degli esempi rumorosi eliminandoli utilizzando il consenso di un ensemble di reti perturbate. Uno dei LEC proposti, LTEC, supera gli attuali metodi all'avanguardia su MNIST rumoroso, CIFAR-10, e CIFAR-100 in modo efficiente.
Una formulazione popolare del problema è una stima di massima verosimiglianza regolarizzata $ell_1$.Molti algoritmi di ottimizzazione convessi sono stati progettati per risolvere questa formulazione per recuperare la struttura del grafico.Recentemente, c'è un aumento di interesse per imparare algoritmi direttamente basati sui dati, e in questo caso, imparare a mappare la covarianza empirica alla matrice di precisione sparsa. Tuttavia, è un compito impegnativo in questo caso, poiché la definizione positiva simmetrica (SPD) e la sparsità della matrice non sono facili da applicare negli algoritmi appresi, e una mappatura diretta dai dati alla matrice di precisione può contenere molti parametri.Proponiamo un'architettura di apprendimento profondo, GLAD, che utilizza un algoritmo di minimizzazione alternata (AM) come bias induttivo del nostro modello, e apprende i parametri del modello tramite apprendimento supervisionato.Mostriamo che GLAD apprende un modello molto compatto ed efficace per il recupero di grafi radi dai dati.
La minimizzazione del rimpianto controfattuale (CFR) è una tecnica fondamentale ed efficace per risolvere i giochi di informazione imperfetta (IIG).Tuttavia, l'algoritmo originale del CFR funziona soltanto per gli stati discreti e gli spazi di azione e la strategia risultante è mantenuta come rappresentazione tabulare.Tale rappresentazione tabulare limita il metodo dall'essere applicato direttamente ai grandi giochi.In questa carta, proponiamo una doppia rappresentazione neurale per i IIG, dove una rete neurale rappresenta il rimpianto cumulativo e l'altra rappresenta la strategia media.  Tali rappresentazioni neurali ci permettono di evitare l'astrazione manuale del gioco e di effettuare l'ottimizzazione end-to-end.Per rendere l'apprendimento efficiente, abbiamo anche sviluppato diverse tecniche nuove, tra cui un metodo di campionamento robusto e un metodo di minimizzazione del rimpianto controfattuale Monte Carlo mini-batch (MCCFR), che possono essere di interesse indipendente.  Empiricamente, su giochi trattabili con approcci tabellari, le strategie neurali addestrate con il nostro algoritmo convergono comparabilmente alle loro controparti tabellari, e superano significativamente quelle basate sul deep reinforcement learning.  Su giochi estremamente grandi con miliardi di nodi di decisione, il nostro approccio ha raggiunto una forte performance mentre utilizza centinaia di volte meno memoria rispetto al CFR tabulare.su partite testa a testa di hands-up no-limit texas hold'em, il nostro agente neurale ha battuto l'agente forte ABS-CFR di $9.8\pm4.1$ chips per gioco.è un'applicazione di successo del CFR neurale in grandi giochi.
Presentiamo la prima verifica che una rete neurale per compiti di percezione produca un output corretto entro una tolleranza specificata per ogni input di interesse.Definiamo la correttezza relativa a una specifica che identifica 1) uno spazio di stato costituito da tutti gli stati rilevanti del mondo e 2) un processo di osservazione che produce gli input della rete neurale dagli stati del mondo. La piastrellatura degli spazi di stato e di input con un numero finito di piastrelle, l'ottenimento di limiti di verità di base dalle piastrelle di stato e i limiti di output della rete dalle piastrelle di input, quindi il confronto tra la verità di base e i limiti di output della rete fornisce un limite superiore all'errore di output della rete per qualsiasi input di interesse.I risultati di due casi studio evidenziano la capacità della nostra tecnica di fornire limiti di errore stretti per tutti gli input di interesse e mostrano come i limiti di errore variano sugli spazi di stato e di input.
I modelli generativi profondi hanno raggiunto progressi notevoli negli ultimi anni. Nonostante questi progressi, la valutazione quantitativa e il confronto dei modelli generativi rimane una delle sfide importanti. Mentre il calcolo diretto della log-likelihood può essere intrattabile, è stato recentemente dimostrato che la log-likelihood di alcuni dei modelli generativi più interessanti come gli autocodificatori variazionali (VAE) o le reti generative avversarie (GAN) può essere stimata in modo efficiente usando il campionamento di importanza ricucito (AIS). In questo lavoro, sosteniamo che la metrica di log-likelihood da sola non può rappresentare tutte le diverse caratteristiche di performance dei modelli generativi, e proponiamo di usare le curve di distorsione del tasso per valutare e confrontare i modelli generativi profondi. Mostriamo che possiamo approssimare l'intera curva di distorsione del tasso usando una singola esecuzione di AIS per circa lo stesso costo computazionale di una singola stima di log-likelihood. Valutiamo i tassi di compressione lossy di diversi modelli generativi profondi come VAEs, GANs (e le sue varianti) e adversarial autoencoders (AAE) su MNIST e CIFAR10, e arriviamo a una serie di intuizioni non ottenibili dalle sole log-likelihoods.
Anche se i metodi di apprendimento di rinforzo possono raggiungere risultati impressionanti nella simulazione, il mondo reale presenta due sfide principali: la generazione dei campioni è eccessivamente costosa, e le perturbazioni inaspettate o le situazioni non viste causano politiche abili ma specializzate per fallire al momento del test.Dato che non è pratico addestrare politiche separate per accogliere tutte le situazioni che l'agente può vedere nel mondo reale, questo lavoro propone di imparare come adattarsi rapidamente ed efficacemente online ai nuovi compiti.Per consentire un apprendimento efficiente del campione, consideriamo l'adattamento online nel contesto dell'apprendimento di rinforzo basato sul modello. Il nostro approccio utilizza il meta-apprendimento per addestrare un modello dinamico prioritario tale che, quando combinato con dati recenti, questo prior può essere rapidamente adattato al contesto locale. I nostri esperimenti dimostrano l'adattamento online per compiti di controllo continuo sia su agenti simulati che del mondo reale. Mostriamo prima gli agenti simulati che adattano il loro comportamento online a nuovi terreni, parti del corpo paralizzate e ambienti altamente dinamici: Dimostriamo la capacità appresa dell'agente di adattarsi rapidamente online a una gamba mancante, di adattarsi a nuovi terreni e pendii, di tenere conto di calibrazioni errate o errori nella stima della posa e di compensare i carichi utili che si tirano.
Gli approcci di apprendimento di rinforzo profondo senza modello hanno mostrato prestazioni sovrumane in ambienti simulati (ad es, Durante l'addestramento, questi approcci spesso costruiscono implicitamente uno spazio latente che contiene informazioni chiave per il processo decisionale. In questo articolo, impariamo un modello in avanti su questo spazio latente e lo applichiamo alla pianificazione basata sul modello nel gioco di strategia in tempo reale in miniatura con informazioni incomplete (MiniRTS). Dimostriamo innanzitutto che lo spazio latente costruito dai modelli attori-critici esistenti contiene informazioni rilevanti del gioco, e progettiamo la procedura di addestramento per imparare i modelli forward.Mostriamo anche che il nostro modello forward appreso può prevedere lo stato futuro significativo ed è utilizzabile per la ricerca ad albero Monte-Carlo dello spazio latente (MCTS), in termini di tassi di vittoria contro agenti basati su regole.
Queste tecniche hanno permesso ai professionisti di addestrare con successo reti convoluzionali profonde con centinaia di strati. In particolare, è stato introdotto un nuovo modo di interconnettere gli strati come la Dense Convolutional Network (DenseNet) e ha raggiunto lo stato dell'arte delle prestazioni sui compiti di riconoscimento delle immagini rilevanti. In questo lavoro, affrontiamo questo problema analizzando l'effetto dell'interconnessione degli strati sulla potenza espressiva complessiva di una rete convoluzionale; in particolare, le connessioni utilizzate in DenseNet sono confrontate con altri tipi di connettività tra gli strati. Eseguiamo un'analisi tensoriale sulla potenza espressiva delle interconnessioni sui circuiti aritmetici convoluzionali (ConvACs) e mettiamo in relazione i nostri risultati con le reti convoluzionali standard.L'analisi porta a limiti di performance e a linee guida pratiche per la progettazione di ConvACs.La generalizzazione di questi risultati viene discussa per altri tipi di reti convoluzionali attraverso decomposizioni tensorie generalizzate.
Consideriamo la seguente domanda centrale nel campo del Deep Reinforcement Learning (DRL): come possiamo utilizzare il feedback umano implicito per accelerare e ottimizzare l'addestramento di un algoritmo DRL? Le reazioni intrinseche dell'uomo al comportamento dell'agente vengono rilevate come feedback implicito posizionando degli elettrodi sul cuoio capelluto umano e monitorando quelli che sono noti come potenziali elettrici correlati agli eventi. Il feedback implicito viene poi utilizzato per aumentare l'apprendimento dell'agente nei compiti di RL. Sviluppiamo un sistema per ottenere e decodificare accuratamente il feedback umano implicito (in particolare i potenziali di evento legati all'errore) per coppie stato-azione in un ambiente di tipo Atari. Come contributo di base, dimostriamo la fattibilità di catturare i potenziali di errore di un osservatore umano che guarda un agente che impara a giocare a diversi giochi Atari utilizzando un tappo elettroencefalogramma (EEG), e poi decodificare i segnali in modo appropriato e usarli come una funzione di ricompensa ausiliaria per un algoritmo DRL con l'intento di accelerare il suo apprendimento del gioco. Costruendo in cima alla linea di base, facciamo poi i seguenti nuovi contributi nel nostro lavoro: (i) Sosteniamo che la definizione di errore-potenziali è generalizzabile attraverso diversi ambienti; in particolare mostriamo che l'errore-potenziali di un osservatore può essere appreso per un gioco specifico, e la definizione utilizzata così com'è per un altro gioco senza richiedere un nuovo apprendimento dell'errore-potenziali.  (ii) Proponiamo due diverse strutture per combinare i recenti progressi nella DRL nel sistema di feedback basato sui potenziali di errore in un modo efficiente dal punto di vista del campione, permettendo agli esseri umani di fornire un feedback implicito durante l'addestramento nel loop, o prima dell'addestramento dell'agente RL.(iii) Infine, scaliamo il feedback umano implicito (tramite ErrP) basato sulla RL ad ambienti ragionevolmente complessi (giochi) e dimostriamo l'importanza del nostro approccio attraverso esperimenti sintetici e reali.
L'apprendimento profondo ha dimostrato la capacità di apprendere strutture complesse, ma può essere limitato dai dati disponibili. Recentemente, le reti di consenso (CN) sono state proposte per alleviare la scarsità dei dati utilizzando le caratteristiche da più modalità, ma anche loro sono state limitate dalla dimensione dei dati etichettati. In questo articolo, estendiamo le CN alle reti di consenso transduttivo (TCN), adatte all'apprendimento semi-supervisionato. Nelle TCN, diverse modalità di input sono compresse in rappresentazioni latenti, che incoraggiamo a diventare indistinguibili durante l'addestramento avversario iterativo. Per comprendere i due meccanismi di TCNs, il consenso e la classificazione, abbiamo presentato le sue tre varianti in studi di ablazione su questi meccanismi.Per indagare ulteriormente i modelli TCN, trattiamo le rappresentazioni latenti come distribuzioni di probabilità e misuriamo le loro somiglianze come le divergenze relative negative di Jensen-Shannon. Mostriamo che uno stato di consenso vantaggioso per la classificazione desidera una somiglianza stabile ma imperfetta tra le rappresentazioni.Nel complesso, i TCN superano o si allineano con i migliori algoritmi di riferimento dati da 20 a 200 campioni etichettati sui set di dati Bank Marketing e DementiaBank.
Diversi metodi di ottimizzazione stocastica del primo ordine comunemente usati nel dominio euclideo, come la discesa del gradiente stocastico (SGD), la discesa del gradiente accelerata o i metodi a varianza ridotta, sono già stati adattati ad alcune impostazioni riemanniane, ma alcuni dei più popolari di questi strumenti di ottimizzazione - cioè Adam, Adagrad e il più recente Amsgrad - devono ancora essere generalizzati ai collettori riemanniani. Discutiamo la difficoltà di generalizzare tali schemi adattivi all'impostazione riemanniana più agnostica, e quindi forniamo algoritmi e prove di convergenza per obiettivi geodeticamente convessi nel caso particolare di un prodotto di collettori riemanniani, in cui l'adattabilità è implementata attraverso i collettori nel prodotto cartesiano. La nostra generalizzazione è stretta nel senso che scegliendo lo spazio euclideo come collettore riemanniano si ottengono gli stessi algoritmi e gli stessi limiti di rammarico di quelli già noti per gli algoritmi standard. Sperimentalmente, mostriamo una convergenza più veloce e un valore di perdita del treno più basso per i metodi adattivi riemanniani rispetto alle loro linee base corrispondenti sul compito realistico di incorporare la tassonomia WordNet nella sfera di Poincare.
In primo luogo, dimostriamo che i due metodi più scalabili ed efficaci per l'apprendimento di modelli robusti, l'addestramento avversario con attacchi PGD e lo smoothing randomizzato, mostrano un'efficacia molto limitata contro tre degli attacchi fisici di più alto profilo. In seguito, proponiamo un nuovo modello astratto, gli attacchi di occlusione rettangolare, in cui un avversario inserisce un piccolo rettangolo creato in modo avverso in un'immagine, e sviluppiamo due approcci per calcolare in modo efficiente gli esempi avversi risultanti. Infine, dimostriamo che l'addestramento avverso utilizzando il nostro nuovo attacco produce modelli di classificazione delle immagini che mostrano un'elevata robustezza contro gli attacchi fisicamente realizzabili che studiamo, offrendo la prima difesa generica efficace contro tali attacchi.
L'apprendimento continuo Ã¨ il problema dell'apprendimento sequenziale dei nuovi compiti o della conoscenza mentre protegge la conoscenza precedentemente acquisita.Tuttavia, l'oblio catastrofico pone una grande sfida per le reti neurali che eseguono tale processo di apprendimento.CosÃ¬, le reti neurali che sono schierate nel mondo reale lottano spesso negli scenari dove la distribuzione dei dati Ã¨ non stazionaria (deriva di concetto), squilibrata, o non sempre completamente disponibile, cioÃ¨, Proponiamo un modello di consolidamento Hebbian differenziabile che è composto da uno strato Softmax di plasticità Hebbian differenziabile (DHP) che aggiunge una componente plastica di apprendimento rapido (memoria episodica compressa) ai parametri fissi (che cambiano lentamente) dello strato di uscita softmax; permettendo alle rappresentazioni apprese di essere conservate per una scala temporale più lunga. Dimostriamo la flessibilità del nostro metodo integrando i ben noti metodi di consolidamento sinaptico task-specific per penalizzare i cambiamenti nei pesi lenti che sono importanti per ogni target task.Valutiamo il nostro approccio sui benchmark Permuted MNIST, Split MNIST e Vision Datasets Mixture, e introduciamo una variante imbalanced di Permuted MNIST --- un dataset che combina le sfide dello squilibrio di classe e del concept drift.Il nostro modello proposto non richiede iperparametri aggiuntivi e supera le baseline comparabili riducendo il forgetting.
Per scegliere un'architettura di rete neurale che sia efficace per un particolare problema di modellazione, è necessario comprendere le limitazioni imposte da ciascuna delle opzioni potenziali. Queste limitazioni sono tipicamente descritte in termini di limiti teorici dell'informazione, o confrontando la complessità relativa necessaria per approssimare funzioni di esempio tra diverse architetture. In questo articolo, esaminiamo i vincoli topologici che l'architettura di una rete neurale impone sugli insiemi di livello di tutte le funzioni che è in grado di approssimare. Questo approccio è nuovo sia per la natura delle limitazioni che per il fatto che sono indipendenti dalla profondità della rete per una vasta famiglia di funzioni di attivazione.
La verifica dei programmi offre un quadro di riferimento per garantire la correttezza del programma e quindi eliminare sistematicamente diverse classi di bug. L'inferenza delle invarianti di loop è una delle sfide principali dietro la verifica automatizzata dei programmi del mondo reale che spesso contengono molti loop. In questo articolo, presentiamo Continuous Logic Network (CLN), una nuova architettura neurale per l'apprendimento automatico delle invarianti di loop direttamente dalle tracce di esecuzione del programma.A differenza delle reti neurali esistenti, le CLN possono imparare rappresentazioni precise ed esplicite delle formule nelle teorie di Satisfiability Modulo (SMT) per invarianti di loop dalle tracce di esecuzione del programma. Sviluppiamo una nuova mappatura semantica solida e completa per assegnare le formule SMT a valori di verità continui che permette alle CLN di essere addestrate in modo efficiente. Usiamo le CLN per implementare un nuovo sistema di inferenza per invarianti di loop, CLN2INV, che supera significativamente gli approcci esistenti sul popolare dataset Code2Inv. CLN2INV è il primo strumento a risolvere tutti i 124 problemi teoricamente risolvibili nel dataset Code2Inv. Inoltre, CLN2INV impiega solo 1,1 secondi in media per ogni problema, che è 40 volte più veloce degli approcci esistenti. dimostriamo inoltre che CLN2INV può anche imparare 12 invarianti di loop significativamente più complessi di quelli richiesti per il dataset Code2Inv.
La tecnologia di sequenziamento dell'RNA di una singola cellula (scRNAseq) permette di quantificare i profili di espressione genica delle singole cellule all'interno del cancro.I metodi di riduzione delle dimensioni sono stati comunemente utilizzati per l'analisi di clustering delle cellule e la visualizzazione dei dati.Gli attuali metodi di riduzione delle dimensioni tendono ad eliminare eccessivamente le variazioni di espressione corrispondono a caratteristiche meno dominanti, così non riusciamo a trovare le proprietà omogenee dello sviluppo del cancro. In questo documento, abbiamo proposto un nuovo metodo di analisi di clustering per i dati scRNAseq, vale a dire BBSC, attraverso l'implementazione di una binarizzazione del profilo di espressione genica in cambiamenti di frequenza on/off con una fattorizzazione della matrice booleana.La rappresentazione di basso rango della matrice di espressione recuperata da BBSC aumentare la risoluzione nell'identificazione di tipi di cellule distinte o funzioni.Applicazione di BBSC su due dati scRNAseq cancro scoperto con successo sia omogeneo ed eterogeneo cluster di cellule tumorali.ulteriore scoperta ha mostrato potenziale nella prevenzione della progressione del cancro.
Mentre la normalizzazione dei flussi ha portato a significativi progressi nella modellazione di distribuzioni continue ad alta dimensione, la loro applicabilità alle distribuzioni discrete rimane sconosciuta. In questo articolo, mostriamo che i flussi possono essere estesi agli eventi discreti - e sotto una semplice formula di cambio di variabili che non richiede calcoli log-determinant-Jacobian. I flussi discreti hanno numerose applicazioni. Mostriamo prove di concetto sotto 2 architetture di flusso: i flussi autoregressivi discreti permettono la bidirezionalità, permettendo per esempio ai token nel testo di dipendere da entrambi i contesti da sinistra a destra e da destra a sinistra in un modello esatto del linguaggio; e i flussi bipartiti discreti (cioè, con la struttura a strati di RealNVP) permettono una generazione parallela come la modellazione esatta non autoregressiva del testo.
Presentiamo una rete neurale profonda con estrazione di caratteristiche assistita da spike (SAFE-DNN) per migliorare la robustezza della classificazione sotto la perturbazione stocastica degli input. La rete proposta aumenta una DNN con l'apprendimento non supervisionato di caratteristiche di basso livello usando una rete di neuroni spiking (SNN) con Spike-Time-Dependent-Plasticity (STDP). La rete completa impara a ignorare le perturbazioni locali mentre esegue il rilevamento e la classificazione delle caratteristiche globali. I risultati sperimentali su CIFAR-10 e sul sottoinsieme ImageNet dimostrano una migliore robustezza del rumore per più architetture DNN senza sacrificare la precisione sulle immagini pulite.
Le incorporazioni neurali sono state utilizzate con grande successo nel Natural Language Processing (NLP), dove forniscono rappresentazioni compatte che incapsulano la somiglianza delle parole e raggiungono prestazioni allo stato dell'arte in una serie di compiti linguistici. Il successo delle incorporazioni neurali ha spinto una quantità significativa di ricerche in applicazioni in domini diversi da quello linguistico. Uno di questi domini è quello dei dati strutturati a grafo, dove le incorporazioni dei vertici possono essere apprese per incapsulare la somiglianza dei vertici e migliorare le prestazioni in compiti che includono la previsione dei bordi e l'etichettatura dei vertici. Tuttavia, un lavoro recente ha dimostrato che lo spazio isometrico appropriato per l'incorporazione di reti complesse non è lo spazio piano euclideo, ma uno spazio iperbolico negativamente curvo. Presentiamo un nuovo concetto che sfrutta queste recenti intuizioni e proponiamo l'apprendimento di embeddings neurali di grafi nello spazio iperbolico.
L'International Competition on Knowledge Engineering for Planning and Scheduling (ICKEPS) gioca un ruolo fondamentale nel promuovere lo sviluppo di nuovi strumenti di Knowledge Engineering (KE), e nel sottolineare l'importanza di approcci di principio per tutti i diversi aspetti KE che sono necessari per un uso di successo a lungo termine della pianificazione nelle applicazioni del mondo reale.  In questo articolo, come esercizio di sintesi e per stimolare pensieri e discussioni, rivediamo il formato dei precedenti ICKEPS, per suggerire formati alternativi per le competizioni future, idealmente per motivare qualcuno a farsi avanti e organizzare le prossime.
Dimostriamo che la generazione di articoli di Wikipedia in inglese può essere affrontata come un riassunto multi-documento dei documenti di origine. Usiamo il riassunto estrattivo per identificare grossolanamente le informazioni salienti e un modello neurale astrattivo per generare l'articolo. Per il modello astrattivo, introduciamo un'architettura di solo decodificatore che può scalabilmente assistere a sequenze molto lunghe, molto più lunghe delle tipiche architetture encoder-decoder usate nella trasduzione di sequenze. Dimostriamo che questo modello può generare paragrafi multisentenza fluenti e coerenti e persino interi articoli di Wikipedia. Quando gli vengono dati documenti di riferimento, dimostriamo che può estrarre informazioni fattuali rilevanti come riflesso nella perplessità, nei punteggi ROUGE e nelle valutazioni umane.
Abstract Stochastic gradient descent (SGD) e Adam sono comunemente usati per ottimizzare le reti neurali profonde, ma sceglierne uno di solito significa fare dei compromessi tra velocità, accuratezza e stabilità.Qui presentiamo un'intuizione del perché i compromessi esistono, così come un metodo per unificare i due in modo continuo.Questo rende possibile controllare il modo in cui i modelli sono addestrati in modo molto più dettagliato.Mostriamo che per parametri predefiniti, il nuovo algoritmo eguaglia o supera SGD e Adam in una gamma di modelli per compiti di classificazione delle immagini e supera SGD per compiti di modellazione linguistica.
L'uso dell'apprendimento per imitazione per imparare una singola politica per un compito complesso che ha più modalità o una struttura gerarchica può essere impegnativo. Infatti, il lavoro precedente ha dimostrato che quando le modalità sono note, l'apprendimento di politiche separate per ogni modalità o sottocompito può migliorare notevolmente le prestazioni dell'apprendimento per imitazione.In questo lavoro, scopriamo l'interazione tra sottocompiti dalle loro sequenze di traiettorie stato-azione risultanti utilizzando un modello grafico diretto. Il nostro approccio massimizza il flusso di informazioni dirette nel modello grafico tra le variabili latenti delle sotto-attività e le loro traiettorie generate. Mostriamo anche come il nostro approccio si colleghi con il quadro esistente di Options, che è comunemente usato per imparare politiche gerarchiche.
La rete neurale convoluzionale (CNN) è stata applicata con successo in molti campi negli ultimi decenni; tuttavia manca la capacità di utilizzare la conoscenza del dominio precedente quando si tratta di molti problemi realistici. Presentiamo una struttura chiamata Geometric Operator Convolutional Neural Network (GO-CNN) che utilizza la conoscenza del dominio, dove il kernel del primo strato convoluzionale è sostituito con un kernel generato da una funzione operatore geometrico. In determinate condizioni, analizziamo teoricamente la convergenza e il limite degli errori di generalizzazione tra GO-CNNs e CNNs comuni.Anche se i kernel di convoluzione dell'operatore geometrico hanno meno parametri addestrabili rispetto ai kernel di convoluzione comuni, i risultati sperimentali indicano che GO-CNN esegue più accuratamente di CNN comuni su CIFAR-10/100.Inoltre, GO-CNN riduce la dipendenza dalla quantità di esempi di formazione e migliora la stabilità avversaria.
Nel quadro dell'apprendimento profondo, il DPP è tipicamente ottimizzato tramite approssimazione, che non è semplice e ha qualche conflitto con il requisito di diversità. Notiamo, tuttavia, che non ci sono stati paradigmi di apprendimento profondo per ottimizzare il DPP direttamente, dal momento che coinvolge l'inversione della matrice che può comportare un'elevata instabilità computazionale, questo fatto ostacola notevolmente l'ampio uso del DPP su alcuni obiettivi specifici in cui il DPP serve come termine per misurare la diversità delle caratteristiche. In questo articolo, mettiamo a punto un algoritmo semplice ma efficace per affrontare questo problema per ottimizzare il termine DPP direttamente espresso con L-ensemble nel dominio spettrale sulla matrice gram, che è più flessibile dell'apprendimento sui kernel parametrici. Tenendo conto di alcuni vincoli geometrici, il nostro algoritmo cerca di generare dei sottogradienti validi del termine DPP nel caso in cui la matrice gramma DPP non sia invertibile (non esistono gradienti in questo caso).In questo senso, il nostro algoritmo può essere facilmente incorporato con più compiti di apprendimento profondo.Gli esperimenti mostrano l'efficacia del nostro algoritmo, indicando prestazioni promettenti per problemi di apprendimento pratico.
La qualità di un sistema di traduzione automatica dipende in gran parte dalla disponibilità di corpora paralleli considerevoli.Per la struttura di traduzione automatica neurale (NMT), recentemente popolare, il problema della scarsità dei dati può diventare ancora più grave.Con una grande quantità di parametri regolabili, il modello NMT può adattarsi eccessivamente alle coppie di lingue esistenti, mentre non riesce a comprendere la diversità generale della lingua. In questo documento, sosteniamo di trasmettere ogni coppia di frasi come due gruppi di frasi simili per incorporare una maggiore diversità nelle espressioni linguistiche, che chiamiamo cluster paralleli, quindi definiamo un punteggio di corrispondenza cluster-to-cluster più generale e addestriamo il nostro modello per massimizzare questo punteggio. Poiché la massimizzazione diretta è difficile, deriviamo il suo limite inferiore come nostro obiettivo surrogato, che si trova a generalizzare gli algoritmi punto-punto Maximum Likelihood Estimation (MLE) e punto-cluster Reward Augmented Maximum Likelihood (RAML) come casi speciali.Sulla base di questa nuova funzione obiettivo, delineiamo quattro sistemi potenziali per realizzare la nostra struttura cluster-to-cluster e testiamo le loro prestazioni in tre compiti di traduzione riconosciuti, ogni compito con direzioni di traduzione avanti e indietro. In ognuno dei sei esperimenti, i nostri quattro sistemi paralleli proposti hanno costantemente dimostrato di superare i sistemi MLE baseline, RL (Reinforcement Learning) e RAML in modo significativo.Infine, abbiamo eseguito un caso di studio per analizzare empiricamente la forza del quadro NMT cluster-to-cluster.
La capacità di prendere decisioni interpretabili e autoesplicative è essenziale per lo sviluppo di sistemi di apprendimento automatico responsabili.In questo lavoro, studiamo l'apprendimento per spiegare il problema nell'ambito della programmazione logica induttiva (ILP).Proponiamo l'apprendimento induttivo della logica neurale (NLIL), un efficiente quadro ILP differenziabile che impara le regole logiche del primo ordine che possono spiegare i modelli nei dati. Negli esperimenti, rispetto ai modelli allo stato dell'arte, troviamo che NLIL è in grado di cercare regole che sono x10 volte più lunghe pur rimanendo x3 volte più veloce.Mostriamo anche che NLIL può scalare a grandi dataset di immagini, ad esempio Visual Genome, con 1M di entità.
I classificatori basati su reti neurali sono paralleli o superano l'accuratezza a livello umano su molti compiti comuni e sono utilizzati in sistemi pratici. Tuttavia, le reti neurali sono suscettibili di esempi avversari, input accuratamente perturbati che inducono le reti a comportarsi male in modi scelti arbitrariamente. Quando vengono generati con metodi standard, questi esempi non ingannano costantemente un classificatore nel mondo fisico a causa di una combinazione di spostamenti del punto di vista, rumore della telecamera e altre trasformazioni naturali. Introduciamo il primo metodo per la costruzione di oggetti 3D del mondo reale che ingannano in modo coerente una rete neurale attraverso un'ampia distribuzione di angoli e punti di vista.Presentiamo un algoritmo generico per la generazione di esempi avversari che sono robusti attraverso qualsiasi distribuzione scelta di trasformazioni. Dimostriamo la sua applicazione in due dimensioni, producendo immagini avversarie che sono robuste al rumore, alla distorsione e alla trasformazione affine.Infine, applichiamo l'algoritmo per produrre oggetti avversari fisici arbitrari stampati in 3D, dimostrando che il nostro approccio funziona end-to-end nel mondo reale.I nostri risultati mostrano che gli esempi avversari sono una preoccupazione pratica per i sistemi del mondo reale.
Le rappresentazioni degli insiemi sono impegnative da imparare perché le operazioni sugli insiemi dovrebbero essere invarianti alle permutazioni.A questo scopo, proponiamo un modulo di Permutazione-Ottimizzazione che impara come permutare un insieme end-to-end.L'insieme permutato può essere ulteriormente elaborato per imparare una rappresentazione invariante alle permutazioni di quell'insieme, evitando un collo di bottiglia nei modelli tradizionali degli insiemi. Dimostriamo la capacità del nostro modello di imparare le permutazioni e le rappresentazioni degli insiemi con una supervisione esplicita o implicita su quattro insiemi di dati, sui quali otteniamo risultati all'avanguardia: ordinamento dei numeri, mosaici di immagini, classificazione da mosaici di immagini e risposta a domande visive.
La progettazione fisica di un robot e la politica che controlla il suo movimento sono intrinsecamente accoppiati. Tuttavia, gli approcci esistenti in gran parte ignorano questo accoppiamento, scegliendo invece di alternare tra le fasi separate di progettazione e controllo, che richiede l'intuizione dell'esperto in tutto e rischia di convergere verso disegni subottimali. In questo lavoro, proponiamo un metodo che ottimizza congiuntamente la progettazione fisica di un robot e la politica di controllo corrispondente in un modo senza modello, senza alcun bisogno di supervisione di esperti. Durante l'addestramento, perfezioniamo la distribuzione del robot per massimizzare la ricompensa prevista, ottenendo così un'assegnazione ai parametri del robot e alla politica della rete neurale che sono congiuntamente ottimali. Valutiamo il nostro approccio nel contesto della locomozione a gambe, e dimostriamo che scopre nuovi progetti di robot e andature per diverse morfologie, ottenendo prestazioni paragonabili o migliori di quelle dei progetti fatti a mano.
Le applicazioni delle reti neurali spesso considerano l'apprendimento nel contesto di un singolo compito, ma in molti scenari ciò che speriamo di imparare non è solo un singolo compito, ma un modello che può essere utilizzato per risolvere più compiti diversi. Tuttavia, in alcune impegnative impostazioni di apprendimento multi-task, in particolare nell'apprendimento di rinforzo, è molto difficile imparare un singolo modello che possa risolvere tutti i compiti, realizzando al contempo l'efficienza dei dati e i benefici in termini di prestazioni.Imparare ciascuno dei compiti indipendentemente da zero può effettivamente dare risultati migliori in tali impostazioni, ma non beneficia della condivisione della rappresentazione che l'apprendimento multi-task può potenzialmente fornire. In questo lavoro, sviluppiamo un approccio che conferisce a un singolo modello la capacità di rappresentare entrambi gli estremi: formazione congiunta e formazione indipendente. A tal fine, introduciamo la matrice-interleaving (Mint), una modifica ai modelli di rete neurale standard che proietta le attivazioni per ogni compito in un diverso sottospazio appreso, rappresentato da una matrice per compito e per strato. Imparando queste matrici congiuntamente con gli altri parametri del modello, l'ottimizzatore stesso può decidere quanto condividere le rappresentazioni tra i compiti. Su tre impegnativi problemi di apprendimento supervisionato multi-task e di apprendimento di rinforzo con vari gradi di struttura di compiti condivisi, troviamo che questo modello corrisponde costantemente o supera l'allenamento congiunto e l'allenamento indipendente, combinando i migliori elementi di entrambi.
Tuttavia, a causa delle numerose variazioni che possono verificarsi nel mondo reale, all'agente è spesso richiesto di essere robusto per essere utile, il che non è stato il caso degli agenti addestrati con algoritmi di apprendimento per rinforzo (RL). In questo articolo, esaminiamo l'overfitting degli agenti RL agli ambienti di formazione in compiti di navigazione visiva. Proponiamo un metodo di regolarizzazione che combina RL con metodi di apprendimento supervisionato aggiungendo un termine all'obiettivo RL che incoraggia l'invarianza di una politica alle variazioni nelle osservazioni che non dovrebbero influenzare l'azione intrapresa. I risultati di questo metodo, chiamato regolarizzazione dell'invarianza, mostrano un miglioramento nella generalizzazione delle politiche ad ambienti non visti durante l'allenamento.
Sebbene le informazioni visive siano state introdotte per migliorare la traduzione automatica neurale (NMT), la loro efficacia dipende fortemente dalla disponibilità di grandi quantità di coppie di frasi parallele bilingui con annotazioni di immagini manuali. In questo articolo, presentiamo una rappresentazione visiva universale appresa sui corpora monolingui con annotazioni di immagini, che supera la mancanza di coppie frase-immagine bilingui su larga scala, estendendo così l'applicabilità delle immagini nella NMT. In dettaglio, un gruppo di immagini con argomenti simili alla frase di origine sarà recuperato da una tabella di ricerca leggera argomento-immagine appresa sulle coppie frase-immagine esistenti, e poi viene codificata come rappresentazione dell'immagine da una ResNet pre-addestrata. In particolare, il metodo proposto permette di integrare le informazioni visive in una NMT su larga scala di solo testo, oltre alla NMT multimodello. Gli esperimenti su quattro dataset di traduzione ampiamente utilizzati, tra cui WMT'16 dall'inglese al rumeno, WMT'14 dall'inglese al tedesco, WMT'14 dall'inglese al francese, e Multi30K, mostrano che l'approccio proposto raggiunge miglioramenti significativi rispetto alle basi forti.
Questo articolo introduce una nuova struttura per l'apprendimento di algoritmi per risolvere problemi di ottimizzazione combinatoria online, introducendo una serie di idee chiave dagli algoritmi tradizionali e dalla teoria della complessità, per prima cosa stabiliamo una nuova connessione tra i metodi primal-dual e il reinforcement learning. Poi, introduciamo il concetto di distribuzioni avversarie (set di allenamento universali e ad alta entropia), che sono distribuzioni che incoraggiano l'allievo a trovare algoritmi che funzionano bene nel caso peggiore.Testiamo le nostre nuove idee su una serie di problemi di ottimizzazione come il problema AdWords, il problema knapsack online, e il problema del segretario.I nostri risultati indicano che i modelli hanno imparato comportamenti che sono coerenti con gli algoritmi ottimali tradizionali per questi problemi.
Nonostante la loro popolarità e i loro successi, le reti neurali profonde sono poco conosciute dal punto di vista teorico e trattate come sistemi a "scatola nera". L'utilizzo di una visione funzionale di queste reti ci offre un'utile nuova lente con cui comprenderle, che ci permette di sondare teoricamente o sperimentalmente le proprietà di queste reti, incluso l'effetto delle inizializzazioni standard, il valore della profondità, la superficie di perdita sottostante e le origini della generalizzazione. Un risultato chiave è che la generalizzazione deriva dalla morbidezza dell'approssimazione funzionale, combinata con un'approssimazione iniziale piatta. Questa morbidezza aumenta con il numero di unità, spiegando perché le reti massicciamente iperparametrizzate continuano a generalizzare bene.
È ben noto che le reti neurali più profonde sono più difficili da addestrare rispetto a quelle meno profonde. In questo breve articolo, usiamo lo spettro (completo) degli autovalori dell'Hessiano per esplorare come il panorama delle perdite cambia man mano che la rete diventa più profonda, e come le connessioni residue vengono aggiunte all'architettura. Calcolando una serie di misure quantitative sullo spettro di Hessian, mostriamo che la distribuzione degli autovalori di Hessian nelle reti più profonde ha code sostanzialmente più pesanti (equivalentemente, più autovalori outlier), il che rende la rete più difficile da ottimizzare con metodi del primo ordine.
Nel contesto dell'ottimizzazione, il gradiente di una rete neurale indica la quantità che un peso specifico dovrebbe cambiare rispetto alla perdita.Pertanto, piccoli gradienti indicano un buon valore del peso che non richiede alcun cambiamento e può essere tenuto congelato durante l'addestramento.Questo articolo fornisce uno studio sperimentale sull'importanza dei pesi di una rete neurale, e in che misura devono essere aggiornati.Vogliamo dimostrare che a partire dalla terza epoca, congelare i pesi che non hanno un gradiente informativo e hanno meno probabilità di essere cambiati durante l'addestramento, risulta in un calo molto leggero dell'accuratezza complessiva (e a volte migliore). Sperimentiamo sui dataset MNIST, CIFAR10 e Flickr8k utilizzando diverse architetture (VGG19, ResNet-110 e DenseNet-121). Su CIFAR10, mostriamo che congelare l'80% dei parametri della rete VGG19 dalla terza epoca in poi comporta un calo di precisione dello 0,24%, mentre congelare il 50% dei parametri di Resnet-110 comporta un calo di precisione dello 0,9% e infine congelare il 70% dei parametri di Densnet-121 comporta un calo dello 0. 57%. Inoltre, per sperimentare con applicazioni di vita reale, addestriamo un modello di didascalia delle immagini con meccanismo di attenzione sul dataset Flickr8k utilizzando reti LSTM, congelando il 60% dei parametri dalla terza epoca in poi, ottenendo un migliore punteggio BLEU-4 rispetto al modello completamente addestrato.Il nostro codice sorgente può essere trovato in appendice.
Come gli esseri umani, le reti profonde imparano meglio quando i campioni sono organizzati e introdotti in un ordine significativo o curriculum.Mentre gli approcci convenzionali all'apprendimento del curriculum enfatizzano la difficoltà dei campioni come strategia incrementale di base, costringe le reti a imparare da piccoli sottoinsiemi di dati introducendo sovraccarichi di pre-computazione. In questo lavoro, proponiamo Learning with Incremental Labels and Adaptive Compensation (LILAC), che introduce un nuovo approccio all'apprendimento del curriculum. LILAC enfatizza l'apprendimento incrementale delle etichette anziché l'apprendimento incrementale di campioni difficili e funziona in due fasi distinte: in primo luogo, nella fase di introduzione incrementale delle etichette, smascheriamo le etichette di verità a incrementi fissi durante la formazione, per migliorare il punto di partenza da cui le reti imparano. Nella fase di compensazione adattiva, compensiamo le previsioni fallite alterando in modo adattivo il vettore di destinazione a una distribuzione più liscia.Valutiamo LILAC contro i metodi comparabili più vicini nell'apprendimento in batch e curriculum e nell'appianamento delle etichette, attraverso tre benchmark di immagini standard, CIFAR-10, CIFAR-100 e STL-10. Mostriamo che il nostro metodo supera l'apprendimento in batch con un'accuratezza di riconoscimento media più alta e una deviazione standard più bassa nelle prestazioni in modo coerente su tutti i benchmark. Estendiamo ulteriormente LILAC alle prestazioni allo stato dell'arte su CIFAR-10 usando un semplice aumento dei dati mentre esibisce l'invarianza dell'ordine delle etichette tra le altre importanti proprietà.
Le parole nel linguaggio naturale seguono una distribuzione Zipfian in cui alcune parole sono frequenti ma la maggior parte sono rare.Imparare rappresentazioni per le parole nella "coda lunga" di questa distribuzione richiede enormi quantità di dati. Le rappresentazioni delle parole rare addestrate direttamente sui compiti finali sono solitamente scarse, richiedendoci di pre-addestrare le incorporazioni su dati esterni, o di trattare tutte le parole rare come parole fuori dal vocabolario con una rappresentazione unica. Forniamo un metodo per prevedere le incorporazioni delle parole rare al volo da piccole quantità di dati ausiliari con una rete addestrata end-to-end per il compito a valle.
La capacità di rilevare in modo affidabile i campioni fuori distribuzione è uno dei fattori chiave nell'implementazione di un buon classificatore, poiché la distribuzione di prova non corrisponde sempre alla distribuzione di formazione nella maggior parte delle applicazioni del mondo reale. In questo lavoro, proponiamo un classificatore generativo profondo che è efficace per rilevare i campioni fuori distribuzione così come classificare i campioni in distribuzione, integrando il concetto di analisi discriminante gaussiana nelle reti neurali profonde. A differenza del classificatore discriminativo (o softmax) che si concentra solo sul confine decisionale che suddivide il suo spazio latente in più regioni, il nostro classificatore generativo mira a modellare esplicitamente le distribuzioni delle condizioni di classe come distribuzioni gaussiane separabili, per cui possiamo definire il punteggio di fiducia in base alla distanza tra un campione di prova e il centro di ogni distribuzione. La nostra valutazione empirica su immagini multiclasse e dati tabulari dimostra che il classificatore generativo raggiunge le migliori prestazioni nel distinguere i campioni fuori distribuzione, e può anche essere generalizzato bene per vari tipi di reti neurali profonde.
Uno dei sintomi più diffusi tra la popolazione anziana, la demenza, può essere rilevato da classificatori addestrati su caratteristiche linguistiche estratte da trascrizioni narrative. Tuttavia, queste caratteristiche linguistiche sono influenzate in modo simile ma diverso dal normale processo di invecchiamento, che è quindi un fattore di confusione, i cui effetti sono stati difficili da isolare per i classificatori di apprendimento automatico. In questo articolo, mostriamo che i classificatori di reti neurali profonde (DNN) possono dedurre l'età dalle caratteristiche linguistiche, che è un intreccio che potrebbe portare a un'ingiustizia tra i gruppi di età. Mostriamo che questo problema è causato da attivazioni indesiderate di strutture v nei diagrammi di causalità, e potrebbe essere affrontato con l'apprendimento di rappresentazioni eque.Costruiamo classificatori di reti neurali che imparano rappresentazioni bidimensionali che riflettono gli impatti della demenza ma scartano gli effetti dell'età. Per valutare questi classificatori, specifichiamo un punteggio modello-agnostico $\Delta_{eo}^{(N)}$ che misura come i risultati del classificatore sono separati dall'età.I nostri migliori modelli superano i classificatori di rete neurale di base nella separazione, mentre compromettono l'accuratezza di appena il 2,56% e il 2,25% rispettivamente su DementiaBank e sul set di dati Famous People.
Gran parte dell'attenzione nella progettazione delle reti neurali profonde si è concentrata sul miglioramento della precisione, portando ad architetture di rete più potenti ma altamente complesse che sono difficili da implementare in scenari pratici.  Di conseguenza, c'è stato un recente interesse nella progettazione di metriche quantitative per la valutazione delle reti neurali profonde che tengano conto di più della semplice precisione del modello come unico indicatore delle prestazioni della rete.  In questo studio, continuiamo la conversazione verso una metrica universale per valutare le prestazioni delle reti neurali profonde per l'uso pratico su dispositivo introducendo NetScore, una nuova metrica progettata specificamente per fornire una valutazione quantitativa dell'equilibrio tra precisione, complessità computazionale e complessità dell'architettura di una rete neurale profonda.  In quella che è una delle più grandi analisi comparative tra reti neurali profonde in letteratura, la metrica NetScore, la metrica dell'accuratezza top-1 e la popolare metrica della densità di informazioni sono state confrontate su un set diversificato di 60 diverse reti neurali convoluzionali profonde per la classificazione delle immagini sul dataset ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2012).  I risultati della valutazione su queste tre metriche per questo diverso set di reti sono presentati in questo studio per fungere da guida di riferimento per i professionisti del settore.  
Le reti neurali profonde (DNN) sono vulnerabili agli attacchi avversari, in particolare agli attacchi mirati white-box. Questo articolo studia il problema di quanto possano essere aggressivi gli attacchi mirati white-box per andare oltre gli attacchi Top-1. Proponiamo di imparare attacchi Top-k ordinati (k>=1), che impongono che le etichette Top-k previste di un esempio avversario siano le k etichette (casualmente) selezionate e ordinate (l'etichetta di verità è esclusiva). In primo luogo, estendiamo il metodo vanilla Carlini-Wagner (C&W) e lo usiamo come base forte. In secondo luogo, presentiamo una struttura di distillazione avversaria che consiste di due componenti: (i) Calcolo di una distribuzione di probabilità avversaria per qualsiasi etichetta ordinata Top-$k$ mirata. (ii) Imparare esempi avversari minimizzando la divergenza Kullback-Leibler (KL) tra la distribuzione avversaria e la distribuzione prevista, insieme alla penalità di energia di perturbazione.Nel calcolare le distribuzioni avversarie, esploriamo come sfruttare le somiglianze semantiche delle etichette, portando ad attacchi orientati alla conoscenza. Negli esperimenti, testiamo gli attacchi Top-k (k=1,2,5,10) nel set di dati ImageNet-1000 val utilizzando due DNN popolari addestrate con il set di dati pulito di ImageNet-1000, ResNet-50 e DenseNet-121. Nel complesso, l'approccio di distillazione avversaria ottiene i migliori risultati, soprattutto con un ampio margine quando il budget di calcolo è limitato. Riduce l'energia di perturbazione in modo coerente con lo stesso tasso di successo dell'attacco su tutti e quattro i k, e migliora il tasso di successo dell'attacco con ampio margine rispetto al metodo C&W modificato per k=10.   
Gli algoritmi neurali di message passing per la classificazione semi-supervisionata sui grafi hanno recentemente ottenuto un grande successo.Tuttavia, per classificare un nodo questi metodi considerano solo i nodi che si trovano a pochi passi di propagazione e la dimensione di questo quartiere utilizzato è difficile da estendere.In questo articolo, usiamo la relazione tra le reti convoluzionali a grafo (GCN) e PageRank per derivare uno schema di propagazione migliorato basato su PageRank personalizzato. Utilizziamo questa procedura di propagazione per costruire un modello semplice, la propagazione personalizzata delle previsioni neurali (PPNP), e la sua approssimazione veloce, APPNP.Il tempo di formazione del nostro modello è alla pari o più veloce e il suo numero di parametri alla pari o inferiore rispetto ai modelli precedenti.Sfrutta un grande quartiere regolabile per la classificazione e può essere facilmente combinato con qualsiasi rete neurale.Mostriamo che questo modello supera diversi metodi recentemente proposti per la classificazione semi-supervisionata nello studio più approfondito fatto finora per modelli simili a GCN.La nostra implementazione è disponibile online.
La stragrande maggioranza dei dizionari di valutazione dell'induzione del lessico sono tra l'inglese e un'altra lingua, e lo spazio di incorporazione inglese è selezionato per default come hub quando si impara in un ambiente multilingue. Valutando i metodi consolidati su tutte queste coppie di lingue, facciamo luce sulla loro idoneità e presentiamo nuove sfide per il campo. Infine, nella nostra analisi identifichiamo le linee guida generali per una forte base di embeddings trans-lingue, basata su più di semplici esperimenti anglocentrici.
Interpretare l'addestramento delle reti generative avversarie (GAN) come minimizzazione approssimativa della divergenza è stato teoricamente perspicace, ha stimolato la discussione e ha portato a estensioni teoricamente e praticamente interessanti come le f-GAN e le Wasserstein GAN. Sia per le GAN classiche che per le f-GAN, esiste una variante originale di addestramento e una variante "non saturante" che utilizza una forma alternativa di aggiornamento del generatore. L'aggiornamento alternativo del generatore è spesso considerato come una semplice modifica per affrontare i problemi di ottimizzazione, e sembra essere un errore comune che le due varianti minimizzino la stessa divergenza.In questa breve nota deriviamo le divergenze approssimativamente minimizzate dalle varianti originali e alternative di addestramento GAN e f-GAN.Questo evidenzia importanti differenze tra le due varianti. Per esempio, mostriamo che la variante alternativa dell'addestramento KL-GAN minimizza effettivamente la divergenza KL inversa, e che la variante alternativa dell'addestramento GAN convenzionale minimizza una versione "ammorbidita" della KL inversa.Speriamo che questi risultati possano aiutare a chiarire alcune delle discussioni teoriche che circondano la visione della minimizzazione della divergenza dell'addestramento GAN.
REINFORCE può essere usato per addestrare modelli in impostazioni di predizione strutturate per ottimizzare direttamente l'obiettivo di test-time, ma il caso comune di campionare una predizione per datapoint (input) è inefficiente dal punto di vista dei dati. Inoltre deriviamo uno stimatore REINFORCE con linea di base, basato sul campionamento senza sostituzione.Combinato con una tecnica recente per campionare sequenze senza sostituzione usando la ricerca stocastica a fascio, questo migliora la procedura di addestramento per un modello di sequenza che predice la soluzione del Travelling Salesman Problem.
L'apprendimento per rinforzo (RL) è una tecnica potente per addestrare un agente ad eseguire un compito.  Tuttavia, un agente che viene addestrato usando RL è solo in grado di raggiungere il singolo compito che è specificato attraverso la sua funzione di ricompensa.   Tale approccio non si adatta bene alle impostazioni in cui un agente ha bisogno di eseguire una serie di compiti diversi, come la navigazione in diverse posizioni in una stanza o lo spostamento di oggetti in diverse posizioni.  Invece, proponiamo un metodo che permette ad un agente di scoprire automaticamente la gamma di compiti che è in grado di eseguire nel suo ambiente.  Usiamo una rete generatrice per proporre compiti che l'agente deve cercare di raggiungere, ogni compito è specificato come raggiungere un certo sottoinsieme parametrizzato dello spazio di stato.  La rete generatrice è ottimizzata usando l'addestramento avversario per produrre compiti che sono sempre al livello di difficoltà appropriato per l'agente.  Il nostro metodo produce così automaticamente un curriculum di compiti che l'agente deve imparare.  Dimostriamo che, utilizzando questa struttura, un agente può imparare in modo efficiente e automatico ad eseguire un ampio insieme di compiti senza richiedere alcuna conoscenza preliminare del suo ambiente (video e codice disponibili su: https://sites.google.com/view/goalgeneration4rl). Il nostro metodo può anche imparare a realizzare compiti con ricompense sparse, che pongono sfide significative ai metodi RL tradizionali.
Una vasta gamma di difese sono state proposte per rendere più resistenti le reti neurali contro gli attacchi avversari, ma è emerso un modello in cui la maggior parte delle difese avversarie sono rapidamente infrante da nuovi attacchi.  Data la mancanza di successo nel generare difese robuste, siamo portati a porre una domanda fondamentale:  Questo articolo analizza esempi di attacchi avversari da una prospettiva teorica, e identifica i limiti fondamentali sulla suscettibilità di un classificatore agli attacchi avversari.   Mostriamo che, per certe classi di problemi, gli esempi avversari sono inevitabili.  Utilizzando gli esperimenti, esploriamo le implicazioni delle garanzie teoriche per i problemi del mondo reale e discutiamo come fattori quali la dimensionalità e la complessità dell'immagine limitino la robustezza di un classificatore contro gli esempi avversari.
Per le applicazioni di computer vision, i lavori precedenti hanno dimostrato l'efficacia della riduzione della precisione numerica dei parametri del modello (pesi di rete) nelle reti neurali profonde.Le mappe di attivazione, tuttavia, occupano una grande impronta di memoria sia durante la fase di addestramento che di inferenza quando si usano mini-batch di input.Un modo per ridurre questa grande impronta di memoria è ridurre la precisione delle attivazioni.Tuttavia, i lavori passati hanno dimostrato che ridurre la precisione delle attivazioni danneggia l'accuratezza del modello.Studiamo schemi per addestrare le reti da zero usando attivazioni a precisione ridotta senza danneggiare l'accuratezza. Riduciamo la precisione delle mappe di attivazione (insieme ai parametri del modello) e aumentiamo il numero di mappe di filtraggio in uno strato, e troviamo che questo schema corrisponde o supera l'accuratezza della rete di base a piena precisione.Di conseguenza, si può migliorare significativamente l'efficienza di esecuzione (ad es. riducendo l'ingombro della memoria dinamica, la larghezza della banda di memoria e l'energia computazionale) e accelerare il processo di formazione e inferenza con il supporto hardware appropriato. Chiamiamo il nostro schema WRPN - reti a precisione ridotta ampia.Riportiamo i risultati e dimostriamo che lo schema WRPN è migliore delle precisioni precedentemente riportate sul set di dati ILSVRC-12 pur essendo computazionalmente meno costoso rispetto alle reti a precisione ridotta precedentemente riportate.
Studiamo metodi per l'apprendimento semi-supervisionato (SSL) di un campo casuale condizionale a catena lineare neurale (CRF) per il Named Entity Recognition (NER) trattando il tagger come il posteriore variazionale ammortizzato in un modello generativo del testo dato i tag.Illustriamo prima come incorporare un CRF in un VAE, permettendo l'addestramento end-to-end su dati semi-supervisionati. Abbiamo poi studiato una serie di modelli generativi profondi sempre più complessi di token dati i tag abilitati dall'ottimizzazione end-to-end, confrontando i modelli proposti contro le baseline SSL supervisionate e forti CRF sul dataset NER di Ontonotes5. Troviamo che il nostro miglior modello proposto migliora costantemente le prestazioni di circa 1% F1 in regimi di risorse basse e moderate e affronta facilmente il comportamento degenerato del modello in un ambiente più difficile, parzialmente supervisionato.
Per rendere le reti neurali profonde fattibili in ambienti con risorse limitate (come i dispositivi mobili), è vantaggioso quantizzare i modelli utilizzando pesi a bassa precisione. Una tecnica comune per quantizzare le reti neurali è il metodo del gradiente straight-through, che consente la retropropagazione attraverso la mappatura della quantizzazione. Partendo da una nuova osservazione che il metodo del gradiente passante è di fatto identico al ben noto algoritmo di Nesterov a doppia media su un problema di ottimizzazione con vincoli di quantizzazione, proponiamo un approccio alternativo con più principi, chiamato ProxQuant, che formula l'addestramento della rete quantizzata come un problema di apprendimento regolarizzato e lo ottimizza attraverso il metodo prox-gradient. ProxQuant esegue il back-propagation sul vettore di piena precisione sottostante e applica un efficiente prox-operatore tra i passi del gradiente stocastico per incoraggiare la quantizzazione. Per quantizzare ResNets e LSTMs, ProxQuant supera i risultati dello stato dell'arte sulla quantizzazione binaria ed è alla pari con lo stato dell'arte sulla quantizzazione multi-bit. Per la quantizzazione binaria, la nostra analisi mostra sia teoricamente che sperimentalmente che ProxQuant è più stabile del metodo a gradiente diretto (cioè BinaryConnect), sfidando l'indispensabilità del metodo a gradiente diretto e fornendo una potente alternativa.
Le vulnerabilità delle reti neurali profonde contro gli esempi avversari sono diventate una preoccupazione significativa per l'implementazione di questi modelli in domini sensibili. Elaborare una difesa definitiva contro tali attacchi è dimostrato essere impegnativo, e i metodi che si basano sul rilevamento di campioni avversari sono validi solo quando l'attaccante è ignaro del meccanismo di rilevamento. In questo articolo, consideriamo il problema di rilevamento avversario sotto il quadro di ottimizzazione robusta. L'integrazione del classificatore e dei rilevatori presenta un meccanismo di rilevamento che fornisce una garanzia di prestazioni all'avversario considerato.Dimostriamo che AAT promuove l'apprendimento di distribuzioni condizionali di classe, che dà luogo a ulteriori approcci generativi di rilevamento/classificazione che sono sia robusti che più interpretabili.Forniamo valutazioni complete dei metodi di cui sopra, e dimostriamo le loro prestazioni competitive e proprietà convincenti sul rilevamento avversario e problemi di classificazione robusta.
L'esplorazione è una componente chiave del successo dell'apprendimento di rinforzo, ma gli approcci ottimali sono computazionalmente intrattabili, così i ricercatori si sono concentrati sulla progettazione a mano di meccanismi basati su bonus di esplorazione e ricompensa intrinseca, alcuni ispirati dal comportamento curioso nei sistemi naturali.  In questo lavoro, proponiamo una strategia per codificare gli algoritmi di curiosità come programmi in un linguaggio specifico del dominio e cercare, durante una fase di meta-apprendimento, gli algoritmi che permettono agli agenti RL di eseguire bene in nuovi domini.  Il nostro ricco linguaggio di programmi, che può combinare reti neurali con altri elementi costitutivi tra cui i moduli nearest-neighbor e può scegliere le proprie funzioni di perdita, permette l'espressione di programmi altamente generalizzabili che si comportano bene in domini così disparati come la navigazione a griglia con input di immagine, l'acrobata, il lander lunare, la formica e la tramoggia.   Per rendere questo approccio fattibile, sviluppiamo diverse tecniche di pruning, compreso l'apprendimento per prevedere il successo di un programma basato sulle sue proprietà sintattiche.   Dimostriamo l'efficacia dell'approccio empiricamente, trovando strategie di curiosità che sono simili a quelle della letteratura pubblicata, così come nuove strategie che sono competitive con esse e generalizzano bene.
Molti algoritmi di apprendimento automatico rappresentano i dati di input con embeddings vettoriali o codici discreti. Quando gli input presentano una struttura compositiva (ad esempio oggetti costruiti da parti o procedure da subroutine), è naturale chiedersi se questa struttura compositiva si rifletta nelle rappresentazioni apprese degli input. Descriviamo una procedura per valutare la composizionalità misurando quanto bene il vero modello che produce la rappresentazione può essere approssimato da un modello che compone esplicitamente una collezione di primitive rappresentazionali dedotte. Usiamo la procedura per fornire caratterizzazioni formali ed empiriche della struttura composizionale in una varietà di impostazioni, esplorando la relazione tra composizionalità e dinamiche di apprendimento, giudizi umani, similarità rappresentazionale e generalizzazione.
In questo articolo proponiamo un modello di deep learning end-to-end, chiamato E2Efold, per la previsione della struttura secondaria dell'RNA che può effettivamente tenere conto dei vincoli inerenti al problema. L'idea chiave di E2Efold è quella di prevedere direttamente la matrice di accoppiamento delle basi dell'RNA, e utilizzare un algoritmo di programmazione vincolata come un blocco di costruzione nell'architettura per far rispettare i vincoli. Con esperimenti completi su set di dati di riferimento, dimostriamo le prestazioni superiori di E2Efold: predice strutture significativamente migliori rispetto ai precedenti SOTA (29,7% di miglioramento in alcuni casi nei punteggi F1 e un miglioramento ancora maggiore per le strutture pseudoknotted) ed è efficiente quanto gli algoritmi più veloci in termini di tempo di inferenza.
L'apprendimento nelle reti neurali ricorrenti (RNN) è più spesso implementato dalla discesa del gradiente utilizzando la backpropagation through time (BPTT), ma la BPTT non modella accuratamente come il cervello impara.Invece, molti risultati sperimentali sulla plasticità sinaptica possono essere riassunti come regole di apprendimento a tre fattori che coinvolgono tracce di ammissibilità dell'attività neurale locale e un terzo fattore. Presentiamo qui la propagazione di ammissibilità (e-prop), una nuova fattorizzazione dei gradienti di perdita nelle RNN che si adatta al quadro delle regole di apprendimento a tre fattori quando derivate per i modelli biofisici di neuroni spiking.Quando testato sul benchmark di riconoscimento vocale TIMIT, è competitivo con BPTT sia per la formazione di reti LSTM artificiali che di RNN spiking.Ulteriori analisi suggeriscono che la diversità dei segnali di apprendimento e la considerazione delle lente dinamiche neurali interne sono decisive per l'efficienza di apprendimento di e-prop.
Le reti neurali ricorrenti (RNN) sono una rappresentazione efficace delle politiche di controllo per una vasta gamma di problemi di apprendimento per rinforzo e imitazione.Le politiche RNN, tuttavia, sono particolarmente difficili da spiegare, capire e analizzare a causa del loro uso di vettori di memoria a valore continuo e caratteristiche di osservazione.In questo articolo, introduciamo una nuova tecnica, Quantized Bottleneck Insertion, per imparare rappresentazioni finite di questi vettori e caratteristiche. Il risultato è una rappresentazione quantizzata della RNN che può essere analizzata per migliorare la nostra comprensione dell'uso della memoria e del comportamento generale. Presentiamo i risultati di questo approccio su ambienti sintetici e sei giochi Atari. Le rappresentazioni finite risultanti sono sorprendentemente piccole in alcuni casi, utilizzando solo 3 stati di memoria discreti e 10 osservazioni per una politica perfetta di Pong.
Sulla base del recente successo dei metodi di apprendimento di rinforzo profondo, indaghiamo la possibilità di migliorare l'apprendimento di rinforzo on-policy riutilizzando i dati di diverse politiche consecutive.I metodi on-policy portano molti benefici, come la capacità di valutare ogni politica risultante.Tuttavia, di solito scartano tutte le informazioni sulle politiche che esistevano prima.In questo lavoro, proponiamo un adattamento del concetto di replay buffer, preso in prestito dall'impostazione di apprendimento off-policy, agli algoritmi on-policy. Per raggiungere questo obiettivo, l'algoritmo proposto generalizza le funzioni Q-, valore e vantaggio per i dati di più politiche.Il metodo utilizza l'ottimizzazione della regione di fiducia, evitando alcuni dei problemi comuni degli algoritmi come TRPO o ACKTR: utilizza iperparametri per sostituire l'euristica di selezione della regione di fiducia, così come la matrice di covarianza allenabile invece di quella fissa. In molti casi, il metodo non solo migliora i risultati rispetto allo stato dell'arte degli algoritmi di apprendimento delle regioni di fiducia on-policy come ACKTR e TRPO, ma anche rispetto alla loro controparte off-policy DDPG.  
Le reti neurali convoluzionali (CNN) hanno avuto successo nell'elaborazione di segnali di dati che sono uniformemente campionati nel dominio spaziale (ad es, Tuttavia, la maggior parte dei segnali di dati non esistono nativamente su una griglia, e nel processo di campionamento su una griglia fisica uniforme soffrono di un significativo errore di aliasing e di perdita di informazioni. Inoltre, i segnali possono esistere in diverse strutture topologiche come, per esempio, punti, linee, superfici e volumi. A questo scopo, sviluppiamo formulazioni matematiche per Trasformate di Fourier non uniformi (NUFT) per campionare direttamente, e in modo ottimale, segnali di dati non uniformi di diverse topologie definite su una maglia simplex nel dominio spettrale senza errori di campionamento spaziale.La trasformazione spettrale viene eseguita nello spazio euclideo, che rimuove l'ambiguità di traduzione dai lavori sullo spettro del grafico.La nostra rappresentazione ha quattro vantaggi distinti: (1) il processo non causa alcun errore di campionamento spaziale durante il campionamento iniziale, (2) la generalità di questo approccio fornisce un quadro unificato per l'utilizzo di CNN per analizzare i segnali di topologie miste, (3) ci permette di sfruttare le architetture CNN backbone state-of-the-art per l'apprendimento efficace senza dover progettare una particolare architettura per una particolare struttura di dati in una moda ad-hoc, e (4) la rappresentazione permette mesh ponderate dove ogni elemento ha un peso diverso (cioè, Otteniamo buoni risultati alla pari con lo stato dell'arte per il compito di recupero della forma 3D, e un nuovo stato dell'arte per il compito di ricostruzione di nuvole di punti e superfici.
Scoprire la struttura causale tra un insieme di variabili è un problema fondamentale in molte scienze empiriche.I metodi tradizionali di scoperta casuale basati sul punteggio si basano su varie euristiche locali per cercare un Directed Acyclic Graph (DAG) secondo una funzione di punteggio predefinita.Mentre questi metodi, ad es, Motivati dai recenti progressi nell'ottimizzazione combinatoria neurale, proponiamo di utilizzare l'apprendimento per rinforzo (RL) per cercare il DAG con il miglior punteggio.Il nostro modello di codifica-decodifica prende i dati osservabili come input e genera matrici di adiacenza del grafico che vengono utilizzate per calcolare i premi. La ricompensa incorpora sia la funzione di punteggio predefinita che due termini di penalità per far rispettare l'aciclicità.In contrasto con le tipiche applicazioni RL in cui l'obiettivo è quello di imparare una politica, usiamo RL come una strategia di ricerca e il nostro output finale sarebbe il grafico, tra tutti i grafi generati durante la formazione, che raggiunge la migliore ricompensa.Conduciamo esperimenti sia su set di dati sintetici che reali, e dimostriamo che l'approccio proposto non solo ha una migliore capacità di ricerca ma permette anche una funzione di punteggio flessibile sotto il vincolo di aciclicità.
Per generare l'argomento più significativo che meglio rappresenta il documento dato, abbiamo proposto un metodo universale che può essere utilizzato nella fase di pre-elaborazione dei dati. Il metodo consiste di tre passi: in primo luogo, genera la coppia di parole da ogni singolo documento; in secondo luogo, applica un algoritmo parallelo TF-IDF a due vie alla coppia di parole per il filtraggio semantico; in terzo luogo, utilizza l'algoritmo k-means per unire le coppie di parole che hanno un significato semantico simile. Gli esperimenti sono condotti su Open Movie Database (OMDb), Reuters Dataset e 20NewsGroup Dataset e utilizzano il punteggio medio di precisione come metrica di valutazione. Confrontando i nostri risultati con altri modelli di argomento all'avanguardia, come la Latent Dirichlet allocation e le tradizionali Restricted Boltzmann Machines, la nostra proposta di pre-elaborazione dei dati può migliorare l'accuratezza dell'argomento generato fino al 12,99%.Come il numero di cluster e il numero di coppie di parole dovrebbe essere regolato per diversi tipi di documenti di testo è anche discusso.
L'apprendimento multi-task ha avuto successo nella modellazione di più compiti correlati con grandi set di dati etichettati, accuratamente curati.Facendo leva sulle relazioni tra i diversi compiti, il quadro di apprendimento multi-task può migliorare le prestazioni in modo significativo.Tuttavia, la maggior parte dei lavori esistenti sono sotto il presupposto che i compiti predefiniti sono correlati tra loro.Quindi, le loro applicazioni nel mondo reale sono limitate, perché rari problemi del mondo reale sono strettamente correlati.Inoltre, la comprensione delle relazioni tra i compiti è stata ignorata dalla maggior parte dei metodi attuali. Su questa linea, proponiamo un nuovo quadro di apprendimento multi-task - Learning To Transfer Via Modelling Multi-level Task Dependency, che ha costruito relazioni di dipendenza basate sull'attenzione tra diversi compiti.Allo stesso tempo, la relazione di dipendenza può essere utilizzata per guidare quale conoscenza dovrebbe essere trasferita, quindi le prestazioni del nostro modello sono anche migliorate.Per dimostrare l'efficacia del nostro modello e l'importanza di considerare la relazione di dipendenza multi-level, conduciamo esperimenti su diversi set di dati pubblici, sui quali otteniamo miglioramenti significativi rispetto ai metodi attuali.
 Progettiamo un test semplice e quantificabile dell'invarianza globale della traduzione nei modelli di deep learning addestrati sul dataset MNIST.Gli esperimenti sulle reti neurali convoluzionali e a capsule mostrano che entrambi i modelli hanno scarse prestazioni nel trattare l'invarianza globale della traduzione; tuttavia, le prestazioni sono migliorate utilizzando l'aumento dei dati.Anche se la rete a capsule è migliore sul dataset di test MNIST, la rete neurale convoluzionale ha generalmente migliori prestazioni sull'invarianza della traduzione.
I processi gaussiani sono onnipresenti in natura e in ingegneria. Un caso esemplare è una classe di reti neurali nel limite di larghezza infinita, i cui priori corrispondono a processi gaussiani. Qui estendiamo perturbativamente questa corrispondenza alle reti neurali di larghezza finita, producendo processi non gaussiani come priori. La metodologia qui sviluppata ci permette di tracciare il flusso delle distribuzioni di preattivazione integrando progressivamente le variabili casuali dagli strati più bassi a quelli più alti, ricordando il flusso del gruppo di rinormalizzazione.sviluppiamo ulteriormente una prescrizione perturbativa per eseguire l'inferenza bayesiana con priori debolmente non gaussiani.
La distillazione è un metodo per trasferire la conoscenza da un modello all'altro e spesso raggiunge una maggiore accuratezza con la stessa capacità.In questo articolo, miriamo a fornire una comprensione teorica su ciò che principalmente aiuta la distillazione.La nostra risposta è "l'arresto precoce".Assumendo che la rete insegnante sia iperparametrizzata, sosteniamo che la rete insegnante stia essenzialmente raccogliendo conoscenza oscura dai dati tramite l'arresto precoce. Questo può essere giustificato da un nuovo concetto, Anisotropic In- formation Retrieval (AIR), che significa che la rete neurale tende ad adattare le informazioni informative prima e le informazioni non informative (incluso il rumore) dopo.Motivati dai recenti sviluppi sull'analisi teorica delle reti neurali iperparametrizzate, possiamo caratterizzare AIR attraverso l'eigenspace del Neural Tangent Kernel(NTK).AIR facilita una nuova comprensione della distillazione. Proponiamo un algoritmo di autodistillazione per distillare sequenzialmente la conoscenza dalla rete nell'epoca di addestramento precedente per evitare di memorizzare le etichette sbagliate, e dimostriamo, sia teoricamente che empiricamente, che l'autodistillazione può beneficiare di più del semplice arresto anticipato. Teoricamente, dimostriamo la convergenza dell'algoritmo proposto alle etichette di verità al suolo per reti neurali sovraparametrate inizializzate a caso in termini di distanza l2, mentre il risultato precedente era sulla convergenza in perdita 0-1.Il risultato teorico assicura che la rete neurale appresa goda di un margine sui dati di formazione che porta a una migliore generalizzazione.Empiricamente, otteniamo una migliore accuratezza di test ed evitiamo completamente l'arresto anticipato che rende l'algoritmo più user-friendly.
L'apprendimento permanente pone notevoli sfide in termini di efficacia (minimizzando gli errori di previsione per tutti i compiti) e la trattabilità computazionale complessiva per le prestazioni in tempo reale.  Questo articolo affronta l'apprendimento multitask continuo per tutta la vita ristimando congiuntamente le relazioni inter-task (kernel \textit{output}) e i parametri del modello per-task ad ogni round, assumendo che i dati arrivino in modo streaming.proponiamo un nuovo algoritmo chiamato \textit{Online Output Kernel Learning Algorithm} (Per evitare l'esplosione della memoria, proponiamo una robusta versione a budget limitato dell'algoritmo proposto che utilizza in modo efficiente la relazione tra i compiti per limitare il numero totale di esempi rappresentativi nel set di supporto.  I nostri risultati empirici su tre set di dati indicano prestazioni AUC superiori per OOKLA e i suoi cugini con budget limitato rispetto alle linee di base forti.
Minecraft è un videogioco che offre molte sfide interessanti per i sistemi AI.In questo articolo, ci concentriamo in scenari di costruzione in cui un agente deve costruire una struttura complessa fatta di singoli blocchi.Poiché gli oggetti di livello superiore sono formati da oggetti di livello inferiore, la costruzione può essere naturalmente modellata come una rete gerarchica di compiti.Modelliamo uno scenario di costruzione di una casa in pianificazione classica e HTN e confrontiamo i vantaggi e gli svantaggi di entrambi i tipi di modelli.
Gli attacchi ai modelli di linguaggio naturale sono difficili da confrontare a causa delle loro diverse definizioni di ciò che costituisce un attacco riuscito.Presentiamo una tassonomia di vincoli per categorizzare questi attacchi.Per ogni vincolo, presentiamo un caso d'uso del mondo reale e un modo per misurare quanto bene i campioni generati fanno rispettare il vincolo. Utilizziamo quindi la nostra struttura per valutare due attacchi allo stato dell'arte che ingannano i modelli con la sostituzione dei sinonimi. Questi attacchi affermano che le loro perturbazioni avversarie preservano la semantica e la correttezza sintattica degli input, ma la nostra analisi mostra che questi vincoli non sono fortemente applicati. Per una porzione significativa di questi esempi avversari, un controllore grammaticale rileva un aumento degli errori.Inoltre, gli studi umani indicano che molti di questi esempi avversari divergono nel significato semantico dall'input o non sembrano essere scritti dall'uomo.Infine, evidenziamo la necessità di una valutazione standardizzata degli attacchi che condividono i vincoli.Senza metriche di valutazione condivise, spetta ai ricercatori impostare le soglie che determinano il trade-off tra qualità dell'attacco e successo dell'attacco.Consigliamo studi umani ben progettati per determinare la soglia migliore per approssimare il giudizio umano.
Affinché l'apprendimento automatico sia implementato e fidato in molte applicazioni, è fondamentale essere in grado di spiegare in modo affidabile perché l'algoritmo di apprendimento automatico fa certe previsioni.Per esempio, se un algoritmo classifica una data immagine patologica come un tumore maligno, allora il medico potrebbe aver bisogno di sapere quali parti dell'immagine hanno portato l'algoritmo a questa classificazione.Come interpretare i predittori black-box è quindi un'importante e attiva area di ricerca.  Una domanda fondamentale è: quanto possiamo fidarci dell'interpretazione stessa? In questo articolo, mostriamo che l'interpretazione delle previsioni di deep learning è estremamente fragile nel seguente senso: a due input percettivamente indistinguibili con la stessa etichetta prevista possono essere assegnate interpretazioni molto diverse. Caratterizziamo sistematicamente la fragilità delle interpretazioni generate da diversi metodi di interpretazione dell'importanza delle caratteristiche ampiamente utilizzati (mappe di salienza, gradiente integrato e DeepLIFT) su ImageNet e CIFAR-10. I nostri esperimenti mostrano che anche piccole perturbazioni casuali possono cambiare l'importanza delle caratteristiche e nuove perturbazioni sistematiche possono portare a interpretazioni drammaticamente diverse senza cambiare l'etichetta. Estendiamo questi risultati per mostrare che le interpretazioni basate sugli esemplari (per esempio le funzioni di influenza) sono similmente fragili. La nostra analisi della geometria della matrice Hessiana fornisce informazioni sul perché la fragilità potrebbe essere una sfida fondamentale per gli attuali approcci di interpretazione.
Tuttavia, i lavori esistenti sono limitati alla massimizzazione stocastica dell'AUC con un modello predittivo lineare, che limita il suo potere predittivo quando si tratta di dati estremamente complessi. In questo articolo, consideriamo il problema della massimizzazione stocastica dell'AUC con una rete neurale profonda come modello predittivo, basandoci sulla riformulazione del punto di sella di una perdita surrogata dell'AUC, il problema può essere trasformato in un problema min-max non convesso. In particolare, proponiamo di esplorare la condizione di Polyak-L-L'ojasiewicz (PL) che è stata dimostrata e osservata nell'apprendimento profondo, che ci permette di sviluppare nuovi algoritmi stocastici con un tasso di convergenza ancora più veloce e uno schema di step size più pratico.Un algoritmo in stile AdaGrad è anche analizzato sotto la condizione PL con tasso di convergenza adattivo.I nostri risultati sperimentali dimostrano l'efficacia degli algoritmi proposti.
Progettare ricompense per il Reinforcement Learning (RL) è impegnativo perché deve trasmettere il compito desiderato, essere efficiente da ottimizzare, ed essere facile da calcolare.Quest'ultimo è particolarmente problematico quando si applica la RL alla robotica, dove rilevare se la configurazione desiderata viene raggiunta potrebbe richiedere una notevole supervisione e strumentazione.Inoltre, siamo spesso interessati ad essere in grado di raggiungere una vasta gamma di configurazioni, quindi impostare una ricompensa diversa ogni volta potrebbe essere poco pratico.Metodi come Hindsight Experience Replay (HER) hanno recentemente mostrato la promessa di imparare politiche capaci di raggiungere molti obiettivi, senza la necessità di una ricompensa. Sfortunatamente, senza trucchi come il ripristino di punti lungo la traiettoria, HER potrebbe impiegare molto tempo per scoprire come raggiungere certe aree dello spazio di stato.In questo lavoro studiamo diversi approcci per incorporare dimostrazioni per accelerare drasticamente la convergenza verso una politica in grado di raggiungere qualsiasi obiettivo, superando anche le prestazioni di un agente addestrato con altri algoritmi di Imitation Learning.Inoltre, il nostro metodo può essere utilizzato quando sono disponibili solo traiettorie senza azioni esperte, che possono sfruttare la dimostrazione cinestetica o in terza persona.
Le reti neurali bayesiane, che utilizzano sia la funzione di perdita di log-likelihood negativo che la media delle loro previsioni utilizzando un posteriore appreso sui parametri, sono state utilizzate con successo in molti campi scientifici, in parte grazie alla loro capacità di estrarre "senza sforzo" le rappresentazioni desiderate da molti set di dati su larga scala. In questo articolo, presentiamo un nuovo limite di generalizzazione PAC-Bayesiano per la perdita negativa di log-likelihood che utilizza l'argomentazione di Herbst per la disuguaglianza log-Sobolev per delimitare la funzione generatrice di momenti del rischio degli studenti.
Le tecniche di aumento dei dati, ad esempio il flipping o il cropping, che allargano sistematicamente il set di dati di allenamento generando esplicitamente più campioni di allenamento, sono efficaci per migliorare le prestazioni di generalizzazione delle reti neurali profonde.Nell'impostazione supervisionata, una pratica comune per l'aumento dei dati è quella di assegnare la stessa etichetta a tutti i campioni aumentati della stessa fonte.Tuttavia, se l'aumento comporta una grande discrepanza distributiva tra loro (ad es, Per affrontare questa sfida, suggeriamo un'idea semplice ma efficace di imparare la distribuzione congiunta delle etichette originali e auto-supervisionate dei campioni aumentati. Il quadro di apprendimento congiunto è più facile da addestrare e consente un'inferenza aggregata che combina le previsioni di diversi campioni aumentati per migliorare le prestazioni. Inoltre, per accelerare il processo di aggregazione, proponiamo anche una tecnica di trasferimento della conoscenza, l'autodistillazione, che trasferisce la conoscenza dell'aumento nel modello stesso.
Le reti di memoria a breve termine (LSTM) permettono di esibire un comportamento dinamico temporale con connessioni di feedback e sembrano una scelta naturale per l'apprendimento di sequenze di mesh 3D. Per bypassare la complicazione dell'uso di mesh 3D, trasformiamo le sequenze di mesh di superficie in descrittori spettrali che codificano efficientemente la forma.Un'architettura di rete basata su due rami LSTM è scelta per imparare le rappresentazioni e le dinamiche dell'incidente durante la simulazione.L'architettura è basata sulla predizione video non supervisionata da un LSTM senza alcun livello di convoluzione. Utilizza un encoder LSTM per mappare una sequenza di input in una rappresentazione vettoriale di lunghezza fissa; su questa rappresentazione un decoder LSTM esegue la ricostruzione della sequenza di input, mentre l'altro decoder LSTM predice il comportamento futuro ricevendo i passi iniziali della sequenza come seed. Il comportamento dell'errore spazio-temporale del modello viene analizzato per studiare quanto bene il modello può estrapolare i descrittori spettrali appresi nel futuro, cioè quanto bene ha imparato a rappresentare la meccanica strutturale dinamica sottostante.Considerando che sono disponibili solo pochi esempi di allenamento, che è il caso tipico delle simulazioni numeriche, la rete si comporta molto bene.
Lo scopo di un modello di codifica è quello di prevedere l'attività cerebrale dato uno stimolo.In questo contributo, tentiamo di stimare un modello di codifica dell'intero cervello della percezione uditiva in un contesto di stimolazione naturalistica.Analizziamo i dati di un set di dati aperto, in cui 16 soggetti guardavano un breve filmato mentre la loro attività cerebrale veniva misurata con la risonanza magnetica funzionale. Abbiamo estratto i vettori di caratteristiche allineati con i tempi dell'audio dal film, a diversi livelli di una rete neurale profonda preaddestrata sulla classificazione delle scene uditive.I dati fMRI sono stati parcellizzati utilizzando il clustering gerarchico su 500 pacchi, e i modelli di codifica sono stati stimati utilizzando una rete neurale completamente connessa con uno strato nascosto, addestrato a prevedere i segnali per ogni pacco dalle caratteristiche DNN. I modelli di codifica individuali sono stati addestrati con successo e hanno previsto l'attività cerebrale su dati non visti, in parcelle situate nel lobo temporale superiore, così come le regioni prefrontali dorsolaterali, che di solito sono considerate come aree coinvolte nell'elaborazione uditiva e linguistica. Nel complesso, questo contributo estende i tentativi precedenti sulla stima dei modelli di codifica, mostrando la capacità di modellare l'attività cerebrale utilizzando una DNN generica (cioè non specificamente addestrata per questo scopo) per estrarre le caratteristiche uditive, suggerendo un grado di somiglianza tra rappresentazioni DNN interne e attività cerebrale in ambienti naturali.
In questo articolo, descriviamo l'"autoencoder implicito" (IAE), un autoencoder generativo in cui sia il percorso generativo che il percorso di riconoscimento sono parametrizzati da distribuzioni implicite. Usiamo due reti generative avversarie per definire la ricostruzione e le funzioni di costo di regolarizzazione dell'autoencoder implicito, e deriviamo le regole di apprendimento basate sull'apprendimento della massima verosimiglianza. L'apprendimento di una distribuzione di verosimiglianza condizionale espressiva permette al codice latente di catturare solo l'informazione astratta e di alto livello dei dati, mentre l'informazione rimanente è catturata dalla distribuzione di verosimiglianza condizionale implicita.Per esempio, mostriamo che gli autocodificatori impliciti possono distinguere l'informazione globale e locale, ed eseguire ricostruzioni deterministiche o stocastiche delle immagini.Mostriamo inoltre che gli autocodificatori impliciti possono distinguere i fattori discreti di variazione sottostante dai fattori continui in modo non supervisionato, ed eseguire il clustering e l'apprendimento semi-supervisionato.
I bambini usano il bias di mutua esclusività (ME) per aiutare a disambiguare come le parole mappano i referenti, assumendo che se un oggetto ha un'etichetta allora non ha bisogno di un'altra. Inoltre, dimostriamo che i loro bias induttivi sono poco adatti alle formulazioni di apprendimento permanente di classificazione e traduzione.Dimostriamo che c'è un caso convincente per la progettazione di reti neurali che ragionano per esclusività reciproca, che rimane una sfida aperta.
I neuroni corticali elaborano e integrano le informazioni su più scale temporali; inoltre, queste scale temporali o campi recettivi temporali mostrano un'organizzazione funzionale e gerarchica; per esempio, aree importanti per la memoria di lavoro (WM), come la corteccia prefrontale, utilizzano neuroni con campi recettivi temporali stabili e lunghe scale temporali per sostenere rappresentazioni affidabili degli stimoli. Nonostante i recenti progressi nelle tecniche sperimentali, i meccanismi sottostanti per l'emergere di scale temporali neuronali abbastanza lunghe da supportare la WM sono poco chiari e impegnativi da indagare sperimentalmente. Qui, dimostriamo che le reti neurali ricorrenti spiking (RNNs) progettate per eseguire un compito WM riproducono i risultati sperimentali precedentemente osservati e che questi modelli potrebbero essere utilizzati in futuro per studiare come emergono le scale temporali neuronali specifiche per WM.
I classificatori convenzionali di apprendimento profondo sono statici nel senso che sono addestrati su un set predefinito di classi e l'apprendimento per classificare una nuova classe richiede tipicamente il ri-allenamento.In questo lavoro, affrontiamo il problema del Low-shot network-expansionlearning. Introduciamo un quadro di apprendimento che consente di espandere una rete profonda pre-addestrata (base) per classificare nuove classi quando il numero di esempi per le nuove classi è particolarmente piccolo. Presentiamo un metodo di distillazione semplice ma potente in cui la rete base viene aumentata con pesi aggiuntivi per classificare le nuove classi, mantenendo invariati i pesi della rete base. Abbiamo definito questa distillazione dura, poiché conserviamo la risposta della rete sulle vecchie classi per essere uguale sia nella rete di base che in quella espansa, e poiché solo un piccolo numero di pesi deve essere addestrato, la distillazione dura eccelle per scenari di addestramento a basso costo. Infine, mostriamo che l'espansione della rete a basso numero di colpi può essere fatta con un'impronta di memoria molto piccola utilizzando un modello generativo compatto dei dati di formazione delle classi di base con una degradazione trascurabile rispetto all'apprendimento con il set di formazione completo.
Le persone fanno domande che sono molto più ricche, più informative e più creative degli attuali sistemi AI.Proponiamo un quadro di generazione di programmi neurali per la modellazione delle domande umane, che rappresenta le domande come programmi formali e genera programmi con una rete neurale profonda basata su encoder-decoder.Da ampi esperimenti utilizzando un gioco di ricerca di informazioni, dimostriamo che il nostro metodo può porre domande ottimali in ambienti sintetici e prevedere quali domande gli umani sono propensi a porre in ambienti non vincolati.Proponiamo anche un nuovo quadro di generazione di domande basato su grammatica addestrato con apprendimento di rinforzo, che è in grado di generare domande creative senza dati supervisionati.
La classificazione delle immagini scattate in ambienti di imaging speciali, ad eccezione dell'aria, è la prima sfida nell'estendere le applicazioni del deep learning.Segnaliamo una UW-Net (Underwater Network), una nuova rete neurale convoluzionale (CNN) basata su una rete per la classificazione delle immagini subacquee.In questo modello, simuliamo la correlazione visiva dell'attenzione dello sfondo con la comprensione dell'immagine per ambienti speciali, come la nebbia e sott'acqua costruendo un modulo di inception-attention (I-A).I risultati sperimentali dimostrano che la proposta UW-Net raggiunge una precisione del 99. I risultati sperimentali dimostrano che la rete UW proposta raggiunge un'accuratezza del 99,3% nella classificazione delle immagini subacquee, che è significativamente migliore di altre reti di classificazione delle immagini, come AlexNet, InceptionV3, ResNet e Se-ResNet.Inoltre, dimostriamo che il modulo IA proposto può essere utilizzato per aumentare le prestazioni delle reti di riconoscimento degli oggetti esistenti.Sostituendo il modulo di inception con il modulo I-A, la rete Inception-ResnetV2 raggiunge un tasso di errore top1 del 10,7% e un tasso di errore top5 dello 0% sul sottoinsieme di ILSVRC-2012, che illustra ulteriormente la funzione dell'attenzione di fondo nelle classificazioni delle immagini.
Le reti profonde hanno raggiunto risultati impressionanti in una varietà di compiti importanti. Tuttavia, una nota debolezza è l'incapacità di ottenere buone prestazioni quando vengono valutate su dati che differiscono dalla distribuzione di formazione, anche se queste differenze sono molto piccole, come nel caso degli esempi avversari.  Proponiamo \emph{Fortified Networks}, una semplice trasformazione di reti esistenti, che "fortifica" gli strati nascosti in una rete profonda identificando quando gli stati nascosti sono fuori dal manifold dei dati, e rimappa questi stati nascosti in parti del manifold dei dati dove la rete funziona bene. Il nostro contributo principale è mostrare che fortificare questi stati nascosti migliora la robustezza delle reti profonde e i nostri esperimenti (i) dimostrano una migliore robustezza agli attacchi avversari standard in entrambi i modelli di minaccia black-box e white-box; (ii) suggeriscono che i nostri miglioramenti non sono principalmente dovuti al problema dei risultati ingannevolmente buoni dovuti alla qualità degradata nel segnale del gradiente (il problema del gradient masking) e (iii) mostrano il vantaggio di fare questa fortificazione negli strati nascosti invece che nello spazio di input.  Dimostriamo miglioramenti nella robustezza avversaria su tre dataset (MNIST, Fashion MNIST, CIFAR10), attraverso diversi parametri di attacco, sia white-box che black-box, e gli attacchi più ampiamente studiati (FGSM, PGD, Carlini-Wagner).  Mostriamo che questi miglioramenti sono ottenuti attraverso un'ampia varietà di iperparametri.  
Le reti neurali potrebbero classificare male gli input che sono leggermente diversi dai loro dati di allenamento, il che indica un piccolo margine tra i loro confini decisionali e il dataset di allenamento.In questo lavoro, studiamo la classificazione binaria di dataset linearmente separabili e mostriamo che i classificatori lineari potrebbero anche avere confini decisionali che si trovano vicino al loro dataset di allenamento se la perdita di cross-entropia viene utilizzata per l'allenamento. In particolare, mostriamo che se le caratteristiche del dataset di allenamento giacciono in un sottospazio affine a bassa dimensione e la perdita di cross-entropia viene minimizzata utilizzando un metodo a gradiente, il margine tra i punti di allenamento e il confine decisionale potrebbe essere molto più piccolo del valore ottimale.Questo risultato è contrario alle conclusioni di recenti lavori correlati come (Soudry et al, 2018), e identifichiamo la ragione di questa contraddizione.Per migliorare il margine, introduciamo l'addestramento differenziale, che è un paradigma di addestramento che utilizza una funzione di perdita definita su coppie di punti di ogni classe.Mostriamo che il confine decisionale di un classificatore lineare addestrato con l'addestramento differenziale raggiunge effettivamente il margine massimo.I risultati rivelano l'uso della perdita di cross-entropia come uno dei colpevoli nascosti degli esempi avversi e introduce una nuova direzione per rendere le reti neurali robuste contro di loro.
I concetti di matrici di evoluzione unitaria e di memoria associativa hanno spinto il campo delle reti neurali ricorrenti (RNN) a prestazioni all'avanguardia in una varietà di compiti sequenziali.  Tuttavia, le RNN hanno ancora una capacità limitata di manipolare la memoria a lungo termine.  Per aggirare questa debolezza, le applicazioni di maggior successo di RNN utilizzano tecniche esterne come i meccanismi di attenzione. In questo articolo proponiamo un nuovo modello RNN che unifica gli approcci allo stato dell'arte: Il nucleo di RUM è la sua operazione di rotazione, che è, naturalmente, una matrice unitaria, che fornisce alle architetture il potere di imparare le dipendenze a lungo termine superando il problema dei gradienti che svaniscono ed esplodono.  Inoltre, l'unità di rotazione serve anche come memoria associativa. Valutiamo il nostro modello su compiti di memorizzazione sintetica, di risposta alle domande e di modellazione del linguaggio.   RUM apprende completamente il compito Copying Memory e migliora il risultato dello stato dell'arte nel compito Recall.  La performance di RUM nel compito di risposta alle domande bAbI è paragonabile a quella dei modelli con meccanismo di attenzione.Miglioriamo anche il risultato dello stato dell'arte a 1.189 bit per carattere (BPC) di perdita nel compito di Penn Treebank (PTB) a livello di carattere, che sta a significare le applicazioni di RUM ai dati sequenziali del mondo reale.L'universalità della nostra costruzione, al centro di RNN, stabilisce RUM come un approccio promettente alla modellazione del linguaggio, al riconoscimento vocale e alla traduzione automatica.
Mentre molti recenti progressi nell'apprendimento di rinforzo profondo si basano su metodi senza modello, gli approcci basati sul modello rimangono una prospettiva allettante per il loro potenziale di sfruttare i dati non supervisionati per imparare le dinamiche dell'ambiente.Una prospettiva è quella di perseguire approcci ibridi, come in AlphaGo, che combina Monte-Carlo Tree Search (MCTS) - un metodo basato sul modello - con reti Q profonde (DQNs) - un metodo senza modello. MCTS richiede la generazione di rollout, che è computazionalmente costoso.In questo articolo, proponiamo di simulare i rollout, sfruttando le ultime scoperte nella trasduzione immagine-immagine, vale a dire Pix2Pix GANs, per prevedere le dinamiche dell'ambiente.Il nostro algoritmo proposto, generative adversarial tree search (GATS), simula i rollout fino ad una profondità specificata utilizzando sia un modello dinamico basato su GAN che un predittore di ricompensa. GATS impiega MCTS per la pianificazione sui campioni simulati e usa DQN per stimare la funzione Q agli stati foglia.La nostra analisi teorica stabilisce alcune proprietà favorevoli di GATS rispetto al trade-off bias-varianza e i risultati empirici mostrano che su 5 popolari giochi Atari, la dinamica e i predittori di ricompensa convergono rapidamente verso soluzioni accurate. Tuttavia, GATS non riesce a superare DQNs in 4 giochi su 5. In particolare, in questi esperimenti, MCTS ha solo brevi rollout (fino alla profondità dell'albero 4), mentre i precedenti successi di MCTS hanno coinvolto la profondità dell'albero in centinaia.
Presentiamo una nuova architettura di GAN per l'apprendimento di una rappresentazione disentangled.La nuova architettura del modello è ispirata dalla teoria Information Bottleneck (IB) e quindi chiamata IB-GAN.L'obiettivo di IB-GAN è simile a quello di InfoGAN ma ha una differenza cruciale; viene adottata una regolarizzazione di capacità per l'informazione reciproca, grazie alla quale il generatore di IB-GAN può sfruttare una rappresentazione latente in modo disentangled e interpretabile. Con esperimenti su CelebA, 3DChairs, e dataset dSprites, dimostriamo che la qualità visiva dei campioni generati dalla IB-GAN è spesso migliore di quelli generati da β-VAEs.Inoltre, la IB-GAN raggiunge un punteggio molto più alto di disentanglement metrics rispetto a β-VAEs o InfoGAN sul dataset dSprites.
Indaghiamo l'addestramento e le prestazioni delle reti generative avversarie utilizzando la Maximum Mean Discrepancy (MMD) come critica, denominata MMD GANs.Come nostro principale contributo teorico, chiariamo la situazione con bias nelle funzioni di perdita GAN sollevata da lavori recenti: mostriamo che gli stimatori del gradiente utilizzati nel processo di ottimizzazione sia per MMD GANs che per Wasserstein GANs sono imparziali, ma imparare un discriminatore basato sui campioni porta a gradienti biased per i parametri del generatore. Discutiamo anche la questione della scelta del kernel per il critico MMD, e caratterizziamo il kernel corrispondente alla distanza energetica utilizzato per il critico Cramér GAN.Essendo una metrica di probabilità integrale, il MMD beneficia delle strategie di addestramento recentemente sviluppate per le GAN di Wasserstein. Negli esperimenti, la MMD GAN è in grado di impiegare una rete critica più piccola della Wasserstein GAN, risultando in un algoritmo di addestramento più semplice e veloce con prestazioni corrispondenti.Proponiamo anche una misura migliorata della convergenza GAN, la Kernel Inception Distance, e mostriamo come utilizzarla per adattare dinamicamente i tassi di apprendimento durante l'addestramento GAN.
Estendiamo la struttura di Consensus Network alla Transductive Consensus Network (TCN), una struttura di classificazione multimodale semi-supervisionata, e identifichiamo i suoi due meccanismi: consenso e classificazione.Proponendo tre varianti come studi di ablazione, dimostriamo che entrambi i meccanismi dovrebbero funzionare insieme.Nel complesso, le TCN superano o si allineano con i migliori algoritmi di riferimento quando sono disponibili solo da 20 a 200 punti dati etichettati.
La separazione di distribuzioni miste è una sfida di lunga data per l'apprendimento automatico e l'elaborazione dei segnali.Le applicazioni includono: separazione di più altoparlanti a canale singolo (problema del cocktail party), separazione della voce cantata e separazione dei riflessi dalle immagini.La maggior parte dei metodi attuali si basa su ipotesi forti sulle distribuzioni di origine (ad esempio, sparsità, rango basso, ripetitività) o si basa sull'avere campioni di formazione di ogni fonte nella miscela.In questo lavoro, affrontiamo lo scenario di estrazione di una distribuzione non osservata mescolata additivamente con un segnale da una distribuzione osservata (arbitraria).Introduciamo un nuovo metodo: Neural Egg Separation - un metodo iterativo che impara a separare la distribuzione nota da stime progressivamente più fini della distribuzione sconosciuta.In alcune impostazioni, Neural Egg Separation è sensibile all'inizializzazione, quindi introduciamo GLO Masking che assicura una buona inizializzazione.Esperimenti estesi mostrano che il nostro metodo supera i metodi attuali che utilizzano lo stesso livello di supervisione e spesso raggiunge prestazioni simili alla supervisione completa.
Nella salute, l'apprendimento automatico è sempre più comune, ma l'apprendimento delle reti neurali di embedding (rappresentazione) è probabilmente sottoutilizzato per i segnali fisiologici.  Questa inadeguatezza spicca in netto contrasto con domini informatici più tradizionali, come la computer vision (CV) e l'elaborazione del linguaggio naturale (NLP).  Per i segnali fisiologici, l'apprendimento di feature embeddings è una soluzione naturale all'insufficienza di dati causata dalle preoccupazioni sulla privacy dei pazienti - piuttosto che condividere i dati, i ricercatori possono condividere modelli informativi di embedding (cioè, modelli di rappresentazione), che mappano i dati dei pazienti in un output embedding.   Qui, presentiamo la struttura PHASE (PHysiologicAl Signal Embeddings), che consiste di tre componenti: i) apprendimento di embeddings di reti neurali di segnali fisiologici, ii) previsione dei risultati basata sull'embedding appreso, e iii) interpretazione dei risultati di previsione stimando le attribuzioni delle caratteristiche nei modelli "impilati" (cioè, modello di embedding delle caratteristiche seguito dal modello di previsione).  PHASE è nuovo in tre modi: 1) A nostra conoscenza, PHASE è il primo esempio di trasferimento di reti neurali per creare embeddings del segnale fisiologico.2) Presentiamo un metodo praticabile per ottenere attribuzioni di caratteristiche attraverso modelli impilati.  Dimostriamo che le nostre attribuzioni di modelli impilati possono approssimare i valori di Shapley - attribuzioni note per avere proprietà desiderabili - per insiemi arbitrari di modelli.3) PHASE è stato ampiamente testato in un ambiente cross-ospedaliero che include dati pubblicamente disponibili.  Nei nostri esperimenti, mostriamo che PHASE supera significativamente le incorporazioni alternative -- come grezzo, media mobile esponenziale/varianza, e autoencoder -- attualmente in uso.Inoltre, forniamo la prova che il trasferimento degli allievi di incorporamento/rappresentazione della rete neurale fra gli ospedali distinti ancora produce le incorporazioni performanti ed offre le raccomandazioni quando il trasferimento Ã¨ inefficace.
Consideriamo il problema dell'apprendimento del dizionario, dove lo scopo è quello di modellare i dati dati dati come una combinazione lineare di alcune colonne di una matrice nota come dizionario, dove i pesi sparsi che formano la combinazione lineare sono noti come coefficienti.Poiché il dizionario e i coefficienti, che parametrizzano il modello lineare sono sconosciuti, l'ottimizzazione corrispondente è intrinsecamente non convessa. Questa è stata una grande sfida fino a poco tempo fa, quando sono stati proposti algoritmi dimostrabili per l'apprendimento del dizionario, ma questi forniscono garanzie solo sul recupero del dizionario, senza garanzie esplicite di recupero sui coefficienti; inoltre, qualsiasi errore di stima nel dizionario ha un impatto negativo sulla capacità di localizzare e stimare con successo i coefficienti. Questo limita potenzialmente l'utilità degli attuali metodi di apprendimento del dizionario dimostrabile in applicazioni in cui il recupero dei coefficienti è di interesse.A tal fine, sviluppiamo NOODL: un semplice algoritmo di apprendimento del dizionario online basato sull'ottimizzazione neuralmente plausibile, che recupera sia il dizionario che i coefficienti esattamente ad un tasso geometrico, quando inizializzato in modo appropriato. Il nostro algoritmo, NOODL, è anche scalabile e adatto a implementazioni distribuite su larga scala in architetture neurali, il che significa che coinvolge solo semplici operazioni lineari e non lineari. Infine, corroboriamo questi risultati teorici attraverso la valutazione sperimentale dell'algoritmo proposto con le attuali tecniche allo stato dell'arte.
In questo lavoro, presentiamo Data Augmented Relation Extraction (DARE), un metodo semplice per aumentare i dati di addestramento, mettendo a punto GPT2 per generare esempi per specifici tipi di relazioni. In una serie di esperimenti mostriamo i vantaggi del nostro metodo, che porta a miglioramenti fino a 11 punti di punteggio F1 rispetto a una linea di base forte. Inoltre, DARE raggiunge un nuovo stato dell'arte in tre dataset RE biomedici ampiamente utilizzati, superando i migliori risultati precedenti di 4,7 punti F1 in media.
Questo articolo affronta il problema di rappresentare la credenza di un sistema usando distribuzioni normali multivariate (MND) quando il modello sottostante è basato su una rete neurale profonda (DNN).La sfida principale con le DNN è la complessità computazionale che è necessaria per ottenere l'incertezza del modello usando le MND.Per ottenere un metodo scalabile, proponiamo un nuovo approccio che esprime il parametro posteriore in forma di informazioni sparse. Il nostro algoritmo di inferenza si basa su un nuovo schema di approssimazione di Laplace, che implica una correzione diagonale dell'eigenbasis fattore Kronecker. Poiché questo rende l'inversione della matrice di informazione intrattabile - un'operazione necessaria per l'analisi bayesiana completa, mettiamo a punto un'approssimazione a basso rango di questa eigenbasis e uno schema di campionamento efficiente in termini di memoria.Forniamo sia un'analisi teorica che una valutazione empirica su vari set di dati di riferimento, dimostrando la superiorità del nostro approccio rispetto ai metodi esistenti.
La dimensione sempre crescente dei moderni set di dati combinata con la difficoltà di ottenere informazioni sulle etichette ha reso l'apprendimento semi-supervisionato di significativa importanza pratica nelle moderne applicazioni di apprendimento automatico.Rispetto all'apprendimento supervisionato, la difficoltà chiave nell'apprendimento semi-supervisionato è come fare pieno uso dei dati senza etichetta.Al fine di utilizzare le molteplici informazioni fornite dai dati senza etichetta, proponiamo una nuova regolarizzazione chiamata la regolarizzazione avversaria tangente-normale, che è composta da due parti. Una è applicata lungo lo spazio tangente del manifold dei dati, con lo scopo di rafforzare l'invarianza locale del classificatore sul manifold, mentre l'altra è eseguita sullo spazio normale ortogonale allo spazio tangente, con lo scopo di imporre robustezza al classificatore contro il rumore che causa la deviazione dei dati osservati dal manifold di dati sottostante.  Entrambi i regolatori sono raggiunti dalla strategia di addestramento virtuale avversario. Il nostro metodo ha raggiunto prestazioni allo stato dell'arte su compiti di apprendimento semi-supervisionato sia su set di dati artificiali che su set di dati pratici.
La proprietà di approssimazione universale delle reti neurali è una delle motivazioni per utilizzare questi modelli in vari problemi del mondo reale.Tuttavia, questa proprietà non è l'unica caratteristica che rende le reti neurali uniche in quanto vi è una vasta gamma di altri approcci con proprietà simili.Un'altra caratteristica che rende questi modelli interessanti è che possono essere addestrati con l'algoritmo di backpropagation che permette un efficiente calcolo del gradiente e dà a questi approssimatori universali la capacità di apprendere in modo efficiente collettori complessi da una grande quantità di dati in diversi domini. Nonostante il loro uso abbondante nella pratica, le reti neurali non sono ancora ben comprese e una vasta gamma di ricerche in corso è quella di studiare l'interpretabilità delle reti neurali.D'altra parte, l'analisi topologica dei dati (TDA) si basa su un forte quadro teorico della topologia (algebrica) insieme ad altri strumenti matematici per analizzare set di dati eventualmente complessi. In questo lavoro, facciamo leva su un teorema di approssimazione universale originato dalla topologia algebrica per costruire una connessione tra la TDA e il comune quadro di formazione delle reti neurali, introducendo la nozione di suddivisione automatica e ideando un particolare tipo di reti neurali per compiti di regressione: L'architettura delle SCN è definita con un insieme di funzioni di bias insieme a una particolare politica durante il passaggio in avanti che altera il comune quadro di ricerca dell'architettura nelle reti neurali.Crediamo che la visione delle SCN possa essere usata come un passo verso la costruzione di modelli di apprendimento profondo interpretabili.Infine, verifichiamo le sue prestazioni su una serie di problemi di regressione.
L'obiettivo dell'apprendimento della rappresentazione della rete è quello di imparare embeddings a bassa dimensione del nodo che catturano la struttura del grafico e sono utili per risolvere i compiti a valle.Tuttavia, nonostante la proliferazione di tali metodi non c'è attualmente alcuno studio della loro robustezza agli attacchi avversari.Forniamo la prima analisi della vulnerabilità avversaria sulla famiglia ampiamente utilizzata dei metodi basati sulle passeggiate casuali. Deriviamo efficienti perturbazioni avversarie che avvelenano la struttura della rete e hanno un effetto negativo sia sulla qualità delle embeddings che sui compiti a valle. Mostriamo inoltre che i nostri attacchi sono trasferibili poiché si generalizzano a molti modelli e hanno successo anche quando l'attaccante è limitato.
I metodi di apprendimento di rinforzo gerarchico offrono un mezzo potente di pianificazione del comportamento flessibile nei domini complicati. Tuttavia, l'apprendimento di una decomposizione gerarchica appropriata di un dominio in sottocompiti rimane una sfida sostanziale. Presentiamo un nuovo algoritmo per la scoperta dei sottocompiti, basato sul processo decisionale di Markov linearmente risolvibile multitask recentemente introdotto (MLMDP). Il MLMDP può eseguire compiti mai visti prima rappresentandoli come una combinazione lineare di un insieme di base precedentemente appreso di compiti. In questa impostazione, il problema della scoperta dei sottocompiti può essere posto naturalmente come trovare un'approssimazione ottimale a basso rango dell'insieme dei compiti che l'agente dovrà affrontare in un dominio. Il nostro metodo ha diverse caratteristiche qualitativamente desiderabili: non è limitato all'apprendimento di sottocompiti con singoli stati obiettivo, ma apprende invece modelli distribuiti di stati preferiti; apprende decomposizioni gerarchiche qualitativamente diverse nello stesso dominio a seconda dell'insieme di compiti che l'agente dovrà affrontare; e può essere iterato direttamente per ottenere decomposizioni gerarchiche più profonde.
Questo articolo introduce una struttura per risolvere problemi di ottimizzazione combinatoria imparando da esempi di input-output di problemi di ottimizzazione. Introduciamo un nuovo modello neurale a memoria aumentata in cui la memoria non è resettabile (cioè l'informazione immagazzinata nella memoria dopo l'elaborazione di un esempio di input viene mantenuta per i prossimi esempi visti). Abbiamo usato il deep reinforcement learning per addestrare un agente controllore della memoria per immagazzinare ricordi utili. Il nostro modello è stato in grado di superare il solutore artigianale sulla Programmazione Lineare Binaria (LP binaria). Il modello proposto è stato testato su diverse istanze LP binarie con un gran numero di variabili (fino a 1000 variabili) e vincoli (fino a 700 vincoli).
I modelli Sequence-to-sequence (Seq2Seq) con attenzione hanno eccelso in compiti che coinvolgono la generazione di frasi in linguaggio naturale, come la traduzione automatica, la didascalia delle immagini e il riconoscimento vocale.Le prestazioni sono state ulteriormente migliorate sfruttando i dati non etichettati, spesso sotto forma di un modello linguistico. In questo lavoro, presentiamo il metodo Cold Fusion, che sfrutta un modello linguistico pre-addestrato durante l'addestramento, e mostriamo la sua efficacia sul compito di riconoscimento vocale.Mostriamo che i modelli Seq2Seq con Cold Fusion sono in grado di utilizzare meglio le informazioni linguistiche godendoi) di una convergenza più veloce e di una migliore generalizzazione, eii) di un trasferimento quasi completo a un nuovo dominio utilizzando meno del 10% dei dati di formazione etichettati.
Una capacità centrale dei sistemi intelligenti è la capacità di costruire continuamente sulle esperienze precedenti per accelerare e migliorare l'apprendimento di nuovi compiti.Due distinti paradigmi di ricerca hanno studiato questa questione.Il meta-apprendimento vede questo problema come l'apprendimento di un priore sui parametri del modello che è suscettibile di un rapido adattamento su un nuovo compito, ma tipicamente assume che l'insieme dei compiti siano disponibili insieme come un batch.Al contrario, l'apprendimento online (basato sul rimpianto) considera un ambiente sequenziale in cui i problemi sono rivelati uno dopo l'altro, ma convenzionalmente addestra solo un singolo modello senza alcun adattamento specifico per il compito. Questo lavoro introduce un'impostazione di meta-apprendimento online, che fonde le idee di entrambi i paradigmi summenzionati per catturare meglio lo spirito e la pratica dell'apprendimento permanente continuo.Proponiamo l'algoritmo Follow the Meta Leader (FTML) che estende l'algoritmo MAML a questa impostazione.Teoricamente, questo lavoro fornisce una garanzia di rammarico O(logT) per l'algoritmo FTML.La nostra valutazione sperimentale su tre diversi compiti su larga scala suggerisce che l'algoritmo proposto supera significativamente le alternative basate sui tradizionali approcci di apprendimento online.
Indaghiamo la misura in cui le singole teste di attenzione nei modelli linguistici trasformatori preaddestrati, come BERT e RoBERTa, catturano implicitamente le relazioni di dipendenza sintattica; impieghiamo due metodi - prendendo il massimo peso di attenzione e calcolando il massimo albero di estensione - per estrarre le relazioni di dipendenza implicite dai pesi di attenzione di ogni strato/testa, e li confrontiamo con gli alberi della dipendenza universale (UD). Mostriamo che, per alcuni tipi di relazioni UD, esistono teste che possono recuperare il tipo di dipendenza significativamente meglio delle basi sul testo inglese analizzato, suggerendo che alcune teste di auto-attenzione fungono da proxy per la struttura sintattica. Analizziamo anche il BERT finemente sintonizzato su due dataset - il CoLA orientato alla sintassi e il MNLI orientato alla semantica - per indagare se la sintonizzazione fine influisce sui modelli di autoattenzione, ma non osserviamo differenze sostanziali nelle relazioni di dipendenza complessive estratte con i nostri metodi. I nostri risultati suggeriscono che questi modelli hanno alcune teste di attenzione specializzate che tracciano i singoli tipi di dipendenza, ma nessuna testa generalista che esegue il parsing olistico significativamente meglio di una banale linea di base, e che analizzare direttamente i pesi di attenzione potrebbe non rivelare molta della conoscenza sintattica che i modelli in stile BERT sono noti per apprendere.
Lo stato dell'arte dei metodi di super-risoluzione del viso impiega reti neurali convoluzionali profonde per imparare una mappatura tra i modelli facciali a bassa e ad alta risoluzione esplorando la conoscenza dell'aspetto locale. Tuttavia, la maggior parte di questi metodi non sfrutta bene le strutture facciali e le informazioni sull'identità, e lotta per affrontare le immagini facciali che presentano grandi variazioni di posa e disallineamento.In questo articolo, proponiamo un nuovo metodo di super-risoluzione del viso che incorpora esplicitamente priori facciali 3D che afferrano le strutture facciali nitide. In primo luogo, il ramo di rendering del volto 3D è impostato per ottenere priori 3D delle strutture facciali salienti e la conoscenza dell'identità.In secondo luogo, il meccanismo di attenzione spaziale viene utilizzato per sfruttare meglio queste informazioni gerarchiche (cioè la somiglianza dell'intensità, la struttura facciale 3D, il contenuto dell'identità) per il problema della super-risoluzione.Esperimenti estesi dimostrano che l'algoritmo proposto raggiunge risultati superiori di super-risoluzione del volto e supera lo stato dell'arte.
Presentiamo Cross-View Training (CVT), un metodo semplice ma efficace per l'apprendimento semi-supervisionato profondo: su esempi etichettati, il modello viene addestrato con la perdita standard di cross-entropia; su un esempio non etichettato, il modello esegue prima l'inferenza (agendo come un "insegnante") per produrre obiettivi soft. Il modello poi impara da questi obiettivi morbidi (agendo come uno ``studente'').Ci discostiamo dal lavoro precedente aggiungendo più strati ausiliari di predizione degli studenti al modello.L'input di ogni strato studente è una sotto-rete del modello completo che ha una visione limitata dell'input (ad es, Gli studenti possono imparare dall'insegnante (il modello completo) perché l'insegnante vede più di ogni esempio. Allo stesso tempo, gli studenti migliorano la qualità delle rappresentazioni usate dall'insegnante mentre imparano a fare previsioni con dati limitati. Quando è combinata con il Virtual Adversarial Training, CVT migliora l'attuale stato dell'arte su CIFAR-10 e SVHN semi-supervisionato. Applichiamo anche la CVT per addestrare modelli su cinque compiti di elaborazione del linguaggio naturale utilizzando centinaia di milioni di frasi di dati non etichettati. Su tutti i compiti la CVT supera sostanzialmente l'apprendimento supervisionato da solo, risultando in modelli che migliorano o sono competitivi con l'attuale stato dell'arte.
Mostro come possa essere vantaggioso esprimere le decisioni di accettazione/rifiuto di Metropolis in termini di confronto con un valore uniforme [0,1], e poi aggiornare questo valore uniforme in modo non reversibile, come parte dello stato della catena di Markov, piuttosto che campionarlo indipendentemente ad ogni iterazione.  Produce un miglioramento maggiore quando si usano gli aggiornamenti di Langevin con momento persistente, dando prestazioni paragonabili a quelle di Hamiltonian Monte Carlo (HMC) con traiettorie lunghe.  Questo è significativo quando alcune variabili sono aggiornate da altri metodi, poiché se viene usato HMC, questi aggiornamenti possono essere fatti solo tra le traiettorie, mentre possono essere fatti più spesso con gli aggiornamenti Langevin.  Questo si vede per un modello di rete neurale bayesiano, in cui i pesi di connessione sono aggiornati da Langevin o HMC persistente, mentre gli iperparametri sono aggiornati dal campionamento di Gibbs.
Introduciamo ES-MAML, un nuovo framework per risolvere il problema del model agnostic meta learning (MAML) basato su Evolution Strategies (ES).Gli algoritmi esistenti per MAML sono basati su gradienti di policy, e incorrono in difficoltà significative quando si cerca di stimare le derivate secondarie usando la backpropagation su politiche stocastiche. Mostriamo come ES può essere applicato a MAML per ottenere un algoritmo che evita il problema della stima delle seconde derivate, ed è anche concettualmente semplice e facile da implementare.Inoltre, ES-MAML può gestire nuovi tipi di operatori di adattamento non lisci, e altre tecniche per migliorare le prestazioni e la stima dei metodi ES diventano applicabili.Mostriamo empiricamente che ES-MAML è competitivo con i metodi esistenti e spesso produce un adattamento migliore con meno richieste.
Trasformare una distribuzione di probabilità in un'altra è un potente strumento nell'inferenza bayesiana e nell'apprendimento automatico. Alcuni esempi importanti sono le trasformazioni da vincolate a non vincolate di distribuzioni per l'uso in Hamiltonian Monte-Carlo e la costruzione di densità flessibili e apprendibili come i flussi normalizzanti. Presentiamo Bijectors.jl, un pacchetto software per la trasformazione di distribuzioni implementato in Julia, disponibile su github.com/TuringLang/Bijectors.jl. Il pacchetto fornisce un modo flessibile e componibile di implementare trasformazioni di distribuzioni senza essere legato a una struttura computazionale. Dimostriamo l'uso di Bijectors.jl per migliorare l'inferenza variazionale codificando le dipendenze statistiche conosciute nel posteriore variazionale usando i flussi di normalizzazione, fornendo un approccio generale per rilassare l'assunzione di campo medio solitamente fatta nell'inferenza variazionale.
L'apprendimento multidominio (MDL) mira a ottenere un modello con un rischio medio minimo attraverso domini multipli. La nostra motivazione empirica è costituita da dati di microscopia automatizzata, in cui le cellule coltivate vengono fotografate dopo essere state esposte a perturbazioni chimiche note e sconosciute, e ogni set di dati presenta un significativo bias sperimentale. I nostri contributi includono: i) un limite sul rischio medio e peggiore del dominio in MDL, ottenuto usando la divergenza H; ii) una nuova perdita per ospitare l'apprendimento multidominio semi-supervisionato e l'adattamento al dominio; iii) la validazione sperimentale dell'approccio, migliorando lo stato dell'arte su due benchmark di immagini standard e un nuovo dataset di bioimmagini, Cell.
Rispetto ai metodi esistenti, DRN apprende con un minor numero di parametri del modello e si estende facilmente a più distribuzioni di input e di output. Su set di dati sintetici e del mondo reale, DRN ha prestazioni simili o migliori rispetto allo stato dell'arte. Inoltre, DRN generalizza il tradizionale perceptron multistrato (MLP).
I metodi esistenti di predizione delle sequenze si occupano per lo più di sequenze indipendenti dal tempo, in cui l'intervallo di tempo effettivo tra gli eventi è irrilevante e la distanza tra gli eventi è semplicemente la differenza tra le loro posizioni d'ordine nella sequenza.Mentre questa visione indipendente dal tempo delle sequenze è applicabile a dati come le lingue naturali, ad esempio, trattando le parole in una frase, è inappropriata e inefficiente per molti eventi del mondo reale che sono osservati e raccolti in punti di tempo non equamente distanziati come si presentano naturalmente, ad es, In questo lavoro, proponiamo una serie di metodi per utilizzare il tempo nella predizione delle sequenze. Poiché i modelli neurali di sequenza come RNN sono più adatti a gestire input di tipo token, proponiamo due metodi per la rappresentazione di eventi dipendenti dal tempo, basati sull'intuizione di come il tempo viene simbolizzato nella vita quotidiana e sul lavoro precedente sull'incorporazione della contestualizzazione. Introduciamo anche due metodi per utilizzare la durata dell'evento successivo come regolarizzazione per l'addestramento di un modello di predizione della sequenza.Discutiamo questi metodi basati su reti neurali ricorrenti.Valutiamo questi metodi così come i modelli di base su cinque set di dati che assomigliano a una varietà di compiti di predizione della sequenza.Gli esperimenti hanno rivelato che i metodi proposti offrono un guadagno di precisione rispetto ai modelli di base in una gamma di impostazioni.
Gli algoritmi di apprendimento di rinforzo off-policy promettono di essere applicabili nelle impostazioni in cui è disponibile solo un set di dati fisso (batch) di interazioni con l'ambiente e nessuna nuova esperienza può essere acquisita.Questa proprietà rende questi algoritmi interessanti per problemi del mondo reale come il controllo dei robot.In pratica, tuttavia, gli algoritmi off-policy standard falliscono nell'impostazione batch per il controllo continuo.In questo articolo, proponiamo una soluzione semplice a questo problema. Ammette l'uso dei dati generati dalle politiche arbitrarie di comportamento ed usa un priore imparato -- il modello di comportamento vantaggio-pesato (ABM) -- per orientare la politica di RL verso le azioni che sono state eseguite precedentemente e sono probabili riuscire sul nuovo metodo di task.Our puÃ² essere veduto come estensione di lavoro recente su batch-RL che permette l'apprendimento stabile dalle dati-sorgenti contrastanti.We trova i miglioramenti sulle basi competitive in una varietÃ di compiti di RL -- compreso i benchmark continui standard di controllo e l'apprendimento di multi-task per i robot simulati e reali.
Una delle sfide principali nell'applicazione delle reti neurali di convoluzione del grafico sui dati di gene-interazione Ã¨ la mancanza di comprensione dello spazio vettoriale a cui appartengono ed anche le difficoltÃ inerenti coinvolte nella rappresentazione delle interazioni su una dimensione significativamente piÃ¹ bassa, viz spazi euclidei.La sfida diventa piÃ¹ prevalente quando tratta i vari tipi di dati eterogenei. Introduciamo un metodo sistematico e generalizzato, chiamato iSOM-GSN, utilizzato per trasformare i dati ``multi-omici'' con dimensioni più elevate in una griglia bidimensionale; in seguito, applichiamo una rete neurale convoluzionale per prevedere gli stati di malattia di vari tipi, basandoci sull'idea della mappa auto-organizzante di Kohonen, generiamo una griglia bidimensionale per ogni campione per un dato insieme di geni che rappresentano una rete di somiglianza genica.  Abbiamo testato il modello per prevedere il cancro al seno e alla prostata usando l'espressione genica, la metilazione del DNA e l'alterazione del numero di copie, ottenendo accuratezze di previsione nell'intervallo 94-98% per gli stadi tumorali del cancro al seno e calcolando i punteggi Gleason del cancro alla prostata con solo 11 geni di input per entrambi i casi. Lo schema non solo produce un'accuratezza di classificazione quasi perfetta, ma fornisce anche uno schema migliorato per l'apprendimento della rappresentazione, la visualizzazione, la riduzione della dimensionalità e l'interpretazione dei risultati.
Le prestazioni allo stato dell'arte sui compiti di comprensione del linguaggio sono raggiunte da enormi modelli linguistici pre-addestrati su enormi corpora di testo non etichettato, con un fine-tuning successivo molto leggero in un modo supervisionato specifico per il compito.Sembra che la procedura di pre-addestramento apprenda un'inizializzazione comune molto buona per un ulteriore addestramento su vari compiti di comprensione del linguaggio naturale, in modo tale che solo pochi passi devono essere fatti nello spazio dei parametri per imparare ogni compito. In questo lavoro, usando Bidirectional Encoder Representations from Transformers (BERT) come esempio, verifichiamo questa ipotesi mostrando che i modelli linguistici finemente sintonizzati specifici per ogni compito sono molto vicini nello spazio dei parametri a quello pre-addestrato. Approfittando di tali osservazioni, dimostriamo inoltre che le versioni finemente sintonizzate di questi enormi modelli, che hanno parametri in virgola mobile dell'ordine di $10^8$, possono essere rese molto efficienti dal punto di vista computazionale; in primo luogo, è sufficiente sintonizzare solo una frazione degli strati critici; in secondo luogo, la sintonizzazione fine può essere adeguatamente eseguita con l'apprendimento di una maschera binaria moltiplicativa sui pesi pre-addestrati, cioè con la sparizione dei parametri: (1) imparare ad eseguire compiti specifici, (2) risparmiare memoria memorizzando solo le maschere binarie di certi strati per ogni compito, e (3) risparmiare calcoli sull'hardware appropriato eseguendo operazioni sparse con i parametri del modello.  
Presentiamo un algoritmo semplice ed efficace progettato per affrontare il problema dello spostamento delle covariate nell'apprendimento per imitazione, che opera addestrando un insieme di politiche sui dati dimostrativi degli esperti e utilizzando la varianza delle loro previsioni come un costo che viene minimizzato con RL insieme a un costo di clonazione comportamentale supervisionato. Dimostriamo un regret bound per l'algoritmo nell'impostazione tabellare che è lineare nell'orizzonte temporale moltiplicato per un coefficiente che mostriamo essere basso per certi problemi in cui la clonazione comportamentale fallisce.Valutiamo empiricamente il nostro algoritmo attraverso più ambienti Atari basati su pixel e compiti di controllo continuo, e dimostriamo che corrisponde o supera significativamente la clonazione comportamentale e l'apprendimento generativo per imitazione avversaria.
Presentiamo e discutiamo un semplice metodo di pre-elaborazione dell'immagine per l'apprendimento di fattori latenti dissociati. In particolare, utilizziamo il bias induttivo implicito contenuto nelle caratteristiche delle reti preaddestrate sul database ImageNet. Miglioriamo questo bias sintonizzando esplicitamente tali reti preaddestrate su compiti utili per la sfida di disentanglement di NeurIPS2019, come la stima dell'angolo e della posizione o la classificazione del colore. Inoltre, addestriamo un VAE su mappe di caratteristiche aggregate a livello regionale, e discutiamo le sue prestazioni di disentanglement utilizzando le metriche proposte nella letteratura recente.
Un agente di apprendimento di rinforzo che deve perseguire gli obiettivi differenti attraverso gli episodi richiede una politica obiettivo-condizionata.Oltre al loro potenziale per generalizzare il comportamento desiderabile agli obiettivi non visti, tali politiche possono anche permettere la pianificazione di livello superiore basata su subgoals.In sparse-ricompensa gli ambienti, la capacitÃ di sfruttare le informazioni circa il grado a cui un obiettivo arbitrario Ã¨ stato realizzato mentre un altro obiettivo era inteso sembra cruciale per permettere l'apprendimento efficiente del campione. Tuttavia, gli agenti di apprendimento di rinforzo sono stati dotati solo recentemente di tale capacità di hindsight.In questo articolo, dimostriamo come l'hindsight può essere introdotto ai metodi di gradiente della politica, generalizzando questa idea a un'ampia classe di algoritmi di successo.I nostri esperimenti su una diversa selezione di ambienti sparse-reward mostrano che l'hindsight porta a un notevole aumento dell'efficienza del campione.
Tuttavia, gran parte di questi miglioramenti si sono concentrati sull'analisi delle immagini statiche; la comprensione dei video ha visto miglioramenti piuttosto modesti. Anche se sono stati proposti nuovi set di dati e modelli spazio-temporali, i semplici metodi di classificazione frame-by-frame spesso rimangono ancora competitivi. In questo lavoro, costruiamo un set di dati video con bias di oggetti e scene completamente osservabili e controllabili, e che richiede veramente la comprensione spazio-temporale per essere risolto. Il nostro set di dati, chiamato CATER, è reso sinteticamente usando una libreria di oggetti 3D standard, e testa la capacità di riconoscere composizioni di movimenti di oggetti che richiedono un ragionamento a lungo termine. Oltre ad essere un dataset impegnativo, CATER fornisce anche una pletora di strumenti diagnostici per analizzare le moderne architetture video spazio-temporali essendo completamente osservabile e controllabile.Utilizzando CATER, forniamo approfondimenti su alcune delle più recenti architetture video profonde allo stato dell'arte.
Affrontiamo i problemi di efficienza causati dall'effetto straggler nell'apprendimento federato recentemente emerso, che allena in modo collaborativo un modello su dati decentralizzati non-i.i.d. (non indipendenti e identicamente distribuiti) attraverso dispositivi di lavoro massicci senza scambiare dati di allenamento nelle reti inaffidabili ed eterogenee.Proponiamo una nuova analisi in due fasi sui limiti di errore dell'apprendimento federato generale, che fornisce intuizioni pratiche nell'ottimizzazione. Come risultato, proponiamo un nuovo algoritmo di apprendimento federato facile da implementare che utilizza impostazioni e strategie asincrone per controllare le discrepanze tra il modello globale e i modelli ritardati e regolare il numero di epoche locali con la stima della staleness per accelerare la convergenza e resistere al deterioramento delle prestazioni causato dai ritardatari.
Le unità LSTM (Long Short-Term Memory) hanno la capacità di memorizzare e utilizzare le dipendenze a lungo termine tra gli input per generare previsioni su dati di serie temporali.Introduciamo il concetto di modificare lo stato delle celle (memoria) delle LSTM usando matrici di rotazione parametrate da un nuovo set di pesi allenabili.Questa aggiunta mostra significativi aumenti di performance su alcuni dei compiti del dataset bAbI.
Affrontiamo il problema dell'inferenza marginale per una famiglia esponenziale definita sull'insieme delle matrici di permutazione. Questo problema è noto per diventare rapidamente intrattabile all'aumentare della dimensione della permutazione, poiché comporta il calcolo della permanente di una matrice, un problema #P-hard. Introduciamo l'inferenza marginale variazionale di Sinkhorn come un'alternativa scalabile, un metodo la cui validità è infine giustificata dalla cosiddetta approssimazione di Sinkhorn della permanente.Dimostriamo l'efficacia del nostro metodo nel problema dell'identificazione probabilistica dei neuroni nel verme C.elegans
La robustezza delle reti neurali agli esempi avversari ha ricevuto grande attenzione a causa delle implicazioni di sicurezza.Nonostante i vari approcci di attacco per creare esempi avversari visivamente impercettibili, poco è stato sviluppato verso una misura completa di robustezza.In questo articolo, forniamo una giustificazione teorica per convertire l'analisi di robustezza in un problema di stima della costante Lipschitz locale, e proponiamo di utilizzare la teoria dei valori estremi per una valutazione efficiente.La nostra analisi produce una nuova metrica di robustezza chiamata CLEVER, che è l'abbreviazione di Cross Lipschitz Extreme Value for nEtwork Robustness. I risultati sperimentali su varie reti, tra cui ResNet, Inception-v3 e MobileNet, mostrano che(i) CLEVER è allineato con l'indicazione di robustezza misurata dalle norme $\ell_2$ e $\ell_\infty$ di esempi avversari da attacchi potenti, e(ii) le reti difese usando la distillazione difensiva o ReLU delimitato danno effettivamente migliori punteggi CLEVER. Per quanto ne sappiamo, CLEVER è la prima metrica di robustezza indipendente dagli attacchi che può essere applicata a qualsiasi classificatore di reti neurali.
La collaborazione multi-agente è richiesta da numerosi problemi del mondo reale.Anche se l'impostazione distribuita è di solito adottata dai sistemi pratici, la comunicazione locale e l'aggregazione delle informazioni sono ancora importanti nell'adempimento di compiti complessi.Per l'apprendimento di rinforzo multi-agente, molti studi precedenti sono stati dedicati a progettare un'architettura di comunicazione efficace.Tuttavia, i modelli esistenti di solito soffrono di una struttura di comunicazione ossificata, ad es, La maggior parte di loro predefinisce una particolare modalità di comunicazione specificando una frequenza temporale fissa e un ambito spaziale per gli agenti per comunicare indipendentemente dalla necessità.Tale progettazione è incapace di affrontare scenari multi-agente che sono capricciosi e complicati, soprattutto quando sono disponibili solo informazioni parziali.Motivato da questo, sosteniamo che la soluzione è quella di costruire uno schema di apprendimento di comunicazione spontanea e auto-organizzante (SSoC).Trattando il comportamento di comunicazione come un'azione esplicita, SSoC impara a organizzare la comunicazione in modo efficace ed efficiente. In particolare, permette ad ogni agente di decidere spontaneamente quando e chi inviare messaggi in base ai suoi stati osservati.In questo modo, un canale di comunicazione inter-agente dinamico è stabilito in modo online e auto-organizzante.Gli agenti imparano anche come aggregare in modo adattivo i messaggi ricevuti e i propri stati nascosti per eseguire le azioni.Sono stati condotti vari esperimenti per dimostrare che SSoC impara davvero il passaggio intelligente dei messaggi tra agenti situati molto lontano.Con tali comunicazioni agili, osserviamo che emergono tattiche di collaborazione efficaci che non sono state padroneggiate dalle linee base confrontate.
Studiamo il modello di rappresentazione linguistica BERT e il modello di generazione di sequenze con codificatore BERT per compiti di classificazione del testo multietichetta.Sperimentiamo entrambi i modelli ed esploriamo le loro qualità speciali per questa impostazione.Introduciamo ed esaminiamo sperimentalmente anche un modello misto, che è un insieme di modelli BERT multietichetta e modelli BERT generatori di sequenze.I nostri esperimenti hanno dimostrato che i modelli basati su BERT e il modello misto, in particolare, superano le attuali basi in diverse metriche raggiungendo risultati all'avanguardia su tre dataset di classificazione multietichetta ben studiati con testi inglesi e due dataset privati Yandex Taxi con testi russi.
La previsione del Click Through Rate (CTR) è un compito critico nelle applicazioni industriali, specialmente per le applicazioni sociali e commerciali online. è impegnativo trovare un modo adeguato per scoprire automaticamente le caratteristiche trasversali efficaci nei compiti CTR. proponiamo un nuovo modello per i compiti CTR, chiamato reti neurali profonde con Encoder enhanced Factorization Machine (DeepEnFM). invece di imparare le caratteristiche trasversali direttamente, DeepEnFM adotta il codificatore Transformer come una spina dorsale per allineare le embeddings delle caratteristiche con gli indizi di altri campi. In particolare, DeepEnFM utilizza un approccio bilineare per generare diverse funzioni di somiglianza rispetto a diverse coppie di campi. Inoltre, il metodo di max-pooling rende DeepEnFM fattibile per catturare sia le informazioni supplementari che quelle di soppressione tra diverse teste di attenzione.Il nostro modello è convalidato sui dataset Criteo e Avazu, e raggiunge prestazioni all'avanguardia.
Affinché gli agenti autonomi operino con successo nel mondo reale, la capacità di anticipare gli stati futuri della scena è una competenza chiave.Negli scenari del mondo reale, gli stati futuri diventano sempre più incerti e multi-modali, in particolare su orizzonti temporali lunghi. L'inferenza bayesiana basata sul dropout fornisce un approccio computazionalmente trattabile e teoricamente ben fondato per imparare diverse ipotesi/modelli per affrontare futuri incerti e fare previsioni che corrispondono bene alle osservazioni -- sono ben calibrate.Tuttavia, si scopre che tali approcci non riescono a catturare scene complesse del mondo reale, anche rimanendo indietro nell'accuratezza se confrontati con i semplici approcci deterministici. In questo lavoro, proponiamo una nuova formulazione bayesiana per l'anticipazione degli stati futuri della scena che sfrutta le verosimiglianze sintetiche che incoraggiano l'apprendimento di modelli diversi per catturare accuratamente la natura multimodale degli stati futuri della scena. mostriamo che il nostro approccio raggiunge previsioni accurate allo stato dell'arte e probabilità calibrate attraverso ampi esperimenti per l'anticipazione della scena sul dataset Cityscapes. inoltre, mostriamo che il nostro approccio si generalizza attraverso compiti diversi come la generazione di cifre e la previsione delle precipitazioni.
Le reti generative condizionali avversarie (cGAN) hanno portato a grandi miglioramenti nel compito di generazione condizionale dell'immagine, che si trova nel cuore della computer vision.L'attenzione principale finora è stata sul miglioramento delle prestazioni, mentre c'è stato poco sforzo nel rendere cGAN più robusto al rumore.La regressione (del generatore) potrebbe portare a errori arbitrariamente grandi in uscita, che rende cGAN inaffidabile per applicazioni del mondo reale. In questo lavoro, introduciamo un nuovo modello di GAN condizionale, chiamato RoCGAN, che sfrutta la struttura nello spazio di destinazione del modello per affrontare il problema. Il nostro modello aumenta il generatore con un percorso non supervisionato, che promuove le uscite del generatore per abbracciare il manifold di destinazione anche in presenza di rumore intenso. dimostriamo che RoCGAN condivide proprietà teoriche simili a GAN e verifichiamo sperimentalmente che il nostro modello supera le architetture cGAN esistenti allo stato dell'arte con un ampio margine in una varietà di domini, comprese le immagini di scene naturali e volti.
Sebbene le reti neurali profonde abbiano raggiunto lo stato dell'arte delle prestazioni nella classificazione visiva, studi recenti hanno dimostrato che sono tutte vulnerabili all'attacco di esempi avversari.Per risolvere il problema, sono stati studiati alcuni metodi di regolarizzazione dell'addestramento avversario, vincolando l'etichetta di uscita o logit.In questo articolo, proponiamo un nuovo quadro di addestramento avversario regolarizzato ATLPA, cioè Adversarial Tolerant Logit Pairing with Attention.Invece di vincolare una distribuzione dura (es, vettori one-hot o logit) nell'addestramento avversario, ATLPA utilizza il Logit tollerante che consiste in una distribuzione di fiducia sulle classi top-k e cattura le somiglianze interclasse a livello di immagine. In particolare, oltre a minimizzare la perdita empirica, ATLPA incoraggia la mappa di attenzione per coppie di esempi che siano simili. Quando viene applicato agli esempi puliti e alle loro controparti avversarie, ATLPA migliora l'accuratezza sugli esempi avversari rispetto alla formazione avversaria.Valutiamo ATLPA con lo stato dell'arte degli algoritmi, i risultati degli esperimenti mostrano che il nostro metodo supera queste linee di base con una maggiore accuratezza.Rispetto ai lavori precedenti, il nostro lavoro viene valutato sotto un attacco PGD altamente impegnativo: la perturbazione massima $\epsilon$ è 64 e 128 con 10 a 200 iterazioni di attacco.
Un tratto fondamentale dell'intelligenza è la capacità di raggiungere gli obiettivi di fronte a nuove circostanze.In questo lavoro, affrontiamo una di queste impostazioni che richiede la risoluzione di un compito con un nuovo insieme di azioni.Mettere in grado le macchine di questa capacità richiede una generalizzazione nel modo in cui un agente percepisce le sue azioni disponibili insieme al modo in cui utilizza queste azioni per risolvere i compiti.Quindi, proponiamo un quadro per consentire la generalizzazione su entrambi questi aspetti: la comprensione della funzionalità di un'azione e l'utilizzo delle azioni per risolvere i compiti attraverso l'apprendimento di rinforzo. In particolare, un agente interpreta il comportamento di un'azione utilizzando un apprendimento di rappresentazione non supervisionato su una collezione di campioni di dati che riflettono le diverse proprietà di quell'azione.Impieghiamo un'architettura di apprendimento di rinforzo che lavora su queste rappresentazioni di azioni, e proponiamo metriche di regolarizzazione essenziali per consentire la generalizzazione in una politica.Illustriamo la generalizzabilità del metodo di apprendimento di rappresentazione e la politica, per consentire la generalizzazione zero-shot ad azioni precedentemente non viste su ambienti decisionali sequenziali impegnativi.I nostri risultati e video possono essere trovati su sites.google.com/view/action-generalization/
I processi puntuali temporali sono il paradigma dominante per la modellazione di sequenze di eventi che accadono a intervalli irregolari. Il modo standard di apprendimento in tali modelli è la stima della funzione di intensità condizionata.  Tuttavia, la parametrizzazione della funzione di intensità di solito comporta diversi compromessi. Mostriamo come superare le limitazioni degli approcci basati sull'intensità modellando direttamente la distribuzione condizionale dei tempi tra gli eventi.  Attingiamo alla letteratura sulla normalizzazione dei flussi per progettare modelli che siano flessibili ed efficienti e proponiamo inoltre un semplice modello di miscela che corrisponde alla flessibilità dei modelli basati sul flusso, ma che permette anche il campionamento e il calcolo dei momenti in forma chiusa.  I modelli proposti raggiungono prestazioni allo stato dell'arte in compiti di previsione standard e sono adatti per nuove applicazioni, come l'apprendimento di sequenze di embeddings e l'imputazione di dati mancanti.
Il metodo si basa sull'addestramento di una struttura autoencoder in cui il collo di bottiglia rappresenta lo spazio della distribuzione degli argomenti e le uscite del decoder rappresentano lo spazio delle distribuzioni delle parole sugli argomenti, sfruttando un decoder ausiliario per prevenire il collasso della modalità nel nostro modello.  Una caratteristica chiave per un metodo efficace di modellazione di argomenti è avere distribuzioni di argomenti e parole sparse, dove c'è un trade-off tra il livello di sparsità degli argomenti e delle parole.Questa caratteristica è implementata nel nostro modello dalla regolarizzazione L-2 e gli iperparametri del modello si prendono cura del trade-off.  Mostriamo nei nostri esperimenti che il nostro modello raggiunge risultati competitivi rispetto ai modelli profondi allo stato dell'arte per la modellazione di argomenti, nonostante la sua architettura semplice e la procedura di addestramento.I dataset "New York Times" e "20 Newsgroups" sono utilizzati negli esperimenti.
Voice Conversion (VC) è un compito di convertire l'identità percepita di un altoparlante da un altoparlante di origine a un particolare altoparlante di destinazione.approcci precedenti nella letteratura principalmente trovare una mappatura tra la fonte data-paia di altoparlanti di destinazione.sviluppo di tecniche di mappatura per molti-a-molti VC utilizzando dati non paralleli, tra cui zero-shot apprendimento rimane meno esplorato aree in VC. La maggior parte delle architetture VC molti-a-molti richiedono dati di formazione da tutti gli altoparlanti di destinazione per i quali vogliamo convertire le voci.In questo documento, proponiamo una nuova architettura di trasferimento di stile, che può anche essere esteso per generare voci anche per gli altoparlanti di destinazione i cui dati non sono stati utilizzati nella formazione (cioè, In particolare, proponiamo Adaptive Generative Adversarial Network (AdaGAN), una nuova procedura di formazione architettonica che aiuta ad apprendere una rappresentazione latente normalizzata indipendente dal parlante, che verrà utilizzata per generare un discorso con diversi stili di conversazione nel contesto di VC. 73%, e il 10.37% di miglioramento relativo rispetto allo StarGAN nei test MOS per la qualità del discorso e la somiglianza degli speaker, rispettivamente.La forza chiave delle architetture proposte è che produce questi risultati con meno complessità computazionale.AdaGAN è 88.6% meno complesso di StarGAN-VC in termini di FLoating Operation Per Second (FLOPS), e 85.46% meno complesso in termini di parametri allenabili.  
Il trasformatore basato sull'auto-attenzione ha dimostrato le prestazioni allo stato dell'arte in una serie di compiti di elaborazione del linguaggio naturale.L'auto-attenzione è in grado di modellare le dipendenze a lungo termine, ma può soffrire dell'estrazione di informazioni irrilevanti nel contesto.Per affrontare il problema, proponiamo un nuovo modello chiamato Sparse Transformer. Sparse Transformer è in grado di migliorare la concentrazione dell'attenzione sul contesto globale attraverso una selezione esplicita dei segmenti più rilevanti.Ampi risultati sperimentali su una serie di compiti di elaborazione del linguaggio naturale, tra cui la traduzione automatica neurale, la didascalia delle immagini e la modellazione del linguaggio, dimostrano i vantaggi di Sparse Transformer nelle prestazioni del modello.   Sparse Transformer raggiunge lo stato dell'arte delle prestazioni nella traduzione inglese-vietnamita IWSLT 2015 e nella traduzione tedesco-inglese IWSLT 2014.
Gli osservatori umani possono imparare a riconoscere nuove categorie di oggetti da una manciata di esempi, ma farlo con la percezione automatica rimane una sfida aperta. Noi ipotizziamo che il riconoscimento efficiente dei dati sia abilitato da rappresentazioni che rendono la variabilità dei segnali naturali più prevedibile, come suggerito da recenti evidenze percettive, quindi rivisitiamo e miglioriamo il Contrastive Predictive Coding, un quadro di apprendimento non supervisionato proposto di recente, e arriviamo a una rappresentazione che permette la generalizzazione da piccole quantità di dati etichettati. Quando gli viene fornito solo l'1% delle etichette di ImageNet (cioè 13 per classe), questo modello mantiene una forte performance di classificazione, il 73% di accuratezza Top-5, superando le reti supervisionate del 28% (un miglioramento relativo del 65%) e i metodi semi-supervisionati allo stato dell'arte del 14%.Troviamo anche che questa rappresentazione serva come substrato utile per il rilevamento degli oggetti sul dataset PASCAL-VOC 2007, avvicinandosi alle performance delle rappresentazioni allenate con un dataset ImageNet completamente annotato.
Dimostriamo la possibilità di quello che noi chiamiamo sparse learning: l'addestramento accelerato di reti neurali profonde che mantengono pesi sparsi durante l'addestramento pur raggiungendo livelli di prestazioni dense, sviluppando lo sparse momentum, un algoritmo che usa gradienti esponenzialmente smussati (momentum) per identificare gli strati e i pesi che riducono l'errore in modo efficiente. All'interno di uno strato, lo sparse momentum fa crescere i pesi in base alla grandezza del momentum dei pesi a valore zero.dimostriamo lo stato dell'arte delle prestazioni sparse su MNIST, CIFAR-10 e ImageNet, diminuendo l'errore medio di un relativo 8%, 15% e 6% rispetto ad altri algoritmi sparse.inoltre, dimostriamo che lo sparse momentum riproduce in modo affidabile i livelli di prestazioni dense, fornendo al contempo un addestramento fino a 5.61x più veloce.nella nostra analisi, le ablazioni mostrano che i benefici della ridistribuzione e della crescita del momentum aumentano con la profondità e la dimensione della rete.
Per fornire dei principi per la progettazione di adeguati modelli di Deep Neural Network (DNN), è essenziale comprendere la superficie di perdita delle DNN sotto ipotesi realistiche.Introduciamo aspetti interessanti per comprendere i minimi locali e la struttura complessiva della superficie di perdita.Il dominio dei parametri della superficie di perdita può essere decomposto in regioni in cui i valori di attivazione (zero o uno per unità lineari rettificate) sono coerenti. Abbiamo trovato che, in ogni regione, la superficie di perdita ha proprietà simili a quelle delle reti neurali lineari dove ogni minimo locale è un minimo globale.Questo significa che ogni minimo locale differenziabile è il minimo globale della regione corrispondente.Proviamo che per una rete neurale con uno strato nascosto che usa unità lineari rettificate sotto ipotesi realistiche.Ci sono regioni povere che portano a minimi locali poveri, e spieghiamo perché tali regioni esistono anche nelle DNN iperparametrizzate.
Le rappresentazioni contestualizzate delle parole, come ELMo e BERT, hanno dimostrato di avere buone prestazioni su una serie di compiti semantici e strutturali (sintattici).In questo lavoro, affrontiamo il compito di dissociazione non supervisionata tra semantica e struttura nelle rappresentazioni neurali del linguaggio: ci proponiamo di imparare una trasformazione dei vettori contestualizzati, che scarti la semantica lessicale, ma mantenga le informazioni strutturali. A questo scopo, generiamo automaticamente gruppi di frasi che sono strutturalmente simili ma semanticamente diverse, e usiamo un approccio di apprendimento metrico per imparare una trasformazione che enfatizza la componente strutturale che è codificata nei vettori. dimostriamo che la nostra trasformazione raggruppa i vettori nello spazio per proprietà strutturali, piuttosto che per semantica lessicale. infine, dimostriamo l'utilità delle nostre rappresentazioni distillate mostrando che superano le rappresentazioni originali contestualizzate in un'impostazione di parsing a pochi colpi.
La memoria topologica semi-parametrica (SPTM) proposta consiste in un grafo (non parametrico) con nodi corrispondenti a luoghi nell'ambiente e una rete profonda (parametrica) in grado di recuperare i nodi dal grafo in base alle osservazioni. Utilizziamo SPTM come modulo di pianificazione in un sistema di navigazione: dati solo 5 minuti di filmati di un labirinto mai visto prima, un agente di navigazione basato su SPTM può costruire una mappa topologica dell'ambiente e usarla per navigare con fiducia verso gli obiettivi. Il tasso di successo medio dell'agente SPTM nella navigazione diretta agli obiettivi in ambienti di prova è superiore alla linea di base più performante di un fattore tre.
La risoluzione disponibile nel nostro mondo visivo è estremamente alta, se non infinita. Le CNN esistenti possono essere applicate in modo completamente convoluzionale a immagini di risoluzione arbitraria, ma quando la dimensione dell'input aumenta, non possono catturare le informazioni contestuali. Inoltre, i requisiti computazionali scalano linearmente al numero di pixel di input, e le risorse sono allocate uniformemente su tutto l'input, non importa quanto siano informative le diverse regioni dell'immagine.Cerchiamo di affrontare questi problemi proponendo una nuova architettura che attraversa una piramide di immagini in modo top-down, mentre utilizza un meccanismo di attenzione dura per elaborare selettivamente solo le parti di immagine più informative. Conduciamo esperimenti sui dataset MNIST e ImageNet, e dimostriamo che i nostri modelli possono superare significativamente le controparti completamente convoluzionali, quando la risoluzione dell'input è così grande che il campo recettivo delle linee di base non può coprire adeguatamente gli oggetti di interesse.I guadagni in termini di prestazioni vengono per meno FLOP, a causa dell'elaborazione selettiva che seguiamo.Inoltre, il nostro meccanismo di attenzione rende le nostre previsioni più interpretabili, e crea un trade-off tra accuratezza e complessità che può essere regolato sia durante l'addestramento che durante i test.
L'ottimizzazione degli iperparametri può essere formulata come un problema di ottimizzazione a due livelli, in cui i parametri ottimali sul set di allenamento dipendono dagli iperparametri. Il nostro obiettivo è adattare gli iperparametri di regolarizzazione per le reti neurali adattando approssimazioni compatte alla funzione di risposta migliore, che mappa gli iperparametri ai pesi e ai bias ottimali. Mostriamo come costruire approssimazioni scalabili della migliore risposta per le reti neurali modellando la migliore-risposta come una singola rete le cui unità nascoste sono gated condizionatamente al regolatore. Giustifichiamo questa approssimazione mostrando che la migliore-risposta esatta per una rete lineare poco profonda con L2-regolarizzata Jacobian può essere rappresentata da un simile meccanismo di gating. Ci adattiamo a questo modello usando un algoritmo di ottimizzazione degli iperparametri basato sul gradiente che alterna l'approssimazione della migliore risposta intorno agli iperparametri correnti e l'ottimizzazione degli iperparametri usando la funzione di migliore risposta approssimata.A differenza di altri approcci basati sul gradiente, non richiediamo di differenziare la perdita di addestramento rispetto agli iperparametri, permettendoci di sintonizzare iperparametri discreti, iperparametri di aumento dei dati e probabilità di abbandono. Poiché gli iperparametri sono adattati online, il nostro approccio scopre programmi di iperparametri che possono superare i valori fissi degli iperparametri. empiricamente, il nostro approccio supera i metodi concorrenti di ottimizzazione degli iperparametri su problemi di apprendimento profondo su larga scala. chiamiamo le nostre reti, che aggiornano i propri iperparametri online durante l'allenamento, Self-Tuning Networks (STNs).
Nonostante i progressi eccezionali, la valutazione quantitativa di tali modelli spesso coinvolge più metriche distinte per valutare diverse proprietà desiderabili, come la qualità dell'immagine, la coerenza condizionale e la diversità intra-condizione. In questa impostazione, il benchmarking del modello diventa una sfida, poiché ogni metrica può indicare un diverso modello "migliore". In questo articolo, proponiamo la Frechet Joint Distance (FJD), che è definita come la distanza Frechet tra le distribuzioni congiunte di immagini e condizionamento, consentendo di catturare implicitamente le proprietà di cui sopra in una singola metrica.conduciamo esperimenti di proof-of-concept su un set di dati sintetici controllabili, che evidenziano costantemente i vantaggi di FJD rispetto alle metriche attualmente stabilite. Inoltre, usiamo la nuova metrica introdotta per confrontare i modelli esistenti basati su cGAN per una varietà di modalità di condizionamento (ad esempio, etichette di classe, maschere di oggetti, scatole di delimitazione, immagini e didascalie di testo).Mostriamo che FJD può essere utilizzato come una metrica singola promettente per il benchmarking dei modelli.
Combinare l'apprendimento di rinforzo profondo senza modello con la pianificazione on-line è un approccio promettente per costruire sui successi della RL profonda.La pianificazione on-line con alberi look-ahead ha dimostrato di avere successo in ambienti in cui i modelli di transizione sono noti a priori.Tuttavia, in ambienti complessi in cui i modelli di transizione devono essere appresi dai dati, le carenze dei modelli appresi hanno limitato la loro utilità per la pianificazione.Per affrontare queste sfide, proponiamo TreeQN, un modello differenziabile, ricorsivo, strutturato ad albero che serve come sostituzione drop-in per qualsiasi rete di funzioni valore nella RL profonda con azioni discrete. TreeQN costruisce dinamicamente un albero applicando ricorsivamente un modello di transizione in uno spazio di stato astratto appreso e poi aggregando le ricompense previste e i valori di stato usando un albero di backup per stimare i valori Q. Proponiamo anche ATreeC, una variante actor-critic che aumenta TreeQN con un livello softmax per formare una rete di policy stocastica. Entrambi gli approcci sono addestrati end-to-end, in modo che il modello appreso sia ottimizzato per il suo uso effettivo nell'albero.Mostriamo che TreeQN e ATreeC superano n-step DQN e A2C su un compito di box-pushing, così come n-step DQN e reti di previsione del valore (Oh et al, 2017) su più giochi Atari.Inoltre, presentiamo studi di ablazione che dimostrano l'effetto di diverse perdite ausiliarie sull'apprendimento di modelli di transizione.
Classificazione multi-etichetta (MLC) è il compito di assegnare un insieme di etichette di destinazione per un dato campione.Modellazione delle interazioni combinatorie etichetta in MLC è stato un lungo-haul challenge.Recurrent rete neurale (RNN) modelli basati encoder-decoder hanno mostrato state-of-the-art prestazioni per risolvere MLC.Tuttavia, la natura sequenziale di modellazione dipendenze etichetta attraverso un RNN limita la sua capacità di calcolo parallelo, prevedendo etichette dense, e fornendo risultati interpretabili. In questo articolo, proponiamo le reti Message Passing Encoder-Decoder (MPED), che mirano a fornire un modello veloce, accurato e interpretabile di MLC. Le reti MPED modellano la previsione congiunta delle etichette sostituendo tutte le RNN nell'architettura encoder-decoder con meccanismi di passaggio dei messaggi e rinunciando completamente all'inferenza autoregressiva.  I modelli proposti sono semplici, veloci, accurati, interpretabili e indipendenti dalla struttura (possono essere utilizzati su dati strutturati noti o sconosciuti). Gli esperimenti su sette set di dati MLC del mondo reale mostrano che i modelli proposti superano i modelli RNN autoregressivi in cinque diverse metriche con un significativo aumento della velocità durante i tempi di formazione e test.
I recenti algoritmi di apprendimento few-shot hanno permesso ai modelli di adattarsi rapidamente a nuovi compiti basati solo su pochi campioni di formazione.I precedenti lavori di apprendimento few-shot si sono concentrati principalmente sulla classificazione e sull'apprendimento di rinforzo.In questo articolo, proponiamo un sistema di meta-apprendimento few-shot che si concentra esclusivamente sui compiti di regressione.Il nostro modello si basa sull'idea che il grado di libertà della funzione sconosciuta può essere significativamente ridotto se viene rappresentata come una combinazione lineare di un insieme di funzioni base sparsificanti. Progettiamo una rete Basis Function Learner per codificare le funzioni di base per una distribuzione di compiti, e una rete Weights Generator per generare il vettore di pesi per un nuovo compito. Mostriamo che il nostro modello supera lo stato attuale dei metodi di meta-apprendimento in vari compiti di regressione.
Il modello linguistico pre-addestrato su larga scala, come il BERT, ha recentemente ottenuto un grande successo in una vasta gamma di compiti di comprensione del linguaggio; tuttavia, rimane una questione aperta su come utilizzare il BERT per compiti di generazione del testo, insegnante) viene poi sfruttato come supervisione extra per migliorare i modelli convenzionali Seq2Seq (cioè l'allievo) per la generazione di testo, Sfruttando la natura bidirezionale idiosincratica di BERT, la distillazione della conoscenza appresa da BERT può incoraggiare i modelli auto-regressivi Seq2Seq a pianificare in anticipo, imponendo una supervisione globale a livello di sequenza per una generazione di testo coerente.Gli esperimenti mostrano che l'approccio proposto supera significativamente le forti linee di base di Transformer su più compiti di generazione di testo, compresa la traduzione automatica (MT) e il riassunto del testo.Il nostro modello proposto raggiunge anche nuovi risultati all'avanguardia sui set di dati IWSLT tedesco-inglese e inglese-vietnamita MT.
Molti studi hanno suggerito che questa caratteristica della visione umana deriva dall'interazione tra i segnali feedforward provenienti dai percorsi bottom-up della corteccia visiva e i segnali di feedback forniti dai percorsi top-down. Motivati da tale interazione, proponiamo un nuovo modello neuro-ispirato, ovvero le reti neurali convoluzionali con feedback (CNN-F).  Dimostriamo che l'inferenza iterativa di CNN-F permette il disentanglement delle variabili latenti attraverso gli strati.Convalidiamo i vantaggi di CNN-F rispetto alla CNN di base.I nostri risultati sperimentali suggeriscono che la CNN-F è più robusta alla degradazione dell'immagine come il rumore dei pixel, l'occlusione e la sfocatura.  Inoltre, dimostriamo che la CNN-F è in grado di ripristinare le immagini originali da quelle degradate con un'alta accuratezza di ricostruzione mentre introduce artefatti trascurabili.
Sviluppiamo nuove teorie di approssimazione e di apprendimento statistico delle reti neurali convoluzionali (CNN) attraverso la struttura di tipo ResNet in cui la dimensione del canale, la dimensione del filtro e la larghezza sono fisse. Si dimostra che una CNN di tipo ResNet è un approssimatore universale e la sua capacità di espressione non è peggiore delle reti neurali completamente connesse (FNN) con una struttura \textit{block-sparse} anche se la dimensione di ogni strato nella CNN è fissa. Il nostro risultato è generale nel senso che possiamo tradurre automaticamente qualsiasi tasso di approssimazione raggiunto da FNNs block-sparse in quello di CNNs.Grazie alla teoria generale, si dimostra che l'apprendimento su CNNs soddisfa l'ottimalità nell'approssimazione e nella stima di diverse importanti classi di funzioni. Come applicazioni, consideriamo due tipi di classi di funzioni da stimare: la classe Barron e la classe più vecchia di H. Dimostriamo che lo stimatore di minimizzazione del rischio empirico (ERM) ritagliato può raggiungere lo stesso tasso di FNNs anche se la dimensione del canale, la dimensione del filtro e la larghezza delle CNNs sono costanti rispetto alla dimensione del campione. La nostra dimostrazione si basa su sofisticate valutazioni del numero di copertura delle CNN e sulla tecnica non banale di ridimensionamento dei parametri per controllare la costante di Lipschitz delle CNN da costruire.
Generare i pesi di classificazione è stato applicato in molti approcci di meta-apprendimento per la classificazione di immagini di pochi colpi grazie alla sua semplicità ed efficacia. Tuttavia, sosteniamo che è difficile generare i pesi di classificazione esatti e universali per tutti i diversi campioni di query da pochissimi campioni di formazione. In questo lavoro, introduciamo la generazione di pesi attentivi per l'apprendimento di pochi colpi tramite la massimizzazione delle informazioni (AWGIM), che affronta i problemi attuali con due nuovi contributi. i) AWGIM genera diversi pesi di classificazione per diversi campioni di query lasciando che ciascuno dei campioni di query si occupi dell'intero set di supporto.ii) Per garantire che i pesi generati si adattino ai diversi campioni di query, riformuliamo il problema per massimizzare il limite inferiore dell'informazione reciproca tra i pesi generati e la query e i dati di supporto.Per quanto possiamo vedere, questo è il primo tentativo di unificare la massimizzazione dell'informazione nell'apprendimento di pochi colpi.Entrambi i contributi si sono dimostrati efficaci negli esperimenti estesi e dimostriamo che AWGIM è in grado di ottenere prestazioni all'avanguardia su serie di dati di riferimento.
La risposta alle domande conversazionali (CQA) è un nuovo compito di QA che richiede la comprensione del contesto del dialogo. Diverso dalla tradizionale comprensione di lettura automatica a turno singolo (MRC), CQA è un compito completo che comprende la lettura del passaggio, la risoluzione delle coreferenze e la comprensione contestuale. Il nostro modello sfrutta sia l'inter-attenzione che l'auto-attenzione per comprendere la conversazione e il passaggio.Inoltre, dimostriamo un nuovo metodo per integrare il modello contestuale BERT come un sottomodulo nella nostra rete.I risultati empirici mostrano l'efficacia di SDNet.Sulla classifica CoQA, supera il punteggio F1 del modello migliore precedente del 1,6%.Il nostro modello ensemble migliora ulteriormente il punteggio F1 del 2,7%.
Le reti generative avversarie (GAN) sono uno degli approcci più popolari quando si tratta di addestrare modelli generativi, tra i quali le varianti di Wasserstein GAN sono considerate superiori alla formulazione GAN standard in termini di stabilità di apprendimento e qualità del campione.Tuttavia, le GAN di Wasserstein richiedono che il critico sia 1-Lipschitz, che è spesso applicato implicitamente penalizzando la norma del suo gradiente, o limitando globalmente la sua costante Lipschitz tramite tecniche di normalizzazione dei pesi. L'addestramento con un termine di regolarizzazione che penalizza esplicitamente la violazione del vincolo di Lipschitz, invece che attraverso la norma del gradiente, è risultato praticamente inattuabile nella maggior parte delle situazioni. Ispirati dall'addestramento virtuale avversario, proponiamo un metodo chiamato Adversarial Lipschitz Regularization, e dimostriamo che l'utilizzo di una penalizzazione esplicita di Lipschitz è effettivamente fattibile e porta a prestazioni competitive quando applicato a Wasserstein GANs, evidenziando un importante collegamento tra la regolarizzazione di Lipschitz e l'addestramento avversario.
L'apprendimento multi-task promette di utilizzare meno dati, parametri e tempo rispetto all'addestramento di modelli separati single-task, ma realizzare questi benefici nella pratica è impegnativo, in particolare è difficile definire un'architettura adatta che abbia abbastanza capacità per supportare molti compiti senza richiedere un calcolo eccessivo per ogni singolo compito, ci sono difficili compromessi quando si decide come allocare parametri e livelli in un grande insieme di compiti. Definiamo una parametrizzazione delle strategie di condivisione delle caratteristiche per un'efficace copertura e campionamento delle architetture. Presentiamo anche un metodo per una rapida valutazione di tali architetture con la distillazione delle caratteristiche. Insieme questi contributi ci permettono di ottimizzare rapidamente per modelli multi-task efficienti dal punto di vista dei parametri.
Con lo sviluppo e la diversificazione degli approcci distribuiti alla semantica del linguaggio naturale, gli embedders per le unità linguistiche più grandi delle parole (ad esempio, le frasi) hanno assunto un ruolo sempre più importante.  Fino ad oggi, tali embedders sono stati valutati utilizzando compiti di riferimento (ad esempio, GLUE) e sonde linguistiche.  Noi proponiamo un approccio comparativo, nearest neighbor overlap (N2O), che quantifica la somiglianza tra embedders in un modo indipendente dal compito.  N2O richiede solo una collezione di esempi ed è semplice da capire: due embedders sono più simili se, per lo stesso set di input, c'è una maggiore sovrapposizione tra i vicini più vicini degli input.  Usiamo N2O per confrontare 21 embedder di frasi e mostrare gli effetti di diverse scelte di design e architetture.
I Generative Adversarial Networks (GANs) possono raggiungere lo stato dell'arte della qualità del campione in compiti di modellazione generativa, ma soffrono del problema del collasso della modalità.Variational Autoencoders (VAE) d'altra parte massimizzano esplicitamente una log-likelihood dei dati basata sulla ricostruzione costringendola a coprire tutte le modalità, ma soffrono di una qualità del campione più scarsa.Lavori recenti hanno proposto quadri VAE-GAN ibridi che integrano una likelihood sintetica basata su GAN all'obiettivo VAE per affrontare sia il collasso della modalità che i problemi di qualità del campione, con successo limitato. Questo perché l'obiettivo VAE forza un trade-off tra la log-likelihood dei dati e la divergenza rispetto al priore latente.Il termine del rapporto di likelihood sintetico mostra anche instabilità durante l'addestramento.Proponiamo un nuovo obiettivo con un costo di ricostruzione ``"Best-of-Many-Samples" e una stima diretta stabile della likelihood sintetica.Questo permette alla nostra struttura ibrida VAE-GAN di raggiungere un'alta log-likelihood dei dati e una bassa divergenza rispetto al priore latente allo stesso tempo e mostra un miglioramento significativo rispetto sia alle VAE-GAN ibride che alle GAN semplici nella copertura e qualità della modalità.
Al fine di imitare l'abilità umana di acquisizione e trasferimento continuo della conoscenza attraverso vari compiti, un sistema di apprendimento ha bisogno della capacità di apprendimento permanente, utilizzando efficacemente le competenze precedentemente acquisite.In quanto tale, la sfida chiave è quella di trasferire e generalizzare la conoscenza appresa da un compito ad altri compiti, evitando interferenze dalla conoscenza precedente e migliorando le prestazioni complessive.In questo documento, all'interno del paradigma di apprendimento continuo, introduciamo un metodo che effettivamente dimentica i campioni di dati meno utili continuamente attraverso diversi compiti. Il metodo utilizza informazioni statistiche sul punteggio di leva per misurare l'importanza dei campioni di dati in ogni compito e adotta un approccio di direzioni frequenti per consentire una proprietà di apprendimento continuo.Questo mantiene efficacemente una dimensione di formazione costante attraverso tutti i compiti.Forniamo prima alcune intuizioni matematiche per il metodo e poi dimostriamo la sua efficacia con esperimenti su varianti di MNIST e CIFAR100 datasets.
Le reti neurali convoluzionali (CNN) sono intrinsecamente equivarianti alla traslazione.Gli sforzi per incorporare altre forme di equivarianza si sono concentrati esclusivamente sulla rotazione.Noi espandiamo la nozione di equivarianza nelle CNN attraverso la rete di trasformazione polare (PTN).PTN combina idee dalla rete di trasformazione spaziale (STN) e rappresentazioni di coordinate canoniche.Il risultato è una rete invariante alla traduzione ed equivariante sia alla rotazione che alla scala. PTN è addestrata end-to-end e composta da tre fasi distinte: un predittore di origine polare, il modulo di trasformazione polare di nuova introduzione e un classificatore. PTN raggiunge lo stato dell'arte su MNIST ruotato e sul nuovo set di dati SIM2MNIST, una variazione MNIST ottenuta aggiungendo clutter e perturbando le cifre con traslazione, rotazione e scala.
La modellazione gerarchica bayesiana fornisce un quadro teorico per formalizzare il meta-apprendimento come inferenza per un insieme di parametri che sono condivisi tra i compiti. Qui, riformuliamo l'algoritmo di meta-apprendimento model-agnostic (MAML) di Finn et al. (2017) come un metodo per l'inferenza probabilistica in un modello bayesiano gerarchico. In contrasto con i metodi precedenti per il meta-apprendimento tramite Bayes gerarchico, MAML è naturalmente applicabile a approssimazioni di funzioni complesse attraverso il suo uso di una procedura di discesa del gradiente scalabile per l'inferenza posteriore. Inoltre, l'identificazione di MAML come Bayes gerarchico fornisce un modo per comprendere il funzionamento dell'algoritmo come una procedura di meta-apprendimento, così come l'opportunità di fare uso di strategie computazionali per un'inferenza efficiente.Usiamo questa opportunità per proporre un miglioramento dell'algoritmo MAML che fa uso di tecniche di inferenza approssimativa e stima della curvatura.
Questo lavoro fornisce un'architettura di modellazione automatica per l'apprendimento automatico delle macchine (AutoML) chiamata Autostacker.Autostacker migliora l'accuratezza di previsione delle linee di base dell'apprendimento automatico utilizzando un'innovativa architettura di impilamento gerarchico e un efficiente algoritmo di ricerca dei parametri.Non sono necessarie né conoscenze di dominio precedenti sui dati né la pre-elaborazione delle caratteristiche.Riduciamo significativamente il tempo di AutoML con un algoritmo di ispirazione naturale - Parallel Hill Climbing (PHC).Parallelizzando PHC, Autostacker può fornire pipeline candidate con sufficiente precisione di previsione in un breve lasso di tempo. Concentrandosi sul processo di modellazione, Autostacker rompe la tradizione di seguire pipeline di ordine fisso esplorando non solo pipeline di modelli singoli ma anche combinazioni e strutture innovative.Come mostreremo nella sezione degli esperimenti, Autostacker raggiunge prestazioni significativamente migliori sia in termini di accuratezza del test che di costo del tempo, confrontando con le prove iniziali umane e il recente e popolare sistema AutoML.
I modelli surrogati possono essere usati per accelerare il calcolo bayesiano approssimativo (ABC).In un tale quadro la discrepanza tra i dati simulati e osservati è modellata con un processo gaussiano.Finora sono state proposte strategie di principio solo per la selezione sequenziale dei luoghi di simulazione.Per affrontare questa limitazione, sviluppiamo strategie di progettazione ottimale bayesiana per parallelizzare le simulazioni costose.Tocchiamo anche il problema di quantificare l'incertezza del posteriore ABC a causa del budget limitato delle simulazioni.
Gli algoritmi di ottimizzazione del primo ordine inizializzati in modo casuale sono il metodo di scelta per la soluzione di molti problemi non convessi ad alta dimensione nell'apprendimento automatico, tuttavia le garanzie teoriche generali non possono escludere la convergenza verso punti critici del valore obiettivo povero.Per alcuni problemi non convessi altamente strutturati tuttavia, il successo della discesa del gradiente può essere compreso studiando la geometria dell'obiettivo. I tassi risultanti scalano come polinomi di basso ordine nella dimensione anche se l'obiettivo possiede un numero esponenziale di punti di sella. Questa convergenza efficiente può essere vista come una conseguenza della curvatura negativa normale ai collettori stabili associati ai punti di sella, e forniamo la prova che questa caratteristica è condivisa da altri problemi non convessi di importanza pure.
La teoria della codifica è una disciplina centrale alla base dei modem wireline e wireless che sono i cavalli di battaglia dell'era dell'informazione. Il progresso nella teoria della codifica è in gran parte guidato dal singolo ingegno umano con scoperte sporadiche nel corso del secolo scorso. In questo articolo studiamo se è possibile automatizzare la scoperta di algoritmi di decodifica tramite l'apprendimento profondo. Mostriamo che le architetture RNN progettate e addestrate creativamente possono decodificare codici sequenziali ben noti come i codici convoluzionali e turbo con prestazioni vicine a quelle ottimali sul canale AWGN (additive white Gaussian noise), che a sua volta è raggiunto da algoritmi di svolta dei nostri tempi (decodificatori Viterbi e BCJR, che rappresentano la programmazione dinamica e algoritmi forward-backward), ci addestriamo ad uno specifico rapporto segnale-rumore e lunghezza del blocco, ma testiamo un'ampia gamma di queste quantità, così come la robustezza e l'adattabilità alle deviazioni dall'impostazione AWGN.
I ricercatori hanno recentemente proposto diversi algoritmi per evitare il problema della non convergenza di Adam, ma la loro efficienza si è rivelata insoddisfacente nella pratica. In questo articolo, forniamo una nuova comprensione del problema della non convergenza di Adam e di altri metodi di apprendimento adattivi. Sosteniamo che esiste una correlazione inappropriata tra il gradiente $g_t$ e il termine del secondo momento $v_t$ in Adam ($t$ è il timetep), il che risulta nel fatto che un grande gradiente è probabile che abbia un piccolo step size mentre un piccolo gradiente può avere un grande step size. Dimostriamo che tali dimensioni sbilanciate del passo sono la causa fondamentale della non convergenza di Adam, e dimostriamo inoltre che decorrelando $v_t$ e $g_t$ si otterrà una dimensione del passo imparziale per ogni gradiente, risolvendo così il problema della non convergenza di Adam.Infine, proponiamo AdaShift, un nuovo metodo adattivo di tasso di apprendimento che decorrela $v_t$ e $g_t$ tramite spostamento temporale, cioè, I risultati degli esperimenti dimostrano che AdaShift è in grado di affrontare il problema della non-convergenza di Adam, pur mantenendo una performance competitiva con Adam sia in termini di velocità di formazione che di generalizzazione.
La maggior parte dei metodi di adattamento al dominio considerano il problema di trasferire la conoscenza al dominio di destinazione da un singolo set di dati sorgente.Tuttavia, nelle applicazioni pratiche, abbiamo tipicamente accesso a fonti multiple.In questo articolo proponiamo il primo approccio per il Multi-Source Domain Adaptation (MSDA) basato su Generative Adversarial Networks.Il nostro metodo è ispirato dall'osservazione che l'aspetto di una data immagine dipende da tre fattori: il dominio, lo stile (caratterizzato in termini di variazioni di caratteristiche di basso livello) e il contenuto. Per questo motivo proponiamo di proiettare le caratteristiche dell'immagine su uno spazio in cui solo la dipendenza dal contenuto viene mantenuta, e poi riproiettare questa rappresentazione invariante sullo spazio dei pixel utilizzando il dominio e lo stile di destinazione.In questo modo, nuove immagini etichettate possono essere generate che vengono utilizzate per addestrare un classificatore finale di destinazione.Testiamo il nostro approccio utilizzando comuni benchmark MSDA, dimostrando che supera i metodi allo stato dell'arte.
L'inferenza della configurazione più probabile per un sottoinsieme di variabili di una distribuzione congiunta date le rimanenti - a cui ci riferiamo come co-generazione - è una sfida importante che è computazionalmente impegnativa per tutte le impostazioni tranne le più semplici.Questo compito ha ricevuto una notevole quantità di attenzione, in particolare per i modi classici di modellazione delle distribuzioni come la predizione strutturata. Al contrario, non si sa quasi nulla di questo compito quando si considerano le tecniche recentemente proposte per la modellazione di distribuzioni ad alta dimensione, in particolare le reti generative avversarie (GANs).Pertanto, in questo articolo, studiamo le sfide che si presentano per la co-generazione con le GANs.Per affrontare queste sfide, sviluppiamo un algoritmo di co-generazione basato su Hamiltonian Monte Carlo (HMC) annealed importance sampling (AIS).L'approccio presentato supera significativamente i metodi classici basati sul gradiente su dati sintetici e su CelebA.
I modelli probabilistici impliciti sono modelli definiti naturalmente in termini di una procedura di campionamento e spesso inducono una funzione di verosimiglianza che non può essere espressa esplicitamente.Sviluppiamo un semplice metodo per stimare i parametri nei modelli impliciti che non richiede la conoscenza della forma della funzione di verosimiglianza o di qualsiasi quantità derivata, ma può essere dimostrato di essere equivalente alla massimizzazione della verosimiglianza sotto alcune condizioni.Il nostro risultato vale nell'impostazione parametrica non asintotica, dove sia la capacità del modello che il numero di esempi di dati sono finiti.Dimostriamo anche risultati sperimentali incoraggianti.
Mentre la maggior parte degli approcci al problema dell'Inverse Reinforcement Learning (IRL) si concentrano sulla stima di una funzione di ricompensa che spieghi al meglio la politica di un agente esperto o il comportamento dimostrato su un compito di controllo, è spesso il caso che tale comportamento sia più succintamente rappresentato da una semplice ricompensa combinata con un insieme di vincoli difficili. Noi riformuliamo il problema dell'IRL sui processi decisionali di Markov (MDP) in modo tale che, dato un modello nominale dell'ambiente e una funzione di ricompensa nominale, cerchiamo di stimare i vincoli di stato, azione e caratteristica nell'ambiente che motivano il comportamento di un agente. Il nostro approccio si basa sul framework Maximum Entropy IRL, che ci permette di ragionare sulla probabilità delle dimostrazioni di un agente esperto data la nostra conoscenza di un MDP.Usando il nostro metodo, possiamo dedurre quali vincoli possono essere aggiunti al MDP per aumentare al massimo la probabilità di osservare queste dimostrazioni.Presentiamo un algoritmo che infila iterativamente il Maximum Likelihood Constraint per spiegare al meglio il comportamento osservato, e valutiamo la sua efficacia usando sia un comportamento simulato che dati registrati di umani che navigano intorno a un ostacolo.
Il successo del reinforcement learning nel mondo reale è stato limitato a scenari di laboratorio strumentati, spesso richiedendo un'ardua supervisione umana per permettere un apprendimento continuo. In questo lavoro, discutiamo gli elementi necessari di un sistema robotico che può continuamente e autonomamente migliorare con i dati raccolti nel mondo reale, e proponiamo una particolare istanza di un tale sistema. Successivamente, esaminiamo una serie di sfide dell'apprendimento senza strumentazione - compreso la mancanza di azzeramenti episodici, di stima dello stato e di ricompense costruite a mano - e proponiamo le soluzioni semplici e scalabili a questi challenge.We dimostrano l'efficacia del nostro sistema proposto sulle mansioni abili di manipolazione robotica nella simulazione e nel mondo reale ed inoltre forniscono un'analisi penetrante e uno studio di ablazione delle sfide connesse con questo paradigma imparante.
La maggior parte dei lavori precedenti richiedono o corpora paralleli o una quantità sufficiente di dati di allenamento da un parlante di destinazione, ma noi convertiamo una frase arbitraria di un parlante di origine arbitraria in quella di un parlante di destinazione data solo una frase di allenamento del parlante di destinazione. Per raggiungere questo obiettivo, formuliamo il problema come apprendimento di rappresentazioni dissociate specifiche del parlante e del contesto e seguiamo l'idea di [1] che utilizza il Factorized Hierarchical Variational Autoencoder (FHVAE). Dopo aver addestrato FHVAE su dati di addestramento di più parlanti, dato un discorso arbitrario del parlante di origine e di destinazione, stimiamo queste rappresentazioni latenti e poi ricostruiamo il discorso desiderato della voce convertita in quello del parlante di destinazione. Usiamo un corpus vocale multilingue per apprendere un modello universale che funziona per tutte le lingue, esaminiamo l'uso di un embedding linguistico one-hot per condizionare il modello sulla lingua dell'enunciato che viene interrogato e mostriamo l'efficacia dell'approccio. conduciamo esperimenti di conversione della voce con dimensioni variabili di enunciati di allenamento e siamo stati in grado di raggiungere prestazioni ragionevoli anche con un solo enunciato di allenamento. Indaghiamo anche l'effetto dell'utilizzo o meno del condizionamento linguistico.Inoltre, visualizziamo le incorporazioni delle diverse lingue e dei sessi.Infine, nei test soggettivi, per una lingua e la conversione vocale interlinguistica, il nostro approccio ha raggiunto risultati moderatamente migliori o comparabili rispetto alla linea di base nella qualità del discorso e nella somiglianza.
Proponiamo un modulo di attenzione addestrabile end-to-end per architetture di reti neurali convoluzionali (CNN) costruite per la classificazione delle immagini. Il modulo prende come input le mappe vettoriali di caratteristiche 2D che formano le rappresentazioni intermedie dell'immagine di input in diverse fasi della pipeline CNN, e produce una matrice 2D di punteggi per ogni mappa. Le architetture CNN standard sono modificate attraverso l'incorporazione di questo modulo, e addestrate sotto il vincolo che una combinazione convessa dei vettori di caratteristiche intermedie 2D, come parametrato dalle matrici di punteggio, deve essere utilizzato solo per la classificazione.Incentivato ad amplificare il rilevante e sopprimere l'irrilevante o fuorviante, i punteggi assumono così il ruolo di valori di attenzione. Le nostre osservazioni sperimentali forniscono una chiara prova di questo effetto: le mappe di attenzione apprese evidenziano chiaramente le regioni di interesse mentre sopprimono il disordine dello sfondo. Di conseguenza, la funzione proposta è in grado di eseguire il bootstrap delle architetture CNN standard per il compito di classificazione delle immagini, dimostrando una generalizzazione superiore su 6 set di dati di riferimento non visti. Quando binarised, le nostre mappe di attenzione superano altre mappe di attenzione basate su CNN, le mappe di salienza tradizionali e le proposte di oggetto superiore per la segmentazione debolmente supervisionata come dimostrato sul set di dati Object Discovery.
La rete neurale ricorrente (RNN) è una rete neurale efficace nel risolvere compiti supervisionati e non supervisionati molto complessi. C'è stato un miglioramento significativo nel campo della RNN come l'elaborazione del linguaggio naturale, l'elaborazione del discorso, la visione artificiale e altri domini multipli. La rete viene eseguita fino a 1000 epoche con un tasso di apprendimento impostato nell'intervallo da 0.01 a 0.5. Ovviamente, la RNN si è comportata molto bene rispetto ai classici algoritmi di apprendimento automatico, soprattutto perché la RNN estrae implicitamente le caratteristiche sottostanti e identifica anche le caratteristiche dei dati, il che porta a una migliore precisione.
Gli studi anatomici dimostrano che il cervello riformatta le informazioni di input per generare risposte affidabili per l'esecuzione di calcoli.Tuttavia, rimane poco chiaro come i circuiti neurali codificano modelli spazio-temporali complessi.Mostriamo che le dinamiche neurali sono fortemente influenzate dall'allineamento di fase tra l'input e l'attività caotica spontanea. L'allineamento dell'input lungo le proiezioni caotiche dominanti fa sì che le traiettorie caotiche diventino canali stabili (o attrattori), migliorando così la capacità computazionale di una rete ricorrente. Usando l'analisi del campo medio, deriviamo l'impatto dell'allineamento dell'input sulla stabilità complessiva degli attrattori formati. I nostri risultati indicano che l'allineamento dell'input determina l'entità della soppressione intrinseca del rumore e quindi altera la stabilità dello stato di attrazione, controllando così la capacità di inferenza della rete.
Le reti generative avversarie sono una struttura di apprendimento che si basa sull'addestramento di un discriminatore per stimare una misura della differenza tra un obiettivo e le distribuzioni generate. Le GAN, come normalmente formulate, si basano sul fatto che i campioni generati siano completamente differenziabili rispetto ai parametri generativi, e quindi non funzionano per dati discreti. Introduciamo un metodo per l'addestramento delle GAN con dati discreti che utilizza la misura della differenza stimata dal discriminatore per calcolare i pesi di importanza per i campioni generati, fornendo così un gradiente di politica per l'addestramento del generatore.I pesi di importanza hanno una forte connessione con il confine di decisione del discriminatore, e chiamiamo il nostro metodo boundary-seeking GANs (BGANs).Dimostriamo l'efficacia dell'algoritmo proposto con un'immagine discreta e la generazione di linguaggio naturale basata sui caratteri.  Inoltre, l'obiettivo boundary-seeking si estende ai dati continui, che possono essere utilizzati per migliorare la stabilità della formazione, e lo dimostriamo su Celeba, Large-scale Scene Understanding (LSUN) camere da letto, e Imagenet senza condizionamento.
I metodi del gradiente delle politiche hanno goduto di grande successo nell'apprendimento di rinforzo profondo, ma soffrono di un'alta varianza delle stime del gradiente. Il problema dell'alta varianza è particolarmente esasperato nei problemi con orizzonti lunghi o spazi d'azione ad alta dimensione. Per mitigare questo problema, deriviamo una linea di base azione-dipendente senza bias per la riduzione della varianza che sfrutta completamente la forma strutturale della politica stocastica stessa e non fa alcuna ipotesi aggiuntiva sul MDP. Dimostriamo e quantifichiamo il beneficio della linea di base azione-dipendente attraverso sia l'analisi teorica che i risultati numerici, compresa un'analisi della subottimalità della linea di base ottimale stato-dipendente. Il risultato è un algoritmo efficiente dal punto di vista computazionale, che si adatta a problemi di controllo ad alta densità, come dimostrato da un compito sintetico di 2000 dimensioni di corrispondenza dell'obiettivo. I nostri risultati sperimentali indicano che le linee di base dipendenti dall'azione permettono un apprendimento più veloce su benchmark standard di apprendimento di rinforzo e su compiti sintetici e di manipolazione della mano ad alta densità.Infine, dimostriamo che l'idea generale di includere informazioni aggiuntive nelle linee di base per una migliore riduzione della varianza può essere estesa a compiti parzialmente osservati e multi-agente.
Il costo di annotare i dati di addestramento è stato tradizionalmente un collo di bottiglia per gli approcci di apprendimento supervisionato.Il problema è ulteriormente aggravato quando l'apprendimento supervisionato è applicato a un certo numero di compiti correlati simultaneamente, poiché la quantità di etichette richieste scala con il numero di compiti.Per mitigare questa preoccupazione, proponiamo un algoritmo attivo di apprendimento multitask che realizza il trasferimento di conoscenza tra i compiti. L'approccio forma un cosiddetto comitato per ogni compito che prende decisioni congiuntamente e condivide direttamente i dati tra compiti simili. Il nostro approccio riduce il numero di query necessarie durante l'addestramento, mantenendo un'elevata precisione sui dati di prova.
Il rilevamento della manipolazione delle foto si basa su sottili tracce statistiche, notoriamente rimosse dalla compressione lossy aggressiva impiegata online.dimostriamo che la modellazione end-to-end di complessi canali di diffusione delle foto permette l'ottimizzazione del codec con obiettivi di provenienza espliciti. I nostri risultati dimostrano che miglioramenti significativi nell'accuratezza del rilevamento della manipolazione sono possibili a costi frazionari in termini di larghezza di banda/storage. Il nostro codec ha migliorato l'accuratezza dal 37% all'86% anche a bit-rate molto bassi, ben al di sotto della praticità di JPEG (QF 20).
Le reti neurali ricorrenti sono state a lungo la scelta dominante per la modellazione delle sequenze, ma soffrono gravemente di due problemi: impotenza nel catturare le dipendenze a lungo termine e incapacità di parallelizzare la procedura di calcolo sequenziale. Pertanto, molti modelli di sequenza non ricorrenti che sono costruiti su operazioni di convoluzione e attenzione sono stati proposti recentemente. Nonostante il loro successo, tuttavia, questi modelli mancano dei componenti necessari per modellare le strutture locali nelle sequenze e si basano pesantemente su embeddings di posizione che hanno effetti limitati e richiedono una notevole quantità di sforzi di progettazione.In questo articolo, proponiamo il R-Transformer che gode dei vantaggi di entrambe le RNN e del meccanismo di attenzione multitesta mentre evita i loro rispettivi svantaggi. Il modello proposto può catturare efficacemente sia le strutture locali che le dipendenze globali a lungo termine nelle sequenze senza alcun uso di embeddings di posizione.Valutiamo R-Transformer attraverso ampi esperimenti con dati da una vasta gamma di domini e i risultati empirici mostrano che R-Transformer supera i metodi all'avanguardia con un ampio margine nella maggior parte dei compiti.
Questo livello di controllo a grana fine può essere difficile da ottenere in modelli di rete neurale su larga scala. In questo lavoro, proponiamo un approccio strutturato a variabili latenti che aggiunge stati di controllo discreti all'interno di un paradigma neurale autoregressivo standard. In questa formulazione, possiamo includere una serie di vincoli ricchi e posteriori per applicare la conoscenza specifica del compito che è effettivamente addestrata nel modello neurale. Questo approccio ci permette di fornire un grounding arbitrario delle decisioni interne del modello, senza sacrificare alcun potere rappresentazionale dei modelli neurali. Gli esperimenti considerano le applicazioni di questo approccio per la generazione di testo e l'induzione part-of-speech.Per la generazione del linguaggio naturale, troviamo che questo metodo migliora rispetto ai benchmark standard, fornendo anche un controllo a grana fine.
Supponiamo che un modello di classificazione profonda sia addestrato con campioni che devono essere mantenuti privati per motivi di privacy o riservatezza.In questa situazione, un avversario può ottenere i campioni privati se il modello di classificazione è dato all'avversario? Chiamiamo questo reverse engineering contro il modello di classificazione l'attacco Classifier-to-Generator (C2G).Questa situazione si verifica quando il modello di classificazione è incorporato in dispositivi mobili per la previsione offline (ad es, Per l'attacco C2G, introduciamo un nuovo GAN, PreImageGAN. In PreImageGAN, il generatore è progettato per stimare la distribuzione del campione condizionata dalla preimmagine del modello di classificazione $f$, $P(X|f(X)=y)$, dove $X$ è la variabile casuale nello spazio del campione e $y$ è il vettore di probabilità che rappresenta l'etichetta di destinazione arbitraria specificata dall'avversario. Nel riconoscimento dei caratteri, dimostriamo che, dato un modello di riconoscimento di cifre scritte a mano, PreImageGAN permette all'avversario di estrarre immagini di lettere dell'alfabeto senza sapere che il modello è costruito per immagini di lettere dell'alfabeto.Nel riconoscimento del volto, dimostriamo che, quando un avversario ottiene un modello di riconoscimento del volto per un insieme di individui, PreImageGAN permette all'avversario di estrarre immagini del volto di specifici individui contenuti nell'insieme, anche quando l'avversario non ha conoscenza del volto degli individui.
L'obiettivo del rilevamento compressivo standard è quello di stimare un vettore sconosciuto da misurazioni lineari sotto l'assunzione di sparsità in qualche base.Recentemente, è stato dimostrato che possono essere richieste significativamente meno misurazioni se l'assunzione di sparsità è sostituita dall'assunzione che il vettore sconosciuto si trova vicino al range di un modello generativo opportunamente scelto.  In particolare, in (Bora {\em et al.}, 2017) è stato dimostrato che approssimativamente $O(k\log L)$ misure gaussiane casuali sono sufficienti per un recupero accurato quando il modello generativo $k$-input è delimitato e $L$-Lipschitz, e che $O(kd \log w)$ misure sono sufficienti per $k$-input reti ReLU con profondità $d$ e larghezza $w$.  In questo articolo, stabiliamo i corrispondenti limiti inferiori indipendenti dall'algoritmo sulla complessità del campione usando gli strumenti dell'analisi statistica minimax.  In accordo con i limiti superiori di cui sopra, i nostri risultati sono riassunti come segue: (i) Costruiamo un modello generativo $L$-Lipschitz capace di generare segnali gruppo-sparsi, e dimostriamo che il numero necessario di misurazioni risultante è $\Omega(k \log L)$;(ii) Usando idee simili, costruiamo reti ReLU a due strati di larghezza elevata che richiedono $\Omega(k \log w)$ misure, così come reti ReLU profonde di larghezza inferiore che richiedono $\Omega(k d)$ misure.  Come risultato, stabiliamo che le leggi di scala derivate in (Bora {\em et al.}, 2017) sono ottimali o quasi ottimali in assenza di ulteriori ipotesi.
Scoprire e sfruttare la struttura causale nell'ambiente è una sfida cruciale per gli agenti intelligenti.Qui esploriamo se il moderno apprendimento di rinforzo profondo può essere utilizzato per addestrare gli agenti a eseguire il ragionamento causale.Adottiamo un approccio di meta-apprendimento, in cui l'agente impara una politica per condurre esperimenti tramite interventi causali, al fine di sostenere un compito successivo che premia il fare inferenze causali accurate.Abbiamo anche trovato l'agente potrebbe fare sofisticate previsioni controfattuali, così come imparare a trarre inferenze causali da dati puramente osservativi. Sebbene siano stati sviluppati potenti formalismi per il ragionamento causale, la loro applicazione nei domini del mondo reale può essere difficile perché l'adattamento a grandi quantità di dati ad alta dimensione spesso richiede di fare ipotesi idealizzate. I nostri risultati suggeriscono che il ragionamento causale in ambienti complessi può beneficiare di potenti approcci basati sull'apprendimento.Più in generale, questo lavoro può offrire nuove strategie per l'esplorazione strutturata nel reinforcement learning, fornendo agli agenti la capacità di eseguire e interpretare gli esperimenti.
La classificazione dei sentimenti è un'area di ricerca attiva con diverse applicazioni tra cui l'analisi delle opinioni politiche, la classificazione dei commenti, le recensioni di film, le recensioni di notizie e le recensioni di prodotti.Per impiegare la classificazione dei sentimenti basata su regole, abbiamo bisogno di lessici di sentimenti.Tuttavia, la costruzione manuale del lessico dei sentimenti richiede tempo e costi per lingue con risorse limitate. Per bypassare i tempi e i costi dello sviluppo manuale, abbiamo cercato di costruire lessici di sentimento amarico basandoci su un approccio basato sul corpus.L'intenzione di questo approccio è quella di gestire i termini di sentimento specifici della lingua amarica dal corpus amarico.Un piccolo set di termini seed viene preparato manualmente da tre parti del discorso come nome, aggettivo e verbo.Abbiamo sviluppato algoritmi per costruire lessici di sentimento amarico automaticamente dal corpus di notizie amarico. L'approccio basato sul corpus è proposto basandosi sull'incorporazione distributiva della co-occorrenza delle parole, inclusa l'incorporazione basata sulla frequenza (cioè la Positive Point-wise Mutual Information PPMI). Per prima cosa costruiamo la matrice di conteggio della frequenza di unigrammi parola-contesto e la trasformiamo in matrice point-wise mutual Information. Usando PPMI con un valore di soglia di 100 e 200, abbiamo ottenuto lessici di sentimento amarico basati sul corpus di dimensioni 1811 e 3794 rispettivamente, espandendo 519 semi.Infine, il lessico generato nell'approccio basato sul corpus viene valutato.
L'inizializzazione ottimistica è una strategia efficace per l'esplorazione efficiente nel reinforcement learning (RL). Nel caso tabellare, tutti gli algoritmi model-free provatamente efficienti si basano su di essa. Tuttavia, gli algoritmi RL profondi model-free non usano l'inizializzazione ottimistica nonostante si ispirino a questi algoritmi tabulari provatamente efficienti. In particolare, negli scenari con solo ricompense positive, i valori Q sono inizializzati ai valori più bassi possibili a causa degli schemi di inizializzazione della rete comunemente usati, un'inizializzazione pessimistica. La semplice inizializzazione della rete per produrre valori Q ottimistici non è sufficiente, poiché non possiamo garantire che rimangano ottimistici per nuove coppie stato-azione, che è cruciale per l'esplorazione. Proponiamo un semplice incremento basato sul conteggio dei valori Q inizializzati in modo pessimistico, che separa la fonte dell'ottimismo dalla rete neurale, e dimostriamo che questo schema è provabilmente efficiente nell'impostazione tabellare e lo estendiamo all'impostazione RL profonda. Il nostro algoritmo, Optimistic Pessimistically Initialised Q-Learning (OPIQ), aumenta le stime dei valori Q di un agente basato su DQN con bonus derivati dal conteggio per garantire l'ottimismo sia durante la selezione delle azioni che durante il bootstrapping. Mostriamo che OPIQ supera le varianti DQN non ottimistiche che utilizzano una motivazione intrinseca basata su pseudoconte in compiti di esplorazione difficili, e che predice stime ottimistiche per nuove coppie stato-azione.
Tuttavia, questi metodi soffrono tipicamente di due sfide principali: complessità del campione molto alta e proprietà di convergenza fragili, che richiedono una meticolosa messa a punto degli iperparametri. Entrambe queste sfide limitano gravemente l'applicabilità di tali metodi a domini complessi e reali. In questo quadro, l'attore mira a massimizzare la ricompensa attesa mentre massimizza anche l'entropia - cioè, riuscire nel compito mentre agisce nel modo più casuale possibile.I metodi RL profondi precedenti basati su questo quadro sono stati formulati come metodi off-policy Q-learning, o on-policy policy gradient. Combinando gli aggiornamenti off-policy con una formulazione stocastica stabile dell'attore-critico, il nostro metodo raggiunge prestazioni all'avanguardia su una serie di compiti di controllo continuo, superando i precedenti metodi on-policy e off-policy.Inoltre, dimostriamo che, in contrasto con altri algoritmi off-policy, il nostro approccio è molto stabile, ottenendo prestazioni molto simili attraverso diversi semi casuali.
I metodi più comuni in questo contesto sono il Behaviour Cloning (BC) e l'Inverse Reinforcement Learning (IRL). Metodi recenti per IRL hanno dimostrato la capacità di imparare politiche efficaci con l'accesso a un insieme molto limitato di dimostrazioni, uno scenario in cui i metodi BC spesso falliscono. Sfortunatamente, confrontare direttamente gli algoritmi di questi metodi non fornisce un'intuizione adeguata per comprendere questa differenza di prestazioni.Questo è il fattore motivante del nostro lavoro.Iniziamo presentando $f$-MAX, una generalizzazione di AIRL (Fu et al, 2018), un metodo IRL allo stato dell'arte.$f$-MAX fornisce le basi per confrontare più direttamente gli obiettivi per LfD.Dimostriamo che $f$-MAX, e per eredità AIRL, è un sottoinsieme del quadro IRL regolarizzato dai costi esposto da Ho & Ermon (2016).Concludiamo valutando empiricamente i fattori di differenza tra vari obiettivi LfD nel dominio del controllo continuo.
I metodi basati sul valore costituiscono una metodologia fondamentale nella pianificazione e nel deep reinforcement learning (RL).In questo articolo, proponiamo di sfruttare le strutture sottostanti della funzione di valore stato-azione, cioè, In particolare, se le dinamiche di sistema sottostanti portano ad alcune strutture globali della funzione Q, si dovrebbe essere in grado di inferire meglio la funzione sfruttando tali strutture. In particolare, indaghiamo la struttura a basso rango, che esiste ampiamente per le matrici di grandi dati. Verifichiamo empiricamente l'esistenza di funzioni Q a basso rango nel contesto del controllo e dei compiti di RL profonda (giochi Atari). Come nostro contributo chiave, sfruttando le tecniche di Matrix Estimation (ME), proponiamo un quadro generale per sfruttare la struttura di basso rango sottostante nelle funzioni Q, portando a una procedura di pianificazione più efficiente per il controllo classico e, inoltre, un semplice schema che può essere applicato a qualsiasi tecnica RL basata sul valore per ottenere costantemente prestazioni migliori su compiti "low-rank".
Le rappresentazioni apprese del codice sorgente permettono a vari strumenti di sviluppo del software, ad es, Al centro delle rappresentazioni del codice spesso ci sono le incorporazioni di parole dei nomi degli identificatori nel codice sorgente, perché gli identificatori rappresentano la maggior parte del vocabolario del codice sorgente e trasmettono importanti informazioni semantiche. Sfortunatamente, attualmente non esiste un modo generalmente accettato di valutare la qualità delle incorporazioni di parole degli identificatori, e le valutazioni attuali sono orientate verso specifici compiti a valle. I nostri risultati mostrano che l'efficacia delle incorporazioni varia significativamente tra le diverse tecniche di incorporamento e che le migliori incorporazioni disponibili rappresentano con successo la somiglianza semantica; d'altro canto, nessuna incorporamento esistente fornisce una rappresentazione soddisfacente delle somiglianze semantiche, ad es, perché le incorporazioni considerano gli identificatori con significati opposti come simili, il che può portare a errori fatali negli strumenti di sviluppo a valle.IdBench fornisce un gold standard per guidare lo sviluppo di nuove incorporazioni che affrontano i limiti attuali.
Le reti generative avversarie (GAN) sono ampiamente utilizzate per imparare il processo di campionamento dei dati e le loro prestazioni possono dipendere pesantemente dalle funzioni di perdita, dato un budget computazionale limitato.Questo studio rivisita MMD-GAN che utilizza la massima discrepanza media (MMD) come funzione di perdita per GAN e fornisce due contributi. In primo luogo, sosteniamo che la funzione di perdita MMD esistente può scoraggiare l'apprendimento di dettagli fini nei dati in quanto tenta di contrarre gli output discriminatori dei dati reali.Per affrontare questo problema, proponiamo una funzione di perdita repulsiva per imparare attivamente la differenza tra i dati reali semplicemente riorganizzando i termini in MMD. In secondo luogo, ispirati dalla perdita della cerniera, proponiamo un kernel gaussiano delimitato per stabilizzare l'addestramento di MMD-GAN con la funzione di perdita repulsiva.I metodi proposti sono applicati ai compiti di generazione di immagini non supervisionate sui set di dati CIFAR-10, STL-10, CelebA e LSUN. I risultati mostrano che la funzione di perdita repulsiva migliora significativamente rispetto alla perdita MMD senza costi computazionali aggiuntivi e supera altre funzioni di perdita rappresentative. I metodi proposti raggiungono un punteggio FID di 16,21 sul set di dati CIFAR-10 utilizzando una singola rete DCGAN e la normalizzazione spettrale.
Purtroppo, la maggior parte delle reti profonde attuali sono enormi strutture basate su cloud che richiedono uno spazio di archiviazione significativo, il che limita la scalabilità del deep learning as a service (DLaaS) e l'uso per l'intelligenza aumentata su dispositivo.  Questo articolo trova algoritmi che utilizzano direttamente rappresentazioni compresse senza perdite di reti feedforward profonde (con pesi sinaptici tratti da insiemi discreti), per eseguire l'inferenza senza decompressione completa.L'intuizione di base che permette un tasso inferiore rispetto agli approcci ingenui è il riconoscimento che gli strati grafici bipartiti delle reti feedforward hanno una sorta di invarianza di permutazione all'etichettatura dei nodi, in termini di operazione inferenziale e che l'operazione di inferenza dipende localmente dai bordi direttamente collegati ad essa.Forniamo anche risultati sperimentali del nostro approccio sul dataset MNIST.
Le reti generative avversarie (GAN) formano un approccio di modellazione generativa noto per la produzione di campioni attraenti, ma sono notevolmente difficili da addestrare. Un modo comune per affrontare questo problema è stato quello di proporre nuove formulazioni dell'obiettivo GAN, ma sorprendentemente pochi studi hanno esaminato i metodi di ottimizzazione progettati per questa formazione avversaria. Attingendo alla letteratura di programmazione matematica, ci opponiamo ad alcune idee sbagliate comuni sulle difficoltà dell'ottimizzazione del punto di sella e proponiamo di estendere i metodi progettati per le disuguaglianze variazionali all'addestramento dei GAN. Applichiamo la media, l'estrapolazione e una variante computazionalmente più economica che chiamiamo estrapolazione dal passato al metodo del gradiente stocastico (SGD) e Adam.
Per imparare in modo efficiente con una piccola quantità di dati su nuovi compiti, il meta-apprendimento trasferisce la conoscenza appresa dai compiti precedenti a quelli nuovi.Tuttavia, una sfida critica nel meta-apprendimento è l'eterogeneità del compito che non può essere ben gestita dai tradizionali metodi di meta-apprendimento globalmente condivisi. Inoltre, gli attuali metodi di meta-apprendimento specifici per i compiti possono soffrire di una struttura progettata a mano o non hanno la capacità di catturare relazioni complesse tra i compiti. In questo articolo, motivato dal modo di organizzare la conoscenza nelle basi di conoscenza, proponiamo un quadro di meta-apprendimento relazionale automatizzato (ARML) che estrae automaticamente le relazioni tra i compiti e costruisce il grafico della meta-conoscenza. Quando arriva un nuovo compito, può trovare rapidamente la struttura più rilevante e adattare la conoscenza della struttura appresa al meta-apprendente.Di conseguenza, il quadro proposto non solo affronta la sfida dell'eterogeneità del compito da un grafo di meta-conoscenza appreso, ma aumenta anche l'interpretabilità del modello.conduciamo ampi esperimenti sulla regressione giocattolo 2D e sulla classificazione delle immagini a pochi scatti e i risultati dimostrano la superiorità di ARML rispetto alle linee di base dello stato dell'arte.
In questo articolo, un algoritmo di boosting profondo è sviluppato per apprendere un classificatore d'insieme più discriminante combinando senza soluzione di continuità un insieme di CNN profonde di base (esperti di base) con diverse capacità, ad esempio, queste CNN profonde di base sono addestrate in modo sequenziale a riconoscere una serie di classi di oggetti in un modo facile o difficile secondo le loro complessità di apprendimento.i nostri risultati sperimentali hanno dimostrato che il nostro algoritmo di boosting profondo può migliorare significativamente i tassi di precisione sul riconoscimento visivo su larga scala.
Questo metodo si basa sull'addestramento non supervisionato di un autocodificatore wavenet multi-dominio, con un codificatore condiviso e uno spazio latente indipendente dal dominio che viene addestrato end-to-end sulle forme d'onda. Utilizzando un set di dati di addestramento diverso e una grande capacità netta, il codificatore singolo ci permette di tradurre anche da domini musicali che non sono stati visti durante la formazione. Valutiamo il nostro metodo su un dataset raccolto da musicisti professionisti, e otteniamo traduzioni convincenti.Studiamo anche le proprietà della traduzione ottenuta e dimostriamo di tradurre anche da un fischio, permettendo potenzialmente la creazione di musica strumentale da esseri umani non addestrati.
La maggior parte delle difese esistenti contro gli attacchi avversari considerano solo la robustezza alle distorsioni L_p-bounded.In realtà, l'attacco specifico è raramente noto in anticipo e gli avversari sono liberi di modificare le immagini in modi che si trovano al di fuori di qualsiasi modello di distorsione fisso; per esempio, le rotazioni avversarie si trovano al di fuori dell'insieme delle distorsioni L_p-bounded.In questo lavoro, sosteniamo la misurazione della robustezza contro una gamma molto più ampia di attacchi imprevisti, attacchi la cui forma precisa è sconosciuta durante la progettazione della difesa. Proponiamo diversi nuovi attacchi e una metodologia per valutare una difesa contro una gamma diversificata di distorsioni impreviste. In primo luogo, costruiamo nuove distorsioni avversarie JPEG, Fog, Gabor e Snow per simulare avversari più diversi, quindi introduciamo UAR, una metrica sintetica che misura la robustezza di una difesa contro una data distorsione.  Usando UAR per valutare la robustezza contro gli attacchi esistenti e nuovi, eseguiamo un ampio studio della robustezza avversaria. Troviamo che la valutazione contro gli attacchi L_p esistenti produce informazioni ridondanti che non si generalizzano ad altri attacchi; raccomandiamo invece di valutare contro il nostro set di attacchi significativamente più diversificato. Troviamo inoltre che l'addestramento avversario contro una o più distorsioni non riesce a conferire robustezza agli attacchi con altri tipi di distorsione.  Questi risultati sottolineano la necessità di valutare e studiare la robustezza contro distorsioni impreviste.
Le reti neurali profonde (DNN) sono state testimoniate come un potente approccio in questo anno, risolvendo compiti di intelligenza artificiale (AI) supervisionati e non supervisionati di lunga data esiste nell'elaborazione del linguaggio naturale, elaborazione del discorso, visione del computer e altri: Questi casi d'uso fanno parte della Cybersecurity Data Mining Competition (CDMC) 2017.L'architettura di rete efficiente per le DNN è scelta conducendo vari percorsi di esperimenti per i parametri di rete e le strutture di rete.Gli esperimenti di tali configurazioni efficienti scelte di DNN sono eseguiti fino a 1000 epoche con tasso di apprendimento impostato nell'intervallo [0.01-0.5]. Gli esperimenti di DNNs hanno funzionato bene rispetto al classico algoritmo di apprendimento automatico in tutti i casi di esperimenti di casi d'uso di sicurezza informatica.Questo è dovuto al fatto che DNNs implicitamente estrarre e costruire caratteristiche migliori, identifica le caratteristiche dei dati che portano ad una migliore precisione. La migliore accuratezza ottenuta da DNNs e XGBoost sulla classificazione di malware Android 0.940 e 0.741, rilevamento di incidenti 1.00 e 0.997, e rilevamento di frodi 0.972 e 0.916 rispettivamente.L'accuratezza ottenuta da DNNs varia -0.05%, +0.02%, -0.01% dal sistema con il punteggio migliore nei compiti CDMC 2017.
In questo documento, presentiamo un approccio per imparare primitive motorie ricomponibili attraverso dimostrazioni di manipolazione su larga scala e diverse.Gli approcci attuali per decomporre le dimostrazioni in primitive spesso assumono primitive definite manualmente e aggirano la difficoltà di scoprire queste primitive.D'altra parte, gli approcci nella scoperta delle primitive mettono presupposti restrittivi sulla complessità di una primitiva, che limitano l'applicabilità a compiti ristretti.Il nostro approccio cerca di aggirare queste sfide imparando congiuntamente sia le primitive motorie sottostanti che ricomponendo queste primitive per formare la dimostrazione originale. Attraverso i vincoli sia sulla parsimonia della decomposizione delle primitive che sulla semplicità di una data primitiva, siamo in grado di imparare una serie diversificata di primitive motorie, così come una rappresentazione latente coerente per queste primitive. dimostriamo, sia qualitativamente che quantitativamente, che le nostre primitive apprese catturano aspetti semanticamente significativi di una dimostrazione.
L'utilizzo di moderni modelli di deep learning per fare previsioni su dati di serie temporali da sensori indossabili richiede generalmente grandi quantità di dati etichettati, ma l'etichettatura di questi grandi set di dati può essere sia ingombrante che costosa. In questo articolo, applichiamo una supervisione debole ai dati di serie temporali ed etichettiamo programmaticamente un set di dati da sensori indossati da pazienti con Parkinson, quindi costruiamo un modello LSTM che predice quando questi pazienti mostrano un comportamento di congelamento clinicamente rilevante (incapacità di fare un passo avanti efficace). Mostriamo che (1) quando il nostro modello è addestrato usando dati specifici del paziente (sessioni precedenti del sensore), arriviamo entro il 9% AUROC di un modello addestrato usando dati etichettati a mano e (2) quando non assumiamo osservazioni precedenti dei soggetti, il nostro modello debolmente supervisionato ha eguagliato le prestazioni con dati etichettati a mano.
L'apprendimento della corrispondenza semantica tra i dati strutturati (ad esempio, coppie slot-valore) e i testi associati è un problema centrale per molte applicazioni NLP a valle, ad es, Tuttavia, le coppie dati-testo raccolte per l'addestramento sono di solito poco corrispondenti, dove i testi contengono informazioni aggiuntive o contraddittorie rispetto al suo input accoppiato. In questo articolo, proponiamo un quadro di allineamento locale-globale (L2GA) per imparare le corrispondenze semantiche da coppie dati-testo poco correlate.In primo luogo, un modello di allineamento locale basato sull'apprendimento multi istanza viene applicato per costruire le corrispondenze semantiche all'interno di una coppia dati-testo. Poi, un modello di allineamento globale costruito sopra uno strato di campo casuale condizionato (CRF) guidato dalla memoria è progettato per sfruttare le dipendenze tra gli allineamenti nell'intero corpus di allenamento, dove la memoria è usata per integrare gli indizi di allineamento forniti dal modello di allineamento locale. Pertanto, è in grado di indurre gli allineamenti mancanti per gli intervalli di testo che non sono supportati dal suo input accoppiato imperfetto. Gli esperimenti sul recente set di dati del ristorante mostrano che il nostro metodo proposto può migliorare l'accuratezza dell'allineamento e come prodotto secondario, il nostro metodo è anche applicabile per indurre coppie di testo-dati di allenamento semanticamente equivalenti per i modelli di generazione neurale.
Massimizzando la probabilità di buone azioni fornite da un dimostratore esperto, l'apprendimento per imitazione supervisionato può produrre politiche efficaci senza le complessità algoritmiche e le sfide di ottimizzazione dell'apprendimento di rinforzo, al costo di richiedere un dimostratore esperto - tipicamente una persona - per fornire le dimostrazioni. In questo articolo, ci chiediamo: possiamo usare l'apprendimento per imitazione per addestrare politiche efficaci senza alcuna dimostrazione da parte di esperti? L'osservazione chiave che lo rende possibile è che, nell'impostazione multi-task, le traiettorie che sono generate da una politica subottimale possono ancora servire come esempi ottimali per altri compiti; in particolare, nell'impostazione in cui i compiti corrispondono a obiettivi diversi, ogni traiettoria è una dimostrazione di successo per lo stato che effettivamente raggiunge. Informati da questa osservazione, proponiamo un algoritmo molto semplice per l'apprendimento di comportamenti senza dimostrazioni, funzioni di ricompensa fornite dall'utente o metodi di apprendimento di rinforzo complessi. Il nostro metodo massimizza semplicemente la probabilità di azioni che l'agente ha effettivamente intrapreso nei suoi rollout precedenti, a condizione che l'obiettivo sia lo stato che ha effettivamente raggiunto. Sebbene varianti correlate di questo approccio siano state proposte in precedenza nelle impostazioni di apprendimento per imitazione con dimostrazioni di esempio, presentiamo la prima istanza di questo approccio come metodo per l'apprendimento di politiche di raggiungimento dell'obiettivo interamente da zero. Presentiamo un risultato teorico che collega l'apprendimento per imitazione auto-supervisionato e l'apprendimento per rinforzo, e risultati empirici che dimostrano che esegue in modo competitivo con metodi di apprendimento per rinforzo più complessi su una serie di problemi impegnativi di raggiungimento dell'obiettivo.
Le reti neurali hanno recentemente mostrato prestazioni eccellenti su numerosi compiti di classificazione. Queste reti hanno spesso un gran numero di parametri e quindi richiedono molti dati da addestrare. Quando il numero di punti di dati di addestramento è piccolo, tuttavia, una rete con un'elevata flessibilità si adatta rapidamente ai dati di addestramento, risultando in una grande varianza del modello e una scarsa prestazione di generalizzazione. Nella fase di formazione, InterBoost prima genera casualmente due set di dati complementari per addestrare due reti di base della stessa struttura, separatamente, e poi i prossimi due set di dati complementari per l'ulteriore formazione delle reti sono generati attraverso l'interazione (o condivisione delle informazioni) tra le due reti di base addestrate in precedenza.Questo processo di formazione interattiva continua iterativamente fino a quando un criterio di arresto è soddisfatto.Nella fase di test, le uscite delle due reti sono combinate per ottenere un punteggio finale per la classificazione.Analisi dettagliata del metodo è fornito per una comprensione approfondita del suo meccanismo.
L'interpretazione delle reti neurali è un compito cruciale e impegnativo nel machine learning.In questo articolo, sviluppiamo un nuovo quadro per rilevare le interazioni statistiche catturate da una rete neurale multistrato feedforward interpretando direttamente i suoi pesi appresi.A seconda delle interazioni desiderate, il nostro metodo può raggiungere prestazioni di rilevamento delle interazioni significativamente migliori o simili rispetto allo stato dell'arte senza cercare uno spazio di soluzione esponenziale di possibili interazioni. Otteniamo questa precisione ed efficienza osservando che le interazioni tra le caratteristiche di input sono create dall'effetto non additivo delle funzioni di attivazione non lineari, e che i percorsi di interazione sono codificati nelle matrici dei pesi.Dimostriamo le prestazioni del nostro metodo e l'importanza delle interazioni scoperte attraverso risultati sperimentali sia su set di dati sintetici che su set di dati applicativi del mondo reale.
Il modello lineare neurale è un semplice metodo di regressione lineare adattativo bayesiano che è stato recentemente utilizzato in una serie di problemi che vanno dall'ottimizzazione bayesiana al reinforcement learning.Nonostante i suoi apparenti successi in queste impostazioni, al meglio delle nostre conoscenze non c'è stata un'esplorazione sistematica delle sue capacità su semplici compiti di regressione. In questo lavoro li caratterizziamo sui dataset UCI, un popolare benchmark per i modelli di regressione bayesiani, così come sui dataset ''gap'' introdotti di recente, che sono migliori test di incertezza fuori dalla distribuzione e dimostriamo che il modello lineare neurale è un metodo semplice che mostra prestazioni competitive su questi compiti.
In questo articolo, presentiamo un caso di studio per riprodurre i risultati di un algoritmo innovativo, AlphaZero, un sistema di apprendimento per rinforzo che impara a giocare a Go a un livello sovrumano, date solo le regole del gioco. Descriviamo Minigo, una riproduzione del sistema AlphaZero utilizzando l'infrastruttura pubblica di Google Cloud Platform e le TPU di Google Cloud. Con dieci giorni di allenamento da zero su 800 Cloud TPUs, Minigo può giocare alla pari contro LeelaZero e ELF OpenGo, due delle più forti IA di Go disponibili pubblicamente. Discutiamo le difficoltà di scalare un sistema di apprendimento per rinforzo e i sistemi di monitoraggio necessari per comprendere la complessa interazione delle configurazioni degli iperparametri.
Le reti generative avversarie (GAN) addestrano modelli generativi impliciti attraverso la risoluzione di problemi minimax. Tali problemi minimax sono noti come non convessi e non concavi, per i quali la dinamica dei metodi del primo ordine non è ben compresa. In questo articolo, consideriamo i GAN nel tipo delle metriche di probabilità integrali (IPM) con il generatore rappresentato da una rete neurale iperparametrizzata. Quando il discriminatore è risolto all'ottimalità approssimata in ogni iterazione, dimostriamo che la discesa del gradiente stocastico su un obiettivo IPM regolarizzato converge globalmente a un punto stazionario con un tasso sublineare. Inoltre, dimostriamo che quando la larghezza della rete del generatore è sufficientemente grande e la classe della funzione discriminante ha abbastanza capacità discriminante, il punto stazionario ottenuto corrisponde a un generatore che produce una distribuzione che è vicina alla distribuzione dei dati osservati in termini di variazione totale. Per quanto ne sappiamo, sembra che stabiliamo per la prima volta sia la convergenza globale che l'ottimalità globale di GANs di formazione quando il generatore è parametrizzato da una rete neurale.
Presentiamo algoritmi di embedding della rete che catturano informazioni su un nodo dalla distribuzione locale degli attributi del nodo intorno ad esso, come osservato su passeggiate casuali seguendo un approccio simile a Skip-gram.Le osservazioni da quartieri di diverse dimensioni sono raggruppate (AE) o codificate separatamente in un approccio multi-scala (MUSAE).  Catturare le relazioni attributo-vicinato su scale multiple è utile per una vasta gamma di applicazioni, tra cui l'identificazione della caratteristica latente attraverso reti disconnesse con attributi simili. dimostriamo teoricamente che le matrici di informazione reciproca pointwise nodo-caratteristica sono implicitamente fattorizzate dalle embeddings.Experiments mostrano che i nostri algoritmi sono robusti, computazionalmente efficienti e superano i modelli comparabili su reti sociali, web e dataset di citazione.
Few-shot classiﬁcation ha lo scopo di imparare un classiﬁcatore a riconoscere classi non viste durante l'addestramento con limitati esempi etichettati.Mentre sono stati fatti progressi significativi, la crescente complessità dei disegni di rete, algoritmi di meta-apprendimento, e le differenze nei dettagli di implementazione rendono un confronto equo difﬁcile. In questo articolo, presentiamo1) un'analisi comparativa coerente di diversi algoritmi di classificazione rappresentativi di pochi colpi, con risultati che mostrano che le dorsali più profonde riducono significativamente il divario tra i metodi, compresa la linea di base, 2) un metodo di base leggermente modificato che raggiunge sorprendentemente prestazioni competitive rispetto allo stato dell'arte sia sul mini-ImageNet che sui dataset CUB, e 3) una nuova impostazione sperimentale per valutare la capacità di generalizzazione interdominio per gli algoritmi di classificazione a pochi scatti. I nostri risultati rivelano che la riduzione della variazione intra-classe è un fattore importante quando il backbone delle caratteristiche è poco profondo, ma non così critico quando si usano backbone più profondi. In un'impostazione realistica di valutazione cross-domain, mostriamo che un metodo di base con una pratica standard di ﬁne-tuning si confronta favorevolmente con altri algoritmi di apprendimento few-shot all'avanguardia.
Le logiche temporali sono utili per descrivere il comportamento dinamico del sistema, e sono state usate con successo come linguaggio per le definizioni degli obiettivi durante la pianificazione delle attività.I lavori precedenti sull'inferenza delle specifiche della logica temporale si sono concentrati sul "riassumere" il set di dati di input - cioè, In questo lavoro esaminiamo il problema di dedurre le specifiche che descrivono le differenze temporali tra due insiemi di tracce di piano. formalizziamo il concetto di fornire tali spiegazioni contrastanti, quindi presentiamo un modello probabilistico bayesiano per dedurre spiegazioni contrastanti come specifiche logiche temporali lineari. dimostriamo l'efficacia, la scalabilità e la robustezza del nostro modello per dedurre specifiche corrette in vari domini di pianificazione di riferimento e per una missione di combattimento aereo simulata.
Questo lavoro affronta il problema della caratterizzazione e della comprensione dei confini decisionali delle reti neurali con attivazioni lineari non lineari piece-wise.Usiamo la geometria tropicale, un nuovo sviluppo nell'area della geometria algebrica, per fornire una caratterizzazione dei confini decisionali di una semplice rete neurale della forma (Affine, ReLU, Affine). In particolare, mostriamo che i confini decisionali sono un sottoinsieme di un'ipersuperficie tropicale, che è intimamente legata a un politopo formato dalla carena convessa di due zonotopi, i cui generatori sono funzioni precise dei parametri della rete neurale. Così facendo, proponiamo una nuova prospettiva tropicale per l'ipotesi del biglietto della lotteria, dove vediamo l'effetto di diverse inizializzazioni sulla rappresentazione geometrica tropicale dei confini decisionali. Inoltre, sfruttiamo questa caratterizzazione come un nuovo set di regolarizzatori tropicali, che si occupano direttamente dei confini decisionali di una rete. Indaghiamo l'uso di questi regolatori nel pruning delle reti neurali (rimuovendo i parametri di rete che non contribuiscono alla rappresentazione geometrica tropicale dei confini di decisione) e nella generazione di attacchi di input avversari (con perturbazioni di input che perturbano esplicitamente la geometria dei confini di decisione per cambiare la previsione di rete dell'input).
I metodi del primo ordine come la discesa stocastica del gradiente (SGD) sono attualmente l'algoritmo standard per l'addestramento delle reti neurali profonde; i metodi del secondo ordine, nonostante il loro migliore tasso di convergenza, sono raramente usati nella pratica a causa del costo computazionale proibitivo nel calcolo delle informazioni del secondo ordine. In questo articolo, proponiamo un nuovo algoritmo Gram-Gauss-Newton (GGN) per addestrare reti neurali profonde per problemi di regressione con perdita quadrata. Il nostro metodo trae ispirazione dalla connessione tra l'ottimizzazione delle reti neurali e la regressione del kernel della tangente neurale (NTK). A differenza dei tipici metodi del secondo ordine che hanno un costo computazionale pesante in ogni iterazione, GGN ha solo un overhead minore rispetto ai metodi del primo ordine come SGD.Diamo anche risultati teorici per dimostrare che per reti neurali sufficientemente ampie, il tasso di convergenza di GGN è quadratico. Inoltre, forniamo una garanzia di convergenza per l'algoritmo GGN mini-batch, che è, a nostra conoscenza, il primo risultato di convergenza per la versione mini-batch di un metodo del secondo ordine su reti neurali iperparametrizzate.Esperimenti preliminari su compiti di regressione dimostrano che per l'addestramento di reti standard, il nostro algoritmo GGN converge molto più velocemente e raggiunge prestazioni migliori di SGD.
I recenti codificatori di frasi preaddestrati raggiungono risultati allo stato dell'arte nei compiti di comprensione del linguaggio, ma questo significa che hanno una conoscenza implicita delle strutture sintattiche? Introduciamo un set di sviluppo annotato grammaticalmente per il Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), che usiamo per indagare la conoscenza grammaticale di tre codificatori preaddestrati, tra cui il popolare OpenAI Transformer (Radford et al., 2018) e BERT (Devlin et al., 2018).Mettiamo a punto questi codificatori per fare la classificazione dell'accettabilità su CoLA e confrontiamo le prestazioni dei modelli sul set di analisi annotato.Alcuni fenomeni, ad esempio la modifica da parte degli aggiunti, sono facili da apprendere per tutti i modelli, mentre altri, ad esempio il movimento a lunga distanza, sono appresi efficacemente solo dai modelli con forti prestazioni complessive, e altri ancora, ad esempio l'accordo morfologico, sono difficilmente appresi da qualsiasi modello.
Quando si considera simultaneamente un numero finito di compiti, l'apprendimento multi-output permette di tenere conto delle somiglianze tra i compiti tramite regolatori appropriati. Proponiamo una generalizzazione dell'impostazione classica a un continuum di compiti utilizzando RKHS valutate come vettori.
Analizziamo la distribuzione di probabilità congiunta sulle lunghezze dei vettori delle variabili nascoste in diversi strati di una rete completamente connessa e profonda, quando i pesi e i bias sono scelti casualmente secondo distribuzioni gaussiane, e l'input è binario-valutato. Mostriamo che, se la funzione di attivazione soddisfa un insieme minimo di presupposti, soddisfatti da tutte le funzioni di attivazione che sappiamo essere utilizzate nella pratica, allora, man mano che la larghezza della rete diventa grande, il ``processo di lunghezza'' converge con probabilità a una mappa di lunghezza che è determinata come una semplice funzione delle varianze dei pesi e dei bias casuali, e della funzione di attivazione.Mostriamo anche che questa convergenza può fallire per funzioni di attivazione che violano i nostri presupposti.
L'aumento dei dati è uno degli approcci più efficaci per migliorare l'accuratezza dei moderni modelli di apprendimento automatico, ed è anche indispensabile per addestrare un modello profondo per il meta-apprendimento.Tuttavia, la maggior parte delle attuali implementazioni di aumento dei dati applicate nel meta-apprendimento sono le stesse utilizzate nella classificazione convenzionale delle immagini.In questo articolo, introduciamo un nuovo metodo di aumento dei dati per il meta-apprendimento, che si chiama ``Task Level Data Augmentation'' (indicato come Task Aug). L'idea di base di Task Aug è quella di aumentare il numero di classi di immagini piuttosto che il numero di immagini in ogni classe.Al contrario, con una maggiore quantità di classi, possiamo campionare istanze di attività più diverse durante la formazione.Questo ci permette di addestrare una rete profonda con metodi di meta-apprendimento con poco over-fitting.I risultati sperimentali mostrano che il nostro approccio raggiunge lo stato dell'arte delle prestazioni su miniImageNet, CIFAR-FS, e FC100 few-shot benchmark di apprendimento.Una volta che la carta è accettata, forniremo il link al codice.
In questo articolo, presentiamo un quadro generale per distillare le aspettative rispetto alla distribuzione posteriore bayesiana di una rete neurale profonda, estendendo significativamente il lavoro precedente su un metodo noto come ``Bayesian Dark Knowledge.  La nostra struttura generalizzata si applica al caso dei modelli di classificazione e prende come input l'architettura di una rete "insegnante", un'aspettativa posteriore generale di interesse e l'architettura di una rete "studente". Il metodo di distillazione esegue una compressione online dell'aspettativa posteriore selezionata usando campioni Monte Carlo generati iterativamente dal parametro posteriore del modello insegnante. Presentiamo risultati sperimentali che indagano su più set di dati, obiettivi di distillazione, architetture del modello dell'insegnante e approcci alla ricerca di architetture del modello dello studente. Stabiliamo il risultato chiave che la distillazione in un modello dello studente con un'architettura che corrisponde all'insegnante, come viene fatto nella conoscenza oscura bayesiana, può portare a prestazioni sub-ottimali.
Gli autocodificatori variazionali (VAE) hanno dimostrato di essere potenti modelli di variabili latenti.Tuttavia, la forma del posteriore approssimato può limitare l'espressività del modello.Le distribuzioni categoriali sono blocchi di costruzione flessibili e utili per esempio negli strati di memoria neurali.Introduciamo l'autocodificatore variazionale discreto gerarchico (HD-VAE): una gerarchia di strati di memoria variazionale. Il rilassamento Concrete/Gumbel-Softmax permette di massimizzare un surrogato dell'Evidence Lower Bound tramite ascesa stocastica del gradiente.Mostriamo che, quando si usa un numero limitato di variabili latenti, HD-VAE supera la baseline gaussiana sulla modellazione di più dataset di immagini binarie.Addestrare HD-VAE molto profondo rimane una sfida a causa del bias di rilassamento che è indotto dall'uso di un obiettivo surrogato.Introduciamo una definizione formale e conduciamo uno studio preliminare teorico ed empirico del bias.
In questo articolo, proponiamo una nuova tecnica per migliorare il metodo di discesa del gradiente stocastico (SGD) per addestrare reti profonde, che chiamiamo PowerSGD. Il metodo PowerSGD proposto eleva semplicemente il gradiente stocastico a una certa potenza $\gamma\in[0,1]$ durante le iterazioni e introduce solo un parametro aggiuntivo, cioè l'esponente di potenza $\gamma$ (quando $\gamma=1$, PowerSGD si riduce a SGD).Proponiamo inoltre PowerSGD con momentum, che chiamiamo \emph{PowerSGDM}, e forniamo analisi del tasso di convergenza su entrambi i metodi PowerSGD e PowerSGDM. I risultati empirici mostrano che il PowerSGD e il PowerSGDM proposti ottengono una velocità di formazione iniziale più veloce rispetto ai metodi a gradiente adattivo, una capacità di generalizzazione comparabile con SGD e una maggiore robustezza alla selezione di iperparametri e ai gradienti che svaniscono.PowerSGD è essenzialmente un modificatore di gradiente attraverso una trasformazione non lineare e, come tale, è ortogonale e complementare ad altre tecniche per accelerare l'ottimizzazione basata sul gradiente.
Il nostro obiettivo è quello di costruire agenti umanoidi complessi che integrino percezione, controllo motorio e memoria. In questo lavoro, abbiamo in parte fattorizzato questo problema in un controllo motorio di basso livello dalla propriocezione e un coordinamento di alto livello delle abilità di basso livello informate dalla visione. Sviluppiamo un'architettura capace di un controllo motorio sorprendentemente flessibile e diretto al compito di un corpo umanoide relativamente high-DoF, combinando il pre-training dei controller motori di basso livello con un controller di alto livello focalizzato sul compito che passa tra le sotto-politiche di basso livello. Il sistema risultante è in grado di controllare un corpo umanoide fisicamente simulato per risolvere compiti che richiedono l'accoppiamento della percezione visiva da una telecamera RGB egocentrica non stabilizzata durante la locomozione nell'ambiente.Link video supplementare: https://youtu.be/fBoir7PNxPk
Osservando che un neurone ReLU è un prodotto di una funzione lineare con un gate (quest'ultimo determina se il neurone è attivo o meno), dove entrambi condividono un vettore di peso addestrato congiuntamente, proponiamo di disaccoppiare i due.Introduciamo reti GaLU - reti in cui ogni neurone è un prodotto di una unità lineare, definita da un vettore di peso che viene addestrato, con un gate, definito da un vettore di peso diverso che non viene addestrato. In generale, dato un modello di base e una versione più semplice di esso, i due parametri che determinano la qualità della versione più semplice sono se la sua performance pratica è abbastanza vicina al modello di base e se è più facile analizzarla teoricamente.Mostriamo che le reti GaLU hanno prestazioni simili alle reti ReLU su dataset standard e iniziamo uno studio delle loro proprietà teoriche, dimostrando che sono effettivamente più facili da analizzare.Crediamo che ulteriori ricerche sulle reti GaLU possano essere fruttuose per lo sviluppo di una teoria del deep learning.
I sistemi di apprendimento automatico spesso incontrano errori di Out-of-Distribution (OoD) quando hanno a che fare con dati di test provenienti da una distribuzione diversa da quella utilizzata per la formazione.Con il loro crescente utilizzo in applicazioni critiche, diventa importante sviluppare sistemi che siano in grado di quantificare con precisione la sua incertezza predittiva e schermare questi input anomali.Tuttavia, a differenza dei compiti di apprendimento standard, non esiste attualmente un principio guida ben stabilito per la progettazione di architetture che possano quantificare con precisione l'incertezza.Inoltre, gli approcci di rilevamento OoD comunemente utilizzati sono soggetti a errori e talvolta assegnano anche probabilità superiori ai campioni OoD. Per affrontare questi problemi, cerchiamo innanzitutto di identificare i principi guida per la progettazione di architetture consapevoli dell'incertezza, proponendo la ricerca della distribuzione delle architetture neurali (NADS). A differenza dei metodi standard di ricerca delle architetture neurali che cercano una singola architettura con le migliori prestazioni, NADS cerca una distribuzione di architetture che funzionano bene su un dato compito, permettendoci di identificare i blocchi di costruzione comuni tra tutte le architetture consapevoli dell'incertezza. Con questa formulazione, siamo in grado di ottimizzare un obiettivo di rilevamento stocastico di outlier e costruire un insieme di modelli per eseguire il rilevamento di OoD. Eseguiamo diversi esperimenti di rilevamento OoD e osserviamo che il nostro NADS si comporta favorevolmente rispetto ai metodi di rilevamento OoD all'avanguardia.
Le applicazioni moderne, dai veicoli autonomi alla videosorveglianza, generano quantità massicce di dati di immagine. In questo lavoro proponiamo un nuovo approccio di rilevamento di outlier di immagine (IOD in breve) che sfrutta il classificatore di immagini all'avanguardia per scoprire outlier senza utilizzare alcun outlier etichettato. osserviamo che anche se intuitivamente la fiducia che una rete neurale convoluzionale (CNN) ha che un'immagine appartiene a una classe particolare potrebbe servire come misura di outlierness per ogni immagine, applicando direttamente questa fiducia per rilevare outlier non funziona bene. Questo perché la CNN ha spesso un'alta fiducia su un'immagine outlier che non appartiene a nessuna classe di destinazione a causa della sua capacità di generalizzazione che garantisce l'alta precisione nella classificazione. Per risolvere questo problema, proponiamo un approccio basato su una foresta neurale profonda che armonizza i requisiti contraddittori di classificare accuratamente le immagini e rilevare correttamente le immagini outlier. I nostri esperimenti utilizzando diversi dataset di immagini di riferimento tra cui MNIST, CIFAR-10, CIFAR-100 e SVHN dimostrano l'efficacia del nostro approccio IOD per il rilevamento degli outlier, catturando più del 90% degli outlier generati dall'iniezione di un dataset di immagini in un altro, pur conservando l'accuratezza di classificazione del problema di classificazione multiclasse.
Questo articolo introduce CloudLSTM, un nuovo ramo di modelli neurali ricorrenti su misura per la previsione su flussi di dati generati da fonti geospaziali point-cloud. Progettiamo un operatore Dynamic Point-cloud Convolution (D-Conv) come componente centrale di CloudLSTMs, che esegue la convoluzione direttamente su point-clouds ed estrae le caratteristiche spaziali locali da set di punti vicini che circondano diversi elementi dell'input. Questo operatore mantiene l'invarianza di permutazione delle strutture di apprendimento sequenza-sequenza, mentre rappresenta le correlazioni vicine ad ogni passo temporale - un aspetto importante nell'apprendimento predittivo spazio-temporale. L'operatore D-Conv risolve i requisiti di dati strutturali della griglia dei modelli di previsione spazio-temporale esistenti e può essere facilmente inserito nelle architetture LSTM tradizionali con apprendimento sequenza-sequenza e meccanismi di attenzione.    I nostri risultati, ottenuti con set di dati del mondo reale raccolti in diversi scenari per ogni caso d'uso, mostrano che CloudLSTM fornisce previsioni accurate a lungo termine, superando una varietà di modelli di reti neurali.
I Knowledge Graphs (KG), composti da entità e relazioni, forniscono una rappresentazione strutturata della conoscenza.Per un facile accesso agli approcci statistici sui dati relazionali, sono stati introdotti diversi metodi per incorporare un KG come componenti di R^d. Noi proponiamo TransINT, un nuovo e interpretabile metodo di incorporazione KG che conserva isomorficamente l'ordine di implicazione tra le relazioni nello spazio di incorporazione.TransINT mappa set di entità (legate da una relazione) a set continui di vettori che sono ordinati in modo isomorfo alle implicazioni delle relazioni. Con un nuovo schema di condivisione dei parametri, TransINT permette l'addestramento automatico su fatti mancanti ma impliciti senza regole di base.Raggiungiamo nuove performance allo stato dell'arte con margini significativi nella predizione dei collegamenti e nella classificazione delle triple sul dataset FB122, con prestazioni potenziate anche su istanze di test che non possono essere dedotte da regole logiche.Gli angoli tra gli insiemi continui incorporati da TransINT forniscono un modo interpretabile per estrarre la relazione semantica e le regole di implicazione tra le relazioni.
Unsupervised domain adaptive object detection ha lo scopo di imparare un rivelatore robusto nella circostanza del cambio di dominio, dove il dominio di formazione (sorgente) è ricco di etichette con annotazioni bounding box, mentre il dominio di test (destinazione) è privo di etichette e le distribuzioni delle caratteristiche tra i domini di formazione e di test sono dissimili o addirittura totalmente diverse. In questo articolo, proponiamo un metodo Stacked Complementary Losses (SCL) basato sul gradiente che utilizza l'obiettivo di rilevamento (entropia incrociata e regressione l1 liscia) come obiettivo primario, e taglia diverse perdite ausiliarie in diverse fasi della rete per utilizzare le informazioni dai dati complementari (immagini di destinazione) che possono essere efficaci nell'adattare i parametri del modello a entrambi i domini di origine e destinazione. Un'operazione di distacco del gradiente viene applicata tra le sottoreti di rilevamento e di contesto durante l'addestramento per forzare le reti ad apprendere rappresentazioni discriminanti. Noi sosteniamo che l'addestramento convenzionale con obiettivo primario sfrutta principalmente le informazioni dal dominio di origine per massimizzare la probabilità e ignora i dati di complemento negli strati superficiali delle reti, il che porta a un'integrazione insufficiente all'interno dei diversi domini. Per esempio, da Cityscapes a FoggyCityscapes, raggiungiamo un mAP del 37,9%, superando l'arte precedente Strong-Weak del 3,6%.
Le reti neurali convoluzionali (CNN) sono diventate l'approccio di maggior successo e popolare in molti domini legati alla visione.Mentre le CNN sono particolarmente adatte a catturare una corretta gerarchia di concetti dalle immagini del mondo reale, sono limitate a domini in cui i dati sono abbondanti.Tentativi recenti hanno cercato di mitigare questo problema di scarsità di dati fondendo il loro problema originale a compito singolo in un nuovo problema di apprendimento multi-task (MTL). L'obiettivo principale di questo meccanismo di trasferimento induttivo è quello di sfruttare le informazioni specifiche del dominio dai compiti correlati, al fine di migliorare la generalizzazione sul compito principale.Mentre i recenti risultati nella comunità di apprendimento profondo (DL) hanno dimostrato il potenziale promettente di CNNs compito-specifici di formazione in un quadro morbido di condivisione dei parametri, integrando i recenti progressi DL per migliorare la condivisione della conoscenza è ancora un problema aperto.In questo documento, proponiamo la rete di collaborazione profonda (DCNet), un nuovo approccio per la connessione task-specifici CNNs in un quadro MTL. Definiamo la connettività in termini di due distinti blocchi di trasformazione non lineare: uno aggrega le caratteristiche specifiche di un'attività in caratteristiche globali, mentre l'altro fonde le caratteristiche globali con ogni rete specifica di un'attività. Sulla base dell'osservazione che la rilevanza dell'attività dipende dalla profondità, i nostri blocchi di trasformazione usano connessioni saltate come suggerito dagli approcci di rete residuali, per disattivare più facilmente le caratteristiche non correlate dipendenti dall'attività. Per convalidare il nostro approccio, abbiamo impiegato i set di dati FLD (facial landmark detection) in quanto sono facilmente adattabili a MTL, dato il numero di compiti che includono.I risultati sperimentali mostrano che possiamo raggiungere fino al 24,31% di miglioramento relativo nel tasso di fallimento dei punti di riferimento rispetto ad altri approcci MTL allo stato dell'arte.Eseguiamo infine uno studio di ablazione che dimostra che il nostro approccio permette effettivamente la condivisione della conoscenza, sfruttando le caratteristiche specifiche del dominio a particolari profondità da compiti che sappiamo essere correlati.
Zero-Shot Learning (ZSL) è un compito di classificazione in cui alcune classi indicate come classi non viste non hanno immagini di allenamento etichettate.Invece, abbiamo solo informazioni laterali (o descrizione) sulle classi viste e non viste, spesso sotto forma di attributi semantici o descrittivi. La mancanza di immagini di addestramento da un insieme di classi limita l'uso delle tecniche di classificazione standard e delle perdite, compresa la popolare perdita di cross-entropia. Il passo chiave nell'affrontare il problema ZSL è il collegamento tra lo spazio visivo e quello semantico attraverso l'apprendimento di un approccio non lineare di embedding. In questo articolo, proponiamo una nuova architettura di fusione di ZSL come una rete neurale completamente connessa con perdita di entropia incrociata per incorporare lo spazio visivo nello spazio semantico. Durante l'addestramento per introdurre informazioni visive non viste nella rete, utilizziamo un soft-labeling basato sulle somiglianze semantiche tra le classi viste e non viste. Valutiamo il modello proposto su cinque set di dati di riferimento per l'apprendimento a zero colpi, AwA1, AwA2, aPY, SUN e CUB, e dimostriamo che, nonostante la semplicità, il nostro approccio raggiunge lo stato dell'arte delle prestazioni in impostazione Generalized-ZSL su tutti questi set di dati e supera lo stato dell'arte per alcuni set di dati.
In compiti complessi, come quelli con grandi spazi d'azione combinatori, l'esplorazione casuale può essere troppo inefficiente per ottenere progressi significativi nell'apprendimento. In questo lavoro, usiamo un curriculum di spazi d'azione che crescono progressivamente per accelerare l'apprendimento, assumendo che l'ambiente sia fuori dal nostro controllo, ma che l'agente possa impostare un curriculum interno limitando inizialmente il suo spazio d'azione. Il nostro approccio utilizza l'apprendimento di rinforzo off-policy per stimare le funzioni di valore ottimali per più spazi d'azione simultaneamente e trasferisce efficientemente i dati, le stime di valore e le rappresentazioni di stato dagli spazi d'azione limitati al compito completo. Mostriamo l'efficacia del nostro approccio in compiti di controllo proof-of-concept e in compiti di microgestione su larga scala di StarCraft con grandi spazi d'azione multi-agente.
Recentemente, i ricercatori hanno scoperto che i classificatori di oggetti allo stato dell'arte possono essere ingannati facilmente da piccole perturbazioni nell'input impercettibili agli occhi umani. È noto che un attaccante può generare forti esempi avversari se conosce i parametri del classificatore. Al contrario, un difensore può robustizzare il classificatore con un nuovo addestramento se ha gli esempi avversari. La natura di gioco del gatto e del topo degli attacchi e delle difese solleva la questione della presenza di equilibri nella dinamica. In questo articolo, presentiamo una classe di attacchi basata su reti neurali per approssimare una classe di attacchi più grande ma intrattabile, e formuliamo l'interazione attaccante-difensore come un gioco leader-follower a somma zero. Presentiamo algoritmi di ottimizzazione con penalizzazione della sensibilità per trovare soluzioni minimax, che sono le migliori difese nel caso peggiore contro gli attacchi whitebox. I vantaggi degli attacchi e delle difese basati sull'apprendimento rispetto a quelli basati sul gradiente sono dimostrati con MNIST e CIFAR-10.
L'apprendimento supervisionato con serie temporali campionate in modo irregolare è stato una sfida per i metodi di Machine Learning a causa dell'ostacolo di trattare con intervalli di tempo irregolari. Alcuni articoli hanno introdotto recentemente modelli di reti neurali ricorrenti che trattano l'irregolarità, ma la maggior parte di essi si basa su meccanismi complessi per ottenere una migliore performance. Questo lavoro propone un nuovo metodo per rappresentare i timestamp (ore o date) come vettori densi usando funzioni sinusoidali, chiamato Time Embeddings.come metodo di input dei dati e può essere applicato alla maggior parte dei modelli di apprendimento automatico.il metodo è stato valutato con due compiti predittivi da MIMIC III, un dataset di serie temporali campionate irregolarmente di cartelle cliniche elettroniche.i nostri test hanno mostrato un miglioramento ai modelli di apprendimento automatico basati su LSTM e classici, specialmente con dati molto irregolari.
Concentrandosi su famiglie di grafi casuali come il modello a blocchi stocastico, la ricerca recente ha unificato entrambi gli approcci e identificato le soglie di rilevamento sia statistiche che computazionali in termini di rapporto segnale-rumore. Presentiamo una nuova famiglia di Graph Neural Networks (GNNs) per risolvere i problemi di rilevamento di comunità in un contesto di apprendimento supervisionato. Mostriamo che, in modo guidato dai dati e senza accesso ai modelli generativi sottostanti, essi possono eguagliare o addirittura superare le prestazioni dell'algoritmo di propagazione delle credenze su modelli binari e multiclasse a blocchi stocastici, che si ritiene raggiungano la soglia computazionale in questi casi. In particolare, proponiamo di aumentare le GNN con l'operatore di non-backtracking definito sul grafico a linee delle adiacenze dei bordi. Le GNN hanno ottenuto buone prestazioni su set di dati reali.  Inoltre, eseguiamo la prima analisi del panorama di ottimizzazione dell'uso di GNN (lineari) per risolvere i problemi di rilevamento delle comunità, dimostrando che sotto certe semplificazioni e ipotesi, il valore di perdita a qualsiasi minimo locale è vicino al valore di perdita al minimo/minimo globale.
Le reti residue (Resnet) sono diventate un'architettura prominente nel deep learning.Tuttavia, una comprensione completa delle Resnet è ancora un argomento di ricerca in corso.Una visione recente sostiene che le Resnet eseguono un raffinamento iterativo delle caratteristiche.Noi tentiamo di esporre ulteriormente le proprietà di questo aspetto.A tal fine, studiamo le Resnet sia analiticamente che empiricamente.Formalizziamo la nozione di raffinamento iterativo nelle Resnet mostrando che le architetture residue incoraggiano naturalmente le caratteristiche a muoversi lungo il gradiente negativo della perdita durante la fase feedforward. Inoltre, la nostra analisi empirica suggerisce che le Resnet sono in grado di eseguire sia l'apprendimento della rappresentazione che il raffinamento iterativo.In generale, un blocco Resnet tende a concentrare il comportamento di apprendimento della rappresentazione nei primi strati mentre gli strati superiori eseguono il raffinamento iterativo delle caratteristiche.Infine osserviamo che la condivisione degli strati residui porta ingenuamente all'esplosione della rappresentazione e danneggia le prestazioni di generalizzazione, e mostriamo che semplici strategie esistenti possono aiutare ad alleviare questo problema.
Sviluppiamo ricostruzioni apprese end-to-end per telecamere basate su maschere lensless, compreso un sistema sperimentale per catturare immagini allineate lensless e lensed per la formazione.  Vengono esplorati vari metodi di ricostruzione, su una scala che va dai classici approcci iterativi (basati sul modello fisico dell'immagine) ai metodi appresi in profondità con molti parametri appresi.  La struttura della rete combina la conoscenza del modello fisico di imaging con parametri appresi aggiornati dai dati, che compensano gli artefatti causati dalle approssimazioni fisiche. Il nostro approccio srotolato è 20 volte più veloce dei metodi classici e produce una migliore qualità di ricostruzione sia dei metodi classici che di quelli profondi sul nostro sistema sperimentale.  
L'apprendimento profondo, un rebranding dei lavori di ricerca sulle reti neurali profonde, ha ottenuto un notevole successo negli ultimi anni.Con più strati nascosti, i modelli di apprendimento profondo mirano a calcolare le rappresentazioni gerarchiche delle caratteristiche dei dati osservativi.Nel frattempo, a causa dei suoi gravi svantaggi nel consumo di dati, risorse computazionali, costi di regolazione dei parametri e la mancanza di spiegabilità dei risultati, l'apprendimento profondo ha anche subito molte critiche. In questo articolo, introdurremo un nuovo modello di apprendimento della rappresentazione, vale a dire "Sample-Ensemble Genetic Evolutionary Network" (SEGEN), che può servire come un approccio alternativo ai modelli di apprendimento profondo.Invece di costruire un singolo modello profondo, basato su un insieme di sub-istanze campionate, SEGEN adotta una strategia di apprendimento genetico-evolutivo per costruire un gruppo di modelli unitari generazione per generazione. I modelli unitari incorporati in SEGEN possono essere sia modelli tradizionali di apprendimento automatico che i recenti modelli di apprendimento profondo con un'architettura molto più "stretta" e "poco profonda". I risultati di apprendimento di ogni istanza alla generazione finale saranno efficacemente combinati da ogni modello unitario attraverso la propagazione diffusiva e le strategie di apprendimento dell'ensemble. Dal punto di vista computazionale, SEGEN richiede molti meno dati, meno risorse computazionali e sforzi di regolazione dei parametri, ma ha una solida interpretabilità teorica del processo di apprendimento e dei risultati. Sono stati fatti esperimenti approfonditi su diversi set di dati di riferimento del mondo reale, e i risultati sperimentali ottenuti da SEGEN hanno dimostrato i suoi vantaggi rispetto ai modelli di apprendimento di rappresentazione allo stato dell'arte.
Come possiamo insegnare agli agenti artificiali a usare il linguaggio umano in modo flessibile per risolvere i problemi in un ambiente del mondo reale? Abbiamo un esempio in natura di agenti in grado di risolvere questo problema: i bambini umani alla fine imparano a usare il linguaggio umano per risolvere i problemi, e vengono istruiti con un umano adulto-in-the-loop.Purtroppo, gli attuali metodi di apprendimento automatico (ad es. Un obiettivo importante è trovare un algoritmo con un adeguato "priore di apprendimento del linguaggio" che gli permetta di imparare il linguaggio umano, minimizzando il numero di interazioni umane richieste. In questo articolo, proponiamo di imparare un tale priore in simulazione, sfruttando la crescente quantità di calcolo disponibile per gli esperimenti di apprendimento automatico (1). In particolare, in L2C addestriamo un agente di meta-apprendimento in simulazione per interagire con popolazioni di agenti pre-addestrati, ciascuno con il proprio protocollo di comunicazione distinto.Una volta che l'agente di meta-apprendimento è in grado di adattarsi rapidamente a ciascuna popolazione di agenti, può essere distribuito in nuove popolazioni non viste durante la formazione, comprese le popolazioni di esseri umani. Per mostrare la promessa del quadro L2C, conduciamo alcuni esperimenti preliminari in un gioco di segnalazione Lewis (4), dove dimostriamo che agentstrained con L2C sono in grado di imparare una semplice forma di linguaggio umano (rappresentato da un linguaggio compositivo codificato a mano) in meno iterazioni di agenti inizializzati in modo casuale.
Studiamo il problema di adattare i programmi di tasso di apprendimento specifici del compito dal punto di vista dell'ottimizzazione degli iperparametri.  Descriviamo la struttura del gradiente di un errore di convalida rispetto ai tassi di apprendimento, l'ipergradiente, e sulla base di questo introduciamo un nuovo algoritmo online. Il nostro metodo interpola in modo adattivo tra due tecniche proposte di recente (Franceschi et al., 2017; Baydin et al., 2018), con una maggiore stabilità e una convergenza più veloce.
Questi approcci si sono concentrati principalmente sulla generazione di immagini da una descrizione testuale statica e sono limitati alla generazione di immagini in un singolo passaggio, ma non sono in grado di generare un'immagine in modo interattivo sulla base di una descrizione testuale incrementale (qualcosa che è più intuitivo e simile al modo in cui descriviamo un'immagine). Proponiamo un metodo per generare un'immagine in modo incrementale sulla base di una sequenza di grafici di descrizioni di scene (scene-graphs).proponiamo un'architettura di rete ricorrente che conserva il contenuto dell'immagine generata nei passi precedenti e modifica l'immagine cumulativa secondo le nuove informazioni di scena fornite. Il nostro modello utilizza Graph Convolutional Networks (GCN) per soddisfare i grafici di scena di dimensioni variabili insieme alle reti di traduzione di immagini Generative Adversarial per generare immagini realistiche di multi-oggetto senza bisogno di alcuna supervisione intermedia durante la formazione. Sperimentiamo con il dataset Coco-Stuff che ha immagini di multi-oggetto con annotazioni che descrivono la scena visiva e mostriamo che il nostro modello supera significativamente altri approcci sullo stesso dataset nella generazione di immagini visivamente coerenti per grafici di scena in crescita incrementale.
In alcuni importanti domini di computer vision, come l'imaging medico o iperspettrale, ci interessa la classificazione di oggetti minuscoli in immagini di grandi dimensioni, ma la maggior parte delle reti neurali convoluzionali (CNN) per la classificazione delle immagini sono state sviluppate utilizzando set di dati distorti che contengono grandi oggetti, per lo più in posizioni centrali dell'immagine. Per valutare se le architetture CNN classiche funzionano bene per la classificazione di oggetti minuscoli, costruiamo un testbed completo che contiene due set di dati: uno derivato da cifre MNIST e uno da immagini istopatologiche.Questo testbed permette esperimenti controllati per stress-testare le architetture CNN con un ampio spettro di rapporti segnale-rumore.Le nostre osservazioni indicano che: (1) esiste un limite al rapporto segnale-rumore al di sotto del quale le CNN non riescono a generalizzare e che questo limite è influenzato dalla dimensione del dataset - più dati portano a prestazioni migliori; tuttavia, la quantità di dati di addestramento necessari al modello per generalizzare scala rapidamente con l'inverso del rapporto oggetto-immagine (2) in generale, i modelli a più alta capacità mostrano una migliore generalizzazione; (3) quando si conoscono le dimensioni approssimative degli oggetti, adattare il campo recettivo è vantaggioso; e (4) per un rapporto segnale-rumore molto piccolo la scelta dell'operazione di pooling globale influisce sull'ottimizzazione, mentre per valori segnale-rumore relativamente grandi, tutte le operazioni di pooling globale testate mostrano prestazioni simili.
Le recenti tendenze di incorporare meccanismi di attenzione nella visione hanno portato i ricercatori a riconsiderare la supremazia degli strati di convoluzione come blocco di costruzione primario.Oltre ad aiutare le CNN a gestire le dipendenze a lungo raggio, Ramachandran et al. (2019) hanno dimostrato che l'attenzione può sostituire completamente la convoluzione e raggiungere prestazioni allo stato dell'arte su compiti di visione.Questo solleva la domanda: gli strati di attenzione appresi operano in modo simile agli strati di convoluzione? Questo lavoro fornisce la prova che gli strati di attenzione possono eseguire la convoluzione e, in effetti, spesso imparano a farlo nella pratica.In particolare, dimostriamo che uno strato di autoattenzione a più teste con un numero sufficiente di teste è almeno altrettanto espressivo di qualsiasi strato convoluzionale.I nostri esperimenti numerici mostrano quindi che gli strati di autoattenzione assistono a modelli di pixel-griglia in modo simile agli strati CNN, corroborando la nostra analisi.Il nostro codice è pubblicamente disponibile.
Introduciamo un algoritmo "learning-based" per il problema della decomposizione low-rank: data una matrice $n volte d$ $A$ e un parametro $k$, calcoliamo una matrice rank-$k$ $A'$ che minimizza la perdita di approssimazione $||A- A'||_F$. In particolare, alcuni degli algoritmi approssimativi più efficienti per il calcolo di approssimazioni a basso rango procedono calcolando una proiezione $SA$, dove $S$ è una "matrice di schizzo" casuale sparsa $m volte n$, e poi eseguendo la decomposizione del valore singolare di $SA$. Mostriamo come sostituire la matrice casuale $S$ con una matrice "appresa" della stessa sparsità per ridurre l'errore.I nostri esperimenti mostrano che, per diversi tipi di set di dati, una matrice di sketch appresa può ridurre sostanzialmente la perdita di approssimazione rispetto a una matrice casuale $S$, a volte di un ordine di grandezza.Studiamo anche matrici miste in cui solo alcune delle righe sono addestrate e le rimanenti sono casuali, e mostriamo che le matrici offrono ancora prestazioni migliori pur mantenendo le garanzie del worst-case.
I modelli neurali di conversazione sono ampiamente utilizzati in applicazioni come gli assistenti personali e i chat bot.Questi modelli sembrano dare prestazioni migliori quando operano a livello di parola.Tuttavia, per le lingue di fusione come il francese, il russo e il polacco le dimensioni del vocabolario a volte diventano inattuabili poiché la maggior parte delle parole ha molte forme di parola.Proponiamo un'architettura di rete neurale per trasformare un testo normalizzato in uno grammaticalmente corretto. Il nostro modello utilizza in modo efficiente la corrispondenza tra le parole normalizzate e quelle di destinazione e supera significativamente i modelli a livello di carattere, essendo 2 volte più veloce nell'addestramento e 20 volte più veloce nella valutazione.Proponiamo anche una nuova pipeline per la costruzione di modelli di conversazione: prima genera una risposta normalizzata e poi la trasforma in una grammaticalmente corretta utilizzando la nostra rete.La pipeline proposta dà prestazioni migliori rispetto ai modelli di conversazione a livello di carattere secondo i test del valutatore.
Questo articolo propone l'uso di metodi di elementi spettrali \citep{canuto_spectral_1988} per l'addestramento veloce e accurato delle Equazioni Differenziali Ordinarie Neurali (ODE-Net; \citealp{Chen2018NeuralOD}) per l'identificazione del sistema.Questo si ottiene esprimendo la loro dinamica come una serie troncata di polinomi Legendre. I coefficienti della serie, così come i pesi della rete, sono calcolati minimizzando la somma ponderata della funzione di perdita e la violazione della dinamica dell'ODE-Net.Il problema è risolto mediante una discesa di coordinate che minimizza alternativamente, rispetto ai coefficienti e ai pesi, due sottoproblemi non vincolati utilizzando metodi standard di backpropagation e gradiente. Il confronto sperimentale con metodi standard, come la backpropagation attraverso solutori espliciti e la tecnica adjoint \citep{Chen2018NeuralOD}, su modelli surrogati di formazione di sistemi dinamici di piccola e media scala mostra che è almeno un ordine di grandezza più veloce a raggiungere un valore comparabile della funzione di perdita.Il corrispondente test MSE è un ordine di grandezza più piccolo pure, suggerendo capacità di generalizzazione aumentare.
Molti metodi all'avanguardia utilizzano la motivazione intrinseca per integrare il segnale di ricompensa estrinseco rado, dando all'agente più opportunità di ricevere feedback durante l'esplorazione.Comunemente questi segnali vengono aggiunti come ricompense bonus, il che si traduce in una politica di miscela che non conduce né l'esplorazione né l'adempimento del compito in modo risoluto.In questo documento, invece impariamo politiche di compito intrinseche ed estrinseche separate e programmiamo tra queste diverse unità per accelerare l'esplorazione e stabilizzare l'apprendimento. Inoltre, introduciamo un nuovo tipo di ricompensa intrinseca denotata come successor feature control (SFC), che è generale e non specifica del compito, tiene conto delle statistiche sulle traiettorie complete e quindi differisce dai metodi precedenti che usano solo informazioni locali per valutare la motivazione intrinseca: VizDoom, DeepMind Lab e DeepMind Control Suite.I risultati mostrano un'efficienza di esplorazione sostanzialmente migliorata con SFC e l'uso gerarchico dei drive intrinseci.Un video dei nostri risultati sperimentali può essere trovato su https://gofile.io/?c=HpEwTd.
Gli approcci di apprendimento di meta-rinforzo mirano a sviluppare procedure di apprendimento che possono adattarsi rapidamente a una distribuzione di compiti con l'aiuto di pochi esempi.Sviluppare strategie di esplorazione efficienti in grado di trovare i campioni più utili diventa critico in tali impostazioni.Gli approcci esistenti per trovare strategie di esplorazione efficienti aggiungono obiettivi ausiliari per promuovere l'esplorazione dalla politica pre-update, tuttavia, questo rende l'adattamento utilizzando pochi passi di gradiente difficile come le politiche pre-update (esplorazione) e post-update (sfruttamento) sono molto diverse. Avere due politiche diverse dà più flessibilità nell'addestramento della politica di esplorazione e rende anche più facile l'adattamento a qualsiasi compito specifico. Mostriamo che l'uso di obiettivi di apprendimento auto-supervisionati o supervisionati per l'adattamento stabilizza il processo di addestramento e dimostra anche le prestazioni superiori del nostro modello rispetto ai lavori precedenti in questo dominio.
La "Rete Neurale Artificiale Supersimmetrica" nell'apprendimento profondo (denotata (x; θ, bar{θ})Tw), sposa l'importanza di considerare i vincoli biologici allo scopo di generalizzare ulteriormente la propagazione a ritroso. Guardando la progressione delle "geometrie di soluzione"; passare dalla rappresentazione SO(n) (come i modelli simili a Perceptron) alla rappresentazione SU(n) (come leRNNs Unitarie) ha garantito rappresentazioni sempre più ricche nello spazio dei pesi della rete neurale artificiale, e quindi ipotesi sempre migliori erano generabili.La Rete Neurale Artificiale Supersimmetrica esplora un passo avanti naturale, cioè la rappresentazione SU(m|n). Queste rappresentazioni cerebrali biologiche supersimmetriche (Perez et al.) possono essere rappresentate dalla notazione unitaria speciale compatibile con la supercarica SU(m|n), o (x; θ, bar{θ})Tw parametrizzata da θ, bar{θ}, che sono direzioni supersimmetriche, a differenza di θ visto nel tipico modello di apprendimento profondo non supersimmetrico.In particolare, i valori supersimmetrici possono codificare o rappresentare più informazioni del tipico modello di apprendimento profondo, in termini di segnali "potenziali partner" per esempio.
Gli approcci di apprendimento continuo basati sulla regolarizzazione generalmente prevengono la dimenticanza catastrofica aumentando la perdita di addestramento con un obiettivo ausiliario.Tuttavia nella maggior parte degli scenari pratici di ottimizzazione con dati rumorosi e/o gradienti, è possibile che la discesa del gradiente stocastico possa cambiare inavvertitamente i parametri critici.In questo articolo, sosteniamo l'importanza di regolarizzare direttamente le traiettorie di ottimizzazione. Deriviamo una nuova regola di aggiornamento del gradiente co-naturale per l'apprendimento continuo in cui i nuovi gradienti dei compiti sono precondizionati con le informazioni empiriche di Fisher dei compiti precedentemente appresi. Mostriamo che l'uso del gradiente co-naturale riduce sistematicamente l'oblio nell'apprendimento continuo e aiuta a combattere l'overfitting quando si apprende un nuovo compito in uno scenario di risorse scarse.
Studiamo il problema della generazione di codice sorgente in un linguaggio di programmazione fortemente tipizzato, simile a Java, dato un'etichetta (per esempio un insieme di chiamate API o tipi) che porta una piccola quantità di informazioni sul codice desiderato. I programmi generati dovrebbero rispettare una relazione "realistica" tra programmi ed etichette, come esemplificato da un corpus di programmi etichettati disponibili durante l'addestramento.Due sfide in tale generazione di programmi condizionali sono che i programmi generati devono soddisfare un ricco insieme di vincoli sintattici e semantici, e che il codice sorgente contiene molte caratteristiche di basso livello che impediscono l'apprendimento. Affrontiamo questi problemi addestrando un generatore neurale non sul codice ma su *schemi di programma*, o modelli di sintassi di programma che astraggono i nomi e le operazioni che non si generalizzano nei programmi. Durante la generazione, deduciamo una distribuzione anteriore sugli schemi, quindi concretizziamo i campioni di questa distribuzione in programmi sicuri per tipo usando tecniche combinatorie. Implementiamo le nostre idee in un sistema per la generazione di codice Java pesante per le API, e dimostriamo che spesso può prevedere l'intero corpo di un metodo dato solo alcune chiamate API o tipi di dati che appaiono nel metodo.
Proponiamo un approccio per la modellazione di sequenze basato su flussi normalizzanti autoregressivi.Ogni trasformazione autoregressiva, agendo nel tempo, serve come un quadro di riferimento mobile per la modellazione di dinamiche di livello superiore.Questa tecnica fornisce un metodo semplice e generico per migliorare la modellazione di sequenze, con collegamenti a tecniche esistenti e classiche.Dimostriamo l'approccio proposto sia con modelli autonomi, sia come parte di modelli di variabili latenti sequenziali più grandi.I risultati sono presentati su tre set di dati video di riferimento, dove le dinamiche basate sul flusso migliorano le prestazioni log-likelihood rispetto ai modelli di base.
È noto che molti modelli di apprendimento automatico sono suscettibili di attacchi avversari, in cui un aggressore elude un classificatore facendo piccole perturbazioni agli input.Questo articolo discute come gli strumenti industriali di rilevamento del copyright, che svolgono un ruolo centrale sul web, siano suscettibili di attacchi avversari.Discutiamo una serie di sistemi di rilevamento del copyright, e perché sono particolarmente vulnerabili agli attacchi.  Queste vulnerabilità sono particolarmente evidenti per i sistemi basati su reti neurali.  Come prova del concetto, descriviamo un noto metodo di identificazione della musica e implementiamo questo sistema sotto forma di una rete neurale.Attacchiamo poi questo sistema usando semplici metodi a gradiente.La musica avversaria creata in questo modo inganna con successo i sistemi industriali, tra cui il rilevatore di copyright AudioTag e il sistema Content ID di YouTube.Il nostro obiettivo è quello di aumentare la consapevolezza delle minacce poste dagli esempi avversari in questo spazio e di evidenziare l'importanza di indurire i sistemi di rilevamento del copyright agli attacchi.
La Propagazione dell'Equilibrio (EP) è un algoritmo di apprendimento che unisce l'apprendimento automatico e le neuroscienze, calcolando i gradienti che corrispondono strettamente a quelli della Backpropagation Through Time (BPTT), ma con una regola di apprendimento locale nello spazio. Dato un input x e un target associato y, EP procede in due fasi: nella prima fase i neuroni evolvono liberamente verso un primo stato stazionario; nella seconda fase i neuroni in uscita sono spinti verso y fino a raggiungere un secondo stato stazionario. Tuttavia, nelle implementazioni esistenti di EP, la regola di apprendimento non è locale nel tempo: l'aggiornamento dei pesi viene eseguito dopo che le dinamiche della seconda fase sono convergenti e richiede informazioni della prima fase che non sono più disponibili fisicamente. In questo lavoro, proponiamo una versione di EP chiamata Continual Equilibrium Propagation (C-EP) in cui le dinamiche dei neuroni e delle sinapsi avvengono simultaneamente durante tutta la seconda fase, in modo che l'aggiornamento dei pesi diventi locale nel tempo. dimostriamo teoricamente che, a condizione che i tassi di apprendimento siano sufficientemente piccoli, ad ogni passo temporale della seconda fase le dinamiche dei neuroni e delle sinapsi seguono i gradienti della perdita data da BPTT (teorema 1). Dimostriamo l'addestramento con C-EP su MNIST e generalizziamo C-EP alle reti neurali in cui i neuroni sono collegati da connessioni asimmetriche. Mostriamo attraverso gli esperimenti che più gli aggiornamenti della rete seguono i gradienti di BPTT, meglio si comporta in termini di addestramento.Questi risultati portano EP un passo più vicino alla biologia pur mantenendo il suo intimo legame con la backpropagation.
Ci sono due linee principali di ricerca sul ragionamento visivo: la rete a moduli neurali (NMN) con ragionamento esplicito multi-hop attraverso moduli neurali artigianali, e la rete monolitica con ragionamento implicito nello spazio delle caratteristiche latenti.La prima eccelle in interpretabilità e componibilità, mentre la seconda di solito raggiunge prestazioni migliori grazie alla flessibilità del modello e all'efficienza dei parametri.  Al fine di colmare il divario tra i due, presentiamo Meta Module Network (MMN), un nuovo approccio ibrido che può utilizzare in modo efficiente un Meta Modulo per eseguire funzionalità versatili, pur preservando la componibilità e l'interpretabilità attraverso il design modularizzato.Il modello proposto analizza prima una domanda di input in un programma funzionale attraverso un Program Generator.Invece di creare a mano una rete specifica per rappresentare ogni funzione come il tradizionale NMN, usiamo Recipe Encoder per tradurre le funzioni nelle loro ricette corrispondenti (specifiche), che vengono utilizzate per istanziare dinamicamente il Meta Modulo in Moduli Istanza. Per dotare i diversi moduli di istanza di funzionalità designate, viene proposta una struttura insegnante-studente, dove un insegnante simbolico pre-esegue contro i grafici di scena per fornire linee guida per i moduli istanziati (studente) da seguire.In poche parole, MMN adotta il meta modulo per aumentare la sua efficienza di parametrizzazione, e utilizza la codifica delle ricette per migliorare la sua capacità di generalizzazione su NMN.Esperimenti condotti sul benchmark GQA dimostra che: (1) MMN raggiunge un miglioramento significativo sia rispetto a NMN che alla rete monolitica di base; (2) MMN è in grado di generalizzare a funzioni non viste ma correlate.
Il nostro contributo principale è CopyCAT, un attacco mirato in grado di attirare in modo coerente un agente a seguire la politica di un estraneo, che è precompilata, quindi rapidamente dedotta, e potrebbe quindi essere utilizzabile in uno scenario in tempo reale, mostrando la sua efficacia sui giochi Atari 2600 nella nuova impostazione di sola lettura. In quest'ultimo caso, l'avversario non può modificare direttamente lo stato dell'agente - la sua rappresentazione dell'ambiente - ma può solo attaccare l'osservazione dell'agente - la sua percezione dell'ambiente. Modificare direttamente lo stato dell'agente richiederebbe un accesso in scrittura al funzionamento interno dell'agente e noi sosteniamo che questa assunzione è troppo forte nelle impostazioni realistiche.
I precedenti metodi di raccomandazione ibridi sono efficaci per affrontare i problemi di partenza a freddo estraendo i fattori latenti reali degli elementi che partono a freddo (utenti) dalle informazioni laterali, ma soffrono ancora di una bassa efficienza nella raccomandazione online causata dalla costosa ricerca di somiglianza nello spazio latente reale. Questo articolo presenta un collaborative generated hashing (CGH) per migliorare l'efficienza denotando gli utenti e gli elementi come codici binari, che si applica a varie impostazioni: utenti che partono a freddo, elementi che partono a freddo e quelli che partono a caldo. In particolare, CGH è progettato per imparare le funzioni di hash degli utenti e degli oggetti attraverso il principio della lunghezza minima di descrizione (MDL); quindi, può trattare con varie impostazioni di raccomandazione.Inoltre, CGH avvia una nuova strategia di marketing attraverso l'estrazione di potenziali utenti da un passo generativo.Per ricostruire gli utenti efficaci, il principio MDL è usato per imparare codici binari compatti e informativi dai dati del contenuto.Esperimenti estesi su due set di dati pubblici mostrano i vantaggi per le raccomandazioni in varie impostazioni rispetto alle linee di base concorrenti e analizzano la fattibilità dell'applicazione nel marketing.
I recenti sforzi per combinare l'apprendimento delle rappresentazioni con i metodi formali, comunemente noti come i metodi neuro-simbolici, hanno dato vita a una nuova tendenza di applicare ricche architetture neurali per risolvere i classici problemi di ottimizzazione combinatoria.In questo articolo, proponiamo una struttura neurale che può imparare a risolvere il problema della Satisfiabilità dei Circuiti. La nostra struttura è costruita su due contributi fondamentali: una ricca architettura di incorporamento che codifica la struttura del problema e una procedura di addestramento differenziabile end-to-end che imita il Reinforcement Learning e addestra il modello direttamente verso la risoluzione del problema SAT.I risultati sperimentali mostrano la performance di generalizzazione out-of-sample superiore della nostra struttura rispetto al metodo NeuroSAT recentemente sviluppato.
Per esempio, l'apprendimento della massima verosimiglianza è semplice ed efficiente, ma soffre del problema della distorsione dell'esposizione; l'apprendimento di rinforzo come il gradiente della politica affronta il problema, ma può avere un'efficienza di esplorazione proibitiva; una varietà di altri algoritmi come RAML, SPG e data noising sono stati sviluppati in diverse prospettive; questo articolo stabilisce una connessione formale tra questi algoritmi. Presentiamo una formulazione generalizzata di ottimizzazione della politica regolarizzata dall'entropia, e mostriamo che gli algoritmi apparentemente divergenti possono essere tutti riformulati come istanze speciali della struttura, con l'unica differenza che le configurazioni della funzione di ricompensa e un paio di iperparametri.L'interpretazione unificata offre una visione sistematica delle diverse proprietà di esplorazione e di efficienza di apprendimento.Inoltre, basandoci sulla struttura, presentiamo un nuovo algoritmo che interpola dinamicamente tra gli algoritmi esistenti per un apprendimento migliore.Gli esperimenti sulla traduzione automatica e la sintesi del testo dimostrano la superiorità dell'algoritmo proposto.
Segnaliamo il progetto SHINRA, un progetto di strutturazione di Wikipedia con schema di costruzione collaborativa.L'obiettivo del progetto è quello di creare una base di conoscenza enorme e ben strutturata da utilizzare in applicazioni NLP, come QA, sistemi di dialogo e sistemi NLP spiegabili.Viene creato sulla base di uno schema di "Resource by Collaborative Contribution (RbCC)". Ci sono basi di conoscenza leggibili dalla macchina come CYC, DBpedia, YAGO, Freebase Wikidata e così via, ma ognuno di loro ha problemi da risolvere. CYC ha un problema di copertura, e altri hanno un problema di coerenza dovuto al fatto che questi sono basati su Wikipedia e/o creati da molti ma intrinsecamente incoerenti lavoratori della folla. Al fine di risolvere quest'ultimo problema, abbiamo iniziato un progetto per la strutturazione di Wikipedia utilizzando la costruzione automatica di basi di conoscenza shared-task.Gli shared-task di costruzione automatica di basi di conoscenza sono stati popolari e ben studiati per decenni.Tuttavia, questi compiti sono progettati solo per confrontare le prestazioni di diversi sistemi, e per trovare quale sistema si classifica meglio su dati di prova limitati.I risultati dei sistemi partecipanti non sono condivisi e i sistemi possono essere abbandonati una volta che il compito è finito.Crediamo che questa situazione possa essere migliorata con i seguenti cambiamenti:1. progettando lo shared-task per costruire basi di conoscenza piuttosto che valutare solo dati di prova limitati2. rendendo gli output di tutti i sistemi aperti al pubblico in modo da poter eseguire l'ensemble learning per creare i risultati migliori dei sistemi migliori3. ripetendo il compito in modo da poter eseguire il compito con i dati di allenamento più grandi e migliori dall'output del compito precedente (bootstrapping e apprendimento attivo)Abbiamo condotto "SHINRA2018" con lo schema sopra menzionato e in questo documento riportiamo i risultati e le direzioni future del progetto. Il compito è quello di estrarre i valori degli attributi predefiniti dalle pagine di Wikipedia.Abbiamo categorizzato la maggior parte delle entità nella Wikipedia giapponese (cioè 730 mila entità) nelle 200 categorie ENE.Sulla base di questi dati, il compito condiviso è quello di estrarre i valori degli attributi dalle pagine di Wikipedia. Abbiamo distribuito 600 dati di formazione e i partecipanti sono tenuti a presentare i valori degli attributi per tutte le entità rimanenti dello stesso tipo di categoria, quindi 100 dati per ogni categoria sono utilizzati per valutare l'output del sistema nel compito condiviso. Abbiamo condotto un apprendimento ensemble preliminare sugli output e abbiamo trovato 15 miglioramenti del punteggio F1 su una categoria e una media di 8 miglioramenti del punteggio F1 su tutte le 5 categorie che abbiamo testato rispetto a una linea di base forte. Sulla base di questi risultati promettenti, abbiamo deciso di condurre tre compiti nel 2019; compito di categorizzazione multilingue (ML), estrazione per le stesse 5 categorie in giapponese con dati di formazione più grandi (JP-5) ed estrazione per 34 nuove categorie in giapponese (JP-34).
I recenti studi sulla super-risoluzione delle immagini (SR) sfruttano le reti neurali convoluzionali molto profonde e le ricche caratteristiche gerarchiche che hanno offerto, il che porta a migliori prestazioni di ricostruzione rispetto ai metodi convenzionali.Tuttavia, i piccoli campi recettivi nel processo di up-sampling e ricostruzione di quei modelli impediscono loro di trarre pieno vantaggio dalle informazioni contestuali globali.Questo causa problemi per un ulteriore miglioramento delle prestazioni.In questo articolo, ispirato dai principi di ricostruzione delle immagini del sistema visivo umano, proponiamo una rete di ragionamento globale di super-risoluzione delle immagini (SRGRN) per imparare efficacemente le correlazioni tra diverse regioni di un'immagine, attraverso il ragionamento globale. In particolare, proponiamo un modulo di up-sampling di ragionamento globale (GRUM) e un blocco di ricostruzione di ragionamento globale (GRRB), che costruiscono un modello a grafo per eseguire il ragionamento di relazione sulle regioni di immagini a bassa risoluzione (LR), con l'obiettivo di ragionare sulle interazioni tra diverse regioni nel processo di up-sampling e ricostruzione e quindi di sfruttare più informazioni contestuali per generare dettagli accurati. La nostra proposta SRGRN è più robusta e può gestire immagini a bassa risoluzione che sono corrotte da più tipi di degrado. esperimenti estesi su diversi set di dati di riferimento mostrano che il nostro modello supera altri metodi all'avanguardia. inoltre il nostro modello è leggero e consuma meno potenza di calcolo, il che lo rende molto adatto per l'implementazione nella vita reale.
Le reti neurali a grafo (GNN) hanno ricevuto un'enorme attenzione di recente grazie alla loro potenza nella gestione dei dati a grafo per diversi compiti a valle in diversi domini applicativi.La chiave di GNN è i suoi filtri convoluzionali a grafo, e recentemente sono stati progettati vari tipi di filtri. Tuttavia, manca ancora un'analisi approfondita su (1) se esiste un filtro migliore che possa funzionare meglio su tutti i dati del grafico; (2) quali proprietà del grafico influenzano la scelta ottimale del filtro del grafico; (3) come progettare il filtro appropriato adattivo ai dati del grafico.In questo documento, ci concentriamo sull'affrontare le tre domande di cui sopra. Usando lo strumento di valutazione, scopriamo che non c'è un singolo filtro come una "pallottola d'argento" che esegue il meglio su tutti i grafi possibili.Inoltre, le proprietà diverse della struttura del grafico influenzano la scelta di progettazione del filtro convoluzionale del grafico ottimale.Sulla base di questi risultati, sviluppiamo la rete neurale adattiva del grafico del filtro (AFGNN), un modello semplice ma potente che può imparare adattativamente il filtro compito-specifico. Per un dato grafico, sfrutta la valutazione del filtro del grafico come regolarizzazione e impara a combinare da un insieme di filtri di base.Gli esperimenti su entrambi i set di dati sintetici e reali di riferimento dimostrano che il nostro modello proposto può effettivamente imparare un filtro appropriato ed eseguire bene sulle attività del grafico.
L'avanzamento delle operazioni di pooling dei nodi nelle reti neurali a grafo (GNN) è rimasto indietro rispetto alla progettazione febbrile di nuove tecniche di message-passing, e il pooling rimane un'impresa importante e impegnativa per la progettazione di architetture profonde.In questo articolo, proponiamo un'operazione di pooling per GNN che sfrutta una perdita differenziabile non supervisionata basata sull'obiettivo di ottimizzazione minCut.Per ogni nodo, il nostro metodo impara un vettore di assegnazione soft cluster che dipende dalle caratteristiche del nodo, il compito di inferenza obiettivo (ad es, Il pooling dei grafi si ottiene applicando la matrice dei vettori di assegnazione alla matrice di adiacenza e alle caratteristiche dei nodi. Convalidiamo l'efficacia del metodo di pooling proposto su una varietà di compiti supervisionati e non supervisionati.
I grafi di conoscenza sono rappresentazioni strutturate dei fatti del mondo reale, ma tipicamente contengono solo un piccolo sottoinsieme di tutti i fatti possibili. La previsione dei collegamenti è il compito di dedurre i fatti mancanti basandosi su quelli esistenti. Proponiamo TuckER, un modello lineare relativamente semplice ma potente basato sulla decomposizione Tucker della rappresentazione tensoriale binaria delle triple dei grafi di conoscenza.
Grazie alle innovazioni nella progettazione dell'architettura, i modelli di rete neurale più profondi e ampi offrono prestazioni migliori su una varietà di compiti diversi, ma l'aumento dell'impronta di memoria di questi modelli rappresenta una sfida durante l'addestramento, quando tutte le attivazioni degli strati intermedi devono essere memorizzate per la back-propagation. La memoria limitata della GPU costringe i professionisti a fare scelte sub-ottimali: o allenarsi in modo inefficiente con lotti più piccoli di esempi; o limitare l'architettura per avere una profondità e una larghezza inferiori, e meno strati a risoluzioni spaziali più elevate.Questo lavoro introduce una strategia di approssimazione che riduce significativamente l'impronta di memoria di una rete durante l'allenamento, ma ha un effetto trascurabile sulle prestazioni di allenamento e sulla spesa computazionale. Durante il passaggio in avanti, sostituiamo le attivazioni con approssimazioni a bassa precisione immediatamente dopo che sono state utilizzate dagli strati successivi, liberando così la memoria. Le attivazioni approssimate vengono quindi utilizzate durante il passaggio all'indietro. Questo approccio limita l'accumulo di errori attraverso il passaggio in avanti e all'indietro - perché il calcolo in avanti attraverso la rete avviene ancora a piena precisione, e l'approssimazione ha un effetto limitato quando si calcolano i gradienti all'ingresso di uno strato. Gli esperimenti, su CIFAR e ImageNet, mostrano che l'utilizzo del nostro approccio con approssimazioni in virgola fissa a 8 e anche a 4 bit delle attivazioni in virgola mobile a 32 bit ha solo un effetto minore sulle prestazioni di addestramento e convalida, mentre offre un risparmio significativo nell'uso della memoria.
La stima delle frequenze degli elementi in un flusso di dati è un compito fondamentale nell'analisi dei dati e nell'apprendimento automatico.Il problema è tipicamente affrontato utilizzando algoritmi di streaming che possono elaborare dati molto grandi utilizzando uno storage limitato.Gli algoritmi di streaming di oggi, tuttavia, non possono sfruttare i modelli nel loro input per migliorare le prestazioni.Proponiamo una nuova classe di algoritmi che imparano automaticamente i modelli rilevanti nei dati di input e li usano per migliorare le sue stime di frequenza.   Gli algoritmi proposti combinano i vantaggi dell'apprendimento automatico con le garanzie formali disponibili attraverso la teoria degli algoritmi.  Dimostriamo che i nostri algoritmi basati sull'apprendimento hanno errori di stima più bassi rispetto alle loro controparti senza apprendimento.  Valutiamo anche i nostri algoritmi su due set di dati del mondo reale e dimostriamo empiricamente i loro guadagni di performance.
La predizione dei collegamenti nei grafi semplici è un problema fondamentale in cui i nuovi collegamenti tra i nodi sono previsti sulla base della struttura osservata del grafo. Tuttavia, in molte applicazioni del mondo reale, c'è la necessità di modellare le relazioni tra i nodi che vanno oltre le associazioni a coppie, per esempio in una reazione chimica, la relazione tra i reagenti e i prodotti è intrinsecamente di ordine superiore. Inoltre, c'è bisogno di rappresentare la direzione dai reagenti ai prodotti.Gli ipergrafi forniscono un modo naturale per rappresentare tali relazioni complesse di ordine superiore.Anche se i Graph Convolutional Networks (GCN) sono recentemente emersi come un potente approccio basato sull'apprendimento profondo per la predizione dei collegamenti sui grafi semplici, la loro idoneità per la predizione dei collegamenti negli ipergrafi è inesplorata -- noi riempiamo questa lacuna in questo articolo e proponiamo Neural Hyperlink Predictor (NHP). NHP adatta le GCN per la predizione dei link negli ipergrafi. Proponiamo due varianti di NHP --NHP-U e NHP-D - per la predizione dei link su ipergrafi indiretti e diretti, rispettivamente.Per quanto ne sappiamo, NHP-D è il primo metodo per la predizione dei link su ipergrafi diretti.Attraverso ampi esperimenti su più dataset del mondo reale, dimostriamo l'efficacia di NHP.
In questo articolo, presentiamo un metodo per la decomposizione avversaria della rappresentazione del testo.Questo metodo può essere usato per decomporre una rappresentazione di una frase di input in diversi vettori indipendenti, dove ogni vettore è responsabile di un aspetto specifico della frase di input.Valutiamo il metodo proposto su due casi di studio: la conversione tra diversi registri sociali e il cambiamento diacronico della lingua.Mostriamo che il metodo proposto è capace di un cambiamento controllato a grana fine di questi aspetti della frase di input. Per esempio, il nostro modello è in grado di apprendere una rappresentazione continua (piuttosto che categorica) dello stile della frase, in linea con la realtà dell'uso della lingua.Il modello utilizza un addestramento adversariale-motivazionale e include una speciale perdita motivazionale, che agisce in modo opposto al discriminatore e incoraggia una migliore decomposizione.Infine, valutiamo le embeddings di significato ottenute su un compito a valle di rilevamento di parafrasi e dimostriamo che sono significativamente migliori delle embeddings di un normale autoencoder.
Siamo stati contattati da un gruppo di operatori sanitari che sono coinvolti nella cura dei pazienti cronici alla ricerca di potenziali tecnologie per facilitare il processo di revisione dei dati generati dal paziente durante le visite cliniche.  Al fine di comprendere gli atteggiamenti degli operatori sanitari verso la revisione dei dati generati dal paziente, abbiamo (1) condotto un focus group con un gruppo misto di operatori sanitari.Successivamente, per ottenere le prospettive dei pazienti, abbiamo (2) intervistato otto pazienti cronici, raccolto un campione dei loro dati e progettato una serie di visualizzazioni che rappresentano i dati dei pazienti che abbiamo raccolto.Infine, abbiamo (3) cercato un feedback sui disegni di visualizzazione dagli operatori sanitari che hanno richiesto questa esplorazione. Abbiamo trovato quattro fattori che modellano i dati generati dal paziente: dati e contesto, motivazione del paziente, impegno di tempo del paziente e cerchio di supporto del paziente.Informati dai risultati dei nostri studi, abbiamo discusso l'importanza di progettare visualizzazioni generate dal paziente per gli individui considerando sia il paziente che l'operatore sanitario piuttosto che progettare con lo scopo di generalizzare e abbiamo fornito linee guida per progettare future visualizzazioni di dati generate dal paziente.
Recentemente, sono stati proposti modelli dinamici in avanti basati su reti neurali che tentano di apprendere le dinamiche dei sistemi fisici in modo deterministico. Mentre il movimento a breve termine può essere previsto con precisione, le previsioni a lungo termine soffrono dell'accumulo di errori di input e di previsione che possono portare a traiettorie plausibili ma diverse che divergono dalla verità del terreno.  Un sistema che predice le distribuzioni degli stati fisici futuri per orizzonti temporali lunghi basati sulla sua incertezza è quindi una soluzione promettente.  In questo lavoro, introduciamo un nuovo metodo robusto di campionamento Monte Carlo basato sul dropout grafo-convoluzionale che ci permette di campionare più traiettorie plausibili per uno stato iniziale dato un predittore di dinamiche in avanti basato su reti neurali.  Introducendo una nuova perdita di conservazione della forma e addestrando il nostro modello dinamico in modo ricorrente, stabilizziamo le previsioni a lungo termine. Mostriamo che gli errori di previsione delle dinamiche a lungo termine del nostro modello su complicate interazioni fisiche di oggetti rigidi e deformabili di varie forme sono significativamente più bassi rispetto alle forti linee di base esistenti.
C'è stata una grande quantità di interesse, sia in passato che particolarmente di recente, sul vantaggio relativo di diverse famiglie di approssimatori di funzioni universali, per esempio reti neurali, polinomi, funzioni razionali, ecc. Tuttavia, la ricerca attuale si è concentrata quasi esclusivamente sulla comprensione di questo problema in un'impostazione di caso peggiore: ad esempio, caratterizzando la migliore approssimazione L1 o L_{infty} in una scatola (o talvolta, anche sotto una distribuzione di dati costruita avversariamente). Tuttavia, nelle applicazioni tipiche ci aspettiamo che i dati siano ad alta dimensione, ma strutturati -- così, sarebbe importante solo approssimare bene la funzione desiderata sulla parte rilevante del suo dominio, per esempio un piccolo collettore su cui i dati di input reali giacciono effettivamente.Inoltre, anche all'interno di questo dominio la qualità desiderata di approssimazione può non essere uniforme; per esempio nei problemi di classificazione, l'approssimazione deve essere più accurata vicino al confine della decisione.Questi problemi, al meglio della nostra conoscenza, sono rimasti inesplorati fino ad ora.	Con questo in mente, analizziamo le prestazioni delle reti neurali e dei kernel polinomiali in un'impostazione di regressione naturale in cui i dati godono di una struttura latente sparsa, e le etichette dipendono in modo semplice dalle variabili latenti.Diamo un'analisi teorica quasi stretta delle prestazioni sia delle reti neurali che dei polinomi per questo problema, oltre a verificare la nostra teoria con simulazioni.I nostri risultati coinvolgono sia nuove tecniche (complesse-analitiche), che possono essere di interesse indipendente, e mostrano differenze qualitative sostanziali con ciò che è noto nell'impostazione del worst-case.
I recenti modelli generativi profondi possono fornire immagini foto-realistiche così come embeddings di contenuto visivo o testuale utili per affrontare vari compiti di computer vision e di elaborazione del linguaggio naturale.La loro utilità è tuttavia spesso limitata dalla mancanza di controllo sul processo generativo o dalla scarsa comprensione della rappresentazione appresa.Per superare questi importanti problemi, lavori molto recenti hanno mostrato l'interesse di studiare la semantica dello spazio latente dei modelli generativi. In questo articolo, proponiamo di avanzare sull'interpretabilità dello spazio latente dei modelli generativi introducendo un nuovo metodo per trovare direzioni significative nello spazio latente di qualsiasi modello generativo lungo le quali possiamo muoverci per controllare precisamente proprietà specifiche dell'immagine generata, come la posizione o la scala dell'oggetto nell'immagine. Il nostro metodo è debolmente supervisionato e particolarmente adatto per la ricerca di direzioni che codificano semplici trasformazioni dell'immagine generata, come la traduzione, lo zoom o variazioni di colore.
Le moderne architetture di reti neurali utilizzano trasformazioni lineari strutturate, come le matrici a basso rango, le matrici rade, le permutazioni e la trasformata di Fourier, per migliorare la velocità di inferenza e ridurre l'uso della memoria rispetto alle mappe lineari generali. Tuttavia, scegliere quale delle miriadi di trasformazioni strutturate utilizzare (e la sua parametrizzazione associata) è un compito laborioso che richiede di barattare velocità, spazio e precisione. Consideriamo un approccio diverso: introduciamo una famiglia di matrici chiamate matrici caleidoscopio (K-matrici) che catturano in modo dimostrabile qualsiasi matrice strutturata con una complessità di spazio (parametro) e tempo (operazione aritmetica) quasi ottimale. Convalidiamo empiricamente che le matrici K possono essere apprese automaticamente all'interno di pipeline end-to-end per sostituire procedure fatte a mano, al fine di migliorare la qualità del modello. Ad esempio, la sostituzione dei rimescolamenti dei canali in ShuffleNet migliora l'accuratezza della classificazione su ImageNet fino al 5%. Le matrici K apprendibili possono anche semplificare le pipeline costruite a mano - sostituiamo il calcolo delle caratteristiche dei banchi di filtri nella pre-elaborazione dei dati vocali con uno strato di caleidoscopio, ottenendo solo una perdita dello 0,4% nell'accuratezza del compito di riconoscimento vocale TIMIT. Le matrici K possono anche catturare la struttura latente nei modelli: per un impegnativo compito di classificazione delle immagini permutate, l'aggiunta di una matrice K a un'architettura convoluzionale standard può consentire l'apprendimento della permutazione latente e migliorare l'accuratezza di oltre 8 punti.Forniamo un'implementazione praticamente efficiente del nostro approccio e usiamo le matrici K in una rete Transformer per raggiungere una velocità di inferenza end-to-end più veloce del 36% in un compito di traduzione linguistica.
Proponiamo un metodo, chiamato Label Embedding Network, che può imparare la rappresentazione delle etichette (label embedding) durante il processo di formazione delle reti profonde.Con il metodo proposto, l'embedding delle etichette viene appreso automaticamente e in modo adattivo attraverso la back propagation.L'originale funzione di perdita rappresentata da un solo punto viene convertita in una nuova funzione di perdita con distribuzioni morbide, in modo che le etichette originariamente non correlate abbiano interazioni continue tra loro durante il processo di formazione. I risultati sperimentali basati su compiti competitivi dimostrano l'efficacia del metodo proposto, e l'incorporazione delle etichette apprese è ragionevole e interpretabile. Il metodo proposto raggiunge risultati comparabili o addirittura migliori rispetto ai sistemi all'avanguardia.
L'addestramento decentralizzato dei modelli di apprendimento profondo è un elemento chiave per consentire la privacy dei dati e l'apprendimento on-device sulle reti, così come per la scalabilità efficiente a grandi cluster di calcolo.Poiché gli approcci attuali sono limitati dalla larghezza di banda della rete, proponiamo l'uso della compressione della comunicazione nel contesto dell'addestramento decentralizzato.Mostriamo che Choco-SGD raggiunge un'accelerazione lineare nel numero di lavoratori per rapporti di compressione arbitrariamente alti su funzioni generali non convesse, e dati di addestramento non IID. Dimostriamo le prestazioni pratiche dell'algoritmo in due scenari chiave: la formazione di modelli di apprendimento profondo (i) su dispositivi utente decentralizzati, collegati da una rete peer-to-peer e (ii) in un centro dati.
Mostriamo che Entropy-SGD (Chaudhari et al., 2017), se visto come un algoritmo di apprendimento, ottimizza un PAC-Bayes bound sul rischio di un classificatore di Gibbs (posteriore), cioè, un classificatore randomizzato ottenuto da una perturbazione sensibile al rischio dei pesi di un classificatore appreso.Entropia-SGD funziona ottimizzando la priorità del limite, violando l'ipotesi del teorema PAC-Bayes che la priorità sia scelta indipendentemente dai dati. Infatti, le implementazioni disponibili di Entropy-SGD ottengono rapidamente un errore di addestramento nullo su etichette casuali e lo stesso vale per il posteriore di Gibbs. Per ottenere un limite di generalizzazione valido, mostriamo che un anteriore ε-differenzialmente privato produce un limite PAC-Bayes valido, una conseguenza diretta dei risultati che collegano la generalizzazione alla privacy differenziale. Usando la dinamica stocastica del gradiente Langevin (SGLD) per approssimare il ben noto meccanismo di rilascio esponenziale, osserviamo che l'errore di generalizzazione su MNIST (misurato sui dati trattenuti) cade entro i limiti (empiricamente nonvacui) calcolati sotto l'assunzione che SGLD produca campioni perfetti.In particolare, Entropy-SGLD può essere configurato per produrre limiti di generalizzazione relativamente stretti e ancora adattarsi alle etichette reali, sebbene queste stesse impostazioni non ottengano prestazioni all'avanguardia.
In questo articolo, studiamo l'apprendimento delle reti neurali profonde per l'ispezione ottica automatizzata nella produzione industriale.Il nostro risultato preliminare ha mostrato il sorprendente miglioramento delle prestazioni con l'apprendimento di trasferimento dal dominio di origine completamente dissimile: Un ulteriore studio per demistificare questo miglioramento mostra che il transfer learning produce una rete altamente comprimibile, che non era il caso della rete appresa da zero.Il risultato sperimentale mostra che c'è un calo trascurabile di precisione nella rete appresa dal transfer learning fino a quando non viene compressa a 1/128 di riduzione del numero di ﬁltri di convoluzione.Questo risultato è contrario alla compressione senza transfer learning che perde più del 5% di precisione allo stesso tasso di compressione.
La sfida chiave nell'apprendimento semi-supervisionato è come sfruttare efficacemente i dati senza etichetta per migliorare le prestazioni di apprendimento.Il metodo classico di propagazione dell'etichetta, nonostante la sua popolarità, ha una capacità di modellazione limitata in quanto sfrutta solo le informazioni del grafico per fare previsioni.In questo articolo, consideriamo la propagazione dell'etichetta da una prospettiva di elaborazione del segnale del grafico e la decomponiamo in tre componenti: segnale, filtro e classificatore.Estendendo i tre componenti, proponiamo una semplice struttura generalizzata di propagazione dell'etichetta (GLP) per l'apprendimento semi-supervisionato. GLP integra naturalmente le informazioni della caratteristica di dati e del grafico ed offre la flessibilitÃ di selezione dei filtri adatti e dei classificatori dominio-specifici per le applicazioni differenti. interessante, GLP inoltre fornisce la nuova comprensione nella rete convolutional popolare del grafico e chiarisce i relativi meccanismi di funzionamento. gli esperimenti estesi su tre reti della citazione, un grafico di conoscenza e un insieme di dati di immagine dimostrano l'efficienza e l'efficacia di GLP.
Poiché la scelta e la messa a punto dell'ottimizzatore influenzano la velocità e, in ultima analisi, le prestazioni dell'apprendimento profondo, vi è una significativa ricerca passata e recente in questo settore; tuttavia, forse sorprendentemente, non esiste un protocollo generalmente concordato per la valutazione quantitativa e riproducibile delle strategie di ottimizzazione dell'apprendimento profondo. Suggeriamo routine e benchmark per l'ottimizzazione stocastica, con particolare attenzione agli aspetti unici dell'apprendimento profondo, come la stocasticità, la sintonizzabilità e la generalizzazione.Come contributo principale, presentiamo DeepOBS, un pacchetto Python di benchmark per l'ottimizzazione dell'apprendimento profondo.Il pacchetto affronta le sfide chiave nella valutazione quantitativa degli ottimizzatori stocastici, e automatizza la maggior parte delle fasi del benchmarking. La libreria include un ampio ed estensibile set di problemi di ottimizzazione realistici pronti all'uso, come l'addestramento di reti residue per la classificazione delle immagini su ImageNet o modelli di predizione del linguaggio a livello di carattere, così come classici popolari come MNIST e CIFAR-10. Il pacchetto fornisce anche risultati di base realistici per gli ottimizzatori più popolari su questi problemi di test, garantendo un confronto equo con la concorrenza quando si fa il benchmarking di nuovi ottimizzatori, e senza dover eseguire esperimenti costosi.Viene fornito con back-end di output che produce direttamente codice LaTeX per l'inclusione in pubblicazioni accademiche.Supporta TensorFlow ed è disponibile open source.
I lavori recenti si sono concentrati sull'uso di tecniche non supervisionate come la modellazione del linguaggio per ottenere queste embeddings. Al contrario, questo lavoro si concentra sull'estrazione di rappresentazioni da più modelli supervisionati pre-addestrati, che arricchisce le embeddings di parole con conoscenze specifiche del compito e del dominio. Gli esperimenti eseguiti in impostazioni cross-task, cross-domain e cross-lingual indicano che tali embeddings supervisionati sono utili, specialmente in ambienti con poche risorse, ma l'entità dei guadagni dipende dalla natura del compito e del dominio.
Costruiamo un quadro teorico per la comprensione dei metodi pratici di meta-apprendimento che permette l'integrazione di formalizzazioni sofisticate della somiglianza dei compiti con la vasta letteratura sull'ottimizzazione convessa online e gli algoritmi di predizione sequenziale al fine di fornire garanzie di prestazioni all'interno del compito. Il nostro approccio migliora le recenti analisi del trasferimento dei parametri consentendo che la somiglianza dei compiti sia appresa in modo adattivo e migliorando i limiti del rischio di trasferimento nell'impostazione dell'apprendimento statistico, ma porta anche a semplici derivazioni dei limiti del rammarico medio per algoritmi efficienti nelle impostazioni in cui l'ambiente dei compiti cambia dinamicamente o i compiti condividono una certa struttura geometrica.
In questo lavoro, proponiamo un metodo auto-supervisionato per imparare rappresentazioni di frasi con un'iniezione di conoscenza linguistica.Molteplici quadri linguistici propongono diverse strutture di frasi da cui il significato semantico potrebbe essere espresso da operazioni compositive di parole.Miriamo a trarre vantaggio da questa diversità linguistica e imparare a rappresentare frasi contrastando questi diversi punti di vista. Formalmente, più punti di vista della stessa frase sono mappati in rappresentazioni vicine; al contrario, i punti di vista di altre frasi sono mappati ulteriormente. Contrastando diversi punti di vista linguistici, miriamo a costruire embeddings che catturino meglio la semantica e che siano meno sensibili alla forma esteriore della frase.
Il sistema nervoso periferico rappresenta il sistema di input/output per il cervello.Gli elettrodi da polso impiantati sul sistema nervoso periferico permettono l'osservazione e il controllo di questo sistema, tuttavia, i dati prodotti da questi elettrodi hanno un basso rapporto segnale/rumore e un contenuto di segnale complesso.In questo articolo, consideriamo l'analisi dei dati neurali registrati dal nervo vago in modelli animali, e sviluppiamo un learner non supervisionato basato su reti neurali convoluzionali che è in grado di de-noise e clusterare simultaneamente regioni dei dati in base al contenuto del segnale.
Gli attacchi avversari alle reti neurali convoluzionali (CNN) hanno guadagnato un'attenzione significativa e ci sono stati sforzi di ricerca attivi sui meccanismi di difesa. Sono stati proposti metodi di trasformazione stocastica dell'input, dove l'idea è quella di recuperare l'immagine dall'attacco avversario con una trasformazione casuale, e di prendere il voto di maggioranza come consenso tra i campioni casuali. Tuttavia, la trasformazione migliora l'accuratezza sulle immagini avversarie a scapito dell'accuratezza sulle immagini pulite.Mentre è intuitivo che l'accuratezza sulle immagini pulite si deteriorerebbe, il meccanismo esatto in cui ciò avviene non è chiaro.In questo articolo, studiamo la distribuzione di softmax indotta dalle trasformazioni stocastiche. Osserviamo che con trasformazioni casuali sulle immagini pulite, anche se la massa della distribuzione di softmax potrebbe spostarsi verso la classe sbagliata, la distribuzione risultante di softmax potrebbe essere usata per correggere la previsione.Inoltre, sulle controparti avversarie, con la trasformazione dell'immagine, le forme risultanti della distribuzione di softmax sono simili alle distribuzioni dalle immagini pulite.Con queste osservazioni, proponiamo un metodo per migliorare le difese esistenti basate sulla trasformazione. I nostri studi empirici dimostrano che il nostro classificatore di distribuzione, addestrandosi solo sulle distribuzioni ottenute dalle immagini pulite, supera il voto di maggioranza sia per le immagini pulite che per quelle avversarie.Il nostro metodo è generico e può essere integrato con le difese esistenti basate sulla trasformazione.
L'apprendimento di rinforzo in scenari multi-agente è importante per le applicazioni del mondo reale, ma presenta sfide oltre a quelle viste nelle impostazioni a singolo agente.Presentiamo un algoritmo actor-critic che allena le politiche decentralizzate in impostazioni multi-agente, utilizzando critiche calcolate centralmente che condividono un meccanismo di attenzione che seleziona le informazioni rilevanti per ogni agente ad ogni timetep. Questo meccanismo di attenzione permette un apprendimento più efficace e scalabile in ambienti multi-agente complessi, se confrontato con approcci recenti. Il nostro approccio è applicabile non solo alle impostazioni cooperative con ricompense condivise, ma anche alle impostazioni di ricompensa individualizzate, comprese le impostazioni avversarie, e non fa ipotesi sugli spazi di azione degli agenti.
La recente espansione delle applicazioni di machine learning alla biologia molecolare ha dimostrato di avere un contributo significativo alla nostra comprensione dei sistemi biologici, e del funzionamento del genoma in particolare.I progressi tecnologici hanno permesso la raccolta di grandi set di dati epigenetici, comprese le informazioni sui vari fattori di legame del DNA (ChIP-Seq) e la struttura spaziale del DNA (Hi-C).Diversi studi hanno confermato la correlazione tra i fattori di legame del DNA e i domini topologicamente associati (TADs) nella struttura del DNA.Tuttavia, le informazioni sulla vicinanza fisica rappresentata dalle coordinate genomiche non sono state ancora utilizzate per il miglioramento dei modelli di previsione. In questa ricerca, ci concentriamo sui metodi di apprendimento automatico per la predizione dei modelli di ripiegamento del DNA in un classico organismo modello Drosophila melanogaster. L'articolo considera modelli lineari con quattro tipi di regolarizzazione, Gradient Boosting e reti neurali ricorrenti per la predizione dei modelli di ripiegamento della cromatina dai segni epigenetici. Il modello bidirezionale LSTM RNN ha superato tutti i modelli e ha ottenuto i migliori punteggi di predizione, dimostrando l'utilizzo di modelli complessi e l'importanza della memoria degli stati sequenziali del DNA per il ripiegamento della cromatina.
Il lavoro precedente ha dimostrato empiricamente che le reti neurali di grandi dimensioni possono essere ridotte significativamente in termini di dimensioni, pur conservando la loro accuratezza.La compressione del modello è diventata un argomento di ricerca centrale, in quanto è cruciale per la distribuzione di reti neurali su dispositivi con risorse di calcolo e di memoria limitate. La maggior parte dei metodi di compressione sono basati sull'euristica e non offrono garanzie nel caso peggiore sul trade-off tra il tasso di compressione e l'errore di approssimazione per un campione arbitrariamente nuovo.Proponiamo il primo algoritmo di pruning neurale efficiente e indipendente dai dati con un trade-off dimostrabile tra il suo tasso di compressione e l'errore di approssimazione per qualsiasi campione futuro. Il nostro metodo si basa sulla struttura del coreset, che trova un piccolo sottoinsieme ponderato di punti che approssima in modo dimostrabile gli input originali. In particolare, approssimiamo l'output di uno strato di neuroni con un coreset di neuroni nello strato precedente e scartiamo il resto. A differenza dei lavori precedenti, il nostro coreset è indipendente dai dati, il che significa che garantisce in modo dimostrabile l'accuratezza della funzione per qualsiasi input $x\in \mathbb{R}^d$, incluso uno avverso.Dimostriamo l'efficacia del nostro metodo su architetture di rete popolari.In particolare, i nostri coreset producono una compressione del 90% dell'architettura LeNet-300-100 su MNIST migliorando l'accuratezza.
I problemi di pianificazione in ambienti parzialmente osservabili non possono essere risolti direttamente con le reti convoluzionali e richiedono una qualche forma di memoria.Ma, anche le reti di memoria con schemi di indirizzamento sofisticati non sono in grado di imparare il ragionamento intelligente in modo soddisfacente a causa della complessità di imparare contemporaneamente ad accedere alla memoria e a pianificare.Per mitigare queste sfide proponiamo la Memory Augmented Control Network (MACN).La rete divide la pianificazione in un processo gerarchico. Ad un livello più basso, impara a pianificare in uno spazio osservato localmente; ad un livello più alto, usa un insieme di politiche calcolate sugli spazi osservati localmente per imparare un piano ottimale nell'ambiente globale in cui sta operando. La performance della rete viene valutata su compiti di pianificazione del percorso in ambienti in presenza di ostacoli semplici e complessi e, inoltre, viene testata la sua capacità di generalizzare a nuovi ambienti non visti nel set di allenamento.
Le rappresentazioni contestualizzate di parole come ELMo e BERT sono diventate il punto di partenza de facto per incorporare rappresentazioni preaddestrate per compiti NLP a valle. In queste impostazioni, le rappresentazioni contestuali hanno in gran parte reso obsoleti i loro predecessori di incorporazione statica come Word2Vec e GloVe.Tuttavia, le incorporazioni statiche hanno i loro vantaggi in quanto sono semplici da capire e più veloci da usare. In questo lavoro, introduciamo metodi semplici per la generazione di embeddings statici a partire da rappresentazioni contestuali preaddestrate esistenti e dimostriamo che essi superano le embeddings di Word2Vec e GloVe in una varietà di compiti di somiglianza e correlazione di parole. In questo modo, i nostri risultati rivelano anche intuizioni che possono essere utili per i successivi compiti a valle che utilizzano le nostre incorporazioni o i modelli contestuali originali. Inoltre, dimostriamo il maggiore potenziale di analisi applicando gli approcci esistenti per stimare i bias sociali nelle incorporazioni di parole. La nostra analisi costituisce lo studio più completo dei bias sociali nelle rappresentazioni contestuali delle parole (attraverso la proxy delle nostre embeddings distillate) e rivela una serie di incongruenze nelle tecniche attuali per quantificare i bias sociali nelle embeddings delle parole.Rilasciamo pubblicamente il nostro codice e le embeddings delle parole distillate per sostenere la ricerca riproducibile e la più ampia comunità NLP.
Il cervello esegue l'apprendimento non supervisionato e (forse) l'apprendimento supervisionato simultaneo.Questo solleva la questione se un ibrido di metodi supervisionati e non supervisionati produrrà un apprendimento migliore.Ispirati dal ricco spazio di regole di apprendimento Hebbian, ci siamo proposti di imparare direttamente la regola di apprendimento non supervisionato sulle informazioni locali che meglio aumenta un segnale supervisionato. Presentiamo l'algoritmo di formazione Hebbian-augmented (HAT) per combinare l'apprendimento basato sul gradiente con una regola non supervisionata sull'attività pre-sinaptica, le attività post-sinaptiche e i pesi attuali. Testiamo l'effetto di HAT su un problema semplice (Fashion-MNIST) e troviamo prestazioni costantemente superiori all'apprendimento supervisionato da solo.        Troviamo inoltre che la regola di aggiornamento meta-appresa è una funzione variabile nel tempo; quindi, è difficile individuare una regola di aggiornamento Hebbian interpretabile che aiuti nella formazione.  Troviamo che il meta-apprendista alla fine degenera in una regola non-hebbiana che conserva i pesi importanti in modo da non disturbare la convergenza dell'apprendista.
Le reti convoluzionali profonde spesso aggiungono termini di costante additiva ("bias") alle loro operazioni di convoluzione, consentendo un repertorio più ricco di mappature funzionali. I bias sono anche usati per facilitare l'addestramento, sottraendo la risposta media su lotti di immagini di addestramento (una componente della "normalizzazione dei lotti"). I recenti metodi di denoising cieco allo stato dell'arte sembrano richiedere questi termini per il loro successo.Qui, tuttavia, dimostriamo che i termini di bias utilizzati nella maggior parte delle CNN (costanti additive, comprese quelle utilizzate per la normalizzazione dei lotti) interferiscono con l'interpretabilità di queste reti, non aiutano le prestazioni, e in effetti impediscono la generalizzazione delle prestazioni a livelli di rumore non inclusi nei dati di formazione. In particolare, le CNN prive di bias (BF-CNNs) sono localmente lineari, e quindi suscettibili di un'analisi diretta con strumenti di algebra lineare. Queste analisi forniscono interpretazioni della funzionalità della rete in termini di proiezione su un'unione di sottospazi bidimensionali, collegando il metodo basato sull'apprendimento alla metodologia di denoising più tradizionale. Inoltre, le BF-CNN generalizzano in modo robusto, raggiungendo prestazioni quasi allo stato dell'arte a livelli di rumore ben oltre l'intervallo in cui sono state addestrate.
La selezione dei valori dei parametri iniziali per l'ottimizzazione basata sul gradiente delle reti neurali profonde è una delle scelte iperparametriche più impattanti nei sistemi di apprendimento profondo, influenzando sia i tempi di convergenza che le prestazioni del modello. Nonostante la significativa analisi empirica e teorica, è stato dimostrato relativamente poco sugli effetti concreti dei diversi schemi di inizializzazione. In questo lavoro, analizziamo l'effetto dell'inizializzazione nelle reti lineari profonde e forniamo per la prima volta una prova rigorosa che disegnare i pesi iniziali dal gruppo ortogonale accelera la convergenza rispetto all'inizializzazione gaussiana standard con pesi iidi. Mostriamo che per le reti profonde, l'ampiezza necessaria per una convergenza efficiente verso un minimo globale con inizializzazioni ortogonali è indipendente dalla profondità, mentre l'ampiezza necessaria per una convergenza efficiente con inizializzazioni gaussiane scala linearmente nella profondità. I nostri risultati dimostrano come i benefici di una buona inizializzazione possono persistere durante l'apprendimento, suggerendo una spiegazione per i recenti successi empirici trovati inizializzando reti non lineari molto profonde secondo il principio di isometria dinamica.
La stima della funzione di sopravvivenza è usata in molte discipline, ma è più comune nell'analitica medica sotto forma di stimatore Kaplan-Meier. I dati sensibili (registri dei pazienti) sono usati nella stima senza alcun controllo esplicito sulla perdita di informazioni, che è una preoccupazione significativa per la privacy. Proponiamo un primo stimatore differentemente privato della funzione di sopravvivenza e mostriamo che può essere facilmente esteso per fornire intervalli di confidenza differentemente privati e statistiche di prova senza spendere alcun budget extra per la privacy.Forniamo inoltre estensioni per la stima differentemente privata della funzione di incidenza cumulativa del rischio concorrente.Utilizzando nove set di dati clinici della vita reale, forniamo prove empiriche che il nostro metodo proposto fornisce una buona utilità e contemporaneamente fornisce forti garanzie di privacy.
Le reti neurali possono imparare ad estrarre proprietà statistiche dai dati, ma raramente fanno uso di informazioni strutturate dallo spazio delle etichette per aiutare l'apprendimento della rappresentazione. Anche se una certa struttura delle etichette può essere implicitamente ottenuta quando ci si allena su enormi quantità di dati, in un contesto di apprendimento a pochi colpi dove sono disponibili pochi dati, fare uso esplicito della struttura delle etichette può informare il modello a rimodellare lo spazio di rappresentazione per riflettere un senso globale delle dipendenze di classe.  Proponiamo una struttura di meta-apprendimento, Conditional class-Aware Meta-Learning (CAML), che trasforma condizionatamente le rappresentazioni delle caratteristiche sulla base di uno spazio metrico addestrato a catturare le dipendenze interclasse, consentendo una modulazione condizionale delle rappresentazioni delle caratteristiche del base-learner per imporre le regolarità informate dallo spazio delle etichette.Gli esperimenti mostrano che la trasformazione condizionale in CAML porta a rappresentazioni più distanziate e raggiunge risultati competitivi sul benchmark miniImageNet.
A differenza dei problemi esistenti nell'apprendimento supervisionato, come la classificazione, la classificazione e la generazione di sequenze, non c'è un ordine noto tra gli elementi in un multiset di destinazione, e ogni elemento nel multiset può apparire più di una volta, rendendo questo problema estremamente impegnativo. La funzione di perdita multiset proposta viene valutata empiricamente su due famiglie di set di dati, uno sintetico e l'altro reale, con vari livelli di difficoltà, contro varie funzioni di perdita di base tra cui l'apprendimento di rinforzo, la sequenza e le funzioni di perdita di corrispondenza della distribuzione aggregata.
Comprendere le proprietà teoriche delle reti profonde e localmente connesse non lineari, come le reti neurali convoluzionali profonde (DCNN), è ancora un problema difficile nonostante il loro successo empirico.In questo articolo, proponiamo un nuovo quadro teorico per tali reti con non linearità ReLU. La struttura collega la distribuzione dei dati con le regole di discesa del gradiente, favorisce le rappresentazioni dissociate ed è compatibile con le comuni tecniche di regolarizzazione come Batch Norm, dopo una nuova scoperta della sua natura di proiezione. La struttura è costruita sull'impostazione insegnante-studente, proiettando il passaggio avanti/indietro dello studente sul grafico computazionale dell'insegnante, La nostra struttura potrebbe aiutare a facilitare l'analisi teorica di molti problemi pratici, per esempio le rappresentazioni dissociate nelle reti profonde.
Molti compiti coinvolgono incentivi individuali che sono disallineati con il bene comune, eppure una vasta gamma di organismi, dai batteri agli insetti e agli esseri umani, sono in grado di superare le loro differenze e collaborare.Pertanto, l'emergere di un comportamento cooperativo tra individui con interessi personali è una questione importante per i campi di apprendimento di rinforzo multi-agente (MARL) e la teoria evolutiva. Qui studiamo una classe particolare di problemi multi-agente chiamati dilemmi sociali intertemporali (ISD), dove il conflitto tra l'individuo e il gruppo è particolarmente acuto. Combinando MARL con la selezione naturale opportunamente strutturata, dimostriamo che i pregiudizi induttivi individuali per la cooperazione possono essere appresi in un modo privo di modelli.Per ottenere questo, introduciamo un'architettura modulare innovativa per agenti di apprendimento di rinforzo profondo che supporta la selezione multilivello.
Negli attacchi avversari ai classificatori di apprendimento automatico, piccole perturbazioni sono aggiunte all'input che è classificato correttamente. le perturbazioni producono esempi avversari, che sono virtualmente indistinguibili dall'input non perturbato, e tuttavia sono classificati male. nelle reti neurali standard utilizzate per l'apprendimento profondo, gli attaccanti possono creare esempi avversari dalla maggior parte dell'input per causare un errore di classificazione di loro scelta. Introduciamo un nuovo tipo di unità di rete, chiamate unità RBFI, la cui struttura non lineare le rende intrinsecamente resistenti agli attacchi avversari. Su MNIST invariante per permutazione, in assenza di attacchi avversari, le reti che utilizzano unità RBFI eguagliano le prestazioni delle reti che utilizzano unità sigmoidi e sono leggermente al di sotto della precisione delle reti con unità ReLU. Quando sono sottoposte ad attacchi avversari basati su metodi di discesa del gradiente proiettato o di segno del gradiente veloce, le reti con unità RBFI mantengono accuratezze superiori al 75%, mentre ReLU o Sigmoid vedono le loro accuratezze ridotte a meno dell'1%. Inoltre, le reti RBFI addestrate su input regolari superano o eguagliano da vicino l'accuratezza delle reti sigmoid e ReLU addestrate con l'aiuto di esempi avversari.La struttura non lineare delle unità RBFI le rende difficili da addestrare usando la discesa del gradiente standard.Mostriamo che le reti RBFI di unità RBFI possono essere addestrate in modo efficiente ad alte precisioni usando pseudogradienti, calcolati usando funzioni appositamente create per facilitare l'apprendimento invece delle loro vere derivate.
Mostriamo che esiste una tensione intrinseca tra l'obiettivo della robustezza avversaria e quello della generalizzazione standard. In particolare, l'addestramento di modelli robusti può non solo essere più dispendioso in termini di risorse, ma anche portare a una riduzione dell'accuratezza standard. dimostriamo che questo trade-off tra l'accuratezza standard di un modello e la sua robustezza alle perturbazioni avversarie esiste in modo dimostrabile anche in un'impostazione abbastanza semplice e naturale, e questi risultati confermano un fenomeno simile osservato nella pratica. Inoltre, sosteniamo che questo fenomeno è una conseguenza dei classificatori robusti che imparano rappresentazioni di caratteristiche fondamentalmente diverse dai classificatori standard. Queste differenze, in particolare, sembrano portare a benefici inaspettati: le caratteristiche apprese dai modelli robusti tendono ad allinearsi meglio con le caratteristiche salienti dei dati e la percezione umana.
Le grandi reti neurali profonde sono potenti, ma presentano comportamenti indesiderati come la memorizzazione e la sensibilità agli esempi avversari. In questo lavoro, proponiamo mixup, un semplice principio di apprendimento per alleviare questi problemi.  Così facendo, mixup regolarizza la rete neurale per favorire un semplice comportamento lineare tra gli esempi di allenamento.  I nostri esperimenti sui dataset ImageNet-2012, CIFAR-10, CIFAR-100, Google commands e UCI mostrano che mixup migliora la generalizzazione delle architetture di rete neurale allo stato dell'arte.  Troviamo anche che il mixup riduce la memorizzazione delle etichette corrotte, aumenta la robustezza agli esempi avversari e stabilizza l'addestramento delle reti generative avversarie.
Presentiamo un nuovo approccio all'ordinamento dei picchi per le sonde multielettrodo ad alta densità utilizzando il processo di clustering neurale (NCP), un'architettura neurale introdotta di recente che esegue un'inferenza bayesiana approssimativa ammortizzata scalabile per un clustering probabilistico efficiente.per codificare in modo ottimale le forme d'onda dei picchi per il clustering, abbiamo esteso NCP aggiungendo un codificatore di picchi convoluzionale, che viene appreso end-to-end con la rete NCP. Addestrato puramente su picchi sintetici etichettati da un semplice modello generativo, il modello di ordinamento dei picchi NCP mostra prestazioni promettenti per il clustering delle forme d'onda dei picchi multicanale. Il modello fornisce una qualità di clustering superiore rispetto a un algoritmo bayesiano alternativo, trova più modelli di spike con campi recettivi chiari sui dati reali e recupera più neuroni di verità su dati di prova ibridi rispetto a un recente algoritmo di ordinamento di spike.Inoltre, NCP è in grado di gestire l'incertezza di clustering di piccoli spike ambigui tramite il campionamento posteriore paralellizzato da GPU.
L'obiettivo dell'articolo è quello di proporre un algoritmo per l'apprendimento della soluzione più generalizzabile da dati di addestramento dati. Si dimostra che l'approccio bayesiano porta a una soluzione che dipende dalle statistiche dei dati di addestramento e non da campioni particolari. La soluzione è stabile sotto perturbazioni dei dati di addestramento perché è definita da un contributo integrale di più massimi della probabilità e non da un singolo massimo globale. In particolare, la distribuzione di probabilità bayesiana dei parametri (pesi) di un modello probabilistico dato da una rete neurale è stimata tramite approssimazioni variazionali ricorrenti. Le regole di aggiornamento ricorrenti derivate corrispondono a regole di tipo SGD per trovare un minimo di una perdita effettiva che è una media di una log-likelihood negativa originale sulle distribuzioni gaussiane dei pesi, che la rende una funzione di mezzi e varianze. La perdita effettiva è convessa per grandi varianze e non convessa nel limite di piccole varianze. Tra le soluzioni stazionarie delle regole di aggiornamento ci sono soluzioni banali con varianze zero ai minimi locali della perdita originale e una singola soluzione non banale con varianze finite che è un punto critico alla fine della convessità della perdita effettiva nello spazio media-varianza. Lo studio empirico conferma che il punto critico rappresenta la soluzione più generalizzabile, mentre la posizione del punto critico nello spazio dei pesi dipende dalle specifiche del modello probabilistico usato, alcune proprietà del punto critico sono universali e indipendenti dal modello.
Gli esseri umani si affidano costantemente alla memoria episodica, nel ricordare il nome di qualcuno che hanno incontrato 10 minuti fa, la trama di un film mentre si svolge, o dove hanno parcheggiato l'auto.dotando gli agenti di apprendimento per rinforzo di memoria episodica è un passo fondamentale sulla strada verso la replica di un'intelligenza generale simile a quella umana.analizziamo perché gli agenti RL standard mancano di memoria episodica oggi, e perché i compiti RL esistenti non la richiedono. Per valutare la memoria episodica, definiamo un compito di RL basato sul comune gioco per bambini della Concentrazione e scopriamo che un agente RL MEM sfrutta efficacemente la memoria episodica per padroneggiare la Concentrazione, a differenza degli agenti di base che abbiamo testato.
Piuttosto che reimparare i parametri da zero, sostituendo l'apprendimento con l'ottimizzazione, proponiamo una struttura basata sulla teoria del "trasporto ottimale" per adattare i parametri del modello scoprendo le corrispondenze tra i modelli e i dati, ammortizzando significativamente il costo di formazione. Anche se le recenti tecniche di mappatura hanno facilitato una robusta mappatura dell'occupazione, l'apprendimento di tutti i parametri spazialmente diversi in tali modelli bayesiani approssimativi richiede un tempo computazionale considerevole, scoraggiando il loro utilizzo nella mappatura robotica del mondo reale.Considerando il fatto che le caratteristiche geometriche che un robot osserva con i suoi sensori sono simili in vari ambienti, in questo articolo, dimostriamo come riutilizzare i parametri e gli iperparametri appresi in diversi domini. Questo adattamento è computazionalmente più efficiente dell'inferenza variazionale e delle tecniche Monte Carlo. Una serie di esperimenti condotti su impostazioni realistiche ha verificato la possibilità di trasferire migliaia di tali parametri con un tempo trascurabile e un costo di memoria, consentendo una mappatura su larga scala in ambienti urbani.
Viene introdotto un algoritmo per l'apprendimento di una rappresentazione predittiva dello stato con l'apprendimento della differenza temporale (TD) off-policy che viene poi utilizzato per imparare a guidare un veicolo con l'apprendimento di rinforzo.  Ci sono tre componenti che vengono apprese simultaneamente:  (1) le previsioni off-policy come rappresentazione compatta dello stato, (2) la distribuzione della politica di comportamento per stimare le previsioni off-policy, e (3) il gradiente deterministico della politica per imparare ad agire.  Un discriminatore della politica di comportamento viene appreso e utilizzato per stimare i rapporti di campionamento importanti necessari per apprendere la rappresentazione predittiva off-policy con funzioni di valore generali (GVF).  Un metodo deterministico lineare del gradiente della politica è usato per addestrare l'agente con solo le rappresentazioni predittive mentre le previsioni vengono apprese.  Tutti e tre i componenti sono combinati, dimostrati e valutati sul problema della guida del veicolo da immagini nell'ambiente del simulatore di corse TORCS. La guida da sole immagini è un problema impegnativo in cui la valutazione è completata su un set di piste mai viste durante l'addestramento per misurare la generalizzazione delle previsioni e del controller.  Gli esperimenti dimostrano che il metodo proposto è in grado di sterzare senza problemi e di navigare su molte ma non tutte le piste disponibili in TORCS con prestazioni che superano DDPG usando solo immagini come input e si avvicinano alle prestazioni di un modello ideale di cinematica non basato sulla visione.
Per numerosi domini, tra cui ad esempio l'osservazione della terra, l'imaging medico, l'astrofisica, ..., i set di dati di immagini e segnali disponibili spesso hanno modelli di campionamento spazio-temporale irregolari e grandi tassi di dati mancanti.Queste proprietà di campionamento è un problema critico per applicare lo stato dell'arte basato sull'apprendimento (ad es, In questo articolo, ci occupiamo dell'apprendimento end-to-end di rappresentazioni di segnali, immagini e sequenze di immagini da dati a campionamento irregolare, cioè quando i dati di addestramento coinvolgono dati mancanti.Da un'analogia alla formulazione bayesiana, consideriamo rappresentazioni basate sull'energia. La fase di apprendimento di queste rappresentazioni basate sull'energia (o priori) coinvolge un problema di interpolazione congiunta, che ricorre alla risoluzione di un problema di minimizzazione dell'energia sotto vincoli di osservazione.Utilizzando un'implementazione basata su reti neurali delle forme di energia considerate, possiamo affermare uno schema di apprendimento end-to-end da dati a campionamento irregolare.Dimostriamo la rilevanza delle rappresentazioni proposte per diversi casi-studio: in particolare, serie temporali multivariate, immagini 2{\sc } e sequenze di immagini.
L'assegnazione dei crediti nel Meta-reinforcement learning (Meta-RL) è ancora poco compresa.I metodi esistenti o trascurano l'assegnazione dei crediti al comportamento di pre-adattamento o lo implementano ingenuamente.Questo porta a una scarsa efficienza del campione durante il meta-training così come a strategie inefficaci di identificazione dei compiti.Questo articolo fornisce un'analisi teorica dell'assegnazione dei crediti nel Meta-RL basato sul gradiente. Controllando la distanza statistica di entrambe le politiche di pre-adattamento e adattate durante la ricerca della meta-politica, l'algoritmo proposto offre un meta-apprendimento efficiente e stabile. Il nostro approccio porta a un comportamento superiore della politica di pre-adattamento e supera costantemente i precedenti algoritmi Meta-RL in efficienza del campione, tempo di wall-clock e performance asintotiche.
Quando si addestra una rete neurale per un compito desiderato, si può preferire adattare una rete preaddestrata piuttosto che iniziare con una inizializzata in modo casuale - a causa della mancanza di dati di formazione sufficienti, eseguendo l'apprendimento permanente in cui il sistema deve imparare un nuovo compito mentre è stato precedentemente addestrato per altri compiti, o desiderando codificare i priori nella rete tramite pesi preimpostati. In questo articolo proponiamo un'alternativa diretta: Il Side-Tuning adatta una rete preaddestrata addestrando una rete leggera "laterale" che viene fusa con la rete preaddestrata (invariata) utilizzando un semplice processo additivo. Questo semplice metodo funziona bene o meglio delle soluzioni esistenti, mentre risolve alcuni dei problemi di base con il fine-tuning, le caratteristiche fisse, e molte altre basi comuni. In particolare, il side-tuning è meno incline all'overfitting quando sono disponibili pochi dati di formazione, produce risultati migliori rispetto all'uso di un estrattore di caratteristiche fisso e non soffre di dimenticanze catastrofiche nell'apprendimento permanente.  Dimostriamo le prestazioni del side-tuning in una serie di scenari diversi, tra cui l'apprendimento permanente (iCIFAR, Taskonomy), l'apprendimento per rinforzo, l'apprendimento per imitazione (navigazione visiva in Habitat), l'NLP question-answering (SQuAD v2), e l'apprendimento per trasferimento a singolo compito (Taskonomy), con risultati sempre promettenti.
In questo articolo, presentiamo una tecnica per la generazione di set di dati artificiali che mantengono le proprietà statistiche dei dati reali, fornendo garanzie di privacy differenziale rispetto a questi dati. Includiamo uno strato di rumore gaussiano nel discriminatore di una rete generativa avversaria per rendere l'uscita e i gradienti differentemente privati rispetto ai dati di formazione, e quindi utilizziamo il componente generatore per sintetizzare set di dati artificiali che preservano la privacy.
Questo articolo presenta due metodi per distinguere e interpretare gli effetti contestuali che sono codificati in una rete neurale profonda pre-addestrata. A differenza degli studi convoluzionali che visualizzano le apparenze delle immagini corrispondenti all'uscita della rete o un'attivazione neurale da una prospettiva globale, la nostra ricerca mira a chiarire come una certa unità di input (dimensione) collabora con altre unità (dimensioni) per costituire i modelli di inferenza della rete neurale e quindi contribuire all'uscita della rete. L'analisi degli effetti contestuali locali con certe unità di input ha un valore speciale nelle applicazioni reali. In particolare, abbiamo usato i nostri metodi per spiegare la strategia di gioco del modello alphaGo Zero negli esperimenti, e il nostro metodo ha distinto con successo la logica di ogni mossa durante il gioco.
L'obiettivo principale della potatura della rete è imporre la sparsità alla rete neurale aumentando il numero di parametri con valore zero al fine di ridurre la dimensione dell'architettura e la velocità di calcolo.
La maggior parte dei lavori precedenti fornisce garanzie di recupero dei parametri per reti a uno strato nascosto, tuttavia le reti utilizzate in pratica hanno più strati non lineari. In questo lavoro, mostriamo come possiamo rafforzare tali risultati a reti più profonde - affrontiamo il problema di scoprire lo strato più basso in una rete neurale profonda assumendo che lo strato più basso utilizzi una soglia elevata prima di applicare l'attivazione, la rete superiore possa essere modellata come un polinomio ben tollerato e la distribuzione degli input sia gaussiana.
L'apprendimento federato implica l'addestramento e la combinazione efficace di modelli di apprendimento automatico da partizioni distribuite di dati (cioè, attività) su dispositivi di bordo, ed è naturalmente visto come un problema di apprendimento multi-task.Mentre Federated Averaging (FedAvg) è il principale metodo di ottimizzazione per l'addestramento di modelli non convessi in questa impostazione, il suo comportamento non è ben compreso nelle impostazioni federate realistiche quando i dispositivi/attività sono statisticamente eterogenei, cioè, In questo lavoro, introduciamo una struttura, chiamata FedProx, per affrontare l'eterogeneità statistica.FedProx comprende FedAvg come caso speciale.Forniamo garanzie di convergenza per FedProx attraverso un presupposto di dissimilarità del dispositivo.La nostra valutazione empirica convalida la nostra analisi teorica e dimostra la migliore robustezza e stabilità di FedProx per l'apprendimento in reti eterogenee.
I giochi referenziali offrono un ambiente di apprendimento fondato per gli agenti neurali che tiene conto del fatto che il linguaggio è usato funzionalmente per comunicare.Tuttavia, non prendono in considerazione un secondo vincolo considerato fondamentale per la forma del linguaggio umano: che deve essere imparabile da nuovi apprendenti del linguaggio e quindi deve superare un collo di bottiglia di trasmissione.In questo lavoro, inseriamo un tale collo di bottiglia in un gioco referenziale, introducendo una popolazione mutevole di agenti in cui i nuovi agenti imparano giocando con agenti più esperti. Mostriamo che la mera trasmissione culturale porta a un sostanziale miglioramento dell'efficienza del linguaggio e del successo comunicativo, misurato in velocità di convergenza, grado di struttura nelle lingue emerse e consistenza all'interno della popolazione del linguaggio.Tuttavia, come nostro contributo principale, mostriamo che la situazione ottimale è quella di co-evolvere il linguaggio e gli agenti.Quando permettiamo alla popolazione di agenti di evolvere attraverso l'evoluzione genotipica, otteniamo miglioramenti su tutta la linea su tutte le metriche considerate.Questi risultati sottolineano che per gli studi sull'emergenza del linguaggio l'evoluzione culturale è importante, ma anche l'idoneità dell'architettura stessa dovrebbe essere considerata.
La necessità di grandi quantità di dati di formazione delle immagini con caratteristiche chiaramente definite è un grande ostacolo all'applicazione delle reti generative avversarie (GAN) sulla generazione di immagini dove i dati di formazione sono limitati ma diversi, poiché l'insufficiente rappresentazione delle caratteristiche latenti nei dati già scarsi spesso porta all'instabilità e al collasso della modalità durante la formazione GAN. Per superare l'ostacolo dei dati limitati quando si applicano le GAN ad insiemi di dati limitati, proponiamo in questo articolo la strategia di \textit{parallel recurrent data augmentation}, in cui il modello GAN arricchisce progressivamente il suo set di allenamento con immagini campione costruite da GAN addestrate in parallelo a epoche di allenamento consecutive. Gli esperimenti su una varietà di insiemi di dati piccoli ma diversi dimostrano che il nostro metodo, con poche considerazioni specifiche del modello, produce immagini di qualità migliore rispetto alle immagini generate senza tale strategia.
Sviluppiamo un simulatore stocastico dell'intero cervello e del corpo del nematode Caenorhabditis elegans (C. elegans) e dimostriamo che è sufficientemente regolarizzante da permettere l'imputazione di potenziali di membrana latenti da osservazioni parziali di fluorescenza del calcio. Questo è il primo tentativo che conosciamo di ``completare il cerchio'', dove un simulatore anatomico di whole-connectome viene utilizzato per imputare uno stato di ``cervello'' variabile nel tempo alla fedeltà della singola cellula da covariati che sono misurabili in pratica.  Utilizzando lo stato dell'arte dei metodi di apprendimento automatico bayesiano per condizionare i dati facilmente ottenibili, il nostro metodo apre la strada ai neuroscienziati per recuperare rappresentazioni di stato interpretabili a livello di connettoma, stimare automaticamente i valori dei parametri fisiologicamente rilevanti dai dati ed eseguire simulazioni che indagano forme di vita intelligenti in silico.
Le incorporazioni dei nodi di alta qualità apprese dalle reti neurali grafiche (GNN) sono state applicate a una vasta gamma di applicazioni basate sui nodi e alcune di esse hanno raggiunto prestazioni all'avanguardia (SOTA). Tuttavia, quando si applicano le incorporazioni dei nodi apprese dalle GNN per generare incorporazioni dei grafici, la rappresentazione scalare dei nodi potrebbe non essere sufficiente a preservare le proprietà dei nodi/grafi in modo efficiente, con conseguenti incorporazioni dei grafici subottimali. Ispirandoci alla Capsule Neural Network (CapsNet), proponiamo la Capsule Graph Neural Network (CapsGNN), che adotta il concetto di capsule per affrontare la debolezza degli attuali algoritmi di embeddings di grafi basati su GNN. Estraendo le caratteristiche dei nodi sotto forma di capsule, il meccanismo di routing può essere utilizzato per catturare informazioni importanti a livello di grafo.Di conseguenza, il nostro modello genera più embeddings per ogni grafico per catturare le proprietà del grafico da diversi aspetti.Il modulo di attenzione incorporato in CapsGNN viene utilizzato per affrontare grafi di varie dimensioni che permette anche al modello di concentrarsi su parti critiche dei grafi. Le nostre estese valutazioni con 10 set di dati strutturati a grafo dimostrano che CapsGNN ha un potente meccanismo che opera per catturare le proprietà macroscopiche dell'intero grafo tramite data-driven.Supera le altre tecniche SOTA su diversi compiti di classificazione dei grafi, in virtù del nuovo strumento.
Introduciamo una nuova struttura per modelli generativi basati su Restricted Kernel Machines (RKMs) con generazione multi-vista e capacità di apprendimento di caratteristiche non correlate, chiamata Gen-RKM.Per incorporare la generazione multi-vista, questo meccanismo utilizza una rappresentazione condivisa dei dati da varie viste.Il meccanismo è flessibile per incorporare sia modelli basati su kernel, rete neurale (profonda) che modelli basati su convoluzione all'interno della stessa impostazione.Per aggiornare i parametri della rete, proponiamo una nuova procedura di formazione che apprende congiuntamente le caratteristiche e la rappresentazione condivisa.Gli esperimenti dimostrano il potenziale della struttura attraverso una valutazione qualitativa dei campioni generati.
Il lavoro sul problema della rappresentazione contestualizzata delle parole - lo sviluppo di componenti di reti neurali riutilizzabili per la comprensione delle frasi - ha recentemente visto un'ondata di progressi incentrati sul compito di preformazione non supervisionata della modellazione linguistica con metodi come ELMo (Peters et al, I risultati primari dello studio supportano l'uso della modellazione linguistica come compito di preformazione e stabiliscono un nuovo stato dell'arte tra i modelli comparabili utilizzando l'apprendimento multitask con modelli linguistici. Tuttavia, uno sguardo più attento a questi risultati rivela linee di base preoccupantemente forti e risultati sorprendentemente variati tra i compiti target, suggerendo che il paradigma ampiamente utilizzato di preformazione e congelamento dei codificatori di frasi potrebbe non essere una piattaforma ideale per ulteriori lavori.
In questo articolo, studiamo il problema dell'attacco e della difesa avversaria nell'apprendimento profondo dal punto di vista dell'analisi di Fourier. Prima calcoliamo esplicitamente la trasformata di Fourier delle reti neurali ReLU profonde e dimostriamo che esistono componenti ad alta frequenza decadenti ma non nulle nello spettro di Fourier delle reti neurali. Dimostriamo poi che la vulnerabilità delle reti neurali nei confronti dei campioni avversari può essere attribuita a queste componenti ad alta frequenza insignificanti ma non nulle. Sulla base di questa analisi, proponiamo di utilizzare una semplice tecnica di post-averaging per smussare queste componenti ad alta frequenza per migliorare la robustezza delle reti neurali contro gli attacchi avversari. I risultati sperimentali sui dataset ImageNet e CIFAR-10 hanno dimostrato che il nostro metodo proposto è universalmente efficace per difendere molti metodi di attacco avversari esistenti proposti in letteratura, tra cui FGSM, PGD, DeepFool e gli attacchi C&W. Il nostro metodo di post-averaging è semplice poiché non richiede alcun re-training, e nel frattempo può difendere con successo oltre l'80-96% dei campioni avversari generati da questi metodi senza introdurre una degradazione significativa delle prestazioni (meno del 2%) sulle immagini originali pulite.
I metodi di apprendimento per rinforzo che apprendono continuamente le reti neurali attraverso la generazione di episodi con la ricerca di alberi di gioco hanno avuto successo nei giochi deterministici a due persone con informazioni complete come gli scacchi, lo shogi e il Go, ma ci sono solo rapporti di casi pratici e ci sono poche prove per garantire la stabilità e le prestazioni finali del processo di apprendimento. In questa ricerca ci si è concentrati sulla coordinazione della generazione dell'episodio, e considerando l'intero sistema come una ricerca ad albero del gioco, il nuovo metodo può gestire il trade-off tra sfruttamento ed esplorazione durante la generazione dell'episodio, e gli esperimenti con un piccolo problema hanno dimostrato che il metodo ha prestazioni robuste rispetto al metodo esistente, Alpha Zero.
Le reti neurali message-passing (MPNNs) sono state applicate con successo in un'ampia varietà di applicazioni nel mondo reale.Tuttavia, due debolezze fondamentali degli aggregatori MPNNs limitano la loro capacità di rappresentare i dati strutturati a grafo: perdere le informazioni strutturali dei nodi nelle vicinanze e mancare della capacità di catturare le dipendenze a lungo raggio nei grafi disassortativi. Dalle osservazioni sulla rete neurale classica e sulla geometria della rete, proponiamo un nuovo schema di aggregazione geometrica per le reti neurali a grafo per superare le due debolezze.  Lo schema di aggregazione proposto è invariante alle permutazioni e consiste di tre moduli, l'incorporazione dei nodi, la vicinanza strutturale e l'aggregazione a due livelli. Presentiamo anche un'implementazione dello schema nelle reti convoluzionali a grafo, denominata Geom-GCN, per eseguire l'apprendimento trasduttivo sui grafi.
Consideriamo il compito di sintesi del programma in presenza di una funzione di ricompensa sull'output dei programmi, dove l'obiettivo è quello di trovare programmi con ricompense massime.Introduciamo un nuovo schema di ottimizzazione iterativo, dove addestriamo una RNN su un dataset di K programmi migliori da una coda di priorità dei programmi generati finora.Poi, sintetizziamo nuovi programmi e li aggiungiamo alla coda di priorità campionando dalla RNN. I nostri risultati sperimentali mostrano che il nostro algoritmo, chiamato priority queue training (PQT), si confronta con l'algoritmo genetico e il reinforcement learning baseline su un semplice ma espressivo linguaggio di programmazione completo di Turing chiamato BF.I nostri risultati sperimentali mostrano che il nostro algoritmo PQT, ingannevolmente semplice, supera significativamente i baseline.Aggiungendo una penalità per la lunghezza del programma alla funzione di ricompensa, siamo in grado di sintetizzare programmi brevi e leggibili dall'uomo.
Presentiamo la rete neurale di wavelet del grafico (GWNN), una rete neurale convoluzionaria del grafico novello (CNN), facendo leva sulla trasformazione di wavelet del grafico per affrontare le carenze dei metodi precedenti di CNN del grafico spettrale che dipendono dalla trasformazione di Fourier del grafico.differente dalla trasformazione di Fourier del grafico, la trasformazione di wavelet del grafico può essere ottenuta via un algoritmo veloce senza richiedere la eigendecomposizione della matrice con alto costo computazionale. Inoltre, le wavelet del grafico sono sparse e localizzate nel dominio dei vertici, offrendo alta efficienza e buona interpretabilità per la convoluzione del grafico.Il GWNN proposto supera significativamente i precedenti CNN del grafico spettrale nel compito di classificazione semi-supervisionata basata sul grafico su tre set di dati di riferimento: Cora, Citeseer e Pubmed.
Il decadimento del tasso di apprendimento (lrDecay) è una tecnica di fatto per l'addestramento delle reti neurali moderne, che inizia con un grande tasso di apprendimento e poi lo fa decadere più volte, ed è stato osservato empiricamente per aiutare sia l'ottimizzazione che la generalizzazione. Le credenze comuni su come funziona lrDecay provengono dall'analisi dell'ottimizzazione di (Stochastic) Gradient Descent:1) un tasso di apprendimento inizialmente grande accelera l'addestramento o aiuta la rete a sfuggire a minimi locali spuri; 2) il decadimento del tasso di apprendimento aiuta la rete a convergere verso un minimo locale e ad evitare l'oscillazione.Nonostante la popolarità di queste credenze comuni, gli esperimenti suggeriscono che sono insufficienti a spiegare l'efficacia generale di lrDecay nell'addestramento delle reti neurali moderne che sono profonde, ampie e non convesse. La spiegazione proposta viene convalidata su un set di dati accuratamente costruito con una complessità di pattern trattabile e la sua implicazione, ovvero che i pattern aggiuntivi appresi nelle fasi successive di lrDecay sono più complessi e quindi meno trasferibili, è giustificata nei set di dati del mondo reale.
Mostriamo come un insieme di funzioni $Q^*$ possa essere sfruttato per un'esplorazione più efficace nell'apprendimento di rinforzo profondo. Ci basiamo su algoritmi consolidati dall'impostazione bandit, e li adattiamo all'impostazione $Q$-learning.Proponiamo una strategia di esplorazione basata su limiti di confidenza superiori (UCB).I nostri esperimenti mostrano guadagni significativi sul benchmark Atari.
Ci concentriamo sul problema dell'apprendimento di un singolo modulo motorio che possa esprimere in modo flessibile una serie di comportamenti per il controllo di umanoidi ad alta densità fisicamente simulati.Per fare questo, proponiamo un'architettura motoria che ha la struttura generale di un modello inverso con un collo di bottiglia latente-variabile. Dimostriamo che è possibile addestrare questo modello interamente offline per comprimere migliaia di politiche esperte e imparare uno spazio di incorporazione delle primitive motorie. Il sistema neurale probabilistico addestrato delle primitive motorie può eseguire l'imitazione one-shot dei comportamenti umanoidi del corpo intero, imitando in modo robusto le traiettorie non viste. Inoltre, dimostriamo che è anche semplice addestrare i controllori a riutilizzare lo spazio delle primitive motorie apprese per risolvere i compiti, e i movimenti risultanti sono relativamente naturalistici.Per supportare l'addestramento del nostro modello, confrontiamo due approcci per la clonazione offline delle politiche, compreso un metodo efficiente per l'esperienza che chiamiamo clonazione lineare delle politiche di feedback.Incoraggiamo i lettori a visualizzare un video supplementare (https://youtu.be/CaDEf-QcKwA ) che riassume i nostri risultati.
L'aumento dei dati è una tecnica utile per ampliare la dimensione del set di formazione e prevenire l'overfitting per diversi compiti di apprendimento automatico quando i dati di formazione sono scarsi.Tuttavia, le attuali tecniche di aumento dei dati si basano pesantemente sulla progettazione umana e la conoscenza del dominio, e gli approcci automatici esistenti devono ancora sfruttare appieno le caratteristiche latenti nel set di dati di formazione. In questo articolo proponiamo \Parallel Adaptive GAN Data Augmentation}(PAGANDA), dove il set di allenamento si arricchisce in modo adattivo con immagini campione costruite automaticamente da Generative Adversarial Networks (GANs) addestrate in parallelo. Dimostriamo con gli esperimenti che la nostra strategia di aumento dei dati, con poche considerazioni specifiche del modello, può essere facilmente adattata a compiti di apprendimento profondo/macchina di apprendimento cross-domain come la classificazione delle immagini e l'inpainting delle immagini, migliorando significativamente le prestazioni del modello in entrambi i compiti.Il nostro codice sorgente e i dettagli sperimentali sono disponibili su https://github.com/miaojiang1987/k-folder-data-augmentation-gan/.
Per mitigare il problema, proponiamo un nuovo metodo di regolarizzazione che penalizza la distribuzione predittiva tra campioni simili. In particolare, distilliamo la distribuzione predittiva tra diversi campioni della stessa etichetta e campioni aumentati della stessa fonte durante l'addestramento. In altre parole, regolarizziamo la conoscenza oscura (cioè, la conoscenza sulle previsioni sbagliate) di una singola rete, cioè, una tecnica di distillazione della conoscenza personale, per costringerla a produrre previsioni più significative.  Dimostriamo l'efficacia del metodo proposto tramite esperimenti su vari compiti di classificazione delle immagini: migliora non solo la capacità di generalizzazione, ma anche la precisione di calibrazione delle moderne reti neurali.
In questo articolo, consideriamo il problema specifico della modellazione linguistica a livello di parola e indaghiamo le strategie per regolarizzare e ottimizzare i modelli basati su LSTM. Proponiamo il LSTM a peso caduto, che usa DropConnect sui pesi nascosti, come forma di regolarizzazione ricorrente. Inoltre, introduciamo NT-ASGD, una variante non monotonamente innescata (NT) del metodo del gradiente stocastico mediato (ASGD), in cui l'innesco della media è determinato utilizzando una condizione NT invece di essere sintonizzato dall'utente.Utilizzando queste e altre strategie di regolarizzazione, il nostro ASGD Weight-Dropped LSTM (AWD-LSTM) raggiunge lo stato dell'arte perplessità a livello di parola su due set di dati: 57. 3 su Penn Treebank e 65.8 su WikiText-2. Nell'esplorare l'efficacia di una cache neurale insieme al nostro modello proposto, raggiungiamo una perplessità ancora più bassa dello stato dell'arte di 52.8 su Penn Treebank e 52.0 su WikiText-2. Esploriamo anche la fattibilità delle strategie di regolarizzazione e ottimizzazione proposte nel contesto della rete neurale quasi-recorrente (QRNN) e dimostriamo prestazioni comparabili alla controparte AWD-LSTM. Il codice per riprodurre i risultati è open sourced ed è disponibile su https://github.com/salesforce/awd-lstm-lm.
Vari metodi di misurazione della selettività delle unità sono stati sviluppati con l'obiettivo di comprendere meglio il funzionamento delle reti neurali.  Ma le diverse misure forniscono stime divergenti di selettività, e questo ha portato a conclusioni diverse per quanto riguarda le condizioni in cui le rappresentazioni selettive dell'oggetto vengono apprese e la rilevanza funzionale di queste rappresentazioni.Nel tentativo di caratterizzare meglio la selettività dell'oggetto, intraprendiamo un confronto di varie misure di selettività su un ampio set di unità in AlexNet, tra cui la selettività localista, la precisione, la selettività di attività media condizionata dalla classe (CCMAS), la dissezione della rete, l'interpretazione umana delle immagini di massimizzazione dell'attivazione (AM), e misure standard di rilevamento del segnale.  Troviamo che le diverse misure forniscono diverse stime di selettività dell'oggetto, con precisione e misure CCMAS che forniscono stime ingannevolmente alte. Infatti, le unità più selettive avevano un povero hit-rate o un alto tasso di falso allarme (o entrambi) nella classificazione degli oggetti, rendendoli poveri rilevatori di oggetti.  Per generalizzare questi risultati, abbiamo confrontato le misure di selettività su alcune unità in VGG-16 e GoogLeNet addestrate sui dataset ImageNet o Places-365 che sono stati descritti come "rilevatori di oggetti".
La struttura di ripiegamento della molecola di DNA combinata con molecole aiutanti, chiamata anche cromatina, è molto importante per le proprietà funzionali del DNA. La struttura della cromatina è in gran parte determinata dalla sottostante sequenza primaria del DNA, anche se l'interazione non è ancora completamente compresa. In questo articolo sviluppiamo una rete neurale convoluzionale che prende una rappresentazione per immagini della sequenza primaria del DNA come input, e predice i determinanti chiave della struttura della cromatina. Il metodo è sviluppato in modo tale che sia in grado di rilevare le interazioni tra elementi distali nella sequenza del DNA, che sono noti per essere altamente rilevanti.
L'intuizione chiave è che, oltre alle caratteristiche, possiamo trasferire informazioni di somiglianza e questo è sufficiente per imparare una funzione di somiglianza e una rete di clustering per eseguire sia l'adattamento al dominio che l'apprendimento di trasferimento tra compiti. Questa somiglianza è indipendente dalla categoria e può essere appresa dai dati nel dominio di origine usando una rete di somiglianza. Presentiamo poi due nuovi approcci per eseguire l'apprendimento di trasferimento usando questa funzione di somiglianza, Poiché la rete di somiglianza è rumorosa, la chiave è utilizzare un algoritmo di clustering robusto, e noi dimostriamo che la nostra formulazione è più robusta degli approcci alternativi di clustering vincolato e non vincolato. Utilizzando questo metodo, mostriamo prima lo stato dell'arte dei risultati per l'impegnativo problema del cross-task, applicato su Omniglot e ImageNet.I nostri risultati mostrano che possiamo ricostruire i cluster semantici con alta accuratezza.Valutiamo poi le prestazioni del trasferimento cross-domain utilizzando immagini dai task Office-31 e SVHN-MNIST e presentiamo la massima accuratezza su entrambi i dataset.  Il nostro approccio non si occupa esplicitamente della discrepanza dei domini e se lo combiniamo con una perdita di adattamento al dominio, mostra un ulteriore miglioramento.
I recenti progressi nel cross-lingual word embeddings si sono basati principalmente su metodi basati sulla mappatura, che proiettano le embeddings di parole preaddestrate da lingue diverse in uno spazio condiviso attraverso una trasformazione lineare, Questo motiva lo studio di metodi di apprendimento congiunto che possono superare questo impedimento, imparando simultaneamente le incorporazioni tra le lingue attraverso un termine interlinguistico nell'obiettivo di formazione. Data l'abbondanza di dati paralleli disponibili (Tiedemann, 2012), proponiamo un'estensione bilingue del metodo CBOW che sfrutta i corpora allineati alle frasi per ottenere robuste rappresentazioni interlinguistiche di parole e frasi. Il nostro approccio migliora significativamente le prestazioni di recupero delle frasi multilingue rispetto a tutti gli altri approcci, e supera in modo convincente i metodi di mappatura, mantenendo la parità con i metodi addestrati congiuntamente sulla traduzione delle parole; inoltre, raggiunge la parità con un metodo RNN profondo su un compito di classificazione di documenti multilingue a zero colpi, richiedendo molte meno risorse computazionali per l'addestramento e l'inferenza; come ulteriore vantaggio, il nostro metodo bilingue migliora anche la qualità dei vettori di parole monolingue, nonostante l'addestramento su dataset molto più piccoli.  Rendiamo il nostro codice e i nostri modelli disponibili pubblicamente.
Per agire e pianificare in ambienti complessi, sosteniamo che gli agenti dovrebbero avere un simulatore mentale del mondo con tre caratteristiche:(a) dovrebbe costruire uno stato astratto che rappresenti la condizione del mondo;(b) dovrebbe formare una credenza che rappresenti l'incertezza sul mondo;(c) dovrebbe andare oltre la semplice simulazione passo dopo passo, e mostrare un'astrazione temporale. Motivati dall'assenza di un modello che soddisfi tutti questi requisiti, proponiamo TD-VAE, un modello generativo di sequenza che apprende rappresentazioni contenenti credenze esplicite su stati a diversi passi nel futuro, e che può essere srotolato direttamente senza transizioni a passo singolo.TD-VAE è addestrato su coppie di punti temporali separati, usando un analogo dell'apprendimento per differenza temporale usato nell'apprendimento di rinforzo.
Questo articolo introduce un obiettivo di co-formazione teorico dell'informazione per l'apprendimento non supervisionato. Consideriamo il problema di predire il futuro. Piuttosto che predire le sensazioni future (pixel di immagini o onde sonore) prevediamo delle ``ipotesi'' che saranno confermate dalle sensazioni future. Più formalmente, assumiamo una distribuzione di popolazione su coppie $(x,y)$ dove possiamo pensare a $x$ come una sensazione passata e $y$ come una sensazione futura. Addestriamo sia un modello predittivo $P_\Phi(z|x)$ che un modello di conferma $P_\Psi(z|y)$ dove consideriamo $z$ come ipotesi (quando previsto) o fatti (quando confermato). Per una distribuzione di popolazione su coppie $(x,y)$ ci concentriamo sul problema di misurare l'informazione reciproca tra $x$ e $y$. Per la disuguaglianza nell'elaborazione dei dati, questa informazione reciproca è almeno grande quanto l'informazione reciproca tra $x$ e $z$ sotto la distribuzione sulle triple $(x,z,y)$ definita dal modello di conferma $P_\Psi(z|y)$. L'obiettivo di formazione teorica dell'informazione per $P_\Phi(z|x)$ e $P_\Psi(z|y)$ può essere visto come una forma di co-formazione dove vogliamo che la previsione da $x$ corrisponda alla conferma da $y$.Diamo esperimenti su applicazioni per l'apprendimento della fonetica sul dataset TIMIT.
I modelli generativi per la voce cantata si sono occupati principalmente del compito di "sintesi della voce cantata", cioè In questo lavoro, esploriamo un'alternativa nuova ma impegnativa: la generazione di voci cantate senza partiture e testi pre-assegnati, sia nel tempo di addestramento che in quello di inferenza. In particolare, sperimentiamo tre diversi schemi: 1) cantante libero, dove il modello genera voci cantate senza prendere alcuna condizione; 2) cantante accompagnato, dove il modello genera voci cantate su una forma d'onda di musica strumentale; e 3) cantante solista, dove il modello improvvisa prima una sequenza di accordi e poi la usa per generare voci. Questo comporta lo sviluppo di modelli di separazione delle fonti e di trascrizione per la preparazione dei dati, di reti avversarie per la generazione dell'audio e di metriche personalizzate per la valutazione.
L'impronta di carbonio della ricerca sull'elaborazione del linguaggio naturale (NLP) è aumentata negli ultimi anni a causa della sua dipendenza da implementazioni di reti neurali grandi e inefficienti.La distillazione è una tecnica di compressione della rete che tenta di impartire la conoscenza da un grande modello a uno più piccolo.Usiamo la distillazione insegnante-studente per migliorare l'efficienza del parser di dipendenze Biaffine che ottiene prestazioni allo stato dell'arte per quanto riguarda la precisione e la velocità di parsing (Dozat & Manning, 2016).  Quando si distilla al 20% dei parametri addestrabili del modello originale, osserviamo solo una diminuzione media di ∼1 punto sia per UAS che per LAS su una serie di diversi treebanks di dipendenze universali, pur essendo 2.26x (1.21x) più veloce del modello di base su CPU (GPU) al tempo di inferenza.  Infine, attraverso la distillazione otteniamo un parser che non è solo più veloce ma anche più accurato del più veloce parser moderno sulla Penn Treebank.
Mentre gli autocodificatori sono una tecnica chiave nell'apprendimento di rappresentazioni per strutture continue, come immagini o forme d'onda, lo sviluppo di autocodificatori generici per strutture discrete, come sequenze di testo o immagini discretizzate, si è rivelato più impegnativo; in particolare, gli input discreti rendono più difficile l'apprendimento di un codificatore uniforme che preservi le complesse relazioni locali nello spazio di input. ARAE allena congiuntamente sia un ricco codificatore a spazio discreto, come un RNN, sia una più semplice funzione generatrice a spazio continuo, mentre utilizza l'addestramento generativo della rete adversariale (GAN) per vincolare le distribuzioni ad essere simili. Questo metodo produce uno spazio di codice contratto più liscio che mappa gli input simili ai codici vicini, e anche un modello GAN implicito di variabile latente per la generazione. Gli esperimenti sul testo e sulle immagini discretizzate dimostrano che il modello GAN produce interpolazioni pulite e cattura la multimodalità dello spazio originale, e che l'autocodificatore produce miglioramenti nell'apprendimento semi-supervisionato così come risultati allo stato dell'arte nel compito di trasferimento dello stile del testo non allineato usando solo una rappresentazione condivisa dello spazio continuo.
Quando i dati provengono da più sottopopolazioni latenti, le strutture di apprendimento automatico tipicamente stimano i valori dei parametri in modo indipendente per ogni sottopopolazione. In questo articolo, proponiamo di superare questi limiti considerando i campioni come compiti in una struttura di apprendimento multitask.
In questo lavoro, in primo luogo conduciamo un'analisi matematica sulla memoria, che è definita come una funzione che mappa un elemento in una sequenza all'uscita corrente, di tre celle RNN; vale a dire, la rete neurale ricorrente semplice (SRN), la memoria a breve termine lungo (LSTM) e l'unità ricorrente gated (GRU).sulla base dell'analisi, proponiamo un nuovo design, chiamato la memoria a breve termine estesa (ELSTM), per estendere la lunghezza della memoria di una cella. Successivamente, presentiamo un modello multi-taskRNN che è robusto alle previsioni errate precedenti, chiamato rete neurale ricorrente bidirezionale dipendente (DBRNN), per il problema sequence-in-sequenceout (SISO).infine, le prestazioni del modello DBRNN con la cellaELSTM sono dimostrate dai risultati sperimentali.
Molte reti neurali recentemente addestrate impiegano un gran numero di parametri per ottenere buone prestazioni. Si può intuitivamente usare il numero di parametri richiesti come un indicatore approssimativo della difficoltà di un problema, ma quanto sono accurate queste nozioni? Quanti parametri sono realmente necessari? In questo articolo cerchiamo di rispondere a questa domanda addestrando le reti non nel loro spazio dei parametri nativo, ma invece in un sottospazio più piccolo, orientato in modo casuale. Aumentiamo lentamente la dimensione di questo sottospazio, notiamo a quale dimensione appaiono per la prima volta le soluzioni, e definiamo questa come la dimensione intrinseca del paesaggio obiettivo. L'approccio è semplice da implementare, computazionalmente trattabile, e produce diverse conclusioni suggestive: molti problemi hanno dimensioni intrinseche più piccole di quanto si possa sospettare, e la dimensione intrinseca per un dato set di dati varia poco attraverso una famiglia di modelli con dimensioni molto diverse. Quest'ultimo risultato ha la profonda implicazione che una volta che uno spazio di parametri è abbastanza grande per risolvere un problema, i parametri extra servono direttamente ad aumentare la dimensionalità del manifold della soluzione. La dimensione intrinseca permette un confronto quantitativo della difficoltà del problema attraverso l'apprendimento supervisionato, il rinforzo e altri tipi di apprendimento dove concludiamo, per esempio, che risolvere il problema del pendolo invertito è 100 volte più facile che classificare le cifre da MNIST, e giocare ad Atari Pong dai pixel è difficile quanto classificare CIFAR-10. Oltre a fornire una nuova cartografia dei paesaggi oggettivi percorsi dai modelli parametrizzati, il metodo è una tecnica semplice per ottenere in modo costruttivo un limite superiore alla lunghezza minima di descrizione di una soluzione; un sottoprodotto di questa costruzione è un approccio semplice per comprimere le reti, in alcuni casi di più di 100 volte.
Graph Neural Networks come combinazione di Graph Signal Processing e Deep Convolutional Networks mostra un grande potere nel pattern recognition in domini non euclidei.In questo articolo, proponiamo un nuovo metodo per implementare due pipeline basate sulla dualità di un grafico per migliorare l'accuratezza.Esplorando il grafico primordiale e il suo grafico duale dove nodi e bordi possono essere trattati l'uno come l'altro, abbiamo sfruttato i benefici sia delle caratteristiche dei vertici che dei bordi.Come risultato, siamo arrivati a una struttura che ha un grande potenziale sia nell'apprendimento semisupervisionato che non supervisionato.
Il primo modello è un Local Autoencoding Parser (LAP) che codifica l'input utilizzando variabili latenti continue in modo sequenziale; il secondo modello è un Global Autoencoding Parser (GAP) che codifica l'input in alberi di dipendenza come variabili latenti, con inferenza esatta. Entrambi i modelli consistono di due parti: un encoder potenziato da reti neurali profonde (DNN) che può utilizzare le informazioni contestuali per codificare l'input in variabili latenti, e un decoder che è un modello generativo in grado di ricostruire l'input. Sia LAP che GAP ammettono una struttura unificata con diverse funzioni di perdita per dati etichettati e non etichettati con parametri condivisi. Abbiamo condotto esperimenti su set di dati WSJ e UD dependency parsing, dimostrando che i nostri modelli possono sfruttare i dati non etichettati per aumentare le prestazioni data una quantità limitata di dati etichettati.
Un meccanismo di gate aggiorna i pesi veloci ad ogni passo temporale di una sequenza attraverso due matrici separate basate sul prodotto esterno generate dalle parti lente della rete. Il sistema è addestrato su una complessa variazione da sequenza a sequenza dell'Associative Retrieval Problem con circa 70 volte più memoria temporale (cioè variabili variabili nel tempo). In termini di accuratezza e numero di parametri, la nostra architettura supera una varietà di RNN, tra cui Long Short-Term Memory, Hypernetworks e le relative architetture a peso veloce.
Il campo dell'apprendimento profondo è stato bramoso di un metodo di ottimizzazione che mostri proprietà eccezionali sia per l'ottimizzazione che per la generalizzazione.  Proponiamo un metodo per l'ottimizzazione matematica basato sui flussi lungo le geodetiche, cioè i percorsi più brevi tra due punti, rispetto alla metrica Riemanniana indotta da una funzione non lineare.Nel nostro metodo, i flussi si riferiscono a Exponentially Decaying Flows (EDF), in quanto possono essere progettati per convergere sulle soluzioni locali in modo esponenziale.In questo articolo, conduciamo esperimenti per mostrare le sue alte prestazioni sui benchmark di ottimizzazione (cioè, proprietà di convergenza), così come il suo potenziale per produrre buoni benchmark di apprendimento automatico (cioè, proprietà di generalizzazione).
Introduciamo le Quantum Graph Neural Networks (QGNN), una nuova classe di ansatze di reti neurali quantistiche che sono fatte su misura per rappresentare processi quantistici che hanno una struttura a grafo, e sono particolarmente adatte ad essere eseguite su sistemi quantistici distribuiti su una rete quantistica. Insieme a questa classe generale di ansatze, introduciamo ulteriori architetture specializzate, vale a dire le reti neurali ricorrenti del grafo quantistico (QGRNN) e le reti neurali convoluzionali del grafo quantistico (QGCNN). Forniamo quattro applicazioni di esempio di QGNN: imparare la dinamica hamiltoniana dei sistemi quantistici, imparare come creare l'entanglement multipartito in una rete quantistica, apprendimento non supervisionato per il clustering spettrale e apprendimento supervisionato per la classificazione di isomorfismo del grafico.
La nostra ricerca riguarda la percezione degli utenti delle violazioni dei dati, in particolare le questioni relative alla responsabilità. Uno studio preliminare ha indicato che molte persone avevano una comprensione debole dei problemi, e sentivano di essere in qualche modo responsabili. Abbiamo quindi confrontato i testi delle organizzazioni con fonti esterne, come i media, suggerendo che le organizzazioni utilizzano metodi di comunicazione di crisi ben noti per ridurre il loro danno reputazionale, e che queste strategie si allineano con il riposizionamento degli elementi narrativi coinvolti nella storia. I risultati di questo studio sono in linea con la nostra analisi dei documenti e suggeriscono che la comunicazione organizzativa influisce sulla percezione di vittimizzazione degli utenti, sulle attitudini alla protezione dei dati e sulla responsabilità. Il nostro studio suggerisce alcune implicazioni legali e di design del software che supportano gli utenti a proteggersi e a sviluppare migliori modelli mentali delle violazioni della sicurezza.
Il riconoscimento dell'obiettivo è il problema di dedurre l'obiettivo corretto verso il quale un agente esegue un piano, dato un insieme di ipotesi di obiettivo, un modello di dominio e un campione (possibilmente rumoroso) del piano in esecuzione.   Questo è un problema chiave sia nelle interazioni cooperative che competitive tra agenti e approcci recenti hanno prodotto algoritmi di riconoscimento degli obiettivi veloci e accurati.   In questo articolo, sfruttiamo i progressi nell'euristica di conteggio degli operatori calcolata usando programmi lineari su vincoli derivati dai classici problemi di pianificazione per risolvere i problemi di riconoscimento degli obiettivi.   Il nostro approccio utilizza ulteriori vincoli di conteggio degli operatori derivati dalle osservazioni per dedurre in modo efficiente l'obiettivo corretto, e serve come base per una serie di ulteriori metodi con vincoli aggiuntivi.
Per aiutare a risolvere questo problema, proponiamo di utilizzare la distillazione della conoscenza in cui i modelli a compito singolo insegnano a un modello a compito multiplo. miglioriamo questo addestramento con la ricottura dell'insegnante, un metodo nuovo che gradualmente passa il modello dalla distillazione all'apprendimento supervisionato, aiutando il modello a compito multiplo a superare i suoi insegnanti a compito singolo. valutiamo il nostro approccio da BERT di fine-tuning multi-task sul benchmark GLUE.
Il rilevamento di campioni fuori distribuzione per la classificazione delle immagini è stato ampiamente studiato.Le applicazioni critiche per la sicurezza, come la guida autonoma, beneficerebbero della capacità di localizzare gli oggetti insoliti che causano l'immagine fuori distribuzione.Questo documento adatta i metodi all'avanguardia per il rilevamento delle immagini fuori distribuzione per la classificazione delle immagini al nuovo compito di rilevare i pixel fuori distribuzione, che possono localizzare gli oggetti insoliti. Inoltre confronta sperimentalmente i metodi adattati su due nuovi set di dati derivati da set di dati esistenti di segmentazione semantica utilizzando le architetture PSPNet e DeeplabV3+, oltre a proporre una nuova metrica per il compito. La valutazione mostra che la classifica delle prestazioni dei metodi confrontati non si trasferisce al nuovo compito e ogni metodo esegue significativamente peggio delle loro controparti a livello di immagine.
Presentiamo un metodo di adattamento del dominio per trasferire le rappresentazioni neurali dai domini di origine ricchi di etichette ai domini di destinazione non etichettati.I metodi avversari recenti proposti per questo compito imparano ad allineare le caratteristiche attraverso i domini ``fooling'' una rete speciale del classificatore del dominio.Tuttavia, uno svantaggio di questo approccio è che il classificatore del dominio etichetta semplicemente le caratteristiche generate come nel dominio o non, senza considerare i confini tra le classi.Questo significa che le caratteristiche ambigue dell'obiettivo possono essere generate vicino ai confini di classe, riducendo l'accuratezza della classificazione dell'obiettivo. Proponiamo un nuovo approccio, Adversarial Dropout Regularization (ADR), che incoraggia il generatore a produrre caratteristiche più discriminanti per il dominio di destinazione.La nostra idea chiave è quella di sostituire il tradizionale critico di dominio con un critico che rileva le caratteristiche non discriminanti utilizzando il dropout sulla rete del classificatore.Il generatore impara quindi a evitare queste aree dello spazio delle caratteristiche e quindi crea caratteristiche migliori.Applichiamo il nostro approccio ADR al problema dell'adattamento di dominio non supervisionato per la classificazione delle immagini e i compiti di segmentazione semantica, e dimostriamo miglioramenti significativi rispetto allo stato dell'arte.
L'uso dell'apprendimento profondo per una vasta gamma di problemi di dati ha aumentato la necessità di comprendere e diagnosticare questi modelli, e le tecniche di interpretazione dell'apprendimento profondo sono diventate uno strumento essenziale per gli analisti dei dati.Sebbene negli ultimi anni siano stati proposti numerosi metodi di interpretazione del modello, la maggior parte di queste procedure si basa su euristiche con poche o nessuna garanzia teorica.In questo lavoro, proponiamo un quadro statistico per la stima della salienza per i modelli di visione informatica della scatola nera.Costruiamo una procedura di stima modello-agnostica che è statisticamente coerente e supera i controlli di salienza di Adebayo et al. (Attraverso la nostra analisi teorica, stabiliamo un limite superiore sul numero di valutazioni del modello necessarie per recuperare la regione di importanza con alta probabilità, e costruiamo un nuovo schema di perturbazione per la stima dei gradienti locali che si dimostra più efficiente degli schemi di perturbazione casuale comunemente usati.
Esploriamo l'idea di set embeddings compositivi che possono essere utilizzati per dedurre non solo una singola classe, ma l'insieme delle classi associate ai dati di input (ad es, Questo può essere utile, per esempio, nel rilevamento di più oggetti nelle immagini, o nella diarizzazione di più parlanti (apprendimento one-shot) nell'audio. In particolare, abbiamo studiato e implementato due nuovi modelli che consistono in (1) una funzione di embeddingf addestrata congiuntamente a una funzione "composita" g che calcola le operazioni di unione tra le classi codificate in due vettori di embedding; e (2) embeddingf addestrata congiuntamente a una funzione "query" h che calcola se le classi codificate in un embedding sussumono le classi codificate in un altro embedding. Contrariamente ai lavori precedenti, questi modelli devono sia percepire le classi associate agli esempi di input, sia codificare le relazioni tra diversi set di etichette di classe. Negli esperimenti condotti su dati simulati, OmniGlot e COCOdatasets, i modelli di embedding compositi proposti superano le prestazioni di base basate su approcci di embedding tradizionali.
In questo lavoro, proponiamo un compito collaborativo guidato da un obiettivo che contiene il linguaggio, la visione e l'azione in un ambiente virtuale come componenti principali. In particolare, sviluppiamo un gioco collaborativo di disegno di immagini tra due agenti, chiamato CoDraw. Il cassiere vede una scena astratta contenente più pezzi di clip art in una configurazione semanticamente significativa, mentre il disegnatore cerca di ricostruire la scena su una tela vuota utilizzando i pezzi di clip art disponibili. I due giocatori comunicano attraverso una comunicazione bidirezionale utilizzando il linguaggio naturale. Raccogliamo il dataset CoDraw di ~10K dialoghi che consistono in ~138K messaggi scambiati tra agenti umani.Definiamo protocolli e metriche per valutare l'efficacia degli agenti appresi su questo banco di prova, evidenziando la necessità di una nuova condizione "crosstalk" che accoppia agenti addestrati indipendentemente su sottoinsiemi disgiunti dei dati di formazione per la valutazione. Presentiamo i modelli per il nostro compito, comprese le linee di base semplici ma efficaci e gli approcci di rete neurale addestrati utilizzando una combinazione di apprendimento per imitazione e formazione guidata dagli obiettivi.
La presenza di bias ed effetti di confondimento è indiscutibilmente una delle sfide più critiche nelle applicazioni di apprendimento automatico che ha alluso a dibattiti cruciali negli ultimi anni. Tali sfide vanno dalle associazioni spurie di variabili di confondimento negli studi medici al bias di razza nei sistemi di riconoscimento di genere o del volto. Una soluzione è quella di migliorare i set di dati e organizzarli in modo che non riflettano i bias, che è un compito ingombrante e intenso.L'alternativa è quella di fare uso dei dati disponibili e costruire modelli considerando questi bias.I metodi statistici tradizionali applicano tecniche semplici come la residualizzazione o la stratificazione alle caratteristiche precompilate per tenere conto delle variabili di confondimento. Tuttavia, queste tecniche non sono in generale applicabili ai metodi di apprendimento profondo end-to-end. In questo articolo, proponiamo un metodo basato sulla strategia di formazione avversaria per imparare caratteristiche discriminative imparziali e invarianti rispetto ai confonditori. Applichiamo il nostro metodo a un set di dati sintetico, una diagnosi medica e una classificazione di genere (Gender Shades). I nostri risultati mostrano che le caratteristiche apprese dal nostro metodo non solo portano a prestazioni di previsione superiori, ma sono anche non correlate con le variabili di bias o di confondimento.Il codice è disponibile su http://blinded_for_review/.
I modelli neurali esistenti di risposta alle domande (QA) devono ragionare e trarre inferenze complicate da un lungo contesto per la maggior parte dei dataset QA su larga scala.Tuttavia, se consideriamo QA come un compito combinato di recupero e ragionamento, possiamo assumere l'esistenza di un contesto minimo che è necessario e sufficiente per rispondere a una data domanda. Un lavoro recente ha dimostrato che un modulo selettore di frasi che seleziona un contesto più breve e lo fornisce al modello di AQ a valle raggiunge prestazioni paragonabili a un modello di AQ addestrato sul contesto completo, pur essendo anche più interpretabile.Un lavoro recente ha anche dimostrato che la maggior parte dei modelli di AQ allo stato dell'arte si rompono quando al contesto vengono aggiunte frasi generate dall'avversario. Mentre gli esseri umani sono immuni da tali frasi distrattive, i modelli di AQ vengono facilmente indotti a selezionare le risposte da queste frasi. Noi ipotizziamo che il modulo selettore di frasi possa filtrare il contesto estraneo, permettendo così al modello di AQ a valle di concentrarsi e ragionare sulle parti del contesto che sono rilevanti per la domanda. In questo articolo, mostriamo che il selettore di frasi stesso è suscettibile agli input avversari.Tuttavia, dimostriamo che una pipeline costituita da un modulo selettore di frasi seguito dal modello di AQ può essere reso più robusto agli attacchi avversari rispetto a un modello di AQ addestrato sul contesto completo.Quindi, forniamo prove verso un approccio modulare per la risposta alle domande che è più robusto e interpretabile.
Anche se i dati della base di conoscenza di solito contengono una struttura ad albero o ciclica, nessuno degli approcci esistenti può incorporare questi dati in uno spazio compatibile con la struttura. Per superare questo problema, un nuovo quadro, chiamato Riemannian TransE, è proposto in questo articolo per incorporare le entità in un collettore Riemanniano. Riemannian TransE modella ogni relazione come uno spostamento verso un punto e definisce una nuova distanza dissimilarità specifica per ogni relazione, in modo che tutte le relazioni siano naturalmente incorporate in corrispondenza della struttura dei dati. Gli esperimenti su diversi compiti di completamento della base di conoscenza hanno dimostrato che, sulla base di una scelta appropriata del collettore, Riemannian TransE raggiunge buone prestazioni anche con parametri significativamente ridotti.
La dimenticanza catastrofica nelle reti neurali è uno dei problemi più noti nell'apprendimento continuo.I tentativi precedenti di affrontare il problema si concentrano sull'impedire che i pesi importanti cambino.Tali metodi spesso richiedono i confini dei compiti per imparare efficacemente e non supportano l'apprendimento di trasferimento a ritroso.In questo articolo, proponiamo un algoritmo di meta-apprendimento che impara a ricostruire i gradienti dei vecchi compiti w. r.t. i parametri attuali e combina questi gradienti ricostruiti con il gradiente attuale per consentire l'apprendimento continuo e l'apprendimento di trasferimento a ritroso dal compito attuale ai compiti precedenti. Gli esperimenti sui benchmark standard di apprendimento continuo mostrano che il nostro algoritmo può prevenire efficacemente la dimenticanza catastrofica e supporta l'apprendimento di trasferimento a ritroso.
 Diamo una procedura formale per il calcolo delle preimmagini degli output delle reti convoluzionali usando la base duale definita dall'insieme degli iperpiani associati agli strati della rete, sottolineando la speciale simmetria associata alle disposizioni degli iperpiani delle reti convoluzionali che prendono la forma di coni poliedrici multidimensionali regolari. Discutiamo l'efficienza di un gran numero di strati di coni annidati che risultano da convoluzioni incrementali di piccole dimensioni al fine di dare un buon compromesso tra la contrazione efficiente dei dati a basse dimensioni e la formazione di collettori di preimmagini. Dimostriamo come una rete specifica appiattisce un collettore di input non lineare in un collettore di output affine e discutiamo la sua rilevanza per comprendere le proprietà di classificazione delle reti profonde.
Il meta-apprendimento ha fatto progressi impressionanti per l'adattamento veloce del modello, ma è stato fatto un lavoro limitato sull'apprendimento dell'adattamento veloce dell'incertezza per la modellazione bayesiana. In questo articolo, proponiamo di raggiungere l'obiettivo ponendo il meta-apprendimento sullo spazio delle misure di probabilità, inducendo il concetto di meta-campionamento per l'adattamento veloce dell'incertezza. In particolare, proponiamo una struttura di metacampionamento bayesiano che consiste di due componenti principali: un metacampionatore e un adattatore di campioni. Il metacampionatore è costruito adottando una struttura neural-inverse-autoregressive-flow (NIAF), una variante dei flussi neurali autoregressivi recentemente proposti, per generare in modo efficiente metacampioni da adattare. La combinazione dei due componenti permette di sviluppare una semplice procedura di apprendimento per il campionatore di meta, che può essere efficientemente ottimizzata tramite back-propagation standard. Ampi risultati sperimentali dimostrano l'efficienza e l'efficacia della struttura proposta, ottenendo una migliore qualità del campione e un più veloce adattamento dell'incertezza rispetto ai metodi correlati.
Il nostro modello sfrutta due tipi di contesti, contesti che consumano bit e contesti privi di bit, distinti in base alla necessità o meno di un'allocazione aggiuntiva di bit. Sulla base di questi contesti, consentiamo al modello di stimare più accuratamente la distribuzione di ogni rappresentazione latente con una forma più generalizzata dei modelli di approssimazione, che di conseguenza porta a un miglioramento delle prestazioni di compressione. Sulla base dei risultati sperimentali, il metodo proposto supera i codec tradizionali dell'immagine, come BPG e JPEG2000, così come altri approcci precedenti basati su reti neurali artificiali (ANN), in termini di rapporto segnale-rumore di picco (PSNR) e indice di somiglianza strutturale multiscala (MS-SSIM). Il codice di prova è pubblicamente disponibile su https://github.com/JooyoungLeeETRI/CA_Entropy_Model.
Le reti profonde spesso si comportano bene sulla distribuzione dei dati su cui sono addestrate, ma danno risposte errate (e spesso molto sicure) quando vengono valutate su punti al di fuori della distribuzione di addestramento.Questo è esemplificato dal fenomeno degli esempi avversari, ma può anche essere visto in termini di generalizzazione del modello e spostamento del dominio.  Idealmente, un modello dovrebbe assegnare una fiducia più bassa ai punti diversi da quelli della distribuzione di addestramento.  Proponiamo un regolatore che affronta questo problema allenandosi con stati nascosti interpolati e incoraggiando il classificatore ad essere meno fiducioso in questi punti.  Poiché gli stati nascosti vengono appresi, questo ha l'importante effetto di incoraggiare gli stati nascosti per una classe ad essere concentrati in modo tale che le interpolazioni all'interno della stessa classe o tra due classi diverse non si intersechino con i punti di dati reali di altre classi.  Questo ha un grande vantaggio in quanto evita l'underfitting che può risultare dall'interpolazione nello spazio di input.  Dimostriamo che la condizione esatta perché questo problema di underfitting sia evitato da Manifold Mixup è che la dimensionalità degli stati nascosti superi il numero di classi, che è spesso il caso nella pratica.  Inoltre, questa concentrazione può essere vista come un modo per rendere le caratteristiche negli strati precedenti più discriminanti.  Mostriamo che, nonostante non richieda alcun calcolo aggiuntivo significativo, Manifold Mixup raggiunge grandi miglioramenti rispetto a forti basi nell'apprendimento supervisionato, nella robustezza agli attacchi avversari a passo singolo, nell'apprendimento semi-supervisionato e nella Log-Likelihood negativa su campioni trattenuti.
A volte SRS (Stereotactic Radio Surgery) richiede l'utilizzo di sphere packing su una regione di interesse (ROI) come il cancro per determinare un piano di trattamento.  Abbiamo sviluppato un algoritmo di impacchettamento delle sfere che impacchetta le sfere non intersecanti all'interno della ROI.  La regione di interesse nel nostro caso sono quei voxel che sono identificati come tessuti cancerosi.  In questo articolo, analizziamo le proprietà invarianti alla rotazione del nostro algoritmo di impacchettamento delle sfere che si basa sulle trasformazioni di distanza.Epsilon-Rotation invariant significa la capacità di ruotare arbitrariamente il ROI 3D mantenendo le proprietà del volume che rimangono (quasi) uguali entro un certo limite di epsilon.Le rotazioni applicate producono un impacchettamento sferico che rimane altamente correlato mentre analizziamo le proprietà geometriche dell'impacchettamento delle sfere prima e dopo la rotazione dei dati del volume del ROI. Il nostro nuovo algoritmo di impacchettamento sferico ha un alto grado di invarianza di rotazione nell'intervallo di +/- epsilon.Il nostro metodo ha usato un descrittore di forma derivato dai valori dell'insieme disgiunto di sfere forma l'algoritmo di impacchettamento sferico basato sulla distanza per estrarre il descrittore invariante dal ROI.Abbiamo dimostrato implementando queste idee utilizzando la piattaforma Slicer3D disponibile per la nostra ricerca.  I dati sono basati su immagini stereotassiche MRI sing.Abbiamo presentato diversi risultati di prestazioni su diversi dati di benchmark di oltre 30 pazienti nella piattaforma Slicer3D.
Mostriamo che la sparsità implicita a livello di filtro si manifesta nelle reti neurali convoluzionali (CNN) che impiegano la normalizzazione del lotto e l'attivazione ReLU, e sono addestrate utilizzando tecniche di discesa del gradiente adattative con regolarizzazione L2 o decadimento del peso.Attraverso un ampio studio empirico (Anonimo, 2019) ipotizziamo il meccanismo essere dietro il processo di sparsificazione.Troviamo che l'interazione di vari fenomeni influenza la forza dei regolarizzatori L2 e del decadimento del peso, portando i regolarizzatori presumibilmente non sparsity inducono la sparsità del filtro.  In questo articolo del workshop riassumiamo alcuni dei nostri risultati chiave e degli esperimenti, e presentiamo ulteriori risultati su architetture di rete moderne come ResNet-50.
Gli esseri umani possono imparare una varietà di concetti e abilità in modo incrementale nel corso della loro vita, esibendo una serie di proprietà desiderabili, come la non dimenticanza, la ripetizione del concetto, il trasferimento in avanti e all'indietro della conoscenza, l'apprendimento in pochi istanti e la dimenticanza selettiva.Gli approcci precedenti all'apprendimento automatico per tutta la vita possono dimostrare solo sottoinsiemi di queste proprietà, spesso combinando più meccanismi complessi.  In questa prospettiva, proponiamo un potente quadro unificato che può dimostrare tutte le proprietà utilizzando un piccolo numero di parametri di consolidamento del peso nelle reti neurali profonde.Inoltre, siamo in grado di tracciare molti paralleli tra i comportamenti e i meccanismi del nostro quadro proposto e quelli che circondano l'apprendimento umano, come la perdita di memoria o la privazione del sonno.Questa prospettiva serve come un canale di ispirazione bidirezionale per comprendere ulteriormente l'apprendimento permanente nelle macchine e negli umani.
La ricostruzione CT ad angolo limitato è un problema inverso lineare sottodeterminato che richiede appropriate tecniche di regolarizzazione per essere risolto.In questo lavoro studiamo come le reti generative adversariali pre-addestrate (GAN) possono essere utilizzate per pulire le ricostruzioni rumorose e altamente cariche di artefatti dalle tecniche convenzionali, proiettando efficacemente sul manifold dell'immagine dedotta. L'approccio proposto opera direttamente nello spazio dell'immagine, per cui non ha bisogno di essere addestrato o di accedere al modello di misurazione, è agnostico rispetto allo scanner e può funzionare su una vasta gamma di scenari di rilevamento.
Proponiamo un'efficace configurazione di apprendimento multitask per ridurre il rumore di supervisione a distanza sfruttando la supervisione a livello di frase.Mostriamo come la supervisione a livello di frase può essere utilizzata per migliorare la codifica delle singole frasi, e per imparare quali frasi di input hanno maggiori probabilità di esprimere la relazione tra una coppia di entità.Introduciamo anche una nuova architettura neurale per raccogliere segnali da più frasi di input, che combina i vantaggi dell'attenzione e del maxpooling.Il metodo proposto aumenta l'AUC del 10% (da 0.261 a 0.284), e supera i risultati recentemente pubblicati sul dataset FB-NYT.
Lo strato di attenzione in un modello di rete neurale fornisce intuizioni sul ragionamento del modello dietro la sua previsione, che sono solitamente criticate per essere opache.Recentemente, sono emersi punti di vista apparentemente contraddittori sull'interpretabilità dei pesi di attenzione (Jain & Wallace, 2019; Vig & Belinkov, 2019).In mezzo a tale confusione sorge la necessità di comprendere il meccanismo di attenzione in modo più sistematico.In questo lavoro, cerchiamo di colmare questa lacuna fornendo una spiegazione completa che giustifica entrambi i tipi di osservazioni (cioè, Attraverso una serie di esperimenti su diversi compiti NLP, convalidiamo le nostre osservazioni e rafforziamo la nostra affermazione di interpretabilità dell'attenzione attraverso una valutazione manuale.
I modelli generativi per il codice sorgente sono un interessante problema di predizione strutturata, che richiede di ragionare sia su vincoli sintattici e semantici rigidi che su programmi naturali e probabili. Presentiamo un nuovo modello per questo problema che utilizza un grafico per rappresentare lo stato intermedio dell'output generato. Il nostro modello genera codice intrecciando fasi di espansione guidate dalla grammatica con l'aumento del grafico e fasi di passaggio dei messaggi neurali.
L'inferenza dei modelli, la previsione dei simboli futuri e la stima del tasso di entropia dei processi a tempo discreto e a eventi discreti è un terreno ben battuto. Tuttavia, molte serie temporali sono meglio concettualizzate come processi a tempo continuo e a eventi discreti. I metodi si basano su un'estensione dell'inferenza strutturale bayesiana che sfrutta il potere di approssimazione universale delle reti neurali. Sulla base di esperimenti con semplici dati sintetici, questi nuovi metodi sembrano essere competitivi con i metodi allo stato dell'arte per la predizione e la stima del tasso di entropia, a condizione che il modello corretto sia dedotto.
Studi recenti dimostrano che le reti neurali convoluzionali (CNN) sono vulnerabili in varie impostazioni, compresi esempi avversari, attacchi backdoor e spostamento della distribuzione.  Motivato dalle scoperte che il sistema visivo umano presta più attenzione alla struttura globale (ad es, forma) per il riconoscimento mentre le CNN sono orientate verso le caratteristiche locali della texture nelle immagini, proponiamo un quadro unificato EdgeGANRob basato su robuste caratteristiche del bordo per migliorare la robustezza delle CNN in generale, che prima estrae esplicitamente le caratteristiche di forma/struttura da una data immagine e poi ricostruisce una nuova immagine riempiendo le informazioni della texture con una rete generativa adversariale addestrata (GAN). Inoltre, per ridurre la sensibilità dell'algoritmo di rilevamento del bordo alla perturbazione avversaria, proponiamo un robusto approccio di rilevamento del bordo Robust Canny basato sull'algoritmo Canny vaniglia. Per ottenere maggiori informazioni, confrontiamo EdgeGANRob con la sua procedura semplificata EdgeNetRob, che esegue i compiti di apprendimento direttamente sulle caratteristiche del bordo robusto estratte. Troviamo che EdgeNetRob può aiutare ad aumentare la robustezza del modello in modo significativo, ma al costo della precisione del modello pulito. EdgeGANRob, d'altra parte, è in grado di migliorare la precisione del modello pulito rispetto a EdgeNetRob e senza perdere i benefici di robustezza introdotti da EdgeNetRob.
Imparare una rappresentazione ricca dai dati è un compito importante per i modelli generativi profondi come il variational auto-encoder (VAE).Tuttavia, estraendo astrazioni di alto livello nel processo di inferenza bottom-up, l'obiettivo di preservare tutti i fattori di variazioni per la generazione top-down è compromesso.Motivato dal concetto di "iniziare in piccolo", presentiamo una strategia per imparare progressivamente rappresentazioni gerarchiche indipendenti da alti a bassi livelli di astrazione.Il modello inizia con l'apprendimento della rappresentazione più astratta, e poi crescere progressivamente l'architettura di rete per introdurre nuove rappresentazioni a diversi livelli di astrazione. Dimostriamo quantitativamente la capacità del modello presentato di migliorare il disentanglement rispetto ai lavori esistenti su due set di dati di riferimento usando tre metriche di disentanglement, inclusa una nuova metrica che abbiamo proposto per completare la metrica precedentemente presentata del gap di informazione reciproca. Presentiamo inoltre prove sia qualitative che quantitative su come la progressione dell'apprendimento migliora la separazione delle rappresentazioni gerarchiche. Attingendo al rispettivo vantaggio dell'apprendimento della rappresentazione gerarchica e dell'apprendimento progressivo, questo è a nostra conoscenza il primo tentativo di migliorare la separazione facendo crescere progressivamente la capacità di VAE di imparare le rappresentazioni gerarchiche.
I modelli profondi di variabili latenti sono strumenti potenti per l'apprendimento delle rappresentazioni. In questo articolo, adottiamo il modello profondo del collo di bottiglia dell'informazione, identifichiamo i suoi difetti e proponiamo un modello che li aggira. A tal fine, applichiamo una trasformazione di copula che, ripristinando le proprietà di invarianza del metodo del collo di bottiglia dell'informazione, porta alla disentanglement delle caratteristiche nello spazio latente.  Valutiamo il nostro metodo su dati artificiali e reali.
Introduciamo un nuovo metodo per costruire sistematicamente tali benchmark massimizzando la divergenza composta, garantendo al contempo una piccola divergenza atomica tra gli insiemi di treno e di prova, e confrontiamo quantitativamente questo metodo con altri approcci per la creazione di benchmark di generalizzazione compositiva. Presentiamo un ampio e realistico set di dati di risposta a domande in linguaggio naturale costruito secondo questo metodo, e lo usiamo per analizzare la capacità di generalizzazione compositiva di tre architetture di apprendimento automatico, scoprendo che non riescono a generalizzare in modo compositivo e che c'è una correlazione negativa sorprendentemente forte tra la divergenza composta e l'accuratezza, dimostrando anche come il nostro metodo possa essere usato per creare nuovi benchmark di compositività sopra il set di dati SCAN esistente, che conferma questi risultati.
Per capire come la visione degli oggetti si sviluppa nell'infanzia e nella fanciullezza, sarà necessario sviluppare modelli computazionali testabili.Le reti neurali profonde (DNN) si sono dimostrate preziose come modelli della visione adulta, ma non è ancora chiaro se hanno valore come modelli di sviluppo.Come primo modello, abbiamo misurato l'apprendimento in una DNN progettata per imitare l'architettura e la geometria rappresentazionale del sistema visivo (CORnet). Abbiamo quantificato lo sviluppo di rappresentazioni esplicite di oggetti ad ogni livello di questa rete attraverso l'addestramento congelando gli strati convoluzionali e addestrando un ulteriore strato di decodifica lineare.Valutiamo l'accuratezza della decodifica sull'intero set di validazione ImageNet, e anche per singole classi visive. CORnet, tuttavia, utilizza un addestramento supervisionato e poiché i neonati hanno solo un accesso estremamente povero alle etichette, devono invece imparare in modo non supervisionato.Abbiamo quindi anche misurato l'apprendimento in una rete non supervisionata allo stato dell'arte (DeepCluster).CORnet e DeepCluster differiscono sia nella supervisione che nelle reti convoluzionali al loro cuore, quindi per isolare l'effetto della supervisione, abbiamo eseguito un esperimento di controllo in cui abbiamo addestrato la rete convoluzionale di DeepCluster (una variante di AlexNet) in modo supervisionato. In tutte e tre le reti, abbiamo anche testato per una relazione nell'ordine in cui i bambini e le macchine acquisiscono le classi visive, e abbiamo trovato solo prove per una relazione contro-intuitiva.
Mostriamo che in una varietà di scenari di apprendimento profondo su larga scala, il gradiente converge dinamicamente in un sottospazio molto piccolo dopo un breve periodo di formazione. Il sottospazio è attraversato da alcuni autovettori superiori dell'Hessiano (pari al numero di classi nel set di dati), ed è per lo più conservato per lunghi periodi di formazione.
Il nostro modello di retriever addestra una rete neurale ricorrente che impara a recuperare sequenzialmente i paragrafi di prova nel percorso di ragionamento condizionando i documenti precedentemente recuperati. Il nostro modello di lettore classifica i percorsi di ragionamento ed estrae la campata di risposta inclusa nel miglior percorso di ragionamento.I risultati sperimentali mostrano risultati all'avanguardia in tre serie di dati di QA a dominio aperto, mostrando l'efficacia e la robustezza del nostro metodo.In particolare, il nostro metodo raggiunge un miglioramento significativo in HotpotQA, superando il modello migliore precedente di oltre 14 punti.
Negli ultimi anni, gli algoritmi di corrispondenza stereo basati sull'apprendimento profondo hanno raggiunto prestazioni eccellenti e sono diventati la direzione principale della ricerca. Gli algoritmi esistenti utilizzano generalmente reti neurali convoluzionali profonde (DCNN) per estrarre informazioni semantiche più astratte, ma noi crediamo che le informazioni dettagliate della struttura spaziale siano più importanti per i compiti di corrispondenza stereo. La rete consiste di tre parti: un modulo primario di estrazione delle caratteristiche, un modulo ASPP (atrous spatial pyramid pooling) e un modulo di fusione delle caratteristiche. Questa rete utilizza la capacità di estrazione delle caratteristiche di base della rete superficiale per estrarre e conservare le informazioni dettagliate della struttura spaziale.In questo documento, la convoluzione dilatata e il modulo ASPP (atrous spatial pyramid pooling) è introdotto per aumentare la dimensione del campo ricettivo.Inoltre, un modulo di fusione delle caratteristiche è progettato, che integra le mappe delle caratteristiche con campi ricettivi multiscala e integra reciprocamente le informazioni delle caratteristiche di scale diverse. Abbiamo sostituito la parte di estrazione delle caratteristiche degli algoritmi di corrispondenza stereo esistenti con la nostra rete di estrazione delle caratteristiche superficiali, e abbiamo ottenuto prestazioni all'avanguardia sul set di dati KITTI 2015. Rispetto alla rete di riferimento, il numero di parametri è ridotto del 42%, e la precisione di corrispondenza è migliorata del 1,9%.
Proponiamo un nuovo strato di output per le reti neurali profonde che permette l'uso di un feedback contestuale per l'addestramento, che può essere disponibile in grandi quantità (ad es, A tal fine, proponiamo un approccio Counterfactual Risk Minimization (CRM) per l'addestramento di reti profonde utilizzando uno stimatore di rischio empirico equivariante con regolarizzazione della varianza, BanditNet, e mostriamo come l'obiettivo risultante possa essere decomposto in modo da consentire l'addestramento Stochastic Gradient Descent (SGD). Dimostriamo empiricamente l'efficacia del metodo mostrando come le reti profonde - ResNets in particolare - possono essere addestrate per il riconoscimento degli oggetti senza immagini convenzionalmente etichettate.
La classificazione delle proteine è responsabile della sequenza biologica, abbiamo elaborato un'idea che si occupa della classificazione della proteomica usando un algoritmo di apprendimento profondo, questo algoritmo si concentra principalmente sulla classificazione delle sequenze di vettore proteico che viene utilizzato per la rappresentazione della proteomica. La selezione del tipo di rappresentazione della proteina è impegnativa in base alla quale dipende l'output in termini di accuratezza, la rappresentazione della proteina utilizzata qui è n-gramma cioè 3-gramma e Kerasembedding utilizzato per le sequenze biologiche come la proteina.In questo documento stiamo lavorando sulla Proteinclassificazione per mostrare la forza e la rappresentazione della sequenza biologica delle proteine
In questo lavoro, cerchiamo di rispondere a una domanda critica: se esiste una sequenza di input che causerà un modello ben addestrato di rete neurale discreto-spaziale sequenza-sequenza (seq2seq) per generare output egregi (aggressivo, dannoso, attaccante, ecc.) E se tali input esistono, come trovarli in modo efficiente. Adottiamo una metodologia empirica, in cui prima creiamo elenchi di sequenze di output egregi, e poi progettiamo un algoritmo di ottimizzazione discreta per trovare sequenze di input che causeranno al modello di generarli.Inoltre, l'algoritmo di ottimizzazione è migliorato per la ricerca di grandi vocabolari e vincolato alla ricerca di sequenze di input che è probabile che vengano inserite da utenti del mondo reale.Nei nostri esperimenti, applichiamo questo approccio a modelli di generazione di risposte di dialogo addestrati su tre serie di dati di dialogo del mondo reale: Ubuntu, Switchboard e OpenSubtitles, testando se il modello può generare risposte maliziose.Dimostriamo che, dati gli input di trigger che il nostro algoritmo trova, un numero significativo di frasi maliziose viene assegnato dal modello con grande probabilità, il che rivela una conseguenza indesiderata dell'addestramento seq2seq standard.
Nel problema dell'apprendimento non supervisionato di rappresentazioni dissociate, uno dei metodi promettenti è quello di penalizzare la correlazione totale delle variabili latenti campionate.  Sfortunatamente, questa strategia ben motivata spesso non riesce a raggiungere la disentanglement a causa di una problematica differenza tra la rappresentazione latente campionata e la sua corrispondente rappresentazione media.  Forniamo una spiegazione teorica del fatto che una bassa correlazione totale della distribuzione campionata non può garantire una bassa correlazione totale della rappresentazione media; dimostriamo che per la rappresentazione media con una correlazione totale arbitrariamente alta, esistono distribuzioni di variabili latenti con una correlazione totale abbondante.  Tuttavia, crediamo ancora che la correlazione totale potrebbe essere una chiave per il disinserimento dell'apprendimento rappresentativo non supervisionato, e proponiamo un rimedio, RTC-VAE, che rettifica la penalità di correlazione totale.   Gli esperimenti mostrano che il nostro modello ha una distribuzione più ragionevole della rappresentazione media rispetto ai modelli di base, per esempio, β-TCVAE e FactorVAE.
In questo lavoro, per collegare meglio la struttura dell'immagine con il testo generato, sostituiamo il tradizionale meccanismo di attenzione softmax con due trasformazioni alternative che promuovono la sparsità: sparsemax e Total-Variation Sparse Attention (TVmax).con sparsemax, otteniamo pesi di attenzione sparsi, selezionando caratteristiche rilevanti.  Per promuovere la sparsità e incoraggiare la fusione delle posizioni spaziali adiacenti correlate, proponiamo TVmax.  Selezionando gruppi di caratteristiche rilevanti, la trasformazione TVmax migliora l'interpretabilità. Presentiamo risultati nei dataset Microsoft COCO e Flickr30k, ottenendo guadagni rispetto a softmax.  TVmax supera gli altri meccanismi di attenzione confrontati in termini di qualità della didascalia valutata dall'uomo e di rilevanza dell'attenzione.
Anche se ci sono più di 65.000 lingue nel mondo, le pronunce di molti fonemi suonano simili tra le lingue.Quando le persone imparano una lingua straniera, la loro pronuncia spesso riflette le caratteristiche della loro lingua madre.Questo ci motiva a studiare come la rete di sintesi vocale impara la pronuncia quando il dataset multilingue è dato.In questo studio, addestriamo la rete di sintesi vocale bilingue in inglese e coreano, e analizzare come la rete impara le relazioni della pronuncia del fonema tra le lingue. Il nostro risultato sperimentale mostra che i vettori di incorporazione dei fonemi appresi si trovano più vicini se le loro pronunce sono simili tra le lingue.Sulla base del risultato, mostriamo anche che è possibile addestrare reti che sintetizzano il discorso coreano di un parlante inglese e viceversa.In un altro esperimento, addestriamo la rete con una quantità limitata di dataset inglese e un grande dataset coreano, e analizziamo la quantità necessaria di dataset per addestrare una lingua povera di risorse con l'aiuto di lingue ricche di risorse.
I Generative Adversarial Networks (GAN) hanno recentemente raggiunto risultati impressionanti per molte applicazioni del mondo reale, e molte varianti di GAN sono emerse con miglioramenti nella qualità del campione e nella stabilità dell'addestramento.Tuttavia, la visualizzazione e la comprensione dei GAN è in gran parte mancante.Come fa un GAN a rappresentare il nostro mondo visivo internamente?Cosa causa gli artefatti nei risultati GAN? In questo lavoro, presentiamo un quadro analitico per visualizzare e comprendere le GAN a livello di unità, oggetto e scena, identificando prima un gruppo di unità interpretabili che sono strettamente correlate ai concetti di oggetto con un metodo di dissezione della rete basato sulla segmentazione. Infine, esaminiamo la relazione contestuale tra queste unità e l'ambiente circostante inserendo i concetti di oggetto scoperti in nuove immagini. Mostriamo diverse applicazioni pratiche consentite dal nostro quadro, dal confronto delle rappresentazioni interne tra diversi livelli, modelli e set di dati, al miglioramento di GANs individuando e rimuovendo le unità che causano artefatti, alla manipolazione interattiva degli oggetti nella scena.Forniamo strumenti di interpretazione open source per aiutare i ricercatori alla pari e i professionisti a comprendere meglio i loro modelli GAN.
Storicamente, la ricerca di un'inferenza efficiente è stata una delle forze trainanti per la ricerca di nuove architetture di apprendimento profondo e blocchi di costruzione. Alcuni esempi recenti includono: il modulo squeeze-and-excitation di (Hu et al.,2018), convoluzioni separabili in profondità in Xception (Chollet, 2017), e il collo di bottiglia invertito in MobileNet v2 (Sandler et al., 2018).  In particolare, in tutti questi casi, i blocchi di costruzione risultanti hanno permesso non solo una maggiore efficienza, ma anche una maggiore precisione, e hanno trovato ampia adozione nel campo.In questo lavoro, espandiamo ulteriormente l'arsenale di blocchi di costruzione efficienti per le architetture di reti neurali; ma invece di combinare primitive standard (come la convoluzione), sosteniamo la sostituzione di queste primitive dense con le loro controparti sparse.  Mentre l'idea di usare la sparsità per diminuire il numero di parametri non è nuova (Mozer & Smolensky, 1989), la saggezza convenzionale è che questa riduzione dei FLOP teorici non si traduce in guadagni di efficienza nel mondo reale.  Abbiamo l'obiettivo di correggere questo equivoco introducendo una famiglia di efficienti kernel sparsi per diverse piattaforme hardware, che abbiamo intenzione di rendere open-source a beneficio della comunità. Equipaggiati con la nostra efficiente implementazione di primitive sparse, mostriamo che le versioni sparse delle architetture MobileNet v1 e MobileNet v2 superano sostanzialmente le forti basi dense sulla curva efficienza-precisione.   Su Snapdragon 835 le nostre reti sparse superano i loro equivalenti densi di 1.1-2.2x - equivalente a circa un'intera generazione di miglioramento.  Speriamo che i nostri risultati facilitino una più ampia adozione della sparsità come strumento per creare architetture di apprendimento profondo efficienti e accurate.
In questo lavoro, affrontiamo la classificazione semi-supervisionata dei dati del grafico, dove le categorie di quei nodi non etichettati sono dedotte dai nodi etichettati così come le strutture del grafico.lavori recenti spesso risolvono questo problema con la convoluzione avanzata del grafico in un modo convenzionale supervisionato, ma le prestazioni potrebbero essere pesantemente influenzate quando i dati etichettati sono scarsi.qui proponiamo un quadro Graph Inference Learning (GIL) per aumentare le prestazioni della classificazione dei nodi imparando l'inferenza delle etichette dei nodi sulla topologia del grafico. Per collegare due nodi, definiamo formalmente una relazione di struttura incapsulando insieme gli attributi dei nodi, i percorsi tra i nodi e le strutture topologiche locali, che possono rendere l'inferenza convenientemente dedotta da un nodo a un altro nodo. Per l'apprendimento del processo di inferenza, introduciamo ulteriormente la meta-ottimizzazione sulle relazioni di struttura dai nodi di addestramento ai nodi di convalida, in modo tale che la capacità di inferenza dei grafi appresa possa essere meglio auto-adattata nei nodi di prova. Valutazioni complete su quattro set di dati di riferimento (tra cui Cora, Citeseer, Pubmed e NELL) dimostrano la superiorità del nostro GIL rispetto ad altri metodi all'avanguardia nel compito di classificazione semi-supervisionata dei nodi.
Questo articolo propone un metodo per l'addestramento efficiente di Q-function per Markov Decision Processes (MDP) a stato continuo, in modo che le tracce delle politiche risultanti soddisfino una proprietà di Logica Temporale Lineare (LTL).LTL, una logica modale, può esprimere una vasta gamma di proprietà logiche dipendenti dal tempo, tra cui sicurezza e liveness.Noi convertiamo la proprietà LTL in un automa Buchi deterministico limite con cui viene costruito un prodotto MDP sincronizzato. Il metodo proposto è valutato in uno studio numerico per testare la qualità della politica di controllo generata ed è confrontato con i metodi convenzionali per la sintesi della politica come l'astrazione MDP (quantizzatore di Voronoi) e la programmazione dinamica approssimata (iterazione del valore montato).
Introduciamo due approcci per condurre un'inferenza bayesiana efficiente nei simulatori stocastici che contengono sottoprocedure stocastiche annidate, cioè, La classe risultante di simulatori è ampiamente utilizzata in tutte le scienze e può essere interpretata come modelli generativi probabilistici. Tuttavia, trarre inferenze da essi pone una sfida sostanziale a causa dell'incapacità di valutare anche la loro densità non normalizzata, impedendo l'uso di molte procedure di inferenza standard come Markov Chain Monte Carlo (MCMC). Per risolvere questo problema, introduciamo algoritmi di inferenza basati su un approccio a due fasi che prima approssima le densità condizionali delle singole sottoprocedure, prima di utilizzare queste approssimazioni per eseguire metodi MCMC sul programma completo. Poiché le sottoprocedure possono essere trattate separatamente e sono di dimensioni inferiori a quelle del problema complessivo, questo processo a due fasi permette loro di essere isolate e quindi essere trattate in modo tracotante, senza porre restrizioni alla dimensionalità complessiva del problema.
Mentre l'addestramento avversario può migliorare l'accuratezza robusta (contro un avversario), a volte danneggia l'accuratezza standard (quando non c'è un avversario). Il lavoro precedente ha studiato questo compromesso tra accuratezza standard e robusta, ma solo nell'impostazione in cui nessun predittore funziona bene su entrambi gli obiettivi nel limite infinito dei dati. In questo lavoro, mostriamo che anche quando il predittore ottimale con dati infiniti esegue bene entrambi gli obiettivi, un tradeoff può ancora manifestarsi con dati finiti. Inoltre, poiché la nostra costruzione si basa su un problema di apprendimento convesso, escludiamo preoccupazioni di ottimizzazione, mettendo così a nudo una tensione fondamentale tra robustezza e generalizzazione.
In particolare, la recente DenseNet è efficiente nel calcolo e nei parametri, e raggiunge previsioni allo stato dell'arte collegando direttamente ogni strato funzionale a tutti quelli precedenti. Tuttavia, il modello di connettività estrema di DenseNet può ostacolare la sua scalabilità a profondità elevate, e in applicazioni come le reti completamente convoluzionali, le connessioni DenseNet complete sono proibitivamente costose. Questo lavoro mostra sperimentalmente che un vantaggio chiave delle connessioni di salto è quello di avere brevi distanze tra gli strati di feature durante la backpropagation.specificatamente, usando un numero fisso di connessioni di salto, i modelli di connessione con una più breve distanza di backpropagation tra gli strati hanno previsioni più accurate. Seguendo questa intuizione, proponiamo un modello di connessione, Log-DenseNet, che, rispetto a DenseNet, aumenta solo leggermente le distanze di backpropagation tra gli strati da 1 a ($1 + \log_2 L$), ma usa solo $L\log_2 L$ di connessioni totali invece di $O(L^2)$. Quindi, le \logdenses sono più facili da scalare rispetto alle DenseNets, e non richiedono più un'attenta gestione della memoria della GPU.Dimostriamo l'efficacia del nostro principio di progettazione mostrando prestazioni migliori delle DenseNets sulla segmentazione semantica tabula rasa, e risultati competitivi sul riconoscimento visivo.
Tuttavia, i compiti di navigazione sul web sono difficili per gli attuali modelli di deep reinforcement learning (RL) a causa dell'ampio spazio d'azione discreto e del numero variabile di azioni tra gli stati. In questo lavoro, introduciamo DOM-Q-NET, una nuova architettura per la navigazione web basata su RL per affrontare entrambi questi problemi, che parametrizza le funzioni Q con reti separate per diverse categorie di azioni: cliccare su un elemento DOM e digitare una stringa di input.  Il nostro modello utilizza una rete neurale a grafo per rappresentare l'HTML strutturato ad albero di una pagina web standard.  Dimostriamo le capacità del nostro modello nell'ambiente MiniWoB, dove possiamo eguagliare o superare il lavoro esistente senza l'uso di dimostrazioni di esperti. Inoltre, mostriamo 2x miglioramenti nell'efficienza del campione quando ci si allena nell'impostazione multi-task, permettendo al nostro modello di trasferire i comportamenti appresi attraverso i compiti.
Imparare rappresentazioni disentangled che corrispondono a fattori di variazione nei dati del mondo reale è fondamentale per l'apprendimento di macchine interpretabili e controllabili dall'uomo.Recentemente, le preoccupazioni circa la fattibilità di imparare rappresentazioni disentangled in un modo puramente non supervisionato ha stimolato uno spostamento verso l'incorporazione di supervisione debole.Tuttavia, attualmente non esiste un formalismo che identifichi quando e come la supervisione debole garantirà disentanglement. Per affrontare questo problema, forniamo un quadro teorico - compreso un calcolo di disentanglement - per aiutare ad analizzare le garanzie di disentanglement (o la loro mancanza) conferite dalla supervisione debole quando accoppiata con algoritmi di apprendimento basati sulla corrispondenza della distribuzione. Verifichiamo empiricamente le garanzie e i limiti di diversi metodi di supervisione debole (etichettatura limitata, match-pairing e rank-pairing), dimostrando il potere di previsione e l'utilità del nostro quadro teorico.
Nonostante il crescente interesse per l'apprendimento continuo, la maggior parte dei lavori contemporanei sono stati studiati in un contesto piuttosto ristretto in cui i compiti sono chiaramente distinguibili, e i confini dei compiti sono noti durante l'addestramento.Tuttavia, se il nostro obiettivo è quello di sviluppare un algoritmo che impara come gli esseri umani, questa impostazione è tutt'altro che realistica, ed è essenziale sviluppare una metodologia che funzioni in un modo senza compiti.Nel frattempo, tra diversi rami di apprendimento continuo, i metodi basati sull'espansione hanno il vantaggio di eliminare l'oblio catastrofico, assegnando nuove risorse per imparare nuovi dati. Il nostro modello, chiamato Continual Neural Dirichlet Process Mixture (CN-DPM), consiste in un insieme di esperti di reti neurali che sono responsabili di un sottoinsieme dei dati. CN-DPM espande il numero di esperti in un modo basato su principi sotto il quadro Bayesiano nonparametrico.
Stochastic Gradient Descent (SGD) con Nesterov's momentum è un ottimizzatore ampiamente utilizzato nell'apprendimento profondo, che si osserva avere eccellenti prestazioni di generalizzazione.Tuttavia, a causa della grande stocasticità, SGD con Nesterov's momentum non è robusto, cioè, In questo lavoro, proponiamo il Momento di Nesterov ammortizzato, una variante speciale del momento di Nesterov che ha iterati più robusti, una convergenza più veloce nella fase iniziale e una maggiore efficienza.I nostri risultati sperimentali mostrano che questo nuovo momento raggiunge prestazioni di generalizzazione simili (a volte migliori) con poco o nessun tuning.Nel caso convesso, forniamo tassi di convergenza ottimali per i nostri nuovi metodi e discutiamo come i teoremi spiegano i risultati empirici.
Un modello generativo allo stato dell'arte, un "factorized action variational autoencoder (FAVAE)", è presentato per l'apprendimento di rappresentazioni disentangled e interpretabili dai dati sequenziali attraverso il collo di bottiglia dell'informazione senza supervisione.Lo scopo dell'apprendimento della rappresentazione disentangled è quello di ottenere rappresentazioni interpretabili e trasferibili dai dati. Ci siamo concentrati sulla rappresentazione disentangled dei dati sequenziali perché c'è una vasta gamma di applicazioni potenziali se la rappresentazione disentanglement è estesa ai dati sequenziali come i dati video, il discorso e il prezzo delle azioni.I dati sequenziali sono caratterizzati da fattori dinamici e fattori statici: i fattori dinamici sono dipendenti dal tempo, e i fattori statici sono indipendenti dal tempo. I lavori precedenti riescono a separare i fattori statici da quelli dinamici modellando esplicitamente i priori delle variabili latenti per distinguere tra fattori statici e dinamici, ma questo modello non riesce a separare le rappresentazioni tra i fattori dinamici, come la separazione tra "picking" e "throwing" nei compiti robotici. In questo articolo, proponiamo un nuovo modello che può distinguere più fattori dinamici, poiché il nostro metodo non richiede priori di modellazione, ed è in grado di distinguere "tra" fattori dinamici, e negli esperimenti dimostriamo che FAVAE può estrarre i fattori dinamici separati.
Le reti generative sono modelli promettenti per specificare le trasformazioni visive. Sfortunatamente, la certificazione dei modelli generativi è impegnativa, poiché è necessario catturare una sufficiente non-convessità per produrre limiti precisi sull'output. In questo lavoro, presentiamo un nuovo verificatore, chiamato ApproxLine, che può certificare le proprietà non banali delle reti generative.ApproxLine esegue un'interpretazione astratta sia deterministica che probabilistica e cattura set infiniti di output di reti generative.Mostriamo che ApproxLine può verificare interpolazioni interessanti nello spazio latente della rete.
La sintesi video multi vista (MVS) manca dell'attenzione dei ricercatori a causa delle sfide principali delle correlazioni inter-vista e della sovrapposizione delle telecamere.La maggior parte dei lavori MVS precedenti sono offline, si basano solo sulla sintesi, hanno bisogno di una larghezza di banda di comunicazione extra e di tempo di trasmissione e non si concentrano sugli ambienti incerti. A differenza dei metodi esistenti, proponiamo un MVS basato sull'intelligenza del bordo e un riconoscimento delle attività basato sulle caratteristiche spazio-temporali per gli ambienti IoT.Segmentiamo i video multi-vista su ogni dispositivo slave sopra il bordo in scatti utilizzando il modello leggero di rilevamento degli oggetti CNN e calcoliamo le informazioni reciproche tra loro per generare il riepilogo. Il nostro sistema non si basa solo sul riepilogo, ma codificare e trasmettere a un dispositivo master con neural computing stick (NCS) per il calcolo intelligente inter-view correlazioni e riconoscere in modo efficiente le attività, risparmiando così risorse di calcolo, larghezza di banda di comunicazione, e tempo di trasmissione.esperimenti riportano un aumento di 0,4 in F-measure punteggio su MVS Office dataset così come 0. 2% e 2% aumento della precisione di riconoscimento delle attività. La complessità temporale è diminuita da 1,23 a 0,45 secondi per l'elaborazione di un singolo fotogramma, generando così 0,75 secondi di MVS più veloce.Inoltre, abbiamo creato un nuovo set di dati aggiungendo sinteticamente la nebbia a un set di dati MVS per mostrare l'adattabilità del nostro sistema per ambienti di sorveglianza sia certi che incerti.
Un obiettivo centrale nello studio della corteccia visiva dei primati e dei modelli gerarchici per il riconoscimento degli oggetti è capire come e perché le singole unità scambino l'invarianza con la sensibilità alle trasformazioni dell'immagine. Per esempio, sia nelle reti profonde che nella corteccia visiva c'è una sostanziale variazione da strato a strato e da unità a unità nel grado di invarianza di traduzione. La nostra intuizione critica deriva dal fatto che la rettifica diminuisce simultaneamente la varianza delle risposte e la correlazione tra le risposte agli stimoli trasformati, inducendo naturalmente una relazione positiva tra l'invarianza e la gamma dinamica.Le unità di input invarianti tendono quindi a guidare la rete più di quelle sensibili alle piccole trasformazioni dell'immagine.Discutiamo le conseguenze di questa relazione per l'IA: le reti profonde pesano naturalmente le unità invarianti sulle unità sensibili, e questo può essere rafforzato con la formazione, forse contribuendo alle prestazioni di generalizzazione.I nostri risultati prevedono una relazione di firma tra invarianza e gamma dinamica che ora può essere testata in futuri studi neurofisiologici.
Studiamo l'uso dell'architettura Wave-U-Net per il miglioramento del parlato, un modello introdotto da Stoller et al per la separazione di voci musicali e accompagnamento.  Questo metodo di apprendimento end-to-end per la separazione delle fonti audio opera direttamente nel dominio del tempo, permettendo la modellazione integrata delle informazioni di fase ed essendo in grado di prendere in considerazione grandi contesti temporali.  I nostri esperimenti mostrano che il metodo proposto migliora diverse metriche, in particolare PESQ, CSIG, CBAK, COVL e SSNR, rispetto allo stato dell'arte per quanto riguarda il compito di miglioramento del discorso sul corpus Voice Bank (VCTK). Troviamo che un numero ridotto di strati nascosti è sufficiente per il miglioramento del parlato rispetto al sistema originale progettato per la separazione della voce del canto nella musica. Vediamo questo risultato iniziale come un segnale incoraggiante per esplorare ulteriormente il miglioramento del parlato nel dominio del tempo, sia come fine a se stesso che come fase di pre-elaborazione dei sistemi di riconoscimento vocale.
L'apprendimento per rinforzo (RL) è una struttura potente per risolvere i problemi esplorando e imparando dagli errori.Tuttavia, nel contesto del controllo dei veicoli autonomi (AV), richiedere a un agente di commettere errori, o anche permettere errori, può essere piuttosto pericoloso e costoso nel mondo reale.Per questo motivo, AV RL è generalmente solo fattibile in simulazione. Poiché queste simulazioni hanno rappresentazioni imperfette, in particolare per quanto riguarda la grafica, la fisica e l'interazione umana, troviamo la motivazione per un quadro simile a RL, adatto al mondo reale.A tal fine, formuliamo un quadro di apprendimento che impara dall'esplorazione limitata facendo fare l'esplorazione a un dimostratore umano.Il lavoro esistente sull'apprendimento dalla dimostrazione tipicamente o presuppone che i dati raccolti siano eseguiti da un esperto ottimale, o richiede un'esplorazione potenzialmente pericolosa per trovare la politica ottimale. Una delle nostre intuizioni chiave è che il problema diventa trattabile se il punteggio di feedback che valuta la dimostrazione si applica all'azione atomica, al contrario dell'intera sequenza di azioni. Usiamo esperti umani per raccogliere dati di guida e per etichettare i dati di guida attraverso una struttura che chiamiamo ``Backseat Driver'', dandoci coppie stato-azione abbinate a valori scalari che rappresentano il punteggio per l'azione. Chiamiamo il quadro di apprendimento più generale ReNeg, dal momento che impara una regressione dagli stati alle azioni dati esempi negativi e positivi. validiamo empiricamente diversi modelli nel quadro ReNeg, testando il lane-following con dati limitati. troviamo che la soluzione migliore in questo contesto supera la clonazione comportamentale ha forti connessioni con approcci di gradiente di politica stocastica.
Esploriamo l'uso di modelli Vector Quantized Variational AutoEncoder (VQ-VAE) per la generazione di immagini su larga scala. A questo scopo, scaliamo e miglioriamo i priori autoregressivi utilizzati in VQ-VAE per generare campioni sintetici di coerenza e fedeltà molto più elevate di quanto fosse possibile prima.  Usiamo semplici reti di codifica e decodifica feed-forward, quindi il nostro modello è un candidato attraente per le applicazioni in cui la velocità di codifica e decodifica è critica.Inoltre, questo ci permette di campionare solo autoregressivamente nello spazio latente compresso, che è un ordine di grandezza più veloce del campionamento nello spazio dei pixel, specialmente per immagini di grandi dimensioni. Dimostriamo che un'organizzazione gerarchica multiscala di VQ-VAE, aumentata con potenti priori sui codici latenti, è in grado di generare campioni con una qualità che rivaleggia con quella dello stato dell'arte delle Generative Adversarial Networks su set di dati sfaccettati come ImageNet, pur non soffrendo dei noti difetti di GAN come il collasso della modalità e la mancanza di diversità.
Presentiamo un approccio di ricerca ad albero Monte Carlo assistito da una rete neurale a grafo per il classico problema del commesso viaggiatore (TSP). Adottiamo un algoritmo greedy per costruire la soluzione ottimale di TSP aggiungendo i nodi successivamente. Una rete neurale a grafo (GNN) è addestrata per catturare la struttura locale e globale del grafico e dare la probabilità previa di selezionare ogni vertice ad ogni passo. La probabilità a priori fornisce un'euristica per MCTS, e l'output MCTS è una probabilità migliorata per la selezione del vertice successivo, in quanto è l'informazione di ritorno fondendo il priore con la procedura di scouting.I risultati sperimentali su TSP fino a 100 nodi dimostrano che il metodo proposto ottiene tour più brevi di altri metodi basati sull'apprendimento.
Questi modelli rappresentano una molecola come un grafo usando solo la distanza tra gli atomi (nodi) e non la direzione spaziale da un atomo all'altro. Tuttavia, l'informazione direzionale gioca un ruolo centrale nei potenziali empirici per le molecole, ad esempio nei potenziali angolari. Per alleviare questa limitazione proponiamo il passaggio direzionale dei messaggi, in cui incorporiamo i messaggi passati tra gli atomi invece degli atomi stessi. Queste incorporazioni direzionali dei messaggi sono rotativamente equivarianti poiché le direzioni associate ruotano con la molecola. Proponiamo uno schema di passaggio dei messaggi analogo alla propagazione delle credenze, che utilizza le informazioni direzionali trasformando i messaggi in base all'angolo tra loro. Inoltre, usiamo le funzioni sferiche di Bessel per costruire una base radiale ortogonale teoricamente ben fondata che raggiunge prestazioni migliori rispetto alle funzioni a base radiale gaussiane attualmente prevalenti, utilizzando più di 4 volte meno parametri. sfruttiamo queste innovazioni per costruire la rete neurale di passaggio dei messaggi direzionali (DimeNet).
Addestrare un agente a risolvere compiti di controllo direttamente da immagini ad alta densità con l'apprendimento di rinforzo senza modello (RL) si è dimostrato difficile.L'agente ha bisogno di imparare una rappresentazione latente insieme a una politica di controllo per eseguire il compito.Adattare un codificatore ad alta capacità utilizzando un segnale di ricompensa scarso non è solo estremamente inefficiente del campione, ma anche incline alla convergenza subottimale.Due modi per migliorare l'efficienza del campione sono imparare una buona rappresentazione delle caratteristiche e utilizzare algoritmi fuori politica. Sezioniamo vari approcci di apprendimento di buone caratteristiche latenti, e concludiamo che la perdita di ricostruzione dell'immagine è l'ingrediente essenziale che permette l'apprendimento efficiente e stabile della rappresentazione nella RL basata sulle immagini.Seguendo questi risultati, ideiamo un algoritmo off-policy actor-critic con un decodificatore ausiliario che si allena end-to-end e abbina le prestazioni allo stato dell'arte sia con algoritmi senza modello che basati sul modello su molti compiti di controllo impegnativi.Rilasciamo il nostro codice per incoraggiare la ricerca futura sulla RL basata sulle immagini.
 Presentiamo un nuovo operatore di rete neurale, chopout, con il quale le reti neurali vengono addestrate, anche in un singolo processo di addestramento, in modo da troncare le sottoreti per ottenere le migliori prestazioni possibili. Chopout è facile da implementare e da integrare nella maggior parte delle reti neurali esistenti e permette di ridurre le dimensioni delle reti e delle rappresentazioni latenti anche dopo l'addestramento semplicemente troncando gli strati.
Le reti generative avversarie (GAN) hanno dimostrato di produrre immagini sintetiche dall'aspetto realistico con notevole successo, ma le loro prestazioni sembrano meno impressionanti quando il set di allenamento è altamente diversificato. Al fine di fornire un migliore adattamento alla distribuzione dei dati di destinazione quando il set di dati include molte classi diverse, proponiamo una variante del modello GAN di base, una Multi-Modal Gaussian-Mixture GAN (GM-GAN), dove la distribuzione di probabilità sullo spazio latente è una miscela di Gaussiane. Per valutare le prestazioni del modello, proponiamo un nuovo metodo di punteggio che tiene conto separatamente di due misure (tipicamente contrastanti): la diversità e la qualità dei dati generati.  Attraverso una serie di esperimenti, utilizzando sia insiemi di dati sintetici che del mondo reale, dimostriamo quantitativamente che le GM-GAN superano le baseline, sia quando vengono valutate utilizzando il comunemente usato Inception Score, sia quando vengono valutate utilizzando il nostro metodo di punteggio alternativo.Inoltre, dimostriamo qualitativamente come la variante non supervisionata delle GM-GAN tenda a mappare vettori latenti campionati da diverse gaussiane nello spazio latente a campioni di classi diverse nello spazio dei dati. Mostriamo come questo fenomeno può essere sfruttato per il compito di clustering non supervisionato, e forniamo una valutazione quantitativa che mostra la superiorità del nostro metodo per il clustering non supervisionato di dataset di immagini. Infine, dimostriamo una caratteristica che distingue ulteriormente il nostro modello da altri modelli GAN: la possibilità di controllare il trade-off qualità-diversità alterando, post-training, la distribuzione di probabilità dello spazio latente, il che permette di campionare campioni di qualità superiore e diversità inferiore, o viceversa, secondo le proprie esigenze.
L'ottimizzazione distribuita è essenziale per l'addestramento di grandi modelli su grandi insiemi di dati. Sono stati proposti diversi approcci per ridurre l'overhead di comunicazione nell'addestramento distribuito, come la sincronizzazione solo dopo aver eseguito più passi locali di SGD, e metodi decentralizzati (ad es, Sebbene questi metodi siano più veloci dei metodi basati su AllReduce, che utilizzano la comunicazione di blocco prima di ogni aggiornamento, i modelli risultanti possono essere meno accurati dopo lo stesso numero di aggiornamenti.Ispirandoci al metodo BMUF di Chen & Huo (2016), proponiamo un framework slow momentum (SloMo), dove i lavoratori si sincronizzano periodicamente ed eseguono un aggiornamento momentum, dopo molteplici iterazioni di un algoritmo di ottimizzazione di base. Gli esperimenti sulla classificazione delle immagini e sui compiti di traduzione automatica dimostrano che SloMo produce costantemente miglioramenti nell'ottimizzazione e nelle prestazioni di generalizzazione rispetto all'ottimizzatore di base, anche quando l'overhead aggiuntivo viene ammortizzato su molti aggiornamenti in modo che il runtime di SloMo sia alla pari con quello dell'ottimizzatore di base.Forniamo garanzie teoriche di convergenza dimostrando che SloMo converge a un punto stazionario di perdite lisce non convesse.Poiché BMUF è un'istanza particolare del framework SloMo, i nostri risultati corrispondono anche alle prime garanzie teoriche di convergenza per BMUF.
La pianificazione strutturale è importante per la produzione di frasi lunghe, che è una parte mancante negli attuali modelli di generazione linguistica.In questo lavoro, aggiungiamo una fase di pianificazione nella traduzione automatica neurale per controllare la struttura grossolana delle frasi in uscita.Il modello genera prima alcuni codici di pianificazione, poi predice parole reali in uscita condizionate da essi.I codici sono appresi per catturare la struttura grossolana della frase di destinazione. Per imparare i codici, progettiamo una rete neurale end-to-end con un collo di bottiglia di discretizzazione, che predice i tag part-of-speech semplificati delle frasi di destinazione.Gli esperimenti mostrano che le prestazioni di traduzione sono generalmente migliorate dalla pianificazione anticipata.Troviamo anche che traduzioni con strutture diverse possono essere ottenute manipolando i codici di pianificazione.
L'interpolazione dei dati nelle reti neurali profonde è diventata un argomento di notevole interesse per la ricerca.  Dimostriamo che gli autoencoder iperparametrizzati a singolo strato completamente connessi non si limitano a interpolare, ma piuttosto memorizzano i dati di addestramento: producono output in (una versione non lineare di) l'arco degli esempi di addestramento.In contrasto con gli autoencoder completamente connessi, dimostriamo che la profondità è necessaria per la memorizzazione negli autoencoder convoluzionali.  Inoltre, osserviamo che l'aggiunta di non linearità agli autocodificatori convoluzionali profondi risulta in una forma più forte di memorizzazione: invece di produrre punti nell'arco delle immagini di allenamento, gli autocodificatori convoluzionali profondi tendono a produrre singole immagini di allenamento.  Poiché i componenti dell'autocodificatore convoluzionale sono elementi costitutivi delle reti convoluzionali profonde, prevediamo che i nostri risultati faranno luce sull'importante questione del bias induttivo nelle reti profonde iper-parametrizzate.
L'enorme successo delle reti neurali profonde ha motivato la necessità di comprendere meglio le proprietà fondamentali di queste reti, ma molti dei risultati teorici proposti sono stati solo per reti poco profonde. In questo articolo, studiamo una primitiva importante per comprendere lo spazio di input significativo di una rete profonda: il recupero dello span.Per $k<n$, sia $\mathbf{A} \in \mathbb{R}^{k volte n}$ sia la matrice dei pesi più interna di una rete neurale feed forward arbitraria $M: \mathbb{R}^n \mathbb{R}$, così $M(x)$ può essere scritta come $M(x) = \sigma(\mathbf{A} x)$, per qualche rete $sigma: \L'obiettivo è quindi quello di recuperare lo span di riga di $mathbf{A}$ dato solo l'accesso oracolo al valore di $M(x)$. Mostriamo che se $M$ è una rete multistrato con funzioni di attivazione ReLU, allora il recupero parziale è possibile: in particolare, possiamo recuperare in modo dimostrabile $k/2$ vettori linearmente indipendenti nell'arco di riga di $\mathbf{A}$ usando poli$(n)$ query non adattative a $M(x)$.  Inoltre, se $M$ ha funzioni di attivazione differenziabili, dimostriamo che il recupero dello span completo è possibile anche quando l'output è prima passato attraverso una funzione di soglia di segno o di $0/1$; in questo caso il nostro algoritmo è adattivo.Empiricamente, confermiamo che il recupero dello span completo non è sempre possibile, ma solo per strati irrealisticamente sottili. Per reti ragionevolmente ampie, otteniamo un recupero completo dello span sia su reti casuali che su reti addestrate su dati MNIST.Inoltre, dimostriamo l'utilità del recupero dello span come attacco inducendo le reti neurali a classificare erroneamente i dati offuscati da rumore casuale controllato come input sensibili. 
In questo articolo, studiamo la regolarizzazione implicita dell'algoritmo di discesa del gradiente in reti neurali omogenee, comprese le reti neurali completamente connesse e convoluzionali con attivazioni ReLU o LeakyReLU, In particolare studiamo la discesa a gradiente o il gradient flow (cioè la discesa a gradiente con passo infinitesimale) che ottimizza la perdita logistica o la perdita di cross-entropia di qualsiasi modello omogeneo (possibilmente non liscio), e dimostriamo che se la perdita di addestramento diminuisce sotto una certa soglia, allora possiamo definire una versione smussata del margine normalizzato che aumenta nel tempo. I nostri risultati generalizzano i risultati precedenti per la regressione logistica con reti lineari monostrato o multistrato, e forniscono risultati di convergenza più quantitativi con presupposti più deboli rispetto ai risultati precedenti per le reti neurali omogenee lisce.conduciamo diversi esperimenti per giustificare le nostre scoperte teoriche sui set di dati MNIST e CIFAR-10.infine, poiché il margine è strettamente correlato alla robustezza, discutiamo i potenziali benefici dell'allenamento più lungo per migliorare la robustezza del modello.
I modelli ricorrenti standard sono inefficaci poiché sono soggetti alla propagazione dell'errore e non possono catturare efficacemente le correlazioni di ordine superiore. Una soluzione potenziale è quella di estendere i modelli ricorrenti spazio-temporali di ordine superiore. In questo lavoro, proponiamo convolutional tensor-train LSTM (Conv-TT-LSTM), che impara LSTM di ordine superiore (ConvLSTM) in modo efficiente usando la decomposizione convolutional tensor-train (CTTD).Il nostro modello proposto incorpora naturalmente informazioni spazio-temporali di ordine superiore a un piccolo costo di memoria e calcolo usando rappresentazioni efficienti di tensori a basso rango. Valutiamo il nostro modello sui dataset Moving-MNIST e KTH e mostriamo miglioramenti rispetto al ConvLSTM standard e risultati migliori/comparabili ad altri approcci basati su ConvLSTM, ma con molti meno parametri.
Mentre le reti neurali profonde hanno mostrato risultati eccezionali in una vasta gamma di applicazioni, l'apprendimento da un numero molto limitato di esempi è ancora un compito impegnativo.Nonostante le difficoltà dell'apprendimento a pochi colpi, le tecniche di apprendimento metrico hanno mostrato il potenziale delle reti neurali per questo compito.Anche se questi metodi funzionano bene, non forniscono risultati soddisfacenti.In questo lavoro, l'idea di apprendimento metrico è estesa con il meccanismo di lavoro delle Support Vector Machines (SVM), che è ben noto per le capacità di generalizzazione su un piccolo dataset. Inoltre, questo lavoro presenta un quadro di apprendimento end-to-end per la formazione di kernel adattivi SVM, che elimina il problema della scelta di un kerneland corretto di buone caratteristiche per SVMs.Next, il problema di apprendimento one-shot è ridefinito per i segnali audio. Poi il modello è stato testato sul compito di visione (usando Omniglotdataset) e sul compito del discorso (usando TIMIT dataset). In realtà, l'algoritmo che usa Omniglot dataset ha migliorato la precisione dal 98,1% al 98,5% sul compito di classificazione one-shot e dal 98,9% al 99,3% sul compito di classificazione few-shot.
La maggior parte delle strutture CNN 3D esistenti per l'apprendimento della rappresentazione video sono metodi basati su clip, e non considerano l'evoluzione temporale a livello video delle caratteristiche spazio-temporali. In questo articolo, proponiamo reti neurali convoluzionali 4D a livello video, cioè V4D, per modellare l'evoluzione della rappresentazione spazio-temporale a lungo raggio con convoluzioni 4D, oltre a preservare le rappresentazioni spazio-temporali 3D con connessioni residue. Introduciamo inoltre i metodi di addestramento e inferenza per il V4D proposto. esperimenti estesi sono condotti su tre benchmark di riconoscimento video, dove V4D raggiunge risultati eccellenti, superando le recenti CNN 3D con un ampio margine.
Studiamo il problema dell'apprendimento e dell'ottimizzazione attraverso simulazioni fisiche tramite la programmazione differenziabile.Presentiamo DiffSim, un nuovo linguaggio di programmazione differenziabile su misura per la costruzione di simulazioni fisiche differenziabili ad alte prestazioni.Dimostriamo le prestazioni e la produttività del nostro linguaggio in compiti di apprendimento e ottimizzazione basati sul gradiente su 10 diversi simulatori fisici.Per esempio, un simulatore di oggetti elastici differenziabile scritto nel nostro linguaggio è 4. 6x più veloce del simulatore a mano. Infine, condividiamo le lezioni apprese dalla nostra esperienza nello sviluppo di questi simulatori, vale a dire che differenziare i simulatori fisici non sempre produce gradienti utili del sistema fisico che viene simulato.Studiamo sistematicamente le ragioni sottostanti e proponiamo soluzioni per migliorare la qualità dei gradienti.
Le tecniche esistenti sono spesso limitate a un tipo specifico di predittore o basate sulla salienza dell'input, che può essere indesiderabilmente sensibile a fattori estranei al processo decisionale del modello. Noi proponiamo invece sottoinsiemi di input sufficienti che identificano sottoinsiemi minimi di caratteristiche i cui valori osservati da soli sono sufficienti per raggiungere la stessa decisione, anche se tutti gli altri valori delle caratteristiche di input sono mancanti. I principi generali che governano globalmente il processo decisionale di un modello possono essere rivelati anche attraverso la ricerca di cluster di tali modelli di input in molti punti dati. Il nostro approccio è concettualmente semplice, completamente indipendente dal modello, semplicemente implementato utilizzando la selezione all'indietro per istanza, e in grado di produrre razionali più concisi rispetto alle tecniche esistenti.
In questo articolo, affrontiamo il problema di rilevare i campioni che non sono tratti dalla distribuzione di addestramento, cioè Molti studi precedenti hanno tentato di risolvere questo problema considerando i campioni con bassa confidenza di classificazione come esempi OOD utilizzando reti neurali profonde (DNN). Tuttavia, su set di dati difficili o modelli con bassa capacità di classificazione, questi metodi considerano erroneamente i campioni in-distribuzione vicini al confine della decisione come campioni OOD. Questo problema sorge perché i loro approcci utilizzano solo le caratteristiche vicine allo strato di uscita e non tengono conto dell'incertezza delle caratteristiche. Pertanto, proponiamo un metodo che estrae le incertezze delle caratteristiche in ogni strato di DNN utilizzando un trucco di riparametrizzazione e le combina.Negli esperimenti, il nostro metodo supera i metodi esistenti con un ampio margine, raggiungendo prestazioni di rilevamento allo stato dell'arte su diversi set di dati e modelli di classificazione.Ad esempio, il nostro metodo aumenta il punteggio AUROC del lavoro precedente (83,8%) al 99,8% in DenseNet sui set di dati CIFAR-100 e Tiny-ImageNet.
In questo articolo, esploriamo nuovi approcci per combinare le informazioni codificate all'interno delle rappresentazioni apprese degli autoencoder, esploriamo modelli che sono in grado di combinare gli attributi di più ingressi in modo che un output risintetizzato sia addestrato per ingannare un discriminatore avversario per i dati reali rispetto a quelli sintetizzati. Inoltre, esploriamo l'uso di una tale architettura nel contesto dell'apprendimento semi-supervisionato, dove impariamo una funzione di miscelazione il cui obiettivo è quello di produrre interpolazioni di stati nascosti, o combinazioni mascherate di rappresentazioni latenti che sono coerenti con un'etichetta di classe condizionata.Mostriamo prove quantitative e qualitative che tale formulazione è un interessante percorso di ricerca.
In questo lavoro, analizziamo l'incoerenza temporale dei segnali wireless in streaming nel contesto della localizzazione passiva indoor senza dispositivi e dimostriamo che i dati ottenuti dalle informazioni sullo stato del canale WiFi (CSI) possono essere utilizzati per addestrare un sistema robusto in grado di eseguire la localizzazione a livello di stanza. Uno dei problemi più impegnativi per un tale sistema è lo spostamento della distribuzione dei dati di ingresso in uno spazio inesplorato nel tempo, che porta a uno spostamento indesiderato nei confini appresi dello spazio di uscita.In questo lavoro, proponiamo una fase e magnitudine aumentata spazio caratteristica insieme a una tecnica di standardizzazione che è poco influenzata dalle derive.Mostriamo che questa rappresentazione robusta dei dati produce una migliore precisione di apprendimento e richiede meno numero di riqualificazione.
L'apprendimento federato migliora la privacy dei dati e l'efficienza nell'apprendimento automatico eseguito su reti di dispositivi distribuiti, come telefoni cellulari, IoT e dispositivi indossabili, ecc.Eppure i modelli addestrati con l'apprendimento federato possono ancora non riuscire a generalizzare a nuovi dispositivi a causa del problema del domain shift.Domain shift si verifica quando i dati etichettati raccolti dai nodi sorgente differiscono statisticamente dai dati non etichettati del nodo di destinazione. In questo lavoro, presentiamo un approccio di principio al problema dell'adattamento del dominio federato, che mira ad allineare le rappresentazioni apprese tra i diversi nodi con la distribuzione dei dati del nodo di destinazione.Il nostro approccio estende le tecniche di adattamento adversariale ai vincoli dell'impostazione federata.Inoltre, elaboriamo un meccanismo di attenzione dinamica e sfruttiamo il disentanglement delle caratteristiche per migliorare il trasferimento delle conoscenze.Empiricamente, eseguiamo ampi esperimenti su diverse immagini e compiti di classificazione del testo e mostriamo risultati promettenti nell'impostazione di adattamento del dominio federato non supervisionato.
Una capsula è un gruppo di neuroni le cui uscite rappresentano diverse proprietà della stessa entità.Ogni strato in una rete di capsule contiene molte capsule.Descriviamo una versione di capsule in cui ogni capsula ha un'unità logistica per rappresentare la presenza di un'entità e una matrice 4x4 che potrebbe imparare a rappresentare la relazione tra quell'entità e lo spettatore (la posa). Una capsula in uno strato vota la matrice di posa di molte capsule diverse nello strato superiore moltiplicando la propria matrice di posa per matrici di trasformazione viewpoint-invariant addestrabili che potrebbero imparare a rappresentare le relazioni parte-intero. Ognuno di questi voti è ponderato da un coefficiente di assegnazione; questi coefficienti sono aggiornati iterativamente per ogni immagine usando l'algoritmo Expectation-Maximization in modo che l'uscita di ogni capsula sia indirizzata a una capsula nello strato superiore che riceve un gruppo di voti simili. Le matrici di trasformazione sono addestrate in modo discriminatorio tramite il backpropagating attraverso le iterazioni non arrotolate di EM tra ogni coppia di strati adiacenti della capsula.Sul piccolo benchmarkNORB, le capsule riducono il numero di errori di prova del 45% rispetto allo stato dell'arte.Le capsule mostrano anche una resistenza molto maggiore agli attacchi avversari di white box rispetto alla nostra rete neurale convoluzionale di base.
Studiamo una formulazione generale della sintesi del programma chiamata syntax-guided synthesis (SyGuS) che riguarda la sintesi di un programma che segue una data grammatica e soddisfa una data specifica logica.Sia la specifica logica che la grammatica hanno strutture complesse e possono variare da compito a compito, ponendo sfide significative per l'apprendimento attraverso compiti diversi.Inoltre, i dati di formazione sono spesso non disponibili per compiti di sintesi specifici del dominio.Per affrontare queste sfide, proponiamo una struttura di meta-apprendimento che impara una politica trasferibile solo da una supervisione debole. La nostra struttura consiste di tre componenti: 1) un codificatore, che incorpora sia la specifica logica che la grammatica allo stesso tempo usando una rete neurale a grafo; 2) una rete di politica adattiva della grammatica che permette di imparare una politica trasferibile; e 3) un algoritmo di apprendimento di rinforzo che allena congiuntamente l'incorporazione e la politica adattiva. Ne risolve 141 nell'impostazione out-of-box solver, superando significativamente un approccio simile basato sulla ricerca ma senza apprendimento, che ne risolve solo 31. Il risultato è paragonabile a due motori di sintesi classici allo stato dell'arte, che ne risolvono rispettivamente 129 e 153. Nell'impostazione meta-solver, la struttura può adattarsi in modo efficiente a compiti non visti e raggiunge una velocità che va da 2x fino a 100x.
I modelli di traduzione automatica simultanea iniziano a generare una sequenza di destinazione prima di aver codificato o letto la sequenza di origine.Gli approcci recenti per questo compito o applicano una politica fissa sul trasformatore, o un'attenzione monotonica apprendibile su una struttura più debole basata su reti neurali ricorrenti.In questo articolo, proponiamo un nuovo meccanismo di attenzione, Monotonic Multihead Attention (MMA), che ha introdotto il meccanismo di attenzione monotonica all'attenzione multihead. Abbiamo anche introdotto due nuovi approcci interpretabili per il controllo della latenza che sono specificamente progettati per le attenzioni multiple.Applichiamo MMA al compito di traduzione automatica simultanea e dimostriamo migliori compromessi di latenza-qualità rispetto a MILk, il precedente approccio allo stato dell'arte.Il codice sarà rilasciato dopo la pubblicazione.
Questo articolo presenta un nuovo approccio in due fasi per il problema fondamentale dell'apprendimento di una mappa ottimale da una distribuzione a un'altra.In primo luogo, impariamo un piano di trasporto ottimale (OT), che può essere pensato come una mappa uno-a-molti tra le due distribuzioni.A tal fine, proponiamo un approccio duale stocastico di OT regolarizzato, e dimostriamo empiricamente che scala meglio di un recente approccio collegato quando la quantità di campioni è molto grande. In secondo luogo, stimiamo una mappa di Monge come una rete neurale profonda appresa approssimando la proiezione baricentrica del piano OT precedentemente ottenuto.Questa parametrizzazione permette la generalizzazione della mappatura al di fuori del supporto della misura di input.Dimostriamo due risultati teorici di stabilità dell'OT regolarizzato che mostrano che le nostre stime convergono all'OT e alla mappa di Monge tra le misure continue sottostanti.Mostriamo il nostro approccio proposto su due applicazioni: adattamento del dominio e modellazione generativa.
Mentre la celebre tecnica Word2Vec produce rappresentazioni di parole semanticamente ricche, è meno chiaro se le rappresentazioni di frasi o documenti debbano essere costruite su rappresentazioni di parole o da zero. (WMD) che allinea parole semanticamente simili, produce un'accuratezza di classificazione KNN senza precedenti.Tuttavia, WMD è molto costoso da calcolare, ed è più difficile da applicare oltre il semplice KNN rispetto alle feature embeddings.In questo articolo, proponiamo il \emph{Word Mover's Embedding } (La nostra tecnica estende la teoria di \emph{Random Features} per mostrare la convergenza del prodotto interno tra WMEs ad un kernel positivo-definito che può essere interpretato come una versione soft del WMD (inverso).L'incorporazione proposta è più efficiente e flessibile del WMD in molte situazioni. Ad esempio, WME con un semplice classificatore lineare riduce il costo computazionale del KNN basato su WMD da cubico a lineare per la lunghezza del documento e da quadratico a lineare per il numero di campioni, migliorando contemporaneamente l'accuratezza. Negli esperimenti su 9 set di dati di classificazione del testo di riferimento e 22 compiti di somiglianza testuale, la tecnica proposta corrisponde o supera costantemente le tecniche all'avanguardia, con un'accuratezza significativamente maggiore sui problemi di lunghezza ridotta.
Presentiamo un nuovo approccio per valutare la robustezza delle reti neurali basato sulla stima della proporzione di input per i quali una proprietà è violata. In particolare, stimiamo la probabilità dell'evento che la proprietà sia violata sotto un modello di input. Il nostro approccio varia criticamente dal quadro di verifica formale in quanto quando la proprietà può essere violata, fornisce una nozione informativa di quanto sia robusta la rete, piuttosto che la semplice affermazione convenzionale che la rete non è verificabile. Sebbene la struttura fornisca ancora una garanzia formale di soddisfacibilità ogni volta che trova con successo una o più violazioni, questi vantaggi hanno il costo di fornire solo una stima statistica di insoddisfacibilità ogni volta che non viene trovata alcuna violazione. La chiave del successo pratico del nostro approccio è un adattamento della divisione multilivello, un approccio Monte Carlo per stimare la probabilità di eventi rari, alla nostra struttura di robustezza statistica.
I recenti modelli linguistici preaddestrati basati su trasformatori hanno fissato lo stato dell'arte delle prestazioni su vari set di dati NLP.Tuttavia, nonostante il loro grande progresso, soffrono di vari bias strutturali e sintattici.In questo lavoro, indaghiamo il bias di sovrapposizione lessicale, ad es, il modello classifica due frasi che hanno un'alta sovrapposizione lessicale come implicanti indipendentemente dal loro significato sottostante.Per migliorare la robustezza, arricchiamo le frasi di input dei dati di formazione con le loro strutture di predicato-argomento rilevate automaticamente.Questa rappresentazione migliorata permette ai modelli basati sul trasformatore di imparare diversi modelli di attenzione concentrandosi e riconoscendo le parti semanticamente e sintatticamente più importanti delle frasi.  Valutiamo la nostra soluzione per i compiti di inferenza in linguaggio naturale e di inferenza commonsense basata su basi scientifiche utilizzando i modelli BERT, RoBERTa e XLNET, valutando la comprensione dei modelli delle variazioni sintattiche, delle relazioni antonimiche e delle entità nominate in presenza di sovrapposizioni lessicali. I nostri risultati mostrano che l'incorporazione delle strutture di predicato-argomento durante la messa a punto migliora notevolmente la robustezza, ad esempio, circa 20pp sulla discriminazione di diverse entità nominate, mentre non comporta costi aggiuntivi al momento del test e non richiede la modifica del modello o della procedura di formazione.
Proponiamo un metodo per quantificare l'incertezza nei modelli di regressione delle reti neurali quando gli obiettivi sono valori reali su un simplex $d$-dimensionale, come le probabilità. Mostriamo che ogni obiettivo può essere modellato come un campione da una distribuzione di Dirichlet, dove i parametri di Dirichlet sono forniti dall'output di una rete neurale, e che il modello combinato può essere addestrato usando il gradiente della verosimiglianza dei dati. Questo approccio fornisce previsioni interpretabili sotto forma di distribuzioni multidimensionali, piuttosto che stime puntuali, da cui si possono ottenere intervalli di confidenza o quantificare il rischio nel processo decisionale.Inoltre, dimostriamo che lo stesso approccio può essere utilizzato per modellare gli obiettivi sotto forma di conteggi empirici come campioni dalla distribuzione composta Dirichlet-multinomiale.Negli esperimenti, verifichiamo che il nostro approccio fornisce questi benefici senza danneggiare le prestazioni delle previsioni della stima puntuale su due diverse applicazioni: (1) distillazione di reti convoluzionali profonde addestrate su CIFAR-100, e (2) previsione della posizione delle collisioni di particelle nel rivelatore di materia oscura XENON1T.
Le reti neurali profonde hanno raggiunto prestazioni eccezionali in molte applicazioni del mondo reale con la spesa di enormi risorse computazionali.La DenseNet, una delle architetture di rete neurale recentemente proposte, ha raggiunto lo stato dell'arte delle prestazioni in molti compiti visivi.Tuttavia, ha grande ridondanza a causa delle connessioni dense della struttura interna, che porta ad alti costi computazionali nella formazione di tali reti dense. Per affrontare questo problema, progettiamo un quadro di apprendimento di rinforzo per cercare architetture DenseNet efficienti con la potatura a strati (LWP) per diversi compiti, pur mantenendo i vantaggi originali di DenseNet, come il riutilizzo delle caratteristiche, percorsi brevi, ecc. In questa struttura, un agente valuta l'importanza di ogni connessione tra qualsiasi due livelli di blocco, e pota le connessioni ridondanti. Inoltre, viene introdotto un nuovo trucco di reward-shaping per far sì che DenseNet raggiunga un migliore trade-off tra accuratezza e operazioni in virgola mobile (FLOPs).I nostri esperimenti mostrano che DenseNet con LWP è più compatta ed efficiente delle alternative esistenti.   
Consideriamo l'impostazione di un agente con un corpo fisso che interagisce con un mondo esterno sconosciuto e incerto.Mostriamo che i modelli addestrati per prevedere le informazioni propriocettive sul corpo dell'agente arrivano a rappresentare gli oggetti nel mondo esterno.Nonostante siano addestrati solo con segnali disponibili internamente, questi modelli dinamici del corpo arrivano a rappresentare gli oggetti esterni attraverso la necessità di prevedere i loro effetti sul corpo stesso dell'agente.Cioè, il modello impara rappresentazioni olistiche persistenti degli oggetti nel mondo, anche se gli unici segnali di allenamento sono segnali del corpo. Il nostro modello dinamico è in grado di prevedere con successo le distribuzioni di 132 letture di sensori in 100 passi nel futuro e dimostriamo che anche quando il corpo non è più in contatto con un oggetto, le variabili latenti del modello dinamico continuano a rappresentare la sua forma. Dimostriamo che la raccolta attiva di dati massimizzando l'entropia delle previsioni sul corpo - sensori tattili, propriocezione e informazioni vestibolari - porta all'apprendimento di modelli dinamici che mostrano prestazioni superiori quando vengono usati per il controllo. Raccogliamo anche dati da una vera mano robotica e dimostriamo che gli stessi modelli possono essere usati per rispondere a domande sulle proprietà degli oggetti nel mondo reale.
Ispirato dal successo delle reti generative avversarie (GANs) nei domini di immagine, introduciamo un'architettura gerarchica novella per l'apprendimento delle caratteristiche topologiche caratteristiche da un singolo grafico arbitrario dell'input via GANs.The che consiste dell'architettura gerarchica di GANs multiplo conserva sia le caratteristiche topologiche locali che globali e partiziona automaticamente il grafico dell'input nelle fasi rappresentative per l'apprendimento della caratteristica. Le fasi facilitano la ricostruzione e possono essere usate come indicatori dell'importanza delle strutture topologiche associate.Gli esperimenti mostrano che il nostro metodo produce sottografi che conservano una vasta gamma di caratteristiche topologiche, anche nelle prime fasi di ricostruzione.Questo articolo contiene una ricerca originale sulla combinazione dell'uso di GANs e dell'analisi topologica del grafico.
Nelle società cinesi, la superstizione è di fondamentale importanza, e le targhe dei veicoli con numeri desiderabili possono ottenere prezzi molto alti nelle aste.A differenza di altri oggetti di valore, alle targhe non viene assegnato un prezzo stimato prima dell'asta. Propongo che il compito di prevedere i prezzi delle targhe possa essere visto come un compito di elaborazione del linguaggio naturale (NLP), poiché il valore dipende dal significato di ogni singolo carattere sulla targa e dalla sua semantica.Costruisco una rete neurale ricorrente profonda (RNN) per prevedere i prezzi delle targhe dei veicoli a Hong Kong, sulla base dei caratteri su una targa. Valutate su 13 anni di prezzi d'asta storici, le previsioni della RNN profonda possono spiegare oltre l'80% delle variazioni di prezzo, superando i modelli precedenti con un margine significativo.  Dimostro anche come il modello può essere esteso per diventare un motore di ricerca per le targhe e per fornire stime della distribuzione dei prezzi prevista.
Presentiamo un nuovo modello latente di immagini naturali che può essere appreso su set di dati su larga scala. Il processo di apprendimento fornisce un'inclusione latente per ogni immagine nel set di dati di formazione, così come una rete convoluzionale profonda che mappa lo spazio latente nello spazio dell'immagine. Dopo l'addestramento, il nuovo modello fornisce un forte e universale priore d'immagine per una varietà di compiti di restauro dell'immagine come l'inpainting di grandi buchi, la superrisoluzione e la colorazione.Per modellare le immagini naturali ad alta risoluzione, il nostro approccio utilizza spazi latenti di dimensionalità molto alta (da uno a due ordini di grandezza superiori ai precedenti modelli di immagini latenti). Per affrontare questa alta dimensionalità, usiamo spazi latenti con una speciale struttura a collettore (collettori convoluzionali) parametrizzati da una ConvNet di una certa architettura.Negli esperimenti, confrontiamo i modelli latenti appresi con modelli latenti appresi da autoencoder, varianti avanzate di reti generative avversarie, e un forte sistema di base che usa una parametrizzazione più semplice dello spazio latente.Il nostro modello supera gli approcci concorrenti su una gamma di compiti di restauro.
La maggior parte delle recenti soluzioni AES applicano modelli basati su reti neurali profonde (DNN) con regressione, dove il codificatore neurale impara una rappresentazione del saggio che aiuta a differenziare i saggi e il corrispondente punteggio del saggio viene dedotto da un regressore. Tale approccio DNN di solito richiede un sacco di saggi valutati da esperti come dati di allenamento al fine di imparare una buona rappresentazione del saggio per un punteggio accurato, ma tali dati sono di solito costosi e quindi sono scarsi. Ispirandoci all'osservazione che l'uomo di solito assegna un punteggio ad un saggio confrontandolo con alcuni riferimenti, proponiamo una struttura siamese chiamata Referee Network (RefNet) che permette al modello di confrontare la qualità di due saggi catturando le caratteristiche relative che possono differenziare la coppia di saggi. La struttura proposta può essere applicata come un'estensione dei modelli di regressione in quanto può catturare ulteriori caratteristiche relative in cima alle informazioni interne. Inoltre, aumenta intrinsecamente i dati di accoppiamento ed è quindi ideale per gestire la sparsità dei dati. L'esperimento mostra che la nostra struttura può migliorare significativamente i modelli di regressione esistenti e raggiungere prestazioni accettabili anche quando i dati di formazione sono notevolmente ridotti.
Uno dei compiti fondamentali nella comprensione della genomica è il problema di predire i siti di legame del fattore di trascrizione (TFBSs).Con più di centinaia di fattori di trascrizione (TFs) come etichette, la predizione TFBS basata sulla sequenza genomica è un impegnativo compito di classificazione multietichetta.Ci sono due principali meccanismi biologici per il legame TF: (1) modelli di legame specifici della sequenza sui genomi noti come "motivi" e (2) interazioni tra i TF noti come effetti di co-binding.In questo articolo, proponiamo una nuova architettura profonda, la Prototype Matching Network (PMN) per imitare i meccanismi di legame dei TF. Il nostro modello PMN estrae automaticamente i prototipi (caratteristiche simili ai "motivi") per ogni TF attraverso una nuova perdita di corrispondenza dei prototipi. Prendendo in prestito idee dai modelli di corrispondenza a pochi colpi, usiamo la nozione di set di supporto dei prototipi e un LSTM per imparare come le TF interagiscono e si legano alle sequenze genomiche. Su un set di dati TFBS di riferimento con 2,1 milioni di sequenze genomiche, PMN supera significativamente le prestazioni di base e convalida empiricamente le nostre scelte di progettazione.A nostra conoscenza, questa è la prima architettura di deep learning che introduce l'apprendimento dei prototipi e considera le interazioni TF-TF per la previsione TFBS su larga scala.Non solo l'architettura proposta è accurata, ma modella anche la biologia sottostante.
Il lavoro precedente mostra che la generalizzazione adversamente robusta richiede una maggiore complessità del campione, e lo stesso set di dati, ad es, Poiché la raccolta di nuovi dati di formazione potrebbe essere costosa, ci concentriamo su un migliore utilizzo dei dati forniti, inducendo le regioni con alta densità di campioni nello spazio delle caratteristiche, che potrebbe portare a campioni localmente sufficienti per l'apprendimento robusto.Mostriamo prima formalmente che la perdita softmax cross-entropia (SCE) e le sue varianti trasmettono segnali di supervisione inappropriati, che incoraggiano i punti di caratteristica appresi a diffondersi nello spazio in modo sparsamente in formazione. La perdita MMC incoraggia il modello a concentrarsi sull'apprendimento di rappresentazioni ordinate e compatte, che si riuniscono intorno ai centri ottimali preimpostati per le diverse classi. Dimostriamo empiricamente che l'applicazione della perdita MMC può migliorare significativamente la robustezza anche sotto forti attacchi adattativi, mantenendo una precisione allo stato dell'arte su input puliti con pochi calcoli extra rispetto alla perdita SCE.
Presentiamo un quadro analitico per visualizzare e comprendere i GAN a livello di unità, oggetto e scena: per prima cosa identifichiamo un gruppo di unità interpretabili che sono strettamente correlate ai concetti di oggetto con un metodo di dissezione della rete basato sulla segmentazione, quindi esaminiamo l'effetto causale delle unità interpretabili misurando la capacità degli interventi di controllare gli oggetti nell'output. Infine, esaminiamo la relazione contestuale tra queste unità e ciò che le circonda inserendo i concetti di oggetto scoperti in nuove immagini.Mostriamo diverse applicazioni pratiche abilitate dal nostro framework, dal confronto delle rappresentazioni interne tra diversi livelli e modelli, al miglioramento dei GAN individuando e rimuovendo le unità che causano artefatti, alla manipolazione interattiva degli oggetti nella scena.
Presentiamo un semplice approccio nearest-neighbor (NN) che sintetizza immagini fotorealistiche ad alta frequenza da un segnale ``incompleto'' come un'immagine a bassa risoluzione, una mappa normale della superficie o dei bordi: (1) non sono in grado di generare un grande insieme di uscite diverse, a causa del problema del collasso della modalità.(2) non sono interpretabili, rendendo difficile il controllo dell'uscita sintetizzata.Dimostriamo che gli approcci NN potenzialmente affrontano tali limitazioni, ma soffrono di precisione su piccoli set di dati. Progettiamo una semplice pipeline che combina il meglio di entrambi i mondi: la prima fase utilizza una rete neurale convoluzionale (CNN) per mappare l'input in un'immagine (eccessivamente smussata), e la seconda fase utilizza un metodo pixel-wise nearest neighbor per mappare l'output smussato a più output di alta qualità e ad alta frequenza in modo controllabile.importante, la corrispondenza pixel-wise permette al nostro metodo di comporre nuovi contenuti ad alta frequenza tagliando e incollando pixel da diversi esemplari di formazione.  Dimostriamo il nostro approccio per varie modalità di input e per vari domini che vanno da volti umani, animali domestici, scarpe e borse.
Le reti neurali sono vulnerabili agli esempi avversari e i ricercatori hanno proposto molti meccanismi euristici di attacco e difesa. Noi affrontiamo questo problema attraverso la lente di principio dell'ottimizzazione distribuzionalmente robusta, che garantisce le prestazioni sotto le perturbazioni avversarie dell'input.  Considerando una formulazione lagrangiana della penalità di perturbare la distribuzione di dati sottostante in una palla di Wasserstein, forniamo una procedura di formazione che aumenta gli aggiornamenti dei parametri del modello con le perturbazioni del caso peggiore dei dati di formazione. per le perdite lisce, la nostra procedura raggiunge in modo dimostrabile livelli moderati di robustezza con poco costo computazionale o statistico rispetto alla minimizzazione empirica del rischio. inoltre, le nostre garanzie statistiche ci permettono di certificare in modo efficiente la robustezza per la perdita della popolazione. per le perturbazioni impercettibili, il nostro metodo corrisponde o supera i metodi euristici.
La qualità dell'inferenza posteriore è in gran parte determinata da due fattori: a) la capacità della distribuzione variazionale di modellare il vero posteriore e b) la capacità della rete di riconoscimento di generalizzare l'inferenza su tutti i punti dati. Troviamo che l'inferenza subottimale è spesso dovuta all'ammortamento dell'inferenza piuttosto che alla complessità limitata della distribuzione approssimativa. Mostriamo che questo è dovuto in parte all'apprendimento del generatore per adattarsi alla scelta dell'approssimazione. Inoltre, mostriamo che i parametri utilizzati per aumentare l'espressività dell'approssimazione giocano un ruolo nel generalizzare l'inferenza piuttosto che migliorare semplicemente la complessità dell'approssimazione.
In questo articolo, proponiamo un framework che sfrutta i modelli semi-supervisionati per migliorare le prestazioni del clustering non supervisionato.Per sfruttare i modelli semi-supervisionati, abbiamo prima bisogno di generare automaticamente le etichette, chiamate pseudo-etichette.Troviamo che gli approcci precedenti per la generazione di pseudo-etichette danneggiano le prestazioni del clustering a causa della loro bassa accuratezza.Invece, usiamo un ensemble di reti profonde per costruire un grafico di similarità, dal quale estraiamo pseudo-etichette di alta accuratezza. L'approccio di trovare pseudo-etichette di alta qualità usando gli ensemble e addestrando il modello semi-supervisionato viene iterato, producendo un miglioramento continuo. Mostriamo che il nostro approccio supera lo stato dell'arte dei risultati di clustering per più dataset di immagini e testo. Per esempio, raggiungiamo il 54,6% di accuratezza per CIFAR-10 e il 43,9% per 20news, superando lo stato dell'arte dell'8-12% in termini assoluti.
Questo articolo riguarda l'apprendimento del dizionario, vale a dire, Dimostriamo che un algoritmo di discesa subgradiente, con inizializzazione casuale, può recuperare dizionari ortogonali su una formulazione naturale non liscia e non convessa di minimizzazione L1 del problema, sotto una lieve assunzione statistica dei dati. Questo è in contrasto con i metodi dimostrabili precedenti che richiedono calcoli costosi o schemi di inizializzazione delicati. La nostra analisi sviluppa diversi strumenti per caratterizzare i paesaggi di funzioni non lisce, che potrebbero essere di interesse indipendente per l'addestramento dimostrabile di reti profonde con attivazioni non lisce (ad esempio, Esperimenti preliminari sintetici e reali corroborano la nostra analisi e mostrano che il nostro algoritmo funziona bene empiricamente nel recupero di dizionari ortogonali.
Studiamo il recupero del modello per la classificazione dei dati, dove le etichette di addestramento sono generate da una rete neurale completamente connessa a uno strato nascosto con attivazioni sigmoidali, e l'obiettivo è quello di recuperare i vettori di peso della rete neurale. dimostriamo che sotto input gaussiani, la funzione di rischio empirica utilizzando l'entropia incrociata presenta una forte convessità e scorrevolezza uniformemente in un quartiere locale della verità di base, non appena la complessità del campione è sufficientemente grande. Questo implica che se inizializzato in questo quartiere, che può essere raggiunto tramite il metodo tensore, la discesa del gradiente converge linearmente ad un punto critico che è provabilmente vicino alla verità di base senza richiedere un nuovo set di campioni ad ogni iterazione. Al meglio della nostra conoscenza, questa è la prima garanzia di convergenza globale stabilita per la minimizzazione del rischio empirico usando l'entropia incrociata tramite la discesa del gradiente per l'apprendimento di reti neurali ad uno strato nascosto, al campione quasi ottimale e alla complessità computazionale rispetto alla dimensione dell'input della rete.
Con la diffusione delle reti neurali su dispositivi mobili e la necessità di trasmettere le reti neurali su canali limitati o costosi, la dimensione del file del modello addestrato è stata identificata come collo di bottiglia. Proponiamo un codec per la compressione delle reti neurali che si basa sulla codifica di trasformazione per gli strati convoluzionali e densi e sul clustering per i bias e le normalizzazioni. Con questo codec, otteniamo fattori di compressione medi tra 7.9-9.3 mentre la precisione delle reti compresse per la classificazione delle immagini diminuisce solo del 1%-2%, rispettivamente.
Una soluzione pragmatica proviene da Trusted Execution Environments (TEEs), che utilizzano protezioni hardware e software per isolare i calcoli sensibili dallo stack di software non fidato.Tuttavia, queste garanzie di isolamento hanno un prezzo in termini di prestazioni, rispetto alle alternative non fidate. Questo articolo inizia lo studio dell'esecuzione ad alte prestazioni delle Reti Neurali Profonde (DNN) in TEEs, suddividendo in modo efficiente i calcoli DNN tra dispositivi fidati e non fidati.basandosi su un efficiente schema di outsourcing per la moltiplicazione delle matrici, proponiamo Slalom, un framework che delega in modo sicuro l'esecuzione di tutti gli strati lineari in una DNN da una TEE (ad es, Valutiamo Slalom eseguendo DNN in un'enclave Intel SGX, che delega selettivamente il lavoro a una GPU non fidata. Per le DNN canoniche (varianti VGG16, MobileNet e ResNet) otteniamo aumenti da 6x a 20x del throughput per l'inferenza verificabile e da 4x a 11x per l'inferenza verificabile e privata.
Mentre le reti neurali profonde sono una classe di modelli di grande successo, la loro grande impronta di memoria mette a dura prova il consumo di energia, la larghezza di banda di comunicazione e i requisiti di archiviazione, di conseguenza la riduzione delle dimensioni del modello è diventata un obiettivo fondamentale nell'apprendimento profondo. Un approccio tipico è quello di addestrare un insieme di pesi deterministici, applicando alcune tecniche come il pruning e la quantizzazione, in modo che la distribuzione empirica dei pesi diventi adattabile a schemi di codifica in stile Shannon.Tuttavia, come mostrato in questo articolo, rilassare il determinismo dei pesi e utilizzare una distribuzione variazionale completa sui pesi permette schemi di codifica più efficienti e di conseguenza tassi di compressione più elevati. In particolare, seguendo la classica argomentazione del bits-back, codifichiamo i pesi della rete usando un campione casuale, richiedendo solo un numero di bit corrispondente alla divergenza di Kullback-Leibler tra la distribuzione variazionale campionata e la distribuzione di codifica.imponendo un vincolo sulla divergenza di Kullback-Leibler, siamo in grado di controllare esplicitamente il tasso di compressione, mentre ottimizziamo la perdita attesa sul set di allenamento. Si può dimostrare che lo schema di codifica impiegato è vicino al limite inferiore ottimale della teoria dell'informazione, rispetto alla famiglia variazionale impiegata. Il nostro metodo stabilisce un nuovo stato dell'arte nella compressione delle reti neurali, poiché domina strettamente gli approcci precedenti in senso Pareto: Sui benchmark LeNet-5/MNIST e VGG-16/CIFAR-10, il nostro approccio fornisce le migliori prestazioni di prova per un budget di memoria fisso, e viceversa, raggiunge i più alti tassi di compressione per una prestazione di prova fissa.
La maggior parte delle reti neurali esistenti per l'apprendimento dei grafi affronta il problema dell'invarianza di permutazione concependo la rete come uno schema di passaggio di messaggi, in cui ogni nodo somma i vettori di caratteristiche provenienti dai suoi vicini. Noi sosteniamo che questo impone un limite al loro potere di rappresentazione, e proponiamo invece una nuova architettura generale per rappresentare oggetti che consistono in una gerarchia di parti, che chiamiamo reti composizionali covarianti (CCNs). Qui la covarianza significa che l'attivazione di ogni neurone deve trasformarsi in un modo specifico sotto le permutazioni, in modo simile alla sterzabilità nelle CNN. Raggiungiamo la covarianza facendo trasformare ogni attivazione secondo una rappresentazione tensoriale del gruppo di permutazione, e deriviamo le corrispondenti regole di aggregazione tensoriale che ogni neurone deve implementare.Gli esperimenti mostrano che le CCN possono superare i metodi concorrenti su alcuni benchmark standard di apprendimento dei grafi.
 Negli ultimi anni, le reti neurali convoluzionali tridimensionali (3D CNN) sono intensamente applicate nell'analisi video e nel riconoscimento delle azioni e ricevono buone prestazioni.Tuttavia, 3D CNN porta ad un massiccio consumo di calcolo e di memoria, che ostacola la sua distribuzione su dispositivi mobili ed embedded.In questo documento, proponiamo un metodo di potatura basato sulla regolarizzazione tridimensionale per assegnare diversi parametri di regolarizzazione a diversi gruppi di peso in base alla loro importanza per la rete.I nostri esperimenti mostrano che il metodo proposto supera gli altri metodi popolari in questo settore.
In questo articolo, proponiamo le dichiarazioni dei dati come soluzione di design e pratica professionale per i tecnologi dell'elaborazione del linguaggio naturale, sia nella ricerca che nello sviluppo - attraverso l'adozione e l'uso diffuso delle dichiarazioni dei dati, il campo può iniziare ad affrontare le questioni scientifiche ed etiche critiche che derivano dall'uso dei dati di certe popolazioni nello sviluppo della tecnologia per altre popolazioni. Sosteniamo che le dichiarazioni sui dati aiuteranno ad alleviare i problemi relativi all'esclusione e ai pregiudizi nella tecnologia linguistica; a portare a una migliore precisione nelle affermazioni su come la ricerca NLP può generalizzare e quindi a migliori risultati ingegneristici; a proteggere le aziende dall'imbarazzo pubblico; e infine a portare a una tecnologia linguistica che soddisfi i suoi utenti nel loro stile linguistico preferito e inoltre non li rappresenti in modo errato agli altri.** Per apparire in TACL **
Introduciamo CGNN, una struttura per imparare modelli causali funzionali come reti neurali generative. Queste reti sono addestrate usando la backpropagation per minimizzare la massima discrepanza media rispetto ai dati osservati. A differenza degli approcci precedenti, CGNN sfrutta sia le indipendenze condizionali che le asimmetrie distributive per scoprire senza problemi strutture causali bivariate e multivariate, con o senza variabili nascoste. CGNN non stima solo la struttura causale, ma un modello generativo completo e differenziabile dei dati. Attraverso un'ampia varietà di esperimenti, illustriamo i risultati competitivi di CGNN w.r.t le alternative all'avanguardia nella scoperta causale osservazionale sia su dati simulati che reali, nei compiti di inferenza causa-effetto, identificazione della struttura v e scoperta causale multivariata.
La potatura della rete è ampiamente utilizzata per ridurre il pesante costo computazionale dei modelli profondi.Un tipico algoritmo di potatura è una pipeline a tre stadi, cioè, In questo lavoro, facciamo un'osservazione piuttosto sorprendente: il fine-tuning di un modello potato dà solo prestazioni paragonabili o addirittura peggiori rispetto all'allenamento di quel modello con pesi inizializzati in modo casuale. I nostri risultati hanno diverse implicazioni: 1) l'addestramento di un modello grande e iperparametrizzato non è necessario per ottenere un modello finale efficiente, 2) i pesi "importanti" appresi del modello grande non sono necessariamente utili per il piccolo modello potato, 3) l'architettura potata stessa, piuttosto che un insieme di pesi ereditati, è ciò che porta al beneficio di efficienza nel modello finale, il che suggerisce che alcuni algoritmi di potatura potrebbero essere visti come una ricerca dell'architettura di rete.
Il clustering è il compito centrale nell'apprendimento non supervisionato e nel data mining.k-means è uno degli algoritmi di clustering più usati.sfortunatamente, è generalmente non banale estendere k-means per clusterizzare punti di dati oltre la distribuzione gaussiana, in particolare, i cluster con forme non convesse (Beliakov & King, 2006).a questo scopo, noi, per la prima volta, introduciamo la teoria dei valori estremi (EVT) per migliorare la capacità di clustering di k-means. In particolare, lo spazio euclideo è stato trasformato in un nuovo spazio di probabilità denotato come spazio dei valori estremi da EVT.Proponiamo quindi un nuovo algoritmo chiamato Extreme Value k-means (EV k-means), compreso GEV k-means e GPD k-means.Inoltre, introduciamo anche i trucchi per accelerare il calcolo della distanza euclidea per migliorare l'efficienza di calcolo del k-means classico.Inoltre, il nostro EV k-means è esteso ad una versione online, cioè, I risultati sperimentali mostrano che i nostri algoritmi superano significativamente i concorrenti nella maggior parte dei casi.
Tuttavia, i compiti con ricompense sparse rimangono impegnativi quando lo spazio di stato è grande.I compiti orientati all'obiettivo sono tra i problemi più tipici in questo dominio, dove una ricompensa può essere ricevuta solo quando l'obiettivo finale è realizzato.In questo lavoro, proponiamo una potenziale soluzione a tali problemi con l'introduzione di un meccanismo di ricompensa di tendenza basato sull'esperienza, che fornisce all'agente suggerimenti aggiuntivi basati su un apprendimento discriminante sulle esperienze passate durante un curriculum inverso automatico. Questo meccanismo non solo fornisce densi segnali di apprendimento aggiuntivi su quali stati portano al successo, ma permette anche all'agente di conservare solo questa ricompensa di tendenza invece dell'intera storia di esperienze durante l'apprendimento del curriculum multifase. Studiamo ampiamente i vantaggi del nostro metodo sui domini standard a ricompensa sparsa come Maze e Super Mario Bros e dimostriamo che il nostro metodo funziona in modo più efficiente e robusto rispetto agli approcci precedenti in compiti con lunghi orizzonti temporali e grandi spazi di stato. Inoltre, dimostriamo che usando uno schema opzionale di keyframe con una quantità molto piccola di stati chiave, il nostro approccio può risolvere difficili sfide di manipolazione dei robot direttamente dalla percezione e dalle ricompense sparse.
La percezione umana della scena va oltre il riconoscimento di un insieme di oggetti e delle loro relazioni a coppie, comprendiamo regolarità astratte di livello superiore all'interno della scena, come la simmetria e la ripetizione. Gli esperimenti dimostrano che il nostro modello funziona bene su dati sintetici e si trasferisce a immagini reali con tale struttura compositiva.L'uso di programmi di scena ha permesso una serie di applicazioni, come la creazione di analogie visive complesse e l'estrapolazione di scene.
Le reti neurali moderne spesso richiedono composizioni profonde di funzioni non lineari ad alta dimensione (architettura ampia) per raggiungere un'alta accuratezza di prova, e quindi possono avere un numero schiacciante di parametri.L'alto costo ripetuto nella previsione al tempo di prova rende le reti neurali mal adatte a dispositivi con memoria o potenza computazionale limitata. Introduciamo un meccanismo efficiente, la decomposizione tensoriale rimodellata, per comprimere le reti neurali sfruttando tre tipi di strutture invarianti: periodicità, modulazione e basso rango. Il nostro metodo di decomposizione tensoriale rimodellata sfrutta tali strutture invarianti utilizzando una tecnica chiamata tensorizzazione (rimodellare gli strati in tensori di ordine superiore) combinata con decomposizioni tensorie di ordine superiore sopra gli strati tensorizzati. Il nostro metodo di compressione migliora i metodi di approssimazione di basso rango e può essere incorporato (è complementare a) la maggior parte dei metodi di compressione esistenti per le reti neurali per ottenere una migliore compressione. Gli esperimenti su LeNet-5 (MNIST), ResNet-32 (CI- FAR10) e ResNet-50 (ImageNet) dimostrano che la nostra decomposizione tensoriale rimodellata supera (5% di miglioramento della precisione del test universalmente su CIFAR10) lo stato dell'arte delle tecniche di approssimazione di basso rango sotto lo stesso tasso di compressione, oltre a raggiungere ordini di grandezza più veloci tassi di convergenza.
Con l'aumento dell'impiego di metodi di deep learning in scenari critici per la sicurezza, l'interpretabilità è più essenziale che mai. Sebbene siano state esplorate molte direzioni diverse riguardo all'interpretabilità per le modalità visive, i dati delle serie temporali sono stati trascurati con solo una manciata di metodi testati a causa della loro scarsa intelligibilità. Ci avviciniamo al problema dell'interpretabilità in un modo nuovo, proponendo TSInsight, dove attacchiamo al classificatore un autocodificatore con una norma che induce sparsità sulla sua uscita e lo mettiamo a punto in base ai gradienti del classificatore e a una penalità di ricostruzione. In altre parole, chiediamo alla rete di ricostruire solo le parti che sono utili al classificatore, cioè che sono correlate o causali per la predizione. In contrasto con la maggior parte delle altre strutture di attribuzione, TSInsight è in grado di generare sia spiegazioni basate sull'istanza che sul modello. Abbiamo valutato TSInsight insieme ad altri metodi di attribuzione comunemente usati su una serie di diversi set di dati di serie temporali per convalidare la sua efficacia. Inoltre, abbiamo analizzato l'insieme di proprietà che TSInsight raggiunge out of the box tra cui la robustezza avversaria e la contrazione dello spazio di output.
I metodi di riduzione della varianza che utilizzano una miscela di gradienti grandi e piccoli, come SVRG (Johnson & Zhang, 2013) e SpiderBoost (Wang et al., 2018), richiedono significativamente più risorse computazionali per aggiornamento rispetto a SGD (Robbins & Monro, 1951).Riduciamo il costo computazionale per aggiornamento dei metodi di riduzione della varianza introducendo un operatore di gradiente sparso che fonde l'operatore top-K (Stich et al, 2018; Aji & Heafield, 2017) e l'operatore di discesa delle coordinate randomizzate.Mentre il costo computazionale del calcolo della derivata di un parametro del modello è costante, facciamo l'osservazione che i guadagni nella riduzione della varianza sono proporzionali alla grandezza della derivata.In questo articolo, dimostriamo che un gradiente sparso basato sulla grandezza dei gradienti passati riduce il costo computazionale degli aggiornamenti del modello senza una perdita significativa nella riduzione della varianza.Teoricamente, il nostro algoritmo è almeno altrettanto buono come il miglior algoritmo disponibile (e. Empiricamente, il nostro algoritmo supera costantemente SpiderBoost utilizzando vari modelli per risolvere vari compiti di classificazione delle immagini. Forniamo anche prove empiriche per sostenere l'intuizione dietro il nostro algoritmo attraverso un semplice calcolo di entropia del gradiente, che serve a quantificare la sparsità del gradiente ad ogni iterazione.
Nonostante i promettenti progressi sull'imputazione dei dati unimodali (ad esempio l'inpainting dell'immagine), i modelli per l'imputazione dei dati multimodali sono lontani dall'essere soddisfacenti.In questo lavoro, proponiamo il variational selective autoencoder (VSAE) per questo compito. Imparando solo dai dati parzialmente osservati, VSAE può modellare la distribuzione congiunta delle modalità osservate/non osservate e la maschera di imputazione, risultando in un modello unificato per vari compiti down-stream tra cui la generazione dei dati e l'imputazione. La valutazione su set di dati multimodali sintetici ad alta densità e a bassa densità mostra un miglioramento significativo rispetto ai modelli di imputazione allo stato dell'arte.
In molti domini, specialmente nell'analisi del testo aziendale, c'è un'abbondanza di dati che possono essere utilizzati per lo sviluppo di nuove esperienze intelligenti alimentate dall'IA per migliorare la produttività delle persone.Tuttavia, ci sono forti garanzie di privacy che impediscono un ampio campionamento ed etichettatura dei dati di testo personali per imparare o valutare i modelli di interesse.Fortunatamente, in alcuni casi come le e-mail aziendali, l'annotazione manuale è possibile su alcuni set di dati pubblici. La speranza è che i modelli addestrati su questi set di dati pubblici possano funzionare bene sui set di dati privati di interesse.In questo articolo, studiamo le sfide del trasferimento di informazioni da un set di dati e-mail a un altro, per prevedere l'intento dell'utente.In particolare, presentiamo approcci per caratterizzare il gap di trasferimento nei corpora di testo da un punto di vista sia intrinseco che estrinseco, e valutiamo diversi metodi proposti in letteratura per colmare questo gap.Concludiamo sollevando questioni per ulteriori discussioni in questo campo.
Sfortunatamente, la maggior parte dei metodi disaccoppia ancora il processo di acquisizione delle abilità di livello inferiore e l'addestramento di un livello superiore che controlla le abilità in un nuovo compito. Trattare le abilità come fisse può portare ad una significativa sub-ottimalità nell'impostazione di trasferimento. In questo lavoro, proponiamo un nuovo algoritmo per scoprire un insieme di abilità, e adattarle continuamente insieme al livello superiore anche quando si addestra su un nuovo compito. Introduciamo Hierarchical Proximal Policy Optimization (HiPPO), un metodo on-policy per addestrare in modo efficiente tutti i livelli della gerarchia simultaneamente. In secondo luogo, proponiamo un metodo di formazione delle astrazioni temporali che migliora la robustezza delle competenze ottenute ai cambiamenti ambientali.
L'apprendimento può essere inquadrato come il tentativo di codificare l'informazione reciproca tra input e output scartando altre informazioni nell'input.Poiché la distribuzione tra input e output è sconosciuta, anche la vera informazione reciproca lo è.Per quantificare quanto sia difficile imparare un compito, calcoliamo un punteggio di informazione reciproca osservato dividendo l'informazione reciproca stimata per l'entropia dell'input.Sostanziamo questo punteggio analiticamente mostrando che l'informazione reciproca stimata ha un errore che aumenta con l'entropia dei dati. Sperimentalmente analizziamo le rappresentazioni dei dati di input basate su immagini e dimostriamo che i risultati delle prestazioni delle ricerche di architetture di rete estese sono ben allineati al punteggio calcolato, quindi per garantire migliori risultati di apprendimento, le rappresentazioni potrebbero dover essere adattate sia al compito che al modello per allinearsi con la distribuzione implicita del modello.
La sepsi è una complicazione pericolosa per la vita da infezione e una delle principali cause di mortalità negli ospedali.  Mentre la rilevazione precoce della sepsi migliora i risultati dei pazienti, c'è poco consenso sulle linee guida esatte per il trattamento, e il trattamento dei pazienti settici rimane un problema aperto.  In questo lavoro presentiamo un nuovo metodo profondo di apprendimento di rinforzo che usiamo per imparare le politiche di trattamento personalizzate ottimali per i pazienti settici. modelliamo le serie di tempo fisiologiche continuo-valutate del paziente usando i processi gaussiani dell'multiuscita, un modello probabilistico che maneggia facilmente i valori mancanti ed i tempi irregolari di osservazione spaziati mentre mantiene le stime di incertezza. il processo gaussiano Ã¨ legato direttamente ad una rete Q ricorrente profonda che impara le politiche clinicamente interpretabili di trattamento ed entrambi i modelli sono imparati insieme end-to-end.  Valutiamo il nostro approccio su un set di dati eterogeneo di settico che si estende per 15 mesi dal nostro sistema sanitario universitario, e troviamo che la nostra politica appresa potrebbe ridurre la mortalità dei pazienti fino al 8,2% da un tasso di mortalità generale di base del 13,3%.  Il nostro algoritmo potrebbe essere usato per fare raccomandazioni di trattamento ai medici come parte di uno strumento di supporto decisionale, e il quadro si applica facilmente ad altri problemi di apprendimento di rinforzo che si basano su dati di serie temporali multivariati scarsamente campionati e spesso mancanti.
L'apprendimento non supervisionato e semi-supervisionato sono problemi importanti che sono particolarmente impegnativi con dati complessi come le immagini naturali. Il progresso su questi problemi accelererebbe se avessimo accesso a modelli generativi appropriati sotto i quali porre i compiti di inferenza associati. Ispirati dal successo delle Reti Neurali Convoluzionali (CNN) per la predizione supervisionata nelle immagini, progettiamo il Neural Rendering Model (NRM), un nuovo modello generativo probabilistico gerarchico i cui calcoli di inferenza corrispondono a quelli di una CNN. Il NRM introduce un piccolo insieme di variabili latenti ad ogni livello del modello e impone le dipendenze tra tutte le variabili latenti attraverso una distribuzione di priorità coniugata. Il priore coniugato produce un nuovo regolatore per l'apprendimento basato sui percorsi resi nel modello generativo per l'addestramento delle CNN: la Rendering Path Normalization (RPN). dimostriamo che questo regolatore migliora la generalizzazione sia in teoria che in pratica. la stima della probabilità nell'NRM produce la nuova perdita di formazione a entropia incrociata Max-Min, che suggerisce una nuova architettura di rete profonda: la rete Max Min, che supera o corrisponde allo stato dell'arte per l'apprendimento semi-supervisionato e supervisionato su SVHN, CIFAR10 e CIFAR100.
Gli agenti di apprendimento di rinforzo profondo (RL) spesso non riescono a generalizzare ad ambienti non visti (ma semanticamente simili agli agenti addestrati), in particolare quando sono addestrati su spazi di stato ad alta densità, come le immagini. In questo articolo, proponiamo una semplice tecnica per migliorare la capacità di generalizzazione degli agenti RL profondi introducendo una rete neurale randomizzata (convoluzionale) che perturba casualmente le osservazioni di input. Inoltre, consideriamo un metodo di inferenza basato sull'approssimazione Monte Carlo per ridurre la varianza indotta da questa randomizzazione. Dimostriamo la superiorità del nostro metodo in 2D CoinRun, 3D DeepMind Lab exploration e 3D robotics control tasks: supera significativamente vari metodi di regolarizzazione e aumento dei dati per lo stesso scopo.
Le interazioni tattili con gli attuali dispositivi mobili hanno un'espressività limitata. aumentare i dispositivi con ulteriori gradi di libertà può aggiungere potenza all'interazione, e diversi aumenti sono stati proposti e testati. tuttavia, si sa ancora poco sugli effetti dell'apprendimento di più set di interazioni aumentate che sono mappati a diverse applicazioni. per capire meglio se più mappature di comando possono interferire tra loro, o influenzare il trasferimento e la ritenzione, abbiamo sviluppato un prototipo con tre pulsanti su una custodia per smartphone che può essere utilizzata per fornire input aumentati al sistema. Abbiamo mappato questi pulsanti a tre diverse serie di azioni e abbiamo condotto uno studio per vedere se le mappature multiple influenzano l'apprendimento e le prestazioni, il trasferimento e la ritenzione. I nostri risultati mostrano che tutte le mappature sono state apprese rapidamente e non c'è stata alcuna riduzione delle prestazioni con le mappature multiple. Il trasferimento a un compito più realistico ha avuto successo, anche se con una leggera riduzione della precisione.La ritenzione dopo una settimana è stata inizialmente scarsa, ma le prestazioni degli esperti sono state rapidamente ripristinate.Il nostro lavoro fornisce nuove informazioni sulla progettazione e l'uso di input aumentati nelle interazioni mobili.
Le reti neurali sono ampiamente utilizzate nel Natural Language Processing, ma nonostante i loro successi empirici, il loro comportamento è fragile: sono sia sovrasensibili ai piccoli cambiamenti di input, sia poco sensibili alle cancellazioni di grandi frazioni di testo. Questo articolo ha lo scopo di affrontare la sotto-sensibilità nel contesto dell'inferenza del linguaggio naturale, assicurando che i modelli non diventino più fiduciosi nelle loro previsioni quando vengono cancellati sottoinsiemi arbitrari di parole dal testo di input. Sviluppiamo una nuova tecnica per la verifica formale di questa specifica per i modelli basati sul popolare meccanismo di attenzione scomponibile, utilizzando l'efficiente ma efficace approccio IBP (interval bound propagation). Utilizzando questo metodo possiamo dimostrare in modo efficiente, dato un modello, se un particolare campione è libero dal problema della sottosensibilità.Confrontiamo diversi metodi di formazione per affrontare la sottosensibilità, e confrontiamo le metriche per misurarla.Nei nostri esperimenti sui dataset SNLI e MNLI, osserviamo che la formazione IBP porta a una precisione verificata significativamente migliorata.Sul set di test SNLI, possiamo verificare il 18,4% dei campioni, un miglioramento sostanziale rispetto al solo 2,8% utilizzando la formazione standard.
L'addestramento con un numero maggiore di parametri mantenendo iterazioni veloci è una strategia e una tendenza sempre più adottata per lo sviluppo di modelli di reti neurali profonde (DNN) più performanti, il che richiede un maggiore ingombro di memoria e requisiti computazionali per l'addestramento. Chiamiamo questo metodo Shifted and Squeezed FP8 (S2FP8) e mostriamo che, a differenza dei precedenti metodi di addestramento con precisione a 8 bit, il metodo proposto funziona immediatamente per i modelli rappresentativi: ResNet50, Transformer e NCF.Il metodo può mantenere l'accuratezza del modello senza richiedere parametri di scala di perdita fine-tuning o mantenendo alcuni strati in precisione singola.Introduciamo due statistiche imparabili dei tensori DNN - fattori spostati e spremuti che sono usati per regolare in modo ottimale la gamma dei tensori in 8-bit, minimizzando così la perdita di informazioni dovuta alla quantizzazione.
I Variational Auto Encoders (VAE) sono in grado di generare immagini, suoni e sequenze video realistiche.Dal punto di vista dei professionisti, di solito siamo interessati a risolvere problemi in cui i compiti vengono appresi in modo sequenziale, in modo da evitare di rivisitare tutti i dati precedenti in ogni fase.Noi affrontiamo questo problema introducendo un approccio end-to-end concettualmente semplice e scalabile di incorporare la conoscenza del passato imparando i precedenti direttamente dai dati.  Consideriamo un'approssimazione scalabile di tipo boosting per il priore ottimale teorico intrattabile.Forniamo studi empirici su due benchmark comunemente usati, cioè MNIST e Fashion MNIST su compiti di generazione di immagini sequenziali disgiunte.Per ogni set di dati il metodo proposto offre i migliori risultati tra gli approcci comparabili, evitando la dimenticanza catastrofica in modo completamente automatico con un'architettura del modello fissa.
Il campo dell'apprendimento di pochi colpi ha recentemente visto progressi sostanziali. La maggior parte di questi progressi sono venuti dalla fusione dell'apprendimento di pochi colpi come un problema di meta-apprendimento.Model Agnostic Meta Learning o MAML è attualmente uno dei migliori approcci per l'apprendimento di pochi colpi tramite meta-apprendimento. MAML è semplice, elegante e molto potente, tuttavia, ha una serie di problemi, come l'essere molto sensibile alle architetture delle reti neurali, spesso portando all'instabilità durante l'addestramento, richiedendo ardue ricerche di iperparametri per stabilizzare l'addestramento e raggiungere un'alta generalizzazione ed essendo molto costoso dal punto di vista computazionale sia in fase di addestramento che di inferenza.In questo articolo, proponiamo varie modifiche a MAML che non solo stabilizzano il sistema, ma migliorano anche sostanzialmente le prestazioni di generalizzazione, la velocità di convergenza e l'overhead computazionale di MAML, che noi chiamiamo MAML++.
Il rilevamento delle comunità nei grafi è di importanza centrale nel graph mining, nell'apprendimento automatico e nella scienza delle reti. Rilevare le comunità che si sovrappongono è particolarmente impegnativo e rimane un problema aperto.  Motivati dal successo del deep learning basato sui grafi in altri compiti legati ai grafi, studiamo l'applicabilità di questa struttura per il rilevamento di comunità sovrapposte. Proponiamo un modello probabilistico per il rilevamento di comunità sovrapposte basato sull'architettura delle reti neurali a grafo.  Nonostante la sua semplicità, il nostro modello supera gli approcci esistenti nel compito di recupero delle comunità con un ampio margine.  Inoltre, grazie alla formulazione induttiva, il modello proposto è in grado di eseguire il rilevamento di comunità fuori campione per i nodi che non erano presenti al momento della formazione.
La ricerca dell'architettura neurale (NAS) mira a facilitare la progettazione di reti profonde per nuovi compiti.Le tecniche esistenti si basano su due fasi: la ricerca nello spazio dell'architettura e la convalida della migliore architettura.Gli algoritmi NAS sono attualmente confrontati esclusivamente sulla base dei loro risultati sul compito a valle.Anche se intuitivo, questo non riesce a valutare esplicitamente l'efficacia delle loro strategie di ricerca.In questo articolo, proponiamo di valutare la fase di ricerca NAS. A questo scopo, confrontiamo la qualità delle soluzioni ottenute dalle politiche di ricerca NAS con quella della selezione casuale dell'architettura.Troviamo che:(i) in media, gli algoritmi NAS allo stato dell'arte si comportano in modo simile alla politica casuale;(ii) la strategia di condivisione dei pesi, ampiamente utilizzata, degrada la classifica dei candidati NAS al punto da non riflettere la loro vera performance, riducendo così l'efficacia del processo di ricerca.Crediamo che il nostro quadro di valutazione sarà la chiave per progettare strategie NAS che scoprano costantemente architetture superiori a quelle casuali.
In questo articolo utilizziamo le proprietà geometriche del problema del trasporto ottimale (OT) e le distanze di Wasserstein per definire una distribuzione a priori per lo spazio latente di un autocodificatore. Introduciamo gli Sliced-Wasserstein Auto-Encoders (SWAE), che permettono di modellare la distribuzione dello spazio latente in qualsiasi distribuzione di probabilità campionabile senza la necessità di addestrare una rete avversaria o di avere una funzione di probabilità specificata. In breve, regolarizziamo la perdita dell'autocodificatore con la distanza di Wasserstein a fette tra la distribuzione dei campioni di formazione codificati e una distribuzione a priori campionabile. Mostriamo che la formulazione proposta ha una soluzione numerica efficiente che fornisce capacità simili agli Auto-Encoders di Wasserstein (WAE) e agli Auto-Encoders Variazionali (VAE), mentre beneficia di un'implementazione imbarazzantemente semplice.
Il formalismo hamiltoniano gioca un ruolo centrale nella fisica classica e quantistica: gli hamiltoniani sono lo strumento principale per modellare l'evoluzione temporale continua dei sistemi con quantità conservate, e sono dotati di molte proprietà utili, come la reversibilità temporale e l'interpolazione regolare nel tempo. Queste proprietà sono importanti per molti problemi di apprendimento automatico - dalla previsione delle sequenze all'apprendimento di rinforzo e alla modellazione della densità - ma non sono tipicamente fornite fuori dalla scatola da strumenti standard come le reti neurali ricorrenti. In questo articolo, introduciamo la rete generativa hamiltoniana (HGN), il primo approccio capace di apprendere in modo coerente le dinamiche hamiltoniane da osservazioni ad alta dimensione (come le immagini) senza presupposti di dominio restrittivi. Una volta addestrato, possiamo usare HGN per campionare nuove traiettorie, eseguire rollout sia in avanti che indietro nel tempo, e persino accelerare o rallentare le dinamiche apprese.Dimostriamo come una semplice modifica dell'architettura di rete trasforma HGN in un potente modello di flusso normalizzante, chiamato Neural Hamiltonian Flow (NHF), che usa le dinamiche hamiltoniane per modellare densità espressive.Quindi, speriamo che il nostro lavoro serva come prima dimostrazione pratica del valore che il formalismo hamiltoniano può portare al machine learning.Ulteriori risultati e valutazioni video sono disponibili su: http://tiny.cc/hgn
La migrazione del cloud trasforma i dati, le applicazioni e i servizi del cliente dalla piattaforma IT originale a uno o più ambienti cloud, con l'obiettivo di migliorare le prestazioni del sistema IT e ridurre i costi di gestione dell'IT. I progetti di migrazione del cloud a livello aziendale sono generalmente complessi e comportano la pianificazione e la ripianificazione dinamica di vari tipi di trasformazioni per un massimo di 10k endpoint. Attualmente la pianificazione e la ripianificazione in Cloud Migration sono generalmente fatte manualmente o semi-manualmente con una forte dipendenza dalla conoscenza del dominio dell'esperto di migrazione, che richiede giorni o addirittura settimane per ogni round di pianificazione o ripianificazione.Di conseguenza, il motore di pianificazione automatizzato che è in grado di generare un piano di migrazione di alta qualità in breve tempo è particolarmente desiderabile per l'industria della migrazione.In questo breve documento, introduciamo brevemente i vantaggi di utilizzare la pianificazione AI in Cloud Migration, un prototipo preliminare, così come le sfide che richiede attenzione dalla società di pianificazione e programmazione.
Unsupervised domain adaptation mira a generalizzare l'ipotesi addestrata in un dominio di origine a un dominio di destinazione non etichettato.Un approccio popolare a questo problema è quello di imparare una rappresentazione invariante al dominio per entrambi i domini.In questo lavoro, studiamo, teoricamente ed empiricamente, l'effetto esplicito dell'embedding sulla generalizzazione al dominio di destinazione.In particolare, la complessità della classe di embeddings influenza un upper bound sul rischio del dominio di destinazione.Questo si riflette anche nei nostri esperimenti.
Il dominio della previsione delle serie temporali è stato ampiamente studiato perché è di fondamentale importanza in molte applicazioni della vita reale.La previsione del tempo, la previsione del flusso di traffico o le vendite sono esempi convincenti di fenomeni sequenziali.I modelli predittivi generalmente fanno uso delle relazioni tra valori passati e futuri.Tuttavia, nel caso delle serie temporali stazionarie, i valori osservati dipendono anche drasticamente da una serie di caratteristiche esogene che possono essere utilizzate per migliorare la qualità della previsione. In questo lavoro, proponiamo un cambio di paradigma che consiste nell'apprendere tali caratteristiche in vettori di embeddings all'interno di reti neurali ricorrenti. Applichiamo il nostro quadro per prevedere i registri di spillatura delle smart card nella rete della metropolitana parigina.I risultati mostrano che i modelli context-embedded eseguono quantitativamente meglio nella previsione one-step ahead e multi-step ahead.
La capacità di un agente di scoprire i propri obiettivi di apprendimento è stata a lungo considerata un ingrediente chiave per l'intelligenza artificiale generale. Le scoperte nel processo decisionale autonomo e nell'apprendimento di rinforzo sono state principalmente in domini in cui l'obiettivo dell'agente è delineato e chiaro: come giocare un gioco per vincere, o guidare in modo sicuro. Diversi studi hanno dimostrato che l'apprendimento di sotto-compiti extramurali e di previsioni ausiliarie può migliorare (1) l'apprendimento di un singolo compito specificato dall'uomo, (2) il trasferimento dell'apprendimento, (3) e la rappresentazione del mondo appresa dall'agente.In tutti questi esempi, l'agente ha ricevuto istruzioni su cosa imparare. In particolare, il nostro sistema mantiene una grande collezione di previsioni, potando e sostituendo continuamente le previsioni. Evidenziamo l'importanza di considerare la stabilità piuttosto che la convergenza per un tale sistema, e sviluppiamo un algoritmo adattivo e regolarizzato a tale scopo.
Stimare il luogo in cui è stata scattata un'immagine basandosi solo sul contenuto dell'immagine è un compito impegnativo, anche per gli esseri umani, poiché etichettare correttamente un'immagine in questo modo si basa molto sulle informazioni contestuali, e non è semplice come identificare un singolo oggetto nell'immagine. Questo lavoro contribuisce allo stato della ricerca nell'inferenza di geolocalizzazione dell'immagine introducendo una nuova strategia di meshing globale, delineando una varietà di procedure di formazione per superare le notevoli limitazioni di dati quando si addestrano questi modelli, e dimostrando come l'incorporazione di informazioni aggiuntive può essere utilizzata per migliorare le prestazioni complessive di un modello di inferenza di geolocalizzazione.In questo lavoro, è dimostrato che i triangoli Delaunay sono un tipo efficace di maglia per la geolocalizzazione in scenari di volume relativamente basso rispetto ai risultati dello stato dell'arte dei modelli che utilizzano alberi quad e un ordine di grandezza più dati di formazione.  Inoltre, il tempo di pubblicazione, l'albuming dell'utente appreso e altri metadati sono facilmente incorporati per migliorare la geolocalizzazione fino all'11% per la precisione di località a livello di paese (750 km) e al 3% per località a livello di città (25 km).
I metodi bayesiani gerarchici hanno il potenziale per unificare molti compiti correlati (ad esempio la classificazione k-shot, la generazione condizionale e incondizionata) inquadrando ciascuno come inferenza all'interno di un singolo modello generativo. Mostriamo che gli approcci esistenti per l'apprendimento di tali modelli possono fallire su reti generative espressive come PixelCNNs, descrivendo la distribuzione globale con poco affidamento sulle variabili latenti. Per risolvere questo problema, sviluppiamo una modifica del Variational Autoencoder in cui le osservazioni codificate vengono decodificate in nuovi elementi della stessa classe; il risultato, che chiamiamo Variational Homoencoder (VHE), può essere inteso come l'addestramento di un modello gerarchico a variabili latenti che utilizza meglio le variabili latenti in questi casi. L'utilizzo di questa struttura ci permette di addestrare una PixelCNN gerarchica per il set di dati Omniglot, superando tutti i modelli esistenti sulla verosimiglianza del set di test. Con un singolo modello raggiungiamo sia una forte generazione di un solo colpo che una classificazione vicina al livello umano, competitiva con i classificatori discriminativi allo stato dell'arte. L'obiettivo VHE si estende naturalmente a strutture di set di dati più ricchi, come categorie fattoriali o gerarchiche, come illustriamo addestrando modelli per separare il contenuto dei caratteri da semplici variazioni nello stile di disegno, e per generalizzare lo stile di un alfabeto a nuovi caratteri.
Il senso comune o la conoscenza di fondo sono necessari per capire il linguaggio naturale, ma nella maggior parte dei sistemi neurali di comprensione del linguaggio naturale (NLU), la conoscenza di fondo richiesta è acquisita indirettamente da corpora statici. Un nuovo modulo di lettura task-agnostic fornisce rappresentazioni di parole raffinate a un'architettura NLU task-specific elaborando la conoscenza di fondo sotto forma di dichiarazioni a testo libero, insieme agli input task-specific.Le forti prestazioni sui compiti di risposta alle domande dei documenti (DQA) e di riconoscimento dell'implicazione testuale (RTE) dimostrano l'efficacia e la flessibilità del nostro approccio.L'analisi mostra che i nostri modelli imparano a sfruttare la conoscenza selettivamente e in modo semanticamente appropriato.
Nonostante i vantaggi dell'aritmetica a precisione mista in termini di riduzione della necessità di risorse chiave come la larghezza di banda della memoria o la dimensione del file di registro, essa ha una capacità limitata di diminuire i costi di calcolo e richiede 32 bit per rappresentare i suoi operandi in uscita.Questo articolo propone due approcci per sostituire l'aritmetica a precisione mista con quella a mezza precisione durante gran parte dell'allenamento. Il primo approccio raggiunge rapporti di accuratezza leggermente più lenti dello stato dell'arte utilizzando l'aritmetica a mezza precisione durante più del 99% dell'addestramento.Il secondo approccio raggiunge la stessa accuratezza dello stato dell'arte passando dinamicamente dall'aritmetica a mezza precisione a quella a precisione mista durante l'addestramento.Utilizza l'aritmetica a mezza precisione durante più del 94% del processo di addestramento.Questo articolo è il primo a dimostrare che la mezza precisione può essere utilizzata per una parte molto ampia dell'addestramento delle DNN e raggiungere ancora l'accuratezza dello stato dell'arte.
Introduciamo la "inverse square root linear unit" (ISRLU) per accelerare l'apprendimento nelle reti neurali profonde.ISRLU ha prestazioni migliori di ELU ma ha molti degli stessi benefici.ISRLU e ELU hanno curve e caratteristiche simili. Entrambi hanno valori negativi, permettendo loro di spingere l'attivazione media dell'unità più vicina allo zero, e portare il gradiente normale più vicino al gradiente naturale dell'unità, assicurando uno stato di disattivazione robusto dal punto di vista del rumore, diminuendo il rischio di over fitting.Il significativo vantaggio prestazionale di ISRLU sulle CPU tradizionali si trasferisce anche a implementazioni HW più efficienti nel codesign HW/SW per CNNs/RNNNs. Negli esperimenti con TensorFlow, ISRLU porta ad un apprendimento più veloce e ad una migliore generalizzazione rispetto a ReLU su CNNs.Questo lavoro suggerisce anche una variante computazionalmente efficiente chiamata "inverse square root unit" (ISRU) che può essere utilizzata per RNNs.Many RNNs utilizzare sia la memoria a lungo termine (LSTM) e gated recurrent units (GRU) che sono implementati con funzioni di attivazione tanh e sigmoid.ISRU ha meno complessità computazionale ma ha ancora una curva simile a tanh e sigmoid.
Un discente generalmente intelligente dovrebbe generalizzare ai compiti piÃ¹ complessi che ha incontrato precedentemente, ma i due paradigmi comuni nell'apprendimento automatico -- o addestrando un discente separato per compito o addestrando un singolo discente per tutti i compiti -- entrambi hanno difficoltÃ con tale generalizzazione perchÃ© non sfruttano la struttura compositiva della distribuzione di compito. Questo articolo introduce il grafico compositivo del problema come un formalismo ampiamente applicabile per riferire i compiti della complessitÃ differente in termini di problemi con i sottoproblemi comuni. Come primo passo per affrontare la generalizzazione compositiva, introduciamo l'apprendista ricorsivo compositivo, un quadro generale di dominio per l'apprendimento di procedure algoritmiche per comporre trasformazioni di rappresentazione, producendo un apprendista che ragiona su quale calcolo eseguire facendo analogie con problemi visti in precedenza.
Il metodo del collo di bottiglia dell'informazione fornisce un metodo teorico dell'informazione per l'apprendimento della rappresentazione, addestrando un codificatore a mantenere tutte le informazioni che sono rilevanti per prevedere l'etichetta, minimizzando la quantità di altre informazioni superflue nella rappresentazione.La formulazione originale, tuttavia, richiede dati etichettati per identificare quali informazioni sono superflue.  In questo lavoro, estendiamo questa capacità all'impostazione multi-vista non supervisionata, in cui vengono fornite due viste della stessa entità sottostante, ma l'etichetta è sconosciuta, il che ci permette di identificare le informazioni superflue come quelle che non sono condivise da entrambe le viste. Un'analisi teorica porta alla definizione di un nuovo modello multi-vista che produce risultati all'avanguardia sul dataset Sketchy e sulle versioni con etichetta limitata del dataset MIR-Flickr.  Estendiamo inoltre la nostra teoria all'impostazione single-view sfruttando le tecniche standard di incremento dei dati, mostrando empiricamente migliori capacità di generalizzazione rispetto ai tradizionali approcci non supervisionati per l'apprendimento della rappresentazione.
La plausibilità biologica dell'algoritmo di backpropagation è stata a lungo messa in dubbio dai neuroscienziati: due motivi principali sono che i neuroni avrebbero bisogno di inviare due diversi tipi di segnale nelle fasi avanti e indietro, e che le coppie di neuroni dovrebbero comunicare attraverso connessioni bidirezionali simmetriche. Presentiamo una semplice procedura di apprendimento a due fasi per reti ricorrenti a punto fisso che affronta entrambi questi problemi. Come conseguenza di questa generalizzazione, l'algoritmo non calcola il vero gradiente della funzione obiettivo, ma piuttosto lo approssima con una precisione che si dimostra essere direttamente correlata al grado di simmetria dei pesi di feedforward e di feedback, dimostrando sperimentalmente che le proprietà intrinseche del sistema portano all'allineamento dei pesi di feedforward e di feedback, e che il nostro algoritmo ottimizza la funzione obiettivo.
In questo articolo presentiamo, al meglio delle nostre conoscenze, il primo metodo per imparare un modello generativo di forme 3D da immagini naturali in modo completamente non supervisionato. Il nostro approccio segue la strategia generale delle Generative Adversarial Networks, dove una rete generatrice di immagini impara a creare campioni di immagini che sono abbastanza realistici da ingannare una rete discriminatrice a credere che siano immagini naturali.Al contrario, nel nostro approccio la genesi dell'immagine è divisa in 2 fasi.Nella prima fase una rete generatrice produce degli oggetti 3D.Nella seconda, un renderer differenziabile produce un'immagine dell'oggetto 3D da un punto di vista casuale. L'osservazione chiave è che un oggetto 3D realistico dovrebbe produrre un rendering realistico da qualsiasi punto di vista plausibile, quindi, randomizzando la scelta del punto di vista, l'addestramento proposto costringe la rete generatrice ad apprendere una rappresentazione 3D interpretabile disgiunta dal punto di vista. In questo lavoro, una rappresentazione 3D consiste in una mesh triangolare e una mappa di texture che viene utilizzata per colorare la superficie del triangolo utilizzando la tecnica UV-mapping.Forniamo un'analisi del nostro approccio di apprendimento, esponiamo le sue ambiguità e mostriamo come superarle.Sperimentalmente, dimostriamo che il nostro metodo può imparare forme 3D realistiche di volti utilizzando solo le immagini naturali del dataset FFHQ.
Ci concentriamo sul problema degli attacchi avversari black-box, dove l'obiettivo è quello di generare esempi avversari utilizzando informazioni limitate alle valutazioni delle funzioni di perdita delle coppie input-output.Usiamo l'ottimizzazione bayesiana (BO) per adattarci specificamente agli scenari che coinvolgono bassi budget di query per sviluppare attacchi avversari efficienti. Il nostro approccio proposto raggiunge prestazioni paragonabili allo stato dell'arte degli attacchi black-box adversarial, anche se con un numero medio di query molto inferiore. In particolare, in regimi di budget di query bassi, il nostro metodo proposto riduce il numero di query fino all'80% rispetto allo stato dell'arte dei metodi.
Le reti neurali profonde (DNN) allo stato dell'arte hanno tipicamente decine di milioni di parametri, che potrebbero non rientrare nei livelli superiori della gerarchia della memoria, aumentando così il tempo di inferenza e il consumo di energia in modo significativo, e proibendo il loro utilizzo su dispositivi edge come i telefoni cellulari. La compressione dei modelli DNN è quindi diventata un'area attiva di ricerca di recente, con il pruning delle connessioni che emerge come una delle strategie di maggior successo. Un approccio molto naturale è quello di potare le connessioni delle DNN attraverso la regolarizzazione $ell_1$, ma recenti indagini empiriche hanno suggerito che questo non funziona così bene nel contesto della compressione DNN. In questo lavoro, rivisitiamo questa semplice strategia e la analizziamo rigorosamente, per dimostrare che:(a) qualsiasi \emph{punto stazionario} di un obiettivo $\ell_1$-regolarizzato layerwise-pruning ha il suo numero di elementi non nulli delimitato dal numero di logit di predizione penalizzati, indipendentemente dalla forza della regolarizzazione;(b) un pruning di successo si basa altamente su un solutore di ottimizzazione accurato, e c'è un trade-off tra la velocità di compressione e la distorsione della precisione di predizione, controllata dalla forza della regolarizzazione. I nostri risultati teorici suggeriscono quindi che il pruning di $\ell_1$ potrebbe avere successo a condizione di utilizzare un solutore di ottimizzazione accurato, e lo confermiamo nei nostri esperimenti, dove dimostriamo che la semplice regolarizzazione di $\ell_1$ con un solutore Adamax-L1(cumulativo) dà un rapporto di pruning competitivo con lo stato dell'arte.
L'autonomia e l'adattamento delle macchine richiedono che esse siano in grado di misurare i propri errori.Consideriamo i vantaggi e i limiti di un tale approccio quando una macchina deve misurare l'errore in un compito di regressione.Come può una macchina misurare l'errore delle sotto-componenti di regressione quando non ha la verità di base per le previsioni corrette?Un approccio di rilevamento compresso applicato al segnale di errore dei regressori può recuperare il loro errore di precisione senza alcuna verità di base. Le sue soluzioni, tuttavia, non sono uniche - una proprietà delle soluzioni di inferenza della verità a terra.Aggiungendo $\ell_1$--minimizzazione come condizione si può recuperare la soluzione corretta nelle impostazioni in cui la correzione degli errori è possibile.Discutiamo brevemente la somiglianza della matematica dell'inferenza della verità a terra per i regressori con quella per i classificatori.
Proponiamo una nuova rappresentazione, one-pixel signature, che può essere utilizzata per rivelare le caratteristiche delle reti neurali di convoluzione (CNN).Qui, ogni classificatore CNN è associato a una firma che viene creata generando, pixel per pixel, un valore avversario che è il risultato del più grande cambiamento alla previsione di classe. La firma di un pixel è agnostica alle scelte di progettazione delle architetture CNN, come il tipo, la profondità, la funzione di attivazione, e come sono state addestrate, e può essere calcolata in modo efficiente per un classificatore black-box senza accedere ai parametri della rete. Reti classiche come LetNet, VGG, AlexNet, e ResNet dimostrano diverse caratteristiche nelle loro immagini di firma.Per l'applicazione, ci concentriamo sul problema di rilevamento del classificatore backdoor in cui un classificatore CNN è stato inserito maliziosamente con un Trojan sconosciuto.Mostriamo l'efficacia della firma a un pixel nel rilevare CNN backdoored.La nostra proposta di rappresentazione della firma a un pixel è generale e può essere applicata in problemi in cui i classificatori discriminativi, in particolare basati su reti neurali, devono essere caratterizzati.
Estendiamo il paradigma dell'apprendimento dalla dimostrazione fornendo un metodo per l'apprendimento di vincoli sconosciuti condivisi tra le attività, utilizzando le dimostrazioni delle attività, le loro funzioni di costo e la conoscenza delle dinamiche del sistema e dei vincoli di controllo.Date le dimostrazioni sicure, il nostro metodo utilizza il campionamento hit-and-run per ottenere traiettorie a basso costo, e quindi non sicure. Entrambe le traiettorie sicure e non sicure sono utilizzate per ottenere una rappresentazione coerente dell'insieme non sicuro attraverso la risoluzione di un programma intero misto.Inoltre, sfruttando una parametrizzazione nota del vincolo, modifichiamo il nostro metodo per imparare i vincoli parametrici in alte dimensioni.Mostriamo che il nostro metodo può imparare un vincolo di posa a sei dimensioni per un braccio robotico 7-DOF.
Proponiamo una struttura di apprendimento metrico per il calcolo di mappe che preservano la distanza e che generano embeddings a bassa dimensione per una certa classe di manifold. Impieghiamo reti siamesi per risolvere il problema del minimo quadrato di scalatura multidimensionale per generare mappature che preservano le distanze geodetiche sul manifold. Inoltre, l'uso di una rete per modellare la mappa che conserva la distanza riduce la complessità del problema di scaling multidimensionale e porta a una migliore generalizzazione non locale del manifold rispetto ad analoghe controparti non parametriche. Dimostriamo le nostre affermazioni su dati point-cloud e su manifold di immagini e mostriamo un'analisi numerica della nostra tecnica per facilitare una maggiore comprensione del potere di rappresentazione delle reti neurali nella modellazione di dati manifold.
L'esplorazione è un aspetto fondamentale del Reinforcement Learning, tipicamente implementato usando la selezione stocastica dell'azione.L'esplorazione, tuttavia, può essere più efficiente se diretta ad acquisire nuova conoscenza del mondo.I contatori di visite si sono dimostrati utili sia nella pratica che nella teoria per l'esplorazione diretta.Tuttavia, una grande limitazione dei contatori è la loro località.Mentre ci sono alcune soluzioni basate sul modello a questa lacuna, manca ancora un approccio senza modello. Proponiamo $E$-valori, una generalizzazione dei contatori che può essere usata per valutare il valore esplorativo che si propaga sulle traiettorie stato-azione.Confrontiamo il nostro approccio con le tecniche RL comunemente usate, e dimostriamo che l'uso di $E$-valori migliora l'apprendimento e le prestazioni rispetto ai contatori tradizionali.Mostriamo anche come il nostro metodo possa essere implementato con l'approssimazione delle funzioni per imparare in modo efficiente gli MDP continui.Dimostriamo questo dimostrando che il nostro approccio supera le prestazioni dello stato dell'arte nel gioco Freeway Atari 2600.
La neuroevoluzione profonda e gli algoritmi di deep reinforcement learning (deep RL) sono due approcci popolari per la ricerca di policy.Il primo è ampiamente applicabile e piuttosto stabile, ma soffre di una bassa efficienza del campione.Al contrario, il secondo è più efficiente del campione, ma le varianti più efficienti del campione sono anche piuttosto instabili e altamente sensibili all'impostazione degli iper-parametri.Finora, queste famiglie di metodi sono state per lo più confrontate come strumenti concorrenti.Tuttavia, un approccio emergente consiste nel combinarli in modo da ottenere il meglio dei due mondi. Due combinazioni precedentemente esistenti usano un algoritmo evolutivo ad hoc o un processo di esplorazione degli obiettivi insieme all'algoritmo Deep Deterministic Policy Gradient (DDPG), un algoritmo RL profondo off-policy efficiente.In questo articolo, proponiamo un diverso schema di combinazione usando il semplice cross-entropymethod (CEM) e Twin Delayed Deep Deterministic policy gradient (TD3), un altro algoritmo RL profondo off-policy che migliora il DDPG. Valutiamo il metodo risultante, CEM-RL, su un set di benchmark classicamente utilizzati nella RL profonda e dimostriamo che CEM-RL beneficia di diversi vantaggi rispetto ai suoi concorrenti e offre un compromesso soddisfacente tra prestazioni ed efficienza del campione.
I recenti progressi nell'apprendimento di rinforzo profondo hanno fatto passi da gigante nelle prestazioni su applicazioni come Go e i giochi Atari.Tuttavia, lo sviluppo di metodi pratici per bilanciare l'esplorazione e lo sfruttamento in domini complessi rimane in gran parte irrisolto.Thompson Sampling e la sua estensione all'apprendimento di rinforzo forniscono un approccio elegante all'esplorazione che richiede solo l'accesso ai campioni posteriori del modello.Allo stesso tempo, i progressi nei metodi bayesiani approssimati hanno reso pratica l'approssimazione posteriore per modelli di reti neurali flessibili. Per capire l'impatto dell'uso di un'approssimazione a posteriori su Thompson Sampling, abbiamo messo a confronto metodi consolidati e di recente sviluppo per il campionamento a posteriori approssimato combinato con Thompson Sampling su una serie di problemi contestuali di bandit. Abbiamo scoperto che molti approcci che hanno avuto successo nell'impostazione di apprendimento supervisionato hanno sottoperformato nello scenario del processo decisionale sequenziale, in particolare, abbiamo evidenziato la sfida di adattare stime di incertezza lentamente convergenti all'impostazione online.
Le etichette di classe sono state dimostrate empiricamente utili nel migliorare la qualità del campione delle reti generative avversarie (GANs).In questo documento, studiamo matematicamente le proprietà delle varianti correnti di GANs che fanno uso delle informazioni dell'etichetta di classe.Con il gradiente consapevole di classe e la decomposizione di cross-entropia, riveliamo come le etichette di classe e le perdite associate influenzano l'addestramento di GAN.Based su quello, proponiamo Activation Maximization Generative Adversarial Networks (AM-GAN) come una soluzione avanzata. Sono stati condotti esperimenti completi per convalidare la nostra analisi e valutare l'efficacia della nostra soluzione, dove AM-GAN supera altre forti basi e raggiunge lo stato dell'arte dell'Inception Score (8.91) su CIFAR-10. Inoltre, dimostriamo che, con il classificatore Inception ImageNet, Inception Score traccia principalmente la diversità del generatore, e non c'è, tuttavia, alcuna prova affidabile che possa riflettere la vera qualità del campione.Proponiamo quindi una nuova metrica, chiamata AM Score, per fornire una stima più accurata sulla qualità del campione.Il nostro modello proposto supera anche i metodi baseline nella nuova metrica.
Le reti neurali moderne sono iper-parametrizzate, in particolare, ogni unità nascosta lineare rettificata può essere modificata da un fattore moltiplicativo, regolando i pesi di ingresso e di uscita, senza cambiare il resto della rete, ispirandosi all'algoritmo Sinkhorn-Knopp, introduciamo un metodo iterativo veloce per minimizzare la norma l2 dei pesi, equivalentemente il regolatore del decadimento dei pesi. Interleaving il nostro algoritmo con SGD durante l'addestramento migliora l'accuratezza del test.Per piccoli lotti, il nostro approccio offre un'alternativa alla normalizzazione dei lotti e dei gruppi su CIFAR-10 e ImageNet con un ResNet-18.
Gli agenti di apprendimento per rinforzo sono tipicamente addestrati e valutati in base alle loro prestazioni mediate su una certa distribuzione di impostazioni dell'ambiente.Ma la distribuzione sulle impostazioni dell'ambiente contiene importanti distorsioni, e queste portano ad agenti che falliscono in certi casi nonostante le alte prestazioni del caso medio? In questo lavoro, consideriamo l'analisi del caso peggiore degli agenti sulle impostazioni dell'ambiente al fine di rilevare se ci sono direzioni in cui gli agenti possono non essere riusciti a generalizzare.In particolare, consideriamo un compito 3D in prima persona dove gli agenti devono navigare in labirinti generati proceduralmente, e dove gli agenti di apprendimento per rinforzo hanno recentemente raggiunto prestazioni del caso medio a livello umano. Ottimizzando sulla struttura dei labirinti, troviamo che gli agenti possono soffrire di fallimenti catastrofici, non riuscendo a trovare l'obiettivo anche su labirinti sorprendentemente semplici, nonostante le loro impressionanti prestazioni medio-caso. Crediamo che i nostri risultati evidenzino un ruolo importante per l'analisi del caso peggiore nell'identificare se ci sono direzioni in cui gli agenti non sono riusciti a generalizzare.La nostra speranza è che la capacità di identificare automaticamente i fallimenti di generalizzazione faciliterà lo sviluppo di agenti più generali e robusti.A tal fine, riportiamo i risultati iniziali sull'arricchimento della formazione con impostazioni che causano il fallimento.
Per risolvere questo problema, è stato suggerito di sostituire la verosimiglianza con una pseudo-liquidità, che è l'esponenziale di una funzione di perdita che gode di adeguate proprietà di robustezza. In questo articolo, costruiamo una pseudo-liquidità basata sulla massima discrepanza media, definita attraverso un'inclusione delle distribuzioni di probabilità in uno spazio di Hilbert con kernel riproducente. Dimostriamo che questa posteriorità MMD-Bayes è coerente e robusta alla specificazione errata del modello. Poiché la posteriorità ottenuta in questo modo potrebbe essere intrattabile, dimostriamo anche che approssimazioni variazionali ragionevoli di questa posteriorità godono delle stesse proprietà. Forniamo dettagli su un algoritmo a gradiente stocastico per calcolare queste approssimazioni variazionali.
I limiti inferiori variazionali standard utilizzati per addestrare i modelli di variabili latenti producono stime distorte della maggior parte delle quantità di interesse. Introduciamo uno stimatore imparziale della verosimiglianza marginale log e dei suoi gradienti per i modelli di variabili latenti basati sulla troncatura randomizzata di serie infinite. Mostriamo che i modelli addestrati usando il nostro stimatore danno migliori probabilità di test-set rispetto a un approccio standard basato sul campionamento d'importanza per lo stesso costo computazionale medio. Questo stimatore permette anche l'uso di modelli di variabili latenti per compiti in cui sono preferiti stimatori imparziali, piuttosto che limiti inferiori di verosimiglianza marginale, come la minimizzazione delle divergenze KL inverse e la stima delle funzioni di punteggio.
Questo articolo apporta due contributi alla comprensione di come gli iperparametri della discesa del gradiente stocastico influenzino la perdita finale di addestramento e l'accuratezza del test delle reti neurali. In primo luogo, sosteniamo che la discesa del gradiente stocastico esibisce due regimi con comportamenti diversi: un regime dominato dal rumore che si presenta tipicamente per lotti di dimensioni piccole o moderate, e un regime dominato dalla curvatura che si presenta tipicamente quando la dimensione del lotto è grande. Nel regime dominato dal rumore, il tasso di apprendimento ottimale aumenta con l'aumentare della dimensione del lotto, e la perdita di formazione e l'accuratezza del test sono indipendenti dalla dimensione del lotto sotto un budget di epoch costante; nel regime dominato dalla curvatura, il tasso di apprendimento ottimale è indipendente dalla dimensione del lotto, e la perdita di formazione e l'accuratezza del test degradano con l'aumentare della dimensione del lotto. Sosteniamo queste affermazioni con esperimenti su una serie di architetture tra cui ResNets, LSTMs e autoencoders.Eseguiamo sempre una ricerca a griglia sui tassi di apprendimento a tutte le dimensioni dei lotti.In secondo luogo, dimostriamo che lotti piccoli o moderatamente grandi continuano a superare lotti molto grandi sul set di test, anche quando entrambi i modelli sono addestrati per lo stesso numero di passi e raggiungono perdite di formazione simili. Inoltre, quando si addestrano Wide-ResNets su CIFAR-10 con una dimensione costante dei lotti di 64, il tasso di apprendimento ottimale per massimizzare la precisione del test decade solo di un fattore 2 quando il budget delle epoche viene aumentato di un fattore 128, mentre il tasso di apprendimento ottimale per minimizzare la perdita di formazione decade di un fattore 16. Questi risultati confermano che il rumore nei gradienti stocastici può introdurre una benefica regolarizzazione implicita.
Counterfactual Regret Minimization (CFR) è l'algoritmo di maggior successo per trovare equilibri di Nash approssimativi in giochi a informazione imperfetta. Pertanto, lo spazio di stato e di azione del gioco è spesso astratto (cioè semplificato) per il CFR, e la strategia risultante è poi mappata di nuovo al gioco completo. Questo richiede una vasta conoscenza da parte di esperti, non è pratico in molti giochi al di fuori del poker, e spesso converge a politiche altamente sfruttabili. Un metodo recentemente proposto, Deep CFR, applica l'apprendimento profondo direttamente al CFR, permettendo all'agente di astrarre intrinsecamente e generalizzare sullo spazio di stato dai campioni, senza richiedere la conoscenza di esperti.In questo articolo, introduciamo Single Deep CFR (SD-CFR), una variante di Deep CFR che ha un errore di approssimazione generale più basso evitando l'addestramento di una rete di strategia media.Mostriamo che SD-CFR è più attraente da una prospettiva teorica e supera empiricamente Deep CFR per quanto riguarda la sfruttabilità e il gioco uno contro uno nel poker.
Mentre i modelli generativi hanno mostrato un grande successo nella generazione di campioni ad alta dimensione condizionati da descrittori a bassa dimensione (imparando ad esempio lo spessore del tratto in MNIST, il colore dei capelli in CelebA, o l'identità del parlante in Wavenet), la loro generazione fuori dal campione pone problemi fondamentali. L'autocodificatore variazionale condizionale (CVAE) come semplice modello generativo condizionale non mette esplicitamente in relazione le condizioni durante l'addestramento e, quindi, non ha alcun incentivo ad apprendere una distribuzione congiunta compatta tra le condizioni. Noi superiamo questa limitazione facendo corrispondere le loro distribuzioni utilizzando la massima discrepanza media (MMD) nello strato di decodifica che segue il collo di bottiglia. Questo introduce una forte regolarizzazione sia per la ricostruzione dei campioni all'interno della stessa condizione che per la trasformazione dei campioni attraverso le condizioni, con conseguente generalizzazione molto migliorata.Ci riferiamo all'architettura come trasformatore VAE (trVAE).Benchmarking trVAE sull'immagine ad alta densità e sui dati tabulari, dimostriamo una maggiore robustezza e una maggiore precisione rispetto agli approcci esistenti. In particolare, mostriamo previsioni qualitativamente migliorate per la risposta di perturbazione cellulare al trattamento e alla malattia basata su dati di espressione genica mono-cellulare ad alta densità, affrontando classi minoritarie precedentemente problematiche e condizioni multiple.Per compiti generici, miglioriamo le correlazioni di Pearson dei mezzi e delle varianze stimate ad alta densità con le loro verità di base da 0.89 a 0.97 e da 0.75 a 0.87, rispettivamente.
Analizziamo la velocità di convergenza all'optimum globale per l'addestramento a discesa del gradiente di una rete neurale lineare profonda minimizzando la perdita L2 sui dati sbiancati.  La convergenza ad una velocità lineare è garantita quando le seguenti condizioni sono soddisfatte: (i) le dimensioni degli strati nascosti sono almeno il minimo delle dimensioni di input e output; (ii) le matrici dei pesi all'inizializzazione sono approssimativamente bilanciate; e (iii) la perdita iniziale è più piccola della perdita di qualsiasi soluzione rank-deficient.  Le assunzioni sull'inizializzazione (condizioni (ii) e (iii)) sono necessarie, nel senso che la violazione di una qualsiasi di esse può portare al fallimento della convergenza.  Inoltre, nel caso importante della dimensione di uscita 1, cioè la regressione scalare, esse sono soddisfatte, e quindi la convergenza verso l'ottimo globale si mantiene, con probabilità costante sotto uno schema di inizializzazione casuale.  I nostri risultati estendono significativamente le analisi precedenti, ad esempio, delle reti residuali lineari profonde (Bartlett et al., 2018).
Uno dei problemi fondamentali nella classificazione supervisionata e nell'apprendimento automatico in generale, è la modellazione delle invarianze non parametriche che esistono nei dati.La maggior parte dell'arte precedente si è concentrata sull'imposizione di priori sotto forma di invarianze alle trasformazioni di nuisance parametriche che dovrebbero essere presenti nei dati.Tuttavia, l'apprendimento delle invarianze non parametriche direttamente dai dati rimane un importante problema aperto.In questo articolo, introduciamo un nuovo strato architetturale per le reti convoluzionali che è in grado di imparare invarianze generali dai dati stessi. Questo strato può imparare l'invarianza alle trasformazioni non parametriche e, cosa interessante, motiva e incorpora connettori casuali permanenti, chiamandoli Permanent Random Connectome Non-Parametric Transformation Networks (PRC-NPTN). Le reti PRC-NPTN sono inizializzate con connessioni casuali (non solo pesi) che sono un piccolo sottoinsieme delle connessioni in uno strato di convoluzione completamente connesso.  Le connessioni casuali rendono queste architetture vagamente più biologicamente plausibili di molte altre architetture di rete tradizionali che richiedono strutture altamente ordinate. motiviamo le connessioni inizializzate casualmente come un metodo semplice per imparare l'invarianza dai dati stessi mentre invochiamo l'invarianza verso più trasformazioni fastidiose simultaneamente. troviamo che queste connessioni permanenti inizializzate casualmente hanno effetti positivi sulla generalizzazione, superano le prestazioni di baseline ConvNet molto più grandi e la rete di trasformazione non parametrica recentemente proposta (NPTN) su benchmark che impongono l'apprendimento di invarianze dai dati stessi.
Proponiamo un modello generativo condizionale completamente rivoluzionario, la rete neurale di trasformazione latente (LTNN), in grado di sintetizzare la vista utilizzando una rete neurale leggera adatta ad applicazioni in tempo reale. In contrasto con i modelli generativi condizionali esistenti che incorporano informazioni di condizionamento attraverso la concatenazione, introduciamo un componente di rete dedicato, l'unità di trasformazione condizionale (CTU), progettato per imparare le trasformazioni dello spazio latente corrispondenti a viste specifiche. Inoltre, viene definito un termine di perdita di coerenza per guidare la rete verso l'apprendimento delle mappature dello spazio latente desiderate, viene costruito un decodificatore diviso in compiti per raffinare la qualità delle viste generate, e viene introdotto un discriminatore adattivo per migliorare il processo di formazione avversaria. La generalità della metodologia proposta è dimostrata su una collezione di tre compiti diversi: ricostruzione multi-vista su immagini di profondità di mani reali, sintesi della vista di volti reali e sintetici, e la rotazione di oggetti rigidi. Il modello proposto ha dimostrato di superare i risultati dello stato dell'arte in ogni categoria, ottenendo contemporaneamente una riduzione della domanda computazionale richiesta per l'inferenza del 30% in media.
Descriviamo tre approcci per permettere a uno scheduler incorporato estremamente limitato dal punto di vista computazionale di considerare un piccolo numero di attività alternative basate sulla disponibilità delle risorse.Consideriamo il caso in cui lo scheduler è così limitato dal punto di vista computazionale che non può effettuare la ricerca backtrack.I primi due approcci precompilano i controlli delle risorse (chiamati guardie) che permettono solo la selezione di un'attività alternativa preferita se si stima che siano disponibili risorse sufficienti per programmare le attività rimanenti. Presentiamo una valutazione di queste tecniche su scenari di missione (chiamati tipi di sol) del prossimo rover planetario della NASA, dove queste tecniche vengono valutate per l'inclusione in uno scheduler di bordo.
Ispirati dalla modularità e dal ciclo di vita dei neuroni biologici, introduciamo il Continual Learning via Neural Pruning (CLNP), un nuovo metodo finalizzato all'apprendimento permanente in modelli a capacità fissa basati sulla potatura dei neuroni a bassa attività.In questo metodo, un regolatore L1 viene utilizzato per promuovere la presenza di neuroni di attività zero o bassa le cui connessioni ai neuroni precedentemente attivi vengono definitivamente interrotte alla fine dell'addestramento. I compiti successivi sono addestrati usando questi neuroni potati dopo la reinizializzazione e causano un deterioramento nullo alle prestazioni dei compiti precedenti. Mostriamo empiricamente che questo metodo biologicamente ispirato porta a risultati allo stato dell'arte che battono o corrispondono ai metodi attuali di maggiore complessità computazionale.
In questo articolo, indaghiamo empiricamente diversi modi per migliorare le prestazioni delle reti neurali convoluzionali (CNN) sulle caratteristiche spettrali dell'audio. Più specificamente, esploriamo tre aspetti della progettazione CNN: la profondità della rete, l'uso di blocchi residui insieme all'uso della convoluzione raggruppata e l'aggregazione globale nel tempo. Il contesto applicativo è la classificazione dei cantanti e l'incorporazione delle performance canore e crediamo che le conclusioni si estendano ad altri tipi di analisi musicale utilizzando le reti neurali convoluzionali.I risultati mostrano che l'aggregazione globale nel tempo aiuta a migliorare le performance delle CNN più.Un altro contributo di questo articolo è il rilascio di un dataset di registrazioni canore che può essere utilizzato per la formazione e la valutazione.
La funzione Softmax è usata nello strato finale di quasi tutti i modelli sequenza-sequenza esistenti per la generazione del linguaggio. Tuttavia, di solito è lo strato più lento da calcolare e limita la dimensione del vocabolario a un sottoinsieme dei tipi più frequenti; inoltre ha un grande ingombro di memoria. Le nostre innovazioni principali sono una nuova perdita probabilistica e una procedura di addestramento e inferenza in cui generiamo una distribuzione di probabilità su embeddings di parole pre-addestrate, invece di una distribuzione multinomiale sul vocabolario ottenuto tramite softmax. Valutiamo questa nuova classe di modelli sequenza-sequenza con uscite continue sul compito di traduzione automatica neurale e dimostriamo che i nostri modelli ottengono fino a 2,5 volte più veloci nel tempo di addestramento, mentre si comportano alla pari con i modelli allo stato dell'arte in termini di qualità della traduzione. Questi modelli sono in grado di gestire vocabolari molto grandi senza compromettere la qualità della traduzione e producono anche errori più significativi rispetto ai modelli basati su softmax, poiché questi errori si trovano tipicamente in un sottospazio dello spazio vettoriale delle traduzioni di riferimento.
Proponiamo un quadro di riferimento per comprendere le prestazioni e la robustezza senza precedenti delle reti neurali profonde utilizzando la teoria dei campi.Le correlazioni tra i pesi all'interno dello stesso strato possono essere descritte da simmetrie in quello strato, e le reti generalizzano meglio se tali simmetrie vengono rotte per ridurre le ridondanze dei pesi. Usando una teoria di campo a due parametri, troviamo che la rete può rompere tali simmetrie da sola verso la fine dell'addestramento in un processo comunemente noto in fisica come rottura spontanea della simmetria.Questo corrisponde a una rete che si generalizza da sola senza alcuno strato di input dell'utente per rompere la simmetria, ma attraverso la comunicazione con gli strati adiacenti.Nel limite di disaccoppiamento degli strati applicabile alle reti residuali (He et al, 2015), mostriamo che le simmetrie residue che sopravvivono agli strati non lineari sono spontaneamente rotte sulla base di risultati empirici.La lagrangiana per gli strati non lineari e ponderali insieme ha sorprendenti somiglianze con quella nella teoria quantistica dei campi di uno scalare.Utilizzando i risultati della teoria quantistica dei campi mostriamo che il nostro quadro è in grado di spiegare molti fenomeni osservati sperimentalmente, come l'addestramento su etichette casuali con errore zero (Zhang et al, 2017), il collo di bottiglia dell'informazione e la transizione di fase per uscirne (Shwartz-Ziv & Tishby, 2017), i gradienti frantumati (Balduzzi et al., 2017), e molti altri.
Durante gli ultimi anni, un notevole passo avanti è stato fatto nel dominio dell'IA grazie alle reti neurali profonde artificiali che hanno raggiunto un grande successo nei compiti di apprendimento di molte macchine nella visione artificiale, nell'elaborazione del linguaggio naturale, nel riconoscimento vocale, nel rilevamento di malware e così via, ma sono altamente vulnerabili agli esempi avversari facilmente creati. Molte indagini hanno evidenziato questo fatto e sono stati proposti diversi approcci per generare attacchi aggiungendo una perturbazione limitata ai dati originali.Il metodo più robusto conosciuto finora è il cosiddetto attacco C&W [1].Tuttavia, una contromisura nota come fea-ture squeezing accoppiata alla difesa dell'ensemble ha dimostrato che la maggior parte di questi attacchi può essere distrutta [6]. In questo articolo, presentiamo un nuovo metodo che chiamiamo CenteredInitial Attack (CIA) il cui vantaggio è duplice: in primo luogo, assicura per costruzione che la perturbazione massima sia più piccola di una soglia fissata in anticipo, senza il processo di clipping che degrada la qualità degli attacchi; in secondo luogo, è robusto contro le difese recentemente introdotte come feature squeezing, JPEG en-coding e anche contro un insieme di difese con voto. Sebbene la sua applicazione non sia limitata alle immagini, lo illustriamo utilizzando cinque degli attuali migliori classificatori del dataset ImageNet, due dei quali sono stati addestrati in modo da essere robusti contro gli attacchi. Con una perturbazione massima fissa di solo 1,5% su qualsiasi pixel, circa l'80% degli attacchi (mirati) inganna la difesa dell'insieme di voto e quasi il 100% quando la perturbazione è solo del 6%.
Recentemente, un approccio promettente a questo problema è stato quello di incorporare le entità KG e la query in uno spazio vettoriale in modo che le entità che rispondono alla query siano incorporate vicino alla query stessa. Tuttavia, il lavoro precedente modella le query come singoli punti nello spazio vettoriale, il che è problematico perché una query complessa rappresenta un insieme potenzialmente grande di entità di risposta, ma non è chiaro come un tale insieme possa essere rappresentato come un singolo punto. Inoltre, i lavori precedenti possono gestire solo le query che usano congiunzioni ($\wedge$) e quantificatori esistenziali ($\exists$); gestire le query con disgiunzioni logiche ($\vee$) rimane un problema aperto. Qui proponiamo query2box, un framework basato sull'embedding per ragionare su query arbitrarie con operatori $\wedge$, $\vee$ e $\exists$ in KGs massicci e incompleti, Mostriamo che le congiunzioni possono essere naturalmente rappresentate come intersezioni di caselle e dimostriamo anche un risultato negativo che la gestione delle disgiunzioni richiederebbe un'inclusione con una dimensione proporzionale al numero di entità KG. Tuttavia, dimostriamo che trasformando le query in una Disjunctive Normal Form, query2box è in grado di gestire query logiche arbitrarie con $\wedge$, $\vee$, $\exists$ in modo scalabile.Dimostriamo l'efficacia di query2box su due grandi KG e dimostriamo che query2box raggiunge fino al 25% di miglioramento relativo rispetto allo stato dell'arte.
Spinto dalla necessità di metodi di ottimizzazione iperparametrica parallelizzabili, questo articolo studia i metodi di ricerca ad anello aperto: sequenze che sono predeterminate e possono essere generate prima che una singola configurazione sia valutata.Gli esempi includono la ricerca a griglia, la ricerca casuale uniforme, le sequenze a bassa discrepanza e altre distribuzioni di campionamento.In particolare, proponiamo l'uso di processi di punti k-determinantici nell'ottimizzazione iperparametrica tramite la ricerca casuale.Rispetto alla ricerca casuale uniforme convenzionale dove le impostazioni iperparametriche sono campionate indipendentemente, un k-DPP promuove la diversità.  Descriviamo un approccio che trasforma gli spazi di ricerca iperparametrica per un uso efficiente con un k-DPP.Inoltre, introduciamo un nuovo algoritmo Metropolis-Hastings che può campionare da k-DPP definiti su qualsiasi spazio da cui possono essere tratti campioni uniformi, compresi gli spazi con una miscela di dimensioni discrete e continue o la struttura ad albero.I nostri esperimenti mostrano benefici significativi in scenari realistici con un budget limitato per la formazione di apprendisti supervisionati, sia in serie che in parallelo.
Le tecniche di embedding dei grafi sono state sempre più impiegate in una moltitudine di applicazioni diverse che coinvolgono l'apprendimento su dati non euclidei. Tuttavia, i modelli di embedding dei grafi esistenti non riescono a incorporare le informazioni sugli attributi dei nodi durante la formazione o soffrono del rumore degli attributi dei nodi, che compromette la precisione. In questo articolo proponiamo GraphZoom, una struttura multilivello per migliorare sia l'accuratezza che la scalabilità degli algoritmi di incorporazione di grafici non supervisionati.GraphZoom esegue prima la fusione dei grafici per generare un nuovo grafico che codifica efficacemente la topologia del grafico originale e le informazioni sugli attributi dei nodi. GraphZoom permette a qualsiasi metodo di embedding esistente di essere applicato al grafo coarsened, prima di raffinare progressivamente le embeddings ottenute al livello più grossolano in grafi sempre più fini. Abbiamo valutato il nostro approccio su una serie di popolari dataset di grafi sia per compiti trasduttivi che induttivi. I nostri esperimenti mostrano che GraphZoom aumenta la precisione di classificazione e riduce significativamente il tempo di esecuzione rispetto ai metodi di embedding non supervisionati di ultima generazione.
I modelli generativi profondi hanno riscosso successo nella modellazione di dati continui, ma rimane una sfida catturare le rappresentazioni di strutture discrete con grammatiche e semantiche formali, ad es, Come generare dati sintatticamente e semanticamente corretti rimane ancora in gran parte un problema aperto. Ispirato dalla teoria del compilatore dove il controllo della sintassi e della semantica è fatto attraverso la traduzione sintattico-diretta (SDT), proponiamo un nuovo autocodificatore variazionale sintattico-diretto (SD-VAE) introducendo attributi pigri stocastici.Questo approccio converte il controllo SDT offline in una guida generata on-the-fly per vincolare il decodificatore. Rispetto ai metodi all'avanguardia, il nostro approccio impone dei vincoli sullo spazio di uscita in modo che l'uscita sia non solo sintatticamente valida, ma anche semanticamente ragionevole. Valutiamo il modello proposto con applicazioni nel linguaggio di programmazione e nelle molecole, tra cui la ricostruzione e l'ottimizzazione del programma/molecola.I risultati dimostrano l'efficacia nell'incorporare vincoli sintattici e semantici nei modelli generativi discreti, che è significativamente migliore degli attuali approcci all'avanguardia.
Con la recente disponibilità di grandi dati annotati, è diventato possibile addestrare modelli complessi come le reti neurali per eseguire l'inferenza del linguaggio naturale (NLI), che hanno raggiunto prestazioni all'avanguardia.Anche se esistono dati annotati relativamente grandi, le macchine possono imparare tutta la conoscenza necessaria per eseguire NLI dai dati? In caso contrario, come possono i modelli NLI beneficiare della conoscenza esterna e come costruire modelli NLI per sfruttarla? In questo articolo, miriamo a rispondere a queste domande arricchendo i modelli neurali di inferenza del linguaggio naturale allo stato dell'arte con la conoscenza esterna.Dimostriamo che i modelli proposti con conoscenza esterna migliorano ulteriormente lo stato dell'arte sul dataset Stanford Natural Language Inference (SNLI).
L'alta diversità intra-classe e la somiglianza inter-classe è una caratteristica degli insiemi di dati di immagine di scena di telerilevamento che attualmente pongono una difficoltà significativa per gli algoritmi di apprendimento profondo sui compiti di classificazione.Per migliorare la precisione, i metodi di post-classificazione sono stati proposti per lisciare i risultati delle previsioni del modello.Tuttavia, questi approcci richiedono una rete neurale supplementare per eseguire l'operazione di lisciatura, che aggiunge overhead al compito.Noi proponiamo un approccio che coinvolge l'apprendimento delle caratteristiche profonde direttamente sulle immagini di scena vicine senza richiedere l'uso di un modello di pulizia. Il nostro approccio utilizza una rete siamese per migliorare il potere discriminatorio delle reti neurali convoluzionali su una coppia di immagini di scena vicine, quindi sfrutta la coerenza semantica tra questa coppia per arricchire il vettore di caratteristiche dell'immagine per la quale vogliamo predire un'etichetta. Per esempio, il nostro modello ha migliorato l'accuratezza della predizione di 1 punto percentuale e ha diminuito il valore dell'errore quadratico medio di 0,02 rispetto alla linea di base, su un compito di stima della densità delle malattie.Questi guadagni di prestazioni sono paragonabili ai risultati dei metodi di post-classificazione esistenti, inoltre senza spese generali di implementazione.
I punti di dati scarsi causano un errore numerico sulle differenze finite che ostacolano la modellazione delle dinamiche dei sistemi fisici.L'errore di discretizzazione diventa ancora più grande quando i dati scarsi sono distribuiti irregolarmente in modo che i dati definiti su una griglia non strutturata, rendendo difficile costruire modelli di apprendimento profondo per gestire osservazioni fisiche sulla griglia non strutturata. In questo articolo, proponiamo una nuova architettura chiamata Physics-aware Difference Graph Networks (PA-DGN) che sfrutta le informazioni dei vicini per imparare le differenze finite ispirate alle equazioni della fisica.PA-DGN sfrutta ulteriormente l'apprendimento end-to-end guidato dai dati per scoprire le relazioni dinamiche sottostanti tra le differenze spaziali e temporali nelle osservazioni date.dimostriamo la superiorità di PA-DGN nell'approssimazione delle derivate direzionali e la previsione dei segnali grafici sui dati sintetici e sulle osservazioni climatiche del mondo reale dalle stazioni meteorologiche.
Risolvere compiti decisionali sequenziali a lungo orizzonte in ambienti con ricompense sparse è un problema di lunga data nella ricerca sull'apprendimento di rinforzo (RL).L'apprendimento di rinforzo gerarchico (HRL) ha mantenuto la promessa di migliorare le capacità degli agenti RL attraverso il funzionamento su diversi livelli di astrazione temporale.Nonostante il successo di lavori recenti nel trattare la non stazionarietà inerente e la complessità del campione, rimane difficile generalizzare ad ambienti non visti e trasferire diversi livelli della politica ad altri agenti. In questo articolo, proponiamo una nuova architettura HRL, Hierarchical Decompositional Reinforcement Learning (HiDe), che permette la decomposizione degli strati gerarchici in sottoattività indipendenti, ma consente l'addestramento congiunto di tutti gli strati in modo end-to-end.L'intuizione principale è quella di combinare una politica di controllo su un livello inferiore con una politica di pianificazione basata sulle immagini su un livello superiore.Valutiamo il nostro metodo su vari compiti complessi di controllo continuo per la navigazione, dimostrando che la generalizzazione attraverso ambienti e il trasferimento di politiche di livello superiore può essere raggiunto.Vedi video https://sites.google.com/view/hide-rl
I linguisti li usano per dimostrare le relazioni tra le lingue, per ricostruire le protoforme e per la ricostruzione filogenetica classica basata su innovazioni condivise. Le parole cognate che non si conformano ai modelli previsti possono inoltre indicare vari tipi di eccezioni nel cambiamento del suono, come l'analogia o l'assimilazione di parole frequenti. Il compito di dedurre tutti gli insiemi di corrispondenza compatibili può essere gestito come il ben noto problema del minimum clique cover nella teoria dei grafi, che essenzialmente cerca di dividere il grafico nel minor numero di cliques in cui ogni nodo è rappresentato esattamente da una clique. Le partizioni risultanti rappresentano tutti i modelli di corrispondenza che possono essere trasferiti per un dato set di dati; escludendo i modelli che si verificano solo in pochi insiemi cognitivi, il nucleo delle corrispondenze sonore regolarmente ricorrenti può essere dedotto; sulla base di questa idea, l'articolo presenta un metodo per il riconoscimento automatico dei modelli di corrispondenza, che è implementato come parte di una libreria Python che integra l'articolo. Per illustrare l'utilità del metodo, vengono presentati vari test ed esempi concreti dell'output del metodo.Oltre al codice sorgente, lo studio è completato da un breve tutorial interattivo che illustra come utilizzare il nuovo metodo e come controllare i suoi risultati.
Descriviamo Kernel RNN Learning (KeRNL), un'approssimazione basata sulla traccia di ammissibilità temporale e di rango ridotto alla backpropagation through time (BPTT) per l'addestramento delle reti neurali ricorrenti (RNN), che offre prestazioni competitive alla BPTT su compiti a lunga dipendenza temporale. L'approssimazione sostituisce un tensore di gradiente di apprendimento di rango 4, che descrive come le attivazioni passate delle unità nascoste influenzano lo stato attuale, con un semplice prodotto di rango ridotto di un peso di sensibilità e una traccia di ammissibilità temporale. In questa approssimazione strutturata motivata dalla perturbazione dei nodi, i pesi di sensibilità e le scale temporali del kernel di ammissibilità sono essi stessi appresi applicando le perturbazioni.La regola rappresenta un altro passo verso una ML biologicamente plausibile o ispirata a livello neuronale, con una minore complessità in termini di requisiti architettonici rilassati (nessun peso di ritorno simmetrico), una minore richiesta di memoria (nessun svolgimento e memorizzazione degli stati nel tempo), e un tempo di feedback più breve.
Presentiamo Deep Graph Infomax (DGI), un approccio generale per l'apprendimento delle rappresentazioni dei nodi all'interno dei dati strutturati a grafo in modo non supervisionato. DGI si basa sulla massimizzazione dell'informazione reciproca tra le rappresentazioni delle patch e i corrispondenti riassunti di alto livello dei grafi - entrambi derivati utilizzando architetture di rete convoluzionali a grafo stabilite. In contrasto con la maggior parte degli approcci precedenti all'apprendimento non supervisionato con le GCN, DGI non si basa su obiettivi di camminata casuale, ed è facilmente applicabile sia a setup di apprendimento trasduttivo che induttivo. dimostriamo prestazioni competitive su una varietà di benchmark di classificazione dei nodi, che a volte superano persino le prestazioni dell'apprendimento supervisionato.
In molti ambienti solo un piccolo sottoinsieme di tutti gli stati produce un'alta ricompensa.  In questi casi, poche delle interazioni con l'ambiente forniscono un segnale di apprendimento rilevante. Quindi, potremmo voler addestrare preferenzialmente quegli stati ad alta ricompensa e le probabili traiettorie che portano ad essi. A tal fine, sosteniamo l'uso di un modello di backtracking che predice gli stati precedenti che terminano in un dato stato ad alta ricompensa.  Possiamo addestrare un modello che, partendo da uno stato di alto valore (o uno che si stima abbia un alto valore), predice e campiona quali (stato, azione) -tuple possono aver portato a quello stato di alto valore. Queste tracce di coppie (stato, azione), che chiamiamo Recall Traces, campionate da questo modello di backtracking partendo da uno stato di alto valore, sono informative perché terminano in stati buoni, e quindi possiamo usare queste tracce per migliorare una politica. Forniamo un'interpretazione variazionale per questa idea e un algoritmo pratico in cui il modello di backtracking campiona da una distribuzione posteriore approssimata sulle traiettorie che portano a grandi ricompense. Il nostro metodo migliora l'efficienza di campionamento degli algoritmi di RL on e off-policy in diversi ambienti e compiti.  
Le preposizioni sono tra le parole più frequenti.Una buona rappresentazione preposizionale è di grande interesse sintattico e semantico nella linguistica computazionale.I metodi esistenti sulla rappresentazione delle preposizioni o trattano le preposizioni come parole di contenuto (es, word2vec e GloVe) o dipendono pesantemente da risorse linguistiche esterne tra cui il parsing sintattico, il compito di addestramento e le rappresentazioni specifiche del dataset.In questo articolo usiamo i conteggi di parole-triplo (una delle parole è una preposizione) per catturare l'interazione della preposizione con la sua testa e i suoi figli.Le embeddings preposizionali sono derivate tramite decomposizioni tensoriali su un grande corpus non etichettato.  Riveliamo una nuova geometria che coinvolge i prodotti di Hadamard e dimostriamo empiricamente la sua utilità nella parafrasi dei verbi frasali. Inoltre, le nostre incorporazioni preposizionali sono usate come semplici caratteristiche per due impegnativi compiti a valle: la selezione delle preposizioni e la disambiguazione degli attacchi preposizionali.  
Una promettente classe di modelli generativi mappa i punti da una distribuzione semplice a una distribuzione complessa attraverso una rete neurale invertibile.   L'addestramento basato sulla probabilità di questi modelli richiede la limitazione delle loro architetture per consentire il calcolo economico dei determinanti Jacobiani.  In alternativa, la traccia Jacobiana può essere usata se la trasformazione è specificata da un'equazione differenziale ordinaria. In questo articolo, usiamo lo stimatore della traccia di Hutchinson per dare una stima imparziale scalabile della log-densità.  Il risultato è un modello generativo invertibile a tempo continuo con stima imparziale della densità e campionamento one-pass, consentendo architetture di reti neurali illimitate. Dimostriamo il nostro approccio nella stima della densità ad alta densità, nella generazione di immagini e nell'inferenza variazionale, raggiungendo lo stato dell'arte tra i metodi di verosimiglianza esatta con campionamento efficiente.
Il nostro approccio, Generative Feature Matching Networks (GFMN), sfrutta reti neurali preaddestrate come autoencoder e classificatori ConvNet per eseguire l'estrazione delle caratteristiche. Eseguiamo un ampio numero di esperimenti con diversi set di dati impegnativi, tra cui ImageNet. I nostri risultati sperimentali dimostrano che, grazie all'espressività delle caratteristiche dei classificatori preaddestrati di ImageNet, anche solo abbinando le statistiche del primo ordine, il nostro approccio può raggiungere risultati all'avanguardia per benchmark impegnativi come CIFAR10 e STL10.
Proponiamo una nuova architettura per la classificazione di k-shot sul set di dati Omniglot. Basandoci sulle reti prototipiche, estendiamo la loro architettura a ciò che chiamiamo reti prototipiche gaussiane. Le reti prototipiche imparano una mappa tra le immagini e i vettori di incorporazione, e usano il loro clustering per la classificazione. Nel nostro modello, una parte dell'output del codificatore è interpretata come una stima della regione di fiducia sul punto di incorporazione, ed espressa come una matrice di covarianza gaussiana. La nostra rete costruisce quindi una metrica di distanza dipendente dalla direzione e dalla classe sullo spazio di incorporazione, utilizzando le incertezze dei singoli punti dati come pesi. Mostriamo che le reti prototipiche gaussiane sono un'architettura preferita rispetto alle reti prototipiche alla vaniglia con un numero equivalente di parametri.Riportiamo risultati coerenti con le prestazioni allo stato dell'arte nella classificazione 1-shot e 5-shot sia in regime 5-way che 20-way sul dataset Omniglot.Esploriamo il down-sampling artificiale di una frazione di immagini nel set di allenamento, che migliora le nostre prestazioni.I nostri esperimenti ci portano quindi a ipotizzare che le reti prototipiche gaussiane potrebbero funzionare meglio in dataset meno omogenei e più rumorosi, che sono comuni nelle applicazioni del mondo reale.
Mostriamo che l'output di una CNN (residua) con un'appropriata priorità sui pesi e sui bias è una GP nel limite di infiniti filtri convoluzionali, estendendo risultati simili per reti dense.Per una CNN, il kernel equivalente può essere calcolato esattamente e, a differenza dei "kernel profondi", ha pochi parametri: solo gli iperparametri della CNN originale. Inoltre, dimostriamo che questo kernel ha due proprietà che gli permettono di essere calcolato in modo efficiente; il costo della valutazione del kernel per una coppia di immagini è simile a un singolo passaggio in avanti attraverso la CNN originale con un solo filtro per strato. Il kernel equivalente a una ResNet a 32 strati ottiene lo 0,84% di errore di classificazione su MNIST, un nuovo record per GP con un numero comparabile di parametri.
A differenza dei metodi precedenti che ottimizzano separatamente l'architettura della rete neurale, la politica di sfrondamento e la politica di quantizzazione, noi li ottimizziamo congiuntamente in modo end-to-end.Per affrontare il più ampio spazio di progettazione che ne deriva, addestriamo un predittore di precisione consapevole della quantizzazione che alimenta la ricerca evolutiva per selezionare il miglior adattamento.Generiamo prima un grande set di dati di <architettura NN, precisione ImageNet> coppie senza addestrare ogni architettura, ma campionando una supernet unificata. Poi usiamo questi dati per addestrare un predittore di accuratezza senza quantizzazione, utilizzando ulteriormente la tecnica di trasferimento del predittore per ottenere il predittore consapevole della quantizzazione, che riduce la quantità di tempo di messa a punto post-quantizzazione.Gli esperimenti estesi su ImageNet mostrano i vantaggi della metodologia end-to-end: mantiene la stessa accuratezza (75. 1%) del modello float ResNet34 mentre risparmia 2.2× BitOps rispetto al modello a 8-bit; otteniamo lo stesso livello di accuratezza di MobileNetV2+HAQ mentre raggiungiamo 2×/1.3× di latenza/risparmio energetico; l'ottimizzazione end-to-end supera le ottimizzazioni separate usando ProxylessNAS+AMC+HAQ del 2.3% di accuratezza mentre riduce ordini di grandezza di ore GPU ed emissioni di CO2.
Una caratteristica ampiamente condivisa delle tecniche di ottimizzazione distribuita è il requisito che tutti i nodi completino i loro compiti assegnati in ogni epoca di calcolo prima che il sistema possa procedere all'epoca successiva.In tali impostazioni, i nodi lenti, chiamati ritardatari, possono rallentare notevolmente il progresso.Per mitigare l'impatto dei ritardatari, proponiamo un metodo di ottimizzazione distribuita online chiamato Anytime Minibatch.In questo approccio, a tutti i nodi viene dato un tempo fisso per calcolare i gradienti di quanti più campioni di dati possibile. Il risultato è una dimensione del minibatch variabile per nodo, quindi i lavoratori ottengono un tempo di comunicazione fisso per calcolare la media dei loro gradienti del minibatch attraverso diversi round di consenso, che vengono poi utilizzati per aggiornare le variabili primarie attraverso la media duale.Anytime Minibatch impedisce ai ritardatari di bloccare il sistema senza sprecare il lavoro che i ritardatari possono completare. I nostri risultati numerici mostrano che il nostro approccio è fino a 1,5 volte più veloce in Amazon EC2 ed è fino a cinque volte più veloce quando c'è una maggiore variabilità nelle prestazioni dei nodi di calcolo.
Molti classificatori di immagini di apprendimento automatico sono vulnerabili agli attacchi avversari, input con perturbazioni progettate per innescare intenzionalmente l'errore di classificazione.I metodi avversari attuali alterano direttamente i colori dei pixel e valutano contro i pixel norm-ball: perturbazioni dei pixel più piccole di una grandezza specificata, secondo una norma di misura. Questa valutazione, tuttavia, ha un'utilità pratica limitata poiché le perturbazioni nello spazio dei pixel non corrispondono ai fenomeni sottostanti del mondo reale della formazione dell'immagine che le portano e non ha alcuna motivazione di sicurezza allegata.I pixel nelle immagini naturali sono misure di luce che ha interagito con la geometria di una scena fisica. Uno dei contributi che presentiamo è un renderer differenziabile basato sulla fisica che ci permette di propagare i gradienti dei pixel allo spazio parametrico dell'illuminazione e della geometria. Il nostro approccio permette attacchi avversari basati sulla fisica, e il nostro renderer differenziabile sfrutta i modelli della letteratura di rendering interattivo per bilanciare le prestazioni e i compromessi di precisione necessari per un flusso di lavoro di aumento dei dati avversari efficiente e scalabile.
Le alternative alle reti neurali ricorrenti, in particolare le architetture basate sull'attenzione o sulle convoluzioni, hanno guadagnato slancio per l'elaborazione delle sequenze di input. Nonostante la loro rilevanza, le proprietà computazionali di queste alternative non sono ancora state esplorate a fondo, 2017) e la GPU Neurale (Kaiser & Sutskever, 2016).Mostriamo che entrambi i modelli sono completi di Turing esclusivamente sulla base della loro capacità di calcolare e accedere a rappresentazioni interne dense dei dati.In particolare, né il Transformer né la GPU Neurale richiedono l'accesso a una memoria esterna per diventare completi di Turing.Il nostro studio rivela anche alcuni set minimi di elementi necessari per ottenere questi risultati di completezza.
Di solito è difficile per un sistema di apprendimento prevedere correttamente gli eventi rari, e non c'è eccezione per gli algoritmi di segmentazione.Pertanto, speriamo di costruire un sistema di allarme per far scattare gli allarmi quando il risultato della segmentazione è probabilmente insoddisfacente. Una soluzione plausibile è quella di proiettare i risultati di segmentazione in uno spazio delle caratteristiche a bassa dimensione, e poi imparare classificatori/regressori nello spazio delle caratteristiche per prevedere le qualità dei risultati di segmentazione.In questo documento, formiamo lo spazio delle caratteristiche usando la caratteristica di forma che è una forte informazione preliminare condivisa tra diversi dati, quindi è capace di prevedere le qualità dei risultati di segmentazione dati diversi algoritmi di segmentazione su diversi set di dati. La caratteristica di forma di un risultato di segmentazione è catturata usando il valore della funzione di perdita quando il risultato di segmentazione è testato usando un Variational Auto-Encoder (VAE).il VAE è addestrato usando solo le maschere della verità di terra, quindi i cattivi risultati di segmentazione con cattive forme diventano gli eventi rari per VAE e risulteranno in un grande valore di perdita.utilizzando questo fatto, il VAE è in grado di rilevare tutti i tipi di forme che sono fuori dalla distribuzione delle forme normali nella verità di terra (GT). Infine, impariamo la rappresentazione nello spazio delle caratteristiche unidimensionali per predire le qualità dei risultati di segmentazione.Valutiamo il nostro sistema di allarme su diversi algoritmi di segmentazione recenti per il compito di segmentazione medica.Gli algoritmi di segmentazione eseguono in modo diverso su diversi set di dati, ma il nostro sistema fornisce costantemente una previsione affidabile sulle qualità dei risultati di segmentazione.
Le trasformazioni lineari nelle reti profonde convergenti mostrano un veloce decadimento degli autovalori.La distribuzione degli autovalori sembra una distribuzione Heavy-tail, dove la stragrande maggioranza degli autovalori è piccola, ma non effettivamente zero, e solo alcuni picchi di grandi autovalori esistono.Usiamo un approssimatore stocastico per generare gli istogrammi degli autovalori.Questo ci permette di studiare strati con centinaia di migliaia di dimensioni.Mostriamo come le distribuzioni cambiano nel corso della formazione della rete di immagini, convergendo verso uno spettro simile heavy-tail in tutti gli strati intermedi.
Catturare le relazioni a lungo raggio delle caratteristiche è stato un problema centrale nelle reti neurali convoluzionali (CNN). Per affrontare questo problema, i tentativi di integrare il modulo di attenzione addestrabile end-to-end nelle CNN sono molto diffusi, e l'obiettivo principale di questi lavori è quello di regolare le mappe delle caratteristiche considerando la correlazione spazio-canale all'interno di uno strato di convoluzione. In questo articolo, ci concentriamo sulle relazioni di modellazione tra gli strati e proponiamo una nuova struttura, 'Recurrent Layer Attention network', che memorizza la gerarchia delle caratteristiche in reti neurali ricorrenti (RNN) che si propagano simultaneamente con CNN e scala in modo adattivo i volumi delle caratteristiche di tutti gli strati.introduciamo inoltre diversi derivati strutturali per dimostrare la compatibilità sui moduli di attenzione recenti e l'espandibilità della rete proposta.per la comprensione semantica sulle caratteristiche apprese, visualizziamo anche gli strati intermedi e tracciamo la curva dei coefficienti di scala degli strati (cioè, La rete Recurrent Layer Attention raggiunge un significativo miglioramento delle prestazioni che richiede un leggero aumento dei parametri in un compito di classificazione delle immagini con il dataset CIFAR e ImageNet-1K 2012 e un compito di rilevamento degli oggetti con il dataset Microsoft COCO 2014.
Le reti neurali biologiche (BNN) eccellono nell'apprendimento veloce, il che implica che estraggono caratteristiche altamente informative. In particolare, la rete olfattiva dell'insetto impara nuovi odori molto rapidamente, per mezzo di tre elementi chiave: In questo lavoro impieghiamo MothNet, un modello computazionale della rete olfattiva di falena, come un generatore automatico di caratteristiche, che viene collegato come un pre-processore front-end, i neuroni di lettura di MothNet forniscono nuove caratteristiche, derivate dalle caratteristiche originali, per l'uso da parte di classificatori ML standard. Questi ``insetti cyborg'' (parte BNN e parte metodo ML) hanno prestazioni significativamente migliori rispetto ai metodi ML di base da soli sui set di dati MNIST e Omniglot vettorizzati, riducendo le medie di errore del test set dal 20% al 55%. Il generatore di caratteristiche MothNet inoltre supera sostanzialmente le prestazioni di altri metodi di generazione di caratteristiche tra cui PCA, PLS e NNs.
Presentiamo un approccio per le previsioni in qualsiasi momento nelle reti neurali profonde (DNNs).Per ogni campione di prova, un predittore in qualsiasi momento produce rapidamente un risultato grossolano, e poi continua a raffinarlo fino a quando il budget computazionale del test-time è esaurito.Tali predittori possono affrontare il crescente problema computazionale delle DNNs adattandosi automaticamente al variare del budget di test-time. In questo lavoro, studiamo un incremento \emph{generale} alle reti feed-forward per formare reti neurali (ANNs) anytime tramite previsioni ausiliarie e perdite.In particolare, evidenziamo un punto cieco nei recenti studi su tali ANNs: l'importanza di un'alta accuratezza finale. Infatti, mostriamo su più serie di dati e architetture di riconoscimento che, avendo previsioni finali quasi ottimali in piccoli modelli anytime, possiamo effettivamente raddoppiare la velocità di quelli grandi per raggiungere il livello di accuratezza corrispondente.Otteniamo tale accelerazione con una semplice ponderazione delle perdite anytime che oscillano durante l'addestramento.Assembliamo anche una sequenza di ANNs esponenzialmente più profonde, per ottenere sia teoricamente che praticamente risultati anytime quasi ottimali a qualsiasi budget, al costo di una frazione costante di budget consumato aggiuntivo.
I sistemi di imaging computazionale progettano congiuntamente il calcolo e l'hardware per recuperare informazioni che non sono tradizionalmente accessibili con i sistemi di imaging standard.Recentemente, aspetti critici come il design sperimentale e i priori dell'immagine sono ottimizzati attraverso reti neurali profonde formate da iterazioni srotolate di ricostruzioni classiche basate sulla fisica (dette reti basate sulla fisica). Tuttavia, per i sistemi su larga scala del mondo reale, il calcolo dei gradienti tramite backpropagation limita l'apprendimento a causa delle limitazioni di memoria delle unità di elaborazione grafica. In questo lavoro, proponiamo una procedura di apprendimento efficiente dal punto di vista della memoria che sfrutta la reversibilità degli strati della rete per consentire la progettazione guidata dai dati per l'imaging computazionale su larga scala.dimostriamo la praticità dei nostri metodi su due sistemi su larga scala: microscopia ottica di super risoluzione e risonanza magnetica multicanale.
I metodi di comunicazione esistenti di apprendimento di rinforzo multi-agente (MARL) si sono basati su una terza parte fidata (TTP) per distribuire la ricompensa agli agenti, lasciandoli inapplicabili in ambienti peer-to-peer. (NaaA) in MARL senza un TTP con due idee chiave: (i) la distribuzione della ricompensa tra gli agenti e (ii) la teoria delle aste.La teoria delle aste è introdotta perché la distribuzione della ricompensa tra gli agenti è insufficiente per l'ottimizzazione.Gli agenti in NaaA massimizzano i loro profitti (la differenza tra la ricompensa e il costo) e, come risultato teorico, il meccanismo delle aste è dimostrato per avere agenti che valutano autonomamente i rendimenti controfattuali come i valori degli altri agenti. NaaA permette di rappresentare gli scambi in ambienti peer-to-peer, considerando in ultima analisi l'unità nelle reti neurali come agenti.Infine, gli esperimenti numerici (un ambiente mono-agente di OpenAI Gym e un ambiente multi-agente di ViZDoom) confermano che l'ottimizzazione del framework NaaA porta a migliori prestazioni nell'apprendimento per rinforzo.
Aumentando la dimensione del modello durante il pretraining delle rappresentazioni del linguaggio naturale spesso si ottengono migliori prestazioni sui compiti a valle.Tuttavia, ad un certo punto ulteriori aumenti del modello diventano più difficili a causa delle limitazioni di memoria della GPU/TPU, dei tempi di formazione più lunghi e della degradazione imprevista del modello.Per affrontare questi problemi, presentiamo due tecniche di riduzione dei parametri per ridurre il consumo di memoria e aumentare la velocità di formazione del BERT. Usiamo anche una perdita auto-supervisionata che si concentra sulla modellazione della coerenza tra le frasi, e dimostriamo che aiuta in modo consistente i compiti a valle con input multisentenza. Di conseguenza, il nostro miglior modello stabilisce nuovi risultati allo stato dell'arte sui benchmark GLUE, RACE e SQuAD pur avendo meno parametri rispetto a BERT-large.
I dati tabellari strutturati sono la forma di dati più comunemente usata nell'industria secondo un sondaggio Kaggle ML e DS.Gradient Boosting Trees, Support Vector Machine, Random Forest, e Logistic Regression sono tipicamente usati per compiti di classificazione su dati tabellari.Il recente lavoro del metodo Super Characters usando embeddings di parole bidimensionali ha raggiunto risultati all'avanguardia in compiti di classificazione del testo, mostrando la promessa di questo nuovo approccio. In questo articolo, proponiamo il metodo SuperTML, che prende in prestito l'idea del metodo Super Characters e delle embeddings bidimensionali per affrontare il problema della classificazione su dati tabulari.Per ogni input di dati tabulari, le caratteristiche sono prima proiettate in embeddings bidimensionali come un'immagine, e poi questa immagine è alimentata in modelli ImageNet CNN per la classificazione.I risultati sperimentali hanno dimostrato che il metodo SuperTML proposto ha raggiunto risultati all'avanguardia su set di dati sia grandi che piccoli.
La codifica predittiva, nell'ambito delle neuroscienze teoriche, e gli autoencoder variazionali, nell'ambito dell'apprendimento automatico, coinvolgono entrambi i modelli gaussiani latenti e l'inferenza variazionale. Mentre queste aree condividono un'origine comune, si sono evolute in modo largamente indipendente. Delineiamo connessioni e contrasti tra queste aree, usando le loro relazioni per identificare nuovi parallelismi tra l'apprendimento automatico e le neuroscienze.
I metodi di ottimizzazione adattivi come AdaGrad, RMSprop e Adam sono stati proposti per ottenere un processo di addestramento rapido con un termine di scala elementare sui tassi di apprendimento.Anche se prevalenti, si osserva che generalizzano male rispetto a SGD o addirittura non riescono a convergere a causa di tassi di apprendimento instabili ed estremi.Un lavoro recente ha proposto alcuni algoritmi come AMSGrad per affrontare questo problema, ma non sono riusciti a ottenere un miglioramento notevole rispetto ai metodi esistenti.Nel nostro articolo, dimostriamo che i tassi di apprendimento estremi possono portare a prestazioni scadenti. Forniamo nuove varianti di Adam e AMSGrad, chiamate rispettivamente AdaBound e AMSBound, che impiegano limiti dinamici sui tassi di apprendimento per ottenere una transizione graduale e regolare dai metodi adattivi a SGD e diamo una prova teorica della convergenza. I risultati sperimentali mostrano che le nuove varianti possono eliminare il gap di generalizzazione tra i metodi adattativi e SGD e mantenere una maggiore velocità di apprendimento all'inizio della formazione allo stesso tempo.Inoltre, possono portare un miglioramento significativo rispetto ai loro prototipi, soprattutto su reti profonde complesse.L'implementazione dell'algoritmo può essere trovata su https://github.com/Luolc/AdaBound .
Un problema importante che si presenta nell'apprendimento di rinforzo e nei metodi Monte Carlo è la stima di quantità definite dalla distribuzione stazionaria di una catena di Markov.In molte applicazioni del mondo reale, l'accesso all'operatore di transizione sottostante è limitato a un insieme fisso di dati che sono già stati raccolti, senza che siano disponibili ulteriori interazioni con l'ambiente.Mostriamo che la stima coerente rimane possibile in questo scenario, e che una stima efficace può ancora essere ottenuta in applicazioni importanti. Il nostro approccio si basa sulla stima di un rapporto che corregge la discrepanza tra le distribuzioni stazionarie ed empiriche, derivato dalle proprietà fondamentali della distribuzione stazionaria, e sfruttando le riformulazioni dei vincoli basate sulla minimizzazione della divergenza variazionale. l'algoritmo risultante, GenDICE, è semplice ed efficace. dimostriamo la coerenza del metodo in condizioni generali, forniamo un'analisi dettagliata degli errori, e dimostriamo forti prestazioni empiriche su compiti di riferimento, tra cui PageRank off-line e valutazione delle politiche off-policy.
Il potere delle reti neurali risiede nella loro capacità di generalizzare a dati non visti, ma le ragioni sottostanti a questo fenomeno rimangono elusive. Sono stati fatti numerosi tentativi rigorosi per spiegare la generalizzazione, ma i limiti disponibili sono ancora piuttosto vaghi, e l'analisi non sempre porta a una vera comprensione.L'obiettivo di questo lavoro è rendere la generalizzazione più intuitiva.
Sosteniamo che la simmetria è una considerazione importante nell'affrontare il problema della sistematicità e studiamo due forme di simmetria rilevanti per i processi simbolici. Implementiamo questo approccio in termini di convoluzione e mostriamo che può essere usato per ottenere una generalizzazione efficace in tre problemi giocattolo: apprendimento di regole, composizione e apprendimento della grammatica.
Fenomeni climatici equatoriali chiave come la QBO e l'ENSO non sono mai stati adeguatamente spiegati come processi deterministici, nonostante le recenti ricerche mostrino prove crescenti di un comportamento prevedibile. Questo studio applica le fondamentali equazioni di marea di Laplace con ipotesi semplificative lungo l'equatore - cioè nessuna forza di Coriolis e un'approssimazione di un piccolo angolo - le soluzioni alle equazioni differenziali parziali sono altamente non lineari relative a Navier-Stokes e solo approcci di ricerca possono essere usati per adattarsi ai dati.
Nel campo dell'Apprendimento Continuo, l'obiettivo è quello di imparare diversi compiti uno dopo l'altro senza accedere ai dati dei compiti precedenti.Sono state proposte diverse soluzioni per affrontare questo problema, ma di solito assumono che l'utente sappia quale dei compiti eseguire al momento del test su un particolare campione, o si basano su piccoli campioni di dati precedenti e la maggior parte di loro soffre di un sostanziale calo di precisione quando viene aggiornato con lotti di una sola classe alla volta.In questo articolo, proponiamo un nuovo metodo, OvA-INN, che è in grado di imparare una classe alla volta e senza memorizzare nessuno dei dati precedenti. Per ottenere questo, per ogni classe, addestriamo una specifica rete neurale invertibile per produrre il vettore zero per la sua classe.Al momento del test, possiamo prevedere la classe di un campione identificando quale rete produce il vettore con la norma più piccola.Con questo metodo, dimostriamo che possiamo trarre vantaggio dai modelli preaddestrati impilando una rete invertibile sopra un estrattore di caratteristiche. In questo modo, siamo in grado di superare lo stato dell'arte degli approcci che si basano sull'apprendimento delle caratteristiche per l'apprendimento continuo dei dataset MNIST e CIFAR-100. Nei nostri esperimenti, stiamo raggiungendo il 72% di precisione su CIFAR-100 dopo aver addestrato il nostro modello una classe alla volta.
I sistemi di conversazione uomo-computer hanno attirato molta attenzione nel Natural Language Processing.I sistemi di conversazione possono essere approssimativamente divisi in due categorie: sistemi basati sul recupero e sistemi basati sulla generazione.I sistemi di recupero cercano un enunciato emesso dall'utente (cioè una query) in un grande archivio di conversazione e restituiscono una risposta che corrisponde meglio alla query.Gli approcci generativi sintetizzano nuove risposte.Entrambi i modi hanno alcuni vantaggi ma soffrono dei propri svantaggi. I candidati recuperati, oltre alla domanda originale, sono alimentati a un generatore di risposte attraverso una rete neurale, in modo che il modello sia a conoscenza di più informazioni. La risposta generata insieme a quelle recuperate partecipa poi a un processo di ri-ranking per trovare la risposta finale in uscita.
Le reti neurali profonde addestrate su una vasta gamma di set di dati dimostrano un'impressionante trasferibilità. Le caratteristiche profonde appaiono generali, nel senso che sono applicabili a molti set di dati e compiti. Tale proprietà è in uso prevalente nelle applicazioni del mondo reale. Nonostante la sua pervasività, pochi sforzi sono stati dedicati a scoprire la ragione della trasferibilità nelle rappresentazioni di caratteristiche profonde. Questo articolo cerca di capire la trasferibilità dal punto di vista della generalizzazione migliorata, dell'ottimizzazione e della fattibilità della trasferibilità. Dimostriamo che1) i modelli trasferiti tendono a trovare minimi più piatti, poiché le loro matrici di peso rimangono vicine alla regione piatta originale dei parametri preaddestrati quando vengono trasferiti a un set di dati simile; 2) le rappresentazioni trasferite rendono il panorama delle perdite più favorevole con una migliore Lipschitzness, che accelera e stabilizza sostanzialmente l'addestramento; il miglioramento è dovuto in gran parte al fatto che la componente principale del gradiente viene soppressa nei parametri preaddestrati, stabilizzando così la grandezza del gradiente nella back-propagation.3 ) La fattibilità della trasferibilità è legata alla somiglianza sia dell'input che dell'etichetta.E una scoperta sorprendente è che la fattibilità è anche influenzata dalle fasi di addestramento in quanto la trasferibilità aumenta prima durante l'addestramento e poi diminuisce.Forniamo inoltre un'analisi teorica per verificare le nostre osservazioni.
Affrontiamo la seguente domanda: Quanto è ridondante la parametrizzazione delle reti ReLU? In particolare, consideriamo trasformazioni dello spazio dei pesi che lasciano intatta la funzione implementata dalla rete.Due trasformazioni di questo tipo sono note per le architetture feed-forward: la permutazione dei neuroni all'interno di uno strato, e lo scaling positivo di tutti i pesi in entrata di un neurone accoppiato allo scaling inverso dei suoi pesi in uscita.In questo lavoro, mostriamo per le architetture con larghezze non crescenti che la permutazione e lo scaling sono in effetti le uniche trasformazioni di peso che preservano la funzione. Per ogni architettura ammissibile diamo una costruzione esplicita di una rete neurale tale che ogni altra rete che implementa la stessa funzione può essere ottenuta da quella originale mediante l'applicazione di permutazioni e ridimensionamenti. La dimostrazione si basa su una comprensione geometrica dei confini tra regioni lineari di reti ReLU, e speriamo che gli strumenti matematici sviluppati siano di interesse indipendente.
Un problema generale che ha ricevuto una considerevole attenzione di recente è come eseguire più compiti nella stessa rete, massimizzando sia l'efficienza che l'accuratezza della previsione. Un approccio popolare consiste in un'architettura a più rami sopra una dorsale condivisa, addestrata congiuntamente su una somma ponderata di perdite. Tuttavia, in molti casi, la rappresentazione condivisa risulta in prestazioni non ottimali, principalmente a causa di un'interferenza tra gradienti contrastanti di compiti non correlati. La nostra architettura non utilizza rami specifici delle attività, né moduli specifici delle attività, ma una rete di modulazione dall'alto verso il basso che è condivisa tra tutte le attività. Mostriamo l'efficacia del nostro schema raggiungendo risultati alla pari o migliori rispetto agli approcci alternativi su set di attività correlate e non correlate, dimostrando inoltre i nostri vantaggi in termini di dimensioni del modello, l'aggiunta di nuove attività e l'interpretabilità. Il codice sarà rilasciato.
Gli algoritmi di clustering hanno ampie applicazioni e giocano un ruolo importante nell'analisi dei dati, inclusa l'analisi delle serie temporali. La performance di un algoritmo di clustering dipende dalle caratteristiche estratte dai dati. Tuttavia, nell'analisi delle serie temporali, c'è stato un problema che i metodi convenzionali basati sulla forma del segnale sono instabili per le variazioni di fase, ampiezza e lunghezza del segnale. In questo articolo, proponiamo un nuovo algoritmo di clustering incentrato sull'aspetto del sistema dinamico del segnale utilizzando una rete neurale ricorrente e il metodo Bayes variazionale. I nostri esperimenti mostrano che il nostro algoritmo proposto ha una robustezza contro le variazioni di cui sopra e aumenta le prestazioni di classificazione.
Due argomenti importanti nell'apprendimento profondo coinvolgono entrambi l'incorporazione di esseri umani nel processo di modellazione: I priori del modello trasferiscono informazioni dagli esseri umani a un modello regolarizzando i parametri del modello; le attribuzioni del modello trasferiscono informazioni da un modello agli esseri umani spiegando il comportamento del modello.Il lavoro precedente ha fatto passi importanti per collegare questi argomenti attraverso varie forme di regolarizzazione del gradiente.Troviamo, tuttavia, che i metodi esistenti che utilizzano le attribuzioni per allineare il comportamento di un modello con l'intuizione umana sono inefficaci. I nostri esperimenti dimostrano che i modelli addestrati con i priori di attribuzione sono più intuitivi e raggiungono prestazioni di generalizzazione migliori sia delle linee di base equivalenti che dei metodi esistenti per regolarizzare il comportamento del modello.
Le reti neurali ricorrenti (RNN) hanno mostrato prestazioni eccellenti nell'elaborazione di dati sequenziali, ma sono complesse e richiedono molta memoria a causa della loro natura ricorsiva. Queste limitazioni rendono le RNN difficili da incorporare nei dispositivi mobili che richiedono processi in tempo reale con risorse hardware limitate. Per affrontare i problemi di cui sopra, introduciamo un metodo che può imparare pesi binari e ternari durante la fase di formazione per facilitare le implementazioni hardware di RNNs.Come risultato, utilizzando questo approccio sostituisce tutte le operazioni di moltiplicazione-accumulo da semplici accumuli, portando vantaggi significativi per l'hardware personalizzato in termini di area di silicio e consumo energetico. Sul lato software, valutiamo le prestazioni (in termini di accuratezza) del nostro metodo utilizzando memorie a breve termine lunghe (LSTMs) e unità ricorrenti gated (GRUs) su vari modelli sequenziali tra cui la classificazione delle sequenze e la modellazione del linguaggio.dimostriamo che il nostro metodo raggiunge risultati competitivi sui compiti di cui sopra, pur utilizzando pesi binari/ternari durante il runtime. Per quanto riguarda l'hardware, presentiamo un hardware personalizzato per accelerare i calcoli ricorrenti di LSTM con pesi binari/ternari; in definitiva, dimostriamo che LSTM con pesi binari/ternari possono raggiungere un risparmio di memoria fino a 12 volte e un'accelerazione dell'inferenza fino a 10 volte rispetto al progetto di implementazione hardware a piena precisione.
Questo articolo affronta il riconoscimento non supervisionato di oggetti a pochi scatti, dove tutte le immagini di allenamento non sono etichettate e non condividono le classi con le immagini di supporto etichettate per il riconoscimento a pochi scatti nei test. Usiamo una nuova architettura profonda simile a GAN che mira all'apprendimento non supervisionato di una rappresentazione dell'immagine che codificherà le parti latenti dell'oggetto e quindi generalizzerà bene alle classi non viste nel nostro compito di riconoscimento a pochi scatti. Il nostro addestramento non supervisionato integra l'apprendimento adversariale, l'auto-supervisione e l'apprendimento metrico profondo; in primo luogo, estendiamo il GAN vaniglia con perdita di ricostruzione per far sì che il discriminatore catturi le caratteristiche più rilevanti delle immagini "false" generate da codici campionati a caso. In secondo luogo, compiliamo un set di formazione di esempi di immagini in tripletta per stimare la perdita di tripletta nell'apprendimento metrico utilizzando una procedura di mascheramento dell'immagine opportunamente progettata per identificare le parti latenti dell'oggetto.Quindi, l'apprendimento metrico assicura che la rappresentazione profonda delle immagini che mostrano classi di oggetti simili che condividono alcune parti siano più vicine delle rappresentazioni delle immagini che non hanno parti comuni. I nostri risultati mostrano che superiamo significativamente lo stato dell'arte, così come otteniamo prestazioni simili al comune addestramento episodico per l'apprendimento completamente supervisionato di pochi scatti sui set di dati Mini-Imagenet e Tiered-Imagenet.
Gli attacchi black-box esistenti sulle reti neurali profonde (DNN) finora si sono in gran parte concentrati sulla trasferibilità, dove un'istanza avversaria generata per un modello addestrato localmente può "trasferirsi" per attaccare altri modelli di apprendimento.In questo articolo, proponiamo nuovi attacchi black-box Gradient Estimation per gli avversari con accesso alla query alle probabilità di classe del modello target, che non si basano sulla trasferibilità. Proponiamo anche strategie per disaccoppiare il numero di query necessarie per generare ogni campione avversario dalla dimensionalità dell'input. Una variante iterativa del nostro attacco raggiunge quasi il 100% di successo avversario sia per attacchi mirati che non mirati su DNNs. Eseguiamo ampi esperimenti per una valutazione comparativa approfondita degli attacchi black-box e dimostriamo che gli attacchi Gradient Estimation proposti superano tutti gli attacchi black-box basati sulla trasferibilità che abbiamo testato su entrambi i dataset MNIST e CIFAR-10, raggiungendo tassi di successo avversari simili ai ben noti attacchi white-box all'avanguardia. Applichiamo anche gli attacchi Gradient Estimation con successo contro un classiﬁer di moderazione dei contenuti del mondo reale ospitato da Clarifai.Inoltre, valutiamo gli attacchi black-box contro le difese state-of-the-art. Mostriamo che gli attacchi Gradient Estimation sono molto efficaci anche contro queste difese.
Proponiamo una nuova rappresentazione dell'immagine del sottografo per la classificazione dei frammenti di rete con l'obiettivo di essere le loro reti madri.La rappresentazione dell'immagine del grafo è basata su embeddings di immagini 2D di matrici di adiacenza.Usiamo questa rappresentazione dell'immagine in due modi.Primo, come input per un algoritmo di apprendimento automatico. Le nostre conclusioni da più serie di dati sono che1. l'apprendimento profondo che utilizza le caratteristiche strutturate dell'immagine è il migliore rispetto al kernel del grafico e ai metodi classici basati sulle caratteristiche; e, 2. l'apprendimento puro di trasferimento funziona efficacemente con un'interferenza minima da parte dell'utente ed è robusto contro i dati piccoli.
Questa carta esplora l'uso di auto-assemblaggio per problemi di adattamento del dominio visivo.La nostra tecnica è derivata dalla variante insegnante media (Tarvainen et. al 2017) di ensembling temporale (Laine et al. 2017), una tecnica che ha raggiunto lo stato dell'arte dei risultati nel settore dell'apprendimento semi-supervisionato.Introduciamo una serie di modifiche al loro approccio per scenari di adattamento del dominio impegnativi e valutiamo la sua efficacia. Il nostro approccio raggiunge lo stato dell'arte in una varietà di benchmark, compresa la nostra voce vincente nel VISDA-2017 visual domain adaptation challenge.In piccoli benchmark di immagini, il nostro algoritmo non solo supera l'arte precedente, ma può anche raggiungere una precisione che è vicina a quella di un classificatore addestrato in modo supervisionato.
È facile per le persone immaginare come sia un uomo con i capelli rosa, anche se non hanno mai visto una persona del genere prima d'ora.Chiamiamo la capacità di creare immagini di nuovi concetti semantici immaginazione visivamente fondata.In questo articolo, mostriamo come possiamo modificare gli autocodificatori variazionali per eseguire questo compito.Il nostro metodo utilizza un nuovo obiettivo di formazione e una nuova rete di inferenza prodotto di esperti, che può gestire concetti parzialmente specificati (astratti) in un modo efficiente e basato su principi. Proponiamo anche una serie di metriche di valutazione facili da calcolare che catturano le nostre nozioni intuitive di ciò che significa avere una buona immaginazione visiva, vale a dire correttezza, copertura e composizionalità (le 3 C).Infine, eseguiamo un confronto dettagliato del nostro metodo con due metodi VAE congiunti immagine-attributo esistenti (il metodo JMVAE di Suzuki et al, 2017 e il metodo BiVCCA di Wang et al., 2016) applicandoli a due dataset: il dataset MNIST-with-attributes (che presentiamo qui), e il dataset CelebA (Liu et al., 2015).
Introduciamo "Search with Amortized Value Estimates" (SAVE), un approccio per combinare il Q-learning senza modello con la ricerca ad albero Monte-Carlo basata sul modello (MCTS).In SAVE, un priore appreso sui valori stato-azione viene utilizzato per guidare MCTS, che stima un set migliorato di valori stato-azione.Le nuove stime Q sono quindi utilizzate in combinazione con l'esperienza reale per aggiornare il priore.Questo ammortizza efficacemente il calcolo del valore eseguito da MCTS, risultando in una relazione cooperativa tra apprendimento senza modello e ricerca basata sul modello. SAVE può essere implementato su qualsiasi agente di Q-learning con accesso a un modello, cosa che dimostriamo incorporandolo in agenti che eseguono impegnativi compiti di ragionamento fisico e Atari.SAVE raggiunge costantemente ricompense più alte con meno passi di addestramento, e--in contrasto con i tipici approcci di ricerca basati sul modello--rende forti prestazioni con budget di ricerca molto piccoli.Combinando l'esperienza reale con informazioni calcolate durante la ricerca, SAVE dimostra che è possibile migliorare sia le prestazioni dell'apprendimento senza modello che il costo computazionale della pianificazione.
Due famiglie principali di algoritmi di apprendimento di rinforzo, Q-learning e gradienti di policy, hanno recentemente dimostrato di essere equivalenti quando si usa un rilassamento softmax da una parte, e una regolarizzazione entropica dall'altra. Mettiamo in relazione questo risultato con la ben nota dualità convessa dell'entropia di Shannon e la funzione softmax.
La simulazione al computer fornisce un modo automatico e sicuro per addestrare le politiche di controllo robotico per realizzare compiti complessi come la locomozione.Tuttavia, una politica addestrata nella simulazione di solito non si trasferisce direttamente all'hardware reale a causa delle differenze tra i due ambienti.L'apprendimento di trasferimento utilizzando la randomizzazione del dominio è un approccio promettente, ma di solito presuppone che l'ambiente di destinazione sia vicino alla distribuzione degli ambienti di allenamento, quindi si basa molto sull'identificazione accurata del sistema.In questo articolo, presentiamo un approccio diverso che sfrutta la randomizzazione del dominio per trasferire le politiche di controllo ad ambienti noti. L'idea chiave è che, invece di imparare una singola politica nella simulazione, impariamo simultaneamente una famiglia di politiche che mostrano comportamenti diversi. Quando testato nell'ambiente di destinazione, cerchiamo direttamente la migliore politica nella famiglia basata sulle prestazioni del compito, senza la necessità di identificare i parametri dinamici. Valutiamo il nostro metodo su cinque problemi di controllo robotico simulati con diverse discrepanze nell'ambiente di formazione e di test e dimostriamo che il nostro metodo può superare errori di modellazione più grandi rispetto alla formazione di una politica robusta o una politica adattiva.
Proponiamo vq-wav2vec per imparare rappresentazioni discrete di segmenti audio attraverso un compito di predizione del contesto auto-supervisionato in stile wav2vec.L'algoritmo utilizza un softmax gumbel o un clustering k-means online per quantizzare le rappresentazioni dense.La discretizzazione permette l'applicazione diretta di algoritmi della comunità NLP che richiedono input discreti.Gli esperimenti mostrano che il pre-training BERT raggiunge un nuovo stato dell'arte nella classificazione dei fonemi TIMIT e nel riconoscimento vocale WSJ.
Gli algoritmi di Deep Reinforcement Learning portano ad agenti che possono risolvere difficili problemi decisionali in ambienti complessi.Tuttavia, molti difficili giochi competitivi multi-agente, specialmente i giochi di strategia in tempo reale sono ancora considerati al di là della capacità degli attuali algoritmi di deep reinforcement learning, anche se c'è stato un recente sforzo per cambiare questo \citep{openai_2017_dota, vinyals_2017_starcraft}. Inoltre, quando gli avversari in un gioco competitivo sono subottimali, gli attuali algoritmi di ricerca dell'equilibrio di Nash sono spesso incapaci di generalizzare le loro strategie ad avversari che giocano strategie molto diverse dalle loro. Sviluppiamo Hierarchical Agent with Self-play (HASP), un approccio di apprendimento per ottenere politiche gerarchicamente strutturate che possono raggiungere prestazioni più elevate rispetto al self-play convenzionale su giochi competitivi attraverso l'uso di un pool diversificato di sotto-politiche che otteniamo dal Counter Self-Play (CSP).dimostriamo che la politica ensemble generata da HASP può raggiungere prestazioni migliori mentre affronta avversari invisibili che usano politiche sub-ottimali. Su un motivante gioco iterato Rock-Paper-Scissor e su un gioco strategico in tempo reale parzialmente osservabile (http://generals.io/), siamo giunti alla conclusione che HASP può avere prestazioni migliori del convenzionale self-play e raggiungere un tasso di vittoria del 77% contro FloBot, un agente open-source che si è classificato alla posizione numero 2 nelle classifiche online.
Introduciamo rappresentazioni di input adattive per la modellazione neurale del linguaggio che estendono il softmax adattivo di Grave et al. (2017) a rappresentazioni di input di capacità variabile.Ci sono diverse scelte su come fattorizzare gli strati di input e di output, e se modellare parole, caratteri o unità di sub-parola.Eseguiamo un confronto sistematico delle scelte popolari per un'architettura auto-attentiva. I nostri esperimenti dimostrano che i modelli dotati di embeddings adattivi sono più del doppio più veloci da addestrare rispetto alla popolare CNN di input dei caratteri, pur avendo un numero inferiore di parametri.Sul benchmark WikiText-103 raggiungiamo 18.7 perplessità, un miglioramento di 10.5 perplessità rispetto al risultato precedentemente pubblicato e sul benchmark Billion Word, raggiungiamo 23.02 perplessità.
In questo articolo, consideriamo il problema del rilevamento di oggetti sotto occlusione.La maggior parte dei rilevatori di oggetti formulano la regressione del bounding box come un compito unimodale (cioè, Tuttavia, osserviamo che i confini del bounding box di un oggetto occluso possono avere molteplici configurazioni plausibili; inoltre, i confini del bounding box occlusi hanno correlazioni con quelli visibili; motivati da queste due osservazioni, proponiamo una profonda miscela multivariata di modelli gaussiani per la regressione del bounding box sotto occlusione. Le componenti della miscela apprendono potenzialmente diverse configurazioni di una parte occlusa, e le covarianze tra le variabili aiutano ad apprendere la relazione tra le parti occluse e quelle visibili.Quantitativamente, il nostro modello migliora l'AP delle linee di base del 3,9% e dell'1,2% su CrowdHuman e MS-COCOCO rispettivamente con quasi nessun overhead computazionale o di memoria.Qualitativamente, il nostro modello gode di spiegabilità poiché possiamo interpretare i bounding box risultanti attraverso le matrici di covarianza e le componenti della miscela.
Gli esempi avversi sono un fenomeno pervasivo dei modelli di apprendimento automatico in cui perturbazioni apparentemente impercettibili all'input portano a errori di classificazione per modelli altrimenti statisticamente accurati.  L'addestramento avversario, una delle difese empiriche di maggior successo contro gli esempi avversari, si riferisce all'addestramento su esempi avversari generati all'interno di un set di vincoli geometrici. Il vincolo geometrico più comunemente usato è una palla $L_p$ di raggio $\epsilon$ in qualche norma. Introduciamo l'addestramento avversario con i vincoli di Voronoi, che sostituisce il vincolo $L_p$-ball con la cella di Voronoi per ogni punto del set di addestramento. Mostriamo che l'addestramento avversario con i vincoli di Voronoi produce modelli robusti che migliorano significativamente rispetto allo stato dell'arte su MNIST e sono competitivi su CIFAR-10.
I recenti sforzi di ricerca permettono di studiare la navigazione basata sul linguaggio naturale in ambienti fotorealistici, ad es, Tuttavia, i metodi esistenti tendono ad adattare eccessivamente i dati di formazione in ambienti visti e non riescono a generalizzare bene in ambienti precedentemente non visti. Al fine di colmare il divario tra gli ambienti visti e non visti, miriamo ad apprendere un modello di navigazione generalizzabile da due nuove prospettive: (1) introduciamo un modello di navigazione multitask che può essere addestrato senza soluzione di continuità sia sulla navigazione Vision-Language (VLN) che sulla navigazione dalla storia del dialogo (NDH), che beneficia di una guida più ricca in linguaggio naturale e trasferisce efficacemente la conoscenza tra le attività; (2) proponiamo di imparare rappresentazioni ambiente-agnostiche per la politica di navigazione che sono invarianti tra gli ambienti, generalizzando così meglio sugli ambienti non visti. Esperimenti approfonditi dimostrano che il nostro modello di navigazione multitask indipendente dall'ambiente riduce significativamente il divario di prestazioni tra ambienti visti e non visti e supera le prestazioni di base su ambienti non visti del 16% (misura relativa del tasso di successo) su VLN e del 120% (progresso dell'obiettivo) su NDH, stabilendo il nuovo stato dell'arte per il compito NDH.
In questo articolo proponiamo di eseguire l'ensembling dei modelli in un'impostazione di apprendimento multiclasse o multilabel utilizzando i baricentri di Wasserstein (W.).Le metriche di trasporto ottimali, come la distanza di Wasserstein, permettono di incorporare informazioni semantiche collaterali come le embeddings delle parole.Utilizzando i baricentri di W. I baricentri per trovare il consenso tra i modelli ci permettono di bilanciare la fiducia e la semantica nel trovare l'accordo tra i modelli.Mostriamo applicazioni di Wasserstein ensembling nella classificazione basata sugli attributi, nell'apprendimento multilabel e nella generazione di didascalie di immagini.Questi risultati mostrano che il W. ensembling è una valida alternativa all'ensembling geometrico o aritmetico di base.
Mentre l'apprendimento profondo ha avuto un incredibile successo nella modellazione di compiti con grandi set di dati etichettati e accuratamente curati, la sua applicazione a problemi con dati etichettati limitati rimane una sfida. Lo scopo del presente lavoro è quello di migliorare l'efficienza dell'etichetta di grandi reti neurali che operano su dati audio attraverso una combinazione di apprendimento multitask e apprendimento auto-supervisionato su dati non etichettati.Abbiamo addestrato un estrattore di caratteristiche audio end-to-end basato su WaveNet che alimenta reti neurali semplici, ma versatili per compiti specifici. Descriviamo diversi compiti di apprendimento auto-supervisionato facilmente implementati che possono operare su qualsiasi grande corpus audio non etichettato.Dimostriamo che, in scenari con dati di formazione etichettati limitati, si possono migliorare significativamente le prestazioni di tre diversi compiti di classificazione supervisionati individualmente fino al 6% attraverso l'allenamento simultaneo con questi compiti aggiuntivi auto-supervisionati.Mostriamo anche che incorporare l'aumento dei dati nella nostra impostazione multitask porta a ulteriori guadagni nelle prestazioni.
I recenti modelli di rete neurale e di linguaggio hanno iniziato a fare affidamento su distribuzioni softmax con un numero estremamente grande di categorie. In questo contesto, calcolare la costante di normalizzazione softmax è proibitivamente costoso e questo ha stimolato una crescente letteratura di stime efficientemente calcolabili ma distorte della softmax. In questo articolo presentiamo i primi due algoritmi imparziali per massimizzare la probabilità di softmax il cui lavoro per iterazione è indipendente dal numero di classi e di punti dati (e non richiede lavoro extra alla fine di ogni epoca).
La creazione di esempi contraddittori su input discreti come le sequenze di testo è fondamentalmente diversa dalla generazione di tali esempi per input continui come le immagini.Questo articolo cerca di rispondere alla domanda: in un'impostazione black-box, possiamo creare automaticamente esempi contraddittori per ingannare efficacemente i classificatori di deep learning sui testi apportando modifiche impercettibili? Gli sforzi precedenti si sono per lo più basati sull'uso di prove a gradiente, e sono meno efficaci sia perché trovare automaticamente la parola più vicina (rispetto al significato) è difficile, sia perché si basano pesantemente su regole linguistiche costruite a mano. Noi, invece, usiamo la ricerca ad albero Monte Carlo (MCTS) per trovare le poche parole più importanti da perturbare ed eseguire un attacco omoglifo sostituendo un carattere in ogni parola selezionata con un simbolo di forma identica.  Il nostro nuovo algoritmo, che chiamiamo MCTSBug, è black-box ed estremamente efficace allo stesso tempo. I nostri risultati sperimentali indicano che MCTSBug può ingannare i classificatori di deep learning con tassi di successo del 95% su sette set di dati di riferimento su larga scala, perturbando solo pochi caratteri.  Sorprendentemente, MCTSBug, senza fare affidamento sulle informazioni del gradiente, è più efficace della baseline white-box basata sul gradiente.Grazie alla natura dell'attacco omoglifo, le perturbazioni avversarie generate sono quasi impercettibili agli occhi umani.
  Introduciamo un modello che impara a convertire semplici disegni a mano in programmi grafici scritti in un sottoinsieme di \LaTeX.~Il modello combina tecniche di apprendimento profondo e sintesi di programmi.  Impariamo una rete neurale convoluzionale che propone primitive di disegno plausibili che spiegano un'immagine; queste primitive di disegno sono come una traccia dell'insieme di comandi primitivi emessi da un programma grafico; impariamo un modello che usa tecniche di sintesi del programma per recuperare un programma grafico da quella traccia; questi programmi hanno costrutti come legami variabili, cicli iterativi o semplici tipi di condizionali; con un programma grafico in mano, possiamo correggere gli errori fatti dalla rete profonda ed estrapolare i disegni.  Presi insieme, questi risultati sono un passo avanti verso agenti che inducono programmi utili e leggibili dall'uomo da input percettivi.
Gli esempi avversari rimangono un problema per le reti neurali contemporanee.Questo articolo attinge al Background Check (Perello-Nieto et al., 2016), una tecnica nella calibrazione del modello, per assistere le reti neurali a due classi nel rilevare gli esempi avversari, utilizzando la differenza unidimensionale tra i valori logit come misura sottostante.Questo metodo tende in modo interessante a raggiungere il più alto richiamo medio sui set di immagini che sono generati con grandi vettori di perturbazione, che è diverso dalla letteratura esistente sugli attacchi avversari (Cubuk et al., Il metodo proposto non ha bisogno della conoscenza dei parametri o dei metodi di attacco al momento dell'addestramento, a differenza di gran parte della letteratura che utilizza metodi basati sull'apprendimento profondo per rilevare gli esempi avversari, come Metzen et al. (2017), dotando il metodo proposto di ulteriore flessibilità.
Presentiamo un nuovo approccio di formazione multi-task per l'apprendimento di rappresentazioni multilingue distribuite del testo. Il nostro sistema impara le incorporazioni di parole e frasi congiuntamente addestrando un modello multilingue skip-gram insieme a un modello di somiglianza della frase cross-lingue. Costruiamo le incorporazioni di frasi elaborando le incorporazioni di parole con un LSTM e facendo una media dei risultati. La nostra architettura può utilizzare in modo trasparente sia i corpora bilingui monolingui che quelli allineati alle frasi per imparare le embeddings multilingue, coprendo così un vocabolario significativamente più grande del vocabolario dei corpora bilingui da soli.Il nostro modello mostra prestazioni competitive in un compito standard di classificazione di documenti multilingue.Mostriamo anche l'efficacia del nostro metodo in uno scenario a basse risorse.
Wasserstein GAN (WGAN) è una struttura promettente per affrontare il problema dell'instabilità in quanto ha una buona proprietà di convergenza. Uno svantaggio di WGAN è che valuta la distanza di Wasserstein nel dominio duale, che richiede una certa approssimazione, in modo che possa non riuscire a ottimizzare la vera distanza di Wasserstein. In questo documento, proponiamo di valutare l'esatto costo di trasporto empirico ottimale in modo efficiente nel dominio primario e di eseguire la discesa del gradiente rispetto alla sua derivata per addestrare la rete del generatore.Gli esperimenti sul dataset MNIST mostrano che il nostro metodo è significativamente stabile per convergere, e raggiunge la più bassa distanza Wasserstein tra le varianti WGAN al costo di una certa nitidezza delle immagini generate.Gli esperimenti sul dataset 8-Gaussian toy mostrano che i migliori gradienti per il generatore sono ottenuti nel nostro metodo.Inoltre, il metodo proposto permette una modellazione generativa più flessibile di WGAN.
Neural Architecture Search (NAS) è un campo nuovo ed eccitante che promette di essere un game-changer tanto quanto lo erano le reti neurali convoluzionali nel 2012.Nonostante molti grandi lavori che portano a miglioramenti sostanziali su una varietà di compiti, il confronto tra diversi metodi è ancora una questione molto aperta.Mentre la maggior parte degli algoritmi sono testati sugli stessi dataset, non c'è un protocollo sperimentale condiviso seguito da tutti.Come tale, e a causa del sottoutilizzo degli studi di ablazione, c'è una mancanza di chiarezza riguardo al perché alcuni metodi sono più efficaci di altri. Il nostro primo contributo è un benchmark di 8 metodi NAS su 5 set di dati. Per superare l'ostacolo di confrontare i metodi con diversi spazi di ricerca, proponiamo di utilizzare il miglioramento relativo di un metodo rispetto all'architettura media campionata in modo casuale, che rimuove efficacemente i vantaggi derivanti da spazi di ricerca o protocolli di formazione progettati da esperti. Eseguiamo ulteriori esperimenti con lo spazio di ricerca DARTS comunemente usato per capire il contributo di ogni componente nella pipeline NAS. Questi esperimenti evidenziano che:(i) l'uso di trucchi nel protocollo di valutazione ha un impatto predominante sulle prestazioni riportate delle architetture;(ii) lo spazio di ricerca basato sulle celle ha una gamma di precisione molto stretta, tale che il seme ha un impatto considerevole sulle classifiche delle architetture;(iii) la macrostruttura progettata a mano (celle) è più importante della microstruttura ricercata (operazioni); e(iv) il gap di profondità è un fenomeno reale, evidenziato dal cambiamento nelle classifiche tra architetture a 8 e 20 celle. Per concludere, suggeriamo le migliori pratiche, che speriamo si rivelino utili per la comunità e aiutino a mitigare le attuali insidie della NAS, ad esempio le difficoltà di riproducibilità e di confronto dei metodi di ricerca. Il codice utilizzato è disponibile su https://github.com/antoyang/NAS-Benchmark.
Identificare analogie tra i domini senza supervisione è un compito chiave per l'intelligenza artificiale.I recenti progressi nella mappatura delle immagini tra domini incrociati si sono concentrati sulla traduzione delle immagini tra i domini.Anche se i progressi fatti sono impressionanti, la fedeltà visiva molte volte non è sufficiente per identificare il campione corrispondente dall'altro dominio.In questo articolo, affrontiamo proprio questo compito di trovare analogie esatte tra set di dati, cioè per ogni immagine dal dominio A trovare un'immagine analoga nel dominio B. Presentiamo un approccio di corrispondenza per sintesi: AN-GAN, e dimostriamo che supera le tecniche attuali. Mostriamo inoltre che il compito di mappatura tra i domini può essere suddiviso in due parti: allineamento dei domini e apprendimento della funzione di mappatura. I compiti possono essere risolti iterativamente, e man mano che l'allineamento viene migliorato, la funzione di traduzione non supervisionata raggiunge una qualità paragonabile alla supervisione completa.
L'inferenza efficace di argomenti latenti discriminanti e coerenti di testi brevi è un compito critico per molte applicazioni del mondo reale. Tuttavia, il compito ha dimostrato di essere una grande sfida per i modelli di argomento tradizionali a causa del problema della scarsità di dati indotta dalle caratteristiche dei testi brevi. In questo articolo, proponiamo un nuovo modello chiamato Neural Variational Sparse Topic Model (NVSTM), basato su un modello di argomento con una maggiore sparsità, chiamato Sparse Topical Coding (STC), in cui le incorporazioni di parole ausiliarie sono utilizzate per migliorare la generazione di rappresentazioni. L'approccio Variational Autoencoder (VAE) è applicato per inferenza il modello in modo efficiente, che rende il modello facile da esplorare estensioni per il suo processo di inferenza black-box.risultati sperimentali su Web Snippets, 20Newsgroups, BBC e dataset biomedici mostrano l'efficacia e l'efficienza del modello.
Neural Tangents è una libreria per lavorare con reti neurali a larghezza infinita, che fornisce un'API di alto livello per specificare architetture di reti neurali complesse e gerarchiche, che possono essere addestrate e valutate sia a larghezza finita come al solito, sia nel loro limite di larghezza infinita. Per le reti ad ampiezza infinita, Neural Tangents esegue l'inferenza esatta sia tramite la regola di Bayes che tramite la discesa del gradiente, e genera i corrispondenti kernel del Processo Gaussiano di Rete Neurale e di Tangente Neurale; inoltre, Neural Tangents fornisce strumenti per studiare le dinamiche di formazione in discesa del gradiente di reti ampie ma finite. L'intera libreria funziona out-of-the-box su CPU, GPU, o TPU.Tutti i calcoli possono essere automaticamente distribuiti su più acceleratori con una scalabilità quasi lineare nel numero di dispositivi. Oltre al repository qui sotto, forniamo un notebook Colab interattivo di accompagnamento ahttps://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb
La logica simbolica permette ai professionisti di costruire sistemi che eseguono ragionamenti basati su regole che sono interpretabili e che possono essere facilmente incrementati con conoscenze pregresse.Tuttavia, tali sistemi sono tradizionalmente difficili da applicare a problemi che coinvolgono il linguaggio naturale a causa della grande variabilità linguistica del linguaggio.Attualmente, la maggior parte del lavoro nell'elaborazione del linguaggio naturale si concentra sulle reti neurali che imparano rappresentazioni distribuite di parole e la loro composizione, quindi eseguendo bene in presenza di grande variabilità linguistica. Proponiamo di raccogliere i benefici di entrambi gli approcci applicando una combinazione di reti neurali e programmazione logica alla risposta alle domande del linguaggio naturale.Proponiamo di impiegare un prover esterno, non differenziabile Prolog che utilizza una funzione di somiglianza su codificatori di frasi preaddestrati.Mettiamo a punto queste rappresentazioni tramite strategie di evoluzione con l'obiettivo di ragionare a più livelli sul linguaggio naturale.  Questo ci permette di creare un sistema che può applicare il ragionamento basato su regole al linguaggio naturale e indurre regole di linguaggio naturale specifiche del dominio dai dati di formazione.Valutiamo il sistema proposto su due diversi compiti di risposta alle domande, mostrando che integra due linee di base molto forti - BIDAF (Seo et al., 2016a) e FASTQA (Weissenborn et al.,2017) - e supera entrambi quando viene utilizzato in un ensemble.
Le etichette di addestramento sono costose da ottenere e possono essere di qualità variabile, in quanto alcune possono provenire da etichettatori esperti di fiducia mentre altre possono provenire da euristiche o altre fonti di supervisione debole come il crowd-sourcing.  Impariamo dalla piccola quantità di dati di alta qualità o dalla quantità potenzialmente grande di dati debolmente etichettati? Noi sosteniamo che se il discente potesse in qualche modo conoscere e prendere in considerazione la qualità dell'etichetta, potremmo ottenere il meglio di entrambi i mondi.   A questo scopo, introduciamo il "fidelity-weighted learning" (FWL), un approccio semi-supervisionato studente-insegnante per l'addestramento di reti neurali profonde utilizzando dati con etichette deboli.FWL modula gli aggiornamenti dei parametri di una rete di studenti, addestrata sul compito che ci interessa su una base per campione secondo la fiducia posteriore della sua qualità delle etichette stimata da un insegnante, che ha accesso a campioni limitati con etichette di alta qualità.
In particolare, forniamo prove empiriche e teoriche che i metodi del primo ordine, come il gradient descent, sono provatamente robusti al rumore/corruzione su una frazione costante delle etichette, nonostante l'iperparametrizzazione sotto un modello di dataset ricco. In particolare: i) In primo luogo, dimostriamo che nelle prime iterazioni, dove gli aggiornamenti sono ancora nelle vicinanze dell'inizializzazione, questi algoritmi si adattano solo alle etichette corrette, ignorando essenzialmente le etichette rumorose. ii) In secondo luogo, dimostriamo che per iniziare ad adattare eccessivamente le etichette rumorose, questi algoritmi devono allontanarsi dal modello iniziale, il che può avvenire solo dopo molte altre iterazioni. Insieme, ciò dimostra che la discesa a gradiente con arresto anticipato è provatamente robusta al rumore delle etichette e fa luce sulla robustezza empirica delle reti profonde così come sulle euristiche di arresto anticipato comunemente adottate.
Anche se la potatura rimuove una quantità significativa di pesi in una rete, la riduzione dei requisiti di memoria è stata limitata poiché i formati convenzionali di matrice sparsa richiedono una quantità significativa di memoria per memorizzare le informazioni relative all'indice. Inoltre, i calcoli associati a tali formati di matrice sparsa sono lenti perché il processo sequenziale di decodifica della matrice sparsa non utilizza i sistemi di calcolo altamente parallelizzati in modo efficiente.Come un tentativo di comprimere le informazioni di indice mantenendo il processo di decodifica parallelizzabile, Viterbi-based pruning è stato suggerito.Decodifica dei pesi non nulli, tuttavia, è ancora sequenziale in Viterbi-based pruning. In questo documento, proponiamo un nuovo formato di matrice sparsa per consentire un processo di decodifica altamente parallelo dell'intera matrice sparsa, costruita combinando la potatura e la quantizzazione dei pesi. Per gli ultimi modelli RNN sul corpus PTB e WikiText-2, il requisito di memorizzazione dei parametri LSTM è compresso di 19 volte utilizzando il formato di matrice sparsa proposto rispetto al modello di base. Il peso e gli indici compressi possono essere ricostruiti in una matrice densa velocemente usando i codificatori Viterbi. I risultati della simulazione mostrano che lo schema proposto può alimentare i parametri agli elementi di elaborazione dal 20% al 106% più velocemente rispetto al caso in cui i valori della matrice densa provengono direttamente dalla DRAM.
L'uso di modelli di apprendimento profondo come priori per compiti di rilevamento compressivo presenta un nuovo potenziale per l'acquisizione di dati sismici poco costosi.Una rete generativa adversariale di Wasserstein, progettata in modo appropriato, è basata su un'architettura di rete generativa adversariale addestrata su diverse indagini storiche, in grado di apprendere le proprietà statistiche delle wavelets sismiche.L'uso di convalida e test delle prestazioni del rilevamento compressivo sono tre passi.In primo luogo, l'esistenza di una rappresentazione sparsa con diversi tassi di compressione per le indagini sismiche è studiato. Poi, vengono studiati i campionamenti non uniformi, utilizzando la metodologia proposta.Infine, vengono proposte raccomandazioni per la griglia di indagine sismica non uniforme, basate sulla valutazione delle immagini sismiche ricostruite e delle metriche.L'obiettivo primario del modello di apprendimento profondo proposto è quello di fornire le basi di un design ottimale per l'acquisizione sismica, con una minore perdita di qualità di imaging. Lungo queste linee, un design di rilevamento compressivo di una griglia non uniforme su una risorsa nel Golfo del Messico, rispetto a una griglia di indagine sismica tradizionale che raccoglie dati uniformemente a ogni pochi piedi, è suggerito, sfruttando il metodo proposto.
Negli ultimi anni abbiamo visto un rapido progresso su una serie di problemi di riferimento nell'IA, con metodi moderni che raggiungono prestazioni vicine o super umane in Go, Poker e Dota. Un aspetto comune a tutte queste sfide è che sono per progettazione avversarie o, tecnicamente parlando, a somma zero.In contrasto con queste impostazioni, il successo nel mondo reale richiede comunemente agli umani di collaborare e comunicare con gli altri, in impostazioni che sono, almeno parzialmente, cooperative. Nell'ultimo anno, il gioco di carte Hanabi si è affermato come un nuovo ambiente di riferimento per l'IA per colmare questa lacuna.In particolare, Hanabi è interessante per gli esseri umani in quanto è interamente incentrato sulla teoria della mente, cioè la capacità di ragionare efficacemente sulle intenzioni, le credenze e il punto di vista di altri agenti quando si osservano le loro azioni.Imparare a essere informativo quando viene osservato da altri è una sfida interessante per il Reinforcement Learning (RL): Tuttavia, se fatto ingenuamente, questa casualità renderà le loro azioni meno informative per gli altri durante l'addestramento. Noi presentiamo un nuovo metodo di RL profondo multi-agente, il Simplified Action Decoder (SAD), che risolve questa contraddizione sfruttando la fase di addestramento centralizzato. Durante l'addestramento SAD permette agli altri agenti di non osservare solo l'azione (esplorativa) scelta, ma gli agenti invece osservano anche l'azione avida dei loro compagni di squadra.Combinando questa semplice intuizione con un compito ausiliario per la previsione di stato e le migliori pratiche per l'apprendimento multi-agente, SAD stabilisce un nuovo stato dell'arte per 2-5 giocatori sulla parte self-play della sfida Hanabi.
Presentiamo un approccio addestrabile end-to-end per il riconoscimento ottico dei caratteri (OCR) su documenti stampati, basato sulla previsione di una rappresentazione bidimensionale della griglia di caratteri ('chargrid') dell'immagine di un documento come compito di segmentazione semantica. per identificare le singole istanze di carattere dalla chargrid, consideriamo i caratteri come oggetti e usiamo tecniche di rilevamento degli oggetti dalla computer vision. dimostriamo sperimentalmente che il nostro metodo supera i precedenti approcci allo stato dell'arte in accuratezza ed è facilmente parallelizzabile su GPU (quindi significativamente più veloce), oltre che più facile da allenare.
Proponiamo NovoGrad, un metodo di discesa del gradiente stocastico adattivo con normalizzazione del gradiente a livello e decadimento dei pesi disaccoppiato, che nei nostri esperimenti su reti neurali per la classificazione delle immagini, il riconoscimento vocale, la traduzione automatica e la modellazione del linguaggio, si comporta alla pari o meglio di SGD ben sintonizzato con momentum e Adam/AdamW. Inoltre, NovoGrad (1) è robusto alla scelta del tasso di apprendimento e dell'inizializzazione dei pesi, (2) funziona bene in un'impostazione di grandi lotti e (3) ha un'impronta di memoria due volte più piccola di Adam.
La maggior parte dei modelli generativi dell'audio generano direttamente campioni in uno dei due domini: tempo o frequenza; sebbene siano sufficienti per esprimere qualsiasi segnale, queste rappresentazioni sono inefficienti, poiché non utilizzano la conoscenza esistente di come il suono viene generato e percepito. Un terzo approccio (vocoder/sintetizzatori) incorpora con successo una forte conoscenza del dominio dell'elaborazione del segnale e della percezione, ma è stato meno attivamente studiato a causa della limitata espressività e della difficoltà di integrazione con i moderni metodi di apprendimento automatico basati sull'auto-differenziazione.In questo articolo, introduciamo la libreria Differentiable Digital Signal Processing (DDSP), che permette l'integrazione diretta di elementi classici di elaborazione del segnale con metodi di apprendimento profondo. Concentrandosi sulla sintesi audio, raggiungiamo una generazione ad alta fedeltà senza la necessità di grandi modelli autoregressivi o perdite avversarie, dimostrando che DDSP permette di utilizzare forti bias induttivi senza perdere la potenza espressiva delle reti neurali. Inoltre, dimostriamo che la combinazione di moduli interpretabili permette la manipolazione di ogni componente separato del modello, con applicazioni quali il controllo indipendente di tonalità e loudness, l'estrapolazione realistica a tonalità non viste durante l'allenamento, la dereverberazione cieca dell'acustica della stanza, il trasferimento dell'acustica della stanza estratta a nuovi ambienti e la trasformazione del timbro tra fonti disparate. In breve, DDSP permette un approccio interpretabile e modulare alla modellazione generativa, senza sacrificare i benefici del deep learning.La libreria sarà disponibile all'indirizzo https://github.com/magenta/ddsp e incoraggiamo ulteriori contributi dalla comunità e dagli esperti di dominio.
Le reti convoluzionali spettrali del grafico (GCNs) sono una generalizzazione delle reti convoluzionali all'apprendimento su dati strutturati a grafo.Le applicazioni delle GCNs spettrali hanno avuto successo, ma sono limitate ad alcuni problemi in cui il grafico è fisso, come la corrispondenza di forma e la classificazione dei nodi. In questo lavoro, affrontiamo questa limitazione rivisitando una particolare famiglia di reti di grafi spettrali, Chebyshev GCNs, mostrando la sua efficacia nel risolvere compiti di classificazione dei grafi con una struttura e una dimensione variabili dei grafi.le GCNs attuali inoltre limitano i grafi ad avere al massimo un bordo tra qualsiasi coppia di nodi. A questo scopo, proponiamo una nuova rete multigrafo che apprende da grafi multirelazionali.modelliamo esplicitamente diversi tipi di bordi: bordi annotati, bordi appresi con significato astratto e bordi gerarchici.sperimentiamo anche diversi modi per fondere le rappresentazioni estratte da diversi tipi di bordi.questa restrizione è a volte implicita da un set di dati, tuttavia, rilassiamo questa restrizione per tutti i tipi di set di dati.otteniamo risultati allo stato dell'arte su una varietà di benchmark di classificazione di grafi chimici, sociali e di visione.
Le reti neurali profonde hanno recentemente dimostrato di essere vulnerabili agli attacchi backdoor. In particolare, alterando un piccolo set di esempi di formazione, un avversario è in grado di installare una backdoor che può essere utilizzata durante l'inferenza per controllare completamente il comportamento del modello. In questo articolo, introduciamo un nuovo approccio all'esecuzione di attacchi backdoor, utilizzando esempi avversari e dati generati da GAN. La caratteristica chiave è che gli input avvelenati risultanti sembrano essere coerenti con la loro etichetta e quindi sembrano benigni anche all'ispezione umana.
Nonostante la loro capacità di memorizzare grandi insiemi di dati, le reti neurali profonde spesso raggiungono buone prestazioni di generalizzazione.Tuttavia, le differenze tra le soluzioni apprese delle reti che generalizzano e quelle che non lo fanno rimangono poco chiare.Inoltre, le proprietà di sintonizzazione delle singole direzioni (definite come l'attivazione di una singola unità o qualche combinazione lineare di unità in risposta a qualche input) sono state evidenziate, ma la loro importanza non è stata valutata. Qui, colleghiamo queste linee di indagine per dimostrare che la dipendenza di una rete da singole direzioni è un buon predittore delle sue prestazioni di generalizzazione, attraverso reti addestrate su set di dati con diverse frazioni di etichette corrotte, attraverso ensemble di reti addestrate su set di dati con etichette non modificate, attraverso diversi iper-parametri, e nel corso della formazione. Mentre il dropout regolarizza questa quantità solo fino a un certo punto, la normalizzazione dei lotti scoraggia implicitamente la dipendenza da una singola direzione, in parte diminuendo la selettività di classe delle singole unità. Infine, troviamo che la selettività di classe è un povero predittore dell'importanza del compito, suggerendo non solo che le reti che generalizzano bene minimizzano la loro dipendenza dalle singole unità riducendo la loro selettività, ma anche che le unità selettive individualmente potrebbero non essere necessarie per una forte performance di rete.
Invece di una coppia encoder-decoder, possiamo addestrare una singola rete di inferenza direttamente dai dati, utilizzando una funzione di costo che è stocastica non solo sui campioni, ma anche sulle interrogazioni. Possiamo usare questa rete per eseguire gli stessi compiti di inferenza che faremmo in un modello grafico indiretto con variabili nascoste, senza dover affrontare l'intrattabile funzione di partizione. I risultati possono essere mappati all'apprendimento di un vero modello indiretto, che è un problema notoriamente difficile.  Mostriamo che il nostro approccio si generalizza alle interrogazioni probabilistiche non viste su dati di prova non visti, fornendo un'inferenza veloce e flessibile. Gli esperimenti mostrano che questo approccio supera o eguaglia PCD e AdVIL su 9 set di dati di riferimento.
Prima della rinascita dell'apprendimento profondo, la famiglia dominante per risolvere tali problemi di ottimizzazione era l'ottimizzazione numerica, ad esempio, Gauss-Newton (GN). Più di recente, sono stati fatti diversi tentativi di formulare fasi di GN apprendibili come architetture di regressione a cascata.In questo articolo, esaminiamo le recenti architetture di apprendimento automatico, come le reti neurali profonde con connessioni residue, sotto la suddetta prospettiva. A questo scopo, dimostriamo prima come i blocchi residui (quando considerati come discretizzazione di ODE) possono essere visti come passi GN.Poi, facciamo un passo avanti e proponiamo un nuovo blocco residuo, che ricorda il metodo di Newton nell'ottimizzazione numerica ed esibisce una convergenza più veloce.Valutiamo accuratamente la Newton-ResNet proposta conducendo esperimenti sulla classificazione di immagini e discorsi e sulla generazione di immagini, utilizzando 4 dataset.Tutti gli esperimenti dimostrano che Newton-ResNet richiede meno parametri per raggiungere le stesse prestazioni con la ResNet originale.
In situazioni competitive, gli agenti possono intraprendere azioni per raggiungere i loro obiettivi che facilitano inconsapevolmente gli obiettivi di un avversario: (1) un utente (umano), (2) un agente attaccante (umano o software) e (3) un agente osservatore (software).L'utente e l'attaccante competono per raggiungere obiettivi diversi.Quando c'è una disparità nella conoscenza del dominio che l'utente e l'attaccante possiedono, l'attaccante può usare la scarsa familiarità dell'utente con il dominio a suo vantaggio e promuovere il proprio obiettivo.In questa situazione, l'osservatore, il cui obiettivo è sostenere l'utente può avere bisogno di intervenire, e questo intervento deve avvenire online, in tempo ed essere preciso. Abbiamo formalizzato il problema dell'intervento online sul piano e proponiamo una soluzione che utilizza un classificatore ad albero decisionale per identificare i punti di intervento in situazioni in cui gli agenti facilitano involontariamente l'obiettivo di un avversario.Abbiamo addestrato un classificatore utilizzando caratteristiche indipendenti dal dominio estratte dallo spazio decisionale dell'osservatore per valutare la "criticità" dello stato corrente.Il modello addestrato viene quindi utilizzato in un ambiente online su benchmark IPC per identificare le osservazioni che giustificano l'intervento.I nostri contributi gettano una base per ulteriori lavori nel campo della decisione di quando intervenire.
I modelli generativi profondi possono emulare le proprietà percettive di dataset di immagini complesse, fornendo una rappresentazione latente dei dati. Tuttavia, la manipolazione di tale rappresentazione per eseguire trasformazioni significative e controllabili nello spazio dei dati rimane impegnativa senza qualche forma di supervisione. Mentre il lavoro precedente si è concentrato sullo sfruttamento dell'indipendenza statistica per distinguere i fattori latenti, noi sosteniamo che tale requisito può essere vantaggiosamente rilassato e proponiamo invece un quadro non statistico che si basa sull'identificazione di un'organizzazione modulare della rete, basata su manipolazioni controfattuali. I nostri esperimenti sostengono che la modularità tra gruppi di canali è raggiunta in una certa misura su una varietà di modelli generativi, il che ha permesso la progettazione di interventi mirati su dataset di immagini complesse, aprendo la strada ad applicazioni come il trasferimento di stile computazionalmente efficiente e la valutazione automatica della robustezza ai cambiamenti contestuali nei sistemi di riconoscimento dei modelli.
La dimenticanza catastrofica pone una grande sfida per i sistemi di apprendimento continuo, che impedisce alle reti neurali di proteggere le vecchie conoscenze mentre imparano nuovi compiti in modo sequenziale.Proponiamo uno strato Softmax di plasticità Hebbian differenziabile (DHP) che aggiunge una componente plastica di apprendimento veloce ai pesi lenti dello strato di uscita softmax.Il DHP Softmax si comporta come una memoria episodica compressa che riattiva le tracce di memoria esistenti, mentre ne crea di nuove. Valutiamo il nostro approccio sui benchmark Permuted MNIST e Split MNIST, e introduciamo Imbalanced Permuted MNIST - un set di dati che combina le sfide dello squilibrio di classe e della deriva dei concetti. Il nostro modello non richiede iperparametri aggiuntivi e supera le prestazioni delle linee di base comparabili riducendo la dimenticanza.
Mentre le reti cerebrali reali mostrano una modularità funzionale, indaghiamo se la modularità funzionale esiste anche nelle Reti Neurali Profonde (DNN) addestrate tramite back-propagation.Sotto l'ipotesi che le DNN siano anche organizzate in moduli task-specifici, in questo articolo cerchiamo di sezionare uno strato nascosto in gruppi disgiunti di neuroni nascosti task-specifici con l'aiuto di metodi di attribuzione dei neuroni relativamente ben studiati. Dicendo task-specific, intendiamo che i neuroni nascosti nello stesso gruppo sono funzionalmente correlati per la previsione di un insieme di campioni di dati simili, cioè campioni con modelli di caratteristiche simili.Sosteniamo che tali gruppi di neuroni che chiamiamo Moduli Funzionali possono servire come unità funzionale di base in DNN.Proponiamo un metodo preliminare per identificare i Moduli Funzionali attraverso i punteggi di attribuzione bi-clustering dei neuroni nascosti.Troviamo che in primo luogo, senza sorpresa, i neuroni funzionali sono altamente sparsi, cioè, solo un piccolo sottoinsieme di neuroni è importante per prevedere un piccolo sottoinsieme di campioni di dati e, mentre non usiamo alcuna supervisione dell'etichetta, i campioni corrispondenti allo stesso gruppo (bicluster) mostrano modelli di caratteristiche sorprendentemente coerenti.Mostriamo anche che questi moduli funzionali svolgono un ruolo critico nella discriminazione dei campioni di dati attraverso un esperimento di ablazione.
I chip CNN Domain Specific Accelerator (CNN-DSA) ad alta efficienza energetica sono attualmente disponibili per un ampio utilizzo nei dispositivi mobili.Questi chip sono principalmente utilizzati in applicazioni di computer vision.Tuttavia, il recente lavoro del metodo Super Characters per la classificazione del testo e i compiti di sentiment analysis utilizzando modelli CNN bidimensionali ha anche raggiunto risultati all'avanguardia attraverso il metodo di apprendimento di trasferimento dalla visione al testo.In questo articolo, abbiamo implementato le applicazioni di classificazione del testo e sentiment analysis su dispositivi mobili utilizzando chip CNN-DSA. Nel chip CNN-DSA sono utilizzate rappresentazioni di rete compatte che utilizzano una precisione di un bit e tre bit per i coefficienti e cinque bit per le attivazioni con un consumo di energia inferiore a 300mW.Per i dispositivi edge sotto vincoli di memoria e calcolo, la rete è ulteriormente compressa approssimando gli strati esterni Fully Connected (FC) all'interno del chip CNN-DSA.Al workshop, abbiamo due dimostrazioni di sistema per compiti NLP.La prima demo classifica la frase inglese di Wikipedia in una delle 14 classi.La seconda demo classifica la recensione cinese di shopping online in positiva o negativa.
Confrontare le inferenze di diversi modelli candidati è una parte essenziale del controllo del modello e della fuga dagli ottimismi locali. Per consentire un confronto efficiente, introduciamo un quadro di inferenza variazionale ammortizzata che può eseguire una stima posteriore veloce e affidabile tra modelli della stessa architettura. Il nostro Any Parameter Encoder (APE) estende la rete neurale encoder comune nell'inferenza ammortizzata per prendere sia un vettore di caratteristiche dei dati che un vettore di parametri del modello come input. Negli esperimenti che confrontano modelli di argomenti candidati per dati sintetici e recensioni di prodotti, il nostro Any Parameter Encoder produce posteriors comparabili a metodi più costosi in molto meno tempo, specialmente quando l'architettura dell'encoder è progettata in modo consapevole del modello.
L'abilità di trasferire la conoscenza ad ambienti e compiti nuovi è un desiderata ragionevole per gli agenti di apprendimento generale. Nonostante le apparenti promesse, il trasferimento nella RL è ancora un'area di ricerca aperta e poco sfruttata. In questo articolo, adottiamo una nuova prospettiva sul trasferimento: suggeriamo che la capacità di assegnare crediti svela invarianti strutturali nei compiti che possono essere trasferiti per rendere la RL più efficiente. Il nostro contributo principale è Secret, un nuovo approccio all'apprendimento del trasferimento per RL che utilizza un meccanismo di assegnazione del credito a ritroso basato su un'architettura auto-attentiva. Due aspetti sono fondamentali per la sua generalità: impara ad assegnare il credito come un processo supervisionato offline separato e modifica esclusivamente la funzione di ricompensa, quindi può essere integrato da metodi di trasferimento che non modificano la funzione di ricompensa e può essere inserito in cima a qualsiasi algoritmo RL.
I recenti progressi nell'inferenza variazionale neurale hanno permesso una rinascita nei modelli di variabili latenti in una varietà di domini che coinvolgono dati ad alta densità. In questo articolo, introduciamo due framework generativi di inferenza variazionale per modelli generativi di Knowledge Graphs; Latent Fact Model e Latent Information Model.  Mentre i tradizionali metodi variazionali derivano un'approssimazione analitica per l'intrattabile distribuzione sulle variabili latenti, qui costruiamo una rete di inferenza condizionata dalla rappresentazione simbolica delle entità e dei tipi di relazione nel Knowledge Graph, per fornire le distribuzioni variazionali. La nuova struttura può creare modelli in grado di scoprire la semantica probabilistica sottostante alla rappresentazione simbolica utilizzando distribuzioni parametrizzabili che permettono l'addestramento tramite back-propagation nel contesto dell'inferenza variazionale neurale, risultando in un metodo altamente scalabile. In un quadro di campionamento Bernoulli, forniamo una giustificazione alternativa per le tecniche comunemente usate nell'inferenza variazionale stocastica su larga scala, che riduce drasticamente il tempo di addestramento al costo di un'approssimazione aggiuntiva al limite inferiore variazionale.  Le strutture generative sono abbastanza flessibili da permettere l'addestramento sotto qualsiasi distribuzione di priorità che permetta un trucco di ri-parametrizzazione, così come sotto qualsiasi funzione di punteggio che permetta la stima di massima verosimiglianza dei parametri. I risultati degli esperimenti mostrano il potenziale e l'efficienza di questa struttura, migliorando su più benchmark con rappresentazioni di priorità gaussiane.
La mappa generativa dallo spazio latente allo spazio dei dati segue un sistema dinamico, in cui una funzione potenziale apprendibile guida un fluido comprimibile a fluire verso la distribuzione di densità di destinazione.L'addestramento del modello equivale a risolvere un problema di controllo ottimale. Si possono facilmente imporre vincoli di simmetria nel modello generativo progettando adeguate funzioni potenziali scalari. Applichiamo l'approccio alla stima non supervisionata della densità del dataset MNIST e al calcolo variazionale del modello Ising bidimensionale al punto critico. Questo approccio porta intuizioni e tecniche dall'equazione di Monge-Ampere, dal trasporto ottimale e dalla dinamica dei fluidi in modelli generativi basati sul flusso reversibile.
Graph Neural Networks (GNNs) è una classe di modelli profondi che opera su dati con topologia arbitraria e struttura invariante all'ordine rappresentata come grafi.Introduciamo un efficiente strato di memoria per GNNs che può imparare ad eseguire congiuntamente l'apprendimento della rappresentazione del grafico e il pooling del grafico.Introduciamo anche due nuove reti basate sul nostro strato di memoria: I risultati sperimentali dimostrano che i modelli proposti raggiungono risultati all'avanguardia in sei dei sette benchmark di classificazione e regressione dei grafi e dimostrano che le rappresentazioni apprese potrebbero corrispondere alle caratteristiche chimiche nei dati delle molecole.
Per affrontare questo problema, proponiamo un nuovo schema di codifica utilizzando {-1,+1} per decomporre le reti neurali quantizzate (QNNs) in reti binarie a più rami, che possono essere efficientemente implementate da operazioni bitwise (xnor e bitcount) per ottenere la compressione del modello, l'accelerazione computazionale e il risparmio di risorse. Il nostro metodo può raggiungere al massimo ~59 di accelerazione e ~32 di risparmio di memoria rispetto alle sue controparti a piena precisione.Pertanto, gli utenti possono facilmente raggiungere diverse precisioni di codifica arbitrariamente in base alle loro esigenze e risorse hardware.Il nostro meccanismo è molto adatto per l'uso di FPGA e ASIC in termini di memorizzazione dei dati e di calcolo, che fornisce un'idea fattibile per chip intelligenti.Convalidiamo l'efficacia del nostro metodo sia su larga scala di classificazione delle immagini (ad esempio, ImageNet) che su compiti di rilevamento di oggetti.
Proponiamo un nuovo metodo per incorporare le informazioni condizionali in una rete generativa avversaria (GAN) per compiti di predizione strutturati. Questo metodo si basa sulla fusione delle caratteristiche dalle informazioni generate e condizionali nello spazio delle caratteristiche e permette al discriminatore di catturare meglio le statistiche di ordine superiore dai dati. Il metodo proposto è concettualmente più semplice dei modelli congiunti di rete neurale convoluzionale - campo casuale di Markov condizionale (CNN-CRF) e fa rispettare la coerenza di ordine superiore senza essere limitato a una classe molto specifica di potenziali di ordine superiore. I risultati sperimentali dimostrano che questo metodo porta a un miglioramento in una varietà di compiti di previsione strutturati diversi, tra cui la sintesi di immagini, la segmentazione semantica e la stima della profondità.
Le creature intelligenti possono esplorare i loro ambienti e imparare abilità utili senza supervisione.In questo articolo, proponiamo ``Diversity is All You Need'' (DIAYN), un metodo per imparare abilità utili senza una funzione di ricompensa.Il nostro metodo proposto impara abilità massimizzando un obiettivo teorico dell'informazione usando una politica di massima entropia.Su una varietà di compiti robotici simulati, dimostriamo che questo semplice obiettivo porta all'emergere senza supervisione di diverse abilità, come camminare e saltare. In un certo numero di ambienti di riferimento per l'apprendimento di rinforzo, il nostro metodo è in grado di imparare un'abilità che risolve il compito di riferimento, nonostante non riceva mai la vera ricompensa del compito.Mostriamo come le abilità preaddestrate possono fornire una buona inizializzazione dei parametri per i compiti a valle, e possono essere composte gerarchicamente per risolvere compiti complessi, con ricompensa sparsa.I nostri risultati suggeriscono che la scoperta non supervisionata delle abilità può servire come un efficace meccanismo di preformazione per superare le sfide di esplorazione e di efficienza dei dati nell'apprendimento di rinforzo.
L'ambiguità lessicale, cioè la presenza di due o più significati per una singola parola, è un problema intrinseco e impegnativo per i sistemi di traduzione automatica. Anche se l'uso di reti neurali ricorrenti e i meccanismi di attenzione dovrebbero risolvere questo problema, i sistemi di traduzione automatica non sono sempre in grado di tradurre correttamente frasi lessicalmente ambigue. In questo lavoro, cerco di risolvere il problema dell'ambiguità lessicale nei sistemi di traduzione automatica inglese-giapponese combinando un modello linguistico preaddestrato Bidirectional Encoder Representations from Transformer (BERT) che può produrre embeddings di parole contestualizzate e un modello di traduzione Transformer, che è un'architettura all'avanguardia per il compito di traduzione automatica. Queste due architetture proposte hanno dimostrato di essere più efficaci nel tradurre frasi ambigue rispetto a un modello Transformer vanilla e al sistema Google Translate. Inoltre, uno dei modelli proposti, il Transformer_BERT-WE, raggiunge un punteggio BLEU più alto rispetto al modello Transformer vanilla in termini di traduzione generale, il che è la prova concreta che l'uso di embeddings di parole contestualizzate da BERT non solo può risolvere il problema dell'ambiguità lessicale, ma anche aumentare la qualità della traduzione in generale.
Questo articolo si concentra sulla generazione sintetica di dati sulla mobilità umana nelle aree urbane, presentando un'applicazione nuova e scalabile di reti generative avversarie (GAN) per la modellazione e la generazione di dati sulla mobilità umana, sfruttando le richieste reali di corse da servizi di ride sharing/hailing di quattro grandi città degli Stati Uniti per addestrare il nostro modello GANs. I lavori precedenti hanno caratterizzato succintamente le proprietà spaziali e temporali dei set di dati sulla mobilità umana utilizzando rispettivamente la dimensionalità frattale e la legge di potenza di densificazione, che utilizziamo per convalidare i nostri set di dati sintetici generati da GANs.
Mentre le reti neurali profonde sono una classe di modelli di grande successo, la loro grande impronta di memoria mette a dura prova il consumo di energia, la larghezza di banda di comunicazione e i requisiti di archiviazione. Di conseguenza, la riduzione delle dimensioni del modello è diventata un obiettivo fondamentale nell'apprendimento profondo. Seguendo il classico argomento bit-back, codifichiamo i pesi della rete utilizzando un campione casuale, richiedendo solo un numero di bit corrispondente alla divergenza di Kullback-Leibler tra la distribuzione variazionale campionata e la distribuzione di codifica. Imponendo un vincolo sulla divergenza di Kullback-Leibler, siamo in grado di controllare esplicitamente il tasso di compressione, ottimizzando al contempo la perdita attesa sul set di allenamento. Lo schema di codifica impiegato può essere dimostrato essere vicino al limite inferiore ottimale teorico dell'informazione, rispetto alla famiglia variazionale impiegata. Sui benchmark LeNet-5/MNIST e VGG-16/CIFAR-10, il nostro approccio fornisce le migliori prestazioni di prova per un budget di memoria fisso, e viceversa, raggiunge i tassi di compressione più elevati per una prestazione di prova fissa.
Le reti neurali profonde forniscono prestazioni allo stato dell'arte per il denoising delle immagini, dove l'obiettivo è quello di recuperare un'immagine quasi priva di rumore da un'immagine rumorosa.Il principio sottostante è che le reti neurali addestrate su grandi set di dati hanno empiricamente dimostrato di essere in grado di generare bene immagini naturali da una rappresentazione latente a bassa dimensione dell'immagine.Data una tale rete generatrice, o priore, un'immagine rumorosa può essere denoised trovando l'immagine più vicina nell'intervallo del priore.Tuttavia, c'è poca teoria per giustificare questo successo, e tanto meno per prevedere le prestazioni di denoising in funzione dei parametri della rete. In questo articolo consideriamo il problema di denoising di un'immagine dal rumore additivo gaussiano, assumendo che l'immagine sia ben descritta da una rete neurale profonda con funzioni di attivazione ReLu, mappando uno spazio latente k-dimensionale a un'immagine n-dimensionale. Dichiariamo e analizziamo un semplice algoritmo iterativo simile al gradiente che minimizza una funzione di perdita non convessa, e rimuove provatamente una frazione di (1 - O(k/n)) dell'energia del rumore, dimostrando anche con esperimenti numerici che questa prestazione di denoising è, in effetti, ottenuta da priori generativi appresi dai dati.
L'apprendimento profondo produce grandi risultati in molti campi, dal riconoscimento vocale, alla classificazione delle immagini, alla traduzione.Ma per ogni problema, ottenere un modello profondo che funzioni bene implica una ricerca sull'architettura e un lungo periodo di messa a punto.Presentiamo un singolo modello che produce buoni risultati su un certo numero di problemi che abbracciano più domini.In particolare, questo singolo modello è addestrato simultaneamente su ImageNet, più compiti di traduzione, didascalie di immagini (dataset COCO), un corpus di riconoscimento vocale e un compito di parsing inglese. Ognuno di questi blocchi computazionali è cruciale per un sottoinsieme dei compiti su cui ci addestriamo. È interessante notare che, anche se un blocco non è cruciale per un compito, osserviamo che l'aggiunta non danneggia mai le prestazioni e nella maggior parte dei casi le migliora su tutti i compiti.
Gli algoritmi di apprendimento automatico per il controllo dei dispositivi dovranno imparare rapidamente, con poche prove.Un tale obiettivo può essere raggiunto con concetti presi in prestito dalla filosofia continentale e formalizzati utilizzando strumenti della teoria matematica delle categorie.Illustrazioni di questo approccio sono presentate su un sistema cyberfisico: il gioco delle slot car, e anche su giochi Atari 2600.
I segnali audio sono campionati ad alte risoluzioni temporali, e imparare a sintetizzare l'audio richiede la cattura della struttura attraverso una gamma di scale temporali.Le reti generative avversarie (GAN) hanno visto un ampio successo nella generazione di immagini che sono sia localmente che globalmente coerenti, ma hanno visto poca applicazione alla generazione audio.In questo articolo presentiamo WaveGAN, un primo tentativo di applicare GAN alla sintesi non supervisionata di forme d'onda audio grezze. I nostri esperimenti dimostrano che - senza etichette - WaveGAN impara a produrre parole intelligibili quando viene addestrato su un set di dati di vocabolario piccolo, e può anche sintetizzare audio da altri domini come tamburi, vocalizzazioni di uccelli e pianoforte. Confrontiamo WaveGAN con un metodo che applica i GAN progettati per la generazione di immagini su rappresentazioni di caratteristiche audio simili alle immagini, trovando entrambi gli approcci promettenti.
La difficoltà di ottenere sufficienti dati etichettati per l'apprendimento supervisionato ha motivato l'adattamento al dominio, in cui un classificatore è addestrato in un dominio, quello di origine, ma opera in un altro, quello di destinazione.Ridurre la discrepanza del dominio ha migliorato le prestazioni, ma è ostacolato dalle caratteristiche incorporate che non formano cluster chiaramente separabili e allineati.Affrontiamo questo problema propagando le etichette utilizzando una struttura a collettore, e facendo rispettare la coerenza del ciclo per allineare più strettamente i cluster di caratteristiche in ogni dominio. In particolare, dimostriamo che la coerenza del ciclo porta le caratteristiche incorporate lontane da tutti i cluster tranne uno se il dominio di origine è idealmente clusterizzato.Utilizziamo inoltre più informazioni dal collettore locale approssimato e perseguiamo la coerenza del collettore locale per un maggiore miglioramento.I risultati per vari scenari di adattamento del dominio mostrano un clustering più stretto e un miglioramento della precisione di classificazione.
Proponiamo un nuovo metodo per l'addestramento delle reti neurali online in un'impostazione bandit.Simile al lavoro precedente, modelliamo l'incertezza solo nell'ultimo strato della rete, trattando il resto della rete come un estrattore di caratteristiche.Questo ci permette di bilanciare con successo tra esplorazione e sfruttamento grazie alle stime di incertezza efficienti e in forma chiusa disponibili per i modelli lineari. Per addestrare il resto della rete, ci avvantaggiamo del posteriore che abbiamo sull'ultimo strato, ottimizzando su tutti i valori nella distribuzione dell'ultimo strato ponderata per la probabilità. Deriviamo una forma chiusa, un'approssimazione differenziale a questo obiettivo e dimostriamo empiricamente su vari modelli e set di dati che addestrare il resto della rete in questo modo porta a prestazioni migliori sia online che offline rispetto ad altri metodi.
	Nonostante una crescente letteratura sulla spiegazione delle reti neurali, non è stato raggiunto un consenso su come spiegare una decisione di rete neurale o su come valutare una spiegazione.	Il nostro contributo in questo articolo è duplice: in primo luogo, studiamo schemi per combinare metodi di spiegazione e ridurre l'incertezza del modello per ottenere una singola spiegazione aggregata, che è più robusta e si allinea meglio con la rete neurale rispetto a qualsiasi singolo metodo di spiegazione.	In secondo luogo, proponiamo un nuovo approccio alla valutazione dei metodi di spiegazione che aggira la necessità della valutazione manuale e non dipende dall'allineamento delle reti neurali e dei processi decisionali umani.
Presentiamo SOSELETO (SOurce SELEction for Target Optimization), un nuovo metodo per sfruttare un dataset sorgente per risolvere un problema di classificazione su un dataset target.  SOSELETO si basa sulla seguente semplice intuizione: alcuni esempi sorgente sono più informativi di altri per il problema target.  Per catturare questa intuizione, ai campioni di origine vengono dati dei pesi; questi pesi vengono risolti insieme ai problemi di classificazione di origine e di destinazione attraverso uno schema di ottimizzazione a due livelli.  L'obiettivo quindi sceglie i campioni sorgente che sono più informativi per il suo compito di classificazione.  Inoltre, la natura bilivello dell'ottimizzazione agisce come una sorta di regolarizzazione sul target, mitigando l'overfitting.  SOSELETO può essere applicato sia al classico apprendimento di trasferimento, sia al problema dell'addestramento su set di dati con etichette rumorose; mostriamo risultati allo stato dell'arte su entrambi questi problemi.
Presentiamo un nuovo approccio per l'addestramento di architetture neurali astratte, che prevede una supervisione (parziale) sulle componenti interpretabili della macchina. Per catturare in modo pulito l'insieme di architetture neurali a cui si applica il nostro metodo, introduciamo il concetto di macchina computazionale neurale differenziale (∂NCM) e dimostriamo che diverse architetture esistenti (ad es, Sulla base del nostro metodo, abbiamo eseguito una valutazione sperimentale dettagliata con entrambe le architetture NTM e NRAM, e abbiamo dimostrato che l'approccio porta a una convergenza e capacità di generalizzazione della fase di apprendimento significativamente migliori rispetto all'addestramento utilizzando solo esempi di input-output.
L'apprendimento bayesiano dei parametri del modello nelle reti neurali è importante negli scenari in cui le stime con incertezza ben calibrata sono importanti. In questo articolo, proponiamo reti quantizzate bayesiane (BQNs), reti neurali quantizzate (QNNs) per le quali impariamo una distribuzione posteriore sui loro parametri discreti. Forniamo un insieme di algoritmi efficienti per l'apprendimento e la previsione in BQNs senza la necessità di campionare dai loro parametri o attivazioni, il che non solo permette un apprendimento differenziabile nei modelli quantizzati ma riduce anche la varianza nella stima dei gradienti. Valutiamo i BQNs sui set di dati di classificazione MNIST, Fashion-MNIST e KMNIST confrontati con l'ensemble bootstrap di QNNs (E-QNN) e dimostriamo che i BQNs ottengono sia errori predittivi inferiori che incertezze meglio calibrate rispetto agli E-QNN (con meno del 20% della log-likelihood negativa).
L'iniezione di esempi avversari durante l'addestramento, noto come addestramento avversario, può migliorare la robustezza contro gli attacchi a un passo, ma non per gli attacchi iterativi sconosciuti.Per affrontare questa sfida, mostriamo innanzitutto che le immagini avversarie generate iterativamente si trasferiscono facilmente tra le reti addestrate con la stessa strategia. Ispirati da questa osservazione, proponiamo l'addestramento avversario a cascata, che trasferisce la conoscenza dei risultati finali dell'addestramento avversario.Addestriamo una rete da zero iniettando immagini avversarie generate iterativamente realizzate da reti già difese, oltre alle immagini avversarie one-step della rete che viene addestrata. Proponiamo inoltre di utilizzare lo spazio di embedding sia per la classificazione che per l'apprendimento della somiglianza a basso livello (a livello di pixel) per ignorare le perturbazioni sconosciute a livello di pixel.Durante l'addestramento, iniettiamo immagini avversarie senza sostituire le corrispondenti immagini pulite e penalizziamo la distanza tra le due embeddings (pulite e avversarie). I risultati sperimentali mostrano che l'addestramento avversario a cascata insieme alla nostra proposta di apprendimento della somiglianza a basso livello migliora efficacemente la robustezza contro gli attacchi iterativi, ma a spese di una minore robustezza contro gli attacchi a un solo passo.
Mentre si crede che tecniche come Adam, la normalizzazione batch e, più recentemente, le nonlinearità SeLU ``solvono'' il problema del gradiente esplosivo, mostriamo che questo non è il caso e che in una gamma di architetture MLP popolari, i gradienti esplosivi esistono e che limitano la profondità a cui le reti possono essere efficacemente addestrate, sia in teoria che in pratica. Le ResNet hanno gradienti significativamente più bassi e quindi possono aggirare il problema del gradiente esplosivo, permettendo l'effettivo addestramento di reti molto più profonde, che dimostriamo essere una conseguenza di una sorprendente proprietà matematica. Notando che ogni rete neurale è una rete residuale, escogitiamo il trucco dei residui, che rivela che l'introduzione di connessioni saltanti semplifica matematicamente la rete, e che questa semplicità può essere la causa principale del loro successo.
In questo articolo, siamo interessati a due concetti apparentemente diversi: \In particolare, come queste tecniche funzionano per migliorarsi a vicenda. A tal fine, analizziamo la limitazione dell'addestramento avversario come metodo di difesa, partendo dalla domanda su quanto bene la robustezza di un modello possa generalizzare. Poi, miglioriamo con successo la generalizzabilità attraverso l'aumento dei dati con le immagini ``fake'' campionate dalla rete generativa avversaria.Dopo di che, siamo sorpresi di vedere che il classificatore robusto risultante porta a un generatore migliore, gratuitamente.Spieghiamo intuitivamente questo fenomeno interessante e lasciamo l'analisi teorica per il lavoro futuro. Motivati da queste osservazioni, proponiamo un sistema che combina generatore, discriminatore e attaccante avversario insieme in una singola rete.Dopo l'addestramento end-to-end e la messa a punto, il nostro metodo può migliorare simultaneamente la robustezza dei classificatori, misurata dalla precisione sotto forti attacchi avversari, e la qualità dei generatori, valutata sia esteticamente che quantitativamente. In termini di classificatore, raggiungiamo una migliore robustezza rispetto all'algoritmo di addestramento adversariale allo stato dell'arte proposto in (Madry \textit{et al.}, 2017), mentre il nostro generatore raggiunge prestazioni competitive rispetto a SN-GAN (Miyato e Koyama, 2018).
Presentiamo un metodo per addestrare reti neurali auto-binarizzanti, cioè reti che evolvono i loro pesi e attivazioni durante l'addestramento per diventare binarie.Per ottenere reti binarie simili, i metodi esistenti si basano sulla funzione di attivazione del segno.Questa funzione, tuttavia, non ha gradienti per valori non nulli, il che rende impossibile la backpropagation standard.Per aggirare la difficoltà di addestrare una rete basandosi sulla funzione di attivazione del segno, questi metodi alternano tra rappresentazioni in virgola mobile e binarie della rete durante l'addestramento, che è sub-ottimale e inefficiente. Ci avviciniamo al compito di binarizzazione allenandoci su una rappresentazione unica che coinvolge una funzione di attivazione liscia, che viene iterativamente affinata durante l'allenamento fino a diventare una rappresentazione binaria equivalente alla funzione di attivazione del segno. Questo è diverso dai metodi esistenti, che sono costretti a mantenere la convenzionale normalizzazione batch in virgola mobile. Le nostre reti binarie, oltre a mostrare i vantaggi di una minore memoria e calcolo rispetto alle reti convenzionali in virgola mobile e binarie, mostrano anche una maggiore precisione di classificazione rispetto ai metodi esistenti allo stato dell'arte su più set di dati di riferimento.
  In molte applicazioni, i dati di formazione per un compito di apprendimento automatico sono suddivisi tra più nodi, e l'aggregazione di questi dati può essere impossibile a causa di vincoli di archiviazione, comunicazione o privacy.In questo lavoro, presentiamo Good-Enough Model Spaces (GEMS), un nuovo quadro per l'apprendimento di un modello globale satisficing (cioè "In esperimenti su benchmark e dataset medici, il nostro approccio supera altre tecniche di aggregazione di base come l'ensembling o la media dei modelli, e si comporta in modo comparabile ai modelli ideali non distribuiti.
Proponiamo il Wasserstein Auto-Encoder (WAE) - un nuovo algoritmo per la costruzione di un modello generativo della distribuzione dei dati. WAE minimizza una forma penalizzata della distanza di Wasserstein tra la distribuzione del modello e la distribuzione target, che porta a un regolatore diverso da quello utilizzato dal Variational Auto-Encoder (VAE). Confrontiamo il nostro algoritmo con diverse altre tecniche e dimostriamo che si tratta di una generalizzazione degli autocodificatori adversariali (AAE). I nostri esperimenti dimostrano che WAE condivide molte delle proprietà dei VAE (formazione stabile, architettura encoder-decoder, bella struttura del manifold latente), mentre genera campioni di qualità migliore.
In questo articolo, miriamo a sviluppare un nuovo meccanismo per preservare la privacy differenziale (DP) nell'apprendimento avversario per le reti neurali profonde, con una robustezza dimostrabile agli esempi avversari, sfruttando la teoria della composizione sequenziale in DP, per stabilire una nuova connessione tra la conservazione DP e la robustezza dimostrabile. Per affrontare il trade-off tra l'utilità del modello, la perdita di privacy e la robustezza, progettiamo una funzione obiettivo avversaria originale, differentemente privata, basata sulla proprietà di post-elaborazione in DP, per rafforzare la sensibilità del nostro modello.Un'analisi teorica end-to-end e valutazioni approfondite mostrano che il nostro meccanismo migliora notevolmente la robustezza delle reti neurali profonde DP.
Nelle impostazioni di apprendimento di rinforzo ad alta dimensione con ricompense sparse, l'esplorazione efficace per ottenere anche solo un segnale di ricompensa è una sfida aperta.Mentre gli approcci basati sul modello promettono una migliore esplorazione attraverso la pianificazione, è estremamente difficile imparare un processo decisionale di Markov (MDP) abbastanza affidabile in dimensioni elevate (ad es, in 10^100 stati).In questo articolo, proponiamo l'apprendimento di un MDP astratto su un numero molto più piccolo di stati (ad esempio, 10^5), su cui possiamo pianificare per un'esplorazione efficace.Assumiamo di avere una funzione di astrazione che mappa stati concreti (ad esempio, pixel grezzi) a stati astratti (ad es, Nel nostro approccio, un manager mantiene un MDP astratto su un sottoinsieme degli stati astratti, che cresce monotonicamente attraverso l'esplorazione mirata (possibile grazie al MDP astratto), e Private Eye), il nostro approccio supera il precedente stato dell'arte di oltre un fattore 2 in ogni gioco. In Pitfall!, il nostro approccio è il primo a raggiungere prestazioni sovrumane senza dimostrazioni.
La maggior parte dei sistemi profondi di apprendimento di rinforzo (RL) non sono in grado di imparare efficacemente dai dati off-policy, specialmente se non possono esplorare online nell'ambiente.Questo è un difetto critico per l'applicazione di RL ai problemi del mondo reale in cui la raccolta dei dati è costosa, e i modelli devono essere testati offline prima di essere distribuiti per interagire con l'ambiente -- es. Così, sviluppiamo una nuova classe di algoritmi off-policy batch RL che utilizzano KL-control per penalizzare la divergenza da un modello precedente pre-addestrato di azioni probabili. Questo KL-constraint riduce l'errore di estrapolazione, consentendo un efficace apprendimento offline, senza esplorazione, da un batch fisso di dati. Questo algoritmo Way Off-Policy (WOP) è stato testato sia su compiti RL tradizionali di OpenAI Gym, sia sul problema della generazione di dialoghi in domini aperti; un impegnativo problema di apprendimento per rinforzo con uno spazio d'azione di 20.000 dimensioni. WOP permette l'estrazione di più funzioni di ricompensa diverse post-hoc dai dati di interazione umana raccolti, e può imparare efficacemente da tutte queste. Testiamo la generalizzazione nel mondo reale distribuendo modelli di dialogo dal vivo per conversare con gli umani in un ambiente di dominio aperto, e dimostriamo che WOP raggiunge miglioramenti significativi rispetto ai metodi precedenti allo stato dell'arte nella RL profonda in batch.
Il trasferimento e l'adattamento a nuove dinamiche ambientali sconosciute è una sfida chiave per l'apprendimento di rinforzo (RL).Una sfida ancora più grande è l'esecuzione quasi ottimale in un singolo tentativo al momento del test, possibilmente senza accesso a ricompense dense, che non è affrontato dai metodi attuali che richiedono più rollout di esperienza per l'adattamento.Per ottenere il trasferimento di un singolo episodio in una famiglia di ambienti con dinamiche correlate, proponiamo un algoritmo generale che ottimizza una sonda e un modello di inferenza per stimare rapidamente le variabili latenti sottostanti delle dinamiche del test, che sono poi immediatamente utilizzate come input per una politica di controllo universale. Questo approccio modulare permette l'integrazione di algoritmi all'avanguardia per l'inferenza variazionale o RL.Inoltre, il nostro approccio non richiede l'accesso alle ricompense al momento del test, consentendo di eseguire in impostazioni in cui gli approcci adattativi esistenti non possono.In diversi domini sperimentali con un vincolo di test a singolo episodio, il nostro metodo supera significativamente gli approcci adattativi esistenti e mostra prestazioni favorevoli rispetto alle linee di base per il trasferimento robusto.
I sistemi di dialogo orientati agli obiettivi specifici del dominio richiedono tipicamente la modellazione di tre tipi di input, vale a dire, (i) la base di conoscenza associata al dominio, (ii) la storia della conversazione, che è una sequenza di enunciati e (iii) l'enunciato corrente per il quale deve essere generata la risposta. Mentre si modellano questi input, gli attuali modelli all'avanguardia come Mem2Seq tipicamente ignorano la ricca struttura inerente al grafico della conoscenza e le frasi nel contesto della conversazione. Ispirati dal recente successo delle reti convoluzionali a struttura consapevole (GCN) per vari compiti NLP come la traduzione automatica, l'etichettatura semantica dei ruoli e la datazione dei documenti, proponiamo una GCN aumentata dalla memoria per i dialoghi orientati agli obiettivi. Il nostro modello sfrutta (i) il grafo delle relazioni tra entità in una base di conoscenza e (ii) il grafo delle dipendenze associato a un enunciato per calcolare rappresentazioni più ricche per le parole e le entità. Inoltre, prendiamo atto del fatto che in certe situazioni, come quando la conversazione è in una lingua mista a codice, i parser delle dipendenze potrebbero non essere disponibili. Mostriamo che in tali situazioni potremmo usare il grafico di co-occorrenza globale delle parole e usarlo per arricchire le rappresentazioni degli enunciati. Sperimentiamo con il dataset DSTC2 modificato e le sue versioni miste a codice rilasciate di recente in quattro lingue e mostriamo che il nostro metodo supera i metodi esistenti allo stato dell'arte, usando una vasta gamma di metriche di valutazione.
In particolare, proponiamo SENSE-S (Semantically Enhanced Node Sequence Embedding - for Single nodes), un nuovo meccanismo di embedding basato su skip-gram, per i singoli nodi del grafico che apprende la struttura del grafico e le sue descrizioni testuali. Dimostriamo che i vettori SENSE-S aumentano l'accuratezza dei compiti di classificazione multietichetta fino al 50% e i compiti di previsione dei collegamenti fino al 78% in una varietà di scenari utilizzando set di dati reali. Sulla base di SENSE-S, proponiamo poi SENSE generico per calcolare vettori compositi che rappresentano una sequenza di nodi, dove è importante preservare l'ordine del nodo.
Recenti prove dimostrano che le reti neurali convoluzionali (CNN) sono orientate verso le texture, così che le CNN non sono robuste alle perturbazioni avversarie sulle texture, mentre le tradizionali caratteristiche visive robuste come SIFT (scale-invariant feature transforms) sono progettate per essere robuste attraverso una gamma sostanziale di distorsioni affini, aggiunta di rumore, ecc. con l'imitazione della natura della percezione umana. Questo articolo mira a sfruttare le buone proprietà di SIFT per rinnovare le architetture CNN verso una migliore accuratezza e robustezza. Prendiamo in prestito l'idea del valore estremo dello spazio di scala da SIFT, e proponiamo EVPNet (extreme value preserving network) che contiene tre nuovi componenti per modellare i valori estremi: (1) differenze parametriche della gaussiana (DoG) per estrarre gli estremi, (2) ReLU troncato per sopprimere gli estremi non stabili e (3) livello di normalizzazione proiettato (PNL) per imitare la normalizzazione delle caratteristiche PCA-SIFT. Gli esperimenti dimostrano che le EVPNet possono raggiungere una precisione simile o migliore delle CNN convenzionali, mentre raggiungono una robustezza molto migliore su una serie di attacchi avversari (FGSM, PGD, ecc) anche senza addestramento avversario.
Le rappresentazioni compresse generalizzano meglio (Shamir et al, Il metodo Information Bottleneck (IB) (Tishby et al. (2000)) fornisce un approccio intuitivo e di principio per bilanciare la compressione e la previsione nell'apprendimento della rappresentazione. L'obiettivo IB I(X; Z) - βI(Y ; Z) impiega un moltiplicatore di Lagrange β per regolare questo trade-off. Tuttavia, c'è poca guida teorica su come selezionare β.C'è anche una mancanza di comprensione teorica sulla relazione tra β, il set di dati, la capacità del modello e la capacità di apprendimento.In questo lavoro, mostriamo che se β è scelto impropriamente, l'apprendimento non può avvenire: la rappresentazione banale P(Z|X) = P(Z) diventa il minimo globale dell'obiettivo IB. Mostriamo come questo possa essere evitato, identificando una netta transizione di fase tra l'inapprendibile e l'apprendibile che si verifica al variare di β. Questa transizione di fase definisce il concetto di IB-Learnability, dimostrando diverse condizioni sufficienti per IB-Learnability, fornendo una guida teorica per la selezione di β. Dimostriamo inoltre che l'IB-Learnability è determinata dal più grande sottoinsieme fiducioso, tipico e squilibrato degli esempi di formazione.Diamo un algoritmo pratico per stimare il minimo β per un dato set di dati.Testiamo i nostri risultati teorici su set di dati sintetici, MNIST e CIFAR10 con etichette rumorose, e facciamo la sorprendente osservazione che l'accuratezza può essere non monotona in β.
 Consideriamo una nuova classe di attacchi \emph{data poisoning} alle reti neurali, in cui l'attaccante prende il controllo di un modello facendo piccole perturbazioni a un sottoinsieme dei suoi dati di allenamento.  Formuliamo il compito di trovare i veleni come un problema di ottimizzazione a due livelli, che può essere risolto utilizzando metodi presi in prestito dalla comunità del meta-apprendimento.  A differenza delle precedenti strategie di avvelenamento, il meta-avvelenamento può avvelenare le reti che sono addestrate da zero usando un'inizializzazione sconosciuta all'attaccante e trasferire attraverso gli iperparametri.Inoltre dimostriamo che i nostri attacchi sono più versatili: possono causare l'errata classificazione dell'immagine target in una classe arbitrariamente scelta.I nostri risultati mostrano un tasso di successo dell'attacco superiore al 50% quando si avvelena solo il 3-10% del dataset di allenamento.
Diamo un nuovo algoritmo per l'apprendimento di una rete neurale a due strati sotto una classe molto generale di distribuzioni di input.Supponendo che ci sia una rete a due strati ground-truth y = A \sigma(Wx) + \xi, dove A, W sono matrici di peso, \xi rappresenta il rumore, e il numero di neuroni nello strato nascosto non è più grande dell'input o dell'output, il nostro algoritmo è garantito per recuperare i parametri A, W della rete ground-truth. L'unico requisito dell'input x è che sia simmetrico, il che permette ancora input molto complicati e strutturati. Il nostro algoritmo si basa sul metodo dei momenti ed estende diversi risultati nelle decomposizioni tensoriali. Usiamo algoritmi spettrali per evitare la complicata ottimizzazione non convessa nell'apprendimento delle reti neurali. Gli esperimenti mostrano che il nostro algoritmo può apprendere in modo robusto la rete neurale ground-truth con un piccolo numero di campioni per molte distribuzioni simmetriche di input.
Tuttavia, se l'insegnante e lo studente sono reti neurali, gli esempi che la rete dell'insegnante impara a dare, anche se efficaci per insegnare allo studente, sono tipicamente non interpretabili. Valutiamo l'interpretabilità (1) misurando la somiglianza delle strategie emergenti dell'insegnante alle strategie intuitive in ogni dominio e (2) conducendo esperimenti umani per valutare quanto siano efficaci le strategie dell'insegnante nell'insegnare agli umani.Mostriamo che la rete dell'insegnante impara a selezionare o generare esempi pedagogici interpretabili per insegnare concetti basati su regole, probabilistici, booleani e gerarchici.
La discesa del gradiente stocastico (SGD), che scambia gli aggiornamenti rumorosi del gradiente con l'efficienza computazionale, è l'algoritmo di ottimizzazione de-facto per risolvere problemi di apprendimento automatico su larga scala.SGD può fare rapidi progressi nell'apprendimento eseguendo gli aggiornamenti usando dati di allenamento sottocampionati, ma gli aggiornamenti rumorosi portano anche a una convergenza asintotica lenta.   Diversi algoritmi di riduzione della varianza, come SVRG, introducono varianti di controllo per ottenere una stima del gradiente di varianza più bassa e una convergenza più veloce.  Nonostante le loro attraenti garanzie asintotiche, gli algoritmi simili a SVRG non sono stati ampiamente adottati nell'apprendimento profondo.La tradizionale analisi asintotica nell'ottimizzazione stocastica fornisce una visione limitata nell'addestramento di modelli di apprendimento profondo sotto un numero fisso di epoche.In questo articolo, presentiamo un'analisi non asintotica di SVRG sotto un problema di regressione rumorosa dei minimi quadrati.Il nostro obiettivo principale è quello di confrontare la perdita esatta di SVRG a quella di SGD ad ogni iterazione t. Mostriamo che la dinamica di apprendimento del nostro modello di regressione corrisponde da vicino a quella delle reti neurali su MNIST e CIFAR-10 sia per i modelli sottoparametrizzati che per quelli sovraparametrizzati. La nostra analisi e i risultati sperimentali suggeriscono che c'è un trade-off tra il costo computazionale e la velocità di convergenza nelle reti neurali sottoparametrizzate.SVRG supera SGD dopo alcune epoche in questo regime.Tuttavia, SGD è dimostrato di superare sempre SVRG nel regime sovraparametrizzato.
I sistemi di traduzione automatica non autoregressiva (NAT) prevedono una sequenza di token di output in parallelo, ottenendo miglioramenti sostanziali nella velocità di generazione rispetto ai modelli autoregressivi.I modelli NAT esistenti di solito si basano sulla tecnica di distillazione della conoscenza, che crea i dati di formazione da un modello autoregressivo preaddestrato per ottenere prestazioni migliori.La distillazione della conoscenza è empiricamente utile, portando a grandi guadagni di precisione per i modelli NAT, ma la ragione di questo successo è stata, finora, poco chiara.In questo articolo, abbiamo prima progettato esperimenti sistematici per indagare perché la distillazione della conoscenza è fondamentale per la formazione NAT. Troviamo che la distillazione della conoscenza può ridurre la complessità dei set di dati e aiutare NAT a modellare le variazioni nei dati in uscita.Inoltre, si osserva una forte correlazione tra la capacità di un modello NAT e la complessità ottimale dei dati distillati per la migliore qualità di traduzione.Sulla base di questi risultati, proponiamo diversi approcci che possono modificare la complessità dei set di dati per migliorare le prestazioni dei modelli NAT.Raggiungiamo lo stato dell'arte delle prestazioni per i modelli basati su NAT, e chiudiamo il gap con la linea di base autoregressiva sul benchmark WMT14 En-De.
Grazie alla loro capacità di integrare efficacemente le informazioni su orizzonti temporali lunghi e di scalare a quantità massicce di dati, le architetture di auto-attenzione hanno recentemente mostrato un successo rivoluzionario nell'elaborazione del linguaggio naturale (NLP), raggiungendo risultati all'avanguardia in domini come la modellazione del linguaggio e la traduzione automatica. Sfruttare la capacità del trasformatore di elaborare lunghi orizzonti temporali di informazioni potrebbe fornire un simile incremento delle prestazioni nei domini di apprendimento di rinforzo (RL) parzialmente osservabili, ma i trasformatori su larga scala utilizzati in NLP devono ancora essere applicati con successo all'impostazione RL.In questo lavoro dimostriamo che l'architettura standard del trasformatore è difficile da ottimizzare, cosa che è stata precedentemente osservata nell'impostazione di apprendimento supervisionato ma diventa particolarmente pronunciata con obiettivi RL. L'architettura proposta, la Gated Transformer-XL (GTrXL), supera le LSTM su ambienti di memoria difficili e raggiunge risultati all'avanguardia sulla suite di benchmark multi-task DMLab-30, superando le prestazioni di un'architettura a memoria esterna. Dimostriamo che il GTrXL, addestrato utilizzando le stesse perdite, ha stabilità e prestazioni che corrispondono costantemente o superano una linea di base LSTM competitiva, anche su compiti più reattivi dove la memoria è meno critica.GTrXL offre un'alternativa architettonica facile da addestrare, semplice da implementare ma sostanzialmente più espressiva allo standard multistrato LSTM onnipresente per agenti RL in ambienti parzialmente osservabili.  
La distillazione della conoscenza è un'efficace tecnica di compressione del modello in cui un modello più piccolo viene addestrato per imitare un modello più grande preaddestrato. Tuttavia, al fine di rendere questi modelli compatti adatti al dispiegamento nel mondo reale, non solo abbiamo bisogno di ridurre il divario di prestazioni, ma anche di renderli più robusti alle perturbazioni comuni e avversarie. Crediamo quindi che il rumore potrebbe essere un elemento cruciale per migliorare l'addestramento delle reti neurali e affrontare gli obiettivi apparentemente contraddittori di migliorare sia la generalizzazione che la robustezza di themodel.Inspirato dalla variabilità trial-to-trial nel cervello che può derivare da più fonti di rumore, introduciamo la variabilità attraverso il rumore sia a livello di input che dei segnali di supervisione. I nostri risultati mostrano che il rumore può migliorare sia la generalizzazione che la robustezza del modello. "Insegnante volubile", che utilizza il dropout nel modello dell'insegnante come fonte di variazione della risposta, porta a un significativo miglioramento della generalizzazione. "Soft Randomization", che abbina la distribuzione dell'output del modello dello studente sull'immagine con rumore gaussiano all'output dell'insegnante sull'immagine originale, migliora la robustezza avversaria rispetto al modello dello studente allenato con rumore gaussiano. Lo studio evidenzia i benefici dell'aggiunta di rumore costruttivo nel quadro della distillazione della conoscenza e spera di ispirare ulteriori lavori in quest'area.
Il nostro obiettivo di clustering si basa sull'ottimizzazione dei tagli normalizzati, un criterio che misura sia la somiglianza intra-cluster che la dissimilarità inter-cluster, e definisce una funzione di perdita differenziabile equivalente ai tagli normalizzati attesi. A differenza di gran parte del lavoro nel deep learning non supervisionato, il nostro modello addestrato produce direttamente le assegnazioni finali dei cluster, piuttosto che le embeddings che hanno bisogno di ulteriori elaborazioni per essere utilizzabili.Il nostro approccio si generalizza ai set di dati non visti in un'ampia varietà di domini, inclusi testo e immagini.In particolare, raggiungiamo risultati all'avanguardia sui benchmark popolari di clustering non supervisionato (ad esempio, MNIST, Reuters, CIFAR-10, e CIFAR-100), superando le linee di base più forti fino al 10,9%. I nostri risultati di generalizzazione sono superiori (fino al 21,9%) al recente approccio di clustering più performante con la capacità di generalizzare.
Introduciamo il più grande (tra quelli pubblicamente disponibili) set di dati per il riconoscimento del testo scritto a mano in cirillico e il primo set di dati per il riconoscimento del testo cirillico in natura, oltre a suggerire un metodo per il riconoscimento del testo scritto a mano in cirillico e del testo in natura.Sulla base di questo approccio, sviluppiamo un sistema che può ridurre il tempo di elaborazione dei documenti per uno dei più grandi concorsi matematici in Ucraina di 12 giorni e la quantità di carta usata di 0,5 tonnellate.
La maggior parte del deep learning per NLP rappresenta ogni parola con un singolo punto o una regione monomodale nello spazio semantico, mentre le incorporazioni di parole multimodali esistenti non possono rappresentare sequenze di parole più lunghe come le frasi o le sentenze.Introduciamo una rappresentazione della frase (applicabile anche alle frasi) dove ogni frase ha un insieme distinto di incorporazioni codebook multimodali per catturare diversi aspetti semantici del significato della frase. Le incorporazioni del codebook possono essere viste come i centri dei cluster che riassumono la distribuzione delle parole eventualmente co-occorrenti in uno spazio di incorporamento delle parole pre-addestrato.Proponiamo un modello neurale addestrabile end-to-end che predice direttamente l'insieme dei centri dei cluster dalla sequenza di testo di input (ad es, Troviamo che le incorporazioni per frase/sentenza del codebook non solo forniscono una rappresentazione semantica più interpretabile, ma superano anche le forti linee di base (con un ampio margine in alcuni compiti) sui set di dati di riferimento per la similarità di frase non supervisionata, la similarità di frase, il rilevamento di ipernimi e la sintesi estrattiva.
Presentiamo un approccio di meta-apprendimento per il text-to-speech (TTS) adattivo con pochi dati. durante l'addestramento, impariamo un modello multi-speaker usando un nucleo WaveNet condizionale condiviso e embeddings imparati indipendenti per ogni speaker. lo scopo dell'addestramento non è quello di produrre una rete neurale con pesi fissi, che viene poi distribuito come un sistema TTS. invece, lo scopo è quello di produrre una rete che richiede pochi dati al momento della distribuzione per adattarsi rapidamente a nuovi speaker. Gli esperimenti mostrano che questi approcci hanno successo nell'adattare la rete neurale multi-speaker a nuovi altoparlanti, ottenendo risultati allo stato dell'arte sia nella naturalezza del campione che nella somiglianza della voce con solo pochi minuti di dati audio da nuovi altoparlanti.
L'obiettivo è quello di generalizzare da un compito iniziale di classificazione su larga scala a un compito separato che comprende nuove classi e un piccolo numero di esempi. Il nuovo approccio non solo sfrutta la rappresentazione basata sulle caratteristiche appresa da una rete neurale dal compito iniziale (trasferimento rappresentazionale), ma anche le informazioni sulle classi (trasferimento concettuale). Le informazioni sui concetti sono incapsulate in un modello probabilistico per i pesi dello strato finale della rete neurale, che agisce come un priore per l'apprendimento probabilistico di k-shot. Mostriamo che anche un semplice modello probabilistico raggiunge lo stato dell'arte su un set di dati standard di apprendimento di k-shot con un ampio margine. Inoltre, è in grado di modellare accuratamente l'incertezza, portando a classificatori ben calibrati, ed è facilmente estensibile e flessibile, a differenza di molti approcci recenti di apprendimento di k-shot.
Basandoci sui recenti successi dell'addestramento distribuito degli agenti RL, in questo articolo esaminiamo l'addestramento degli agenti RL basati su RNN dal replay distribuito dell'esperienza prioritaria. Studiamo gli effetti del ritardo dei parametri con conseguente deriva rappresentazionale e stalking dello stato ricorrente e deriviamo empiricamente una migliore strategia di addestramento. Usando una singola architettura di rete e un set fisso di iper-parametri, l'agente risultante, Recurrent Replay Distributed DQN, quadruplica il precedente stato dell'arte su Atari-57, e corrisponde allo stato dell'arte su DMLab-30. È il primo agente a superare le prestazioni di livello umano in 52 dei 57 giochi Atari.
L'attuale stato dell'arte dell'etichettatura dei ruoli semantici (SRL) è un'architettura di rete neurale profonda senza caratteristiche linguistiche esplicite. In questo lavoro, presentiamo l'auto-attenzione linguisticamente informata (LISA): un nuovo modello di rete neurale che combina l'auto-attenzione a più teste con l'apprendimento multi-task attraverso il parsing delle dipendenze, il part-of-speech, l'individuazione dei predicati e la SRL, per esempio, la sintassi è incorporata allenando una delle teste di attenzione ad assistere ai genitori sintattici per ogni token. Il nostro modello può prevedere tutti i compiti di cui sopra, ma è anche addestrato in modo tale che se un parsing sintattico di alta qualità è già disponibile, può essere vantaggiosamente iniettato al momento del test senza ri-addestrare il nostro modello SRL.Negli esperimenti sul dataset CoNLL-2005 SRL LISA raggiunge un aumento di 2,5 F1 assoluto rispetto allo stato dell'arte precedente su newswire con predetti predicati e più di 2,0 F1 su dati fuori dal dominio.Su ConLL-2012 English SRL mostriamo anche un miglioramento di più di 3,0 F1, una riduzione del 13% degli errori.
Le strutture a collo di bottiglia con connessione di identità (ad esempio, residua) sono ora paradigmi popolari emergenti per la progettazione di reti neurali convoluzionali profonde (CNN), per l'elaborazione di caratteristiche su larga scala in modo efficiente.In questo articolo, ci concentriamo sulla natura di conservazione delle informazioni della connessione di identità e utilizzare questo per consentire uno strato convoluzionale per avere una nuova funzionalità di canale-selettività, cioè, In particolare, proponiamo Selective Convolutional Unit (SCU), un'unità architettonica ampiamente applicabile che migliora l'efficienza dei parametri di varie CNN moderne con colli di bottiglia. Durante l'addestramento, SCU impara gradualmente la selettività del canale al volo attraverso l'uso alternativo di (a) potatura dei canali non importanti, e (b) ricablaggio dei parametri potati ai canali importanti. I parametri ricablati enfatizzano il canale di destinazione in un modo che allarga selettivamente i kernel convoluzionali corrispondenti ad esso. I nostri risultati sperimentali dimostrano che i modelli basati su SCU senza alcun postprocessing generalmente raggiungono sia la compressione del modello che il miglioramento della precisione rispetto alle linee di base, in modo coerente per tutte le architetture testate.
Un esempio è la stima degli effetti del trattamento da dati osservazionali, dove un sottocompito è quello di prevedere l'effetto di un trattamento su soggetti che sono sistematicamente diversi da quelli che hanno ricevuto il trattamento nei dati.Un tipo correlato di spostamento distributivo appare nell'adattamento di dominio non supervisionato, dove abbiamo il compito di generalizzare a una distribuzione di input che è diversa da quella in cui osserviamo le etichette.Poniamo entrambi questi problemi come previsione sotto uno spostamento nel design. I metodi popolari per superare lo spostamento distributivo sono spesso euristici o si basano su presupposti che raramente sono veri nella pratica, come avere un modello ben specificato o conoscere la politica che ha dato origine ai dati osservati.Altri metodi sono ostacolati dalla loro necessità di una metrica pre-specificata per confrontare le osservazioni, o da scarse proprietà asintotiche. In questo lavoro, mettiamo a punto un limite all'errore di generalizzazione sotto spostamento di progettazione, basato su metriche di probabilità integrali e sulla riponderazione dei campioni, e combiniamo questa idea con l'apprendimento delle rappresentazioni, generalizzando e rendendo più rigorosi i risultati esistenti in questo spazio.
Gli insiemi profondi hanno dimostrato empiricamente di essere un approccio promettente per migliorare l'accuratezza, l'incertezza e la robustezza fuori dalla distribuzione dei modelli di apprendimento profondo.Mentre gli insiemi profondi sono stati teoricamente motivati dal bootstrap, gli insiemi non-bootstrap addestrati con una semplice inizializzazione casuale funzionano bene anche nella pratica, il che suggerisce che ci potrebbero essere altre spiegazioni del perché gli insiemi profondi funzionano bene. Le reti neurali bayesiane, che imparano le distribuzioni sui parametri della rete, sono teoricamente ben motivate dai principi bayesiani, ma non si comportano così bene come gli ensemble profondi nella pratica, in particolare in caso di spostamento del dataset. Una possibile spiegazione di questo divario tra teoria e pratica è che i metodi bayesiani approssimativi scalabili popolari tendono a concentrarsi su una singola modalità, mentre gli ensemble profondi tendono a esplorare diverse modalità nello spazio delle funzioni. I nostri risultati mostrano che le inizializzazioni casuali esplorano modalità completamente diverse, mentre le funzioni lungo una traiettoria di ottimizzazione o campionate dal relativo sottospazio si raggruppano all'interno di una singola modalità di previsione, mentre spesso deviano significativamente nello spazio dei pesi. Dimostriamo che mentre i connettori a bassa perdita tra le modalità esistono, non sono collegati nello spazio delle previsioni. sviluppando il concetto di diversità - piano di precisione, mostriamo che il potere di decorrelazione delle inizializzazioni casuali è ineguagliato dai metodi popolari di campionamento del sottospazio.
Da una prospettiva di conservazione della privacy, le informazioni visive in ingresso non sono protette dal modello; consentendo al modello di diventare più intelligente di quanto sia addestrato ad essere.Gli approcci esistenti per sopprimere l'apprendimento di compiti aggiuntivi presuppongono la presenza di etichette di verità a terra per i compiti da sopprimere durante il tempo di formazione. In questa ricerca, proponiamo un triplice contributo innovativo: (i) una nuova metrica per misurare il punteggio di fiducia di un modello di apprendimento profondo addestrato, (ii) un quadro di soluzione modello-agnostico per il miglioramento del punteggio di fiducia sopprimendo tutti i compiti indesiderati, e (iii) un set di dati di riferimento simulato, PreserveTask, con cinque diversi compiti fondamentali di classificazione delle immagini per studiare la natura di generalizzazione dei modelli.Nella prima serie di esperimenti, misuriamo e miglioriamo il punteggio di fiducia di cinque popolari modelli di apprendimento profondo: VGG16, VGG19, Inception-v1, MobileNet, e DenseNet e dimostriamo che Inception-v1 ha il punteggio di fiducia più basso.Inoltre, mostriamo i risultati del nostro framework sul dataset colore-MNIST e le applicazioni pratiche di conservazione degli attributi del viso in Diversity in Faces (DiF) e IMDB-Wiki dataset.
  L'apprendimento di una politica utilizzando solo dati osservativi è impegnativo perché la distribuzione degli stati che induce al momento dell'esecuzione può differire dalla distribuzione osservata durante l'addestramento. In questo lavoro, proponiamo di addestrare una politica penalizzando esplicitamente la discrepanza tra queste due distribuzioni su un orizzonte temporale fisso, utilizzando un modello appreso della dinamica dell'ambiente che viene srotolato per più passi temporali, e addestrando una rete politica per minimizzare un costo differenziabile su questa traiettoria srotolata. Questo costo contiene due termini: un costo di politica che rappresenta l'obiettivo che la politica cerca di ottimizzare, e un costo di incertezza che rappresenta la sua divergenza dagli stati su cui è addestrata.Proponiamo di misurare questo secondo costo usando l'incertezza del modello dinamico sulle sue stesse previsioni, usando idee recenti dalla stima dell'incertezza per le reti profonde.Valutiamo il nostro approccio usando un dataset osservazionale su larga scala del comportamento di guida registrato dalle telecamere del traffico, e mostriamo che siamo in grado di imparare politiche di guida efficaci da dati puramente osservazionali, senza interazione con l'ambiente.
I modelli di sistemi dinamici (compreso RNNs) spesso non hanno la capacità di adattare la generazione o la previsione della sequenza ad un determinato contesto, limitando la loro applicazione nel mondo reale. In questa carta mostriamo che i sistemi dinamici multi-task gerarchici (MTDSs) forniscono il controllo diretto dell'utente sulla generazione della sequenza, tramite l'uso di un codice latente z che specifica la personalizzazione alla sequenza dei dati individuali. Questo permette il trasferimento di stile, l'interpolazione e il morphing all'interno delle sequenze generate. Mostriamo che gli MTDS possono migliorare le previsioni attraverso l'interpolazione del codice latente, ed evitare il degrado delle prestazioni a lungo termine degli approcci RNN standard.
Iniettando esempi avversari nei dati di formazione, la formazione avversaria è promettente per migliorare la robustezza dei modelli di apprendimento profondo.Tuttavia, la maggior parte degli approcci di formazione avversaria esistenti sono basati su un tipo specifico di attacco avversario.Potrebbe non fornire campioni sufficientemente rappresentativi del dominio avversario, portando a una debole capacità di generalizzazione su esempi avversari di altri attacchi. Inoltre, durante l'addestramento avversario, le perturbazioni avversarie sugli input sono solitamente realizzate da avversari veloci a passo singolo in modo da scalare a grandi dataset.Questo lavoro si concentra principalmente sull'addestramento avversario ancora efficiente FGSM adversary.In questo scenario, è difficile addestrare un modello con grande generalizzazione a causa della mancanza di campioni rappresentativi avversari, aka i campioni non sono in grado di riflettere accuratamente il dominio avversario. Per alleviare questo problema, proponiamo un nuovo metodo di Adversarial Training with Domain Adaptation (ATDA).La nostra intuizione è quella di considerare l'addestramento avversario su FGSM avversario come un compito di adattamento del dominio con un numero limitato di campioni di dominio target.L'idea principale è quella di imparare una rappresentazione che sia semanticamente significativa e invariante sul dominio pulito e sul dominio avversario. Valutazioni empiriche su Fashion-MNIST, SVHN, CIFAR-10 e CIFAR-100 dimostrano che ATDA può migliorare notevolmente la generalizzazione dell'addestramento avversario e la scorrevolezza dei modelli appresi, e supera i metodi all'avanguardia su set di dati standard di riferimento.Per mostrare la capacità di trasferimento del nostro metodo, estendiamo anche ATDA all'addestramento avversario su attacchi iterativi come PGD-Adversial Training (PAT) e le prestazioni di difesa sono migliorate notevolmente.
La modellazione del linguaggio a livello di carattere è un compito essenziale ma impegnativo nel Natural Language Processing. I lavori precedenti si sono concentrati sull'identificazione delle dipendenze a lungo termine tra i caratteri e hanno costruito reti più profonde e più ampie per una migliore performance.Tuttavia, i loro modelli richiedono notevoli risorse computazionali, il che ostacola l'usabilità dei modelli linguistici a livello di carattere in applicazioni con risorse limitate.In questo articolo, proponiamo un modello leggero, chiamato Group-Transformer, che riduce i requisiti di risorse per un Transformer, un metodo promettente per la modellazione di sequenza con dipendenze a lungo termine. In particolare, il metodo proposto partiziona le operazioni lineari per ridurre il numero di parametri e il costo computazionale.Come risultato, Group-Transformer utilizza solo il 18.2\% dei parametri rispetto al modello basato su LSTM più performante, fornendo allo stesso tempo prestazioni migliori su due compiti di benchmark, enwik8 e text8.Se confrontato con Transformers con un numero comparabile di parametri e complessità temporale, il modello proposto mostra prestazioni migliori.Il codice di implementazione sarà disponibile.
  L'adattamento al dominio affronta il problema di trasferire la conoscenza da un dominio di origine ricco di etichette a un dominio di destinazione privo di etichette o con poche etichette.Recentemente l'addestramento domain-adversarial (DAT) ha mostrato una promettente capacità di apprendere uno spazio di caratteristiche invarianti al dominio invertendo la propagazione del gradiente di un classificatore di dominio. Tuttavia, DAT è ancora vulnerabile in diversi aspetti tra cui (1) l'instabilità di formazione a causa della capacità discriminatoria schiacciante del classificatore di dominio in formazione avversaria, (2) l'allineamento restrittivo a livello di caratteristica, e (3) la mancanza di interpretabilità o spiegazione sistematica dello spazio di caratteristica appreso.In questo documento, proponiamo un nuovo Max-margin Domain-Adversarial Training (MDAT) progettando una Adversarial Reconstruction Network (ARN). Il MDAT proposto stabilizza l'inversione del gradiente in ARN sostituendo il classificatore di dominio con una rete di ricostruzione, e in questo modo ARN conduce sia l'allineamento di dominio a livello di caratteristica che a livello di pixel senza coinvolgere strutture di rete extra.Inoltre, ARN dimostra una forte robustezza a una vasta gamma di impostazioni iper-parametri, alleviando notevolmente il compito di selezione del modello.Ampi risultati empirici convalidano che il nostro approccio supera altri metodi di allineamento di dominio all'avanguardia.Inoltre, i campioni di destinazione ricostruiti sono visualizzati per interpretare lo spazio di caratteristica invariante al dominio che è conforme alla nostra intuizione.
Presentiamo un approccio per espandere le tassonomie con sinonimi, o alias.Ci rivolgiamo a grandi tassonomie di shopping, con migliaia di nodi.Un set completo di alias di entità è una componente importante per identificare le entità in testi non strutturati come recensioni di prodotti o query di ricerca.Il nostro metodo consiste in due fasi: generiamo candidati sinonimi da WordNet e query di ricerca di shopping, quindi utilizziamo un classiﬁer binario per filtrare i candidati. Elaboriamo tassonomie con migliaia di sinonimi al fine di generare oltre 90.000 sinonimi.Mostriamo che l'utilizzo della tassonomia per derivare le caratteristiche contestuali migliora le prestazioni di classiﬁcazione rispetto all'utilizzo delle caratteristiche dal solo nodo di destinazione.Mostriamo che il nostro approccio ha un potenziale per l'apprendimento di trasferimento tra domini di tassonomia diﬀerenti, che riduce la necessità di raccogliere dati di formazione per nuove tassonomie.
Il ragionamento relazionale, la capacità di modellare le interazioni e le relazioni tra gli oggetti, è prezioso per un robusto tracciamento multi-oggetto e fondamentale per la predizione della traiettoria. In questo articolo, proponiamo MOHART, un algoritmo di tracciamento multi-oggetto end-to-end, classe-agnostico, e di predizione della traiettoria, che tiene esplicitamente conto dell'invarianza di permutazione nel suo ragionamento relazionale.Esploriamo una serie di architetture invarianti di permutazione e dimostriamo che l'autoattenzione a più teste supera le prestazioni di base fornite e rappresenta meglio le interazioni fisiche complesse in un difficile esperimento giocattolo. Mostriamo su tre set di dati di tracciamento del mondo reale che l'aggiunta di capacità di ragionamento relazionale in questo modo aumenta le prestazioni di tracciamento e di previsione della traiettoria, in particolare in presenza di ego-motion, occlusioni, scene affollate e input di sensori difettosi.Per quanto ne sappiamo, MOHART è il primo approccio di tracciamento multi-oggetto completamente end-to-end dalla visione applicato ai dati del mondo reale riportati in letteratura.
Indaghiamo la combinazione di algoritmi di apprendimento di rinforzo actor-critic con replay di esperienza su larga scala uniforme e proponiamo soluzioni per due sfide: (a) apprendimento actor-critic efficiente con replay di esperienza (b) stabilità dell'apprendimento molto off-policy.We impiegare tali intuizioni per accelerare iper-parametro sweeps in cui tutti gli agenti partecipanti eseguire contemporaneamente e condividere la loro esperienza tramite un modulo comune replay.To questo fine analizziamo i bias-varianza compromessi in V-trace, una forma di campionamento di importanza per attore-critico metodi. Sulla base della nostra analisi, sosteniamo poi la necessità di mescolare l'esperienza campionata dal replay con l'esperienza on-policy, e proponiamo un nuovo schema di regione di fiducia che scala efficacemente alle distribuzioni di dati in cui V-trace diventa instabile.Forniamo un'ampia validazione empirica della soluzione proposta.Mostriamo inoltre i benefici di questa configurazione dimostrando un'efficienza dei dati allo stato dell'arte su Atari tra gli agenti addestrati fino a 200M frame ambientali.
Questo articolo analizza i tassi di convergenza di SGD in funzione del tempo, piuttosto che delle iterazioni, e fornisce una semplice regola per selezionare lo stimatore che porta alla migliore garanzia di convergenza dell'ottimizzazione. Questa scelta è la stessa per diverse varianti di SGD, e con diverse assunzioni sull'obiettivo (per esempio convessità o scorrevolezza). Ispirandoci a questo principio, proponiamo una tecnica per selezionare automaticamente uno stimatore quando è dato un pool finito di stimatori. Poi, ci estendiamo a pool infiniti di stimatori, dove ognuno è indicizzato da pesi di variabili di controllo.
La discrepanza tra i valori obiettivo minimax e maximin potrebbe servire come indicatore delle difficoltà che la discesa a gradiente alternato incontra nell'ottimizzazione di GANs.In questo lavoro, diamo nuovi risultati sui benefici dell'architettura multi-generatore di GANs.We mostrano che il gap minimax si riduce a \epsilon come il numero di generatori aumenta con tasso O(1/\epsilon). Al centro delle nostre tecniche c'è una nuova applicazione del lemma di Shapley-Folkman al generico problema minimax, dove in letteratura la tecnica era nota solo per funzionare quando la funzione obiettivo è limitata alla funzione lagrangiana di un problema di ottimizzazione vincolata. La nostra proposta di Stackelberg GAN si comporta bene sperimentalmente in entrambi i set di dati sintetici e del mondo reale, migliorando la Frechet Inception Distance del 14,61% rispetto alle precedenti GAN multi-generatore sui set di dati di riferimento.
Nesterov SGD è ampiamente utilizzato per l'addestramento delle moderne reti neurali e di altri modelli di apprendimento automatico, ma i suoi vantaggi rispetto a SGD non sono stati chiariti teoricamente. Infatti, come dimostriamo in questo articolo, sia teoricamente che empiricamente, Nesterov SGD con qualsiasi selezione di parametri non fornisce in generale un'accelerazione rispetto a SGD ordinario, inoltre Nesterov SGD può divergere per dimensioni di passo che garantiscono la convergenza di SGD ordinario. Questo è in contrasto con i risultati classici nell'impostazione deterministica, dove la stessa dimensione del passo assicura una convergenza accelerata del metodo di Nesterov rispetto alla discesa ottimale del gradiente.Per affrontare il problema della non-accelerazione, introduciamo un termine di compensazione a Nesterov SGD.L'algoritmo risultante, che chiamiamo MaSS, converge per le stesse dimensioni del passo di SGD.Proviamo che MaSS ottiene un tasso di convergenza accelerato rispetto a SGD per qualsiasi dimensione del mini-batch nell'impostazione lineare.  Per il batch completo, il tasso di convergenza di MaSS corrisponde al ben noto tasso accelerato del metodo di Nesterov. Analizziamo anche la questione praticamente importante della dipendenza del tasso di convergenza e degli iper-parametri ottimali dalla dimensione del mini-batch, dimostrando tre regimi distinti: scaling lineare, rendimenti decrescenti e saturazione. La valutazione sperimentale di MaSS per diverse architetture standard di reti profonde, tra cui ResNet e reti convoluzionali, mostra prestazioni migliori rispetto a SGD, Nesterov SGD e Adam.
Proponiamo il Fixed Grouping Layer (FGL); un nuovo strato feedforward progettato per incorporare il bias induttivo della morbidezza strutturata in un modello di apprendimento profondo.FGL raggiunge questo obiettivo collegando i nodi tra gli strati in base alla somiglianza spaziale.L'uso della morbidezza strutturata, come implementato da FGL, è motivato da applicazioni a dati spaziali strutturati, che è, a sua volta, motivato dalla conoscenza del dominio.L'architettura del modello proposto supera le architetture di rete neurale convenzionali attraverso una varietà di set di dati simulati e reali con morbidezza strutturata.
Le reti convoluzionali non sono consapevoli delle variazioni geometriche di un oggetto, il che porta a un utilizzo inefficiente del modello e della capacità dei dati. Per superare questo problema, lavori recenti sulla modellazione della deformazione cercano di riconfigurare spazialmente i dati verso una disposizione comune in modo che il riconoscimento semantico soffra meno della deformazione. Questo è tipicamente fatto aumentando gli operatori statici con griglie di campionamento a forma libera apprese nello spazio dell'immagine, sintonizzate dinamicamente sui dati e sul compito di adattare il campo recettivo.Tuttavia adattare il campo recettivo non raggiunge del tutto l'obiettivo reale - ciò che conta davvero per la rete è il campo recettivo *effettivo* (ERF), che riflette quanto ogni pixel contribuisce.È quindi naturale progettare altri approcci per adattare l'ERF direttamente durante l'esecuzione. In questo lavoro, istanziamo una possibile soluzione come Deformable Kernels (DKs), una famiglia di operatori convoluzionari nuovi e generici per gestire le deformazioni degli oggetti adattando direttamente l'ERF mentre lasciamo il campo recettivo intatto. Il cuore del nostro metodo è la capacità di ricampionare lo spazio originale del kernel per recuperare la deformazione degli oggetti. Implementiamo i DK come sostituzioni generiche di kernel rigidi e conduciamo una serie di studi empirici i cui risultati sono conformi alle nostre teorie. Su diversi compiti e modelli di base standard, il nostro approccio si confronta favorevolmente con i lavori precedenti che si adattano durante il runtime.
L'inferenza delle proprietà strutturali di una proteina dalla sua sequenza aminoacidica è un problema impegnativo ma importante in biologia: le strutture non sono note per la stragrande maggioranza delle sequenze proteiche, ma la struttura è fondamentale per comprendere la funzione. Gli approcci esistenti per rilevare la somiglianza strutturale tra le proteine dalla sequenza non sono in grado di riconoscere e sfruttare i modelli strutturali quando le sequenze sono troppo diverse, limitando la nostra capacità di trasferire la conoscenza tra le proteine strutturalmente correlate.Noi ci avviciniamo di nuovo a questo problema attraverso la lente dell'apprendimento della rappresentazione.Introduciamo una struttura che mappa qualsiasi sequenza di proteine a una sequenza di embeddings vettoriali --- uno per posizione aminoacidica --- che codifica le informazioni strutturali. Addestriamo modelli bidirezionali di memoria a breve termine (LSTM) sulle sequenze proteiche con un meccanismo di feedback in due parti che incorpora informazioni da (i) somiglianza strutturale globale tra le proteine e (ii) mappe di contatto dei residui a coppie per le singole proteine.Per consentire l'apprendimento da informazioni di somiglianza strutturale, definiamo una nuova misura di somiglianza tra sequenze di lunghezza arbitraria di embeddings vettoriali basata su un allineamento simmetrico morbido (SSA) tra loro. Mostriamo empiricamente che la nostra struttura multi-task supera altri metodi basati sulla sequenza e anche un metodo di allineamento basato sulla struttura con le migliori prestazioni quando si predice la somiglianza strutturale, il nostro obiettivo. Infine, dimostriamo che le nostre embeddings apprese possono essere trasferite ad altri problemi di sequenza proteica, migliorando lo stato dell'arte nella previsione del dominio transmembrana.
Ispirandoci al fenomeno di adattamento degli spari neuronali biologici, proponiamo la normalizzazione della regolarità: una riparametrizzazione dell'attivazione nella rete neurale che tiene conto della regolarità statistica nello spazio implicito.Considerando il processo di ottimizzazione della rete neurale come un problema di selezione del modello, lo spazio implicito è limitato dal fattore di normalizzazione, la lunghezza minima di descrizione del codice universale ottimale. Introduciamo una versione incrementale del calcolo di questo codice universale come massima verosimiglianza normalizzata e abbiamo dimostrato la sua flessibilità per includere dati precedenti come l'attenzione top-down e altre informazioni oracolari e la sua compatibilità per essere incorporata nella normalizzazione dei lotti e nella normalizzazione dei livelli. I risultati preliminari hanno mostrato che il metodo proposto supera i metodi di normalizzazione esistenti nell'affrontare i dati limitati e sbilanciati da una distribuzione non stazionaria, con un benchmark sul compito di computer vision.Come meccanismo di attenzione non supervisionato dato i dati di input, questa normalizzazione biologicamente plausibile ha il potenziale per affrontare altri scenari complicati del mondo reale, così come l'impostazione di apprendimento di rinforzo dove i premi sono sparsi e non uniformi.Ulteriori ricerche sono proposte per scoprire questi scenari ed esplorare i comportamenti tra diverse varianti.
In questo articolo studiamo la modellazione generativa tramite autoencoder utilizzando le eleganti proprietà geometriche del problema del trasporto ottimale (OT) e le distanze di Wasserstein.Introduciamo gli Sliced-Wasserstein Autoencoder (SWAE), che sono modelli generativi che permettono di modellare la distribuzione dello spazio latente in qualsiasi distribuzione di probabilità campionabile senza la necessità di addestrare una rete avversaria o definire una forma chiusa per la distribuzione. In breve, regolarizziamo la perdita dell'autoencoder con la distanza di Wasserstein a fette tra la distribuzione dei campioni di allenamento codificati e una distribuzione campionabile predefinita. Mostriamo che la formulazione proposta ha una soluzione numerica efficiente che fornisce capacità simili agli Autoencoder di Wasserstein (WAE) e agli Autoencoder Variazionali (VAE), pur beneficiando di un'implementazione imbarazzantemente semplice. 
L'obiettivo dell'apprendimento per imitazione (IL) Ã¨ di imparare una buona politica dalle dimostrazioni di alta qualitÃ . Tuttavia, la qualitÃ delle dimostrazioni nella realtÃ puÃ² essere varia, poichÃ© Ã¨ piÃ¹ facile e piÃ¹ economico raccogliere le dimostrazioni da un misto di esperti e dilettanti. IL in tali situazioni puÃ² essere difficile, particolarmente quando il livello di competenza dei dimostratori Ã¨ sconosciuto. Mostriamo che un approccio di stima ingenuo non è adatto a grandi spazi di stato e di azione, e risolviamo questo problema utilizzando un approccio variazionale che può essere facilmente implementato utilizzando i metodi di apprendimento di rinforzo esistenti.Gli esperimenti sui benchmark a controllo continuo dimostrano che il VILD supera i metodi all'avanguardia.Il nostro lavoro consente un IL scalabile ed efficiente in termini di dati in impostazioni più realistiche di prima.
Con un numero crescente di servizi disponibili, ognuno dei quali ha parametri, precondizioni ed effetti leggermente diversi, la pianificazione automatizzata su servizi semantici generali diventa molto rilevante.Tuttavia, la maggior parte dei pianificatori esistenti considera solo PDDL, o se pretendono di usare OWL-S, di solito lo traducono in PDDL, perdendo gran parte della semantica lungo la strada. In questo articolo, proponiamo una nuova euristica indipendente dal dominio basata su una distanza semantica che può essere usata da algoritmi di pianificazione generici come A* per la pianificazione automatizzata di servizi semantici descritti con OWL-S. Per includere più informazioni rilevanti, calcoliamo l'euristica in fase di esecuzione e, usando questa euristica, siamo in grado di produrre risultati migliori (meno stati espansi) in meno tempo rispetto alle tecniche stabilite.
Nei grafi colorati, le classi dei nodi sono spesso associate o alla classe dei loro vicini o a informazioni non incorporate nel grafico associato a ciascun nodo.Noi qui proponiamo che le classi dei nodi siano anche associate alle caratteristiche topologiche dei nodi.Usiamo questa associazione per migliorare l'apprendimento automatico dei grafi in generale e in particolare, le reti convoluzionali dei grafi (GCN). In primo luogo, mostriamo che anche in assenza di qualsiasi informazione esterna sui nodi, è possibile ottenere una buona accuratezza sulla previsione della classe del nodo utilizzando sia le caratteristiche topologiche, sia utilizzando la classe dei vicini come input per una GCN. In secondo luogo, dimostriamo che l'aggiunta esplicita della topologia come input alla GCN non migliora l'accuratezza quando è combinata con informazioni esterne sui nodi. Tuttavia, l'aggiunta di una matrice di adiacenza aggiuntiva con bordi tra nodi distanti con topologia simile alla GCN migliora significativamente la sua accuratezza, portando a risultati migliori di tutti i metodi allo stato dell'arte in più dataset.
Mosche e topi sono specie separate da 600 milioni di anni di evoluzione, eppure hanno evoluto sistemi olfattivi che condividono molte somiglianze nella loro organizzazione anatomica e funzionale. Quali funzioni servono queste caratteristiche anatomiche e funzionali condivise, e sono ottimali per il rilevamento degli odori? Abbiamo scoperto che le reti neurali artificiali ricapitolano quantitativamente le strutture inerenti al sistema olfattivo, compresa la formazione di glomeruli su uno strato di compressione e la connettività rada e casuale su uno strato di espansione.
La recente direzione della traduzione immagine-immagine non accoppiata è da un lato molto eccitante in quanto allevia il grande onere di ottenere una supervisione pixel-pixel ad alta intensità di etichette, ma d'altra parte non è pienamente soddisfacente a causa della presenza di artefatti e trasformazioni degenerate. In questo articolo, prendiamo una visione molteplice del problema introducendo un termine di levigatezza sul grafico del campione per ottenere funzioni armoniche per imporre mappature coerenti durante la traduzione. Sviluppiamo HarmonicGAN per apprendere traduzioni bidirezionali tra il dominio di origine e quello di destinazione.Con l'aiuto della similarità-consistenza, la proprietà intrinseca di autoconsistenza dei campioni può essere mantenuta. In un'impostazione del problema identica a quella di CycleGAN, senza ulteriori input manuali e con un piccolo costo di training-time, HarmonicGAN dimostra un significativo miglioramento qualitativo e quantitativo rispetto allo stato dell'arte, così come una migliore interpretabilità. Mostriamo i risultati sperimentali in una serie di applicazioni tra cui l'imaging medico, la trasfigurazione di oggetti e l'etichettatura semantica. Superiamo i metodi concorrenti in tutti i compiti, e per un compito di imaging medico in particolare il nostro metodo trasforma CycleGAN da un fallimento a un successo, dimezzando l'errore quadratico medio, e generando immagini che i radiologi preferiscono rispetto ai metodi concorrenti nel 95% dei casi.
La stereoscopia multiview mira a ricostruire la profondità della scena da immagini acquisite da una telecamera in condizioni di movimento arbitrario.Metodi recenti affrontano questo problema attraverso l'apprendimento profondo, che può utilizzare spunti semantici per affrontare sfide come le regioni senza texture e riflettenti.In questo articolo, presentiamo una rete neurale convoluzionale chiamata DPSNet (Deep Plane Sweep Network) il cui design è ispirato alle migliori pratiche dei tradizionali approcci basati sulla geometria. Piuttosto che stimare direttamente la profondità e/o la corrispondenza del flusso ottico da coppie di immagini come fatto in molti metodi di apprendimento profondo precedenti, DPSNet adotta un approccio di sweep piano che comporta la costruzione di un volume di costo dalle caratteristiche profonde utilizzando l'algoritmo di sweep piano, regolarizzando il volume di costo tramite un'aggregazione di costo consapevole del contesto, e regredendo la mappa di profondità dal volume di costo. Il volume di costo è costruito utilizzando un processo di warping differenziabile che permette l'addestramento end-to-end della rete. Attraverso l'efficace incorporazione di concetti stereo multiview convenzionali all'interno di una struttura di apprendimento profondo, DPSNet raggiunge risultati di ricostruzione all'avanguardia su una varietà di set di dati impegnativi.
La capacità di progettare strutture biologiche come il DNA o le proteine avrebbe un notevole impatto medico e industriale.Fare così presenta un impegnativo problema di ottimizzazione black-box caratterizzato dal grande lotto, impostazione bassa rotonda a causa della necessità di valutazioni laboriose laboratorio umido.In risposta, proponiamo utilizzando l'apprendimento di rinforzo (RL) basato su ottimizzazione prossimale-policy (PPO) per la progettazione di sequenza biologica. RL fornisce una struttura flessibile per i modelli generativi di sequenza di ottimizzazione per realizzare i criteri specifici, quale la diversitÃ fra le sequenze di alta qualitÃ scoperte.Proponiamo una variante modello-basata di PPO, DyNA-PPO, per migliorare l'efficienza del campione, in cui la politica per un nuovo giro Ã¨ addestrata fuori linea usando una misura del simulatore sulle misure funzionali dai tondi precedenti.Per accomodare il numero crescente di osservazioni attraverso i tondi, il modello del simulatore Ã¨ selezionato automaticamente ad ogni giro da un gruppo dei modelli differenti di capacitÃ variabile.  Sui compiti di progettazione dei siti di legame del fattore di trascrizione del DNA, di progettazione delle proteine antimicrobiche e di ottimizzazione dell'energia dei modelli di Ising basati sulla struttura della proteina, troviamo che DyNA-PPO esegue significativamente meglio dei metodi esistenti nelle regolazioni in cui la modellazione è fattibile, mentre ancora non esegue peggio nelle situazioni in cui un modello affidabile non può essere imparato.
Raggiungere l'intelligenza della macchina richiede un'agevole integrazione di percezione e ragionamento, eppure i modelli sviluppati fino ad oggi tendono a specializzarsi in uno o nell'altro; la manipolazione sofisticata dei simboli acquisiti da ricchi spazi percettivi si è finora dimostrata inafferrabile.Consideriamo un compito aritmetico visivo, dove l'obiettivo è quello di eseguire semplici algoritmi aritmetici su cifre presentate in condizioni naturali (es. Il livello inferiore consiste in una collezione eterogenea di moduli di elaborazione delle informazioni, che possono includere reti neurali profonde pre-addestrate per localizzare ed estrarre i caratteri dall'immagine, così come moduli che eseguono trasformazioni simboliche sulle rappresentazioni estratte dalla percezione. Il livello superiore consiste in un controllore, addestrato usando l'apprendimento di rinforzo, che coordina i moduli per risolvere il compito di alto livello. Per esempio, il controllore può imparare in quali contesti eseguire le reti percettive e quali trasformazioni simboliche applicare alle loro uscite. Il modello risultante è in grado di risolvere una varietà di compiti nel dominio dell'aritmetica visiva, e ha diversi vantaggi rispetto alle reti feedforward standard, architettonicamente omogenee, compresa una migliore efficienza del campione.
Gli animali sviluppano nuove abilità non solo attraverso l'interazione con l'ambiente ma anche dall'influenza degli altri. In questo lavoro modelliamo l'influenza sociale nello schema di apprendimento di rinforzo, permettendo agli agenti di imparare sia dall'ambiente che dai loro pari. A differenza dei precedenti approcci di ottimizzazione congiunta precaria, la motivazione di unicità sociale nel nostro lavoro è imposta come un vincolo per incoraggiare l'agente ad imparare una politica diversa dagli agenti esistenti mentre ancora risolve il compito primario.L'algoritmo risultante, cioè Interior Policy Differentiation (IPD), porta un miglioramento delle prestazioni così come una collezione di politiche che risolvono un dato compito con comportamenti distinti
Le reti adversariali generative (GAN) sono state estremamente efficaci nell'approssimazione di distribuzioni complesse di campioni di dati di input ad alta densità, e sono stati fatti progressi sostanziali nella comprensione e nel miglioramento delle prestazioni GAN sia in termini di teoria che di applicazione. Tuttavia, attualmente non disponiamo di metodi quantitativi per la valutazione dei modelli; per questo motivo, mentre vengono proposte molte varianti di GAN, abbiamo una comprensione relativamente scarsa delle loro capacità relative. In questo documento, valutiamo le prestazioni di vari tipi di GAN utilizzando funzioni di divergenza e distanza tipicamente utilizzate solo per l'addestramento; osserviamo la coerenza tra le varie metriche proposte e, cosa interessante, le metriche di test-time non favoriscono le reti che utilizzano lo stesso criterio di training-time; confrontiamo inoltre le metriche proposte con i punteggi percettivi umani.
Le basi di conoscenza (KB) sono spesso rappresentate come una collezione di fatti nella forma (HEAD, PREDICATE, TAIL), dove HEAD e TAIL sono entità mentre PREDICATE è una relazione binaria che collega le due, (PARIGI, LATITUDINE, 48.8).Allo stesso modo, i fatti numerici soffrono anche del problema dell'incompletezza.Per affrontare questo problema, introduciamo il problema della predizione degli attributi numerici.Questo problema comporta un nuovo tipo di query in cui la relazione è un predicato numerico.Di conseguenza, e contrariamente alla predizione dei link, la risposta a questa query è un valore numerico. Sosteniamo che i valori numerici associati alle entità spiegano, in una certa misura, la struttura relazionale della base di conoscenza.Pertanto, sfruttiamo i metodi di incorporazione della base di conoscenza per imparare rappresentazioni che sono utili predittori per gli attributi numerici.Un'ampia serie di esperimenti su versioni di riferimento di FREEBASE e YAGO mostra che i nostri approcci superano ampiamente le linee di base sensibili.Rendiamo i set di dati disponibili sotto una licenza BSD-3 permissiva.
In particolare, dopo la nostra procedura di allineamento proposta, BERT esibisce prestazioni zero-shot significativamente migliorate su XNLI rispetto al modello di base, corrispondendo notevolmente ai modelli translate-train pseudo completamente supervisionati per il bulgaro e il greco. Inoltre, per misurare il grado di allineamento, introduciamo una versione contestuale del recupero delle parole e dimostriamo che si correla bene con il trasferimento a zero colpi a valle. Utilizzando questo compito di recupero delle parole, analizziamo anche BERT e scopriamo che presenta carenze sistematiche, ad esempio un allineamento peggiore per le parti di discorso di classe aperta e per le coppie di parole scritte in scritture diverse, che vengono corrette dalla procedura di allineamento.Questi risultati supportano l'allineamento contestuale come un concetto utile per comprendere grandi modelli multilingue pre-addestrati.
Le reti neurali hanno raggiunto prestazioni eccezionali per la risoluzione di vari problemi inversi mal posti nell'imaging. Tuttavia, gli svantaggi degli approcci di apprendimento end-to-end rispetto ai classici metodi variazionali sono il requisito di un costoso riaddestramento per dichiarazioni di problemi anche leggermente diversi e la mancanza di limiti di errore dimostrabili durante l'inferenza. Lavori recenti hanno affrontato il primo problema utilizzando le reti addestrate per il denoising delle immagini gaussiane come regolatori generici plug-and-play negli algoritmi di minimizzazione dell'energia. Anche se questo ottiene risultati all'avanguardia su molti compiti, devono essere fatte pesanti restrizioni sull'architettura della rete se la convergenza dimostrabile dell'iterazione del punto fisso sottostante è un requisito. Un lavoro più recente ha proposto di addestrare le reti a produrre direzioni di discesa rispetto a una data funzione energetica con una garanzia dimostrabile di convergenza a un minimizzatore di quell'energia.Tuttavia, ogni problema ed energia richiede l'addestramento di una rete separata.In questo articolo consideriamo la combinazione di entrambi gli approcci proiettando le uscite di una rete di denoising plug-and-play sul cono delle direzioni di discesa a una data energia.In questo modo, una singola rete pre-addestrata può essere usata per una grande varietà di compiti di ricostruzione.I nostri risultati mostrano miglioramenti rispetto ai metodi classici di minimizzazione dell'energia pur avendo garanzie di convergenza dimostrabili.
La ricerca sull'incorporazione dei grafi di conoscenza ha trascurato il problema della calibrazione della probabilità. Mostriamo che i modelli popolari di incorporazione sono effettivamente non calibrati, il che significa che le stime di probabilità associate alle triple previste sono inaffidabili. Presentiamo un nuovo metodo per calibrare un modello quando i negativi della verità a terra non sono disponibili, che è il caso usuale nei grafi di conoscenza. Gli esperimenti su tre insiemi di dati con verità di terra negativa mostrano che il nostro contributo porta a modelli ben calibrati rispetto al gold standard dell'uso dei negativi, ottenendo risultati significativamente migliori rispetto ai modelli non calibrati da tutti i metodi di calibrazione, mostrando che la regressione isotonica offre la migliore performance complessiva, non senza compromessi.
Più recentemente, assorbendo l'idea di strategie basate sull'estrazione mineraria, è stata adottata per enfatizzare i campioni mal classificati e ottenere risultati promettenti. Tuttavia, durante l'intero processo di formazione, i metodi precedenti o non enfatizzano esplicitamente il campione in base alla sua importanza, il che rende i campioni difficili non pienamente sfruttati, o enfatizzano esplicitamente gli effetti dei campioni semi-duri/duri anche nella fase iniziale di formazione, il che può portare a problemi di convergenza. In questo lavoro, proponiamo un nuovo Curriculum Adaptive Learning loss (CurricularFace) che incorpora l'idea del curriculum learning nella funzione di perdita per ottenere una nuova strategia di allenamento per il riconoscimento profondo dei volti, che si rivolge principalmente ai campioni facili nella fase iniziale di addestramento e a quelli difficili nella fase successiva. In particolare, il nostro CurricularFace regola in modo adattivo l'importanza relativa dei campioni facili e difficili durante le diverse fasi di addestramento. In ogni fase, ai diversi campioni viene assegnata un'importanza diversa in base alla loro corrispondente difficoltà. I risultati sperimentali estesi su benchmark popolari dimostrano la superiorità del nostro CurricularFace rispetto ai concorrenti allo stato dell'arte.
Gli esempi avversari sono input perturbati progettati per ingannare i modelli di apprendimento automatico.L'addestramento avversario inietta tali esempi nei dati di formazione per aumentare la robustezza.Per scalare questa tecnica a grandi serie di dati, le perturbazioni sono realizzate utilizzando metodi veloci a passo singolo che massimizzano un'approssimazione lineare della perdita del modello. Mostriamo che questa forma di formazione avversaria converge verso un minimo globale degenerato, in cui piccoli artefatti di curvatura vicino ai punti di dati offuscano un'approssimazione lineare della perdita.Il modello impara così a generare perturbazioni deboli, piuttosto che difendersi da quelle forti. Come risultato, troviamo che l'addestramento avversario rimane vulnerabile agli attacchi black-box, dove trasferiamo le perturbazioni calcolate su modelli non difesi, così come ad un potente attacco a passo singolo che sfugge alla vicinanza non liscia dei dati di input attraverso un piccolo passo casuale. Introduciamo inoltre l'Ensemble Adversarial Training, una tecnica che aumenta i dati di formazione con perturbazioni trasferite da altri modelli. Su ImageNet, l'Ensemble Adversarial Training produce modelli con una forte robustezza agli attacchi black-box. In particolare, il nostro modello più robusto ha vinto il primo round del concorso NIPS 2017 sulle difese contro gli attacchi avversari.
Un adversarial feature learning (AFL) è un potente framework per imparare rappresentazioni invarianti a un attributo fastidioso, che utilizza un gioco avversario tra un estrattore di caratteristiche e un classificatore di attributi categorici.suona teoricamente in termini di massimizzare l'entropia condizionale tra attributo e rappresentazione. Tuttavia, come mostrato in questo articolo, l'AFL spesso causa un comportamento instabile che rallenta la convergenza.Proponiamo un {\em attributo percezione corrispondenza} come un approccio alternativo, basato sulla riformulazione della massimizzazione dell'entropia condizionale come {em coppia-saggio distribuzione corrispondenza}. Anche se l'approccio ingenuo per realizzare la corrispondenza della distribuzione a coppie richiede un numero significativamente grande di parametri, il metodo proposto richiede lo stesso numero di parametri con AFL ma ha una migliore proprietà di convergenza.  
Studiamo le proprietà delle superfici di perdita comuni attraverso la loro matrice Hessiana.In particolare, nel contesto dell'apprendimento profondo, dimostriamo empiricamente che lo spettro della Hessiana è composto da due parti: (1) il bulk centrato vicino allo zero, (2) e outliers lontani dal bulk.Presentiamo prove numeriche e giustificazioni matematiche alle seguenti congetture esposte da Sagun et.al. (2016): Fissando i dati, aumentando il numero di parametri si scala semplicemente la massa dello spettro; fissando la dimensione e cambiando i dati (per esempio aggiungendo più cluster o rendendo i dati meno separabili) si influenzano solo gli outlier.Crediamo che le nostre osservazioni abbiano implicazioni sorprendenti per l'ottimizzazione non convessa in alte dimensioni. In primo luogo, la *piattezza* di tali paesaggi (che può essere misurata dalla singolarità dell'Hessiano) implica che le nozioni classiche di bacini di attrazione possono essere piuttosto fuorvianti e che la discussione sui bacini larghi/stretti può aver bisogno di una nuova prospettiva sulla sovra-parametrizzazione e sulla ridondanza che sono in grado di creare componenti connessi *grandi* nella parte inferiore del paesaggio. In secondo luogo, la dipendenza di un piccolo numero di grandi autovalori dalla distribuzione dei dati può essere collegata allo spettro della matrice di covarianza dei gradienti delle uscite del modello.Con questo in mente, possiamo rivalutare le connessioni all'interno della struttura dati-architettura-algoritmo di un modello, sperando che possa far luce sulla geometria degli spazi ad alta dimensione e non convessi nelle applicazioni moderne. In particolare, presentiamo un caso che collega le due osservazioni: la discesa a gradiente di piccoli e grandi lotti sembra convergere a diversi bacini di attrazione, ma dimostriamo che sono in realtà collegati attraverso la loro regione piatta e quindi appartengono allo stesso bacino.
L'allocazione delle risorse di calcolo nella spina dorsale è una questione cruciale nel rilevamento degli oggetti.Tuttavia, il modello di allocazione della classificazione è di solito adottato direttamente al rilevatore di oggetti, che si dimostra essere sub-ottimale.Al fine di riallocare le risorse di calcolo impegnate in modo più efficiente, presentiamo CR-NAS (Computation Reallocation Neural Architecture Search) che può imparare strategie di riallocazione del calcolo attraverso la risoluzione delle caratteristiche diverse e la posizione spaziale diectly sul dataset di rilevamento dell'obiettivo.Uno spazio di riallocazione a due livelli è proposto sia per la fase e la riallocazione spaziale. Una nuova procedura di ricerca gerarchica è adottata per far fronte al complesso spazio di ricerca.Applichiamo CR-NAS a più dorsali e otteniamo miglioramenti coerenti.I nostri CR-ResNet50 e CR-MobileNetV2 superano la baseline di 1,9% e 1,7% COCO AP rispettivamente senza alcun budget di calcolo aggiuntivo. I modelli scoperti da CR-NAS possono essere dotati di altri potenti collo/testa di rilevamento ed essere facilmente trasferiti ad altri dataset, ad esempio PASCAL VOC, e altri compiti di visione, ad esempio la segmentazione delle istanze.Il nostro CR-NAS può essere utilizzato come plugin per migliorare le prestazioni di varie reti, che è impegnativo.
L'incertezza è una caratteristica molto importante dell'intelligenza e aiuta il cervello a diventare un sistema intelligente flessibile, creativo e potente.I chip di calcolo neuromorfo basati su crossbar, in cui il calcolo viene eseguito principalmente da circuiti analogici, hanno l'incertezza e possono essere utilizzati per imitare il cervello.Tuttavia, la maggior parte delle attuali reti neurali profonde non hanno preso in considerazione l'incertezza del chip di calcolo neuromorfo. Pertanto, le loro prestazioni sui chip di calcolo neuromorfo non sono buone come sulle piattaforme originali (CPU/GPUs).In questo lavoro, abbiamo proposto lo schema di formazione di adattamento dell'incertezza (UATS) che racconta l'incertezza alla rete neurale nel processo di formazione.I risultati sperimentali mostrano che le reti neurali possono raggiungere prestazioni di inferenza comparabili sul chip di calcolo neuromorfo incerto rispetto ai risultati sulle piattaforme originali, e molto meglio delle prestazioni senza questo schema di formazione.
Una proprietà importante dei sistemi di classificazione delle immagini nel mondo reale è che entrambi classificano accuratamente gli oggetti dalle classi di destinazione (``sconosciuti'') e rifiutano in modo sicuro gli oggetti sconosciuti (``sconosciuti'') che appartengono a classi non presenti nei dati di formazione.Sfortunatamente, anche se la forte capacità di generalizzazione delle CNN esistenti assicura la loro accuratezza quando classificano oggetti conosciuti, fa anche sì che spesso assegnino uno sconosciuto a una classe di destinazione con alta fiducia. Di conseguenza, usare semplicemente le rilevazioni a bassa confidenza come un modo per rilevare gli sconosciuti non funziona bene.In questo lavoro, proponiamo una rete neurale profonda Unknown-aware (UDN in breve) per risolvere questo problema impegnativo.L'idea chiave di UDN è di migliorare le CNN esistenti per supportare un'operazione di prodotto che modella la relazione di prodotto tra le caratteristiche prodotte dagli strati convoluzionali. In questo modo, la mancanza di una singola caratteristica chiave di una classe target ridurrà notevolmente la probabilità di assegnare un oggetto a questa classe.UDN utilizza un insieme appreso di queste operazioni di prodotto, che gli permette di bilanciare i requisiti contraddittori di classificare accuratamente gli oggetti conosciuti e di rifiutare correttamente gli sconosciuti.Per migliorare ulteriormente le prestazioni di UDN nel rilevare gli sconosciuti, proponiamo una strategia di regolarizzazione teorica dell'informazione che incorpora l'obiettivo di rifiutare gli sconosciuti nel processo di apprendimento di UDN. Sperimentiamo su serie di dati di immagini di riferimento tra cui MNIST, CIFAR-10, CIFAR-100, e SVHN, aggiungendo incognite iniettando una serie di dati in un'altra. I nostri risultati dimostrano che UDN supera significativamente i metodi allo stato dell'arte nel rifiutare le incognite con 25 punti percentuali di miglioramento della precisione, pur conservando la precisione della classificazione.
Le serie temporali multivariate con valori mancanti sono comuni in aree come l'assistenza sanitaria e la finanza, e sono cresciute in numero e complessità nel corso degli anni.Questo solleva la questione se le metodologie di apprendimento profondo possono superare i metodi classici di imputazione dei dati in questo dominio.Tuttavia, le applicazioni ingenue di apprendimento profondo non riescono a dare stime di fiducia affidabili e mancano di interpretabilità.Noi proponiamo un nuovo modello profondo di variabile latente sequenziale per la riduzione della dimensionalità e l'imputazione dei dati. La riduzione non lineare della dimensionalità in presenza di dati mancanti è ottenuta utilizzando un approccio VAE con una nuova approssimazione variazionale strutturata. Dimostriamo che il nostro approccio supera diversi metodi di imputazione dei dati classici e basati sull'apprendimento profondo su dati ad alta dimensione provenienti dai domini della computer vision e della sanità, migliorando inoltre la fluidità delle imputazioni e fornendo stime di incertezza interpretabili.
In pratica si trova spesso che le grandi reti neurali iper-parametrizzate generalizzano meglio delle loro controparti più piccole, un'osservazione che sembra essere in conflitto con le nozioni classiche di complessità delle funzioni, che tipicamente favoriscono i modelli più piccoli. In questo lavoro, indaghiamo questa tensione tra complessità e generalizzazione attraverso una vasta esplorazione empirica di due metriche naturali di complessità legate alla sensibilità alle perturbazioni di input. Troviamo che le reti neurali addestrate sono più robuste alle perturbazioni di input in prossimità del manifold dei dati di allenamento, come misurato dal Jacoano di input-output della rete, e che questo si correla bene con la generalizzazione. Stabiliamo inoltre che i fattori associati a una scarsa generalizzazione - come l'addestramento full-batch o l'uso di etichette casuali - corrispondono a una maggiore sensibilità, mentre i fattori associati a una buona generalizzazione - come l'aumento dei dati e le non linearità di ReLU - danno luogo a funzioni più robuste.
I metodi profondi di apprendimento di rinforzo (RL) si impegnano generalmente nel comportamento esplorativo attraverso l'iniezione di rumore nello spazio di azione.Un'alternativa Ã¨ di aggiungere il rumore direttamente ai parametri dell'agente, che possono condurre ad esplorazione piÃ¹ costante e ad un insieme piÃ¹ ricco di comportamenti.Metodi quali le strategie evolutive usano le perturbazioni del parametro, ma scartano tutta la struttura temporale nel processo e richiedono significativamente piÃ¹ campioni. Combinando la perturbazione dei parametri con i metodi RL tradizionali è possibile combinare il meglio di entrambi i mondi.Dimostriamo che entrambi i metodi off e on-policy beneficiano di questo approccio attraverso il confronto sperimentale di DQN, DDPG e TRPO su ambienti d'azione discreti ad alta densità e compiti di controllo continui.
I modelli linguistici di reti neurali profonde pre-addestrate come ELMo, GPT, BERT e XLNet hanno recentemente raggiunto prestazioni all'avanguardia su una varietà di compiti di comprensione del linguaggio, ma la loro dimensione li rende poco pratici per un certo numero di scenari, soprattutto su dispositivi mobili e periferici. In particolare, la matrice di incorporazione delle parole di input rappresenta una parte significativa dell'impronta di memoria del modello, a causa del grande vocabolario di input e delle dimensioni di incorporazione. Le tecniche di distillazione della conoscenza hanno avuto successo nella compressione di grandi modelli di reti neurali, ma sono inefficaci nel produrre modelli di studenti con vocabolari diversi dai modelli originali dell'insegnante. Introduciamo una nuova tecnica di distillazione della conoscenza per l'addestramento di un modello di studente con un vocabolario significativamente più piccolo e con dimensioni di stato nascoste più basse. In particolare, impieghiamo un meccanismo di doppio addestramento che addestra il modello dell'insegnante e quello dello studente simultaneamente per ottenere le incorporazioni di parole ottimali per il vocabolario dello studente. Il nostro metodo è in grado di comprimere il modello BERT-BASE di oltre 60x, con solo un calo minore nella metrica del compito a valle, ottenendo un modello linguistico con un ingombro inferiore a 7MB. I risultati sperimentali dimostrano anche una maggiore efficienza di compressione e precisione rispetto ad altre tecniche di compressione all'avanguardia.
Il ragionamento umano implica il riconoscimento di principi di base comuni in molti esempi utilizzando le variabili; i sottoprodotti di tale ragionamento sono invarianti che catturano i modelli attraverso gli esempi, come "se qualcuno è andato da qualche parte, allora è lì" senza menzionare persone o luoghi specifici; gli esseri umani imparano cosa sono le variabili e come usarle in giovane età, e la domanda di questo articolo è se le macchine possono anche imparare e usare le variabili esclusivamente dagli esempi senza richiedere una pre-ingegnerizzazione umana. Proponiamo reti di unificazione che incorporano l'unificazione morbida nelle reti neurali per imparare le variabili e così facendo sollevare gli esempi in invarianti che possono poi essere utilizzati per risolvere un dato compito. valutiamo il nostro approccio su quattro set di dati per dimostrare che l'apprendimento delle invarianti cattura i modelli nei dati e può migliorare le prestazioni rispetto alle linee di base.
Ci rivolgiamo al caso in cui i campioni di addestramento della verità al suolo sono rari e il problema è gravemente mal posto - sia a causa della fisica sottostante sia perché possiamo ottenere solo poche misurazioni. Questa impostazione è comune nell'imaging geofisico e nel telerilevamento. Invece, proponiamo di imparare prima un insieme di mappature più semplici dai dati alle proiezioni dell'immagine sconosciuta in sottospazi casuali piecewise-constant, quindi combiniamo le proiezioni per formare una ricostruzione finale risolvendo un problema simile alla deconvoluzione e dimostriamo sperimentalmente che il metodo proposto è più robusto al rumore di misurazione e alle corruzioni non viste durante la formazione rispetto a un inverso imparato direttamente.
La progettazione di architetture per le reti neurali profonde richiede la conoscenza di esperti e un tempo di calcolo sostanziale.Proponiamo una tecnica per accelerare la selezione dell'architettura imparando una HyperNet ausiliaria che genera i pesi di un modello principale condizionato dall'architettura di quel modello.Confrontando le prestazioni di convalida relative delle reti con i pesi generati dalla HyperNet, possiamo efficacemente cercare su una vasta gamma di architetture al costo di una singola sessione di allenamento. Per facilitare questa ricerca, sviluppiamo un meccanismo flessibile basato sulla memoria read-writes che ci permette di definire un'ampia gamma di modelli di connettività di rete, con blocchi ResNet, DenseNet, e FractalNet come casi speciali.Convalidiamo il nostro metodo (SMASH) su CIFAR-10 e CIFAR-100, STL-10, ModelNet10, e Imagenet32x32, ottenendo prestazioni competitive con reti di dimensioni simili progettate a mano.
I dati di entailment testuale (o NLI) si sono dimostrati utili come dati di preaddestramento per compiti che richiedono la comprensione del linguaggio, anche quando si costruisce su un modello già preaddestrato come RoBERTa.Il protocollo standard per la raccolta di NLI non è stato progettato per la creazione di dati di preaddestramento, ed è probabilmente lontano dall'essere ideale per questo scopo.Con questa applicazione in mente proponiamo quattro protocolli alternativi, ciascuno volto a migliorare sia la facilità con cui gli annotatori possono produrre validi esempi di allenamento sia la qualità e diversità di tali esempi. Utilizzando queste alternative e una semplice MNLIbased baseline, raccogliamo e confrontiamo cinque nuovi set di allenamento di 9k esempi.I nostri risultati primari sono in gran parte negativi, con nessuno di questi nuovi metodi che mostrano grandi miglioramenti nel transfer learning.Tuttavia, facciamo diverse osservazioni che dovrebbero informare il lavoro futuro sui dati NLI, come l'uso di frasi seed fornite automaticamente per l'ispirazione migliora la qualità dei dati risultanti sulla maggior parte delle misure, e tutti gli interventi che abbiamo indagato riducono drasticamente i problemi precedentemente osservati con artefatti di annotazione.
Predire il futuro nel mondo reale, in particolare da osservazioni sensoriali grezze come le immagini, è eccezionalmente impegnativo: gli eventi del mondo reale possono essere stocastici e imprevedibili, e l'alta dimensionalità e complessità delle immagini naturali richiede che il modello predittivo costruisca un'intricata comprensione del mondo naturale.Molti metodi esistenti affrontano questo problema facendo ipotesi semplificative sull'ambiente. In questo articolo, sviluppiamo un metodo di predizione video variazionale stocastica (SV2P) che predice un diverso futuro possibile per ogni campione delle sue variabili latenti.Per quanto ne sappiamo, il nostro modello è il primo a fornire un'efficace predizione stocastica multi-frame per video del mondo reale. Dimostriamo la capacità del metodo proposto nel predire dettagliati fotogrammi futuri di video su più set di dati del mondo reale, sia senza azione che condizionati dall'azione. Troviamo che il nostro metodo proposto produce previsioni video sostanzialmente migliorate rispetto allo stesso modello senza stocasticità, e ad altri metodi di predizione video stocastici.
Nei sistemi multi-agente, i comportamenti complessi di interazione sorgono a causa delle correlazioni elevate tra gli agenti. Tuttavia, il lavoro precedente sulla modellazione delle interazioni multi-agente dalle dimostrazioni è principalmente limitato dall'assunzione dell'indipendenza tra le politiche e le loro strutture di ricompensa. In questo documento, abbiamo inserito il problema della modellazione delle interazioni multi-agente in una struttura di apprendimento per imitazione multi-agente con una modellazione esplicita delle politiche correlate approssimando le politiche degli avversari, che può recuperare le politiche degli agenti che possono rigenerare interazioni simili. Di conseguenza, sviluppiamo un algoritmo Decentralized Adversarial Imitation Learning con politiche correlate (CoDAIL), che permette l'addestramento e l'esecuzione decentralizzati.Vari esperimenti dimostrano che CoDAIL può rigenerare meglio le interazioni complesse vicine ai dimostratori e supera i metodi di apprendimento per imitazione multi-agente allo stato dell'arte.Il nostro codice è disponibile presso \url{https://github.com/apexrl/CoDAIL}.
Il plagio e il riutilizzo del testo sono diventati più disponibili con lo sviluppo di Internet, quindi è importante controllare i documenti scientifici per il fatto di imbrogliare, soprattutto in ambito accademico. I sistemi esistenti di rilevamento del plagio mostrano buone prestazioni e hanno un enorme database di origine, quindi ora non è sufficiente copiare il testo così com'è dal documento di origine per ottenere il lavoro originale, quindi un altro tipo di plagio è diventato popolare - plagio multilingue.
Il parallelismo dei dati è diventato un metodo dominante per scalare la formazione delle reti neurali profonde (DNN) su più nodi.  Poiché la sincronizzazione dei modelli locali o dei gradienti può essere un collo di bottiglia per la formazione distribuita su larga scala, la compressione del traffico di comunicazione ha guadagnato molta attenzione di recente.  Tra i vari algoritmi di compressione proposti di recente, la compressione del gradiente residuo (RGC) è uno degli approcci di maggior successo - può comprimere significativamente la dimensione del messaggio di trasmissione (0,1% della dimensione del gradiente) di ogni nodo e preservare ancora la precisione. Tuttavia, la letteratura sulla compressione delle reti profonde si concentra quasi esclusivamente sul raggiungimento di un buon tasso di compressione, mentre l'efficienza di RGC nell'implementazione reale è stata meno studiata.In questo articolo, sviluppiamo un metodo RGC che raggiunge un miglioramento significativo del tempo di formazione nei sistemi multi-GPU del mondo reale. La nostra proposta di sistema RGC, chiamata RedSync, introduce una serie di ottimizzazioni per ridurre la larghezza di banda di comunicazione introducendo un overhead limitato.Esaminiamo le prestazioni di RedSync su due diverse piattaforme multi GPU, tra cui un supercomputer e un server multi-card.I nostri test case includono la classificazione delle immagini su Cifar10 e ImageNet, e compiti di modellazione del linguaggio su Penn Treebank e Wiki2 datasets.Per DNNs caratterizzato da un elevato rapporto comunicazione/computazione, che è stato a lungo considerato con scarsa scalabilità, RedSync mostra un significativo miglioramento delle prestazioni.
Il backpropagation sta guidando le reti neurali artificiali di oggi, tuttavia, nonostante la ricerca estesa, non è ancora chiaro se il cervello implementa questo algoritmo. tra i neuroscienziati, gli algoritmi di apprendimento di rinforzo (RL) sono spesso visti come un'alternativa realistica, tuttavia, il tasso di convergenza di tale apprendimento scala male con il numero di neuroni coinvolti. Qui proponiamo un approccio di apprendimento ibrido, in cui ogni neurone usa una strategia di tipo RL per imparare come approssimare i gradienti che la backpropagation fornirebbe.Mostriamo che il nostro approccio impara ad approssimare il gradiente, e può eguagliare le prestazioni dell'apprendimento basato sul gradiente su reti completamente connesse e convoluzionali.Apprendere i pesi di feedback fornisce un meccanismo biologicamente plausibile per ottenere buone prestazioni, senza la necessità di regole di apprendimento precise e pre-specificate.
La maggior parte delle reti neurali profonde (DNN) richiedono modelli complessi per raggiungere alte prestazioni.La quantizzazione dei parametri è ampiamente utilizzata per ridurre le complessità di implementazione.Gli studi precedenti sulla quantizzazione erano per lo più basati su ampie simulazioni con dati di allenamento.Scegliamo un approccio diverso e cerchiamo di misurare la capacità per parametro dei modelli DNN e interpretiamo i risultati per ottenere intuizioni sulla quantizzazione ottimale dei parametri.Questa ricerca utilizza dati generati artificialmente e forme generiche di DNN completamente connesse, reti neurali convoluzionali e reti neurali ricorrenti. Il modello e le capacità per parametro sono valutati misurando l'informazione reciproca tra l'input e l'output classificato. estendiamo anche i risultati della misurazione della capacità di memorizzazione alla classificazione delle immagini e ai compiti di modellazione del linguaggio. per ottenere informazioni sulla quantizzazione dei parametri quando si eseguono compiti reali, si confrontano le prestazioni di allenamento e di test.
Ispirati dalla combinazione di calcoli feedforward e iterativi nella corteccia visiva, e sfruttando la capacità degli autocodificatori di denoising di stimare il punteggio di una distribuzione congiunta, proponiamo un nuovo approccio all'inferenza iterativa per catturare e sfruttare la complessa distribuzione congiunta delle variabili di uscita condizionata da alcune variabili di input. Questo approccio è applicato alla segmentazione pixel-wise dell'immagine, con il punteggio condizionale stimato utilizzato per eseguire la salita del gradiente verso un modo della distribuzione condizionale stimata. Questo estende il lavoro precedente sulla stima del punteggio da autoencoder denoising al caso di una distribuzione condizionale, con un nuovo uso di un predittore feedforward corrotto che sostituisce la corruzione gaussiana. Un vantaggio di questo approccio rispetto ai modi più classici di eseguire l'inferenza iterativa per le uscite strutturate, come i campi casuali condizionali (CRF), è che non è più necessario definire una funzione energetica esplicita che colleghi le variabili di uscita. Troviamo sperimentalmente che l'inferenza iterativa proposta dalla stima del punteggio condizionale da autoencoder di denoising condizionale esegue meglio di modelli comparabili basati su CRFs o quelli che non utilizzano alcuna modellazione esplicita della distribuzione congiunta condizionale delle uscite.
Ispirati dal successo del meccanismo di autoattenzione e dell'architettura Transformer nella trasduzione di sequenze e nelle applicazioni di generazione di immagini, proponiamo nuove architetture basate sull'autoattenzione per migliorare le prestazioni degli schemi basati su codici latenti avversari nella generazione di testo.Adversarial latent code-based text generationhas recentemente guadagnato molta attenzione a causa dei loro risultati promettenti.In questo articolo, facciamo un passo per fortificare le architetture utilizzate in queste configurazioni, in particolare AAEe ARAE. Nei nostri esperimenti, il dataset di compressione delle frasi di Google viene utilizzato per confrontare il nostro metodo con questi metodi utilizzando varie misure oggettive e soggettive. Gli esperimenti dimostrano che i modelli basati sull'attenzione (auto) proposti superano lo stato dell'arte nella generazione di testo basata su adversarialcode.
Alcuni lavori cercano di nascondere i trigger di attacco sui loro campioni avversari quando attaccano le reti neurali e altri vogliono filigranare le reti neurali per dimostrare la loro proprietà contro il plagio. Impiantare un modulo di filigrana backdoor in una rete neurale sta ottenendo più attenzione da parte della comunità.In questo documento, presentiamo un metodo di formazione congiunta encoder-decoder per scopi generali, ispirato alle reti generative avversarie (GANs).A differenza delle GANs, tuttavia, le nostre reti neurali encoder e decoder cooperano per trovare il miglior schema di watermarking dati i campioni di dati. In altre parole, non progettiamo nessuna nuova strategia di watermarking ma le nostre due reti neurali proposte troveranno il metodo più adatto da sole.Dopo essere stato addestrato, il decodificatore può essere impiantato in altre reti neurali per attaccarle o proteggerle (vedi Appendice per i loro casi d'uso e le implementazioni reali). A tal fine, il decodificatore dovrebbe essere molto piccolo in modo da non incorrere in alcun overhead quando attaccato ad altre reti neurali, ma allo stesso tempo fornire tassi di successo di decodifica molto elevati, il che è molto impegnativo.Il nostro metodo di formazione congiunta risolve con successo il problema e nei nostri esperimenti mantenere quasi il 100% di codifica-decodifica tassi di successo per più set di dati con modifiche molto poco sui campioni di dati per nascondere watermark.Presentiamo anche diversi casi di utilizzo del mondo reale in Appendice.
Deriviamo uno stimatore imparziale per le aspettative su variabili casuali discrete basate sul campionamento senza sostituzione, che riduce la varianza in quanto evita i campioni duplicati.Mostriamo che il nostro stimatore può essere derivato come la Rao-Blackwellizzazione di tre stimatori diversi.Combinando il nostro stimatore con REINFORCE, otteniamo uno stimatore a gradiente politico e riduciamo la sua varianza utilizzando una varianza di controllo integrata che si ottiene senza valutazioni aggiuntive del modello. Gli esperimenti con un problema giocattolo, un Variational Auto-Encoder categorico e un problema di predizione strutturato mostrano che il nostro stimatore è l'unico stimatore che è costantemente tra i migliori stimatori in entrambe le impostazioni ad alta e bassa entropia.
Introduciamo uno schema di condivisione dei parametri, in cui diversi strati di una rete neurale convoluzionale (CNN) sono definiti da una combinazione lineare di tensori di parametri appresi da una banca globale di modelli.  Limitando il numero di modelli si ottiene un'ibridazione flessibile delle CNN tradizionali e delle reti ricorrenti.  Rispetto alle CNN tradizionali, dimostriamo un sostanziale risparmio di parametri su compiti standard di classificazione delle immagini, mantenendo l'accuratezza. Il nostro semplice schema di condivisione dei parametri, anche se definito tramite pesi morbidi, in pratica spesso produce reti addestrate con una struttura ricorrente quasi rigida; con effetti collaterali trascurabili, si convertono in reti con cicli effettivi. L'addestramento di queste reti implica quindi implicitamente la scoperta di adeguate architetture ricorrenti; pur considerando solo l'aspetto dei collegamenti ricorrenti, le nostre reti addestrate raggiungono un'accuratezza competitiva con quelle costruite utilizzando le procedure di ricerca dell'architettura neurale (NAS) allo stato dell'arte.La nostra ibridazione di reti ricorrenti e convoluzionali può anche rappresentare un vantaggio architettonico.  In particolare, su compiti sintetici che sono di natura algoritmica, le nostre reti ibride si allenano più velocemente ed estrapolano meglio gli esempi di test al di fuori dell'ambito del set di allenamento.
Il gradient clipping è una tecnica ampiamente utilizzata nell'addestramento delle reti profonde, ed è generalmente motivata da una lente di ottimizzazione: informalmente, controlla la dinamica degli iterati, migliorando così il tasso di convergenza verso un minimo locale. Questa intuizione è stata precisata in una serie di lavori recenti, che dimostrano che il clipping adeguato può produrre una convergenza significativamente più veloce rispetto alla discesa del gradiente vaniglia.In questo articolo, proponiamo una nuova lente per studiare il clipping del gradiente, cioè la robustezza: informalmente, ci si aspetta che il clipping fornisca robustezza al rumore, poiché non ci si fida eccessivamente di nessun singolo campione. Sorprendentemente, dimostriamo che per il problema comune del rumore delle etichette nella classificazione, il clipping a gradiente standard non fornisce in generale robustezza; d'altra parte, dimostriamo che una semplice variante del clipping a gradiente è provatamente robusta, e corrisponde ad una opportuna modifica della funzione di perdita sottostante; questo produce una semplice alternativa robusta al rumore alla perdita standard di cross-entropia che si comporta bene empiricamente.
 Tra i modelli generativi profondi, i modelli basati sui flussi, chiamati semplicemente flussi in questo articolo, differiscono dagli altri modelli in quanto forniscono una verosimiglianza trattabile. Oltre ad essere una metrica di valutazione dei dati sintetizzati, si suppone che i flussi siano robusti contro gli input out-of-distribution~(OoD) in quanto non scartano alcuna informazione degli input. Tuttavia, è stato osservato che i flussi addestrati su FashionMNIST assegnano maggiori probabilità ai campioni OoD da MNIST.Questa osservazione controintuitiva solleva la preoccupazione sulla robustezza della probabilità dei flussi.In questo articolo, esploriamo la correlazione tra la probabilità dei flussi e la semantica delle immagini.Scegliamo due flussi tipici come modelli target: I nostri esperimenti rivelano una correlazione sorprendentemente debole tra la verosimiglianza dei flussi e la semantica dell'immagine: la verosimiglianza predittiva dei flussi può essere pesantemente influenzata da trasformazioni banali che mantengono invariata la semantica dell'immagine, che noi chiamiamo trasformazioni semantico-invarianti~(SITs). Esploriamo tre SITs~(tutte piccole modifiche a livello di pixel): traduzione dei pixel dell'immagine, perturbazione casuale del rumore, azzeramento dei fattori latenti~(limitato ai flussi che utilizzano un'architettura multiscala, ad es. Questi risultati, anche se controintuitivi, risuonano con il fatto che la probabilità predittiva di un flusso è la probabilità congiunta di tutti i pixel dell'immagine, quindi le probabilità dei flussi, modellando sulle intensità a livello di pixel, non è in grado di indicare la probabilità di esistenza della semantica dell'immagine di alto livello.
Questo articolo indaga le strategie che difendono contro gli attacchi adversarial-example sui sistemi di classificazione delle immagini trasformando gli input prima di alimentarli al sistema. In particolare, studiamo l'applicazione di trasformazioni dell'immagine come la riduzione della profondità di bit, la compressione JPEG, la minimizzazione della varianza totale e il quilting dell'immagine prima di alimentare l'immagine a un classificatore di rete convoluzionale. I nostri esperimenti su ImageNet mostrano che la minimizzazione della varianza totale e la trapuntatura dell'immagine sono difese molto efficaci in pratica, in particolare, quando la rete viene addestrata su immagini trasformate.La forza di queste difese risiede nella loro natura non differenziabile e nella loro casualità intrinseca, che rende difficile per un avversario aggirare le difese.La nostra migliore difesa elimina il 60% dei forti attacchi gray-box e il 90% dei forti attacchi black-box da una varietà di metodi di attacco principali.
	In questo articolo, proponiamo l'algoritmo Asynchronous Accelerated Nonuniform Randomized Block Coordinate Descent (A2BCD).dimostriamo che A2BCD converge linearmente a una soluzione del problema di minimizzazione convessa alla stessa velocità di NU_ACDM, a patto che il ritardo massimo non sia troppo grande.questo è il primo algoritmo asincrono accelerato da Nesterov che raggiunge una velocità dimostrabile.inoltre, dimostriamo che questi algoritmi hanno entrambi una complessità ottimale. Gli algoritmi asincroni completano iterazioni molto più veloci, e A2BCD ha una complessità ottimale. Quindi osserviamo negli esperimenti che A2BCD è l'algoritmo di discesa delle coordinate più performante, convergendo fino a 4-5 volte più velocemente di NU_ACDM su alcuni set di dati in termini di tempo wall-clock.Per motivare la nostra teoria e le tecniche di prova, deriviamo e analizziamo anche un analogo a tempo continuo del nostro algoritmo e dimostriamo che converge alla stessa velocità.
Una struttura per l'inferenza bayesiana efficiente nei programmi probabilistici è introdotta incorporando un campionatore all'interno di un'approssimazione posteriore variazionale. La sua forza risiede sia nella facilità di implementazione che nella sintonizzazione automatica dei parametri del campionatore per accelerare il tempo di miscelazione.Sono introdotte diverse strategie per approssimare il calcolo del limite inferiore delle prove (ELBO), compresa una riscrittura dell'obiettivo ELBO. Le prove sperimentali sono mostrate eseguendo esperimenti su un VAE incondizionato su compiti di stima della densità; risolvendo un diagramma di influenza in uno spazio ad alta densità con un autoencoder variazionale condizionale (cVAE) come un classificatore di Bayes profondo; e modelli di stato-spazio per dati di serie temporali.
Le stime puntuali delle reti di classificazione ReLU, probabilmente l'architettura di rete neurale più utilizzata, hanno recentemente dimostrato di avere una confidenza arbitrariamente alta lontano dai dati di formazione, L'analisi teorica di tali approssimazioni bayesiane è limitata, anche per le reti di classificazione ReLU. Presentiamo un'analisi delle distribuzioni posteriori gaussiane approssimate sui pesi delle reti ReLU. Mostriamo che anche una distribuzione gaussiana semplicistica (quindi economica) e non bayesiana risolve il problema asintotico della sovrafiducia.Inoltre, quando un metodo bayesiano, anche se semplice, viene impiegato per ottenere la gaussiana, la fiducia diventa meglio calibrata.Questo risultato teorico motiva una serie di approssimazioni di Laplace lungo un trade-off fedeltà-costo.Convalidiamo empiricamente questi risultati attraverso esperimenti utilizzando comuni reti ReLU profonde.
Gli allineamenti di parole sono utili per compiti come la traduzione automatica statistica e neurale (NMT) e la proiezione delle annotazioni.Gli allineatori statistici di parole funzionano bene, così come i metodi che estraggono gli allineamenti insieme alle traduzioni in NMT.Tuttavia, la maggior parte degli approcci richiede dati di allenamento paralleli e la qualità diminuisce quando sono disponibili meno dati di allenamento.Noi proponiamo metodi di allineamento di parole che richiedono pochi o nessun dato parallelo.L'idea chiave è di sfruttare le embeddings di parole multilingue - sia statiche che contestualizzate - per l'allineamento di parole. I nostri embeddings multilingue sono creati solo da dati monolingue senza basarsi su dati paralleli o dizionari. Troviamo che gli allineatori statistici tradizionali sono superati dagli embeddings contestualizzati - anche in scenari con abbondanti dati paralleli. Per esempio, per un set di 100k frasi parallele, gli embeddings contestualizzati raggiungono un allineamento di parole F1 che è più del 5% più alto (assoluto) di eflomal.
Studi recenti hanno dimostrato la vulnerabilità dei modelli di apprendimento per rinforzo (RL) in ambienti rumorosi.Le fonti di rumore differiscono tra gli scenari.Per esempio, in pratica, il canale di ricompensa osservato è spesso soggetto a rumore (ad es, Inoltre, in applicazioni come la robotica, un algoritmo di deep reinforcement learning (DRL) può essere manipolato per produrre errori arbitrari.In questo articolo, consideriamo problemi RL rumorosi in cui le ricompense osservate dagli agenti RL sono generate con una matrice di confusione di ricompensa.Chiamiamo tali ricompense osservate come ricompense perturbate.Sviluppiamo uno stimatore di ricompensa imparziale aiutato da un quadro RL robusto che consente agli agenti RL di imparare in ambienti rumorosi mentre osservano solo le ricompense perturbate. Le idee centrali della nostra soluzione includono la stima di una matrice di confusione delle ricompense e la definizione di un insieme di ricompense surrogate imparziali.dimostriamo la convergenza e la complessità del campione del nostro approccio.esperimenti estesi su diverse piattaforme DRL mostrano che le politiche basate sulla nostra stima delle ricompense surrogate possono ottenere ricompense attese più alte e convergere più velocemente delle linee di base esistenti.per esempio, l'algoritmo PPO all'avanguardia è in grado di ottenere il 67,5% e il 46,7% di miglioramenti in media su cinque giochi Atari, quando i tassi di errore sono rispettivamente del 10% e del 30%.
L'addestramento delle reti neurali ricorrenti (RNN) su lunghe sequenze utilizzando la backpropagation through time (BPTT) rimane una sfida fondamentale. È stato dimostrato che l'aggiunta di un termine di perdita locale non supervisionato nell'obiettivo di ottimizzazione rende l'addestramento delle RNN su lunghe sequenze più efficace. Mentre l'importanza di un compito non supervisionato può in linea di principio essere controllata da un coefficiente nella funzione obiettivo, i gradienti rispetto al termine di perdita non supervisionato influenzano ancora tutte le dimensioni dello stato nascosto, il che potrebbe causare la degradazione o la cancellazione di informazioni importanti sul compito supervisionato. Rispetto ai metodi esistenti di apprendimento semi-supervisionato delle sequenze, questo documento si concentra su un meccanismo tradizionalmente trascurato - un'architettura con unità nascoste private e condivise esplicitamente progettate per mitigare l'influenza dannosa della perdita ausiliaria non supervisionata sul compito principale supervisionato. Presentiamo ampi esperimenti con la struttura proposta su diversi set di dati di riferimento per la modellazione di lunghe sequenze. I risultati indicano che la struttura proposta può produrre guadagni di prestazioni in modelli RNN in cui le dipendenze a lungo termine sono notoriamente difficili da gestire.
Per comprendere meglio il trasferimento delle informazioni sulle attività, studiamo un'architettura con un modulo condiviso per tutte le attività e un modulo di uscita separato per ogni attività. studiamo la teoria di questa impostazione su modelli lineari e attivati da ReLU. la nostra osservazione chiave è che se i dati delle attività sono ben allineati o meno può influenzare significativamente le prestazioni dell'apprendimento multi-task. Ispirandoci alle intuizioni teoriche, dimostriamo che l'allineamento degli strati di incorporamento delle attività porta a guadagni di prestazioni per l'addestramento multi-task e l'apprendimento di trasferimento sul benchmark GLUE e sulle attività di analisi del sentimento; per esempio, abbiamo ottenuto un miglioramento medio del punteggio GLUE del 2,35% su 5 attività GLUE rispetto a BERT LARGE utilizzando il nostro metodo di allineamento.
Esaminiamo tre limitazioni di BLEU e ROUGE - le metriche più popolari utilizzate per valutare i riassunti di riferimento rispetto ai riassunti di ipotesi, proponiamo criteri su come dovrebbe comportarsi una buona metrica e proponiamo modi concreti per valutare le prestazioni di una metrica in dettaglio e mostriamo il potenziale dei modelli linguistici basati su Transformers per valutare i riassunti di riferimento rispetto ai riassunti di ipotesi.
In questo articolo, esploriamo il meta-apprendimento per la classificazione del testo in pochi scatti. Il meta-apprendimento ha mostrato una forte performance nella computer vision, dove i modelli di basso livello sono trasferibili tra i compiti di apprendimento. Tuttavia, applicare direttamente questo approccio al testo è una sfida: le caratteristiche lessicali altamente informative per un compito possono essere insignificanti per un altro. Il nostro modello è addestrato all'interno di un quadro di meta-apprendimento per mappare queste firme in punteggi di attenzione, che vengono poi utilizzati per ponderare le rappresentazioni lessicali delle parole.dimostriamo che il nostro modello supera costantemente le reti prototipiche apprese sulla conoscenza lessicale (Snell et al., 2017) sia nella classificazione del testo a pochi scatti che nella classificazione delle relazioni con un margine significativo su sei set di dati di riferimento (19,96% in media nella classificazione a 1 scatto).
La descrizione delle computazioni neurali nel campo delle neuroscienze si basa su due visioni concorrenti:(i) una classica visione a singola cellula che mette in relazione l'attività dei singoli neuroni con variabili sensoriali o comportamentali, e si concentra su come le diverse classi di cellule mappano le computazioni;(ii) una più recente visione di popolazione che invece caratterizza le computazioni in termini di traiettorie neurali collettive, e si concentra sulla dimensionalità di queste traiettorie mentre gli animali svolgono compiti. Come i due concetti chiave delle classi di cellule e delle traiettorie a bassa dimensionalità interagiscano per dare forma alle computazioni neurali non è tuttavia attualmente compreso. Qui affrontiamo questa domanda combinando strumenti di apprendimento automatico per l'addestramento di RNN con analisi di reverse-engineering e teoriche delle dinamiche di rete. In queste reti, il rango della connettività controlla la dimensionalità della dinamica, mentre il numero di componenti nella miscela gaussiana corrisponde al numero di classi di cellule. Utilizzando la back-propagation, determiniamo il rango minimo e il numero di classi di cellule necessari per implementare compiti di neuroscienza di crescente complessità. Mostriamo che il rango determina lo spazio di fase disponibile per le dinamiche che implementano le mappature input-output, mentre avere più classi cellulari permette alle reti di passare in modo flessibile tra diversi tipi di dinamiche nello spazio di fase disponibile.I nostri risultati hanno implicazioni per l'analisi degli esperimenti di neuroscienze e lo sviluppo di IA spiegabile.
Proponiamo il discriminatore di fusione, una singola struttura unificata per incorporare informazioni condizionali in una rete generativa avversaria (GAN) per una varietà di compiti di previsione strutturata distinti, tra cui la sintesi di immagini, la segmentazione semantica e la stima della profondità. Molto simile ai modelli di rete neurale convoluzionale - campo casuale di Markov condizionale (CNN-CRF) comunemente usati, il metodo proposto è in grado di imporre la coerenza di ordine superiore nel modello, ma senza essere limitato a una classe molto specifica di potenziali.Il metodo è concettualmente semplice e flessibile, e i nostri risultati sperimentali dimostrano un miglioramento su diversi compiti di previsione strutturata.
L'apprendimento di rinforzo basato sul modello (MBRL) ha dimostrato di essere una struttura potente per il controllo di apprendimento efficiente dei dati di compiti continui.Il lavoro recente in MBRL si è concentrato principalmente sull'utilizzo di approssimazioni di funzioni più avanzate e schemi di pianificazione, lasciando la struttura generale praticamente invariata dal suo concepimento.In questo articolo, identifichiamo un problema fondamentale della struttura standard MBRL - quello che chiamiamo il problema di mismatch obiettivo. Nel contesto di MBRL, caratterizziamo il mismatch dell'obiettivo tra l'addestramento del modello dinamico in avanti con la probabilità della previsione di un passo avanti, e l'obiettivo generale di migliorare le prestazioni su un compito di controllo a valle. Per esempio, questo problema può emergere con la realizzazione che i modelli dinamici efficaci per un compito specifico non devono necessariamente essere accurati a livello globale, e viceversa i modelli accurati a livello globale potrebbero non essere sufficientemente accurati a livello locale per ottenere buone prestazioni di controllo su un compito specifico.Nei nostri esperimenti, studiamo questo problema di mismatch obiettivo e dimostriamo che la probabilità della previsione di un passo avanti non è sempre correlata alle prestazioni di controllo a valle. Questa osservazione evidenzia un difetto critico nell'attuale struttura MBRL che richiederà ulteriori ricerche per essere pienamente compreso e affrontato.Proponiamo un metodo iniziale per mitigare il problema del mismatch riponderando la formazione del modello dinamico.Basandoci su di esso, concludiamo con una discussione su altre potenziali direzioni della ricerca futura per affrontare questo problema.
Recentemente c'è stato un acceso dibattito (ad esempio Schwartz-Ziv & Tishby (2017), Saxe et al. (2018), Noshad et al. (2018), Goldfeld et al. (2018)) sulla misurazione del flusso di informazioni nelle Reti Neurali Profonde usando tecniche della teoria dell'informazione. Si sostiene che le Reti Neurali Profonde in generale hanno buone capacità di generalizzazione poiché non solo imparano a mappare da un input a un output, ma anche a comprimere le informazioni sull'input dei dati di allenamento (Schwartz-Ziv & Tishby, 2017).Cioè, astraggono le informazioni di input e spogliano qualsiasi informazione non necessaria o troppo specifica. Se così fosse, il metodo di compressione dei messaggi, Information Bottleneck (IB), potrebbe essere utilizzato come un comparatore naturale per le prestazioni della rete, dal momento che questo metodo dà un limite ottimale di compressione delle informazioni.Questa affermazione è stata poi successivamente denunciata così come riaffermata (ad esempio Saxe et al. (2018), Achille et al. (2017), Noshad et al. (2018)), poiché il metodo impiegato per misurare l'informazione reciproca non sta in realtà misurando l'informazione ma il raggruppamento delle rappresentazioni degli strati interni (Goldfeld et al. (2018)).In questo articolo, presenteremo una spiegazione dettagliata dello sviluppo nella Pianura delle Informazioni (IP), che è una trama-tipo che compara l'informazione reciproca per giudicare la compressione (Schwartz-Ziv & Tishby (2017)), quando il rumore viene aggiunto retroattivamente (utilizzando la stima del binning).  Abbiamo anche spiegato perché diverse funzioni di attivazione mostrano traiettorie diverse sull'IP.Inoltre, abbiamo esaminato l'effetto del clustering sulla perdita della rete attraverso l'arresto precoce e perfetto utilizzando il piano di informazione e come il clustering può essere utilizzato per aiutare il pruning della rete.
La comprensione formale del bias induttivo dietro le reti convoluzionali profonde, cioè la relazione tra le caratteristiche architetturali della rete e le funzioni che è in grado di modellare, è limitata. In questo lavoro, stabiliamo una connessione fondamentale tra i campi della fisica quantistica e dell'apprendimento profondo, e la usiamo per ottenere nuove osservazioni teoriche riguardanti il bias induttivo delle reti convoluzionali. In particolare, mostriamo un'equivalenza strutturale tra la funzione realizzata da un circuito aritmetico convoluzionale (ConvAC) e una funzione d'onda quantistica a molti corpi, che facilita l'uso di misure di entanglement quantistico come quantificatori della capacità espressiva di una rete profonda di modellare le correlazioni. Inoltre, la costruzione di una ConvAC profonda in termini di una rete quantistica Tensor Network ci permette di eseguire un'analisi teorica del grafico di una rete convoluzionale, legando la sua espressività a un min-cut nel suo grafico sottostante e dimostrando un risultato pratico nella forma di un controllo diretto sul bias induttivo attraverso il numero di canali (larghezza) di ogni strato. Convalidiamo empiricamente i nostri risultati su reti convoluzionali standard che coinvolgono attivazioni ReLU e max pooling.La descrizione di una rete convoluzionale profonda in strumenti grafici teorici ben definiti e la connessione strutturale all'entanglement quantistico, sono due ponti interdisciplinari che vengono portati avanti da questo lavoro.
Gli algoritmi di apprendimento profondo sono sempre più utilizzati nella modellazione dei processi chimici. Tuttavia, le previsioni della scatola nera senza razionali hanno limitato l'uso in applicazioni pratiche, come la progettazione della droga, Formuliamo questo problema come un problema di apprendimento di rinforzo sul grafo molecolare, parametrizzato da due reti di convoluzione che corrispondono alla selezione e alla previsione della logica basata su di esso, dove quest'ultimo induce la funzione di ricompensa. Valutiamo l'approccio su due set di dati di tossicità di riferimento e dimostriamo che il nostro modello sostiene alte prestazioni sotto il vincolo aggiuntivo che le previsioni seguono rigorosamente i razionali. Inoltre, convalidiamo i razionali estratti attraverso il confronto con quelli descritti nella letteratura chimica e attraverso esperimenti sintetici.
In questo articolo, per la prima volta, analizziamo teoricamente le connessioni tra GCN e matrix factorization (MF), e unifichiamo GCN come matrix factorization con co-training e unitization.inoltre, sotto la guida di questa analisi teorica, proponiamo un modello alternativo a GCN chiamato Co-training and Unitized Matrix Factorization (CUMF).la correttezza della nostra analisi è verificata da esperimenti approfonditi. I risultati sperimentali mostrano che CUMF raggiunge prestazioni simili o superiori rispetto a GCN.Inoltre, CUMF eredita i vantaggi dei metodi basati su MF per supportare naturalmente la costruzione di mini-batch, ed è più amichevole per il calcolo distribuito rispetto a GCN.Il CUMF distribuito sulla classificazione semi-supervisionata dei nodi supera significativamente i metodi GCN distribuiti.Quindi, CUMF beneficia notevolmente le applicazioni su larga scala e complesse del mondo reale.
La generazione di grafi molecolari è un problema fondamentale per la scoperta di farmaci e sta attirando una crescente attenzione.Il problema è impegnativo in quanto richiede non solo la generazione di strutture molecolari chimicamente valide ma anche l'ottimizzazione delle loro proprietà chimiche nel frattempo.Ispirato dai recenti progressi nei modelli generativi profondi, in questo documento proponiamo un modello autoregressivo basato sul flusso per la generazione di grafi chiamato GraphAF.GraphAF combina i vantaggi di entrambi gli approcci autoregressivi e basati sul flusso e gode di: (I risultati sperimentali mostrano che GraphAF è in grado di generare il 68% di molecole chimicamente valide anche senza regole di conoscenza chimica e il 100% di molecole valide con regole chimiche. Il processo di addestramento di GraphAF è due volte più veloce dell'attuale approccio GCPN.Dopo aver messo a punto il modello per l'ottimizzazione delle proprietà orientata all'obiettivo con il reinforcement learning, GraphAF raggiunge prestazioni all'avanguardia sia nell'ottimizzazione delle proprietà chimiche che nell'ottimizzazione delle proprietà vincolate.
Studiamo due tipi di precondizionatori e metodi di discesa del gradiente stocastico (SGD) precondizionati in un quadro unificato. Chiamiamo il primo tipo Newton a causa della sua stretta relazione con il metodo Newton, e il secondo tipo Fisher in quanto il suo precondizionatore è strettamente legato all'inverso della matrice di informazione di Fisher.Entrambi i precondizionatori possono essere derivati da un quadro, e stimati in modo efficiente su qualsiasi matrice Lie gruppi designati dall'utente utilizzando la discesa del gradiente naturale o relativo che minimizza certi criteri di stima del precondizionatore.Molti precondizionatori e metodi esistenti, ad es, RMSProp, Adam, KFAC, SGD equilibrato, normalizzazione batch, ecc., sono casi speciali o strettamente legati al tipo Newton o al tipo Fisher.
Presentiamo EDA: semplici tecniche di aumento dei dati per aumentare le prestazioni nei compiti di classificazione del testo.EDA consiste in quattro semplici ma potenti operazioni: sostituzione dei sinonimi, inserimento casuale, scambio casuale e cancellazione casuale.Su cinque compiti di classificazione del testo, dimostriamo che EDA migliora le prestazioni sia per le reti neurali convoluzionali che ricorrenti. EDA dimostra risultati particolarmente forti per i set di dati più piccoli; in media, attraverso cinque set di dati, l'addestramento con EDA mentre si utilizza solo il 50% del set di allenamento disponibile ha raggiunto la stessa precisione dell'addestramento normale con tutti i dati disponibili.Abbiamo anche eseguito ampi studi di ablazione e suggeriamo parametri per l'uso pratico.
Proponiamo un modello che è in grado di eseguire la stima dei parametri fisici dei sistemi dal video, dove le equazioni differenziali che governano la dinamica della scena sono note, ma gli stati o gli oggetti etichettati non sono disponibili.I metodi esistenti di comprensione della scena fisica richiedono la supervisione dello stato dell'oggetto, o non si integrano con la fisica differenziabile per imparare parametri di sistema interpretabili e stati.Affrontiamo questo problema attraverso un approccio \textit{physics-as-inverse-graphics} che riunisce la visione come grafica inversa e motori fisici differenziabili, dove gli oggetti e le rappresentazioni esplicite di stato e velocità sono scoperti dal modello. Il nostro approccio supera significativamente i metodi non supervisionati correlati nella previsione a lungo termine dei frame futuri di sistemi con oggetti interagenti (come i sistemi gravitazionali a sfera o a 3 corpi), grazie alla sua capacità di costruire dinamiche nel modello come un bias induttivo. Dimostriamo inoltre il valore di questa stretta integrazione visione-fisica dimostrando un apprendimento efficiente dei dati del controllo basato sul modello azionato dalla visione per un sistema a pendolo e che l'interpretabilità del controller fornisce capacità uniche nel controllo guidato dagli obiettivi e nel ragionamento fisico per l'adattamento a zero dati.
Questo articolo propone ASAL, un nuovo metodo di apprendimento attivo basato su pool che genera campioni ad alta entropia. Invece di annotare direttamente i campioni sintetici, ASAL cerca campioni simili dal pool e li include per la formazione.  ASAL è particolarmente adatto per grandi insiemi di dati perché raggiunge una migliore complessità di esecuzione (sub-lineare) per la selezione dei campioni rispetto al tradizionale campionamento dell'incertezza (lineare). Presentiamo una serie completa di esperimenti su due insiemi di dati e dimostriamo che ASAL supera i metodi simili e supera chiaramente la linea di base stabilita (campionamento casuale).  Nella sezione di discussione analizziamo in quali situazioni ASAL si comporta meglio e perché a volte è difficile superare la selezione casuale del campione.Per quanto ne sappiamo questa è la prima tecnica di apprendimento attivo avversaria che viene applicata per problemi di classi multiple utilizzando classificatori convoluzionali profondi e dimostra prestazioni superiori alla selezione casuale del campione.
Ogni volta che un modello viene addestrato, si ottiene un risultato diverso a causa di fattori casuali nel processo di formazione, che includono l'inizializzazione casuale dei parametri e il rimescolamento casuale dei dati.
Un approccio popolare a questo problema è quello di imparare embeddings invarianti per entrambi i domini. In questo lavoro, studiamo, teoricamente ed empiricamente, l'effetto della complessità dell'embedding sulla generalizzazione al dominio di destinazione. In particolare, questa complessità influisce su un limite superiore del rischio di destinazione; questo si riflette anche negli esperimenti.Successivamente, specifichiamo il nostro quadro teorico alle reti neurali multistrato.Come risultato, sviluppiamo una strategia che attenua la sensibilità alla complessità di embedding, ed empiricamente raggiunge prestazioni alla pari o migliori del miglior tradeoff di complessità dipendente dallo strato.
Proponiamo una nuova architettura denominata Dual Adversarial Transfer Network (DATNet) per affrontare il Named Entity Recognition (NER) a basse risorse, DATNet-F e DATNet-P, sono proposte per esplorare la fusione efficace delle caratteristiche tra risorse alte e basse.Per affrontare i dati di formazione rumorosi e squilibrati, proponiamo un nuovo Discriminatore Generalizzato di Risorse-Adversariale (GRAD).Inoltre, la formazione avversaria è adottata per aumentare la generalizzazione del modello. Esaminiamo gli effetti dei diversi componenti in DATNet attraverso i domini e le lingue e dimostriamo che un miglioramento significativo può essere ottenuto soprattutto per i dati a bassa risorsa. Senza aumentare alcuna caratteristica aggiuntiva creata a mano, raggiungiamo nuove prestazioni allo stato dell'arte su CoNLL e Twitter NER---88.16% F1 per lo spagnolo, 53.43% F1 per WNUT-2016, e 42.83% F1 per WNUT-2017.
Le reti generative avversarie (GAN) si sono evolute in una delle tecniche non supervisionate di maggior successo per la generazione di immagini realistiche.Anche se è stato recentemente dimostrato che l'addestramento GAN converge, i modelli GAN spesso finiscono in equilibri Nash locali che sono associati al collasso della modalità o altrimenti non riescono a modellare la distribuzione di destinazione.Introduciamo le GAN di Coulomb, che pongono il problema di apprendimento GAN come un campo potenziale, dove i campioni generati sono attratti dai campioni del set di allenamento ma si respingono a vicenda. Il discriminatore impara un campo potenziale mentre il generatore diminuisce l'energia spostando i suoi campioni lungo il campo vettoriale (forza) determinato dal gradiente del campo potenziale.Attraverso la diminuzione dell'energia, il modello GAN impara a generare campioni secondo l'intera distribuzione target e non copre solo alcune delle sue modalità. Dimostriamo che le GAN di Coulomb possiedono un solo equilibrio di Nash che è ottimale nel senso che la distribuzione del modello è uguale alla distribuzione di destinazione. Mostriamo l'efficacia delle GAN di Coulomb sulle camere da letto LSUN, i volti CelebA, CIFAR-10 e la generazione di testo Google Billion Word.
Prevedere le proprietà dei nodi in un grafico è un problema importante con applicazioni in una varietà di domini. I metodi di apprendimento semi-supervisionato (SSL) basati sul grafico mirano ad affrontare questo problema etichettando un piccolo sottoinsieme di nodi come semi, e quindi utilizzando la struttura del grafico per prevedere i punteggi delle etichette per il resto dei nodi nel grafico. Recentemente, le Graph Convolutional Networks (GCNs) hanno raggiunto prestazioni impressionanti nel compito SSL basato sul grafo. Oltre ai punteggi delle etichette, è anche desiderabile avere un punteggio di fiducia associato ad esse. ConfGCN utilizza queste confidenze stimate per determinare l'influenza di un nodo su un altro durante l'aggregazione del vicinato, acquisendo così capacità anisotrope.Attraverso un'ampia analisi ed esperimenti su benchmark standard, troviamo che ConfGCN è in grado di superare significativamente lo stato dell'arte delle baseline.Abbiamo reso disponibile il codice sorgente di ConfGCN per incoraggiare la ricerca riproducibile.
Il nostro primo benchmark, ImageNet-C, standardizza ed espande l'argomento della robustezza della corruzione, mostrando al contempo quali classificatori sono preferibili in applicazioni critiche per la sicurezza, quindi proponiamo un nuovo set di dati chiamato ImageNet-P che permette ai ricercatori di valutare la robustezza di un classificatore a perturbazioni comuni. A differenza della recente ricerca sulla robustezza, questo benchmark valuta le prestazioni su corruzioni e perturbazioni comuni e non sul caso peggiore di perturbazioni avversarie.Troviamo che ci sono cambiamenti trascurabili nella robustezza relativa della corruzione dai classificatori AlexNet ai classificatori ResNet.In seguito scopriamo modi per migliorare la robustezza della corruzione e della perturbazione.Troviamo persino che una difesa avversaria bypassata fornisce una sostanziale robustezza della perturbazione comune.Insieme i nostri benchmark possono aiutare il lavoro futuro verso reti che generalizzano robustamente.
La carta esplora una metodologia novella nell'offuscamento del codice sorgente attraverso l'applicazione dei modelli ricorrenti basati sul testo della rete neurale (RNN) del codificatore-decodificatore nella generazione del ciphertext e nei modelli di generazione della chiave.Sequence-to-sequence incorporati nell'architettura del modello per generare il codice offuscato, generare la chiave di deobfuscation e vivere l'esecuzione. Il confronto quantitativo di benchmark con i metodi di offuscamento esistenti indica un miglioramento significativo nella furtività e nel costo di esecuzione per la soluzione proposta, e gli esperimenti riguardanti le proprietà del modello danno risultati positivi per quanto riguarda la sua variazione di carattere, la dissimilarità rispetto al codice originale e la lunghezza costante del codice offuscato.
Proponiamo un metodo per la sintesi congiunta di immagini e annotazioni per-pixel con GAN.dimostriamo che GAN ha una buona rappresentazione di alto livello dei dati di destinazione che può essere facilmente proiettata a maschere di segmentazione semantica.questo metodo può essere utilizzato per creare un set di dati di formazione per insegnare una rete di segmentazione semantica separata.i nostri esperimenti mostrano che tale rete di segmentazione generalizza con successo su dati reali.inoltre, il metodo supera la formazione supervisionata quando il numero di campioni di formazione è piccolo, e funziona su varietà di scene e classi diverse.il codice sorgente del metodo proposto sarà disponibile al pubblico.
Vision-Language Navigation (VLN) è il compito in cui un agente viene comandato a navigare in ambienti sconosciuti foto-realistici con istruzioni in linguaggio naturale.La ricerca precedente su VLN è principalmente condotta sul set di dati Room-to-Room (R2R) con solo istruzioni in inglese.  L'obiettivo finale di VLN, tuttavia, è quello di servire persone che parlano lingue arbitrarie.Verso VLN multilingue con numerose lingue, raccogliamo un set di dati R2R multilingue, che estende il benchmark originale con le corrispondenti istruzioni cinesi.Ma è lungo e costoso raccogliere istruzioni umane su larga scala per ogni lingua esistente.Sulla base del nuovo set di dati introdotto, proponiamo un quadro generale VLN multilingue per consentire la navigazione che segue le istruzioni per diverse lingue.Esploriamo prima la possibilità di costruire un agente multilingue quando non sono disponibili dati di formazione della lingua di destinazione. L'agente multilingue è dotato di un meta-learner per aggregare rappresentazioni multilingue e di un modulo di allineamento multilingue visivamente fondato per allineare le rappresentazioni testuali di lingue diverse.Sotto lo scenario di apprendimento zero-shot, il nostro modello mostra risultati competitivi anche rispetto a un modello addestrato con tutte le istruzioni della lingua di destinazione.Inoltre, introduciamo una perdita di adattamento del dominio avversario per migliorare la capacità di trasferimento del nostro modello quando viene data una certa quantità di dati della lingua di destinazione.I nostri metodi e il set di dati dimostrano le potenzialità di costruire un agente multilingue per servire altoparlanti con lingue diverse.
I modelli generativi profondi hanno avanzato lo stato dell'arte nella classificazione semi-supervisionata, tuttavia la loro capacità di derivare utili caratteristiche discriminative in modo completamente non supervisionato per la classificazione in difficili set di dati del mondo reale, dove è richiesta un'adeguata separazione dei manifold, non è stata adeguatamente esplorata.La maggior parte dei metodi si basa sulla definizione di una pipeline di derivazione delle caratteristiche attraverso la modellazione generativa e poi l'applicazione di algoritmi di clustering, separando i processi di modellazione e discriminazione. Proponiamo un modello generativo gerarchico profondo che utilizza una miscela di distribuzioni discrete e continue per imparare a separare efficacemente i diversi collettivi di dati ed è allenabile end-to-end. Mostriamo che specificando la forma della distribuzione delle variabili discrete stiamo imponendo una struttura specifica alle rappresentazioni latenti del modello.
Tuttavia, la maggior parte dei progressi sono stati fatti in impostazioni grafiche statiche, mentre gli sforzi per l'apprendimento congiunto della dinamica del grafico e della dinamica sul grafico sono ancora in una fase infantile.Due domande fondamentali sorgono nell'apprendimento su grafici dinamici: (i) Come modellare elegantemente i processi dinamici sui grafici? (ii) Come sfruttare un tale modello per codificare efficacemente le informazioni in evoluzione del grafo nelle rappresentazioni basso-dimensionali? Presentiamo DyRep - una struttura di modellistica novella per i grafi dinamici che presuppone l'apprendimento della rappresentazione come processo latente di mediazione che collega due processi osservati cioÃ¨ -- dinamica della rete (realizzata come evoluzione topologica) e dinamica sulla rete (realizzata come attivitÃ fra i nodi). Concretamente, proponiamo un modello di processo puntiforme temporale profondo su due scale temporali che cattura le dinamiche intrecciate dei processi osservati. Questo modello è ulteriormente parametrizzato da una rete di rappresentazione temporale-attentiva che codifica le informazioni strutturali temporalmente in evoluzione nelle rappresentazioni dei nodi che a loro volta guidano l'evoluzione non lineare delle dinamiche osservate del grafico. La nostra struttura unificata è addestrata usando una procedura efficiente non supervisionata e ha la capacità di generalizzare sopra i nodi non visti. dimostriamo che DyRep supera le linee di base dello stato dell'arte per la previsione dinamica dei collegamenti e le attività di previsione del tempo e presentiamo ampie intuizioni qualitative nella nostra struttura.
Con il successo dell'apprendimento automatico moderno, sta diventando sempre piÃ¹ importante capire e controllare come gli algoritmi di apprendimento interagiscono.Purtroppo, i risultati negativi dalla teoria dei giochi mostrano che c'Ã¨ poca speranza di capire o controllare i giochi generali del n-giocatore.Introduciamo quindi i mercati lisci (SM-giochi), una classe di giochi del n-giocatore con le interazioni pairwise della somma zero.SM-giochi codificano un modello comune di disegno nell'apprendimento automatico che include alcuni GANs, addestramento adversarial ed altri algoritmi recenti.Mostriamo che SM-giochi sono adatti ad analisi e ad ottimizzazione usando i metodi del primo ordine.
Mentre le contromacchine hanno ricevuto poca attenzione nell'informatica teorica a partire dagli anni '60, hanno recentemente raggiunto una ritrovata rilevanza nel campo dell'elaborazione del linguaggio naturale (NLP).Un lavoro recente ha suggerito che alcune reti neurali ricorrenti ad alte prestazioni utilizzano la loro memoria come contatori.Quindi, un modo potenziale per comprendere il successo di queste reti è quello di rivisitare la teoria della controcomputazione.Pertanto, abbiamo scelto di studiare le capacità delle contromacchine in tempo reale come grammatiche formali.Mostriamo prima che diverse varianti della contromacchina convergono per esprimere la stessa classe di lingue formali. Questo ha implicazioni per l'interpretabilità e la valutazione dei sistemi di reti neurali: riuscire a far corrispondere i modelli sintattici non garantisce che un modello simile al contatore rappresenti accuratamente le strutture semantiche sottostanti.
I modelli di encoder-decoder basati su RNN per la sintesi astrattiva hanno raggiunto buone prestazioni su brevi sequenze di input e output.Per documenti più lunghi e riassunti, tuttavia, questi modelli spesso includono frasi ripetitive e incoerenti.Introduciamo un modello di rete neurale con una nuova intra-attenzione che presta attenzione all'input e all'output continuamente generato separatamente, e un nuovo metodo di formazione che combina la predizione di parole standard supervisionata e l'apprendimento di rinforzo (RL). I modelli addestrati solo con l'apprendimento supervisionato spesso mostrano un "bias di esposizione" - assumono che la verità di base sia fornita ad ogni passo durante l'addestramento.Tuttavia, quando la predizione standard delle parole è combinata con l'addestramento di predizione globale della sequenza di RL i riassunti risultanti diventano più leggibili.Valutiamo questo modello sui dataset CNN/Daily Mail e New York Times.Il nostro modello ottiene un punteggio di 41.16 ROUGE-1 sul dataset CNN/Daily Mail, un miglioramento rispetto ai precedenti modelli all'avanguardia.La valutazione umana mostra anche che il nostro modello produce riassunti di qualità superiore.
Knowledge Distillation (KD) è un metodo comune per trasferire la ``conoscenza'' appresa da un modello di apprendimento automatico (il maestro) in un altro modello (lo studente), dove tipicamente, il maestro ha una maggiore capacità (ad es, A nostra conoscenza, i metodi esistenti trascurano il fatto che anche se lo studente assorbe conoscenza extra dall'insegnante, entrambi i modelli condividono gli stessi dati di input - e questi dati sono l'unico mezzo con cui la conoscenza dell'insegnante può essere dimostrata. D'altra parte, un insegnante umano può dimostrare un pezzo di conoscenza con esempi individualizzati adattati a un particolare studente, per esempio, in termini di background culturale e di interessi.Ispirandoci a questo comportamento, progettiamo agenti di incremento dei dati con ruoli distinti per facilitare la distillazione della conoscenza. Ci concentriamo specificamente su KD quando la rete dell'insegnante ha una maggiore precisione (bit-width) rispetto alla rete dello studente.Troviamo empiricamente che i punti di dati appositamente adattati permettono alla conoscenza dell'insegnante di essere dimostrata più efficacemente allo studente.Confrontiamo il nostro approccio con i metodi KD esistenti sulla formazione di architetture neurali popolari e dimostriamo che l'aumento dei dati in base al ruolo migliora l'efficacia di KD rispetto a forti approcci precedenti.Il codice per riprodurre i nostri risultati sarà reso disponibile al pubblico.
I modelli di rete neurale hanno mostrato un'eccellente fluidità e performance quando applicati al riassunto astrattivo.Molti approcci al riassunto astrattivo neurale coinvolgono l'introduzione di significativi bias induttivi, come le architetture generatrici di puntatori, la copertura, e procedure parzialmente estrattive, progettate per imitare il riassunto umano. Introduciamo una semplice procedura costruita su decoder-trasformatori pre-addestrati per ottenere punteggi ROUGE competitivi utilizzando una perdita di modellazione linguistica da sola, senza ricerca del fascio o altre ottimizzazioni del tempo di decodifica, e invece contare su un efficiente campionamento del nucleo e una decodifica avida.
Questo articolo affronta il problema dell'adattamento incrementale del dominio (IDA), supponendo che ogni dominio arrivi in modo sequenziale e che si possa accedere solo ai dati del dominio corrente. Proponiamo di aumentare una rete neurale ricorrente (RNN) con una banca di memoria direttamente parametrizzata, che viene recuperata da un meccanismo di attenzione ad ogni passo di transizione RNN.La banca di memoria fornisce un modo naturale di IDA: quando adattiamo il nostro modello ad un nuovo dominio, aggiungiamo progressivamente nuovi slot alla banca di memoria, che aumenta la capacità del modello.Impariamo i nuovi slot di memoria e mettiamo a punto i parametri esistenti tramite back-propagation. Gli esperimenti mostrano che il nostro approccio supera significativamente il fine-tuning ingenuo e i lavori precedenti su IDA, compreso il consolidamento elastico dei pesi e la rete neurale progressiva.  Rispetto all'espansione degli stati nascosti, il nostro approccio è più robusto per i vecchi domini, dimostrato da risultati sia empirici che teorici.
Nel rilevamento compresso, un problema primario da risolvere è quello di ricostruire un segnale sparso ad alta dimensione da un piccolo numero di osservazioni. In questo lavoro, sviluppiamo un nuovo algoritmo di recupero del segnale sparso utilizzando l'apprendimento di rinforzo (RL) e la ricerca di Monte CarloTree (MCTS), in modo simile all'inseguimento di corrispondenza ortogonale (OMP), il nostro algoritmo RL+MCTS sceglie il supporto del segnale in modo sequenziale. La novità chiave è che l'algoritmo proposto impara a scegliere il supporto successivo, invece di seguire una regola predefinita come in OMP. Vengono forniti risultati empirici per dimostrare le prestazioni superiori dell'algoritmo RL+MCTS proposto rispetto agli algoritmi di recupero dei segnali sparsi esistenti.
Molti problemi decisionali sequenziali del mondo reale possono essere formulati come controllo ottimale con osservazioni ad alta densità e dinamiche sconosciute. Un approccio promettente è quello di incorporare le osservazioni ad alta densità in uno spazio di rappresentazione latente a bassa densità, stimare il modello di dinamica latente, quindi utilizzare questo modello per il controllo nello spazio latente.Un'importante questione aperta è come imparare una rappresentazione che sia adatta agli algoritmi di controllo esistenti. Formulando e analizzando il problema dell'apprendimento della rappresentazione da una prospettiva di controllo ottimale, stabiliamo tre principi di base che la rappresentazione appresa dovrebbe comprendere: 1) previsione accurata nello spazio di osservazione, 2) coerenza tra le dinamiche dello spazio latente e di osservazione, e 3) bassa curvatura nelle transizioni dello spazio latente. Questi principi corrispondono naturalmente a una funzione di perdita che consiste di tre termini: predizione, coerenza e curvatura (PCC).Crucialmente, per rendere PCC trattabile, deriviamo un limite variazionale ammortizzato per la funzione di perdita PCC.Esperimenti estesi su domini di riferimento dimostrano che il nuovo algoritmo di apprendimento variazionale-PCC beneficia di una formazione significativamente più stabile e riproducibile, e porta a prestazioni di controllo superiori.  Ulteriori studi di ablazione danno sostegno all'importanza di tutte e tre le componenti PCC per l'apprendimento di un buon spazio latente per il controllo.
L'interazione tra la topologia della rete inter-neuronale e la cognizione è stata studiata profondamente dai ricercatori di connettomica e dagli scienziati di rete, che è cruciale per comprendere la notevole efficacia delle reti neurali biologiche.Curiosamente, la rivoluzione dell'apprendimento profondo che ha fatto rivivere le reti neurali non ha prestato molta attenzione agli aspetti topologici. Le architetture delle reti neurali profonde (DNN) non assomigliano alle loro controparti biologiche in senso topologico. Noi colmiamo questa lacuna presentando i risultati iniziali delle Deep Connectomics Networks (DCNs) come DNNs con topologie ispirate alle reti neuronali del mondo reale.Mostriamo un'elevata precisione di classificazione ottenuta dalle DCNs la cui architettura è stata ispirata dalle reti neuronali biologiche di C. Elegans e dalla corteccia visiva del topo.
Le reti neurali convoluzionali e le reti neurali ricorrenti sono progettate con strutture di rete ben adatte alla natura dei dati spaziali e sequenziali, rispettivamente. Tuttavia, la struttura delle reti neurali feed-forward standard (FNN) è semplicemente una pila di strati completamente connessi, indipendentemente dalle correlazioni delle caratteristiche nei dati, inoltre il numero di strati e il numero di neuroni sono sintonizzati manualmente sui dati di convalida, il che richiede tempo e può portare a reti subottimali. Il nostro metodo determina il numero di strati, il numero di neuroni per ogni strato e la connettività rada tra gli strati adiacenti automaticamente dai dati. I modelli risultanti sono chiamati reti neurali Backbone-Skippath (BSNNs). Gli esperimenti su 17 compiti mostrano che, in confronto a FNNs, BSNNs può raggiungere prestazioni di classificazione migliori o comparabili con molti meno parametri.
L'inferenza bayesiana è ampiamente utilizzata per dedurre e quantificare l'incertezza in un campo di interesse da una misurazione di un campo correlato quando i due sono collegati da un modello matematico.Nonostante le sue molte applicazioni, l'inferenza bayesiana affronta delle sfide quando si deducono campi che hanno rappresentazioni discrete di grandi dimensioni, e/o hanno distribuzioni di priorità che sono difficili da caratterizzare matematicamente.In questo lavoro dimostriamo come la distribuzione approssimativa appresa da una rete generativa avversaria (GAN) può essere utilizzata come priorità in un aggiornamento bayesiano per affrontare entrambe queste sfide. Dimostriamo l'efficacia di questo approccio inferendo e quantificando l'incertezza in un problema inverso basato sulla fisica e in un problema inverso derivante dalla computer vision. In quest'ultimo esempio, dimostriamo anche come la conoscenza della variazione spaziale dell'incertezza possa essere usata per selezionare una strategia ottimale di collocare i sensori (cioè prendere le misure), dove le informazioni sull'immagine sono rivelate una sottoregione alla volta.
Noi sosteniamo che i benchmark Omniglot e miniImageNet, ampiamente utilizzati, sono troppo semplici perché la semantica delle loro classi non varia tra gli episodi, il che vanifica il loro scopo di valutare i metodi di classificazione a pochi scatti. La semantica di classe di Omniglot è invariabilmente "personaggi" e la semantica di classe di miniImageNet, "categoria di oggetti". Poiché la semantica di classe è così simile, proponiamo un nuovo metodo chiamato Centroid Networks che può raggiungere accuratezze sorprendentemente alte su Omniglot e miniImageNet senza usare alcuna etichetta al momento della metavalutazione. I nostri risultati suggeriscono che questi benchmark non sono adatti per la classificazione supervisionata a pochi colpi poiché la supervisione stessa non è necessaria durante la metavalutazione. Il Meta-Dataset, una collezione di 10 set di dati, è stato recentemente proposto come un benchmark di classificazione a pochi colpi più difficile. Usando il nostro metodo, deriviamo una nuova metrica, il Class Semantics Consistency Criterion, e lo usiamo per quantificare la difficoltà del Meta-Dataset.Infine, sotto alcune ipotesi restrittive, dimostriamo che Centroid Networks è più veloce e più accurato di un metodo di apprendimento a cluster allo stato dell'arte (Hsu et al, 2018).
Impariamo a identificare gli stati di decisione, cioè l'insieme parsimonioso di stati in cui le decisioni influenzano significativamente gli stati futuri che un agente può raggiungere in un ambiente. Utilizziamo il quadro VIC, che massimizza l'"empowerment" di un agente, cioè la capacità di raggiungere in modo affidabile un diverso insieme di stati - e formuliamo un limite a sandwich sull'obiettivo di empowerment che permette di identificare gli stati di decisione. A differenza dei lavori precedenti, i nostri stati decisionali sono scoperti senza ricompense estrinseche - semplicemente interagendo con il mondo. I nostri risultati mostrano che i nostri stati decisionali sono: 1) spesso interpretabili, e 2) portano a una migliore esplorazione su compiti guidati da obiettivi a valle in ambienti parzialmente osservabili.
I problemi inversi sono onnipresenti nelle scienze naturali e si riferiscono all'impegnativo compito di dedurre distribuzioni posteriori complesse e potenzialmente multi-modali su parametri nascosti dati un insieme di osservazioni.Tipicamente, un modello del processo fisico sotto forma di equazioni differenziali è disponibile ma porta a un'inferenza intrattabile sui suoi parametri.Mentre la propagazione in avanti dei parametri attraverso il modello simula l'evoluzione del sistema, il problema inverso di trovare i parametri data la sequenza di stati non è unico. In questo lavoro, proponiamo una generalizzazione della struttura di ottimizzazione bayesiana per approssimare l'inferenza. Il metodo risultante impara approssimazioni alla distribuzione posteriore applicando la discesa del gradiente variazionale di Stein sopra le stime da un modello di processo gaussiano.
Presentiamo un approccio guidato dai dati per costruire una libreria di primitive di movimento a feedback per veicoli non olonomici che garantisce un errore limitato nel seguire traiettorie arbitrariamente lunghe, il che assicura che la ripianificazione del movimento possa essere evitata finché i disturbi al veicolo rimangono entro un certo limite e anche potenzialmente quando gli ostacoli sono spostati entro un certo limite. La libreria è costruita lungo astrazioni locali della dinamica che permette l'aggiunta di nuove primitive di movimento attraverso il raffinamento dell'astrazione. Forniamo condizioni sufficienti per la costruzione di tali primitive di movimento robuste per una vasta classe di dinamiche non lineari, compresi i modelli comunemente usati, come il modello standard di Reeds-Shepp.L'algoritmo è applicato per la pianificazione e il controllo del movimento di un rover con slittamento senza la sua modellazione preliminare.
È stato dimostrato che le Deep Convolutional Networks (DCNs) sono sensibili alle Universal Adversarial Perturbations (UAPs): perturbazioni indipendenti dall'input che ingannano un modello su ampie porzioni di un dataset.Queste UAPs mostrano interessanti modelli visivi, ma questo fenomeno è, ancora, poco compreso. Il nostro lavoro mostra che anche modelli di rumore procedurale visivamente simili agiscono come UAPs.In particolare, dimostriamo che diverse architetture DCN sono sensibili ai modelli di rumore Gabor.Questo comportamento, le sue cause e le implicazioni meritano ulteriori studi approfonditi.
Le reti neurali profonde (DNN) eseguono bene una varietà di compiti nonostante il fatto che la maggior parte utilizzata nella pratica sia ampiamente sovraparametrizzata e persino in grado di adattarsi perfettamente a dati etichettati in modo casuale.Prove recenti suggeriscono che lo sviluppo di rappresentazioni "comprimibili" è fondamentale per adattare la complessità delle reti sovraparametrizzate al compito da svolgere ed evitare l'overfitting (Arora et al., 2018; Zhou et al, In questo lavoro, forniamo nuove prove empiriche che supportano questa ipotesi, identificando due meccanismi indipendenti che emergono quando la larghezza della rete viene aumentata: la robustezza (avere unità che possono essere rimosse senza influenzare la precisione) e la ridondanza (avere unità con attività simili). In una serie di esperimenti con le reti AlexNet, ResNet e Inception nei dataset CIFAR-10 e ImageNet, e anche usando reti poco profonde con dati sintetici, dimostriamo che le DNN aumentano costantemente la loro robustezza, la loro ridondanza, o entrambe a larghezze maggiori per una serie completa di iperparametri.
Le reti profonde realizzano mappature complesse che sono spesso comprese dal loro comportamento localmente lineare nei punti di interesse o intorno ad essi. Per esempio, usiamo la derivata della mappatura rispetto ai suoi ingressi per l'analisi della sensibilità, o per spiegare (ottenere la pertinenza delle coordinate per) una previsione.Una sfida chiave è che tali derivate sono esse stesse intrinsecamente instabili.In questo articolo, proponiamo un nuovo problema di apprendimento per incoraggiare le reti profonde ad avere derivate stabili su regioni più ampie. Il nostro algoritmo consiste in una fase di inferenza che identifica una regione intorno a un punto in cui l'approssimazione lineare è provatamente stabile, e una fase di ottimizzazione per espandere tali regioni, e proponiamo un nuovo rilassamento per scalare l'algoritmo a modelli realistici, illustrando il nostro metodo con reti residue e ricorrenti su dataset di immagini e sequenze.
Gli ultimi anni hanno assistito a due sviluppi apparentemente opposti delle reti neurali convoluzionali profonde (CNNs).Da un lato, aumentando la densità di CNNs con l'aggiunta di connessioni cross-layer si ottiene una maggiore accuratezza.D'altra parte, la creazione di strutture di sparsità attraverso metodi di regolarizzazione e pruning gode di costi computazionali inferiori.In questo documento, facciamo un ponte tra questi due proponendo una nuova struttura di rete con collegamenti localmente densi ma esternamente sparsi. I risultati sperimentali dimostrano che la struttura localmente densa ma esternamente rada potrebbe acquisire prestazioni competitive su compiti di riferimento (CIFAR10, CIFAR100 e ImageNet) mantenendo la struttura della rete snella.
La maggior parte degli algoritmi di inferenza allo stato dell'arte sono varianti di Markov chain Monte Carlo (MCMC) o di inferenza variazionale (VI): I metodi MCMC possono essere computazionalmente impegnativi; i metodi VI possono avere grandi distorsioni. Il metodo proposto può generare campioni a basso bias aumentando la lunghezza della simulazione MCMC e ottimizzando gli iper-parametri MCMC, il che offre un equilibrio attraente tra bias di approssimazione ed efficienza computazionale. Mostriamo che il nostro metodo produce risultati promettenti su benchmark popolari rispetto ai recenti metodi ibridi di MCMC e VI.
Per dimostrare questo effetto, addestriamo una nuova rete "meta" per prevedere dall'output finale della rete "base" sottostante o dall'output di uno degli strati intermedi della rete base se la rete base sarà corretta o scorretta per un particolare input. Troviamo che, su una vasta gamma di compiti e reti base, la rete meta può raggiungere un'accuratezza che va dal 65% all'85% nel fare questa determinazione.
Sviluppiamo un nuovo algoritmo per l'apprendimento dell'imitazione da una singola dimostrazione dell'esperto.In contrasto con molti approcci precedenti di apprendimento dell'imitazione one-shot, il nostro algoritmo non presuppone l'accesso a più di una dimostrazione dell'esperto durante la fase di formazione.Invece, sfruttiamo una politica di esplorazione per acquisire traiettorie non supervisionate, che sono poi utilizzate per addestrare sia un codificatore che una politica di imitazione consapevole del contesto. Le procedure di ottimizzazione per il codificatore, l'allievo di imitazione e la politica di esplorazione sono tutte strettamente collegate.Questo collegamento crea un ciclo di feedback in cui la politica di esplorazione raccoglie nuove dimostrazioni che sfidano l'allievo di imitazione, mentre il codificatore cerca di aiutare la politica di imitazione al meglio delle sue capacità.valutiamo il nostro algoritmo su 6 compiti di robotica MujoCo.
Un lavoro significativo è stato dedicato allo sviluppo di metodi per comunicare agli utenti umani le ragioni del processo decisionale all'interno di sistemi di pianificazione automatizzati. Tuttavia, molta meno attenzione è stata posta sulla comunicazione delle ragioni per cui i sistemi di pianificazione non sono in grado di arrivare a una soluzione fattibile quando sono sovraccarichi di lavoro. Delineiamo una struttura generica, basata sull'enumerazione efficiente di insiemi minimi insoddisfacibili (MUS) e insiemi massimi soddisfacibili (MSS), per produrre piccole descrizioni della fonte di infeasibilità. Queste descrizioni sono completate con potenziali rilassamenti che sistemerebbero l'infeasibilità trovata all'interno dell'istanza del problema. Illustriamo come questo metodo può essere applicato all'ARCPSP e dimostriamo come generare diversi tipi di spiegazioni per un'istanza sovra vincolata dell'ARCPSP.
Dimostriamo che gli approcci di salienza basati sul gradiente non sono in grado di catturare questo spostamento e sviluppiamo una nuova difesa che individua invece gli esempi avversari basati su modelli di salienza appresi.Studiamo due approcci: una CNN addestrata a distinguere tra immagini naturali e avversarie usando le maschere di salienza prodotte dal nostro modello di salienza appreso e una CNN addestrata sui pixel salienti stessi come input. Su MNIST, CIFAR-10 e ASSIRA, le nostre difese sono in grado di rilevare vari attacchi avversari, compresi gli attacchi forti come C&W e DeepFool, contrariamente ai rilevatori di salienza basati sul gradiente e ai rilevatori che si basano sull'immagine di input. Questi ultimi non sono in grado di rilevare le immagini avversarie quando le norme L_2- e L_infinity- delle perturbazioni sono troppo piccole.Infine, troviamo che il rilevatore basato sui pixel salienti migliora i rilevatori basati sulle mappe di salienza in quanto è più robusto agli attacchi white-box.
La codifica disentangled è un passo importante verso un migliore apprendimento della rappresentazione.Tuttavia, nonostante i numerosi sforzi, non esiste ancora un chiaro vincitore che catturi le caratteristiche indipendenti dei dati in modo non supervisionato.In questo lavoro valutiamo empiricamente le prestazioni di sei approcci di disentanglement non supervisionati sul dataset giocattolo mpi3d curato e rilasciato per il NeurIPS 2019 Disentanglement Challenge.I metodi indagati in questo lavoro sono Beta-VAE, Factor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE e Beta-TCVAE. Le capacità di tutti i modelli sono state progressivamente aumentate durante l'addestramento e gli iper-parametri sono stati mantenuti intatti attraverso gli esperimenti. I metodi sono stati valutati sulla base di cinque metriche di disentanglement, cioè DCI, Factor-VAE, IRS, MIG e SAP-Score. Entro i limiti di questo studio, l'approccio Beta-TCVAE è stato trovato per superare le sue alternative per quanto riguarda la somma normalizzata delle metriche.Tuttavia, uno studio qualitativo dei latenti codificati rivela che non c'è una correlazione coerente tra le metriche riportate e il potenziale di disentanglement del modello.
La maggior parte dei lavori precedenti sull'apprendimento di rinforzo multi-agente (MARL) raggiunge una collaborazione ottimale imparando direttamente una politica per ogni agente per massimizzare una ricompensa comune.In questo lavoro, miriamo ad affrontare questo da un'angolazione diversa.In particolare, consideriamo scenari in cui ci sono agenti auto-interessati (cioè, agenti lavoratori) che hanno le loro menti (preferenze, intenzioni, abilità, ecc.) e non possono essere dettate per eseguire compiti che non vogliono fare.Per ottenere una coordinazione ottimale tra questi agenti, addestriamo un super agente (cioè, L'obiettivo del manager è quello di massimizzare la produttività complessiva e di minimizzare i pagamenti fatti ai lavoratori per il teaming ad-hoc. Per addestrare il manager, proponiamo Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), che consiste nella modellazione dell'agente e nell'apprendimento della politica.Abbiamo valutato il nostro approccio in due ambienti, Resource Collection e Crafting, per simulare problemi di gestione multi-agente con varie impostazioni di compiti e design multipli per gli agenti lavoratori.I risultati sperimentali hanno convalidato l'efficacia del nostro approccio nel modellare online le menti degli agenti lavoratori, e nel raggiungere un ottimo teaming ad-hoc con una buona generalizzazione e un rapido adattamento.
Proponiamo un'architettura di rete che introduce connessioni ricorrenti temporali per lo stato interno dei blocchi residuali ampiamente utilizzati.Dimostriamo che, con queste connessioni, le reti neurali convoluzionali possono apprendere più robustamente stati temporali stabili che persistono tra le valutazioni.Dimostriamo il loro potenziale per l'inferenza di immagini di super-risoluzione di alta qualità da immagini a bassa risoluzione prodotte con renderer in tempo reale. Questi dati si presentano in una vasta gamma di applicazioni, e sono particolarmente impegnativi in quanto contengono un segnale fortemente alias, quindi i dati differiscono sostanzialmente dagli input lisci incontrati nei video naturali, e le tecniche esistenti non riescono a produrre una qualità d'immagine accettabile. Proponiamo inoltre una serie di attenti aggiustamenti delle tipiche architetture generative avversarie per la super-risoluzione video per arrivare a un primo modello che può produrre immagini dettagliate, ma temporalmente coerenti da un flusso alias di input da un renderer in tempo reale.
L'algoritmo di backpropagation è il più popolare algoritmo di addestramento delle reti neurali al giorno d'oggi, ma soffre dei problemi di forward locking, backward locking e update locking, specialmente quando una rete neurale è così grande che i suoi strati sono distribuiti su più dispositivi. Le soluzioni esistenti o sono in grado di gestire solo un problema di blocco o portano a gravi perdite di precisione o inefficienza di memoria, inoltre, nessuna di esse considera il problema dei ritardatari tra i dispositivi.In questo articolo, proponiamo \textbf{Layer-wise Staleness} e un nuovo algoritmo di formazione efficiente, \textbf{Diversely Stale Parameters} (Analizziamo anche la convergenza di DSP con due metodi popolari basati sul gradiente e dimostriamo che entrambi sono garantiti per convergere a punti critici per problemi non convessi. Infine, ampi risultati sperimentali sull'addestramento di reti neurali convoluzionali profonde dimostrano che il nostro algoritmo DSP proposto può raggiungere una significativa accelerazione della formazione con una maggiore robustezza e una migliore generalizzazione rispetto ai metodi comparati.
L'emergere del linguaggio in ambienti multi-agente è una direzione di ricerca promettente per fondare il linguaggio naturale in agenti simulati.Se l'IA fosse in grado di comprendere il significato del linguaggio attraverso il suo utilizzo, potrebbe anche trasferirlo ad altre situazioni in modo flessibile.Questo è visto come un passo importante verso il raggiungimento di un'IA generale.La portata della comunicazione emergente è finora, tuttavia, ancora limitata.È necessario migliorare le possibilità di apprendimento delle abilità associate alla comunicazione per aumentare la complessità emergente. Abbiamo preso un esempio dall'acquisizione del linguaggio umano e l'importanza della connessione empatica in questo processo.Proponiamo un approccio per introdurre la nozione di empatia all'apprendimento di rinforzo profondo multi-agente.Estendiamo gli approcci esistenti sui giochi referenziali con un compito ausiliario per il parlante di prevedere il cambiamento di mente dell'ascoltatore migliorando il tempo di apprendimento.I nostri esperimenti mostrano l'alto potenziale di questo elemento architettonico raddoppiando la velocità di apprendimento della configurazione di prova.
La didascalia dei paragrafi delle immagini è il compito di generare automaticamente più frasi per descrivere le immagini in un testo a grana fine e coerente.I tipici modelli esistenti basati sul deep learning per la didascalia delle immagini consistono in un codificatore di immagini per estrarre le caratteristiche visive e un decodificatore di modelli linguistici, che ha mostrato risultati promettenti nella generazione di singole frasi di alto livello. Tuttavia, solo il segnale di guida scalare a livello di parola è disponibile quando il codificatore di immagini è ottimizzato per estrarre le caratteristiche visive.L'incoerenza tra l'estrazione parallela delle caratteristiche visive e la supervisione sequenziale del testo limita il suo successo quando la lunghezza del testo generato è lunga (più di 50 parole).In questo documento, proponiamo un nuovo modulo, chiamato il modulo Text Embedding Bank (TEB), per affrontare il problema per la didascalia di paragrafi di immagini. Questo modulo utilizza il modello vettoriale di paragrafo per apprendere rappresentazioni di caratteristiche di lunghezza fissa da un paragrafo di lunghezza variabile.Ci riferiamo alla caratteristica di lunghezza fissa come il TEB.Questo modulo TEB svolge due ruoli per beneficiare delle prestazioni della didascalia di paragrafo.Primo, agisce come una forma di supervisione profonda globale e coerente per regolarizzare l'estrazione di caratteristiche visive nel codificatore di immagini. Aggiungendo questo modulo a due metodi all'avanguardia esistenti, si ottiene un nuovo risultato all'avanguardia con un ampio margine sul set di dati Visual Genome per la didascalia dei paragrafi.
L'apprendimento degli spazi metrici di Mahalanobis è un problema importante che ha trovato numerose applicazioni.Diversi algoritmi sono stati progettati per questo problema, tra cui l'Apprendimento Teorico delle Metriche (ITML) [Davis et al. 2007] e la classificazione Large Margin Nearest Neighbor (LMNN) [Weinberger e Saul 2009].  Consideriamo una formulazione dell'apprendimento della metrica di Mahalanobis come un problema di ottimizzazione, dove l'obiettivo è quello di minimizzare il numero di vincoli di similarità/dissimilarità violati.  Dimostriamo che per qualsiasi dimensione ambientale fissa, esiste uno schema di approssimazione in tempo completamente polinomiale (FPTAS) con un tempo di esecuzione quasi lineare. Questo risultato è ottenuto utilizzando strumenti della teoria della programmazione lineare in basse dimensioni. Discutiamo anche i miglioramenti dell'algoritmo nella pratica e presentiamo risultati sperimentali su set di dati sintetici e reali.
I compiti standard di didascalia delle immagini come COCO e Flickr30k sono fattuali, neutrali nel tono e (per un umano) dichiarano l'ovvio (ad esempio, "un uomo che suona la chitarra").Mentre tali compiti sono utili per verificare che una macchina capisca il contenuto di un'immagine, non sono coinvolgenti per gli umani come didascalie.   Con questo in mente definiamo un nuovo compito, Personality-Captions, dove l'obiettivo è quello di essere il più coinvolgente possibile per gli esseri umani incorporando tratti di stile e personalità controllabili.Raccogliamo e rilasciamo un grande dataset di 201.858 di tali didascalie condizionate su 215 tratti possibili.  Costruiamo modelli che combinano il lavoro esistente di (i) rappresentazioni di frasi (Mazaré et al., 2018) con Transformers addestrati su 1,7 miliardi di esempi di dialogo; e (ii) rappresentazioni di immagini (Mahajan et al., 2018) con ResNets addestrati su 3,5 miliardi di immagini di social media.  Otteniamo prestazioni allo stato dell'arte su Flickr30k e COCO, e forti prestazioni sul nostro nuovo compito.Infine, le valutazioni online convalidano che il nostro compito e i nostri modelli sono coinvolgenti per gli umani, con il nostro miglior modello vicino alle prestazioni umane.
Per mitigare questa degradazione, proponiamo un DP Laplacian smoothing SGD (DP-LSSGD) per addestrare modelli ML con garanzie di privacy differenziale (DP).Al centro del DP-LSSGD c'è il Laplacian smoothing, che smussa il rumore gaussiano usato nel meccanismo gaussiano. Sotto la stessa quantità di rumore utilizzato nel meccanismo gaussiano, DP-LSSGD raggiunge la stessa garanzia DP, ma una migliore utilità soprattutto per gli scenari con forti garanzie DP.In pratica, DP-LSSGD rende l'addestramento di modelli ML convessi e non convessi più stabile e permette ai modelli addestrati di generalizzare meglio.L'algoritmo proposto è semplice da implementare e la complessità computazionale extra e l'overhead di memoria rispetto a DP-SGD sono trascurabili.DP-LSSGD è applicabile per addestrare una grande varietà di modelli ML, compresi i DNN.
Studiamo il problema del rilevamento compresso robusto di un bit il cui obiettivo è quello di progettare un algoritmo che recuperi fedelmente qualsiasi vettore sparso$theta_0\in\mathbb{R}^d$ \emph{uniformemente} da $m$ misure rumorose quantizzate. Sotto l'ipotesi che le misure siano sub-gaussiane, per recuperare qualsiasi $k$-sparso $\theta_0$ ($k\ll d$) \emph{uniformemente} fino ad un errore $\varepsilon$ con alta probabilità, il miglior algoritmo computazionalmente trattabile conosciuto richiede $footnote{ Qui, un algoritmo è ``computabilmente trattabile'' se ha garanzie di convergenza dimostrabili. La notazione $ ‗tilde{mathcal{O}}(\cdot)$ omette un fattore logaritmico di $ ‗varepsilon^{-1}$.} In questo articolo, consideriamo una nuova struttura per il problema del rilevamento di un bit in cui la sparsità è implicitamente applicata attraverso la mappatura di una rappresentazione a bassa dimensione $x_0$ attraverso una rete generativa ReLU nota $n$-layer $G:\mathbb{R}^krightarrowmathbb{R}^d$. Un tale quadro pone priori a bassa dimensione su $\theta_0$ senza una base nota. Noi proponiamo di recuperare l'obiettivo $G(x_0)$ attraverso un problema di minimizzazione del rischio empirico (ERM) non vincolato sotto un'ipotesi di misurazione sub-esponenziale molto più debole. Per tale problema, stabiliamo un'analisi statistica e computazionale congiunta. In particolare, dimostriamo che lo stimatore ERM in questo nuovo quadro raggiunge un tasso statistico migliorato di $m=\tilde{mathcal{O}} (kn\log d /\epsilon^2)$ recuperando qualsiasi $G(x_0)$ uniformemente fino a un errore $\varepsilon$.Inoltre, dalla lente del calcolo, dimostriamo che sotto opportune condizioni sui pesi ReLU, il nostro rischio empirico proposto, nonostante la non-convessità, non ha punti stazionari al di fuori di piccoli quartieri intorno alla rappresentazione vera $x_0$ e il suo multiplo negativo. Inoltre, mostriamo che il minimizzatore globale del rischio empirico rimane all'interno del quartiere intorno a $x_0$ piuttosto che il suo multiplo negativo. La nostra analisi fa luce sulla possibilità di invertire un modello generativo profondo sotto misure parziali e quantizzate, completando il recente successo di usare modelli generativi profondi per problemi inversi.
Introduciamo un algoritmo di apprendimento strutturale non supervisionato per reti neurali profonde, feed-forward, e proponiamo una nuova interpretazione per la profondità e la connettività tra gli strati, dove una gerarchia di indipendenza nella distribuzione degli input è codificata nella struttura della rete, il che si traduce in strutture che permettono ai neuroni di collegarsi a neuroni in qualsiasi strato più profondo, saltando gli strati intermedi. Inoltre, i neuroni negli strati più profondi codificano le indipendenze di basso ordine (piccoli insiemi di condizioni) e hanno un'ampia portata dell'input, mentre i neuroni nei primi strati codificano le indipendenze di ordine superiore (insiemi di condizioni più grandi) e hanno una portata più stretta. L'algoritmo proposto costruisce due modelli grafici principali: 1) un grafo latente generativo (una rete di credenza profonda) appreso dai dati e 2) un grafo discriminativo profondo costruito a partire dal grafo latente generativo; dimostriamo che le dipendenze condizionali tra i nodi nel grafo latente generativo appreso sono conservate nel grafo discriminativo condizionale di classe; infine, una struttura di rete neurale profonda è costruita sulla base del grafo discriminativo. Dimostriamo su benchmark di classificazione delle immagini che l'algoritmo sostituisce gli strati più profondi (strati convoluzionali e densi) delle comuni reti convoluzionali, raggiungendo un'elevata accuratezza di classificazione, mentre costruisce strutture significativamente più piccole.L'algoritmo di apprendimento della struttura proposto richiede un piccolo costo computazionale e funziona in modo efficiente su una CPU desktop standard.
I regolarizzatori L1 e L2 sono strumenti critici nell'apprendimento automatico grazie alla loro capacità di semplificare le soluzioni. Tuttavia, l'imposizione di una forte regolarizzazione L1 o L2 con il metodo di discesa del gradiente fallisce facilmente, e questo limita la capacità di generalizzazione delle reti neurali sottostanti. Per comprendere questo fenomeno, studiamo come e perché l'allenamento fallisce per una forte regolarizzazione. Troviamo che esiste un livello di tolleranza della forza di regolarizzazione, dove l'apprendimento fallisce completamente se la forza di regolarizzazione va oltre.Proponiamo un metodo semplice ma nuovo, Delayed Strong Regularization, al fine di moderare il livello di tolleranza.I risultati degli esperimenti mostrano che il nostro approccio proposto raggiunge effettivamente una forte regolarizzazione per entrambi i regolarizzatori L1 e L2 e migliora sia la precisione che la sparsità su set di dati pubblici.Il nostro codice sorgente è pubblicato.
Nonostante le alte prestazioni della rete neurale, la mancanza di interpretabilità è stata il principale collo di bottiglia per il suo utilizzo sicuro nella pratica.In domini con alte poste in gioco (ad esempio, la diagnosi medica), ottenere intuizioni sulla rete è fondamentale per ottenere fiducia ed essere adottati.Uno dei modi per migliorare l'interpretabilità di una NN è quello di spiegare l'importanza di un particolare concetto (ad esempio, Questo lavoro ha lo scopo di fornire risposte quantitative al problema dell'importanza relativa dei concetti di interesse attraverso i vettori di attivazione concettuale (CAV); in particolare, questa struttura consente agli esperti di apprendimento non meccanici di esprimere i concetti di interesse e di testare le ipotesi utilizzando esempi (ad es, dimostriamo che i CAV possono essere appresi con un insieme relativamente piccolo di esempi.I test con CAV, ad esempio, possono rispondere se un particolare concetto (ad esempio, il genere) è più importante nel predire una determinata classe (ad esempio, il medico) rispetto ad altri insiemi di concetti.L'interpretazione con CAV non richiede alcun riaddestramento o modifica della rete.Mostriamo che vengono appresi molti livelli di concetti significativi (ad esempio, colore, texture, oggetti, occupazione di una persona), e presentiamo il ‗textit‖ di CAV - dove massimizziamo un'attivazione usando un insieme di immagini di esempio.Mostriamo come vari approfondimenti possono essere ottenuti dal test di importanza relativa con CAV.
Presentiamo una nuova famiglia di funzioni obiettivo, che definiamo il Conditional Entropy Bottleneck (CEB).Questi obiettivi sono motivati dal criterio della Minimum Necessary Information (MNI).Dimostriamo l'applicazione del CEB a compiti di classificazione. Mostriamo che il CEB dà: previsioni ben calibrate; forte rilevamento di esempi impegnativi fuori distribuzione e potenti esempi avversari whitebox; e sostanziale robustezza a tali avversari.Infine, riportiamo che il CEB non riesce a imparare da set di dati privi di informazioni, fornendo una possibile risoluzione al problema della generalizzazione osservato in Zhang et al. (2016).
L'apprendimento delle rappresentazioni profonde è diventato uno degli approcci più ampiamente adottati per la ricerca visiva, la raccomandazione e l'identificazione.Il recupero di tali rappresentazioni da un grande database è comunque computazionalmente impegnativo.Metodi approssimativi basati sull'apprendimento di rappresentazioni compatte, sono stati ampiamente esplorati per questo problema, come l'hashing sensibile alla località, la quantizzazione del prodotto, e PCA. In questo lavoro, in contrasto con l'apprendimento di rappresentazioni compatte, proponiamo di imparare rappresentazioni ad alta dimensione e sparse che hanno una capacità di rappresentazione simile alle incorporazioni dense, pur essendo più efficienti grazie alle operazioni di moltiplicazione delle matrici sparse che possono essere molto più veloci della moltiplicazione densa. Seguendo l'intuizione chiave che il numero di operazioni diminuisce quadraticamente con la sparsità delle incorporazioni, a condizione che le voci non nulle siano distribuite uniformemente attraverso le dimensioni, proponiamo un nuovo approccio per imparare tali incorporazioni sparse distribuite attraverso l'uso di una funzione di regolarizzazione accuratamente costruita che minimizza direttamente un rilassamento continuo del numero di operazioni in virgola mobile (FLOPs) sostenute durante il recupero.I nostri esperimenti mostrano che il nostro approccio è competitivo rispetto alle altre linee di base e produce un tradeoff velocità-vs-accuratezza simile o migliore su dataset pratici.
Nel dominio della fisica intuitiva, studiamo il compito di prevedere visivamente la stabilità delle torri di blocchi con l'obiettivo di comprendere e influenzare il ragionamento del modello. In primo luogo, introduciamo gli stetoscopi neurali come struttura per quantificare il grado di importanza di specifici fattori di influenza nelle reti profonde, nonché per promuovere attivamente e sopprimere le informazioni in modo appropriato. In questo modo, uniamo i concetti dell'apprendimento multitask e dell'addestramento con perdite ausiliarie e avversarie. Mostriamo che il modello di base è suscettibile di essere ingannato da spunti visivi errati, il che porta a un crollo delle prestazioni a livello di indovinelli casuali quando ci si allena su scenari in cui gli spunti visivi sono inversamente correlati alla stabilità. Al contrario, addestrandosi su un set di dati facile in cui gli spunti visivi sono correlati positivamente con la stabilità, il modello di base impara una distorsione che porta a scarse prestazioni su un set di dati più difficile. Usando uno stetoscopio avversario, la rete viene de-biasata con successo, portando a un aumento delle prestazioni dal 66% all'88%.
I modelli basati sul flusso come Real NVP sono un approccio estremamente potente alla stima della densità.Tuttavia, i modelli basati sul flusso esistenti sono limitati a trasformare le densità continue su uno spazio di input continuo in distribuzioni analogamente continue su variabili latenti continue.Questo li rende poco adatti a modellare e rappresentare strutture discrete nelle distribuzioni di dati, per esempio l'appartenenza di classe o simmetrie discrete.Per affrontare questa difficoltà, presentiamo un'architettura di flusso normalizzante che si basa sul partizionamento del dominio utilizzando funzioni localmente invertibili, e possiede sia reali che discrete valutate variabili latenti.  Questo approccio reale e discreto (RAD) mantiene le desiderabili proprietà di flusso normalizzante di campionamento esatto, inferenza esatta e probabilità analiticamente calcolabili, mentre allo stesso tempo permette la modellazione simultanea della struttura continua e discreta in una distribuzione di dati.
Indaghiamo la superficie di perdita delle reti neurali e dimostriamo che anche per le reti a uno strato nascosto con una "minima" non linearità, i rischi empirici hanno minimi locali spuri nella maggior parte dei casi. I nostri risultati indicano quindi che in generale "nessun minimo locale spurio" è una proprietà limitata alle reti lineari profonde, e le intuizioni ottenute dalle reti lineari potrebbero non essere robuste. Presentiamo anche un controesempio per attivazioni più generali (sigmoide, tanh, arctan, ReLU, ecc.), per le quali esiste un cattivo minimo locale. I nostri risultati fanno le ipotesi meno restrittive rispetto ai risultati esistenti sugli ottimi locali spuri nelle reti neurali.
I compiti che un agente dovrà risolvere spesso non sono noti durante l'addestramento; tuttavia, se l'agente conosce le proprietà dell'ambiente che consideriamo importanti, allora dopo aver imparato come le sue azioni influenzano tali proprietà, l'agente può essere in grado di utilizzare questa conoscenza per risolvere compiti complessi senza addestrarsi specificamente per essi. Dato un compito al momento del test che può essere espresso in termini di un set di attributi di destinazione e uno stato corrente, il nostro modello infonde gli attributi dello stato corrente e cerca su percorsi attraverso lo spazio degli attributi per ottenere un piano di alto livello, e poi usa la sua politica di basso livello per eseguire il piano. Mostriamo nei giochi grid-world e nell'impilamento di blocchi 3D che il nostro modello è in grado di generalizzare a compiti più lunghi e complessi al momento del test anche quando vede solo compiti brevi e semplici al momento della formazione.
La capacità di imparare nuovi concetti con piccole quantità di dati è un aspetto critico dell'intelligenza che si è rivelato impegnativo per i metodi di apprendimento profondo. Il meta-apprendimento è emerso come una tecnica promettente per sfruttare i dati dei compiti precedenti per consentire l'apprendimento efficiente di nuovi compiti. Tuttavia, la maggior parte degli algoritmi di meta-apprendimento richiede implicitamente che i compiti di meta-formazione siano mutuamente esclusivi, in modo che nessun modello singolo possa risolvere tutti i compiti contemporaneamente. Per esempio, quando si creano compiti per la classificazione di immagini a pochi scatti, il lavoro precedente utilizza un'assegnazione casuale per compito delle classi di immagini alle etichette di classificazione a N vie.Se questo non viene fatto, il meta-apprendente può ignorare i dati di addestramento del compito e imparare un singolo modello che esegue tutti i compiti di meta-formazione a zero scatti, ma non si adatta efficacemente alle nuove classi di immagini.  Questo requisito significa che l'utente deve fare molta attenzione nel progettare i compiti, per esempio mischiando le etichette o rimuovendo le informazioni di identificazione del compito dagli input.In alcuni domini, questo rende il meta-apprendimento completamente inapplicabile.In questo articolo, affrontiamo questa sfida progettando un obiettivo di meta-regolarizzazione usando la teoria dell'informazione che pone la precedenza sull'adattamento guidato dai dati.Questo fa sì che il meta-apprendista decida cosa deve essere appreso dai dati di allenamento del compito e cosa deve essere dedotto dall'input del test del compito. Così facendo, il nostro algoritmo può utilizzare con successo i dati da compiti non mutualmente esclusivi per adattarsi in modo efficiente a nuovi compiti. Dimostriamo la sua applicabilità ad algoritmi di meta-apprendimento contestuali e basati sul gradiente, e lo applichiamo in impostazioni pratiche in cui l'applicazione del meta-apprendimento standard è stata difficile. 
L'apprendimento di una regola di aggiornamento efficiente dai dati che promuove l'apprendimento rapido di nuovi compiti dalla stessa distribuzione rimane un problema aperto nel meta-apprendimento.Tipicamente, i lavori precedenti hanno affrontato questo problema o tentando di addestrare una rete neurale che produce direttamente gli aggiornamenti o tentando di imparare migliori inizializzazioni o fattori di scala per una regola di aggiornamento basata sul gradiente. Entrambi questi approcci pongono delle sfide: da un lato, producendo direttamente un aggiornamento si rinuncia a un utile bias induttivo e può facilmente portare a un comportamento non convergente; dall'altro, gli approcci che cercano di controllare una regola di aggiornamento basata sul gradiente ricorrono tipicamente al calcolo dei gradienti attraverso il processo di apprendimento per ottenere i loro meta-gradienti, portando a metodi che non possono scalare oltre l'adattamento a pochi colpi di compito. In questo lavoro proponiamo Warped Gradient Descent (WarpGrad), un metodo che interseca questi approcci per mitigare le loro limitazioni.WarpGrad meta-apprende una matrice di precondizionamento efficientemente parametrizzata che facilita la discesa del gradiente attraverso la distribuzione dei compiti.Il precondizionamento nasce dall'interleaving di strati non lineari, indicati come warp-layers, tra gli strati di un task-learner. WarpGrad è computazionalmente efficiente, facile da implementare, e può scalare a problemi di meta-apprendimento arbitrariamente grandi. Forniamo un'interpretazione geometrica dell'approccio e valutiamo la sua efficacia in una varietà di impostazioni, compreso l'apprendimento a pochi colpi, standard supervisionato, continuo e di rinforzo.
Negli ultimi anni, gli approcci delle reti neurali profonde sono stati ampiamente adottati per i compiti di apprendimento automatico, compresa la classificazione. Tuttavia, hanno dimostrato di essere vulnerabili alle perturbazioni avversarie: piccole perturbazioni accuratamente elaborate possono causare una classificazione errata delle immagini legittime.Proponiamo Defense-GAN, una nuova struttura che sfrutta la capacità espressiva dei modelli generativi per difendere le reti neurali profonde da tali attacchi. Il nostro metodo proposto può essere usato con qualsiasi modello di classificazione e non modifica la struttura del classificatore o la procedura di addestramento; può anche essere usato come difesa contro qualsiasi attacco, dato che non presuppone la conoscenza del processo di generazione degli esempi avversari.Mostriamo empiricamente che Defense-GAN è costantemente efficace contro diversi metodi di attacco e migliora le strategie di difesa esistenti.
Questi modelli sono tipicamente addestrati usando SGD con un campionamento casuale di coppie non osservate, con una dimensione del campione che cresce quadraticamente con la dimensione del corpus, rendendolo costoso da scalare.Proponiamo nuovi metodi efficienti per addestrare questi modelli senza dover campionare coppie non osservate. Ispirato alla fattorizzazione della matrice, il nostro approccio si basa sull'aggiunta di una penalità quadratica globale e sull'espressione di questo termine come prodotto interno di due Gramiani generalizzati.Mostriamo che il gradiente di questo termine può essere calcolato in modo efficiente mantenendo le stime dei Gramiani, e sviluppiamo schemi di riduzione della varianza per migliorare la qualità delle stime.Conduciamo esperimenti su larga scala che mostrano un miglioramento significativo sia nel tempo di formazione che nelle prestazioni di generalizzazione rispetto ai metodi di campionamento.
Per la comprensione del linguaggio naturale (NLU) la tecnologia per essere massimamente utile, deve essere in grado di elaborare la lingua in un modo che non è esclusivo ad un singolo compito, genere, o dataset.In perseguimento di questo obiettivo, introduciamo il benchmark General Language Understanding Evaluation (GLUE), una raccolta di strumenti per valutare le prestazioni dei modelli attraverso una serie diversificata di compiti NLU esistenti.Includendo compiti con dati di formazione limitata, GLUE è progettato per favorire e incoraggiare i modelli che condividono la conoscenza linguistica generale tra i compiti. GLUE include anche una suite di test diagnostici fatti a mano che consente un'analisi linguistica dettagliata dei modelli. valutiamo le linee di base basate sui metodi attuali per l'apprendimento di trasferimento e rappresentazione e troviamo che l'addestramento multi-task su tutte le attività esegue meglio dell'addestramento di un modello separato per attività. tuttavia, la bassa prestazione assoluta del nostro miglior modello indica la necessità di migliorare i sistemi NLU generali.
Una varietà di problemi di controllo cooperativo multi-agente richiede che gli agenti raggiungano obiettivi individuali mentre contribuiscono al successo collettivo.Questa impostazione multi-agente multi-obiettivo pone difficoltà per i recenti algoritmi, che principalmente mirano a impostazioni con una singola ricompensa globale, a causa di due nuove sfide: esplorazione efficiente per imparare sia il raggiungimento dell'obiettivo individuale che la cooperazione per il successo degli altri, e assegnazione di crediti per le interazioni tra azioni e obiettivi di diversi agenti. Per affrontare entrambe le sfide, ristrutturiamo il problema in un nuovo curriculum a due fasi, in cui il raggiungimento dell'obiettivo di un singolo agente viene appreso prima di imparare la cooperazione tra più agenti, e deriviamo un nuovo gradiente di politica multi-agente con una funzione di credito per l'assegnazione localizzata del credito. L'architettura completa, chiamata CM3, impara significativamente più velocemente degli adattamenti diretti degli algoritmi esistenti su tre impegnativi problemi multi-goal multi-agente: navigazione cooperativa in formazioni difficili, negoziazione di cambi di corsia multi-veicolo nel simulatore di traffico SUMO, e cooperazione strategica in un ambiente Checkers.
Identifichiamo due problemi con la famiglia di algoritmi basati sul framework Adversarial Imitation Learning.Il primo problema è la distorsione implicita presente nelle funzioni di ricompensa utilizzate in questi algoritmi.Mentre queste distorsioni potrebbero funzionare bene per alcuni ambienti, possono anche portare a un comportamento sub-ottimale in altri.In secondo luogo, anche se questi algoritmi possono imparare da poche dimostrazioni di esperti, richiedono un numero proibitivo di interazioni con l'ambiente per imitare l'esperto per molte applicazioni del mondo reale. Al fine di affrontare questi problemi, proponiamo un nuovo algoritmo chiamato Discriminator-Actor-Critic che utilizza l'apprendimento di rinforzo off-policy per ridurre la complessità del campione di interazioni politica-ambiente di un fattore medio di 10. Inoltre, poiché la nostra funzione di ricompensa è progettata per essere imparziale, possiamo applicare il nostro algoritmo a molti problemi senza fare alcuna regolazione specifica per il compito.
Le reti a capsula hanno mostrato risultati incoraggianti su set di dati di defacto benchmark di computer vision come MNIST, CIFAR e smallNORB, anche se devono ancora essere testati su compiti in cui (1) le entità rilevate hanno intrinsecamente rappresentazioni interne più complesse e (2) ci sono pochissime istanze per classe da cui imparare e (3) dove la classificazione point-wise non è adatta.Quindi, questo articolo esegue esperimenti sulla verifica del volto in entrambe le impostazioni controllate e non controllate che insieme affrontano questi punti. Nel fare ciò, introduciamo le reti di capsule siamesi, una nuova variante che può essere utilizzata per compiti di apprendimento a coppie. Il modello è addestrato utilizzando la perdita contrastiva con le caratteristiche di posa codificate dalla capsula l2. Troviamo che le reti di capsule siamesi si comportano bene contro le linee di base forti su entrambi i set di dati di apprendimento a coppie, dando i migliori risultati nell'impostazione di apprendimento a pochi scatti in cui le coppie di immagini nel set di prova contengono soggetti non visti.
Gli animali eccellono nell'adattare le loro intenzioni, l'attenzione e le azioni all'ambiente, rendendoli notevolmente efficienti nell'interagire con un mondo esterno ricco, imprevedibile e in continua evoluzione, una proprietà che attualmente manca alle macchine intelligenti. Tale proprietà di adattamento si basa fortemente sulla neuromodulazione cellulare, il meccanismo biologico che controlla dinamicamente le proprietà intrinseche dei neuroni e la risposta agli stimoli esterni in modo dipendente dal contesto. I risultati mostrano che la neuromodulazione è in grado di adattare un agente a diversi compiti e che gli approcci basati sulla neuromodulazione forniscono un modo promettente per migliorare l'adattamento dei sistemi artificiali.
L'operatore di convoluzione è il cuore delle reti neurali convoluzionali (CNN) e occupa il maggior costo di calcolo.Per rendere le CNN più efficienti, sono stati proposti molti metodi per progettare reti leggere o comprimere i modelli.Anche se sono state proposte alcune strutture di rete efficienti, come MobileNet o ShuffleNet, troviamo che esistono ancora informazioni ridondanti tra i kernel di convoluzione. Per affrontare questo problema, proponiamo un nuovo metodo di convoluzione dinamica chiamato \textbf{DyNet} in questo documento, che può generare in modo adattivo i kernel di convoluzione basati sui contenuti dell'immagine.Per dimostrare l'efficacia, applichiamo DyNet su più CNN all'avanguardia. In particolare, per ShuffleNetV2 (1.0), MobileNetV2 (1.0), ResNet18 e ResNet50, DyNet riduce rispettivamente del 40,0%, 56,7%, 68,2% e 72,4% i FLOP mentre la precisione Top-1 su ImageNet cambia solo di +1,0%, -0,27%, -0,6% e -0,08%. Nel frattempo, DyNet accelera ulteriormente la velocità di inferenza di MobileNetV2 (1.0), ResNet18 e ResNet50 di 1.87x, 1.32x e 1.48x su piattaforma CPU rispettivamente.Per verificare la scalabilità, applichiamo DyNet anche al compito di segmentazione, i risultati mostrano che DyNet può ridurre il 69.3% FLOPs mantenendo la media IoU sul compito di segmentazione.
Il funzionamento delle reti neurali profonde su dispositivi con risorse limitate richiede la riduzione delle loro impronte di memoria e dei requisiti computazionali. In questo articolo introduciamo un metodo di addestramento, chiamato look-up table quantization (LUT-Q), che impara un dizionario e assegna ogni peso a uno dei valori del dizionario.Mostriamo che questo metodo è molto flessibile e che molte altre tecniche possono essere viste come casi speciali di LUT-Q. Per esempio, possiamo vincolare il dizionario addestrato con LUT-Q per generare reti con matrici di peso sfrondate o limitare il dizionario a potenze di due per evitare la necessità di moltiplicazioni.Per ottenere reti completamente prive di moltiplicatore, introduciamo anche una versione senza moltiplicatore della normalizzazione batch.Esperimenti estesi su compiti di riconoscimento di immagini e di rilevamento di oggetti mostrano che LUT-Q raggiunge costantemente prestazioni migliori di altri metodi con la stessa larghezza di bit di quantizzazione.
Una conseguenza indesiderata della condivisione delle caratteristiche è l'adattamento del modello a compiti correlati all'interno del set di dati, definito trasferimento negativo.  In questo articolo, rivisitiamo il problema del trasferimento negativo nell'impostazione multitask e scopriamo che i suoi effetti corrosivi sono applicabili a una vasta gamma di modelli lineari e non lineari, comprese le reti neurali. Proponiamo un approccio di formazione avversaria per mitigare gli effetti del trasferimento negativo visualizzando il problema in un'impostazione di adattamento del dominio.Infine, i risultati empirici sulla previsione degli attributi multi-task sui dataset AWA e CUB convalidano ulteriormente la necessità di correggere la condivisione negativa in modo end-to-end.
I recenti risultati teorici e sperimentali suggeriscono la possibilità di utilizzare l'hardware quantistico attuale e futuro in compiti di campionamento impegnativi.In questo articolo, introduciamo l'apprendimento di rinforzo basato sull'energia libera (FERL) come applicazione dell'hardware quantistico.Proponiamo un metodo per l'elaborazione delle configurazioni di spin di qubit misurate da un annealer quantistico nell'approssimazione dell'energia libera di una macchina quantistica di Boltzmann (QBM). I risultati sperimentali mostrano che la nostra tecnica è un metodo promettente per sfruttare la potenza del campionamento quantistico nei compiti di apprendimento di rinforzo.
I modelli di apprendimento profondo sono vulnerabili agli esempi avversari costruiti applicando perturbazioni impercettibili all'uomo su input benigni. Tuttavia, sotto l'impostazione black-box, la maggior parte degli avversari esistenti hanno spesso una scarsa trasferibilità per attaccare altri modelli di difesa. In questo lavoro, dalla prospettiva di considerare la generazione di esempi avversari come un processo di ottimizzazione, proponiamo due nuovi metodi per migliorare la trasferibilitÃ degli esempi avversari, cioÃ¨ il metodo Nesterov Iterative Fast Gradient Sign (NI-FGSM) e il metodo di attacco Scale-Invariant (SIM).NI-FGSM mira ad adattare il gradiente accelerato di Nesterov negli attacchi iterativi in modo da guardare effettivamente avanti e migliorare la trasferibilitÃ degli esempi avversari. Mentre SIM si basa sulla nostra scoperta sulla proprietà scale-invariant dei modelli di apprendimento profondo, per cui facciamo leva per ottimizzare le perturbazioni avversarie sulle copie di scala delle immagini di input in modo da evitare "overfitting" sul modello white-box che viene attaccato e generare esempi avversari più trasferibili. NI-FGSM e SIM possono essere naturalmente integrati per costruire un attacco robusto basato sul gradiente per generare esempi avversari più trasferibili contro i modelli di difesa. I risultati empirici sul set di dati ImageNet dimostrano che i nostri metodi di attacco mostrano una maggiore trasferibilità e raggiungono tassi di successo più elevati rispetto agli attacchi basati sul gradiente all'avanguardia.
I pesi e le attivazioni a bassa larghezza di bit sono un modo efficace per combattere la crescente necessità di memoria e potenza di calcolo delle reti neurali profonde. In questo lavoro, presentiamo un metodo di formazione probabilistico per le reti neurali con pesi e attivazioni binarie, chiamato PBNet. Abbracciando la stocasticità durante l'addestramento, aggiriamo la necessità di approssimare il gradiente di funzioni per le quali la derivata è quasi sempre zero, come $\textrm{sign}(\cdot)$, ottenendo comunque una Rete Neurale completamente binaria al momento del test. Inoltre, permette in qualsiasi momento previsioni d'insieme per migliorare le prestazioni e le stime di incertezza campionando dalla distribuzione dei pesi. Poiché tutte le operazioni in uno strato della PBNet operano su variabili casuali, introduciamo versioni stocastiche della Normalizzazione Batch e del max pooling, che si trasferiscono bene ad una rete deterministica al momento del test.  Valutiamo due metodi di addestramento correlati per la PBNet: uno in cui le distribuzioni di attivazione sono propagate in tutta la rete, e uno in cui le attivazioni binarie sono campionate in ogni strato. I nostri esperimenti indicano che il campionamento delle attivazioni binarie è un elemento importante per l'addestramento stocastico delle reti neurali binarie.
L'obiettivo dei modelli generativi è quello di modellare la distribuzione dei dati sottostante al set di dati basato sul campione.La nostra intuizione è che un modello accurato dovrebbe in linea di principio includere anche il set di dati basato sul campione come parte della sua distribuzione di probabilità indotta.Per indagare su questo, guardiamo ai modelli generativi completamente addestrati usando il framework GenerativeAdversarial Networks (GAN) e analizziamo il generatore risultante sulla sua capacità di memorizzare il set di dati.Inoltre, mostriamo che la dimensione dello spazio iniziale è fondamentale per consentire una ricostruzione accurata dei dati di addestramento. Questo ci dà un collegamento alla teoria della compressione, dove gli autocodificatori (AE) vengono utilizzati per ridurre le capacità di ricostruzione del nostro modello generativo.Qui, osserviamo risultati simili al compromesso percezione-distorsione (Blau & Michaeli(2018)). Dato un piccolo spazio latente, l'AE produce una bassa qualità e il GAN produce output di alta qualità da un punto di vista percettivo.Al contrario, l'errore di distorsione è minore per l'AE.Aumentando la dimensionalità dello spazio latente la distorsione diminuisce per entrambi i modelli, ma la qualità percettiva aumenta solo per l'AE.
Affrontiamo il problema della verifica dell'autore in set aperti, un compito di classificazione che consiste nell'attribuire testi di autore sconosciuto a un determinato autore quando i documenti sconosciuti nel set di prova sono esclusi dal set di addestramento. Presentiamo un processo di costruzione del modello end-to-end che è universalmente applicabile a un'ampia varietà di corpora con poca o nessuna modifica o messa a punto, che si basa sull'apprendimento per trasferimento di un modello linguistico profondo e utilizza una rete generativa avversaria e una serie di tecniche di aumento del testo per migliorare la capacità di generalizzazione del modello. Il modello linguistico codifica i documenti di autori noti e sconosciuti in uno spazio invariante al dominio, allineando le coppie di documenti come input al classificatore, pur mantenendoli separati.Le embeddings risultanti sono utilizzate per addestrare un insieme di reti neurali ricorrenti e quasi ricorrenti. L'intera pipeline è bidirezionale; i risultati dei passaggi in avanti e indietro sono mediati.Eseguiamo esperimenti su quattro set di dati tradizionali di verifica della paternità, una raccolta di documenti di apprendimento automatico estratti dal web, e un grande set di dati Amazon-Reviews.I risultati sperimentali superano la baseline e le attuali tecniche all'avanguardia, convalidando l'approccio proposto.
Consideriamo di affrontare un problema di RL a singolo agente distribuendolo a $n$ apprendenti.Questi apprendenti, chiamati consulenti, cercano di risolvere il problema da un punto di vista diverso.Il loro consiglio, sotto forma di valori di azione, viene poi comunicato a un aggregatore, che ha il controllo del sistema. Mostriamo che il metodo di pianificazione locale per i consulenti è critico e che nessuno di quelli trovati in letteratura è impeccabile: la pianificazione \textit{egocentrica} sovrastima i valori degli stati in cui gli altri consulenti non sono d'accordo, e la pianificazione \textit{agnostica} è inefficiente intorno alle zone di pericolo.Introduciamo un nuovo approccio chiamato \textit{empatico} e discutiamo i suoi aspetti teorici.Esaminiamo empiricamente e validiamo i nostri risultati teorici su un compito di raccolta frutta.
Presentiamo una struttura ibrida che sfrutta il trade-off tra precisione temporale e di frequenza nelle rappresentazioni audio per migliorare le prestazioni del compito di miglioramento del discorso.Mostriamo innanzitutto che gli approcci convenzionali che utilizzano rappresentazioni specifiche come raw-audio e spettrogrammi sono ciascuno efficace nel colpire diversi tipi di rumore.Integrando entrambi gli approcci, il nostro modello può imparare caratteristiche multi-scala e multi-dominio, rimuovendo efficacemente il rumore esistente su diverse regioni sullo spazio tempo-frequenza in modo complementare.I risultati sperimentali mostrano che il modello ibrido proposto produce prestazioni e robustezza migliori rispetto all'utilizzo di ciascun modello singolarmente.
Dopo aver introdotto i concetti di test statistici, passiamo in rassegna i test statistici rilevanti e li confrontiamo empiricamente in termini di tasso di falsi positivi e potenza statistica in funzione della dimensione del campione (numero di semi) e della dimensione dell'effetto. Oltre alle simulazioni, confrontiamo le distribuzioni empiriche ottenute eseguendo Soft-Actor Critic e Twin-Delayed Deep Deterministic Policy Gradient su Half-Cheetah.Concludiamo fornendo linee guida e codice per eseguire confronti rigorosi delle prestazioni degli algoritmi RL.
Nell'Information Bottleneck (IB), quando si accorda la forza relativa tra i termini di compressione e predizione, come si comportano i due termini e qual è la loro relazione con il dataset e la rappresentazione appresa? IB_β[p(z|x)] = I(X; Z) - βI(Y; Z) definito sulla distribuzione di codifica p(z|x) per l'input X, l'obiettivo Y e la rappresentazione Z, dove si osservano salti improvvisi di dI(Y; Z)/dβ e di accuratezza della predizione all'aumentare di β. Introduciamo una definizione per le transizioni di fase IB come un cambiamento qualitativo del paesaggio di perdita IB, e mostriamo che le transizioni corrispondono all'inizio dell'apprendimento di nuove classi. Utilizzando il calcolo delle variazioni del secondo ordine, deriviamo una formula che fornisce una condizione pratica per le transizioni di fase IB, e tracciamo la sua connessione con la matrice di informazione di Fisher per modelli parametrizzati. Forniamo due prospettive per comprendere la formula, rivelando che ogni transizione di fase IB sta trovando una componente di massima correlazione (non lineare) tra X e Y ortogonale alla rappresentazione appresa, in stretta analogia con l'analisi di correlazione canonica (CCA) nelle impostazioni lineari.Sulla base della teoria, presentiamo un algoritmo per scoprire i punti di transizione di fase.Infine, verifichiamo che la nostra teoria e l'algoritmo prevedono accuratamente le transizioni di fase nei dataset categorici, prevedono l'inizio dell'apprendimento di nuove classi e la difficoltà di classe in MNIST, e prevedono le transizioni di fase prominenti in CIFAR10.
L'adattamento locale della funzione di attivazione si ottiene introducendo iper-parametri scalabili in ogni strato (layer-wise) e per ogni neurone separatamente (neuron-wise), e poi ottimizzandolo usando l'algoritmo di discesa del gradiente stocastico. L'introduzione della funzione di attivazione neurone-saggio agisce come una funzione di attivazione vettoriale in contrapposizione alla tradizionale funzione di attivazione scalare data da attivazioni fisse, globali e di livello; al fine di aumentare ulteriormente la velocità di formazione, un termine di recupero della pendenza basato sulla pendenza di attivazione viene aggiunto nella funzione di perdita, che accelera ulteriormente la convergenza, riducendo così il costo di formazione. Per gli esperimenti numerici, una funzione discontinua non lineare viene approssimata usando una rete neurale profonda con funzioni di attivazione adattative locali a livello di strato e di neurone con e senza il termine di recupero della pendenza e confrontata con la sua controparte globale.Inoltre, la soluzione dell'equazione di Burgers non lineare, che mostra gradienti ripidi, è anche ottenuta usando i metodi proposti. Dal punto di vista teorico, dimostriamo che nel metodo proposto gli algoritmi di discesa del gradiente non sono attratti da punti critici sub-ottimali o minimi locali in condizioni pratiche sull'inizializzazione e sul tasso di apprendimento. Inoltre, le funzioni di attivazione adattive proposte con il recupero della pendenza hanno dimostrato di accelerare il processo di formazione nei benchmark standard di apprendimento profondo utilizzando i set di dati CIFAR-10, CIFAR-100, SVHN, MNIST, KMNIST, Fashion-MNIST e Semeion con e senza aumento dei dati.
L'apprendimento nei modelli di processo gaussiano avviene attraverso l'adattamento degli iperparametri della media e della funzione di covarianza. L'approccio classico comporta la massimizzazione della verosimiglianza marginale che produce stime a punto fisso (un approccio chiamato massima verosimiglianza di tipo II o ML-II). Una procedura di apprendimento alternativa consiste nell'inferire il posteriore sugli iperparametri in una specifica gerarchica di GP che chiamiamo Fully Bayesian Gaussian Process Regression (GPR). Questo lavoro considera due approssimazioni al posteriore intrattabile degli iperparametri, 1) Hamiltonian Monte Carlo (HMC) che produce un'approssimazione basata sul campionamento e 2) Variational Inference (VI) dove il posteriore degli iperparametri è approssimato da una gaussiana fattorizzata (mean-field) o da una gaussiana full rank che tiene conto delle correlazioni tra gli iperparametri.
Consideriamo il problema dell'uso di modelli variazionali latente-variabile per la compressione dei dati.Affinché tali modelli producano una sequenza binaria compressa, che è la rappresentazione universale dei dati in un mondo digitale, la rappresentazione latente deve essere sottoposta a codifica di entropia.La codifica di intervallo come tecnica di codifica di entropia è ottimale, ma può fallire catastroficamente se il calcolo della priorità differisce anche leggermente tra il lato di invio e quello di ricezione. Sfortunatamente, questo è uno scenario comune quando viene utilizzata la matematica in virgola mobile e il mittente e il ricevitore operano su diverse piattaforme hardware o software, poiché l'arrotondamento numerico è spesso dipendente dalla piattaforma.Proponiamo di utilizzare le reti di interi come soluzione universale a questo problema, e dimostriamo che consentono una codifica e decodifica affidabile e multipiattaforma delle immagini utilizzando modelli variazionali.
Le reti neurali alimentate con una memoria esterna simulano i comportamenti del computer.Questi modelli, che usano la memoria per immagazzinare i dati per un controller neurale, possono imparare algoritmi e altri compiti complessi.In questo articolo, introduciamo una nuova memoria per immagazzinare i pesi per il controller, analoga alla memoria a programma memorizzato nelle moderne architetture dei computer. Il modello proposto, soprannominato Neural Stored-program Memory, aumenta le attuali reti neurali dotate di memoria, creando macchine differenziabili che possono cambiare programma nel tempo, adattarsi a contesti variabili e quindi assomigliare completamente alla Universal Turing Machine. Una vasta gamma di esperimenti dimostra che le macchine risultanti non solo eccellono in problemi algoritmici classici, ma hanno anche un potenziale per compiti compositivi, continui, di apprendimento a pochi colpi e di risposta alle domande.
Questa procedura ha successo per la discesa del gradiente stocastico (SGD), SGD con slancio, slancio di Nesterov e Adam, e raggiunge accuratezze di test equivalenti dopo lo stesso numero di epoche di addestramento, ma con meno aggiornamenti dei parametri, portando a un maggiore parallelismo e a tempi di addestramento più brevi. Possiamo ridurre ulteriormente il numero di aggiornamenti dei parametri aumentando il tasso di apprendimento $ $epsilon$ e scalando la dimensione del batch $B \propto \epsilon$.Infine, si può aumentare il coefficiente di slancio $m$ e scalare $B \propto 1/(1-m)$, anche se questo tende a ridurre leggermente l'accuratezza del test.Fondamentalmente, le nostre tecniche ci permettono di riutilizzare i programmi di formazione esistenti per la formazione di grandi batch senza la regolazione degli iper-parametri.Addestriamo ResNet-50 su ImageNet al 76,1% di precisione di validazione in meno di 30 minuti.
I modelli tradizionali per la risposta alle domande ottimizzano utilizzando la perdita di entropia incrociata, che incoraggia le risposte esatte al costo di penalizzare le risposte vicine o sovrapposte che a volte sono ugualmente accurate. Noi proponiamo un obiettivo misto che combina la perdita di entropia incrociata con l'apprendimento autocritico delle politiche, utilizzando i premi derivati dalla sovrapposizione di parole per risolvere il disallineamento tra la metrica di valutazione e l'obiettivo di ottimizzazione. Oltre all'obiettivo misto, introduciamo un codificatore profondo di coattenzione residua che si ispira al recente lavoro nell'autoattenzione profonda e nelle reti residue.Le nostre proposte migliorano le prestazioni del modello attraverso i tipi di domande e la lunghezza degli input, specialmente per le domande lunghe che richiedono la capacità di catturare le dipendenze a lungo termine.Sul dataset Stanford Question Answering, il nostro modello raggiunge lo stato dell'arte dei risultati con il 75,1% di precisione di corrispondenza esatta e l'83,1% F1, mentre l'ensemble ottiene il 78,9% di precisione di corrispondenza esatta e l'86,0% F1.
La ricerca di architetture neurali (NAS) ha raggiunto un successo rivoluzionario in un gran numero di applicazioni negli ultimi anni.Potrebbe essere il momento di fare un passo indietro e analizzare gli aspetti buoni e cattivi nel campo della NAS.Una varietà di algoritmi cerca architetture sotto diversi spazi di ricerca.Queste architetture cercate sono addestrate utilizzando diverse configurazioni, ad es, In questo lavoro, proponiamo un'estensione di NAS-Bench-101: NAS-Bench-201 con uno spazio di ricerca diverso, risultati su più set di dati e più informazioni diagnostiche.NAS-Bench-201 ha uno spazio di ricerca fisso e fornisce un benchmark unificato per quasi tutti gli algoritmi NAS aggiornati. Il design del nostro spazio di ricerca è ispirato a quello utilizzato nei più popolari algoritmi di ricerca basati su celle, dove una cella è rappresentata come un grafo aciclico diretto.Ogni bordo qui è associato a un'operazione selezionata da un set di operazioni predefinito.Per essere applicabile a tutti gli algoritmi NAS, lo spazio di ricerca definito in NAS-Bench-201 include tutte le possibili architetture generate da 4 nodi e 5 opzioni di operazioni associate, il che risulta in 15.625 candidati neurali in totale. Il registro di addestramento utilizzando la stessa configurazione e le prestazioni per ogni candidato di architettura sono forniti per tre set di dati.Questo permette ai ricercatori di evitare inutili addestramenti ripetitivi per l'architettura selezionata e concentrarsi esclusivamente sull'algoritmo di ricerca stesso.Il tempo di addestramento risparmiato per ogni architettura migliora anche ampiamente l'efficienza della maggior parte degli algoritmi NAS e presenta una comunità NAS più computazionalmente amichevole per una più ampia gamma di ricercatori. Forniamo ulteriori informazioni diagnostiche come la perdita e l'accuratezza a grana fine, che possono dare ispirazioni a nuovi progetti di algoritmi NAS.A ulteriore sostegno della proposta NAS-Bench-102, l'abbiamo analizzata da molti aspetti e abbiamo fatto un benchmark di 10 algoritmi NAS recenti, che verificano la sua applicabilità.
I Generative Adversarial Networks (GANs) sono uno degli strumenti più popolari per l'apprendimento di complesse distribuzioni ad alta dimensione.Tuttavia, le proprietà di generalizzazione dei GANs non sono state ben comprese.In questo articolo, analizziamo la generalizzazione dei GANs in impostazioni pratiche.Mostriamo che i discriminatori addestrati su set di dati discreti con la perdita GAN originale hanno scarsa capacità di generalizzazione e non approssimano il discriminatore teoricamente ottimale. Proponiamo una penalità a gradiente centrato sullo zero per migliorare la generalizzazione del discriminatore spingendolo verso il discriminatore ottimale.La penalità garantisce la generalizzazione e la convergenza di GANs.Experiments su set di dati sintetici e su larga scala verificano la nostra analisi teorica.
Le reti generative avversarie (GAN) sono una classe di modelli generativi profondi che mirano ad apprendere una distribuzione di destinazione in modo non supervisionato. Mentre sono state applicate con successo a molti problemi, l'addestramento di una GAN è un compito notoriamente impegnativo e richiede una quantità significativa di regolazione degli iperparametri, l'ingegneria dell'architettura neurale e una quantità non banale di "trucchi". Il successo in molte applicazioni pratiche accoppiato con la mancanza di una misura per quantificare le modalità di fallimento delle GAN ha portato a una pletora di perdite proposte, schemi di regolarizzazione e normalizzazione, e architetture neurali.In questo lavoro prendiamo una visione sobria dello stato attuale delle GAN da una prospettiva pratica.Riproduciamo lo stato attuale dell'arte e andiamo oltre l'esplorazione equa del panorama GAN.Discutiamo le insidie comuni e i problemi di riproducibilità, open-source il nostro codice su Github, e forniamo modelli pre-addestrati su TensorFlow Hub.
Come l'apprendimento di rinforzo profondo (RL) è applicato a più compiti, c'è la necessità di visualizzare e comprendere il comportamento degli agenti appresi.Le mappe di salienza spiegano il comportamento dell'agente evidenziando le caratteristiche dello stato di input che sono più rilevanti per l'agente nel prendere un'azione. Gli attuali approcci basati sulla perturbazione per calcolare la salienza spesso evidenziano regioni dell'input che non sono rilevanti per l'azione intrapresa dall'agente.Il nostro approccio genera mappe di salienza più mirate bilanciando due aspetti (specificità e rilevanza) che catturano diversi desiderata di salienza.Il primo cattura l'impatto della perturbazione sulla relativa ricompensa attesa dell'azione da spiegare.  Il secondo pondera le caratteristiche irrilevanti che alterano le ricompense relative attese di azioni diverse dall'azione da spiegare.  Confrontiamo il nostro approccio con approcci esistenti su agenti addestrati a giocare a giochi da tavolo (Chess e Go) e giochi Atari (Breakout, Pong e Space Invaders).  Mostriamo attraverso esempi illustrativi (Chess, Atari, Go), studi umani (Chess), e metodi di valutazione automatica (Chess) che il nostro approccio genera mappe di salienza che sono più interpretabili per gli umani rispetto agli approcci esistenti.
Per capire il lavoro interno delle reti neurali profonde e fornire possibili spiegazioni teoriche, studiamo le rappresentazioni profonde attraverso il non addestrato, peso casuale CNN-DCN architettura.Come un AutoEncoder convoluzionale, CNN indica la porzione di una rete neurale convoluzionale dall'ingresso a un livello convoluzionale intermedio, e DCN indica la porzione deconvolutiva corrispondente.Rispetto alla formazione DCN per CNN pre-addestrato, la formazione del DCN per CNN a peso casuale converge più rapidamente e produce una ricostruzione di immagine di qualità superiore. Poi, cosa succede per il CNN-DCN casuale complessivo? Otteniamo risultati intriganti che l'immagine può essere ricostruita con buona qualità.Per ottenere una maggiore comprensione sulla rappresentazione casuale intermedia, indaghiamo l'impatto della larghezza della rete rispetto alla profondità, il numero di canali casuali, e la dimensione dei kernel casuali sulla qualità della ricostruzione, e forniamo giustificazioni teoriche sulle osservazioni empiriche.Forniamo inoltre un'applicazione di trasferimento di stile veloce utilizzando l'architettura CNN-DCN a peso casuale per mostrare il potenziale della nostra osservazione.
 L'attuale trade-off tra profondità e costo computazionale rende difficile l'adozione di reti neurali profonde per molte applicazioni industriali, soprattutto quando la potenza di calcolo è limitata.Qui, siamo ispirati dall'idea che, mentre le embeddings più profonde sono necessarie per discriminare campioni difficili, un gran numero di campioni può essere ben discriminato attraverso embeddings molto meno profonde. In questo studio, introduciamo il concetto di cancelli decisionali (d-gate), moduli addestrati per decidere se un campione deve essere proiettato in un'embedding più profondo o se una predizione anticipata può essere fatta al d-gate, permettendo così il calcolo di rappresentazioni dinamiche a diverse profondità.  I moduli d-gate proposti possono essere integrati con qualsiasi rete neurale profonda e riducono il costo computazionale medio delle reti neurali profonde mantenendo l'accuratezza della modellazione. I risultati sperimentali mostrano che l'utilizzo dei moduli d-gate proposti ha portato a un ~38% di accelerazione e ~39% di riduzione FLOPS su ResNet-101 e ~46% di accelerazione e $\sim$36\% di riduzione FLOPS su DenseNet-201 addestrato sul dataset CIFAR10 con solo ~2% di calo della precisione.
Questo lavoro fornisce un ulteriore passo avanti nella comprensione teorica delle reti neurali: consideriamo le reti neurali con uno strato nascosto e dimostriamo che quando si imparano funzioni simmetriche, si possono scegliere le condizioni iniziali in modo che l'addestramento standard SGD produca efficientemente garanzie di generalizzazione; lo verifichiamo empiricamente e dimostriamo che ciò non vale quando le condizioni iniziali sono scelte a caso.
La ricerca di architetture mira a trovare automaticamente architetture neurali che siano competitive con le architetture progettate da esperti umani.Mentre gli approcci recenti hanno raggiunto prestazioni predittive allo stato dell'arte per il riconoscimento delle immagini, essi sono problematici sotto vincoli di risorse per due motivi: (1) le architetture neurali trovate sono ottimizzate solo per alte prestazioni predittive, senza penalizzare l'eccessivo consumo di risorse; (2) la maggior parte dei metodi di ricerca di architetture richiede vaste risorse computazionali. Noi affrontiamo la prima carenza proponendo LEMONADE, un algoritmo evolutivo per la ricerca di architetture multi-obiettivo che permette di approssimare il fronte di Pareto delle architetture sotto molteplici obiettivi, come le prestazioni predittive e il numero di parametri, in una singola esecuzione del metodo. Affrontiamo la seconda lacuna proponendo un meccanismo di eredità lamarckiana per LEMONADE che genera reti di figli che sono avviati a caldo con le prestazioni predittive dei loro genitori addestrati, utilizzando operatori di morfismo di rete (approssimati) per generare i figli. La combinazione di questi due contributi permette di trovare modelli che sono alla pari o addirittura superano NASNets, MobileNets, MobileNets V2 e Wide Residual Networks di diverse dimensioni su CIFAR-10 e ImageNet64x64 entro una sola settimana su otto GPU, che è circa 20-40 volte meno potenza di calcolo dei precedenti metodi di ricerca dell'architettura che producono prestazioni all'avanguardia.
Affrontiamo due sfide della modellazione probabilistica dei temi per stimare meglio la probabilità di una parola in un dato contesto, cioè P(wordjcontext): (1) Nessuna struttura linguistica nel contesto: I modelli probabilistici di argomento ignorano l'ordine delle parole riassumendo un contesto dato come un "bag-of-word" e di conseguenza la semantica delle parole nel contesto è persa. In questo lavoro, incorporiamo la struttura della lingua combinando un modello neurale autoregressivo di argomento (TM) con un modello di lingua basato su LSTM (LSTM-LM) in una singola struttura probabilistica. L'LSTM-LM apprende una rappresentazione vettoriale di ogni parola tenendo conto dell'ordine delle parole nei modelli di collocazione locali, mentre il TM apprende simultaneamente una rappresentazione latente dall'intero documento.Inoltre, l'LSTM-LM modella caratteristiche complesse del linguaggio (ad es, sintassi e semantica), mentre il TM scopre la struttura tematica sottostante in una collezione di documenti.Uniamo due paradigmi complementari di apprendimento del significato delle occorrenze delle parole combinando un modello di argomento e un modello di linguaggio in un quadro probabilistico unificato, chiamato come ctx-DocNADE.(2) Contesto limitato e/o corpus di allenamento più piccolo di documenti:In impostazioni con un piccolo numero di occorrenze di parole (cioè Noi affrontiamo questa sfida incorporando la conoscenza esterna nei modelli neurali autoregressivi di argomento attraverso un approccio di modellazione del linguaggio: usiamo le embeddings di parole come input di un LSTM-LM con l'obiettivo di migliorare il wordtopicmapping su un corpus più piccolo e/o di breve testo. Presentiamo nuove varianti di modelli neurali autoregressivi di argomento accoppiati con modelli neurolinguistici e priori di embeddings che superano costantemente i modelli di argomento generativi allo stato dell'arte in termini di generalizzazione (perplessità), interpretabilità (coerenza dell'argomento) e applicabilità (recupero e classificazione) su 6 dataset di testo lungo e 8 di testo breve da diversi domini.
I grandi requisiti di memoria delle reti neurali profonde mettono a dura prova le capacità di molti dispositivi, limitando la loro distribuzione e adozione. I metodi di compressione dei modelli riducono efficacemente i requisiti di memoria di questi modelli, di solito attraverso l'applicazione di trasformazioni come la potatura dei pesi o la quantizzazione. La codifica si basa sul filtro Bloomier, una struttura dati probabilistica che può risparmiare spazio a costo di introdurre errori casuali. Sfruttando la capacità delle reti neurali di tollerare queste imperfezioni e ri-addestrandosi intorno agli errori, la tecnica proposta, Weightless, può comprimere i pesi DNN fino a 496x; con la stessa precisione del modello, questo si traduce in un miglioramento fino a 1.51x rispetto allo stato dell'arte.
Un lavoro recente ha dimostrato che le rappresentazioni contestualizzate delle parole derivate dalla traduzione automatica neurale (NMT) sono una valida alternativa a quelle derivanti da semplici compiti di previsione delle parole, perché la comprensione interna che deve essere costruita per poter tradurre da una lingua all'altra è molto più completa. Sfortunatamente, le limitazioni computazionali e di memoria attuali impediscono ai modelli NMT di utilizzare grandi vocabolari di parole, e quindi sono state utilizzate alternative come le unità di sottoparola (BPE e segmentazioni morfologiche) e i caratteri.Qui studiamo l'impatto dell'utilizzo di diversi tipi di unità sulla qualità delle rappresentazioni risultanti quando vengono utilizzate per modellare la sintassi, la semantica e la morfologia.  Abbiamo scoperto che mentre le rappresentazioni derivate dalle sottoparole sono leggermente migliori per la modellazione della sintassi, le rappresentazioni basate sui caratteri sono superiori per la modellazione della morfologia e sono anche più robuste agli input rumorosi.
L'apprendimento a pochi colpi è il processo di apprendimento di nuove classi utilizzando solo pochi esempi e rimane un compito impegnativo nell'apprendimento automatico. Molti sofisticati algoritmi di apprendimento a pochi colpi sono stati proposti sulla base della nozione che le reti possono facilmente adattarsi eccessivamente a nuovi esempi se sono semplicemente messe a punto utilizzando solo pochi esempi. In questo studio, mostriamo che nel set di dati mini-ImageNet a bassa risoluzione comunemente usato, il metodo di fine-tuning raggiunge una maggiore accuratezza rispetto ai comuni algoritmi di apprendimento few-shot nel compito 1-shot e quasi la stessa accuratezza dell'algoritmo state-of-the-art nel compito 5-shot. Abbiamo poi valutato il nostro metodo con compiti più pratici, vale a dire i compiti ad alta risoluzione a dominio singolo e a dominio incrociato. Con entrambi i compiti, dimostriamo che il nostro metodo raggiunge una maggiore accuratezza rispetto ai comuni algoritmi di apprendimento a pochi scatti. Analizziamo ulteriormente i risultati sperimentali e dimostriamo che: 1) il processo di riqualificazione può essere stabilizzato impiegando un basso tasso di apprendimento, 2) l'utilizzo di ottimizzatori di gradiente adattivi durante il fine-tuning può aumentare la precisione del test, e 3) la precisione del test può essere migliorata aggiornando l'intera rete quando esiste un grande domain-shift tra le classi base e nuove.
Proponiamo un approccio per generare dati di mercato azionario realistici e ad alta fedeltà basati su reti generative avversarie. modelliamo il flusso degli ordini come un processo stocastico con dipendenza dalla storia finita e impieghiamo un GAN Wasserstein condizionale per catturare la dipendenza dalla storia degli ordini in un mercato azionario. Testiamo il nostro approccio con il mercato reale e dati sintetici su una serie di statistiche diverse, e troviamo che i dati generati sono vicini ai dati reali.
Presentiamo un nuovo algoritmo di attacco avversario black-box con tassi di evasione del modello allo stato dell'arte per l'efficienza della query sotto la metrica $\ell_infty$ e $\ell_2$, che sfrutta un approccio di stima del gradiente basato sul testo, piuttosto che sulla grandezza, che sposta la stima del gradiente dall'ottimizzazione continua a quella binaria black-box. Costruisce in modo adattivo le query per stimare il gradiente, una query basandosi sulla precedente, piuttosto che ri-stimare il gradiente ad ogni passo con la costruzione di query casuali. La sua dipendenza dai bit di segno produce una minore impronta di memoria e non richiede né la sintonizzazione dell'iperparametro né la riduzione della dimensionalità. Inoltre, la sua performance teorica è garantita e può caratterizzare i sottospazi avversari meglio dei sottospazi allineati a gradiente white-box.Su due sfide pubbliche di attacco black-box e su un modello addestrato in modo robusto contro gli attacchi di trasferimento, i tassi di evasione dell'algoritmo superano tutti gli attacchi presentati. Per una suite di modelli pubblicati, l'algoritmo è $3.8 volte meno soggetto a fallimenti mentre spende $2.5 volte meno query rispetto alla migliore combinazione di algoritmi allo stato dell'arte. Ad esempio, evade un modello MNIST standard usando solo $12$ di query in media e prestazioni simili sono osservate su un modello IMAGENET standard con una media di $579$ di query.
Le reti neurali ricorrenti (RNN) sono modelli ampiamente utilizzati per i dati di sequenza.Proprio come per le reti feedforward, è diventato comune costruire RNN "profonde", cioè impilare più strati ricorrenti per ottenere astrazioni di livello superiore dei dati.Tuttavia, questo funziona solo per una manciata di strati.A differenza delle reti feedforward, impilare più di poche unità ricorrenti (ad es, A differenza delle reti feedforward, impilare più di poche unità ricorrenti (ad esempio, celle LSTM) di solito danneggia le prestazioni del modello, perché i gradienti scompaiono o esplodono durante l'addestramento. Indaghiamo l'addestramento delle RNN multistrato ed esaminiamo l'ampiezza dei gradienti mentre si propagano attraverso la rete e dimostriamo che, a seconda della struttura dell'unità ricorrente di base, i gradienti sono sistematicamente attenuati o amplificati, in modo che con una profondità crescente tendono a scomparire o ad esplodere. Sulla base della nostra analisi progettiamo un nuovo tipo di cella gated che conserva meglio la grandezza del gradiente, e quindi rende possibile addestrare RNN più profonde.Convalidiamo sperimentalmente il nostro progetto con cinque diversi compiti di modellazione di sequenza su tre diversi set di dati.La cella ricorrente impilabile proposta (STAR) permette architetture ricorrenti sostanzialmente più profonde, con prestazioni migliori.
Nonostante il suo successo empirico, i fondamenti teorici delle proprietà di stabilità, convergenza e accelerazione della normalizzazione batch (BN) rimangono sfuggenti.In questo articolo, attacchiamo questo problema da un approccio di modellazione, dove eseguiamo un'analisi teorica approfondita sulla BN applicata a un modello semplificato: i minimi quadrati ordinari (OLS). Scopriamo che la discesa del gradiente su OLS con BN ha proprietà interessanti, tra cui una legge di scala, la convergenza per tassi di apprendimento arbitrari per i pesi, effetti di accelerazione asintotici, così come l'insensibilità alla scelta dei tassi di apprendimento. Dimostriamo poi numericamente che questi risultati non sono specifici al problema OLS e tengono qualitativamente per problemi di apprendimento supervisionato più complessi.
Risolvere i compiti nel Reinforcement Learning non è un'impresa facile: poiché l'obiettivo dell'agente è quello di massimizzare la ricompensa accumulata, spesso impara a sfruttare le scappatoie e le errate specificazioni del segnale di ricompensa con conseguente comportamento indesiderato. In questo lavoro presentiamo un nuovo approccio multi-timescala per l'ottimizzazione delle politiche vincolate, chiamato `Reward Constrained Policy Optimization' (RCPO), che utilizza un segnale di penalità alternativo per guidare la politica verso un vincolo che soddisfi uno.dimostriamo la convergenza del nostro approccio e forniamo prove empiriche della sua capacità di formare politiche che soddisfino i vincoli.
L'Handheld Virtual Panel (HVP) è il pannello virtuale collegato al controller della mano non dominante nella realtà virtuale (VR). I nostri risultati mostrano che tutti e quattro i fattori hanno effetti significativi sulle prestazioni dell'utente e, sulla base di questi risultati, proponiamo delle linee guida per la progettazione ergonomica e performante delle interfacce HVP.
Le reti neurali profonde hanno dimostrato un successo senza precedenti in varie applicazioni di gestione della conoscenza.Tuttavia, le reti create sono spesso molto complesse, con un gran numero di bordi addestrabili che richiedono ampie risorse computazionali.Notiamo che molte reti di successo contengono spesso un gran numero di bordi ridondanti.Inoltre, molti di questi bordi possono avere contributi trascurabili verso le prestazioni complessive della rete.In questo articolo, proponiamo un nuovo quadro iSparse e dimostriamo sperimentalmente che possiamo sparsificare la rete, del 30-50%, senza impatto sulle prestazioni della rete. iSparse sfrutta un nuovo punteggio di significatività del bordo, E, per determinare l'importanza di un bordo rispetto all'output finale della rete. inoltre, iSparse può essere applicato sia durante l'addestramento di un modello che sopra un modello pre-addestrato, rendendolo un approccio privo di riqualificazione - portando ad un minimo sovraccarico computazionale. confronti di iSparse contro PFEC, NISP, DropConnect, e Retraining-Free su set di dati di riferimento mostrano che iSparse porta a sparsificazioni di rete efficaci.
Motivati dall'osservazione che gli sforzi per codificare la conoscenza del mondo in basi di conoscenza leggibili dalla macchina tendono ad essere centrati sulle entità, studiamo l'uso di un compito di riempimento in bianco per imparare rappresentazioni indipendenti dal contesto delle entità dai contesti in cui tali entità sono state menzionate. Dimostriamo che l'addestramento su larga scala dei modelli neurali ci permette di apprendere informazioni di digitazione delle entità ad altissima fedeltà, che dimostriamo con la ricostruzione in pochi scatti delle categorie di Wikipedia.Il nostro approccio di apprendimento è abbastanza potente per codificare argomenti specializzati come i ciclisti del Giro d'Italia.
Gli autocodificatori variazionali (VAE) hanno avuto successo nell'apprendimento di un collettore a bassa dimensione da dati ad alta dimensione con dipendenze complesse; nel loro nucleo, consistono in un potente modello di inferenza probabilistica bayesiana, per catturare le caratteristiche salienti dei dati; nell'addestramento, sfruttano la potenza dell'inferenza variazionale, ottimizzando un limite inferiore sulla prova del modello; la rappresentazione latente e le prestazioni dei VAE sono fortemente influenzate dal tipo di limite usato come funzione di costo. Un significativo lavoro di ricerca è stato condotto nello sviluppo di limiti più stretti dell'originale ELBO, per approssimare più accuratamente la vera log-likelihood.Facendo leva sul logaritmo q-deformato nei limiti inferiori tradizionali, ELBO e IWAE, e il limite superiore CUBO, portiamo contributi a questa direzione di ricerca. In questo studio proof-of-concept, esploriamo diversi modi di creare questi limiti q-deformati che sono più stretti di quelli classici e mostriamo miglioramenti nelle prestazioni di tali VAE sul dataset MNIST binarizzato.
Una convinzione persiste a lungo nell'apprendimento automatico che l'allargamento dei margini sui dati di addestramento spiega la resistenza dei modelli all'overfitting aumentando la robustezza.Tuttavia Breiman mostra un dilemma (Breiman, 1999) che un miglioramento uniforme sulla distribuzione dei margini \emph{non} riduce necessariamente l'errore di generalizzazione.In questo articolo, rivisitiamo il dilemma di Breiman nelle reti neurali profonde con i margini normalizzati recentemente proposti usando i limiti costanti di Lipschitz per prodotti di norma spettrale. Con la teoria semplificata e gli esperimenti estesi, il dilemma di Breiman è dimostrato basarsi sulla dinamica delle distribuzioni dei margini normalizzati, che riflette il trade-off tra la potenza di espressione del modello e la complessità dei dati. Quando la complessità dei dati è paragonabile al potere di espressione del modello, nel senso che i dati di addestramento e di test condividono transizioni di fase simili nella dinamica dei margini normalizzati, due modi efficienti sono derivati attraverso i classici limiti di generalizzazione basati sui margini per prevedere con successo la tendenza dell'errore di generalizzazione.D'altra parte, i modelli sovraespressi che mostrano miglioramenti uniformi sui margini normalizzati di addestramento possono perdere tale potere di previsione e non riuscire a prevenire l'overfitting. 
È stata una sfida di ricerca aperta per lo sviluppo di un sistema di dialogo orientato ai compiti end-to-end multi-dominio, in cui un essere umano può conversare con l'agente di dialogo per completare i compiti in più di un dominio.In primo luogo, il monitoraggio degli stati di credenza dei dialoghi multi-dominio è difficile, poiché l'agente di dialogo deve ottenere gli stati di credenza completi da tutti i domini pertinenti, ognuno dei quali può avere slot condivisi comuni tra i domini, nonché slot unici specifici per il solo dominio. In secondo luogo, l'agente di dialogo deve anche elaborare vari tipi di informazioni, comprese le informazioni contestuali dal contesto del dialogo, gli stati di dialogo decodificati del turno di dialogo corrente, e i risultati interrogati da una base di conoscenza, per modellare semanticamente le risposte consapevoli del contesto e specifiche del compito dell'uomo. Per affrontare queste sfide, proponiamo un'architettura neurale end-to-end per i dialoghi orientati ai compiti in domini multipli. Proponiamo un nuovo inseguitore di credenze neurali multilivello che segue gli stati di credenza del dialogo imparando segnali sia a livello di slot che di dominio in modo indipendente.Le rappresentazioni sono combinate in un approccio di fusione tardiva per formare vettori di caratteristiche congiunte di (dominio, slot) coppie. Seguendo il recente lavoro nei sistemi di dialogo end-to-end, incorporiamo il belief tracker con componenti di generazione per affrontare compiti di dialogo end-to-end. Raggiungiamo prestazioni allo stato dell'arte sul benchmark MultiWOZ2.1 con il 50,91% di accuratezza dell'obiettivo congiunto e misure competitive nel completamento dei compiti e nella generazione delle risposte.
Lo score matching fornisce un approccio efficace all'apprendimento di modelli flessibili non normalizzati, ma la sua scalabilità è limitata dalla necessità di valutare una derivata del secondo ordine.  Questa connessione ci permette di progettare un'approssimazione scalabile a questi obiettivi, con una forma simile alla divergenza contrastiva a passo singolo. Presentiamo applicazioni nell'addestramento di autocodificatori variazionali e Wasserstein impliciti con priori a valori multipli.
Questo articolo sviluppa l'apprendimento continuo variazionale (VCL), una struttura semplice ma generale per l'apprendimento continuo che fonde l'inferenza variazionale online (VI) e i recenti progressi nel Monte Carlo VI per le reti neurali. La struttura può addestrare con successo sia modelli discriminativi profondi che modelli generativi profondi in complesse impostazioni di apprendimento continuo in cui i compiti esistenti si evolvono nel tempo ed emergono compiti completamente nuovi.
In ambienti parzialmente osservabili (PO), gli agenti di apprendimento di rinforzo profondo (RL) spesso soffrono di prestazioni insoddisfacenti, poiché due problemi devono essere affrontati insieme: come estrarre informazioni dalle osservazioni grezze per risolvere il compito, e come migliorare la politica. L'algoritmo proposto è stato testato in due tipi di compiti di controllo robotico PO, quelli in cui le coordinate o le velocità non erano osservabili e quelli che richiedono una memorizzazione a lungo termine. I nostri esperimenti mostrano che l'algoritmo proposto ha raggiunto una migliore efficienza dei dati e/o ha appreso una politica più ottimale rispetto ad altri approcci alternativi in compiti in cui gli stati non osservati non possono essere dedotti dalle osservazioni grezze in modo semplice.
Questo articolo formalizza il problema della selezione degli algoritmi online nel contesto del Reinforcement Learning (RL): dato un compito episodico e un numero finito di algoritmi RL off-policy, un meta-algoritmo deve decidere quale algoritmo RL è in controllo durante il prossimo episodio in modo da massimizzare il rendimento atteso. L'articolo presenta un nuovo meta-algoritmo, chiamato Epochal Stochastic Bandit Algorithm Selection (ESBAS), il cui principio è quello di congelare gli aggiornamenti della politica ad ogni epoca, e di lasciare un bandito stocastico riavviato per la selezione dell'algoritmo. ESBAS viene prima valutato empiricamente su un compito di dialogo, dove si dimostra che supera ogni singolo algoritmo nella maggior parte delle configurazioni.ESBAS viene poi adattato a una vera impostazione online dove gli algoritmi aggiornano le loro politiche dopo ogni transizione, che noi chiamiamo SSBAS.SSBAS viene valutato su un compito di raccolta della frutta dove si dimostra che adatta il parametro stepsize in modo più efficiente del classico decadimento iperbolico, e su un gioco Atari, dove migliora le prestazioni con un ampio margine.
Rispetto al processo generativo classico, in cui ogni punto di dati osservato è generato da una variabile latente individuale, il nostro approccio presuppone una variabile latente globale per generare l'intero set di punti di dati osservati; proponiamo quindi un obiettivo di apprendimento che è derivato come approssimazione ad un limite inferiore alla verosimiglianza del log dei dati, portando al nostro algoritmo, WiSE-ALE. Rispetto all'obiettivo ELBO standard, dove il posteriore variazionale per ogni punto di dati è incoraggiato a corrispondere alla distribuzione precedente, l'obiettivo WiSE-ALE corrisponde al posteriore medio, su tutti i campioni, con il precedente, permettendo alle distribuzioni posteriori campione-saggio di avere una gamma più ampia di media e varianza di incorporamento accettabile e portando a una migliore qualità di ricostruzione nel processo di autocodifica.Attraverso vari esempi e confronto con altri modelli VAE all'avanguardia, dimostriamo che WiSE-ALE ha eccellenti proprietà di incorporazione delle informazioni, pur mantenendo la capacità di imparare una rappresentazione regolare e compatta.
Miglioriamo la robustezza delle reti neurali profonde agli attacchi avversari utilizzando una funzione interpolante come attivazione dell'uscita.   Questa funzione di attivazione dipendente dai dati migliora notevolmente sia l'accuratezza della classificazione che la stabilità alle perturbazioni avversarie. Insieme alla minimizzazione della variazione totale delle immagini avversarie e all'addestramento aumentato, sotto l'attacco più forte, raggiungiamo fino al 20,6%, 50,7% e 68,7% di miglioramento della precisione con il metodo del segno del gradiente veloce, il metodo iterativo del segno del gradiente veloce e gli attacchi Carlini-WagnerL2, rispettivamente.  La nostra strategia di difesa è additiva a molti dei metodi esistenti.  Diamo una spiegazione intuitiva della nostra strategia di difesa attraverso l'analisi della geometria dello spazio delle caratteristiche.Per la riproducibilità, il codice sarà disponibile su GitHub.
Questo lavoro presenta una soluzione scalabile per il riconoscimento visivo continuo del parlato.Per raggiungere questo obiettivo, abbiamo costruito il più grande set di dati esistente per il riconoscimento visivo del parlato, composto da coppie di testo e video clip di volti che parlano (3.886 ore di video).In tandem, abbiamo progettato e addestrato un sistema integrato di lettura delle labbra, composto da una pipeline di elaborazione video che mappa il video grezzo a video stabili di labbra e sequenze di fonemi, una rete neurale profonda scalabile che mappa i video delle labbra a sequenze di distribuzioni di fonemi, e un decodificatore di discorso a livello di produzione che produce sequenze di parole. Il sistema proposto raggiunge un tasso di errore di parola (WER) del 40,9% misurato su un set tenuto fuori.In confronto, i lettori di labbra professionali raggiungono l'86,4% o il 92,9% WER sullo stesso set di dati quando hanno accesso a tipi aggiuntivi di informazioni contestuali.Il nostro approccio migliora significativamente i precedenti approcci di lettura delle labbra, comprese le varianti di LipNet e di Watch, Attend, and Spell (WAS), che sono solo capaci di 89,8% e 76,8% WER rispettivamente.
Lavori precedenti (Bowman et al., 2015; Yang et al, 2017) ha trovato difficoltà nello sviluppo di modelli generativi basati su autoencoder variazionali (VAEs) per il testo.Per affrontare il problema del decoder ignorando le informazioni dal codificatore (collasso posteriore), questi modelli precedenti indeboliscono la capacità del decoder per forzare il modello a utilizzare le informazioni dalle variabili latenti.Tuttavia, questa strategia non è ideale in quanto degrada la qualità del testo generato e aumenta gli iper-parametri. In questo articolo, proponiamo un nuovo VAE per il testo utilizzando una distribuzione multimodale a priori, un codificatore modificato e l'apprendimento multi-task; dimostriamo che il nostro modello può generare frasi ben condizionate senza indebolire la capacità del decodificatore, inoltre la distribuzione multimodale a priori migliora l'interpretabilità delle rappresentazioni acquisite.
Il pruning del modello cerca di indurre sparsità nelle varie matrici di connessione di una rete neurale profonda, riducendo così il numero di parametri non valutati a zero nel modello.Rapporti recenti (Han et al., 2015; Narang et al, Questo suggerisce la possibilità che i modelli di base in questi esperimenti siano forse gravemente iperparametrizzati all'inizio e che una valida alternativa per la compressione del modello potrebbe essere quella di ridurre semplicemente il numero di unità nascoste mantenendo la struttura di connessione densa del modello, esponendo un simile trade-off nella dimensione del modello e nella precisione. Indaghiamo questi due percorsi distinti per la compressione del modello nel contesto di un'inferenza efficiente dal punto di vista energetico in ambienti con risorse limitate e proponiamo una nuova tecnica di potatura graduale che è semplice e diretta da applicare a una varietà di modelli/dataset con una regolazione minima e può essere incorporata senza soluzione di continuità nel processo di formazione. Confrontiamo l'accuratezza dei modelli grandi, ma potati (large-sparse) e le loro controparti più piccole, ma dense (small-dense) con identica impronta di memoria. Attraverso una vasta gamma di architetture di reti neurali (CNN profonde, LSTM impilati e modelli LSTM seq2seq), troviamo che i modelli large-sparse superano costantemente i modelli small-dense e raggiungono una riduzione fino a 10 volte del numero di parametri non nulli con una perdita minima di accuratezza.
Tuttavia, il controllo degli attributi della lingua generata (ad esempio il cambio di argomento o di sentimento) è difficile senza modificare l'architettura del modello o la messa a punto su dati specifici dell'attributo e comporta il costo significativo della riqualificazione. Proponiamo una semplice alternativa: il modello di lingua Plug and Play (PPLM) per la generazione di lingua controllabile, che combina un LM preaddestrato con uno o più classificatori di attributi semplici che guidano la generazione del testo senza ulteriore formazione del LM. Nello scenario canonico che presentiamo, i modelli di attributo sono semplici classificatori che consistono in un sacchetto di parole specificato dall'utente o in un singolo strato appreso con un numero di parametri 100.000 volte inferiore rispetto al LM. Il campionamento comporta un passaggio avanti e indietro in cui i gradienti dal modello di attributo spingono le attivazioni nascoste del LM e quindi guidano la generazione. I campioni del modello dimostrano il controllo su una gamma di argomenti e stili di sentimento, e le estese valutazioni automatiche e umane annotate mostrano l'allineamento degli attributi e la fluidità.I PPLM sono flessibili in quanto qualsiasi combinazione di modelli di attributi differenziabili può essere utilizzata per guidare la generazione del testo, il che consentirà applicazioni diverse e creative oltre gli esempi forniti in questo articolo.
Gli approcci esistenti di apprendimento multitask profondo (MTL) allineano gli strati condivisi tra i compiti in un ordinamento parallelo. Tale organizzazione limita significativamente i tipi di struttura condivisa che possono essere appresi. La necessità dell'ordinamento parallelo per l'MTL profondo viene prima testata confrontandola con l'ordinamento permutato degli strati condivisi. I risultati indicano che un ordinamento flessibile può consentire una condivisione più efficace, motivando così lo sviluppo di un approccio di ordinamento morbido, che impara come gli strati condivisi sono applicati in modi diversi per compiti diversi.La MTL profonda con ordinamento morbido supera i metodi di ordinamento parallelo in una serie di domini.Questi risultati suggeriscono che la potenza della MTL profonda deriva dall'apprendimento di blocchi di costruzione altamente generali che possono essere assemblati per soddisfare le esigenze di ogni compito.
Proponiamo una struttura generica per calibrare l'accuratezza e la fiducia (punteggio) di una previsione attraverso inferenze stocastiche in reti neurali profonde. analizziamo prima la relazione tra la variazione di più parametri del modello per un singolo esempio di inferenza e la varianza dei punteggi di previsione corrispondenti attraverso la modellazione bayesiana della regolarizzazione stocastica. la nostra osservazione empirica mostra che l'accuratezza e il punteggio di una previsione sono altamente correlati con la varianza di più inferenze stocastiche date dalla profondità stocastica o dal dropout. Motivati da questi fatti, progettiamo una nuova funzione di perdita integrata nella fiducia ponderata dalla varianza che è composta da due termini di perdita cross-entropia rispetto alla verità di base e alla distribuzione uniforme, che sono bilanciati dalla varianza dei punteggi di predizione stocastica. Il nostro algoritmo presenta un'eccezionale performance di calibrazione della fiducia e migliora l'accuratezza della classificazione con due popolari tecniche di regolarizzazione stocastica - profondità stocastica e dropout - in più modelli e insiemi di dati; allevia significativamente il problema dell'overconfidence nelle reti neurali profonde addestrando le reti a raggiungere un'accuratezza di predizione proporzionale alla fiducia della predizione.
I simulatori basati su particelle sono stati sviluppati per modellare le dinamiche di queste scene complesse; tuttavia, basandosi su tecniche di approssimazione, la loro simulazione spesso si discosta dalla fisica del mondo reale, specialmente a lungo termine. Combinando l'apprendimento con i sistemi basati su particelle si ottengono due grandi vantaggi: in primo luogo, il simulatore appreso, proprio come altri sistemi basati su particelle, agisce ampiamente su oggetti di materiali diversi; in secondo luogo, la rappresentazione basata su particelle pone un forte bias induttivo per l'apprendimento: le particelle dello stesso tipo hanno la stessa dinamica all'interno. Questo permette al modello di adattarsi rapidamente a nuovi ambienti di dinamiche sconosciute entro poche osservazioni.Dimostriamo che i robot raggiungono compiti di manipolazione complessi utilizzando il simulatore appreso, come la manipolazione di fluidi e schiuma deformabile, con esperimenti sia nella simulazione che nel mondo reale.Il nostro studio aiuta a gettare le basi per l'apprendimento robotico di scene dinamiche con rappresentazioni basate su particelle.
Le Reti Generative Adversariali (GAN), quando addestrate su grandi set di dati con diverse modalità, sono note per produrre immagini confuse che non appartengono distintamente a nessuna delle modalità: (1) Per set di dati con una grande varietà, è probabile che le modalità si trovino su collettori separati.(2) Il generatore (G) è formulato come una funzione continua, e il rumore in ingresso è derivato da un insieme connesso, per cui l'uscita di G è un insieme connesso. Se G copre tutte le modalità, allora ci deve essere qualche porzione dell'output di G che le collega, il che corrisponde a immagini indesiderate e confuse. Sviluppiamo argomenti teorici per sostenere queste intuizioni e proponiamo un nuovo metodo per rompere il secondo presupposto attraverso discontinuità imparabili nello spazio del rumore latente. Equivalentemente, può essere visto come l'addestramento di diversi generatori, creando così discontinuità nella funzione G. Aumentiamo anche la formulazione GAN con un classificatore C che predice quale partizione/generatore di rumore ha prodotto le immagini in uscita, incoraggiando la diversità tra ogni partizione/generatore. Sperimentiamo su MNIST, CeleA, STL-10, e un difficile set di dati con modalità chiaramente distinte, e dimostriamo che le partizioni di rumore corrispondono a diverse modalità della distribuzione dei dati, e producono immagini di qualità superiore.
Per sfruttare i dati crowd-sourced per addestrare modelli text-to-speaker text-to-speech (TTS) che possono sintetizzare un discorso pulito per tutti gli altoparlanti, è essenziale imparare rappresentazioni disentangled che possono controllare indipendentemente l'identità dell'altoparlante e il rumore di fondo nei segnali generati.Tuttavia, l'apprendimento di tali rappresentazioni può essere difficile, a causa della mancanza di etichette che descrivono le condizioni di registrazione di ogni esempio di formazione, e il fatto che gli altoparlanti e le condizioni di registrazione sono spesso correlati, ad esempio poiché gli utenti spesso fanno molte registrazioni utilizzando la stessa attrezzatura.Questo articolo propone tre componenti per affrontare questo problema da: (1) formulando un modello generativo condizionale con variabili latenti fattorizzate, (2) usando l'aumento dei dati per aggiungere rumore che non è correlato all'identità dell'altoparlante e la cui etichetta è nota durante l'addestramento, e (3) usando la fattorizzazione avversaria per migliorare il disentanglement.I risultati sperimentali dimostrano che il metodo proposto può disentangle gli attributi dell'altoparlante e del rumore anche se sono correlati nei dati di addestramento e può essere usato per sintetizzare costantemente il discorso pulito per tutti gli altoparlanti.Studi di ablazione verificano l'importanza di ogni componente proposto.
I modelli linguistici basati su LSTM esibiscono la compositività nelle loro rappresentazioni, ma come questo comportamento emerga nel corso dell'addestramento non è stato esplorato.Analizzando gli esperimenti di dati sintetici con la decomposizione contestuale, troviamo che LSTMs impara le dipendenze a lungo raggio in modo composito costruendole da costituenti più brevi durante l'addestramento.
L'apprendimento di una rete neurale profonda richiede la risoluzione di un problema di ottimizzazione impegnativo: si tratta di un problema di minimizzazione ad alta dimensione, non convesso e non liscio con un gran numero di termini.La pratica corrente nell'ottimizzazione delle reti neurali è quella di affidarsi all'algoritmo di discesa del gradiente stocastico (SGD) o alle sue varianti adattative. Tuttavia, SGD richiede un programma progettato a mano per il tasso di apprendimento.Inoltre, le sue varianti adattive tendono a produrre soluzioni che generalizzano meno bene su dati non visti che SGD con un programma progettato a mano.Presentiamo un metodo di ottimizzazione che offre empiricamente il meglio dei due mondi: il nostro algoritmo produce buone prestazioni di generalizzazione mentre richiede solo un iper-parametro. Il nostro approccio si basa su una struttura prossimale composita, che sfrutta la natura compositiva delle reti neurali profonde e può sfruttare potenti algoritmi di ottimizzazione convessi per progettazione. In particolare, impieghiamo l'algoritmo Frank-Wolfe (FW) per SVM, che calcola un passo ottimale in forma chiusa ad ogni passo temporale. Mostriamo inoltre che la direzione di discesa è data da un semplice passaggio all'indietro nella rete, producendo lo stesso costo computazionale per iterazione di SGD. Presentiamo esperimenti sui set di dati CIFAR e SNLI, dove dimostriamo la significativa superiorità del nostro metodo rispetto ad Adam, Adagrad, così come i recentemente proposti BPGrad e AMSGrad. Inoltre, confrontiamo il nostro algoritmo con SGD con un programma di apprendimento progettato a mano, e dimostriamo che fornisce una generalizzazione simile mentre spesso converge più velocemente.
In questo articolo, mostriamo come le nuove tecniche di apprendimento di rinforzo di trasferimento possono essere applicate al complesso compito di navigazione guidata dal bersaglio usando il simulatore fotorealisticoAI2THOR.In particolare, ci basiamo sul concetto di Universal SuccessorFeatures con un agente A3C. Introduciamo il nuovo contributo architettonico di una Successor Feature Dependent Policy (SFDP) e adottiamo il concetto di VariationalInformation Bottlenecks per raggiungere lo stato dell'arte delle prestazioni.VUSFA, la nostra architettura finale, è un approccio semplice che può essere implementato utilizzando il nostro repository open source.Il nostro approccio è generalizzabile, ha mostrato una maggiore stabilità nella formazione, e ha superato i recenti approcci in termini di capacità di apprendimento di trasferimento.
Una delle maggiori sfide nell'apprendimento delle rappresentazioni delle immagini è la separazione dei fattori di variazione alla base della formazione dell'immagine.  Questo è tipicamente ottenuto con un'architettura autoencoder dove un sottoinsieme delle variabili latenti è vincolato a corrispondere a fattori specifici, e il resto di esse sono considerate variabili di disturbo. Questo approccio ha un importante svantaggio: quando la dimensione delle variabili di disturbo è aumentata, la ricostruzione dell'immagine è migliorata, ma il decoder ha la flessibilità di ignorare i fattori specificati, perdendo così la capacità di condizionare l'output su di essi.  In questo lavoro, proponiamo di superare questo trade-off facendo crescere progressivamente la dimensione del codice latente, mentre vincoliamo la Jacobiana dell'immagine di uscita rispetto alle variabili dissentite a rimanere la stessa.  Come risultato, i modelli ottenuti sono efficaci sia per la dissociazione che per la ricostruzione.  Dimostriamo l'applicabilità di questo metodo in scenari sia non supervisionati che supervisionati per l'apprendimento di rappresentazioni disentangled.In un compito di manipolazione degli attributi facciali, otteniamo una generazione di immagini di alta qualità mentre controlliamo agevolmente decine di attributi con un unico modello.Questo è un ordine di grandezza più fattori disentangled rispetto ai metodi all'avanguardia, pur ottenendo risultati visivamente simili o superiori, ed evitando la formazione avversaria.
Proponiamo una nuova nozione di 'non linearità' di uno strato di rete rispetto a un batch di input che si basa sulla sua vicinanza a un sistema lineare, che si riflette nel rango non negativo della matrice di attivazione.Misuriamo questa non linearità applicando la fattorizzazione non negativa alla matrice di attivazione.Considerando lotti di campioni simili, troviamo che un'elevata non linearità negli strati profondi è indicativa di memorizzazione. Inoltre, applicando il nostro approccio layer-by-layer, troviamo che il meccanismo di memorizzazione consiste in fasi distinte.Eseguiamo esperimenti su reti neurali completamente connesse e convoluzionali addestrate su diversi dataset di immagini e audio.I nostri risultati dimostrano che come indicatore di memorizzazione, la nostra tecnica può essere utilizzata per eseguire un arresto anticipato.
Le reti neurali profonde hanno raggiunto lo stato dell'arte delle prestazioni in vari campi, ma devono essere ridimensionate per essere utilizzate per le applicazioni del mondo reale.Come mezzo per ridurre le dimensioni di una rete neurale preservando le sue prestazioni, il trasferimento della conoscenza ha portato molta attenzione.Un metodo popolare di trasferimento della conoscenza è la distillazione della conoscenza (KD), dove le uscite ammorbidite di una rete insegnante pre-addestrata aiutano a formare reti di studenti.Da KD, sono stati proposti altri metodi di trasferimento, e si concentrano principalmente sulle funzioni di perdita, attivazioni di strati nascosti, o moduli aggiuntivi per trasferire bene la conoscenza dalle reti insegnanti alle reti di studenti. In questo lavoro, ci concentriamo sulla struttura di una rete di insegnanti per ottenere l'effetto di più reti di insegnanti senza risorse aggiuntive.Proponiamo di cambiare la struttura di una rete di insegnanti per avere blocchi stocastici e saltare le connessioni.Così facendo, una rete di insegnanti diventa l'aggregato di un enorme numero di percorsi. Nella fase di addestramento, ogni sottorete viene generata lasciando cadere i blocchi stocastici in modo casuale e utilizzata come rete insegnante.Questo permette di addestrare la rete studente con più reti insegnante e migliora ulteriormente la rete studente sulle stesse risorse in una singola rete insegnante.Verifichiamo che la struttura proposta porta un ulteriore miglioramento alle reti studente su set di dati di riferimento.
I sistemi di suggerimento di emoji basati sul testo digitato sono stati proposti per incoraggiare l'uso di emoji e arricchire la messaggistica di testo; tuttavia, gli effetti reali di tali sistemi sull'esperienza di chat rimangono sconosciuti.Abbiamo costruito una tastiera Android con entrambe le capacità di suggerimento di emoji lessicali (basate sulle parole) e semantiche (basate sul significato) e le abbiamo confrontate in due diversi studi.Per indagare l'effetto del suggerimento di emoji nelle conversazioni online, abbiamo condotto uno studio di laboratorio di messaggistica di testo con 24 partecipanti, e anche un'implementazione longitudinale di 15 giorni sul campo con 18 partecipanti. Abbiamo scoperto che i suggerimenti lessicali di emoji hanno aumentato l'uso di emoji del 31,5% rispetto a una tastiera senza suggerimenti, mentre i suggerimenti semantici hanno aumentato l'uso di emoji del 125,1%.Tuttavia, i meccanismi di suggerimento non hanno influenzato l'esperienza di chat in modo significativo.Da questi studi, formuliamo una serie di linee guida di progettazione per i futuri sistemi di suggerimento di emoji che supportano meglio le esigenze degli utenti.
I Generative Adversarial Networks (GANs) convenzionali per la generazione di testo tendono ad avere problemi di sparsità di ricompensa e collasso della modalità che influenzano la qualità e la diversità dei campioni generati.Per affrontare i problemi, proponiamo un nuovo paradigma di apprendimento auto-avversariale (SAL) per migliorare le prestazioni dei GANs nella generazione di testo.In contrasto con i GANs standard che utilizzano un classificatore binario come discriminatore per prevedere se un campione è reale o generato, SAL impiega un discriminatore comparativo che è un classificatore a coppie per confrontare la qualità del testo tra una coppia di campioni. Durante l'addestramento, SAL premia il generatore quando la sua frase attualmente generata risulta essere migliore dei suoi campioni generati in precedenza.Questo meccanismo di ricompensa di auto-miglioramento permette al modello di ricevere crediti più facilmente ed evitare di collassare verso il numero limitato di campioni reali, che non solo aiuta ad alleviare il problema della sparsità di ricompensa ma riduce anche il rischio di collasso della modalità.Gli esperimenti sui set di dati di riferimento per la generazione di testo mostrano che il nostro approccio proposto migliora sostanzialmente sia la qualità che la diversità, e produce prestazioni più stabili rispetto alle GANs precedenti per la generazione di testo.
Determinare il numero di dimensioni latenti è un problema onnipresente nel machinelearning.In questo studio, introduciamo un nuovo metodo che si basa su SVD per scoprire il numero di dimensioni latenti.Il principio generale dietro il metodo è quello di confrontare la curva dei valori singolari della decomposizione SVD di un set di dati con la curva del set di dati randomizzato.Il numero dedotto di dimensioni latenti corrisponde al punto di incrocio delle due curve. Per valutare la nostra metodologia, la confrontiamo con metodi concorrenti come Kaisers eigenvalue-greater-than-onerule (K1), Parallel Analysis (PA), Velicers MAP test (Minimum Average Partial).Confrontiamo anche il nostro metodo con la tecnica Silhouette Width (SW) che viene utilizzata in diversi metodi di clustering per determinare il numero ottimale di cluster.Il risultato su dati sintetici mostra che Parallel Analysis e il nostro metodo hanno risultati simili e più accurati degli altri metodi, e che il nostro metodo è leggermente migliore del metodo Parallel Analysis per i set di dati radi.
I modelli di apprendimento profondo sono spesso sensibili agli attacchi avversari, dove i campioni di input accuratamente progettati possono far sì che il sistema produca decisioni errate.Qui ci concentriamo sul problema del rilevamento degli attacchi, piuttosto che sulla classificazione robusta, poiché rilevare che si verifica un attacco può essere ancora più importante che evitare l'errore di classificazione.Ci basiamo sui progressi nella spiegabilità, dove le spiegazioni di tipo activity-map sono utilizzate per giustificare e convalidare le decisioni, evidenziando le caratteristiche che sono coinvolte in una decisione di classificazione.L'osservazione chiave è che è difficile creare spiegazioni per decisioni errate. Proponiamo EXAID, un nuovo approccio di rilevamento degli attacchi, che utilizza la spiegabilità del modello per identificare le immagini le cui spiegazioni non sono coerenti con la classe prevista. In particolare, usiamo SHAP, che utilizza i valori di Shapley nello spazio dell'immagine di input, per identificare quali caratteristiche di input contribuiscono a una decisione di classe. È interessante notare che questo approccio non richiede di modificare il modello di attacco, e può essere applicato senza modellare un attacco specifico. Può quindi essere applicato con successo per rilevare attacchi sconosciuti, che erano sconosciuti al momento in cui il modello di rilevamento è stato progettato. Valutiamo EXAID su due set di dati di riferimento CIFAR-10 e SVHN, e contro tre tecniche di attacco leader, FGSM, PGD e C&W. Troviamo che EXAID migliora rispetto ai metodi di rilevamento SoTA con un ampio margine su una vasta gamma di livelli di rumore, migliorando il rilevamento dal 70% a oltre il 90% per piccole perturbazioni.
Descriviamo un approccio semplice e generale di compressione dei pesi delle reti neurali, in cui i parametri della rete (pesi e bias) sono rappresentati in uno spazio "latente", che equivale a una riparametrizzazione. Questo spazio è dotato di un modello di probabilità appreso, che viene utilizzato per imporre una penalità di entropia sulla rappresentazione dei parametri durante la formazione, e per comprimere la rappresentazione utilizzando un semplice codificatore aritmetico dopo la formazione. Valutiamo il metodo sui benchmark di classificazione MNIST, CIFAR-10 e ImageNet usando sei diverse architetture di modello. I nostri risultati mostrano che la compressione del modello allo stato dell'arte può essere ottenuta in modo scalabile e generale senza richiedere procedure complesse come l'addestramento a più stadi.
Le reti neurali possono convergere più velocemente con l'aiuto di una strategia di selezione dei lotti più intelligente.A questo proposito, proponiamo Ada-Boundary, un nuovo algoritmo adattivo di selezione dei lotti che costruisce un mini-lotto efficace in base al progresso di apprendimento del modello.La nostra idea chiave è quella di presentare campioni confusi quale sia la vera etichetta.Quindi, i campioni vicino al confine della decisione corrente sono considerati come i più efficaci per accelerare la convergenza. Approfittando del nostro design, Ada-Boundary mantiene il suo dominio in vari gradi di difficoltà di formazione.Dimostriamo il vantaggio di Ada-Boundary con ampi esperimenti utilizzando due reti neurali convoluzionali per tre set di dati di riferimento.I risultati degli esperimenti mostrano che Ada-Boundary migliora il tempo di formazione fino al 31,7% rispetto alla strategia dello stato dell'arte e fino al 33,5% rispetto alla strategia di base.
Lo stato dell'arte della classificazione degli eventi sonori si affida alle reti neurali per imparare le associazioni tra le etichette di classe e le registrazioni audio all'interno di un set di dati. Questi set di dati tipicamente definiscono un'ontologia per creare una struttura che mette in relazione queste classi di suoni con classi superiori più astratte. Proponiamo due architetture di rete neurale basate sull'ontologia per la classificazione degli eventi sonori, definendo un quadro per progettare semplici architetture di rete che conservano una struttura ontologica. Le reti sono addestrate e valutate utilizzando due dei più comuni set di dati di classificazione degli eventi sonori.
In contrasto con le reti completamente connesse, le reti neurali convoluzionali (CNN) raggiungono l'efficienza imparando i pesi associati ai filtri locali con un'estensione spaziale finita.Un'implicazione di questo è che un filtro può sapere cosa sta guardando, ma non dove è posizionato nell'immagine.Le informazioni riguardanti la posizione assoluta sono intrinsecamente utili, ed è ragionevole assumere che le CNN profonde possano implicitamente imparare a codificare queste informazioni se c'è un mezzo per farlo. In questo articolo, mettiamo alla prova questa ipotesi rivelando il sorprendente grado di informazione sulla posizione assoluta che viene codificata nelle reti neurali comunemente usate.Una serie completa di esperimenti mostra la validità di questa ipotesi e fa luce su come e dove questa informazione viene rappresentata, offrendo al contempo indizi su dove derivano le informazioni posizionali nelle CNN profonde.
Il parsing semantico, che mappa una frase del linguaggio naturale in una rappresentazione formale leggibile dalla macchina del suo significato, è altamente vincolato dai limitati dati di allenamento annotati. Ispirati dall'idea di grossolano-a-fine, proponiamo una rete neurale generale-dettagliata (GDNN) incorporando lo schizzo cross-domain (CDS) tra gli enunciati e le loro forme logiche.Per gli enunciati in diversi domini, la rete generale estrarrà CDS usando un modello encoder-decoder in una configurazione di apprendimento multi-task. I nostri esperimenti mostrano che, rispetto all'apprendimento diretto multi-task, il CDS ha migliorato le prestazioni nel compito di parsing semantico che converte le richieste degli utenti in un linguaggio di rappresentazione del significato (MRL); inoltre usiamo gli esperimenti per illustrare che il CDS funziona aggiungendo alcuni vincoli al processo di decodifica del target, il che dimostra ulteriormente l'efficacia e la razionalità del CDS.
La capacità di apprendimento di diverse architetture neurali può essere caratterizzata direttamente da misure computabili della complessità dei dati. In questo articolo, riformuliamo il problema della selezione dell'architettura per capire come i dati determinano le architetture più espressive e generalizzabili adatte a quei dati, al di là del bias induttivo. Dopo aver suggerito la topologia algebrica come misura della complessità dei dati, mostriamo che il potere di una rete di esprimere la complessità topologica di un set di dati nel suo confine decisionale è un fattore strettamente limitante nella sua capacità di generalizzare. La nostra analisi empirica mostra che ad ogni livello di complessità dei dati, le reti neurali esibiscono transizioni e stratificazioni di fase topologica. Questa osservazione ci ha permesso di collegare la teoria esistente a congetture empiricamente guidate sulla scelta di architetture per reti neurali a singolo strato nascosto.
Un esempio in chimica è la progettazione di materiali e molecole organiche su misura, che richiede metodi efficienti per esplorare lo spazio chimico. Presentiamo un algoritmo genetico (GA) che è migliorato con un modello discriminatore basato su una rete neurale (DNN) per migliorare la diversità delle molecole generate e allo stesso tempo guidare il GA.Mostriamo che il nostro algoritmo supera altri modelli generativi in compiti di ottimizzazione.Presentiamo inoltre un modo per aumentare l'interpretabilità degli algoritmi genetici, che ci ha aiutato a derivare principi di progettazione
Bidirectional Encoder Representations from Transformers (BERT) raggiungono risultati allo stato dell'arte in una varietà di compiti di Natural Language Processing.Tuttavia, la comprensione del loro funzionamento interno è ancora insufficiente e insoddisfacente.Per comprendere meglio BERT e altri modelli basati su Transformer, presentiamo un'analisi layer-wise degli stati nascosti di BERT. A differenza della ricerca precedente, che si concentra principalmente sulla spiegazione dei modelli Transformer attraverso i loro pesi di \hbox{attenzione}, noi sosteniamo che gli stati nascosti contengono informazioni altrettanto preziose.In particolare, la nostra analisi si concentra sui modelli messi a punto sul compito di Question Answering (QA) come esempio di un complesso compito a valle. A tal fine, applichiamo una serie di compiti di sondaggio generali e specifici di QA che rivelano le informazioni memorizzate in ogni strato di rappresentazione. La nostra analisi qualitativa delle visualizzazioni degli stati nascosti fornisce ulteriori informazioni sul processo di ragionamento del BERT. I nostri risultati mostrano che le trasformazioni all'interno di BERT passano attraverso fasi che sono legate ai tradizionali compiti della pipeline.Il sistema può quindi implicitamente incorporare informazioni specifiche del compito nelle sue rappresentazioni di token.Inoltre, la nostra analisi rivela che il fine-tuning ha poco impatto sulle capacità semantiche dei modelli e che gli errori di previsione possono essere riconosciuti nelle rappresentazioni vettoriali anche dei primi strati.
Il nostro approccio sfrutta i dati dimostrativi per assistere un agente di rinforzo nell'apprendimento per risolvere un'ampia gamma di compiti, soprattutto quelli precedentemente irrisolti, e addestra le politiche visuomotorie end-to-end per imparare una mappatura diretta dagli input della telecamera RGB alle velocità congiunte. I nostri esperimenti indicano che il nostro approccio di rinforzo e imitazione può risolvere compiti di manipolazione del robot ricchi di contatti che né il metodo di apprendimento di rinforzo né quello di imitazione allo stato dell'arte possono risolvere da soli.Illustriamo anche che queste politiche hanno raggiunto il trasferimento sim2reale a zero colpi allenandosi con grandi variazioni visive e dinamiche.
Gli ensemble, in cui più reti neurali sono addestrate individualmente e le loro previsioni sono mediate, hanno dimostrato di avere molto successo per migliorare sia l'accuratezza che l'incertezza predittiva delle singole reti neurali.Tuttavia, il costo di un ensemble sia per l'addestramento che per i test aumenta linearmente con il numero di reti. In questo articolo, proponiamo BatchEnsemble, un metodo di ensemble i cui costi computazionali e di memoria sono significativamente più bassi dei tipici ensemble. BatchEnsemble raggiunge questo risultato definendo ogni matrice di peso come il prodotto Hadamard di un peso condiviso tra tutti i membri dell'ensemble e una matrice rank-one per membro. A differenza degli ensemble, BatchEnsemble non è solo parallelizzabile tra i dispositivi, dove un dispositivo allena un membro, ma anche parallelizzabile all'interno di un dispositivo, dove più membri dell'ensemble sono aggiornati simultaneamente per un dato mini-batch. Attraverso CIFAR-10, CIFAR-100, traduzione WMT14 EN-DE/EN-FR, e compiti di banditi contestuali, BatchEnsemble produce una precisione e incertezze competitive come gli ensemble tipici; l'aumento di velocità al momento del test è 3X e la riduzione della memoria è 3X ad un ensemble di dimensioni 4. Applichiamo anche BatchEnsemble all'apprendimento permanente, dove su Split-CIFAR-100, BatchEnsemble fornisce prestazioni paragonabili alle reti neurali progressive pur avendo un costo computazionale e di memoria molto più basso. Mostriamo inoltre che BatchEnsemble può facilmente scalare fino all'apprendimento permanente su Split-ImageNet che coinvolge 100 compiti di apprendimento sequenziale.
Presentiamo un nuovo metodo di stima della ricompensa che si basa su un campione finito di traiettorie di stato ottimali da dimostrazioni di esperti e può essere utilizzato per guidare un agente a imitare il comportamento dell'esperto. Le traiettorie di stato ottimali sono utilizzate per imparare un modello generativo o predittivo della distribuzione degli stati "buoni". Con questa funzione di ricompensa dedotta, eseguiamo l'apprendimento di rinforzo standard nel ciclo interno per guidare l'agente ad imparare il compito dato.Le valutazioni sperimentali attraverso una gamma di compiti dimostrano che il metodo proposto produce prestazioni superiori rispetto all'apprendimento di rinforzo standard con ricompense sia complete che sparse costruite a mano.Inoltre, dimostriamo che il nostro metodo permette con successo ad un agente di imparare buone azioni direttamente da video di giocatori esperti di giochi come Super Mario Bros e Flappy Bird.
Per spiegare come il modello gestisce la semantica composizionale di parole e frasi, studiamo il problema della spiegazione gerarchica. Evidenziamo che la sfida chiave è calcolare l'importanza non additiva e indipendente dal contesto per le singole parole e frasi. Mostriamo che alcuni sforzi precedenti sulle spiegazioni gerarchiche, ad esempio la decomposizione contestuale, non soddisfano matematicamente le proprietà desiderate, portando a una qualità della spiegazione incoerente in diversi modelli. In questo articolo, proponiamo un modo formale per quantificare l'importanza di ogni parola o frase per generare spiegazioni gerarchiche.Modifichiamo gli algoritmi di decomposizione contestuale secondo la nostra formulazione, e proponiamo un algoritmo di spiegazione modello-agnostico con prestazioni competitive.La valutazione umana e la valutazione automatica delle metriche sia sui modelli LSTM che sui modelli BERT Transformer fine-tuned su più set di dati mostrano che i nostri algoritmi superano robustamente i lavori precedenti sulle spiegazioni gerarchiche.Mostriamo che i nostri algoritmi aiutano a spiegare la compostezza della semantica, estraggono regole di classificazione, e migliorano la fiducia umana dei modelli.
La discesa del gradiente stocastico (SGD) con slancio stocastico è popolare nell'ottimizzazione stocastica non convessa e in particolare per l'addestramento delle reti neurali profonde. In SGD standard, i parametri sono aggiornati migliorando lungo il percorso del gradiente all'iterazione corrente su un gruppo di esempi, dove l'aggiunta di un termine di ``momento'' distorce l'aggiornamento nella direzione del cambiamento precedente nei parametri. Nell'ottimizzazione convessa non stocastica si può dimostrare che un aggiustamento del momentum riduce il tempo di convergenza in molte impostazioni, ma tali risultati sono stati sfuggenti nelle impostazioni stocastiche e non convesse. In questo articolo proponiamo una risposta: il momentum stocastico migliora l'addestramento delle reti profonde perché modifica SGD per sfuggire più velocemente ai punti di sella e, di conseguenza, per trovare più rapidamente un punto stazionario di secondo ordine. I nostri risultati teorici fanno anche luce sulla questione correlata di come scegliere il parametro ideale del momentum - la nostra analisi suggerisce che $\beta \in [0,1]$ dovrebbe essere grande (vicino a 1), il che corrisponde ai risultati empirici.
Le GAN forniscono una struttura per l'addestramento di modelli generativi che imitano una distribuzione di dati; tuttavia, in molti casi desideriamo addestrare un modello generativo per ottimizzare qualche funzione obiettivo ausiliaria all'interno dei dati che genera, ad esempio per rendere le immagini più gradevoli dal punto di vista estetico; in alcuni casi, queste funzioni obiettivo sono difficili da valutare, ad esempio possono richiedere un'interazione umana. Qui, sviluppiamo un sistema per addestrare in modo efficiente una GAN per aumentare un tasso generico di interazioni positive dell'utente, per esempio le valutazioni estetiche.Per fare questo, costruiamo un modello del comportamento umano nel dominio mirato da un insieme relativamente piccolo di interazioni, e poi usiamo questo modello comportamentale come una funzione di perdita ausiliaria per migliorare il modello generativo.Come prova di concetto, dimostriamo che questo sistema ha successo nel migliorare i tassi di interazione positiva simulati da una varietà di obiettivi, e caratterizziamo s
L'apprendimento semi-supervisionato (SSL) è uno studio che sfrutta in modo efficiente una grande quantità di dati non etichettati per migliorare le prestazioni in condizioni di dati etichettati limitati.La maggior parte dei metodi SSL convenzionali assume che le classi di dati non etichettati siano incluse nell'insieme delle classi di dati etichettati. Inoltre, questi metodi non separano i campioni inutili non etichettati e usano tutti i dati non etichettati per l'apprendimento, il che non è adatto a situazioni realistiche.In questo documento, proponiamo un metodo SSL chiamato auto-addestramento selettivo (SST), che decide selettivamente se includere ogni campione non etichettato nel processo di formazione. Per i problemi SSL convenzionali che hanno a che fare con dati in cui sia i campioni etichettati che quelli non etichettati condividono le stesse categorie di classe, il metodo proposto non solo è paragonabile ad altri algoritmi SSL convenzionali ma può anche essere combinato con altri algoritmi SSL. Mentre i metodi convenzionali non possono essere applicati ai nuovi problemi SSL in cui i dati separati non condividono le classi, il nostro metodo non mostra alcun degrado delle prestazioni anche se le classi dei dati non etichettati sono diverse da quelle dei dati etichettati.
Le reti neurali generative profonde si sono dimostrate efficaci sia nella modellazione condizionale che incondizionale di distribuzioni di dati complesse.La generazione condizionale permette un controllo interattivo, ma la creazione di nuovi controlli spesso richiede una riqualificazione costosa.In questo articolo, sviluppiamo un metodo per condizionare la generazione senza riqualificare il modello.Imparando post-hoc i vincoli latenti, le funzioni di valore identificano le regioni nello spazio latente che generano output con gli attributi desiderati, possiamo condizionare il campione da queste regioni con un'ottimizzazione basata sul gradiente o funzioni attoriali ammortizzate. Combinando i vincoli degli attributi con un vincolo universale di "realismo", che impone la somiglianza con la distribuzione dei dati, generiamo immagini condizionali realistiche da un autocodificatore variazionale incondizionato. Inoltre, usando l'ottimizzazione basata sul gradiente, dimostriamo le trasformazioni che conservano l'identità e che fanno il minimo aggiustamento nello spazio latente per modificare gli attributi di un'immagine. Infine, con sequenze discrete di note musicali, dimostriamo la generazione condizionale zero-shot, imparando i vincoli latenti in assenza di dati etichettati o di una funzione di ricompensa differenziabile.
I recenti progressi nell'hardware e nella metodologia per l'addestramento delle reti neurali hanno inaugurato una nuova generazione di grandi reti addestrate su dati abbondanti. Questi modelli hanno ottenuto notevoli guadagni nell'accuratezza in molte attività NLP. Tuttavia, questi miglioramenti nell'accuratezza dipendono dalla disponibilità di risorse computazionali eccezionalmente grandi che richiedono un consumo energetico altrettanto sostanziale. Di conseguenza, questi modelli sono costosi da addestrare e sviluppare, sia dal punto di vista finanziario, a causa del costo dell'hardware e dell'elettricità o del tempo di calcolo del cloud, sia dal punto di vista ambientale, a causa dell'impronta di carbonio richiesta per alimentare il moderno hardware di elaborazione tensoriale. In questo articolo portiamo questo problema all'attenzione dei ricercatori di PNL quantificando i costi finanziari e ambientali approssimativi dell'addestramento di una varietà di modelli di reti neurali di recente successo per il PNL.
Molti modelli basati sul Variational Autoencoder sono proposti per ottenere variabili latenti disentangled nell'inferenza.Tuttavia, la maggior parte del lavoro attuale si concentra sulla progettazione di potenti regolatori di disentangling, mentre il numero dato di dimensioni per la rappresentazione latente all'inizializzazione potrebbe gravemente inﬂuenzare il disentanglement. Così, viene introdotto un meccanismo di pruning che mira a cercare automaticamente la dimensione intrinseca dei dati, promuovendo nel contempo rappresentazioni disentangolate. Il metodo proposto è convalidato su MPI3D e MNIST per avanzare i metodi allo stato dell'arte nel disentanglement, ricostruzione e robustezza. Il codice è fornito su https://github.com/WeyShi/FYP-of-Disentanglement.
Esploriamo le proprietà dei modelli linguistici ricorrenti a livello di byte.Quando sono date quantità sufficienti di capacità, dati di addestramento e tempo di calcolo, le rappresentazioni apprese da questi modelli includono caratteristiche disentangled corrispondenti a concetti di alto livello.In particolare, troviamo una singola unità che esegue l'analisi del sentimento.Queste rappresentazioni, apprese in modo non supervisionato, raggiungono lo stato dell'arte sul sottoinsieme binario della Stanford Sentiment Treebank. Sono anche molto efficienti dal punto di vista dei dati: quando si usa solo una manciata di esempi etichettati, il nostro approccio eguaglia le prestazioni di forti baseline addestrate su set di dati completi. Dimostriamo anche che l'unità di sentimento ha un'influenza diretta sul processo generativo del modello: semplicemente fissando il suo valore come positivo o negativo si generano campioni con il corrispondente sentimento positivo o negativo.
 Il campionamento di variabili latenti discrete può risultare in stimatori di gradiente ad alta varianza per due ragioni principali: 1) la ramificazione sui campioni all'interno del modello, e 2) la mancanza di una derivata pathwise per i campioni. Mentre gli attuali metodi all'avanguardia impiegano schemi di controllo-variati per il primo e metodi di rilassamento continuo per il secondo, la loro utilità è limitata dalla complessità di implementare e addestrare schemi di controllo-variati efficaci e dalla necessità di valutare (potenzialmente in modo esponenziale) molti percorsi di ramificazione nel modello. Qui, rivisitiamo l'algoritmo Reweighted Wake Sleep (RWS; Bornschein e Bengio, 2015), e attraverso ampie valutazioni, dimostriamo che esso aggira entrambi questi problemi, superando gli attuali metodi allo stato dell'arte nell'apprendimento di modelli discreti a variabili latenti. Inoltre, osserviamo che, a differenza dell'Importance-weighted Autoencoder, RWS apprende modelli e reti di inferenza migliori con l'aumentare del numero di particelle, e che i suoi benefici si estendono anche ai modelli continui a latente variabile.I nostri risultati suggeriscono che RWS è un'alternativa competitiva, spesso preferibile, per apprendere modelli generativi profondi.
L'obiettivo del deep extreme multi-label learning è quello di imparare congiuntamente rappresentazioni di caratteristiche e classificatori per etichettare automaticamente i punti dati con il sottoinsieme più rilevante di etichette da un set di etichette estremamente grande. Questo articolo sviluppa l'algoritmo DeepXML che affronta entrambe le limitazioni introducendo una nuova architettura che divide la formazione delle etichette di testa e di coda.  DeepXML aumenta l'accuratezza attraverso (a) l'apprendimento di embeddings di parole sulle etichette di testa e il loro trasferimento attraverso una nuova connessione residua alle etichette di coda impoverite di dati; (b) l'aumento della quantità di dati negativi disponibili per l'addestramento estendendo le tecniche di sotto-campionamento negativo allo stato dell'arte; e (c) la ri-assegnazione dell'insieme di etichette predette per eliminare i negativi più difficili per il classificatore originale.Tutti questi contributi sono implementati in modo efficiente estendendo l'algoritmo Slice altamente scalabile per embeddings pre-addestrati per imparare l'architettura DeepXML proposta. Come risultato, DeepXML potrebbe scalare in modo efficiente a problemi che coinvolgono milioni di etichette che erano al di là dello stato dell'arte dei classificatori estremi profondi in quanto potrebbe essere più di 10 volte più veloce nell'addestramento rispetto a XML-CNN e AttentionXML.Allo stesso tempo, DeepXML è stato anche determinato empiricamente per essere fino al 19% più accurato rispetto alle tecniche principali per abbinare le query dei motori di ricerca alle frasi di offerta degli inserzionisti.
La stima robusta sotto il modello di contaminazione di Huber è diventata un argomento importante nella statistica e nell'informatica teorica. Le procedure ottimali come la mediana di Tukey e altri stimatori basati su funzioni statistiche di profondità sono impraticabili a causa della loro intrattabilità computazionale. In questo articolo, stabiliamo un'intrigante connessione tra le f-GAN e varie funzioni di profondità attraverso la lente del f-Learning.Simile alla derivazione delle f-GAN, mostriamo che queste funzioni di profondità che portano a stimatori robusti ottimali per il tasso possono essere tutte viste come limiti inferiori variazionali della distanza di variazione totale nel quadro del f-Learning. In particolare, mostriamo che una JS-GAN che usa una rete neurale discriminatrice con almeno uno strato nascosto è in grado di raggiungere il tasso minimox di stima robusta della media sotto il modello di contaminazione di Huber, ed è interessante notare che gli strati nascosti della struttura della rete neurale nella classe discriminatrice sono necessari per una stima robusta.
Long Short-Term Memory (LSTM) è uno dei modelli di sequenza più potenti.Nonostante le forti prestazioni, tuttavia, manca la bella interpretabilità come nei modelli di spazio di stato.In questo articolo, presentiamo un modo per combinare il meglio dei due mondi introducendo State Space LSTM (SSL), che generalizza il lavoro precedente \cite{zaheer2017latent} di combinare modelli di argomento con LSTM. Tuttavia, a differenza di \cite{zaheer2017latent}, non facciamo alcuna ipotesi di fattorizzazione nel nostro algoritmo di inferenza.Presentiamo un campionatore efficiente basato sul metodo Monte Carlo sequenziale (SMC) che attinge direttamente dal posteriore congiunto.I risultati sperimentali confermano la superiorità e la stabilità di questo algoritmo di inferenza SMC su una varietà di domini.
Dato un grande database di concetti, ma solo uno o pochi esempi di ciascuno, possiamo imparare modelli per ogni concetto che siano non solo generalizzabili, ma interpretabili? In questo lavoro, ci proponiamo di affrontare questo problema attraverso l'induzione gerarchica di programmi bayesiani. Presentiamo un nuovo algoritmo di apprendimento che può dedurre i concetti come brevi programmi stocastici generativi, mentre impariamo un priore globale sui programmi per migliorare la generalizzazione e una rete di riconoscimento per un'inferenza efficiente. Il nostro algoritmo, Wake-Sleep-Remember (WSR), combina l'apprendimento a gradiente per i parametri continui con la ricerca neuralmente guidata sui programmi.Mostriamo che WSR impara programmi latenti convincenti in due domini simbolici difficili: automi cellulari e kernel del processo gaussiano.Raccogliamo e valutiamo anche su un nuovo dataset, Text-Concepts, per scoprire modelli strutturati in dati di testo naturale.
La conoscenza della funzione delle proteine è necessaria in quanto fornisce un quadro chiaro dei processi biologici. Tuttavia, ci sono molte sequenze di proteine trovate e aggiunte ai database, ma manca l'annotazione funzionale. Gli esperimenti di laboratorio richiedono una notevole quantità di tempo per l'annotazione delle sequenze. Nel nostro lavoro, abbiamo raccolto i dati da Swiss-Prot che contengono 40433 proteine raggruppate in 30 famiglie, le passiamo alla rete neurale ricorrente (RNN), alla memoria a breve termine lunga (LSTM) e al modello GRU (gated recurrent unit) e le confrontiamo applicando il trigramma con la rete neurale profonda e la rete neurale superficiale sullo stesso set di dati, attraverso questo approccio, abbiamo potuto raggiungere un massimo di circa il 78% di precisione per la classificazione delle famiglie di proteine. 
Gli algoritmi per la STD sono spesso mirati a massimizzare il divario tra i punteggi degli esempi positivi e negativi, quindi si concentrano nel garantire che gli enunciati in cui il termine appare siano classificati più alti degli enunciati in cui il termine non appare, ma non determinano una soglia di rilevamento tra i due. Il vantaggio di minimizzare questa funzione di perdita durante l'addestramento è che mira a massimizzare non solo i punteggi di classificazione relativi, ma anche a regolare il sistema in modo da utilizzare una soglia fissa, migliorando così la robustezza del sistema e massimizzando i tassi di precisione di rilevamento. Usiamo la nuova funzione di perdita nell'impostazione di predizione strutturata ed estendiamo l'algoritmo discriminativo di individuazione delle parole chiave per l'apprendimento del rilevatore di termini parlati con una singola soglia per tutti i termini.dimostriamo ulteriormente l'efficacia della nuova funzione di perdita applicandola su una rete neurale profonda siamese in un'impostazione debolmente supervisionata per il rilevamento di termini parlati basato su template, di nuovo con una singola soglia fissa.gli esperimenti con i corpora TIMIT, WSJ e Switchboard hanno mostrato che il nostro approccio non solo migliora i tassi di precisione quando viene utilizzata una soglia fissa ma ottiene anche un'area sotto la curva (AUC) più alta.
L'apprendimento federato implica l'apprendimento congiunto su partizioni massicciamente distribuite di dati generati su dispositivi remoti; minimizzare ingenuamente una funzione di perdita aggregata in una tale rete può avvantaggiare o svantaggiare in modo sproporzionato alcuni dei dispositivi. In questo lavoro, proponiamo q-Fair Federated Learning (q-FFL), un nuovo obiettivo di ottimizzazione ispirato alle strategie di allocazione delle risorse nelle reti wireless che incoraggia una distribuzione più equa dell'accuratezza tra i dispositivi nelle reti federate.Per risolvere q-FFL, ideiamo un metodo scalabile, q-FedAvg, che può essere eseguito nelle reti federate.Convalidiamo sia la maggiore equità e flessibilità di q-FFL che l'efficienza di q-FedAvg attraverso simulazioni su dataset federati.
Proponiamo un nuovo modello di autocodifica chiamato Pairwise Augmented GANs.Addestriamo un generatore e un codificatore congiuntamente e in maniera avversaria.La rete generatrice impara a campionare oggetti realistici.A sua volta, la rete codificatrice allo stesso tempo è addestrata a mappare la vera distribuzione dei dati al priore nello spazio latente.Per assicurare buone ricostruzioni, introduciamo una perdita di ricostruzione avversaria aumentata. Qui addestriamo un discriminatore per distinguere due tipi di coppie: un oggetto con il suo incremento e quello con la sua ricostruzione.Mostriamo che tale perdita avversaria confronta gli oggetti in base al contenuto piuttosto che alla corrispondenza esatta.Dimostriamo sperimentalmente che il nostro modello genera campioni e ricostruzioni di qualità competitiva con lo state-of-the-art sui dataset MNIST, CIFAR10, CelebA e raggiunge buoni risultati quantitativi su CIFAR10.
Ispirandoci al dropout, uno strumento popolare per la regolarizzazione e l'ensemble di modelli, assegniamo priori sparsi ai pesi nelle reti neurali profonde (DNN) al fine di ottenere un "dropout" automatico ed evitare l'over-fitting. Campionando alternativamente dalla distribuzione posteriore attraverso il gradiente stocastico Markov Chain Monte Carlo (SG-MCMC) e ottimizzando le variabili latenti attraverso l'approssimazione stocastica (SA), si dimostra che la traiettoria dei pesi target converge alla vera distribuzione posteriore condizionata dalle variabili latenti ottimali. Questo assicura una più forte regolarizzazione sullo spazio dei parametri sovra-applicati e una più accurata quantificazione dell'incertezza sulle variabili decisive. Simulazioni di regressioni large-p-small-n mostrano la robustezza di questo metodo quando applicato a modelli con variabili latenti. Inoltre, la sua applicazione alle reti neurali convoluzionali (CNN) porta a prestazioni allo stato dell'arte sui dataset MNIST e Fashion MNIST e a una migliore resistenza agli attacchi avversari.
L'attività di popolazioni di neuroni sensoriali porta informazioni di stimolo in entrambe le dimensioni temporali e spaziali, il che pone la questione di come rappresentare in modo compatto tutte le informazioni che i codici della popolazione portano attraverso tutte queste dimensioni. In particolare, abbiamo esteso la decomposizione tensoriale spazio-temporale precedentemente utilizzata, basata sulla fattorizzazione della matrice non negativa, per scontare in modo efficiente l'attività della linea di base pre-stimolo. Su dati registrati da cellule gangliari retiniche con una forte linea di base pre-stimolo, abbiamo dimostrato che in situazioni in cui lo stimolo provoca un forte cambiamento nella frequenza di accensione, le nostre estensioni producono un aumento delle prestazioni di decodifica dello stimolo.I nostri risultati suggeriscono quindi che prendere in considerazione la linea di base può essere importante per trovare una rappresentazione compatta ricca di informazioni dell'attività neurale.
Proponiamo una struttura per la compressione di immagini appresa estrema basata su Generative Adversarial Networks (GANs), ottenendo immagini visivamente piacevoli a bitrate significativamente inferiori rispetto ai metodi precedenti.Questo è reso possibile attraverso la nostra formulazione GAN di compressione appresa combinata con un generatore/decoder che opera sull'immagine a piena risoluzione ed è addestrato in combinazione con un discriminatore multiscala. Inoltre, se una mappa di etichette semantiche dell'immagine originale è disponibile, il nostro metodo può sintetizzare completamente le regioni non importanti nell'immagine decodificata come strade e alberi dalla mappa di etichette, richiedendo quindi solo la memorizzazione della regione conservata e la mappa di etichette semantiche.Uno studio utente conferma che per bitrate bassi, il nostro approccio è preferito ai metodi all'avanguardia, anche quando usano più del doppio dei bit.
Nell'imparare a classificare, si è interessati a ottimizzare l'ordinamento globale di una lista di elementi in base alla loro utilità per gli utenti.Gli approcci più diffusi imparano una funzione di punteggio che assegna un punteggio agli elementi individualmente (cioè senza il contesto di altri elementi nella lista) ottimizzando una perdita puntiforme, a coppie o a lista.La lista viene poi ordinata in ordine decrescente dei punteggi.Le possibili interazioni tra elementi presenti nella stessa lista sono prese in considerazione nella fase di addestramento a livello di perdita.Tuttavia, durante l'inferenza, gli elementi sono valutati individualmente, e le possibili interazioni tra loro non sono considerate. In questo articolo, proponiamo un modello di rete neurale context-aware che impara i punteggi degli elementi applicando un meccanismo di auto-attenzione: la rilevanza di un dato elemento è quindi determinata nel contesto di tutti gli altri elementi presenti nella lista, sia in fase di addestramento che di inferenza. Infine, dimostriamo empiricamente che l'architettura neurale basata sull'auto-attenzione guadagna significativamente in termini di prestazioni rispetto alle baseline di Multi-Layer Perceptron: questo effetto è coerente con le popolari perdite pointwise, pairwise e listwise su dataset con feedback di rilevanza sia implicito che esplicito.
Proponiamo un'architettura algoritmica di apprendimento attivo, capace di organizzare il suo processo di apprendimento al fine di raggiungere un campo di compiti complessi imparando sequenze di politiche motorie primitive: Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB).L'allievo può generalizzare sulla sua esperienza per imparare continuamente nuovi risultati, scegliendo attivamente cosa e come imparare guidato da misure empiriche dei propri progressi.In questo articolo, stiamo considerando l'apprendimento di un insieme di risultati complessi interconnessi gerarchicamente organizzati.Introduciamo una nuova struttura chiamata "procedure", che consente la scoperta autonoma di come combinare le competenze precedentemente apprese per imparare politiche motorie sempre più complesse (combinazioni di politiche motorie primitive). La nostra architettura può decidere attivamente su quale risultato concentrarsi e quale strategia di esplorazione applicare; queste strategie potrebbero essere l'esplorazione autonoma o la guida sociale attiva, dove si affida all'esperienza di un insegnante umano che fornisce dimostrazioni su richiesta dell'allievo. Mostriamo su un ambiente simulato che la nostra nuova architettura è in grado di affrontare l'apprendimento di politiche motorie complesse, di adattare la complessità delle sue politiche al compito a portata di mano e che le nostre "procedure" aumentano la capacità dell'agente di imparare compiti complessi.
Monte Carlo Tree Search (MCTS) ha raggiunto risultati impressionanti su una serie di ambienti discreti, come Go, Mario e giochi Arcade, ma non ha ancora realizzato il suo vero potenziale nei domini continui.In questo lavoro, introduciamoTPO, un metodo di ottimizzazione delle politiche basato sulla ricerca ad albero per ambienti continui.TPO adotta un approccio ibrido all'ottimizzazione delle politiche.  Costruire l'albero MCTS in uno spazio d'azione continuo e aggiornare il gradiente della politica usando traiettorie MCTS fuori dalla politica non è banale. Per superare queste sfide, proponiamo di limitare il fattore di ramificazione della ricerca ad albero disegnando solo pochi campioni di azioni dalla distribuzione della politica e definiamo una nuova funzione di perdita basata sulla media e sulle deviazioni standard delle traiettorie.  Il nostro approccio ha portato ad alcuni risultati non intuitivi.  Tuttavia, abbiamo osservato che la ricerca bootstrappingtree con una politica pre-addestrata ci permette di ottenere risultati di alta qualità con un basso fattore di ramificazione MCTS e un numero ridotto di simulazioni.Senza il bootstrapping della politica proposta, MCTS continuo richiederebbe un fattore di ramificazione molto più grande e un numero di simulazioni, rendendolo computazionalmente e proibitivamente costoso.Nei nostri esperimenti, usiamo PPO come nostro algoritmo di ottimizzazione della politica di base.TPO migliora significativamente la politica su quasi tutti i nostri benchmark.  Per esempio, in ambienti complessi come Humanoid, raggiungiamo un miglioramento di 2.5× rispetto all'algoritmo di base.
L'autoencoder variazionale (VAE) ha trovato il successo nel modellare il manifold delle immagini naturali su alcuni set di dati, permettendo di generare immagini significative mentre si interpola o si estrapola nello spazio del codice latente, ma non è chiaro se capacità simili siano fattibili per il testo considerando la sua natura discreta.In questo lavoro, indaghiamo il motivo per cui l'apprendimento non supervisionato di rappresentazioni controllabili fallisce per il testo. Troviamo che le tradizionali VAE di sequenza possono imparare rappresentazioni disentangled attraverso i loro codici latenti in una certa misura, ma spesso non riescono a decodificare correttamente quando il fattore latente viene manipolato, perché i codici manipolati spesso atterrano in buchi o regioni vacanti nello spazio latente posteriore aggregato, che la rete di decodifica non è allenata ad elaborare. Sia come convalida della spiegazione che come soluzione al problema, proponiamo di vincolare la media posteriore a un simplex di probabilità appreso, ed esegue la manipolazione all'interno di questo simplex.Il nostro metodo proposto mitiga il problema di spazio latente vacante e raggiunge il primo successo nell'apprendimento non supervisionato di rappresentazioni controllabili per il testo.Empiricamente, il nostro metodo supera significativamente le linee di base non supervisionate ed è competitivo con forti approcci supervisionati sul trasferimento dello stile del testo.Inoltre, quando si cambia il fattore latente (es, Inoltre, quando si cambia il fattore latente (ad esempio, l'argomento) durante la generazione di una lunga frase, la nostra struttura proposta può spesso completare la frase in un modo apparentemente naturale - una capacità che non è mai stata tentata dai metodi precedenti.
In questo articolo abbiamo sviluppato un modello di rete gerarchico, chiamato Hierarchical Prediction Network (HPNet) per capire come le memorie spazio-temporali possano essere apprese e codificate in una gerarchia rappresentazionale per prevedere i futuri fotogrammi video. Il modello è ispirato ai circuiti ricorrenti feedforward, feedback e laterali nel sistema visivo gerarchico dei mammiferi e presuppone che le memorie spazio-temporali siano codificate nelle connessioni ricorrenti all'interno di ogni livello e tra diversi livelli della gerarchia. Il modello contiene un percorso feed-forward che calcola e codifica le caratteristiche spazio-temporali di complessità successiva e un percorso di feedback che proietta l'interpretazione da un livello superiore al livello sottostante. All'interno di ogni livello, il percorso feed-forward e il percorso di feedback si intersecano in un circuito ricorrente gated che integra i loro segnali e gli stati della memoria interna del circuito per generare una previsione dei segnali in arrivo. La rete impara confrontando i segnali in arrivo con la sua previsione, aggiornando il suo modello interno del mondo minimizzando gli errori di previsione ad ogni livello della gerarchia nello stile dell'apprendimento predittivo auto-supervisionato. La rete elabora i dati in blocchi di fotogrammi video piuttosto che su base fotogramma per fotogramma.  Abbiamo osservato che l'interazione gerarchica nella rete introduce la sensibilità alle memorie dei modelli globali di movimento anche nella rappresentazione della popolazione delle unità nel livello più precoce. Infine, abbiamo fornito prove neurofisiologiche, mostrando che i neuroni nella corteccia visiva precoce delle scimmie sveglie mostrano una sensibilità e comportamenti molto simili. Questi risultati suggeriscono che l'apprendimento predittivo auto-supervisionato potrebbe essere un principio importante per l'apprendimento rappresentazionale nella corteccia visiva.  
Le mappe di salienza sono spesso usate per suggerire spiegazioni del comportamento degli agenti profondi di apprendimento di rinforzo (RL).Tuttavia, le spiegazioni derivate dalle mappe di salienza sono spesso non verificabili e possono essere altamente soggettive.Introduciamo un approccio empirico basato sul ragionamento controfattuale per verificare le ipotesi generate dalle mappe di salienza e mostrare che le spiegazioni suggerite dalle mappe di salienza spesso non sono supportate dagli esperimenti.I nostri esperimenti suggeriscono che le mappe di salienza sono meglio viste come uno strumento esplorativo piuttosto che uno strumento esplicativo.
Una delle questioni irrisolte nell'apprendimento profondo è la natura delle soluzioni che vengono scoperte, e si studia l'insieme delle soluzioni raggiunte dalla stessa architettura di rete, con diverse inizializzazioni casuali dei pesi e mini-batch casuali.  Sorprendentemente, tutte le istanze della rete sembrano condividere la stessa dinamica di apprendimento, per cui inizialmente gli stessi esempi di treno e di test sono correttamente riconosciuti dal modello appreso, seguiti da altri esempi che vengono appresi più o meno nello stesso ordine. Quando si estende l'indagine a collezioni eterogenee di architetture di reti neurali, ancora una volta si vede che gli esempi vengono appresi nello stesso ordine indipendentemente dall'architettura, anche se l'architettura più potente può continuare ad apprendere e quindi raggiungere una maggiore accuratezza. Questo modello di risultati rimane vero anche quando la composizione delle classi nel set di prova non è correlata al set di allenamento, per esempio, quando si utilizzano immagini naturali fuori campione o anche immagini artificiali. Per mostrare la robustezza di questi fenomeni forniamo un ampio riassunto del nostro studio empirico, che include centinaia di grafici che descrivono decine di migliaia di reti con diverse architetture NN, iper-parametri e domini.Discutiamo anche i casi in cui questo modello di somiglianza si rompe, che mostrano che la somiglianza riportata non è un artefatto dell'ottimizzazione per discesa del gradiente.Piuttosto, il modello di somiglianza osservato è caratteristico dell'apprendimento di problemi complessi con grandi reti.Infine, mostriamo che questo modello di somiglianza sembra essere fortemente correlato con una generalizzazione efficace.
Proponiamo una struttura unificata per la costruzione di rappresentazioni non supervisionate di singoli oggetti o entità (e delle loro composizioni), associando ad ogni oggetto sia una stima distributiva che una stima puntuale (vector embedding), il che è reso possibile dall'uso del trasporto ottimale, che ci permette di costruire queste stime associate mentre sfruttiamo la geometria sottostante dello spazio terrestre.Il nostro metodo offre una nuova prospettiva per costruire rappresentazioni di caratteristiche ricche e potenti che catturano simultaneamente l'incertezza (tramite una stima distributiva) e l'interpretabilità (con la mappa di trasporto ottimale). Come esempio guida, formuliamo rappresentazioni non supervisionate per il testo, in particolare per la rappresentazione delle frasi e l'individuazione di implicazione.I risultati empirici mostrano forti vantaggi ottenuti attraverso il quadro proposto.Questo approccio può essere utilizzato per qualsiasi problema non supervisionato o supervisionato (su testo o altre modalità) con una struttura di co-occorrenza, come qualsiasi dato di sequenza.Gli strumenti chiave alla base del quadro sono le distanze Wasserstein e i baricentri Wasserstein (e, da qui il titolo!).
In questo articolo, proponiamo due metodi, vale a dire Trace-norm regression (TNR) e Stable Trace-norm Analysis (StaTNA), per migliorare le prestazioni dei sistemi di raccomandazione con informazioni laterali. Inoltre, il nostro nuovo recommender framework StaTNA non solo cattura i driver latenti di basso rango comuni per le preferenze degli utenti, ma considera anche il gusto idiosincratico per i singoli utenti. Confrontiamo le prestazioni di TNR e StaTNA sui dataset MovieLens contro i modelli all'avanguardia, e dimostriamo che StaTNA e TNR in generale superano questi metodi.
Questo lavoro presenta un agente basato sull'esplorazione e l'apprendimento per imitazione capace di prestazioni all'avanguardia nel giocare a giochi per computer basati sul testo.I giochi per computer basati sul testo descrivono il loro mondo al giocatore attraverso il linguaggio naturale e si aspettano che il giocatore interagisca con il gioco usando il testo.Questi giochi sono interessanti perché possono essere visti come un banco di prova per la comprensione del linguaggio, la risoluzione dei problemi e la generazione del linguaggio da parte di agenti artificiali.Inoltre, essi forniscono un ambiente di apprendimento in cui queste abilità possono essere acquisite attraverso interazioni con un ambiente piuttosto che utilizzando corpora fissi. Un aspetto che rende questi giochi particolarmente impegnativi per gli agenti di apprendimento è lo spazio di azione combinatoriamente grande.I metodi esistenti per risolvere i giochi basati sul testo sono limitati a giochi che sono molto semplici o hanno uno spazio di azione limitato a un insieme predeterminato di azioni ammissibili.In questo lavoro, proponiamo di utilizzare l'approccio di esplorazione di Go-Explore (Ecoffet et al, I nostri esperimenti dimostrano che questo approccio supera le soluzioni esistenti nella risoluzione di giochi basati sul testo, ed è più efficiente in termini di numero di interazioni con l'ambiente.Inoltre, dimostriamo che la politica appresa può generalizzare meglio delle soluzioni esistenti a giochi non visti senza utilizzare alcuna restrizione sullo spazio di azione.
Il recente articolo "Lottery Ticket Hypothesis" di Frankle & Carbin ha mostrato che un semplice approccio alla creazione di reti rade (mantenere i pesi grandi) porta a modelli che sono addestrabili da zero, ma solo quando si parte dagli stessi pesi iniziali. Le prestazioni di queste reti spesso superano le prestazioni del modello base non rado, ma per ragioni che non sono state ben comprese.In questo articolo studiamo i tre componenti critici dell'algoritmo Lottery Ticket (LT), mostrando che ognuno può essere variato significativamente senza impattare sui risultati generali. Ablando questi fattori si ottengono nuove intuizioni sul motivo per cui le reti LT funzionano così bene come fanno.Mostriamo perché impostare i pesi a zero è importante, come i segni siano tutto ciò che serve per far allenare la rete reinizializzata, e perché il mascheramento si comporta come l'allenamento.Infine, scopriamo l'esistenza di Supermaschere, o maschere che possono essere applicate a una rete non allenata e inizializzata in modo casuale per produrre un modello con prestazioni molto migliori del caso (86% su MNIST, 41% su CIFAR-10).
In questo studio, ci siamo concentrati su uno di questi modelli di rete di auto-attenzione, vale a dire BERT, che ha ottenuto buoni risultati in termini di impilamento degli strati in diversi benchmark di comprensione linguistica. Tuttavia, in molti compiti a valle, le informazioni tra gli strati sono ignorate da BERT per il fine-tuning.Inoltre, anche se le reti di auto-attenzione sono ben note per la loro capacità di catturare le dipendenze globali, rimane spazio per il miglioramento in termini di enfatizzazione dell'importanza dei contesti locali. Alla luce di questi vantaggi e svantaggi, questo articolo propone SesameBERT, un metodo generalizzato di fine-tuning che (1) permette l'estrazione di informazioni globali tra tutti gli strati attraverso Squeeze ed Excitation e (2) arricchisce le informazioni locali catturando i contesti vicini attraverso la sfocatura gaussiana. Inoltre, abbiamo dimostrato l'efficacia del nostro approccio nel set di dati HANS, che viene utilizzato per determinare se i modelli hanno adottato euristiche superficiali invece di imparare generalizzazioni sottostanti. Gli esperimenti hanno rivelato che SesameBERT ha superato BERT rispetto al benchmark GLUE e al set di valutazione HANS.
Un numero crescente di metodi di apprendimento sono in realtà giochi differenziabili in cui i giocatori ottimizzano più obiettivi interdipendenti in parallelo - da GANs e curiosità intrinseca a RL multi-agente.Opponent shaping è un potente approccio per migliorare le dinamiche di apprendimento in questi giochi, contabilizzando l'influenza del giocatore sugli aggiornamenti degli altri. Learning with Opponent-Learning Awareness (LOLA) è un algoritmo recente che sfrutta questa risposta e porta alla cooperazione in ambienti come l'Iterated Prisoner's Dilemma.Sebbene sperimentalmente abbia successo, mostriamo che gli agenti LOLA possono esibire un comportamento "arrogante" direttamente in contrasto con la convergenza. In questo articolo presentiamo Stable Opponent Shaping (SOS), un nuovo metodo che interpola tra LOLA e una variante stabile chiamata LookAhead.dimostriamo che LookAhead converge localmente agli equilibri ed evita le selle rigide in tutti i giochi differenziabili.SOS eredita queste garanzie essenziali, mentre modella anche l'apprendimento degli avversari e costantemente corrisponde o supera sperimentalmente LOLA.
Il tasso al quale le domande mediche sono poste online supera significativamente la capacità delle persone qualificate di rispondere, lasciando molte domande senza risposta o con una risposta inadeguata.Molte di queste domande non sono uniche, e l'identificazione affidabile di domande simili consentirebbe uno schema di risposta alle domande più efficiente ed efficace.Mentre molti sforzi di ricerca si sono concentrati sul problema della similarità generale delle domande, questi approcci non si generalizzano bene al dominio medico, dove la competenza medica è spesso richiesta per determinare la similarità semantica. In questo articolo, mostriamo come un approccio semi-supervisionato di pre-addestramento di una rete neurale su coppie domanda-risposta medica sia un compito intermedio particolarmente utile per l'obiettivo finale di determinare la somiglianza della domanda medica.Mentre altri compiti di pre-addestramento producono un'accuratezza inferiore al 78,7% su questo compito, il nostro modello raggiunge un'accuratezza dell'82,6% con lo stesso numero di esempi di formazione, un'accuratezza dell'80,0% con un set di formazione molto più piccolo, e un'accuratezza dell'84,5% quando viene utilizzato l'intero corpus di dati medici domanda-risposta.
Studiamo il vantaggio di condividere le rappresentazioni tra i compiti per consentire l'uso efficace delle reti neurali profonde nel Multi-Task Reinforcement Learning.Sfruttiamo l'ipotesi che l'apprendimento da diversi compiti, che condividono proprietà comuni, è utile per generalizzare la conoscenza di essi risultando in un'estrazione di caratteristiche più efficace rispetto all'apprendimento di un singolo compito.Intuitivamente, l'insieme risultante di caratteristiche offre vantaggi di performance quando viene utilizzato da algoritmi di Reinforcement Learning. Dimostriamo questo fornendo garanzie teoriche che evidenziano le condizioni per le quali è conveniente condividere le rappresentazioni tra i compiti, estendendo i ben noti limiti a tempo finito dell'Approximate Value-Iteration all'impostazione multi-task.Inoltre, completiamo la nostra analisi proponendo estensioni multi-task di tre algoritmi di Reinforcement Learning che valutiamo empiricamente su benchmark di Reinforcement Learning ampiamente utilizzati mostrando miglioramenti significativi rispetto alle controparti single-task in termini di efficienza del campione e prestazioni.
Presentiamo un'architettura a capsule 3D per l'elaborazione di nuvole di punti che è equivariante rispetto al gruppo di rotazione SO(3), alla traslazione e alla permutazione degli insiemi di input non ordinati. La rete opera su un insieme sparso di frame di riferimento locali, calcolati da una nuvola di punti di input e stabilisce l'equivarianza end-to-end attraverso un nuovo strato di capsule di gruppi di quaternioni 3D, compresa una procedura di routing dinamico equivariante. Nel processo, colleghiamo teoricamente il processo di instradamento dinamico tra le capsule al ben noto algoritmo Weiszfeld, uno schema per risolvere problemi iterativi ri-pesati ai minimi quadrati (IRLS) con proprietà di convergenza dimostrabili, consentendo una stima robusta della posa tra gli strati della capsula.A causa delle capsule di quaternioni equivarianti sparse, la nostra architettura permette la classificazione congiunta degli oggetti e la stima dell'orientamento, che validiamo empiricamente su comuni set di dati di riferimento. 
La semantica vettoriale, in particolare i vettori di frase, è stata recentemente utilizzata con successo in molte aree dell'elaborazione del linguaggio naturale. Tuttavia, relativamente poco lavoro ha esplorato la struttura interna e le proprietà degli spazi dei vettori di frase.In questo articolo, esploreremo le proprietà dei vettori di frase studiando una particolare applicazione del mondo reale: In particolare, mostriamo che la somiglianza del coseno tra i vettori delle frasi e i vettori dei documenti è fortemente correlata all'importanza delle frasi e che la semantica vettoriale può identificare e correggere le lacune tra le frasi scelte finora e il documento.Inoltre, identifichiamo dimensioni specifiche che sono collegate a riassunti efficaci.A nostra conoscenza, questa è la prima volta che dimensioni specifiche di embeddings di frasi sono state collegate alle proprietà delle frasi.Confrontiamo anche le caratteristiche di diversi metodi di embeddings di frasi.Molte di queste intuizioni hanno applicazioni in usi di embeddings di frasi ben oltre il riassunto.
Presentiamo Value Propagation (VProp), un modulo di pianificazione differenziabile ed efficiente dal punto di vista dei parametri costruito su Value Iteration che può essere addestrato con successo in un modo di apprendimento di rinforzo per risolvere compiti non visti, ha la capacità di generalizzare a mappe di dimensioni maggiori e può imparare a navigare in ambienti dinamici. Valutiamo su configurazioni di MazeBase grid-worlds, con ambienti generati casualmente di diverse dimensioni.Inoltre, mostriamo che il modulo permette di imparare a pianificare quando l'ambiente include anche elementi stocastici, fornendo un sistema di apprendimento economico per costruire pianificatori di basso livello invarianti alle dimensioni per una varietà di problemi di navigazione interattiva.
La raccomandazione è un'applicazione prevalente dell'apprendimento automatico che interessa molti utenti; pertanto, è fondamentale che i modelli di raccomandazione siano accurati e interpretabili.In questo lavoro, proponiamo un metodo per interpretare e aumentare le previsioni dei sistemi di raccomandazione black-box.In particolare, proponiamo di estrarre le interpretazioni di interazione delle caratteristiche da un modello di raccomandazione di origine e codificare esplicitamente queste interazioni in un modello di raccomandazione di destinazione, dove entrambi i modelli di origine e di destinazione sono black-boxes.By non assumendo la struttura del sistema di raccomandazione, il nostro approccio può essere utilizzato in ambienti generali.  Nei nostri esperimenti, ci concentriamo su un uso prominente della raccomandazione di apprendimento automatico: la previsione di click pubblicitari. Abbiamo trovato che le nostre interpretazioni di interazione sono sia informative che predittive, cioè, significativamente superiori ai modelli di raccomandazione esistenti.
Le unità lineari rettificate, o ReLU, sono diventate una funzione di attivazione preferita per le reti neurali artificiali.In questo articolo consideriamo il problema dell'apprendimento di un modello generativo in presenza di non linearità (modellate dalle funzioni ReLU).Dato un insieme di vettori segnale $\mathbf{y}^i \in \mathbb{R}^d, i =1, 2, \punti , n$, ci proponiamo di imparare i parametri di rete, cioè la matrice $A$, sotto il modello $mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i +mathbf{b})$, dove $mathbf{b} in \mathbb{R}^d$ è un vettore di bias casuale, e {$mathbf{c}^i \mathbb{R}^k$ sono vettori latenti sconosciuti arbitrari}. Mostriamo che è possibile recuperare lo spazio delle colonne di $A$ entro un errore di $O(d)$ (in norma Frobenius) sotto certe condizioni sulla distribuzione di $\mathbf{b}$.
I metodi che calcolano rappresentazioni vettoriali dense per le caratteristiche in dati non strutturati - come le parole in un documento - hanno dimostrato di avere molto successo per la rappresentazione della conoscenza. Studiamo come stimare rappresentazioni dense quando esistono più tipi di caratteristiche in un set di dati per l'apprendimento supervisionato dove sono disponibili etichette esplicite, così come per l'apprendimento non supervisionato dove non ci sono etichette.Feat2Vec calcola embeddings per dati con più tipi di caratteristiche, imponendo che tutti i diversi tipi di caratteristiche esistono in uno spazio comune. Nel caso supervisionato, dimostriamo che il nostro metodo ha dei vantaggi rispetto ai metodi proposti di recente, come ad esempio permettere una maggiore accuratezza nella predizione e fornire un modo per evitare il problema dell'inizio a freddo. Nel caso non supervisionato, i nostri esperimenti suggeriscono che Feat2Vec supera significativamente gli algoritmi esistenti che non sfruttano la struttura dei dati.
Indaghiamo le rappresentazioni interne che una rete neurale ricorrente (RNN) utilizza mentre impara a riconoscere un linguaggio formale regolare. In particolare, addestriamo una RNN su esempi positivi e negativi di un linguaggio regolare, e chiediamo se esiste una semplice funzione di decodifica che mappa gli stati di questa RNN agli stati dell'automa deterministico minimo finito (MDFA) per il linguaggio. I nostri esperimenti dimostrano che una tale funzione di decodifica esiste davvero, e che essa mappa gli stati della RNN non agli stati MDFA, ma agli stati di un'astrazione ottenuta raggruppando piccoli insiemi di stati MDFA in "superstati". 
A differenza degli approcci precedenti che applicano algoritmi di ricerca su un piccolo spazio di ricerca progettato dall'uomo senza considerare la diversità dell'hardware, proponiamo HURRICANE che esplora la ricerca automatica hardware-aware su uno spazio di ricerca molto più grande e uno schema di ricerca multistep in un quadro di ascesa coordinata, per generare modelli su misura per diversi tipi di hardware. Esperimenti approfonditi su ImageNet dimostrano che il nostro algoritmo raggiunge costantemente una latenza di inferenza molto più bassa con una precisione simile o migliore rispetto ai metodi NAS allo stato dell'arte su tre tipi di hardware.notevolmente, HURRICANE raggiunge una precisione del 76,63% top-1 su ImageNet con una latenza di inferenza di soli 16,5 ms per DSP, che è una precisione superiore del 3,4% e un 6.. Per VPU, HURRICANE raggiunge un'accuratezza superiore dello 0,53% rispetto a Proxyless-mobile con un'accelerazione di 1,49x.Anche per le CPU mobili ben studiate, HURRICANE raggiunge un'accuratezza superiore dell'1,63% rispetto a FBNet-iPhoneX con una latenza di inferenza comparabile.HURRICANE riduce anche il tempo di formazione del 54,7% in media rispetto a SinglePath-Oneshot.
Il nostro metodo modella esplicitamente le strutture delle frasi nelle sequenze di uscita utilizzando le reti Sleep-WAke (SWAN), un metodo di modellazione delle sequenze basato sulla segmentazione proposto di recente.Per mitigare il requisito di allineamento monotono di SWAN, introduciamo un nuovo strato per eseguire un riordino locale (morbido) delle sequenze di ingresso.A differenza degli approcci esistenti di traduzione automatica neurale (NMT), NPMT non utilizza meccanismi di decodifica basati sull'attenzione.  I nostri esperimenti dimostrano che NPMT raggiunge prestazioni superiori su compiti di traduzione automatica IWSLT 2014 tedesco-inglese/inglese-tedesco e IWSLT 2015 inglese-vietnamita rispetto a forti baseline NMT e che il nostro metodo produce frasi significative nelle lingue di uscita.
Le Reti Generative Adversariali (GAN) hanno mostrato risultati impressionanti nella modellazione di distribuzioni su collettori complicati come quelli delle immagini naturali.Tuttavia, le GAN spesso soffrono di collasso della modalità, il che significa che sono inclini a caratterizzare solo una singola o poche modalità della distribuzione dei dati.Al fine di affrontare questo problema, proponiamo un nuovo quadro chiamato LDMGAN. Per prima cosa introduciamo il vincolo Latent Distribution Matching (LDM) che regolarizza il generatore allineando la distribuzione dei campioni generati con quella dei campioni reali nello spazio latente.Per fare uso di tale spazio latente, proponiamo un AutoEncoder regolarizzato (AE) che mappa la distribuzione dei dati alla distribuzione precedente nello spazio codificato.Esperimenti estesi su dati sintetici e dataset del mondo reale mostrano che il nostro quadro proposto migliora significativamente la stabilità e la diversità di GAN.
Tuttavia, l'attuale paradigma di esecuzione delle reti neurali si basa su librerie ottimizzate a mano, euristica di compilazione tradizionale, o molto recentemente algoritmi genetici e altri metodi stocastici. Questi metodi soffrono di frequenti e costose misurazioni hardware che li rendono non solo troppo lunghi ma anche subottimali. Questa soluzione denominata CHAMELEON sfrutta l'apprendimento per rinforzo la cui soluzione richiede meno passaggi per convergere, e sviluppa un algoritmo di campionamento adattivo che non solo si concentra sui campioni costosi (misurazioni hardware reali) su punti rappresentativi, ma utilizza anche una logica ispirata alla conoscenza del dominio per migliorare i campioni stessi.La sperimentazione con hardware reale mostra che CHAMELEON fornisce 4.45×speed up nel tempo di ottimizzazione rispetto a AutoTVM, migliorando anche il tempo di inferenza delle moderne reti profonde del 5.6%.
L'obiettivo è quello di modellare una grammatica formale in termini di funzioni differenziabili e rappresentazioni latenti, in modo che il loro apprendimento sia possibile attraverso la backpropagation standard. L'apprendimento di una grammatica formale rappresentata con terminali latenti, non terminali e regole di produzione permette di catturare strutture sequenziali con possibilità multiple dai dati. La grammatica avversaria è progettata in modo da poter apprendere regole di produzione stocastiche dalla distribuzione dei dati, ed essere in grado di selezionare più regole di produzione porta a diversi risultati previsti, modellando così in modo efficiente molti futuri plausibili.  Confermiamo il beneficio della grammatica avversaria su due compiti diversi: la previsione della posa umana 3D futura e la previsione dell'attività futura. Per tutte le impostazioni, la grammatica avversaria proposta supera gli approcci allo stato dell'arte, essendo in grado di prevedere molto più accuratamente e più in là nel futuro, rispetto al lavoro precedente.
Il nostro approccio, che chiamiamo Janossy pooling, esprime una funzione invariante alle permutazioni come la media di una funzione sensibile alle permutazioni applicata a tutti i riordinamenti della sequenza di input, il che ci permette di sfruttare la ricca e matura letteratura sulle funzioni sensibili alle permutazioni per costruire nuove e flessibili funzioni invarianti alle permutazioni. Per consentire la trattabilità computazionale, consideriamo tre tipi di approssimazioni: ordinamenti canonici delle sequenze, funzioni con interazioni di k-ordine e algoritmi di ottimizzazione stocastica con permutazioni casuali. Il nostro quadro unifica una varietà di lavori esistenti in letteratura e suggerisce possibili estensioni modellistiche e algoritmiche, che esploriamo nei nostri esperimenti, che dimostrano prestazioni migliori rispetto agli attuali metodi all'avanguardia.
Mentre i compiti potrebbero venire con il variare del numero di istanze e classi nelle impostazioni realistiche, gli approcci esistenti di meta-apprendimento per la classificazione di pochi colpi assumono che il numero di istanze per compito e classe sia fisso. Inoltre, non considerano la differenza distributiva nei compiti non visti, sui quali la meta-conoscenza può avere meno utilità a seconda della relazione tra i compiti. Per superare queste limitazioni, proponiamo un nuovo modello di meta-apprendimento che bilancia in modo adattivo l'effetto del meta-apprendimento e dell'apprendimento specifico per ogni compito. Attraverso l'apprendimento delle variabili di bilanciamento, possiamo decidere se ottenere una soluzione affidandoci alla meta-conoscenza o all'apprendimento specifico del compito.Formuliamo questo obiettivo in una struttura di inferenza bayesiana e lo affrontiamo usando l'inferenza variazionale.Convalidiamo il nostro Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) su due set di dati realistici bilanciati per compito e classe, sui quali supera significativamente gli approcci di meta-apprendimento esistenti.Un ulteriore studio di ablazione conferma l'efficacia di ogni componente di bilanciamento e la struttura di apprendimento bayesiana.
Molti compiti nell'intelligenza artificiale richiedono la collaborazione di più agenti.Esaminiamo l'apprendimento di rinforzo profondo per i domini multi-agente.Gli sforzi di ricerca recenti prendono spesso la forma di due prospettive apparentemente contrastanti, la prospettiva decentralizzata, dove si suppone che ogni agente abbia il proprio controllore; e la prospettiva centralizzata, dove si presuppone che ci sia un modello più grande che controlla tutti gli agenti.A questo proposito, rivisitiamo l'idea dell'architettura master-slave incorporando entrambe le prospettive in un quadro. L'idea di combinare entrambe le prospettive è intuitiva e può essere ben motivata da molti sistemi del mondo reale, tuttavia, da una varietà di possibili realizzazioni, mettiamo in evidenza tre ingredienti chiave, vale a dire la rappresentazione dell'azione composta, la comunicazione apprendibile e il ragionamento indipendente. con disegni di rete per facilitare questi esplicitamente, la nostra proposta supera costantemente gli ultimi metodi concorrenti sia negli esperimenti sintetici che quando viene applicata alle impegnative attività di microgestione di StarCraft.
Il classificatore è descritto da un modello ReLU non lineare e la funzione obiettivo adotta la funzione di perdita esponenziale. caratterizziamo prima il paesaggio della funzione di perdita e mostriamo che possono esistere minimi locali asintotici spuri oltre ai minimi globali asintotici. Mostriamo poi che la discesa del gradiente (GD) può convergere verso una direzione globale o locale di massimo margine, o può divergere dalla direzione di massimo margine desiderata in un contesto generale.Per la discesa stocastica del gradiente (SGD), mostriamo che converge in aspettativa alla direzione globale o locale di massimo margine se SGD converge. Esploriamo ulteriormente il bias implicito di questi algoritmi nell'apprendimento di una rete multi-neurone in certe condizioni stazionarie, e mostriamo che il classificatore appreso massimizza i margini di ogni partizione di pattern campione sotto l'attivazione ReLU.
La potatura del modello è diventata una tecnica utile che migliora l'efficienza computazionale dell'apprendimento profondo, rendendo possibile l'implementazione di soluzioni in scenari con risorse limitate.Una pratica ampiamente utilizzata nel lavoro pertinente presuppone che un parametro o una caratteristica a norma minore giochi un ruolo meno informativo al momento dell'inferenza. In questo articolo, proponiamo una tecnica di potatura del canale per accelerare i calcoli delle reti neurali convoluzionali profonde (CNN) che non si basa criticamente su questo presupposto, ma si concentra sulla semplificazione diretta del grafico di calcolo da canale a canale di una CNN senza la necessità di eseguire un compito computazionalmente difficile e non sempre utile di rendere i tensori ad alta densità di CNN strutturati sparsi. Il nostro approccio si articola in due fasi: prima adottare un metodo di addestramento stocastico end-to-end che alla fine costringe le uscite di alcuni canali ad essere costanti, e poi sfrondare quei canali costanti dalla rete neurale originale regolando i bias dei loro strati di impatto in modo che il modello compatto risultante possa essere rapidamente messo a punto.Il nostro approccio è matematicamente attraente da una prospettiva di ottimizzazione e facile da riprodurre.Abbiamo sperimentato il nostro approccio attraverso diversi benchmark di apprendimento delle immagini e dimostrare i suoi aspetti interessanti e le prestazioni competitive.
La discesa stocastica del gradiente (SGD) è stato il metodo di ottimizzazione dominante per l'addestramento delle reti neurali profonde grazie alle sue numerose proprietà desiderabili. Una delle qualità più notevoli e meno comprese di SGD è che generalizza relativamente bene su dati non visti anche quando la rete neurale ha milioni di parametri. Noi ipotizziamo che in certi casi sia desiderabile rilassare le sue proprietà intrinseche di generalizzazione e introdurre un'estensione di SGD chiamata deep gradient boosting (DGB). L'idea chiave di DGB è che i gradienti retropropagati dedotti usando la regola della catena possono essere visti come obiettivi pseudo-residuali di un problema di gradient boosting. Così ad ogni strato di una rete neurale l'aggiornamento del peso è calcolato risolvendo il corrispondente problema di boosting usando un learner lineare di base.La formula di aggiornamento del peso risultante può anche essere vista come una procedura di normalizzazione dei dati che arrivano ad ogni strato durante il passaggio in avanti.Quando viene implementata come uno strato separato di normalizzazione dell'input (INN) la nuova architettura mostra prestazioni migliori su compiti di riconoscimento delle immagini se confrontata con la stessa architettura senza strati di normalizzazione.Al contrario della normalizzazione batch (BN), INN non ha parametri apprendibili, ma uguaglia le sue prestazioni su CIFAR10 e compiti di classificazione ImageNet.
Proponiamo di estendere gli algoritmi esistenti di deep reinforcement learning (Deep RL) permettendo loro di scegliere anche sequenze di azioni come parte della loro politica.  Questa modifica costringe la rete ad anticipare la ricompensa delle sequenze di azioni, il che, come dimostriamo, migliora l'esplorazione portando a una migliore convergenza. La nostra proposta è semplice, flessibile e può essere facilmente incorporata in qualsiasi struttura di Deep RL. Mostriamo la potenza del nostro schema superando costantemente l'algoritmo GA3C all'avanguardia su diversi giochi Atari popolari.
Per ottimizzare una rete neurale si pensa spesso di ottimizzarne i parametri, ma in ultima analisi si tratta di ottimizzare la funzione che mappa gli input agli output.Dal momento che un cambiamento nei parametri potrebbe servire come un povero proxy per il cambiamento della funzione, è di una certa preoccupazione che il primato sia dato ai parametri ma che la corrispondenza non sia stata testata. Qui, mostriamo che è semplice e computazionalmente fattibile calcolare le distanze tra le funzioni in uno spazio di Hilbert $L^2$. Esaminiamo come le reti tipiche si comportano in questo spazio, e confrontiamo come le distanze $ell^2$ dei parametri si confrontano con le distanze $L^2$ delle funzioni tra vari punti di una traiettoria di ottimizzazione, scoprendo che le due distanze sono non banalmente correlate. In particolare, il rapporto $L^2/\ell^2$ diminuisce nel corso dell'ottimizzazione, raggiungendo un valore costante intorno al momento in cui l'errore di test si stabilizza.Indaghiamo quindi su come la distanza $L^2$ possa essere applicata direttamente all'ottimizzazione.Proponiamo innanzitutto che nell'apprendimento multitask, si possa evitare la dimenticanza catastrofica limitando direttamente quanto la funzione di input/output cambi tra i compiti. In secondo luogo, proponiamo una nuova regola di apprendimento che vincola la distanza che una rete può percorrere attraverso $L^2$-spazio in ogni aggiornamento, il che permette di imparare nuovi esempi in un modo che interferisce minimamente con ciò che è stato appreso in precedenza. Queste applicazioni dimostrano come si possano misurare e regolarizzare le distanze delle funzioni direttamente, senza fare affidamento su parametri o approssimazioni locali come la curvatura di perdita.
Il problema del collo di bottiglia informativo (IB) affronta il problema di ottenere rappresentazioni compresse rilevanti T di qualche variabile casuale X per il compito di predire Y. È definito come un problema di ottimizzazione vincolato che massimizza l'informazione che la rappresentazione ha sul compito, I(T;Y), garantendo al contempo che un livello minimo di compressione r sia raggiunto (cioè, I(X;T) <= r).Per ragioni pratiche il problema è solitamente risolto massimizzando la lagrangiana IB per molti valori del moltiplicatore di Lagrange, quindi disegnando la curva IB (cioè la curva di I(T;Y) massima per un dato I(X;Y)) e selezionando la rappresentazione di prevedibilità e compressione desiderata.È noto che quando Y è una funzione deterministica di X, la curva IB non può essere esplorata e altre lagrangiane sono state proposte per affrontare questo problema (ad es, In questo articolo (i) presentiamo una famiglia generale di Lagrangiani che permettono l'esplorazione della curva IB in tutti gli scenari; (ii) dimostriamo che se questi Lagrangiani sono usati, c'è una mappatura uno-a-uno tra il moltiplicatore di Lagrange e il tasso di compressione desiderato r per forme note della curva IB, quindi, liberando dal peso di risolvere il problema di ottimizzazione per molti valori del moltiplicatore di Lagrange.
Proponiamo la Neural Logic Machine (NLM), un'architettura neurale-simbolica sia per l'apprendimento induttivo che per il ragionamento logico. Le NLM sfruttano la potenza delle reti neurali - come approssimatori di funzioni - e della programmazione logica - come processore simbolico per oggetti con proprietà, relazioni, connettivi logici e quantificatori.  Nei nostri esperimenti, gli NLM raggiungono una perfetta generalizzazione in un certo numero di compiti, dai compiti di ragionamento relazionale sull'albero genealogico e sui grafi generali, ai compiti decisionali che includono l'ordinamento delle matrici, la ricerca dei percorsi più brevi e il gioco dei blocchi, la maggior parte dei quali sono difficili da realizzare solo per le reti neurali o la programmazione logica induttiva.
Tuttavia, i sistemi neurali astrattivi esistenti generano frequentemente riassunti non corretti dal punto di vista fattuale e sono vulnerabili alle informazioni avversarie, suggerendo una mancanza cruciale di comprensione semantica. In questo articolo, proponiamo un nuovo modello di riassunto neurale semantico consapevole che impara a generare riassunti di alta qualità attraverso l'interpretazione semantica sul contenuto saliente. Un nuovo schema di valutazione con campioni avversari è introdotto per misurare quanto bene un modello identifica le informazioni fuori tema, dove il nostro modello produce prestazioni significativamente migliori rispetto al popolare pointer-generator summarizer.Human valutazione conferma anche che il nostro sistema riassunti sono uniformemente più informativo e fedele così come meno ridondante che il modello seq2seq.
Il recente lavoro del metodo Super Characters utilizzando l'incorporazione bidimensionale delle parole ha raggiunto risultati all'avanguardia nei compiti di classificazione del testo, mostrando la promessa di questo nuovo approccio.Questo articolo prende in prestito l'idea del metodo Super Characters e dell'incorporazione bidimensionale, e propone un metodo di generazione di risposte conversazionali per dialoghi di dominio aperto.I risultati sperimentali su un set di dati pubblici mostrano che il metodo SuperChat proposto genera risposte di alta qualità.Una demo interattiva è pronta a mostrare al workshop.E il codice sarà presto disponibile su github.
Le reti neurali convoluzionali (CNN) sono state generalmente riconosciute come una delle forze trainanti per il progresso della computer vision.Nonostante le loro promettenti prestazioni su molti compiti, le CNN devono ancora affrontare grandi ostacoli sulla strada per raggiungere l'intelligenza artificiale ideale.Uno è che le CNN sono complesse e difficili da interpretare. Un altro è che le CNN standard richiedono grandi quantità di dati annotati, che a volte è molto difficile da ottenere, ed è auspicabile essere in grado di imparare da pochi esempi.In questo lavoro, affrontiamo queste limitazioni delle CNN sviluppando modelli nuovi, semplici e interpretabili per l'apprendimento di pochi colpi. I nostri modelli si basano sull'idea di codificare gli oggetti in termini di concetti visivi, che sono spunti visivi interpretabili rappresentati dai vettori di caratteristiche all'interno delle CNN. Motivati da queste proprietà, presentiamo due modelli intuitivi per il problema dell'apprendimento di pochi scatti. Gli esperimenti mostrano che i nostri modelli raggiungono prestazioni competitive, pur essendo molto più flessibili e interpretabili rispetto ai metodi alternativi di apprendimento di pochi scatti allo stato dell'arte.
Di recente sono state proposte varie reti neurali per dati strutturati in modo irregolare, come grafi e manifold.A nostra conoscenza, tutte le reti di grafi esistenti hanno profondità discreta.Ispirati dall'equazione differenziale ordinaria neurale (NODE) per i dati nel dominio euclideo, estendiamo l'idea di modelli a profondità continua ai dati del grafico e proponiamo l'equazione differenziale ordinaria del grafico (GODE).La derivata degli stati dei nodi nascosti è parametrizzata con una rete neurale del grafico e gli stati di uscita sono la soluzione di questa equazione differenziale ordinaria.Dimostriamo due metodi end-to-end per un addestramento efficiente di GODE: (1) back-propagation indiretta con il metodo adjoint; (2) back-propagation diretta attraverso il solutore ODE, che calcola accuratamente il gradiente.dimostriamo che la backprop diretta supera il metodo adjoint negli esperimenti.introduciamo poi una famiglia di blocchi bijettivi, che permette un consumo di memoria di $mathcal{O}(1)$. Dimostriamo che GODE può essere facilmente adattato a diverse reti neurali a grafo esistenti e migliorare l'accuratezza.Convalidiamo le prestazioni di GODE sia in compiti di classificazione semi-supervisionata dei nodi che in compiti di classificazione a grafo.Il nostro modello GODE raggiunge un modello continuo nel tempo, efficienza di memoria, stima accurata del gradiente, e generalizzabilità con diverse reti a grafo.
La traduzione immagine-immagine non supervisionata è un compito recentemente proposto per tradurre un'immagine in uno stile o in un dominio diverso, dati solo esempi di immagini non accoppiate al momento della formazione. La traduzione di video implica l'apprendimento non solo dell'aspetto degli oggetti e delle scene, ma anche del movimento realistico e delle transizioni tra fotogrammi consecutivi. Studiamo le prestazioni della traduzione video-video per fotogramma utilizzando le reti di traduzione immagine-immagine esistenti, e proponiamo un traduttore 3D spazio-temporale come soluzione alternativa a questo problema. Valutiamo il nostro metodo 3D su più set di dati sintetici, come il movimento di cifre colorate, così come il realistico set di dati GTA di segmentazione-video e un nuovo set di dati di traduzione di immagini volumetriche CT-to-MRI.I nostri risultati mostrano che la traduzione frame-wise produce risultati realistici a livello di singolo fotogramma ma sottoperforma significativamente sulla scala dell'intero video rispetto al nostro approccio di traduzione tridimensionale, che è meglio in grado di imparare la complessa struttura del video e del movimento e la continuità dell'aspetto degli oggetti.
Le reti a capsula sono limitate dalla natura costosa dei parametri dei loro strati e dalla mancanza generale di garanzie di equivarianza dimostrabili. Presentiamo una variante delle reti a capsula che mira a rimediare a questo, individuando che l'apprendimento di tutte le relazioni parte-intero a coppie tra le capsule di strati successivi è inefficiente. Inoltre, ci rendiamo conto che la scelta delle reti di predizione e il meccanismo di routing sono entrambi fondamentali per l'equivarianza. Su queste basi, proponiamo un quadro alternativo per le reti di capsule che impara a codificare in modo proiettivo il complesso delle variazioni di posa, definito spazio di variazione (SOV), per ogni capsula-tipo di ogni strato, utilizzando una funzione allenabile ed equivariante definita su una griglia di trasformazioni di gruppo. Così, la fase di predizione dell'instradamento comporta la proiezione nel SOV di una capsula più profonda usando la funzione corrispondente.Come istanziazione specifica di questa idea, e anche al fine di raccogliere i benefici di una maggiore condivisione dei parametri, usiamo convoluzioni gruppo-equivarianti omogenee di tipo di capsule meno profonde in questa fase.Introduciamo anche un meccanismo di instradamento equivariante basato sul grado di centralità. Mostriamo che questa particolare istanza del nostro modello generale è equivariante, e quindi conserva la rappresentazione composizionale di un input sotto trasformazioni.Conduciamo diversi esperimenti su set di dati standard di classificazione degli oggetti che mostrano l'aumento della robustezza alle trasformazioni, così come le prestazioni generali, del nostro modello a diverse capsule di base.
Recentemente le reti neurali profonde hanno mostrato la loro capacità di memorizzare i dati di allenamento, anche con etichette rumorose, il che danneggia le prestazioni di generalizzazione.Per mitigare questo problema, proponiamo un metodo semplice ma efficace che è robusto alle etichette rumorose, anche con rumore grave.  Il nostro obiettivo coinvolge un termine di regolarizzazione della varianza che penalizza implicitamente la norma Jacobiana della rete neurale sull'intero set di allenamento (compresi i dati con etichette rumorose), il che incoraggia la generalizzazione e previene l'overfitting alle etichette corrotte.Gli esperimenti su benchmark rumorosi dimostrano che il nostro approccio raggiunge prestazioni all'avanguardia con un'elevata tolleranza al rumore grave.
Una recente ricerca suggerisce che la traduzione automatica neurale raggiunge la parità con la traduzione umana professionale nel compito di traduzione di notizie cinese-inglese WMT.Noi testiamo empiricamente questa affermazione con protocolli di valutazione alternativi, contrastando la valutazione di singole frasi e di interi documenti. In un esperimento di classificazione a coppie, i revisori umani che valutano l'adeguatezza e la fluidità mostrano una preferenza più forte per la traduzione umana rispetto a quella automatica quando valutano i documenti rispetto alle frasi isolate. I nostri risultati sottolineano la necessità di passare alla valutazione a livello di documento man mano che la traduzione automatica migliora al punto che gli errori che sono difficili o impossibili da individuare a livello di frase diventano decisivi nel discriminare la qualità dei diversi risultati di traduzione.
L'apprendimento per imitazione mira a imparare inversamente una politica dalle dimostrazioni degli esperti, che è stato ampiamente studiato in letteratura sia per l'impostazione a singolo agente con il modello del processo decisionale di Markov (MDP), sia per l'impostazione multi-agente con il modello del gioco di Markov (MG). Proponiamo una nuova struttura per l'apprendimento di imitazione adversariale generativa multi-agente asincrona (AMAGAIL) nelle impostazioni generali dei giochi di Markov estesi, e le politiche degli esperti apprese sono dimostrate per garantire l'equilibrio perfetto di subgame (SPE), un equilibrio più generale e più forte dell'equilibrio di Nash (NE).i risultati dell'esperimento dimostrano che rispetto alle linee di base dello stato dell'arte, il nostro modello AMAGAIL può dedurre meglio la politica di ogni agente esperto utilizzando i loro dati di dimostrazione raccolti da scenari decisionali asincroni (cioè, giochi di Markov estesi).
L'auto-addestramento è uno dei primi e più semplici metodi semi-supervisionati. L'idea chiave è quella di aumentare il dataset originale etichettato con dati non etichettati accoppiati alla previsione del modello. Tuttavia, in compiti di generazione di sequenze complesse come la traduzione automatica, non è ancora chiaro come funziona l'auto-addestramento a causa della composizionalità dello spazio di destinazione.In questo lavoro, dimostriamo innanzitutto che non solo è possibile, ma consigliato applicare l'auto-addestramento nella generazione di sequenze.Attraverso un attento esame dei guadagni di prestazioni, troviamo che il rumore aggiunto sugli stati nascosti (es. Per incoraggiare ulteriormente questo meccanismo, proponiamo di iniettare rumore nello spazio di input, ottenendo una versione "rumorosa" dell'auto-addestramento. Uno studio empirico su benchmark standard attraverso compiti di traduzione automatica e di sintesi del testo sotto diverse impostazioni di risorse mostra che l'auto-addestramento rumoroso è in grado di utilizzare efficacemente i dati non etichettati e migliorare le prestazioni di base con un ampio margine.
Presentiamo un sistema di memoria addestrato end-to-end che si adatta rapidamente ai nuovi dati e genera campioni come loro.Ispirato dalla memoria distribuita sparsa di Kanerva, ha un robusto meccanismo di lettura e scrittura distribuita.La memoria è analiticamente trattabile, che consente una compressione online ottimale tramite una regola di aggiornamento bayesiana.Formuliamo come un modello generativo condizionale gerarchico, dove la memoria fornisce una ricca distribuzione di priorità dipendente dai dati. Di conseguenza, la memoria top-down e la percezione bottom-up sono combinate per produrre il codice che rappresenta un'osservazione.Empiricamente, dimostriamo che la memoria adattiva migliora significativamente i modelli generativi addestrati su entrambi i dataset Omniglot e CIFAR.Rispetto al Differentiable Neural Computer (DNC) e alle sue varianti, il nostro modello di memoria ha una maggiore capacità ed è significativamente più facile da allenare.
La potatura di grandi reti neurali mantenendo le loro prestazioni è spesso auspicabile a causa della ridotta complessità spaziale e temporale.Nei metodi esistenti, la potatura viene effettuata all'interno di una procedura di ottimizzazione iterativa con programmi di potatura progettati euristicamente o iperparametri aggiuntivi, minando la loro utilità.In questo lavoro, presentiamo un nuovo approccio che pota una data rete una volta all'inizializzazione prima della formazione.Per ottenere questo, introduciamo un criterio di salienza basato sulla sensibilità della connessione che identifica le connessioni strutturalmente importanti nella rete per un determinato compito. Dopo la potatura, la rete rada viene addestrata in modo standard. Il nostro metodo ottiene reti estremamente rade con la stessa accuratezza della rete di riferimento sui compiti di classificazione MNIST, CIFAR-10 e Tiny-ImageNet ed è ampiamente applicabile a varie architetture tra cui reti convoluzionali, residue e ricorrenti.
L'apprendimento per trasferimento utilizza i pesi formati da un modello sorgente come pesi iniziali per la formazione di un set di dati di destinazione.  Una fonte ben scelta con un gran numero di dati etichettati porta ad un significativo miglioramento della precisione.  Dimostriamo una tecnica che etichetta automaticamente grandi set di dati non etichettati in modo da poter addestrare i modelli di origine per il transfer learning. Valutiamo sperimentalmente questo metodo, utilizzando un set di dati di base di etichette ImageNet1K annotate dall'uomo, contro cinque variazioni di questa tecnica.  Mostriamo che le prestazioni di questi modelli addestrati automaticamente sono in media entro il 17% della linea di base.
In questo articolo proponiamo un modulo semplice e altamente efficiente dal punto di vista parametrico chiamato Tree-structured Attention Module (TAM) che incoraggia ricorsivamente i canali vicini a collaborare per produrre una mappa di attenzione spaziale come output.A differenza di altri moduli di attenzione che cercano di catturare dipendenze a lungo raggio in ogni canale, il nostro modulo si concentra sull'imposizione di non linearità tra i canali utilizzando la convoluzione di gruppo point-wise. Questo modulo non solo rafforza il potere di rappresentazione di un modello, ma agisce anche come un cancello che controlla il flusso del segnale.Il nostro modulo permette a un modello di raggiungere prestazioni più elevate in un modo altamente efficiente dal punto di vista dei parametri.Con il nostro modulo di attenzione proposto impiegato, i modelli ResNet50 e ResNet101 guadagnano il 2,3% e l'1,2% di miglioramento della precisione con meno dell'1,5% di sovraccarico dei parametri.Il nostro codice di implementazione PyTorch è disponibile al pubblico.
Una sfida significativa per l'applicazione pratica del reinforcement learning ai problemi del mondo reale è la necessità di specificare una funzione di ricompensa oracolo che definisca correttamente un compito.Inverse reinforcement learning (IRL) cerca di evitare questa sfida deducendo invece una funzione di ricompensa dal comportamento degli esperti.  Anche se attraente, può essere impraticabilmente costoso raccogliere serie di dati di dimostrazioni che coprono la variazione comune nel mondo reale (ad esempio, l'apertura di qualsiasi tipo di porta), quindi in pratica, IRL deve essere comunemente eseguita con solo un insieme limitato di dimostrazioni in cui può essere estremamente difficile recuperare in modo univoco una funzione di ricompensa. In questo lavoro, sfruttiamo l'intuizione che le dimostrazioni di altri compiti possono essere utilizzate per limitare l'insieme delle possibili funzioni di ricompensa imparando un "priore" che è specificamente ottimizzato per la capacità di dedurre funzioni di ricompensa espressive da un numero limitato di dimostrazioni.  Dimostriamo che il nostro metodo può recuperare in modo efficiente le ricompense dalle immagini per nuovi compiti e forniamo un'intuizione su come il nostro approccio sia analogo all'apprendimento di un priore.
Il lavoro recente si è concentrato sulla combinazione dei metodi del kernel e dell'apprendimento profondo, introduciamo le reti Deepström, una nuova architettura di reti neurali che utilizziamo per sostituire gli strati densi superiori delle architetture convoluzionali standard con un'approssimazione di una funzione kernel, basandoci sull'approssimazione di Nyström. Il nostro approccio è facile e altamente flessibile, è compatibile con qualsiasi funzione kernel e permette di sfruttare kernel multipli. Mostriamo che le reti Deepström raggiungono prestazioni allo stato dell'arte su dataset standard come SVHN e CIFAR100. Un vantaggio del metodo risiede nel suo numero limitato di parametri apprendibili che lo rendono particolarmente adatto a set di allenamento di piccole dimensioni, ad esempio da 5 a 20 campioni per classe. Infine illustriamo due modi di utilizzare kernel multipli, compresa un'impostazione Deepström multipla, che sfrutta un kernel su ogni mappa di caratteristiche in uscita dalla parte convoluzionale del modello.    
L'obiettivo principale di questo breve articolo è quello di informare la comunità dell'arte neurale in generale sulle ramificazioni etiche dell'utilizzo di modelli addestrati sul dataset imagenet, ovvero utilizzando immagini seed delle classi 445 -n02892767- ['bikini, due pezzi'] e 459- n02837789- ['brassiere, reggiseno, bandeau'] dello stesso. Abbiamo scoperto che molte delle immagini appartenenti a queste classi erano palesemente pornografiche, girate in un ambiente non consensuale, voyeuristiche e implicavano anche nudità di minorenni. Analogamente ai nessi \textit{intaglio dell'avorio-caccia di frodo illegale} e \textit{arte dei gioielli in diamanti- diamante sanguigno}, riteniamo che ci sia un simile enigma morale in gioco qui e vorremmo istigare una conversazione tra gli artisti neurali nella comunità.
L'apprendimento induttivo e non supervisionato dei grafi è una tecnica critica per le attività di previsione o di recupero delle informazioni in cui è difficile ottenere informazioni sulle etichette. è anche impegnativo rendere l'apprendimento dei grafi induttivo e non supervisionato allo stesso tempo, poiché i processi di apprendimento guidati da funzioni di perdita basate su errori di ricostruzione richiedono inevitabilmente una valutazione della somiglianza dei grafi che di solito è computazionalmente intrattabile. in questo articolo, proponiamo un quadro generale SEED (Sampling, Encoding, and Embedding Distributions) per l'apprendimento induttivo e non supervisionato della rappresentazione su oggetti strutturati a grafo. Invece di affrontare direttamente le sfide computazionali sollevate dalla valutazione della somiglianza dei grafi, dato un grafico di input, il quadro SEED campiona un certo numero di sottografi i cui errori di ricostruzione potrebbero essere valutati in modo efficiente, codifica i campioni di sottografi in una collezione di vettori di sottografi, e impiega l'incorporazione della distribuzione dei vettori di sottografi come rappresentazione vettoriale di output per il grafico di input. Utilizzando set di dati pubblici di riferimento, il nostro studio empirico suggerisce che la struttura SEED proposta è in grado di ottenere fino al 10% di miglioramento, rispetto ai metodi di base competitivi.
Le risposte delle popolazioni neurali agli stimoli sensoriali possono esibire sia una dipendenza non lineare dallo stimolo che una variabilità condivisa riccamente strutturata. Qui mostriamo come l'addestramento avversario possa essere usato per ottimizzare i modelli di codifica neurali per catturare sia le componenti deterministiche che stocastiche dei dati delle popolazioni neurali. Per tenere conto della natura discreta dei treni di spike neurali, usiamo il metodo REBAR per stimare gradienti imparziali per l'ottimizzazione avversaria dei modelli di codifica neurali. Illustriamo il nostro approccio sulle registrazioni di popolazione dalla corteccia visiva primaria e dimostriamo che l'aggiunta di fonti di rumore latenti a una rete neurale convoluzionale produce un modello che cattura sia la dipendenza dallo stimolo che le correlazioni di rumore dell'attività della popolazione.
Come nucleo di questa struttura, introduciamo un nuovo compito di apprendimento di istanze multiple basato su un'etichetta a livello di borsa chiamata unique class count (ucc), che è il numero di classi uniche tra tutte le istanze all'interno della borsa. In questo compito, non sono necessarie annotazioni sulle singole istanze all'interno della borsa durante la formazione dei modelli. Abbiamo costruito un classificatore ucc basato su una rete neurale e dimostrato sperimentalmente che le prestazioni di clustering del nostro quadro con il nostro classificatore ucc debolmente supervisionato sono paragonabili a quelle dei modelli di apprendimento completamente supervisionati in cui le etichette per tutte le istanze sono note.Inoltre, abbiamo testato l'applicabilità del nostro quadro a un compito del mondo reale di segmentazione semantica delle metastasi del cancro al seno nelle sezioni istologiche dei linfonodi e dimostrato che le prestazioni del nostro quadro debolmente supervisionato sono comparabili alle prestazioni di un modello Unet completamente supervisionato.
I metodi di apprendimento di rinforzo senza modello (RL) stanno avendo successo in un numero crescente di compiti, aiutati dai recenti progressi nell'apprendimento profondo.  Tuttavia, tendono a soffrire di un'elevata complessità del campione, che ostacola il loro uso nei domini del mondo reale.  In alternativa, l'apprendimento di rinforzo basato sul modello promette di ridurre la complessità del campione, ma tende a richiedere un'attenta messa a punto e fino ad oggi ha avuto successo principalmente in domini restrittivi in cui i modelli semplici sono sufficienti per l'apprendimento. In questo articolo, analizziamo il comportamento dei metodi di apprendimento di rinforzo basati sul modello di vaniglia quando vengono utilizzate reti neurali profonde per imparare sia il modello che la politica, e mostriamo che la politica appresa tende a sfruttare le regioni in cui sono disponibili dati insufficienti per il modello da apprendere, causando instabilità nella formazione. Per superare questo problema, proponiamo di utilizzare un ensemble di modelli per mantenere l'incertezza del modello e regolarizzare il processo di apprendimento. Mostriamo inoltre che l'uso delle derivate del rapporto di verosimiglianza produce un apprendimento molto più stabile rispetto alla backpropagation nel tempo. Nel complesso, il nostro approccio Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) riduce significativamente la complessità del campione rispetto ai metodi RL profondi senza modello su impegnativi compiti di controllo continuo.
L'alta complessità computazionale e dei parametri delle reti neurali rende il loro addestramento molto lento e difficile da distribuire sull'energia e sui sistemi di calcolo con limitazioni di storage.Molte tecniche di riduzione della complessità della rete sono state proposte tra cui l'implementazione a punto fisso.Tuttavia, un approccio sistematico per progettare l'addestramento a punto fisso completo e l'inferenza delle reti neurali profonde rimane sfuggente.Descriviamo una metodologia di assegnazione di precisione per la formazione delle reti neurali in cui tutti i parametri di rete, cioè Le attivazioni e i pesi nel percorso di feedforward, i gradienti e gli accumulatori di peso nel percorso di feedback, sono assegnati vicino alla precisione minima. L'assegnazione della precisione è derivata analiticamente e permette di seguire il comportamento di convergenza dell'addestramento a precisione completa, noto per convergere a priori. Così, il nostro lavoro porta ad una metodologia sistematica di determinazione della precisione adatta per l'addestramento a punto fisso. La quasi ottimalità (minimalità) dell'assegnazione di precisione risultante è convalidata empiricamente per quattro reti sui set di dati CIFAR-10, CIFAR-100 e SVHN. La riduzione della complessità derivante dal nostro approccio è confrontata con altri progetti di reti neurali a punto fisso.
La ricerca sull'apprendimento automatico (ML) ha studiato i prototipi: esempi che sono rappresentativi del comportamento da apprendere. Valutiamo sistematicamente cinque metodi per identificare i prototipi, sia quelli precedentemente introdotti che quelli nuovi che proponiamo, trovando tutti loro per fornire interpretazioni significative ma diverse.Attraverso uno studio umano, confermiamo che tutte le cinque metriche sono ben abbinate all'intuizione umana. Esaminare i casi in cui le metriche sono in disaccordo offre una prospettiva informativa sulle proprietà dei dati e degli algoritmi utilizzati nell'apprendimento, con implicazioni per la costruzione del data-corpus, l'efficienza, la robustezza avversaria, l'interpretabilità e altri aspetti della ML.In particolare, confermiamo che l'approccio del curriculum "train on hard" può migliorare la precisione su molti set di dati e compiti, ma che è strettamente peggiore quando ci sono molti esempi mal etichettati o ambigui.
In questo lavoro, proponiamo la Sparse Deep Scattering Croisé Network (SDCSN) una nuova architettura basata sulla Deep Scattering Network (DSN).La DSN è ottenuta tramite convoluzioni in cascata di trasformate wavelet con un modulo complesso e un operatore invariante nel tempo. Fornendo così una rappresentazione latente più informativa e beneficiando dello sviluppo di filtri wavelet altamente specializzati negli ultimi decenni, inoltre, combinando tutte le diverse rappresentazioni wavelet, riduciamo la quantità di informazioni precedenti necessarie per quanto riguarda i segnali a portata di mano. In secondo luogo, sviluppiamo una strategia di soglia ottimale per banchi di filtri troppo completi che regolarizza la rete e controlla le instabilità come il rumore non stazionario intrinseco nel segnale.La nostra soluzione sistematica e di principio sparsifica la rappresentazione latente della rete agendo come una maschera locale che distingue tra attività e rumore.Quindi, proponiamo di migliorare il DSN aumentando la varianza della rappresentazione dei coefficienti di dispersione, nonché di migliorare la sua robustezza rispetto al rumore non stazionario.Mostriamo che il nostro nuovo approccio è più robusto e supera il DSN su un compito di rilevamento degli uccelli.
Proponiamo un modello di clustering neurale che impara congiuntamente sia le caratteristiche latenti che il modo in cui si raggruppano.A differenza di metodi simili, il nostro modello non richiede un numero predefinito di cluster.Utilizzando un approccio supervisionato, agglomeriamo le caratteristiche latenti verso obiettivi campionati casualmente all'interno dello stesso spazio, rimuovendo progressivamente gli obiettivi fino a quando non ci rimangono solo obiettivi che rappresentano i centroidi dei cluster. Per mostrare il comportamento del nostro modello attraverso diverse modalità, applichiamo il nostro modello sia su dati di testo che di immagine e otteniamo risultati molto competitivi su MNIST. Infine, forniamo anche risultati contro modelli di base per fashion-MNIST, il dataset 20 newsgroups e un dataset Twitter creato da noi.
Un lavoro recente sulla generazione di spiegazioni per problemi decisionali ha visto il processo di spiegazione come un processo di riconciliazione del modello in cui un agente AI porta il modello mentale umano (delle sue capacità, credenze e obiettivi) alla stessa pagina per quanto riguarda un compito a portata di mano. Questa formulazione cattura succintamente molti tipi possibili di spiegazioni, così come affronta esplicitamente le varie proprietà - ad esempio gli aspetti sociali, la contrastività e la selettività - delle spiegazioni studiate nelle scienze sociali tra le interazioni uomo-uomo. Tuttavia, si scopre che lo stesso processo può essere dirottato nella produzione di "spiegazioni alternative" -- cioè spiegazioni che non sono vere ma che soddisfano ancora tutte le proprietà di una spiegazione adeguata.Nel lavoro precedente, abbiamo guardato a come tali spiegazioni possono essere percepite dall'umano nel ciclo e abbiamo alluso a un modo possibile di generarle.In questo articolo, entriamo più nei dettagli di questa curiosa caratteristica del processo di riconciliazione dei modelli e discutiamo implicazioni simili alla nozione generale di processo decisionale spiegabile.
Consideriamo nuove varianti di algoritmi di ottimizzazione.I nostri algoritmi si basano sull'osservazione che i mini-batch di gradienti stocastici in iterazioni consecutive non cambiano drasticamente e di conseguenza possono essere prevedibili.Ispirati dalla simile impostazione nella letteratura sull'apprendimento online chiamata Optimistic Online learning, proponiamo due nuovi algoritmi ottimistici per AMSGrad e Adam, rispettivamente, sfruttando la prevedibilità dei gradienti.  I nuovi algoritmi combinano l'idea del metodo del momento, il metodo del gradiente adattivo, e gli algoritmi dell'apprendimento online ottimistico, che porta ad accelerare l'addestramento delle reti neurali profonde nella pratica.
L'uso delle reti neurali ricorrenti (RNN) nei compiti di modellazione delle sequenze è promettente nel fornire risultati di alta qualità, ma è difficile soddisfare i requisiti di latenza rigorosi a causa del modello di esecuzione legato alla memoria delle RNN. Sfruttando la caratteristica di resilienza agli errori delle funzioni di attivazione non lineari utilizzate nelle RNN, proponiamo di utilizzare un piccolo modulo leggero che approssima lo strato RNN originale, a cui ci si riferisce come il grande modulo, per calcolare le attivazioni della regione insensibile che sono più resistenti agli errori. Il costoso accesso alla memoria e il calcolo del modulo grande possono essere ridotti in quanto i risultati sono utilizzati solo nella regione sensibile. Il nostro metodo può ridurre l'accesso complessivo alla memoria del 40% in media e raggiungere un'accelerazione da 1,54x a 1,75x su piattaforma server basata su CPU con un impatto trascurabile sulla qualità del modello.
Fine-grained Entity Recognition (FgER) è il compito di rilevare e classificare le menzioni di entità in un grande insieme di tipi che abbracciano diversi domini come quello biomedico, finanziario e sportivo.   Osserviamo che quando il set di tipi abbraccia diversi domini, il rilevamento delle menzioni di entità diventa una limitazione per i modelli di apprendimento supervisionato.  La ragione principale è la mancanza di set di dati in cui i confini delle entità sono annotati correttamente e che coprono un ampio spettro di tipi di entità.  Il nostro lavoro affronta direttamente questo problema.  Proponiamo il framework Heuristics Allied with Distant Supervision (HAnDS) per costruire automaticamente un dataset di qualità adatto al compito FgER.  Il framework HAnDS sfrutta l'alta interconnessione tra Wikipedia e Freebase in modo pipeline, riducendo gli errori di annotazione introdotti ingenuamente usando un approccio di supervisione a distanza.  Utilizzando il framework HAnDS, creiamo due set di dati, uno adatto a costruire sistemi FgER che riconoscono fino a 118 tipi di entità basati sulla gerarchia del tipo FIGER e un altro per fino a 1115 tipi di entità basati sulla gerarchia TypeNet.  La nostra vasta sperimentazione empirica garantisce la qualità dei set di dati generati.  Insieme a questo, forniamo anche un set di dati annotati manualmente per il benchmarking dei sistemi FgER.
L'implementazione di una corretta invocazione del metodo è un compito importante per gli sviluppatori di software, ma si tratta di un lavoro impegnativo, poiché la struttura dell'invocazione del metodo può essere complicata. In questo articolo, proponiamo InvocMap, uno strumento di completamento del codice che permette agli sviluppatori di ottenere un'implementazione di più invocazioni di metodo da una lista di nomi di metodi nel contesto del codice, ed è in grado di prevedere le invocazioni di metodi annidati i cui nomi non appaiono nella lista dei nomi dei metodi forniti dagli sviluppatori. Per ottenere questo, analizziamo le invocazioni di metodo da quattro livelli di astrazione.Costruiamo un motore di traduzione automatica per imparare la mappatura dal primo livello al terzo livello di astrazione delle invocazioni di metodo multiple, che richiede solo agli sviluppatori di aggiungere manualmente le variabili locali dall'espressione generata per ottenere il codice finale.Valutiamo il nostro approccio proposto su sei librerie popolari: Con il corpus di formazione di 2,86 milioni di invocazioni di metodi estratti da 1000 progetti Java Github e il corpus di test estratto da 120 frammenti di codice di forum online, InvocMap raggiunge il tasso di precisione fino a 84 nel punteggio F1- a seconda di quante informazioni di contesto fornite insieme ai nomi dei metodi, che mostra il suo potenziale per il completamento automatico del codice.
Gli avversari nelle reti neurali hanno attirato molta attenzione dal loro primo debutto. Mentre la maggior parte dei metodi esistenti mirano a ingannare i modelli di classificazione delle immagini in errori di classificazione o a creare attacchi per specifiche istanze di oggetti nei compiti di setection degli oggetti, noi ci concentriamo sulla creazione di avversari universali per ingannare i rilevatori di oggetti e nascondere gli oggetti dai rilevatori. Gli avversari che esaminiamo sono universali in tre modi: (1) Non sono specifici per specifiche istanze di oggetti; (2) Sono indipendenti dall'immagine; (3) Possono essere trasferiti a diversi modelli sconosciuti. Per ottenere questo, proponiamo due nuove tecniche per migliorare la trasferibilità degli avversari: \textit{piling-up} e \textit{monochromatizzazione}. Entrambe le tecniche dimostrano di semplificare gli schemi degli avversari generati, e in definitiva risultano in una maggiore trasferibilità.
Questo lavoro presenta il Poincaré Wasserstein Autoencoder, una riformulazione del quadro Wasserstein autoencoder recentemente proposto su un manifold non euclideo, il modello della sfera di Poincaré dello spazio iperbolico H n. Assumendo che lo spazio latente sia iperbolico, possiamo usare la sua gerarchia intrinseca per imporre una struttura alle rappresentazioni dello spazio latente apprese. Dimostriamo che per i set di dati con gerarchie latenti, possiamo recuperare la struttura in uno spazio latente a bassa dimensione. Dimostriamo inoltre il modello nel dominio visivo per analizzare alcune delle sue proprietà e mostriamo risultati competitivi su un compito di predizione dei collegamenti grafici.
In questo articolo, introduciamo un metodo per comprimere le mappe di caratteristiche intermedie delle reti neurali profonde (DNN) per diminuire i requisiti di memoria e di larghezza di banda durante l'inferenza.A differenza dei lavori precedenti, il metodo proposto si basa sulla conversione delle attivazioni a punto fisso in vettori sul più piccolo campo finito GF(2) seguito da strati di riduzione della dimensionalità non lineare (NDR) incorporati in una DNN. Tale rappresentazione appresa end-to-end trova mappe di caratteristiche più compatte sfruttando le ridondanze di quantizzazione all'interno delle attivazioni a punto fisso lungo il canale o le dimensioni spaziali. Applichiamo l'architettura di rete proposta ai compiti di classificazione di ImageNet e al rilevamento di oggetti PASCAL VOC. Rispetto agli approcci precedenti, gli esperimenti condotti mostrano un fattore di 2 di riduzione dei requisiti di memoria con una minore degradazione della precisione, aggiungendo solo calcoli bitwise.
L'addestramento avversario è una delle difese più forti contro gli attacchi avversari, ma richiede la generazione di esempi avversari per ogni mini-batch durante l'ottimizzazione.  La spesa per produrre questi esempi durante l'addestramento spesso preclude l'uso dell'addestramento adversariale su dataset di immagini complesse. In questo studio, esploriamo i meccanismi attraverso i quali l'addestramento avversario migliora la robustezza del classificatore, e dimostriamo che questi meccanismi possono essere efficacemente imitati utilizzando semplici metodi di regolarizzazione, tra cui l'attenuazione delle etichette e la compressione del logit.  Notevolmente, usando questi semplici metodi di regolarizzazione in combinazione con l'iniezione di rumore gaussiano, siamo in grado di raggiungere una forte robustezza avversaria -- spesso superando quella dell'addestramento avversario -- non usando esempi avversari.
Uno dei principali obiettivi dell'apprendimento non supervisionato è quello di scoprire le rappresentazioni dei dati che sono utili per i compiti successivi, senza accesso alle etichette supervisionate durante l'addestramento.tipicamente, questo comporta la minimizzazione di un obiettivo surrogato, come il log negativo della probabilità di un modello generativo, con la speranza che le rappresentazioni utili per i compiti successivi sorgano come effetto collaterale.in questo lavoro, proponiamo invece di mirare direttamente ai compiti successivi desiderati, meta-apprendendo una regola di apprendimento non supervisionato che porta a rappresentazioni utili per quei compiti.  Inoltre, vincoliamo la nostra regola di aggiornamento non supervisionata ad essere una funzione biologicamente motivata e locale al neurone, il che le permette di generalizzarsi a diverse architetture di reti neurali, insiemi di dati e modalità di dati. Mostriamo che la regola di aggiornamento meta-appresa produce caratteristiche utili e a volte supera le tecniche di apprendimento non supervisionato esistenti. Mostriamo inoltre che la regola di aggiornamento meta-appresa non supervisionata generalizza per allenare reti con diverse larghezze, profondità e non linearità, inoltre generalizza per allenarsi su dati con dimensioni di input permutate in modo casuale e generalizza anche da set di dati di immagini a un compito di testo.
C'è una significativa evidenza recente nell'apprendimento supervisionato che, nell'impostazione sovra-parametrizzata, le reti più larghe ottengono un migliore errore di test. In altre parole, il tradeoff bias-varianza non è direttamente osservabile quando si aumenta arbitrariamente la larghezza della rete. Sperimentiamo su quattro ambienti OpenAI Gym, aumentando la larghezza delle reti di valore e politica oltre i loro valori prescritti.I nostri risultati empirici danno supporto a questa ipotesi.Tuttavia, sintonizzare gli iperparametri di ogni larghezza di rete separatamente rimane come importante lavoro futuro in ambienti/algoritmi dove gli iperparametri ottimali variano notevolmente tra le larghezze, confondendo i risultati quando gli stessi iperparametri sono usati per tutte le larghezze.
L'apprendimento di rappresentazioni disgiunte dei dati è uno dei temi centrali dell'apprendimento non supervisionato in generale e della modellazione generativa in particolare.  In questo lavoro, affrontiamo uno scenario leggermente più intricato in cui le osservazioni sono generate da una distribuzione condizionale di una qualche variante di controllo nota e una qualche variante di rumore latente.  A tal fine, presentiamo un modello gerarchico e un metodo di formazione (CZ-GEM) che sfrutta alcuni dei recenti sviluppi nei modelli generativi basati sulla verosimiglianza e senza verosimiglianza.  Mostriamo che per formulazione, CZ-GEM introduce i giusti bias induttivi che assicurano la separazione delle variabili di controllo da quelle di rumore, mantenendo anche le componenti della variante di controllo separate, senza compromettere la qualità dei campioni generati.
L'apprendimento profondo ha prodotto prestazioni allo stato dell'arte su molti compiti di elaborazione del linguaggio naturale, tra cui il riconoscimento delle entità nominate (NER).Tuttavia, questo richiede tipicamente grandi quantità di dati etichettati.In questo lavoro, dimostriamo che la quantità di dati di formazione etichettati può essere drasticamente ridotta quando l'apprendimento profondo è combinato con l'apprendimento attivo.Mentre l'apprendimento attivo è campione-efficiente, può essere computazionalmente costoso in quanto richiede la riqualificazione iterativa.Per accelerare questo, introduciamo una architettura leggera per NER, cioè, Il modello raggiunge quasi lo stato dell'arte delle prestazioni su set di dati standard per il compito, mentre è computazionalmente molto più efficiente dei modelli più performanti. Eseguiamo l'apprendimento attivo incrementale, durante il processo di formazione, e siamo in grado di eguagliare quasi lo stato dell'arte delle prestazioni con solo il 25 % dei dati di formazione originali.
La quantizzazione della rete è una tecnica di compressione e accelerazione del modello che è diventata essenziale per l'implementazione delle reti neurali.La maggior parte dei metodi di quantizzazione per- forma un fine-tuning su una rete preaddestrata, ma questo a volte si traduce in una grande perdita di precisione rispetto alla rete originale.Introduciamo una nuova tecnica per formare reti amiche della quantizzazione, che possono essere direttamente convertite in una rete quantizzata accurata senza la necessità di un ulteriore fine-tuning.La nostra tecnica permette di quantizzare i pesi e le attivazioni di tutti gli strati della rete fino a 4 bit, raggiungendo un'alta efficienza e facilitando la distribuzione nelle impostazioni pratiche. Rispetto ad altre reti completamente quantizzate che operano a 4 bit, mostriamo sostanziali miglioramenti nell'accuratezza, ad esempio il 66,68% di accuratezza top-1 su ImageNet usando ResNet-18, rispetto alla precedente accuratezza allo stato dell'arte del 61,52% Louizos et al. (2019) e un'accuratezza di riferimento a precisione completa del 69,76%.Abbiamo eseguito una serie approfondita di esperimenti per testare l'efficacia del nostro metodo e abbiamo anche condotto studi di ablazione su diversi aspetti del metodo e delle tecniche per migliorare la stabilità e la precisione dell'allenamento.La nostra codebase e i modelli addestrati sono disponibili su GitHub.
Mentre gran parte del lavoro nella progettazione di reti convoluzionali negli ultimi cinque anni ha ruotato intorno all'indagine empirica dell'importanza della profondità, delle dimensioni dei filtri e del numero di canali di feature, studi recenti hanno dimostrato che la ramificazione, cioè Per combattere la complessità delle scelte di progettazione nelle architetture multiramo, il lavoro precedente ha adottato strategie semplici, come un fattore di ramificazione fisso, lo stesso input che viene alimentato a tutti i rami paralleli e una combinazione additiva degli output prodotti da tutti i rami nei punti di aggregazione. In questo lavoro rimuoviamo queste scelte predefinite e proponiamo un algoritmo per imparare le connessioni tra i rami nella rete, invece di essere scelto a priori dal progettista umano, la connettività multiramo viene appresa simultaneamente ai pesi della rete ottimizzando una singola funzione di perdita definita rispetto al compito finale. dimostriamo il nostro approccio sul problema della classificazione di immagini multiclasse utilizzando quattro diversi set di dati dove produce una precisione costantemente superiore rispetto alla rete multiramo all'avanguardia ``ResNeXt'' data la stessa capacità di apprendimento.
Sebbene le reti convoluzionali profonde abbiano ottenuto prestazioni migliori in molti compiti di linguaggio naturale, sono state trattate come scatole nere perché sono difficili da interpretare. In particolare, si sa poco su come rappresentano il linguaggio nei loro strati intermedi. Nel tentativo di comprendere le rappresentazioni delle reti convoluzionali profonde addestrate su compiti linguistici, mostriamo che le singole unità rispondono selettivamente a morfemi, parole e frasi specifiche, piuttosto che rispondere a modelli arbitrari e non interpretabili. Al fine di analizzare quantitativamente questo intrigante fenomeno, proponiamo un metodo di allineamento concettuale basato su come le unità rispondono al testo replicato. conduciamo analisi con diverse architetture su più dataset per compiti di classificazione e traduzione e forniamo nuove intuizioni su come i modelli profondi comprendono il linguaggio naturale.
L'applicazione dell'apprendimento di rinforzo (RL) ai problemi del mondo reale richiederà di ragionare sulla correlazione azione-ricompensa su orizzonti temporali lunghi. I metodi di apprendimento di rinforzo gerarchico (HRL) gestiscono questo dividendo il compito in gerarchie, spesso con struttura di rete sintonizzata a mano o sotto-obiettivi predefiniti. Proponiamo un nuovo quadro HRL TAIC, che impara l'astrazione temporale dall'esperienza passata o dalle dimostrazioni di esperti senza conoscenze specifiche del compito. Formuliamo il problema dell'astrazione temporale come apprendimento di rappresentazioni latenti di sequenze di azioni e presentiamo un nuovo approccio di regolarizzazione dello spazio latente aggiungendo vincoli teorici dell'informazione. Una visualizzazione dello spazio latente dimostra che il nostro algoritmo impara un'efficace astrazione delle lunghe sequenze di azioni. L'astrazione imparata ci permette di imparare nuovi compiti a un livello più alto in modo più efficiente. Trasmettiamo un significativo aumento della convergenza su problemi di apprendimento di riferimento.
Le reti neurali ricorrenti (RNNs) hanno raggiunto lo stato dell'arte delle prestazioni su molti compiti diversi, dalla traduzione automatica al riconoscimento dell'attività chirurgica, ma l'addestramento RNNs per catturare le dipendenze a lungo termine rimane difficile.Ad oggi, la stragrande maggioranza delle architetture RNN successo alleviare questo problema utilizzando connessioni quasi-additive tra gli stati, come introdotto dalla memoria a breve termine lungo (LSTM). Noi adottiamo un approccio ortogonale e introduciamo MIST RNN, un'architettura RNN NARX che permette connessioni dirette dal passato molto lontano. Mostriamo che MIST RNNs1) esibisce proprietà superiori di vanishing-gradient rispetto a LSTM e NARX RNNs precedentemente proposte; 2) sono molto più efficienti delle architetture NARX RNN precedentemente proposte, richiedendo anche meno calcoli di LSTM; e3) migliorano sostanzialmente le prestazioni rispetto a LSTM e Clockwork RNNs su compiti che richiedono dipendenze a lungo termine.
Un modello ben addestrato dovrebbe classificare gli oggetti con un punteggio unanime per ogni categoria.Questo richiede che le caratteristiche semantiche di alto livello siano uguali tra i campioni, nonostante un ampio intervallo di risoluzione, texture, deformazione, ecc.I lavori precedenti si concentrano sulla riprogettazione della funzione di perdita o sulla proposta di nuovi vincoli di regolarizzazione sulla perdita.In questo documento, affrontiamo questo problema attraverso una nuova prospettiva.Per ogni categoria, si presume che ci siano due set nello spazio delle caratteristiche: uno con informazioni più affidabili e l'altro con fonte meno affidabile. Sosteniamo che l'insieme affidabile potrebbe guidare l'apprendimento delle caratteristiche dell'insieme meno affidabile durante l'addestramento - nello spirito dello studente che imita il comportamento dell'insegnante e quindi spinge verso un centroide di classe più compatto nello spazio ad alta densità. Ci riferiamo a questo processo di apprendimento reciproco come feature intertwiner e incorporiamo lo spirito nel rilevamento degli oggetti.È noto che gli oggetti di bassa risoluzione sono più difficili da rilevare a causa della perdita di informazioni dettagliate durante il passaggio in avanti della rete.Consideriamo quindi gli oggetti di alta risoluzione come l'insieme affidabile e gli oggetti di bassa risoluzione come l'insieme meno affidabile.In particolare, un intertwiner si ottiene minimizzando la divergenza di distribuzione tra due insiemi. Progettiamo un buffer storico per rappresentare tutti i campioni precedenti nell'insieme affidabile e li utilizziamo per guidare l'apprendimento delle caratteristiche dell'insieme meno affidabile.Il progetto di ottenere una rappresentazione efficace delle caratteristiche per l'insieme affidabile è ulteriormente studiato, dove introduciamo l'algoritmo di trasporto ottimale (OT) nel framework.I campioni nell'insieme meno affidabile sono meglio allineati con l'insieme affidabile con l'aiuto della metrica OT.Incorporato con un tale intertwiner plug-and-play, otteniamo un evidente miglioramento rispetto al precedente state-of-the-arts sul benchmark di rilevamento degli oggetti COCO.
Gli approcci all'apprendimento continuo mirano ad apprendere con successo un insieme di compiti correlati che arrivano in modo online.Recentemente, sono stati sviluppati diversi framework che permettono di implementare l'apprendimento profondo in questo scenario di apprendimento.Una decisione chiave di modellazione è in che misura l'architettura dovrebbe essere condivisa tra i compiti. Da un lato, modellare separatamente ogni compito evita la dimenticanza catastrofica, ma non supporta l'apprendimento di trasferimento e porta a modelli di grandi dimensioni; dall'altro, specificare rigidamente un componente condiviso e una parte specifica del compito permette il trasferimento del compito e limita la dimensione del modello, ma è vulnerabile alla dimenticanza catastrofica e limita la forma di trasferimento del compito che può avvenire. Qui introduciamo un approccio chiamato Apprendimento Continuo con Pesi Adattivi (CLAW), che si basa sulla modellazione probabilistica e sull'inferenza variazionale. Gli esperimenti mostrano che CLAW raggiunge prestazioni all'avanguardia su sei benchmark in termini di prestazioni complessive di apprendimento continuo, misurate dall'accuratezza della classificazione, e in termini di gestione della dimenticanza catastrofica.
I dati ad alta dimensione spesso si trovano in o vicino a sottospazi a bassa dimensione. I metodi di clustering del sottospazio sparso con sparsità indotta da L0-norm, come L0-Sparse Subspace Clustering (L0-SSC), hanno dimostrato di essere più efficaci della sua controparte L1 come Sparse Subspace Clustering (SSC). Tuttavia, questi metodi di clustering basati su L0-norm sono limitati ai dati puliti che si trovano esattamente nei sottospazi. I dati reali spesso soffrono di rumore e possono trovarsi vicino ai sottospazi. Mostriamo che la soluzione ottimale al problema di ottimizzazione di L0-SSC rumoroso raggiunge la proprietà di rilevamento del sottospazio (SDP), un elemento chiave con il quale i dati da diversi sottospazi sono separati, sotto modelli deterministici e randomizzati.I nostri risultati forniscono una garanzia teorica sulla correttezza di L0-SSC rumoroso in termini di SDP su dati rumorosi. Proponiamo inoltre Noisy-DR-L0-SSC che recupera in modo dimostrabile i sottospazi su dati di dimensione ridotta.Noisy-DR-L0-SSC proietta prima i dati in uno spazio di dimensione inferiore tramite trasformazione lineare, poi esegue L0-SSC rumoroso sui dati di dimensione ridotta in modo da migliorare l'efficienza.I risultati sperimentali dimostrano l'efficacia di Noisy L0-SSC e Noisy-DR-L0-SSC.
La connettività di modo fornisce nuove intuizioni geometriche sull'analisi dei paesaggi di perdita e permette di costruire percorsi di alta precisione tra reti neurali ben addestrate. In questo lavoro, proponiamo di impiegare la connettività di modo nei paesaggi di perdita per studiare la robustezza avversaria delle reti neurali profonde, e fornire nuovi metodi per migliorare questa robustezza.  Quando i modelli di rete vengono manomessi con attacchi backdoor o di iniezione di errori, i nostri risultati dimostrano che la connessione del percorso appresa utilizzando una quantità limitata di dati in buona fede può mitigare efficacemente gli effetti avversari mantenendo l'accuratezza originale su dati puliti.Pertanto, la connettività di modalità fornisce agli utenti il potere di riparare i modelli backdoor o di iniezione di errori.  Usiamo anche la connettività di modalità per studiare i paesaggi di perdita dei modelli regolari e robusti contro gli attacchi di evasione.Gli esperimenti mostrano che esiste una barriera nella perdita di robustezza avversaria sul percorso che collega i modelli regolari e addestrati avversariamente.  Si osserva un'alta correlazione tra la perdita di robustezza avversaria e il più grande autovalore della matrice Hessiana di input, per la quale vengono fornite giustificazioni teoriche.  I nostri risultati suggeriscono che la connettività del modo offre uno strumento olistico e un mezzo pratico per valutare e migliorare la robustezza avversaria.
Le reti generative avversarie (GAN) imparano a mappare i campioni da una distribuzione di rumore a una distribuzione di dati scelta. Un lavoro recente ha dimostrato che le GAN sono di conseguenza sensibili e limitate dalla forma della distribuzione del rumore. Noi affrontiamo questo problema imparando a generare da modelli multipli in modo che l'output del generatore sia effettivamente la combinazione di diverse reti distinte. Contribuiamo ad una nuova formulazione di modelli multi-generatore in cui impariamo un priore sui generatori condizionati sul rumore, parametrizzato da una rete neurale. Così, questa rete non solo impara il tasso ottimale per campionare da ogni generatore ma anche modella in modo ottimale il rumore ricevuto da ogni generatore. Il Noise Prior GAN (NPGAN) risultante raggiunge un'espressività e una flessibilità che supera sia i modelli a generatore singolo che i precedenti modelli multi-generatore.
I recenti progressi nelle Generative Adversarial Networks (GANs) - nella progettazione architettonica, nelle strategie di addestramento e nei trucchi empirici - hanno portato a campioni quasi fotorealistici su datasets su larga scala come ImageNet.  Infatti, per un modello in particolare, BigGAN, metriche come Inception Score o Frechet Inception Distance quasi corrispondono a quelle del dataset, suggerendo che questi modelli sono vicini a corrispondere alla distribuzione del set di allenamento.   Data la qualità di questi modelli, vale la pena capire fino a che punto questi campioni possono essere utilizzati per l'aumento dei dati, un compito espresso come un obiettivo a lungo termine del progetto di ricerca GAN.  A tal fine, addestriamo i classificatori ResNet-50 utilizzando esclusivamente immagini BigGAN o miscele di immagini ImageNet e BigGAN, e testiamo sul set di validazione ImageNet. I nostri risultati preliminari suggeriscono sia una visione misurata dello stato dell'arte della qualità GAN che evidenziano i limiti delle metriche attuali. Usando solo le immagini BigGAN, troviamo che gli errori Top-1 e Top-5 sono aumentati rispettivamente del 120% e del 384%, e inoltre, l'aggiunta di più dati BigGAN al set di training ImageNet, nel migliore dei casi, migliora solo marginalmente le prestazioni del classificatore.Infine, troviamo che né Inception Score, né FID, né le loro combinazioni sono predittive della precisione di classificazione.   Questi risultati suggeriscono che, dato che le GAN stanno iniziando ad essere impiegate nei compiti a valle, dovremmo creare delle metriche che misurino meglio le prestazioni dei compiti a valle.  Proponiamo la performance di classificazione come una di queste metriche che, oltre a valutare la qualità del campione per classe, è più adatta a questi compiti a valle.
Le moderne reti federate, come quelle composte da dispositivi indossabili, telefoni cellulari o veicoli autonomi, generano enormi quantità di dati ogni giorno. Questa ricchezza di dati può aiutare ad apprendere modelli che possono migliorare l'esperienza dell'utente su ogni dispositivo, ma la scala e l'eterogeneità dei dati federati presentano nuove sfide in aree di ricerca come l'apprendimento federato, il meta-apprendimento e l'apprendimento multi-task. Mentre la comunità dell'apprendimento automatico inizia ad affrontare queste sfide, siamo in un momento critico per assicurare che gli sviluppi fatti in queste aree siano basati su benchmark realistici. A questo scopo, proponiamo Leaf, un framework modulare di benchmarking per l'apprendimento in ambienti federati. Leaf include una suite di dataset federati open-source, un framework di valutazione rigoroso e un set di implementazioni di riferimento, tutti orientati a catturare gli ostacoli e le complessità degli ambienti federati pratici.
Comprendere il movimento degli oggetti è uno dei problemi fondamentali della computer vision, che richiede la segmentazione e il tracciamento degli oggetti nel tempo. Sono stati fatti progressi significativi nella segmentazione delle istanze, ma tali modelli non possono tracciare gli oggetti e, cosa più importante, non sono in grado di ragionare sia nello spazio 3D che nel tempo. Il nostro modello include una rete temporale che impara a modellare il contesto temporale e il movimento, che è essenziale per produrre incorporazioni uniformi nel tempo. Inoltre, il nostro modello stima anche la profondità monoculare, con una perdita auto-supervisionata, poiché la distanza relativa di un oggetto limita efficacemente dove può essere il prossimo, assicurando un'incorporazione coerente nel tempo.Infine, dimostriamo che il nostro modello può accuratamente tracciare e segmentare le istanze, anche con occlusioni e rilevamenti mancati, avanzando lo stato dell'arte sul KITTI Multi-Object and Tracking Dataset.
Motivato dalla richiesta di un efficace algoritmo di apprendimento di rinforzo profondo che si adatti all'ambiente di ricompensa sparsa, questo articolo presenta Hindsight Trust Region Policy Optimization (HTRPO), un metodo che utilizza in modo efficiente le interazioni in condizioni di ricompensa sparsa per ottimizzare le politiche all'interno della regione di fiducia e, nel frattempo, mantiene la stabilità di apprendimento. In primo luogo, adattiamo teoricamente la funzione obiettivo TRPO, sotto forma di rendimento atteso della politica, alla distribuzione dei dati di hindsight generati dagli obiettivi alternativi.Poi, applichiamo Monte Carlo con campionamento di importanza per stimare la KL-divergenza tra due politiche, prendendo i dati di hindsight come input.Sotto la condizione che le distribuzioni sono sufficientemente vicine, la KL-divergenza è approssimata da un'altra f-divergenza.Tale approssimazione risulta nella diminuzione della varianza e allevia l'instabilità durante l'aggiornamento della politica.  I risultati sperimentali su compiti di riferimento sia discreti che continui dimostrano che HTRPO converge significativamente più velocemente dei precedenti metodi di gradiente delle politiche, ottenendo prestazioni efficaci e un'alta efficienza dei dati per l'addestramento delle politiche in ambienti di ricompensa sparsi.
L'open-domain question answering (QA) è un problema importante in AI e NLP che sta emergendo come un campanello d'allarme per il progresso sulla generalizzabilità dei metodi e delle tecniche di AI.Gran parte del progresso nei sistemi di QA open-domain è stato realizzato attraverso i progressi nei metodi di information retrieval e nella costruzione del corpus. In questo articolo, ci concentriamo sul recente set di dati ARC Challenge, che contiene 2.590 domande a scelta multipla create per gli esami di scienze delle scuole elementari. Queste domande sono state selezionate per essere le più impegnative per gli attuali sistemi QA, e lo stato attuale delle prestazioni dell'arte è solo leggermente migliore del caso. Presentiamo un sistema che riformula una domanda data in query che sono usate per recuperare il testo di supporto da un grande corpus di testo relativo alla scienza.Il nostro riscrittore è in grado di incorporare la conoscenza di fondo da ConceptNet e -- in tandem con un generico sistema di entailment testuale addestrato su SciTail che identifica il supporto nei risultati recuperati -- supera diverse linee di base forti sul compito di QA end-to-end nonostante sia addestrato solo a identificare i termini essenziali nella domanda originale di origine. Usiamo una metodologia di decisione generalizzabile sulle prove recuperate e sui candidati alla risposta per selezionare la migliore risposta. Combinando la riformulazione della domanda, la conoscenza di base e l'implicazione testuale, il nostro sistema è in grado di superare diverse linee di base forti sul set di dati ARC.
La teoria dell'apprendimento automatico implica che tali reti sono altamente iperparametrizzate e che dovrebbe essere possibile ridurre le loro dimensioni senza sacrificare la precisione, e infatti molti studi recenti hanno iniziato a evidenziare le ridondanze specifiche che possono essere sfruttate per raggiungere questo obiettivo. In questo articolo, facciamo un ulteriore passo in questa direzione proponendo un approccio di condivisione dei filtri per comprimere le CNN profonde che riduce la loro impronta di memoria applicando ripetutamente una singola mappatura convoluzionale dei filtri appresi per simulare una pipeline CNN. Mostriamo, attraverso esperimenti su CIFAR-10, CIFAR-100, Tiny ImageNet, e ImageNet che questo ci permette di ridurre il numero di parametri delle reti basate su progetti comuni come VGGNet e ResNet di un fattore proporzionale alla loro profondità, lasciando la loro accuratezza in gran parte inalterata.A un livello più ampio, il nostro approccio indica anche come le regolarità dello spazio di scala trovate nei segnali visivi possono essere sfruttate per costruire architetture neurali che sono più parsimoniose e interpretabili.
Estendiamo i recenti risultati di (Arora et al., 2019) con un'analisi spettrale delle rappresentazioni corrispondenti a kernel e embeddings neurali.Hanno dimostrato che in una semplice rete a singolo strato, l'allineamento delle etichette agli autovettori della matrice di Gram corrispondente determina sia la convergenza dell'ottimizzazione durante l'addestramento che le proprietà di generalizzazione.Generalizziamo il loro risultato a kernel e rappresentazioni neurali e mostriamo che queste estensioni migliorano sia l'ottimizzazione che la generalizzazione della configurazione di base studiata in (Arora et al., 2019).
Ottenere stime di incertezza di alta qualità è essenziale per molte applicazioni di reti neurali profonde.In questo articolo, giustifichiamo teoricamente uno schema per stimare le incertezze, basato sul campionamento da una distribuzione prioritaria.Crucialmente, le stime di incertezza sono mostrate per essere conservative nel senso che non sottostimano mai un'incertezza posteriore ottenuta da un ipotetico algoritmo bayesiano. Le stime di incertezza ottenute da priori casuali possono essere adattate a qualsiasi architettura di rete profonda e addestrate utilizzando pipeline standard di apprendimento supervisionato. Forniamo una valutazione sperimentale dei priori casuali sulla calibrazione e sul rilevamento fuori distribuzione su compiti tipici della computer vision, dimostrando che essi superano gli ensemble profondi nella pratica.
Il modello proposto è in grado di mappare qualsiasi dato parziale in una distribuzione variazionale latente multimodale e il campionamento da tale distribuzione porta all'imputazione stocastica. La valutazione preliminare sul dataset MNIST mostra una promettente imputazione stocastica condizionata su immagini parziali come input.
Questo articolo studia gli attacchi di inversione del modello, in cui si abusa dell'accesso a un modello per dedurre informazioni sui dati di addestramento. Dalla sua prima introduzione da parte di Fredrikson2014privacy, tali attacchi hanno sollevato serie preoccupazioni dato che i dati di addestramento di solito contengono informazioni sensibili. Finora, gli attacchi di inversione del modello di successo sono stati dimostrati solo su modelli semplici, come la regressione lineare e la regressione logistica.I tentativi precedenti di invertire le reti neurali, anche quelle con architetture semplici, non sono riusciti a produrre risultati convincenti.Presentiamo un nuovo metodo di attacco, chiamato \emph{attacco generativo di inversione del modello}, che può invertire le reti neurali profonde con alti tassi di successo. Piuttosto che ricostruire da zero i dati di allenamento privati, sfruttiamo le informazioni pubbliche parziali, che possono essere molto generiche, per imparare un priore distributivo attraverso le reti generative avversarie (GAN) e usarlo per guidare il processo di inversione. Inoltre, dimostriamo teoricamente che il potere predittivo di un modello e la sua vulnerabilità agli attacchi di inversione sono effettivamente due lati della stessa medaglia - i modelli altamente predittivi sono in grado di stabilire una forte correlazione tra caratteristiche ed etichette, che coincide esattamente con ciò che un avversario sfrutta per montare gli attacchi. I nostri esperimenti dimostrano che l'attacco proposto migliora l'accuratezza dell'identificazione rispetto al lavoro esistente di circa 75$%$ per la ricostruzione delle immagini del volto da un classificatore di riconoscimento del volto allo stato dell'arte.Mostriamo anche che la privacy differenziale, nella sua forma canonica, è poco utile per proteggere dai nostri attacchi.
I metodi GAN basati sullo spazio latente e le architetture encoder-decoder basate sull'attenzione hanno raggiunto risultati impressionanti nella generazione di testo e nella NMT non supervisionata, rispettivamente. sfruttando i due domini, proponiamo un'architettura basata sullo spazio latente avversaria in grado di generare frasi parallele in due lingue contemporaneamente e di tradurre bidirezionalmente. l'obiettivo di generazione bilingue è raggiunto campionando dallo spazio latente che è condizionato avversariamente ad essere condiviso tra le due lingue. Per prima cosa viene addestrato un modello NMT, con una back-translation e una configurazione avversaria, per imporre uno stato latente tra le due lingue.Il codificatore e il decodificatore sono condivisi per le due direzioni di traduzione.Successivamente, una GAN viene addestrata per generare codice 'sintetico' che imita lo spazio latente condiviso delle lingue.Questo codice viene quindi inserito nel decodificatore per generare testo in entrambe le lingue.Eseguiamo i nostri esperimenti sui dataset Europarl e Multi30k, sulla coppia linguistica inglese-francese, e documentiamo le nostre prestazioni utilizzando sia la NMT supervisionata che non supervisionata.
Man mano che l'intelligenza artificiale (AI) diventa parte integrante della nostra vita, lo sviluppo dell'AI spiegabile, incarnata nel processo decisionale di un agente AI o robotico, diventa imperativo.  Per un compagno di squadra robotico, la capacità di generare spiegazioni per spiegare il suo comportamento è uno dei requisiti chiave di un'agenzia spiegabile.Il lavoro precedente sulla generazione di spiegazioni si concentra sul supporto del ragionamento dietro il comportamento del robot.Questi approcci, tuttavia, non riescono a considerare il carico di lavoro mentale necessario per comprendere la spiegazione ricevuta. In altre parole, ci si aspetta che il compagno di squadra umano capisca qualsiasi spiegazione fornita, spesso prima dell'esecuzione del compito, non importa quante informazioni siano presentate nella spiegazione.In questo lavoro, sosteniamo che una spiegazione, specialmente quelle complesse, dovrebbe essere fatta in modo online durante l'esecuzione, che aiuta a diffondere le informazioni da spiegare e quindi a ridurre il carico di lavoro mentale degli umani. Tuttavia, una sfida qui è che le diverse parti di una spiegazione sono dipendenti l'una dall'altra, il che deve essere preso in considerazione quando si generano spiegazioni online.A tal fine, viene presentata una formulazione generale della generazione di spiegazioni online insieme a tre diverse implementazioni che soddisfano diverse proprietà online.Basiamo il nostro metodo di generazione di spiegazioni su un'impostazione di riconciliazione dei modelli introdotta nel nostro lavoro precedente.I nostri approcci sono valutati sia con soggetti umani in una competizione di pianificazione standard (IPC) dominio, utilizzando NASA Task Load Index (TLX), così come in simulazione con dieci diversi problemi attraverso due domini IPC.
Le reti neurali profonde utilizzano strutture più ampie e profonde per ottenere prestazioni migliori e, di conseguenza, utilizzano sempre più memoria della GPU. Tuttavia, una memoria GPU limitata limita molti potenziali progetti di reti neurali. In questo articolo, proponiamo un algoritmo di scambio e ricomputazione delle variabili basato sull'apprendimento di rinforzo per ridurre il costo della memoria, senza sacrificare la precisione dei modelli. La ricomputazione può scambiare tempo per spazio rimuovendo alcune mappe di caratteristiche durante la propagazione in avanti.Le funzioni in avanti vengono eseguite ancora una volta per ottenere le mappe di caratteristiche prima del riutilizzo.Tuttavia, come decidere automaticamente quali variabili devono essere scambiate o ricomputate rimane un problema impegnativo.Per affrontare questo problema, proponiamo di utilizzare una rete Q profonda (DQN) per fare piani.Combinando lo scambio di variabili e la ricomputazione, i nostri risultati superano diversi benchmark noti.
In vanilla backpropagation (VBP), la funzione di attivazione conta considerevolmente in termini di non linearità e differenziabilità.Vanishing gradient è stato un problema importante legato alla cattiva scelta della funzione di attivazione nel deep learning (DL).Questo lavoro dimostra che una funzione di attivazione differenziabile non è più necessaria per la backpropagation degli errori. La derivata della funzione di attivazione può essere sostituita da una differenziazione temporale iterativa (ITD) usando un allineamento casuale fisso dei pesi di feedback (FBA).Usando FBA con ITD, possiamo trasformare il VBP in un approccio biologicamente più plausibile per l'apprendimento di architetture di reti neurali profonde.Non sosteniamo che ITD funzioni completamente come la plasticità dipendente dai picchi temporali (STDP) nel nostro cervello ma questo lavoro può essere un passo verso l'integrazione della backpropagation degli errori basata su STDP nell'apprendimento profondo.
Ispirato dai recenti successi dei modelli generativi profondi per Text-To-Speech (TTS) come WaveNet (van den Oord et al., 2016) e Tacotron (Wang et al., 2017), questo articolo propone l'uso di un modello generativo profondo su misura per il riconoscimento automatico del parlato (ASR) come modello acustico primario (AM) per un sistema di riconoscimento complessivo con un modello linguistico separato (LM).Vengono considerate due dimensioni di profondità: (1) l'uso di reti di densità di miscele, sia autoregressive che non autoregressive, per generare funzioni di densità in grado di modellare sequenze di input acustici con un condizionamento molto più potente rispetto ai modelli generativi di prima generazione per ASR, Gaussian Mixture Models / Hidden Markov Models (GMM/HMMs), e (2) l'uso di LSTMs standard, nello spirito dell'approccio tandem originale, per produrre vettori di caratteristiche discriminative per la modellazione generativa. La combinazione di reti a densità di miscela e caratteristiche discriminative profonde porta a una nuova architettura LSTM dual-stack direttamente collegata al Trasduttore RNN (Graves, 2012), ma con la forma funzionale esplicita di una densità, e che si combina naturalmente con un modello linguistico separato, utilizzando la regola di Bayes.I modelli generativi qui discussi sono confrontati sperimentalmente in termini di log-likelihoods e accuratezze di frame.
Studi recenti dimostrano che le reti neurali profonde (DNNs) ampiamente utilizzate sono vulnerabili agli esempi adversari attentamente creati.Molti algoritmi avanzati sono stati proposti per generare esempi adversari sfruttando la distanza L_p per penalizzare le perturbazioni.Sono stati esplorati anche diversi metodi di difesa per difendersi da tali attacchi adversari. Mentre l'efficacia della distanza L_p come metrica della qualità percettiva rimane un'area di ricerca attiva, in questo documento ci concentreremo invece su un diverso tipo di perturbazione, vale a dire la trasformazione spaziale, al contrario della manipolazione diretta dei valori dei pixel come nei lavori precedenti. Le perturbazioni generate attraverso la trasformazione spaziale potrebbero risultare in grandi misure di distanza L_p, ma i nostri ampi esperimenti mostrano che tali esempi avversari trasformati spazialmente sono percettivamente realistici e più difficili da difendere con i sistemi di difesa esistenti. Visualizziamo la perturbazione basata sulla trasformazione spaziale per diversi esempi e mostriamo che la nostra tecnica è in grado di produrre esempi realistici con una deformazione dell'immagine liscia. Infine, visualizziamo l'attenzione delle reti profonde con diversi tipi di esempi avversari per capire meglio come questi esempi vengono interpretati.
Motivati dal progetto di design Game AI per King of Glory (KOG), uno dei giochi mobili più popolari del mondo, consideriamo lo scenario con lo spazio d'azione ibrido discreto-continuo. Per applicare direttamente i quadri DLR esistenti, gli approacheseither esistenti approssimano lo spazio ibrido da un insieme discreto o rilassandolo in un insieme continuo, che è solitamente meno efficiente e robusto. Il nostro algoritmo combina DQN e DDPG e può essere visto come un'estensione del DQN alle azioni ibride. Lo studio empirico sul gioco KOG convalida l'efficienza e l'efficacia del nostro metodo.
Nell'ultimo decennio, i grafi di conoscenza sono diventati popolari per catturare la conoscenza strutturata del dominio. I modelli di apprendimento relazionale permettono la previsione dei collegamenti mancanti all'interno dei grafi di conoscenza.Più specificamente, gli approcci a distanza latente modellano le relazioni tra le entità attraverso una distanza tra rappresentazioni latenti.I modelli di embedding traslativo (ad esempio, TransE) sono tra gli approcci a distanza latente più popolari che usano una funzione di distanza per imparare modelli di relazioni multiple. Tuttavia, sono per lo più inefficienti nel catturare le relazioni simmetriche poiché la norma del vettore di rappresentazione per tutte le relazioni simmetriche diventa uguale a zero.Perdono anche informazioni quando imparano relazioni con modelli riflessivi poiché diventano simmetriche e transitive.Proponiamo il modello Multiple Distance Embedding (MDE) che affronta queste limitazioni e una struttura che permette combinazioni collaborative di termini latenti basati sulla distanza (MDE). La nostra soluzione si basa su due principi: 1) usare una perdita basata sui limiti invece di una perdita basata sui margini e 2) imparando vettori di incorporamento indipendenti per ciascuno dei termini, possiamo addestrare e prevedere collettivamente l'uso di termini a distanza contraddittoria.Dimostriamo inoltre che MDE permette di modellare relazioni con (anti)simmetria, inversione e modelli di composizione. Proponiamo MDE come un modello di rete neurale che ci permette di mappare le relazioni non lineari tra i vettori di incorporazione e l'output previsto della funzione di punteggio.I nostri risultati empirici mostrano che MDE supera i modelli di incorporazione allo stato dell'arte su diversi set di dati di riferimento.
Improved generative adversarial network (Improved GAN) è un metodo di successo per utilizzare modelli generativi avversari per risolvere il problema dell'apprendimento semi-supervisionato, ma soffre del problema della formazione instabile. Per rimediare a questo problema, proponiamo un nuovo metodo per utilizzare l'addestramento collaborativo per migliorare la stabilità del GAN semi-supervisionato con la combinazione di Wasserstein GAN.Gli esperimenti hanno dimostrato che il nostro metodo proposto è più stabile dell'originale GAN migliorato e raggiunge una precisione di classificazione comparabile su diversi set di dati.
È noto da tempo che una rete neurale monostrato completamente connessa con una priorità i.i.d. sui suoi parametri è equivalente a un processo gaussiano (GP), nel limite della larghezza infinita della rete.  Questa corrispondenza permette l'inferenza bayesiana esatta per reti neurali di larghezza infinita su compiti di regressione per mezzo della valutazione della GP corrispondente. Recentemente, sono state sviluppate funzioni kernel che imitano le reti neurali casuali multistrato, ma solo al di fuori di un quadro bayesiano. Come tale, il lavoro precedente non ha identificato che questi kernel possono essere utilizzati come funzioni di covarianza per le GP e consentire una previsione completamente bayesiana con una rete neurale profonda. In questo lavoro, deriviamo l'esatta equivalenza tra reti profonde infinitamente ampie e GP con una particolare funzione di covarianza e sviluppiamo una pipeline computazionalmente efficiente per calcolare questa funzione di covarianza, quindi utilizziamo il GP risultante per eseguire l'inferenza bayesiana per reti neurali profonde su MNIST e CIFAR-10.  Osserviamo che l'accuratezza della rete neurale addestrata si avvicina a quella della GP corrispondente con l'aumentare della larghezza dello strato, e che l'incertezza della GP è fortemente correlata con l'errore di previsione della rete addestrata. Troviamo inoltre che le prestazioni del test aumentano quando le reti addestrate a larghezza finita sono rese più ampie e più simili a una GP, e che le previsioni basate sulla GP tipicamente superano quelle delle reti a larghezza finita.
Lo spazio di ricerca è una considerazione chiave per la ricerca di architetture neurali.Recentemente, Xie et al. (2019a) hanno trovato che le reti generate casualmente dalla stessa distribuzione hanno prestazioni simili, che suggeriscono che dovremmo cercare distribuzioni di grafi casuali invece di grafi.Proponiamo graphon come un nuovo spazio di ricerca. Un graphon è il limite della sequenza di Cauchy di grafi e una distribuzione probabilistica libera da scala, da cui possono essere tratti grafi di diverso numero di vertici.Questa proprietà ci consente di eseguire NAS utilizzando modelli veloci e a bassa capacità e scalare i modelli trovati quando necessario.Sviluppiamo un algoritmo per NAS nello spazio dei graphon e dimostriamo empiricamente che può trovare grafi stage-wise che superano DenseNet e altre baseline su ImageNet.
L'addestramento avversario è di gran lunga la strategia di maggior successo per migliorare la robustezza delle reti neurali agli attacchi avversari.Nonostante il suo successo come meccanismo di difesa, l'addestramento avversario non riesce a generalizzare bene al test set imperturbato.Noi ipotizziamo che questa scarsa generalizzazione sia una conseguenza dell'addestramento avversario con un raggio di perturbazione uniforme intorno ad ogni campione di addestramento.I campioni vicini al confine della decisione possono essere morphati in una classe diversa sotto un piccolo budget di perturbazione, e l'imposizione di grandi margini intorno a questi campioni produce confini decisionali poveri che generalizzano male. Motivati da questa ipotesi, proponiamo l'addestramento adversariale adattivo di istanza - una tecnica che fa rispettare i margini di perturbazione campione-specifici intorno ad ogni campione di addestramento. Mostriamo che usando il nostro metodo, l'esattezza della prova sui campioni imperturbati migliora con un calo marginale nella robustezza.
I modelli di apprendimento automatico, compresi i modelli tradizionali e le reti neurali, possono essere facilmente ingannati da esempi avversari che sono generati dagli esempi naturali con piccole perturbazioni.  Questo pone una sfida critica alla sicurezza dell'apprendimento automatico, e impedisce l'ampia applicazione dell'apprendimento automatico in molti domini importanti come la computer vision e il rilevamento di malware.  Sfortunatamente, anche gli approcci di difesa allo stato dell'arte, come l'addestramento avversario e la distillazione difensiva, soffrono ancora di grandi limitazioni e possono essere aggirati.  Da un'angolazione unica, proponiamo di indagare due importanti domande di ricerca in questo articolo: Gli esempi avversari sono distinguibili dagli esempi naturali?  Gli esempi contraddittori generati da metodi diversi sono distinguibili l'uno dall'altro?  Queste due domande riguardano la distinguibilità degli esempi avversari.  Rispondere a queste domande porterà potenzialmente a un approccio semplice ma efficace, definito in questo articolo come distinzione difensiva sotto la formulazione di classificazione multi-label, per proteggersi dagli esempi avversari.  Progettiamo ed eseguiamo esperimenti utilizzando il dataset MNIST per indagare queste due domande, e otteniamo risultati altamente positivi che dimostrano la forte distinguibilità degli esempi avversari.  Raccomandiamo che questo unico approccio di distinzione difensiva sia seriamente considerato per integrare altri approcci di difesa.
Per molto tempo, la progettazione di architetture neurali che mostrano alte prestazioni è stata considerata un'arte oscura che richiedeva una messa a punto manuale da parte di esperti. Una delle poche linee guida ben conosciute per la progettazione dell'architettura è quella di evitare l'esplosione o la scomparsa dei gradienti. Tuttavia, anche questa linea guida è rimasta relativamente vaga e circostanziale, perché non esiste una metrica ben definita, basata sul gradiente, che possa essere calcolata prima dell'inizio della formazione e che possa prevedere in modo robusto le prestazioni della rete dopo il completamento della formazione. Attraverso un ampio studio empirico, dimostriamo che l'NLC, calcolato nello stato di inizializzazione casuale della rete, è un potente predittore dell'errore di test e che raggiungere un NLC della giusta dimensione è essenziale per ottenere un errore di test ottimale, almeno nelle reti feedforward completamente connesse. L'NLC è anche concettualmente semplice, poco costoso da calcolare, ed è robusto a una serie di confondenti e scelte di progettazione architettonica che le metriche comparabili non sono necessariamente robuste. Quindi, sosteniamo che l'NLC è uno strumento importante per la ricerca e la progettazione dell'architettura, in quanto può prevedere in modo robusto i risultati scadenti dell'addestramento prima ancora che inizi la formazione.
Questo articolo fornisce un'analisi rigorosa delle Reti di Hamming Generalizzate (GHN) addestrate proposte da Fan (2017) e rivela un'interessante scoperta sulle GHN, cioè che gli strati di convoluzione impilati in una GHN sono equivalenti a un singolo ma ampio strato di convoluzione.L'equivalenza rivelata, sul lato teorico, può essere considerata come una manifestazione costruttiva del teorema di approssimazione universale Cybenko (1989); Hornik (1991). Per la visualizzazione della rete, gli epitomi profondi costruiti ad ogni livello forniscono una visualizzazione della rappresentazione interna della rete che non si basa sui dati di input; inoltre, gli epitomi profondi permettono l'estrazione diretta delle caratteristiche in un solo passo, senza ricorrere alle ottimizzazioni regolarizzate utilizzate negli strumenti di visualizzazione esistenti.
Ci sono due paradigmi principali di attacchi white-box adversarial che tentano di imporre perturbazioni di input.  Il primo paradigma, chiamato attacco fix-perturbation, realizza campioni avversari entro un dato livello di perturbazione.  Il secondo paradigma, chiamato attacco zero-confidenza, trova la più piccola perturbazione necessaria per causare un errore di classificazione, noto anche come margine di una caratteristica di input.  Mentre il primo paradigma è ben risolto, il secondo non lo è.  Gli attacchi esistenti a zero confidenza o introducono significativi errori di approssimazione, o sono troppo dispendiosi in termini di tempo.  Proponiamo quindi MarginAttack, una struttura di attacco a zero confidenza che è in grado di calcolare il margine con una migliore precisione ed efficienza.  I nostri esperimenti dimostrano che MarginAttack è in grado di calcolare un margine più piccolo rispetto agli attacchi zero-confidence allo stato dell'arte, e corrisponde agli attacchi fix-perturbation allo stato dell'arte.  Inoltre, è significativamente più veloce dell'attacco Carlini-Wagner, attualmente l'algoritmo di attacco a zero confidenza più accurato.
Piuttosto che enumerare le variazioni tra i canali del filtro o i livelli della piramide, i modelli dinamici predicono localmente la scala e adattano i campi recettivi di conseguenza. Il grado di variazione e la diversità degli input rendono questo un compito difficile. I metodi esistenti o imparano un predittore feedforward, che non è totalmente immune alla variazione di scala che intende contrastare, o selezionano le scale con un algoritmo fisso, che non può imparare dal compito dato e dai dati. Proponiamo un nuovo obiettivo di minimizzazione dell'entropia per l'inferenza e ottimizziamo i parametri del compito e della struttura per sintonizzare il modello ad ogni input. L'ottimizzazione durante l'inferenza migliora la precisione della segmentazione semantica e generalizza meglio le variazioni di scala estreme che fanno vacillare l'inferenza dinamica feedforward.
Il Deep Image Prior (DIP, Ulyanov et al, Questo lavoro mira a far luce su questo approccio indagando le proprietà dei primi risultati del DIP. In primo luogo, dimostriamo che queste prime iterazioni dimostrano l'invarianza alle perturbazioni avversarie classificando i risultati progressivi del DIP e utilizzando un nuovo approccio della mappa di salienza. Infine, esaminiamo l'invarianza avversaria delle prime uscite DIP, e ipotizziamo che queste uscite possano rimuovere le caratteristiche non robuste dell'immagine, confrontando i valori di confidenza della classificazione, mostriamo alcune prove che confermano questa ipotesi.
In questo articolo, affrontiamo la sfida dei dati etichettati limitati e il problema dello squilibrio di classe per il rilevamento delle voci basato sull'apprendimento automatico sui social media. Presentiamo un metodo di aumento dei dati offline basato sulla correlazione semantica per il rilevamento delle voci.A tal fine, i dati non etichettati dei social media vengono sfruttati per aumentare i dati etichettati limitati. Un modello linguistico messo a punto con un grande corpus specifico del dominio mostra un miglioramento drammatico sull'aumento dei dati di formazione per il rilevamento delle voci rispetto ai modelli linguistici preaddestrati.Conduciamo esperimenti su sei diversi eventi del mondo reale basati su cinque set di dati disponibili pubblicamente e un set di dati aumentati.I nostri esperimenti dimostrano che il metodo proposto ci permette di generare dati di formazione più grandi con una qualità ragionevole attraverso una supervisione debole.Presentiamo i risultati preliminari ottenuti utilizzando un modello di rete neurale all'avanguardia con dati aumentati per il rilevamento delle voci.
L'auto-attenzione è un meccanismo utile per costruire modelli generativi per il linguaggio e le immagini.Determina l'importanza degli elementi del contesto confrontando ogni elemento con il passo temporale corrente.In questo articolo, mostriamo che una convoluzione molto leggera può eseguire in modo competitivo i migliori risultati di auto-attenzione riportati.Successivamente, introduciamo convoluzioni dinamiche che sono più semplici e più efficienti dell'auto-attenzione. Prevediamo kernel di convoluzione separati basati esclusivamente sul time-step corrente per determinare l'importanza degli elementi del contesto.Il numero di operazioni richieste da questo approccio scala linearmente nella lunghezza dell'input, mentre l'autoattenzione è quadratica.Gli esperimenti sulla traduzione automatica su larga scala, la modellazione del linguaggio e la sintesi astrattiva mostrano che le convoluzioni dinamiche migliorano rispetto ai modelli forti di autoattenzione.Sul test set inglese-tedesco WMT'14 le convoluzioni dinamiche raggiungono un nuovo stato dell'arte di 29.7 BLEU.
A causa della loro connessione con le reti generative avversarie (GAN), i problemi saddle-point hanno recentemente attirato un notevole interesse nell'apprendimento automatico e non solo.Per necessità, la maggior parte delle garanzie teoriche ruota intorno a problemi convessi-concavi (o addirittura lineari); tuttavia, fare delle incursioni teoriche verso un addestramento efficiente delle GAN dipende in modo cruciale dal muoversi oltre questo quadro classico. Per fare progressi frammentari lungo queste linee, analizziamo il comportamento della discesa a specchio (MD) in una classe di problemi non monotoni le cui soluzioni coincidono con quelle di una disuguaglianza variazionale naturalmente associata - una proprietà che chiamiamo coerenza. Mostriamo prima che la MD ordinaria, "vaniglia", converge sotto una versione rigorosa di questa condizione, ma non altrimenti; in particolare, può non convergere anche in modelli bilineari con una soluzione unica. Mostriamo poi che questa carenza è mitigata dall'ottimismo: prendendo un passo "extra-gradiente", la discesa a specchio ottimistica (OMD) converge in tutti i problemi coerenti. La nostra analisi generalizza ed estende i risultati di Daskalakis et al. [2018] per la discesa a gradiente ottimistica (OGD) nei problemi bilineari, e fa progressi concreti per la convergenza dimostrabile oltre i giochi convessi-concavi.Forniamo anche analoghi stocastici di questi risultati, e convalidiamo la nostra analisi con esperimenti numerici in una vasta gamma di modelli GAN (compresi i modelli a miscela gaussiana, e i dataset CelebA e CIFAR-10).
La normalizzazione dei lotti (BN) è spesso usata nel tentativo di stabilizzare e accelerare l'addestramento nelle reti neurali profonde; in molti casi, infatti, diminuisce il numero di aggiornamenti dei parametri necessari per ottenere un basso errore di addestramento, ma riduce anche la robustezza alle piccole perturbazioni avversarie dell'input e alle comuni corruzioni con percentuali a due cifre, come dimostriamo su cinque set di dati standard; inoltre, troviamo che sostituire il decadimento del peso con BN è sufficiente per annullare una relazione tra la vulnerabilità avversaria e la dimensione dell'input. Una recente analisi del campo medio ha trovato che BN induce un'esplosione del gradiente quando viene utilizzato su più strati, ma questo non può spiegare completamente la vulnerabilità che osserviamo, dato che si verifica già per un singolo strato BN.Noi sosteniamo che la causa effettiva è l'inclinazione del confine decisionale rispetto al classificatore nearest-centroid lungo dimensioni di input di bassa varianza. Di conseguenza, la costante introdotta per la stabilità numerica nella fase BN agisce come un importante iperparametro che può essere sintonizzato per recuperare una certa robustezza al costo dell'accuratezza del test standard.Spieghiamo questo meccanismo esplicitamente su un modello lineare di ``giocattolo e mostriamo negli esperimenti che vale ancora per modelli non lineari del ``mondo reale.
Learning to Optimize è una struttura recentemente proposta per l'apprendimento di algoritmi di ottimizzazione utilizzando il reinforcement learning.In questo articolo, esploriamo l'apprendimento di un algoritmo di ottimizzazione per l'addestramento di reti neurali poco profonde.Tali problemi di ottimizzazione stocastica ad alta densità presentano sfide interessanti per gli algoritmi di reinforcement learning esistenti. Sviluppiamo un'estensione adatta all'apprendimento di algoritmi di ottimizzazione in questo contesto e dimostriamo che l'algoritmo di ottimizzazione appreso supera costantemente altri algoritmi di ottimizzazione noti anche su compiti non visti ed è robusto ai cambiamenti nella stocasticità dei gradienti e nell'architettura della rete neurale. Più specificamente, dimostriamo che un algoritmo di ottimizzazione addestrato con il metodo proposto sul problema dell'addestramento di una rete neurale su MNIST generalizza ai problemi di addestramento delle reti neurali sul Toronto Faces Dataset, CIFAR-10 e CIFAR-100.
La dipendenza dell'errore di generalizzazione delle reti neurali dalla dimensione del modello e del dataset è di importanza critica sia nella pratica che per la comprensione della teoria delle reti neurali, La nostra costruzione segue le intuizioni ottenute dalle osservazioni condotte su una gamma di scale modello/dati, in vari tipi di modelli e insiemi di dati, in compiti di visione e di linguaggio. Mostriamo che la forma si adatta bene alle osservazioni attraverso le scale e fornisce previsioni accurate da modelli e dati su piccola a grande scala.
Imparare a controllare un ambiente senza premi artigianali o dati di esperti rimane impegnativo ed è alla frontiera della ricerca sull'apprendimento di rinforzo. Presentiamo un algoritmo di apprendimento non supervisionato per addestrare gli agenti a raggiungere obiettivi specificati percettivamente usando solo un flusso di osservazioni e azioni. Questa doppia ottimizzazione porta ad un gioco cooperativo, dando origine ad una funzione di ricompensa appresa che riflette la somiglianza negli aspetti controllabili dell'ambiente invece della distanza nello spazio delle osservazioni.Dimostriamo l'efficacia del nostro agente per imparare, in modo non supervisionato, a raggiungere un diverso insieme di obiettivi su tre domini -- Atari, la suite di controllo DeepMind e DeepMind Lab.
Lo stato dell'arte dei modelli sequence-to-sequence per compiti su larga scala esegue un numero fisso di calcoli per ogni sequenza di input, indipendentemente dal fatto che sia facile o difficile da elaborare. In questo articolo, addestriamo modelli Transformer che possono fare previsioni di output in diverse fasi della rete e studiamo diversi modi per prevedere quanti calcoli sono richiesti per una particolare sequenza. A differenza del calcolo dinamico nei trasformatori universali, che applicano lo stesso set di strati in modo iterativo, noi applichiamo diversi strati ad ogni fase per regolare sia la quantità di calcolo che la capacità del modello. Sulla traduzione IWSLT tedesco-inglese il nostro approccio corrisponde all'accuratezza di un trasformatore di base ben sintonizzato utilizzando meno di un quarto degli strati del decoder.
Gli autocodificatori variazionali (VAE) sono modelli generativi profondi di variabili latenti che consistono di due componenti: un modello generativo che cattura una distribuzione di dati p(x) trasformando una distribuzione p(z) sullo spazio latente, e un modello di inferenza che infonde probabili codici latenti per ogni punto di dati (Kingma e Welling, 2013): (1) il modello generativo appreso cattura la distribuzione dei dati osservati, ma lo fa ignorando i codici latenti, risultando in codici che non rappresentano i dati (ad esempio, van den Oord et al. (2017); Kim et al. (2018)); (2) l'aggregato dei codici latenti appresi non corrisponde alla priorità p(z). Questo mismatch significa che il modello generativo appreso non sarà in grado di generare dati realistici con campioni da p(z)(ad esempio Makhzani et al. (2015); Tomczak e Welling (2017)).In questo articolo, dimostriamo che entrambi i problemi derivano dal fatto che gli ottimali globali dell'obiettivo di formazione VAE spesso corrispondono a soluzioni indesiderate.La nostra analisi si basa su due osservazioni: (1) il modello generativo non è identificabile - esistono molti modelli generativi che spiegano ugualmente bene i dati, ognuno con proprietà diverse (e potenzialmente indesiderate) e (2) bias nell'obiettivo VAE - l'obiettivo VAE può preferire modelli generativi che spiegano male i dati ma hanno posteriori facili da approssimare. Presentiamo un nuovo metodo di inferenza, LiBI, che mitiga i problemi identificati nella nostra analisi. Su set di dati sintetici, mostriamo che LiBI può imparare modelli generativi che catturano la distribuzione dei dati e modelli di inferenza che soddisfano meglio le ipotesi di modellazione quando i metodi tradizionali lottano per farlo.
L'apprendimento supervisionato da fonti di ipernimi etichettati, come WordNet, limita la copertura di questi modelli, che può essere affrontata con l'apprendimento di ipernimi da testo non etichettato.  I metodi non supervisionati esistenti non si adattano a grandi vocabolari o producono un'accuratezza inaccettabilmente scarsa.  Questo articolo introduce un metodo semplice da implementare per la scoperta di ipernimi tramite embedding vettoriali non negativi per parola che preservano la proprietà di inclusione dei contesti delle parole. In valutazioni sperimentali più complete di qualsiasi letteratura precedente di cui siamo a conoscenza - valutando su 11 set di dati utilizzando più funzioni di punteggio esistenti e di nuova proposta - troviamo che il nostro metodo fornisce fino al doppio della precisione dei metodi non supervisionati precedenti, e la più alta performance media, utilizzando una rappresentazione delle parole molto più compatta, e producendo molti nuovi risultati allo stato dell'arte.Inoltre, il significato di ogni dimensione in DIVE è interpretabile, il che porta a un nuovo approccio sulla disambiguazione del senso delle parole come un'altra promettente applicazione di DIVE.
L'apprendimento continuo mira ad apprendere nuovi compiti senza dimenticare quelli appresi in precedenza, il che è particolarmente impegnativo quando non si può accedere ai dati dei compiti precedenti e quando il modello ha una capacità fissa. Gli attuali algoritmi di apprendimento continuo basati sulla regolarizzazione hanno bisogno di una rappresentazione esterna e di calcoli extra per misurare l'importanza dei parametri.Al contrario, noi proponiamo le Reti Neurali Bayesiane Continue Guidate dall'Incertezza (UCB) dove il tasso di apprendimento si adatta secondo l'incertezza definita nella distribuzione di probabilità dei pesi nelle reti. L'incertezza è un modo naturale per identificare \textit{cosa ricordare} e \textit{ cosa cambiare} mentre impariamo continuamente, e quindi mitigare la dimenticanza catastrofica. Mostriamo anche una variante del nostro modello, che usa l'incertezza per la potatura dei pesi e mantiene le prestazioni dei compiti dopo la potatura salvando le maschere binarie per i compiti. Valutiamo il nostro approccio UCB ampiamente su diversi set di dati di classificazione di oggetti con sequenze brevi e lunghe di compiti e riportiamo prestazioni superiori o alla pari rispetto agli approcci esistenti. Inoltre, mostriamo che il nostro modello non ha necessariamente bisogno di informazioni sui compiti al momento del test, cioè non presume la conoscenza di quale compito un campione appartiene.
Gli esseri umani hanno una curiositÃ naturale di immaginare come ci si sente ad esistere come qualcuno o qualcosa di diverso.Questa curiositÃ diventa ancora piÃ¹ forte per gli animali domestici di cui ci prendiamo cura.Gli esseri umani non possono veramente sapere che cosa Ã¨ come essere i nostri animali domestici, ma possiamo approfondire la nostra comprensione di che cosa Ã¨ come percepire ed esplorare il mondo come loro.Indaghiamo come i wearables possono offrire alle persone opportunitÃ di prospettiva animale di sperimentare il mondo attraverso i sensi animali che differiscono da quelli biologicamente naturali per noi. Abbiamo creato un'esperienza di esplorazione del labirinto in cui i partecipanti bendati hanno utilizzato i baffi per navigare nel labirinto, attingendo alla ricerca sul comportamento animale per valutare come l'attività dei baffi abbia supportato esperienze autenticamente feline, e discutiamo le implicazioni di questo lavoro per future esperienze di apprendimento.
L'errore di generalizzazione (noto anche come errore fuori campione) misura quanto bene l'ipotesi appresa dai dati di addestramento si generalizza ai dati precedentemente non visti.   In questo articolo, otteniamo i limiti dell'errore di generalizzazione per l'apprendimento di obiettivi generali non convessi, che ha attirato un'attenzione significativa negli ultimi anni.   Sviluppiamo una nuova struttura, chiamata Bayes-Stability, per dimostrare i limiti di errore di generalizzazione dipendenti dall'algoritmo.  La nuova struttura combina idee dalla teoria PAC-Bayesiana e dalla nozione di stabilità algoritmica.  Applicando il metodo Bayes-Stability, otteniamo nuovi limiti di generalizzazione dipendenti dai dati per la dinamica di Langevin a gradiente stocastico (SGLD) e diversi altri metodi a gradiente rumoroso (ad esempio, con slancio, mini-batch e accelerazione, Entropia-SGD). Il nostro risultato recupera (ed è tipicamente più stretto di) un recente risultato in Mou et al. (2018) e migliora i risultati in Pensia et al. (2018).  I nostri esperimenti dimostrano che i nostri limiti dipendenti dai dati possono distinguere i dati etichettati in modo casuale dai dati normali, il che fornisce una spiegazione agli intriganti fenomeni osservati in Zhang et al. (2017a).Studiamo anche l'impostazione in cui la perdita totale è la somma di una perdita limitata e un termine di regolarizzazione additiona l`2. Otteniamo nuovi limiti di generalizzazione per la dinamica continua di Langevin in questa impostazione sviluppando una nuova disuguaglianza Log-Sobolev per la distribuzione dei parametri in qualsiasi momento.I nostri nuovi limiti sono più desiderabili quando il livello di rumore del processo non è molto piccolo, e non diventano vacui anche quando T tende all'infinito.
In questo articolo, viene proposto un nuovo metodo di generazione di ricompensa intrinseca per l'apprendimento di rinforzo a premi sparsi basato su un insieme di modelli dinamici. Nel metodo proposto, la miscela di modelli dinamici multipli viene utilizzata per approssimare la vera probabilità di transizione sconosciuta, e la ricompensa intrinseca viene progettata come il minimo della sorpresa vista da ogni modello dinamico alla miscela dei modelli dinamici. Per dimostrare l'efficacia del metodo di generazione di ricompensa intrinseca proposto, viene costruito un algoritmo di lavoro combinando il metodo di generazione di ricompensa intrinseca proposto con l'algoritmo di ottimizzazione della politica prossimale (PPO). I risultati numerici mostrano che per compiti di locomozione rappresentativi, il metodo di generazione di ricompensa intrinseca basato sull'insieme di modelli supera i metodi precedenti basati su un singolo modello dinamico.
Dato il rapido sviluppo delle tecniche di analisi per i sistemi NLP e di elaborazione del discorso, sono stati condotti pochi studi sistematici per confrontare i punti di forza e di debolezza di ciascun metodo. Usiamo due tecniche analitiche comunemente applicate, i classificatori diagnostici e l'analisi della similarità di rappresentazione, per quantificare in che misura i modelli di attivazione neurale codificano fonemi e sequenze di fonemi. manipoliamo due fattori che possono influenzare il risultato dell'analisi. In secondo luogo, esaminiamo la portata temporale delle attivazioni sondando sia le attivazioni locali che corrispondono a pochi millisecondi del segnale vocale, sia le attivazioni globali raggruppate nell'intera enunciazione. Concludiamo che riportare i risultati dell'analisi con modelli inizializzati a caso è cruciale, e che i metodi di portata globale tendono a dare risultati più coerenti e interpretabili e ne raccomandiamo l'uso come complemento ai metodi diagnostici di portata locale.
La super risoluzione (SR) è un compito fondamentale e importante della computer vision (CV) di basso livello. A differenza dei modelli SR tradizionali, questo studio si concentra su un problema SR specifico ma realistico: Come possiamo ottenere risultati soddisfacenti di SR dall'immagine compressa JPG (C-JPG), che esiste ampiamente su Internet.In generale, C-JPG può liberare spazio di archiviazione mantenendo una notevole qualità in visual.However, ulteriori operazioni di elaborazione delle immagini, ad es, SR, soffrirà di ingrandire i dettagli artificiali interni e risulterà in output inaccettabili.Per affrontare questo problema, proponiamo una nuova struttura SR con due componenti specificamente progettati, così come una perdita di ciclo.In breve, ci sono principalmente tre contributi a questo documento.Primo, la nostra ricerca può generare immagini SR altamente qualificate per le immagini C-JPG prevalenti. In secondo luogo, proponiamo un sottomodello funzionale per recuperare le informazioni per le immagini C-JPG, invece della prospettiva di eliminazione del rumore negli approcci SR tradizionali.In terzo luogo, integriamo ulteriormente la perdita di ciclo nel solutore SR per costruire una funzione di perdita ibrida per una migliore generazione SR.Gli esperimenti mostrano che il nostro approccio raggiunge prestazioni eccezionali tra i metodi all'avanguardia.
L'individuazione delle parole chiave - o il rilevamento delle parole chiave - è una caratteristica essenziale per il funzionamento a mani libere dei moderni dispositivi a controllo vocale. Con tali dispositivi che diventano onnipresenti, gli utenti potrebbero voler scegliere una parola chiave personalizzata. L'algoritmo funziona registrando un piccolo numero di esempi di formazione da parte dell'utente, generando un insieme di ipotesi di sequenza di etichette da questi esempi di formazione, e rilevando la parola d'ordine aggregando i punteggi di tutte le ipotesi data una nuova registrazione audio.Il nostro metodo combina la generalizzazione e l'interpretabilità di CTC-based keyword spotting con l'adattamento all'utente e la convenienza di un sistema convenzionale query-by-example.DONUT ha bassi requisiti computazionali ed è adatto sia per l'apprendimento e l'inferenza su sistemi embedded senza richiedere dati utente privati da caricare nel cloud.
Per ragionare in modo flessibile ed efficiente sulle sequenze temporali, sono necessarie rappresentazioni astratte che rappresentino in modo compatto le informazioni importanti nella sequenza.Un modo per costruire tali rappresentazioni è quello di concentrarsi sugli eventi importanti in una sequenza.In questo articolo, proponiamo un modello che impara sia a scoprire tali eventi chiave (o fotogrammi chiave) sia a rappresentare la sequenza in termini di essi.  Lo facciamo usando un modello gerarchico Keyframe-Inpainter (KeyIn) che prima genera i fotogrammi chiave e il loro posizionamento temporale e poi dipinge le sequenze tra i fotogrammi chiave.Proponiamo una formulazione completamente differenziabile per imparare in modo efficiente il posizionamento dei fotogrammi chiave.Mostriamo che KeyIn trova fotogrammi chiave informativi in diversi set di dati con diverse dinamiche.Quando viene valutato su un compito di pianificazione, KeyIn supera altre recenti proposte per imparare rappresentazioni gerarchiche.
Questo lavoro studia l'apprendimento non supervisionato di rappresentazioni massimizzando l'informazione reciproca tra un input e l'output di un codificatore di una rete neurale profonda; in particolare, dimostriamo che la struttura è importante: incorporare nell'obiettivo la conoscenza della località nell'input può migliorare significativamente l'idoneità di una rappresentazione per i compiti a valle. Il nostro metodo, che chiamiamo Deep InfoMax (DIM), supera un certo numero di metodi popolari di apprendimento non supervisionato e si confronta favorevolmente con l'apprendimento completamente supervisionato su diversi compiti di classificazione con alcune architetture standard.DIM apre nuove strade per l'apprendimento non supervisionato delle rappresentazioni ed è un passo importante verso formulazioni flessibili di obiettivi di apprendimento della rappresentazione per obiettivi finali specifici.
La stima dell'importanza di ogni atomo in una molecola è uno dei problemi più affascinanti e impegnativi in chimica, fisica e ingegneria dei materiali. Il modo più comune per stimare l'importanza atomica è quello di calcolare la struttura elettronica utilizzando la teoria della densità funzionale (DFT), e poi di interpretarla utilizzando la conoscenza del dominio di esperti umani.Tuttavia, questo approccio convenzionale è impraticabile per il grande database molecolare perché il calcolo DFT richiede un calcolo enorme, in particolare, O(n^4) complessità temporale con il numero di elettroni in una molecola. Inoltre, i risultati del calcolo devono essere interpretati dagli esperti umani per stimare l'importanza atomica in termini di proprietà molecolare di destinazione.Per affrontare questo problema, in primo luogo sfruttiamo l'approccio basato sull'apprendimento automatico per la stima dell'importanza atomica.A tal fine, proponiamo l'autoattenzione inversa sulle reti neurali del grafico e la integriamo con la descrizione molecolare basata sul grafico.Il nostro metodo fornisce un modo efficiente-automatizzato e diretto all'obiettivo di stimare l'importanza atomica senza alcuna conoscenza di dominio sulla chimica e la fisica.
Il clustering spettrale è una tecnica leader e popolare nell'analisi dei dati non supervisionata.  Due delle sue principali limitazioni sono la scalabilità e la generalizzazione dell'incorporazione spettrale (cioè La nostra rete, che chiamiamo SpectralNet, apprende una mappa che incorpora i punti di dati di input nell'eigenspace della loro matrice associata del grafico Laplaciano e successivamente li raggruppa.addestriamo SpectralNet usando una procedura che coinvolge l'ottimizzazione stocastica vincolata. L'ottimizzazione stocastica permette di scalare a grandi insiemi di dati, mentre i vincoli, che sono implementati utilizzando uno strato di uscita speciale, ci permettono di mantenere l'uscita della rete ortogonale.Inoltre, la mappa appresa da SpectralNet generalizza naturalmente l'incorporazione spettrale ai punti di dati non visti.Per migliorare ulteriormente la qualità del clustering, sostituiamo le affinità standard Gaussiane a coppie con affinità apprese da dati senza etichetta utilizzando una rete siamese.  Un ulteriore miglioramento può essere ottenuto applicando la rete a rappresentazioni di codice prodotte, ad esempio, da autoencoder standard.La nostra procedura di apprendimento end-to-end è completamente non supervisionata.Inoltre, applichiamo la teoria della dimensione VC per ricavare un limite inferiore alla dimensione di SpectralNet.  I risultati di clustering allo stato dell'arte sono riportati per entrambi i dataset MNIST e Reuters.
I metodi di meta-apprendimento, in particolare il Model-Agnostic Meta-Learning (Finn et al, 2017) o MAML, hanno ottenuto un grande successo nell'adattarsi rapidamente a nuovi compiti, dopo essere stati addestrati su compiti simili.Il meccanismo dietro il loro successo, tuttavia, è poco compreso.Iniziamo questo lavoro con un'analisi sperimentale di MAML, scoprendo che i modelli profondi sono cruciali per il suo successo, anche dati set di compiti semplici dove un modello lineare sarebbe sufficiente su ogni singolo compito. Inoltre, nei compiti di riconoscimento delle immagini, troviamo che i primi strati dei modelli addestrati con MAML imparano caratteristiche invarianti rispetto al compito, mentre gli strati successivi sono utilizzati per l'adattamento, fornendo ulteriori prove che questi modelli richiedono una capacità maggiore di quella strettamente necessaria per i loro singoli compiti. A seguito delle nostre scoperte, proponiamo un metodo che consente un migliore utilizzo della capacità del modello al momento dell'inferenza, separando l'aspetto di adattamento del meta-apprendimento in parametri che sono utilizzati solo per l'adattamento ma non fanno parte del modello in avanti.
Le reti neurali convoluzionali fanno continuamente progredire il progresso della classificazione delle immagini e degli oggetti 2D e 3D.L'uso costante di questo algoritmo richiede una valutazione costante e l'aggiornamento dei concetti fondamentali per mantenere il progresso.Le tecniche di regolarizzazione della rete si concentrano tipicamente sulle operazioni degli strati convoluzionali, mentre lasciano le operazioni degli strati di pooling senza opzioni adeguate.Introduciamo Wavelet Pooling come un'altra alternativa al pooling tradizionale del quartiere. Questo metodo decompone le caratteristiche in una decomposizione di secondo livello, e scarta le sottobande di primo livello per ridurre le dimensioni della caratteristica.Questo metodo affronta il problema di overfitting incontrato da max pooling, mentre riduce le caratteristiche in un modo più strutturalmente compatto rispetto al pooling tramite regioni di quartiere.risultati sperimentali su quattro set di dati di classificazione di riferimento dimostrano il nostro metodo proposto supera o esegue comparativamente con metodi come max, media, misto, e stocastico pooling.
I servizi di ridesharing dinamico (DRS) giocano un ruolo importante nel migliorare l'efficienza del trasporto urbano. La soddisfazione dell'utente nel ridesharing dinamico è determinata da più fattori come il tempo di viaggio, il costo e la compatibilità sociale con i co-passeggeri. I DRS esistenti ottimizzano il profitto massimizzando il valore operativo per i fornitori di servizi o minimizzano il tempo di viaggio per gli utenti, ma trascurano l'esperienza sociale dei passeggeri, che influenza significativamente il valore totale del servizio per gli utenti. Noi proponiamo DROPS, una struttura di ridesharing dinamico che fattorizza le preferenze sociali dei passeggeri nel processo di corrispondenza in modo da migliorare la qualità dei viaggi formati. La programmazione dei viaggi per gli utenti è un'ottimizzazione multi-obiettivo che mira a massimizzare il valore operativo per il fornitore del servizio, mentre simultaneamente massimizza il valore del viaggio per gli utenti. Il valore dell'utente è stimato sulla base della compatibilità tra i co-passeggeri e il tempo di viaggio. Presentiamo poi un algoritmo di corrispondenza in tempo reale per la formazione del viaggio.
Le reti neurali profonde (DNN) hanno recentemente dimostrato di essere vulnerabili agli esempi avversari, che sono istanze accuratamente create che possono indurre le DNN a commettere errori durante la predizione.Per comprendere meglio tali attacchi, è necessaria una caratterizzazione delle proprietà delle regioni (i cosiddetti "sottospazi avversari") in cui si trovano gli esempi avversari. Affrontiamo questa sfida caratterizzando le proprietà dimensionali delle regioni avversarie, attraverso l'uso della dimensionalità intrinseca locale (LID).LID valuta la capacità di riempire lo spazio della regione che circonda un esempio di riferimento, sulla base della distribuzione della distanza dell'esempio dai suoi vicini. In primo luogo forniamo spiegazioni su come la perturbazione avversaria può influenzare la caratteristica LID delle regioni avversarie, e poi dimostriamo empiricamente che le caratteristiche LID possono facilitare la distinzione degli esempi avversari generati utilizzando attacchi allo stato dell'arte. Come proof-of-concept, mostriamo che una potenziale applicazione di LID è quella di distinguere gli esempi avversari, e i risultati preliminari mostrano che può superare diverse misure di rilevamento allo stato dell'arte con ampi margini per cinque strategie di attacco considerate in questo documento su tre set di dati di riferimento.La nostra analisi della caratteristica LID per le regioni avversarie non solo motiva nuove direzioni di difesa avversaria efficace, ma apre anche più sfide per sviluppare nuovi attacchi per comprendere meglio le vulnerabilità di DNNs.
In questo articolo, progettiamo e analizziamo un nuovo algoritmo di ottimizzazione stocastica di ordine zerotico (ZO), ZO-signSGD, che gode dei doppi vantaggi delle operazioni senza gradiente e di signSGD, che richiede solo le informazioni di segno delle stime del gradiente, ma è in grado di raggiungere una velocità di convergenza comparabile o addirittura migliore degli algoritmi di tipo SGD. Il nostro studio mostra che ZO signSGD richiede $sqrt{d}$ volte più iterazioni di signSGD, portando ad un tasso di convergenza di $O(\sqrt{d}/\sqrt{T})$ in condizioni blande, dove $d$ è il numero di variabili di ottimizzazione, e $T$ è il numero di iterazioni. Inoltre, analizziamo gli effetti di diversi tipi di stimatori del gradiente sulla convergenza di ZO-signSGD, e proponiamo due varianti di ZO-signSGD che raggiungono almeno $O(\sqrt{d}/\sqrt{T})$ di convergenza.  Le nostre valutazioni empiriche sui dataset di classificazione delle immagini MNIST e CIFAR-10 dimostrano le prestazioni superiori di ZO-signSGD sulla generazione di esempi avversari dalle reti neurali black-box.
La caratteristica di non stazionarietà dell'energia solare rende i metodi tradizionali di previsione puntuale meno utili a causa dei grandi errori di previsione, il che si traduce in un aumento delle incertezze nel funzionamento della rete, influenzando così negativamente l'affidabilità e provocando un aumento dei costi di funzionamento. I risultati dimostrano che il metodo proposto basato sull'architettura unificata è efficace per la previsione solare multiorizzonte e raggiunge un errore di previsione radice-media quadrata inferiore rispetto ai metodi precedenti che utilizzano un modello per ogni orizzonte temporale. Il metodo proposto consente previsioni multiorizzonte con input in tempo reale, che hanno un alto potenziale per applicazioni pratiche nella smart grid in evoluzione.
La ResNet e la batch-normalization (BN) hanno raggiunto alte prestazioni anche quando sono disponibili solo pochi dati etichettati.Tuttavia, le ragioni per le sue alte prestazioni non sono chiare.Per chiarire le ragioni, abbiamo analizzato l'effetto dello skip-connection in ResNet e la BN sulla capacità di separazione dei dati, che è una capacità importante per il problema di classificazione. I nostri risultati mostrano che, nel perceptron multistrato con pesi inizializzati in modo casuale, l'angolo tra due vettori di input converge a zero in un ordine esponenziale della sua profondità, che il collegamento saltato rende questa diminuzione esponenziale in una diminuzione sub-esponenziale, e che il BN rilassa questa diminuzione sub-esponenziale in una diminuzione reciproca. Inoltre, la nostra analisi mostra che la conservazione dell'angolo all'inizializzazione incoraggia le reti neurali addestrate a separare i punti da classi diverse, il che implica che il collegamento a skip e il BN migliorano la capacità di separazione dei dati e raggiungono alte prestazioni anche quando sono disponibili solo pochi dati etichettati.
L'apprendimento delle rappresentazioni dei dati è una questione importante nell'apprendimento automatico.Anche se GAN ha portato a miglioramenti significativi nelle rappresentazioni dei dati, ha ancora diversi problemi come l'addestramento instabile, il collettore nascosto dei dati, e l'enorme overhead computazionale.GAN tende a produrre i dati semplicemente senza alcuna informazione sul collettore dei dati, che ostacola dal controllo delle caratteristiche desiderate da generare.Inoltre, la maggior parte di GAN ha una grande dimensione del collettore, con conseguente scarsa scalabilità. In questo articolo, proponiamo un nuovo GAN per controllare la rappresentazione semantica latente, chiamato LSC-GAN, che ci permette di produrre i dati desiderati da generare e impara una rappresentazione dei dati in modo efficiente.A differenza dei modelli GAN convenzionali con distribuzione nascosta dello spazio latente, noi definiamo le distribuzioni esplicitamente in anticipo che vengono addestrate per generare i dati in base alle caratteristiche corrispondenti inserendo le variabili latenti che seguono la distribuzione. Poiché la scala più grande dello spazio latente causata dall'impiego di varie distribuzioni in uno spazio latente rende l'addestramento instabile pur mantenendo la dimensione dello spazio latente, abbiamo bisogno di separare il processo di definizione delle distribuzioni esplicitamente e l'operazione di generazione.Proviamo che una VAE è appropriata per la prima e modifichiamo una funzione di perdita di VAE per mappare i dati nello spazio latente predefinito in modo da individuare i dati ricostruiti il più vicino ai dati di input secondo le sue caratteristiche.Inoltre, aggiungiamo la divergenza KL alla funzione di perdita di LSC-GAN per includere questo processo. Il decodificatore di VAE, che genera i dati con le caratteristiche corrispondenti dallo spazio latente predefinito, viene utilizzato come generatore di LSC-GAN.Diversi esperimenti sul set di dati CelebA sono condotti per verificare l'utilità del metodo proposto per generare i dati desiderati in modo stabile ed efficiente, raggiungendo un alto rapporto di compressione che può contenere circa 24 pixel di informazioni in ogni dimensione dello spazio latente.Inoltre, il nostro modello impara l'inverso delle caratteristiche come non ridere (piuttosto accigliarsi) solo con dati di espressione facciale ordinaria e sorridente.
Con la domanda sempre crescente e la conseguente riduzione della qualità dei servizi, l'attenzione si è spostata verso l'alleggerimento della congestione della rete per consentire un flusso più efficiente in sistemi come il traffico, le catene di approvvigionamento e le reti elettriche.Un passo in questa direzione è quello di re-immaginare l'addestramento tradizionale basato sull'euristica dei sistemi in quanto questo approccio è incapace di modellare le dinamiche coinvolte. Mentre si può applicare il Multi-Agent Reinforcement Learning (MARL) a tali problemi considerando ogni vertice della rete come un agente, la maggior parte dei modelli basati su MARL assumono che gli agenti siano indipendenti.In molti compiti del mondo reale, gli agenti devono comportarsi come un gruppo, piuttosto che come un insieme di individui. In questo articolo, proponiamo una struttura che induce la cooperazione e la coordinazione tra gli agenti, collegati tramite una rete sottostante, usando la comunicazione emergente in una configurazione basata su MARL.Formuliamo il problema in un'impostazione generale della rete e dimostriamo l'utilità della comunicazione nelle reti con l'aiuto di un caso di studio sui sistemi di traffico.Inoltre, studiamo il protocollo di comunicazione emergente e mostriamo la formazione di comunità distinte con un vocabolario fondato.Per quanto ne sappiamo, questo è l'unico lavoro che studia il linguaggio emergente in una impostazione MARL in rete.
Introduciamo il Convolutional Conditional Neural Process (ConvCNP), un nuovo membro della famiglia dei processi neurali che modella l'equivarianza di traduzione nei dati. L'equivarianza di traduzione è un importante bias induttivo per molti problemi di apprendimento, tra cui la modellazione delle serie temporali, i dati spaziali e le immagini.Il modello incorpora i set di dati in uno spazio di funzioni infinito-dimensionale, al contrario degli spazi vettoriali finito-dimensionali. Per formalizzare questa nozione, estendiamo la teoria delle rappresentazioni neurali degli insiemi per includere rappresentazioni funzionali, e dimostriamo che qualsiasi incorporazione translation-equivariant può essere rappresentata utilizzando un deep-set convoluzionale.Valutiamo ConvCNPs in diverse impostazioni, dimostrando che raggiungono lo stato dell'arte delle prestazioni rispetto ai NPs esistenti.Dimostriamo che costruire in translation equivariance permette la generalizzazione zero-shot a sfidanti, compiti fuori dal dominio.
I modelli classici descrivono la corteccia visiva primaria (V1) come un banco di filtri di modelli lineari-non lineari (LN) o energetici selettivi per l'orientamento, ma questi modelli non riescono a predire accuratamente le risposte neurali agli stimoli naturali. Un lavoro recente mostra che le reti neurali convoluzionali (CNN) possono essere addestrate a predire l'attività della V1 in modo più accurato, ma non è ancora chiaro quali caratteristiche siano estratte dai neuroni della V1 oltre la selettività di orientamento e l'invarianza di fase. Presentiamo una struttura per identificare le caratteristiche comuni indipendenti dalla selettività di orientamento dei singoli neuroni, utilizzando una rete neurale convoluzionaria con equivarianza di rotazione, che estrae automaticamente ogni caratteristica a più orientamenti diversi.Abbiamo adattato questa CNN con equivarianza di rotazione alle risposte di una popolazione di 6000 neuroni a immagini naturali registrate nella corteccia visiva primaria del topo utilizzando l'imaging a due fotoni. Mostriamo che la nostra rete rotazionale-equivariante supera una CNN regolare con lo stesso numero di mappe di caratteristiche e rivela un certo numero di caratteristiche comuni, che sono condivise da molti neuroni V1 e sono raggruppate sparsamente per prevedere l'attività neurale.I nostri risultati sono un primo passo verso un nuovo strumento potente per studiare l'organizzazione funzionale non lineare della corteccia visiva.
In articoli classici, Zellner (1988, 2002) ha dimostrato che l'inferenza bayesiana potrebbe essere derivata come la soluzione di una teoria dell'informazione funzionale.  Di seguito deriviamo una forma generalizzata di questo funzionale come un limite inferiore variazionale di un obiettivo predittivo del collo di bottiglia dell'informazione.  Questo funzionale generalizzato comprende la maggior parte delle moderne procedure di inferenza e ne suggerisce di nuove.
In molte applicazioni, è auspicabile estrarre solo le informazioni rilevanti da dati di input complessi, il che implica prendere una decisione su quali caratteristiche di input sono rilevanti. Il metodo del collo di bottiglia dell'informazione formalizza questo come un problema di ottimizzazione teorica dell'informazione mantenendo un compromesso ottimale tra la compressione (gettando via le informazioni di input irrilevanti) e la previsione dell'obiettivo. In molti contesti problematici, compresi i problemi di apprendimento per rinforzo che consideriamo in questo lavoro, potremmo preferire comprimere solo una parte dell'input. Questo è tipicamente il caso quando abbiamo un input condizionante standard, come un'osservazione di stato, e un input ``privilegiato'', che potrebbe corrispondere all'obiettivo di un compito, l'output di un algoritmo di pianificazione costoso, o la comunicazione con un altro agente.In questi casi, potremmo preferire comprimere l'input privilegiato, sia per ottenere una migliore generalizzazione (ad es, Le implementazioni pratiche del collo di bottiglia dell'informazione basate sull'inferenza variazionale richiedono l'accesso all'input privilegiato per calcolare la variabile del collo di bottiglia, quindi anche se eseguono la compressione, questa stessa operazione di compressione ha bisogno di un accesso illimitato e senza perdite.In questo lavoro, proponiamo il collo di bottiglia variazionale della larghezza di banda, che decide per ogni esempio sul valore stimato dell'informazione privilegiata prima di vederla, cioè, Formuliamo un'approssimazione trattabile a questo quadro e dimostriamo in una serie di esperimenti di apprendimento per rinforzo che può migliorare la generalizzazione e ridurre l'accesso alle informazioni computazionalmente costose.
Gli esercizi di respirazione sono un modo accessibile per gestire lo stress e molti sintomi delle malattie mentali.Tradizionalmente, l'apprendimento degli esercizi di respirazione ha coinvolto una guida in persona o registrazioni audio.Il passaggio ai dispositivi mobili ha portato a un nuovo modo di imparare e impegnarsi in esercizi di respirazione come si è visto nella crescita di più applicazioni mobili con diverse rappresentazioni di respirazione.Tuttavia, un lavoro limitato è stato fatto per indagare l'efficacia di queste rappresentazioni visive nel sostenere il ritmo di respirazione come misurato dalla sincronizzazione. Attraverso studi di laboratorio controllati e interviste, abbiamo identificato due rappresentazioni con chiari vantaggi rispetto alle altre. Inoltre, abbiamo scoperto che la guida uditiva non era preferita da tutti gli utenti. Identifichiamo potenziali problemi di usabilità con le rappresentazioni e suggeriamo linee guida di progettazione per lo sviluppo futuro dell'allenamento respiratorio supportato dalle app.
L'attuale lavoro sulla sintesi del codice neurale consiste in architetture sempre più sofisticate che vengono addestrate su linguaggi specifici di dominio altamente semplificati, utilizzando un campionamento uniforme attraverso lo spazio di programma di quei linguaggi per l'addestramento.In confronto, lo spazio di programma per un linguaggio simile al C è vasto ed estremamente scarsamente popolato in termini di funzionalità "utili"; questo richiede un approccio molto più intelligente alla generazione del corpus per un addestramento efficace. Usiamo un approccio di programmazione genetica utilizzando un discriminatore iterativamente riqualificato per produrre una popolazione adatta come dati di formazione etichettati per un'architettura di sintesi del codice neurale. Dimostriamo che l'uso di un generatore di corpus di formazione basato su un discriminatore, addestrato usando solo specifiche di problemi non etichettati nel classico formato Programming-by-Example, migliora notevolmente le prestazioni della rete rispetto alle attuali tecniche di campionamento uniforme.
La maggior parte dei lavori precedenti ha studiato le perturbazioni del testo semanticamente invarianti che causano il cambiamento della predizione del modello quando non dovrebbe. In questo lavoro ci concentriamo sul problema complementare: eccessiva sottosensibilità della predizione in cui il testo di input è significativamente cambiato, e la predizione del modello non cambia quando dovrebbe. Formuliamo un attacco avversario rumoroso che cerca tra le variazioni semantiche delle domande di comprensione per le quali un modello continua a fornire erroneamente la stessa risposta della domanda originale - e con una probabilità ancora più alta.Mostriamo che - nonostante comprendano domande senza risposta - i modelli SQuAD2. 0 e NewsQA sono vulnerabili a questo attacco e commettono una frazione sostanziale di errori su domande generate dall'avversario. Questo indica che i modelli attuali - anche quando possono predire correttamente la risposta - si basano su modelli spuri e non sono necessariamente consapevoli di tutte le informazioni fornite in una data domanda di comprensione. Sviluppando ulteriormente questo aspetto, sperimentiamo sia l'aumento dei dati che l'addestramento avversario come strategie di difesa: entrambi sono in grado di diminuire sub-stanzialmente la vulnerabilità di un modello agli attacchi di sottosensibilità sui dati di valutazione trattenuti.Infine, dimostriamo che i modelli adversarialmente robusti generano meglio in un contesto di dati distorti con un disadattamento della distribuzione treno/valutazione; sono meno inclini a fare eccessivo affidamento su spunti predittivi presenti solo nel set di allenamento e superano un modello tradizionale nel contesto dei dati distorti fino all'11% F1.
Questo articolo propone una nuova rappresentazione tensoriale del testo che si basa su tecniche di compressione dell'informazione per assegnare codici più brevi ai caratteri più frequentemente usati; questa rappresentazione è indipendente dalla lingua e non ha bisogno di preallenamento e produce una codifica senza perdita di informazione; fornisce un'adeguata descrizione della morfologia del testo, poiché è in grado di rappresentare prefissi, declinazioni e inflessioni con vettori simili e sono in grado di rappresentare anche parole non viste nel set di dati di allenamento. Allo stesso modo, essendo compatto ma rado, è ideale per accelerare i tempi di addestramento utilizzando librerie di elaborazione tensori.Come parte di questo articolo, mostriamo che questa tecnica è particolarmente efficace quando accoppiata con reti neurali convoluzionali (CNN) per la classificazione del testo a livello di carattere.Applichiamo due varianti di CNN accoppiate con esso.I risultati sperimentali mostrano che riduce drasticamente il numero di parametri da ottimizzare, con conseguente valori di precisione di classificazione competitivi in solo una frazione del tempo speso da rappresentazioni di codifica one-hot, permettendo così la formazione in hardware commodity.
I modelli di predizione delle sequenze possono essere appresi da sequenze di esempio con una varietà di algoritmi di addestramento: l'apprendimento della massima verosimiglianza è semplice ed efficiente, ma può soffrire di un errore composto al momento del test. L'apprendimento di rinforzo come il gradiente di politica affronta il problema ma può avere un'efficienza di esplorazione proibitiva.Un ricco insieme di altri algoritmi, come il data noising, RAML, e il gradiente di politica softmax, sono stati sviluppati anche da prospettive diverse. In questo articolo, presentiamo un formalismo di ottimizzazione della politica regolarizzata dall'entropia, e mostriamo che gli algoritmi apparentemente distinti, incluso MLE, possono essere riformulati come istanze speciali della formulazione.La differenza tra loro è caratterizzata dalla funzione di ricompensa e due iperparametri di peso. L'interpretazione unificante ci permette di confrontare sistematicamente gli algoritmi fianco a fianco, e di ottenere nuove intuizioni sui compromessi della progettazione dell'algoritmo.La nuova prospettiva porta anche a un approccio migliorato che interpola dinamicamente tra la famiglia di algoritmi, e apprende il modello in modo programmato.Gli esperimenti sulla traduzione automatica, la sintesi del testo, e l'apprendimento dell'imitazione del gioco dimostrano la superiorità dell'approccio proposto.
I dati di flusso Origine-Destinazione (OD) sono uno strumento importante negli studi sui trasporti.La previsione precisa delle richieste dei clienti da ogni posizione originale a una destinazione data una serie di istantanee precedenti aiuta le piattaforme di ride-sharing a comprendere meglio il loro meccanismo di mercato.Tuttavia, la maggior parte dei metodi di previsione esistenti ignorano la struttura di rete dei dati di flusso OD e non riescono a utilizzare le dipendenze topologiche tra le coppie OD correlate. In questo documento, proponiamo un modello latente origine-destinazione spazio-temporale (LSTOD), con un nuovo filtro di rete neurale convoluzionale (CNN) per imparare le caratteristiche spaziali delle coppie OD da una prospettiva di grafico e una struttura di attenzione per catturare la loro periodicità a lungo termine. Gli esperimenti su un set di dati reali di richiesta del cliente con informazioni OD disponibili da una piattaforma di ride-sharing dimostrano il vantaggio di LSTOD nel raggiungere almeno il 6,5% di miglioramento nella precisione di previsione rispetto al secondo miglior modello.
L'addestramento avversario è stato dimostrato come uno dei metodi più efficaci per l'addestramento di modelli robusti per difendersi da esempi avversari.Tuttavia, i modelli addestrati avversariamente spesso mancano di generalizzazione avversariamente robusta su dati di test non visti.Lavori recenti mostrano che i modelli addestrati avversariamente sono più distorti verso le caratteristiche della struttura globale. Invece, in questo lavoro, vorremmo indagare la relazione tra la generalizzazione dell'addestramento avversario e le robuste caratteristiche locali, poiché le robuste caratteristiche locali generalizzano bene per le variazioni di forma non viste.Per imparare le robuste caratteristiche locali, sviluppiamo una trasformazione Random Block Shuffle (RBS) per rompere le caratteristiche della struttura globale sui normali esempi avversari. Continuiamo a proporre un nuovo approccio chiamato Robust Local Features for Adversarial Training (RLFAT), che prima impara le robuste caratteristiche locali attraverso l'addestramento avversario sugli esempi avversari trasformati da RBS, e poi trasferisce le robuste caratteristiche locali nell'addestramento dei normali esempi avversari.Per dimostrare la generalità del nostro argomento, implementiamo RLFAT in quadri di addestramento avversario attualmente all'avanguardia. Esperimenti estesi su STL-10, CIFAR-10 e CIFAR-100 mostrano che RLFAT migliora significativamente sia la generalizzazione adversarially robust che la generalizzazione standard dell'addestramento adversarial.Inoltre, dimostriamo che i nostri modelli catturano più caratteristiche locali dell'oggetto sulle immagini, allineandosi meglio alla percezione umana.
La verifica dei modelli del dominio di pianificazione è fondamentale per garantire la sicurezza, l'integrità e la correttezza dei sistemi automatizzati basati sulla pianificazione.  Tuttavia, l'applicazione diretta dei model checker per verificare i modelli del dominio di pianificazione può risultare in falsi positivi, cioè in controesempi che sono irraggiungibili da un pianificatore sano quando si usa il dominio sotto verifica durante un compito di pianificazione.In questo articolo, discutiamo i lati negativi della verifica non vincolata del modello del dominio di pianificazione.Proponiamo quindi una pratica fail-safe per la progettazione di modelli del dominio di pianificazione che può garantire intrinsecamente la sicurezza dei piani prodotti in caso di errori non rilevati nei modelli del dominio.  Inoltre, dimostriamo come i controllori del modello, così come le tecniche di pianificazione dei vincoli di traiettoria di stato, dovrebbero essere usati per verificare i modelli del dominio di pianificazione in modo che non vengano restituiti controesempi irraggiungibili.
Abbiamo visto un enorme successo di modelli di generazione di immagini in questi anni.Generare immagini attraverso una rete neurale è di solito pixel-based, che è fondamentalmente diverso da come gli esseri umani creano opere d'arte utilizzando pennelli.Per imitare il disegno umano, le interazioni tra l'ambiente e l'agente è necessario per consentire prove.Tuttavia, l'ambiente è di solito non differenziabile, portando alla convergenza lento e massiccia computazione.In questo articolo cerchiamo di affrontare la natura discreta di ambiente software con una simulazione intermedia, differenziabile. Presentiamo StrokeNet, un nuovo modello in cui l'agente viene addestrato su un'approssimazione neurale ben fatta dell'ambiente pittorico.Con questo approccio, il nostro agente è stato in grado di imparare a scrivere caratteri come le cifre MNIST più velocemente degli approcci di apprendimento per rinforzo in modo non supervisionato.Il nostro contributo principale è la simulazione neurale di un ambiente del mondo reale.Inoltre, l'agente addestrato con l'ambiente emulato è in grado di trasferire direttamente le sue competenze al software del mondo reale.
Dimostriamo come il machine learning è in grado di modellare esperimenti di fisica quantistica.L'entanglement quantistico è una pietra miliare per le prossime tecnologie quantistiche come il calcolo quantistico e la crittografia quantistica.Di particolare interesse sono gli stati quantistici complessi con più di due particelle e un gran numero di livelli quantistici entangled.Dato un tale stato quantistico multiparticellare ad alta densità, è solitamente impossibile ricostruire un setup sperimentale che lo produce.Per cercare esperimenti interessanti, si devono quindi creare casualmente milioni di setup su un computer e calcolare i rispettivi stati di output. In questo lavoro, mostriamo che i modelli di apprendimento automatico possono fornire un miglioramento significativo rispetto alla ricerca casuale. Dimostriamo che una rete neurale a memoria a breve termine (LSTM) può imparare con successo a modellare gli esperimenti quantistici prevedendo correttamente le caratteristiche dello stato di uscita per determinati setup senza la necessità di calcolare gli stati stessi. Questo approccio non solo permette una ricerca più veloce, ma è anche un passo essenziale verso la progettazione automatizzata di esperimenti quantistici multiparticella ad alta densità utilizzando modelli generativi di apprendimento automatico.
In questo articolo, proponiamo un quadro di apprendimento metrico non lineare non supervisionato per aumentare le prestazioni degli algoritmi di clustering. Nel nostro quadro, l'apprendimento metrico a distanza non lineare e l'incorporazione di manifold sono integrati e condotti simultaneamente per aumentare le separazioni naturali tra i campioni di dati. La componente di apprendimento metrico è implementata attraverso trasformazioni dello spazio delle caratteristiche, regolate da un modello deformabile non lineare chiamato Coherent Point Drifting (CPD). Guidati da CPD, i punti di dati possono raggiungere un livello più alto di separabilità lineare, che viene successivamente raccolto dalla componente di incorporazione del manifold per generare proiezioni di campioni ben separabili per il clustering.I risultati sperimentali su set di dati sintetici e di riferimento mostrano l'efficacia del nostro approccio proposto rispetto alle soluzioni all'avanguardia nell'apprendimento metrico non supervisionato.
Come risultato, i fornitori di servizi di apprendimento automatico hanno un bisogno crescente di dati di allenamento utili e aggiuntivi che favoriscano l'addestramento, senza dare tutti i dettagli sul programma addestrato.Allo stesso tempo, i proprietari dei dati vorrebbero scambiare i loro dati per il loro valore, senza dover dare via i dati stessi prima di ricevere un compenso.è difficile per i fornitori di dati e i fornitori di modelli accordarsi su un prezzo equo senza prima rivelare i dati o il modello addestrato all'altra parte. Attualmente, i proprietari dei dati e i proprietari dei modelli non hanno un sistema di prezzi equi che elimini la necessità di fidarsi di una terza parte e di addestrare il modello sui dati, che1) richiede molto tempo per essere completato,2) non garantisce che i dati utili siano pagati con valore e che i dati inutili non lo siano, senza fidarsi della terza parte sia del modello che dei dati. I miglioramenti esistenti per rendere sicura la transazione si concentrano pesantemente sulla crittografia o l'approssimazione dei dati, come l'addestramento su dati crittografati, e varianti di apprendimento federato.Per quanto potenti sembrino essere i metodi, dimostriamo che sono impraticabili nel nostro caso d'uso con presupposti del mondo reale per preservare la privacy dei proprietari dei dati quando si affrontano modelli black-box. Pertanto, uno schema di prezzo equo che non si basa sulla crittografia e l'offuscamento sicuro dei dati è necessario prima dello scambio di dati.Questo pa- per propone un nuovo metodo per il prezzo equo utilizzando tecniche di efficacia dei dati-modello come funzioni di influenza, estrazione del modello, e metodi di compressione del modello, consentendo così transazioni di dati sicure. Dimostriamo con successo che senza eseguire i dati attraverso il modello, si può approssimare il valore dei dati; cioè, se i dati risultano ridondanti, il prezzo è minimo, e se i dati portano a un miglioramento adeguato, il loro valore è adeguatamente valutato, senza porre forti presupposti sulla natura del modello.Il lavoro futuro sarà incentrato sulla creazione di un sistema con una maggiore sicurezza transazionale contro attacchi avversari che riveleranno dettagli sul modello o sui dati all'altra parte.
Line-Storm è un sistema informatico interattivo per la performance creativa: il contesto che abbiamo studiato è stato quello della scrittura su carta utilizzando Line-Storm. Abbiamo usato questionari di autovalutazione come parte di una ricerca che coinvolgeva partecipanti umani, per valutare Line-Storm. Line-Storm consisteva in uno stilo per scrivere e in un blocco per scrivere, potenziato dall'elettronica: il blocco per scrivere era collegato a un microfono a contatto, e lo stilo per scrivere aveva una piccola scheda a microcontroller e periferiche collegate ad esso; i segnali da questi potenziamenti elettronici sono stati inseriti nell'ambiente di sintesi audio Max/MSP per produrre un paesaggio sonoro interattivo. Abbiamo cercato di scoprire se Line-Storm ha migliorato il senso auto-riferito di presenza e impegno durante un compito di scrittura, e abbiamo confrontato Line-Storm con una condizione di controllo non interattivo. Dopo aver eseguito l'analisi statistica in SPSS, non siamo stati in grado di sostenere la nostra ipotesi di ricerca, che la presenza e l'impegno fossero migliorati da Line-Storm. Un risultato statisticamente significativo del nostro studio è che alcuni partecipanti hanno risposto a Line-Storm più positivamente di altri. Questi preservatori di Line-Storm erano un gruppo, distinto dagli altri partecipanti, che hanno riportato una maggiore presenza e impegno e che hanno scritto più parole con Line-Storm e durante la condizione di controllo. Discutiamo i risultati della nostra ricerca e collochiamo Line-Storm in un contesto artistico-tecnologico, attingendo agli scritti di Martin Heidegger quando consideriamo la natura di Line-Storm. Il lavoro futuro include la modifica dei componenti interattivi, il miglioramento dell'estetica e l'uso di elettronica più miniaturizzata, la sperimentazione di un compito di disegno invece di un compito di scrittura, e la collaborazione con un compositore di musica elettronica per fare un paesaggio sonoro interattivo più interessante, coinvolgente e immersivo per la scrittura o il disegno.
In molte applicazioni, i dati di formazione parallela non sono disponibili e le frasi da trasferire possono avere stili arbitrari e sconosciuti. In questo articolo, presentiamo un quadro codificatore-decodificatore per questo problema: ogni frase viene codificata nelle sue rappresentazioni latenti di contenuto e stile, ricombinando il contenuto con lo stile di destinazione, possiamo decodificare una frase allineata nel dominio di destinazione. La prima è una perdita di discrepanza di stile, che impone che la rappresentazione dello stile codifichi accuratamente le informazioni di stile guidate dalla discrepanza tra lo stile della frase e lo stile di destinazione.La seconda è una perdita di coerenza del ciclo, che assicura che la frase trasferita conservi il contenuto della frase originale separata dal suo stile.Convalidiamo l'efficacia del nostro modello proposto su due compiti: modifica del sentiment delle recensioni dei ristoranti, e revisione delle risposte di dialogo con uno stile romantico.
In questo articolo, proponiamo una nuova funzione di perdita per eseguire l'analisi delle componenti principali (PCA) utilizzando autocodificatori lineari (LAEs).Ottimizzando la perdita L2 standard si ottiene una matrice di decodifica che abbraccia il sottospazio principale della covarianza campione dei dati, ma non riesce a identificare gli autovettori esatti.Questo inconveniente ha origine da un'invarianza che si annulla nella mappa globale.Qui, dimostriamo che la nostra funzione di perdita elimina questo problema, cioè. Per questa nuova perdita, stabiliamo che tutti i minimi locali sono ottimi globali e dimostriamo anche che il calcolo della nuova perdita (e anche i suoi gradienti) ha lo stesso ordine di complessità della perdita classica, una matrice di 60.000 x784), dimostrando che il nostro approccio è praticamente applicabile e corregge i precedenti lati negativi delle LAE.
Gli utenti hanno un enorme potenziale per aiutare la costruzione e la manutenzione delle basi di conoscenza (KB) attraverso il contributo di feedback che identifica gli attributi e le relazioni di entità errate e mancanti. Tuttavia, quando nuovi dati vengono aggiunti alla KB, le entità della KB, che sono costruite eseguendo la risoluzione delle entità (ER), possono cambiare, rendendo gli obiettivi previsti del feedback degli utenti sconosciuti - un problema che chiamiamo incertezza di identità. In questo lavoro, presentiamo una struttura per l'integrazione del feedback dell'utente nella KB in presenza di incertezza di identità. Il nostro approccio si basa sulla partecipazione del feedback dell'utente accanto alle menzioni in ER. Proponiamo una rappresentazione specifica del feedback dell'utente come menzioni di feedback e introduciamo un nuovo algoritmo online per integrare queste menzioni in una KB esistente.
Gli algoritmi di apprendimento automatico sono vulnerabili agli attacchi di avvelenamento: Un avversario può iniettare punti malevoli nel set di dati di formazione per influenzare il processo di apprendimento e degradare le prestazioni dell'algoritmo.Sono già stati proposti attacchi di avvelenamento ottimali per valutare gli scenari peggiori, modellando gli attacchi come un problema di ottimizzazione a due livelli.Risolvere questi problemi è computazionalmente impegnativo e ha un'applicabilità limitata per alcuni modelli come le reti profonde.In questo articolo introduciamo un nuovo modello generativo per realizzare attacchi sistematici di avvelenamento contro classificatori di apprendimento automatico che generano esempi di formazione avversaria, cioè Proponiamo una Generative Adversarial Net con tre componenti: il generatore, il discriminatore e il classificatore di destinazione. Questo approccio ci permette di modellare naturalmente i vincoli di rilevabilità che possono essere previsti in attacchi realistici e di identificare le regioni della distribuzione dei dati sottostanti che possono essere più vulnerabili al data poisoning.
L'algoritmo di Expectation-Maximization (EM) è uno strumento fondamentale nell'apprendimento automatico non supervisionato ed è spesso usato come un modo efficiente per risolvere i problemi di stima di Maximum Likelihood (ML) e Maximum A Posteriori, specialmente per i modelli con variabili latenti. è anche l'algoritmo di scelta per adattare modelli di miscela: modelli generativi che rappresentano punti non etichettati provenienti da $k$ processi diversi, come campioni da $k$ distribuzioni multivariate.in questo lavoro definiamo e usiamo una versione quantistica di EM per adattare un Gaussian Mixture Model. Dato l'accesso quantistico ad un dataset di $n$ vettori di dimensione $d$, il nostro algoritmo ha garanzie di convergenza e precisione simili all'algoritmo classico, ma il tempo di esecuzione è solo polilogaritmico nel numero di elementi nel set di training, ed è polinomiale in altri parametri - come la dimensione del feature space, e il numero di componenti della miscela. Generalizziamo ulteriormente l'algoritmo adattando qualsiasi modello di miscela di distribuzioni di base nella famiglia esponenziale. Discutiamo le prestazioni dell'algoritmo su insiemi di dati che dovrebbero essere classificati con successo da questi algoritmi, sostenendo che in questi casi possiamo dare forti garanzie sul tempo di esecuzione.
Proponiamo un nuovo approccio, noto come iterative regularized dual averaging (iRDA), per migliorare l'efficienza delle reti neurali convoluzionali (CNN) riducendo significativamente la ridondanza del modello senza ridurre la sua precisione.  Il metodo è stato testato per vari set di dati e ha dimostrato di essere significativamente più efficiente della maggior parte delle tecniche di compressione esistenti nella letteratura sull'apprendimento profondo.  Per molti set di dati popolari come MNIST e CIFAR-10, più del 95% dei pesi può essere azzerato senza perdere accuratezza. In particolare, siamo in grado di rendere ResNet18 con il 95% di sparsità per avere una precisione che è paragonabile a quella di un modello molto più grande ResNet50 con la migliore sparsità del 60% come riportato in letteratura.
Imparare a imitare il comportamento di un esperto dalle dimostrazioni può essere impegnativo, specialmente in ambienti con osservazioni continue ad alta densità e dinamiche sconosciute. I metodi di apprendimento supervisionato basati sulla clonazione comportamentale (BC) soffrono dello spostamento della distribuzione: poiché l'agente imita avidamente le azioni dimostrate, può allontanarsi dagli stati dimostrati a causa dell'accumulo di errori. Metodi recenti basati sull'apprendimento di rinforzo (RL), come l'RL inverso e l'apprendimento generativo di imitazione (GAIL), superano questo problema addestrando un agente RL a corrispondere alle dimostrazioni su un lungo orizzonte.Poiché la vera funzione di ricompensa per il compito è sconosciuta, questi metodi imparano una funzione di ricompensa dalle dimostrazioni, spesso usando tecniche di approssimazione complesse e fragili che coinvolgono l'addestramento avversario.Noi proponiamo una semplice alternativa che usa ancora RL, ma non richiede di imparare una funzione di ricompensa. L'idea chiave è quella di fornire all'agente un incentivo per abbinare le dimostrazioni su un lungo orizzonte, incoraggiandolo a tornare agli stati dimostrati quando incontra nuovi stati fuori dalla distribuzione; otteniamo questo dando all'agente una ricompensa costante di r=+1 per abbinare l'azione dimostrata in uno stato dimostrato, e una ricompensa costante di r=0 per tutti gli altri comportamenti; il nostro metodo, che chiamiamo soft Q imitation learning (SQIL), può essere implementato con una manciata di piccole modifiche a qualsiasi algoritmo standard di Q-learning o actor-critic off-policy. Teoricamente, mostriamo che SQIL può essere interpretato come una variante regolarizzata di BC che utilizza un priore di sparsità per incoraggiare l'imitazione a lungo orizzonte.Empiricamente, mostriamo che SQIL supera BC e raggiunge risultati competitivi rispetto a GAIL, su una varietà di compiti basati su immagini e a bassa dimensione in Box2D, Atari, e MuJoCo.Questo documento è una prova di concetto che illustra come un semplice metodo di imitazione basato su RL con ricompense costanti può essere efficace come metodi più complessi che utilizzano ricompense apprese.
Generare visualizzazioni e interpretazioni da dati ad alta dimensione è un problema comune in molti campi. Due approcci chiave per affrontare questo problema sono il clustering e l'apprendimento della rappresentazione. Esistono modelli di deepclustering molto performanti da un lato e tecniche di apprendimento della rappresentazione interpretabili, spesso basate su strutture topologiche latenti come le mappe auto-organizzanti, dall'altro. Presentiamo una nuova architettura profonda per il clustering probabilistico, VarPSOM, e la sua estensione ai dati delle serie temporali, VarTPSOM, composta da moduli VarPSOM collegati da celle LSTM. Mostriamo che raggiungono prestazioni di clustering superiori rispetto agli attuali metodi di clustering profondo sui dati statici MNIST/Fashion-MNIST e sulle serie temporali mediche, inducendo nel contempo una rappresentazione interpretabile.
Molte applicazioni di computer vision richiedono la risoluzione di più compiti in tempo reale. Una rete neurale può essere addestrata a risolvere più compiti simultaneamente usando il "multi-task learning", il che permette di risparmiare calcoli al momento dell'inferenza, dato che solo una singola rete deve essere valutata. Sfortunatamente, questo porta spesso a prestazioni complessive inferiori, dato che gli obiettivi dei compiti competono, il che pone di conseguenza la domanda: quali compiti dovrebbero e non dovrebbero essere appresi insieme in una rete quando si utilizza il multi-task learning? Studiamo sistematicamente la cooperazione e la competizione tra compiti e proponiamo una struttura per assegnare i compiti a poche reti neurali in modo che i compiti cooperanti siano calcolati dalla stessa rete neurale, mentre i compiti concorrenti siano calcolati da reti diverse. La nostra struttura offre un trade-off tempo-accuratezza e può produrre una migliore accuratezza usando meno tempo di inferenza non solo di una singola grande rete neurale multi-task ma anche di molte reti single-task.
Il motore di ricerca è diventato un componente fondamentale in varie applicazioni web e mobili.Recuperare documenti rilevanti da enormi set di dati è una sfida per un sistema di motori di ricerca, soprattutto di fronte a query verbose o di coda.In questo documento, esploriamo un quadro di ricerca vettoriale spazio per il recupero dei documenti. In particolare, abbiamo addestrato un modello di corrispondenza semantica profonda in modo che ogni query e documento possano essere codificati come un'incorporazione a bassa dimensione.Il nostro modello è stato addestrato sulla base dell'architettura BERT.Abbiamo distribuito un servizio di indice k-nearest-neighbor veloce per il servizio online.Entrambe le metriche offline e online dimostrano che il nostro metodo ha migliorato notevolmente le prestazioni di recupero e la qualità della ricerca, soprattutto per le query di coda
I metodi di apprendimento semi-supervisionato (SSL) sono stati un quadro influente per l'uso di dati non etichettati quando non c'è una quantità sufficiente di dati etichettati disponibili nel corso della formazione.I metodi SSL basati su reti neurali convoluzionali (CNN) hanno recentemente fornito risultati positivi su compiti standard di riferimento come la classificazione delle immagini.In questo lavoro, consideriamo l'impostazione generale del problema SSL in cui i dati etichettati e non etichettati provengono dalla stessa distribuzione di probabilità sottostante. Proponiamo un nuovo approccio che adotta una tecnica di trasporto ottimale (OT) che serve come metrica di somiglianza tra misure di probabilità empiriche discrete per fornire pseudo-etichette per i dati non etichettati, che possono poi essere utilizzati insieme ai dati etichettati iniziali per addestrare il modello CNN in un modo SSL.Abbiamo valutato e confrontato il nostro metodo proposto con algoritmi SSL all'avanguardia su set di dati standard per dimostrare la superiorità e l'efficacia del nostro algoritmo SSL.
La normalizzazione dei lotti (batch norm) è spesso usata nel tentativo di stabilizzare e accelerare la formazione nelle reti neurali profonde, in molti casi diminuisce il numero di aggiornamenti dei parametri necessari per ottenere un basso errore di formazione, ma riduce anche la robustezza alle piccole perturbazioni avversarie in ingresso e al rumore con percentuali a due cifre, come dimostriamo su cinque set di dati standard. Inoltre, sostituire il decadimento dei pesi con la norma batch è sufficiente ad annullare la relazione tra la vulnerabilità avversaria e la dimensione dell'input. Il nostro lavoro è coerente con un'analisi del campo medio che ha trovato che la norma batch causa l'esplosione dei gradienti.
Questo articolo presenta le idee preliminari del nostro lavoro per l'apprendimento automatico di reti a obiettivi gerarchici in domini nondeterministici, attualmente stiamo implementando le idee espresse in questo articolo.
Le reti neurali profonde eccellono in regimi con grandi quantità di dati, ma tendono a lottare quando i dati sono scarsi o quando hanno bisogno di adattarsi rapidamente ai cambiamenti nel compito. In risposta, il recente lavoro di meta-apprendimento propone di addestrare un meta-apprendente su una distribuzione di compiti simili, nella speranza di generalizzare a compiti nuovi ma correlati, imparando una strategia di alto livello che catturi l'essenza del problema che gli viene chiesto di risolvere. Tuttavia, molti approcci recenti di meta-apprendimento sono ampiamente progettati a mano, sia utilizzando architetture specializzate per una particolare applicazione, sia codificando componenti algoritmici che vincolano il modo in cui il meta-apprendente risolve il compito. Noi proponiamo una classe di architetture di meta-apprendimento semplici e generiche che utilizzano una nuova combinazione di convoluzioni temporali e attenzione morbida; la prima per aggregare informazioni dall'esperienza passata e la seconda per individuare pezzi specifici di informazioni.  Nella serie più estesa di esperimenti di meta-apprendimento fino ad oggi, valutiamo il Simple Neural AttentIve Learner (o SNAIL) risultante su diversi compiti pesantemente benchmarkati.  Su tutti i compiti, sia nell'apprendimento supervisionato che in quello con rinforzo, SNAIL raggiunge lo stato dell'arte delle prestazioni con margini significativi.
Knowledge Graph Embedding (KGE) ha attirato maggiore attenzione negli ultimi anni. La maggior parte dei modelli KGE impara da triple non consapevoli del tempo, ma l'inclusione di informazioni temporali accanto alle triple migliorerebbe ulteriormente le prestazioni di un modello KGE. A questo proposito, proponiamo LiTSE, un modello KGE temporale che incorpora le informazioni temporali nelle rappresentazioni di entità/relazione usando la decomposizione lineare delle serie temporali; inoltre, considerando l'incertezza temporale durante l'evoluzione delle rappresentazioni di entità/relazione nel tempo, mappiamo le rappresentazioni di KG temporali nello spazio delle distribuzioni gaussiane multidimensionali. La media di ogni incorporazione di entità/relazione ad un passo temporale mostra la posizione attuale attesa, mentre la sua covarianza (che è stazionaria nel tempo) rappresenta la sua incertezza temporale.Gli esperimenti mostrano che LiTSE non solo raggiunge lo stato dell'arte sulla previsione dei collegamenti nei KGs temporali, ma ha anche la capacità di prevedere il tempo di occorrenza dei fatti con annotazioni temporali mancanti, così come l'esistenza di eventi futuri.Al meglio della nostra conoscenza, nessun altro modello è in grado di eseguire tutti questi compiti.
Studiamo l'emergere di comportamenti cooperativi in agenti che apprendono per rinforzo introducendo un impegnativo ambiente di calcio competitivo multi-agente con una fisica simulata continua. Dimostriamo che l'allenamento decentralizzato e basato sulla popolazione con il co-play può portare a una progressione nei comportamenti degli agenti: dal casuale, al semplice inseguimento della palla, e infine mostrando prove di cooperazione. In particolare, dimostriamo che l'ottimizzazione automatica di semplici ricompense, che di per sé non favoriscono il comportamento cooperativo, può portare a un comportamento di squadra a lungo termine. Applichiamo inoltre uno schema di valutazione, basato sui principi della teoria dei giochi, che può valutare le prestazioni degli agenti in assenza di compiti di valutazione predefiniti o di basi umane.
Gli ultimi anni hanno visto la proposta di una serie di approcci neurali per la sintesi dei programmi, molti dei quali adottano un paradigma di generazione di sequenze simile alla traduzione automatica neurale, in cui i modelli sequenza-sequenza sono addestrati per massimizzare la probabilità di programmi di riferimento noti. Massimizzando la probabilità di un solo programma di riferimento, penalizza molti programmi semanticamente corretti, il che può influire negativamente sulle prestazioni del sintetizzatore. In secondo luogo, questa strategia trascura il fatto che i programmi hanno una sintassi rigorosa che può essere controllata in modo efficiente. Per affrontare la prima limitazione, eseguiamo il reinforcementlearning sopra un modello supervisionato con un obiettivo che massimizza esplicitamente la probabilità di generare programmi semanticamente corretti. Per affrontare la seconda limitazione, introduciamo una procedura di addestramento che massimizza direttamente la probabilità di generare programmi sintatticamente corretti che soddisfano la specifica. Mostriamo che i nostri contributi portano a una migliore accuratezza dei modelli, specialmente nei casi in cui i dati di addestramento sono limitati.
La previsione delle serie temporali gioca un ruolo cruciale nel marketing, nella finanza e in molti altri campi quantitativi; è stata sviluppata una grande quantità di metodologie su questo argomento, tra cui ARIMA, Holt-Winters, ecc. Tuttavia, le loro prestazioni sono facilmente minate dall'esistenza di punti di cambiamento e punti di anomalia, due strutture comunemente osservate nei dati reali, ma raramente considerate nei suddetti metodi. Per dedurre tutte le variabili nascoste, sviluppiamo un quadro bayesiano, che è in grado di ottenere distribuzioni e intervalli di previsione per la previsione delle serie temporali, con proprietà teoriche dimostrabili.Per l'implementazione, viene proposto un algoritmo iterativo con Markov chain Monte Carlo (MCMC), filtro di Kalman e smoothing di Kalman.In entrambe le applicazioni di dati sintetici e dati reali, la nostra metodologia produce una migliore performance nella previsione delle serie temporali rispetto ai metodi esistenti, insieme a un rilevamento più accurato dei punti di cambiamento e delle anomalie.
Il complesso mondo che ci circonda è intrinsecamente multimodale e sequenziale (continuo).Le informazioni sono sparse attraverso diverse modalità e richiedono più sensori continui per essere catturate.Come l'apprendimento automatico salta verso una migliore generalizzazione al mondo reale, l'apprendimento sequenziale multimodale diventa un'area di ricerca fondamentale. In questo articolo, presentiamo un nuovo modello di trasformatore, chiamato Factorized Multimodal Transformer (FMT) per l'apprendimento sequenziale multimodale. FMT modella intrinsecamente le dinamiche intramodali e intermodali (che coinvolgono due o più modalità) all'interno del suo input multimodale in modo fattorizzato. La fattorizzazione proposta permette di aumentare il numero di auto-attenzioni per modellare meglio i fenomeni multimodali a portata di mano; senza incontrare difficoltà durante l'addestramento (per esempio, overfitting) anche su setup con risorse relativamente basse.Tutti i meccanismi di attenzione all'interno di FMT hanno un campo recettivo completo nel dominio del tempo che permette loro di catturare asincronicamente dinamiche multimodali a lungo raggio. Nei nostri esperimenti ci concentriamo su set di dati che contengono le tre modalità comunemente studiate del linguaggio, della visione e dell'acustica.Eseguiamo una vasta gamma di esperimenti, che abbracciano 3 set di dati ben studiati e 21 etichette distinte.FMT mostra prestazioni superiori rispetto ai modelli precedentemente proposti, stabilendo un nuovo stato dell'arte nei set di dati studiati.
Il nostro algoritmo, che chiamiamo Stochastic Geodesic Optimization (SGeO), utilizza un coefficiente adattivo sopra il metodo Heavy Ball di Polyak che controlla efficacemente la quantità di peso messo sull'aggiornamento precedente dei parametri in base al cambiamento di direzione nel percorso di ottimizzazione. I risultati sperimentali su funzioni fortemente convesse con gradienti Lipschitz e benchmark Autoencoder profondi mostrano che SGeO raggiunge errori più bassi rispetto ai metodi del primo ordine stabiliti e compete bene con errori più bassi o simili a un recente metodo del secondo ordine chiamato K-FAC (Kronecker-Factored Approximate Curvature).Abbiamo anche incorporato il gradiente lookahead stile Nesterov nel nostro algoritmo (SGeO-N) e osserviamo notevoli miglioramenti.
Introduciamo il modello di traduzione mascherato (MTM) che combina la codifica e la decodifica delle sequenze all'interno dello stesso componente del modello.Il MTM si basa sull'idea della modellazione linguistica mascherata e supporta strategie di decodifica sia autoregressive che non autoregressive semplicemente cambiando l'ordine di mascheramento.Negli esperimenti sul compito WMT 2016 rumeno-inglese, il MTM mostra forti prestazioni di traduzione a tempo costante, battendo tutti gli approcci correlati con complessità comparabile.Confrontiamo anche ampiamente varie strategie di decodifica supportate dal MTM, così come diverse tecniche di modellazione della lunghezza e impostazioni di allenamento.
Presentiamo gli ensemble locali, un metodo per rilevare l'estrapolazione al momento del test in un modello pre-addestrato.Ci concentriamo sulla sottodeterminazione come componente chiave dell'estrapolazione: miriamo a rilevare quando molte previsioni possibili sono coerenti con i dati di addestramento e la classe del modello.Il nostro metodo usa informazioni locali del secondo ordine per approssimare la varianza delle previsioni attraverso un ensemble di modelli della stessa classe. Calcoliamo questa approssimazione stimando la norma della componente del gradiente di un punto di prova che si allinea con le direzioni a bassa curvatura dell'Hessiano, e forniamo un metodo trattabile per stimare questa quantità.Sperimentalmente, dimostriamo che il nostro metodo è in grado di rilevare quando un modello pre-addestrato sta estrapolando sui dati di prova, con applicazioni al rilevamento di out-of-distribution, rilevando correlati spuri, e l'apprendimento attivo.
Le operazioni di coalizione sono essenziali per rispondere al crescente numero di incidenti in tutto il mondo che richiedono assistenza umanitaria su larga scala.Molte nazioni e organizzazioni non governative si coordinano regolarmente per affrontare tali problemi, ma la loro cooperazione è spesso ostacolata da limiti su quali informazioni sono in grado di condividere.In questo articolo, consideriamo l'uso di una tecnica crittografica avanzata chiamata secure multi-party computation per consentire ai membri della coalizione di raggiungere obiettivi comuni, pur rispettando i requisiti di privacy. La nostra particolare attenzione si concentra su un compito di pianificazione della consegna degli aiuti multinazionale che comporta il coordinamento di quando e dove le varie nazioni fornitrici di aiuti consegneranno materiali di soccorso dopo il verificarsi di un disastro naturale. Anche con l'uso di una tecnologia di calcolo sicura per più parti, le informazioni sui dati privati possono trapelare.  Descriviamo come il campo emergente del flusso di informazioni quantitative può essere usato per aiutare i proprietari dei dati a capire fino a che punto i dati privati potrebbero diventare vulnerabili come risultato di operazioni di programmazione possibili o effettive, e per consentire regolazioni automatiche del processo di programmazione per garantire i requisiti di privacy
La sfida dell'apprendimento della rappresentazione disentangled ha recentemente attirato molta attenzione e si riduce a una competizione.Vari metodi basati su auto-encoder variazionale sono stati proposti per risolvere questo problema, forzando l'indipendenza tra la rappresentazione e modificando il termine di regolarizzazione nel lower bound variazionale.Tuttavia il recente lavoro di Locatello et al. (2018) ha dimostrato che i metodi proposti sono fortemente influenzati dalla casualità e dalla scelta dell'iper-parametro.Questo lavoro è costruito sullo stesso quadro della fase 1 (Li et al, 2019), ma con impostazioni diverse; per renderlo autonomo, forniamo questo manoscritto, che è inevitabilmente molto simile al rapporto per la Fase 1.In dettaglio, in questo lavoro, invece di progettare un nuovo termine di regolarizzazione, adottiamo il FactorVAE ma miglioriamo le prestazioni di ricostruzione e aumentiamo la capacità della rete e la fase di formazione.La strategia si rivela molto efficace nel raggiungere il disentanglement.
Il nostro approccio genera domande di chiarimento con l'obiettivo di ottenere nuove informazioni che rendano il contesto dato più completo. Sviluppiamo una Generative Adversarial Network (GAN) dove il generatore è un modello sequenza-sequenza e il discriminatore è una funzione di utilità che modella il valore dell'aggiornamento del contesto con la risposta alla domanda di chiarimento. Valutiamo su due dataset, usando sia metriche automatiche che giudizi umani di utilità, specificità e rilevanza, mostrando che il nostro approccio supera sia un modello basato sul retrieval che ablazioni che escludono il modello di utilità e la formazione avversaria.
I database di modelli sono il fondamento di alcune delle più forti euristiche ammissibili per la pianificazione classica ottimale.Gli esperimenti hanno mostrato che il modo più informativo di combinare le informazioni da più database di modelli è quello di utilizzare il partizionamento a costi saturi.Il lavoro precedente ha selezionato i modelli e ha calcolato i partizionamenti a costi saturi sull'euristica del database di modelli risultante in due fasi separate.Introduciamo un nuovo metodo che utilizza il partizionamento a costi saturi per selezionare i modelli e mostriamo che supera tutti gli algoritmi di selezione dei modelli esistenti.
Vaswani et al. (2017) propongono una nuova architettura che evita completamente la ricorsività e la convoluzione, utilizzando invece solo l'autoattenzione e gli strati feed-forward. Mentre l'architettura proposta raggiunge risultati all'avanguardia su diversi compiti di traduzione automatica, richiede un gran numero di parametri e iterazioni di formazione per convergere. Proponiamo Weighted Transformer, un Transformer con strati di attenzione modificati, che non solo supera la rete di base nel punteggio BLEU ma converge anche il 15-40% più velocemente. In particolare, sostituiamo l'attenzione multitesta con più rami di auto-attenzione che il modello impara a combinare durante il processo di formazione. Il nostro modello migliora le prestazioni allo stato dell'arte di 0,5 punti BLEU sul compito di traduzione dall'inglese al tedesco WMT 2014 e di 0,4 sul compito di traduzione dall'inglese al francese.
L'apprendimento per imitazione fornisce un quadro attraente per il controllo autonomo: in molti compiti, le dimostrazioni del comportamento preferito possono essere facilmente ottenute da esperti umani, eliminando la necessità di una raccolta di dati online costosa e potenzialmente pericolosa nel mondo reale. L'apprendimento di rinforzo basato sul modello (MBRL) offre molta più flessibilità, poiché un modello predittivo appreso dai dati può essere usato per raggiungere vari obiettivi al momento del test.Tuttavia, MBRL soffre di due difetti.Primo, il modello non aiuta a scegliere i risultati desiderati o sicuri - la sua dinamica stima solo ciò che è possibile, non ciò che è preferito. In secondo luogo, MBRL richiede tipicamente un'ulteriore raccolta di dati online per garantire che il modello sia accurato in quelle situazioni che vengono effettivamente incontrate quando si cerca di raggiungere gli obiettivi del tempo di prova.Raccogliere questi dati con un modello parzialmente addestrato può essere pericoloso e richiedere molto tempo.In questo articolo, miriamo a combinare i vantaggi dell'apprendimento per imitazione e MBRL, e proponiamo modelli imitativi: modelli predittivi probabilistici in grado di pianificare traiettorie simili a quelle degli esperti per raggiungere obiettivi arbitrari. Troviamo che questo metodo supera sostanzialmente sia l'imitazione diretta che la MBRL in un compito di guida autonoma simulata, e può essere appreso in modo efficiente da un set fisso di dimostrazioni di esperti senza ulteriore raccolta di dati online. Mostriamo anche che il nostro modello può incorporare in modo flessibile i costi forniti dall'utente durante il test, può pianificare sequenze di obiettivi, e può anche funzionare bene con obiettivi imprecisi, compresi gli obiettivi sul lato sbagliato della strada.
La stima dell'incertezza e i metodi di ensembling vanno di pari passo.La stima dell'incertezza è uno dei principali parametri di riferimento per la valutazione delle prestazioni dell'ensembling.Allo stesso tempo, gli ensemble di deep learning hanno fornito risultati all'avanguardia nella stima dell'incertezza.In questo lavoro, ci concentriamo sull'incertezza nel dominio per la classificazione delle immagini.Esploriamo gli standard per la sua quantificazione e segnaliamo le insidie delle metriche esistenti. Evitando queste insidie, eseguiamo un ampio studio di diverse tecniche di ensembling.Per fornire una maggiore comprensione dell'ampio confronto, introduciamo il deep ensemble equivalent (DEE) e mostriamo che molte sofisticate tecniche di ensembling sono equivalenti a un ensemble di pochissime reti addestrate indipendentemente in termini di log-likelihood di prova.
Le tecniche di verifica formale che calcolano garanzie dimostrabili sulle proprietà dei modelli di apprendimento automatico, come la robustezza alle perturbazioni avversarie norm-bounded, hanno dato risultati impressionanti. Anche se la maggior parte delle tecniche sviluppate finora richiede la conoscenza dell'architettura del modello di apprendimento automatico e rimane difficile da scalare a pipeline di previsione complesse, il metodo dello smoothing randomizzato ha dimostrato di superare molti di questi ostacoli. Tuttavia, il lavoro passato sullo smoothing randomizzato si è concentrato su classi limitate di misure o perturbazioni di smoothing (come Gaussiane o discrete) ed è stato solo in grado di dimostrare la robustezza rispetto a semplici limiti normativi. In questo articolo introduciamo un quadro generale per dimostrare le proprietà di robustezza dei modelli di apprendimento automatico smoothed nell'impostazione black-box. In particolare, estendiamo le procedure di lisciatura randomizzata per gestire misure di lisciatura arbitrarie e dimostriamo la robustezza del classificatore lisciato usando $f$-divergenze.La nostra metodologia raggiunge una robustezza certificata allo stato dell'arte su MNIST, CIFAR-10 e ImageNet e anche un compito di classificazione audio, Librispeech, rispetto a diverse classi di perturbazioni avversarie.
La capacità di decomporre complesse scene multi-oggetto in astrazioni significative come gli oggetti è fondamentale per raggiungere una cognizione di livello superiore. Gli approcci precedenti per l'apprendimento non supervisionato della rappresentazione della scena orientata agli oggetti sono basati su approcci spatial-attention o scene-mixture e limitati nella scalabilità, che è un ostacolo principale verso la modellazione di scene del mondo reale.In questo articolo, proponiamo un modello generativo di variabile latente, chiamato SPACE, che fornisce un quadro di modellazione probabilistica uniﬁcata che unisce il meglio degli approcci spatial-attention e scene-mixture. SPACE può esplicitamente fornire rappresentazioni di oggetti fattorizzati per gli oggetti in primo piano mentre decompone anche i segmenti di sfondo di morfologia complessa.I modelli precedenti sono bravi in uno di questi, ma non in entrambi.SPACE risolve anche i problemi di scalabilità dei metodi precedenti incorporando l'attenzione spaziale parallela e quindi è applicabile a scene con un gran numero di oggetti senza degrado delle prestazioni.Mostriamo attraverso esperimenti su Atari e 3D-Rooms che SPACE raggiunge le proprietà di cui sopra in modo coerente rispetto a SPAIR, IODINE, e GENESIS.I risultati dei nostri esperimenti possono essere trovati sul nostro sito del progetto: https://sites.google.com/view/space-project-page
Proponiamo un singolo modello neurale probabilistico basato su un autocodificatore variazionale che può essere condizionato su un sottoinsieme arbitrario di caratteristiche osservate e poi campionare le caratteristiche rimanenti in "un colpo". Le caratteristiche possono essere sia di valore reale che categoriche. L'addestramento del modello viene eseguito da Bayes variazionale stocastico.
Il nostro approccio combina un nuovo schema di rappresentazione genetica gerarchica che imita il modello di progettazione modulare comunemente adottato dagli esperti umani e uno spazio di ricerca espressivo che supporta topologie complesse. Il nostro algoritmo scopre in modo efficiente le architetture che superano un gran numero di modelli progettati manualmente per la classificazione delle immagini, ottenendo un errore top-1 del 3,6% su CIFAR-10 e del 20,3% quando viene trasferito a ImageNet, che è competitivo con i migliori approcci esistenti per la ricerca di architetture neurali.Presentiamo anche risultati utilizzando la ricerca casuale, ottenendo lo 0,3% in meno di accuratezza top-1 su CIFAR-10 e lo 0,1% in meno su ImageNet, riducendo il tempo di ricerca da 36 ore a 1 ora.
Nella pianificazione visiva (VP), un agente impara a pianificare il comportamento diretto all'obiettivo dalle osservazioni di un sistema dinamico ottenuto offline, ad es, Un approccio recente e promettente alla VP è il metodo della memoria topologica semi-parametrica (SPTM), in cui i campioni di immagine sono trattati come nodi in un grafico e la connettività nel grafico viene appresa utilizzando la classificazione profonda delle immagini. Così, il grafico appreso rappresenta la connettività topologica dei dati, e la pianificazione può essere eseguita utilizzando metodi convenzionali di ricerca dei grafi.Tuttavia, l'addestramento di SPTM richiede una funzione di perdita adatta per il classificatore di connettività, che richiede una sintonizzazione manuale non banale. Più importante, SPTM è limitato nella sua capacità di generalizzare ai cambiamenti nel dominio, poiché il suo grafico è costruito da osservazioni dirette e quindi richiede la raccolta di nuovi campioni per la pianificazione.In questo documento, proponiamo Hallucinative Topological Memory (HTM), che supera questi difetti.In HTM, invece di addestrare un classificatore discriminativo, addestriamo una funzione energetica utilizzando la codifica predittiva contrastiva. Inoltre, impariamo un modello VAE condizionale che genera campioni dati un'immagine di contesto del dominio, e usiamo questi campioni allucinati per costruire il grafo della connettività, permettendo una generalizzazione zero-shot ai cambiamenti di dominio.In domini simulati, HTM supera i metodi convenzionali SPTM e visual foresight in termini sia di qualità del piano che di successo nella pianificazione a lungo orizzonte.
Le reti neurali profonde (DNN) sono cresciute negli ultimi anni, in cui la normalizzazione dei lotti (BN) gioca un ruolo indispensabile. Tuttavia, è stato osservato che la BN è costosa a causa delle operazioni di riduzione. In questo articolo, proponiamo di alleviare il costo della BN utilizzando solo una piccola frazione di dati per la stima della media e della varianza ad ogni iterazione. La sfida chiave per raggiungere questo obiettivo è come raggiungere un equilibrio soddisfacente tra l'efficacia della normalizzazione e l'efficienza dell'esecuzione. Identifichiamo che l'efficacia si aspetta una minore correlazione dei dati mentre l'efficienza si aspetta un modello di esecuzione regolare. A tal fine, proponiamo due categorie di approccio: il campionamento o la creazione di pochi dati non correlati per la stima delle statistiche con determinati vincoli di strategia. Il primo include "Batch Sampling (BS)" che seleziona casualmente pochi campioni da ogni lotto e "Feature Sampling (FS)" che seleziona casualmente una piccola patch da ogni mappa di caratteristiche di tutti i campioni, e il secondo è "Virtual Dataset Normalization (VDN)" che genera pochi campioni casuali sintetici. Tutti i metodi proposti sono valutati in modo completo su vari modelli DNN, dove un'accelerazione complessiva dell'addestramento fino al 21,7% sulle moderne GPU può essere praticamente raggiunta senza il supporto di librerie specializzate, e la perdita di precisione del modello e il tasso di convergenza sono trascurabili.
L'apprendimento federato permette ai dispositivi periferici di imparare in modo collaborativo un modello condiviso mantenendo i dati di formazione sul dispositivo, disaccoppiando la capacità di fare la formazione del modello dalla necessità di memorizzare i dati nel cloud.Proponiamo l'algoritmo Federated matched averaging (FedMA) progettato per l'apprendimento federato delle moderne architetture di rete neurale, ad esempio reti neurali convoluzionali (CNN) e LSTM. FedMA costruisce il modello globale condiviso in modo stratificato abbinando e mediando gli elementi nascosti (cioè i canali per gli strati di convoluzione; gli stati nascosti per le LSTM; i neuroni per gli strati completamente connessi) con firme di estrazione di caratteristiche simili. I nostri esperimenti indicano che FedMA supera gli algoritmi di apprendimento federato allo stato dell'arte su CNN profonde e architetture LSTM addestrate su set di dati del mondo reale, mentre migliora l'efficienza della comunicazione.
Presentiamo SOSELETO (SOurce SELEction for Target Optimization), un nuovo metodo per sfruttare un dataset sorgente per risolvere un problema di classificazione su un dataset target.  SOSELETO si basa sulla seguente semplice intuizione: alcuni esempi sorgente sono più informativi di altri per il problema target.  Per catturare questa intuizione, ai campioni di origine vengono dati dei pesi; questi pesi vengono risolti insieme ai problemi di classificazione di origine e di destinazione attraverso uno schema di ottimizzazione a due livelli.  L'obiettivo quindi sceglie i campioni sorgente che sono più informativi per il suo compito di classificazione.  Inoltre, la natura bilivello dell'ottimizzazione agisce come una sorta di regolarizzazione sul target, mitigando l'overfitting.  SOSELETO può essere applicato sia al classico apprendimento di trasferimento, sia al problema dell'addestramento su set di dati con etichette rumorose; mostriamo risultati allo stato dell'arte su entrambi i problemi.
L'ottimizzazione senza derivate (DFO) usando metodi di regioni di fiducia è frequentemente usata per applicazioni di apprendimento automatico, come l'ottimizzazione di (iper-)parametri senza le derivate delle funzioni obiettivo conosciute.  Ispirato dal recente lavoro sui minimizzatori a tempo continuo, il nostro lavoro modella i comuni metodi trust region con l'esplorazione-sfruttamento usando un sistema dinamico che accoppia una coppia di processi dinamici. Mentre il primo processo di esplorazione cerca il minimo della funzione blackbox attraverso la minimizzazione di una funzione di surrogazione che evolve nel tempo, un altro processo di sfruttamento aggiorna la funzione di surrogazione time-to-time usando i punti attraversati dal processo di esplorazione.L'efficienza dell'ottimizzazione senza derivate dipende quindi da come i due processi si accoppiano. In questo articolo, proponiamo un nuovo sistema dinamico, vale a dire \ThePrev---\underline{S}tocastico \underline{H}amiltoniano \underline{E}xploration e \underline{E}xploitation, che surroga le sottoregioni della funzione blackbox usando una funzione quadratica che evolve nel tempo, poi esplora e traccia il minimo delle funzioni quadratiche usando un sistema hamiltoniano che converge velocemente. Per accelerare ulteriormente l'ottimizzazione, presentiamo "The Name" che parallelizza più thread di "The Prev" per l'esplorazione e lo sfruttamento simultanei. I risultati degli esperimenti basati su una vasta gamma di applicazioni di apprendimento automatico mostrano che "The Name" supera una gamma di algoritmi di ottimizzazione senza derivati con una maggiore velocità di convergenza sotto le stesse impostazioni.
 Le reti neurali binarizzate (BNN) hanno dimostrato di essere efficaci nel migliorare l'efficienza della rete durante la fase di inferenza, dopo che la rete è stata addestrata.Tuttavia, le BNN binarizzano solo i parametri del modello e le attivazioni durante le propagazioni.Pertanto, le BNN non offrono miglioramenti significativi dell'efficienza durante la formazione, poiché i gradienti sono ancora propagati e utilizzati con alta precisione.   Dimostriamo che non c'è alcuna difficoltà intrinseca nell'addestramento dei BNN usando la "BackPropagation binarizzata" (BBP), in cui binarizziamo anche i gradienti.Per evitare una degradazione significativa nell'accuratezza del test, aumentiamo semplicemente il numero di mappe di filtro in ogni strato di convoluzione.L'uso di BBP su hardware dedicato può potenzialmente migliorare significativamente l'efficienza di esecuzione (es. g.}, ridurre l'ingombro della memoria dinamica, la larghezza di banda della memoria e l'energia computazionale) e accelerare il processo di formazione con un adeguato supporto hardware, anche dopo un tale aumento della dimensione della rete.Inoltre, il nostro metodo è ideale per l'apprendimento distribuito in quanto riduce i costi di comunicazione in modo significativo (ad es, Utilizzando questo metodo, dimostriamo una perdita minima nell'accuratezza di classificazione su diversi set di dati e topologie.
L'inizializzazione dei pesi e la funzione di attivazione delle reti neurali profonde hanno un impatto cruciale sulle prestazioni della procedura di formazione. Una selezione inappropriata può portare alla perdita di informazioni dell'input durante la propagazione in avanti e alla scomparsa/esplosione esponenziale dei gradienti durante la retropropagazione. Comprendere le proprietà teoriche delle reti casuali non addestrate è la chiave per identificare quali reti profonde possono essere addestrate con successo, come recentemente dimostrato da Schoenholz et al. (2017) che hanno dimostrato che per le reti neurali feedforward profonde solo una scelta specifica di iperparametri nota come `edge of chaos' può portare a buone prestazioni. Completiamo questa analisi fornendo risultati quantitativi che dimostrano che, per una classe di funzioni di attivazione simili a ReLU, l'informazione si propaga effettivamente più in profondità per un'inizializzazione al margine del caos.estendendo ulteriormente questa analisi, identifichiamo una classe di funzioni di attivazione che migliorano la propagazione dell'informazione rispetto alle funzioni simili a ReLU. Questa classe include l'attivazione Swish, $\phi_{swish}(x) = x \cdot \testo{sigmoide}(x)$, usata in Hendrycks & Gimpel (2016),Elfwing et al. (2017) e Ramachandran et al. (2017).Questo fornisce una base teorica per le eccellenti prestazioni empiriche di $\phi_{swish}$ osservate in questi contributi.Noi completiamo questi risultati precedenti illustrando il beneficio di usare un'inizializzazione casuale sul bordo del caos in questo contesto.
La forza trainante del recente successo delle LSTM è stata la loro capacità di apprendere relazioni complesse e non lineari.Di conseguenza, la nostra incapacità di descrivere queste relazioni ha portato a caratterizzare le LSTM come scatole nere.A tal fine, introduciamo la decomposizione contestuale (CD), un algoritmo di interpretazione per analizzare le singole previsioni fatte dalle LSTM standard, senza alcuna modifica al modello sottostante. Decomponendo l'output di un LSTM, CD cattura i contributi delle combinazioni di parole o variabili alla previsione finale di un LSTM. Sul compito di analisi del sentimento con i set di dati Yelp e SST, dimostriamo che CD è in grado di identificare in modo affidabile parole e frasi di sentimento contrastante, e come sono combinate per produrre la previsione finale del LSTM.Utilizzando le etichette a livello di frase in SST, dimostriamo anche che CD è in grado di estrarre con successo le negazioni positive e negative da un LSTM, qualcosa che non è stato fatto in precedenza.
La maggior parte dei modelli basati sull'apprendimento profondo per il miglioramento del discorso si sono concentrati principalmente sulla stima della magnitudine dello spettrogramma, riutilizzando la fase del discorso rumoroso per la ricostruzione, a causa della difficoltà di stimare la fase del discorso pulito. In primo luogo, proponiamo Deep Complex U-Net, un modello strutturato U-Net avanzato che incorpora blocchi di costruzione a valore complesso ben definiti per gestire gli spettrogrammi a valore complesso; in secondo luogo, proponiamo un metodo di mascheramento a valore complesso con coordinate polari per riflettere la distribuzione delle maschere a rapporto ideale complesso. Il nostro modello è stato valutato su una miscela del corpus Voice Bank e del database DEMAND, che è stato ampiamente utilizzato da molti modelli di deep learning per il miglioramento del parlato. Gli esperimenti di ablazione sono stati condotti sul set di dati misto mostrando che tutti e tre gli approcci proposti sono empiricamente validi.
Tutti gli organismi viventi lottano contro le forze della natura per ritagliarsi delle nicchie dove possono mantenere una relativa stasi.Proponiamo che tale ricerca di ordine in mezzo al caos possa offrire un principio unificante per l'emergere di comportamenti utili negli agenti artificiali.Formalizziamo questa idea in un metodo di apprendimento di rinforzo non supervisionato chiamato RL minimizzante la sorpresa (SMiRL). SMiRL addestra un agente con l'obiettivo di massimizzare la probabilità degli stati osservati sotto un modello addestrato su tutti gli stati visti in precedenza. Gli agenti risultanti acquisiscono diversi comportamenti proattivi per cercare e mantenere stati stabili come il bilanciamento e l'evitare i danni, che sono strettamente legati alle possibilità dell'ambiente e alle sue fonti prevalenti di entropia, come venti, terremoti e altri agenti. Dimostriamo che i nostri agenti che minimizzano la sorpresa possono giocare con successo a Tetris, Doom e controllare un umanoide per evitare le cadute, senza alcuna supervisione di ricompensa specifica del compito.  Dimostriamo inoltre che SMiRL può essere usato come un obiettivo di pre-addestramento non supervisionato che accelera sostanzialmente il successivo apprendimento guidato dalla ricompensa.
Un approccio popolare al meta-apprendimento consiste nell'addestrare un modello ricorrente per leggere un set di dati di formazione come input e produrre i parametri di un modello appreso, o previsioni di output per nuovi input di prova. In alternativa, un approccio più recente al meta-apprendimento mira ad acquisire rappresentazioni profonde che possono essere efficacemente messe a punto, attraverso la discesa del gradiente standard, per nuovi compiti. In questo articolo, consideriamo il problema del meta-apprendimento dal punto di vista dell'universalità, formalizzando la nozione di approssimazione dell'algoritmo di apprendimento e confrontando la potenza espressiva dei suddetti modelli ricorrenti con gli approcci più recenti che incorporano la discesa del gradiente nel meta-apprendente. In particolare, cerchiamo di rispondere alla seguente domanda: la rappresentazione profonda combinata con la discesa del gradiente standard ha una capacità sufficiente per approssimare qualsiasi algoritmo di apprendimento? Troviamo che questo è effettivamente vero, e inoltre troviamo, nei nostri esperimenti, che il meta-apprendimento basato sul gradiente porta costantemente a strategie di apprendimento che generalizzano più ampiamente rispetto a quelle rappresentate dai modelli ricorrenti.
Le interfacce cervello-computer (BCI) possono aiutare i pazienti con capacità di comunicazione vacillante a causa di malattie neurodegenerative a produrre testo o discorso attraverso l'elaborazione neurale diretta. Tuttavia, la loro realizzazione pratica si è dimostrata difficile a causa delle limitazioni di velocità, precisione e generalizzabilità delle interfacce esistenti. a tal fine, miriamo a creare un BCI che decodifica il testo direttamente dai segnali neurali. Queste bande formano un set di caratteristiche che alimenta un LSTM che discerne ad ogni punto temporale le distribuzioni di probabilità su tutti i fonemi pronunciati da un soggetto. Infine, un algoritmo di filtraggio a particelle smussa temporalmente queste probabilità incorporando la conoscenza preliminare della lingua inglese per produrre un testo corrispondente alla parola decodificata. Inoltre, nel produrre un output, ci asteniamo dal vincolare la parola ricostruita per essere da un dato bag-of-words, a differenza degli studi precedenti. Il successo empirico del nostro approccio proposto, offre la promessa per l'impiego di una tale interfaccia da parte dei pazienti in ambienti naturali e senza restrizioni.
L'apprendimento di trasferimento per l'estrazione delle caratteristiche può essere utilizzato per sfruttare le rappresentazioni profonde in contesti in cui ci sono pochi dati di formazione, dove ci sono risorse computazionali limitate, o quando la sintonizzazione degli iper-parametri necessari per la formazione non è un'opzione.Mentre i contributi precedenti all'estrazione delle caratteristiche propongono incorporazioni basate su un singolo strato della rete, in questo articolo proponiamo un'incorporazione full-network che integra con successo le caratteristiche convoluzionali e completamente connesse, provenienti da tutti gli strati di una rete neurale convoluzionale profonda. Per fare ciò, l'incorporazione normalizza le caratteristiche nel contesto del problema, e discretizza i loro valori per ridurre il rumore e regolarizzare lo spazio di incorporazione. Il metodo proposto ha dimostrato di superare le prestazioni delle incorporazioni a singolo strato su diversi compiti di classificazione delle immagini, ed è anche più robusto rispetto alla scelta del modello pre-addestrato utilizzato per ottenere le caratteristiche iniziali. Il divario di prestazioni nell'accuratezza della classificazione tra le soluzioni accuratamente sintonizzate e l'incorporamento a rete completa è anche ridotto, il che rende l'approccio proposto una soluzione competitiva per un ampio set di applicazioni.
Presentiamo un nuovo algoritmo di potatura della rete chiamato Dynamic Sparse Training che può congiuntamente trovare i parametri di rete ottimali e la struttura della rete rada in un processo di ottimizzazione univoco con soglie di potatura allenabili. Queste soglie possono avere aggiustamenti a livello di grana fine dinamicamente tramite backpropagation. L'addestramento sparso dinamico raggiunge prestazioni da primato rispetto ad altri algoritmi di addestramento sparso su varie architetture di rete.Inoltre, abbiamo diverse osservazioni sorprendenti che forniscono una forte prova dell'efficacia e dell'efficienza del nostro algoritmo.Queste osservazioni rivelano i problemi di fondo dei tradizionali algoritmi di pruning a tre stadi e presentano la potenziale guida fornita dal nostro algoritmo alla progettazione di architetture di rete più compatte.
In che misura l'apprendimento automatico di successo può informare la nostra comprensione dell'apprendimento biologico? Una strada popolare di indagine negli ultimi anni è stata quella di mappare direttamente tali algoritmi in un'implementazione realistica del circuito.Qui ci concentriamo sull'apprendimento nelle reti ricorrenti e indaghiamo una serie di algoritmi di apprendimento.Il nostro approccio li scompone nei loro blocchi di costruzione computazionali e discute il loro potenziale astratto come operazioni biologiche.Questa strategia alternativa fornisce un modo "pigro" ma di principio di valutare le idee di ML in termini di loro plausibilità biologica
Negli ultimi anni sono stati proposti diversi attacchi e difese avversarie.Spesso i modelli apparentemente robusti si rivelano non robusti quando vengono utilizzati attacchi più sofisticati.Una via d'uscita da questo dilemma sono le garanzie di robustezza dimostrabili.Mentre sono stati sviluppati modelli provabilmente robusti per specifici modelli di $l_p$-perturbazione, mostriamo che non sono dotati di alcuna garanzia contro altre $l_q$-perturbazioni. Proponiamo un nuovo schema di regolarizzazione, MMR-Universal, per le reti ReLU che rafforza la robustezza rispetto alle perturbazioni $l_1$- \textit{ e} $l_\infty$ e mostriamo come questo porti ai primi modelli provatamente robusti rispetto a qualsiasi $l_p$-norm per $p\geq 1$.
Il problema di controllo proposto contiene una dinamica di restauro che è modellata da una RNN e il punto finale mobile, che è essenzialmente il tempo terminale della dinamica associata, è determinato da una rete di policy. Chiamiamo il modello proposto "dynamically unfolding recurrent restorer" (DURR).Gli esperimenti numerici dimostrano che DURR è in grado di raggiungere prestazioni allo stato dell'arte su blind image denoising e JPEG image deblocking.Inoltre, DURR può generalizzarsi bene a immagini con livelli di degradazione più elevati che non sono inclusi nella fase di formazione.
La distanza di Wasserstein ha ricevuto molta attenzione di recente nella comunità dell'apprendimento automatico, soprattutto per il suo modo principesco di confrontare le distribuzioni, e ha trovato numerose applicazioni in diversi problemi difficili, come l'adattamento al dominio, la riduzione della dimensionalità o i modelli generativi, ma il suo uso è ancora limitato da un pesante costo computazionale. Dimostriamo che tale incorporazione può essere trovata con un'architettura siamese associata a una rete di decodifica che permette di passare dallo spazio di incorporazione allo spazio di input originale.Una volta trovata questa incorporazione, il calcolo dei problemi di ottimizzazione nello spazio di Wasserstein (ad esempio, baricentri, direzioni principali o anche archetipi) può essere condotto in modo estremamente veloce.Esperimenti numerici a sostegno di questa idea sono condotti su dataset di immagini, e mostrano i grandi vantaggi potenziali del nostro metodo.
Continuous Bag of Words (CBOW) è un potente metodo di embedding del testo.Grazie alle sue forti capacità di codificare il contenuto delle parole, le embeddings CBOW si comportano bene in una vasta gamma di compiti a valle e sono efficienti da calcolare.Tuttavia, CBOW non è in grado di catturare l'ordine delle parole.La ragione è che il calcolo delle embeddings delle parole di CBOW è commutativo, cioè, Al fine di affrontare questa lacuna, proponiamo un algoritmo di apprendimento per il modello dello spazio matriciale continuo, che chiamiamo Continual Multiplication of Words (CMOW). Il nostro algoritmo è un adattamento di word2vec, in modo che possa essere addestrato su grandi quantità di testo non etichettato.Mostriamo empiricamente che CMOW cattura meglio le proprietà linguistiche, ma è inferiore a CBOW nella memorizzazione del contenuto delle parole. I nostri risultati mostrano che il modello ibrido CBOW-CMOW mantiene la forte capacità di CBOW di memorizzare il contenuto delle parole e allo stesso tempo migliora sostanzialmente la sua capacità di codificare altre informazioni linguistiche dell'8%. Di conseguenza, l'ibrido ha anche prestazioni migliori su 8 degli 11 compiti a valle supervisionati con un miglioramento medio dell'1,2%.
Questo articolo propone Metagross (Meta Gated Recursive Controller), una nuova unità di modellazione di sequenze neurali, I meccanismi di gating di Metagross sono controllati da istanze di se stesso, che sono ripetutamente chiamate in modo ricorsivo. Questo può essere interpretato come una forma di meta-gating e parametrizzazione ricorsiva di un modello ricorrente, A tal fine, conduciamo ampi esperimenti su compiti logici ricorsivi (ordinamento, attraversamento di alberi, inferenza logica), classificazione sequenziale pixel per pixel, parsing semantico, generazione di codice, traduzione automatica e modellazione di musica polifonica, dimostrando l'utilità diffusa dell'approccio proposto, cioè, raggiungendo lo stato dell'arte (o quasi) delle prestazioni su tutti i compiti.
Quale modello generativo è il più adatto per l'Apprendimento Continuo? Questo articolo ha lo scopo di valutare e confrontare i modelli generativi su compiti di generazione di immagini sequenziali disgiunti.Indaghiamo come diversi modelli imparano e dimenticano, considerando varie strategie: prove, regolarizzazione, replay generativo e fine-tuning.Abbiamo usato due metriche quantitative per stimare la qualità della generazione e la capacità di memoria. Abbiamo sperimentato compiti sequenziali su tre benchmark comunemente usati per l'Apprendimento Continuo (MNIST, Fashion MNIST e CIFAR10) e abbiamo scoperto che tra tutti i modelli, la GAN originale si comporta meglio e tra le strategie di Apprendimento Continuo, il replay generativo supera tutti gli altri metodi. Anche se abbiamo trovato combinazioni soddisfacenti su MNIST e Fashion MNIST, l'addestramento di modelli generativi in sequenza su CIFAR10 è particolarmente instabile e rimane una sfida.
Proponiamo una nuova metodologia efficiente dal punto di vista dei campioni, chiamata Supervised Policy Update (SPU), per il deep reinforcement learning.Partendo dai dati generati dalla policy corrente, SPU formula e risolve un problema di ottimizzazione vincolata nello spazio di policy prossimale non parametrizzato.Usando la regressione supervisionata, converte poi la policy ottimale non parametrizzata in una policy parametrizzata, dalla quale estrae nuovi campioni. La metodologia è generale in quanto si applica a spazi d'azione sia discreti che continui, e può gestire un'ampia varietà di vincoli di prossimità per il problema di ottimizzazione non parametrizzato. Mostriamo come i problemi Natural Policy Gradient e Trust Region Policy Optimization (NPG/TRPO), e il problema Proximal Policy Optimization (PPO) possono essere affrontati da questa metodologia. L'implementazione di SPU è molto più semplice di TRPO.In termini di efficienza del campione, i nostri ampi esperimenti mostrano che SPU supera TRPO nei compiti robotici simulati Mujoco e supera PPO nei compiti dei videogiochi Atari.
Un lavoro recente sulla modellazione delle risposte neurali nel sistema visivo dei primati ha beneficiato di reti neurali profonde addestrate al riconoscimento di oggetti su larga scala, e ha trovato una corrispondenza gerarchica tra gli strati della rete neurale artificiale e le aree cerebrali lungo il flusso visivo ventrale. Tuttavia, non sappiamo se tali reti ottimizzate per il compito consentono modelli altrettanto buoni del sistema visivo dei roditori, né se esiste una corrispondenza gerarchica simile. Qui, affrontiamo queste domande nel sistema visivo del topo estraendo le caratteristiche a diversi strati di una rete neurale convoluzionale (CNN) addestrata su ImageNet per prevedere le risposte di migliaia di neuroni in quattro aree visive (V1, LM, AL, RL) alle immagini naturali. Abbiamo scoperto che le caratteristiche della CNN superano i modelli classici di energia delle subunità, ma non abbiamo trovato alcuna prova di un ordine delle aree che abbiamo registrato attraverso una corrispondenza con la gerarchia degli strati della CNN.Inoltre, la stessa CNN ma con pesi casuali ha fornito uno spazio di caratteristiche equivalentemente utile per prevedere le risposte neurali. I nostri risultati suggeriscono che il riconoscimento dell'oggetto come compito di alto livello non fornisce caratteristiche più discriminanti per caratterizzare il sistema visivo del topo che una rete casuale.A differenza del primate, l'addestramento sui comportamenti visivamente guidati etologicamente rilevanti - oltre il riconoscimento statico dell'oggetto - può essere necessario per svelare l'organizzazione funzionale della corteccia visiva del topo.
Il trucco di reparameterizzazione è diventato uno degli strumenti più utili nel campo dell'inferenza variazionale.Tuttavia, il trucco di reparameterizzazione è basato sulla trasformazione di standardizzazione che limita il campo di applicazione di questo metodo alle distribuzioni che hanno funzioni di distribuzione cumulative inverse trattabili o sono esprimibili come trasformazioni deterministiche di tali distribuzioni.In questo articolo, abbiamo generalizzato il trucco di reparameterizzazione permettendo una trasformazione generale. Scopriamo che il modello proposto è un caso speciale di variazione di controllo che indica che il modello proposto può combinare i vantaggi della CV e della riparametrizzazione generalizzata.Sulla base del modello di gradiente proposto, proponiamo un nuovo stimatore di gradiente basato su polinomi che ha una performance teorica migliore del trucco di riparametrizzazione in determinate condizioni e può essere applicato a una classe più ampia di distribuzioni variazionali.Negli studi di dati sintetici e reali, dimostriamo che il nostro stimatore di gradiente proposto ha una varianza di gradiente significativamente inferiore rispetto ad altri metodi all'avanguardia, consentendo così una procedura di inferenza più veloce.
Per catturare simultaneamente la sintassi e la semantica da un corpus di testo, proponiamo un nuovo modello linguistico a contesto più ampio che estrae la struttura semantica gerarchica ricorrente tramite un modello di argomento profondo dinamico per guidare la generazione del linguaggio naturale. Andando oltre un modello linguistico convenzionale che ignora le dipendenze a lungo raggio delle parole e l'ordine delle frasi, il modello proposto cattura non solo le dipendenze intra-sentenza delle parole, ma anche le transizioni temporali tra le frasi e le dipendenze di argomento inter-sentenza. I risultati sperimentali su una varietà di corpora di testo del mondo reale dimostrano che il modello proposto non solo supera lo stato dell'arte dei modelli linguistici a contesto più ampio, ma impara anche argomenti ricorrenti multistrato interpretabili e genera frasi e paragrafi diversi che sono sintatticamente corretti e semanticamente coerenti.
Presentiamo un nuovo approccio per addestrare una pittura di media naturali usando il reinforcement learning.Data un'immagine di riferimento, la nostra formulazione si basa sul rendering basato sul tratto che imita il disegno umano e può essere appreso da zero senza supervisione.Il nostro agente di pittura calcola una sequenza di azioni che rappresentano i tratti primitivi della pittura. Al fine di garantire che la politica generata sia prevedibile e controllabile, usiamo un metodo di apprendimento vincolato e addestriamo l'agente di pittura utilizzando il modello dell'ambiente e segue i comandi codificati in un'osservazione.Abbiamo applicato il nostro approccio su molti benchmark e i nostri risultati dimostrano che il nostro agente vincolato può gestire diversi media di pittura e diversi vincoli nello spazio di azione per collaborare con gli esseri umani o altri agenti.
La distorsione delirante è una fonte fondamentale di errore nell'apprendimento approssimativo di Q. Ad oggi, le uniche tecniche che affrontano esplicitamente la delusione richiedono una ricerca completa utilizzando stime di valore tabulari. In questo articolo, sviluppiamo metodi efficienti per mitigare la distorsione delirante addestrando Q-approssimatori con etichette che sono "coerenti" con la classe di politica avida sottostante. Introduciamo un semplice schema di penalizzazione che incoraggia le etichette Q utilizzate attraverso i lotti di formazione per rimanere (congiuntamente) coerenti con la classe di politica esprimibile.Proponiamo anche un quadro di ricerca che permette di generare e tracciare più Q-approssimatori, mitigando così l'effetto degli impegni prematuri (impliciti) di politica.I risultati sperimentali dimostrano che questi metodi possono migliorare le prestazioni di Q-learning in una varietà di giochi Atari, a volte in modo drammatico.
L'articolo propone e dimostra un'architettura Deep Convolutional Neural Network (DCNN) per identificare gli utenti con il volto mascherato che tentano una transazione fraudolenta al bancomat.La recente introduzione del framework Disguised Face Identification (DFI) dimostra l'applicabilità delle reti neurali profonde proprio per questo problema.Tutti i bancomat oggi incorporano una telecamera nascosta e catturano i filmati dei loro utenti.Tuttavia, è impossibile per la polizia rintracciare gli impersonatori con volti mascherati dai filmati del bancomat. La rete neurale convoluzionale profonda proposta è addestrata per identificare, in tempo reale, se l'utente nell'immagine catturata sta cercando di nascondere la sua identità o no. L'output della DCNN è quindi riportato all'ATM per prendere le misure appropriate e impedire al truffatore di completare la transazione. La rete è addestrata utilizzando un set di dati di immagini catturate in situazioni simili a quelle di un ATM.
Quando la sua funzione di attivazione è lineare e la dimensione di codifica (larghezza dello strato nascosto) è più piccola della dimensione dell'input, è ben noto che l'auto-encoder è ottimizzato per imparare le componenti principali della distribuzione dei dati (Oja1982). Tuttavia, quando l'attivazione non è lineare e quando la larghezza è più grande della dimensione dell'input (overcomplete), l'auto-encoder si comporta diversamente dalla PCA, e infatti è noto per avere buone prestazioni empiriche per problemi di codifica sparsa. Forniamo una spiegazione teorica per questo fenomeno empiricamente osservato, quando l'unità rettificata-lineare (ReLu) è adottata come funzione di attivazione e la larghezza dello strato nascosto è impostata per essere grande. In questo caso, mostriamo che, con una probabilità significativa, inizializzando la matrice dei pesi di un auto-encoder campionando da una distribuzione gaussiana sferica seguita da un addestramento di discesa del gradiente stocastico (SGD) converge verso la rappresentazione della verità per una classe di modelli di apprendimento di dizionario sparsi. Inoltre, possiamo dimostrare che, condizionando la convergenza, il tasso di convergenza previsto è O(1/t), dove t è il numero di aggiornamenti. La nostra analisi quantifica come l'aumento della larghezza dello strato nascosto aiuta le prestazioni di formazione quando viene utilizzata l'inizializzazione casuale, e come la norma dei pesi di rete influenza la velocità di convergenza di SGD.
Gli agenti gerarchici hanno il potenziale per risolvere compiti decisionali sequenziali con una maggiore efficienza del campione rispetto alle loro controparti non gerarchiche, perché gli agenti gerarchici possono suddividere i compiti in serie di sottocompiti che richiedono solo brevi sequenze di decisioni.  Per realizzare questo potenziale di apprendimento più veloce, gli agenti gerarchici devono essere in grado di apprendere i loro livelli multipli di politiche in parallelo in modo che questi sottoproblemi più semplici possano essere risolti simultaneamente.  Tuttavia, l'apprendimento di più livelli di politiche in parallelo è difficile perché è intrinsecamente instabile: i cambiamenti in una politica ad un livello della gerarchia possono causare cambiamenti nelle funzioni di transizione e di ricompensa ai livelli superiori della gerarchia, rendendo difficile l'apprendimento congiunto di più livelli di politiche.  In questo articolo, introduciamo una nuova struttura HRL (Hierarchical Reinforcement Learning), Hierarchical Actor-Critic (HAC), che può superare i problemi di instabilità che sorgono quando gli agenti cercano di imparare congiuntamente più livelli di politiche.  L'idea principale dietro HAC è di addestrare ogni livello della gerarchia indipendentemente dai livelli inferiori, addestrando ogni livello come se le politiche del livello inferiore fossero già ottimali.  Dimostriamo sperimentalmente in entrambi i domini del mondo della griglia e della robotica simulata che il nostro approccio può accelerare significativamente l'apprendimento rispetto ad altri metodi non gerarchici e gerarchici.  Infatti, la nostra struttura è la prima a imparare con successo gerarchie a 3 livelli in parallelo in compiti con spazi di stato e di azione continui.
L'apprendimento positivo-non etichettato (PU) affronta il problema dell'apprendimento di un classificatore binario da dati positivi (P) e non etichettati (U) e viene spesso applicato a situazioni in cui i dati negativi (N) sono difficili da etichettare completamente, ma raccogliere un insieme N non rappresentativo che contiene solo una piccola parte di tutti i possibili dati N può essere molto più facile in molte situazioni pratiche. Questo articolo studia un nuovo quadro di classificazione che incorpora tali dati N distorti (bN) nell'apprendimento PU.Il fatto che i dati N di formazione siano distorti rende il nostro lavoro molto diverso da quelli dell'apprendimento semi-supervisionato standard. Il nostro approccio può essere considerato come una variante dei tradizionali algoritmi di ponderazione degli esempi, con il peso di ogni esempio calcolato attraverso un passo preliminare che trae ispirazione dall'apprendimento di PU.Inoltre deriviamo un limite di errore di stima per il metodo proposto.I risultati sperimentali dimostrano l'efficacia del nostro algoritmo non solo in scenari di apprendimento PUbN ma anche in scenari ordinari di PU appoggiati su diversi set di dati di riferimento.
L'apprendimento per rinforzo (RL) è frequentemente utilizzato per aumentare le prestazioni nei compiti di generazione del testo, compresa la traduzione automatica (MT), in particolare attraverso l'uso di Minimum Risk Training (MRT) e Generative Adversarial Networks (GAN). Tuttavia, poco si sa su cosa e come questi metodi apprendono nel contesto della traduzione automatica. Dimostriamo che uno dei metodi di RL più comuni per la traduzione automatica non ottimizza la ricompensa attesa, così come dimostriamo che altri metodi impiegano un tempo eccessivamente lungo per convergere. Infatti, i nostri risultati suggeriscono che le pratiche di RL nella traduzione automatica sono in grado di migliorare le prestazioni solo quando i parametri pre-addestrati sono già vicini a produrre la traduzione corretta; i nostri risultati suggeriscono inoltre che i guadagni osservati possono essere dovuti a effetti non correlati al segnale di addestramento, concretamente, cambiamenti nella forma della curva di distribuzione.
Dall'inizio del 2018 sono stati fatti progressi significativi nella modellazione del Natural Language Processing (NLP).I nuovi approcci consentono di ottenere risultati accurati, anche quando ci sono pochi dati etichettati, perché questi modelli NLP possono beneficiare dell'addestramento su dati non etichettati sia task-agnostici che task-specifici.Tuttavia, questi vantaggi vengono con dimensioni e costi computazionali significativi. Questo documento del workshop delinea come la nostra proposta di architettura convoluzionale per studenti, essendo stata addestrata da un processo di distillazione da un modello su larga scala, può raggiungere un'accelerazione dell'inferenza di 300x e una riduzione di 39x nel conteggio dei parametri.In alcuni casi, la performance del modello per studenti supera il suo insegnante sui compiti studiati.
La combinazione di più approssimazioni di funzioni nei modelli di apprendimento automatico porta tipicamente a migliori prestazioni e robustezza rispetto a una singola funzione.Nell'apprendimento di rinforzo, gli algoritmi di ensemble come un metodo di media e un metodo di voto a maggioranza non sono sempre ottimali, perché ogni funzione può imparare traiettorie ottimali fondamentalmente diverse dall'esplorazione. In questo articolo, proponiamo un algoritmo TDW (Temporal Difference Weighted), un metodo di ensemble che regola i pesi di ogni contributo in base agli errori di differenza temporale accumulati.Il vantaggio di questo algoritmo è che migliora le prestazioni dell'ensemble riducendo i pesi delle funzioni Q non familiari con le traiettorie attuali.Forniamo risultati sperimentali per compiti Gridworld e compiti Atari che mostrano miglioramenti significativi delle prestazioni rispetto agli algoritmi di base.
Questo documento introduce la Behaviour Suite for Reinforcement Learning, o bsuite in breve.bsuite è una raccolta di esperimenti accuratamente progettati che indagano le capacità fondamentali degli agenti di apprendimento per rinforzo (RL) con due obiettivi.Primo, raccogliere problemi chiari, informativi e scalabili che catturano le questioni chiave nella progettazione di algoritmi di apprendimento generali ed efficienti.Secondo, studiare il comportamento degli agenti attraverso le loro prestazioni su questi benchmark condivisi.Per completare questo sforzo, abbiamo open source questo URL http, che automatizza la valutazione e l'analisi di qualsiasi agente su bsuite. Questa libreria facilita la ricerca riproducibile e accessibile sulle questioni fondamentali nella RL, e in definitiva la progettazione di algoritmi di apprendimento superiori.Il nostro codice è Python, e facile da usare all'interno di progetti esistenti.Includiamo esempi con OpenAI Baselines, Dopamine così come nuove implementazioni di riferimento.Andando avanti, speriamo di incorporare più esperimenti eccellenti dalla comunità di ricerca, e ci impegniamo a una revisione periodica di bsuite da un comitato di ricercatori importanti.
I modelli basati sull'attenzione sequenza per sequenza sono un approccio promettente per il riconoscimento vocale end-to-end.L'aumento della potenza del modello rende la procedura di addestramento più difficile, e analizzare le modalità di fallimento di questi modelli diventa più difficile a causa della natura end-to-end.In questo lavoro, presentiamo varie analisi per comprendere meglio l'addestramento e le proprietà del modello. Indaghiamo sulle varianti di preformazione come la crescita in profondità e larghezza, e il loro impatto sulla performance finale, che porta a oltre l'8% di miglioramento relativo nel tasso di errore di parola.Per una migliore comprensione di come funziona il processo di attenzione, studiamo l'uscita del codificatore e le energie di attenzione e i pesi.I nostri esperimenti sono stati eseguiti su Switchboard, LibriSpeech e Wall Street Journal.
La convalida è una sfida chiave nella ricerca di un'autonomia sicura.Le simulazioni sono spesso o troppo semplici per fornire una convalida robusta, o troppo complesse per essere calcolate in modo semplice.Pertanto, sono necessari metodi di convalida approssimativi per trovare in modo semplice i fallimenti senza semplificazioni pericolose.Questo articolo presenta la teoria dietro uno di questi approcci black-box: adaptive stress testing (AST).Forniamo anche tre esempi di problemi di convalida formulati per lavorare con AST.
Le politiche multi-step greedy sono state ampiamente utilizzate nel Reinforcement Learning (RL) basato sul modello e nel caso in cui sia disponibile un modello dell'ambiente (ad es, In questo lavoro, esploriamo i benefici delle politiche greedy multi-step in RL senza modello quando sono impiegate nel quadro della programmazione dinamica (DP) multi-step: multi-step Policy and Value Iteration.Questi algoritmi risolvono iterativamente problemi decisionali a breve orizzonte e convergono alla soluzione ottimale di quella originale. Usando algoritmi senza modello come solutori dei problemi a breve orizzonte, deriviamo algoritmi completamente privi di modello che sono istanze della struttura DP a più passi. Poiché gli algoritmi senza modello sono soggetti a instabilità con l'orizzonte del problema decisionale, questo semplice approccio può aiutare a mitigare queste instabilità e risulta in algoritmi senza modello migliorati.Testiamo questo approccio e mostriamo risultati su problemi di controllo sia discreti che continui.
La procedura di addestramento avversario proposta da Madry et al. (2018) è uno dei metodi più efficaci per difendersi dagli esempi avversari nelle deep neural net- works (DNNs). Nel nostro articolo, gettiamo alcune luci sulla praticità e la durezza dell'addestramento avversario mostrando che l'efficacia (robustezza sul set di test) dell'addestramento avversario ha una forte correlazione con la distanza tra un punto di test e il collettore di dati di allenamento incorporato dalla rete.Gli esempi di test che sono relativamente lontani da questo collettore hanno maggiori probabilità di essere vulnerabili agli attacchi avversari. Di conseguenza, una difesa basata sull'addestramento avversario è suscettibile di una nuova classe di attacchi, il "blind-spot attack", in cui le immagini di input risiedono in "blind-spot" (regioni a bassa densità) della distribuzione empirica dei dati di addestramento, ma è ancora sul collettore dei dati ground-truth.Per MNIST, abbiamo trovato che questi blind-spot possono essere facilmente trovati semplicemente scalando e spostando i valori dei pixel delle immagini. Ancora più importante, per i grandi set di dati con manifold di dati ad alta dimensionalità e complessi (CIFAR, ImageNet, ecc), l'esistenza di blind-spot nell'addestramento avversario rende difficile la difesa su qualsiasi esempio di test valido a causa della maledizione della dimensionalità e della scarsità di dati di formazione.Inoltre, troviamo che i blind-spot esistono anche su difese dimostrabili tra cui (Kolter & Wong, 2018) e (Sinha et al, 2018) perché questi certificati di robustezza allenabili possono essere praticamente ottimizzati solo su un insieme limitato di dati di allenamento.
L'intelligenza dei bordi in particolare la rete neurale binaria (BNN) ha attirato una notevole attenzione della comunità di intelligenza artificiale recentemente.BNNs riduce significativamente il costo computazionale, la dimensione del modello e l'impronta di memoria.  Tuttavia, c'è ancora un divario di prestazioni tra la rete neurale di precisione completa di successo con attivazione ReLU e BNNs.We sostengono che il calo di precisione di BNNs è dovuto alla loro geometria. Analizziamo il comportamento della rete neurale a piena precisione con attivazione ReLU e lo confrontiamo con la sua controparte binarizzata.Questo confronto suggerisce l'inizializzazione di bias casuali come rimedio alla saturazione dell'attivazione nelle reti a piena precisione e ci porta verso una migliore formazione BNN.I nostri esperimenti numerici confermano la nostra intuizione geometrica.
Capire come le persone rappresentano le categorie è un problema centrale nella scienza cognitiva, con la flessibilità dell'apprendimento umano che rimane un gold standard a cui l'intelligenza artificiale moderna e l'apprendimento automatico aspirano. Decenni di ricerca psicologica hanno prodotto una varietà di teorie formali delle categorie, ma la convalida di queste teorie con stimoli naturalistici rimane una sfida: il problema è che le rappresentazioni delle categorie umane non possono essere osservate direttamente ed eseguire esperimenti informativi con stimoli naturalistici come le immagini richiede una rappresentazione praticabile di questi stimoli. Le reti neurali profonde hanno recentemente avuto successo in una serie di compiti di computer vision e forniscono un modo per rappresentare le caratteristiche delle immagini. In questo articolo, introduciamo un metodo per stimare la struttura delle categorie umane che attinge a idee sia dalla scienza cognitiva che dall'apprendimento automatico, fondendo algoritmi basati sull'uomo con apprendisti di rappresentazione profonda allo stato dell'arte.Forniamo risultati qualitativi e quantitativi come prova di concetto della fattibilità del metodo.
L'applicazione di reti ricorrenti profonde alla trascrizione audio ha portato a guadagni impressionanti nei sistemi di riconoscimento automatico del parlato (ASR).Molti hanno dimostrato che piccole perturbazioni avversarie possono ingannare le reti neurali profonde nel predire erroneamente un obiettivo specificato con alta fiducia. Gli attuali lavori per ingannare i sistemi ASR si sono concentrati su attacchi white-box, in cui l'architettura del modello e i parametri sono noti.In questo articolo, adottiamo un approccio black-box alla generazione di avversari, combinando gli approcci di entrambi gli algoritmi genetici e la stima del gradiente per risolvere il compito.Raggiungiamo una somiglianza dell'attacco mirato dell'89,25% dopo 3000 generazioni, mantenendo il 94,6% di somiglianza del file audio.
I recenti progressi nell'animazione dei personaggi basata sulla fisica hanno mostrato progressi impressionanti nella sintesi del movimento umano, attraverso l'imitazione dei dati di cattura del movimento tramite l'apprendimento di rinforzo profondo. Tuttavia, i risultati sono stati per lo più dimostrati sull'imitazione di un singolo modello di movimento distinto, e non si generalizzano ai compiti interattivi che richiedono modelli di movimento flessibili dovuti a configurazioni spaziali variabili dell'uomo-oggetto. Proponiamo una struttura gerarchica di apprendimento di rinforzo che si basa su un insieme di subtask controller addestrati a imitare semplici, riutilizzabili movimenti mocap, e un meta controller addestrato a eseguire correttamente i subtask per completare il compito principale.Dimostriamo sperimentalmente la forza del nostro approccio su diversi livelli singoli e baseline gerarchiche.Mostriamo anche che il nostro approccio può essere applicato alla previsione del movimento dato un input di immagine.Un highlight video può essere trovato a https://youtu.be/XWU3wzz1ip8/.
Proponiamo di studiare le dinamiche di formazione GAN come minimizzazione del rimpianto, che è in contrasto con la visione popolare che c'è la minimizzazione coerente di una divergenza tra le distribuzioni reali e generate.Analizziamo la convergenza della formazione GAN da questo nuovo punto di vista per capire perché il collasso della modalità accade.Ipotizziamo l'esistenza di equilibri locali indesiderati in questo gioco non-convesso per essere responsabile del collasso della modalità. Dimostriamo che questi equilibri locali spesso esibiscono gradienti acuti della funzione discriminante intorno ad alcuni punti dati reali e che questi equilibri locali degenerati possono essere evitati con uno schema di penalizzazione del gradiente chiamato DRAGAN. Mostriamo che DRAGAN permette una formazione più veloce, raggiunge una migliore stabilità con meno collassi di modalità e porta a reti generatrici con migliori prestazioni di modellazione attraverso una varietà di architetture e funzioni obiettivo.
Le reti neurali profonde (DNN) hanno raggiunto risultati sorprendenti nell'ultimo decennio grazie ai vantaggi dell'apprendimento automatico delle caratteristiche e della libertà di espressività, ma la loro interpretabilità rimane misteriosa perché le DNN sono combinazioni complesse di trasformazioni lineari e non lineari. Anche se sono stati proposti molti modelli per esplorare l'interpretabilità delle DNN, diverse sfide rimangono irrisolte: 1) la mancanza di misure di quantità di interpretabilità per le DNN, 2) la mancanza di teoria per la stabilità delle DNN, e 3) la difficoltà di risolvere problemi DNN non convessi con vincoli di interpretabilità. Per affrontare queste sfide simultaneamente, questo articolo presenta un nuovo quadro di valutazione dell'interpretabilità intrinseca per le DNN. In particolare, quattro proprietà indipendenti di interpretabilità sono definite sulla base dei lavori esistenti. Inoltre, indaghiamo la teoria della stabilità delle DNN, che è un aspetto importante dell'interpretabilità, e dimostriamo che le DNN sono generalmente stabili date diverse funzioni di attivazione. Infine, viene proposta una versione estesa del metodo di apprendimento profondo a direzione alternata dei moltiplicatori (dlADMM) per risolvere i problemi DNN con vincoli di interpretabilità in modo efficiente e accurato.esperimenti estesi su diversi set di dati di riferimento convalidano diverse DNN con il nostro quadro di interpretabilità proposto.
L'apprendimento per rinforzo (RL) definisce tipicamente un fattore di sconto come parte del processo decisionale di Markov.  Il fattore di sconto valuta le ricompense future con uno schema esponenziale che porta a garanzie teoriche di convergenza dell'equazione di Bellman. Tuttavia, l'evidenza dalla psicologia, dall'economia e dalle neuroscienze suggerisce che gli esseri umani e gli animali hanno invece preferenze temporali iperboliche.  Qui estendiamo il lavoro precedente di Kurth-Nelson e Redish e proponiamo un efficiente agente di apprendimento di rinforzo profondo che agisce attraverso lo sconto iperbolico e altri meccanismi di sconto non esponenziale. Dimostriamo che un semplice approccio approssima le funzioni di sconto iperboliche mentre utilizza ancora le tecniche di apprendimento a differenza temporale familiari nella RL.  Inoltre, e indipendentemente dallo sconto iperbolico, facciamo una scoperta sorprendente che l'apprendimento simultaneo di funzioni di valore su orizzonti temporali multipli è un compito ausiliario efficace che spesso migliora rispetto ai metodi all'avanguardia.
La necessità e l'importanza di un sistema automatico di riconoscimento delle emozioni sta aumentando.Gli approcci tradizionali di riconoscimento delle emozioni si basano su immagini facciali, misurazioni della frequenza cardiaca, pressione sanguigna, temperatura, toni della voce/parlato, ecc.Tuttavia, queste caratteristiche possono potenzialmente essere cambiate in caratteristiche false.Quindi per rilevare le caratteristiche nascoste e reali che non è controllato dalla persona sono dati misurati da segnali cerebrali.Ci sono vari modi di misurare le onde cerebrali: L'obiettivo principale di questo studio è quello di rilevare le emozioni sulla base dell'analisi del segnale EEG registrato dal cervello in risposta a stimoli visivi. Gli approcci utilizzati sono stati gli stimoli visivi selezionati sono stati presentati a 11 soggetti sani e il segnale EEG è stato registrato in una situazione controllata per ridurre al minimo gli artefatti (movimenti muscolari o/e degli occhi).  Il metodo proposto predice un tipo di emozione (positiva/negativa) in risposta agli stimoli presentati.Infine, le prestazioni dell'approccio proposto sono state testate.La precisione media degli algoritmi di apprendimento automatico (cioè J48, Bayes Net, Adaboost e Random Forest) sono 78,86, 74,76, 77,82 e 82,46 rispettivamente.  In questo studio, abbiamo anche applicato le applicazioni EEG nel contesto del neuro-marketing. I risultati hanno dimostrato empiricamente il rilevamento della preferenza del colore preferito dei clienti in risposta al colore del logo di un'organizzazione o servizio.
  Presentiamo un quadro probabilistico per la raccomandazione basata sulla sessione.  Una variabile latente per lo stato dell'utente viene aggiornata man mano che l'utente visualizza più elementi e impariamo di più sui suoi interessi.  Forniamo soluzioni computazionali usando sia il trucco della ri-parametrizzazione che usando il limite di Bouchard per la funzione softmax, esploriamo ulteriormente l'impiego di un auto-encoder variazionale e un algoritmo di Expectation-Maximization variazionale per stringere il limite variazionale.  Infine mostriamo che il vincolo di Bouchard fa sì che il denominatore della softmax si decomponga in una somma che permette di ottenere gradienti rumorosi veloci del vincolo, dando un algoritmo completamente probabilistico che ricorda word2vec e un veloce algoritmo EM online.
Una questione importante nell'apprendimento del trasferimento di compiti è quella di determinare la trasferibilità dei compiti, cioè, dato un dominio di input comune, stimare in che misura le rappresentazioni apprese da un compito di origine possono aiutare nell'apprendimento di un compito di destinazione.Tipicamente, la trasferibilità è misurata sperimentalmente o dedotta attraverso la relatività dei compiti, che è spesso definita senza un chiaro significato operativo.In questo articolo, presentiamo una nuova metrica, H-score, una funzione di valutazione facilmente calcolabile che stima le prestazioni delle rappresentazioni trasferite da un compito all'altro nei problemi di classificazione. Questa formulazione di trasferibilità può essere ulteriormente utilizzata per selezionare un set adeguato di compiti sorgente nei problemi di apprendimento del trasferimento di compiti o per ideare politiche efficienti di apprendimento del trasferimento. Gli esperimenti che utilizzano sia dati di immagini sintetiche che reali mostrano che non solo la nostra formulazione di trasferibilità è significativa nella pratica, ma anche che può generalizzarsi a problemi di inferenza oltre la classificazione, come i compiti di riconoscimento per la comprensione di scene interne 3D.
Questo articolo presenta una struttura generica per affrontare il problema cruciale del mismatch di classe nell'adattamento di dominio non supervisionato (UDA) per distribuzioni multiclasse.  I precedenti metodi di apprendimento avversario condizionano l'allineamento del dominio solo sulle pseudo etichette, ma le pseudo etichette rumorose e imprecise possono perturbare la distribuzione multiclasse incorporata nelle previsioni probabilistiche, portando quindi un'attenuazione insufficiente al problema del mismatch latente.  Rispetto alle pseudo etichette, i prototipi di classe sono più accurati e affidabili poiché riassumono tutte le istanze e sono in grado di rappresentare la distribuzione semantica intrinseca condivisa tra i domini. Pertanto, proponiamo un nuovo schema Prototype-Assisted Adversarial Learning (PAAL), che incorpora le previsioni probabilistiche delle istanze e i prototipi di classe per fornire indicatori affidabili per l'allineamento del dominio avversario.   Con lo schema PAAL, allineiamo sia le rappresentazioni delle caratteristiche dell'istanza che le rappresentazioni dei prototipi di classe per alleviare il mismatch tra classi semanticamente diverse.   Inoltre, sfruttiamo i prototipi di classe come proxy per minimizzare la varianza all'interno della classe nel dominio di destinazione per mitigare il mismatch tra classi semanticamente simili.  Con queste novità, costituiamo un framework Prototype-Assisted Conditional Domain Adaptation (PACDA) che affronta bene il problema del mismatch di classe. Dimostriamo la buona performance e la capacità di generalizzazione dello schema PAAL e anche del framework PACDA su due compiti UDA, cioè il riconoscimento di oggetti (Office-Home,ImageCLEF-DA, eOffice) e la segmentazione semantica sintetica-reale (GTA5→Cityscapes e Synthia→Cityscapes).
Presentiamo una nuova metodologia che costruisce una famiglia di kernel definiti positivi da qualsiasi misura di dissimilarità data su input strutturati i cui elementi sono serie temporali con valore reale o strutture discrete come stringhe, istogrammi e grafici. Il nostro approccio, che chiamiamo D2KE (da Distance to Kernel and Embedding), attinge dalla letteratura delle caratteristiche casuali. Tuttavia, invece di derivare mappe di caratteristiche casuali da un kernel definito dall'utente per approssimare le macchine kernel, costruiamo un kernel da una mappa di caratteristiche casuali, che specifichiamo data la misura della distanza. Proponiamo inoltre l'uso di un numero finito di oggetti casuali per produrre un'incorporazione casuale delle caratteristiche di ogni istanza.Forniamo un'analisi teorica che mostra che D2KE gode di una migliore generalizzabilità rispetto alle stime universali Nearest-Neighbor. Da un lato, D2KE sussume l'ampiamente utilizzato metodo dell'insieme rappresentativo come caso speciale, e si riferisce al ben noto kernel di sostituzione delle distanze in un caso limite. D'altra parte, D2KE generalizza i metodi esistenti \emph{Random Features} applicabili solo a rappresentazioni di input vettoriali a input strutturati complessi di dimensioni variabili. Conduciamo esperimenti di classificazione su domini così disparati come serie temporali, stringhe e istogrammi (per testi e immagini), per i quali la nostra struttura proposta si confronta favorevolmente con i metodi di apprendimento a distanza esistenti sia in termini di accuratezza dei test che di tempo di calcolo.
Le reti neurali a convoluzione profonda (CNN), radicate dal lavoro pionieristico di \cite{Hinton1986,LeCun1985,Alex2012}, e riassunte in \cite{LeCunBengioHinton2015}, hanno dimostrato di essere molto utili in una varietà di campi.  Le macchine CNN allo stato dell'arte, come l'image rest net \cite{He_2016_CVPR}, sono descritte da ingressi a valore reale e convoluzioni kernel seguite da uscite lineari rettificate locali e non lineari.  Comprendere il ruolo di questi strati, la loro precisione e i loro limiti, così come renderli più efficienti (meno parametri) sono tutte domande di ricerca in corso.   Ispirandoci alla teoria quantistica, proponiamo l'uso di funzioni kernel a valore complesso, seguite dall'operatore locale non lineare assoluto (modulo) quadrato. Sosteniamo che un vantaggio dei kernel complessi ispirati alla teoria quantistica è la robustezza a scenari realistici imprevedibili (come il rumore del clutter, le deformazioni dei dati). Studiamo un problema concreto di rilevamento della forma e dimostriamo che quando più forme sovrapposte sono deformate e/o viene aggiunto del rumore di disturbo, uno strato di convoluzione con kernel complessi ispirati ai quanti supera la controparte statistica/classica del kernel e uno "stimatore di forma bayesiano". La performance superiore è dovuta ai fenomeni quantistici di interferenza, non presenti nelle CNN classiche.  
Presentiamo una piattaforma di ricerca di intelligenza artificiale ispirata al genere di gioco umano dei MMORPG (Massively Multiplayer Online Role-Playing Games, a.k.a. MMOs) e dimostriamo come questa piattaforma possa essere usata per studiare il comportamento e l'apprendimento in grandi popolazioni di agenti neurali. L'emergere della vita complessa sulla Terra è spesso attribuito alla corsa agli armamenti che è derivata da un enorme numero di organismi che competono per risorse finite. La nostra piattaforma mira a simulare questa impostazione nel microcosmo: conduciamo una serie di esperimenti per testare come la concorrenza su larga scala di più agenti può incentivare lo sviluppo di un comportamento abile e troviamo che la dimensione della popolazione ingrandisce la complessità dei comportamenti che emergono e risulta in agenti che superano gli agenti addestrati in popolazioni più piccole.
In questo articolo, proponiamo un quadro migliorato di valutazione quantitativa per Generative Adversarial Networks (GANs) sulla generazione di immagini specifiche del dominio, dove miglioriamo i metodi di valutazione convenzionali su due livelli: la rappresentazione delle caratteristiche e la metrica di valutazione.A differenza della maggior parte dei quadri di valutazione esistenti che trasferiscono la rappresentazione del modello di inizio di ImageNet per mappare le immagini nello spazio delle caratteristiche, il nostro quadro utilizza un codificatore specializzato per acquisire la rappresentazione specifica del dominio a grana fine. Inoltre, per i set di dati con più classi, proponiamo Class-Aware Frechet Distance (CAFD), che impiega un modello di miscela gaussiana sullo spazio delle caratteristiche per adattarsi meglio alla distribuzione delle caratteristiche multi-manifold.Experiments e analisi sia a livello di caratteristica che di immagine sono stati condotti per dimostrare i miglioramenti del nostro quadro proposto rispetto al metodo FID recentemente proposto allo stato dell'arte. Per quanto ne sappiamo, siamo i primi a fornire contro esempi in cui FID dà risultati incoerenti con i giudizi umani.È dimostrato negli esperimenti che la nostra struttura è in grado di superare la brevità di FID e migliora la robustezza.Il codice sarà reso disponibile.
I recenti sforzi sull'addestramento di reti neurali binarie leggere offrono una promettente efficienza di esecuzione/memoria.Questo articolo introduce ResBinNet, che è una composizione di due metodologie interconnesse che mirano ad affrontare la lenta velocità di convergenza e la limitata accuratezza delle reti neurali convoluzionali binarie.Il primo metodo, chiamato binarizzazione residua, impara una rappresentazione binaria multilivello per le caratteristiche all'interno di un certo strato della rete neurale. Il secondo metodo, chiamato regolazione della temperatura, binarizza gradualmente i pesi di un particolare strato.I due metodi imparano congiuntamente un insieme di parametri soft-binarizzati che migliorano il tasso di convergenza e l'accuratezza delle reti neurali binarie.Confermiamo l'applicabilità e la scalabilità di ResBinNet implementando un prototipo di acceleratore hardware.L'acceleratore è riconfigurabile in termini di precisione numerica delle caratteristiche binarizzate, offrendo un compromesso tra tempo di esecuzione e precisione di inferenza.
Nelle applicazioni di apprendimento automatico del mondo reale, i grandi outlier e il rumore pervasivo sono comuni, e l'accesso a dati di formazione puliti, come richiesto dagli autoencoder profondi standard, è improbabile.Rilevare in modo affidabile le anomalie in un dato set di immagini è un compito di grande rilevanza pratica per l'ispezione della qualità visiva, la sorveglianza o l'analisi delle immagini mediche. Le reti neurali autoencoder imparano a ricostruire le immagini normali, e quindi possono classificare quelle immagini come anomale se l'errore di ricostruzione supera una certa soglia.In questo articolo, abbiamo proposto un metodo non supervisionato basato sulla scansione di sottoinsiemi di attivazioni autoencoder. I contributi del nostro lavoro sono triplici: in primo luogo, proponiamo un nuovo metodo che combina la rilevazione con i punteggi dell'errore di ricostruzione e della scansione dei sottoinsiemi per migliorare il punteggio di anomalia degli attuali autoencoder senza richiedere alcuna riqualificazione; in secondo luogo, forniamo la possibilità di ispezionare e visualizzare l'insieme dei nodi anomali nello spazio dell'errore di ricostruzione che rendono un campione rumoroso; in terzo luogo, dimostriamo che la scansione dei sottoinsiemi può essere utilizzata per la rilevazione delle anomalie negli strati interni dell'autoencoder; forniamo risultati di potenza di rilevazione per diversi modelli di rumore avversario non mirati in serie di dati standard.
I KGEs convenzionali spesso soffrono di una rappresentazione limitata della conoscenza, che causa meno accuratezza specialmente quando l'addestramento sui grafi di conoscenza sparsi. Per rimediare a questo, presentiamo Pretrain-KGEs, una struttura di addestramento per l'apprendimento delle entità e delle relazioni di conoscenza migliori, facendo leva sull'abbondante conoscenza linguistica dai modelli di lingua preaddestrati. Nello specifico, proponiamo un approccio unificato in cui prima impariamo rappresentazioni di entità e relazioni attraverso modelli linguistici preaddestrati e usiamo le rappresentazioni per inizializzare le embeddings di entità e relazioni per l'addestramento dei modelli KGE. Il nostro metodo proposto è agnostico nel senso che può essere applicato a qualsiasi variante di modelli KGE. I risultati sperimentali mostrano che il nostro metodo può migliorare costantemente i risultati e raggiungere lo stato dell'arte delle prestazioni utilizzando diversi modelli KGE come TransE e QuatE, attraverso quattro dataset KG di riferimento nella previsione di link e compiti di classificazione di terzine.
Descriviamo un nuovo modo di rappresentare una base di conoscenza simbolica (KB) chiamata una sparse-matrix reified KB.  Questa rappresentazione consente ai moduli neurali di essere completamente differenziabili, fedeli alla semantica originale della KB, sufficientemente espressivi per modellare inferenze multi-hop e sufficientemente scalabili da essere utilizzati con KB realisticamente grandi. La KB reificata a matrice sparsa può essere distribuita su più GPU, può scalare fino a decine di milioni di entità e fatti ed è ordini di grandezza più veloce delle implementazioni ingenue a matrice sparsa.  La KB reificata permette ad architetture end-to-end molto semplici di ottenere prestazioni competitive su diversi benchmark che rappresentano due famiglie di compiti: Completamento della KB e apprendimento di parser semantici dalle denotazioni.
Studiamo il Cross-Entropy Method (CEM) per l'ottimizzazione non convessa di una funzione obiettivo continua e parametrizzata e introduciamo una variante differenziabile (DCEM) che ci permette di differenziare l'output del CEM rispetto ai parametri della funzione obiettivo. Mostriamo applicazioni in un compito di previsione strutturata basata sull'energia sintetica e nel controllo continuo non convesso. Nell'impostazione di controllo mostriamo sui compiti simulati del ghepardo e del camminatore che possiamo incorporare le loro sequenze di azioni ottimali con DCEM e poi usare l'ottimizzazione della politica per mettere a punto i componenti del controller come un passo verso la combinazione di RL basata sul modello e senza modello.
Proponiamo una nuova struttura per l'estrazione di entità ed eventi basata sull'apprendimento generativo di imitazione avversaria -- un metodo di apprendimento di rinforzo inverso usando la rete avversaria generativa (GAN).Assumiamo che le istanze e le etichette cedono a vari gradi di difficoltà e i guadagni e le penalità (ricompense) sono previsti per essere diversi.Utilizziamo i discriminatori per stimare le ricompense adeguate secondo la differenza fra le etichette commesse dalla terra-verità (esperto) e l'estrattore (agente).  Gli esperimenti dimostrano anche che la struttura proposta supera i metodi all'avanguardia.
Nonostante i recenti progressi nella modellazione generativa delle immagini, generare con successo campioni diversi e ad alta risoluzione da dataset complessi come ImageNet rimane un obiettivo elusivo. A tal fine, addestriamo reti generative avversarie alla scala più grande ancora tentata, e studiamo le instabilità specifiche di tale scala. Troviamo che l'applicazione della regolarizzazione ortogonale al generatore lo rende suscettibile di un semplice "trucco di troncamento", permettendo un controllo fine sul trade-off tra fedeltà del campione e varietà riducendo la varianza dell'input del generatore. Le nostre modifiche portano a modelli che stabiliscono il nuovo stato dell'arte nella sintesi di immagini condizionate da classi. Quando addestrati su ImageNet alla risoluzione 128x128, i nostri modelli (BigGANs) raggiungono un Inception Score (IS) di 166.3 e Frechet Inception Distance (FID) di 9.6, migliorando il precedente miglior IS di 52.52 e FID di 18.65.
In questo lavoro, presentiamo un nuovo limite superiore dell'errore di destinazione per affrontare il problema dell'adattamento di dominio non supervisionato.Studi recenti rivelano che una rete neurale profonda può imparare caratteristiche trasferibili che generalizzano bene a compiti nuovi.Inoltre, Ben-David et al. (2010) forniscono un limite superiore per l'errore di destinazione quando si trasferisce la conoscenza, che può essere riassunto come la minimizzazione dell'errore di origine e la distanza tra distribuzioni marginali contemporaneamente.Tuttavia, i metodi comuni basati sulla teoria di solito ignorano l'errore congiunto come i campioni da classi diverse potrebbero essere mescolati insieme quando si abbina la distribuzione marginale. E in tal caso, non importa come minimizziamo la discrepanza marginale, l'errore di destinazione non è delimitato a causa di un errore congiunto crescente.Per affrontare questo problema, proponiamo un limite superiore generale che tiene conto dell'errore congiunto, in modo che il caso indesiderato possa essere adeguatamente penalizzato. Inoltre, utilizziamo uno spazio di ipotesi vincolato per formalizzare ulteriormente un limite più stretto, nonché una nuova discrepanza di margine trasversale per misurare la dissimilarità tra le ipotesi, che allevia l'instabilità durante l'apprendimento avversario.un'ampia evidenza empirica mostra che la nostra proposta supera gli approcci correlati nei tassi di errore di classificazione delle immagini sui benchmark standard di adattamento del dominio.
Uno zoo di reti profonde è disponibile in questi giorni per quasi ogni dato compito, ed è sempre meno chiaro con quale rete iniziare quando si affronta un nuovo compito, o quale rete utilizzare come inizializzazione per la messa a punto di un nuovo modello.Per affrontare questo problema, in questo documento, sviluppiamo un flusso di conoscenza che sposta la 'conoscenza' da più reti profonde, indicate come insegnanti, a un nuovo modello di rete profonda, chiamato studente. La struttura degli insegnanti e dello studente può differire arbitrariamente e possono essere addestrati su compiti completamente diversi con spazi di output diversi. Dopo l'addestramento con il flusso di conoscenza, lo studente è indipendente dagli insegnanti. dimostriamo il nostro approccio su una varietà di compiti di apprendimento supervisionato e di rinforzo, superando il fine-tuning e altri metodi di "scambio di conoscenze".
Nonostante le impressionanti prestazioni delle reti neurali profonde (DNN) su numerosi compiti di apprendimento, esse esibiscono ancora comportamenti poco ortodossi.Un comportamento sconcertante è la sottile reazione sensibile delle DNN a vari attacchi di rumore.Tale fastidio ha rafforzato la linea di ricerca intorno allo sviluppo e all'addestramento di reti resistenti al rumore. In questo lavoro, proponiamo un nuovo regolatore di formazione che mira a minimizzare la perdita di formazione probabilistica attesa di una DNN soggetta a un generico input gaussiano e forniamo un approccio semplice ed efficiente per approssimare tale regolatore per reti arbitrariamente profonde, sfruttando l'espressione analitica della media di uscita di una rete neurale poco profonda, evitando la necessità di aumentare i dati con memoria e calcolo costosi. Conduciamo ampi esperimenti su LeNet e AlexNet su vari set di dati tra cui MNIST, CIFAR10 e CIFAR100 per dimostrare l'efficacia del nostro regolatore proposto. In particolare, dimostriamo che le reti che sono addestrate con il regolatore proposto beneficiano di una spinta nella robustezza contro il rumore gaussiano per un importo equivalente all'esecuzione di 3-21 volte di aumento dei dati rumorosi. Inoltre, dimostriamo empiricamente su diverse architetture e set di dati che migliorare la robustezza contro il rumore gaussiano, utilizzando il nuovo regolarizzatore, può migliorare la robustezza complessiva contro altri 6 tipi di attacchi di due ordini di grandezza.
La comprensione della lettura è un compito impegnativo, specialmente quando viene eseguito su documenti più lunghi o su più prove, dove è probabile che la risposta si ripresenti. Le architetture neurali esistenti in genere non si adattano all'intera prova, e quindi ricorrono alla selezione di un singolo passaggio nel documento (tramite troncamento o altri mezzi), e cercano attentamente la risposta in quel passaggio.Tuttavia, in alcuni casi, questa strategia può essere subottimale, poiché concentrandosi su un passaggio specifico, diventa difficile sfruttare le menzioni multiple della stessa risposta in tutto il documento. Ogni sottomodello consiste solo di reti feed-forward dotate di un meccanismo di attenzione, il che lo rende banalmente parallelizzabile. Mostriamo che il nostro approccio può scalare a circa un ordine di grandezza di documenti di prova più grandi e può aggregare informazioni da menzioni multiple di ogni candidato alla risposta attraverso il documento.
Contrariamente agli analoghi domini in cui i sistemi su larga scala sono costruiti come una ripetizione gerarchica di unità su piccola scala, la pratica attuale nel Machine Learning si basa in gran parte su modelli con componenti non ripetitivi. Nello spirito della composizione molecolare con atomi ripetuti, avanziamo lo stato dell'arte nella compressione dei modelli proponendo AtomicCompression Networks (ACNs), una nuova architettura che è costruita dalla ripetizione ricorsiva di un piccolo insieme di neuroni. In altre parole, gli stessi neuroni con gli stessi pesi sono stocasticamente riposizionati negli strati successivi della rete. L'evidenza empirica suggerisce che le ACN raggiungono tassi di compressione fino a tre ordini di grandezza rispetto alle reti neurali completamente connesse (riduzione da 88× a 1116×) con un deterioramento solo frazionale dell'accuratezza di classificazione (da 0,15% a 5,33%). Inoltre il nostro metodo può produrre complessità sub-lineari del modello e permette l'apprendimento di ACN profonde con meno parametri di una regressione logistica senza declino nell'accuratezza di classificazione.
I modelli generativi che possono modellare e prevedere sequenze di eventi futuri possono, in linea di principio, imparare a catturare fenomeni complessi del mondo reale, come le interazioni fisiche.Tuttavia, una sfida centrale nella previsione video è che il futuro è altamente incerto: una sequenza di osservazioni passate di eventi può implicare molti futuri possibili.Anche se un certo numero di lavori recenti hanno studiato modelli probabilistici che possono rappresentare futuri incerti, tali modelli sono o estremamente costosi computazionalmente come nel caso di modelli autoregressivi a livello di pixel, o non ottimizzano direttamente la probabilità dei dati. A nostra conoscenza, il nostro lavoro è il primo a proporre la previsione video multi-frame con flussi normalizzanti, che consente l'ottimizzazione diretta della verosimiglianza dei dati e produce previsioni stocastiche di alta qualità. Descriviamo un approccio per la modellazione delle dinamiche dello spazio latente e dimostriamo che i modelli generativi basati sul flusso offrono un approccio valido e competitivo alla modellazione generativa del video.
Capire il comportamento della discesa stocastica del gradiente (SGD) nel contesto delle reti neurali profonde ha sollevato molte preoccupazioni di recente.Lungo questa linea, studiamo teoricamente una forma generale di dinamica di ottimizzazione basata sul gradiente con rumore imparziale, che unifica SGD e la dinamica standard di Langevin.Attraverso lo studio di questa dinamica generale di ottimizzazione, analizziamo il comportamento di SGD sulla fuga dai minimi e i suoi effetti di regolarizzazione.Un nuovo indicatore è derivato per caratterizzare l'efficienza della fuga dai minimi attraverso la misurazione dell'allineamento della covarianza del rumore e la curvatura della funzione di perdita. Sulla base di questo indicatore, vengono stabilite due condizioni per mostrare quale tipo di struttura del rumore è superiore al rumore isotropo in termini di efficienza di fuga.Mostriamo inoltre che il rumore anisotropo in SGD soddisfa le due condizioni, e quindi aiuta a fuggire dai minimi acuti e poveri in modo efficace, verso minimi più stabili e piatti che tipicamente generalizzano bene.Verifichiamo la nostra comprensione confrontando questa diffusione anisotropa con la discesa a gradiente completa più la diffusione isotropa (cioè la dinamica Langevin) e altri tipi di rumore dipendente dalla posizione.
Gli attuali approcci di apprendimento di rinforzo basati sul modello usano il modello semplicemente come un simulatore di scatola nera appreso per aumentare i dati per l'ottimizzazione della politica o l'apprendimento della funzione di valore. In questo articolo, mostriamo come fare un uso più efficace del modello sfruttando la sua differenziabilità. Costruiamo un algoritmo di ottimizzazione della politica che usa la derivata del percorso del modello appreso e la politica attraverso timesteps futuri. Inoltre, presentiamo una derivazione sul miglioramento monotono del nostro obiettivo in termini di errore di gradiente nel modello e nella funzione di valore. Mostriamo che il nostro approccio (i) è costantemente più efficiente rispetto agli algoritmi esistenti basati sul modello, (ii) corrisponde alle prestazioni asintotiche degli algoritmi senza modello, e (iii) scala a orizzonti lunghi, un regime dove tipicamente gli approcci basati sul modello hanno lottato.
Nel contesto dell'apprendimento per rinforzo, gli algoritmi di meta-apprendimento possono acquisire procedure di apprendimento per rinforzo per risolvere nuovi problemi in modo più efficiente utilizzando l'esperienza dei compiti precedenti. Le prestazioni degli algoritmi di meta-apprendimento dipendono dai compiti disponibili per il meta-apprendimento: allo stesso modo in cui l'apprendimento supervisionato generalizza meglio ai punti di test tratti dalla stessa distribuzione dei punti di addestramento, i metodi di meta-apprendimento generalizzano meglio ai compiti della stessa distribuzione dei compiti di meta-apprendimento.In effetti, l'apprendimento di meta-rinforzo scarica l'onere della progettazione dalla progettazione dell'algoritmo alla progettazione del compito.Se possiamo automatizzare anche il processo di progettazione del compito, possiamo concepire un algoritmo di meta-apprendimento che è veramente automatizzato. In questo lavoro, facciamo un passo in questa direzione, proponendo una famiglia di algoritmi di meta-apprendimento non supervisionati per l'apprendimento di rinforzo.motiviamo e descriviamo una ricetta generale per l'apprendimento di meta-rinforzo non supervisionato, e presentiamo un'istanza di questo approccio. I nostri contributi concettuali e teorici consistono nel formulare il problema dell'apprendimento di meta-rinforzo non supervisionato e nel descrivere come le proposte di attività basate sull'informazione reciproca possono in linea di principio essere utilizzate per addestrare meta-apprendenti ottimali.I nostri risultati sperimentali indicano che l'apprendimento di meta-rinforzo non supervisionato acquisisce efficacemente procedure di apprendimento di rinforzo accelerate senza la necessità di progettare attività manuali e supera significativamente le prestazioni di apprendimento da zero.
Introduciamo un nuovo metodo che permette il trasferimento efficiente dei parametri e l'apprendimento multi-task con le reti neurali profonde.L'approccio di base è quello di imparare una patch del modello - un piccolo insieme di parametri - che si specializzerà per ogni compito, invece di mettere a punto l'ultimo strato o l'intera rete.Per esempio, dimostriamo che l'apprendimento di un insieme di scale e bias è sufficiente per convertire una rete preaddestrata per eseguire bene su problemi qualitativamente diversi (ad esempio Il nostro approccio permette sia l'apprendimento di trasferimento simultaneo (multi-task) che quello sequenziale. In diversi problemi di apprendimento multi-task, nonostante l'utilizzo di un numero molto inferiore di parametri rispetto al tradizionale logits-only fine-tuning, riusciamo a eguagliare le prestazioni single-task. 
Gli esempi avversari hanno in qualche modo interrotto l'enorme successo dell'apprendimento automatico (ML) e stanno causando preoccupazione per quanto riguarda la sua affidabilità: Mentre vengono condotti studi per scoprire le proprietà intrinseche degli esempi avversi, come la loro trasferibilità e universalità, non c'è sufficiente analisi teorica per aiutare a capire il fenomeno in un modo che possa influenzare il processo di progettazione degli esperimenti di ML. In questo articolo, deduciamo un modello teorico dell'informazione che spiega gli attacchi avversi universalmente come l'abuso di ridondanze di caratteristiche negli algoritmi di ML. Il nostro modello aiuta a spiegare le principali questioni sollevate in molti studi aneddotici sugli esempi avversari. La nostra teoria è supportata da misurazioni empiriche del contenuto informativo di esempi benigni e avversari su set di dati sia di immagini che di testo. Le nostre misurazioni dimostrano che i tipici esempi avversari introducono una ridondanza appena sufficiente a sovraccaricare il processo decisionale di un machine learner addestrato sui corrispondenti esempi benigni.Concludiamo con raccomandazioni per migliorare la robustezza dei machine learners contro gli esempi avversari.
Proponiamo un nuovo modello generativo non supervisionato, Elastic-InfoGAN, che impara a distinguere l'identità dell'oggetto da altri aspetti di basso livello in set di dati sbilanciati in base alla classe. La nostra idea chiave è quella di rendere la scoperta del fattore di variazione latente discreto invariante alle trasformazioni che conservano l'identità nelle immagini reali, e usarlo come segnale per imparare i parametri della distribuzione latente. Gli esperimenti su entrambi i set di dati artificiali (MNIST) e del mondo reale (YouTube-Faces) dimostrano l'efficacia del nostro approccio nei dati sbilanciati attraverso:(i) una migliore separazione dell'identità dell'oggetto come fattore di variazione latente; e(ii) una migliore approssimazione dello squilibrio di classe nei dati, come riflesso nei parametri appresi della distribuzione latente.
Molte applicazioni reali mostrano un grande interesse nell'apprendimento di compiti multipli da diverse fonti di dati/modalità con campioni e dimensioni sbilanciate. Sfortunatamente, gli approcci di apprendimento multitasking profondo (MTL) all'avanguardia esistenti non possono essere applicati direttamente a queste impostazioni, a causa delle dimensioni di input eterogenee o dell'eterogeneità nelle architetture di rete ottimali dei diversi compiti. È quindi impegnativo sviluppare un meccanismo di condivisione della conoscenza per gestire le discrepanze intrinseche tra le architetture di rete tra i compiti. A tal fine, proponiamo un quadro flessibile di condivisione della conoscenza per l'apprendimento congiunto di più compiti da fonti di dati/modalità diverse. La struttura proposta permette ad ogni compito di possedere la sua progettazione di rete specifica del compito (dati), attraverso l'utilizzo di una rappresentazione tensoriale compatta, mentre la condivisione è realizzata attraverso i nuclei latenti parzialmente condivisi. Fornendo un controllo di condivisione più elaborato con i nuclei latenti, la nostra struttura è efficace nel trasferimento della conoscenza invariante del compito, ma è anche efficiente nell'apprendimento delle caratteristiche specifiche del compito.
I giochi da tavolo spesso si basano su informazioni visive come la posizione dei pezzi del gioco e le informazioni testuali sulle carte.A causa di questa dipendenza dal feedback visivo, i giocatori non vedenti sono in svantaggio perché non possono leggere le carte o vedere la posizione dei pezzi del gioco e possono non essere in grado di giocare una partita senza aiuto visivo.Presentiamo Game Changer, uno spazio di lavoro aumentato che fornisce sia descrizioni audio che aggiunte tattili per rendere lo stato del gioco da tavolo accessibile ai giocatori non vedenti e ipovedenti. In questo articolo, descriviamo il design di Game Changer e presentiamo i risultati di uno studio utente in cui 7 partecipanti non vedenti hanno usato Game Changer per giocare contro un partner vedente. La maggior parte dei giocatori ha dichiarato che il gioco era più accessibile con le aggiunte di Game Changer e che Game Changer potrebbe essere usato per aumentare altri giochi.
In molte applicazioni i dati etichettati non sono facilmente disponibili e devono essere raccolti attraverso una supervisione umana impegnativa. Noi proponiamo un modello regola-esemplare per raccogliere la supervisione umana per combinare la scalabilità delle regole con la qualità delle etichette delle istanze.  La supervisione è accoppiata in modo che sia naturale per gli esseri umani e sinergica per l'apprendimento. Proponiamo un algoritmo di addestramento che denoises congiuntamente le regole attraverso variabili di copertura latente, e addestra il modello attraverso una perdita di implicazione morbida sulle variabili di copertura e di etichetta.  La valutazione empirica su cinque diversi compiti mostra che (1) il nostro algoritmo è più accurato di diversi metodi esistenti di apprendimento da un mix di supervisione pulita e rumorosa, e (2) la supervisione accoppiata regola-esemplare è efficace nel denoising delle regole.
Consideriamo un problema di apprendimento della ricompensa e della politica da esempi di esperti sotto dinamiche sconosciute.Il nostro metodo proposto si basa sulla struttura delle reti generative avversarie e introduce l'empowerment-regolarizzato maximum-entropy inverse reinforcement learning per imparare ricompense e politiche quasi ottimali.La regolarizzazione basata sull'empowerment impedisce che la politica si adatti troppo alle dimostrazioni degli esperti, il che porta vantaggiosamente a comportamenti più generalizzati che risultano nell'apprendimento di ricompense quasi ottimali. Il nostro metodo impara simultaneamente l'empowerment attraverso la massimizzazione dell'informazione variazionale insieme alla ricompensa e alla politica sotto la formulazione di apprendimento avversaria.Valutiamo il nostro approccio su vari compiti di controllo complessi ad alta densità.Testiamo anche le nostre ricompense apprese in impegnativi problemi di apprendimento di trasferimento in cui gli ambienti di formazione e di prova sono fatti per essere diversi l'uno dall'altro in termini di dinamica o struttura.I risultati mostrano che il nostro metodo proposto non solo impara ricompense e politiche quasi ottimali che corrispondono al comportamento degli esperti ma esegue anche significativamente meglio degli algoritmi di apprendimento di rinforzo inverso dello stato dell'arte.
Le reti neurali ricorrenti (RNN) possono imparare rappresentazioni vettoriali continue di strutture simboliche come sequenze e frasi; queste rappresentazioni spesso mostrano regolarità lineari (analogie). Tali regolarità motivano la nostra ipotesi che le RNN che mostrano tali regolarità compilano implicitamente strutture simboliche in rappresentazioni di prodotti tensoriali (TPR; Smolensky, 1990), che combinano additivamente prodotti tensoriali di vettori che rappresentano ruoli (ad esempio, posizioni di sequenza) e vettori che rappresentano riempitivi (ad esempio, parole particolari). Per testare questa ipotesi, introduciamo le Tensor Product Decomposition Networks (TPDNs), che usano i TPRs per approssimare le rappresentazioni vettoriali esistenti. Dimostriamo utilizzando dati sintetici che le TPDN possono approssimare con successo rappresentazioni RNN lineari e ad albero autoencoder, suggerendo che queste rappresentazioni mostrano una struttura compositiva interpretabile; esploriamo le impostazioni che portano le RNN a indurre tali rappresentazioni sensibili alla struttura.  Al contrario, ulteriori esperimenti TPDN mostrano che le rappresentazioni di quattro modelli addestrati a codificare frasi naturali possono essere ampiamente approssimate con un sacchetto di parole, con solo miglioramenti marginali da strutture più sofisticate. Concludiamo che TPDNs fornisce un metodo potente per interpretare le rappresentazioni vettoriali e che RNNs standard puÃ² indurre le rappresentazioni compositive della sequenza che sono approssimate notevolmente bene daTPRs; allo stesso tempo, i compiti di addestramento attuali per l'apprendimento della rappresentazione della frase possono non essere sufficienti per indurre le rappresentazioni strutturali robuste.
Il nostro approccio consiste nel generalizzare l'idea di parametricità dalla teoria dei linguaggi di programmazione per formulare una proprietà semantica che distingua gli algoritmi comuni dalle funzioni arbitrarie non algoritmiche. Questa caratterizzazione porta naturalmente a uno schema di aumento dei dati appresi che incoraggia le RNN a imparare il comportamento algoritmico e consente l'apprendimento a piccoli campioni in una varietà di compiti di elaborazione delle liste.
L'obiettivo dell'apprendimento multietichetta (MLL) è quello di associare una data istanza con le sue etichette pertinenti da un insieme di concetti.I lavori precedenti di MLL si sono concentrati principalmente sull'impostazione in cui il set di concetti si presume sia fisso, mentre molte applicazioni del mondo reale richiedono l'introduzione di nuovi concetti nel set per soddisfare nuove richieste.Una necessità comune è quella di perfezionare i concetti grossolani originali e dividerli in quelli a grana più fine, dove il processo di perfezionamento inizia tipicamente con dati etichettati limitati per i concetti a grana più fine. Per rispondere a questa esigenza, proponiamo un problema speciale di MLL debolmente supervisionato che non solo si concentra sulla situazione di una limitata supervisione a grana fine, ma sfrutta anche la relazione gerarchica tra i concetti grossolani e quelli a grana fine.Il problema può essere ridotto a una versione multietichetta del problema di apprendimento negativo senza etichetta utilizzando la relazione gerarchica. Affrontiamo il problema ridotto con un approccio di meta-apprendimento che impara ad assegnare pseudo-etichette alle voci senza etichetta.I risultati sperimentali dimostrano che il nostro metodo proposto è in grado di assegnare pseudo-etichette accurate, e a sua volta raggiunge prestazioni di classificazione superiori rispetto ad altri metodi esistenti.
Il nostro approccio si rivolge alla ricerca soggettiva in cui l'utente è alla ricerca di risorse digitali come le immagini, che è fondamentalmente diverso dai compiti che hanno modalità di ricerca oggettiva e limitata.I dati di conversazione etichettati non sono generalmente disponibili in tali compiti di ricerca e la formazione dell'agente attraverso le interazioni umane può richiedere molto tempo. Proponiamo un utente virtuale stocastico che impersona un utente reale e può essere usato per campionare il comportamento dell'utente in modo efficiente per addestrare l'agente che accelera il bootstrapping dell'agente.Sviluppiamo un'architettura di conservazione del contesto basata sull'algoritmo A3C che permette all'agente di fornire assistenza contestuale all'utente.Confrontiamo l'agente A3C con Q-learning e valutiamo le sue prestazioni sulle ricompense medie e sui valori di stato che ottiene con l'utente virtuale negli episodi di validazione.I nostri esperimenti mostrano che l'agente impara a ottenere ricompense più alte e stati migliori.
Presentiamo un semplice approccio basato sui vicini più vicini ai pixel per capire e interpretare il funzionamento delle reti neurali allo stato dell'arte per i compiti a livello di pixel.Ci proponiamo di capire e scoprire i meccanismi di sintesi/predizione delle reti neurali convoluzionali allo stato dell'arte.A tal fine, analizziamo principalmente il processo di sintesi dei modelli generativi e il meccanismo di predizione dei modelli discriminativi. L'ipotesi principale di questo lavoro è che le reti neurali convoluzionali per compiti a livello di pixel apprendano una funzione di sintesi/predizione del vicino più vicino, di tipo compositivo, e i nostri esperimenti sulla segmentazione semantica e sulla traduzione immagine-immagine mostrano prove qualitative e quantitative a sostegno di questa ipotesi.
Consideriamo un mondo in cui si verificano eventi che coinvolgono varie entità.Imparare a predire eventi futuri da modelli di eventi passati diventa più difficile man mano che consideriamo più tipi di eventi.Molti dei modelli rilevati nel dataset da un LSTM ordinario saranno spuri poiché il numero di potenziali correlazioni a coppie, per esempio, cresce quadraticamente con il numero di eventi. Proponiamo un tipo di architettura LSTM fattoriale in cui diversi blocchi di celle LSTM sono responsabili della cattura di diversi aspetti dello stato del mondo. Usiamo regole Datalog per specificare come derivare la struttura LSTM da un database di fatti sulle entità del mondo. Questo è analogo a come un modello relazionale probabilistico (Getoor & Taskar, 2007) specifica una ricetta per derivare la struttura di un modello grafico da un database.In entrambi i casi, l'obiettivo è quello di ottenere utili bias induttivi codificando ipotesi di indipendenza informata nel modello.Consideriamo specificamente il processo neurale Hawkes, che utilizza un LSTM per modulare il tasso di eventi istantanei in tempo continuo.In entrambi i domini sintetici e del mondo reale, dimostriamo che otteniamo una migliore generalizzazione utilizzando appropriati disegni fattoriali specificati da semplici programmi Datalog.
In questo lavoro, miriamo a risolvere problemi di ottimizzazione data-driven, dove l'obiettivo è quello di trovare un input che massimizza una funzione di punteggio sconosciuto dato l'accesso a un set di dati di input, coppie di punteggio.Gli input possono trovarsi su collettori estremamente sottili in spazi ad alta dimensione, rendendo l'ottimizzazione incline a cadere fuori il collettore.Inoltre, la valutazione della funzione sconosciuta può essere costosa, quindi l'algoritmo dovrebbe essere in grado di sfruttare statico, offline data.We proporre reti inversione modello (MINs) come un approccio per risolvere tali problemi. A differenza dei lavori precedenti, le MIN scalano a spazi di input estremamente alti e possono sfruttare in modo efficiente i set di dati registrati offline per l'ottimizzazione in entrambe le impostazioni contestuali e non contestuali. Mostriamo che le MIN possono anche essere estese all'impostazione attiva, comunemente studiata nei lavori precedenti, attraverso uno schema semplice, nuovo ed efficace per la raccolta attiva dei dati.
Generare un linguaggio formale rappresentato da tuple relazionali, come programmi Lisp o espressioni matematiche, da un input in linguaggio naturale è un compito estremamente impegnativo perché richiede di catturare esplicitamente informazioni strutturali simboliche discrete dall'input per generare l'output. La maggior parte dei modelli di sequenze neurali allo stato dell'arte non catturano esplicitamente tali informazioni strutturali, e quindi non si comportano bene in questi compiti.In questo articolo, proponiamo un nuovo modello di codifica-decodifica basato su rappresentazioni di prodotti tensoriali (TPR) per la generazione da linguaggio naturale a formale, chiamato TP-N2F. Il codificatore di TP-N2F impiega il 'binding' TPR per codificare la struttura simbolica del linguaggio naturale nello spazio vettoriale e il decodificatore usa il 'unbinding' TPR per generare una sequenza di tuple relazionali, ciascuna composta da una relazione (o operazione) e un numero di argomenti, nello spazio simbolico. TP-N2F supera notevolmente i modelli Seq2Seq basati su LSTM, creando un nuovo stato dell'arte dei risultati su due benchmark: il dataset MathQA per la risoluzione di problemi matematici, e il dataset AlgoList per la sintesi di programmi.Gli studi di ablazione mostrano che i miglioramenti sono principalmente attribuiti all'uso di TPR sia nel codificatore che nel decodificatore per catturare esplicitamente le informazioni sulla struttura relazionale per il ragionamento simbolico.
In questo articolo presentiamo un defogger, un modello che impara a predire le informazioni nascoste future da osservazioni parziali.Formuliamo questo modello nel contesto della modellazione in avanti e sfruttiamo i vincoli e le correlazioni spaziali e sequenziali tramite reti neurali convoluzionali e reti di memoria a breve termine lunga, rispettivamente.Valutiamo il nostro approccio su un grande dataset di giochi umani di StarCraft: Brood War, un videogioco di strategia in tempo reale. i nostri modelli battono costantemente le linee di base basate su regole forti e producono qualitativamente stati di gioco futuri sensati.
In questo lavoro studiamo la generalizzazione delle reti neurali nel meta-apprendimento basato sul gradiente, analizzando varie proprietà dei paesaggi oggettivi. dimostriamo sperimentalmente che con il progredire del meta-addestramento, le soluzioni meta-test ottenute adattando la soluzione meta-train del modello a nuovi compiti attraverso pochi passi di fine-tuning basato sul gradiente, diventano più piatte, più basse in perdita, e più lontane dalla soluzione meta-train. Mostriamo anche che quelle soluzioni meta-test diventano più piatte anche quando la generalizzazione inizia a degradare, fornendo così una prova sperimentale contro la correlazione tra generalizzazione e minimi piatti nel paradigma del meta-leaning basato sul gradiente. Inoltre, forniamo l'evidenza empirica che la generalizzazione a nuovi compiti è correlata alla coerenza tra le loro traiettorie di adattamento nello spazio dei parametri, misurata dalla similarità media del coseno tra le direzioni delle traiettorie specifiche del compito, a partire da una stessa soluzione meta-train. Mostriamo anche che la coerenza dei gradienti meta-test, misurata dal prodotto interno medio tra i vettori di gradiente specifici del compito valutati alla soluzione meta-train, è anche correlata alla generalizzazione.
Ci sono stati molteplici tentativi con i variational auto-encoders (VAE) per imparare potenti rappresentazioni globali di dati complessi usando una combinazione di variabili stocastiche latenti e un modello autoregressivo sulle dimensioni dei dati. Tuttavia, per i compiti di immagine naturale più impegnativi il modello puramente autoregressivo con variabili stocastiche supera ancora i modelli autoregressivi stocastici combinati. In questo articolo, presentiamo semplici aggiunte alla struttura VAE che si generalizzano alle immagini naturali incorporando informazioni spaziali negli strati stocastici.Miglioriamo significativamente i risultati allo stato dell'arte su MNIST, OMNIGLOT, CIFAR10 e ImageNet quando la parametrizzazione della mappa delle caratteristiche delle variabili stocastiche è combinata con l'approccio autoregressivo PixelCNN.È interessante notare che osserviamo anche risultati vicini allo stato dell'arte senza la parte autoregressiva.Questo apre la possibilità di generare immagini di alta qualità con un solo forward-pass.
Le reti ricorrenti semplici soffrono molto del problema del gradiente che svanisce, mentre le reti neurali gated (GNN) come la Long-short Term Memory (LSTM) e la Gated Recurrent Unit (GRU) offrono risultati promettenti in molti compiti di apprendimento delle sequenze attraverso un design di rete sofisticato. Proponiamo una nuova rete chiamata Recurrent Identity Network (RIN) che permette a una rete ricorrente semplice di superare il problema del gradiente vanificante mentre addestra modelli molto profondi senza l'uso di gate. Confrontiamo questo modello con IRNN e LSTM su più benchmark di modellazione di sequenze. Le RIN dimostrano prestazioni competitive e convergono più velocemente in tutti i compiti. In particolare, i piccoli modelli RIN producono una precisione superiore del 12%-67% sui dataset Sequential e Permuted MNIST e raggiungono prestazioni all'avanguardia sul dataset di risposta alle domande bAbI.
Recentemente, c'è stata un'impennata di interesse per le tecniche sicure e robuste nell'apprendimento per rinforzo (RL). Le attuali nozioni di rischio in RL non riescono a catturare il potenziale per i fallimenti sistemici come gli arresti improvvisi da guasti del sistema o il superamento delle soglie di sicurezza e i controlli di risposta appropriati in tali casi. Proponiamo un nuovo approccio alla tolleranza ai guasti in RL in cui il controllore impara una politica che può far fronte ad attacchi avversari e arresti casuali che portano a guasti dei sottocomponenti del sistema. Dimostrando che la classe di problemi è rappresentata da una variante di SG, dimostriamo l'esistenza di una soluzione che è un unico punto fisso di equilibrio del gioco e caratterizziamo il comportamento ottimale del controllore, quindi introduciamo un algoritmo di approssimazione della funzione valore che converge alla soluzione attraverso la simulazione in ambienti sconosciuti.
Mentre una vasta letteratura si concentra sul recupero di una singola immagine da un'immagine sfocata, in questo lavoro affrontiamo un compito più impegnativo, ovvero il restauro video da un'immagine sfocata. Formuliamo il restauro video da una singola immagine sfocata come un problema inverso, impostando la sequenza di immagini pulite e il loro rispettivo movimento come fattori latenti, e l'immagine sfocata come osservazione. La nostra struttura si basa su una struttura encoder-decoder con moduli di rete di trasformazione spaziale per ripristinare una sequenza video e il suo movimento sottostante in modo end-to-end.Progettiamo una funzione di perdita e regolarizzatori con proprietà complementari per stabilizzare la formazione e analizzare i modelli di variante della rete proposta.L'efficacia e la trasferibilità della nostra rete sono evidenziate attraverso un ampio set di esperimenti su due diversi tipi di set di dati: sfocature di rotazione della telecamera generate da scene panoramiche e sfocature di movimento dinamico in video ad alta velocità.Il nostro codice e i modelli saranno disponibili al pubblico.
Le alte prestazioni dei modelli di deep learning sono tipicamente al costo di una considerevole dimensione del modello e del tempo di calcolo. Questi fattori limitano l'applicabilità per l'implementazione su dispositivi con vincoli di memoria e batteria come i telefoni cellulari o i sistemi embedded. In questo lavoro proponiamo una nuova tecnica di pruning che elimina interi filtri e neuroni in base al loro L1-norm relativo rispetto al resto della rete, producendo una maggiore compressione e una minore ridondanza nei parametri. La rete risultante non è sparsa, tuttavia, molto più compatta e non richiede alcuna infrastruttura speciale per la sua implementazione.Dimostriamo la fattibilità del nostro metodo raggiungendo una compressione del 97,4%, 47,8% e 53% di LeNet-5, ResNet-56 e ResNet-110 rispettivamente, superando i risultati di compressione allo stato dell'arte riportati su ResNet senza perdere alcuna prestazione rispetto alla baseline.Il nostro approccio non solo mostra buone prestazioni, ma è anche facile da implementare su molte architetture.
Il recente successo delle reti neurali per la risoluzione di compiti decisionali difficili ha incentivato l'incorporazione del processo decisionale intelligente "ai margini". Tuttavia, questo lavoro si è tradizionalmente focalizzato sull'inferenza della rete neurale, piuttosto che sulla formazione, a causa delle limitazioni di memoria e di calcolo, soprattutto nei sistemi emergenti di memoria non volatile, dove le scritture sono energicamente costose e riducono la durata della vita. In questo lavoro, affrontiamo quattro sfide chiave per la formazione su dispositivi di bordo con memoria non volatile: bassa densità di aggiornamento del peso, quantizzazione del peso, bassa memoria ausiliaria e apprendimento online. Presentiamo uno schema di formazione a basso rango che affronta queste quattro sfide mantenendo l'efficienza computazionale, quindi dimostriamo la tecnica su una rete neurale convoluzionale rappresentativa attraverso diversi problemi di adattamento, dove supera SGD standard sia nella precisione che nel numero di aggiornamenti del peso.
Le tecniche di estrazione della conoscenza sono utilizzate per convertire le reti neurali in descrizioni simboliche con l'obiettivo di produrre modelli di apprendimento più comprensibili. La sfida centrale è quella di trovare una spiegazione che sia più comprensibile del modello originale, pur rappresentando fedelmente quel modello. La natura distribuita delle reti profonde ha portato molti a credere che le caratteristiche nascoste di una rete neurale non possono essere spiegate da descrizioni logiche abbastanza semplici da essere comprese dagli esseri umani, e che l'estrazione decompositiva della conoscenza dovrebbe essere abbandonata a favore di altri metodi. In questo articolo esaminiamo sistematicamente questa questione proponendo un metodo di estrazione della conoscenza utilizzando regole \textit{M-of-N} che ci permette di mappare il panorama complessità/accuratezza delle regole che descrivono le caratteristiche nascoste in una Rete Neurale Convoluzionale (CNN). Gli esperimenti riportati in questo articolo mostrano che la forma di questo paesaggio rivela un trade off ottimale tra comprensibilità e accuratezza, mostrando che ogni variabile latente ha una regola ottimale \textit{M-of-N} per descrivere il suo comportamento.Troviamo che le regole con tradeoff ottimale nel primo e ultimo strato hanno un alto grado di spiegabilità mentre le regole con il tradeoff ottimale nel secondo e terzo strato sono meno spiegabili.I risultati fanno luce sulla fattibilità dell'estrazione di regole da reti profonde, e indicano il valore dell'estrazione di conoscenza decompositiva come metodo di spiegabilità.
Recenti scoperte dimostrano che i modelli generativi profondi possono giudicare i campioni fuori distribuzione come più probabili di quelli tratti dalla stessa distribuzione dei dati di allenamento.In questo lavoro, ci concentriamo sugli autocodificatori variazionali (VAE) e affrontiamo il problema delle stime di verosimiglianza non allineate sui dati di immagine. Sviluppiamo una nuova funzione di verosimiglianza che si basa non solo sui parametri restituiti dal VAE, ma anche sulle caratteristiche dei dati apprese in modo auto-supervisionato. In questo modo, il modello cattura anche le informazioni semantiche che sono trascurate dalla solita funzione di verosimiglianza VAE.dimostriamo i miglioramenti nell'affidabilità delle stime con esperimenti sui dataset FashionMNIST e MNIST.
I metodi di gradiente diretto delle politiche per l'apprendimento di rinforzo e i problemi di controllo continuo sono un approccio popolare per una serie di ragioni: 1) sono facili da implementare senza una conoscenza esplicita del modello sottostante; 2) sono un approccio "end-to-end", che ottimizza direttamente la metrica di performance di interesse; 3) permettono intrinsecamente politiche riccamente parametrizzate. Uno svantaggio notevole è che anche nel problema di controllo continuo più elementare (quello dei regolatori quadratici lineari), questi metodi devono risolvere un problema di ottimizzazione non convesso, dove si capisce poco della loro efficienza sia dal punto di vista computazionale che statistico. Al contrario, l'identificazione del sistema e la pianificazione basata sul modello nella teoria del controllo ottimale hanno un fondamento teorico molto più solido, dove si sa molto sulle loro proprietà computazionali e statistiche.  Questo lavoro colma questa lacuna mostrando che i metodi di gradiente di politica (senza modello) convergono globalmente alla soluzione ottimale e sono efficienti (in modo polinomiale nelle quantità rilevanti dipendenti dal problema) per quanto riguarda la loro complessità campionaria e computazionale.
Le embeddings vettoriali a bassa dimensione, calcolate usando LSTMs o tecniche più semplici, sono un approccio popolare per catturare il "significato" del testo e una forma di apprendimento non supervisionato utile per i compiti a valle. Tuttavia, il loro potere non è teoricamente compreso. Questo porta a un nuovo risultato teorico sugli LSTM: le incorporazioni a bassa dimensione derivate da un LSTM a bassa memoria sono provatamente almeno altrettanto potenti nei compiti di classificazione, fino a un piccolo errore, quanto un classificatore lineare sui vettori BonG, un risultato che un ampio lavoro empirico non è stato finora in grado di mostrare. I nostri esperimenti supportano questi risultati teorici e stabiliscono linee di base forti, semplici e non supervisionate su benchmark standard che in alcuni casi sono lo stato dell'arte tra i metodi a livello di parola. Mostriamo anche una nuova e sorprendente proprietà delle incorporazioni come GloVe e word2vec: formano una buona matrice di rilevamento per il testo che è più efficiente delle matrici casuali, lo strumento standard di recupero sparso, che può spiegare perché portano a rappresentazioni migliori nella pratica.
    Alcune trasformazioni convenzionali come la Discrete Walsh-Hadamard Transform (DWHT) e la Discrete Cosine Transform (DCT) sono state ampiamente utilizzate come estrattori di caratteristiche nell'elaborazione delle immagini, ma raramente applicate alle reti neurali. Tuttavia, abbiamo scoperto che queste trasformazioni convenzionali hanno la capacità di catturare le correlazioni tra canali senza alcun parametro imparabile nelle DNN. Specialmente per DWHT, non richiede moltiplicazioni in virgola mobile ma solo aggiunte e sottrazioni, il che può ridurre considerevolmente le spese generali di calcolo; inoltre, il suo algoritmo veloce riduce ulteriormente la complessità dell'aggiunta in virgola mobile da O(n^2) a O(nlog n). Queste proprietà non parametriche e di bassa computazione costruiscono reti estremamente efficienti nei parametri numerici e nelle operazioni, godendo di un guadagno di precisione.Il nostro modello proposto basato su DWHT ha ottenuto un aumento di precisione dell'1,49% con parametri ridotti del 79,4% e FLOP ridotti del 48,4% rispetto al suo modello di base (MoblieNet-V1) sul dataset CIFAR 100.
Introduciamo la nozione di \emph{lattice representation learning}, in cui la rappresentazione di un oggetto di interesse (ad esempio una frase o un'immagine) è un punto di reticolo in uno spazio euclideo. Il nostro contributo principale è un risultato per sostituire una funzione obiettivo che impiega la quantizzazione del reticolo con una funzione obiettivo in cui la quantizzazione è assente, permettendo così di applicare le tecniche di ottimizzazione basate sulla discesa del gradiente; chiamiamo gli algoritmi risultanti algoritmi \emph{dithered stochastic gradient descent} poiché sono progettati esplicitamente per consentire una procedura di ottimizzazione in cui vengono impiegate solo informazioni locali. Sosteniamo anche che una tecnica comunemente usata negli autocodificatori variazionali (priori gaussiani e posteriori approssimati gaussiani) è strettamente connessa con l'idea delle rappresentazioni a reticolo, poiché l'errore di quantizzazione in buoni reticoli ad alta dimensione può essere modellato come una distribuzione gaussiana. Usiamo un'architettura tradizionale di codificatore/decodificatore per esplorare l'idea delle rappresentazioni valutate a reticolo, e forniamo prove sperimentali del potenziale dell'uso delle rappresentazioni a reticolo modificando l'architettura generica \texttt{OpenNMT-py} in modo che possa implementare non solo il dithering gaussiano delle rappresentazioni, ma anche il ben noto stimatore straight-through e la sua applicazione alla quantizzazione vettoriale. 
Ci sono stati molti tentativi di spiegare il trade-off tra accuratezza e robustezza avversaria, ma non c'è stata una chiara comprensione del comportamento di un classificatore robusto che abbia una robustezza simile a quella umana. Noi sosteniamo (1) perché abbiamo bisogno di considerare la robustezza avversaria contro diverse grandezze di perturbazioni, non solo concentrandoci su una soglia fissa di perturbazione, (2) perché abbiamo bisogno di usare diversi metodi per generare campioni perturbati avversariamente che possono essere usati per addestrare un classificatore robusto e misurare la robustezza dei classificatori e (3) perché abbiamo bisogno di dare priorità alle accuratezze avversarie con diverse grandezze.Introduciamo la Lexicographical Genuine Robustness (LGR) dei classificatori che unisce i requisiti di cui sopra.  Suggeriamo anche un candidato classificatore oracolo chiamato "Optimal Lexicographically Genuinely Robust Classifier (OLGRC)" che dà la priorità all'accuratezza su esempi significativi perturbati avversariamente generati da perturbazioni di grandezza minore.   L'algoritmo di addestramento per la stima di OLGRC richiede l'ottimizzazione lessicografica a differenza dei metodi di addestramento avversari esistenti. Per applicare l'ottimizzazione lessicografica alla rete neurale, utilizziamo la Gradient Episodic Memory (GEM) che è stata originariamente sviluppata per l'apprendimento continuo, impedendo l'oblio catastrofico.
Le tecniche esistenti per la generazione di distribuzioni complesse con alti gradi di libertà dipendono da modelli generativi standard come Generative Adversarial Networks (GAN), Wasserstein GAN e varianti associate, che si basano su un'ottimizzazione che coinvolge la distanza tra due distribuzioni continue. Introduciamo un modello Discrete Wasserstein GAN (DWGAN) che si basa su una formulazione duale della distanza di Wasserstein tra due distribuzioni discrete; deriviamo un nuovo algoritmo di addestramento e la corrispondente architettura di rete basata su questa formulazione; vengono forniti risultati sperimentali sia per dati discreti sintetici, sia per dati reali discretizzati da cifre scritte a mano MNIST.
Introduciamo l'architettura di unificazione AI Omega, aperta, modulare e auto-migliorativa, che è un perfezionamento dell'architettura Alpha di Solomonoff, considerata a partire dai primi principi; l'architettura incarna diversi principi cruciali dell'intelligenza generale, tra cui la diversità delle rappresentazioni, la diversità dei tipi di dati, la memoria integrata, la modularità e la cognizione di ordine superiore; manteniamo il design di base di un substrato algoritmico fondamentale chiamato "kernel AI" per il problem solving e le funzioni cognitive di base come la memoria, e un'architettura più grande e modulare che riutilizza il kernel in molti modi. Omega include otto linguaggi di rappresentazione e sei classi di reti neurali, che sono brevemente introdotti.L'architettura è destinata inizialmente ad affrontare l'automazione della scienza dei dati, quindi include molti metodi di risoluzione dei problemi per compiti statistici.Esaminiamo l'ampia architettura del software, la cognizione di ordine superiore, l'auto-miglioramento, le architetture neurali modulari, gli agenti intelligenti, la gerarchia dei processi e della memoria, l'astrazione dell'hardware, il peer-to-peer computing, e la struttura di astrazione dei dati.
Per consentire ai modelli tradizionali a turno singolo di codificare la storia in modo completo, introduciamo Flow, un meccanismo che può incorporare rappresentazioni intermedie generate durante il processo di risposta alle domande precedenti, attraverso una struttura di elaborazione parallela alternata. Rispetto agli approcci superficiali che concatenano domande/risposte precedenti come input, Flow integra più profondamente la semantica latente della storia della conversazione. Il nostro modello, FlowQA, mostra prestazioni superiori su due sfide conversazionali recentemente proposte (+7,2% F1 su CoQA e +4,0% su QuAC).L'efficacia di Flow mostra anche in altri compiti.Riducendo la comprensione delle istruzioni sequenziali alla comprensione della macchina conversazionale, FlowQA supera i migliori modelli su tutti e tre i domini in SCONE, con un miglioramento della precisione da +1,8% a +4,4%.
Consideriamo l'apprendimento di rinforzo e i problemi di predizione strutturati a bandit con un feedback di perdita molto rado: solo alla fine di un episodio. Introduciamo un nuovo algoritmo, RESIDUAL LOSS PREDICTION (RESLOPE), che risolve tali problemi imparando automaticamente una rappresentazione interna di una funzione di ricompensa più densa. RESLOPE opera come una riduzione ai banditi contestuali, utilizzando la sua rappresentazione delle perdite appresa per risolvere il problema di assegnazione del credito, e un oracolo di banditi contestuali per scambiare l'esplorazione e lo sfruttamento. RESLOPE gode di una garanzia teorica di riduzione senza rimpianti e supera lo stato dell'arte degli algoritmi di apprendimento di rinforzo in entrambi gli ambienti MDP e nelle impostazioni di previsione strutturata dei banditi.
Le reti neurali avversarie risolvono molti problemi importanti nella scienza dei dati, ma sono notoriamente difficili da addestrare. Queste difficoltà derivano dal fatto che i pesi ottimali per le reti avversarie corrispondono ai punti di sella, e non ai minimizzatori, della funzione di perdita. I metodi a gradiente stocastico alternato tipicamente usati per tali problemi non convergono in modo affidabile ai punti di sella, e quando la convergenza avviene è spesso molto sensibile ai tassi di apprendimento. Proponiamo una semplice modifica della discesa del gradiente stocastico che stabilizza le reti avversarie. Mostriamo, sia in teoria che in pratica, che il metodo proposto converge in modo affidabile verso i punti di sella, il che rende le reti avversarie meno propense al "collasso" e permette un addestramento più veloce con tassi di apprendimento più elevati.
Un importante tipo di domanda che sorge nella pianificazione spiegabile è una domanda contrastiva, della forma "Perché l'azione A invece dell'azione B?". A questo tipo di domande si può rispondere con una spiegazione contrastiva che confronta le proprietà del piano originale contenente A con il piano contrastivo contenente B. Una spiegazione efficace di questo tipo serve a evidenziare le differenze tra le decisioni che sono state prese dal pianificatore e ciò che l'utente si aspetterebbe, così come a fornire ulteriori informazioni sul modello e sul processo di pianificazione. La produzione di questo tipo di spiegazioni richiede la generazione del piano di contrasto.Questo articolo introduce compilazioni indipendenti dal dominio delle domande dell'utente in vincoli.Questi vincoli vengono aggiunti al modello di pianificazione, in modo che una soluzione al nuovo modello rappresenti il piano di contrasto.Introduciamo una descrizione formale della compilazione dalla domanda dell'utente ai vincoli in un ambiente di pianificazione temporale e numerico PDDL2.1.
I metodi che imparano le rappresentazioni dei nodi in un grafo giocano un ruolo critico nell'analisi della rete poiché permettono molti compiti di apprendimento a valle.Proponiamo Graph2Gauss - un approccio che può imparare efficientemente le incorporazioni versatili dei nodi su grafi su larga scala (attribuiti) che mostrano forti prestazioni su compiti come la previsione dei collegamenti e la classificazione dei nodi. A differenza della maggior parte degli approcci che rappresentano i nodi come vettori di punti in uno spazio continuo a bassa dimensione, noi incorporiamo ogni nodo come una distribuzione gaussiana, permettendoci di catturare l'incertezza sulla rappresentazione.Inoltre, proponiamo un metodo non supervisionato che gestisce scenari di apprendimento induttivo ed è applicabile a diversi tipi di grafi: semplici/attribuiti, diretti/indiretti. Sfruttando sia la struttura della rete che gli attributi dei nodi associati, siamo in grado di generalizzare ai nodi non visti senza ulteriore addestramento. Per imparare le embeddings adottiamo una formulazione di ranking personalizzata rispetto alle distanze tra i nodi che sfrutta l'ordinamento naturale dei nodi imposto dalla struttura della rete. Gli esperimenti su reti del mondo reale dimostrano le alte prestazioni del nostro approccio, superando i metodi di embedding di rete allo stato dell'arte su diversi compiti; inoltre, dimostriamo i benefici della modellazione dell'incertezza - analizzandola possiamo stimare la diversità del vicinato e rilevare la dimensionalità latente intrinseca di un grafico.
Mentre sono stati fatti grandi progressi nel rendere le reti neurali efficaci in una vasta gamma di compiti, molte sono sorprendentemente vulnerabili a piccole perturbazioni accuratamente scelte del loro input, note come esempi avversari. In questo documento, sosteniamo e sperimentiamo l'uso di tecniche di regolarizzazione logit come difesa avversaria, che può essere usata insieme ad altri metodi per creare robustezza avversaria a poco o nessun costo. Dimostriamo che gran parte dell'efficacia di un recente meccanismo di difesa avversaria può essere attribuita alla regolarizzazione logit e mostriamo come migliorare la sua difesa sia contro gli attacchi white-box che black-box, creando nel processo un attacco black-box più forte contro i modelli basati su PGD.
Nell'apprendimento profondo, le prestazioni sono fortemente influenzate dalla scelta dell'architettura e degli iperparametri. Mentre c'è stato un ampio lavoro sull'ottimizzazione automatica degli iperparametri per gli spazi semplici, gli spazi complessi come lo spazio delle iperarchitetture rimangono in gran parte inesplorati. In questo articolo descriviamo una struttura per la progettazione automatica e l'addestramento di modelli profondi.Proponiamo un linguaggio estensibile e modulare che permette all'esperto umano di rappresentare in modo compatto spazi di ricerca complessi su architetture e i loro iperparametri.Gli spazi di ricerca risultanti sono strutturati ad albero e quindi facili da attraversare. I modelli possono essere compilati automaticamente in grafi computazionali una volta che i valori per tutti gli iperparametri sono stati scelti. Possiamo sfruttare la struttura dello spazio di ricerca per introdurre diversi algoritmi di ricerca del modello, come la ricerca casuale, la ricerca Monte Carlo ad albero (MCTS), e l'ottimizzazione sequenziale basata sul modello (SMBO).Presentiamo esperimenti che confrontano i diversi algoritmi su CIFAR-10 e mostrano che MCTS e SMBO superano la ricerca casuale. Presentiamo anche esperimenti su MNIST, mostrando che lo stesso spazio di ricerca raggiunge prestazioni vicine allo stato dell'arte con pochi campioni.Questi esperimenti mostrano che il nostro quadro può essere usato efficacemente per la scoperta di modelli, poiché è possibile descrivere spazi di ricerca espressivi e scoprire modelli competitivi senza molto sforzo da parte dell'esperto umano.Il codice per il nostro quadro e gli esperimenti è stato reso disponibile al pubblico
Le reti di apprendimento profondo hanno raggiunto accuratezze allo stato dell'arte su carichi di lavoro di computer vision come la classificazione delle immagini e il rilevamento degli oggetti.I sistemi performanti, tuttavia, tipicamente coinvolgono grandi modelli con numerosi parametri.Una volta addestrati, un aspetto impegnativo per tali modelli ad alte prestazioni è la distribuzione su sistemi di inferenza con risorse limitate - i modelli (spesso reti profonde o reti ampie o entrambi) sono computazionali e di memoria intensivi.I numeri a bassa precisione e la compressione del modello utilizzando la distillazione della conoscenza sono tecniche popolari per ridurre sia i requisiti di calcolo che l'impronta di memoria di questi modelli distribuiti. In questo articolo, studiamo la combinazione di queste due tecniche e dimostriamo che le prestazioni delle reti a bassa precisione possono essere significativamente migliorate utilizzando tecniche di distillazione della conoscenza.Chiamiamo il nostro approccio Apprentice e mostriamo accuratezze allo stato dell'arte utilizzando la precisione ternaria e la precisione a 4 bit per molte varianti dell'architettura ResNet sul dataset ImageNet.Studiamo tre schemi in cui si possono applicare tecniche di distillazione della conoscenza a varie fasi della pipeline train-and-deploy.
Le reti neurali profonde (DNN) sono ampiamente utilizzate in molte applicazioni.Tuttavia, la loro implementazione sui dispositivi di bordo è stata difficile perché sono affamate di risorse.Le reti neurali binarie (BNN) aiutano ad alleviare i requisiti proibitivi di risorse delle DNN, dove sia le attivazioni che i pesi sono limitati a 1 bit.Proponiamo un metodo di allenamento binario migliorato (BNN+), introducendo una funzione di regolarizzazione che incoraggia i pesi di allenamento intorno ai valori binari.Oltre a questo, per migliorare le prestazioni del modello aggiungiamo fattori di scala allenabili alle nostre funzioni di regolarizzazione. Inoltre, usiamo un'approssimazione migliorata della derivata della funzione di attivazione del segno nel calcolo a ritroso.Queste aggiunte sono basate su operazioni lineari che sono facilmente implementabili nel quadro di formazione binaria.Mostriamo risultati sperimentali su CIFAR-10 ottenendo un'accuratezza dell'86,5%, su AlexNet e 91,3% con la rete VGG.Su ImageNet, il nostro metodo supera anche il metodo BNN tradizionale e XNOR-net, usando AlexNet con un margine del 4% e 2% di accuratezza top-1 rispettivamente.
Il clustering è un metodo di apprendimento automatico fondamentale.La qualità dei suoi risultati dipende dalla distribuzione dei dati.Per questo motivo, le reti neurali profonde possono essere utilizzate per l'apprendimento di rappresentazioni migliori dei dati.In questo articolo, proponiamo una tassonomia sistematica per il clustering con apprendimento profondo, oltre a una revisione dei metodi del campo. Sulla base della nostra tassonomia, la creazione di nuovi metodi è più semplice.Proponiamo anche un nuovo approccio che è costruito sulla tassonomia e supera alcune delle limitazioni di alcuni lavori precedenti.La nostra valutazione sperimentale su dataset di immagini mostra che il metodo si avvicina allo stato dell'arte della qualità di clustering, ed esegue meglio in alcuni casi.
I modelli generativi spesso usano valutazioni umane per determinare e giustificare i progressi.Sfortunatamente, i metodi di valutazione umana esistenti sono ad-hoc: non esiste attualmente una valutazione standardizzata e validata che: (1) misuri la fedeltà percettiva, (2) sia affidabile, (3) separi i modelli in un chiaro ordine di rango, e (4) garantisca una misurazione di alta qualità senza costi intrattabili. In risposta, costruiamo Human-eYe Perceptual Evaluation (HYPE), una metrica umana che è (1) fondata sulla ricerca psicofisica nella percezione, (2) affidabile attraverso diversi insiemi di uscite campionate casualmente da un modello, (3) si traduce in prestazioni separabili del modello, e (4) efficiente in termini di costi e tempi.Introduciamo due metodi.Il primo, HYPE-Time, misura la percezione visiva sotto vincoli di tempo adattivi per determinare la lunghezza minima di tempo (ad es, Il secondo, HYPE-Infinity, misura il tasso di errore umano su immagini false e reali senza vincoli di tempo, mantenendo la stabilità e riducendo drasticamente tempi e costi. Testiamo HYPE su quattro reti generative avversarie (GAN) allo stato dell'arte sulla generazione incondizionata di immagini utilizzando due set di dati, il popolare CelebA e il più recente FFHQ a più alta risoluzione, e due tecniche di campionamento degli output del modello. Simulando più volte la valutazione di HYPE, dimostriamo una classificazione coerente dei diversi modelli, identificando StyleGAN con campionamento del trucco di troncamento (27,6% di tasso di inganno di HYPE-Infinity, con circa un quarto delle immagini che sono state classificate male dagli umani) come superiore a StyleGAN senza troncamento (19,0%) su FFHQ.
La traduzione automatica ha recentemente raggiunto prestazioni impressionanti grazie ai recenti progressi nell'apprendimento profondo e alla disponibilità di corpora paralleli su larga scala.Ci sono stati numerosi tentativi di estendere questi successi a coppie di lingue a bassa risorsa, ma che richiedono decine di migliaia di frasi parallele.In questo lavoro, portiamo questa direzione di ricerca all'estremo e studiamo se è possibile imparare a tradurre anche senza alcun dato parallelo. Proponiamo un modello che prende frasi da corpora monolingue in due lingue diverse e le mappa nello stesso spazio latente; imparando a ricostruire in entrambe le lingue da questo spazio di caratteristiche condiviso, il modello impara efficacemente a tradurre senza usare alcun dato etichettato. Dimostriamo il nostro modello su due dataset ampiamente usati e due coppie di lingue, riportando punteggi BLEU di 32,8 e 15,1 sui dataset Multi30k e WMT inglese-francese, senza usare nemmeno una singola frase parallela al momento dell'allenamento.
Deriviamo una nuova motivazione sociale intrinseca per l'apprendimento di rinforzo multi-agente (MARL), in cui gli agenti sono ricompensati per avere un'influenza causale sulle azioni di un altro agente, dove l'influenza causale è valutata utilizzando il ragionamento controfattuale. Mostriamo che la ricompensa per l'influenza causale è legata alla massimizzazione dell'informazione reciproca tra le azioni degli agenti. Testiamo l'approccio in ambienti di dilemma sociale impegnativi, dove porta costantemente a una maggiore cooperazione tra gli agenti e a una più alta ricompensa collettiva. Inoltre, troviamo che premiare l'influenza può portare gli agenti a sviluppare protocolli di comunicazione emergenti, quindi impieghiamo anche l'influenza per addestrare gli agenti a usare un canale di comunicazione esplicito, e troviamo che porta a una comunicazione più efficace e a una ricompensa collettiva più alta. Infine, dimostriamo che l'influenza può essere calcolata dotando ogni agente di un modello interno che prevede le azioni degli altri agenti.
Questo lavoro adotta la prospettiva distributiva di grande successo sull'apprendimento di rinforzo e la adatta all'impostazione di controllo continuo, combinandola all'interno di una struttura distribuita per l'apprendimento off-policy, al fine di sviluppare quello che chiamiamo l'algoritmo Distributed Distributional Deep Deterministic Policy Gradient, D4PG.Combiniamo inoltre questa tecnica con una serie di ulteriori, semplici miglioramenti come l'uso di N-step returns e la riproduzione prioritaria dell'esperienza. Sperimentalmente esaminiamo il contributo di ciascuno di questi singoli componenti, e mostriamo come interagiscono, così come i loro contributi combinati. I nostri risultati mostrano che in un'ampia varietà di semplici compiti di controllo, difficili compiti di manipolazione, e un insieme di difficili compiti di locomozione basati su ostacoli, l'algoritmo D4PG raggiunge prestazioni allo stato dell'arte.
Le funzioni di valore stato-azione (cioè, I valori Q) sono onnipresenti nel reinforcement learning (RL), dando origine ad algoritmi popolari come SARSA e Q-learning. Proponiamo una nuova nozione di valore d'azione definita da una versione lisciata gaussiana del valore Q atteso usato in SARSA. Mostriamo che tali valori Q lisciati soddisfano ancora un'equazione di Bellman, rendendoli naturalmente apprendibili dall'esperienza campionata da un ambiente. Inoltre, i gradienti della ricompensa attesa rispetto alla media e alla covarianza di una politica gaussiana parametrizzata possono essere recuperati dal gradiente e dall'Hessiano della funzione del valore Q lisciato. Sulla base di queste relazioni sviluppiamo nuovi algoritmi per l'addestramento di una politica gaussiana direttamente da un approssimatore di valore Q. L'approccio è anche adatto alle tecniche di ottimizzazione prossimale aumentando l'obiettivo con una penalità sulla divergenza KL da una politica precedente. Troviamo che la capacità di imparare sia una media che una covarianza durante l'addestramento permette a questo approccio di raggiungere forti risultati sui benchmark standard di controllo continuo.
I giochi narrativi interattivi sono simulazioni basate sul testo in cui un agente interagisce con il mondo puramente attraverso il linguaggio naturale. Sono ambienti ideali per studiare come estendere gli agenti di apprendimento del rinforzo per affrontare le sfide della comprensione del linguaggio naturale, dell'osservabilità parziale e della generazione di azioni in spazi di azione basati sul testo combinatoriamente grandi. Noi presentiamo KG-A2C, un agente che costruisce un grafo di conoscenza dinamico mentre esplora e genera azioni usando uno spazio d'azione basato su modelli. Noi sosteniamo che il doppio uso del grafo di conoscenza per ragionare sullo stato del gioco e per vincolare la generazione del linguaggio naturale sono le chiavi per l'esplorazione scalabile di azioni in linguaggio naturale combinatoriamente grandi. I risultati su un'ampia varietà di giochi IF mostrano che KG-A2C supera gli attuali agenti IF nonostante l'aumento esponenziale delle dimensioni dello spazio d'azione.
È ben noto che le reti neurali sono approssimatori universali, ma che le reti più profonde tendono in pratica ad essere più potenti di quelle meno profonde. Facciamo luce su questo dimostrando che il numero totale di neuroni m richiesto per approssimare classi naturali di polinomi multivariati di n variabili cresce solo linearmente con n per reti neurali profonde, ma cresce esponenzialmente quando è consentito solo un singolo strato nascosto. Forniamo anche la prova che quando il numero di strati nascosti viene aumentato da 1 a k, il requisito del neurone cresce esponenzialmente non con n ma con n^{1/k}, suggerendo che il numero minimo di strati richiesto per l'esprimibilità pratica cresce solo logaritmicamente con n.
Le reti neurali convoluzionali (CNN) negli ultimi anni hanno avuto un impatto drammatico nella scienza, nella tecnologia e nell'industria, ma il meccanismo teorico della progettazione dell'architettura CNN rimane sorprendentemente vago. I neuroni CNN, compreso il suo elemento distintivo, i filtri convoluzionali, sono noti per essere caratteristiche imparabili, ma il loro ruolo individuale nella produzione dell'output è piuttosto poco chiaro. La tesi di questo lavoro è che non tutti i neuroni sono ugualmente importanti e alcuni di loro contengono informazioni più utili per eseguire un determinato compito: (1) un approccio teorico del gioco basato sul valore di Shapley che calcola il contributo marginale di ogni filtro; e (2) un approccio probabilistico basato su quello che noi chiamiamo, l'interruttore di importanza usando l'inferenza variazionale.Usando questi due metodi confermiamo la teoria generale che alcuni dei neuroni sono intrinsecamente più importanti degli altri.Vari esperimenti illustrano che i gradi appresi possono essere facilmente utilizzabili per la compressione strutturata della rete e l'interpretabilità delle caratteristiche apprese.
Questo lavoro presenta un approccio modulare e gerarchico per imparare le politiche per l'esplorazione di ambienti 3D. Il nostro approccio sfrutta i punti di forza di entrambi i metodi classici e basati sull'apprendimento, utilizzando pianificatori di percorsi analitici con mappatori appresi e politiche globali e locali. L'uso dell'apprendimento fornisce flessibilità rispetto alle modalità di input (nel mappatore), sfrutta le regolarità strutturali del mondo (nelle politiche globali) e fornisce robustezza agli errori nella stima dello stato (nelle politiche locali). Tale uso dell'apprendimento all'interno di ogni modulo mantiene i suoi benefici, mentre allo stesso tempo, la decomposizione gerarchica e l'addestramento modulare ci permettono di evitare le alte complessità campionarie associate all'addestramento delle politiche end-to-end.I nostri esperimenti in ambienti simulati 3D visivamente e fisicamente realistici dimostrano l'efficacia del nostro approccio proposto rispetto all'apprendimento passato e agli approcci basati sulla geometria.
Il Deep Learning per la Computer Vision dipende principalmente dalla fonte di supervisione.I simulatori foto-realistici possono generare dati sintetici etichettati automaticamente su larga scala, ma introducono un gap di dominio che impatta negativamente sulle prestazioni.Proponiamo un nuovo algoritmo di adattamento al dominio non supervisionato, chiamato SPIGAN, che si basa su Simulator Privileged Information (PI) e Generative Adversarial Networks (GAN). Usiamo i dati interni del simulatore come PI durante l'addestramento di una rete target task.Valutiamo sperimentalmente il nostro approccio sulla segmentazione semantica.Addestriamo le reti sui dataset Cityscapes e Vistas del mondo reale, usando solo immagini reali non etichettate e dati sintetici etichettati con z-buffer (profondità) PI dal dataset SYNTHIA.Il nostro metodo migliora rispetto a nessun adattamento e alle tecniche di adattamento al dominio non supervisionato allo stato dell'arte.
L'addestramento avversario è una delle principali difese contro gli attacchi avversari.In questo articolo, forniamo il primo studio rigoroso sulla diagnosi degli elementi di addestramento avversario su larga scala su ImageNet, che rivela due proprietà intriganti. In primo luogo, studiamo il ruolo della normalizzazione: la normalizzazione dei lotti (BN) è un elemento cruciale per ottenere prestazioni allo stato dell'arte in molti compiti di visione, ma dimostriamo che può impedire alle reti di ottenere una forte robustezza nell'addestramento avversariale.Un'osservazione inaspettata è che, per i modelli addestrati con BN, la semplice rimozione di immagini pulite dai dati di addestramento aumenta ampiamente la robustezza avversariale, cioè, Questo fenomeno è legato all'ipotesi che le immagini pulite e le immagini avversarie siano tratte da due domini diversi. Questa ipotesi a due domini può spiegare il problema del BN quando si addestra con una miscela di immagini pulite e avversarie, poiché stimare le statistiche di normalizzazione di questa distribuzione di miscela è difficile, In secondo luogo, studiamo il ruolo della capacità della rete e troviamo che le nostre cosiddette reti "profonde" sono ancora poco profonde per il compito di apprendimento avversario, ResNet-152), l'addestramento avversariale mostra una richiesta molto più forte sulle reti più profonde per raggiungere una maggiore robustezza avversariale.Questo miglioramento della robustezza può essere osservato sostanzialmente e costantemente anche spingendo la capacità della rete ad una scala senza precedenti, cioè, ResNet-638.  
Il gradiente di una rete neurale profonda (DNN) rispetto all'input fornisce informazioni che possono essere utilizzate per spiegare la previsione dell'output in termini di caratteristiche dell'input ed è stato ampiamente studiato per aiutare nell'interpretazione delle DNN. In un modello lineare (cioè, $g(x)=wx+b$), il gradiente corrisponde unicamente ai pesi $w$.Un tale modello può ragionevolmente approssimare localmente in modo lineare una DNN non lineare liscia, e quindi i pesi di questo modello locale sono il gradiente.L'altra parte, tuttavia, di un modello lineare locale, cioè In questo articolo, osserviamo che poiché il bias in una DNN ha anche un contributo non trascurabile alla correttezza delle previsioni, esso può anche giocare un ruolo significativo nella comprensione del comportamento della DNN. In particolare, studiamo come attribuire il bias di una DNN alle sue caratteristiche di input. Proponiamo un algoritmo di tipo backpropagation ``bias back-propagation (BBp)'' che inizia dallo strato di output e attribuisce iterativamente il bias di ogni strato ai suoi nodi di input e combina il termine bias risultante dello strato precedente. Questo processo si ferma allo strato di input, dove sommando le attribuzioni su tutte le caratteristiche di input si recupera esattamente $b$. Insieme alla backpropagation del gradiente che genera $w$, possiamo recuperare completamente il modello localmente lineare $g(x)=wx+b$. Quindi, l'attribuzione degli output della DNN ai suoi input è decomposta in due parti, il gradiente $w$ e l'attribuzione del bias, fornendo spiegazioni separate e complementari.Studiamo diversi possibili metodi di attribuzione applicati al bias di ogni strato in BBp.Negli esperimenti, mostriamo che BBp può generare spiegazioni complementari e altamente interpretabili delle DNN in aggiunta alle attribuzioni basate sul gradiente.
Questo articolo presenta un metodo per trovare autonomamente le periodicità in un segnale, basato sulla stessa idea di usare la trasformata di Fourier e la funzione di autocorrelazione presentata in Vlachos et al. 2005. Pur mostrando risultati interessanti, questo metodo non funziona bene su segnali rumorosi o segnali con periodicità multiple, quindi il nostro metodo aggiunge diversi nuovi passi (clustering, filtraggio e detrending) per risolvere questi problemi.
Presentiamo una strategia di esplorazione avversaria, uno schema di apprendimento per imitazione semplice ma efficace che incentiva l'esplorazione di un ambiente senza alcuna ricompensa estrinseca o dimostrazione umana. La nostra struttura consiste in un agente di deep reinforcement learning (DRL) e un modello di dinamica inversa che competono tra loro. Il primo raccoglie campioni di allenamento per il secondo, e il suo obiettivo è massimizzare l'errore del secondo. In una tale impostazione competitiva, l'agente DRL impara a generare campioni che il modello di dinamica inversa non riesce a prevedere correttamente, e il modello di dinamica inversa impara ad adattarsi ai campioni difficili.Proponiamo inoltre una struttura di ricompensa che assicura che l'agente DRL raccolga solo campioni moderatamente difficili e non troppo difficili che impediscono al modello inverso di imitare efficacemente. Valutiamo l'efficacia del nostro metodo su diversi compiti di manipolazione del braccio e della mano robotica OpenAI contro una serie di modelli di base. I risultati sperimentali mostrano che il nostro metodo è paragonabile a quello addestrato direttamente con dimostrazioni di esperti, e superiore alle altre linee di base anche senza priori umani.
Questo articolo propone un dual variational autoencoder (DualVAE), un framework per la generazione di immagini corrispondenti a etichette multiclasse.Le recenti ricerche sui modelli generativi condizionali, come il Conditional VAE, mostrano il trasferimento di immagini cambiando le etichette.Tuttavia, quando la dimensione delle etichette multiclasse è grande, questi modelli non possono cambiare le immagini corrispondenti alle etichette, perché l'apprendimento di più distribuzioni della classe corrispondente è necessario per trasferire un'immagine.Questo porta alla mancanza di dati di allenamento.Pertanto, invece di condizionare con etichette, noi condizioniamo con vettori latenti che includono informazioni sulle etichette. DualVAE divide una distribuzione dello spazio latente da confini decisionali lineari utilizzando le etichette.Di conseguenza, DualVAE può facilmente trasferire un'immagine spostando un vettore latente verso un confine decisionale ed è robusto ai valori mancanti delle etichette multiclasse.Per valutare il nostro metodo proposto, introduciamo un punteggio di inizio condizionale (CIS) per misurare quanto un'immagine passa alla classe di destinazione.Valutiamo le immagini trasferite da DualVAE utilizzando il CIS nei dataset CelebA e dimostriamo prestazioni all'avanguardia in un ambiente multiclasse.
Uno dei contributi più notevoli dell'apprendimento profondo è l'applicazione delle reti neurali convoluzionali (ConvNets) alla classificazione dei segnali strutturati, e in particolare alla classificazione delle immagini. Oltre alle loro impressionanti prestazioni nell'apprendimento supervisionato, la struttura di tali reti ha ispirato lo sviluppo di banche di filtri profondi denominati trasformazioni di dispersione. Queste trasformazioni applicano una cascata di trasformazioni wavelet e operatori di moduli complessi per estrarre caratteristiche che sono invarianti alle operazioni di gruppo e stabili alle deformazioni. Inoltre, ConvNets ha ispirato i recenti progressi nell'apprendimento profondo geometrico, che mirano a generalizzare queste reti ai dati del grafico applicando le nozioni dall'elaborazione del segnale del grafico per imparare le cascate profonde del filtro del grafico.avanziamo ulteriormente queste linee di ricerca proponendo una trasformazione di dispersione geometrica usando le wavelets del grafico definite in termini di passeggiate casuali sul grafico. Dimostriamo l'utilità delle caratteristiche estratte con questo banco di filtro profondo progettato nella classificazione del grafico di dati di biochimica e di rete sociale (incl. lo stato dell'arte dei risultati in quest'ultimo caso), e nell'esplorazione dei dati, dove permettono l'inferenza delle preferenze di scambio CE nell'evoluzione degli enzimi.
L'OCR è inevitabilmente legato all'NLP dal momento che il suo output finale è in testo.I progressi nell'intelligenza dei documenti stanno guidando la necessità di una tecnologia unificata che integri l'OCR con vari compiti NLP, specialmente il parsing semantico.Poiché l'OCR e il parsing semantico sono stati studiati come compiti separati finora, i dataset per ogni compito da solo sono ricchi, mentre quelli per i compiti integrati di parsing post-OCR sono relativamente insufficienti. In questo studio, pubblichiamo un set di dati consolidato per il parsing delle ricevute come primo passo verso i compiti di parsing post-OCR.Il set di dati consiste in migliaia di ricevute indonesiane, che contiene immagini e annotazioni box/text per l'OCR, ed etichette semantiche multi-livello per il parsing.Il set di dati proposto può essere utilizzato per affrontare vari compiti OCR e parsing.
La crescita della complessità delle reti neurali convoluzionali (CNN) sta aumentando l'interesse per il partizionamento di una rete su più acceleratori durante l'addestramento e il pipelining dei calcoli di backpropagation sugli acceleratori. Gli approcci esistenti evitano o limitano l'uso di pesi stantii attraverso tecniche come il micro-batching o lo stashing dei pesi. Queste tecniche sottoutilizzano gli acceleratori o aumentano l'impronta di memoria. Usiamo 4 CNN (LeNet-5, AlexNet, VGG e ResNet) e dimostriamo che quando il pipelining è limitato ai primi strati di una rete, l'addestramento con pesi stantii converge e porta a modelli con accuratezze di inferenza paragonabili a quelle risultanti dall'addestramento non pipelinizzato sui dataset MNIST e CIFAR-10; un calo di accuratezza dello 0,4%, 4%, 0,83% e 1,45% per le 4 reti, rispettivamente. Tuttavia, quando il pipelining è più profondo nella rete, l'accuratezza dell'inferenza cala in modo significativo. Proponiamo di combinare l'addestramento pipeline e non pipeline in uno schema ibrido per affrontare questo calo. Dimostriamo l'implementazione e le prestazioni della nostra backpropagation pipeline in PyTorch su 2 GPU usando ResNet, ottenendo accelerazioni fino a 1,8 volte rispetto a una base di 1 GPU, con un piccolo calo nell'accuratezza dell'inferenza.
Nonostante le loro impressionanti prestazioni, le reti neurali profonde mostrano sorprendenti fallimenti su input fuori distribuzione.Un'idea centrale della ricerca sugli esempi avversari è quella di rivelare gli errori delle reti neurali sotto tali spostamenti di distribuzione.Decomponiamo questi errori in due fonti complementari: sensibilità e invarianza. Mostriamo che le reti profonde non solo sono troppo sensibili ai cambiamenti irrilevanti del loro input, come è noto dagli esempi epsilon-adversariali, ma sono anche troppo invarianti a una vasta gamma di cambiamenti rilevanti per il compito, rendendo così vaste regioni nello spazio di input vulnerabili agli attacchi avversari. Su MNIST e ImageNet si può manipolare il contenuto specifico della classe di quasi tutte le immagini senza cambiare le attivazioni nascoste, identificando un'insufficienza della perdita standard di cross-entropia come ragione di questi fallimenti. Inoltre, estendiamo questo obiettivo basato su un'analisi teorica dell'informazione in modo da incoraggiare il modello a considerare tutte le caratteristiche dipendenti dal compito nella sua decisione.
I modelli generativi basati sul flusso sono potenti modelli di verosimiglianza esatta con campionamento e inferenza efficienti. Nonostante la loro efficienza computazionale, i modelli basati sul flusso hanno generalmente prestazioni di modellazione della densità molto peggiori rispetto ai modelli autoregressivi allo stato dell'arte. In questo documento, indaghiamo e miglioriamo tre scelte di progettazione limitanti impiegate dai modelli basati sul flusso nel lavoro precedente: l'uso di rumore uniforme per la dequantizzazione, l'uso di flussi affini inespressivi e l'uso di reti di condizionamento puramente convoluzionali negli strati di accoppiamento. Sulla base delle nostre scoperte, proponiamo Flow++, un nuovo modello basato sul flusso che è ora lo stato dell'arte dei modelli non autoregressivi per la stima della densità incondizionata su benchmark di immagini standard. Il nostro lavoro ha iniziato a colmare il significativo divario di prestazioni che è esistito finora tra i modelli autoregressivi e quelli basati sul flusso.
Le moderne reti neurali artificiali profonde hanno raggiunto risultati impressionanti attraverso modelli con ordini di grandezza più parametri rispetto agli esempi di allenamento che controllano l'overfitting con l'aiuto della regolarizzazione.La regolarizzazione può essere implicita, come nel caso della discesa stocastica del gradiente e della condivisione dei parametri negli strati convoluzionali, o esplicita.Le tecniche di regolarizzazione esplicita, le forme più comuni sono il decadimento dei pesi e il dropout, hanno dimostrato di avere successo in termini di miglioramento della generalizzazione, ma riducono alla cieca la capacità effettiva del modello, introducono iper-parametri sensibili e richiedono architetture più profonde e ampie per compensare la capacità ridotta. Al contrario, le tecniche di incremento dei dati sfruttano la conoscenza del dominio per aumentare il numero di esempi di addestramento e migliorare la generalizzazione senza ridurre la capacità effettiva e senza introdurre parametri dipendenti dal modello, dal momento che viene applicata sui dati di addestramento.In questo articolo contrastiamo sistematicamente l'incremento dei dati e la regolarizzazione esplicita su tre architetture popolari e tre set di dati.I nostri risultati dimostrano che il solo incremento dei dati può raggiungere le stesse prestazioni o superiori ai modelli regolarizzati e mostra un'adattabilità molto più elevata ai cambiamenti nell'architettura e nella quantità di dati di addestramento.
L'apprendimento delle caratteristiche avversarie (AFL) è uno dei modi promettenti per vincolare esplicitamente le reti neurali ad apprendere le rappresentazioni desiderate; per esempio, AFL potrebbe aiutare ad apprendere rappresentazioni anonime in modo da evitare problemi di privacy.AFL impara tali rappresentazioni addestrando le reti ad ingannare l'avversario che predice le informazioni sensibili dalla rete, e quindi, il successo dell'AFL dipende fortemente dalla scelta dell'avversario.Questo articolo propone un nuovo design dell'avversario, {\em multiple adversaries over random subspaces} (Il metodo proposto è motivato dal presupposto che ingannare un avversario potrebbe non fornire informazioni significative se l'avversario è facilmente ingannabile, e gli avversari che si basano su un singolo classificatore soffrono di questo problema. Al contrario, il metodo proposto è progettato per essere meno vulnerabile, utilizzando l'insieme di classificatori indipendenti dove ogni classificatore cerca di prevedere le variabili sensibili da un diverso sottoinsieme delle rappresentazioni. Le convalide empiriche su tre compiti di anonimizzazione degli utenti mostrano che il nostro metodo proposto raggiunge prestazioni all'avanguardia in tutti e tre i set di dati senza danneggiare significativamente l'utilità dei dati. Questo è significativo perché fornisce nuove implicazioni sulla progettazione dell'avversario, che è importante per migliorare le prestazioni di AFL.
Un problema chiave nelle neuroscienze, e nelle scienze della vita più in generale, è che i dati sono generati da una gerarchia di sistemi dinamici. Un esempio di questo è nei dati di imaging del calcio in vivo, dove i dati sono generati da un sistema dinamico di ordine inferiore che governa il flusso di calcio nei neuroni, che a sua volta è guidato da un sistema dinamico di ordine superiore della computazione neurale.Idealmente, gli scienziati della vita sarebbero in grado di dedurre le dinamiche sia dei sistemi di ordine inferiore che dei sistemi di ordine superiore, ma questo è difficile in regimi ad alta densità. Un approccio recente usando autocodificatori variazionali sequenziali ha dimostrato che era possibile imparare la dinamica latente di un singolo sistema dinamico per le computazioni durante il raggiungimento del comportamento nel cervello, usando i dati di spionaggio modellati come un processo di Poisson.Qui estendiamo questo approccio usando un metodo ladder per dedurre una gerarchia di sistemi dinamici, permettendoci di catturare le dinamiche del calcio così come la computazione neurale. In questo approccio, gli eventi di spike guidano le dinamiche del calcio di ordine inferiore e sono essi stessi controllati da un sistema dinamico latente di ordine superiore. Generiamo dati sintetici generando frequenze di sparo, campionando i treni di spike e convertendo i treni di spike in transienti di fluorescenza, da due sistemi dinamici che sono stati usati come punti di riferimento chiave nella letteratura recente: un attrattore di Lorenz e una rete neurale ricorrente caotica. Tuttavia, anche se il nostro modello puÃ² ricostruire i tassi di spike sottostanti e i transienti del calcio dalla rete neurale caotica bene, non esegue cosÃ¬ bene a ricostruire i tassi di infornamento come tecniche di base per l'inferenza dei picchi dai dati del calcio. Questi risultati dimostrano che VLAEs sono un metodo promettente per modellare i dati dei sistemi dinamici gerarchici nelle scienze biologiche, ma che l'inferenza delle dinamiche dei sistemi di ordine inferiore puÃ² potenzialmente essere realizzata meglio con i metodi piÃ¹ semplici.
Con la proliferazione di processori specializzati per reti neurali che operano su numeri interi a bassa precisione, la performance dell'inferenza delle reti neurali profonde diventa sempre più dipendente dal risultato della quantizzazione. Nonostante l'abbondanza di lavori precedenti sulla quantizzazione dei pesi o delle attivazioni per le reti neurali, c'è ancora un ampio divario tra i quantizzatori software e l'implementazione dell'acceleratore a bassa precisione, che degrada l'efficienza delle reti o quella dell'hardware per la mancanza di coordinamento tra software e hardware nella fase di progettazione. In questo articolo, proponiamo un quantizzatore simmetrico lineare appreso per processori di reti neurali intere, che non solo quantizza i parametri neurali e le attivazioni in numeri interi a basso bit, ma accelera anche l'inferenza hardware utilizzando la fusione di normalizzazione in batch e accumulatori a bassa precisione (ad es, 16-bit) e moltiplicatori (es, Usiamo un modo unificato per quantizzare i pesi e le attivazioni, e i risultati superano molti approcci precedenti per varie reti come AlexNet, ResNet, e modelli leggeri come MobileNet, mantenendo l'architettura dell'acceleratore. Infine, distribuiamo i modelli quantizzati sul nostro acceleratore DNN specializzato per soli interi-aritmetici per mostrare l'efficacia del quantizzatore proposto.Mostriamo che anche con la quantizzazione lineare simmetrica, i risultati possono essere migliori dei metodi asimmetrici o non lineari nelle reti a 4 bit.Nella valutazione, il quantizzatore proposto induce meno dello 0,4% di calo di precisione in ResNet18, ResNet34, e AlexNet quando si quantizza l'intera rete come richiesto dai processori interi.
Il costo energetico proibitivo dell'esecuzione di reti neurali convoluzionali ad alte prestazioni (CNN) ha limitato la loro distribuzione su piattaforme con risorse limitate, compresi i dispositivi mobili e indossabili. Proponiamo una CNN per l'instradamento dinamico consapevole dell'energia, chiamata EnergyNet, che raggiunge un'inferenza di complessità adattiva basata sugli input, portando a una riduzione complessiva del costo energetico di esecuzione senza perdere (o addirittura migliorare) in modo evidente la precisione. Questo si ottiene proponendo una perdita di energia che cattura sia i costi computazionali che quelli di movimento dei dati, la combiniamo con la perdita orientata all'accuratezza e apprendiamo una politica di routing dinamico per saltare certi livelli nelle reti, che ottimizza la perdita ibrida.  I nostri risultati empirici dimostrano che, rispetto alle CNN di base, EnergyNet può ridurre il costo energetico fino al 40% e al 65%, durante l'inferenza sui set di test CIFAR10 e Tiny ImageNet, rispettivamente, mantenendo le stesse accuratezze di test.  È ulteriormente incoraggiante osservare che la consapevolezza dell'energia potrebbe servire come una regolarizzazione dell'addestramento e può persino migliorare l'accuratezza della previsione: i nostri modelli possono raggiungere un'accuratezza di test top-1 superiore dello 0,7% rispetto alla linea di base su CIFAR-10 quando si risparmia fino al 27% di energia, e un'accuratezza di test top-5 superiore su Tiny ImageNet quando si risparmia fino al 50% di energia, rispettivamente.
I modelli log-lineari sono ampiamente utilizzati nell'apprendimento automatico, e in particolare sono onnipresenti nelle architetture di apprendimento profondo sotto forma di softmax.Mentre l'inferenza esatta e l'apprendimento di questi richiede tempo lineare, può essere fatto approssimativamente in tempo sub-lineare con forti garanzie di concentrazione.In questo lavoro, presentiamo LSH Softmax, un metodo per eseguire l'apprendimento sub-lineare e l'inferenza dello strato softmax nell'impostazione di apprendimento profondo. Il nostro metodo si basa sul popolare Locality-Sensitive Hashing per costruire uno stimatore di gradiente ben concentrato, usando i vicini più vicini e campioni uniformi. Presentiamo anche uno schema di inferenza in tempo sub-lineare per LSH Softmax usando la distribuzione di Gumbel. Sulla modellazione del linguaggio, mostriamo che le reti neurali ricorrenti addestrate con LSH Softmax eseguono un'operazione alla pari con il calcolo del softmax esatto, mentre richiedono calcoli sub-lineari.
Il successo dei metodi di apprendimento del rinforzo per semplici problemi di ottimizzazione combinatoria può essere esteso alla pianificazione dell'assegnazione sequenziale di più robot? Oltre alla sfida di raggiungere prestazioni quasi ottimali in problemi di grandi dimensioni, la trasferibilità a un numero non visto di robot e compiti è un'altra sfida chiave per le applicazioni del mondo reale.In questo articolo, suggeriamo un metodo che raggiunge il primo successo in entrambe le sfide per i problemi di pianificazione di robot/macchina.  Il nostro metodo comprende tre componenti. in primo luogo, mostriamo che qualsiasi problema di programmazione dei robot può essere espresso come un modello grafico probabilistico casuale (PGM). sviluppiamo un metodo di inferenza del campo medio per PGM casuale e lo usiamo per l'inferenza della funzione Q. in secondo luogo, mostriamo che la trasferibilità può essere ottenuta progettando attentamente la codifica sequenziale a due fasi dello stato del problema. in terzo luogo, risolviamo il problema di scalabilità computazionale dell'iterazione Q adattata suggerendo un metodo euristico di adattamento dell'asta Q-iterazione permesso dalla trasferibilità che abbiamo raggiunto.  Applichiamo il nostro metodo ai problemi a tempo discreto e spazio discreto (Multi-Robot Reward Collection (MRRC)) e raggiungiamo in modo scalabile il 97% di ottimalità con la trasferibilità.Questa ottimalità è mantenuta in contesti stocastici.Estendendo il nostro metodo al tempo continuo, alla formulazione dello spazio continuo, sosteniamo di essere il primo metodo basato sull'apprendimento con prestazioni scalabili in qualsiasi tipo di problemi di programmazione multi-macchina; la scalabilità del nostro metodo raggiunge prestazioni comparabili alla metaheuristica popolare nei problemi di programmazione di macchine parallele identiche (IPMS).
Per rendere questo apprendimento efficiente, dato un curriculum e lo stato attuale di apprendimento di un agente, abbiamo bisogno di trovare quali sono i buoni compiti successivi su cui addestrare l'agente.Gli algoritmi Teacher-Student assumono che i buoni compiti successivi sono quelli su cui l'agente sta facendo il progresso più veloce o divagare. Tuttavia, possono verificarsi due situazioni problematiche in cui l'agente viene addestrato principalmente su compiti che non può ancora imparare o che ha già imparato.Pertanto, introduciamo un nuovo algoritmo che utilizza curricula ordinati min max che presuppone che i buoni compiti successivi siano quelli che sono imparabili ma non ancora imparati.Esso supera gli algoritmi Teacher-Student su piccoli curricula e li supera significativamente su quelli sofisticati con numerosi compiti.
I campi dell'intelligenza artificiale e delle neuroscienze hanno una lunga storia di fertili interazioni bidirezionali. Da un lato, importanti ispirazioni per lo sviluppo di sistemi di intelligenza artificiale sono venute dallo studio dei sistemi naturali di intelligenza, la neocorteccia dei mammiferi in particolare. Dall'altro, importanti ispirazioni per i modelli e le teorie del cervello sono emerse dalla ricerca sull'intelligenza artificiale.Una questione centrale all'intersezione di queste due aree riguarda i processi con cui la neocorteccia impara, e la misura in cui sono analoghi all'algoritmo di formazione back-propagation delle reti profonde. I recenti progressi nella nostra comprensione della fisiologia neuronale, sinaptica e dendritica della neocorteccia suggeriscono nuovi approcci per l'apprendimento non supervisionato delle rappresentazioni, forse attraverso una nuova classe di funzioni obiettivo, che potrebbero agire accanto o al posto della back-propagation.tali regole di apprendimento locale hanno obiettivi impliciti piuttosto che espliciti rispetto ai dati di formazione, facilitando l'adattamento al dominio e la generalizzazione.  Incorporandole nelle reti profonde per l'apprendimento delle rappresentazioni si potrebbero sfruttare meglio i set di dati non etichettati per offrire miglioramenti significativi nell'efficienza dei dati dell'apprendimento di lettura supervisionato a valle, e ridurre la suscettibilità alle perturbazioni avversarie, al costo di un dominio di applicabilità più ristretto.
I modelli esistenti basati su LSTMs richiedono un gran numero di parametri per supportare la memoria esterna e non generalizzano bene per lunghe sequenze di input. Le reti di memoria tentano di affrontare queste limitazioni memorizzando le informazioni in un modulo di memoria esterno, ma devono esaminare tutti gli input nella memoria. Durante l'inferenza, AMN analizza il testo di input in entità all'interno di diversi slot di memoria. Tuttavia, a differenza degli approcci precedenti, AMN è un'architettura di rete dinamica che crea un numero variabile di banchi di memoria ponderati in base alla rilevanza della domanda, quindi il decoder può selezionare un numero variabile di banchi di memoria per costruire una risposta utilizzando meno banchi, creando un trade-off runtime tra precisione e velocità. Nei nostri risultati, dimostriamo che il nostro modello impara a costruire un numero variabile di banchi di memoria in base alla complessità del compito e raggiunge tempi di inferenza più veloci per i compiti bAbI standard e per i compiti bAbI modificati, raggiungendo un'accuratezza allo stato dell'arte su questi compiti con una media del 48% di entità inferiori esaminate durante l'inferenza.
Quando uno studente bilingue impara a risolvere problemi di parole in matematica, ci aspettiamo che lo studente sia in grado di risolvere questi problemi in entrambe le lingue in cui lo studente è fluente, anche se le lezioni di matematica sono state insegnate in una sola lingua. Impariamo queste rappresentazioni ispirandoci alla linguistica, in particolare all'ipotesi della Grammatica Universale e impariamo rappresentazioni latenti universali che sono agnostiche alla lingua (Chomsky, 2014; Montague, 1970). Dimostriamo le capacità di queste rappresentazioni mostrando che i modelli addestrati su una singola lingua usando rappresentazioni agnostiche alla lingua raggiungono accuratezze molto simili in altre lingue.
I modelli generativi con variabili latenti sia discrete che continue sono altamente motivati dalla struttura di molti insiemi di dati del mondo reale, ma presentano, tuttavia, sottigliezze nell'addestramento che spesso si manifestano nella variabile latente discreta che non viene sfruttata. In questo articolo, mostriamo perché tali modelli lottano per l'addestramento usando la tradizionale massimizzazione della log-likelihood, e che sono adatti all'addestramento usando la struttura di trasporto ottimale degli autoencoder di Wasserstein. Troviamo che la nostra variabile latente discreta sia pienamente sfruttata dal modello quando viene addestrata, senza alcuna modifica alla funzione obiettivo o una sintonizzazione fine significativa. Il nostro modello genera campioni paragonabili ad altri approcci mentre usa reti neurali relativamente semplici, poiché la variabile latente discreta porta gran parte dell'onere descrittivo, inoltre la latente discreta fornisce un controllo significativo sulla generazione.
Mentre i modelli di apprendimento automatico raggiungono prestazioni paragonabili a quelle umane su dati sequenziali, lo sfruttamento della conoscenza strutturata è ancora un problema impegnativo. I grafi spazio-temporali hanno dimostrato di essere uno strumento utile per astrarre i grafi di interazione e i lavori precedenti sfruttano un'architettura feed-forward attentamente progettata per conservare tale struttura. L'apprendimento di tale struttura di interazione non è banale: da un lato, un modello deve scoprire le relazioni nascoste tra i diversi fattori del problema in modo non supervisionato; dall'altro, le relazioni estratte devono essere interpretabili. In questo articolo, proponiamo un modulo di attenzione in grado di proiettare una sottostruttura del grafo in un embedding di dimensioni fisse, preservando l'influenza che i vicini esercitano su un dato vertice. In una valutazione completa fatta su compiti reali e giocattolo, abbiamo trovato il nostro modello competitivo contro forti baseline.
Introduciamo NAMSG, un algoritmo adattivo del primo ordine per l'addestramento delle reti neurali: il metodo è efficiente nel calcolo e nella memoria, ed è semplice da implementare; calcola i gradienti in punti di osservazione remoti configurabili, al fine di accelerare la convergenza regolando la dimensione del passo per direzioni con curvature diverse nell'impostazione stocastica; inoltre scala il vettore di aggiornamento in senso elementare con un precondizionatore non crescente per prendere i vantaggi di AMSGRAD. Analizziamo le proprietà di convergenza per entrambi i problemi convessi e non convessi modellando il processo di formazione come un sistema dinamico, e forniamo una strategia per selezionare il fattore di osservazione senza ricerca della griglia.Un limite di rammarico dipendente dai dati è proposto per garantire la convergenza nell'impostazione convessa.Il metodo può inoltre raggiungere un limite di rammarico O(log(T)) per funzioni fortemente convesse.Gli esperimenti dimostrano che NAMSG funziona bene nei problemi pratici e si confronta favorevolmente con i metodi adattivi popolari, come ADAM, NADAM, e AMSGRAD.
I recenti progressi nelle Generative Adversarial Networks, facilitati da miglioramenti alla struttura e da applicazioni di successo a vari problemi, hanno portato a estensioni a domini multipli.IRGAN tenta di sfruttare la struttura per l'Information-Retrieval (IR), un compito che può essere descritto come la modellazione della corretta distribuzione di probabilità condizionata p(d|q) sui documenti (d), data la query (q). Il lavoro che propone IRGAN sostiene che l'ottimizzazione della loro funzione di perdita minimax risulterà in un generatore che può apprendere la distribuzione, ma la loro impostazione e il termine di base allontanano il modello da una formulazione esatta e avversaria, e questo lavoro cerca di evidenziare alcune imprecisioni nella loro formulazione; l'analisi delle loro curve di perdita dà un'idea dei possibili errori nelle funzioni di perdita e una migliore performance può essere ottenuta utilizzando la configurazione di co-formazione come quella che proponiamo, dove due modelli sono addestrati in modo cooperativo piuttosto che avversario.
La personalizzazione collaborativa, ad esempio attraverso rappresentazioni utente apprese (embeddings), può migliorare significativamente la precisione di previsione dei modelli basati su reti neurali. Proponiamo l'apprendimento federato delle rappresentazioni utente (FURL), un modo semplice, scalabile, rispettoso della privacy ed efficiente in termini di risorse per utilizzare le tecniche di personalizzazione neurale esistenti nell'impostazione dell'apprendimento federato (FL). FURL divide i parametri del modello in parametri federati e privati; i parametri privati, come le embeddature degli utenti privati, sono addestrati localmente, ma a differenza dei parametri federati, non sono trasferiti o mediati sul server. Dimostriamo teoricamente che questa divisione dei parametri non influisce sull'addestramento per la maggior parte degli approcci di personalizzazione dei modelli.Memorizzare le embeddings dell'utente localmente non solo preserva la privacy dell'utente, ma migliora anche la localizzazione in memoria della personalizzazione rispetto all'addestramento sul server. Valutiamo FURL su due set di dati, dimostrando un miglioramento significativo della qualità del modello con un aumento delle prestazioni dell'8% e del 51%, e circa lo stesso livello di prestazioni dell'addestramento centralizzato con solo lo 0% e il 4% di riduzioni.Inoltre, dimostriamo che le embeddings degli utenti apprese in FL e l'impostazione centralizzata hanno una struttura molto simile, indicando che FURL può imparare in modo collaborativo attraverso i parametri condivisi, preservando la privacy dell'utente.
Per esempio, anche se la lingua ha una chiara struttura gerarchica che va dai caratteri attraverso le parole fino alle frasi, non è evidente negli attuali modelli linguistici. Noi proponiamo di migliorare la rappresentazione nei modelli di sequenza aumentando gli approcci attuali con un autocodificatore che è costretto a comprimere la sequenza attraverso uno spazio latente discreto intermedio. Per propagare i gradienti attraverso questa rappresentazione discreta, introduciamo una tecnica di hashing semantico migliorata e dimostriamo che questa tecnica ha un buon rendimento su una nuova misura quantitativa di efficienza, analizziamo inoltre i codici latenti prodotti dal modello, mostrando come corrispondono a parole e frasi e presentiamo un'applicazione del modello autoencoder-augmented alla generazione di traduzioni diverse.
 Questi modelli, tuttavia, generalmente impiegano l'intero spazio euclideo o un sottoinsieme delimitato (come $[0,1]^l$) come spazio latente, la cui geometria banale è spesso troppo semplicistica per riflettere significativamente la struttura dei dati.Questo articolo mira ad esplorare una struttura geometrica non banale dello spazio latente per una migliore rappresentazione dei dati. Ispirandoci alla geometria differenziale, proponiamo \textbf{Chart Auto-Encoder (CAE)}, che cattura la struttura manifold dei dati con grafici multipli e funzioni di transizione tra loro.CAE traduce la definizione matematica di manifold attraverso la parametrizzazione dell'intero set di dati come una collezione di grafici sovrapposti, creando rappresentazioni latenti locali.Queste rappresentazioni sono un miglioramento dello spazio latente a singolo grafico comunemente impiegato nei modelli di autocodifica, poiché riflettono la struttura intrinseca del manifold.  Pertanto, il CAE raggiunge un'approssimazione più accurata dei dati e ne genera di nuovi realistici.Conduciamo esperimenti con dati sintetici e reali per dimostrare l'efficacia del CAE proposto.
Affrontiamo il problema della modellazione di fenomeni visivi sequenziali.Dati esempi di un fenomeno che può essere diviso in fasi temporali discrete, ci proponiamo di prendere un input da qualsiasi momento e realizzare questo input in tutte le altre fasi temporali della sequenza.Inoltre, ci proponiamo di fare questo \testo{senza} sequenze allineate ground-truth --- evitando le difficoltà necessarie per raccogliere dati allineati. Questo generalizza il problema dell'immagine-immagine non accoppiata dalla generazione di coppie alla generazione di sequenze.Estendiamo la consistenza del ciclo alla consistenza del ciclo e alleviamo le difficoltà associate all'apprendimento nelle risultanti lunghe catene di calcolo.Mostriamo risultati competitivi rispetto alle tecniche esistenti di immagine-immagine nella modellazione di diversi set di dati, tra cui le stagioni della Terra e l'invecchiamento dei volti umani.
Proponiamo Stochastic Weight Averaging in Parallel (SWAP), un algoritmo per accelerare l'addestramento DNN.Il nostro algoritmo usa grandi mini-batch per calcolare rapidamente una soluzione approssimativa e poi la raffina facendo la media dei pesi di più modelli calcolati indipendentemente e in parallelo.I modelli risultanti generalizzano altrettanto bene di quelli addestrati con piccoli mini-batch ma sono prodotti in un tempo sostanzialmente più breve.Dimostriamo la riduzione del tempo di addestramento e le buone prestazioni di generalizzazione dei modelli risultanti sui dataset di computer vision CIFAR10, CIFAR100, e ImageNet.
Un modo comune per accelerare l'addestramento di grandi reti convoluzionali è quello di aggiungere unità computazionali. L'addestramento viene quindi eseguito utilizzando la Stochastic Gradient Descent (SGD) sincrona parallela ai dati con un mini-batch diviso tra le unità computazionali.con un aumento del numero di nodi, la dimensione del batch cresce. Per superare queste difficoltà di ottimizzazione, proponiamo un nuovo algoritmo di formazione basato su Layer-wise Adaptive Rate Scaling (LARS).Utilizzando LARS, abbiamo scalato AlexNet e ResNet-50 a una dimensione del batch di 16K.
Trovare uno spazio di incorporazione per un'approssimazione lineare di un sistema dinamico non lineare permette un'efficiente identificazione del sistema e la sintesi del controllo.La teoria dell'operatore di Koopman pone le basi per identificare le trasformazioni da non lineare a lineare delle coordinate con metodi data-driven.Recentemente, i ricercatori hanno proposto di usare reti neurali profonde come una classe più espressiva di funzioni di base per calcolare gli operatori di Koopman.Questi approcci, tuttavia, assumono uno spazio di stato a dimensione fissa; non sono quindi applicabili a scenari con un numero variabile di oggetti. In questo articolo, proponiamo di imparare operatori di Koopman compositivi, usando reti neurali a grafo per codificare lo stato in embeddings centrate sull'oggetto e usando una matrice di transizione lineare a blocchi per regolarizzare la struttura condivisa tra gli oggetti. La dinamica appresa può adattarsi rapidamente a nuovi ambienti con parametri fisici sconosciuti e produrre segnali di controllo per raggiungere un obiettivo specifico.
Deriviamo la differenziazione automatica in modalità inversa (o adjoint) per le soluzioni delle equazioni differenziali stocastiche (SDE), consentendo un calcolo efficiente in termini di tempo e di memoria costante dei gradienti pathwise, un analogo a tempo continuo del trucco della riparametrizzazione.In particolare, costruiamo una SDE all'indietro la cui soluzione è il gradiente e forniamo condizioni sotto le quali le soluzioni numeriche convergono. Combiniamo anche il nostro approccio stocastico adjoint con uno schema di inferenza variazionale stocastico per modelli SDE a tempo continuo, permettendoci di imparare distribuzioni su funzioni utilizzando la discesa del gradiente stocastico.Il nostro modello SDE latente raggiunge prestazioni competitive rispetto agli approcci esistenti sulla modellazione delle serie temporali.
È chiaro che gli utenti dovrebbero possedere e controllare i loro dati e la loro privacy.Anche i fornitori di servizi stanno diventando sempre più interessati a garantire la privacy dei dati.Pertanto, gli utenti e i fornitori possono e devono collaborare nelle sfide di protezione della privacy, e questo articolo affronta questo nuovo paradigma. Proponiamo una struttura in cui l'utente controlla quali caratteristiche dei dati vuole condividere (utilità) e quali vuole mantenere private (segreto), senza necessariamente chiedere al fornitore di servizi di cambiare i suoi algoritmi di apprendimento automatico esistenti.Analizziamo prima lo spazio delle rappresentazioni che preservano la privacy e deriviamo limiti naturali di teoria dell'informazione sul trade-off utilità-privacy quando si rivela una versione sterilizzata dei dati X. Descriviamo importanti scenari di casi d'uso in cui i fornitori di servizi sono disposti a collaborare con il processo di sanificazione. Studiamo trasformazioni che preservano lo spazio in cui il fornitore di servizi può utilizzare lo stesso algoritmo sui dati originali e sanificati, un attributo critico e nuovo per aiutare i fornitori di servizi a soddisfare diversi requisiti di privacy con un unico set di algoritmi di utilità. Illustriamo questa struttura attraverso l'implementazione di tre casi d'uso; soggetto-in-soggetto, dove affrontiamo il problema di avere un rilevatore di identità facciale che funziona solo su un sottoinsieme consenziente di utenti, un'applicazione importante, ad esempio, per i dispositivi mobili attivati dal riconoscimento facciale; genere-e-soggetto, dove preserviamo la verifica facciale mentre nascondiamo l'attributo di genere per gli utenti che scelgono di farlo; ed emozione-e-gender, dove nascondiamo variabili indipendenti, come nel caso di nascondere il genere pur preservando il rilevamento delle emozioni.
Risolvere compiti con ricompense sparse è una delle sfide più importanti nell'apprendimento di rinforzo. Nell'impostazione a singolo agente, questa sfida è stata affrontata introducendo ricompense intrinseche che motivano gli agenti a esplorare regioni non viste dei loro spazi di stato. Applicando queste tecniche ingenuamente all'impostazione multi-agente, gli agenti esplorano indipendentemente, senza alcuna coordinazione tra di loro. In questo articolo proponiamo un approccio per imparare come selezionare dinamicamente tra diversi tipi di ricompense intrinseche che considerano non solo ciò che un singolo agente ha esplorato, ma tutti gli agenti, in modo che gli agenti possano coordinare la loro esplorazione e massimizzare i ritorni estrinseci. Concretamente, formuliamo l'approccio come una politica gerarchica in cui un controllore di alto livello seleziona tra insiemi di politiche addestrate su diversi tipi di ricompense intrinseche e i controllori di basso livello imparano le politiche di azione di tutti gli agenti sotto queste ricompense specifiche. Dimostriamo l'efficacia dell'approccio proposto in un dominio gridworld multi-agente con ricompense sparse, e poi dimostriamo che il nostro metodo scala fino a impostazioni più complesse valutando sulla piattaforma VizDoom.
Un sistema visivo ideale deve riconoscere in modo affidabile i concetti visivi popolati e nel frattempo imparare in modo efficiente nuove categorie emergenti con poche istanze di allenamento. L'apprendimento di molti colpi e l'apprendimento di pochi colpi affrontano un lato di questo problema, attraverso l'apprendimento di classificatori forti per le categorie popolate o l'apprendimento di classificatori di pochi colpi per le classi di coda.In questo articolo, indaghiamo il problema dell'apprendimento generalizzato di pochi colpi (GFSL) -- un modello durante lo spiegamento è richiesto non solo per imparare circa le categorie "di coda" con pochi colpi, ma simultaneamente classificare le categorie "della testa" e "della coda". Proponiamo l'apprendimento di sintesi del classificatore (CASTLE), una struttura di apprendimento che impara come sintetizzare i classificatori calibrati di pochi scatti oltre ai classificatori multiclasse delle classi "di testa", facendo leva su un dizionario neurale condiviso.CASTLE fa luce sul GFSL induttivo attraverso l'ottimizzazione di un obiettivo di apprendimento GFSL pulito ed efficace. Dimostra prestazioni superiori rispetto agli algoritmi GFSL esistenti e alle linee di base forti sui set di dati MiniImageNet e TieredImageNet. Più interessante, supera i precedenti metodi allo stato dell'arte quando valutati sull'apprendimento standard a pochi colpi.
I carichi di lavoro dell'apprendimento automatico sono spesso costosi da addestrare, impiegando settimane per convergere.L'attuale generazione di framework si basa su back-end personalizzati per raggiungere l'efficienza, rendendo impraticabile l'addestramento di modelli su hardware meno comuni dove non esistono tali back-end.Knossos si basa su un lavoro recente che evita la necessità di librerie scritte a mano, compilando invece i modelli di apprendimento automatico in modo molto simile a come si compilerebbero altri tipi di software.Per rendere efficiente il codice risultante, il compilatore Knossos ottimizza direttamente l'albero sintattico astratto del programma. Tuttavia, in contrasto con i compilatori tradizionali che impiegano passaggi di ottimizzazione scritti a mano, noi adottiamo un approccio di riscrittura guidato dall'algoritmo di ricerca $A^\star$ e una funzione di apprendimento del valore che valuta la potenziale riduzione dei costi futuri di intraprendere varie azioni di riscrittura del programma.Mostriamo che Knossos può apprendere automaticamente ottimizzazioni che i compilatori del passato dovevano implementare a mano. Inoltre, dimostriamo che Knossos può raggiungere una riduzione del tempo di parete rispetto a un compilatore sintonizzato a mano su una suite di programmi di apprendimento automatico, tra cui algebra lineare di base e reti convoluzionali.Il compilatore Knossos ha dipendenze minime e può essere utilizzato su qualsiasi architettura che supporti una toolchain Cpp. Poiché il modello di costo che l'algoritmo proposto ottimizza può essere adattato a una particolare architettura hardware, l'approccio proposto può potenzialmente essere applicato a una varietà di hardware.
Afferrare un oggetto e impilarlo con precisione su un altro è un compito difficile per il controllo robotico tradizionale o per gli approcci a mano.Qui esaminiamo il problema in simulazione e forniamo tecniche volte a risolverlo tramite l'apprendimento di rinforzo profondo.Introduciamo due semplici estensioni dell'algoritmo Deep Deterministic Policy Gradient (DDPG), che lo rendono significativamente più efficiente in termini di dati e scalabile. I nostri risultati mostrano che facendo un uso estensivo dei dati off-policy e replay, è possibile trovare politiche di controllo ad alte prestazioni.Inoltre, i nostri risultati suggeriscono che potrebbe essere presto fattibile addestrare politiche di impilamento di successo raccogliendo interazioni su robot reali.
Negli ultimi anni l'apprendimento di rinforzo profondo ha dimostrato di essere adatto a risolvere processi decisionali sequenziali con spazi di stato ad alta densità come nei giochi Atari.Molti problemi di apprendimento di rinforzo, tuttavia, coinvolgono spazi d'azione discreti ad alta densità così come spazi di stato ad alta densità.In questo articolo, sviluppiamo una nuova metodologia di gradiente della politica per il caso di grandi spazi d'azione discreti multidimensionali.Proponiamo due approcci per creare politiche parametrizzate: Entrambi questi approcci forniscono modelli espressivi ai quali può essere applicata la backpropagation per l'addestramento, quindi consideriamo il bonus di entropia, che è tipicamente aggiunto alla funzione di ricompensa per migliorare l'esplorazione. Nel caso di spazi d'azione ad alta densità, calcolare l'entropia e il gradiente dell'entropia richiede l'enumerazione di tutte le azioni nello spazio d'azione e l'esecuzione di forward e backpropagation per ogni azione, il che può essere computazionalmente impossibile. Sviluppiamo diversi nuovi stimatori imparziali per il bonus di entropia e il suo gradiente.
Le reti neurali offrono soluzioni ad alta precisione per una serie di problemi, ma sono computazionalmente costose da eseguire nei sistemi di produzione. Proponiamo una tecnica chiamata Deep Learning Approximation per prendere un modello di rete neurale già addestrato e costruire una rete più veloce (e quasi altrettanto accurata) manipolando la struttura della rete e i coefficienti senza richiedere un nuovo addestramento o l'accesso ai dati di formazione. La velocizzazione si ottiene applicando una serie sequenziale di ottimizzazioni indipendenti che riducono le operazioni in virgola mobile (FLOP) necessarie per eseguire un forward pass.Un'approssimazione lossy ottimale viene scelta per ogni strato pesando la perdita di accuratezza relativa e la riduzione FLOP.Su PASCAL VOC 2007 con la rete YOLO, mostriamo una velocizzazione end-to-end 2x in un forward pass di rete con un calo di $5$\% in mAP che può essere riottenuto con il finetuning, permettendo a questa rete (e altre simili) di essere implementata in sistemi con limitazioni di calcolo.
I metodi distribuiti asincroni sono un modo popolare per ridurre i costi di comunicazione e sincronizzazione dell'ottimizzazione su larga scala; tuttavia, per tutto il loro successo, si sa poco sulle loro garanzie di convergenza nel caso impegnativo di obiettivi generali non lisci e non convessi, oltre ai casi in cui sono disponibili soluzioni in forma chiusa per operatori prossimali. La nostra analisi si applica ai metodi stocastici di discesa sub-gradiente sia con e senza partizionamento delle variabili di blocco, sia con e senza slancio, ed è formulata nel contesto di un modello generale probabilistico di programmazione asincrona accuratamente adattato alle proprietà dell'hardware moderno. Convalidiamo sperimentalmente la nostra analisi nel contesto dell'addestramento di architetture di reti neurali profonde, mostrando la loro convergenza asintotica complessiva di successo ed esplorando come slancio, sincronizzazione e partizionamento influenzano le prestazioni.
Le reti neurali stocastiche con variabili casuali discrete sono una classe importante di modelli per la loro espressività e interpretabilità. Poiché la differenziazione diretta e la backpropagation non sono possibili, le tecniche di stima del gradiente di Monte Carlo sono state ampiamente impiegate per l'addestramento di tali modelli. Efficienti stimatori del gradiente stocastico, come Straight-Through e Gumbel-Softmax, funzionano bene per modelli poco profondi con uno o due strati stocastici; le loro prestazioni, tuttavia, soffrono con l'aumentare della complessità del modello. In questo lavoro ci concentriamo sulle reti stocastiche con più strati di variabili latenti booleane.  Lo usiamo per derivare una formulazione analitica per la fonte di bias nello stimatore biased Straight-Through.Basandoci sull'analisi proponiamo \emph{FouST}, un semplice algoritmo di stima del gradiente che si basa su tre semplici passi di riduzione del bias. Esperimenti approfonditi dimostrano che FouST si comporta favorevolmente rispetto agli stimatori bias allo stato dell'arte, mentre è molto più veloce di quelli unbiased. Per quanto ne sappiamo, FouST è il primo stimatore di gradiente per addestrare reti neurali stocastiche molto profonde, con un massimo di 80 strati deterministici e 11 stocastici.
Proponiamo una nuova rete generativa avversaria per la manipolazione degli attributi visivi (ManiGAN), che è in grado di modificare semanticamente gli attributi visivi di immagini date usando descrizioni in linguaggio naturale.La chiave del nostro metodo è di progettare un nuovo modulo di co-attenzione per combinare informazioni di testo e di immagine piuttosto che semplicemente concatenare due caratteristiche lungo la direzione del canale. Infine, proponiamo una nuova metrica per valutare i risultati della manipolazione, sia in termini di generazione di attributi legati al testo che di ricostruzione di contenuti non legati al testo. Esperimenti estesi su set di dati di riferimento dimostrano i vantaggi del nostro metodo proposto, per quanto riguarda l'efficacia della manipolazione delle immagini e la capacità di generare risultati di alta qualità.
Il trasporto ottimale (OT) si presenta naturalmente in molte applicazioni di apprendimento automatico, dove abbiamo bisogno di gestire dati cross-modality da fonti multiple, ma il pesante onere computazionale limita i suoi usi diffusi. Per affrontare il problema della scalabilità, proponiamo una struttura implicita basata sull'apprendimento generativo chiamata SPOT (Scalable Push-forward of Optimal Transport). Mostriamo anche che possiamo recuperare la densità del piano di trasporto ottimale usando equazioni differenziali ordinarie neurali. Gli esperimenti numerici su set di dati sintetici e reali illustrano che SPOT è robusto e ha un comportamento di convergenza favorevole. SPOT ci permette anche di campionare in modo efficiente dal piano di trasporto ottimale, che beneficia di applicazioni a valle come l'adattamento del dominio.
In questo lavoro, proponiamo una nuova formulazione della pianificazione che la vede come un problema di inferenza probabilistica sulle traiettorie ottimali future, il che ci permette di utilizzare metodi di campionamento e quindi di affrontare la pianificazione nei domini continui utilizzando un budget computazionale fisso.   Progettiamo un nuovo algoritmo, Sequential Monte Carlo Planning, sfruttando metodi classici in Sequential Monte Carlo e Bayesian smoothing nel contesto del controllo come inferenza.Inoltre, dimostriamo che Sequential Monte Carlo Planning può catturare politiche multimodali e può apprendere rapidamente compiti di controllo continuo.
Noi proponiamo e studiamo una modifica architetturale, l'auto-modulazione, che migliora le prestazioni di GAN attraverso diversi set di dati, architetture, perdite, regolarizzatori e impostazioni iperparametriche.Intuitivamente, l'auto-modulazione permette alle mappe di caratteristiche intermedie di un generatore di cambiare in funzione del vettore di rumore in ingresso. Mentre ricorda altre tecniche di condizionamento, non richiede dati etichettati.In uno studio empirico su larga scala osserviamo una diminuzione relativa del 5%-35% in FID.Inoltre, a parità di condizioni, l'aggiunta di questa modifica al generatore porta a un miglioramento delle prestazioni in 124/144 (86%) delle impostazioni studiate.L'automodulazione è un semplice cambiamento architettonico che non richiede ulteriori parametri di regolazione, il che suggerisce che può essere applicato facilmente a qualsiasi GAN.
I metodi di classificazione estremi sono diventati di fondamentale importanza, in particolare per i problemi di Information Retrieval (IR), a causa dello sviluppo di algoritmi intelligenti che sono scalabili per le sfide del settore.Una delle prime classi di modelli che mirano a risolvere la sfida della memoria e della velocità dell'apprendimento multi-label estremo è Group Testing. I metodi Multi-label Group Testing (MLGT) costruiscono gruppi di etichette raggruppando le etichette originali in modo casuale o sulla base di una certa somiglianza e poi addestrano classificatori più piccoli per prevedere prima i gruppi e poi recuperare i vettori delle etichette originali.Recentemente, è stato proposto un nuovo approccio chiamato MACH (Merged Average Classifiers via Hashing) che proietta gli enormi vettori di etichette in una piccola e gestibile matrice count-min sketch (CMS) e poi impara a prevedere questa matrice per recuperare le probabilità di previsione originali. In questo modo, la memoria del modello scala O(logK) per K classi.MACH è un algoritmo semplice che funziona eccezionalmente bene in pratica.Nonostante questa semplicità di MACH, c'è un grande divario tra la comprensione teorica dei trade-off con MACH.In questo articolo riempiamo questo divario.Sfruttando la teoria del count-min sketch forniamo una quantificazione precisa dei tradeoffs di memoria-identificabilità. Estendiamo la teoria al caso della classificazione a più etichette, dove le dipendenze rendono gli stimatori difficili da calcolare in forme chiuse. Per mitigare questo problema, proponiamo una nuova approssimazione quadratica usando il principio di inclusione-esclusione. Il nostro stimatore ha un errore di ricostruzione significativamente più basso del tipico stimatore CMS attraverso vari valori di numero di classi K, sparsità di etichette e rapporto di compressione.
Le reti neurali sono comunemente usate come modelli di classificazione per un'ampia varietà di compiti; tipicamente, una trasformazione affine appresa è posta alla fine di tali modelli, producendo un valore per classe usato per la classificazione; questo classificatore può avere un vasto numero di parametri, che cresce linearmente con il numero di classi possibili, richiedendo così sempre più risorse. In questo lavoro sosteniamo che questo classificatore può essere fissato, fino a una costante di scala globale, con poca o nessuna perdita di accuratezza per la maggior parte dei compiti, consentendo vantaggi di memoria e di calcolo.Inoltre, dimostriamo che inizializzando il classificatore con una matrice Hadamard possiamo accelerare anche l'inferenza.Discutiamo le implicazioni per la comprensione attuale dei modelli di rete neurale.
Questo articolo introduce NEMO, un approccio al rilevamento non supervisionato degli oggetti che utilizza il movimento - invece delle etichette dell'immagine - come spunto per imparare il rilevamento dell'oggetto; per discriminare tra il movimento dell'oggetto target e altri cambiamenti nell'immagine, si basa su esempi negativi che mostrano la scena senza l'oggetto. I dati necessari possono essere raccolti molto facilmente registrando due brevi video, uno positivo che mostra l'oggetto in movimento e uno negativo che mostra la scena senza l'oggetto; senza alcuna forma aggiuntiva di preformazione o supervisione e nonostante le occlusioni, le distrazioni, il movimento della telecamera e l'illuminazione avversa, questi video sono sufficienti per imparare i rilevatori di oggetti che possono essere applicati a nuovi video e persino generalizzare a scene e angoli di ripresa mai visti. In un confronto di base, il rilevamento non supervisionato degli oggetti supera gli approcci off-the shelf di corrispondenza dei modelli e di tracciamento che ricevono una scatola iniziale di delimitazione dell'oggetto.Le rappresentazioni degli oggetti apprese hanno anche dimostrato di essere abbastanza accurate da catturare le informazioni rilevanti dalle dimostrazioni dei compiti di manipolazione, il che le rende applicabili all'apprendimento dalla dimostrazione nella robotica.Un esempio di rilevamento degli oggetti che è stato appreso da 3 minuti di video può essere trovato qui: http://y2u.be/u_jyz9_ETz4
È giunto il momento di introdurre set di dati più impegnativi per spingere lo sviluppo di questo campo verso un ragionamento più completo del testo.In questo articolo, introduciamo un nuovo set di dati di comprensione della lettura che richiede un ragionamento logico (ReClor) estratto da esami di ammissione standardizzati per laureati.Come suggeriscono studi precedenti, i set di dati annotati dall'uomo di solito contengono distorsioni, che sono spesso sfruttate dai modelli per ottenere un'elevata precisione senza comprendere veramente il testo. I risultati empirici mostrano che i modelli allo stato dell'arte hanno un'eccezionale capacità di catturare le distorsioni contenute nel set di dati con un'alta accuratezza sul set EASY, ma lottano sul set HARD con prestazioni scarse vicino a quelle di un'ipotesi casuale, indicando che sono necessarie ulteriori ricerche per migliorare essenzialmente la capacità di ragionamento logico dei modelli attuali.
Questo articolo esplora gli scenari in cui un attaccante può affermare che "il rumore e l'accesso allo strato softmax del modello è tutto ciò di cui hai bisogno" per rubare i pesi di una rete neurale convoluzionale la cui architettura è già nota. Siamo stati in grado di raggiungere il 96% di accuratezza del test usando il modello MNIST rubato e l'82% di accuratezza usando il modello KMNIST rubato appreso usando solo input di rumore Bernoulli i.i.d. L'obiettivo di questa divulgazione non è solo quello di mostrare fino a che punto la conoscenza dell'architettura può portare in termini di furto del modello, ma anche di attirare l'attenzione su questi aspetti di apprendibilità dei pesi piuttosto idiosincratici delle CNN stimolati da input di rumore i. i. d. Diffondiamo anche alcuni risultati iniziali ottenuti utilizzando la distribuzione di probabilità di Ising al posto della distribuzione i.i.d. di Bernoulli.
Proponiamo una nuova forma di modello di autocodifica che incorpora le migliori proprietà degli autocodificatori variazionali (VAE) e delle reti generative avversarie (GAN).è noto che GAN può produrre campioni molto realistici mentre VAE non soffre del problema del collasso della modalità. Il nostro modello ottimizza la divergenza λ-Jeffreys tra la distribuzione del modello e la vera distribuzione dei dati.Mostriamo che prende le migliori proprietà degli obiettivi VAE e GAN.Consiste di due parti.Una di queste parti può essere ottimizzata usando l'addestramento adversariale standard, e la seconda è l'obiettivo stesso del modello VAE. Tuttavia, il modo diretto di sostituire la perdita VAE non funziona bene se usiamo una verosimiglianza esplicita come Gaussiana o Laplace che hanno una flessibilità limitata in dimensioni elevate e sono innaturali per modellare le immagini nello spazio dei pixel.Per affrontare questo problema proponiamo un nuovo approccio per addestrare il modello VAE con una verosimiglianza implicita da un discriminatore addestrato avversariamente. In un'ampia serie di esperimenti sui set di dati CIFAR-10 e TinyImagent, dimostriamo che il nostro modello raggiunge lo stato dell'arte della generazione e della qualità della ricostruzione e dimostriamo come possiamo bilanciare il comportamento del nostro modello tra il mode-seeking e il mode-covering regolando il peso λ nel nostro obiettivo.
Questo approccio incontra difficoltà quando il trasferimento non è reciprocamente vantaggioso, per esempio, quando i compiti sono sufficientemente dissimili o cambiano nel tempo. Qui, usiamo la connessione tra meta-apprendimento basato sul gradiente e Bayes gerarchico per proporre una miscela di modelli bayesiani gerarchici sui parametri di un approssimatore di funzione arbitraria come una rete neurale. Generalizzando l'algoritmo model-agnostic meta-learning (MAML), presentiamo una procedura di massimizzazione dell'aspettativa stocastica per stimare congiuntamente le inizializzazioni dei parametri per la discesa del gradiente così come un'assegnazione latente dei compiti alle inizializzazioni. Questo approccio cattura meglio la diversità dei compiti di formazione rispetto al consolidamento dei bias induttivi in un singolo set di iperparametri. I nostri esperimenti dimostrano una migliore generalizzazione sul benchmark standard miniImageNet per la classificazione 1-shot. Inoltre deriviamo una nuova e scalabile variante non parametrica del nostro metodo che cattura l'evoluzione di una distribuzione dei compiti nel tempo come dimostrato su un insieme di compiti di regressione a pochi colpi.
Introduciamo un nuovo algoritmo di routing per le reti di capsule, in cui una capsula figlio viene instradata verso un genitore basato solo sull'accordo tra lo stato del genitore e il voto del bambino.A differenza degli algoritmi di routing precedentemente proposti, la capacità del genitore di ricostruire il figlio non viene esplicitamente presa in considerazione per aggiornare le probabilità di routing.Questo semplifica la procedura di routing e migliora le prestazioni su set di dati di riferimento come CIFAR-10 e CIFAR-100. Il nuovo meccanismo 1) progetta l'instradamento tramite l'attenzione invertita del prodotto di punti; 2) impone la normalizzazione dei livelli come normalizzazione; e 3) sostituisce l'instradamento iterativo sequenziale con l'instradamento iterativo concorrente.Oltre a superare le reti di capsule esistenti, il nostro modello si comporta alla pari con una potente CNN (ResNet-18), usando meno del 25% dei parametri.  Su un compito diverso di riconoscere le cifre da immagini di cifre sovrapposte, il modello di capsula proposto esegue favorevolmente contro CNNs dato lo stesso numero di strati e neuroni per strato.  Crediamo che il nostro lavoro sollevi la possibilità di applicare le reti di capsule a compiti complessi del mondo reale.
 Introduciamo Doc2Dial, un framework end-to-end per la generazione di dati di conversazione basati su documenti aziendali tramite crowdsourcing.Tali dati possono essere utilizzati per addestrare agenti di dialogo automatizzati che svolgono compiti di assistenza clienti per le imprese o le organizzazioni.In particolare, il framework prende i documenti come input e genera i compiti per ottenere le annotazioni per simulare i flussi di dialogo. I flussi di dialogo sono utilizzati per guidare la raccolta degli enunciati prodotti dai lavoratori della folla. I risultati includono dati di dialogo basati sui documenti dati, così come vari tipi di annotazioni che aiutano a garantire la qualità dei dati e la flessibilità per (ri)comporre i dialoghi.
Catturare la struttura di alto livello nelle forme d'onda audio è una sfida perché un singolo secondo di audio comprende decine di migliaia di tempi.  Mentre le dipendenze a lungo raggio sono difficili da modellare direttamente nel dominio del tempo, dimostriamo che possono essere modellate più comodamente in rappresentazioni bidimensionali tempo-frequenza come gli spettrogrammi.  Sfruttando questo vantaggio rappresentazionale, in combinazione con un modello probabilistico altamente espressivo e una procedura di generazione multiscala, progettiamo un modello in grado di generare campioni audio ad alta fedeltà che catturano la struttura su scale temporali che i modelli nel dominio del tempo non hanno ancora raggiunto.  Dimostriamo che il nostro modello cattura le dipendenze a lungo raggio rispetto ai modelli del dominio del tempo come WaveNet su una serie diversificata di compiti di generazione incondizionati, tra cui la generazione del discorso a un solo parlante, la generazione del discorso a più parlanti e la generazione della musica.
In questo articolo mostriamo che le moderne CNN (VGG16, ResNet50 e InceptionResNetV2) possono cambiare drasticamente il loro output quando un'immagine viene traslata nel piano dell'immagine di pochi pixel, e che questo fallimento della generalizzazione avviene anche con altre piccole trasformazioni realistiche dell'immagine, inoltre vediamo questi fallimenti di generalizzazione più frequentemente nelle reti più moderne. Mostriamo che questi fallimenti sono legati al fatto che l'architettura delle moderne CNN ignora il classico teorema del campionamento, per cui la generalizzazione non è garantita. Mostriamo anche che le distorsioni nelle statistiche dei set di dati di immagini comunemente usati rendono improbabile che le CNN imparino a essere invarianti a queste trasformazioni.Presi insieme i nostri risultati suggeriscono che le prestazioni delle CNN nel riconoscimento degli oggetti sono molto inferiori alle capacità di generalizzazione degli umani.
Presentiamo un metodo in tempo reale per sintetizzare movimenti umani altamente complessi utilizzando un nuovo regime di allenamento che chiamiamo la rete neurale ricorrente autocondizionata (acRNN).Recentemente, i ricercatori hanno tentato di sintetizzare nuovi movimenti utilizzando tecniche autoregressive, ma i metodi esistenti tendono a bloccarsi o a divergere dopo un paio di secondi a causa di un accumulo di errori che vengono reimmessi nella rete.Inoltre, tali metodi hanno dimostrato di essere affidabili solo per movimenti umani relativamente semplici, come camminare o correre. Al contrario, il nostro approccio può sintetizzare movimenti arbitrari con stili altamente complessi, tra cui danze o arti marziali oltre alla locomozione. L'acRNN è in grado di realizzare questo accomodando esplicitamente l'accumulo di rumore autoregressivo durante la formazione. Il nostro lavoro è il primo a nostra conoscenza che dimostra la capacità di generare oltre 18.000 fotogrammi continui (300 secondi) di nuovi movimenti umani complessi con diversi stili.
 {I metodi di salienza tentano di spiegare la decisione di una rete profonda assegnando un punteggio ad ogni caratteristica/pixel nell'input, spesso facendo questa assegnazione di crediti attraverso il gradiente dell'output rispetto all'input. Recentemente \citet{adebayosan} ha messo in dubbio la validità di molti di questi metodi poiché non superano semplici controlli di sanità mentale, che verificano se i punteggi cambiano/vaniscono quando gli strati della rete addestrata sono randomizzati, o quando la rete viene ri-addestrata usando etichette casuali per gli input.  %Sorprendentemente, i metodi testati non hanno superato questi controlli: le spiegazioni erano relativamente invariate. Proponiamo una semplice correzione ai metodi di salienza esistenti che li aiuta a passare i controlli di sanità mentale, che noi chiamiamo la competizione del {\em per i pixel}.Questo comporta il calcolo delle mappe di salienza per tutte le possibili etichette nel compito di classificazione, e l'uso di una semplice competizione tra loro per identificare e rimuovere i pixel meno rilevanti dalla mappa.Viene fornita qualche giustificazione teorica e le sue prestazioni sono dimostrate empiricamente su diversi metodi popolari.
I sistemi di classificazione agiscono tipicamente in modo isolato, il che significa che sono tenuti a memorizzare implicitamente le caratteristiche di tutte le classi candidate al fine di classificare.Il costo di questo è un maggiore utilizzo della memoria e una scarsa efficienza del campione.Proponiamo un modello che invece verifica utilizzando immagini di riferimento durante il processo di classificazione, riducendo il peso della memorizzazione.Il modello utilizza query iterative non differenziabili per classificare un'immagine. Tuttavia, dimostriamo che trovare il giusto equilibrio tra riconoscimento e verifica delle immagini è essenziale per spingere il modello verso il comportamento desiderato, suggerendo che una pipeline di riconoscimento seguita da verifica è un approccio più promettente verso la progettazione di reti più potenti con architetture più semplici.
Per ridurre l'impronta di memoria e la latenza di esecuzione, tecniche come la potatura delle reti neurali e la binarizzazione sono state esplorate separatamente. Tuttavia, non è chiaro come combinare il meglio dei due mondi per ottenere modelli estremamente piccoli ed efficienti.  In questo articolo, per la prima volta, definiamo il problema della potatura a livello di filtro per le reti neurali binarie, che non può essere risolto semplicemente migrando i metodi di potatura strutturale esistenti per i modelli a piena precisione.  Un nuovo approccio basato sull'apprendimento è proposto per potare i filtri nella nostra struttura di rete principale/sussidiaria, dove la rete principale è responsabile dell'apprendimento delle caratteristiche rappresentative per ottimizzare le prestazioni di previsione, e la componente sussidiaria funziona come un selettore di filtri sulla rete principale. Per evitare il mismatch del gradiente durante l'addestramento della componente secondaria, proponiamo uno schema a strati e bottom-up.  Forniamo anche il confronto teorico e sperimentale tra i nostri metodi basati sull'apprendimento e quelli basati su regole avide.  Infine, dimostriamo empiricamente l'efficacia del nostro approccio applicato a diversi modelli binari, tra cui binarizedNIN, VGG-11, e ResNet-18, su vari set di dati di classificazione delle immagini.  Per il bi-nario ResNet-18 su ImageNet, usiamo il 78,6% di filtri ma possiamo raggiungere un errore di prova leggermente migliore 49,87% (50,02%-0,15%) rispetto al modello originale
L'ampia adozione di modelli complessi basati su RNN è ostacolata dalle loro prestazioni di inferenza, dal costo e dai requisiti di memoria. Per affrontare questo problema, sviluppiamo AntMan, combinando sinergicamente la sparsità strutturata con la decomposizione a basso rango, per ridurre il calcolo del modello, le dimensioni e il tempo di esecuzione di RNN mentre si raggiunge la precisione desiderata. La nostra valutazione mostra che AntMan offre una riduzione di calcolo fino a 100 volte con meno di 1 punto di caduta di precisione per i modelli di comprensione del linguaggio e di lettura automatica. La nostra valutazione mostra anche che per un dato obiettivo di precisione, AntMan produce modelli 5 volte più piccoli rispetto allo stato dell'arte.
I dati strutturati a grafo come le reti sociali, le reti funzionali del cervello, le reti di regolazione genetica, le reti di comunicazione hanno portato l'interesse a generalizzare le tecniche di deep learning ai domini a grafo. In questo articolo, siamo interessati a progettare reti neurali per grafi con lunghezza variabile al fine di risolvere problemi di apprendimento come la classificazione dei vertici, la classificazione dei grafi, la regressione dei grafi e i compiti generativi dei grafi. La maggior parte dei lavori esistenti si sono concentrati sulle reti neurali ricorrenti (RNN) per imparare rappresentazioni significative dei grafi, e più recentemente sono state introdotte nuove reti neurali convoluzionali (ConvNets). In questo lavoro, vogliamo confrontare rigorosamente queste due famiglie fondamentali di architetture per risolvere compiti di apprendimento dei grafi. Passiamo in rassegna le architetture RNN e ConvNet esistenti, e proponiamo un'estensione naturale di LSTM e ConvNet a grafi di dimensioni arbitrarie. Poi, progettiamo una serie di esperimenti analiticamente controllati su due problemi di grafi di base, cioè la corrispondenza di sottografi e il clustering di grafi, per testare le diverse architetture.  I risultati numerici mostrano che le Convnet a grafo proposte sono 3-17% più accurate e 1.5-4x più veloci delle RNN a grafo.Le Convnet a grafo sono anche il 36% più accurate delle tecniche variazionali (non-apprendimento).Infine, l'architettura ConvNet a grafo più efficace utilizza i bordi gated e la residualità.La residualità gioca un ruolo essenziale per apprendere le architetture multistrato in quanto fornisce un guadagno del 10% delle prestazioni.
Le reti neurali a valori complessi non sono un concetto nuovo, tuttavia, l'uso di valori reali è stato spesso favorito rispetto ai valori complessi a causa delle difficoltà di addestramento e dell'accuratezza dei risultati.La letteratura esistente ignora il numero di parametri utilizzati.Abbiamo confrontato le reti neurali a valori complessi e reali utilizzando cinque funzioni di attivazione.Abbiamo scoperto che quando le reti neurali reali e complesse vengono confrontate utilizzando semplici compiti di classificazione, le reti neurali complesse hanno prestazioni uguali o leggermente peggiori delle reti neurali a valori reali. Tuttavia, quando viene utilizzata un'architettura specializzata, le reti neurali a valori complessi superano le reti neurali a valori reali.Pertanto, le reti neurali a valori complessi dovrebbero essere utilizzate quando i dati di input sono anche complessi o possono essere significativamente al piano complesso, o quando l'architettura della rete utilizza la struttura definita utilizzando numeri complessi.
L'aumento dei dati strutturati a grafo come le reti sociali, le reti di regolazione, i grafi di citazione e le reti cerebrali funzionali, in combinazione con il clamoroso successo del deep learning in varie applicazioni, ha portato l'interesse a generalizzare i modelli di deep learning a domini non euclidei. L'ingrediente centrale del nostro modello è una nuova classe di funzioni complesse razionali parametriche (polinomi di Cayley) che permette di calcolare in modo efficiente i filtri spettrali sui grafi che si specializzano sulle bande di frequenza di interesse. Il nostro modello genera ricchi filtri spettrali che sono localizzati nello spazio, scala linearmente con la dimensione dei dati di input per i grafi sparsamente connessi, e può gestire diverse costruzioni di operatori Laplaciani. risultati sperimentali estesi mostrano le prestazioni superiori del nostro approccio sulla classificazione spettrale delle immagini, sulla rilevazione delle comunità, sulla classificazione dei vertici e sui compiti di completamento delle matrici.
Presentiamo FasterSeg, una rete di segmentazione semantica progettata automaticamente con non solo prestazioni allo stato dell'arte, ma anche una velocità maggiore rispetto ai metodi attuali. Utilizzando la ricerca dell'architettura neurale (NAS), FasterSeg viene scoperto da un nuovo e più ampio spazio di ricerca che integra rami multirisoluzione, che è stato recentemente trovato essere vitale nei modelli di segmentazione progettati manualmente. Per calibrare meglio l'equilibrio tra gli obiettivi di alta precisione e bassa latenza, proponiamo una regolarizzazione di latenza disaccoppiata e a grana fine, che supera efficacemente i nostri fenomeni osservati che le reti ricercate sono inclini a "collassare" in modelli a bassa latenza e scarsa precisione. Inoltre, estendiamo FasterSeg a una nuova struttura di ricerca collaborativa (co-searching), cercando simultaneamente una rete di insegnanti e una di studenti nella stessa esecuzione. La distillazione insegnante-studente aumenta ulteriormente l'accuratezza del modello dello studente. Gli esperimenti su benchmark popolari di segmentazione dimostrano la competenza di FasterSeg. Per esempio, FasterSeg può funzionare oltre il 30% più velocemente del concorrente più vicino progettato manualmente su Cityscapes, pur mantenendo una precisione comparabile.
Questo articolo introduce R2D3, un agente che fa un uso efficiente delle dimostrazioni per risolvere difficili problemi di esplorazione in ambienti parzialmente osservabili con condizioni iniziali altamente variabili. Introduciamo anche una serie di otto compiti che combinano queste tre proprietà, e dimostriamo che R2D3 può risolvere molti dei compiti in cui altri metodi allo stato dell'arte (sia con che senza dimostrazioni) non riescono a vedere nemmeno una singola traiettoria di successo dopo decine di miliardi di passi di esplorazione.
Indaghiamo il paesaggio dinamico appreso di una rete neurale ricorrente che risolve un semplice compito che richiede l'interazione di due meccanismi di memoria: a lungo e a breve termine. I nostri risultati mostrano che mentre la memoria a lungo termine è implementata da attrattori asintoti, il richiamo sequenziale è ora ulteriormente implementato da dinamiche oscillatorie in un sottospazio trasversale ai bacini di attrazione di questi stati stabili.Sulla base delle nostre osservazioni, proponiamo come diversi tipi di meccanismi di memoria possano coesistere e lavorare insieme in una singola rete neurale, e discutiamo possibili applicazioni ai campi dell'intelligenza artificiale e delle neuroscienze.
Il problema dell'esplorazione nell'apprendimento di rinforzo è ben compreso nel caso tabulare e molti algoritmi efficienti dal punto di vista del campionamento sono noti.Tuttavia, spesso non è chiaro come gli algoritmi nell'impostazione tabulare possano essere estesi a compiti con grandi spazi di stato in cui è richiesta la generalizzazione.Recenti sviluppi promettenti dipendono generalmente da modelli di densità specifici per il problema o caratteristiche artigianali. In questo articolo introduciamo un semplice approccio per l'esplorazione che ci permette di sviluppare algoritmi teoricamente giustificati nel caso tabulare, ma che ci danno anche intuizioni per nuovi algoritmi applicabili alle impostazioni in cui è richiesta l'approssimazione delle funzioni.Il nostro approccio e la sua teoria sottostante si basa sulla rappresentazione substocastica del successore, un concetto che sviluppiamo qui. Mentre la rappresentazione tradizionale del successore è una rappresentazione che definisce la generalizzazione degli stati in base alla somiglianza degli stati successori, la rappresentazione substocastica del successore è anche in grado di contare implicitamente il numero di volte che ogni stato (o caratteristica) è stato osservato.Questa estensione collega due aree di ricerca finora disgiunte. Mostriamo in domini tabellari tradizionali (RiverSwim e SixArms) che il nostro algoritmo si comporta empiricamente bene come altri algoritmi efficienti dal punto di vista del campionamento.Descriviamo poi un algoritmo di apprendimento di rinforzo profondo ispirato a queste idee e mostriamo che corrisponde alle prestazioni di recenti metodi basati sul pseudo-conteggio in giochi Atari 2600 ad esplorazione difficile.
La modellazione generativa profonda utilizzando i flussi ha guadagnato popolarità grazie alla stima esatta di log-likelihood trattabile con un efficiente processo di formazione e sintesi.Tuttavia, i modelli di flusso soffrono della sfida di avere uno spazio latente ad alta dimensionalità, la stessa dimensione dello spazio di input.Una soluzione efficace alla sfida di cui sopra come proposto da Dinh et al. (2016) è un'architettura multi-scala, che si basa sulla fattorizzazione iterativa anticipata di una parte delle dimensioni totali a intervalli regolari.I lavori precedenti sui flussi generativi che coinvolgono un'architettura multi-scala eseguono la fattorizzazione delle dimensioni basata su un mascheramento statico.Noi proponiamo una nuova architettura multi-scala che esegue la fattorizzazione dipendente dai dati per decidere quali dimensioni dovrebbero passare attraverso più livelli di flusso. Per facilitare lo stesso, introduciamo un'euristica basata sul contributo di ogni dimensione alla log-likelihood totale che codifica l'importanza delle dimensioni.La nostra euristica proposta è facilmente ottenuta come parte del processo di formazione del flusso, consentendo l'implementazione versatile della nostra architettura multiscala basata sul contributo di likelihood per modelli di flusso generici.Presentiamo una tale implementazione per il flusso originale introdotto in Dinh et al. (2016), e dimostriamo miglioramenti nel punteggio log-likelihood e qualità del campionamento su benchmark di immagini standard.Conduciamo anche studi di ablazione per confrontare il metodo proposto con altre opzioni per la fattorizzazione delle dimensioni.
Comprendere il flusso di informazioni nelle reti neurali profonde (DNN) è un problema impegnativo che ha guadagnato sempre più attenzione negli ultimi anni.Mentre diversi metodi sono stati proposti per spiegare le previsioni di rete, ci sono stati solo pochi tentativi di confrontarli da una prospettiva teorica.Per di più, nessun confronto empirico esaustivo è stato eseguito in passato.In questo lavoro analizziamo quattro metodi di attribuzione basati sul gradiente e dimostriamo formalmente le condizioni di equivalenza e approssimazione tra loro. Infine, proponiamo una nuova metrica di valutazione, chiamata Sensitivity-n e testiamo i metodi di attribuzione basati sul gradiente insieme a un semplice metodo di attribuzione basato sulla perturbazione su diversi set di dati nel dominio della classificazione delle immagini e del testo, utilizzando varie architetture di rete.
Studiamo SGD e Adam per la stima di un segnale di rango uno piantato nel rumore di matrice o tensore.L'estrema semplicità della configurazione del problema ci permette di isolare gli effetti di vari fattori: rapporto segnale-rumore, densità dei punti critici, stocasticità e inizializzazione.Osserviamo un fenomeno sorprendente: Adam sembra bloccarsi nei minimi locali non appena appaiono molti punti critici (caso matriciale), mentre SGD li evita; tuttavia, quando il numero di punti critici degenera in esponenziali (caso tensoriale), allora entrambi gli algoritmi rimangono intrappolati. La teoria ci dice che a SNR fisso il problema diventa intrattabile per grandi $d$ e nei nostri esperimenti SGD non sfugge a questo e mostriamo i benefici del warm starting in queste situazioni, concludendo che in questa classe di problemi, il warm starting non può essere sostituito dalla stocasticità dei gradienti per trovare il bacino di attrazione.
Questo articolo presenta un metodo per spiegare la conoscenza codificata in una rete neurale convoluzionale (CNN) quantitativamente e semanticamente.Come analizzare la logica specifica di ogni previsione fatta dalla CNN presenta uno dei problemi chiave della comprensione delle reti neurali, ma è anche di significativi valori pratici in certe applicazioni. In questo studio, proponiamo di distillare la conoscenza dalla CNN in un modello additivo spiegabile, in modo da poter utilizzare il modello spiegabile per fornire una spiegazione quantitativa per la previsione della CNN.Analizziamo il tipico problema di bias-interpretativo del modello spiegabile e sviluppiamo perdite precedenti per guidare l'apprendimento del modello additivo spiegabile.I risultati sperimentali hanno dimostrato l'efficacia del nostro metodo.
Presentiamo la metodologia per utilizzare la valutazione dinamica per migliorare i modelli neurali di sequenza.I modelli sono adattati alla storia recente tramite un meccanismo basato sulla discesa del gradiente, inducendoli ad assegnare maggiori probabilità ai modelli sequenziali ricorrenti.La valutazione dinamica supera i metodi di adattamento esistenti nei nostri confronti.La valutazione dinamica migliora lo stato dell'arte delle perplessità a livello di parola sulla Penn Treebank e sui set di dati WikiText-2 a 51.1 e 44.3 rispettivamente, e lo stato dell'arte dei caratteri a livello di cross-entropie sui set di dati Text8 e Hutter Prize a 1.19 bit/char e 1.08 bit/char rispettivamente.
Proponiamo un nuovo modo, basato sulla proiezione, di incorporare le informazioni condizionali nel discriminatore di GANs che rispetta il ruolo delle informazioni condizionali nel modello probabilistico sottostante. Questo approccio è in contrasto con la maggior parte delle strutture di GAN condizionali utilizzate oggi nelle applicazioni, che utilizzano l'informazione condizionale concatenando il vettore condizionale (incorporato) ai vettori delle caratteristiche. Con questa modifica, siamo stati in grado di migliorare significativamente la qualità della generazione di immagini condizionali di classe sul set di dati ILSVRC2012 (ImageNet) dal risultato attuale dello stato dell'arte, e abbiamo ottenuto questo con una singola coppia di un discriminatore e un generatore. Siamo stati anche in grado di estendere l'applicazione alla super-risoluzione e siamo riusciti a produrre immagini di super-risoluzione altamente discriminanti. Questa nuova struttura ha anche permesso una trasformazione di categoria di alta qualità basata sulla trasformazione funzionale parametrica degli strati di normalizzazione batch condizionali nel generatore.
La teoria dell'apprendimento ci dice che più dati sono migliori quando si minimizza l'errore di generalizzazione dei set di formazione e di prova distribuiti in modo identico.Tuttavia, quando la distribuzione di formazione e di prova differiscono, questo spostamento di distribuzione può avere un effetto significativo.Con una nuova prospettiva sull'apprendimento del trasferimento di funzione, siamo in grado di ridurre il cambiamento di prestazioni quando si trasferisce da formazione a set di prova con la distanza Wasserstein tra la formazione incorporata e la distribuzione del set di prova. Troviamo che c'è un compromesso che influisce sulle prestazioni tra quanto una funzione è invariante ai cambiamenti nella distribuzione di formazione e di prova e quanto è grande questo spostamento nella distribuzione. Empiricamente attraverso diversi domini di dati, sosteniamo questo punto di vista mostrando che le prestazioni di prova sono fortemente correlate con la distanza nelle distribuzioni dei dati tra la formazione e il set di prova.Complementare alla credenza popolare che più dati è sempre meglio, i nostri risultati evidenziano l'utilità di scegliere anche una distribuzione dei dati di formazione che è vicina alla distribuzione dei dati di prova quando la funzione appresa non è invariante a tali cambiamenti.
Introduciamo un nuovo sistema dinamico procedurale che può generare una varietà di forme che spesso appaiono come curve, ma tecnicamente, le figure sono trame di molti punti.Le chiamiamo spiroplot e mostriamo come questo nuovo sistema si collega ad altre procedure o processi che generano figure.Spiroplot sono un processo estremamente semplice ma con una sorprendente varietà visiva. Dimostriamo alcune proprietà fondamentali e analizziamo alcune istanze per vedere come la geometria o la topologia dell'input determina le figure generate.Mostriamo che alcuni spiroplot hanno un ciclo finito e ritornano alla situazione iniziale, mentre altri produrranno nuovi punti infinitamente spesso.Questo articolo è accompagnato da un'applicazione JavaScript che permette a chiunque di generare spiroplot.
La traduzione immagine-immagine non supervisionata mira ad apprendere una mappatura tra diversi domini visivi utilizzando coppie di allenamento non accoppiate.Studi recenti hanno mostrato un notevole successo nella traduzione immagine-immagine per domini multipli, ma soffrono di due limitazioni principali: sono costruiti da diverse mappature a due domini che devono essere apprese indipendentemente e/o generano risultati a bassa diversità, un fenomeno noto come collasso del modello.Per superare queste limitazioni, proponiamo un metodo chiamato GMM-UNIT basato su una rappresentazione disentangled contenuto-attributo, dove lo spazio attributo è dotato di un GMM. Ogni componente GMM rappresenta un dominio, e questa semplice assunzione ha due vantaggi importanti: in primo luogo, la dimensione dello spazio degli attributi non cresce linearmente con il numero di domini, come avviene in letteratura; in secondo luogo, la codifica continua dei domini permette l'interpolazione tra i domini e l'estrapolazione a domini non visti; inoltre, mostriamo come GMM-UNIT possa essere limitato a diversi metodi in letteratura, il che significa che GMM-UNIT è un quadro unificante per la traduzione immagine-immagine senza supervisione.
Presentiamo Compositional Attention Networks, una nuova architettura di rete neurale completamente differenziabile, progettata per facilitare il ragionamento esplicito ed espressivo.Mentre molti tipi di reti neurali sono efficaci nell'apprendere e generalizzare da enormi quantità di dati, questo modello si allontana dalle architetture monolitiche black-box verso un design che fornisce un forte priore per il ragionamento iterativo, permettendo di supportare l'apprendimento spiegabile e strutturato, così come la generalizzazione da una modesta quantità di dati.Il modello si basa sul grande successo delle cellule ricorrenti esistenti come le LSTM: Mette in sequenza una singola cellula ricorrente di Memoria, Attenzione e Controllo (MAC), e con un'attenta progettazione impone vincoli strutturali sul funzionamento di ogni cellula e sulle interazioni tra di esse, incorporando meccanismi di controllo esplicito e di attenzione morbida nelle loro interfacce. Dimostriamo la forza e la robustezza del modello sull'impegnativo set di dati CLEVR per il ragionamento visivo, raggiungendo un nuovo stato dell'arte con un'accuratezza del 98,9%, dimezzando il tasso di errore del modello migliore precedente.Ancora più importante, dimostriamo che il nuovo modello è più efficiente dal punto di vista computazionale, efficiente dal punto di vista dei dati, e richiede un ordine di grandezza inferiore di tempo e/o dati per ottenere buoni risultati.
  Gli autocodificatori variazionali (VAE) sono progettati per catturare informazioni comprimibili su un set di dati.  Di conseguenza, le informazioni memorizzate nello spazio latente sono raramente sufficienti per ricostruire una particolare immagine.  Per aiutare a capire il tipo di informazioni memorizzate nello spazio latente, addestriamo un decodificatore stile GAN vincolato a produrre immagini che il codificatore VAE mapperà nella stessa regione dello spazio latente, il che ci permette di ''immaginare'' le informazioni catturate nello spazio latente.  Noi sosteniamo che questo è necessario per rendere un VAE un modello veramente generativo.  Usiamo la nostra GAN per visualizzare lo spazio latente di una VAE standard e di una $\beta$-VAE.
Il nostro lavoro è motivato dalla visione che i modelli generativi forniscono un utile strumento per la comprensione di questa variabilità. A tal fine, questo manoscritto presenta due nuovi modelli generativi addestrati su dati reali di neuroimaging che sintetizzano immagini cerebrali funzionali dipendenti dal compito. Così, entrambi i modelli sono 3D condizionale Generative Adversarial reti (GANs) che applicano reti neurali convoluzionali (CNNs) per imparare anabstraction di rappresentazioni di immagine del cervello.I nostri risultati mostrano che le immagini generate cervello sono diverse, ma compito dipendente.Oltre alla valutazione qualitativa, utilizziamo i volumi generati cervello sintetico come ulteriori dati di formazione per migliorare i classificatori fMRI a valle (noto anche come decodifica, o lettura del cervello). Il nostro approccio raggiunge miglioramenti significativi per una varietà di set di dati, compiti di classificazione e punteggi di valutazione. I nostri risultati di classificazione forniscono una valutazione quantitativa della qualità delle immagini generate, e servono anche come un ulteriore contributo di questo manoscritto.
Il trasferimento di rappresentazioni da compiti supervisionati su larga scala a compiti a valle ha mostrato risultati eccezionali nel Machine Learning sia nella Computer Vision che nell'elaborazione del linguaggio naturale (NLP).Un esempio particolare può essere quello dei modelli sequenza-sequenza per la traduzione automatica (Neural Machine Translation - NMT). Questo perché, una volta addestrati in un setup multilingue, i sistemi NMT possono tradurre tra più lingue e sono anche in grado di eseguire traduzioni zero-shot tra coppie sorgente-target non viste al momento del test.In questo articolo, per prima cosa indaghiamo se possiamo estendere la capacità di trasferimento zero-shot dei sistemi NMT multilingue a compiti di NLP multilingue (compiti diversi dalla MT, ad esempio la classificazione dei sentimenti e l'inferenza del linguaggio naturale). Dimostriamo che una semplice struttura riutilizzando l'encoder di un sistema NMT multilingue, un Encoder-Classificatore multilingue, raggiunge notevoli prestazioni di classificazione cross-lingue a zero colpi, quasi out-of-the-box su tre compiti di benchmark a valle - Amazon Reviews, Stanford sentiment treebank (SST) e Stanford natural language inference (SNLI). Al fine di comprendere i fattori sottostanti che contribuiscono a questo risultato, abbiamo condotto una serie di analisi sull'effetto del vocabolario condiviso, il tipo di dati di formazione per i modelli NMT, la complessità del classificatore, la potenza di rappresentazione del codificatore e la generalizzazione del modello sulle prestazioni zero-shot. I nostri risultati forniscono una forte evidenza che le rappresentazioni apprese dai sistemi NMT multilingue sono ampiamente applicabili attraverso le lingue e i compiti, e le alte prestazioni di classificazione out-of-the-box sono correlate alla capacità di generalizzazione di tali sistemi.
A differenza degli approcci convenzionali che applicano l'evoluzione o il reinforcement learning su uno spazio di ricerca discreto e non differenziabile, il nostro metodo si basa sul rilassamento continuo della rappresentazione dell'architettura, permettendo una ricerca efficiente dell'architettura utilizzando la discesa del gradiente. Esperimenti estesi su CIFAR-10, ImageNet, Penn Treebank e WikiText-2 dimostrano che il nostro algoritmo eccelle nella scoperta di architetture convoluzionali ad alte prestazioni per la classificazione delle immagini e di architetture ricorrenti per la modellazione del linguaggio, essendo ordini di grandezza più veloci delle tecniche non differenziabili allo stato dell'arte.
Nonostante i notevoli progressi nella modellazione neurale del linguaggio, rimane una questione aperta quale sia la migliore strategia di decodifica per la generazione di testo da un modello di linguaggio (ad esempio per generare una storia).  L'osservazione empirica controintuitiva è che anche se l'uso della verosimiglianza come obiettivo di formazione porta a modelli di alta qualità per una vasta gamma di compiti di comprensione del linguaggio, i metodi di decodifica basati sulla massimizzazione come la ricerca del fascio portano alla degenerazione - testo in uscita che è insipido, incoerente, o si blocca in cicli ripetitivi. Il nostro approccio evita la degenerazione del testo troncando la coda inaffidabile della distribuzione di probabilità, campionando dal nucleo dinamico dei token che contengono la maggior parte della massa di probabilità. Per esaminare correttamente gli attuali metodi di decodifica basati sulla massimizzazione e stocastici, confrontiamo le generazioni di ciascuno di questi metodi con la distribuzione del testo umano lungo diversi assi come la probabilità, la diversità e la ripetizione. I nostri risultati mostrano che (1) la massimizzazione è un obiettivo di decodifica inappropriato per la generazione di testo aperto, (2) le distribuzioni di probabilità dei migliori modelli linguistici attuali hanno una coda inaffidabile che deve essere troncata durante la generazione e (3) Nucleus Sampling è la migliore strategia di decodifica per generare testo di forma lunga che sia di alta qualità - come misurato dalla valutazione umana - e diversificato come il testo scritto da un umano.
Fino a poco tempo fa, ispirato da una massa di ricerche su esempi di avversari per la computer vision, c'è stato un crescente interesse nella progettazione di attacchi avversari per compiti di Natural Language Processing (NLP), seguito da pochissimi lavori di difese avversarie per NLP. A nostra conoscenza, non esiste alcun metodo di difesa contro gli attacchi basati sulla sostituzione dei sinonimi, che mirano a soddisfare tutti i vincoli lessicali, grammaticali e semantici e quindi sono difficili da percepire dagli esseri umani. Noi contribuiamo a colmare questa lacuna e proponiamo un nuovo metodo di difesa avversaria chiamato Synonym Encoding Method (SEM), che inserisce un codificatore prima dello strato di input del modello e poi allena il modello per eliminare le perturbazioni avversarie. Per valutare meglio il SEM, progettiamo anche un metodo di attacco forte chiamato Improved Genetic Algorithm (IGA) che adotta la metaeuristica genetica per gli attacchi basati sulla sostituzione dei sinonimi. Rispetto all'attacco avversario esistente basato sulla genetica, IGA può raggiungere un tasso di successo dell'attacco più alto mantenendo la trasferibilità degli esempi avversari.
Uno degli svantaggi principali della backpropagation through time (BPTT) è la difficoltà di apprendere le dipendenze a lungo termine, derivante dal dover propagare le informazioni di credito all'indietro attraverso ogni singolo passo della computazione in avanti.Questo rende la BPTT sia computazionalmente impraticabile che biologicamente implausibile.Per questa ragione, la backpropagation through time completa è raramente usata su lunghe sequenze, e la backpropagation troncata through time è usata come euristica.  Tuttavia, questo di solito porta a stime distorte del gradiente in cui le dipendenze a lungo termine sono ignorate.  Affrontando questo problema, proponiamo un algoritmo alternativo, Sparse Attentive Backtracking, che potrebbe anche essere collegato ai principi usati dal cervello per imparare le dipendenze a lungo termine. Sparse Attentive Backtracking impara un meccanismo di attenzione sugli stati nascosti del passato e retropropaga selettivamente attraverso i percorsi con alti pesi di attenzione.  Questo permette al modello di imparare le dipendenze a lungo termine mentre si effettua il backtracking solo per un piccolo numero di passi temporali, non solo dal passato recente ma anche dagli stati passati rilevanti.   
In questo lavoro proponiamo un nuovo quadro di apprendimento avversaria.I metodi di apprendimento avversaria esistenti coinvolgono due reti separate, cioè Le informazioni catturate dai modelli discriminativi completano quelle dei modelli di predizione strutturati, ma poche ricerche esistenti hanno studiato l'utilizzo di tali informazioni per migliorare i modelli di predizione strutturati nella fase di inferenza.In questo lavoro, proponiamo di raffinare le previsioni dei modelli di predizione strutturati integrando efficacemente i modelli discriminativi nella predizione.I modelli discriminativi sono trattati come modelli basati sull'energia. Simile all'apprendimento avversario, i modelli discriminativi sono addestrati per stimare i punteggi che misurano la qualità delle uscite previste, mentre i modelli di predizione strutturati sono addestrati per prevedere le uscite contrastanti con i punteggi energetici massimi.In questo modo, il problema del gradiente che svanisce è migliorato, e quindi siamo in grado di eseguire l'inferenza seguendo le direzioni del gradiente di salita dei modelli discriminativi per raffinare i modelli di predizione strutturati.Il metodo proposto è in grado di gestire una serie di compiti, ad esempio, la classificazione multietichetta e la segmentazione delle immagini.  I risultati empirici su questi due compiti convalidano l'efficacia del nostro metodo di apprendimento.
Proponiamo RaPP, una nuova metodologia per il rilevamento delle novità utilizzando i valori di attivazione dello spazio nascosto ottenuti da un autocodificatore profondo.Precisamente, RaPP confronta l'input e la sua ricostruzione autoencoder non solo nello spazio di input ma anche negli spazi nascosti.Mostriamo che se alimentiamo nuovamente un input ricostruito allo stesso autoencoder, i suoi valori attivati in uno spazio nascosto sono equivalenti alla ricostruzione corrispondente in quello spazio nascosto dato l'input originale. Per aggregare i valori di attivazione degli spazi nascosti, proponiamo due metriche che migliorano le prestazioni di rilevamento delle novità.Attraverso ampi esperimenti con diversi set di dati, validiamo che RaPP migliora le prestazioni di rilevamento delle novità degli approcci basati su autoencoder.Inoltre, dimostriamo che RaPP supera i recenti metodi di rilevamento delle novità valutati su benchmark popolari.
Imparare le preferenze degli utenti sulle tracce dei piani può essere un compito impegnativo, dato il gran numero di caratteristiche e le query limitate che possiamo chiedere a un singolo utente. Inoltre, la funzione di preferenza stessa può essere abbastanza contorta e non lineare. Valutiamo l'impatto dell'apprendimento attivo sul numero di tracce necessarie per addestrare un modello che sia accurato e interpretabile, confrontando la suddetta rete feedforward con un modello di rete neurale più complesso che utilizza LSTM ed è addestrato con un set di dati più grande senza apprendimento attivo.
L'apprendimento per rinforzo è un framework promettente per risolvere problemi di controllo, ma il suo uso in situazioni pratiche è ostacolato dal fatto che le funzioni di ricompensa sono spesso difficili da ingegnerizzare.Specificare obiettivi e compiti per macchine autonome, come i robot, è una sfida significativa: convenzionalmente, le funzioni di ricompensa e gli stati obiettivo sono stati utilizzati per comunicare obiettivi.Ma le persone possono comunicare obiettivi tra loro semplicemente descrivendoli o dimostrandoli.Come possiamo costruire algoritmi di apprendimento che ci permetteranno di dire alle macchine cosa vogliamo che facciano? In questo lavoro, indaghiamo il problema di fondare i comandi linguistici come funzioni di ricompensa usando l'apprendimento di rinforzo inverso, e sosteniamo che le ricompense condizionate dal linguaggio sono più trasferibili delle politiche condizionate dal linguaggio a nuovi ambienti. Proponiamo l'apprendimento di ricompensa condizionato dal linguaggio (LC-RL), che fonda i comandi linguistici come una funzione di ricompensa rappresentata da una rete neurale profonda. dimostriamo che il nostro modello impara ricompense che si trasferiscono a nuovi compiti e ambienti su ambienti realistici, ad alta densità visiva con comandi in linguaggio naturale, mentre imparare direttamente una politica condizionata dal linguaggio porta a scarse prestazioni.
Molti sistemi biologici di apprendimento come il corpo del fungo, l'ippocampo e il cervelletto sono costruiti da reti di neuroni scarsamente connessi. Per una nuova comprensione di tali reti, studiamo gli spazi di funzione indotti da funzioni casuali sparse e caratterizziamo quali funzioni possono e non possono essere apprese. Una rete con ingressi d per neurone risulta essere equivalente a un modello additivo di ordine d, mentre con una distribuzione di grado la rete combina termini additivi di ordini diversi. Identifichiamo tre vantaggi specifici della sparsità: l'approssimazione della funzione additiva è un potente bias induttivo che limita la maledizione della dimensionalità, le reti sparse sono stabili al rumore outlier negli input, e le caratteristiche casuali sparse sono scalabili.Quindi, anche le semplici architetture del cervello possono essere potenti approssimatori di funzioni.Infine, speriamo che questo lavoro aiuti a rendere popolari le teorie del kernel delle reti tra i neuroscienziati computazionali.
Proponiamo una nuova applicazione delle tecniche di embedding al recupero dei problemi nel tutoring adattivo.L'obiettivo è quello di recuperare problemi simili nei concetti matematici.Ci sono due sfide: In primo luogo, come le frasi, i problemi utili al tutoraggio non sono mai esattamente gli stessi in termini di concetti sottostanti.Invece, i buoni problemi mescolano i concetti in modi innovativi, pur mostrando continuità nelle loro relazioni.In secondo luogo, è difficile per gli esseri umani determinare un punteggio di somiglianza coerente attraverso un set di allenamento abbastanza grande. Proponiamo un algoritmo di incorporazione gerarchica del problema, chiamato Prob2Vec, che consiste in una fase di astrazione e una fase di incorporazione.Prob2Vec raggiunge il 96,88% di accuratezza su un test di somiglianza del problema, in contrasto con il 75% ottenuto applicando direttamente metodi di incorporazione delle frasi allo stato dell'arte. È sorprendente che Prob2Vec sia in grado di distinguere differenze molto fini tra i problemi, un'abilità che gli esseri umani hanno bisogno di tempo e sforzi per acquisire.Inoltre, il sottoproblema dell'etichettatura dei concetti con un set di dati di allenamento sbilanciato è interessante di per sé. è un problema multietichetta che soffre dell'esplosione della dimensionalità, che proponiamo modi per migliorare.proponiamo il nuovo algoritmo di pre-addestramento negativo che riduce drasticamente i rapporti falsi negativi e positivi per la classificazione, utilizzando un set di dati di allenamento sbilanciato.
Introduciamo una nuova rete neurale convoluzionale profonda, CrescendoNet, impilando semplici blocchi di costruzione senza connessioni residue.Ogni blocco Crescendo contiene percorsi di convoluzione indipendenti con profondità aumentate.Il numero di strati di convoluzione e i parametri sono aumentati solo linearmente nei blocchi Crescendo. Negli esperimenti, CrescendoNet con solo 15 strati supera quasi tutte le reti senza connessioni residue sui set di dati di riferimento, CIFAR10, CIFAR100, e SVHN.Data una quantità sufficiente di dati come nel set di dati SVHN, CrescendoNet con 15 strati e 4.1M parametri può eguagliare le prestazioni di DenseNet-BC con 250 strati e 15. 3M parametri. CrescendoNet fornisce un nuovo modo per costruire reti neurali convoluzionali profonde ad alte prestazioni senza connessioni residue.Inoltre, attraverso lo studio del comportamento e delle prestazioni delle sottoreti in CrescendoNet, notiamo che le alte prestazioni di CrescendoNet possono derivare dal suo comportamento implicito di insieme, che differisce da FractalNet che è anche una rete neurale convoluzionale profonda senza connessioni residue.Inoltre, l'indipendenza tra i percorsi in CrescendoNet ci permette di introdurre una nuova procedura di formazione path-wise, che può ridurre la memoria necessaria per la formazione.
I processi gaussiani sono la classe principale di distribuzioni di funzioni casuali, ma soffrono di problemi ben noti tra cui la difficoltà di scalare e l'inflessibilità rispetto a certi vincoli di forma (come la non negatività).Qui proponiamo Deep Random Splines, una classe flessibile di funzioni casuali ottenute trasformando il rumore gaussiano attraverso una rete neurale profonda il cui output sono i parametri di una spline. A differenza dei processi gaussiani, le Deep Random Splines ci permettono di applicare facilmente i vincoli di forma, ereditando la ricchezza e la trattabilità dei modelli generativi profondi. Presentiamo anche un modello osservazionale per i dati dei processi puntuali che utilizza le Deep Random Splines per modellare la funzione di intensità di ogni processo puntuale e lo applichiamo ai dati delle neuroscienze per ottenere una rappresentazione bidimensionale dell'attività di spike.L'inferenza viene eseguita tramite un autocodificatore variazionale che utilizza una nuova architettura di codifica ricorrente che può gestire più processi puntuali come input.
Il recente sviluppo del Natural Language Processing (NLP) ha ottenuto un grande successo utilizzando grandi modelli pre-addestrati con centinaia di milioni di parametri. Tuttavia, questi modelli soffrono della dimensione pesante del modello e dell'alta latenza, così che non possiamo distribuirli direttamente su dispositivi mobili con risorse limitate. In questo articolo, proponiamo MobileBERT per comprimere e accelerare il popolare modello BERT.Come BERT, MobileBERT è task-agnostic; cioè, può essere applicato universalmente a vari compiti di NLP a valle attraverso la regolazione fine.MobileBERT è una versione snella di BERT-LARGE aumentata con strutture bottleneck e un equilibrio attentamente progettato tra auto-attenzioni e reti feed-forward. Per addestrare MobileBERT, usiamo uno schema progressivo bottom-to-top per trasferire la conoscenza intrinseca di un insegnante BERT-LARGE a collo di bottiglia invertito appositamente progettato.Studi empirici dimostrano che MobileBERT è 4,3x più piccolo e 4,0x più veloce dell'originale BERT-BASE, mentre raggiunge risultati competitivi su benchmark NLP noti. Sui compiti di inferenza del linguaggio naturale di GLUE, MobileBERT raggiunge una degradazione delle prestazioni di 0,6 punti GLUE e una latenza di 367 ms su un telefono Pixel 3. Sul compito di risposta alle domande SQuAD v1.1/v2.0, MobileBERT raggiunge un punteggio F1 di 90,0/79,2 dev, che è 1,5/2,1 superiore a BERT-BASE.
L'importance weighted autoencoder (IWAE) (Burda et al, 2016) è un popolare metodo di inferenza variazionale che raggiunge un limite di evidenza più stretto (e quindi un bias più basso) rispetto agli autocodificatori variazionali standard ottimizzando un obiettivo multi-campione, cioè un obiettivo che è esprimibile come un integrale su $K > 1$ campioni Monte Carlo.Purtroppo, IWAE si basa in modo cruciale sulla disponibilità di riparametrizzazioni e anche se queste esistono, l'obiettivo multi-campione porta a gradienti di inferenza-rete che si rompono all'aumentare di $K$ (Rainforth et al., Questa rottura può essere aggirata solo rimuovendo i termini della funzione di punteggio ad alta varianza, ignorandoli euristicamente (il che produce il gradiente IWAE (IWAE-STL) 'sticking-the-landing' di Roeder et al. (2017)) o attraverso un'identità di Tucker et al. (2019) (che produce il gradiente IWAE (IWAE-DREG) 'doubly-reparametrised'). In questo lavoro, sosteniamo che ottimizzare direttamente la distribuzione della proposta nel campionamento di importanza come nell'algoritmo reweighted wake-sleep (RWS) di Bornschein & Bengio (2015) è preferibile all'ottimizzazione degli obiettivi multi-campione di tipo IWAE. Per formalizzare questo argomento, introduciamo un quadro di campionamento adattivo-importante chiamato adaptive importance sampling for learning (AISLE) che generalizza leggermente l'algoritmo RWS. Mostriamo poi che AISLE ammette IWAE-STL e IWAE-DREG (cioè i gradienti IWAE che evitano la ripartizione) come casi speciali.
Man mano che la dimensione e la complessità dei modelli e dei set di dati crescono, cresce anche la necessità di varianti efficienti dal punto di vista della comunicazione della discesa del gradiente stocastico che possono essere distribuite su cluster per eseguire il model fitting in parallelo.Alistarh et al. (2017) descrivono due varianti di SGD in parallelo che quantizzano e codificano i gradienti per ridurre i costi di comunicazione.Per la prima variante, QSGD, forniscono forti garanzie teoriche. Per la seconda variante, che chiamiamo QSGDinf, dimostrano guadagni empirici impressionanti per l'addestramento distribuito di grandi reti neurali. basandoci sul loro lavoro, proponiamo uno schema alternativo per quantizzare i gradienti e mostriamo che produce garanzie teoriche più forti di quelle che esistono per QSGD mentre corrisponde alla performance empirica di QSGDinf.
L'impressionante apprendimento permanente nei cervelli animali è principalmente abilitato da cambiamenti plastici nella connettività sinaptica. è importante notare che questi cambiamenti non sono passivi, ma sono attivamente controllati dalla neuromodulazione, che è essa stessa sotto il controllo del cervello. le risultanti capacità auto-modificanti del cervello svolgono un ruolo importante nell'apprendimento e nell'adattamento e sono una base importante per l'apprendimento di rinforzo biologico. qui mostriamo per la prima volta che le reti neurali artificiali con tale plasticità neuromodulata possono essere addestrate con la discesa del gradiente. In un compito, le LSTM plastiche neuromodulate con milioni di parametri superano le LSTM standard in un compito di modellazione linguistica di riferimento (controllando il numero di parametri), concludendo che la neuromodulazione differenziabile della plasticità offre un nuovo potente quadro per l'addestramento delle reti neurali.
Tuttavia, l'apprendimento dei parametri delle reti neurali di solito richiede una grande quantità di dati etichettati. Gli algoritmi di apprendimento profondo, quindi, incontrano difficoltà quando vengono applicati all'apprendimento supervisionato dove sono disponibili solo pochi dati. Questo compito specifico è chiamato apprendimento a pochi colpi. Il volume del simplex è usato per la misura della dispersione della classe.Durante il test, combinato con il campione di test e i punti della classe, si forma un nuovo simplex.Quindi la somiglianza tra il campione di test e la classe può essere quantizzata con il rapporto tra i volumi del nuovo simplex e il simplex originale della classe. Inoltre, presentiamo un approccio per costruire i simplex usando le regioni locali delle mappe di caratteristiche prodotte dalle reti neurali convoluzionali.Gli esperimenti su Omniglot e miniImageNet verificano l'efficacia del nostro algoritmo simplex sull'apprendimento a pochi colpi.
Il calcolo del serbatoio è un potente strumento per spiegare come il cervello impara sequenze temporali, come i movimenti, ma gli schemi di apprendimento esistenti sono biologicamente poco plausibili o troppo inefficienti per spiegare le prestazioni degli animali.Mostriamo che una rete può imparare sequenze complicate con una regola di apprendimento Hebbian modulata dalla ricompensa se la rete di neuroni del serbatoio è combinata con una seconda rete che serve come una memoria di lavoro dinamica e fornisce un segnale dorsale spazio-temporale al serbatoio. In combinazione con la memoria di lavoro, l'apprendimento Hebbian modulato dalla ricompensa dei neuroni di lettura funziona bene come l'apprendimento FORCE, ma con il vantaggio di un'interpretazione biologicamente plausibile sia della regola di apprendimento che del paradigma di apprendimento.
Le architetture convoluzionali hanno recentemente dimostrato di essere competitive nei compiti di modellazione di molte sequenze rispetto allo standard de-facto delle reti neurali ricorrenti (RNN), fornendo vantaggi computazionali e di modellazione grazie al parallelismo intrinseco. Tuttavia, attualmente, rimane un performancegap alle varianti RNN stocastiche più espressive, specialmente quelle con diversi strati di variabili casuali dipendenti. In questo lavoro, proponiamo reti convoluzionali temporali stocastiche (STCNs), una nuova architettura che combina i vantaggi computazionali delle reti convoluzionali temporali (TCN) con il potere rappresentazionale e la robustezza degli spazi latenti stocastici. In particolare, proponiamo una gerarchia di variabili latenti stocastiche che cattura le dipendenze temporali a diverse scale temporali. L'architettura è modulare e flessibile grazie al disaccoppiamento degli strati deterministici e stocastici. Mostriamo che l'architettura proposta raggiunge lo stato dell'arte delle log-likelihood in diversi compiti.
La mappatura di contrazione debole è una mappatura di sé che l'intervallo è sempre un sottoinsieme del dominio, che ammette un punto fisso unico.L'iterazione della mappatura di contrazione debole è una sequenza di Cauchy che produce il punto fisso unico.Un metodo di ottimizzazione senza gradiente come applicazione della mappatura di contrazione debole è proposto per ottenere una convergenza minima globale.Il metodo di ottimizzazione è robusto ai minimi locali e alla posizione iniziale del punto.
Nell'ultimo decennio, sono emerse due strategie di controllo concorrenti per risolvere compiti di controllo complessi con alta efficacia.Algoritmi di controllo basati sul modello, come il controllo modello-predittivo (MPC) e l'ottimizzazione della traiettoria, peer nei gradienti della dinamica del sistema sottostante al fine di risolvere compiti di controllo con alta efficienza del campione.  Tuttavia, come tutti i metodi di ottimizzazione numerica basati sul gradiente, i metodi di controllo basati sul modello sono sensibili alle inizializzazioni e sono inclini a rimanere intrappolati nei minimi locali.Deep reinforcement learning (DRL), d'altra parte, può in qualche modo alleviare questi problemi esplorando lo spazio della soluzione attraverso il campionamento - a spese del costo computazionale. In questo articolo, presentiamo un metodo ibrido che combina i migliori aspetti dei metodi basati sul gradiente e della DRL. Basiamo il nostro algoritmo sull'algoritmo dei gradienti di politica deterministica profonda (DDPG) e proponiamo una semplice modifica che utilizza i gradienti reali da un simulatore fisico differenziabile per aumentare il tasso di convergenza sia dell'attore che del critico.  Dimostriamo il nostro algoritmo su sette compiti di controllo di robot 2D, il più complesso dei quali è un mezzo ghepardo differenziabile con vincoli di contatto duri. I risultati empirici mostrano che il nostro metodo aumenta le prestazioni di DDPG senza sacrificare la sua robustezza ai minimi locali.
Un hypernetwork bayesiano, h, è una rete neurale che impara a trasformare una semplice distribuzione di rumore, p(e) = N(0,I), in una distribuzione q(t) := q(h(e)) sui parametri t di un'altra rete neurale (la rete primaria). Addestriamo q con inferenza variazionale, utilizzando una h invertibile per consentire una stima efficiente del limite inferiore variazionale sul posteriore p(t | D) attraverso il campionamento. In contrasto con la maggior parte dei metodi per l'apprendimento profondo bayesiano, le iperreti bayesiane possono rappresentare un complesso posteriore approssimativo multimodale con correlazioni tra i parametri, pur consentendo un campionamento iido economico di q(t).  In pratica, le iperreti bayesiane forniscono una migliore difesa contro gli esempi avversi rispetto al dropout, e mostrano anche prestazioni competitive su una serie di compiti che valutano l'incertezza del modello, tra cui la regolarizzazione, l'apprendimento attivo e il rilevamento delle anomalie.
Gli approcci di ottimizzazione basati sull'evoluzione hanno recentemente mostrato risultati promettenti in domini come Atari e la locomozione dei robot, ma meno nella risoluzione di compiti 3D direttamente dai pixel.Questo articolo presenta un metodo chiamato Deep Innovation Protection (DIP) che permette di addestrare modelli complessi del mondo end-to-end per tali ambienti 3D.L'idea principale dietro l'approccio è di impiegare l'ottimizzazione multiobiettivo per ridurre temporalmente la pressione di selezione su componenti specifici in un modello del mondo, permettendo ad altri componenti di adattarsi.Studiamo le rappresentazioni emergenti di queste reti evolute, che imparano un modello del mondo senza la necessità di una perdita specifica per la previsione in avanti.
L'addestramento avversario è uno dei modi più popolari per imparare modelli robusti, ma è di solito dipendente dagli attacchi e costoso in termini di tempo.In questo articolo, proponiamo l'algoritmo MACER, che impara modelli robusti senza usare l'addestramento avversario, ma esegue meglio di tutte le difese l2 dimostrabili esistenti.Un lavoro recente mostra che lo smoothing randomizzato può essere usato per fornire raggio l2 certificato ai classificatori smoothed, e il nostro algoritmo allena classificatori smoothed provabilmente robusti tramite MAximizing the CErtified Radius (MACER). Nei nostri esperimenti, dimostriamo che il nostro metodo può essere applicato alle moderne reti neurali profonde su una vasta gamma di set di dati, tra cui Cifar-10, ImageNet, MNIST, e SVHN.Per tutti i compiti, MACER impiega meno tempo di formazione rispetto agli algoritmi di formazione adversariali allo stato dell'arte, e i modelli appresi raggiungono un raggio medio certificato più grande.
I metodi actor-critic risolvono i problemi di apprendimento di rinforzo aggiornando una politica parametrizzata nota come attore in una direzione che aumenta una stima del ritorno atteso nota come critico.Tuttavia, i metodi actor-critic esistenti usano solo i valori o i gradienti del critico per aggiornare il parametro della politica.In questo articolo, proponiamo un nuovo metodo actor-critic chiamato la guida actor-critic (GAC).GAC impara prima un attore guida che massimizza localmente il critico e poi aggiorna il parametro della politica basato sull'attore guida tramite apprendimento supervisionato. I nostri principali contributi teorici sono due: in primo luogo, dimostriamo che GAC aggiorna l'attore guida eseguendo un'ottimizzazione del secondo ordine nello spazio d'azione dove la matrice di curvatura è basata sugli Hessiani del critico; in secondo luogo, dimostriamo che il metodo deterministico del gradiente della politica è un caso speciale di GAC quando gli Hessiani sono ignorati; attraverso gli esperimenti, dimostriamo che il nostro metodo è un promettente metodo di apprendimento di rinforzo per controlli continui.
Deep Infomax~(DIM) è una struttura di apprendimento della rappresentazione non supervisionata che massimizza l'informazione reciproca tra gli ingressi e le uscite di un codificatore, mentre i vincoli probabilistici sono imposti sulle uscite.In questo articolo, proponiamo Supervised Deep InfoMax~(SDIM), che introduce vincoli probabilistici supervisionati alle uscite del codificatore. I vincoli probabilistici supervisionati sono equivalenti a un classificatore generativo su rappresentazioni di dati di alto livello, dove le log-likelihood condizionali di classe dei campioni possono essere valutate.A differenza di altri lavori che costruiscono classificatori generativi con modelli generativi condizionali, gli SDIM scalano su dataset complessi e possono raggiungere prestazioni comparabili con le controparti discriminative.  Con SDIM, possiamo eseguire una classificazione con rifiuto: invece di riportare sempre un'etichetta di classe, SDIM fa previsioni solo quando i logit più grandi dei campioni di test superano alcune soglie prescelte, altrimenti saranno considerati fuori dalle distribuzioni di dati e saranno rifiutati.  I nostri esperimenti dimostrano che SDIM con la politica di rifiuto può efficacemente rifiutare gli input illegali, compresi i campioni fuori dalla distribuzione e gli esempi avversari.
Abstract In questo lavoro, descriviamo un insieme di regole per la progettazione e l'inizializzazione di reti neurali ben condizionate, guidate dall'obiettivo di bilanciare naturalmente i blocchi diagonali dell'Hessiano all'inizio della formazione. Dimostriamo che per un perceptron multistrato profondo basato su ReLU, un semplice schema di inizializzazione che utilizza la media geometrica del fan-in e del fan-out soddisfa la nostra regola di scaling.Per architetture più sofisticate, mostriamo come il nostro principio di scaling possa essere usato per guidare le scelte di design per produrre reti neurali ben condizionate, riducendo il lavoro di congettura.
Studiamo il problema dell'estrazione del modello nell'elaborazione del linguaggio naturale, in cui un avversario con solo l'accesso alla query a un modello vittima tenta di ricostruire una copia locale di quel modello.Supponendo che sia l'avversario che il modello vittima mettano a punto un grande modello linguistico pretrainato come BERT (Devlin et al, Infatti, l'aggressore non ha nemmeno bisogno di utilizzare query grammaticali o semanticamente significative: dimostriamo che sequenze casuali di parole accoppiate con euristiche specifiche del compito formano query efficaci per l'estrazione del modello su una serie diversificata di compiti NLP tra cui l'inferenza del linguaggio naturale e la risposta alle domande. Il nostro lavoro evidenzia quindi un exploit reso fattibile solo dallo spostamento verso metodi di transfer learning all'interno della comunità NLP: per un budget di query di poche centinaia di dollari, un attaccante può estrarre un modello che esegue solo leggermente peggio del modello vittima.Infine, studiamo due strategie di difesa contro l'estrazione del modello - classificazione di appartenenza e watermarking API - che mentre hanno successo contro alcuni avversari possono anche essere aggirati da altri più intelligenti.
Proponiamo SEARNN, un nuovo algoritmo di addestramento per reti neurali ricorrenti (RNNs) ispirato all'approccio "learning to search" (L2S) alla predizione strutturata.RNNs hanno avuto molto successo in applicazioni di predizione strutturata come la traduzione automatica o parsing, e sono comunemente addestrati utilizzando la stima di massima verosimiglianza (MLE). Sfortunatamente, questa perdita di addestramento non è sempre un surrogato appropriato per l'errore di test: massimizzando solo la probabilità di verità a terra, non riesce a sfruttare la ricchezza di informazioni offerta dalle perdite strutturate. Inoltre, introduce discrepanze tra l'addestramento e la previsione (come il bias di esposizione) che possono danneggiare le prestazioni di test: In seguito, proponiamo una strategia di sottocampionamento per permettere a SEARNN di scalare a vocabolari di grandi dimensioni, il che ci permette di convalidare i benefici del nostro approccio su un compito di traduzione automatica.
L'apprendimento di rinforzo profondo ha dimostrato capacità crescenti per problemi di controllo continuo, compresi gli agenti che possono muoversi con abilità e agilità attraverso il loro ambiente. Un problema aperto in questa impostazione è quello di sviluppare buone strategie per integrare o fondere le politiche per più abilità, dove ogni singola abilità è uno specialista in una specifica abilità e la sua distribuzione di stato associata. Estendiamo i metodi di distillazione delle politiche all'impostazione dell'azione continua e sfruttiamo questa tecnica per combinare le politiche degli esperti, come valutato nel dominio della locomozione bipede simulata attraverso diverse classi di terreno, introducendo anche un metodo di iniezione di input per aumentare una rete di politiche esistenti per sfruttare nuove caratteristiche di input. Infine, il nostro metodo utilizza l'apprendimento di trasferimento per assistere nell'acquisizione efficiente di nuove competenze.La combinazione di questi metodi permette ad una politica di essere incrementata in modo incrementale con nuove competenze.Confrontiamo il nostro apprendimento progressivo e l'integrazione tramite distillazione (PLAID) metodo contro tre linee di base alternative.
Deep Reinforcement Learning (DRL) ha portato a molte scoperte recenti su compiti di controllo complessi, come sconfiggere il miglior giocatore umano nel gioco del Go.Tuttavia, le decisioni prese dall'agente DRL non sono spiegabili, ostacolando la sua applicabilità in ambienti critici per la sicurezza. Viper, una tecnica proposta di recente, costruisce una politica ad albero delle decisioni imitando l'agente DRL.Gli alberi delle decisioni sono interpretabili in quanto ogni azione compiuta può essere ricondotta al percorso delle regole decisionali che l'hanno condotta.Tuttavia, un albero delle decisioni globale che approssima la politica DRL ha limitazioni significative rispetto alla geometria dei confini delle decisioni. Proponiamo MoET, un modello più espressivo, ma ancora interpretabile, basato su Mixture of Experts, che consiste in una funzione di gating che partiziona lo spazio di stato, e più esperti di alberi decisionali che si specializzano su partizioni diverse.Proponiamo una procedura di formazione per supportare gli esperti di alberi decisionali non differenziabili e la integriamo nella procedura di apprendimento per imitazione di Viper. Valutiamo il nostro algoritmo su quattro ambienti di palestra OpenAI, e dimostriamo che la politica costruita in questo modo è più performante e imita meglio l'agente DRL abbassando gli errori di previsione e aumentando la ricompensa.Mostriamo anche che le politiche MoET sono suscettibili di verifica usando teoremi automatici off-the-shelf come Z3.
Consideriamo il problema della minimizzazione non vincolata di una funzione obiettivo liscia in $\mathbb{R}^d$ in un ambiente in cui sono possibili solo valutazioni di funzioni. In particolare, proponiamo, SMTP, una versione momentum del metodo stocastico a tre punti (STP) Bergou et al. (2019).Mostriamo nuovi risultati di complessità per funzioni non convesse, convesse e fortemente convesse.Testiamo il nostro metodo su una collezione di compiti di apprendimento a controllo continuo su diversi MuJoCo Todorov et al. (2012) con difficoltà variabili e confrontiamo con STP, altri algoritmi di ottimizzazione senza derivati allo stato dell'arte e contro i metodi di gradiente della politica.SMTP supera significativamente STP e tutti gli altri metodi che abbiamo considerato nei nostri esperimenti numerici.Il nostro secondo contributo è SMTP con campionamento di importanza che chiamiamo SMTP_IS.Forniamo l'analisi di convergenza di questo metodo per obiettivi non convessi, convessi e fortemente convessi.
Usare le etichette di classe per rappresentare la somiglianza di classe è un approccio tipico all'addestramento di sistemi di hashing profondi per il recupero; i campioni della stessa classe o di classi diverse prendono valori di somiglianza binari 1 o 0. Questa somiglianza non modella la ricca conoscenza completa delle relazioni semantiche che possono essere presenti tra i punti dati. Combiniamo questo tipo di distanza semantica in una funzione di perdita per promuovere distanze simili tra le incorporazioni delle reti neurali profonde; introduciamo anche un termine di perdita empirico di divergenza di Kullback-Leibler per promuovere la binarizzazione e l'uniformità delle incorporazioni. Testiamo il metodo SHREWD risultante e dimostriamo miglioramenti nei punteggi di recupero gerarchico usando codici hash compatti e binari invece di quelli con valore reale, e dimostriamo che in un ambiente hashing debolmente supervisionato siamo in grado di imparare in modo competitivo senza fare esplicitamente affidamento sulle etichette di classe, ma invece sulle somiglianze tra le etichette.
In questo articolo, proponiamo un nuovo approccio per migliorare una data mappatura di superficie attraverso un raffinamento locale. L'approccio riceve una mappatura stabilita tra due superfici e segue quattro fasi:(i) ispezione della mappatura e creazione di uno sparseset di punti di riferimento nelle regioni che non corrispondono;(ii) segmentazione con un processo di crescita regionale a bassa distorsione basato sull'appiattimento delle parti segmentate;(iii) ottimizzazione della deformazione delle parti segmentate per allineare i punti di riferimento nel dominio di parametrizzazione planare; e(iv) aggregazione delle mappature dai segmenti per aggiornare la mappatura della superficie. Inoltre, proponiamo un nuovo metodo per deformare le mesh al fine di soddisfare i vincoli (nel nostro caso, l'allineamento dei punti di riferimento della fase (iii)), regolando in modo incrementale i pesi cotangenti per i vincoli e applicando la deformazione in modo da garantire che la mesh deformata sia priva di facce capovolte e abbia una bassa distorsione conforme. Il nostro nuovo approccio di deformazione, Iterative Least Squares Conformal Mapping (ILSCM), supera gli altri metodi di deformazione a bassa distorsione. L'approccio è generale, e lo abbiamo testato migliorando le mappature di diversi metodi di surfacemapping esistenti.
In questo lavoro, introduciamo un punto di vista teorico dell'informazione sul comportamento dei processi di ottimizzazione delle reti profonde e sulle loro capacità di generalizzazione, studiando il piano dell'informazione, il piano dell'informazione reciproca tra la variabile d'ingresso e l'etichetta desiderata, per ogni strato nascosto; in particolare, mostriamo che l'addestramento della rete è caratterizzato da un rapido aumento dell'informazione reciproca (MI) tra gli strati e l'etichetta target, seguito da una lunga diminuzione della MI tra gli strati e la variabile d'ingresso. Inoltre, dimostriamo esplicitamente che le due fondamentali quantità teoriche dell'informazione corrispondono all'errore di generalizzazione della rete, come risultato dell'introduzione di un nuovo limite di generalizzazione che è esponenziale nella compressione della rappresentazione. L'analisi si concentra sui tipici modelli di problemi su larga scala. A questo scopo, introduciamo un nuovo limite analitico sull'informazione reciproca tra strati consecutivi nella rete.
Proponiamo e studiamo un metodo per l'apprendimento di rappresentazioni interpretabili per il compito di regressione.Le caratteristiche sono rappresentate come reti di alberi di espressione multi-tipo composti da funzioni di attivazione comuni nelle reti neurali oltre ad altre funzioni elementari.Le caratteristiche differenziabili sono addestrate tramite discesa del gradiente, e la performance delle caratteristiche in un modello lineare è usata per pesare il tasso di cambiamento tra i sottocomponenti di ogni rappresentazione.Il processo di ricerca mantiene un archivio di rappresentazioni con trade-off di precisione-complessità per assistere nella generalizzazione e nell'interpretazione. Confrontiamo diversi approcci di ottimizzazione stocastica all'interno di questa struttura e mettiamo a confronto queste varianti su 100 problemi di regressione open-source con approcci di apprendimento automatico all'avanguardia. La nostra scoperta principale è che questo approccio produce i punteggi medi più alti tra i problemi, producendo rappresentazioni che sono ordini di grandezza più piccole del metodo successivo più performante (gradient boosting), riportando anche un risultato negativo in cui i tentativi di ottimizzare direttamente il disentanglement della rappresentazione portano a caratteristiche più altamente correlate.
La maggior parte dei sistemi distribuiti di apprendimento automatico (ML) memorizzano una copia dei parametri del modello localmente su ogni macchina per minimizzare la comunicazione di rete. In pratica, al fine di ridurre il tempo di attesa di sincronizzazione, queste copie del modello non sono necessariamente aggiornate in lock-step, e possono diventare stantie.Nonostante molti sviluppi in ML su larga scala, l'effetto della staleness sull'efficienza di apprendimento non è conclusivo, soprattutto perché è difficile controllare o monitorare la staleness in ambienti distribuiti complessi. I nostri esperimenti rivelano la ricca diversità degli effetti della staleness sulla convergenza degli algoritmi di ML e offrono spunti su rapporti apparentemente contraddittori nella letteratura. I risultati empirici ispirano anche una nuova analisi della convergenza di SGD nell'ottimizzazione non convessa sotto staleness, eguagliando il tasso di convergenza più noto di O(1/\sqrt{T}).
L'identificazione del sistema è il processo di costruzione di un modello matematico di un sistema sconosciuto a partire dalle misurazioni dei suoi ingressi e uscite; è un passo fondamentale per il controllo basato sul modello, la progettazione di stimatori e la previsione dell'uscita. Questo lavoro presenta un algoritmo per l'identificazione non lineare del sistema offline da osservazioni parziali, cioè situazioni in cui lo stato completo del sistema non è direttamente osservabile. Testiamo il nostro algoritmo su un sistema simulato di attrattori di Lorenz accoppiati, mostrando la capacità del nostro algoritmo di identificare sistemi ad alta densità che si dimostrano intrattabili per gli approcci basati sulle particelle. Usiamo anche SISL per identificare la dinamica di un elicottero acrobatico, aumentando lo stato con stati fluidi non osservati, impariamo un modello che prevede l'accelerazione dell'elicottero meglio degli approcci più avanzati.
Vari schemi di compressione del gradiente sono stati proposti per mitigare il costo di comunicazione nell'addestramento distribuito di modelli di apprendimento automatico su larga scala.I metodi basati sul segno, come signSGD (Bernstein et al, In questo articolo, eseguiamo un'analisi generale dei metodi basati sul segno per l'ottimizzazione non convessa. La nostra analisi è costruita su limiti intuitivi delle probabilità di successo e non si basa su particolari distribuzioni di rumore né sulla limitatezza della varianza dei gradienti stocastici. Estendendo la teoria all'impostazione distribuita all'interno di un quadro di server di parametri, assicuriamo una riduzione della varianza esponenzialmente veloce rispetto al numero di nodi, mantenendo la compressione a 1 bit in entrambe le direzioni e utilizzando piccole dimensioni di mini-batch.
L'apprendimento off-policy, il compito di valutare e migliorare le politiche utilizzando i dati storici raccolti da una politica di registrazione, è importante perché la valutazione on-policy è solitamente costosa e ha impatti negativi.Una delle maggiori sfide dell'apprendimento off-policy è quella di derivare stimatori controfattuali che abbiano anche una bassa varianza e quindi un basso errore di generalizzazione. In questo lavoro, ispirato dai limiti di apprendimento per i problemi di campionamento di importanza, presentiamo un nuovo principio di apprendimento controfattuale per l'apprendimento off-policy con feedbacks.Il nostro metodo regolarizza l'errore di generalizzazione minimizzando la divergenza di distribuzione tra la politica di registrazione e la nuova politica, e rimuove la necessità di iterare attraverso tutti i campioni di formazione per calcolare la regolarizzazione della varianza del campione nel lavoro precedente.Con le politiche della rete neurale, i nostri algoritmi di formazione end-to-end usando la minimizzazione della divergenza variazionale hanno mostrato un miglioramento significativo rispetto agli algoritmi di base convenzionali ed è anche coerente con i nostri risultati teorici.
L'obiettivo, e il principale contributo di questo lavoro, è la combinazione di vincoli artigianali con reti neurali convoluzionali profonde, come un modo per sfruttare la loro notevole facilità di generare immagini naturali.La base matematica alla base del nostro metodo è la struttura di massimizzazione delle aspettative, dove i dati sono divisi in lotti e accoppiati a incognite "latenti" aggiuntive. Queste incognite sono coppie di elementi dello spazio dell'incognita originale (ma ora accoppiati a uno specifico lotto di dati) e input di rete. In questa impostazione, la rete neurale controlla la similarità tra questi parametri aggiuntivi, agendo come una variabile "centrale". Il problema risultante equivale a una stima di massima verosimiglianza dei parametri di rete quando il modello di dati aumentato è marginalizzato sulle variabili latenti.
Quando si traducono domande in linguaggio naturale in query SQL per rispondere alle domande di un database, i modelli di analisi semantica contemporanei lottano per generalizzare a schemi di database mai visti.  La sfida della generalizzazione sta nel (a) codificare le relazioni del database in un modo accessibile per il parser semantico, e (b) modellare l'allineamento tra le colonne del database e le loro menzioni in una data query.  Presentiamo una struttura unificata, basata sul meccanismo di auto-attenzione consapevole delle relazioni, per affrontare la codifica dello schema, il collegamento dello schema e la rappresentazione delle caratteristiche all'interno di un codificatore text-to-SQL. Sul difficile set di dati Spider, questa struttura aumenta l'accuratezza della corrispondenza esatta al 53,7%, rispetto al 47,4% del precedente modello allo stato dell'arte non implementato con embeddings BERT.
Come dimostra la nostra esperienza, gli esseri umani possono imparare e impiegare una miriade di competenze diverse per affrontare le situazioni che incontrano quotidianamente; le reti neurali, al contrario, hanno una capacità di memoria fissa che impedisce loro di imparare più di qualche serie di competenze prima di iniziare a dimenticarle. Per testare questo sistema, introduciamo un compito di apprendimento continuo basato sulla modellazione linguistica in cui il modello è esposto a più lingue e domini in sequenza, senza fornire alcun segnale esplicito sul tipo di input che sta attualmente affrontando. Il sistema proposto mostra una migliore capacità di adattamento in quanto può recuperare più velocemente rispetto alle linee di base comparabili dopo un cambiamento nella lingua di input o nel dominio.
Proponiamo di affrontare un problema di regressione di serie temporali calcolando l'evoluzione temporale di una funzione di densità di probabilità per fornire una previsione probabilistica.Un modello basato su reti neurali ricorrenti (RNN) è impiegato per imparare un operatore non lineare per l'evoluzione temporale di una funzione di densità di probabilità.Usiamo uno strato softmax per una discretizzazione numerica di una funzione di densità di probabilità liscia, che trasforma un problema di approssimazione di funzione in un compito di classificazione. Vengono introdotte strategie di regolarizzazione esplicite e implicite per imporre una condizione di morbidezza sulla distribuzione di probabilità stimata.Viene presentata una procedura Monte Carlo per calcolare l'evoluzione temporale della distribuzione per una previsione a più fasi.La valutazione dell'algoritmo proposto su tre set di dati sintetici e due reali mostra un vantaggio rispetto alle linee base confrontate.
Nei sistemi cognitivi, il ruolo di una memoria di lavoro è cruciale per il ragionamento visivo e il processo decisionale. Sono stati fatti enormi progressi nella comprensione dei meccanismi della memoria di lavoro umana/animale, così come nella formulazione di diverse strutture di reti neurali artificiali.  Nel caso degli esseri umani, la memoria di lavoro visiva (VWM) è un compito standard in cui ai soggetti viene presentata una sequenza di immagini, ognuna delle quali deve essere identificata per sapere se è stata già vista o meno. Il nostro lavoro è uno studio di modi multipli per imparare un modello di memoria di lavoro utilizzando reti neurali ricorrenti che imparano a ricordare le immagini di input attraverso i tempi. Addestriamo queste reti neurali a risolvere il compito di memoria di lavoro allenandole con una sequenza di immagini in impostazioni di apprendimento supervisionato e di rinforzo. L'impostazione supervisionata utilizza sequenze di immagini con le loro etichette corrispondenti.L'impostazione di apprendimento di rinforzo è ispirata alla visione popolare nelle neuroscienze che la memoria di lavoro nella corteccia prefrontale è modulata da un meccanismo dopaminergico.Consideriamo il compito VWM come un ambiente che premia l'agente quando ricorda le informazioni passate e lo penalizza se dimentica.  Stimiamo quantitativamente le prestazioni di questi modelli su sequenze di immagini da un set di dati di immagini standard (CIFAR-100); inoltre, valutiamo la loro capacità di ricordare e richiamare quando sono sempre più addestrati nel corso degli episodi. Sulla base della nostra analisi, stabiliamo che un modello di rete neurale ricorrente gated con unità di memoria a breve termine addestrate utilizzando l'apprendimento di rinforzo è potente e più efficiente nel consolidare temporalmente le informazioni spaziali in ingresso. Questo lavoro è un'analisi iniziale come parte del nostro obiettivo finale di utilizzare reti neurali artificiali per modellare il comportamento e l'elaborazione delle informazioni della memoria di lavoro del cervello e di utilizzare i dati di imaging cerebrale catturati da soggetti umani durante il compito cognitivo VWM per comprendere i vari meccanismi di memoria del cervello. 
La non linearità è cruciale per le prestazioni di una rete (neurale) profonda (DN).Fino ad oggi ci sono stati pochi progressi nella comprensione del serraglio di nonlinearità disponibili, ma recentemente sono stati fatti progressi nella comprensione del ruolo giocato dalle nonlinearità affini e convesse piecewise come le funzioni di attivazione ReLU e valore assoluto e il max-pooling.In particolare, gli strati DN costruiti da queste operazioni possono essere interpretati come operatori spline max-affine (Mentre questo è un buon progresso teorico, l'intero approccio MASO è basato sul requisito che le non linearità siano affini e convesse, il che preclude importanti funzioni di attivazione come la sigmoide, la tangente iperbolica e la softmax. {Questo articolo estende la struttura MASO a queste e ad una classe infinitamente grande di nuove nonlinearità, collegando i MASO deterministici con i modelli probabilistici Gaussian Mixture (GMM). Mostriamo che, sotto un GMM, le nonlinearità convesse e affini come ReLU, valore assoluto e max-pooling possono essere interpretate come soluzioni a certi problemi naturali di inferenza VQ ``hard'', mentre sigmoide, tangente iperbolica e softmax possono essere interpretate come soluzioni ai corrispondenti problemi di inferenza VQ ``soft''. Estendiamo ulteriormente il quadro ibridando le ottimizzazioni VQ hard e soft per creare un'inferenza $\beta$-VQ che interpola tra l'inferenza VQ hard, soft e lineare.Un primo esempio di una nonlinearità $\beta$-VQ DN è la nonlinearità {\em swish}, che offre prestazioni allo stato dell'arte in una serie di compiti di computer vision ma è stata sviluppata ad hoc tramite sperimentazione.Infine, validiamo con esperimenti un'importante affermazione della nostra teoria, cioè che le prestazioni DN possono essere significativamente migliorate imponendo l'ortogonalità nei suoi filtri lineari.
Le proteine ingegnerizzate offrono il potenziale per risolvere molti problemi in biomedicina, energia e scienza dei materiali, ma la creazione di progetti che hanno successo è difficile in pratica. Un aspetto significativo di questa sfida è il complesso accoppiamento tra la sequenza della proteina e la struttura 3D, e il compito di trovare un progetto fattibile è spesso indicato come il problema inverso di piegatura della proteina. Il nostro approccio cattura in modo efficiente le complesse dipendenze nelle proteine, concentrandosi su quelle che sono a lungo raggio nella sequenza ma locali nello spazio 3D. La nostra struttura migliora significativamente i modelli parametrici precedenti delle sequenze proteiche data la struttura, e fa un passo avanti verso una progettazione biomolecolare rapida e mirata con l'aiuto di modelli generativi profondi.
In particolare, mostriamo che un passaggio in avanti attraverso uno strato di dropout standard seguito da uno strato lineare e da un'attivazione non lineare è equivalente all'ottimizzazione di un obiettivo convesso con una singola iterazione di un metodo di gradiente stocastico prossimale di $\tau$-nice.mostriamo inoltre che la sostituzione del dropout standard di Bernoulli con un dropout additivo è equivalente all'ottimizzazione dello stesso obiettivo convesso con un metodo prossimale ridotto alla varianza. Esprimendo entrambi gli strati completamente connessi e convoluzionali come casi speciali di un prodotto tensore di alto ordine, unifichiamo il problema di ottimizzazione convessa sottostante nell'impostazione tensoriale e deriviamo una formula per la costante Lipschitz $L$ usata per determinare la dimensione ottimale del passo dei metodi prossimali di cui sopra. conduciamo esperimenti con reti convoluzionali standard applicate ai set di dati CIFAR-10 e CIFAR-100 e dimostriamo che la sostituzione di un blocco di strati con iterazioni multiple del solutore corrispondente, con dimensione del passo impostata tramite $L$, migliora costantemente la precisione di classificazione.
Le reti profonde eseguite con operazioni a bassa precisione al momento dell'inferenza offrono vantaggi in termini di potenza e spazio rispetto alle alternative ad alta precisione, ma devono superare la sfida di mantenere un'elevata accuratezza quando la precisione diminuisce. Qui presentiamo un metodo per l'addestramento di tali reti, Learned Step Size Quantization, che raggiunge la massima accuratezza fino ad oggi sul set di dati ImageNet quando si utilizzano modelli, da una varietà di architetture, con pesi e attivazioni quantizzate a 2, 3 o 4 bit di precisione, e che può formare modelli a 3 bit che raggiungono l'accuratezza di base a precisione completa. Il nostro approccio si basa sui metodi esistenti per l'apprendimento dei pesi nelle reti quantizzate migliorando il modo in cui il quantizzatore stesso è configurato. In particolare, introduciamo un nuovo mezzo per stimare e scalare il gradiente di perdita del compito ad ogni passo di quantizzazione del peso e dello strato di attivazione, in modo che possa essere appreso insieme ad altri parametri di rete.
Ci sono stati diversi studi di recente che dimostrano che i modelli di comprensione del linguaggio naturale (NLU) sono inclini a fare affidamento su pregiudizi indesiderati del set di dati senza imparare il compito sottostante, con il risultato di modelli che non riescono a generalizzare a set di dati fuori dal dominio, e sono suscettibili di eseguire male in scenari del mondo reale. Introduciamo un ulteriore modello leggero di solo bias che impara i bias del dataset e usa la sua previsione per aggiustare la perdita del modello di base per ridurre i bias. In altre parole, i nostri metodi riducono l'importanza degli esempi bias, e concentrano l'addestramento sugli esempi difficili, cioè gli esempi che non possono essere classificati correttamente facendo affidamento solo sui bias.  Sperimentiamo su larga scala l'inferenza del linguaggio naturale e i dataset di verifica dei fatti e i loro dataset fuori dal dominio e dimostriamo che i nostri modelli debiased migliorano significativamente la robustezza in tutte le impostazioni, compreso il guadagno di 9,76 punti sul dataset di valutazione simmetrico FEVER, 5,45 sul dataset HANS e 4,78 punti sul set duro SNLI.  Questi set di dati sono specificamente progettati per valutare la robustezza dei modelli nell'impostazione fuori dal dominio, dove le distorsioni tipiche dei dati di formazione non esistono nel set di valutazione.
La ricostruzione dei dati della tomografia computerizzata (CT) con poche viste è un problema altamente mal posto, spesso utilizzato in applicazioni che richiedono una bassa dose di radiazioni nella CT clinica, nella scansione industriale rapida o nella CT a portale fisso. Gli algoritmi analitici o iterativi esistenti producono generalmente immagini mal ricostruite, gravemente deteriorate da artefatti e rumore, soprattutto quando il numero di proiezioni dei raggi X è notevolmente basso. Il metodo proposto interpreta i dati del sinogramma di poche viste utilizzando reti profonde basate sull'attenzione per dedurre l'immagine ricostruita, che viene poi utilizzata come conoscenza preliminare nell'algoritmo iterativo per la ricostruzione finale.
Nel dialogo a dominio aperto gli agenti intelligenti dovrebbero esibire l'uso della conoscenza, tuttavia ci sono poche dimostrazioni convincenti di questo fino ad oggi.I modelli sequenza per sequenza più popolari tipicamente "generano e sperano" enunciati generici che possono essere memorizzati nei pesi del modello durante la mappatura dall'enunciato(i) di input all'output, piuttosto che impiegare la conoscenza richiamata come contesto. L'uso della conoscenza si è finora dimostrato difficile, in parte a causa della mancanza di un compito di apprendimento supervisionato che esibisce un dialogo aperto e consapevole con una chiara base di partenza. A tal fine raccogliamo e rilasciamo un grande set di dati con conversazioni direttamente basate sulla conoscenza recuperata da Wikipedia.  I nostri modelli di dialogo più performanti sono in grado di condurre discussioni consapevoli su argomenti di dominio aperto, come valutato da metriche automatiche e valutazioni umane, mentre il nostro nuovo benchmark permette di misurare ulteriori miglioramenti in questa importante direzione di ricerca.
Formuliamo un nuovo problema all'intersezione dell'apprendimento semi-supervisionato e dei banditi contestuali, motivato da diverse applicazioni tra cui prove cliniche e sistemi di dialogo.Dimostriamo come il bandito contestuale e le reti convoluzionali a grafo possono essere adattati alla nuova formulazione del problema.Prendiamo quindi il meglio di entrambi gli approcci per sviluppare banditi contestuali incorporati multi-GCN.I nostri algoritmi sono verificati su diversi set di dati del mondo reale.
 Una collezione di articoli scientifici è spesso accompagnata da tag: parole chiave, argomenti, concetti ecc.  A volte questi tag sono generati dall'uomo, a volte dalla macchina.  Proponiamo una semplice misura della consistenza del tagging degli articoli scientifici: se questi tag sono predittivi per i collegamenti del grafico delle citazioni.  Poiché gli autori tendono a citare articoli su argomenti vicini a quelli delle loro pubblicazioni, un sistema di tag coerente potrebbe predire le citazioni.  Presentiamo un algoritmo per calcolare la coerenza, ed esperimenti con tag generati dall'uomo e dalla macchina.  Mostriamo che l'aumento, cioè la combinazione dei tag manuali con quelli generati dalla macchina, può migliorare la coerenza dei tag.  Introduciamo inoltre la cross-consistenza, la capacità di prevedere i collegamenti citazionali tra articoli etichettati da tagger diversi, ad esempio manualmente e da una macchina.  La cross-consistenza può essere usata per valutare la qualità del tagging quando la quantità di dati etichettati è limitata.
Una recente ricerca ha rivelato intensamente la vulnerabilità delle reti neurali profonde, specialmente per le reti neurali convoluzionali (CNN) nel compito di riconoscimento delle immagini, attraverso la creazione di campioni avversari che `"differiscono leggermente" dai campioni legittimi. Questa vulnerabilità indica che questi potenti modelli sono sensibili a perturbazioni specifiche e non possono filtrare queste perturbazioni avversarie. In questo lavoro, proponiamo un metodo basato sulla quantizzazione che permette a una CNN di filtrare le perturbazioni avversarie in modo efficace.In particolare, diversamente dai lavori precedenti sulla quantizzazione dell'input, applichiamo la quantizzazione negli strati intermedi di una CNN. Il nostro approccio è naturalmente allineato con il clustering delle informazioni semantiche a grana grossa apprese da una CNN.Inoltre, per compensare la perdita di informazioni che è inevitabilmente causata dalla quantizzazione, proponiamo la quantizzazione multitesta, dove proiettiamo i punti di dati in diversi sottospazi ed eseguiamo la quantizzazione all'interno di ogni sottospazio. I risultati ottenuti sui dataset MNIST e Fashion-MNSIT dimostrano che l'aggiunta di un solo Q-Layer in una CNN può migliorare significativamente la sua robustezza contro gli attacchi white-box e black-box.
Le reti invarianti ed equivarianti sono state utilizzate con successo per l'apprendimento di immagini, insiemi, nuvole di punti e grafi. Una sfida fondamentale nello sviluppo di tali reti è trovare la massima collezione di strati lineari invarianti ed equivarianti. Anche se questa domanda ha trovato risposta per i primi tre esempi (almeno per le trasformazioni popolari), non è nota una caratterizzazione completa degli strati lineari invarianti ed equivarianti per i grafi. In questo articolo forniamo una caratterizzazione di tutti gli strati lineari invarianti ed equivarianti di permutazione per i dati di (iper-)grafi, e mostriamo che la loro dimensione, nel caso di dati di grafi con valore di bordo, è rispettivamente $2$ e $15$.Più in generale, per dati di grafi definiti su $k$-tuple di nodi, la dimensione è il $k$-esimo e $2k$-esimo numero di Bell.vengono calcolate basi ortogonali per gli strati, inclusa la generalizzazione a dati multi-grafo. Dal punto di vista teorico, i nostri risultati generalizzano e unificano i recenti progressi nel deep learning equivariante; in particolare, dimostriamo che il nostro modello è in grado di approssimare qualsiasi rete neurale message passing; l'applicazione di questi nuovi strati lineari in una semplice rete neurale profonda dimostra di ottenere risultati comparabili allo stato dell'arte e di avere una migliore espressività rispetto alle precedenti basi invarianti ed equivarianti.
Nell'apprendimento di rinforzo, possiamo imparare un modello di osservazioni e ricompense future, e usarlo per pianificare le prossime azioni dell'agente.Tuttavia, modellare congiuntamente le osservazioni future può essere computazionalmente costoso o addirittura intrattabile se le osservazioni sono altamente dimensionali (ad esempio immagini).Per questo motivo, i lavori precedenti hanno considerato modelli parziali, che modellano solo una parte dell'osservazione. In questo articolo, mostriamo che i modelli parziali possono essere causalmente errati: sono confusi dalle osservazioni che non modellano, e possono quindi portare a una pianificazione errata. Per affrontare questo problema, introduciamo una famiglia generale di modelli parziali che sono causalmente corretti, ma evitano la necessità di modellare completamente le osservazioni future.
Nel lifelong learning, l'allievo viene presentato con una sequenza di compiti, costruendo incrementalmente un priore guidato dai dati che può essere sfruttato per accelerare l'apprendimento di un nuovo compito.In questo lavoro, indaghiamo l'efficienza degli attuali approcci lifelong, in termini di complessità del campione, costo computazionale e di memoria.A tal fine, introduciamo prima un nuovo e più realistico protocollo di valutazione, per cui gli allievi osservano ogni esempio solo una volta e la selezione degli iper-parametri viene fatta su un piccolo e disgiunto insieme di compiti, che non viene utilizzato per l'effettiva esperienza di apprendimento e valutazione. In secondo luogo, introduciamo una nuova metrica che misura quanto velocemente un discente acquisisce una nuova abilità.In terzo luogo, proponiamo una versione migliorata di GEM (Lopez-Paz & Ranzato, 2017), soprannominata Averaged GEM (A-GEM), che gode delle stesse prestazioni o addirittura migliori di GEM, pur essendo computazionalmente e di memoria efficiente quasi quanto EWC (Kirkpatrick et al, I nostri esperimenti su diversi benchmark standard di lifelong learning dimostrano che A-GEM ha il miglior compromesso tra precisione ed efficienza
Il calcolo a precisione ridotta è una delle aree chiave per affrontare il crescente 'compute gap', guidato da una crescita esponenziale nelle applicazioni di deep learning.Negli ultimi anni, l'addestramento delle reti neurali profonde è in gran parte migrato verso la precisione a 16 bit, con significativi guadagni in termini di prestazioni ed efficienza energetica.Tuttavia, i tentativi di addestrare le DNN a 8 bit di precisione hanno incontrato sfide significative, a causa della maggiore precisione e dei requisiti di gamma dinamica della back-propagation.   In questo articolo, proponiamo un metodo per addestrare le reti neurali profonde usando una rappresentazione in virgola mobile a 8 bit per pesi, attivazioni, errori e gradienti.  Dimostriamo un'accuratezza allo stato dell'arte su più set di dati (imagenet-1K, WMT16) e un insieme più ampio di carichi di lavoro (Resnet-18/34/50, GNMT e Transformer) rispetto a quanto riportato in precedenza.   Proponiamo un metodo di scalatura delle perdite migliorato per aumentare la gamma subnormale ridotta della virgola mobile a 8 bit, per migliorare la propagazione dell'errore; esaminiamo anche l'impatto del rumore di quantizzazione sulla generalizzazione e proponiamo una tecnica di arrotondamento stocastico per affrontare il rumore del gradiente; come risultato dell'applicazione di tutte queste tecniche, riportiamo una precisione di convalida leggermente superiore rispetto alla linea di base di precisione completa.
Le funzioni di perdita giocano un ruolo cruciale nell'apprendimento metrico profondo, per cui ne sono state proposte diverse. Alcune supervisionano il processo di apprendimento tramite vincoli di somiglianza a coppie o tripli, mentre altre sfruttano le informazioni di somiglianza strutturate tra più punti dati. In questo lavoro, ci avviciniamo all'apprendimento metrico profondo da una nuova prospettiva. In primo luogo, simile all'entropia incrociata categoriale (CCE), ICE ha una chiara interpretazione probabilistica e sfrutta le informazioni di somiglianza semantica strutturata per la supervisione dell'apprendimento.In secondo luogo, ICE è scalabile a dati di formazione infiniti, poiché impara su mini-batch iterativamente ed è indipendente dalle dimensioni del set di formazione. In terzo luogo, motivato dalla nostra analisi dei pesi relativi, è incorporata la riponderazione dei campioni senza soluzione di continuità, che ridimensiona i gradienti dei campioni per controllare il grado di differenziazione sugli esempi di addestramento invece di troncarli con l'estrazione dei campioni.Oltre alla sua semplicità e intuitività, ampi esperimenti su tre benchmark del mondo reale dimostrano la superiorità di ICE.
Nell'apprendimento di rinforzo basato sul modello, l'agente si intreccia tra l'apprendimento del modello e la pianificazione.  Questi due componenti sono inestricabilmente intrecciati.Se il modello non è in grado di fornire una previsione sensata a lungo termine, il pianificatore eseguito sfrutterebbe i difetti del modello, il che può produrre fallimenti catastrofici.Questo articolo si concentra sulla costruzione di un modello che ragiona sul futuro a lungo termine e dimostra come usarlo per una pianificazione ed esplorazione efficiente.A tal fine, costruiamo un modello autoregressivo a variabili latenti sfruttando idee recenti nell'inferenza variazionale. Inoltre, pianificando nello spazio latente, la soluzione del pianificatore è assicurata per essere all'interno delle regioni in cui il modello è valido. una strategia di esplorazione può essere concepita cercando traiettorie improbabili sotto il modello. i nostri metodi raggiungono una ricompensa più alta più velocemente rispetto alle linee di base su una varietà di compiti e ambienti sia nell'apprendimento per imitazione che nelle impostazioni di apprendimento per rinforzo basate sul modello.
Rilevare le anomalie è di crescente importanza per varie applicazioni industriali e infrastrutture mission-critical, compresi i sistemi satellitari.Sebbene ci siano stati diversi studi nel rilevamento delle anomalie basati su approcci basati su regole o sull'apprendimento automatico per i sistemi satellitari, un metodo di decomposizione basato su tensori non è stato ampiamente esplorato per il rilevamento delle anomalie.In questo lavoro, introduciamo un framework Integrative Tensor-based Anomaly Detection (ITAD) per rilevare le anomalie in un sistema satellitare. Costruiamo tensori di terzo ordine con i dati telemetrici raccolti dal Korea Multi-Purpose Satellite-2 (KOMPSAT-2) e calcoliamo il punteggio di anomalia utilizzando una delle matrici componenti ottenute applicando la decomposizione CANDECOMP/PARAFAC per rilevare le anomalie. Il nostro risultato mostra che il nostro approccio basato su tensori può essere efficace nel raggiungere una maggiore precisione e ridurre i falsi positivi nel rilevamento delle anomalie rispetto ad altri approcci esistenti.
In questo lavoro studiamo la possibilità di sfruttare tale struttura ripetuta per accelerare e regolarizzare l'apprendimento, partendo dall'obiettivo di ricompensa attesa regolarizzata KL che introduce una componente aggiuntiva, una politica predefinita, invece di fare affidamento su una politica predefinita fissa, la impariamo dai dati. Formalizziamo questa strategia e discutiamo le connessioni con gli approcci al collo di bottiglia dell'informazione e all'algoritmo variazionale EM. Presentiamo risultati empirici in entrambi i domini di azioni discrete e continue e dimostriamo che, per alcuni compiti, l'apprendimento di una politica predefinita insieme alla politica può accelerare e migliorare significativamente l'apprendimento.Guarda il video che dimostra gli esperti appresi e le politiche predefinite su diversi compiti di controllo continuo ( https://youtu.be/U2qA3llzus8 ).
Quando un classificatore di immagini fa una previsione, quali parti dell'immagine sono rilevanti e perché? Possiamo riformulare questa domanda per chiedere: quali parti dell'immagine, se non fossero viste dal classificatore, cambierebbero maggiormente la sua decisione? Il nostro approccio contrasta con approcci di riempimento ad hoc, come la sfocatura o l'iniezione di rumore, che generano input lontani dalla distribuzione dei dati e ignorano le relazioni informative tra diverse parti dell'immagine. Il nostro metodo produce mappe di salienza più compatte e rilevanti, con meno artefatti rispetto ai metodi precedenti.
Questo articolo presenta il Variation Network (VarNet), un modello generativo che fornisce i mezzi per manipolare gli attributi di alto livello di un dato input.L'originalità del nostro approccio è che VarNet non è solo in grado di gestire gli attributi predefiniti, ma può anche imparare da solo gli attributi rilevanti del dataset.  Queste due impostazioni possono essere facilmente combinate, il che rende VarNet applicabile per un'ampia varietà di compiti. Inoltre, VarNet ha una solida interpretazione probabilistica che ci garantisce un nuovo modo di navigare negli spazi latenti, nonché mezzi per controllare come gli attributi vengono appresi.
Nonostante l'allarme per la dipendenza dei sistemi di apprendimento automatico dai cosiddetti pattern spuri nei dati di training, il termine manca di un significato coerente nei quadri statistici standard, ma il linguaggio della causalità offre chiarezza: le associazioni spurie sono quelle dovute a una causa comune (confondimento) contro gli effetti diretti o indiretti.In questo articolo, ci concentriamo sull'NLP, introducendo metodi e risorse per la formazione di modelli insensibili ai pattern spuri. Dati i documenti e le loro etichette iniziali, ci incarichiamo di rivedere ogni documento per accordarsi con un'etichetta di destinazione controfattuale, chiedendo che i documenti rivisti siano internamente coerenti, evitando qualsiasi cambiamento gratuito.È interessante notare che nei compiti di sentiment analysis e inferenza del linguaggio naturale, i classificatori addestrati sui dati originali falliscono sui loro controfattuali rivisti e viceversa. I classificatori addestrati sui set di dati combinati si comportano notevolmente bene, appena al di sotto di quelli specializzati in uno dei due domini.Mentre i classificatori addestrati sui dati originali o manipolati da soli sono sensibili alle caratteristiche spurie (ad es, menzioni di genere), i modelli addestrati sui dati combinati sono insensibili a questo segnale.
Tra i molteplici modi di interpretare un modello di apprendimento automatico, misurare l'importanza di un insieme di caratteristiche legate a una previsione è probabilmente uno dei modi più intuitivi per spiegare un modello.In questo articolo, stabiliamo il legame tra un insieme di caratteristiche a una previsione con un nuovo criterio di valutazione, l'analisi di robustezza, che misura la tolleranza minima di perturbazione avversaria. Misurando il livello di tolleranza per un attacco avversario, possiamo estrarre un set di caratteristiche che fornisce il supporto più robusto per una previsione corrente, e anche estrarre un set di caratteristiche che contrasta la previsione corrente con una classe di destinazione impostando un attacco avversario mirato. Applicando questa metodologia a vari compiti di previsione in più domini, abbiamo osservato che le spiegazioni derivate stanno effettivamente catturando il set di caratteristiche significative qualitativamente e quantitativamente.
Le reti generative avversarie (GAN) hanno dimostrato di fornire un modo efficace per modellare distribuzioni complesse e hanno ottenuto risultati impressionanti su vari compiti impegnativi.Tuttavia, le GAN tipiche richiedono dati completamente osservati durante l'addestramento.In questa carta, presentiamo una struttura basata su GAN per l'apprendimento da dati incompleti complessi e altamente dimensionali. La struttura proposta impara un generatore di dati completi insieme a un generatore di maschere che modella la distribuzione dei dati mancanti.Dimostriamo inoltre come imputare i dati mancanti dotando la nostra struttura di un imputer addestrato avversariamente.Valutiamo la struttura proposta utilizzando una serie di esperimenti con diversi tipi di processi di dati mancanti sotto il presupposto mancante completamente a caso.
In questo articolo proponiamo un nuovo metodo di compressione, Inter-Layer Weight Prediction (ILWP) e un metodo di quantizzazione che quantizza i residui previsti tra i pesi in tutti gli strati di convoluzione basati su un metodo di predizione inter-frame in schemi di codifica video convenzionali.Inoltre, abbiamo trovato un fenomeno Smoothly Varying Weight Hypothesis (SVWH) che è che i pesi in strati di convoluzione adiacenti condividono una forte somiglianza di forme e valori, cioè, I pesi tendono a variare dolcemente con gli strati.Sulla base di SVWH, proponiamo un secondo ILWP e un metodo di quantizzazione che quantizza i residui previsti tra i pesi negli strati di convoluzione adiacenti.Poiché i residui di peso previsti tendono a seguire distribuzioni di Laplace con varianza molto bassa, la quantizzazione del peso può essere applicata più efficacemente, producendo così più pesi zero e migliorando il rapporto di compressione del peso. Inoltre, proponiamo una nuova perdita inter-strato per l'eliminazione dei bit non-texture, che ci ha permesso di memorizzare più efficacemente solo i bit texture.Cioè, la perdita proposta regolarizza i pesi in modo tale che i pesi collocati tra i due strati adiacenti abbiano gli stessi valori.Infine, proponiamo un ILWP con una perdita inter-strato e un metodo di quantizzazione.I nostri esperimenti completi mostrano che il metodo proposto raggiunge un tasso di compressione del peso molto più elevato allo stesso livello di precisione rispetto ai precedenti metodi di compressione basati sulla quantizzazione nelle reti neurali profonde.
Esiste una pletora di tecniche per indurre la sparsità strutturata nei modelli parametrici durante il processo di ottimizzazione, con l'obiettivo finale di un'inferenza efficiente dal punto di vista delle risorse. Tuttavia, per quanto ne sappiamo, nessuna ha come obiettivo un numero specifico di operazioni in virgola mobile (FLOP) come parte di un singolo obiettivo di ottimizzazione end-to-end, nonostante la segnalazione di FLOP come parte dei risultati. Inoltre, un approccio one-size-fits-all ignora i vincoli realistici del sistema, che differiscono significativamente tra, ad esempio, una GPU e un telefono cellulare - FLOPs sul primo incorrere meno latenza che sul secondo; quindi, Ã¨ importante per i professionisti da potere specificare un numero di obiettivo di FLOPs durante il modello compression.In questo lavoro, estendiamo una tecnica di stato-of-the-art per incorporare direttamente FLOPs come componente dell'obiettivo di ottimizzazione e mostrare che, dato un requisito desiderato di FLOPs, le reti neurali differenti possono essere addestrate con successo per classificazione di immagine.
Gli studi recenti si concentrano principalmente su due sfide: da un lato, tale traduzione è intrinsecamente multimodale a causa delle variazioni delle informazioni specifiche del dominio (ad es, Per un altro, gli approcci multimodali esistenti hanno limitazioni nella gestione di più di due domini, cioè devono costruire indipendentemente un modello per ogni coppia di domini.Per affrontare questi problemi, proponiamo il metodo Hierarchical Image-to-image Translation (HIT) che formula congiuntamente il problema multimodale e multidominio in una struttura gerarchica semantica, e può controllare ulteriormente l'incertezza del multimodale. In particolare, consideriamo le variazioni specifiche del dominio come il risultato della proprietà di multi-granularità dei domini, e si può controllare la granularità della traduzione multimodale dividendo un dominio con grandi variazioni in più sottodomini che catturano variazioni locali e a grana fine. Con l'assunzione di un priore gaussiano, le variazioni dei domini sono modellate in uno spazio comune in modo che le traduzioni possano essere effettuate tra più domini all'interno di un modello. Per imparare questo spazio complicato, proponiamo di sfruttare la relazione di inclusione tra i domini per vincolare le distribuzioni di genitori e figli da annidare.
Le reti neurali ricorrenti (RNN) hanno molto successo nella risoluzione di problemi impegnativi con dati sequenziali. Tuttavia, questa efficienza osservata non è ancora interamente spiegata dalla teoria. è noto che una certa classe di RNN moltiplicative gode della proprietà di efficienza della profondità --- una rete poco profonda di larghezza esponenzialmente grande è necessaria per realizzare la stessa funzione di punteggio calcolata da una RNN. In questo lavoro, cerchiamo di ridurre il divario tra la teoria e la pratica estendendo l'analisi teorica alle RNN che impiegano varie non linearità, come la Rectified Linear Unit (ReLU), e dimostriamo che anch'esse beneficiano delle proprietà di universalità ed efficienza della profondità.I nostri risultati teorici sono verificati da una serie di ampi esperimenti computazionali.
Mentre le reti neurali profonde hanno dimostrato di essere un potente strumento per molti compiti di riconoscimento e classificazione, le loro proprietà di stabilità non sono ancora ben comprese.In passato, i classificatori di immagini hanno dimostrato di essere vulnerabili ai cosiddetti attacchi avversari, che vengono creati perturbando additivamente l'immagine classificata correttamente. In questo articolo, proponiamo l'algoritmo ADef per costruire un diverso tipo di attacco avversario creato applicando iterativamente piccole deformazioni all'immagine, trovate attraverso un passo di discesa del gradiente.dimostriamo i nostri risultati su MNIST con reti neurali convoluzionali e su ImageNet con Inception-v3 e ResNet-101.
I metodi di apprendimento avversari sono stati proposti per una vasta gamma di applicazioni, ma l'addestramento dei modelli avversari può essere notoriamente instabile. Bilanciare efficacemente le prestazioni del generatore e del discriminatore è fondamentale, poiché un discriminatore che raggiunge una precisione molto alta produrrà gradienti relativamente poco informativi. In questo lavoro, proponiamo una tecnica semplice e generale per vincolare il flusso di informazioni nel discriminatore per mezzo di un collo di bottiglia di informazioni; imponendo un vincolo sull'informazione reciproca tra le osservazioni e la rappresentazione interna del discriminatore, possiamo modulare efficacemente la precisione del discriminatore e mantenere gradienti utili e informativi. Dimostriamo che il nostro collo di bottiglia proposto del discriminatore variazionale (VDB) conduce ai miglioramenti significativi attraverso tre aree di applicazione distinte per gli algoritmi di apprendimento avversari.La nostra valutazione primaria studia l'applicabilità del VDB all'apprendimento di imitazione delle abilità dinamiche di controllo continuo, come la corsa. Dimostriamo che il nostro metodo può imparare tali abilità direttamente dalle dimostrazioni video grezze, superando sostanzialmente i precedenti metodi di apprendimento dell'imitazione avversaria.Il VDB può anche essere combinato con l'apprendimento di rinforzo inverso avversario per imparare funzioni di ricompensa parsimoniose che possono essere trasferite e ri-ottimizzate in nuove impostazioni.Infine, dimostriamo che il VDB può addestrare GANs più efficacemente per la generazione di immagini, migliorando su una serie di metodi di stabilizzazione precedenti.
L'implementazione di sistemi di apprendimento automatico nel mondo reale richiede sia un'alta accuratezza su dati puliti che la robustezza alle corruzioni che si verificano naturalmente. Mentre i progressi architettonici hanno portato a una migliore accuratezza, la costruzione di modelli robusti rimane impegnativa, coinvolgendo importanti cambiamenti nella procedura di formazione e nei set di dati.  Il lavoro precedente ha sostenuto che c'Ã¨ un compromesso inerente fra la robustezza e l'esattezza, come esemplificato dalle tecniche standard di aumento dei dati quale il ritaglio, che migliora l'esattezza pulita ma non la robustezza ed il rumore gaussiano additivo, che migliora la robustezza ma danneggia l'esattezza.  I modelli addestrati con Patch Gaussian raggiungono lo stato dell'arte sui benchmark CIFAR-10 e ImageNet Common Corruptions, mantenendo anche l'accuratezza sui dati puliti.Troviamo che questo incremento porta a una ridotta sensibilità al rumore ad alta frequenza (simile a Gaussian), pur mantenendo la capacità di sfruttare le informazioni rilevanti ad alta frequenza nell'immagine (simile a Cutout).Mostriamo che può essere utilizzato in combinazione con altri metodi di regolarizzazione e politiche di incremento dei dati come AutoAugment.  Infine, troviamo che l'idea di limitare le perturbazioni alle patch può essere utile anche nel contesto dell'apprendimento avversario, producendo modelli senza la perdita di precisione che si trova con l'addestramento avversario non vincolato.
La regressione offset è un metodo standard per la localizzazione spaziale in molti compiti di visione, compresa la stima della posa umana, il rilevamento degli oggetti e la segmentazione delle istanze.  Questo può essere attribuito alla localizzazione dell'operazione di convoluzione, esacerbata dalla varianza di scala, dal disordine e dal punto di vista.Un problema ancora più fondamentale è la multi-modalità delle immagini del mondo reale.Di conseguenza, non possono essere approssimate adeguatamente usando un modello a modalità singola.  Invece, proponiamo di usare le reti di densità di miscela (MDN) per la regressione di offset, permettendo al modello di gestire le varie modalità in modo efficiente e imparando a prevedere la densità condizionale completa delle uscite dato l'input.Sulla stima della posa umana 2D in natura, che richiede la localizzazione accurata dei punti chiave del corpo, mostriamo che questo produce un miglioramento significativo nella precisione di localizzazione.In particolare, i nostri esperimenti rivelano la variazione del punto di vista come il fattore multimodale dominante. Inoltre, inizializzando attentamente i parametri della MDN, non affrontiamo alcuna instabilità nell'addestramento, che è noto essere un grande ostacolo per la diffusione della MDN. Il metodo può essere facilmente applicato a qualsiasi compito con una componente di regressione spaziale. I nostri risultati evidenziano la natura multimodale della visione del mondo reale e l'importanza di contabilizzare esplicitamente la variazione del punto di vista, almeno quando si tratta di localizzazione spaziale.
Come il linguaggio, la musica può essere rappresentata come una sequenza di simboli discreti che formano una sintassi gerarchica, con le note che sono più o meno come caratteri e motivi di note come parole.  A differenza del testo, però, la musica si basa molto sulla ripetizione su più scale temporali per costruire struttura e significato.Il Music Transformer ha mostrato risultati convincenti nel generare musica con struttura (Huang et al., 2018).  In questo articolo, introduciamo uno strumento per visualizzare l'autoattenzione sulla musica polifonica con un pianoroll interattivo.  Usiamo il trasformatore di musica sia come strumento descrittivo che come modello generativo.  Per il primo, lo usiamo per analizzare la musica esistente per vedere se la struttura dell'auto-attenzione risultante conferma la struttura musicale conosciuta dalla teoria musicale.  Per il secondo, ispezioniamo l'autoattenzione del modello durante la generazione, al fine di capire come le note passate influenzino quelle future.Confrontiamo e contrastiamo anche la struttura dell'attenzione regolare con quella dell'attenzione relativa (Shaw et al., 2018, Huang et al., 2018), ed esaminiamo il suo impatto sulla musica generata risultante.  Ad esempio, per il dataset JSB Chorales, un modello addestrato con l'attenzione relativa è più coerente nel frequentare tutte le voci nel timestep precedente e gli accordi prima, e alle cadenze all'inizio di una frase, permettendo di creare un arco.  Speriamo che le nostre analisi offrano ulteriori prove dell'autoattenzione relativa come un potente bias induttivo per la modellazione della musica.  Invitiamo il lettore a esplorare le nostre animazioni video dell'attenzione musicale e a interagire con le visualizzazioni su https://storage.googleapis.com/nips-workshop-visualization/index.html.
Studiamo le proprietà statistiche del punto finale della discesa stocastica del gradiente (SGD). approssimiamo SGD come un'equazione differenziale stocastica (SDE) e consideriamo la sua distribuzione di equilibrio di Boltzmann Gibbs sotto l'ipotesi di varianza isotropa nei gradienti di perdita. attraverso questa analisi, troviamo che tre fattori - tasso di apprendimento, dimensione del lotto e la varianza dei gradienti di perdita - controllano il trade-off tra la profondità e la larghezza dei minimi trovati da SGD, con minimi più ampi favoriti da un rapporto più alto tra tasso di apprendimento e dimensione del lotto. Nella distribuzione di equilibrio appare solo il rapporto tra il tasso di apprendimento e la dimensione del lotto, il che implica che è invariante sotto un ridimensionamento simultaneo di entrambi della stessa quantità. Mostriamo sperimentalmente come il tasso di apprendimento e la dimensione dei lotti influenzino SGD da due prospettive: il punto finale di SGD e le dinamiche che portano ad esso. Per il punto finale, gli esperimenti suggeriscono che il punto finale di SGD è simile sotto il ridimensionamento simultaneo della dimensione del batch e della velocità di apprendimento, e anche che un rapporto più alto porta a minimi più piatti, entrambi i risultati sono coerenti con la nostra analisi teorica. Notiamo sperimentalmente che le dinamiche sembrano essere simili anche sotto lo stesso ridimensionamento della velocità di apprendimento e della dimensione del batch, che esploriamo mostrando che si può scambiare la dimensione del batch e la velocità di apprendimento in un programma ciclico della velocità di apprendimento. Successivamente, illustriamo come il rumore influenza la memorizzazione, mostrando che alti livelli di rumore portano a una migliore generalizzazione. Infine, troviamo sperimentalmente che l'analogia sotto il ridimensionamento simultaneo del tasso di apprendimento e della dimensione del batch si rompe se il tasso di apprendimento diventa troppo grande o la dimensione del batch troppo piccola.
Sebbene i problemi di analogia di parole siano diventati uno strumento standard per valutare i vettori di parole, poco si sa sul perché i vettori di parole siano così bravi a risolvere questi problemi. In questo articolo, cerco di approfondire la nostra comprensione dell'argomento, sviluppando un approccio generativo semplice, ma molto accurato per risolvere il problema di analogia di parole nel caso in cui tutti i termini coinvolti nel problema siano sostantivi. I miei risultati dimostrano le ambiguità associate all'apprendimento della relazione tra una coppia di parole, e il ruolo del set di dati di addestramento nel determinare la relazione che viene maggiormente evidenziata. Inoltre, i miei risultati mostrano che la capacità di un modello di risolvere accuratamente il problema dell'analogia delle parole può non essere indicativa della capacità di un modello di imparare la relazione tra una coppia di parole nel modo in cui lo fa un umano.
Il fine-tuning da modelli ImageNet pre-addestrati è diventato lo standard de-facto per vari compiti di computer vision.Le pratiche attuali per il fine-tuning comportano tipicamente la selezione di una scelta ad-hoc di iper-parametri e il loro mantenimento fisso ai valori normalmente utilizzati per l'addestramento da zero.Questo articolo riesamina diverse pratiche comuni di impostazione di iper-parametri per il fine-tuning.I nostri risultati sono basati su una vasta valutazione empirica per il fine-tuning su vari benchmark di apprendimento di trasferimento.(1) Mentre i lavori precedenti hanno studiato a fondo il tasso di apprendimento e la dimensione del batch, il momentum per il fine-tuning è un parametro relativamente inesplorato. Troviamo che la scelta del giusto valore per il momentum è fondamentale per le prestazioni di fine-tuning e lo colleghiamo con i precedenti risultati teorici.(2) Gli iper-parametri ottimali per il fine-tuning, in particolare il tasso di apprendimento effettivo, non dipendono solo dal set di dati, ma sono anche sensibili alla somiglianza tra il dominio di origine e quello di destinazione. Questo è in contrasto con gli iper-parametri per l'addestramento da zero.(3) La regolarizzazione basata sul riferimento che mantiene i modelli vicini al modello iniziale non si applica necessariamente per set di dati "dissimili". I nostri risultati sfidano le pratiche comuni di fine tuning e incoraggiano i professionisti del deep learning a ripensare gli iper-parametri per il fine tuning.
L'apprendimento di trasferimento attraverso il fine-tuning di una rete neurale pre-addestrata con un set di dati estremamente grande, come ImageNet, può accelerare significativamente la formazione, mentre la precisione è spesso strozzata dalla dimensione limitata del set di dati del nuovo compito di destinazione.Per risolvere il problema, alcuni metodi di regolarizzazione, vincolando i pesi dello strato esterno della rete di destinazione utilizzando il punto di partenza come riferimenti (SPAR), sono stati studiati.In questo articolo, proponiamo un nuovo quadro di apprendimento di trasferimento regolarizzato DELTA, cioè DEep Learning Transfer utilizzando Feature Map con attenzione. In particolare, oltre a minimizzare la perdita empirica, DELTA intende allineare le uscite dello strato esterno di due reti, attraverso il vincolo di un sottoinsieme di mappe di caratteristiche che sono precisamente selezionate dall'attenzione che è stata appresa in un modo di apprendimento supervisionato. valutiamo DELTA con lo stato dell'arte degli algoritmi, tra cui L2 e L2-SP. i risultati degli esperimenti mostrano che il nostro metodo proposto supera queste linee di base con maggiore precisione per nuovi compiti.
I modelli neurali hanno raggiunto un notevole miglioramento per molti compiti di elaborazione del linguaggio naturale, ma offrono poca trasparenza, e l'interpretabilità ha un costo.In alcuni domini, le previsioni automatiche senza giustificazioni hanno un'applicabilità limitata.Recentemente, sono stati fatti progressi per quanto riguarda l'analisi del sentimento a singolo aspetto per le recensioni, dove l'ambiguità di una giustificazione è minima.In questo contesto, una giustificazione, o maschera, consiste in (lunghe) sequenze di parole dal testo di input, che sono sufficienti per fare la previsione. Nel nostro lavoro, proponiamo un modello neurale per prevedere i sentimenti multi-aspetto per le recensioni e genera una maschera probabilistica multi-dimensionale (una per aspetto) simultaneamente, in un modo di apprendimento non supervisionato e multi-task.La nostra valutazione mostra che su tre set di dati, nel dominio della birra e dell'hotel, il nostro modello supera le forti basi e genera maschere che sono: forti predittori di caratteristiche, significativi e interpretabili.
La generazione di sequenze neurali è comunemente approcciata utilizzando la stima di massima verosimiglianza (ML) o l'apprendimento di rinforzo (RL), ma è noto che essi hanno i loro difetti; ML presenta una discrepanza tra addestramento e test, mentre RL soffre di inefficienza del campione. Sottolineiamo che è difficile risolvere tutti i difetti simultaneamente a causa di un compromesso tra ML e RL.Al fine di contrastare questi problemi, proponiamo una funzione obiettivo per la generazione di sequenze utilizzando α-divergenza, che porta ad un metodo integrato ML-RL che sfrutta le parti migliori di ML e RL. Dimostriamo che la funzione obiettivo proposta generalizza le funzioni obiettivo ML e RL perché include entrambe come casi speciali (ML corrisponde ad α → 0 e RL ad α → 1).Forniamo una proposizione che afferma che la differenza tra la funzione obiettivo RL e quella proposta diminuisce monotonicamente all'aumentare di α.I risultati sperimentali su compiti di traduzione automatica mostrano che minimizzando la funzione obiettivo proposta si ottengono migliori prestazioni di generazione di sequenze rispetto ai metodi basati su ML.
Le reti a capsula hanno mostrato risultati incoraggianti su set di dati di riferimento di computer vision come MNIST, CIFAR e smallNORB, anche se devono ancora essere testati su compiti in cui (1) le entità rilevate hanno intrinsecamente rappresentazioni interne più complesse e (2) ci sono poche istanze per classe da cui imparare e (3) dove la classificazione point-wise non è adatta. Nel fare ciò, introduciamo \textit{Reti a capsula siamesi}, una nuova variante che può essere usata per compiti di apprendimento a coppie. Troviamo che il modello migliora rispetto alla linea di base nell'impostazione di apprendimento a pochi scatti, suggerendo che le reti a capsula sono efficienti nell'apprendere rappresentazioni discriminanti quando vengono dati pochi campioni.  Troviamo che le reti di capsule siamesi si comportano bene contro le linee di base forti su entrambi i set di dati di apprendimento a coppie quando sono addestrate usando una perdita contrastiva con caratteristiche di posa codificate dalla capsula $ell_2$, ottenendo i migliori risultati nell'impostazione di apprendimento a pochi scatti in cui le coppie di immagini nel set di test contengono soggetti non visti.
In questo lavoro presentiamo una nuova architettura dell'agente, chiamata Reactor, che combina molteplici contributi algoritmici e architettonici per produrre un agente con una maggiore efficienza del campione rispetto al Prioritized Dueling DQN (Wang et al., 2016) e al Categorical DQN (Bellemare et al., 2017), dando allo stesso tempo migliori prestazioni run-time rispetto a A3C (Mnih et al., 2016).Il nostro primo contributo è un nuovo algoritmo di valutazione della politica chiamato Distributional Retrace, che porta gli aggiornamenti off-policy multi-step all'impostazione di apprendimento del rinforzo distributivo.Lo stesso approccio può essere utilizzato per convertire diverse classi di algoritmi di valutazione della politica multi-step progettati per la valutazione del valore atteso in quelli distributivi.Successivamente, introduciamo l'algoritmo β-leaveone-out policy gradient che migliora il trade-off tra varianza e bias utilizzando i valori di azione come base. Il nostro contributo algoritmico finale è un nuovo algoritmo di replay prioritario per le sequenze, che sfrutta la località temporale delle osservazioni vicine per una più efficiente prioritizzazione del replay.Utilizzando i benchmark Atari 2600, dimostriamo che ognuna di queste innovazioni contribuisce sia all'efficienza del campione che alle prestazioni finali dell'agente.Infine, dimostriamo che Reactor raggiunge prestazioni allo stato dell'arte dopo 200 milioni di frame e meno di un giorno di allenamento.
La pianificazione gerarchica, in particolare, Hierarchical Task Networks, è stata proposta come un metodo per descrivere i piani attraverso la decomposizione dei compiti in sotto-compiti fino ad ottenere compiti primitivi, le azioni.La verifica dei piani assume un piano completo come input, e l'obiettivo è trovare un compito che si decomponga in questo piano.Nel riconoscimento dei piani, un prefisso del piano è dato e l'obiettivo è trovare un compito che si decomponga nel piano (più breve) con il prefisso dato.Questo articolo descrive come verificare e riconoscere i piani utilizzando un metodo comune conosciuto dalle grammatiche formali, attraverso il parsing.
La ricerca di architetture neurali (NAS), il compito di trovare automaticamente le architetture neurali, è emersa recentemente come un approccio promettente per svelare modelli migliori di quelli progettati dall'uomo.Tuttavia, la maggior parte delle storie di successo sono per compiti di visione e sono state piuttosto limitate per il testo, ad eccezione di una piccola configurazione di modellazione linguistica.In questo articolo, esploriamo NAS per sequenze di testo su scala, concentrandoci prima sul compito di traduzione linguistica e poi estendendo alla comprensione della lettura. Da un modello standard sequenza-sequenza per la traduzione, conduciamo ampie ricerche sulle celle ricorrenti e sulle funzioni di somiglianza dell'attenzione in due compiti di traduzione, IWSLT inglese-vietnamita e WMT tedesco-inglese. riportiamo le sfide nell'esecuzione delle ricerche sulle celle e dimostriamo il successo iniziale delle ricerche sull'attenzione con miglioramenti della traduzione rispetto alle linee di base forti.inoltre, mostriamo che i risultati sulle ricerche sull'attenzione sono trasferibili alla comprensione della lettura sul dataset SQuAD.
Gli autocodificatori forniscono un quadro potente per l'apprendimento di rappresentazioni compresse codificando tutte le informazioni necessarie per ricostruire un punto di dati in un codice latente.In alcuni casi, gli autocodificatori possono "interpolare": Decodificando la combinazione convessa dei codici latenti per due punti dati, l'autoencoder può produrre un output che mescola semanticamente le caratteristiche dei punti dati.In questo articolo, proponiamo una procedura di regolarizzazione che incoraggia gli output interpolati ad apparire più realistici ingannando una rete critica che è stata addestrata a recuperare il coefficiente di miscelazione dai dati interpolati. Sviluppiamo quindi un semplice compito di riferimento in cui possiamo misurare quantitativamente la misura in cui vari autoencoder possono interpolare e dimostriamo che il nostro regolarizzatore migliora drasticamente l'interpolazione in questo contesto. Dimostriamo anche empiricamente che il nostro regolarizzatore produce codici latenti che sono più efficaci nei compiti a valle, suggerendo un possibile legame tra le capacità di interpolazione e l'apprendimento di rappresentazioni utili.
Consideriamo il problema della generazione di sequenze video plausibili e diverse, quando ci vengono dati solo un inizio e una fine del fotogramma.Questo compito è noto anche come inbetweening, e appartiene all'area più ampia della generazione stocastica di video, che viene generalmente affrontata per mezzo di reti neurali ricorrenti (RNN).In questo articolo, proponiamo invece un modello completamente convoluzionale per generare sequenze video direttamente nel dominio dei pixel.otteniamo prima una rappresentazione video latente utilizzando un meccanismo di fusione stocastica che impara come incorporare le informazioni dai fotogrammi di inizio e fine. Il nostro modello impara a produrre tale rappresentazione latente aumentando progressivamente la risoluzione temporale, e poi decodifica nel dominio spazio-temporale usando le convoluzioni 3D. Il modello è addestrato end-to-end minimizzando una perdita avversaria. Gli esperimenti su diversi set di dati di riferimento ampiamente utilizzati mostrano che è in grado di generare sequenze video significative e diverse tra loro, secondo valutazioni sia quantitative che qualitative.
Allineare i grafi di conoscenza da diverse fonti o lingue, che mira ad allineare sia l'entità che la relazione, è fondamentale per una varietà di applicazioni come la costruzione del grafo di conoscenza e la risposta alle domande.I metodi esistenti di allineamento del grafo di conoscenza di solito si basano su un gran numero di triplette di conoscenza allineate per addestrare modelli efficaci.Tuttavia, queste triplette allineate possono non essere disponibili o sono costose da ottenere per molti domini.Pertanto, in questa carta studiamo come progettare metodi completamente non supervisionati o metodi debolmente supervisionati, cioè, Proponiamo una struttura non supervisionata basata sull'addestramento avversario, che è in grado di mappare le entità e le relazioni in un grafo di conoscenza di origine a quelle in un grafo di conoscenza di destinazione. Questa struttura può essere ulteriormente integrata senza soluzione di continuità con i metodi supervisionati esistenti, dove solo un numero limitato di triplette allineate viene utilizzato come guida.Gli esperimenti su set di dati del mondo reale dimostrano l'efficacia del nostro approccio proposto sia nelle impostazioni debolmente supervisionate che non supervisionate.
L'apprendimento multi-vista può fornire l'auto-supervisione quando sono disponibili diverse viste degli stessi dati.L'ipotesi distributiva fornisce un'altra forma di auto-supervisione utile dalle frasi adiacenti che sono abbondanti nei grandi corpora non etichettati.Motivati dall'asimmetria nei due emisferi del cervello umano, nonché dall'osservazione che diverse architetture di apprendimento tendono a sottolineare diversi aspetti del significato della frase, presentiamo due strutture multi-vista per l'apprendimento di rappresentazioni di frasi in modo non supervisionato. Una struttura utilizza un obiettivo generativo e l'altra uno discriminativo. In entrambe le strutture, la rappresentazione finale è un insieme di due viste, in cui una vista codifica la frase di input con una rete neurale ricorrente (RNN), e l'altra vista la codifica con un semplice modello lineare. Mostriamo che, dopo l'apprendimento, i vettori prodotti dalle nostre strutture multi-vista forniscono rappresentazioni migliori rispetto alle loro controparti apprese da una sola vista, e la combinazione di diverse viste fornisce un miglioramento della rappresentazione rispetto a ciascuna vista e dimostra una solida trasferibilità su compiti standard a valle.
Ci sono miriadi di tipi di segmentazione, e alla fine la "giusta" segmentazione di una data scena è nell'occhio dell'annotatore. Gli approcci standard richiedono grandi quantità di dati etichettati per imparare solo un particolare tipo di segmentazione. Come primo passo verso l'alleggerimento di questo onere di annotazione, proponiamo il problema della segmentazione guidata: date quantità variabili di etichette pixel-wise, segmentare i pixel non annotati propagando la supervisione localmente (all'interno di un'immagine) e non localmente (attraverso le immagini).Proponiamo reti guidate, che estraggono una rappresentazione latente del compito---guidance---da quantità e classi variabili (categorie, istanze, ecc. ) di supervisione dei pixel e ottimizziamo la nostra architettura end-to-end per una segmentazione veloce, accurata ed efficiente dal punto di vista dei dati attraverso il meta-apprendimento. Per spaziare tra i regimi di apprendimento a pochi scatti e a molti scatti, esaminiamo la guida da un minimo di un pixel per concetto fino a più di 1000 immagini, e confrontiamo l'ottimizzazione del gradiente completo ad entrambi gli estremi. Per esplorare la generalizzazione, analizziamo la guida come un ponte tra diversi livelli di supervisione per segmentare le classi come unione di istanze. Il nostro segmentatore concentra diverse quantità di supervisione di diversi tipi di classi in una rappresentazione latente efficiente, propaga non localmente questa supervisione attraverso le immagini, e può essere aggiornato rapidamente e cumulativamente quando viene data più supervisione.
L'addestramento delle reti generative avversarie richiede il bilanciamento di dinamiche avversarie delicate.Anche con un'attenta messa a punto, l'addestramento può divergere o finire in un cattivo equilibrio con modalità abbandonate.In questo lavoro, introduciamo una nuova forma di ottimizzazione latente ispirata al CS-GAN e dimostriamo che migliora le dinamiche avversarie migliorando le interazioni tra il discriminatore e il generatore.Sviluppiamo un'analisi teorica di supporto dalle prospettive dei giochi differenziabili e dell'approssimazione stocastica. I nostri esperimenti dimostrano che l'ottimizzazione latente può migliorare significativamente l'addestramento GAN, ottenendo prestazioni allo stato dell'arte per il dataset ImageNet (128 x 128).Il nostro modello raggiunge un Inception Score (IS) di 148 e una Frechet Inception Distance (FID) di 3.4, un miglioramento del 17% e del 32% in IS e FID rispettivamente, rispetto al modello di base BigGAN-deep con la stessa architettura e numero di parametri.
In questo articolo, studiamo il problema dell'ottimizzazione di una rete neurale artificiale a due strati che si adatti al meglio a un set di dati di addestramento; esaminiamo questo problema nell'impostazione in cui il numero di parametri è maggiore del numero di punti campionati; dimostriamo che per un'ampia classe di funzioni di attivazione differenziabili (questa classe coinvolge la maggior parte delle funzioni non lineari ed esclude le funzioni lineari a sezione quadrata), abbiamo che le soluzioni ottimali di primo ordine arbitrarie soddisfano l'ottimalità globale a condizione che lo strato nascosto non sia singolare. Mostriamo essenzialmente che queste matrici di strato nascosto non singolari soddisfano una proprietà "buona" per questa grande classe di funzioni di attivazione. Le tecniche coinvolte nella dimostrazione di questo risultato ci ispirano a guardare un nuovo algoritmo, dove tra due passi di gradiente dello strato nascosto, aggiungiamo un passo di discesa del gradiente stocastico (SGD) dello strato di uscita. In questa nuova struttura algoritmica, estendiamo il nostro risultato precedente e mostriamo che per tutte le iterazioni finite lo strato nascosto soddisfa la proprietà "buona" menzionata in precedenza, spiegando così in parte il successo dei metodi a gradiente rumoroso e affrontando il problema dell'indipendenza dei dati del nostro risultato precedente. Entrambi questi risultati sono facilmente estesi agli strati nascosti dati da una matrice piatta da quella di una matrice quadrata. I risultati sono applicabili anche se la rete ha più di uno strato nascosto a condizione che tutti gli strati nascosti interni siano arbitrari, soddisfino la non singolarità, tutte le attivazioni siano della data classe di funzioni differenziabili e l'ottimizzazione sia solo rispetto allo strato nascosto più esterno, Usiamo le proprietà di levigatezza per garantire una convergenza asintotica di $O(1/contesto{numero di iterazioni})$ a una soluzione ottimale del primo ordine.
Introduciamo il concetto di aggregazione dei canali nell'architettura ConvNet, una nuova rappresentazione compatta delle caratteristiche CNN utile per modellare esplicitamente la codifica dei canali non lineari, specialmente quando la nuova unità è incorporata all'interno di architetture profonde per il riconoscimento delle azioni.L'aggregazione dei canali è basata sulle caratteristiche a canali multipli di ConvNet e mira a essere sul punto di trovare il percorso ottico di convergenza a velocità elevata.Chiamiamo la nostra architettura convoluzionale proposta "reti di aggregazione dei canali non lineari (NCAN)" e il suo nuovo strato "strato di aggregazione dei canali non lineari (NCAL)". Un altro contributo in questo lavoro è un'implementazione efficiente ed efficace del NCAL, che accelera gli ordini di grandezza, valutando le sue prestazioni sui benchmark standard UCF101 e HMDB51, e i risultati sperimentali dimostrano che questa formulazione non solo ottiene una convergenza veloce ma anche una capacità di generalizzazione più forte senza sacrificare le prestazioni.
A differenza dei metodi precedenti che combinavano metodi basati sul trasferimento e metodi basati sul punteggio usando il gradiente o l'inizializzazione di un modello white-box surrogato, questo nuovo metodo cerca di imparare un'incorporazione a bassa dimensione usando un modello preaddestrato, e poi esegue una ricerca efficiente all'interno dello spazio di incorporazione per attaccare una rete di destinazione sconosciuta.Il metodo produce perturbazioni avversarie con modelli semantici di alto livello che sono facilmente trasferibili. Dimostriamo che questo approccio può migliorare notevolmente l'efficienza delle query dell'attacco black-box adversarial su diverse architetture di rete target.Valutiamo il nostro approccio su MNIST, ImageNet e Google Cloud Vision API, ottenendo una riduzione significativa del numero di query.Attacchiamo anche reti difese avversariamente su CIFAR10 e ImageNet, dove il nostro metodo non solo riduce il numero di query, ma migliora anche il tasso di successo dell'attacco.
Le reti neurali profonde (DNN) si ispirano al cervello umano e l'interconnessione tra i due è stata ampiamente studiata in letteratura.  Tuttavia, è ancora una questione aperta se le DNN siano in grado di prendere decisioni come il cervello.Un lavoro precedente ha dimostrato che le DNN, addestrate abbinando le risposte neurali della corteccia temporale inferiore (IT) nel cervello della scimmia, sono in grado di raggiungere prestazioni di livello umano nei compiti di riconoscimento degli oggetti dell'immagine.Questo indica che le dinamiche neurali possono fornire conoscenze informative per aiutare le DNN a svolgere compiti specifici. In questo articolo, introduciamo il concetto di un'interfaccia neuro-AI, che mira a utilizzare le risposte neurali umane come informazioni supervisionate per aiutare i sistemi AI a risolvere un compito che è difficile quando si utilizzano le strategie tradizionali di apprendimento automatico.Al fine di fornire l'idea di interfacce neuro-AI, ci concentriamo sull'implementazione di uno dei problemi fondamentali nelle reti generative avversarie (GAN): la progettazione di una metrica di valutazione adeguata per valutare la qualità delle immagini prodotte dalle GAN.   
Mentre i recenti sviluppi nella tecnologia dei veicoli autonomi (AV) evidenziano progressi sostanziali, ci mancano strumenti per test rigorosi e scalabili. I test nel mondo reale, l'ambiente di valutazione de facto, mettono in pericolo il pubblico e, a causa della natura rara degli incidenti, richiederanno miliardi di chilometri per convalidare statisticamente le prestazioni. Utilizzando metodi di campionamento adattivi per accelerare la valutazione della probabilità di eventi rari, stimiamo la probabilità di un incidente sotto una distribuzione di base che governa il comportamento standard del traffico e dimostriamo la nostra struttura su uno scenario autostradale.
Molti compiti nella comprensione del linguaggio naturale richiedono l'apprendimento di relazioni tra due sequenze per vari compiti come l'inferenza del linguaggio naturale, la parafrasi e l'entailment.Questi compiti summenzionati sono simili in natura, ma sono spesso modellati individualmente.Il trasferimento della conoscenza può essere efficace per compiti strettamente correlati, che di solito viene effettuato utilizzando il trasferimento dei parametri nelle reti neurali.Tuttavia, il trasferimento di tutti i parametri, alcuni dei quali irrilevanti per un compito di destinazione, può portare a risultati sub-ottimali e può avere un effetto negativo sulle prestazioni, indicato come trasferimento \textit{negativo}. Il nostro contributo principale è un metodo per mitigare il trasferimento negativo tra i compiti quando si usano le reti neurali, che coinvolge dinamicamente il bagging di piccole reti neurali ricorrenti addestrate su diversi sottoinsiemi del/i compito/i di origine. Presentiamo un approccio semplice ma innovativo per incorporare queste reti in un compito di destinazione per l'apprendimento di pochi colpi utilizzando un parametro decadente scelto in base ai cambiamenti di pendenza di una curva di errore spline lisciata a sotto-intervalli durante la formazione. Il nostro metodo proposto mostra miglioramenti rispetto ai metodi di trasferimento di condivisione dei parametri hard e soft nel caso dell'apprendimento di pochi colpi e mostra prestazioni competitive rispetto ai modelli che sono addestrati con piena supervisione sul compito di destinazione, da solo pochi esempi.
La capacità di quantificare e prevedere la progressione di una malattia è fondamentale per la selezione di un trattamento appropriato.Molte metriche cliniche non possono essere acquisite frequentemente a causa del loro costo (ad esempio, la risonanza magnetica, l'analisi dell'andatura) o perché sono scomode o dannose per un paziente (ad esempio, la biopsia, i raggi X).In tali scenari, per stimare le traiettorie individuali di progressione della malattia, è vantaggioso sfruttare le somiglianze tra i pazienti, cioè la covarianza delle traiettorie, e trovare una rappresentazione latente della progressione. La maggior parte dei metodi esistenti per la stima delle traiettorie non tiene conto degli eventi tra le osservazioni, il che diminuisce drasticamente la loro adeguatezza per la pratica clinica. In questo studio, sviluppiamo un quadro di apprendimento automatico chiamato Coordinatewise-Soft-Impute (CSI) per analizzare la progressione della malattia da osservazioni sparse in presenza di eventi confondenti. CSI è garantito per convergere al minimo globale del problema di ottimizzazione corrispondente.
I sistemi di traduzione automatica neurale multilingue (NMT) sono in grado di tradurre tra più lingue di origine e di destinazione all'interno di un singolo sistema.Un importante indicatore di generalizzazione all'interno di questi sistemi è la qualità della traduzione zero-shot - la traduzione tra coppie di lingue che il sistema non ha mai visto durante l'addestramento.Tuttavia, fino ad ora, la performance zero-shot dei modelli multilingue è rimasta molto indietro rispetto alla qualità che può essere raggiunta utilizzando un processo di traduzione a due fasi che fa perno su una lingua intermedia (di solito l'inglese). In questo lavoro, diagnostichiamo perché i modelli multilingue sotto-performano nelle impostazioni zero shot.Proponiamo esplicite perdite di invarianza linguistica che guidano un codificatore NMT verso l'apprendimento di rappresentazioni agnostiche alla lingua.Le nostre strategie proposte migliorano significativamente le prestazioni di traduzione zero-shot su WMT inglese-francese-tedesco e sul compito condiviso IWSLT 2017, e per la prima volta, corrispondono alle prestazioni degli approcci pivoting mantenendo le prestazioni sulle direzioni supervisionate.
Dimostriamo la scala precisa, a profondità e larghezza finite, per la media e la varianza del kernel tangente neurale (NTK) in una rete ReLU inizializzata in modo casuale.La deviazione standard è esponenziale nel rapporto tra la profondità e la larghezza della rete.Quindi, anche nel limite dell'iperparametrizzazione infinita, l'NTK non è deterministico se la profondità e la larghezza tendono contemporaneamente all'infinito. Inoltre, dimostriamo che per tali reti profonde e larghe, l'NTK ha un'evoluzione non banale durante l'addestramento mostrando che la media del suo primo aggiornamento SGD è anche esponenziale nel rapporto tra profondità e larghezza della rete, in netto contrasto con il regime in cui la profondità è fissa e la larghezza della rete è molto grande.
La maggior parte degli algoritmi per l'apprendimento delle rappresentazioni e la predizione dei collegamenti nei dati relazionali sono stati progettati per dati statici.Tuttavia, i dati a cui sono applicati di solito si evolvono nel tempo, come i grafici degli amici nei social network o le interazioni degli utenti con gli elementi nei sistemi di raccomandazione.Questo è anche il caso delle basi di conoscenza, che contengono fatti come (USA, ha presidente, B. Obama, [2009-2017]) che sono validi solo in determinati punti nel tempo.Per il problema della predizione dei collegamenti sotto vincoli temporali, cioè, Per il problema della predizione dei link sotto vincoli temporali, cioè rispondendo a query della forma (US, has president, ?, 2012), proponiamo una soluzione ispirata alla decomposizione canonica dei tensori di ordine 4. Introduciamo nuovi schemi di regolarizzazione e presentiamo un'estensione di ComplEx che raggiunge prestazioni all'avanguardia. Inoltre, proponiamo un nuovo set di dati per il completamento della base di conoscenza costruito da Wikidata, più grande dei precedenti benchmark di un ordine di grandezza, come nuovo riferimento per valutare i metodi di predizione dei link temporali e non temporali.
L'approccio convenzionale per risolvere il problema della raccomandazione classifica avidamente i singoli candidati al documento in base ai punteggi di predizione. Tuttavia, questo metodo non riesce a ottimizzare la lista nel suo complesso, e quindi, spesso fatica a catturare le distorsioni causate dal layout della pagina e dalle interdipendenze dei documenti. Il problema della raccomandazione della lista mira a trovare direttamente il sottoinsieme di documenti ordinati in modo ottimale (cioè le liste) che meglio servono gli interessi degli utenti. Perciò proponiamo un cambio di paradigma dal punto di vista tradizionale di risolvere un problema di ranking a un quadro di generazione diretta di slate.In questo documento, introduciamo List Conditional Variational Auto-Encoders (ListCVAE), che imparano la distribuzione congiunta dei documenti sulla slate condizionata dalle risposte degli utenti, e generano direttamente slate complete.Gli esperimenti su dati simulati e del mondo reale mostrano che List-CVAE supera i metodi di ranking greedy in modo consistente su varie scale di corpora di documenti.
Le reti neurali per i dati strutturati come i grafi sono state studiate estesamente negli ultimi anni. Finora, la maggior parte dell'attività di ricerca si è concentrata principalmente sui grafi statici. Tuttavia, la maggior parte delle reti del mondo reale sono dinamiche poiché la loro topologia tende a cambiare nel tempo. In particolare, utilizziamo una rete neurale a grafo con un'architettura ricorrente per catturare i modelli di evoluzione temporale dei grafi dinamici, quindi impieghiamo un modello generativo che predice la topologia del grafo al prossimo passo temporale e costruisce un'istanza del grafo che corrisponde a quella topologia. valutiamo il modello proposto su diversi set di dati artificiali che seguono le dinamiche di evoluzione della rete comune, così come su set di dati del mondo reale.
Per quanto riguarda la risposta alle domande basate sulla conoscenza, un problema fondamentale è quello di rilassare il presupposto delle domande rispondibili da domande semplici a domande composte.Gli approcci tradizionali in primo luogo rilevano l'entità dell'argomento menzionato nelle domande, quindi attraversano il grafo della conoscenza per trovare le relazioni come un percorso multi-hop verso le risposte, mentre noi proponiamo un nuovo approccio per sfruttare le risposte alle domande semplici per rispondere alle domande composte. Il nostro modello consiste di due parti:(i) un nuovo agente che impara a decomporre che impara una politica per decomporre una domanda composta in domande semplici e(ii) tre risponditori indipendenti di domande semplici che classificano le relazioni corrispondenti per ogni domanda semplice.Gli esperimenti dimostrano che il nostro modello impara regole complesse di composizione come politica stocastica, che beneficia delle reti neurali semplici per ottenere risultati all'avanguardia su WebQuestions e MetaQA.Analizziamo il processo di decomposizione interpretabile così come le partizioni generate.
I modelli basati sull'energia producono valori di log-probabilità non normalizzati dati i campioni di dati.  Tale stima è essenziale in una varietà di problemi applicativi come la generazione di campioni, il denoising, il restauro di campioni, il rilevamento di outlier, il ragionamento bayesiano e molti altri.  Tuttavia, l'addestramento standard della massima verosimiglianza è computazionalmente costoso a causa del requisito di campionamento della distribuzione del modello. Lo score matching allevia potenzialmente questo problema, e il denoising score matching (Vincent, 2011) è una versione particolarmente conveniente.  Tuttavia, i tentativi precedenti non sono riusciti a produrre modelli capaci di una sintesi campionaria di alta qualità.  Per superare questa limitazione, qui impariamo invece una funzione energetica utilizzando tutte le scale di rumore.   Quando il campionamento è stato effettuato utilizzando la dinamica di Annealed Langevin e il salto di denoising a un solo passo, il nostro modello ha prodotto campioni di alta qualità paragonabili a tecniche all'avanguardia come i GAN, oltre ad assegnare probabilità a dati di test paragonabili a modelli di probabilità precedenti.  Il nostro modello ha stabilito una nuova linea di base per la qualità dei campioni nei modelli basati sulla verosimiglianza.  Dimostriamo inoltre che il nostro modello impara la distribuzione del campione e generalizza bene su un compito di inpainting dell'immagine.
Una macchina di Boltzmann ristretta (RBM) impara una distribuzione probabilistica sui suoi campioni di input e ha numerosi usi come la riduzione della dimensionalità, la classificazione e la modellazione generativa.Le RBM convenzionali accettano dati vettoriali che liquidano le informazioni strutturali potenzialmente importanti nel tensore originale (a più vie) di input.Le RBM matrice-variate e tensore-variate, chiamate MvRBM e TvRBM, sono state proposte ma sono tutte restrittive per costruzione. Questo lavoro presenta il prodotto matriciale operatore RBM (MPORBM) che utilizza una generalizzazione della rete tensoriale di Mv/TvRBM, conserva i formati di input sia negli strati visibili che nascosti, e risulta in una maggiore potenza espressiva.
La guida autonoma è ancora considerata un "problema irrisolto", data la sua importante variabilità intrinseca e il fatto che molti processi associati al suo sviluppo, come il controllo del veicolo e il riconoscimento delle scene, rimangono questioni aperte. Nonostante gli algoritmi di apprendimento per rinforzo abbiano raggiunto risultati notevoli nei giochi e in alcune manipolazioni robotiche, questa tecnica non è stata ampiamente scalata fino alle applicazioni più impegnative del mondo reale come la guida autonoma. In questo lavoro, proponiamo un algoritmo di reinforcement learning (RL) profondo che incorpora un'architettura critica dell'attore con ritorni multi-step per ottenere una migliore robustezza delle strategie di apprendimento dell'agente quando agisce in ambienti complessi e instabili.L'esperimento è condotto con il simulatore Carla che offre condizioni di guida urbana personalizzabili e realistiche.L'attore RL profondo sviluppato guidato da un critico valutatore di politiche supera nettamente le prestazioni di un agente RL profondo standard.
Una domanda fondamentale, e ancora in gran parte senza risposta, nel contesto delle Generative Adversarial Networks (GANs) è se le GANs siano effettivamente in grado di catturare le caratteristiche chiave dei dataset su cui sono addestrate.Gli approcci attuali per esaminare questo problema richiedono una significativa supervisione umana, come l'ispezione visiva delle immagini campionate, e spesso offrono solo una scalabilità abbastanza limitata.In questo articolo, proponiamo nuove tecniche che impiegano una prospettiva basata sulla classificazione per valutare le distribuzioni sintetiche delle GAN e la loro capacità di riflettere accuratamente le proprietà essenziali dei dati di allenamento. Queste tecniche richiedono solo una minima supervisione umana e possono essere facilmente scalate e adattate per valutare una varietà di GAN allo stato dell'arte su set di dati ampi e popolari, indicando inoltre che le GAN hanno problemi significativi nel riprodurre le proprietà più distributive del set di dati di allenamento.
L'obiettivo del clustering di sopravvivenza è quello di mappare i soggetti (ad es, I metodi di sopravvivenza esistenti presuppongono la presenza di chiari segnali di fine vita o li introducono artificialmente utilizzando un timeout predefinito. In questo articolo, rinunciamo a questa assunzione e introduciamo una funzione di perdita che differenzia tra le distribuzioni di vita empiriche dei cluster utilizzando una statistica di Kuiper modificata. Apprendiamo una rete neurale profonda ottimizzando questa perdita, che esegue un soft clustering degli utenti in gruppi di sopravvivenza.Applichiamo il nostro metodo a un dataset di social network con oltre 1M soggetti, e mostriamo un miglioramento significativo del C-index rispetto alle alternative.
L'ottimizzazione bayesiana (BO) è una metodologia popolare per sintonizzare gli iperparametri di costose funzioni black-box. Nonostante il suo successo, BO standard si concentra su un singolo compito alla volta e non è progettato per sfruttare le informazioni da funzioni correlate, come la sintonizzazione delle metriche di performance dello stesso algoritmo su più set di dati. In questo lavoro, introduciamo un nuovo approccio per ottenere l'apprendimento di trasferimento attraverso diversi set di dati e diverse metriche. L'idea principale è quella di regredire la mappatura da iperparametri a quantili di metrica con una distribuzione copula gaussiana semi-parametrica, che fornisce robustezza contro diverse scale o outlier che possono verificarsi in diversi compiti. Introduciamo due metodi per sfruttare questa stima: una strategia di campionamento Thompson e un processo Copula Gaussiano che utilizza tale stima quantile come priore. Mostriamo che queste strategie possono combinare la stima di più metriche come il tempo di esecuzione e la precisione, guidando l'ottimizzazione verso iperparametri meno costosi per lo stesso livello di precisione.Gli esperimenti su un ampio set di compiti di sintonizzazione degli iperparametri dimostrano miglioramenti significativi rispetto ai metodi all'avanguardia.
Proponiamo Pure CapsNets (P-CapsNets) senza procedure di routing.  In primo luogo, rimuoviamo le procedure di routing da CapsNets sulla base dell'osservazione che i coefficienti di accoppiamento possono essere appresi implicitamente.In secondo luogo, sostituiamo gli strati convoluzionali in CapsNets per migliorare l'efficienza.In terzo luogo, impacchettiamo le capsule in tensori rank-3 per migliorare ulteriormente l'efficienza. L'esperimento mostra che le P-CapsNets raggiungono prestazioni migliori delle CapsNets con varie procedure di routine usando significativamente meno parametri su MNIST&CIFAR10.L'alta efficienza delle P-CapsNets è persino paragonabile ad alcuni modelli di compressione profonda.Per esempio, raggiungiamo più del 99% di precisione su MNIST usando solo 3888 parametri.  Visualizziamo le capsule così come la matrice di correlazione corrispondente per mostrare un possibile modo di inizializzare le Capsnet in futuro.Esploriamo anche la robustezza avversaria delle P-Capsnet rispetto alle CNN.
	Una recente linea di lavoro ha studiato le proprietà statistiche delle reti neurali con grande successo dal punto di vista della teoria del campo medio, facendo e verificando previsioni molto precise del comportamento delle reti neurali e delle prestazioni a tempo di test.	In questo articolo, ci basiamo su questi lavori per esplorare due metodi per domare il comportamento delle reti residuali casuali (con solo strati completamente connessi e senza batchnorm).	Il primo metodo è la variazione della larghezza (WV), cioè la variazione della larghezza degli strati in funzione della profondità.	Mostriamo che il decadimento della larghezza riduce l'esplosione del gradiente senza influenzare la dinamica media in avanti della rete casuale.	Il secondo metodo è la variazione della varianza di inizializzazione (VV), cioè cambiare le varianze di inizializzazione dei pesi e dei bias in funzione della profondità.	Mostriamo che la VV, usata in modo appropriato, può ridurre l'esplosione del gradiente delle reti tanh e ReLU da $exp(\Theta(\sqrt L))$ e $exp(\Theta(L))$ rispettivamente alla costante $\Theta(1)$.	Un completo diagramma di fase è derivato per il modo in cui il decadimento della varianza influenza diverse dinamiche, come quelle del gradiente e delle norme di attivazione.	In particolare, mostriamo l'esistenza di molte transizioni di fase in cui queste dinamiche passano tra comportamenti esponenziali, polinomiali, logaritmici e persino costanti.	Utilizzando la teoria del campo medio ottenuta, siamo in grado di tracciare sorprendentemente bene come la VV al momento dell'inizializzazione influenzi le prestazioni in tempo di addestramento e di test su MNIST dopo un certo numero di epoche: gli insiemi di livello delle accuratezze del set test/train coincidono con gli insiemi di livello delle aspettative di alcune norme di gradiente o di espressività metrica (come definito in \cite{yang_meanfield_2017}), una misura di espansione in una rete neurale casuale.	Sulla base di intuizioni da lavori passati nella teoria del campo medio profondo e nella geometria dell'informazione, forniamo anche una nuova prospettiva sui problemi di esplosione/vanishing del gradiente: essi portano a un malcondizionamento della matrice di informazione di Fisher, causando problemi di ottimizzazione.
Esploriamo l'impostazione collaborativa multi-agente in cui una squadra di agenti di apprendimento di rinforzo profondo tenta di risolvere un compito condiviso in ambienti parzialmente osservabili.In questo scenario, l'apprendimento di un protocollo di comunicazione efficace è fondamentale.Proponiamo un protocollo di comunicazione che permette una comunicazione mirata, in cui gli agenti imparano \emphè cosa} i messaggi da inviare e \emphè a chi} inviarli.Inoltre, introduciamo un approccio di comunicazione multistadio in cui gli agenti si coordinano attraverso diversi round di comunicazione prima di intraprendere un'azione nell'ambiente. Valutiamo il nostro approccio su diversi compiti cooperativi multi-agente, di varia difficoltà con un numero variabile di agenti, in una varietà di ambienti che vanno da layout di griglia 2D di forme e incroci di traffico simulati ad ambienti 3D complessi al chiuso.Dimostriamo i benefici della comunicazione mirata e a più stadi.Inoltre, dimostriamo che le strategie di comunicazione mirata apprese dagli agenti sono abbastanza interpretabili e intuitive.
È difficile per i principianti dell'arte del latte all'acquaforte fare modelli ben bilanciati usando due fluidi con viscosità diverse, come il latte schiumato e lo sciroppo, anche se fare l'arte del latte all'acquaforte mentre si guardano i video che mostrano la procedura, è difficile mantenere l'equilibrio. In questo articolo, proponiamo un sistema che aiuta i principianti a fare una latte art equilibrata proiettando una procedura di latte art direttamente su un cappuccino. I risultati dell'esperimento mostrano i progressi ottenuti utilizzando il nostro sistema.  Discutiamo anche la somiglianza tra l'arte del latte all'acquaforte e i modelli di design utilizzando la sottrazione dello sfondo.
Mentre l'addestramento avversario produce con successo modelli generativi per una varietà di aree, la relazione temporale nei dati generati è molto meno esplorata, il che è cruciale per i compiti di generazione sequenziale, ad esempio la super-risoluzione video e la traduzione video non accoppiata. Per i primi, i metodi allo stato dell'arte spesso favoriscono perdite di norme più semplici come L2 rispetto all'addestramento avversario. Tuttavia, la loro natura di mediazione porta facilmente a risultati temporalmente lisci con una mancanza indesiderabile di dettagli spaziali.Per la traduzione video non abbinata, gli approcci esistenti modificano le reti di generatori per formare consistenze di cicli spazio-temporali. Al contrario, ci concentriamo sul miglioramento degli obiettivi di apprendimento e proponiamo un algoritmo temporalmente auto-supervisionato.Per entrambi i compiti, dimostriamo che l'apprendimento temporale avversario è la chiave per ottenere soluzioni temporalmente coerenti senza sacrificare il dettaglio spaziale.Proponiamo anche una nuova perdita Ping-Pong per migliorare la coerenza temporale a lungo termine. Proponiamo anche una prima serie di metriche per valutare quantitativamente l'accuratezza e la qualità percettiva dell'evoluzione temporale, e una serie di studi sugli utenti conferma le classifiche calcolate con queste metriche.
La scarsità di dati di formazione etichettati spesso proibisce l'internazionalizzazione dei modelli NLP a più lingue.  La comprensione interlinguistica ha fatto progressi in quest'area utilizzando rappresentazioni universali della lingua.Tuttavia, la maggior parte degli approcci attuali si concentrano sul problema come uno di allineamento della lingua e non affrontano la deriva naturale del dominio attraverso le lingue e le culture.  In questo documento, affrontiamo il divario di dominio nell'impostazione della classificazione semisupervisionata di documenti multilingue, dove i dati etichettati sono disponibili in una lingua di origine e solo i dati non etichettati sono disponibili nella lingua di destinazione.  Combiniamo un metodo di apprendimento non supervisionato allo stato dell'arte, la modellazione linguistica mascherata di pre-formazione, con un metodo recente per l'apprendimento semi-supervisionato, Unsupervised Data Augmentation (UDA), per chiudere simultaneamente la lingua e il divario di dominio.  Dimostriamo che affrontare il divario di dominio nei compiti cross-linguistici è cruciale.  Miglioriamo rispetto alle forti basi e raggiungiamo un nuovo stato dell'arte per la classificazione di documenti multilingue.
Una distinta comunanza tra HMMs e RNNs è che entrambi imparano rappresentazioni nascoste per dati sequenziali.Inoltre, è stato notato che il calcolo a ritroso dell'algoritmo Baum-Welch per HMMs è un caso speciale dell'algoritmo di back propagation utilizzato per le reti neurali (Eisner (2016)).  Queste osservazioni suggeriscono che, nonostante le loro molte differenze apparenti, le HMM sono un caso speciale delle RNN?   In questo articolo, indaghiamo una serie di trasformazioni architetturali tra HMMs e RNNs, sia attraverso derivazioni teoriche che ibridazione empirica, per rispondere a questa domanda. In particolare, indaghiamo tre fattori chiave di progettazione - ipotesi di indipendenza tra gli stati nascosti e l'osservazione, il posizionamento di softmax, e l'uso della non linearità - al fine di appuntare i loro effetti empirici.  Presentiamo uno studio empirico completo per fornire approfondimenti sull'interazione tra espressività e interpretabilità rispetto alla modellazione del linguaggio e all'induzione delle parti del discorso.
Presentiamo un quadro teorico dell'informazione per comprendere i trade-off nell'apprendimento non supervisionato di modelli profondi a variabili latenti che utilizzano l'inferenza variazionale, sottolineando la necessità di considerare i modelli a variabili latenti lungo due dimensioni: la capacità di ricostruire gli input (distorsione) e il costo di comunicazione (tasso); deriviamo la frontiera ottimale dei modelli generativi nel piano bidimensionale tasso-distorsione, e mostriamo come l'obiettivo standard di evidence lower bound sia insufficiente per selezionare i punti lungo questa frontiera. Tuttavia, eseguendo un'ottimizzazione mirata per apprendere modelli generativi con tassi diversi, siamo in grado di apprendere molti modelli che possono raggiungere prestazioni generative simili ma fare compromessi molto diversi in termini di utilizzo della variabile latente. Attraverso esperimenti su MNIST e Omniglot con una varietà di architetture, mostriamo come la nostra struttura faccia luce su molte recenti estensioni proposte alla famiglia degli autoencoder variazionali.
Le reti neurali grafiche (GNN) sono una struttura efficace per l'apprendimento della rappresentazione dei grafi. Le GNN seguono uno schema di aggregazione del vicinato, dove il vettore di rappresentazione di un nodo viene calcolato aggregando e trasformando ricorsivamente i vettori di rappresentazione dei nodi vicini. I nostri risultati caratterizzano il potere discriminatorio delle varianti GNN più diffuse, come Graph Convolutional Networks e GraphSAGE, e mostrano che non possono imparare a distinguere alcune strutture grafiche semplici. Sviluppiamo quindi una semplice architettura che è provabilmente la più espressiva tra la classe delle GNN ed è potente quanto il test di isomorfismo dei grafi di Weisfeiler-Lehman.Convalidiamo empiricamente i nostri risultati teorici su una serie di benchmark di classificazione dei grafi e dimostriamo che il nostro modello raggiunge prestazioni all'avanguardia.
Introduciamo MTLAB, un nuovo algoritmo per l'apprendimento di più compiti correlati con forti garanzie teoriche. la sua idea chiave è quella di eseguire l'apprendimento in modo sequenziale sui dati di tutti i compiti, senza interruzioni o riavvii ai confini dei compiti. i predittori per i singoli compiti sono derivati da questo processo da un ulteriore passo di conversione online-to-batch. imparando attraverso i confini dei compiti, MTLAB raggiunge un rimpianto sublineare dei rischi reali nel numero dei compiti. Nell'impostazione dell'apprendimento permanente, questo porta a un limite di generalizzazione migliorato che converge con il numero totale di campioni in tutte le attività osservate, invece del numero di esempi per attività o il numero di attività in modo indipendente.Allo stesso tempo, è ampiamente applicabile: può gestire insiemi finiti di attività, come comune nell'apprendimento multi-task, così come sequenze di attività stocastiche, come studiato nell'apprendimento permanente.
Un lavoro recente ha mostrato le sorprendenti capacità interlinguistiche del BERT multilingue (M-BERT) - sorprendenti perché è addestrato senza alcun obiettivo interlinguistico e senza dati allineati. In questo lavoro, forniamo uno studio completo del contributo dei diversi componenti del M-BERT alla sua capacità interlinguistica, studiando l'impatto delle proprietà linguistiche delle lingue, dell'architettura del modello e degli obiettivi di apprendimento. Lo studio sperimentale Ã¨ fatto nel contesto di tre lingue tipologicamente differenti -- Spagnolo, Hindi e Russo -- ed usando due compiti di NLP concettualmente differenti, entailment testuale e riconoscimento dell'entitÃ nominata. Fra le nostre conclusioni chiave Ã¨ il fatto che la sovrapposizione lessicale fra le lingue gioca un ruolo trascurabile nel successo multilingue, mentre la profonditÃ della rete Ã¨ una parte importante di esso.
Analizziamo le dinamiche dell'addestramento delle reti ReLU profonde e le loro implicazioni sulla capacità di generalizzazione. Utilizzando un'impostazione insegnante-studente, abbiamo scoperto una nuova relazione tra il gradiente ricevuto dai nodi studenti nascosti e le attivazioni dei nodi insegnanti per le reti ReLU profonde. Con questa relazione e l'assunzione di piccole attivazioni sovrapposte dei nodi insegnanti, dimostriamo che (1) i nodi studenti i cui pesi sono inizializzati per essere vicini ai nodi insegnanti convergono verso di loro ad un ritmo più veloce, e (2) nei regimi iper-parametrizzati e nel caso dei 2 strati, mentre un piccolo gruppo di nodi fortunati converge verso i nodi insegnanti, i pesi a ventaglio degli altri nodi convergono a zero. Questo quadro fornisce una comprensione di molteplici fenomeni sconcertanti nell'apprendimento profondo come l'iperparametrizzazione, la regolarizzazione implicita, i biglietti della lotteria, ecc. Verifichiamo la nostra ipotesi mostrando che la maggior parte delle distorsioni BatchNorm dei modelli VGG11/16 pre-addestrati sono negative. Gli esperimenti su (1) reti di insegnanti profondi casuali con ingressi gaussiani, (2) rete di insegnanti pre-addestrata su CIFAR-10 e (3) ampi studi di ablazione convalidano le nostre molteplici previsioni teoriche.
Gli studi recenti hanno dimostrato che la necessità di una supervisione dei dati paralleli può essere alleviata con informazioni a livello di carattere. Mentre questi metodi hanno mostrato risultati incoraggianti, non sono alla pari con le loro controparti supervisionate e sono limitati a coppie di lingue che condividono un alfabeto comune. In questo lavoro, dimostriamo che possiamo costruire un dizionario bilingue tra due lingue senza utilizzare alcun corpora parallelo, allineando gli spazi di incorporazione di parole monolingue in modo non supervisionato. I nostri esperimenti dimostrano che il nostro metodo funziona molto bene anche per coppie di lingue lontane, come l'inglese-russo o l'inglese-cinese. Infine descriviamo esperimenti sulla coppia di lingue a bassa risorsa inglese-esperanto, sulla quale esiste solo una quantità limitata di dati paralleli, per mostrare il potenziale impatto del nostro metodo nella traduzione automatica completamente non supervisionata.
Le domande che richiedono il conteggio di una varietà di oggetti nelle immagini rimangono una sfida importante nella risposta visiva alle domande (VQA).Gli approcci più comuni alla VQA coinvolgono sia la classificazione delle risposte basate su rappresentazioni di lunghezza fissa sia dell'immagine che della domanda o la somma di conteggi frazionari stimati da ogni sezione dell'immagine.Al contrario, noi trattiamo il conteggio come un processo decisionale sequenziale e costringiamo il nostro modello a fare scelte discrete di cosa contare. In particolare, il modello seleziona sequenzialmente dagli oggetti rilevati e impara le interazioni tra gli oggetti che influenzano le selezioni successive.Una distinzione del nostro approccio è il suo output intuitivo e interpretabile, poiché i conteggi discreti sono automaticamente fondati nell'immagine.Inoltre, il nostro metodo supera lo stato dell'architettura dell'arte per VQA su più metriche che valutano il conteggio.
I grafi sono strutture di dati fondamentali necessari per modellare molti importanti dati del mondo reale, dai grafi di conoscenza, le interazioni fisiche e sociali alle molecole e alle proteine.In questo articolo, studiamo il problema dell'apprendimento di modelli generativi di grafi da un dataset di grafi di interesse.Dopo l'apprendimento, questi modelli possono essere utilizzati per generare campioni con proprietà simili a quelle del dataset.  Tali modelli possono essere utili in molte applicazioni, ad esempio la scoperta di farmaci e la costruzione di grafi di conoscenza. Il compito di apprendere modelli generativi di grafi, tuttavia, ha le sue sfide uniche, in particolare, come gestire le simmetrie nei grafi e l'ordinamento dei suoi elementi durante il processo di generazione sono questioni importanti.  Studiamo le sue prestazioni su alcuni compiti di generazione di grafi rispetto alle linee di base che sfruttano la conoscenza del dominio.  Discutiamo le questioni potenziali e i problemi aperti per tali modelli generativi in futuro.
Introduciamo una rete neurale che rappresenta le frasi componendo le loro parole secondo alberi binari indotti di parsing.Usiamo Tree-LSTM come la nostra funzione di composizione, applicata lungo una struttura ad albero trovata da un parser grafico completamente differenziabile del linguaggio naturale.Il nostro modello ottimizza simultaneamente sia la funzione di composizione che il parser, eliminando così la necessità di alberi di parsing forniti dall'esterno che sono normalmente richiesti per Tree-LSTM.Può quindi essere visto come un RNN basato su alberi che è senza supervisione rispetto agli alberi di parsing. Dimostriamo che raggiunge prestazioni migliori rispetto a varie architetture Tree-LSTM supervisionate su un compito di entailment testuale e su un compito di dizionario inverso. Infine, mostriamo come le prestazioni possano essere migliorate con un meccanismo di attenzione che sfrutta completamente il diagramma di parsing, assistendo su tutti i possibili sottocampi della frase.
Vengono proposte e testate sperimentalmente tre tecniche: la regolarizzazione basata sulla distanza, la potatura nested-rank e la corrispondenza bipartita layer-by-layer. I primi due algoritmi sono usati rispettivamente nelle fasi di addestramento e potatura, mentre il terzo è usato nella fase di organizzazione dei neuroni. Gli esperimenti mostrano che la regolarizzazione basata sulla distanza con la potatura basata sui pesi tende a dare il meglio, con o senza la corrispondenza bipartita layer-by-layer.Questi risultati suggeriscono che queste tecniche possono essere utili nella creazione di reti neurali per l'implementazione in circuiti specializzati ampiamente distribuiti.
Dato un video e una frase, l'obiettivo del video moment retrieval debolmente supervisionato è quello di individuare il segmento video che è descritto dalla frase senza avere accesso alle annotazioni temporali durante la formazione.  Invece, un modello deve imparare come identificare il segmento corretto (cioè il momento) quando gli vengono fornite solo coppie video-sentenza.  Per facilitare questo allineamento, proponiamo il nostro Weakly-supervised Moment Alignment Network (wMAN) che sfrutta un meccanismo di co-attenzione a più livelli per imparare rappresentazioni multimodali più ricche. Il suddetto meccanismo è composto da un modulo di interazione Frame-By-Word e da un nuovo Word-Conditioned Visual Graph (WCVG). Il nostro approccio incorpora anche una nuova applicazione di codifiche posizionali, comunemente usate in Transformers, per imparare rappresentazioni visive-semantiche che contengono informazioni contestuali delle loro posizioni relative nella sequenza temporale attraverso esperimenti iterativi message-passing.Comprehensive sui dataset DiDeMo e Charades-STA dimostrano l'efficacia delle nostre rappresentazioni apprese: il nostro modello combinato wMAN non solo supera il metodo weakly-supervised state-of-the-art da un margine significativo, ma fa anche meglio dei metodi strongly-supervised state-of-the-art su alcune metriche.
Nei compiti di apprendimento automatico, l'overtting si verifica frequentemente quando il numero di campioni del dominio di destinazione è insufficiente, perché la capacità di generalizzazione del classificatore è povera in questa circostanza.Per risolvere questo problema, il transfer learning utilizza la conoscenza di domini simili per migliorare la robustezza del discente. L'idea principale degli algoritmi di apprendimento di trasferimento esistenti è quella di ridurre la differenza tra i domini attraverso la selezione del campione o l'adattamento del dominio.Tuttavia, non importa quale algoritmo di apprendimento di trasferimento usiamo, la differenza esiste sempre e l'addestramento ibrido dei dati di origine e di destinazione porta a ridurre la capacità di ﬁssaggio del discente sul dominio di destinazione. Inoltre, quando la correlazione tra i domini è troppo bassa, è più probabile che si verifichi un trasferimento negativo. Per affrontare il problema, abbiamo proposto un'architettura di apprendimento di trasferimento a due fasi basata sull'apprendimento in ensemble, che utilizza gli algoritmi di apprendimento di trasferimento esistenti per addestrare i discenti deboli nella prima fase, e utilizza le previsioni dei dati di destinazione per addestrare il discente finale nella seconda fase. Con questa architettura, la capacità di ﬁtting e la capacità di generalizzazione possono essere garantite allo stesso tempo.Abbiamo valutato il metodo proposto su set di dati pubblici, che dimostra l'efficacia e la robustezza del nostro metodo proposto.
L'apprendimento profondo ha raggiunto risultati sorprendenti su molti compiti con grandi quantità di dati e la generalizzazione in prossimità dei dati di formazione.Per molte importanti applicazioni del mondo reale, questi requisiti sono irrealizzabili e la conoscenza preliminare aggiuntiva sul dominio del compito è necessaria per superare i problemi risultanti.In particolare, l'apprendimento di modelli fisici per il controllo basato sul modello richiede un'estrapolazione robusta da pochi campioni - spesso raccolti online in tempo reale - e gli errori del modello possono portare a danni drastici del sistema. Come primo esempio, proponiamo Deep Lagrangian Networks (DeLaN) come struttura di rete profonda a cui è stata imposta la meccanica lagrangiana, Il metodo proposto non solo ha superato i precedenti approcci di apprendimento del modello alla velocità di apprendimento, ma mostra un'estrapolazione sostanzialmente migliorata e più robusta a nuove traiettorie e apprende online in tempo reale.
Molti problemi di previsione impegnativi, dall'ottimizzazione molecolare alla sintesi di programma, coinvolgono la creazione di oggetti strutturati complessi come output.Tuttavia, i dati di formazione disponibili possono non essere sufficienti per un modello generativo per imparare tutte le trasformazioni complesse possibili.Facendo leva sull'idea che la valutazione è più facile della generazione, mostriamo come un semplice, ampiamente applicabile, schema iterativo di aumento dell'obiettivo può essere sorprendentemente efficace nel guidare la formazione e l'uso di tali modelli. Il nostro schema vede il modello generativo come una distribuzione prioritaria e impiega un filtro addestrato separatamente come la verosimiglianza. In ogni passo di incremento, filtriamo le uscite del modello per ottenere ulteriori obiettivi di previsione per la prossima epoca di addestramento. Il nostro metodo è applicabile nelle impostazioni supervisionate e semi-supervisionate. dimostriamo che il nostro approccio produce guadagni significativi rispetto alle linee di base forti sia nell'ottimizzazione molecolare che nella sintesi del programma.
La distribuzione di Boltzmann è un modello naturale per molti sistemi, dal cervello ai materiali e alle biomolecole, ma è spesso di utilità limitata per l'adattamento dei dati perché gli algoritmi Monte Carlo non sono in grado di simularla nel tempo disponibile. Questo divario tra le capacità espressive e la praticità di campionamento dei modelli basati sull'energia è esemplificato dal problema della piegatura delle proteine, poiché i paesaggi energetici sono alla base della conoscenza contemporanea della biofisica delle proteine ma le simulazioni al computer sono sfidate a piegare tutte le proteine tranne le più piccole dai primi principi. In questo lavoro miriamo a colmare il divario tra la capacità espressiva delle funzioni energetiche e le capacità pratiche dei loro simulatori utilizzando una simulazione Monte Carlo srotolata come modello per i dati. Componiamo una funzione energetica neurale con un nuovo ed efficiente simulatore basato sulla dinamica di Langevin per costruire un modello end-to-end-differenziabile della struttura atomica delle proteine date informazioni sulla sequenza degli aminoacidi. Introduciamo tecniche per stabilizzare la backpropagation sotto lunghi roll-out e dimostriamo la capacità del modello di fare previsioni multimodali e, in alcuni casi, di generalizzare ai tipi di piega proteica non osservati quando addestrato su un grande corpus di strutture proteiche.
I progressi nella comprensione di come i singoli animali imparano richiedono metodi standardizzati ad alta produttività per l'addestramento comportamentale e modi per adattare l'addestramento. Nel corso dell'addestramento con centinaia o migliaia di prove, un animale può cambiare bruscamente la sua strategia di base, e catturare questi cambiamenti richiede l'inferenza in tempo reale della strategia decisionale latente dell'animale. Per affrontare questa sfida, abbiamo sviluppato una piattaforma integrata per l'addestramento automatizzato degli animali e un modello iterativo di inferenza decisionale che è in grado di dedurre la politica decisionale momentanea e prevedere la scelta dell'animale su ogni prova con una precisione di ~80%, anche quando l'animale si comporta male.Abbiamo anche combinato le previsioni di decisione a risoluzione singola prova con la stima automatica della posa per valutare le traiettorie di movimento.
Le reti neurali ricorrenti (RNNs) sono un potente strumento per la modellazione di dati sequenziali, nonostante il loro uso diffuso, la comprensione di come RNNs risolvere problemi complessi rimane sfuggente.  Nonostante la loro capacità teorica di implementare calcoli complessi e ad alta densità, troviamo che le reti addestrate convergono verso rappresentazioni altamente interpretabili e a bassa densità.  Identifichiamo un meccanismo semplice, l'integrazione lungo un attrattore approssimativo, e troviamo questo meccanismo presente in tutte le architetture RNN (inclusi LSTMs, GRUs, e RNNs vaniglia).Nel complesso, questi risultati dimostrano che computazioni sorprendentemente universali e interpretabili dall'uomo possono sorgere in una gamma di reti ricorrenti.
Gli sviluppi paralleli nelle neuroscienze e nell'apprendimento profondo hanno portato a scambi reciprocamente produttivi, spingendo la nostra comprensione delle reti neurali reali e artificiali nei sistemi sensoriali e cognitivi.Tuttavia, questa interazione tra i campi è meno sviluppata nello studio del controllo motorio.In questo lavoro, sviluppiamo un roditore virtuale come piattaforma per lo studio fondato dell'attività motoria in modelli artificiali di controllo incarnato.Usiamo quindi questa piattaforma per studiare l'attività motoria attraverso i contesti addestrando un modello per risolvere quattro compiti complessi. Usando metodi familiari ai neuroscienziati, descriviamo le rappresentazioni comportamentali e gli algoritmi impiegati dai diversi strati della rete usando un approccio neuroetologico per caratterizzare l'attività motoria relativa al comportamento e agli obiettivi del roditore. Scopriamo che il modello usa due classi di rappresentazioni che codificano rispettivamente le strategie comportamentali specifiche del compito e la cinematica comportamentale invariante per il compito. Queste rappresentazioni si riflettono nell'attività sequenziale e nelle dinamiche di popolazione delle sottopopolazioni neurali.
Presentiamo Optimal Transport GAN (OT-GAN), una variante delle reti generative avversarie che minimizza una nuova metrica che misura la distanza tra la distribuzione del generatore e la distribuzione dei dati. Questa metrica, che chiamiamo distanza energetica mini-batch, combina il trasporto ottimale in forma primordiale con una distanza energetica definita in uno spazio delle caratteristiche imparate avversariamente, risultando in una funzione di distanza altamente discriminativa con gradienti mini-batch imparziali.
Costruiamo un agente virtuale per l'apprendimento della lingua in un mondo simile a un labirinto 2D. L'agente vede immagini dell'ambiente circostante, ascolta un insegnante virtuale e compie azioni per ricevere ricompense. Imparando simultaneamente le rappresentazioni visive del mondo, la lingua e il controllo delle azioni, l'agente, separando il fondamento della lingua da altre routine computazionali e condividendo una funzione di rilevamento dei concetti tra il fondamento della lingua e la predizione, interpola ed estrapola in modo affidabile per interpretare le frasi che contengono nuove combinazioni di parole o nuove parole mancanti nelle frasi di allenamento. Le nuove parole sono trasferite dalle risposte della predizione linguistica.Tale capacità linguistica è addestrata e valutata su una popolazione di oltre 1,6 milioni di frasi distinte che consistono di 119 parole oggetto, 8 parole colore, 9 parole di relazione spaziale, e 50 parole grammaticali.Il modello proposto supera significativamente cinque metodi di confronto per l'interpretazione di frasi a zero colpi.Inoltre, dimostriamo output intermedi interpretabili dall'uomo del modello in appendice.
Gli algoritmi di apprendimento per rinforzo, anche se hanno successo, tendono ad adattarsi eccessivamente agli ambienti di allenamento, ostacolando così la loro applicazione al mondo reale.Questo articolo propone $\testo{W}{text{R}^{2}{L}$ - un algoritmo robusto di apprendimento per rinforzo con significative prestazioni robuste su compiti di controllo a bassa e alta dimensione. Il nostro metodo formalizza l'apprendimento di rinforzo robusto come un nuovo gioco min-max con un vincolo di Wasserstein per un solutore corretto e convergente. Oltre alla formulazione, proponiamo anche un solutore efficiente e scalabile seguendo un nuovo metodo di ottimizzazione di ordine zero che crediamo possa essere utile all'ottimizzazione numerica in generale. Dimostriamo empiricamente guadagni significativi rispetto agli algoritmi standard e robusti dello stato dell'arte su ambienti MuJuCo altamente dimensionali.
I processi decisionali di Markov parzialmente osservabili (POMDPs) sono un modello naturale per gli scenari in cui si ha a che fare con la conoscenza incompleta e gli eventi casuali. Le applicazioni includono, ma non sono limitate a, la robotica e la pianificazione del movimento.Tuttavia, molte proprietà rilevanti di POMDPs sono indecidibili o molto costose da calcolare in termini di tempo di esecuzione e consumo di memoria. Nel nostro lavoro, sviluppiamo un metodo di astrazione basato sul gioco che è in grado di fornire limiti sicuri e approssimazioni strette per importanti sottoclassi di tali proprietà. Discutiamo le implicazioni teoriche e mostriamo l'applicabilità dei nostri risultati su un ampio spettro di benchmark.
In questo articolo ci avviciniamo a due argomenti rilevanti dell'apprendimento profondo: i) la gestione di dati di input strutturati a grafo e ii) una migliore comprensione e analisi delle reti profonde e dei relativi algoritmi di apprendimento.Con questo in mente ci concentriamo sulla classificazione topologica della raggiungibilità in un particolare sottoinsieme di grafi planari (Maze).Facendo questo, siamo in grado di modellare la topologia dei dati rimanendo nello spazio euclideo, permettendo così la sua elaborazione con architetture CNN standard.Suggeriamo un'architettura adatta per questo problema e dimostriamo che può esprimere una soluzione perfetta al compito di classificazione. La forma della funzione di costo intorno a questa soluzione è anche derivata e, notevolmente, non dipende dalla dimensione del labirinto nel limite del grande labirinto.Responsabile di questo comportamento sono eventi rari nel set di dati che regolano fortemente la forma della funzione di costo vicino a questo minimo globale.Identifichiamo ulteriormente un ostacolo all'apprendimento sotto forma di minimi locali poco performanti in cui la rete sceglie di ignorare alcuni degli ingressi.Sosteniamo ulteriormente le nostre affermazioni con esperimenti di allenamento e analisi numerica della funzione di costo su reti con fino a $128$ strati.
Mentre le reti neurali possono essere addestrate a mappare da uno specifico set di dati ad un altro, di solito non imparano una trasformazione generalizzata che possa estrapolare accuratamente al di fuori dello spazio di addestramento.Per esempio, una rete generativa avversaria (GAN) addestrata esclusivamente per trasformare le immagini di automobili da chiare a scure potrebbe non avere lo stesso effetto sulle immagini di cavalli. Questo perché le reti neurali sono brave a generare all'interno dell'insieme dei dati su cui sono addestrate, ma generare nuovi campioni al di fuori dell'insieme o estrapolare "fuori dal campione" è un problema molto più difficile che è stato studiato meno bene. Per affrontare questo problema, introduciamo una tecnica chiamata neuron editing che impara come i neuroni codificano una modifica per una particolare trasformazione in uno spazio latente. Usiamo un autoencoder per decomporre la variazione all'interno del dataset in attivazioni di diversi neuroni e generare dati trasformati definendo una trasformazione di editing su quei neuroni. Eseguendo la trasformazione in uno spazio latente addestrato, codifichiamo trasformazioni abbastanza complesse e non lineari ai dati con spostamenti di distribuzione molto più semplici alle attivazioni dei neuroni.Mostriamo la nostra tecnica sul dominio dell'immagine/trasferimento di stile e due applicazioni biologiche: rimozione di artefatti batch che rappresentano rumore indesiderato e modellazione dell'effetto dei trattamenti farmacologici per prevedere la sinergia tra farmaci.
Presentiamo una rappresentazione per descrivere modelli di transizione in domini complessi e incerti usando regole relazionali.  Per ogni azione, una regola seleziona un insieme di oggetti rilevanti e calcola una distribuzione sulle proprietà di quegli oggetti nello stato risultante, date le loro proprietà nello stato precedente.  Un algoritmo iterativo greedy è usato per costruire un insieme di riferimenti deittici che determinano quali oggetti sono rilevanti in ogni dato stato.   Le reti neurali feed-forward sono utilizzate per imparare la distribuzione di transizione sulle proprietà degli oggetti rilevanti.  Questa strategia è dimostrata essere sia più versatile che più efficiente rispetto all'apprendimento di un modello di transizione monolitico in un dominio simulato in cui un robot spinge pile di oggetti su un tavolo ingombro.
Molte grandi architetture di pianificazione che sono state proposte successivamente in letteratura sono ispirate a questo principio di progettazione in cui un'architettura di rete ricorsiva viene applicata per emulare le operazioni di backup di un algoritmo di iterazione del valore.Tuttavia i frame-work esistenti possono solo imparare e pianificare efficacemente su domini con una struttura a reticolo, cioè grafi regolari incorporati in un certo spazio euclideo. In questo articolo, proponiamo una rete di pianificazione generale, chiamata Graph-based Motion Planning Networks (GrMPN), che sarà in grado di toi) imparare e pianificare su grafi irregolari generali, quindiii) rendere le architetture di rete di pianificazione esistenti casi speciali.La struttura GrMPN proposta è invariante alla permutazione del grafo del compito, cioè l'ismofismo del grafico. Di conseguenza, GrMPN possiede la forza di generalizzazione e la capacità di efficienza dei dati.Dimostriamo la prestazione del metodo proposto GrMPN contro altre basi su tre domini che vanno dai labirinti 2D (grafico regolare), pianificazione del percorso sui grafi irregolari e pianificazione del movimento (un grafico irregolare delle configurazioni del robot).
Descriviamo tecniche per l'addestramento di modelli di denoising dell'immagine di alta qualità che richiedono solo singole istanze di immagini corrotte come dati di addestramento. Ispirati da una tecnica recente che rimuove la necessità di supervisione attraverso coppie di immagini impiegando reti con un "punto cieco" nel campo ricettivo, affrontiamo due dei suoi difetti: formazione inefficiente e scarse prestazioni finali di denoising. Questo si ottiene attraverso una nuova architettura di rete convoluzionale a punto cieco che permette un efficiente addestramento auto-supervisionato, così come l'applicazione della previsione di distribuzione bayesiana sui colori di uscita. Insieme, portano il modello auto-supervisionato alla pari con le tecniche di apprendimento profondo completamente supervisionate in termini sia di qualità che di velocità di addestramento nel caso di rumore gaussiano i.i.d.
Gli agenti di apprendimento per rinforzo (RL) migliorano attraverso il trial-and-error, ma quando la ricompensa è scarsa e l'agente non può scoprire sequenze di azioni di successo, l'apprendimento ristagna. Questo è stato un problema notevole nella formazione di agenti RL profondi per eseguire compiti basati sul web, come prenotare voli o rispondere alle e-mail, dove un singolo errore può rovinare l'intera sequenza di azioni. Un rimedio comune è quello di "scaldare l'agente" pre-addestrandolo a imitare le dimostrazioni degli esperti, ma questo è incline all'overfitting.Invece, proponiamo di vincolare l'esplorazione utilizzando le dimostrazioni.Da ogni dimostrazione, induciamo "flussi di lavoro" di alto livello che vincolano le azioni consentite ad ogni passo temporale per essere simili a quelle nella dimostrazione (ad es, "La nostra politica di esplorazione impara quindi a identificare i flussi di lavoro di successo e a campionare le azioni che soddisfano questi flussi di lavoro. I flussi di lavoro eliminano le cattive direzioni di esplorazione e accelerano la capacità dell'agente di scoprire ricompense. Usiamo il nostro approccio per addestrare una nuova politica neurale progettata per gestire la natura semi-strutturata dei siti web, e valutiamo su una serie di compiti web, tra cui il recente World of Bits benchmark, ottenendo nuovi risultati allo stato dell'arte e dimostrando che l'esplorazione guidata dal flusso di lavoro migliora l'efficienza del campione rispetto alla clonazione comportamentale di oltre 100 volte.
Il problema principale è che questo tipo di apprendimento e di conseguenza le reti neurali, che possono essere definite profonde, sono ad alta intensità di risorse e hanno bisogno di hardware specializzato per eseguire un calcolo in un tempo ragionevole. Purtroppo, non è sufficiente per rendere l'apprendimento profondo "utilizzabile" nella vita reale.Molti compiti sono obbligatori per essere il più possibile in tempo reale.Quindi è necessario ottimizzare molti componenti come il codice, gli algoritmi, la precisione numerica e l'hardware, per renderli "efficienti e utilizzabili".Tutte queste ottimizzazioni possono aiutarci a produrre modelli di apprendimento incredibilmente accurati e veloci.
Il Word embedding è un approccio utile per catturare le strutture di co-occorrenza in un grande corpus di testo.Oltre ai dati del testo stesso, spesso abbiamo covariate aggiuntive associate ai singoli documenti nel corpus---e.g. il profilo demografico dell'autore, il tempo e il luogo di pubblicazione, etc.--- In questo articolo, proponiamo un nuovo modello di decomposizione tensoriale per le incorporazioni di parole con covariate; il nostro modello apprende congiuntamente un'incorporazione di base per tutte le parole e una trasformazione diagonale pesata per modellare come ogni covariata modifica l'incorporazione di base. Per ottenere l'incorporazione specifica per un particolare autore o luogo, per esempio, possiamo semplicemente moltiplicare l'incorporazione di base per la matrice di trasformazione associata a quel tempo o luogo.I principali vantaggi del nostro approccio sono l'efficienza dei dati e l'interpretabilità della matrice di trasformazione delle covariate. I nostri esperimenti dimostrano che il nostro modello congiunto apprende incorporazioni sostanzialmente migliori condizionate da ogni covariata rispetto all'approccio standard di apprendimento di un'incorporamento separato per ogni covariata utilizzando solo il sottoinsieme pertinente di dati.Inoltre, il nostro modello incoraggia le incorporazioni ad essere ``topic-aligned'' nel senso che le dimensioni hanno significati specifici indipendenti. Questo permette alle nostre embeddings specifiche delle covariate di essere confrontate per argomento, permettendo l'analisi differenziale a valle. Valutiamo empiricamente i benefici del nostro algoritmo su diversi set di dati, e dimostriamo come può essere utilizzato per affrontare molte domande naturali sugli effetti delle covariate.
L'apprendimento profondo ha ricevuto un'attenzione significativa grazie alle sue impressionanti prestazioni in molti compiti di apprendimento allo stato dell'arte.Purtroppo, anche se molto potente, l'apprendimento profondo non è ben compreso a livello teorico e in particolare solo recentemente sono stati ottenuti risultati per la complessità dell'addestramento delle reti neurali profonde.In questo lavoro dimostriamo che grandi classi di reti neurali profonde con varie architetture (ad esempio, DNN, CNN, reti neurali binarie e ResNets), funzioni di attivazione (ad esempio, ReLUs e leaky ReLUs), e funzioni di perdita (ad esempio, Hinge loss, Euclidean loss, ecc.) possono essere addestrate in modo quasi ottimale con l'accuratezza desiderata usando la programmazione lineare in un tempo che è esponenziale nella dimensione dei dati di input e dello spazio dei parametri e polinomiale nella dimensione del set di dati; i miglioramenti della dipendenza nella dimensione di input sono noti per essere improbabili assumendo $P\neq NP$, e il miglioramento della dipendenza dalla dimensione dello spazio dei parametri rimane aperto. In particolare, otteniamo algoritmi in tempo polinomiale per l'addestramento per una data architettura di rete fissa.Il nostro lavoro si applica più ampiamente ai problemi di minimizzazione del rischio empirico che ci permette di generalizzare vari risultati precedenti e di ottenere nuovi risultati di complessità per architetture precedentemente non studiate nel setting di apprendimento appropriato.
Il filtro di Kalman esteso (EKF) è un classico algoritmo di elaborazione dei segnali che esegue un'inferenza bayesiana approssimativa efficiente in modelli non coniugati linearizzando la funzione di misura locale, evitando la necessità di calcolare integrali intrattabili quando si calcola il posteriore. In alcuni casi l'EKF supera i metodi che si basano sulla cubatura per risolvere tali integrali, specialmente nei problemi del mondo reale critici per il tempo. Formuliamo la potenza EP come un filtro Kalman non lineare, prima di mostrare che la linearizzazione risulta in un algoritmo iterato globalmente che corrisponde esattamente all'EKF al primo passaggio attraverso i dati, e migliora iterativamente la linearizzazione nei passaggi successivi.Un ulteriore vantaggio è la capacità di calcolare il limite quando la potenza EP tende a zero, che rimuove l'instabilità dell'algoritmo EP-like.Lo schema di inferenza risultante risolve modelli non coniugati di processo gaussiano temporale in tempo lineare, $mathcal{O}(n)$, e in forma chiusa.
Questo articolo esplora la semplicità delle reti neurali apprese in varie impostazioni: apprese su dati reali e casuali, variando la dimensione/architettura e usando grandi dimensioni di minibatch e piccole dimensioni di minibatch, Quanto accuratamente può essere appresa la funzione di predizione di una rete neurale da campioni etichettati da essa.Mentre l'apprendibilità è diversa da (in effetti spesso superiore a) l'accuratezza del test, i risultati qui suggeriscono che c'è una forte correlazione tra piccoli errori di generalizzazione e alta apprendibilità.Questo lavoro mostra anche che esistono differenze qualitative significative nelle reti poco profonde rispetto alle reti profonde popolari. Più in generale, questo lavoro estende in una nuova direzione, il lavoro precedente sulla comprensione delle proprietà delle reti neurali apprese.La nostra speranza è che un tale studio empirico di comprensione delle reti neurali apprese possa far luce sulle giuste ipotesi che possono essere fatte per uno studio teorico dell'apprendimento profondo.
    Con la proliferazione dei modelli per i compiti di elaborazione del linguaggio naturale (NLP), è ancora più difficile capire le differenze tra i modelli e i loro meriti relativi.Semplicemente guardando le differenze tra le metriche olistiche come l'accuratezza, BLEU, o F1 non ci dicono perché o come un particolare metodo è migliore e come i bias del dataset influenzano le scelte del design del modello.    In questo articolo, presentiamo una metodologia generale per una valutazione interpretabile dei sistemi NLP e scegliamo il compito di riconoscimento delle entità nominate (NER) come caso di studio, che è un compito fondamentale per identificare persone, luoghi o organizzazioni nel testo. Il metodo di valutazione proposto ci permette di interpretare i bias dei modelli, i bias dei dataset e come le differenze nei dataset influenzano il design dei modelli, identificando i punti di forza e di debolezza degli approcci attuali. Rendendo disponibile il nostro strumento di analisi, facilitiamo i futuri ricercatori nell'esecuzione di analisi simili e favoriamo il progresso in questo settore.
Il compito del dialogo visuale comporta l'apprendimento di un dialogo cooperativo orientato agli obiettivi tra agenti autonomi che si scambiano informazioni su una scena attraverso diversi round di domande e risposte. Noi sosteniamo che richiedere agli agenti di aderire alle regole del linguaggio umano massimizzando al contempo lo scambio di informazioni è un problema mal posto, e osserviamo che gli umani non si allontanano da un linguaggio comune, perché sono creature sociali e devono comunicare con molte persone ogni giorno, ed è molto più facile attenersi a un linguaggio comune anche a costo di una certa perdita di efficienza. Usando questo come ispirazione, proponiamo e valutiamo una struttura di dialogo multi-agente in cui ogni agente interagisce con, e impara da, più agenti, e dimostriamo che questo risulta in un dialogo più rilevante e coerente (come giudicato da valutatori umani) senza sacrificare le prestazioni del compito (come giudicato da metriche quantitative).
Il collasso posteriore negli autocodificatori variazionali (VAE) si verifica quando la distribuzione variazionale corrisponde strettamente al priore non informativo per un sottoinsieme di variabili latenti. Questo articolo presenta una spiegazione semplice e intuitiva del collasso posteriore attraverso l'analisi di VAE lineari e la loro diretta corrispondenza con la PCA probabilistica (pPCA). Mostriamo che l'addestramento di un VAE lineare con inferenza variazionale recupera un massimo globale identificabile in modo univoco corrispondente alle direzioni delle componenti principali. Forniamo prove empiriche che la presenza di massimi locali causa un collasso a posteriori in VAE profonde non lineari. I nostri risultati aiutano a spiegare una vasta gamma di approcci euristici nella letteratura che tentano di diminuire l'effetto del termine KL nell'ELBO per ridurre il collasso a posteriori.
I trasformatori hanno raggiunto risultati allo stato dell'arte su una varietà di compiti di elaborazione del linguaggio naturale. Nonostante le buone prestazioni, i trasformatori sono ancora deboli nella modellazione di frasi lunghe, dove la mappa di attenzione globale è troppo dispersa per catturare informazioni di valore; in questo caso, le caratteristiche locali/token che sono anche significative per la modellazione della sequenza sono omesse in una certa misura. Per affrontare questo problema, proponiamo un modello di attenzione multiscala (MUSE) concatenando le reti di attenzione con reti convoluzionali e reti feed-forward posizionali per catturare esplicitamente le caratteristiche locali e token.Considerando la dimensione del parametro e l'efficienza di calcolo, riutilizziamo lo strato feed-forward nel trasformatore originale e adottiamo una convoluzione dinamica leggera come implementazione. I risultati sperimentali mostrano che il modello proposto raggiunge miglioramenti sostanziali delle prestazioni rispetto a Transformer, soprattutto su frasi lunghe, e spinge lo stato dell'arte da 35,6 a 36,2 su IWSLT 2014 compito di traduzione dal tedesco all'inglese, da 30,6 a 31,3 su IWSLT 2015 compito di traduzione dall'inglese al vietnamita.Raggiungiamo anche lo stato dell'arte delle prestazioni su WMT 2014 inglese al dataset di traduzione francese, con un punteggio BLEU di 43,2.
L'addestramento delle reti neurali con garanzie di robustezza verificabili è impegnativo.Diversi approcci esistenti utilizzano il rilassamento lineare basato su limiti di uscita della rete neurale sotto perturbazione, ma possono rallentare la formazione di un fattore di centinaia a seconda delle architetture di rete sottostanti.Nel frattempo, la formazione basata sulla propagazione di intervallo (IBP) è efficiente e supera significativamente i metodi basati sul rilassamento lineare su molti compiti, ma può soffrire di problemi di stabilità poiché i limiti sono molto più deboli soprattutto all'inizio della formazione. In questo articolo, proponiamo un nuovo metodo di addestramento adversariale certificato, CROWN-IBP, combinando i limiti veloci IBP in un passaggio di delimitazione in avanti e un limite stretto basato sul rilassamento lineare, CROWN, in un passaggio di delimitazione a ritroso.CROWN-IBP è computazionalmente efficiente e supera costantemente le basi IBP nell'addestramento di reti neurali verificabilmente robuste. Conduciamo esperimenti su larga scala sui set di dati MNIST e CIFAR, e superiamo tutte le precedenti difese certificate basate sul rilassamento lineare e sulla propagazione bound nella robustezza L_inf. In particolare, raggiungiamo il 7,02% di errore di prova verificato su MNIST con epsilon=0,3, e il 66,94% su CIFAR-10 con epsilon=8/255.
La potatura della rete è ampiamente utilizzata per ridurre il pesante costo di inferenza dei modelli profondi in ambienti con poche risorse.Un tipico algoritmo di potatura è una pipeline a tre fasi, cioè, Durante la potatura, secondo un certo criterio, i pesi ridondanti vengono eliminati e i pesi importanti vengono mantenuti per preservare al meglio l'accuratezza.In questo lavoro, facciamo diverse osservazioni sorprendenti che contraddicono le credenze comuni. Per tutti gli algoritmi di pruning strutturati allo stato dell'arte che abbiamo esaminato, il fine-tuning di un modello potato dà solo prestazioni paragonabili o peggiori rispetto all'addestramento di quel modello con pesi inizializzati a caso.Per gli algoritmi di pruning che assumono un'architettura di rete predefinita, ci si può liberare dell'intera pipeline e addestrare direttamente la rete target da zero.Le nostre osservazioni sono coerenti per più architetture di rete, dataset e compiti, che implicano che: 1) l'addestramento di un grande modello iper-parametrizzato spesso non è necessario per ottenere un modello finale efficiente, 2) i pesi ``importanti'' appresi del grande modello non sono tipicamente utili per il piccolo modello potato, 3) l'architettura potata stessa, piuttosto che un insieme di pesi ``importanti'' ereditati, è più cruciale per l'efficienza nel modello finale, il che suggerisce che in alcuni casi la potatura può essere utile come paradigma di ricerca dell'architettura.I nostri risultati suggeriscono la necessità di valutazioni di base più accurate nella ricerca futura sui metodi strutturati di potatura.  Confrontiamo anche con la "Lottery Ticket Hypothesis" (Frankle & Carbin 2019), e troviamo che con un tasso di apprendimento ottimale, l'inizializzazione "biglietto vincente" come usato in Frankle & Carbin (2019) non porta miglioramenti rispetto all'inizializzazione casuale.
Le tecniche di spazzolatura hanno una lunga storia con i primi strumenti di selezione interattiva che appaiono negli anni '90.Da allora, molte tecniche aggiuntive sono state sviluppate per affrontare la precisione della selezione, la scalabilità e i problemi di flessibilità.La selezione è particolarmente difficile in grandi set di dati in cui molti elementi visivi si aggrovigliano e creano sovrapposizioni.Questo articolo indaga una nuova tecnica di spazzolatura che non si basa solo sulla posizione effettiva di spazzolatura ma anche sulla forma dell'area spazzolata. In primo luogo, l'utente spazzola la regione in cui le traiettorie di interesse sono visibili.In secondo luogo, la forma dell'area spazzolata viene utilizzata per selezionare elementi simili.In terzo luogo, l'utente può regolare il grado di somiglianza per filtrare le traiettorie richieste.Questa tecnica comprende due tipi di metriche di confronto, la correlazione Pearson a pezzi e la misura della somiglianza basata sulla geometria delle informazioni.La applichiamo a scenari concreti con set di dati dal controllo del traffico aereo, dati di eye-tracking e traiettorie GPS.
Generative Adversarial Networks (GANs) possono produrre immagini di sorprendente complessità e realismo, ma sono generalmente strutturate per campionare da una singola fonte latente ignorando l'interazione spaziale esplicita tra più entità che potrebbero essere presenti in una scena.Capturing tali interazioni complesse tra diversi oggetti nel mondo, compreso il loro relativo scaling, layout spaziale, occlusione, o trasformazione del punto di vista è un problema impegnativo.In questo lavoro, proponiamo di modellare la composizione degli oggetti in un quadro GAN come una rete autoconsistente composizione-decomposizione. Il nostro modello è condizionato sulle immagini dell'oggetto dalle loro distribuzioni marginali e può generare un'immagine realistica dalla loro distribuzione congiunta.Valutiamo il nostro modello attraverso esperimenti qualitativi e valutazioni dell'utente in scenari in cui esempi accoppiati o non accoppiati per le singole immagini dell'oggetto e le scene congiunte sono dati durante la formazione.I nostri risultati rivelano che il modello appreso cattura le potenziali interazioni tra i due domini di oggetti dati come input per produrre nuove istanze di scena composta al momento del test in modo ragionevole.
Recenti studi hanno evidenziato che gli esempi avversari sono una minaccia onnipresente per diversi modelli di reti neurali e per molte applicazioni a valle; tuttavia, poiché le proprietà uniche dei dati hanno ispirato principi di apprendimento distinti e potenti, questo articolo mira a esplorare le loro potenzialità per mitigare gli input avversari. Testato sul riconoscimento automatico del discorso (ASR) compiti e tre recenti attacchi audio adversarial, troviamo che (i) la trasformazione dell'input sviluppata dalla difesa adversarial di immagine fornisce il miglioramento limitato di robustezza ed è sottile agli attacchi avanzati; (ii) la dipendenza temporale può essere sfruttata per guadagnare il potere discriminativo contro gli esempi adversarial audio ed è resistente agli attacchi adattivi considerati nei nostri risultati experiments.Our non solo mostrano i mezzi promettenti di miglioramento della robustezza dei sistemi di ASR, ma inoltre offrono le comprensioni novelle nello sfruttamento delle proprietà di dati domain-specific per attenuare gli effetti negativi degli esempi adversarial.
Al fine di alleviare il famoso fenomeno del collasso della modalità nelle reti generative avversarie (GAN), proponiamo un nuovo metodo di formazione delle GAN in cui alcuni campioni falsi possono essere riconsiderati come reali durante il processo di formazione. Questa strategia può ridurre il valore del gradiente che il generatore riceve nella regione in cui il gradiente esplode. E questo si traduce in una distribuzione generata sbilanciata che si discosta da quella di destinazione, quando i falsi datapoints si sovrappongono a quelli reali, il che spiega la non stabilità dei GAN. Dimostriamo anche che, penalizzando la differenza tra le uscite del discriminatore e considerando certi falsi datapoints come reali per coppie di campioni reali e falsi adiacenti, l'esplosione del gradiente può essere alleviata.Di conseguenza, viene proposto un metodo di formazione GAN modificato con un processo di formazione più stabile e una migliore generalizzazione.
Presentiamo uno strumento di esplorazione visiva interattiva dello spazio latente (IVELS) per la selezione del modello.  La valutazione dei modelli generativi di sequenze discrete da uno spazio latente continuo è un problema impegnativo, poiché la loro ottimizzazione coinvolge più termini oggettivi in competizione.  Introduciamo una pipeline di selezione del modello per confrontare e filtrare i modelli attraverso fasi consecutive di metriche più complesse e costose, presentando la pipeline in uno strumento visivo interattivo per consentire l'esplorazione delle metriche, l'analisi dello spazio latente appreso e la selezione del miglior modello per un dato compito.  Ci concentriamo specificamente sulla famiglia di auto-encoder variazionali in un caso di studio di modellazione di sequenze di peptidi, che sono brevi sequenze di aminoacidi. Questo compito è particolarmente interessante a causa della presenza di più attributi che vogliamo modellare. Dimostriamo come un confronto visivo interattivo può aiutare a valutare quanto bene un auto-encoder non supervisionato catturi significativamente gli attributi di interesse nel suo spazio latente.
Le reti neurali addestrate attraverso la discesa stocastica del gradiente (SGD) esistono da più di 30 anni, ma sfuggono ancora alla nostra comprensione.Questo articolo ha un approccio sperimentale, con una strategia divide et impera in mente: iniziamo a studiare cosa succede nei singoli neuroni.Pur essendo il blocco di costruzione centrale delle reti neurali profonde, il modo in cui codificano le informazioni sugli input e come tali codifiche emergono è ancora sconosciuto.Riportiamo esperimenti che forniscono forti prove che i neuroni nascosti si comportano come classificatori binari durante la formazione e il test. Durante l'addestramento, l'analisi dei gradienti rivela che un neurone separa due categorie di input, che sono impressionantemente costanti durante l'addestramento. Durante i test, mostriamo che la partizione binaria fuzzy descritta sopra incorpora le informazioni fondamentali utilizzate dalla rete per la sua previsione. Queste osservazioni portano alla luce alcune delle meccaniche interne fondamentali delle reti neurali profonde, e hanno il potenziale per guidare i prossimi sviluppi teorici e pratici.
La classificazione automatica degli oggetti è uno dei compiti più importanti nelle applicazioni di ingegneria e data mining.Anche se l'uso di classificatori più complessi e avanzati può aiutare a migliorare la precisione dei sistemi di classificazione, può essere fatto analizzando i set di dati e le loro caratteristiche per un particolare problema.Featurecombination è quello che può migliorare la qualità delle caratteristiche.In questo documento, una struttura simile a Feed-Forward Neural Network (FFNN) viene utilizzata per generare una combinazione ottimizzata lineare o non lineare di caratteristiche per classification.GeneticAlgorithm (GA) viene applicato per aggiornare i pesi e le polarizzazioni. Poiché la natura dei set di dati e le loro caratteristiche hanno un impatto sull'efficacia della combinazione e del sistema di classificazione, le funzioni di attivazione lineari e non lineari (o la funzione di trasferimento) sono utilizzate per ottenere un sistema più affidabile. Gli esperimenti di diversi set di dati UCI e l'utilizzo del classificatore a distanza minima come classificatore semplice indicano che la combinazione di caratteristiche proposta basata su FFNN lineare e non lineare intelligente può presentare risultati più affidabili e promettenti.
I recenti miglioramenti delle reti generative adversariali (GAN) hanno reso possibile la generazione di immagini realistiche ad alta risoluzione basate su descrizioni in linguaggio naturale come le didascalie delle immagini.Inoltre, le GAN condizionali ci permettono di controllare il processo di generazione delle immagini attraverso etichette o anche descrizioni in linguaggio naturale.Tuttavia, il controllo a grana fine del layout dell'immagine, vale a dire Questo è particolarmente vero per le immagini che dovrebbero contenere più oggetti distinti in posizioni spaziali diverse. Introduciamo un nuovo approccio che ci permette di controllare la posizione di un numero arbitrario di oggetti all'interno di un'immagine aggiungendo un percorso di oggetti sia al generatore che al discriminatore. Il nostro approccio non ha bisogno di un layout semantico dettagliato, ma solo di scatole di delimitazione e delle rispettive etichette degli oggetti desiderati.Il percorso dell'oggetto si concentra solo sui singoli oggetti e viene applicato iterativamente nelle posizioni specificate dalle scatole di delimitazione.Il percorso globale si concentra sullo sfondo dell'immagine e sul layout generale dell'immagine.Eseguiamo esperimenti sul Multi-MNIST, CLEVR, e sul più complesso set di dati MS-COCOCO. I nostri esperimenti mostrano che attraverso l'uso del percorso dell'oggetto possiamo controllare le posizioni degli oggetti all'interno delle immagini e possiamo modellare scene complesse con più oggetti in varie posizioni.Mostriamo inoltre che il percorso dell'oggetto si concentra sui singoli oggetti e impara le caratteristiche rilevanti per questi, mentre il percorso globale si concentra sulle caratteristiche globali dell'immagine e sullo sfondo dell'immagine.
Per esempio, i centri di assistenza clienti o gli ospedali vorrebbero riassumere l'interazione con il servizio clienti e l'interazione medico-paziente. Tuttavia, pochi ricercatori hanno esplorato la sintesi astrattiva dei dialoghi a causa della mancanza di set di dati adatti. Noi proponiamo un set di dati per la sintesi astrattiva dei dialoghi basato su MultiWOZ. Per affrontare questi due inconvenienti, proponiamo Scaffold Pointer Network (SPNet) per utilizzare l'annotazione esistente sul ruolo dell'oratore, lo slot semantico e il dominio del dialogo.SPNet incorpora queste impalcature semantiche per il riassunto dei dialoghi.Poiché ROUGE non può catturare i due inconvenienti menzionati, proponiamo anche una nuova metrica di valutazione che considera le entità informative critiche nel testo.Su MultiWOZ, la nostra proposta SPNet supera i metodi di riassunto astratto allo stato dell'arte su tutte le metriche di valutazione automatica e umana.
Le basi di conoscenza (KB), sia costruite automaticamente che manualmente, sono spesso incomplete --- molti fatti validi possono essere dedotti dalla KB sintetizzando le informazioni esistenti.Un approccio popolare al completamento della KB è quello di dedurre nuove relazioni attraverso il ragionamento combinatorio sulle informazioni trovate lungo altri percorsi che collegano una coppia di entità. Date le enormi dimensioni delle KB e il numero esponenziale di percorsi, i precedenti modelli basati sui percorsi hanno considerato solo il problema di prevedere una relazione mancante data da due entità, o di valutare la verità di una tripla proposta.Inoltre, questi metodi hanno tradizionalmente usato percorsi casuali tra coppie di entità fisse o più recentemente hanno imparato a scegliere i percorsi tra loro. Poiché le passeggiate casuali sono impraticabili in un ambiente con destinazione sconosciuta e molti percorsi combinatori da un nodo iniziale, presentiamo un approccio di apprendimento di rinforzo neurale che impara come navigare il grafico condizionato dalla query di input per trovare percorsi predittivi.su una valutazione completa su sette serie di dati di base di conoscenza, abbiamo trovato MINERVA per essere competitivo con molti metodi attuali dello stato dell'arte.
Ci sono molte differenze tra le reti convoluzionali e i flussi visivi ventrali dei primati. Per esempio, le reti convoluzionali standard mancano di connessioni ricorrenti e laterali, dinamiche cellulari, ecc. Uno studio recente ha scoperto che l'architettura feedforward della corteccia visiva potrebbe essere approssimata da vicino come una rete convoluzionale, ma l'architettura risultante differiva dalle reti profonde ampiamente utilizzate in diversi modi. Lo stesso studio ha anche scoperto, un po' sorprendentemente, che l'addestramento del flusso ventrale di questa rete per il riconoscimento degli oggetti ha portato a scarse prestazioni. In particolare, ho apportato una serie di modifiche all'architettura basata sul flusso ventrale, per renderla più simile a una DenseNet, e ho testato le prestazioni ad ogni passo. Ho scelto DenseNet perché ha un alto BrainScore, e perché ha alcune caratteristiche architettoniche simili alla corteccia, come grandi in-degrees e lunghe connessioni skip. Una possibilità è che i dettagli dell'architettura del ventral-stream possano essere poco adatti al calcolo feedforward, alle unità di elaborazione semplici e/o alla backpropagation, il che potrebbe suggerire differenze tra il modo in cui le reti profonde ad alte prestazioni e l'approccio del cervello al riconoscimento degli oggetti.
Nell'apprendimento per rinforzo, è comune lasciare che un agente interagisca con il suo ambiente per una quantità fissa di tempo prima di resettare l'ambiente e ripetere il processo in una serie di episodi. Il compito che l'agente deve imparare può essere quello di massimizzare le sue prestazioni su (i) quella quantità fissa di tempo, o (ii) un periodo indefinito dove il limite di tempo è usato solo durante l'addestramento. In questo articolo, indaghiamo teoricamente come i limiti di tempo potrebbero essere efficacemente gestiti in ciascuno dei due casi: nel primo, sosteniamo che le terminazioni dovute ai limiti di tempo sono in effetti parte dell'ambiente, e proponiamo di includere una nozione di tempo rimanente come parte dell'input dell'agente. Nel secondo caso, i limiti di tempo non fanno parte dell'ambiente e sono usati solo per facilitare l'apprendimento. Sosteniamo che tali terminazioni non dovrebbero essere trattate come ambientali e proponiamo un metodo, specifico per gli algoritmi basati sul valore, che incorpora questa intuizione continuando a fare il bootstrap alla fine di ogni episodio parziale.Per illustrare l'importanza delle nostre proposte, eseguiamo diversi esperimenti su una gamma di ambienti da semplici grafici di transizione a pochi stati a complessi compiti di controllo, compresi domini di riferimento nuovi e standard.I nostri risultati mostrano che i metodi proposti migliorano le prestazioni e la stabilità degli algoritmi di apprendimento per rinforzo esistenti.
Sebbene la discesa del gradiente stocastico (SGD) sia una forza trainante dietro il recente successo dell'apprendimento profondo, la nostra comprensione delle sue dinamiche in uno spazio di parametri ad alta densità è limitata. Negli ultimi anni, alcuni ricercatori hanno usato la stocasticità dei gradienti minibatch, o il rapporto segnale-rumore, per caratterizzare meglio le dinamiche di apprendimento di SGD. Proponiamo un modello della concentrazione direzionale per i gradienti minibatch attraverso la distribuzione von Mises-Fisher (VMF), e mostriamo che l'uniformità direzionale dei gradienti minibatch aumenta nel corso di SGD.Verifichiamo empiricamente il nostro risultato usando reti convoluzionali profonde e osserviamo una maggiore correlazione tra la stocasticità del gradiente e l'uniformità direzionale proposta che quella contro la stocasticità della norma del gradiente, suggerendo che la statistica direzionale dei gradienti minibatch è un fattore importante dietro SGD.
Per massimizzare la stabilità, analizziamo e sviluppiamo un'implementazione computazionalmente efficiente della regolarizzazione jacobiana che aumenta i margini di classificazione delle reti neurali. L'effetto stabilizzante della regolarizzazione jacobiana porta a miglioramenti significativi nella robustezza, misurata sia contro le perturbazioni casuali che contro quelle avversarie dell'input, senza degradare gravemente le proprietà di generalizzazione sui dati puliti.
Con la crescente domanda di distribuire reti neurali convoluzionali (CNN) su piattaforme mobili, l'approccio sparse kernel è stato proposto, che potrebbe risparmiare più parametri rispetto alla convoluzione standard, pur mantenendo l'accuratezza.Tuttavia, nonostante il grande potenziale, nessuna ricerca precedente ha sottolineato come imbarcazioni un design sparse kernel con tale potenziale (cioè, Nel frattempo, a causa del grande spazio di progettazione, è anche impossibile provare tutte le combinazioni di kernel sparsi esistenti. In questo articolo, siamo i primi nel campo a considerare come creare un design efficace di kernel sparsi eliminando il grande spazio di progettazione. In secondo luogo, per rimuovere i progetti con una grande degradazione della precisione, troviamo una proprietà unificata denominata ~emph{information field} dietro vari progetti di kernel sparsi, che potrebbe indicare direttamente la precisione finale.Infine, rimuoviamo i progetti in due casi in cui si potrebbe ottenere una migliore efficienza dei parametri.Inoltre, forniamo un'analisi dettagliata dell'efficienza sui 4 progetti finali nel nostro schema.I risultati sperimentali convalidano l'idea del nostro schema mostrando che il nostro schema è in grado di trovare progetti che sono più efficienti nell'uso dei parametri e del calcolo con una precisione simile o superiore.
Nella localizzazione temporale debolmente sorvegliata dell'azione, i lavori precedenti non sono riusciti a localizzare le regioni dense e integrali per ogni azione intera a causa della sovrastima delle regioni più salienti.Per alleviare questo problema, proponiamo una rete attenzionale media marginalizzata (MAAN) per sopprimere la risposta dominante delle regioni più salienti in un principio.Il MAAN impiega un nuovo modulo di aggregazione media marginalizzata (MAA) e apprende un insieme di probabilità discriminative latenti in un modo end-to-end. MAA campiona sottoinsiemi multipli dalle caratteristiche dei frammenti di video secondo un insieme di probabilità discriminative latenti e prende l'aspettativa su tutte le caratteristiche medie dei sottoinsiemi. Teoricamente, dimostriamo che il modulo MAA con le probabilità discriminative latenti apprese riduce con successo la differenza nelle risposte tra le regioni più salienti e le altre. Pertanto, MAAN è in grado di generare migliori sequenze di attivazione di classe e di identificare le regioni di azione dense e integre nei video. Inoltre, proponiamo un algoritmo veloce per ridurre la complessità della costruzione di MAA da $O(2^T)$ a $O(T^2)$. Esperimenti estesi su due set di dati video su larga scala mostrano che il nostro MAAN raggiunge una performance superiore sulla localizzazione temporale dell'azione debolmente supervisionata.
Deep image prior (DIP), che utilizza una struttura di rete convoluzionale profonda (ConvNet) come priore dell'immagine, ha attirato enormi attenzioni nella comunità della computer vision.  Mostra empiricamente l'efficacia della struttura ConvNet per varie applicazioni di restauro delle immagini.  Tuttavia, perché il DIP funziona così bene è ancora sconosciuto, e perché l'operazione di convoluzione è essenziale per la ricostruzione o il miglioramento dell'immagine non è molto chiaro.In questo studio, affrontiamo queste domande.L'approccio proposto è dividere la convoluzione in ``delay-embedding'' e ``transformation (\ie encoder-decoder)'', e proporre un semplice, ma essenziale, metodo di modellazione di immagine/tensore che è strettamente legato ai sistemi dinamici e all'autosimilarità. Il metodo proposto, chiamato modellazione di manifold nello spazio incorporato (MMES), è implementato utilizzando un nuovo denoising-auto-encoder in combinazione con multiway delay-embedding transform.nonostante la sua semplicità, il completamento dell'immagine/tensore e i risultati di super-risoluzione di MMES sono abbastanza simili anche competitivi a DIP nei nostri esperimenti estesi, e questi risultati ci aiuterebbero a reinterpretare/caratterizzare il DIP da una prospettiva di ``prima patch-manifold bidimensionale''.
L'apprendimento federato è un recente progresso nella protezione della privacy. In questo contesto, un curatore di fiducia aggrega i parametri ottimizzati in modo decentralizzato da più clienti. Il modello risultante viene quindi distribuito a tutti i clienti, convergendo infine verso un modello rappresentativo congiunto senza dover esplicitamente condividere i dati. Tuttavia, il protocollo è vulnerabile agli attacchi differenziali, che potrebbero provenire da qualsiasi parte che contribuisce durante l'ottimizzazione federata. In un tale attacco, il contributo di un cliente durante la formazione e le informazioni sul loro set di dati sono rivelati attraverso l'analisi del modello distribuito. Affrontiamo questo problema e proponiamo un algoritmo per l'ottimizzazione federata che preserva la privacy del cliente, con l'obiettivo di nascondere i contributi dei clienti durante la formazione, bilanciando il trade-off tra la perdita di privacy e le prestazioni del modello. Gli studi empirici suggeriscono che, dato un numero sufficientemente grande di client partecipanti, la nostra procedura proposta può mantenere la privacy differenziale a livello di client a un costo minore nelle prestazioni del modello.
L'impiego di reti neurali profonde come priori naturali dell'immagine per risolvere problemi inversi richiede grandi quantità di dati per addestrare sufficientemente modelli generativi espressivi o può avere successo con nessun dato tramite reti neurali non addestrate. Tuttavia, pochissimi lavori hanno considerato come interpolare tra questi regimi da zero a dati elevati. In particolare, come si può usare la disponibilità di una piccola quantità di dati (anche 5-25 esempi) a proprio vantaggio nella risoluzione di questi problemi inversi e le prestazioni di un sistema possono aumentare anche con l'aumentare della quantità di dati? In questo lavoro, consideriamo la risoluzione di problemi inversi lineari quando viene dato un piccolo numero di esempi di immagini che sono tratte dalla stessa distribuzione dell'immagine di interesse. Confrontando con reti neurali non addestrate che non usano dati, mostriamo come si può pre-addestrare una rete neurale con pochi esempi dati per migliorare i risultati di ricostruzione nel rilevamento compresso e nei problemi di recupero delle immagini semantiche come la colorazione. Il nostro approccio porta a una migliore ricostruzione man mano che la quantità di dati disponibili aumenta ed è alla pari con modelli generativi completamente addestrati, mentre richiede meno dell'1% dei dati necessari per addestrare un modello generativo.
Proponiamo il Cooperative Training (CoT) per l'addestramento di modelli generativi che misurano una densità trattabile per dati discreti.CoT addestra in modo coordinato un generatore G e un mediatore predittivo ausiliario M. L'obiettivo dell'addestramento di M è quello di stimare una densità di miscela della distribuzione appresa G e la distribuzione di destinazione P, e quello di G è di minimizzare la divergenza Jensen-Shannon stimata attraverso M. CoT raggiunge un successo indipendente senza la necessità di pre-addestramento tramite Maximum Likelihood Estimation o di coinvolgere algoritmi ad alta varianza come REINFORCE.Questo algoritmo a bassa varianza è teoricamente dimostrato essere superiore sia per la generazione di campioni che per la previsione di verosimiglianza.Abbiamo anche dimostrato teoricamente ed empiricamente la superiorità di CoT rispetto alla maggior parte degli algoritmi precedenti in termini di qualità e diversità generativa, capacità di generalizzazione predittiva e costo computazionale.
Le ricompense intrinseche nell'apprendimento di rinforzo forniscono una potente capacità algoritmica per gli agenti di imparare come interagire con il loro ambiente in un modo generato dal compito. Tuttavia, gli incentivi aumentati per la motivazione possono venire al costo di una maggiore fragilità alla stocasticità. Introduciamo un metodo per calcolare una ricompensa intrinseca per la curiosità usando metriche derivate dal campionamento di un modello di variabile latente usato per stimare la dinamica. Nei nostri esperimenti, un agente di videogiochi usa il nostro modello per imparare autonomamente a giocare ai giochi Atari usando la nostra ricompensa per la curiosità in combinazione con le ricompense estrinseche del gioco per ottenere una migliore performance sui giochi con ricompense estrinseche scarse.Quando la stocasticità viene introdotta nell'ambiente, il nostro metodo dimostra ancora una migliore performance rispetto alla linea di base.
L'incorporazione di parole è un potente strumento nell'elaborazione del linguaggio naturale.In questo articolo consideriamo il problema della composizione dell'incorporazione di parole \--- date rappresentazioni vettoriali di due parole, calcoliamo un vettore per l'intera frase.Diamo un modello generativo che può catturare relazioni sintattiche specifiche tra le parole.Sotto il nostro modello, dimostriamo che le correlazioni tra tre parole (misurate dal loro PMI) formano un tensore che ha una decomposizione di Tucker di rango basso approssimato. Il risultato della decomposizione di Tucker fornisce le incorporazioni delle parole così come un tensore centrale, che può essere utilizzato per produrre composizioni migliori delle incorporazioni delle parole.Completiamo anche i nostri risultati teorici con esperimenti che verificano le nostre ipotesi, e dimostrano l'efficacia del nuovo metodo di composizione.
Questo articolo descrive i progressi su questa sfida nel contesto degli ambienti creati dall'uomo, che sono visivamente diversi ma contengono regolarità semantiche intrinseche. Proponiamo un approccio ibrido basato sul modello e senza modello, LEArning and Planning with Semantics (LEAPS), che consiste in una sotto-politica multi-target che agisce sugli input visivi, e un modello bayesiano sulle strutture semantiche. Quando si trova in un ambiente non visto, l'agente pianifica con il modello semantico per prendere decisioni di alto livello, propone il prossimo sotto-obiettivo per la sotto-politica da eseguire, e aggiorna il modello semantico sulla base di nuove osservazioni.Eseguiamo esperimenti in compiti di navigazione visiva utilizzando House3D, un ambiente 3D che contiene diverse scene interne progettate dall'uomo con oggetti del mondo reale.LEAPS supera le forti linee di base che non pianificano esplicitamente utilizzando il contenuto semantico.
In questo articolo, ci concentriamo su due sfide che compensano la promessa di rappresentazione, rilevamento e recupero dei segnali sparsi.In primo luogo, i segnali del mondo reale raramente possono essere descritti come vettori perfettamente sparsi in una base nota, e gli schemi di misurazione casuale tradizionalmente utilizzati sono raramente ottimali per il rilevamento degli stessi.In secondo luogo, gli algoritmi di recupero del segnale esistenti non sono di solito abbastanza veloci da renderli applicabili ai problemi in tempo reale.In questo articolo, affrontiamo queste due sfide presentando un nuovo quadro basato sull'apprendimento profondo. Per la prima sfida, fondiamo il problema di trovare misure informative usando una formulazione di massima verosimiglianza (ML) e mostriamo come possiamo costruire un protocollo di riduzione della dimensionalità guidato dai dati per il rilevamento dei segnali usando architetture convoluzionali. per la seconda sfida, discutiamo e analizziamo un nuovo schema di parallelizzazione e mostriamo che accelera significativamente il processo di recupero del segnale. dimostriamo il miglioramento significativo che il nostro metodo ottiene rispetto ai metodi concorrenti attraverso una serie di esperimenti.
Per selezionare azioni efficaci in ambienti complessi, gli agenti intelligenti hanno bisogno di generalizzare dall'esperienza passata.I modelli del mondo possono rappresentare la conoscenza dell'ambiente per facilitare tale generalizzazione.Mentre l'apprendimento dei modelli del mondo da input sensoriali ad alta densità sta diventando fattibile attraverso l'apprendimento profondo, ci sono molti modi potenziali per derivare comportamenti da essi. Presentiamo Dreamer, un agente di apprendimento di rinforzo che risolve compiti a lungo orizzonte puramente attraverso l'immaginazione latente. Impariamo efficientemente i comportamenti retropropagando i gradienti analitici dei valori di stato appresi attraverso traiettorie immaginate nello spazio di stato compatto di un modello di mondo appreso. su 20 impegnativi compiti di controllo visivo, Dreamer supera gli approcci esistenti in efficienza di dati, tempo di calcolo e prestazioni finali.
L'apprendimento di rinforzo di trasferimento (RL) mira a migliorare l'efficienza di apprendimento di un agente sfruttando la conoscenza da altri agenti di origine addestrati su compiti rilevanti.Tuttavia, rimane impegnativo trasferire la conoscenza tra diverse dinamiche ambientali senza avere accesso agli ambienti di origine.In questo lavoro, esploriamo una nuova sfida in RL di trasferimento, dove solo un insieme di politiche di origine raccolte sotto diverse dinamiche sconosciute è disponibile per imparare un compito obiettivo in modo efficiente.Per affrontare questo problema, l'approccio proposto, MULTI-source POLicy AggRegation (MULTIPOLAR), comprende due tecniche chiave. Nel frattempo, impariamo una rete ausiliaria che predice i residui intorno alle azioni aggregate, il che garantisce l'espressività della politica di destinazione anche quando alcune delle politiche di origine hanno scarse prestazioni. Abbiamo dimostrato l'efficacia di MULTIPOLAR attraverso un'ampia valutazione sperimentale in sei ambienti simulati che vanno dai classici problemi di controllo alle impegnative simulazioni di robotica, in spazi di azione sia continui che discreti.
Gli algoritmi di apprendimento per rinforzo si basano su ricompense attentamente progettate dall'ambiente che sono estrinseche all'agente. Tuttavia, annotare ogni ambiente con ricompense dense progettate a mano è difficile e non scalabile, motivando la necessità di sviluppare funzioni di ricompensa che sono intrinseche all'agente. La curiosità è una funzione di ricompensa intrinseca che utilizza l'errore di previsione come segnale di ricompensa. In questo articolo: (a) Eseguiamo il primo studio su larga scala dell'apprendimento puramente guidato dalla curiosità, cioè senza ricompense estrinseche, su 54$ ambienti di riferimento standard, compresa la suite di giochi Atari. I nostri risultati mostrano prestazioni sorprendentemente buone e un alto grado di allineamento tra l'obiettivo intrinseco della curiosità e le ricompense estrinseche progettate a mano di molti giochi. (b) Indaghiamo l'effetto dell'uso di diversi spazi di caratteristiche per calcolare l'errore di predizione e mostriamo che le caratteristiche casuali sono sufficienti per molti benchmark di giochi RL popolari, ma le caratteristiche apprese sembrano generalizzare meglio (ad esempio ai nuovi livelli di gioco in Super Mario Bros.).(c) Dimostriamo i limiti delle ricompense basate sulla predizione in configurazioni stocastiche.Video di gioco e codice sono su https://doubleblindsupplementary.github.io/large-curiosity/.
Questo lavoro fornisce prove teoriche ed empiriche che i regolarizzatori che inducono invarianza possono aumentare l'accuratezza predittiva per le trasformazioni spaziali del caso peggiore (robustezza spaziale).  Valutato su questi esempi trasformati in modo avverso, dimostriamo che l'aggiunta di una regolarizzazione in cima alla formazione standard o avversa riduce l'errore relativo del 20% per CIFAR10 senza aumentare il costo computazionale.  Questo supera le reti artigianali che sono state esplicitamente progettate per essere spaziali-equivarianti. Inoltre, osserviamo per SVHN, noto per avere una varianza intrinseca nell'orientamento, che l'addestramento robusto migliora anche la precisione standard sul set di test.
Proponiamo l'apprendimento dell'ordine per determinare il grafico dell'ordine delle classi, che rappresenta i ranghi o le priorità, e classificare un'istanza dell'oggetto in una delle classi.A questo scopo, progettiamo un comparatore pairwise per classificare la relazione fra due istanze in uno dei tre casi: un'istanza è `grande che,' `simile a,' o `più piccolo che' l'altro. Poi, confrontando un'istanza di input con istanze di riferimento e massimizzando la coerenza tra i risultati del confronto, la classe dell'input può essere stimata in modo affidabile. Applichiamo l'apprendimento dell'ordine per sviluppare uno stimatore dell'età facciale, che fornisce una performance allo stato dell'arte. Inoltre, la performance è ulteriormente migliorata quando il grafico dell'ordine è diviso in catene disgiunte utilizzando informazioni sul genere e sul gruppo etnico o anche in modo non supervisionato.
Studiamo come la topologia di un set di dati che comprende due componenti che rappresentano due classi di oggetti in un problema di classificazione binaria cambia quando passa attraverso gli strati di una rete neurale ben addestrata, cioè, L'obiettivo è quello di far luce su due misteri ben noti nelle reti neurali profonde:(i) una funzione di attivazione non liscia come ReLU supera una funzione liscia come la tangente iperbolica;(ii) le architetture di rete neurale di successo si basano sull'avere molti strati, nonostante il fatto che una rete poco profonda è in grado di approssimare bene qualsiasi funzione arbitraria.Abbiamo eseguito ampi esperimenti sull'omologia persistente di una serie di serie di dati di nuvole di punti.I risultati dimostrano costantemente quanto segue: (1) Le reti neurali operano cambiando la topologia, trasformando un set di dati topologicamente complicato in uno topologicamente semplice mentre passa attraverso gli strati.Non importa quanto sia complicata la topologia del set di dati con cui iniziamo, quando passa attraverso una rete neurale ben addestrata, i numeri di Betti di entrambi i componenti si riducono invariabilmente ai loro valori più bassi possibili: il numero di Betti zeroth è uno e tutti i numeri di Betti superiori sono zero. Inoltre, (2) la riduzione dei numeri di Betti è significativamente più veloce per l'attivazione ReLU rispetto all'attivazione della tangente iperbolica --- coerente con il fatto che la prima definisce mappe non omeomorfe (che cambiano la topologia) mentre la seconda definisce mappe omeomorfe (che preservano la topologia). Infine, (3) le reti superficiali e profonde elaborano lo stesso set di dati in modo diverso --- una rete superficiale opera principalmente attraverso il cambiamento della geometria e cambia la topologia solo nei suoi strati finali, una rete profonda diffonde i cambiamenti topologici in modo più uniforme in tutti i suoi strati.
Il tasso di convergenza e le prestazioni finali dei comuni modelli di apprendimento profondo hanno beneficiato in modo significativo delle euristiche proposte di recente, come la programmazione del tasso di apprendimento, la distillazione della conoscenza, le connessioni saltate e gli strati di normalizzazione.In assenza di basi teoriche, gli esperimenti controllati volti a spiegare l'efficacia di queste strategie possono aiutare la nostra comprensione dei paesaggi di apprendimento profondo e le dinamiche di formazione. Gli approcci esistenti per l'analisi empirica si basano su strumenti di interpolazione lineare e visualizzazioni con riduzione della dimensionalità, ognuno con le sue limitazioni.Invece, rivisitiamo l'analisi empirica delle euristiche attraverso la lente dei metodi recentemente proposti per la superficie di perdita e l'analisi della rappresentazione, cioè la connettività di modo e l'analisi di correlazione canonica (CCA), e ipotizziamo le ragioni per cui l'euristica ha successo.In particolare, esploriamo la distillazione della conoscenza e l'euristica del tasso di apprendimento di (coseno) restart e warmup usando la connettività di modo e CCA. La nostra analisi empirica suggerisce che: (a) le ragioni spesso citate per il successo del cosine annealing non sono evidenziate nella pratica; (b) che l'effetto del learning rate warmup è quello di evitare che gli strati più profondi creino instabilità di formazione; e (c) che la conoscenza latente condivisa dall'insegnante è principalmente distribuita negli strati più profondi.
La crescente domanda di reti neurali (NN) impiegate su dispositivi incorporati ha portato a un'abbondanza di ricerche che studiano metodi per l'addestramento di NN a bassa precisione. Mentre la maggior parte dei metodi coinvolge un passo di quantizzazione, noi proponiamo un approccio bayesiano di principio in cui prima deduciamo una distribuzione su uno spazio di peso discreto da cui successivamente deriviamo NN di bassa precisione hardware-friendly. A questo scopo, introduciamo un passaggio in avanti probabilistico per approssimare l'intrattabile obiettivo variazionale che ci permette di ottimizzare su distribuzioni di peso a valore discreto per le NN con funzioni di attivazione del segno. Nei nostri esperimenti, dimostriamo che il nostro modello raggiunge lo stato dell'arte delle prestazioni su diversi set di dati del mondo reale, inoltre i modelli risultanti mostrano una notevole quantità di sparsità che può essere utilizzata per ridurre ulteriormente i costi computazionali per l'inferenza.
Molti domini irregolari come le reti sociali, le transazioni finanziarie, le connessioni neuronali e le strutture del linguaggio naturale sono rappresentate come grafi.Negli ultimi anni, una varietà di reti neurali del grafico (GNNs) sono state applicate con successo per l'apprendimento e la previsione della rappresentazione su tali grafi.Tuttavia, in molte delle applicazioni, il grafico sottostante cambia nel tempo e le GNNs esistenti sono inadeguate per la gestione di tali grafi dinamici.In questo articolo proponiamo una tecnica nuova per l'apprendimento delle incorporazioni dei grafi dinamici basati su una struttura di algebra tensoriale. Il nostro metodo estende la popolare rete di convoluzione del grafico (GCN) per l'apprendimento delle rappresentazioni dei grafi dinamici usando la tecnica del prodotto M del tensore recentemente proposta.I risultati teorici che stabiliscono la connessione tra l'approccio del tensore proposto e la convoluzione spettrale dei tensori sono sviluppati.Gli esperimenti numerici sui set di dati reali dimostrano l'utilità del metodo proposto per un compito di classificazione del bordo sui grafi dinamici.
La nostra motivazione principale è quella di proporre un approccio efficiente per generare nuovi composti chimici stabili multi-elemento che possono essere utilizzati in applicazioni del mondo reale.Questo compito può essere formulato come un problema combinatorio, e ci vogliono molte ore di esperti umani per costruire, e per valutare nuovi dati.Metodi di apprendimento non supervisionati come le reti generative avversarie (GAN) possono essere utilizzati in modo efficiente per produrre nuovi dati.  Tuttavia, nel dominio della scienza dei materiali, c'è la necessità di sintetizzare i dati con una complessità di ordine superiore rispetto ai campioni osservati, e lo stato dell'arte del cross-domain GANs non può essere adattato direttamente. In questo contributo, proponiamo un nuovo GAN chiamato CrystalGAN che genera nuove strutture cristallografiche chimicamente stabili con una maggiore complessità del dominio.Introduciamo un'architettura originale, forniamo le funzioni di perdita corrispondenti, e dimostriamo che il CrystalGAN genera dati molto ragionevoli.Illustriamo l'efficienza del metodo proposto su un problema originale reale di scoperta di nuovi idruri che possono essere ulteriormente utilizzati nello sviluppo di materiali di stoccaggio dell'idrogeno.
Dati i campioni da un gruppo di compiti di regressione correlati, un modello arricchito di dati descrive le osservazioni attraverso un parametro comune e uno individuale per gruppo.In un regime ad alta densità, ogni parametro ha una propria struttura come la sparsità o la sparsità di gruppo.In questo articolo, consideriamo la forma generale di arricchimento dei dati dove i dati provengono da un numero fisso ma arbitrario di compiti $G$ e qualsiasi funzione convessa, ad esempio, la norma, può caratterizzare la struttura dei parametri comuni e individuali. 	Proponiamo uno stimatore per il modello arricchito di dati ad alta densità e studiamo le sue proprietà statistiche.  Delineiamo la complessità campionaria del nostro stimatore e forniamo un limite non asintotico ad alta probabilità per l'errore di stima di tutti i parametri sotto una condizione più debole rispetto allo stato dell'arte. Proponiamo un algoritmo di stima iterativo con un tasso di convergenza geometrico. 	
I veicoli autonomi stanno diventando sempre più comuni nel trasporto urbano.  Le aziende inizieranno a trovare la necessità di insegnare a questi veicoli il coordinamento della flotta cittadina intelligente.  Attualmente, la modellazione basata sulla simulazione insieme alle regole codificate a mano dettano il processo decisionale di questi veicoli autonomi. Noi crediamo che il comportamento intelligente complesso possa essere appreso da questi agenti attraverso il Reinforcement Learning. In questo articolo, discutiamo il nostro lavoro per risolvere questo sistema adattando il modello Deep Q-Learning (DQN) all'impostazione multi-agente.  Il nostro approccio applica l'apprendimento di rinforzo profondo combinando reti neurali convoluzionali con DQN per insegnare agli agenti a soddisfare la domanda dei clienti in un ambiente che è parzialmente osservabile per loro.Dimostriamo anche come utilizzare l'apprendimento di trasferimento per insegnare agli agenti a bilanciare obiettivi multipli come la navigazione verso una stazione di ricarica quando il suo livello di energia è basso.Le due valutazioni presentate mostrano che la nostra soluzione ha dimostrato che siamo in grado di insegnare con successo agli agenti politiche di cooperazione mentre bilanciano obiettivi multipli.
La stabilità è un aspetto chiave dell'analisi dei dati.In molte applicazioni, la nozione naturale di stabilità è geometrica, come illustrato per esempio nella computer vision.Le trasformazioni di dispersione costruiscono rappresentazioni convoluzionali profonde che sono certificate stabili alle deformazioni di input.Questa stabilità alle deformazioni può essere interpretata come stabilità rispetto ai cambiamenti nella struttura metrica del dominio. In questo lavoro, mostriamo che le trasformazioni di diffusione possono essere generalizzate ai domini non euclidei usando le wavelets di diffusione, mentre conservano una nozione di stabilità rispetto ai cambiamenti metrici nel dominio, misurata con le mappe di diffusione. La rappresentazione risultante è stabile alle perturbazioni metriche del dominio mentre è capace di catturare l'informazione "ad alta frequenza", simile alla diffusione euclidea.
Proponiamo una nuova architettura di rete profonda per l'apprendimento permanente a cui ci riferiamo come Dynamically Expandable Network (DEN), che può decidere dinamicamente la sua capacità di rete mentre si allena su una sequenza di compiti, per imparare una struttura compatta di condivisione della conoscenza sovrapposta tra i compiti.DEN è efficientemente addestrato in un modo online eseguendo la riqualificazione selettiva, espande dinamicamente la capacità della rete all'arrivo di ogni compito con solo il numero necessario di unità, ed evita efficacemente la deriva semantica dividendo/duplicando le unità e timestampandole. Convalidiamo DEN su più set di dati pubblici in scenari di apprendimento permanente su più set di dati pubblici, sui quali non solo supera significativamente i metodi di apprendimento permanente esistenti per le reti profonde, ma raggiunge anche lo stesso livello di prestazioni del modello batch con un numero sostanzialmente inferiore di parametri.
Questo articolo promuove l'idea che i metodi di apprendimento profondo possano essere affiancati alle classiche pipeline di odometria visiva per migliorarne la precisione e produrre modelli di incertezza alle loro stime. Mostriamo che le distorsioni inerenti al processo di odometria visiva possono essere apprese e compensate fedelmente, e che un'ar-chitettura di apprendimento associata a una funzione di perdita probabilistica può stimare congiuntamente una matrice di covarianza completa degli errori residui, definendo un modello di errore eteroscedastico. Esperimenti su sequenze di immagini di guida autonoma e acquisizioni di microcamere di veicoli aerei valutano la possibilità di migliorare simultaneamente l'odometro visivo e stimare un errore associato alle sue uscite.
La costruzione di robusti sistemi di raccomandazione di contenuti online richiede l'apprendimento di complesse interazioni tra le preferenze dell'utente e le caratteristiche del contenuto. Il campo si è evoluto rapidamente negli ultimi anni dalle tradizionali tecniche di filtraggio collaborativo e di bandit multi-arm, con nuovi metodi che integrano modelli di Deep Learning che permettono di catturare le interazioni non lineari delle caratteristiche.Nonostante i progressi, la natura dinamica delle raccomandazioni online pone ancora grandi sfide, come trovare il delicato equilibrio tra esplorazione e sfruttamento. In questo articolo forniamo un nuovo metodo, Deep Density Networks (DDN) che deconvolve l'incertezza delle misurazioni e dei dati e predice le densità di probabilità del CTR, permettendoci di eseguire un'esplorazione più efficiente dello spazio delle caratteristiche. Mostriamo l'utilità di usare DDN online in un sistema di raccomandazione di contenuti del mondo reale che serve miliardi di raccomandazioni al giorno, e presentiamo risultati online e offline per valutare il beneficio di usare DDN.
Mentre è ben documentato che gli accettatori e i negatori del cambiamento climatico sono diventati sempre più polarizzati negli Stati Uniti nel corso del tempo, non c'è stato alcun esame su larga scala per verificare se questi individui sono inclini a cambiare le loro opinioni a seguito di eventi naturali esterni.Sulla sotto-popolazione di utenti di Twitter, esaminiamo se il sentimento del cambiamento climatico cambia in risposta a cinque disastri naturali separati che si verificano negli Stati Uniti nel 2018. Iniziamo mostrando che i tweet possono essere classificati con oltre il 75% di precisione come accettando o negando il cambiamento climatico quando si utilizza la nostra metodologia per compensare i dati etichettati limitati; i risultati sono robusti attraverso diversi modelli di apprendimento automatico e producono risultati a livello geografico in linea con la ricerca precedente. Applichiamo poi le RNN per condurre un'analisi a livello di coorte, mostrando che gli uragani del 2018 hanno prodotto un aumento statisticamente significativo del sentimento medio dei tweet che affermano il cambiamento climatico; tuttavia, questo effetto non si mantiene per le bufere di neve e gli incendi del 2018 studiati, implicando che le opinioni degli utenti di Twitter sul cambiamento climatico sono abbastanza radicate in questo sottoinsieme di disastri naturali.
Utilizzando una recente tecnica di filtraggio spettrale per rappresentare concisamente tali sistemi in una base lineare, formuliamo il controllo ottimale in questa impostazione come un programma convesso. Questo approccio elimina la necessità di risolvere il problema non convesso dell'identificazione esplicita del sistema e del suo stato latente, e permette garanzie di ottimalità dimostrabili per il segnale di controllo. Diamo il primo algoritmo efficiente per trovare il segnale di controllo ottimale con un orizzonte temporale T arbitrario, con una complessità del campione (numero di rollout di formazione) polinomiale solo in log(T) e altri parametri rilevanti.
Dal loro avvento, numerose varianti di GAN sono state introdotte in letteratura, concentrandosi principalmente sull'utilizzo di nuove funzioni di perdita, strategie di ottimizzazione/regolarizzazione e architetture di rete. In questo articolo, rivolgiamo la nostra attenzione al generatore e studiamo l'uso di polinomi di alto ordine come una classe alternativa di approssimatori di funzioni universali. Concretamente, proponiamo PolyGAN, dove modelliamo il generatore di dati per mezzo di un polinomio di alto ordine i cui parametri sconosciuti sono naturalmente rappresentati da tensori di alto ordine. Introduciamo due decomposizioni tensoriali che riducono significativamente il numero di parametri e mostriamo come possono essere efficientemente implementati da reti neurali gerarchiche che impiegano solo blocchi lineari/convolutivi. Mostriamo per la prima volta che utilizzando il nostro approccio un generatore GAN può approssimare la distribuzione dei dati senza utilizzare alcuna funzione di attivazione. Una valutazione sperimentale approfondita su dati sintetici e reali (immagini e nuvole di punti 3D) dimostra i meriti di PolyGAN rispetto allo stato dell'arte.
Le reti neurali profonde addestrate su grandi set di dati supervisionati hanno portato a risultati impressionanti negli ultimi anni. Tuttavia, poiché i set di dati ben annotati possono essere proibitivamente costosi e richiedono tempo per essere raccolti, il lavoro recente ha esplorato l'uso di set di dati più grandi ma rumorosi che possono essere ottenuti più facilmente. In questo articolo, studiamo il comportamento delle reti neurali profonde su set di allenamento con etichette massicciamente rumorose. Ad esempio, su MNIST troviamo che un'accuratezza superiore al 90% è ancora raggiungibile anche quando il set di dati è stato diluito con 100 esempi rumorosi per ogni esempio pulito.Tale comportamento si mantiene attraverso molteplici modelli di rumore di etichetta, anche quando le etichette rumorose sono distorte verso classi confuse.Inoltre, mostriamo come la dimensione del set di dati richiesto per un addestramento di successo aumenta con un rumore di etichetta più elevato.Infine, presentiamo semplici tecniche attuabili per migliorare l'apprendimento nel regime di alto rumore di etichetta.
In questo articolo, proponiamo di estendere l'algoritmo di meta-apprendimento model-agnostic recentemente introdotto (MAML, Finn et al., 2017) per la traduzione automatica neurale a basse risorse (NMT).Inquadriamo la traduzione a basse risorse come un problema di meta-apprendimento, e impariamo ad adattarci alle lingue a basse risorse sulla base di compiti linguistici multilingue ad alte risorse.Usiamo la rappresentazione lessicale universale (Gu et al., Valutiamo la strategia di meta-apprendimento proposta utilizzando diciotto lingue europee (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv e Ru) come compiti sorgente e cinque lingue diverse (Ro, Lv, Fi, Tr e Ko) come compiti target.Mostriamo che l'approccio proposto supera significativamente l'approccio multilingue basato sul transfer learning (Zoph et al., 2016) e ci permette di addestrare un sistema NMT competitivo con solo una frazione di esempi di formazione.Per esempio, l'approccio proposto può raggiungere fino a 22,04 BLEU su WMT'16 rumeno-inglese vedendo solo 16.000 parole tradotte (~600 frasi parallele).
Questo lavoro presenta un metodo per il rilevamento attivo delle anomalie che può essere costruito su soluzioni esistenti di deep learning per il rilevamento non supervisionato delle anomalie. Mostriamo che è necessario assumere una priorità su quali sono le anomalie, al fine di avere garanzie di prestazioni nel rilevamento non supervisionato delle anomalie. Per risolvere questo problema, presentiamo un nuovo strato che può essere allegato a qualsiasi modello di deep learning progettato per il rilevamento delle anomalie non supervisionato per trasformarlo in un metodo attivo, presentando risultati su set di dati di rilevamento delle anomalie sia sintetici che reali.
In questo articolo, chiediamo i principali fattori che determinano il processo decisionale di un classificatore e scopriamo tali fattori studiando i codici latenti prodotti da framework di autocodifica.Per fornire una spiegazione del comportamento di un classificatore, proponiamo un metodo che fornisce una serie di esempi che evidenziano le differenze semantiche tra le decisioni del classificatore.Generiamo questi esempi attraverso interpolazioni nello spazio latente. Introduciamo e formalizziamo la nozione di percorso stocastico semantico, come un adeguato processo stocastico definito nello spazio delle caratteristiche attraverso interpolazioni di codice latente.Introduciamo poi il concetto di Lagrangiani semantici come un modo per incorporare il comportamento del classificatore desiderato e troviamo che la soluzione del problema variazionale associato permette di evidenziare le differenze nella decisione del classificatore.Molto importante, all'interno del nostro quadro il classificatore è usato come una black-box, ed è richiesta solo la sua valutazione.
La solidità e l'ottimalità di un piano dipende dalla correttezza del modello di dominio.Nelle applicazioni del mondo reale, specificare modelli di dominio completi è difficile poiché le interazioni tra l'agente e il suo ambiente possono essere piuttosto complesse.Proponiamo un quadro per imparare una rappresentazione PPDDL del modello in modo incrementale su più problemi di pianificazione utilizzando solo le esperienze del problema di pianificazione corrente, che si adatta ad ambienti non stazionari. Introduciamo il nuovo concetto di affidabilità come motivazione intrinseca per l'apprendimento di rinforzo, e come mezzo di apprendimento dal fallimento per prevenire istanze ripetute di fallimenti simili.La nostra motivazione è quella di migliorare sia l'efficienza di apprendimento che l'orientamento all'obiettivo.Valutiamo il nostro lavoro con risultati sperimentali per tre domini di pianificazione.
Il campo del Deep Reinforcement Learning (DRL) ha recentemente visto un'impennata nella popolarità degli algoritmi di apprendimento di rinforzo a massima entropia.  La loro popolarità deriva dall'interpretazione intuitiva dell'obiettivo di massima entropia e dalla loro efficienza superiore del campione sui benchmark standard. In questo articolo, cerchiamo di capire il contributo primario del termine di entropia alla performance degli algoritmi di massima entropia. per il benchmark Mujoco, dimostriamo che il termine di entropia in Soft Actor Critic (SAC) affronta principalmente la natura delimitata degli spazi di azione. Con questa intuizione, proponiamo un semplice schema di normalizzazione che permette ad un algoritmo semplificato senza massimizzazione dell'entropia di eguagliare le prestazioni di SAC.I nostri risultati sperimentali dimostrano la necessità di rivedere i benefici della regolarizzazione dell'entropia in DRL.Proponiamo anche un semplice metodo di campionamento non uniforme per selezionare le transizioni dal replay buffer durante l'allenamento.  Mostriamo inoltre che l'algoritmo semplificato con il semplice schema di campionamento non uniforme supera il SAC e raggiunge prestazioni all'avanguardia su compiti di controllo continuo impegnativi.
Molto recentemente, è diventato un approccio popolare per rispondere alle domande a dominio aperto, cercando prima i passaggi relativi alla domanda, poi applicando modelli di comprensione della lettura per estrarre le risposte. I lavori esistenti di solito estraggono le risposte da singoli passaggi in modo indipendente, quindi non sfruttano appieno i passaggi multipli ricercati, soprattutto per alcune domande che richiedono diverse prove, che possono apparire in diversi passaggi, per essere risposte. In particolare, basandoci sui candidati alla risposta generati dal modello di QA esistente, proponiamo due diversi metodi di re-ranking, basati sulla forza e sulla copertura, che fanno uso delle prove aggregate da diversi passaggi per aiutare a implicare la risposta vera e propria alla domanda. Il nostro modello ha raggiunto lo stato dell'arte su tre set di dati pubblici open-domain QA, Quasar-T, SearchQA e la versione open-domain di TriviaQA, con circa l'8% di miglioramento sui primi due set di dati.
Molte grandi collezioni di testo presentano strutture a grafo, sia inerenti al contenuto stesso che codificate nei metadati dei singoli documenti. Esempi di grafi estratti da collezioni di documenti sono le reti di co-autori, le reti di citazioni, o le reti di cooccorrenze di entità nominate; inoltre, le reti sociali possono essere estratte da corpora di e-mail, tweet o social media. Quando si tratta di visualizzare questi grandi corpora, vengono utilizzati o il contenuto testuale o il grafico della rete. In questo articolo, proponiamo di incorporare entrambi, testo e grafico, non solo per visualizzare le informazioni semantiche codificate nel contenuto dei documenti ma anche le relazioni espresse dalla struttura intrinseca della rete. A questo scopo, introduciamo un nuovo algoritmo basato sull'ottimizzazione multi-obiettivo per posizionare congiuntamente i documenti incorporati e i nodi del grafico in un paesaggio bidimensionale. Illustriamo l'efficacia del nostro approccio con set di dati del mondo reale e dimostriamo che possiamo catturare la semantica di grandi collezioni di documenti meglio di altre visualizzazioni basate sul contenuto o sulle informazioni della rete.
I modelli appresi dalle macchine presentano delle distorsioni, spesso perché i set di dati utilizzati per addestrarli sono di parte.Questo rappresenta un serio problema per l'implementazione di tale tecnologia, poiché i modelli risultanti potrebbero funzionare male su popolazioni che sono minoranze all'interno del set di addestramento e, in definitiva, presentare rischi più elevati per loro.Proponiamo di utilizzare simulazioni al computer ad alta fedeltà per interrogare e diagnosticare le distorsioni all'interno dei classificatori ML. Presentiamo una struttura che sfrutta la ricerca bayesiana dei parametri per caratterizzare in modo efficiente lo spazio delle caratteristiche ad alta dimensionalità e identificare più rapidamente le debolezze nelle prestazioni. Applichiamo il nostro approccio a un dominio di esempio, il rilevamento del volto, e dimostriamo che può essere utilizzato per aiutare a identificare le distorsioni demografiche nelle interfacce di programmazione delle applicazioni commerciali del volto (API).
Le nuvole di punti sono un modo flessibile e onnipresente di rappresentare oggetti 3D con risoluzione e precisione arbitraria.Il lavoro precedente ha dimostrato che adattare le reti di codifica per abbinare la semantica delle loro nuvole di punti di input può migliorare significativamente la loro efficacia rispetto alle alternative feedforward ingenue.Tuttavia, la stragrande maggioranza del lavoro sui decodificatori point-cloud è ancora basata su reti completamente connesse che mappano le rappresentazioni di forma a un numero fisso di punti di output.In questo lavoro, studiamo architetture di decoder che abbinano più strettamente la semantica delle nuvole di punti di dimensioni variabili. In particolare, studiamo i decodificatori point-cloud basati su campioni che mappano una rappresentazione di forma a una distribuzione di caratteristiche puntuali, consentendo a un numero arbitrario di caratteristiche campionate di essere trasformate in singoli punti di uscita.Sviluppiamo tre architetture di decodificatori basati su campioni e confrontiamo le loro prestazioni tra loro e mostriamo la loro efficacia migliorata rispetto alle architetture feedforward.Inoltre, indaghiamo le distribuzioni apprese per ottenere informazioni sulla trasformazione dell'uscita.Il nostro lavoro è disponibile come una piattaforma software estensibile per riprodurre questi risultati e servire come base per lavori futuri.
A differenza dei precedenti lavori basati sull'apprendimento che richiedono l'addestramento dell'ottimizzatore sullo stesso grafico da ottimizzare, noi proponiamo un approccio di apprendimento che addestra un ottimizzatore offline e poi generalizza ai grafici precedentemente non visti senza ulteriore addestramento. Questo permette al nostro approccio di produrre decisioni di esecuzione di alta qualità sui grafici TensorFlow del mondo reale in pochi secondi invece che in ore. Consideriamo due compiti di ottimizzazione per i grafi di calcolo: la minimizzazione del tempo di esecuzione e l'utilizzo della memoria di picco.rispetto a un ampio set di linee di base, il nostro approccio raggiunge miglioramenti significativi rispetto ai metodi classici e ad altri metodi basati sull'apprendimento su questi due compiti.
Le teorie di codifica predittiva suggeriscono che il cervello impara prevedendo le osservazioni a vari livelli di astrazione.Uno dei compiti di predizione più basilari è la predizione della vista: come apparirebbe una data scena da un punto di vista alternativo? Gli esseri umani eccellono in questo compito.La nostra capacità di immaginare e riempire le informazioni visive mancanti è strettamente legata alla percezione: ci sembra di vedere il mondo in 3 dimensioni, mentre in realtà le informazioni provenienti solo dalla superficie frontale del mondo colpiscono le nostre retine (2D). Questo articolo esplora la connessione tra l'apprendimento della rappresentazione predittiva della vista e il suo ruolo nello sviluppo del riconoscimento visivo 3D. Proponiamo reti grafiche inverse, che prendono come input flussi video 2.5D catturati da una telecamera in movimento, e li mappano in stabili mappe di caratteristiche 3D della scena, separando il contenuto della scena dal movimento della telecamera. Il modello può anche proiettare le sue mappe di caratteristiche 3D su nuovi punti di vista, per prevedere e confrontare con le viste di destinazione. Proponiamo perdite di predizione contrastive che possono gestire la stocasticità dell'input visivo e possono scalare l'apprendimento predittivo della vista a scene più fotorealistiche di quelle considerate nei lavori precedenti. Mostriamo che il modello proposto impara rappresentazioni visive 3D utili per (1) l'apprendimento semi-supervisionato di rilevatori di oggetti 3D e (2) l'apprendimento non supervisionato di rilevatori di oggetti in movimento 3D, stimando il movimento delle mappe di caratteristiche 3D dedotte in video di scene dinamiche.Al meglio delle nostre conoscenze, questo è il primo lavoro che mostra empiricamente la previsione della vista come un utile e scalabile compito auto-supervisionato utile per il rilevamento di oggetti 3D.  
La modellazione dello stile quando si sintetizza il discorso umano naturale dal testo è stata al centro di una significativa attenzione.Alcuni approcci all'avanguardia addestrano una rete di codifica-decodifica su campioni di testo e audio accoppiati (x_txt, x_aud) incoraggiando il suo output a ricostruire x_aud.La forma d'onda audio sintetizzata dovrebbe contenere il contenuto verbale di x_txt e lo stile uditivo di x_aud. Sfortunatamente, la modellazione dello stile nel TTS è in qualche modo sotto-determinata e l'addestramento di modelli con una perdita di ricostruzione da sola è insufficiente per distinguere il contenuto e lo stile da altri fattori di variazione. In questo lavoro, introduciamo un modello TTS end-to-end che offre una migliore capacità di disindividuazione e controllabilità dello stile del contenuto. Il gioco avversario concentra la vera distribuzione dei dati, e il gioco collaborativo minimizza la distanza tra i campioni reali e i campioni generati sia nello spazio originale che nello spazio latente.Come risultato, il modello proposto offre un generatore altamente controllabile e una rappresentazione distinta. Beneficiando della modellazione separata dello stile e del contenuto, il nostro modello può generare un discorso di fedeltà umana che soddisfa le condizioni di stile desiderate. Il nostro modello raggiunge risultati all'avanguardia in diversi compiti, tra cui il trasferimento di stile (scambio di contenuto e stile), la modellazione delle emozioni e il trasferimento di identità (adattamento della voce di un nuovo parlante).
L'evidenza empirica suggerisce che le reti neurali con attivazioni ReLU generalizzano meglio con over-parameterization.However, non c'è attualmente alcuna analisi teorica che spiega questa osservazione.In questo lavoro, studiamo un compito di apprendimento semplificato con reti convolutional over-parameterized che empiricamente esibisce lo stesso fenomeno qualitativo.  Per questa impostazione, forniamo un'analisi teorica dell'ottimizzazione e delle prestazioni di generalizzazione della discesa del gradiente. In particolare, dimostriamo i limiti di complessità del campione dipendenti dai dati che dimostrano che l'iperparametrizzazione migliora le prestazioni di generalizzazione della discesa del gradiente.
Introduciamo un nuovo algoritmo di meta-apprendimento PAC-Bayes formulato in modo rigoroso che impara implicitamente una distribuzione prioritaria del modello di interesse. Il nostro metodo proposto estende la struttura PAC-Bayes da un'impostazione a compito singolo all'impostazione di meta-apprendimento a pochi colpi per limitare al massimo gli errori di generalizzazione su compiti non visti. Proponiamo anche un approccio basato sulla generazione per modellare il priore condiviso e il posteriore specifico del compito in modo più espressivo rispetto alla solita assunzione diagonale gaussiana. Mostriamo che i modelli addestrati con il nostro algoritmo di meta-apprendimento proposto sono ben calibrati e accurati, con risultati di calibrazione e classificazione allo stato dell'arte sul benchmark mini-ImageNet, e risultati competitivi in una regressione multi-modale della distribuzione dei compiti.
Man mano che l'area della Explainable AI (XAI), e della Explainable AI Planning (XAIP), matura, la capacità degli agenti di generare e curare le spiegazioni crescerà allo stesso modo.Proponiamo una nuova area di sfida sotto forma di spiegazioni ribelli e ingannevoli.Discutiamo come queste spiegazioni potrebbero essere generate e poi discutiamo brevemente i criteri di valutazione.
Indaghiamo una variante degli autocodificatori variazionali in cui c'è una sovrastruttura di variabili latenti discrete in cima alle caratteristiche latenti.In generale, la nostra sovrastruttura è una struttura ad albero di più variabili super latenti e viene appresa automaticamente dai dati.Quando c'è solo una variabile latente nella sovrastruttura, il nostro modello si riduce ad uno che assume che le caratteristiche latenti siano generate da un modello di miscela gaussiana. Mentre i precedenti metodi di deep learning per il clustering producono solo una partizione dei dati, LTVAE produce partizioni multiple di dati, ognuna delle quali è data da una variabile super latente, il che è auspicabile perché i dati ad alta dimensione di solito hanno molte sfaccettature naturali diverse e possono essere significativamente partizionati in più modi.
Molti compiti pratici di locomozione dei robot richiedono che gli agenti usino politiche di controllo che possono essere parametrizzate da obiettivi.I più popolari approcci di deep reinforcement learning in questa direzione comportano l'apprendimento di politiche o funzioni di valore condizionate da obiettivi, o modelli di dinamica inversa (IDM).Gli IDM mappano lo stato corrente di un agente e l'obiettivo desiderato alle azioni richieste.Mostriamo che la chiave per ottenere buone prestazioni con gli IDM sta nell'apprendere le informazioni condivise tra esperienze equivalenti, in modo che possano essere generalizzate a scenari mai visti. Utilizzando un numero limitato di interazioni con l'ambiente, il nostro agente è in grado di navigare in modo efficiente verso punti arbitrari nello spazio dell'obiettivo.Dimostriamo l'efficacia del nostro approccio in ambienti di locomozione ad alta densità come la formica Mujoco, PyBullet Humanoid, e PyBullet Minitaur.Forniamo risultati quantitativi e qualitativi per dimostrare che il nostro metodo supera chiaramente gli approcci di base concorrenti.
In questo articolo, per prima cosa identifichiamo la distorsione angolare (angle bias), un fenomeno semplice ma notevole che causa il problema del gradiente evanescente in un perceptron multistrato (MLP) con funzioni di attivazione sigmoidali, quindi proponiamo pesi vincolati linearmente (LCW) per ridurre la distorsione angolare in una rete neurale, in modo da addestrare la rete sotto i vincoli che la somma degli elementi di ogni vettore di peso sia zero. Viene presentata una tecnica di riparametrizzazione per addestrare in modo efficiente un modello con LCW incorporando i vincoli sui vettori di peso nella struttura della rete.È interessante notare che la normalizzazione batch (Ioffe & Szegedy, 2015) può essere vista come un meccanismo per correggere la distorsione angolare.Esperimenti preliminari mostrano che LCW aiuta ad addestrare un MLP a 100 strati in modo più efficiente rispetto alla normalizzazione batch.
Le reti logiche di Markov (MLN), che combinano elegantemente regole logiche e modelli grafici probabilistici, possono essere utilizzate per affrontare molti problemi di grafi di conoscenza.Tuttavia, l'inferenza in MLN è computazionalmente intensa, rendendo l'applicazione su scala industriale di MLN molto difficile.Negli ultimi anni, le reti neurali a grafo (GNN) sono emerse come strumenti efficienti ed efficaci per problemi di grafi su larga scala.Tuttavia, le GNN non incorporano esplicitamente regole logiche precedenti nei modelli, e possono richiedere molti esempi etichettati per un compito obiettivo. In questo articolo, esploriamo la combinazione di MLNs e GNNs, e usiamo le reti neurali a grafo per l'inferenza variazionale in MLN.Proponiamo una variante GNN, chiamata ExpressGNN, che trova un buon equilibrio tra il potere di rappresentazione e la semplicità del modello.I nostri ampi esperimenti su diversi dataset di riferimento dimostrano che ExpressGNN porta a un ragionamento logico probabilistico efficace ed efficiente.
I metodi di apprendimento per rinforzo (RL) hanno raggiunto importanti progressi in molteplici compiti superando le prestazioni umane.Tuttavia, la maggior parte delle strategie RL mostrano un certo grado di debolezza e possono diventare computazionalmente intrattabili quando si tratta di ambienti ad alta densità e non stazionari.In questo articolo, costruiamo un metodo di apprendimento per rinforzo (MRL) che incorpora una rete neurale adattiva (NN) per un'iterazione efficiente della politica in condizioni di compito mutevoli.Il nostro obiettivo principale è di estendere l'applicazione RL al compito impegnativo della guida autonoma urbana nel simulatore CARLA.
Il principio del collo di bottiglia dell'informazione è un approccio elegante e utile all'apprendimento della rappresentazione.In questo articolo, indaghiamo il problema dell'apprendimento della rappresentazione nel contesto dell'apprendimento di rinforzo usando il quadro del collo di bottiglia dell'informazione, mirando a migliorare l'efficienza del campione degli algoritmi di apprendimento.Deriviamo analiticamente la distribuzione condizionale ottimale della rappresentazione e forniamo un limite inferiore variazionale.Poi, massimizziamo questo limite inferiore con il metodo del gradiente variazionale di Stein (SV). Incorporiamo questo quadro nell'algoritmo critico dell'attore vantaggioso (A2C) e nell'algoritmo di ottimizzazione della politica prossimale (PPO).I nostri risultati sperimentali mostrano che il nostro quadro può migliorare significativamente l'efficienza del campione di vaniglia A2C e PPO.Infine, studiamo la prospettiva dell'information-bottleneck (IB) nella RL profonda con l'algoritmo chiamato mutual information neural estimation(MINE). Verifichiamo sperimentalmente che il processo di estrazione-compressione delle informazioni esiste anche nella RL profonda e il nostro framework è in grado di accelerare questo processo.Analizziamo anche la relazione tra MINE e il nostro metodo, attraverso questa relazione, deriviamo teoricamente un algoritmo per ottimizzare il nostro framework IB senza costruire il lower bound.
 Un aspetto fondamentale dell'intelligenza umana è la capacità di apprendere rapidamente nuovi compiti e di passare da uno all'altro in modo flessibile.Qui, descriviamo un paradigma di apprendimento di rinforzo continuo modulare ispirato a queste capacità.Introduciamo prima un ambiente di interazione visiva che permette a molti tipi di compiti di essere unificati in un unico quadro.Descriviamo poi uno schema di predizione della mappa di ricompensa che apprende nuovi compiti in modo robusto negli spazi di stato e azione molto grandi richiesti da un tale ambiente. Indaghiamo su come le proprietà dell'architettura del modulo influenzino l'efficienza dell'apprendimento dei compiti, mostrando che un motivo del modulo che incorpora principi di progettazione specifici (ad esempio, colli di bottiglia precoci, non linearità polinomiali di basso ordine e simmetria) supera significativamente i motivi delle reti neurali più standard, avendo bisogno di meno esempi di formazione e meno neuroni per raggiungere alti livelli di prestazioni.Infine, presentiamo un'architettura meta-controller per la commutazione dei compiti basata su uno schema di voto neurale dinamico, che consente ai nuovi moduli di utilizzare le informazioni apprese dai compiti visti in precedenza per migliorare sostanzialmente la loro efficienza di apprendimento.
L'interpretabilità e le piccole serie di dati etichettati sono questioni chiave nell'applicazione pratica dell'apprendimento profondo, in particolare in aree come la medicina. In questo articolo, presentiamo una tecnica semi-supervisionata che affronta entrambi questi problemi contemporaneamente. Impariamo rappresentazioni dense da grandi insiemi di dati di immagini non etichettati, quindi utilizziamo tali rappresentazioni sia per imparare classificatori da piccoli insiemi etichettati sia per generare razionali visivi che spiegano le previsioni. Utilizzando la diagnosi della radiografia del torace come applicazione motivante, dimostriamo che il nostro metodo ha una buona capacità di generalizzazione imparando a rappresentare il nostro insieme di dati della radiografia del torace mentre addestriamo un classificatore su un insieme separato da un istituto diverso. Per ogni previsione, generiamo razionali visivi per classificazioni positive ottimizzando una rappresentazione latente per minimizzare la probabilità di malattia mentre è vincolata da una misura di somiglianza nello spazio dell'immagine. la decodifica della rappresentazione latente risultante produce un'immagine senza malattia apparente. la differenza tra l'immagine originale e quella alterata forma un razionale visivo interpretabile per la previsione dell'algoritmo. il nostro metodo produce contemporaneamente razionali visivi che si confrontano favorevolmente con tecniche precedenti e un classificatore che supera lo stato attuale dell'arte.
Le strategie evolutive (ES) sono una famiglia popolare di algoritmi di ottimizzazione zeroth-order black-box che si basano su distribuzioni di ricerca per ottimizzare in modo efficiente una grande varietà di funzioni obiettivo. Questo articolo indaga i potenziali benefici dell'uso di distribuzioni di ricerca altamente flessibili negli algoritmi ES, in contrasto con quelle standard (tipicamente gaussiane). Modelliamo tali distribuzioni con Reti Neurali Generative (GNN) e introduciamo un nuovo algoritmo ES che sfrutta la loro espressività per accelerare la ricerca stocastica. Poiché agisce come un plug-in, il nostro approccio permette di aumentare virtualmente qualsiasi algoritmo ES standard con distribuzioni di ricerca flessibili.dimostriamo i vantaggi empirici di questo metodo su una diversità di funzioni obiettivo.
Proponiamo e valutiamo nuove tecniche per comprimere e accelerare le moltiplicazioni di matrici dense come quelle che si trovano negli strati completamente connessi e ricorrenti delle reti neurali per il riconoscimento vocale continuo incorporato di grande vocabolario (LVCSR).Per la compressione, introduciamo e studiamo una tecnica di regolarizzazione della norma di traccia per la formazione di versioni a basso rango fattorizzate delle moltiplicazioni di matrici. Rispetto all'addestramento standard di basso rango, dimostriamo che il nostro metodo porta a una buona accuratezza rispetto al numero di compromessi dei parametri e può essere utilizzato per accelerare l'addestramento di modelli di grandi dimensioni. Per la velocizzazione, consentiamo un'inferenza più veloce sui processori ARM attraverso nuovi kernel open sourced ottimizzati per le piccole dimensioni dei lotti, con conseguente aumento di velocità da 3x a 7x rispetto alla libreria gemmlowp ampiamente utilizzata.
L'addestramento delle reti neurali quantizzate di attivazione comporta la minimizzazione di una perdita di addestramento costante il cui gradiente svanisce quasi ovunque, che è indesiderabile per la back-propagation standard o la regola della catena, 2013) solo nel passaggio a ritroso, in modo che il "gradiente" attraverso la regola a catena modificata diventi non banale.Poiché questo insolito "gradiente" non è certamente il gradiente della funzione di perdita, si pone la seguente domanda: perché la ricerca nella sua direzione negativa minimizza la perdita di addestramento? In questo articolo, forniamo la giustificazione teorica del concetto di STE rispondendo a questa domanda.Consideriamo il problema dell'apprendimento di una rete a due strati lineari con attivazione ReLU binarizzata e dati di input gaussiani. La scelta dello STE non è unica: dimostriamo che se lo STE è scelto correttamente, il gradiente grossolano previsto correla positivamente con il gradiente della popolazione (non disponibile per l'addestramento), e la sua negazione è una direzione di discesa per la minimizzazione della perdita della popolazione; dimostriamo inoltre che l'algoritmo di discesa del gradiente grossolano associato converge verso un punto critico del problema di minimizzazione della perdita della popolazione.  Inoltre, mostriamo che una cattiva scelta di STE porta all'instabilità dell'algoritmo di addestramento vicino a certi minimi locali, il che è verificato con esperimenti CIFAR-10.
Questo articolo presenta GumbelClip, un insieme di modifiche all'algoritmo actor-critic, per l'apprendimento di rinforzo off-policy.GumbelClip utilizza i concetti di campionamento di importanza troncata insieme al rumore additivo per produrre una funzione di perdita che consente l'uso di campioni off-policy. L'algoritmo modificato raggiunge un aumento della velocità di convergenza e dell'efficienza del campione rispetto agli algoritmi on-policy ed è competitivo con i metodi esistenti di gradiente della politica off-policy, pur essendo significativamente più semplice da implementare.L'efficacia di GumbelClip è dimostrata contro gli algoritmi on-policy e off-policy actor-critic esistenti su un sottoinsieme del dominio Atari.
Negli ultimi anni, sono stati fatti vari progressi nei modelli generativi grazie alla formulazione di Generative Adversarial Networks (GANs).GANs hanno dimostrato di eseguire estremamente bene su un'ampia varietà di compiti relativi alla generazione di immagini e al trasferimento di stile.Nel campo del Natural Language Processing, word embeddings come word2vec e GLoVe sono metodi all'avanguardia per applicare modelli di reti neurali su dati testuali. Questo lavoro presenta un approccio alla generazione del testo usando le embeddings di frase di Skip-Thought in congiunzione con GANs basato sulle funzioni di penalità del gradiente e sulle misure f. I risultati dell'uso delle embeddings di frase con GANs per la generazione del testo condizionato sulle informazioni di input sono comparabili agli approcci dove le embeddings di parola sono usate.
I decodificatori neurali ricorrenti autoregressivi che generano sequenze di token uno per uno e da sinistra a destra sono il cavallo di battaglia della moderna traduzione automatica. In questo lavoro, proponiamo una nuova architettura di decodificatore che può generare sequenze di lingua naturale in un ordine arbitrario. L'architettura di decodifica proposta è completamente compatibile con il framework seq2seq e può essere utilizzata come una sostituzione drop-in di qualsiasi decodificatore classico. Dimostriamo le prestazioni del nostro nuovo decodificatore sul compito di traduzione automatica IWSLT, nonché ispezionare e interpretare i modelli di decodifica appresi analizzando come il modello seleziona nuove posizioni per ogni token successivo.
Sebbene l'approccio basato sull'apprendimento sia inesatto, siamo in grado di generalizzare per contare grandi modelli e grafi di dati in tempo polinomiale rispetto al tempo esponenziale del problema NP-completo originale. Diverso da altri problemi tradizionali di apprendimento del grafico come la classificazione dei nodi e la previsione dei collegamenti, il conteggio degli isomorfismi dei grafi richiede un'inferenza più globale per sorvegliare l'intero grafico. Per affrontare questo problema, proponiamo una rete di memoria di attenzione intermedia dinamica (DIAMNet) che aumenta le diverse architetture di apprendimento della rappresentazione e assiste iterativamente ai grafici di pattern e di dati di destinazione per memorizzare diversi isomorfismi di sottografi per il conteggio globale. Sviluppiamo sia piccoli grafici (<= 1.024 isomorfismi di sottografi in ciascuno) che grandi grafici (<= 4.096 isomorfismi di sottografi in ciascuno) per valutare diversi modelli. I risultati sperimentali mostrano che il conteggio degli isomorfismi di sottografi basato sull'apprendimento può aiutare a ridurre la complessità del tempo con una precisione accettabile.La nostra DIAMNet può migliorare ulteriormente i modelli di apprendimento della rappresentazione esistenti per questo problema più globale.
L'adattamento al dominio è un problema aperto nel deep reinforcement learning (RL).Spesso, agli agenti viene chiesto di esibirsi in ambienti in cui i dati sono difficili da ottenere.In tali impostazioni, gli agenti vengono addestrati in ambienti simili, come i simulatori, e vengono poi trasferiti all'ambiente originale.Il divario tra le osservazioni visive degli ambienti di origine e di destinazione spesso causa il fallimento dell'agente nell'ambiente di destinazione. Presentiamo un nuovo agente RL, SADALA (Soft Attention DisentAngled representation Learning Agent).SADALA impara prima una rappresentazione di stato compressa.Poi impara congiuntamente a ignorare le caratteristiche di distrazione e a risolvere il compito presentato.La separazione di SADALA delle caratteristiche visive importanti e non importanti porta a un trasferimento robusto del dominio.SADALA supera sia i precedenti approcci RL basati sulla rappresentazione disentangled sia quelli basati sulla randomizzazione del dominio in ambienti RL (Visual Cartpole e DeepMind Lab).
La verifica di robustezza che mira a certificare formalmente il comportamento di previsione delle reti neurali è diventata uno strumento importante per comprendere il comportamento di un dato modello e per ottenere garanzie di sicurezza.Tuttavia, i metodi precedenti sono solitamente limitati a reti neurali relativamente semplici.In questo articolo, consideriamo il problema della verifica di robustezza per i trasformatori. I trasformatori hanno strati di auto-attenzione complessi che pongono molte sfide per la verifica, tra cui cross-nonlinearity e cross-position dependency, che non sono stati discussi nel lavoro precedente.Risolviamo queste sfide e sviluppiamo il primo algoritmo di verifica per i trasformatori.I limiti di robustezza certificati calcolati dal nostro metodo sono significativamente più stretti di quelli di Interval Bound Propagation.Questi limiti fanno anche luce sull'interpretazione dei trasformatori in quanto riflettono costantemente l'importanza delle parole nella sentiment analysis.
Negli ultimi anni, l'apprendimento profondo ha avuto un enorme successo in molte applicazioni, ma la nostra comprensione teorica dell'apprendimento profondo, e quindi la capacità di fornire miglioramenti di principio, sembra rimanere indietro. Com'è possibile che la performance di allenamento sia indipendente dalla performance di test? Le reti profonde richiedono infatti una teoria di generalizzazione drasticamente nuova? O ci sono misure basate sui dati di allenamento che sono predittive della performance della rete sui dati futuri? Qui dimostriamo che quando la performance è misurata in modo appropriato, la performance di allenamento è in effetti predittiva della performance attesa, coerentemente con la teoria classica dell'apprendimento automatico.
Il nostro lavoro si basa sulla relazione sinergica tra il controllo locale basato sul modello, l'apprendimento della funzione di valore globale e l'esplorazione. Studiamo come l'ottimizzazione della traiettoria locale possa far fronte agli errori di approssimazione nella funzione di valore e possa stabilizzare e accelerare l'apprendimento della funzione di valore. Infine, dimostriamo anche come l'ottimizzazione della traiettoria può essere usata per eseguire un'esplorazione temporalmente coordinata insieme alla stima dell'incertezza nell'approssimazione della funzione di valore.Questa esplorazione è critica per un apprendimento veloce e stabile della funzione di valore.La combinazione di questi componenti permette di risolvere compiti di controllo complessi, come la locomozione umanoide e la manipolazione abile nella mano, nell'equivalente di pochi minuti di esperienza nel mondo reale.
Le moderne architetture di reti neurali si avvantaggiano di strati sempre più profondi e di vari progressi nella loro struttura per ottenere prestazioni migliori. Mentre le tradizionali tecniche di regolarizzazione esplicita come il dropout, il decadimento dei pesi e l'aumento dei dati sono ancora utilizzati in questi nuovi modelli, poco è stato studiato sugli effetti di regolarizzazione e generalizzazione di queste nuove strutture. Oltre ad essere più profonde dei loro predecessori, le nuove architetture come ResNet e DenseNet potrebbero anche beneficiare delle proprietà implicite di regolarizzazione delle loro strutture? Attraverso gli esperimenti, dimostriamo che alcune architetture di reti neurali contribuiscono alle loro capacità di generalizzazione. In particolare, studiamo l'effetto che le caratteristiche di basso livello hanno sulle prestazioni di generalizzazione quando vengono introdotte in strati più profondi in DenseNet, ResNet e nelle reti con "skip connections", dimostrando che queste rappresentazioni di basso livello aiutano la generalizzazione in diverse impostazioni quando sia la qualità che la quantità dei dati di allenamento sono diminuite.
Una delle sfide nell'addestramento di modelli generativi come il variational auto encoder (VAE) è evitare il collasso posteriore.Quando il generatore ha troppa capacità, è incline a ignorare il codice latente.Questo problema è esacerbato quando il set di dati è piccolo, e la dimensione latente è alta.La radice del problema è l'obiettivo ELBO, in particolare il termine di divergenza Kullback-Leibler (KL) nella funzione obiettivo.Questo articolo propone una nuova funzione obiettivo per sostituire il termine KL con uno che emula l'obiettivo di massima discrepanza media (MMD). Viene anche introdotta una nuova tecnica, chiamata ritaglio latente, che viene usata per controllare la distanza tra i campioni nello spazio latente. Un modello di autocodificatore probabilistico, chiamato $\mu$-VAE, viene progettato e addestrato sui dataset MNIST e MNIST Fashion, usando la nuova funzione obiettivo e si dimostra che supera i modelli addestrati con gli obiettivi ELBO e $\beta$-VAE. Il $\mu$-VAE è meno incline al collasso posteriore, e può generare ricostruzioni e nuovi campioni di buona qualità. Le rappresentazioni latenti apprese dal $mu$-VAE si dimostrano buone e possono essere usate per compiti a valle come la classificazione.  
I modelli generativi basati sulla verosimiglianza sono una risorsa promettente per rilevare gli input out-of-distribution (OOD) che potrebbero compromettere la robustezza o l'affidabilità di un sistema di apprendimento automatico.Tuttavia, le verosimiglianze derivate da tali modelli hanno dimostrato di essere problematiche per rilevare alcuni tipi di input che differiscono significativamente dai dati di allenamento.In questo articolo, poniamo che questo problema sia dovuto all'eccessiva influenza che la complessità degli input ha nelle verosimiglianze dei modelli generativi. Riportiamo una serie di esperimenti che supportano questa ipotesi, e usiamo una stima della complessità dell'input per derivare un punteggio OOD efficiente e privo di parametri, che può essere visto come un rapporto di verosimiglianza, simile al confronto Bayesiano dei modelli. Troviamo che tale punteggio si comporti in modo comparabile, o addirittura migliore, degli approcci esistenti di rilevamento OOD in una vasta gamma di set di dati, modelli, dimensioni del modello e stime della complessità.
 Diversi metodi di ottimizzazione stocastica recentemente proposti che sono stati utilizzati con successo nell'addestramento di reti profonde come RMSProp, Adam, Adadelta, Nadam sono basati sull'utilizzo di aggiornamenti del gradiente scalati dalle radici quadrate delle medie mobili esponenziali dei gradienti passati al quadrato.In molte applicazioni, ad esempio l'apprendimento con grandi spazi di uscita, è stato empiricamente osservato che questi algoritmi non riescono a convergere verso una soluzione ottimale (o un punto critico in ambienti non convessi).Mostriamo che una causa di tali fallimenti è la media mobile esponenziale utilizzata negli algoritmi. La nostra analisi suggerisce che i problemi di convergenza possono essere risolti dotando tali algoritmi di una "memoria a lungo termine" dei gradienti passati, e proponiamo nuove varianti dell'algoritmo Adam che non solo risolvono i problemi di convergenza ma spesso portano anche a migliori prestazioni empiriche.
L'avvelenamento mirato di clean-label è un tipo di attacco avversario ai sistemi di apprendimento automatico in cui l'avversario inietta alcuni campioni correttamente etichettati e minimamente perturbati nei dati di addestramento, inducendo così il modello implementato a sbagliare la classificazione di un particolare campione di prova durante l'inferenza.Sebbene siano state proposte difese per gli attacchi generali di avvelenamento (quelli che mirano a ridurre la precisione complessiva del test), non è stata dimostrata alcuna difesa affidabile per gli attacchi clean-label, nonostante l'efficacia degli attacchi e i loro casi d'uso realistici.In questo lavoro, proponiamo un insieme di difese semplici, ma altamente efficaci contro questi attacchi. Dopo aver riprodotto i loro esperimenti, dimostriamo che le nostre difese sono in grado di rilevare oltre il 99% degli esempi di avvelenamento in entrambi gli attacchi e di rimuoverli senza alcun compromesso sulle prestazioni del modello. Le nostre semplici difese dimostrano che le attuali strategie di attacco clean-label poisoning possono essere annullate, e servono come una difesa di base forte ma semplice da implementare per la quale testare futuri attacchi clean-label poisoning.
Il ragionamento astratto, in particolare nel dominio visivo, è un'abilità umana complessa, ma rimane un problema impegnativo per i sistemi di apprendimento neurale artificiale. In questo lavoro proponiamo MXGNet, una rete neurale a grafo multistrato per compiti di ragionamento diagrammatico a più pannelli. MXGNet combina tre concetti potenti, ovvero la rappresentazione a livello di oggetto, le reti neurali a grafo e i grafi multipli, per risolvere compiti di ragionamento visivo. MXGNet estrae prima le rappresentazioni a livello di oggetto per ogni elemento in tutti i pannelli dei diagrammi, e poi forma un grafo multiplo multistrato che cattura le relazioni multiple tra gli oggetti nei diversi pannelli del diagramma. MXGNet riassume i grafici multipli estratti dai diagrammi del compito e usa questa sintesi per scegliere la risposta più probabile tra i candidati dati. Abbiamo testato MXGNet su due tipi di compiti di ragionamento diagrammatico, cioè i sillogismi dei diagrammi e le matrici progressive di Raven (RPM). Per un compito di sillogismo del diagramma di Eulero, MXGNet raggiunge una precisione del 99,8%.  Per PGM e RAVEN, due set di dati completi per il ragionamento RPM, MXGNet supera i modelli allo stato dell'arte con un margine considerevole.
L'estrazione della struttura semantica per i fogli di calcolo include il rilevamento delle regioni della tabella, il riconoscimento dei componenti strutturali e la classificazione dei tipi di cella.L'estrazione automatica della struttura semantica è la chiave per la trasformazione automatica dei dati da varie strutture di tabella in schema canonico in modo da consentire l'analisi dei dati e la scoperta della conoscenza.Tuttavia, sono sfidati dalle diverse strutture di tabella e dalla semantica spaziale-correlata sulle griglie delle cellule.Per imparare le correlazioni spaziali e catturare la semantica sui fogli di calcolo, abbiamo sviluppato un nuovo quadro basato sull'apprendimento per l'estrazione della struttura semantica dei fogli di calcolo. In primo luogo, proponiamo una struttura multi-task che impara la regione della tabella, i componenti strutturali e i tipi di cella congiuntamente; in secondo luogo, sfruttiamo i progressi del recente modello linguistico per catturare la semantica in ogni valore di cella; in terzo luogo, costruiamo un grande set di dati etichettati dall'uomo con un'ampia copertura delle strutture della tabella.La nostra valutazione mostra che la struttura multi-task proposta è altamente efficace e supera i risultati della formazione di ogni compito separatamente.
Il confronto di questi metodi richiede un mezzo olistico per la valutazione dei dialoghi.Le valutazioni umane sono considerate il gold standard.Poiché la valutazione umana è inefficiente e costosa, è auspicabile un sostituto automatico.In questo articolo, proponiamo una metrica di valutazione olistica che cattura sia la qualità che la diversità dei dialoghi. La nostra metrica consiste in (1) coerenza di contesto basata su GPT-2 tra le frasi in un dialogo, (2) fluenza basata su GPT-2 nel fraseggio, e (3) diversità basata su $n$-grammi nelle risposte alle query aumentate. La validità empirica della nostra metrica è dimostrata da una forte correlazione con i giudizi umani.
La rete neurale di convoluzione (CNN) ha ottenuto un enorme successo nei compiti di computer vision con la sua eccezionale capacità di catturare le caratteristiche latenti locali.Recentemente, c'è stato un crescente interesse nell'estendere le CNN al dominio spaziale generale.Sebbene siano stati proposti vari tipi di convoluzione grafica e metodi di convoluzione geometrica, i loro collegamenti alla tradizionale convoluzione 2D non sono ben compresi. In questo documento, mostriamo che la convoluzione separabile depthwise è un percorso per unificare i due tipi di metodi di convoluzione in una visione matematica, sulla base della quale deriviamo un nuovo Depthwise Separable Graph Convolution che sussume i metodi di convoluzione grafici esistenti come casi speciali della nostra formulazione.
Generare audio musicale direttamente con le reti neurali è notoriamente difficile perché richiede la modellazione coerente della struttura su molte scale temporali diverse. Fortunatamente, la maggior parte della musica è anche altamente strutturata e può essere rappresentata come eventi di note discrete suonate su strumenti musicali. 1 ms a ~ 100 s), un processo che chiamiamo Wave2Midi2Wave.Questo grande progresso nello stato dell'arte è abilitato dal nostro rilascio del nuovo set di dati MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization), composto da oltre 172 ore di performance virtuosistiche di pianoforte catturate con allineamento fine (~ 3 ms) tra le etichette delle note e le forme d'onda audio.Le reti e il set di dati insieme presentano un approccio promettente verso la creazione di nuovi modelli neurali espressivi e interpretabili di musica.
L'inferenza variazionale basata sulla minimizzazione della divergenza del chi-quadrato (CHIVI) fornisce un modo per approssimare il posteriore di un modello mentre si ottiene un limite superiore della verosimiglianza marginale. Tuttavia, in pratica CHIVI si basa su stime Monte Carlo (MC) di un obiettivo superiore che a modeste dimensioni del campione non sono garantite per essere veri limiti della verosimiglianza marginale. Questo articolo fornisce uno studio empirico delle prestazioni di CHIVI su una serie di compiti sintetici di inferenza.Mostriamo che CHIVI è molto più sensibile all'inizializzazione rispetto al classico VI basato sulla minimizzazione KL, spesso ha bisogno di un numero molto grande di campioni (oltre un milione), e può non essere un limite superiore affidabile.Suggeriamo anche possibili modi per rilevare e alleviare alcune di queste patologie, compresi i limiti diagnostici e strategie di inizializzazione.
È stato ampiamente riconosciuto che gli esempi avversari possono essere facilmente realizzati per ingannare le reti profonde, che principalmente radicano dal comportamento localmente non lineare vicino agli esempi di input.Applicare il mixup nella formazione fornisce un meccanismo efficace per migliorare le prestazioni di generalizzazione e la robustezza del modello contro le perturbazioni avversarie, che introduce il comportamento globalmente lineare tra gli esempi di formazione.Tuttavia, nel lavoro precedente, i modelli mixup-allenati solo passivamente difendere gli attacchi avversari nell'inferenza classificando direttamente gli ingressi, dove la linearità globale indotta non è ben sfruttata. Vale a dire, dal momento che la località delle perturbazioni avversarie, sarebbe più efficiente per rompere attivamente la località attraverso la globalità delle previsioni del modello.Ispirato da una semplice intuizione geometrica, sviluppiamo un principio di inferenza, chiamato mixup inference (MI), per i modelli mixup-trained. I nostri esperimenti su CIFAR-10 e CIFAR-100 dimostrano che MI può migliorare ulteriormente la robustezza avversaria per i modelli addestrati da mixup e le sue varianti.
La messa a punto di modelli linguistici, come il BERT, su corpora specifici di un dominio ha dimostrato di essere preziosa in domini come gli articoli scientifici e i testi biomedici.In questo articolo, dimostriamo che la messa a punto del BERT su documenti legali fornisce allo stesso modo miglioramenti preziosi su compiti NLP nel dominio legale.Dimostrare questo risultato è significativo per l'analisi di accordi commerciali, perché ottenere grandi corpora legali è difficile a causa della loro natura riservata.Come tale, dimostriamo che avere accesso a grandi corpora legali è un vantaggio competitivo per applicazioni commerciali, e la ricerca accademica sull'analisi dei contratti.
Presentiamo un modello generativo profondo per il trasferimento non supervisionato dello stile del testo che unifica le tecniche non generative precedentemente proposte.Il nostro approccio probabilistico modella i dati non paralleli di due domini come un corpus parallelo parzialmente osservato.Ipotizzando una sequenza latente parallela che genera ogni sequenza osservata, il nostro modello impara a trasformare le sequenze da un dominio all'altro in modo completamente non supervisionato. In contrasto con i tradizionali modelli generativi di sequenze (ad esempio l'HMM), il nostro modello fa poche assunzioni sui dati che genera: utilizza un modello linguistico ricorrente come priore e un codificatore-decodificatore come distribuzione di trasduzione.Mentre il calcolo della likelihood marginale dei dati è intrattabile in questa classe di modelli, mostriamo che l'inferenza variazionale ammortizzata ammette un surrogato pratico. Inoltre, tracciando i collegamenti tra il nostro obiettivo variazionale e altre recenti tecniche di traduzione automatica e di trasferimento di stile non supervisionato, mostriamo come la nostra visione probabilistica possa unificare alcuni obiettivi non generativi noti, come la backtranslation e la perdita avversaria.Infine, dimostriamo l'efficacia del nostro metodo su una vasta gamma di compiti di trasferimento di stile non supervisionato, tra cui il trasferimento del sentimento, il trasferimento della formalità, la decifrazione delle parole, l'imitazione dell'autore e la traduzione delle lingue correlate. In tutti i compiti di trasferimento di stile, il nostro approccio produce guadagni sostanziali rispetto alle basi non generative allo stato dell'arte, comprese le tecniche di traduzione automatica non supervisionata allo stato dell'arte che il nostro approccio generalizza.
La pratica attuale nell'apprendimento automatico consiste nell'impiegare reti profonde in un limite iperparametrizzato, con il numero nominale di parametri che tipicamente supera il numero di misurazioni. Questo assomiglia alla situazione nel rilevamento compresso, o nella regressione sparsa con termini di penalità $l_1$, e fornisce un percorso teorico per comprendere i fenomeni che sorgono nel contesto delle reti profonde. Uno di questi fenomeni è il successo delle reti profonde nel fornire una buona generalizzazione in un regime di interpolazione con errore di addestramento nullo.La pratica statistica tradizionale richiede la regolarizzazione o lo smussamento per prevenire l'"overfitting" (scarse prestazioni di generalizzazione).Tuttavia, lavori recenti mostrano che esistono procedure di interpolazione dei dati che sono statisticamente coerenti e forniscono buone prestazioni di generalizzazione\cite{belkin2018overfitting} (In questo contesto, è stato suggerito che i regimi "classici" e "moderni" per l'apprendimento automatico sono separati da un picco nella curva dell'errore di generalizzazione ("rischio"), un fenomeno soprannominato "doppia discesa"\cite{belkin2019reconciling}.Mentre tali picchi di overfitting esistono e nascono da matrici di progettazione mal condizionate, qui sfidiamo l'interpretazione del picco di overfitting come demarcazione del regime in cui si verifica una buona generalizzazione sotto overparametrizzazione. Proponiamo un modello di Misparamatrized Sparse Regression (MiSpaR) e calcoliamo analiticamente le curve GE per $l_2$ e $l_1$. Mostriamo che il picco di overfitting che si verifica nel limite di interpolazione è dissociato dal regime di buona generalizzazione. Le espressioni analitiche sono ottenute nel cosiddetto limite "termodinamico". Troviamo un ulteriore fenomeno interessante: aumentando l'iperparametrizzazione nel modello di fitting aumenta la sparsità, che dovrebbe intuitivamente migliorare le prestazioni della regressione penalizzata $l_1$. Tuttavia, allo stesso tempo, il numero relativo di misurazioni diminuisce rispetto al numero di parametri di adattamento, e alla fine l'iperparametrizzazione porta a una scarsa generalizzazione.Tuttavia, la regressione penalizzata $l_1$ può mostrare buone prestazioni di generalizzazione in condizioni di interpolazione dei dati anche con una grande quantità di iperparametrizzazione.Questi risultati forniscono un percorso teorico per studiare i problemi inversi nel regime interpolante utilizzando funzioni di adattamento iperparametrizzate come le reti profonde.
Il filtraggio collaborativo basato sull'hashing impara rappresentazioni vettoriali binarie (codici hash) di utenti e oggetti, in modo che le raccomandazioni possano essere calcolate in modo molto efficiente usando la distanza di Hamming, che è semplicemente la somma di bit diversi tra due codici hash.Un problema con il filtraggio collaborativo basato sull'hashing usando la distanza di Hamming, è che ogni bit è ugualmente pesato nel calcolo della distanza, ma in pratica alcuni bit potrebbero codificare proprietà più importanti di altri bit, dove l'importanza dipende dall'utente. A tal fine, proponiamo un approccio di filtraggio collaborativo basato sull'hashing variazionale addestrabile end-to-end che utilizza il nuovo concetto di auto-mascheramento: il codice hash dell'utente agisce come una maschera sugli elementi (utilizzando l'operazione booleana AND), in modo da imparare a codificare quali bit sono importanti per l'utente, piuttosto che la preferenza dell'utente verso la proprietà sottostante dell'elemento che i bit rappresentano. Questo permette una ponderazione binaria dell'importanza a livello di utente di ogni elemento senza la necessità di memorizzare pesi aggiuntivi per ogni utente. Valutiamo sperimentalmente il nostro approccio contro le linee di base allo stato dell'arte su 4 set di dati, e otteniamo guadagni significativi fino al 12% in NDCG.Rendiamo anche disponibile un'implementazione efficiente dell'automascheramento, che sperimentalmente produce <4% di overhead di runtime rispetto alla distanza standard di Hamming.
Determinare la dimensione appropriata dei lotti per la discesa del gradiente mini-batch è sempre dispendioso in termini di tempo, poiché spesso si basa sulla ricerca a griglia. Questo articolo considera un algoritmo di discesa del gradiente mini-batch ridimensionabile (RMGD) basato su un bandito multi-armato che raggiunge prestazioni equivalenti a quelle della migliore dimensione fissa dei lotti. ad ogni epoca, il RMGD campiona una dimensione del lotto secondo una certa distribuzione di probabilità proporzionale al successo di un lotto nel ridurre la funzione di perdita. il campionamento da questa probabilità fornisce un meccanismo per esplorare diverse dimensioni dei lotti e sfruttare le dimensioni dei lotti con storia di successo.  Dopo aver ottenuto la perdita di convalida ad ogni epoca con la dimensione del lotto campionata, la distribuzione di probabilità viene aggiornata per incorporare l'efficacia della dimensione del lotto campionata.I risultati sperimentali mostrano che RMGD raggiunge prestazioni migliori della dimensione del lotto singolo più performante.È sorprendente che RMGD raggiunga prestazioni migliori della ricerca a griglia.Inoltre, raggiunge queste prestazioni in un tempo più breve della ricerca a griglia.
Il grafo della conoscenza ha guadagnato l'attenzione crescente negli ultimi anni per le sue applicazioni di successo di numerosi compiti.Nonostante la rapida crescita della costruzione della conoscenza, i grafi della conoscenza soffrono ancora di una grave incompletezza e inevitabilmente coinvolgono vari tipi di errori.Diversi tentativi sono stati fatti per completare il grafo della conoscenza così come per rilevare il rumore. Tuttavia, nessuno di loro considera l'unificazione di questi due compiti anche se sono interdipendenti e possono reciprocamente aumentare le prestazioni di ogni altro.In questo documento, abbiamo proposto di combinare congiuntamente questi due compiti con una struttura unificata Generative Adversarial Networks (GAN) per imparare l'incorporamento del grafico di conoscenza consapevole del rumore.esperimenti estesi hanno dimostrato che il nostro approccio è superiore agli algoritmi esistenti dello stato dell'arte sia per quanto riguarda il completamento del grafico di conoscenza che la rilevazione degli errori.
I modelli basati sull'energia (EBMs), a.k.a.un-normalized models, hanno avuto recenti successi negli spazi continui, ma non sono stati applicati con successo per modellare sequenze di testo.  Mentre diminuire l'energia ai campioni di allenamento è semplice, estrarre campioni (negativi) dove l'energia dovrebbe essere aumentata è difficile.   In parte, questo è dovuto al fatto che i metodi standard basati sul gradiente non sono facilmente applicabili quando l'input è altamente dimensionale e discreto.  Qui, aggiriamo questo problema generando i negativi usando modelli linguistici auto-regressivi pre-addestrati.  L'EBM lavora quindi nel residuo del modello linguistico ed è addestrato a discriminare il testo reale dal testo generato dai modelli auto-regressivi. Studiamo la capacità di generalizzazione degli EBM residui, un prerequisito per utilizzarli in altre applicazioni.  Analizziamo ampiamente la generalizzazione per il compito di classificare se un input è generato dalla macchina o dall'uomo, un compito naturale data la perdita di formazione e il modo in cui estraiamo i negativi.Nel complesso, osserviamo che le EBM possono generalizzare notevolmente bene ai cambiamenti nell'architettura dei generatori che producono i negativi.Tuttavia, le EBM mostrano una maggiore sensibilità al set di formazione utilizzato da tali generatori.
L'apprendimento semi-supervisionato, cioè l'apprendimento congiunto da campioni etichettati e non etichettati, è un argomento di ricerca attivo a causa del suo ruolo chiave nel rilassare i vincoli di annotazione umana. Nel contesto della classificazione delle immagini, i recenti progressi per imparare da campioni non etichettati sono principalmente focalizzati sui metodi di regolarizzazione della coerenza che incoraggiano previsioni invarianti per diverse perturbazioni dei campioni non etichettati. Dimostriamo che una pseudo-etichettatura ingenua si adatta eccessivamente a pseudo-etichette errate a causa del cosiddetto bias di conferma e dimostriamo che l'aumento del mixup e l'impostazione di un numero minimo di campioni etichettati per mini-batch sono tecniche di regolarizzazione efficaci per ridurlo.L'approccio proposto raggiunge risultati allo stato dell'arte in CIFAR-10/100 e Mini-ImageNet nonostante sia molto più semplice di altri state-of-the-art.Questi risultati dimostrano che la pseudo-etichettatura può superare i metodi di regolarizzazione della coerenza, mentre il contrario è stato supposto nel lavoro precedente.Il codice sarà reso disponibile.
L'apprendimento di rinforzo senza modello (RL) ha dimostrato di essere uno strumento potente e generale per l'apprendimento di comportamenti complessi.Tuttavia, la sua efficienza del campione è spesso impraticabilmente grande per la risoluzione di problemi impegnativi del mondo reale, anche per algoritmi off-policy come Q-learning. Un fattore limitante nella classica RL senza modello è che il segnale di apprendimento consiste solo di ricompense scalari, ignorando molte delle ricche informazioni contenute nelle tuple di transizione di stato. La RL basata sul modello usa queste informazioni, addestrando un modello predittivo, ma spesso non raggiunge la stessa performance asintotica della RL senza modello a causa della distorsione del modello. I TDM combinano i vantaggi della RL senza modello e basata sul modello: sfruttano le ricche informazioni nelle transizioni di stato per imparare in modo molto efficiente, mentre raggiungono ancora prestazioni asintotiche che superano quelle dei metodi RL diretti basati sul modello. I nostri risultati sperimentali mostrano che, su una serie di compiti di controllo continuo, i TDM forniscono un sostanziale miglioramento dell'efficienza rispetto ai metodi basati sul modello e senza modello.
Introduciamo un'architettura neurale per eseguire un'inferenza bayesiana approssimativa ammortizzata su permutazioni casuali latenti di due insiemi di oggetti.Il metodo comporta l'approssimazione dei permanenti delle matrici di probabilità a coppie utilizzando idee recenti sulle funzioni definite su insiemi.Ogni permutazione campionata è dotata di una stima di probabilità, una quantità non disponibile negli approcci MCMC.Illustriamo il metodo in insiemi di punti 2D e immagini MNIST.
I sistemi di reperimento su larga scala con apprendimento automatico richiedono una grande quantità di dati di addestramento che rappresentino la rilevanza della domanda-voce.Tuttavia, raccogliere il feedback esplicito degli utenti è costoso.In questo articolo, proponiamo di sfruttare i log degli utenti e il feedback implicito come obiettivi ausiliari per migliorare la modellazione della rilevanza nei sistemi di reperimento. In particolare, adottiamo un'architettura a rete neurale a due torri per modellare la rilevanza delle query e degli articoli, tenendo conto delle informazioni collaborative e di contenuto. Introducendo compiti ausiliari addestrati con dati di feedback impliciti degli utenti molto più ricchi, miglioriamo la qualità e la risoluzione delle rappresentazioni apprese delle query e degli articoli, applicando queste rappresentazioni apprese a un sistema di retrieval industriale e ottenendo miglioramenti significativi.
La capacitÃ di esplorare e navigare autonomamente uno spazio fisico Ã¨ un requisito fondamentale per virtualmente tutto l'agente autonomo mobile, dagli aspirapolveri robotici domestici ai veicoli autonomi. Gli approcci basati SLAM tradizionali per esplorazione e navigazione in gran parte si concentrano sulla geometria della scena di leveraging, ma non riescono a modellare gli oggetti dinamici (quali altri agenti) o i vincoli semantici (quali i pavimenti bagnati o i doorways). In questo articolo, combiniamo il meglio di entrambi i mondi con un approccio modulare che impara una rappresentazione spaziale di una scena che è addestrata per essere efficace quando accoppiata con pianificatori geometrici tradizionali. In particolare, progettiamo un agente che impara a prevedere una mappa di affordance spaziale che chiarisce quali parti di una scena sono navigabili attraverso la raccolta attiva di esperienza auto-supervisionata. In contrasto con la maggior parte degli ambienti di simulazione che presuppongono un mondo statico, valutiamo il nostro approccio nel simulatore VizDoom, utilizzando mappe su larga scala generate in modo casuale che contengono una varietà di attori dinamici e pericoli. Mostriamo che le mappe di affordance apprese possono essere utilizzate per aumentare gli approcci tradizionali sia per l'esplorazione che per la navigazione, fornendo significativi miglioramenti nelle prestazioni.
