{"source": "Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.   FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.", "target": ["FearNet是一个记忆效率高的神经网络，其灵感来自于哺乳动物大脑中的记忆形成，能够在没有灾难性遗忘的情况下进行增量类学习。", "本文提出了一种基于双存储器系统的增量分类问题的新解决方案。"]}
{"source": "Multi-view learning can provide self-supervision when different views are available of the same data. Distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora. Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we present two multi-view frameworks for learning sentence representations in an unsupervised fashion. One framework uses a generative objective and the other a discriminative one. In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model. We show that, after learning, the vectors produced by our multi-view frameworks provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view and demonstrates solid transferability on standard downstream tasks.", "target": ["多视角学习改进了无监督的句子表征学习。", "该方法使用不同的、互补的输入句子编码器和共识最大化。", "本文提出了一个多视角框架，使用生成性和鉴别性的目标架构，来改善NLP任务中的句子表征。", "本文表明，在学习句子表征方面，多视角框架比使用单个编码器更有效。"]}
{"source": "We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.   More precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.   Applying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion. During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.   This gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization.", "target": ["我们展示了如何以无监督的方式从像素中学习离散的对象，以及如何使用这种对象表征进行强化学习。", "一种从像素中学习对象表征以进行强化学习的方法。", "本文提出了一个神经架构，利用无监督像素重建损失，将视频流映射到离散的对象集合，而无需人工注释。"]}
{"source": "Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs). Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels. Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition. We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived \"top-down\" attention maps. Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than \"bottom-up\" saliency features for rapid image categorization. As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers.", "target": ["用于训练用于对象识别的注意力模型的大规模数据集可以实现更准确、可解释和类似人类的对象识别。", "论证了视觉识别的最新进展源于在深度卷积网络中使用视觉注意力机制，该机制通过基于图像类标签的弱监督形式来学习哪里是重点。", "提出了一个关于注意力的新方法，其中收集了大型注意力数据集，并用于以监督的方式训练NN，以利用自我报告的人类注意力。", "本文提出了一种新的方法，利用信息量更大的信号，特别是人类认为图像上重要的区域，来改进深度卷积神经网络。"]}
{"source": "In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks. However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.   However, this robustness is only effective tothe same attack method used for adversarial training.   Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.    However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques. In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.   We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.", "target": ["我们提出了一种针对一步式和迭代式对抗性攻击的省时防御方法。", "提出一种名为e2SAD的新的、计算效率高的方法，该方法为每个干净的训练样本生成两套训练对抗样本。", "本文介绍了一种两步对抗性防御方法，为每个干净的样本生成两个对抗性示例，并将其纳入实际的训练循环，以实现稳健性，并声称它可以胜过更昂贵的迭代方法。", "本文提出了一种两步法，与最近的迭代式多步骤对抗性攻击相比，以较少的成本产生强大的对抗性示例。"]}
{"source": "Recently several different deep learning architectures have been proposed that take a string of characters as the raw input signal and automatically derive features for text classification. Little studies are available that compare the effectiveness of these approaches for character based text classification with each other. In this paper we perform such an empirical comparison for the important cybersecurity problem of DGA detection: classifying domain names as either benign vs. produced by malware (i.e., by a Domain Generation Algorithm). Training and evaluating on a dataset with 2M domain names shows that there is surprisingly little difference between various convolutional neural network (CNN) and recurrent neural network (RNN) based architectures in terms of accuracy, prompting a preference for the simpler architectures, since they are faster to train and less prone to overfitting.", "target": ["对五种用于检测恶意域名的深度神经网络架构的比较，发现差异令人惊讶的小。", "作者提出使用五种深度架构进行网络安全任务的领域生成算法检测。", "应用多个NN架构，在良性和恶意软件相关的URL之间对URL进行分类。", "本文提出通过使用经过训练的深度网络直接将字符序列分类为恶意或是良性，来自动识别域名是恶意的还是良性的。"]}
{"source": "Recognizing the relationship between two texts is an important aspect of natural language understanding (NLU), and a variety of neural network models have been proposed for solving NLU tasks. Unfortunately, recent work showed that the datasets these models are trained on often contain biases that allow models to achieve non-trivial performance without possibly learning the relationship between the two texts. We propose a framework for building robust models by using adversarial learning to encourage models to learn latent, bias-free representations. We test our approach in a Natural Language Inference (NLI) scenario, and show that our adversarially-trained models learn robust representations that ignore known dataset-specific biases. Our experiments demonstrate that our models are more robust to new NLI datasets.", "target": ["对抗性学习方法鼓励NLI模型忽略数据集的特定偏差，并帮助模型跨数据集转移。", "本文提出了一种对抗性设置，以减轻自然语言推理数据中的注释artifacts。", "本文提出了一种通过对抗性训练目标来消除文本蕴含模型的偏差的方法。"]}
{"source": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.", "target": ["一种最先进的知识图谱嵌入新方法。", "提出了一个可以推断出知识库中关系的对称性、反对称性、反转性和组成模式的神经链接预测评分函数。", "本文提出了一种通过将关系建模为复向量空间中的旋转来进行知识图谱嵌入的方法。", "提出了一种用于链接预测的图谱嵌入方法。"]}
{"source": "Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification. This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks. However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms. In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels. To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps. Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels. Thereby, weights and kernels are collectively optimized for learning of image classification models robust to  adversarial attacks without employment of additional target detection and rejection algorithms.  We empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack (Narodytska & Kasiviswanathan, 2016).", "target": ["我们使用超网络对CNN进行了修改，并观察到对对抗性例子有更好的稳健性。", "通过使用依赖数据的卷积核提高深度卷积神经网络的稳健性和可靠性。"]}
{"source": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.  Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.  Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.  In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.  The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.  This requires back-propagating errors through the solver steps.  While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.  We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.  Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "target": ["我们提出了一种用于小样本分类的元学习方法，该方法通过快速求解器（例如岭回归或逻辑回归）反向传播，从而在高速下实现强大的性能。", "本文提出了一种元学习的算法，相当于固定了特征（即深度NN的所有隐藏层），并将每个任务视为有自己的最终层，可以是岭回归或逻辑回归。", "本文针对小样本分类问题提出了一种元学习方法，他们使用了一种基于闭式求解器对每个任务的学习者进行参数化的方法。"]}
{"source": "While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback). To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available. To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask. At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class. Each answer eliminates some classes, leaving the learner with a partial label. The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled. Active learning with partial labels requires (i) a sampling strategy to choose (example, class) pairs, and (ii) learning from partial labels between rounds. Experiments on Tiny ImageNet demonstrate that our most effective method improves 26% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30% of the annotation budget that would be required (naively) to annotate the dataset. Moreover, ALPF-learners fully annotate TinyImageNet at 42% lower cost. Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples.", "target": ["我们为在分层标签设置中从头开始训练机器学习模型提供了一个新的视角，即把它看作是人和算法之间的双向交流，并研究我们如何既能衡量和提高效率。", "引入了一个新的主动学习设置，其中 oracle 提供部分或弱标签，而不是查询特定示例的标签，从而简化信息检索。", "本文提出了一种带有部分反馈的主动学习方法，该方法在有限的预算下优于现有基线。", "本文考虑了一个多类分类问题，其中标签被分组在给定数量的M个子集中，这些子集包含所有单个标签的单例。"]}
{"source": "Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions. Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric. Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures. We propose to exploit this flexibility by learning an embedding that captures the semantic information in the Wasserstein distance between embedded distributions. We examine empirically the representational capacity of such learned Wasserstein embeddings, showing that they can embed a wide variety of complex metric structures with smaller distortion than an equivalent Euclidean embedding. We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: we can directly visualize the high-dimensional embedding, as it is a probability distribution on a low-dimensional space. This obviates the need for dimensionality reduction techniques such as t-SNE for visualization.", "target": ["我们表明，Wasserstein空间是嵌入具有复杂语义结构的数据的良好目标。", "使用Wasserstein距离的最小化、正规化版本，在离散的概率分布空间中学习嵌入。", "本文描述了一种新的嵌入方法，该方法将数据嵌入到具有Wasserstein距离的概率测量空间。", "本文提出将数据嵌入到低维的Wasserstein空间，这样可以更准确地捕捉数据的基本结构。"]}
{"source": "Clustering high-dimensional datasets is hard because interpoint distances become less informative in high-dimensional spaces. We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly. The data is embedded into a lower-dimensional space by a deep autoencoder. The autoencoder is optimized as part of the clustering process. The resulting network produces clustered data. The presented approach does not rely on prior knowledge of the number of ground-truth clusters. Joint nonlinear dimensionality reduction and clustering are formulated as optimization of a global continuous objective. We thus avoid discrete reconfigurations of the objective that characterize prior clustering algorithms. Experiments on datasets from multiple domains demonstrate that the presented algorithm outperforms state-of-the-art clustering schemes, including recent methods that use deep networks.", "target": ["通过优化全局连续目标，进行联合非线性降维和聚类的一种聚类算法。", "提出了一种通过联合解决深度自动编码器和聚类作为全局连续目标的聚类算法，呈现出比最先进的聚类方案更好的结果。", "深度连续聚类是一种聚类方法，它将自动编码器目标与聚类目标集成，然后使用 SGD 进行训练。"]}
{"source": "Deep convolutional neural networks (CNNs) are deployed in various applications but demand immense computational requirements. Pruning techniques and Winograd convolution are two typical methods to reduce the CNN computation. However, they cannot be directly combined because Winograd transformation fills in the sparsity resulting from pruning. Li et al. (2017) propose sparse Winograd convolution in which weights are directly pruned in the Winograd domain, but this technique is not very practical because Winograd-domain retraining requires low learning rates and hence significantly longer training time. Besides, Liu et al. (2018) move the ReLU function into the Winograd domain, which can help increase the weight sparsity but requires changes in the network structure. To achieve a high Winograd-domain weight sparsity without changing network structures, we propose a new pruning method, spatial-Winograd pruning. As the first step, spatial-domain weights are pruned in a structured way, which efficiently transfers the spatial-domain sparsity into the Winograd domain and avoids Winograd-domain retraining. For the next step, we also perform pruning and retraining directly in the Winograd domain but propose to use an importance factor matrix to adjust weight importance and weight gradients. This adjustment makes it possible to effectively retrain the pruned Winograd-domain network without changing the network structure. For the three models on the datasets of CIFAR-10, CIFAR-100, and ImageNet, our proposed method can achieve the Winograd-domain sparsities of 63%, 50%, and 74%, respectively.", "target": ["为了加速卷积神经网络的计算，我们提出了一种新的两步修剪技术，在不改变网络结构的情况下实现了更高的Winograd域权重稀疏度。", "提出了一个空间-Winograd修剪框架，允许在Winograd域中保留空间域的修剪权重，并改善Winograd域的稀疏性。", "提出了两种使用Winograd算法的卷积层修剪技术。"]}
{"source": "In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. We develop a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to train local neural network weights, which are modeled through our framework. We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision or data pooling. We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.", "target": ["我们提出了一个用于神经网络联合学习的贝叶斯非参数模型。", "使用β过程来做联合神经匹配。", "本文考虑了神经网络的联合学习，其中数据分布在多台机器上，数据点的分配可能是不均匀和不平衡的。"]}
{"source": "We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.", "target": ["训练具有深度神经网络参数的表达性MCMC核的一般方法。给定一个目标分布p，我们的方法提供一个快速混合采样器，能够有效地探索状态空间。", "提出了一种一般的HMC，通过使用神经网络修改跃迁积分器，使采样器快速收敛和混合。"]}
{"source": "This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation. To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach. Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities. The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization. To solve this we propose a continuation approach that learns failure modes in related but less robust agents. Our approach also allows reuse of data already collected for training the agent. We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving. Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.", "target": ["我们表明，罕见但灾难性的故障可能完全被随机测试所遗漏，这给安全部署带来了问题。我们提出的对抗性测试方法可以解决这个问题。", "提出了一种方法为学习型代理学习失败概率预测器，从而预测哪些初始状态会导致系统失败。", "本文提出了一种基于神经网络对代理训练过程中发生的故障进行学习的函数，对RL算法的失败案例进行采样的重要性抽样方法。", "本文提出了一种对抗性方法来识别强化学习中的灾难性失败案例。"]}
{"source": "The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as \"posterior collapse\" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.", "target": ["为了解决VAE的后验塌陷问题，我们提出了一个新颖而简单的训练程序，通过更多的更新积极优化推理网络。这种新的训练程序减轻了后验塌陷，得到更好的VAE模型。", "研究了后验塌陷的现象，表明增加推理网络的训练可以减少问题并得到更好的优化。", "作者提出改变VAE的训练过程，仅作为后验塌陷的解决方案，而模型和目标保持不变。"]}
{"source": "Online healthcare services can provide the general public with ubiquitous access to medical knowledge and reduce the information access cost for both individuals and societies. To promote these benefits, it is desired to effectively expand the scale of high-quality yet novel relational medical entity pairs that embody rich medical knowledge in a structured form. To fulfill this goal, we introduce a generative model called Conditional Relationship Variational Autoencoder (CRVAE), which can discover meaningful and novel relational medical entity pairs without the requirement of additional external knowledge. Rather than discriminatively identifying the relationship between two given medical entities in a free-text corpus, we directly model and understand medical relationships from diversely expressed medical entity pairs. The proposed model introduces the generative modeling capacity of variational autoencoder to entity pairs, and has the ability to discover new relational medical entity pairs solely based on the existing entity pairs. Beside entity pairs, relationship-enhanced entity representations are obtained as another appealing benefit of the proposed method. Both quantitative and qualitative evaluations on real-world medical datasets demonstrate the effectiveness of the proposed method in generating relational medical entity pairs that are meaningful and novel.", "target": ["通过纯粹地从现有的有意义的实体对当中学习，生成性地发现有意义的、具有一定医学关系的新实体对，而不需要额外的文本语料库来进行分辨性提取。", "提出了一个变分自动编码器，用于在医疗环境中生成给定关系的实体对。", "在医学方面，本文仅从结构化数据中描述了\"知识库完成\"的经典问题。"]}
{"source": "Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood.  In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples .  In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true .  We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning .  Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture . The code for our model is available at \\url{https://github.com/daib13/TwoStageVAE}.", "target": ["我们仔细分析了VAE目标函数，并得出了导致简单增强的新结论。", "提出了一种两阶段的VAE方法，以产生高质量的样本并避免模糊。", "本文分析了高斯VAE。", "本文提供了一些关于\"vanilla\"高斯变分自动编码器的理论结果，之后将其用于构建一种称为\"2阶段VAE\"的新算法。"]}
{"source": "We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.   We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.    Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.   In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.   We also outperform Random Search $8\\times$.     Our method is inspired by provably-efficient algorithms for learning decision trees using the discrete Fourier transform.   We obtain improved sample-complexty bounds for learning decision trees while matching state-of-the-art bounds on running time (polynomial and quasipolynomial, respectively).", "target": ["一种基于离散傅里叶分析和压缩感知的超参数调谐算法。", "研究在未知函数可以被近似的假设下优化超参数的问题，表明近似的最小化可以在布尔超立方体上进行。", "本文通过假设将超参数映射到分类精度的未知函数中的结构来探索超参数的优化。"]}
{"source": "Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data. Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable. In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator.   Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement analog of the softmax operator. With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al. 2016, Maddison2016 et al. 2016) to distributions over latent matchings. We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms.", "target": ["一种排列梯度下降推理的新方法，应用于潜在匹配推理和神经网络排列的监督学习。", "本文利用Sinkhorn算子的有限近似来描述如何构建神经网络，以便从排列值训练数据中学习。", "本文提出了一种近似于离散最大权重的新方法，用于学习潜在的排列组合"]}
{"source": "Recent work in network quantization has substantially reduced the time and space complexity of neural network inference, enabling their deployment on embedded and mobile devices with limited computational and memory resources. However, existing quantization methods often represent all weights and activations with the same precision (bit-width). In this paper, we explore a new dimension of the design space: quantizing different layers with different bit-widths. We formulate this problem as a neural architecture search problem and propose a novel differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization. Experiments show we surpass the state-of-the-art compression of ResNet on CIFAR-10 and ImageNet. Our quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models.", "target": ["一种用于 ConvNet 混合量化的新型可微分神经架构搜索框架。", "作者介绍了一种新的神经结构搜索方法，该方法在每个神经网络层选择权重的精确量化，并将其用于网络压缩的背景下。", "本文提出了一种网络量化的新方法，即用不同的位宽对不同的层进行量化，并介绍了一种新的可微分的神经结构搜索框架。"]}
{"source": "The top-$k$ error is a common measure of performance in machine learning and computer vision. In practice, top-$k$ classification is typically performed with deep neural networks trained with the cross-entropy loss. Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data. In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-$k$ classification can bring significant improvements.  Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a na{\\\"i}ve algorithm would require $\\mathcal{O}(\\binom{n}{k})$ operations, where $n$ is the number of classes. Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\\mathcal{O}(k n)$. Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of $k=5$. Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.", "target": ["用于Top-k误差最小化的平滑损失函数", "建议使用深度模型的top-k loss来解决训练数据集中存在或不存在的类似类别的类混淆问题。", "平滑了top-k的损失。", "本文为top-k SVM引入了一个平滑的代用损失函数，目的是将SVM插入深度神经网络。"]}
{"source": "Designing a molecule with desired properties is one of the biggest challenges in drug development, as it requires optimization of chemical compound structures with respect to many complex properties. To augment the compound design process we introduce Mol-CycleGAN -- a CycleGAN-based model that generates optimized compounds with a chemical scaffold of interest. Namely, given a molecule our model generates a structurally similar one with an optimized value of the considered property. We evaluate the performance of the model on selected optimization objectives related to structural properties (presence of halogen groups, number of aromatic rings) and to a physicochemical property (penalized logP). In the task of optimization of penalized logP of drug-like molecules our model significantly outperforms previous results.", "target": ["我们介绍了Mol-CycleGAN--一种用于优化分子的新生成模型，以增强药物设计。", "本文提出了一种基于CycleGANs应用于分子的变异自动编码器的优化分子特性的方法，并采用了一种领域特定的VAE，称为结点树VAE（JT-VAE）。", "本文使用变分自动编码器来学习一个转换函数，从没有相关属性的分子集合到具有该属性的分子集合。"]}
{"source": "Knowledge distillation is a potential solution for model compression. The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one. Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network. However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered. To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification. By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks. Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates. In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification. Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.", "target": ["我们以人脸识别为突破口，提出了从人脸分类到对齐和验证的知识转移的模型提炼。", "本文提出将分类器从人脸分类的模型转移到对齐和验证的任务中。", "该手稿介绍了如何将知识从面部分类模型提炼到学生模型以进行面部对齐和验证的实验。"]}
{"source": "RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings. The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly.", "target": ["使用新设计的损失函数和抽样，将基于会话的RNN（GRU4Rec）推荐提高了35%。", "本文分析了现有的基于会话的推荐的损失函数，并提出了两个新的损失函数，在现有的基于排序的损失函数中增加了一个权重", "在早期工作的基础上，通过对负面例子的 \"相关性 \"加权，提出了针对使用RNN的基于会话的推荐的修改。", "本文讨论了优化GRU4Rec中损失函数的问题，提出了优化的技巧，并提出了一个加强版。"]}
{"source": "In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks. Under the assumption that future tasks are related to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task. We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework. Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks. We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples.", "target": ["我们开发了一种基于PAC-Bayes理论的终身学习方法，即在遇到新任务时调整预设值，从而促进对新任务的学习。", "一种新的PAC-Bayesian风险约束，作为多任务机器学习的目标函数，以及一种使该目标函数的简化版本最小化的算法。", "将现有的PAC-Bayes界线扩展到多任务学习中，以允许在不同的任务中调整预设值。"]}
{"source": "Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of trained models. While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs). In this work, we identify two problems regarding the direction and step size for updating the weight vectors of hidden units, which may degrade the generalization performance of Adam. As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which controls the update direction and step size more precisely, and thus bridges the generalization gap between Adam and SGD. Following a similar rationale, we further improve the generalization performance in classification tasks by regularizing the softmax logits. By bridging the gap between SGD and Adam, we also shed some light on why certain optimization algorithms generalize better than others.", "target": ["用于训练DNN的Adam的定制版本，它弥补了Adam和SGD之间的泛化差距。", "提出了一种ADAM优化算法的变体，该算法使用批量归一化对每个隐藏单元的权重进行归一化处理。", "Adam 优化算法的扩展，通过使用梯度向量的 L2 范数将传入权重的学习速率调整为隐藏单位来保留更新方向"]}
{"source": "Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning. However, autonomously learning effective sets of options is still a major challenge in the field. In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment. We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available.   We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels. It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation. We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.", "target": ["我们展示了如何使用继任者表征来发现随机领域的特征选项，从原始像素开始。特征选择是为导航所学表征的潜在维度而学习的选项。", "将特征选择的概念扩展到具有随机过渡的领域，并且在这些领域中，状态特征是可以学习的。", "显示了原生价值函数和继任者表征之间的等价性，并推导出特征选项作为选项发现机制的概念。", "该论文是Machado等人（2017）先前工作的后续工作，展示了如何使用原型值函数来定义称为\"特征选项\"的选项。"]}
{"source": "One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled. We have observed substantial progress in this topic through lower and upper bounds on the maximum number of linear regions and a counting procedure. However, these bounds only account for the dimensions of the network and the exact counting may take a prohibitive amount of time, therefore making it infeasible to benchmark the expressiveness of networks. In this work, we approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets. In addition, we present a tighter upper bound that leverages network coefficients. We test both on trained networks. The algorithm for probabilistic lower bounds is several orders of magnitude faster than exact counting and the values reach similar orders of magnitude, hence making our approach a viable method to compare the expressiveness of such networks. The refined upper bound is particularly stronger on networks with narrow layers.", "target": ["我们为网络表达中使用的线性区域的数量提供了改进的上限，并提供了一种高效的（关于精确计数）算法以获得线性区域实际数量的概率下限。", "通过使用近似的概率计数算法和分析，为研究RELU神经网络中的线性区域数量做出贡献", "在之前研究深度神经网络中线性区域计数的工作基础上，通过改变维度约束来改进之前提出的上限", "本文讨论了分段线性神经网络的表现力，其特征在于建模函数的线性区域的数量，并利用概率算法更快地计算边界，并证明更严格的边界。"]}
{"source": "The ability to look multiple times through a series of pose-adjusted glimpses is fundamental to human vision. This critical faculty allows us to understand highly complex visual scenes. Short term memory plays an integral role in aggregating the information obtained from these glimpses and informing our interpretation of the scene. Computational models have attempted to address glimpsing and visual attention but have failed to incorporate the notion of memory. We introduce a novel, biologically inspired visual working memory architecture that we term the Hebb-Rosenblatt memory. We subsequently introduce a fully differentiable Short Term Attentive Working Memory model (STAWM) which uses transformational attention to learn a memory over each image it sees. The state of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space of a layer. By projecting different queries through this layer we can obtain goal-oriented latent representations for tasks including classification and visual reconstruction. Our model obtains highly competitive classification performance on MNIST and CIFAR-10. As demonstrated through the CelebA dataset, to perform reconstruction the model learns to make a sequence of updates to a canvas which constitute a parts-based representation. Classification with the self supervised representation obtained from MNIST is shown to be in line with the state of the art models (none of which use a visual attention mechanism). Finally, we show that STAWM can be trained under the dual constraints of classification and reconstruction to provide an interpretable visual sketchpad which helps open the `black-box' of deep learning.", "target": ["一种受生物启发的工作记忆，可被整合到recurrent视觉注意力模型中，以获得最先进的性能", "介绍了一个新的网络架构，其灵感来自于视觉上的工作记忆，并将其应用于分类任务，然后将其作为一个生成模型。", "本文利用新的Hebb-Rosenblatt工作记忆模型增强了recurrent注意力模型，并在MNIST上取得了有竞争力的成果。"]}
{"source": "Generative models have been successfully applied to image style transfer and domain translation. However, there is still a wide gap in the quality of results when learning such tasks on musical audio. Furthermore, most translation models only enable one-to-one or one-to-many transfer by relying on separate encoders or decoders and complex, computationally-heavy models. In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer. First, we define timbre transfer as applying parts of the auditory properties of a musical instrument onto another. We show that we can achieve and improve this task by conditioning existing domain translation techniques with Feature-wise Linear Modulation (FiLM). Then, by replacing the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective, we alleviate the need for an auxiliary pair of discriminative networks. This allows a faster and more stable training, along with a controllable latent space encoder. By further conditioning our system on several different instruments, we can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers. Our models map inputs to 3-dimensional representations, successfully translating timbre from one instrument to another and supporting sound synthesis on a reduced set of control parameters. We evaluate our method in reconstruction and generation tasks while analyzing the auditory descriptor distributions across transferred domains. We show that this architecture incorporates generative controls in multi-domain transfer, yet remaining rather light, fast to train and effective on small datasets.", "target": ["本文使用变分自动编码和网络调节来实现音乐音色转换，我们开发并推广了我们的架构，用于多对多的乐器转换，并进行了可视化和评估。", "提出了一种调制变分自动编码器，通过用最大平均差异代替通常的对抗性翻译标准来进行音乐音色的转换。", "描述了一个多对多的音乐音色转换模型，该模型建立在领域和风格转换的最新发展之上。", "提出了一个基于VAE的混合模型，在乐器的录音中进行音色转换。"]}
{"source": "We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large. This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature. Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs “approximate inference” as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies. Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts. Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization. Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability.", "target": ["我们研究了在随机权重的假设下，权重绑定的多层vanilla自动编码器的行为。通过在大维极限下的精确描述，我们的分析揭示了有趣的相变现象。", "通过平均场分析对编码器和解码器之间绑定权重（权重绑定）的自动编码器的理论分析", "在高维统计问题分析的最新进展基础上，特别是消息传递算法，分析了加权绑定式自动编码器的性能", "本文研究了几个假设下的自动编码器，并指出这种随机自动编码器的模型可以用一维方程进行优雅而严格的分析。"]}
{"source": "Assessing distance betweeen the true and the sample distribution is a key component of many state of the art generative models, such as Wasserstein Autoencoder (WAE). Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and  kernel smoothing we construct a new generative model – Cramer-Wold AutoEncoder (CWAE). CWAE cost function, based on introduced Cramer-Wold distance between samples, has a simple closed-form in the case of normal prior. As a consequence, while simplifying the optimization procedure (no need of sampling necessary to evaluate the distance function in the training loop), CWAE performance matches quantitatively and qualitatively that of WAE-MMD (WAE using maximum mean discrepancy based distance function) and often improves upon SWAE.", "target": ["受先前关于Sliced-Wasserstein Autoencoders（SWAE）和内核平滑的工作启发，我们构建了一个新的生成模型--Cramer-Wold AutoEncoder（CWAE）。", "本文提出了一种基于编码数据分布和潜在先验分布之间新的统计距离的WAE变体", "介绍了Wasserstein AudoEncoders的一个变体，这是一个新颖的正则化自动编码器架构，提出了发散惩罚的特定选择。", "本文提出了Cramer-Wold自动编码器，它使用基于Cramer-Wold定理的两个分布之间的Cramer-Wold距离。"]}
{"source": "We propose a rejection sampling scheme using the discriminator of a GAN to  approximately correct errors in the GAN generator distribution. We show that  under quite strict assumptions, this will allow us to recover the data distribution  exactly. We then examine where those strict assumptions break down and design a  practical algorithm—called Discriminator Rejection Sampling (DRS)—that can be  used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of  Gaussians and on the state of the art SAGAN model. On ImageNet, we train an  improved baseline that increases the best published Inception Score from 52.52 to  62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use  DRS to further improve on this baseline, improving the Inception Score to 76.08  and the FID to 13.75.", "target": ["我们使用一个GAN判别器对GAN发生器的输出进行近似的拒绝采样方案。", "提出了一种用于从GAN发生器中取样的拒绝取样算法。", "本文提出了一种用于GANs的后处理拒绝采样方案，名为Discriminator Rejection Sampling，以帮助过滤GAN发生器中的 \"好 \"样本。"]}
{"source": "The quality of the features used in visual recognition is of fundamental importance for the overall system. For a long time, low-level hand-designed feature algorithms as SIFT and HOG have obtained the best results on image recognition. Visual features have recently been extracted from trained convolutional neural networks. Despite the high-quality results, one of the main drawbacks of this approach, when compared with hand-designed features, is the training time required during the learning process. In this paper, we propose a simple and fast way to train supervised convolutional models to feature extraction while still maintaining its high-quality. This methodology is evaluated on different datasets and compared with state-of-the-art approaches.", "target": ["从卷积神经网络中提取视觉特征的一种简单快速方法", "提出了一种快速学习卷积特征的方法，以后可以通过减少训练epoch的数量和学习率的特定时间表延迟来使用任何分类器。", "使用相对于训练中使用的epoch数固定的学习率衰减方案，并提取倒数第二层的输出作为特征来训练常规分类器。"]}
{"source": "We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASOs). We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator. The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper. First, we show that an RNN internally partitions the input space during training and that it builds up the partition through time. Second, we show that the affine slope parameter of an RNN corresponds to an input-specific template, from which we can interpret an RNN as performing a simple template matching (matched filtering) given the input. Third, by carefully examining the MASO RNN affine mapping, we prove that using a random initial hidden state corresponds to an explicit L2 regularization of the affine parameters, which can mollify exploding gradients and improve generalization. Extensive experiments on several datasets of various modalities demonstrate and validate each of the above conclusions. In particular, using a random initial hidden states elevates simple RNNs to near state-of-the-art performers on these datasets.", "target": ["我们从最大affine spline算子的角度对RNN进行了新的见解和解释。", "用所谓的最大affine spline算子重写Elman RNN的方程式", "提供一种使用最大affline spline算子 （MASO） 理解 RNN 的新方法，方法是使用分段affine和凸激活 MASO 重写它们。", "作者在最大affine spline算子的基础上，对一大类深度网络进行了解释，重点介绍了使用初始隐藏状态的噪声作为正则化的recurrent神经网络。"]}
{"source": "Reasoning over text and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering.  Transducing text to logical forms which can be operated on is a brittle and error-prone process . Operating directly on text by jointly learning representations and transformations thereof by means of neural architectures that lack the ability to learn and exploit general rules can be very data-inefficient and not generalise correctly . These issues are addressed by Neural Theorem Provers (NTPs) (Rocktäschel & Riedel, 2017), neuro-symbolic systems based on a continuous relaxation of Prolog’s backward chaining algorithm, where symbolic unification between atoms is replaced by a differentiable operator computing the similarity between their embedding representations . In this paper, we first propose Neighbourhood-approximated Neural Theorem Provers (NaNTPs) consisting of two extensions toNTPs, namely a) a method for drastically reducing the previously prohibitive time and space complexity during inference and learning, and b) an attention mechanism for improving the rule learning process, deeming them usable on real-world datasets. Then, we propose a novel approach for jointly reasoning over KB facts and textual mentions, by jointly embedding them in a shared embedding space. The proposed method is able to extract rules and provide explanations—involving both textual patterns and KB relations—from large KBs and text corpora. We show that NaNTPs perform on par with NTPs at a fraction of a cost, and can achieve competitive link prediction results on challenging large-scale datasets, including WN18, WN18RR, and FB15k-237 (with and without textual mentions) while being able to provide explanations for each prediction and extract interpretable rules.", "target": ["我们将神经定理证明器扩展到大型数据集，改进规则学习过程，并将其扩展到文本和知识库的联合推理。", "提出神经定理证明系统的扩展，通过降低模型的时间和空间复杂性来解决该模型的主要问题", "在统一过程中，通过对事实和规则进行近似的近邻搜索来扩展NTPs，并建议使用对已知谓词的关注来设置谓词的参数", "通过使用最近的邻居搜索，在以前提出的神经定理证明方法的基础上进行改进。"]}
{"source": "We investigate the methods by which a Reservoir Computing Network (RCN) learns concepts such as 'similar' and 'different' between pairs of images using a small training dataset and generalizes these concepts to previously unseen types of data. Specifically, we show that an RCN trained to identify relationships between image-pairs drawn from a subset of digits from the MNIST database or the depth maps of subset of visual scenes from a moving camera generalizes the learned transformations to images of digits unseen during training or depth maps of different visual scenes. We infer, using Principal Component Analysis, that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship. Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples. Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space. We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier. Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep Siamese Neural Networks (SNNs) in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera. This work helps bridge the gap between explainable machine learning and biological learning through analogies using small datasets, and points to new directions in the investigation of learning processes.", "target": ["利用可解释的动态系统模型、水库计算和基于类比的生物学上合理的学习技术，将使用少量训练数据学习的成对图像之间的关系推广到以前未见过的图像类型。", "通过使用具有标准tanh积分的回声状态网络，在RC背景下声称\"组合变换\"的结果，其区别在于循环权重未经过训练。", "对MNIST数据的不同失真进行分类的新方法", "本文使用一个回声状态网络来学习将成对图像之间的图像转换分为五类中的一类。"]}
{"source": "We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations of the data. GAPF leverages recent advances in adversarial learning to allow a data holder to learn \"universal\" representations that decouple a set of sensitive attributes from the rest of the dataset. Under GAPF, finding the optimal decorrelation scheme is formulated as a constrained minimax game between a generative decorrelator and an adversary. We show that for appropriately chosen adversarial loss functions, GAPF provides privacy guarantees against strong information-theoretic adversaries and enforces demographic parity. We also evaluate the performance of GAPF on multi-dimensional Gaussian mixture models and real datasets, and show how a designer can certify that representations learned under an adversary with a fixed architecture perform well against more complex adversaries.", "target": ["我们提出了生成对抗性隐私和公平（GAPF），这是一个数据驱动的框架，用于学习具有认证隐私/公平保证的私有和公平表征。", "本文使用一个GAN模型来概述与私有/公平表征学习（PRL）相关的工作。", "本文提出了一种基于对抗性的方法来进行私人和公平的表示，通过学习性的数据失真，最大限度地减少对敏感变量的依赖，同时失真程度受到限制。", "作者描述了一个如何学习可用于训练某些分类器的人口学奇偶校验表示的框架。"]}
{"source": "Current machine learning algorithms can be easily fooled by adversarial examples. One possible solution path is to make models that use confidence thresholding to avoid making mistakes. Such models refuse to make a prediction when they are not confident of their answer. We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples. Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability. We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models. Experiments show the attack attains good results in practice. We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.   We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).", "target": ["我们提出了评估模型的指标和最佳攻击方式，这些模型利用置信度阈值来抵御对抗性的示例", "本文介绍了对置信度阈值算法的攻击系列，主要集中在评估方法上。", "提出了一种置信度阈值防御模型的评估方法，以及在使用定向攻击时，通过选择置信度最高的错误类来生成对抗性例子的方法", "本文提出了一种评价方法，用于评价对置信度阈值法的攻击，并提出了一种新的攻击。"]}
{"source": "Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided. However, in sparse reward environment it still often suffers from the need to carefully shape reward function to guide policy optimization. This limits the applicability of RL in the real world since both reinforcement learning and domain-specific knowledge are required. It is therefore of great practical importance to develop algorithms which can learn from a binary signal indicating successful task completion or other unshaped, sparse reward signals. We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents. Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum. We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm. Each task provides only binary rewards indicating whether or not the goal is achieved. Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration. Extensive experiments demonstrate that this method leads to faster converge and improved task performance.", "target": ["一种利用对抗性奖励重新标记的稀疏奖励学习的新方法", "建议使用竞争性的多代理设置来鼓励探索，并表明CER + HER > HER ~ CER", "提出一种在无模型强化学习环境中从稀疏的奖励中学习的新方法，并使奖励密集化", "为了解决稀疏的奖励问题并鼓励RL算法的探索，作者提出了一种叫做竞争性经验回复（CER）的重新标记策略。"]}
{"source": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "target": ["用高斯混合VAE建立TTS模型，可以对说话风格、噪音条件等进行精细的控制。", "描述了有条件的GAN模型，通过增强与识别相对应的Z空间来生成说话人的条件Mel谱。", "提出一种两层潜变量模型，以获得解缠潜变量表示，从而促进对各种属性的细粒度控制。", "本文提出了一个可以控制非注释属性的模型，如说话风格、口音、背景噪音等。"]}
{"source": "Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.", "target": ["通过处理重叠的对象建议，使视觉问题回答模型能够计数。", "本文提出了一个手工设计的网络架构，在对象建议图上进行软性非最大限度的抑制，以获得对象数量。", "关注视觉问题回答中的计数问题，使用注意力机制并提出一个可微分的计数组件，明确地计算物体的数量。", "本文解决了视觉问题回答中的物体计数问题，它提出了许多启发式方法来寻找正确的计数。"]}
{"source": "We propose a simple and robust training-free approach for building sentence representations. Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence. We model the semantic meaning of a word in a sentence based on two aspects. One is its relatedness to the word vector subspace already spanned by its contextual words. The other is its novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace.   Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representation. This approach requires zero training and zero parameters, along with efficient inference performance. We evaluate our approach on 11 downstream NLP tasks. Experimental results show that our model outperforms all existing zero-training alternatives in all the tasks and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.", "target": ["一种简单的、无需训练的句子嵌入方法，与需要大量训练数据或延长训练时间的复杂模型相比，其性能具有竞争力。", "通过系统分析，提出了一种新的无训练生成句子嵌入的方法", "提出了一种新的基于几何学的方法，通过量化每个词的新颖性、重要性和语料库唯一性，从词的嵌入向量中嵌入句子。", "本文探讨了基于单词嵌入的正交分解扫描空间的句子嵌入。"]}
{"source": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.", "target": ["我们提出了原型网络的新颖扩展，这些扩展通过在生产原型时使用未标记示例的能力进行了增强。", "本文是一个原型网络的扩展，考虑采用现有的未标记的例子来帮助训练每个episode", "通过将原型网络扩展到半监督学习的设置中，以干扰类为例，研究半监督少数镜头分类的问题。", "通过使用分配的伪标签更新原型，处理干扰因素，以及使用与原始原型的距离来衡量样本，将原型网络扩展到半监督环境中。"]}
{"source": "We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space. We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions.", "target": ["我们从理论上证明，线性插值不适合用于分析训练好的隐性生成模型。", "研究当两个随机变量之间的线性插值遵循相同的分布时，与隐性生成模型的先验分布有关的问题", "这项工作问的是如何在给定的潜变量模型中进行插值。"]}
{"source": "Deep neural networks (DNN) have shown promising performance in computer vision. In medical imaging, encouraging results have been achieved with deep learning for applications such as segmentation, lesion detection and classification. Nearly all of the deep learning based image analysis methods work on reconstructed images, which are obtained from original acquisitions via solving inverse problems (reconstruction). The reconstruction algorithms are designed for human observers, but not necessarily optimized for DNNs which can often observe features that are incomprehensible for human eyes. Hence, it is desirable to train the DNNs directly from the original data which lie in a different domain with the images. In this paper, we proposed an end-to-end DNN for abnormality detection in medical imaging. To align the acquisition with the annotations made by radiologists in the image domain, a DNN was built as the unrolled version of iterative reconstruction algorithms to map the acquisitions to images, and followed by a 3D convolutional neural network (CNN) to detect the abnormality in the reconstructed images. The two networks were trained jointly in order to optimize the entire DNN for the detection task from the original acquisitions. The DNN was implemented for lung nodule detection in low-dose chest computed tomography (CT), where a numerical simulation was done to generate acquisitions from 1,018 chest CT images with radiologists' annotations. The proposed end-to-end DNN demonstrated better sensitivity and accuracy for the task compared to a two-step approach, in which the reconstruction and detection DNNs were trained separately. A significant reduction of false positive rate on suspicious lesions were observed, which is crucial for the known over-diagnosis in low-dose lung CT imaging. The images reconstructed by the proposed end-to-end network also presented enhanced details in the region of interest.", "target": ["从投影数据而不是图像开始检测肺部结节。", "DNNs被用于CT投影数据中基于图像块的肺结节检测。", "通过对原始正弦图到检测输出的映射进行端到端训练，对肺部的计算机断层成像重建和病变检测进行联合建模", "介绍了一个结合CT图像信号处理和图像分析的CNN架构的端到端训练。"]}
{"source": "Deep reinforcement learning (DRL) algorithms have demonstrated progress in learning to find a goal in challenging environments. As the title of the paper by Mirowski et al. (2016) suggests, one might assume that DRL-based algorithms are able to “learn to navigate” and are thus ready to replace classical mapping and path-planning algorithms, at least in simulated environments. Yet, from experiments and analysis in this earlier work, it is not clear what strategies are used by these algorithms in navigating the mazes and finding the goal. In this paper, we pose and study this underlying question: are DRL algorithms doing some form of mapping and/or path-planning? Our experiments show that the algorithms are not memorizing the maps of mazes at the testing stage but, rather, at the training stage. Hence, the DRL algorithms fall short of qualifying as mapping or path-planning algorithms with any reasonable definition of mapping. We extend the experiments in Mirowski et al. (2016) by separating the set of training and testing maps and by a more ablative coverage of the space of experiments. Our systematic experiments show that the NavA3C-D1-D2-L algorithm, when trained and tested on the same maps, is able to choose the shorter paths to the goal. However, when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning.", "target": ["我们在各种条件下对基于深度强化学习的导航方法进行定量和定性评估，以回答它们在多大程度上可以取代经典的路径规划器和绘图算法。", "通过测量到达目标的重复延迟和与最短路线的比较，评估基于深度RL的训练迷宫模型。"]}
{"source": "In many robotic applications, it is crucial to maintain a belief about the state of   a system, like the location of a robot or the pose of an object.  These state estimates serve as input for planning and decision making and   provide feedback during task execution.   Recursive Bayesian Filtering algorithms address the state estimation problem,  but they require a model of the process dynamics and the sensory observations as well as   noise estimates that quantify the accuracy of these models.   Recently, multiple works have demonstrated that the process and sensor models can be   learned by end-to-end training through differentiable versions of Recursive Filtering methods.  However, even if the predictive models are known, finding suitable noise models   remains challenging. Therefore, many practical applications rely on very simplistic noise   models.   Our hypothesis is that end-to-end training through differentiable Bayesian   Filters enables us to learn more complex heteroscedastic noise models for  the system dynamics. We evaluate learning such models with different types of   filtering algorithms and on two different robotic tasks. Our experiments show that especially   for sampling-based filters like the Particle Filter, learning heteroscedastic noise   models can drastically improve the tracking performance in comparison to using   constant noise models.", "target": ["我们评估了在不同的微分贝叶滤波器中学习异方差噪声模型的情况。", "建议通过可微分的贝叶斯滤波器和两个不同版本的无中心卡尔曼滤波器，通过端到端优化预测似然，从数据中学习异方差噪声模型。", "重新审视贝叶斯过滤器，并评估在保持所有其他模型固定的情况下训练观察和过程噪声模型的好处。", "本文提出了一种在传统贝叶斯滤波算法中学习和使用依赖于状态和观测的噪声的方法。该方法包括构建一个神经网络模型，将原始观测数据作为输入，并产生一个紧凑的表示和相关的对角线协方差。"]}
{"source": "Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning. These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data. However, we find that the extensive use of Laplacian smoothing at each layer in current approaches can easily dilute the knowledge from distant nodes and consequently decrease the performance in zero-shot learning. In order to still enjoy the benefit brought by the graph structure while preventing the dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes. DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections. These connections are added based on a node's relationship to its ancestors and descendants. A weighting scheme is further used to weigh their contribution depending on the distance to the node. Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches.", "target": ["我们重新思考如何更有效地利用知识图谱中的信息，以提高零点学习任务的性能，并为此提出密集图传播（DGP）模块。", "该作者提出了一个解决Graph conv网络中过度平滑问题的方案，即允许所有相关节点之间的密集传播，并以相互距离为权重。", "提出了一种新颖的图卷积神经网络，通过使用类之间的关系结构作为图卷积网络的输入来学习未见过的类的分类器，以解决零样本分类问题。"]}
{"source": "In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem. By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure. We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network. Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions. Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variant.", "target": ["基于胶囊的语义分割，其中类标签的概率是通过胶囊管道追溯的。", "作者提出了一个回溯机制，将最低级别的胶囊与它们各自的类别联系起来", "提出了胶囊网络的追踪层来做语义分割，并在胶囊层中明确使用部分-整体关系", "提出了一种基于Sabour的CapsNet概念的回溯方法，在进行分类的同时进行语义分割。"]}
{"source": "Studying the evolution of information theoretic quantities during Stochastic Gradient Descent (SGD) learning of Artificial Neural Networks (ANNs) has gained popularity in recent years.   Nevertheless, these type of experiments require estimating mutual information and entropy which becomes intractable for moderately large problems. In this work we propose a framework for understanding SGD learning in the information plane which consists of observing entropy and conditional entropy of the output labels of ANN. Through experimental results and theoretical justifications it is shown that, under some assumptions, the SGD learning trajectories appear to be similar for different ANN architectures. First, the SGD learning is modeled as a Hidden Markov Process (HMP) whose entropy tends to increase to the maximum. Then, it is shown that the SGD learning trajectory appears to move close to the shortest path between the initial and final joint distributions in the space of probability measures equipped with the total variation metric. Furthermore, it is shown that the trajectory of learning in the information plane can provide an alternative for observing the learning process, with potentially richer information about the learning than the trajectories in training and test error.", "target": ["我们把SGD看作是概率度量空间中的轨迹，显示了它与马尔可夫过程的联系，提出了一个简单的SGD学习的马尔可夫模型，并在实验中用信息论的数量与SGD进行比较。", "构建一个马尔科夫链，该链遵循P上TV度量的短路径，并表明SGD和α-SMLC的轨迹具有相似的条件熵", "研究H(\\hat{y})与H(\\hat{y}|y)在训练神经网络的随机梯度下降方法的信息平面上的运行轨迹", "从分布p(y',y)的角度描述SGD，其中y是（可能被破坏的）真类标签，y'是模型预测值。"]}
{"source": "Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling. However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition. This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler. The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of energy landscapes. Experiments validate the proposed approach on Bayesian fully connected neural network, Bayesian convolutional neural network and Bayesian recurrent neural network tasks, showing that the learned sampler outperforms generic, hand-designed SG-MCMC algorithms, and generalizes to different datasets and larger architectures.", "target": ["本文提出了一种利用元学习方法自动设计随机梯度MCMC方案的方法。", "提出了一种元学习方法，自动设计基于哈密尔顿动力学的MCMC采样器，以便在与训练问题类似的问题上更快地混合。", "通过神经网络对扩散和卷曲矩阵进行参数化，并对sg-mcmc算法进行元学习和优化。"]}
{"source": "We propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature. Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions. We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function. We then provide a set of systematic experiments that explore its hyper-parameters. We show that each of the energy function's components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned. We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.", "target": ["图像质量评估技术改善基于能量的生成对抗网络的训练和评估", "对BEGAN模式提出了一个基于能量的表述，并对其进行了修改，以包括一个基于图像质量评估的术语。", "在BEGAN（边界平衡GAN框架）中提出了一些新的能量函数，包括l_1得分、梯度大小相似度得分和色度得分。"]}
{"source": "Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions. Its performance depends crucially on a damping coefficient. Largecamping  coefficients can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9. We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients. AggMo is trivial to implement, but significantly dampens oscillations, enabling it to remain stable even for aggressive damping coefficients such as 0.999. We reinterpret Nesterov's accelerated gradient descent as a special case of AggMo and analyze rates of convergence for quadratic objectives. Empirically, we find that AggMo is a suitable drop-in replacement for other momentum methods, and frequently delivers faster convergence with little to no tuning.", "target": ["我们引入了一种简单的动量优化变体，它能够在深度学习任务上以最小的超参数调整来超越经典动量、Nesterov和Adam。", "引入了一个动量的变体，它聚集了几个具有不同阻尼系数的速度，大大降低了振荡。", "提出了一种基于梯度的优化的聚合动量方法，使用具有不同阻尼系数的多个速度矢量而不是单一速度矢量来提高稳定性", "作者将几个更新步骤结合在一起，实现了聚合动量，也证明了它比其他动量方法更稳定。"]}
{"source": "Recurrent Neural Networks architectures excel at processing sequences by  modelling dependencies over different timescales. The recently introduced  Recurrent Weighted Average (RWA) unit captures long term dependencies  far better than an LSTM on several challenging tasks. The RWA achieves  this by applying attention to each input and computing a weighted average  over the full history of its computations. Unfortunately, the RWA cannot  change the attention it has assigned to previous timesteps, and so struggles  with carrying out consecutive tasks or tasks with changing requirements.  We present the Recurrent Discounted Attention (RDA) unit that builds on  the RWA by additionally allowing the discounting of the past.  We empirically compare our model to RWA, LSTM and GRU units on  several challenging tasks. On tasks with a single output the RWA, RDA and  GRU units learn much quicker than the LSTM and with better performance.  On the multiple sequence copy task our RDA unit learns the task three  times as quickly as the LSTM or GRU units while the RWA fails to learn at  all. On the Wikipedia character prediction task the LSTM performs best  but it followed closely by our RDA unit. Overall our RDA unit performs  well and is sample efficient on a large variety of sequence tasks.", "target": ["我们引入了循环贴现单元，它在线性时间内将注意力应用于任何长度的序列。", "本文提出了循环贴现关注（RDA），这是循环加权平均（RWA）的延伸，加入了一个贴现因子。", "在保持原有方法优势的基础上，扩展了循环权重平均法，克服了原有方法的局限性，提出了以Elman网为基础的RNN方法"]}
{"source": "Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging. In this paper, we introduce variance layers, a different kind of stochastic layers. Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance. It means that each object is represented by a zero-mean distribution in the space of the activations. We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks. We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors. We observe that in these cases such zero-mean parameterization leads to a much better training objective than more flexible conventional parameterizations where the mean is being learned.", "target": ["通过只学习方差，可以在神经网络的权重上学习一个零中心的高斯分布，而且效果出奇地好。", "本文研究了变分后验的平均值的影响，并提出了方差层，它只使用方差来存储信息", "研究方差神经网络，它近似于具有零均值高斯分布的贝叶斯神经网络的后验。"]}
{"source": "Graph Convolutional Networks (GCNs) are a recently proposed architecture which has had success in semi-supervised learning on graph-structured data. At the same time, unsupervised learning of graph embeddings has benefited from the information contained in random walks. In this paper we propose a model, Network of GCNs (N-GCN), which marries these two lines of work. At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective. Our experiments show that our proposed N-GCN model achieves state-of-the-art performance on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI. In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations.", "target": ["我们制作了一个图卷积网络网络，为每个邻接矩阵提供不同的幂，将它们的所有表示组合到一个分类子网络中，实现了最先进的半监督节点分类。", "提出了一种新的GCN网络，有两种方法：在叠加特征之上的全连接层和每个GCN使用标量权重的注意力机制。", "提出了一个图卷积网络的网络，它使用随机行走的统计数据来提取图中近邻和远邻的信息"]}
{"source": "Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy. However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy. In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation. We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network. Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches. Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.", "target": ["一种全连接DNN层的快速修剪算法，具有泛化误差退化的理论分析。", "提出了一种廉价的DNN密集层的修剪算法。", "提出了修剪DNN问题的解决方案，将Net-trim目标函数设定为Difference of convex(DC)函数。"]}
{"source": "Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.", "target": ["我们提出了一个新的混合时空网络，在三个公共数据集的视频动作分割上取得了最先进的性能。", "讨论了通过使用时间卷积编码器-解码器架构在长达10分钟的长视频中进行动作分割的问题", "提出了一种用于视频动作分割的时间卷积和递归网络组合"]}
{"source": "Convolutional Neural Networks (CNNs) become deeper and deeper in recent years, making the study of model acceleration imperative. It is a common practice to employ a shallow network, called student, to learn from a deep one, which is termed as teacher. Prior work made many attempts to transfer different types of knowledge from teacher to student, however, there are two problems remaining unsolved. Firstly, the knowledge used by existing methods is highly dependent on task and dataset, limiting their applications. Secondly, there lacks an effective training scheme for the transfer process, leading to degradation of performance. In this work, we argue that feature is the most important knowledge from teacher. It is sufficient for student to just learn good features regardless of the target task. From this discovery, we further present an efficient learning strategy to mimic features stage by stage. Extensive experiments demonstrate the importance of features and show that the proposed approach significantly narrows down the gap between student and teacher, outperforming the state-of-the-art methods.", "target": ["本文提出通过逐级模仿特征将知识从深层模型转移到浅层模型。", "通过使用不同的 resnet 结构来解释逐阶段知识转换方法", "本文提出将一个网络分为多个部分，并依次对每个部分进行蒸馏，以提高深度教师网络的蒸馏性能"]}
{"source": "We augment adversarial training (AT) with worst case adversarial training  (WCAT) which improves adversarial robustness by 11% over the current state-  of-the-art result in the `2-norm on CIFAR-10. We interpret adversarial training as  Total Variation Regularization, which is a fundamental tool in mathematical im-  age processing, and WCAT as Lipschitz regularization, which appears in Image  Inpainting. We obtain verifiable worst and average case robustness guarantees,  based on the expected and maximum values of the norm of the gradient of the  loss.", "target": ["通过用可操作的Lipschitz正则化来增强对抗性训练，可以获得对抗性稳健性的改进，以及可证明的稳健性保证。", "探讨用一个额外的梯度正则化项来增加训练损失，以提高模型对敌对例子的稳健性。", "使用一个技巧来简化对抗性损失，在其中对抗性扰动以封闭形式出现。"]}
{"source": "The task of Reading Comprehension with Multiple Choice Questions, requires a human (or machine) to read a given \\{\\textit{passage, question}\\} pair and select one of the $n$ given options. The current state of the art model for this task first computes a query-aware representation for the passage and then \\textit{selects} the option which has the maximum similarity with this representation. However, when humans perform this task they do not just focus on option selection but use a combination of \\textit{elimination} and \\textit{selection}. Specifically, a human would first try to eliminate the most irrelevant option and then read the document again in the light of this new information (and perhaps ignore portions corresponding to the eliminated option). This process could be repeated multiple times till the reader is finally ready to select the correct option. We propose \\textit{ElimiNet}, a neural network based model which tries to mimic this process. Specifically, it has gates which decide whether an option can be eliminated given the \\{\\textit{document, question}\\} pair and if so it tries to make the document representation orthogonal to this eliminatedd option (akin to ignoring portions of the document corresponding to the eliminated option). The model makes multiple rounds of partial elimination to refine the document representation and finally uses a selection module to pick the best option. We evaluate our model on the recently released large scale RACE dataset and show that it outperforms the current state of the art model on 7 out of the 13 question types in this dataset. Further we show that taking an ensemble of our \\textit{elimination-selection} based method with a \\textit{selection} based method gives us an improvement of 7\\% (relative) over the best reported performance on this dataset.", "target": ["一种结合了消除和选择的模式来回答多选题", "对 \"有门的注意力阅读器 \"进行了阐述，在选择阅读理解中根据答案的消除来增加门。", "本文提出在阅读理解任务的模型架构中使用消除门，但并没有取得最先进的结果", "本文提出了一个新的多选阅读理解模型，该模型基于这样的理念：应该取消一些选项，以推断出更好的段落/问题的表述。"]}
{"source": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.   In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.", "target": ["我们提出了一个新颖的概率递归推理（PR2）框架，用于多代理深度强化学习任务。", "提出了一种在多代理强化学习中完全去中心化训练的新方法", "根据递归推理有利于收敛到非平凡均衡的假设，解决了在多Agent环境下赋予RL代理递归推理能力的问题。", "本文介绍了一种用于多代理强化学习的去中心化训练方法，代理推断其他代理的政策并使用推断的模型进行决策。"]}
{"source": "Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers. However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections. A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks. To address these issues, we propose a method to reduce the communication overhead of distributed deep learning. Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated. We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost. We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy. We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.", "target": ["一种新的算法，通过区分 \"不明确 \"的梯度来减少分布式深度学习的通信开销。", "提出了一种基于方差的梯度压缩方法，以减少分布式深度学习的通信开销", "提出了一种压缩分布式SGD梯度更新的新方法，以加快整体执行速度", "引入了基于方差的梯度压缩方法，用于神经网络的高效分布式训练和测量模糊性。"]}
{"source": "In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics. Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion. We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.", "target": ["一种新的无监督深层领域适应技术，有效地统一了相关性排列和熵值最小化。", "通过用两个协方差矩阵之间的测地对数欧氏距离取代欧氏距离，并通过目标域上的熵自动选择平衡成本，改进了领域适应的相关排列方法。", "提出了最小熵相关排列的建议，这是一种无监督的领域适应性算法，将熵的最小化和相关排列方法联系起来。"]}
{"source": "Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, \"Conceptor-Aided Backprop\" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.", "target": ["我们提出了一种反向传播算法的变体，在这种算法中，梯度被概念器所屏蔽，以防止以前所学任务的退化。", "本文应用了概念器的概念，一种正则器的形式，以防止在连续任务的神经网络训练中的遗忘。", "介绍了一种使用概念器学习新任务的方法，不干扰以前的任务。"]}
{"source": "Recent advances in neural Sequence-to-Sequence (Seq2Seq) models reveal a purely data-driven approach to the response generation task. Despite its diverse variants and applications, the existing Seq2Seq models are prone to producing short and generic replies, which blocks such neural network architectures from being utilized in practical open-domain response generation tasks. In this research, we analyze this critical issue from the perspective of the optimization goal of models and the specific characteristics of human-to-human conversational corpora. Our analysis is conducted by decomposing the goal of Neural Response Generation (NRG) into the optimizations of word selection and ordering. It can be derived from the decomposing that Seq2Seq based NRG models naturally tend to select common words to compose responses, and ignore the semantic of queries in word ordering. On the basis of the analysis, we propose a max-marginal ranking regularization term to avoid Seq2Seq models from producing the generic and uninformative responses. The empirical experiments on benchmarks with several metrics have validated our analysis and proposed methodology.", "target": ["分析神经反应生成模型偏爱通用回答的原因；提出避免这种情况的方法。", "研究困扰Seq2Seq神经生成模型的通用回复问题", "本文通过修改损失函数和在训练阶段展示共同/普遍的反应来改进神经反应生成任务，不再强调共同反应。"]}
{"source": "The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.  We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.", "target": ["我们利用源代码的句法结构来生成自然语言序列。", "介绍了一种通过解析和产生语法树从代码中生成序列的方法", "本文介绍了一种基于AST的编程代码编码，并展示了其在极端代码总结和代码字幕任务中的有效性。", "本文提出了一个新的代码到序列的模型，利用编程语言的句法结构对源代码片段进行编码，然后将其解码为自然语言。"]}
{"source": "We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative.    Differently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.   Experiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.", "target": ["我们用一种新的注意力机制来增强CNN的细粒度识别。在5个数据集上获得了卓越的性能。", "描述了一种适用于细粒度识别的新型注意机制，该机制能持续提高基线的识别精度", "本文提出了一种用于细粒度图像分类的前馈注意力机制。", "本文提出了一种用于细粒度图像分类的有趣注意力机制。"]}
{"source": "Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator. We argue that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances. We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem. We modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator. Experiments on CIFAR-10 dataset show that our modified models improve the baseline model by a large margin. Furthermore, our models achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in the unsupervised setting.", "target": ["我们将普通卷积替换为自适应卷积，以改进 GAN 生成器。", "建议用自适应卷积块取代生成器中的卷积，自适应卷积块可以学习生成卷积权重和每个像素位置的上采样操作的偏差。", "在GANs的背景下使用自适应卷积，用一个叫做AdaConvBlock的块来代替常规的卷积，这给每个内核权重提供了更多的局部背景，因此它可以生成局部灵活的对象。"]}
{"source": "Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data.", "target": ["我们进行了大规模的实验，表明一个简单的在线蒸馏变体可以帮助我们将分布式神经网络训练扩展到更多的机器。", "提出了一种超越目前迷你批次随机梯度下降极限的分布式训练方法", "提出了一种被称为共同蒸馏的在线蒸馏方法，在规模上的应用，其中训练两个不同的模型以匹配另一个模型的预测，并最大限度地减少其自身损失。", "引入在线蒸馏技术，加速传统算法进行大规模分布式神经网络训练"]}
{"source": "Support Vector Machines (SVMs) are one of the most popular algorithms for classification and regression analysis. Despite their popularity, even efficient implementations have proven to be computationally expensive to train at a large-scale, especially in streaming settings. In this paper, we propose a novel coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training. A coreset is a weighted subset of the original data points such that SVMs trained on the coreset are provably competitive with those trained on the original (massive) data set. We provide both lower and upper bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of the complexity of the input data. Our analysis also establishes sufficient conditions on the existence of sufficiently compact and representative coresets for the SVM problem. We empirically evaluate the practical effectiveness of our algorithm against synthetic and real-world data sets.", "target": ["我们提出了一种算法，通过构建紧凑的表征，在海量数据集上加速SVM的训练，提供高效和可证明的近似推理。", "研究SVM的核心集方法，旨在对一小部分加权点进行采样，从而证明这些点的损失函数与整个数据集的损失函数相近。", "本文提出了一种基于重要性抽样的Coreset构造来表示SVM的大量训练数据。"]}
{"source": "The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates. Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors. For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions. We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension. We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information.", "target": ["我们证明了符号随机梯度方法的非凸收敛率。该算法与Adam和Rprop等算法以及分布式机器学习中使用的梯度量化方案有联系。", "提供了Sign SGD算法在非凸情况下的收敛性分析", "本文探讨了一种使用梯度的符号而不是实际梯度来训练深度模型的算法"]}
{"source": "Deep learning has found numerous applications thanks to its versatility and accuracy on pattern recognition problems such as visual object detection. Learning and inference in deep neural networks, however, are memory and compute intensive and so improving efficiency is one of the major challenges for frameworks such as PyTorch, Tensorflow, and Caffe. While the efficiency problem can be partially addressed with specialized hardware and its corresponding proprietary libraries, we believe that neural network acceleration should be transparent to the user and should support all hardware platforms and deep learning libraries.    To this end, we introduce a transparent middleware layer for neural network acceleration. The system is built around a compiler for deep learning, allowing one to combine device-specific libraries and custom optimizations while supporting numerous hardware devices. In contrast to other projects, we explicitly target the optimization of both prediction and training of neural networks. We present the current development status and some preliminary but encouraging results: on a standard x86 server, using CPUs our system achieves a 11.8x speed-up for inference and a 8.0x for batched-prediction (128); on GPUs we achieve a 1.7x and 2.3x speed-up respectively.", "target": ["我们引入了一个透明的神经网络加速中间件，它有自己的编译器引擎，在CPU上实现了11.8倍的速度，在GPU上实现了2.3倍的速度。", "本文提出了一个用于神经网络加速的透明中间件层，并在基本的CPU和GPU架构上获得了一些加速结果"]}
{"source": "Performance of neural networks can be significantly improved by encoding known invariance for particular tasks. Many image classification tasks, such as those related to cellular imaging, exhibit invariance to rotation. In particular, to aid convolutional neural networks in learning rotation invariance, we consider a simple, efficient conic convolutional scheme that encodes rotational equivariance, along with a method for integrating the magnitude response of the 2D-discrete-Fourier transform (2D-DFT) to encode global rotational invariance. We call our new method the Conic Convolution and DFT Network (CFNet). We evaluated the efficacy of CFNet as compared to a standard CNN and group-equivariant CNN (G-CNN) for several different image classification tasks and demonstrated improved performance, including classification accuracy, computational efficiency, and its robustness to hyperparameter selection. Taken together, we believe CFNet represents a new scheme that has the potential to improve many imaging analysis applications.", "target": ["我们提出了圆锥卷积和2D-DFT来将旋转等方差编码到神经网络中。", "在图像分类的背景下，本文提出了一种卷积神经网络架构，该架构具有旋转等变特征图，最终通过使用2D离散傅里叶变换（DFT）的大小使旋转不变。", "作者通过结合圆锥卷积和2D-DFT提供了一个旋转不变的神经网络"]}
{"source": "The problem of visual metamerism is defined as finding a family of perceptually  indistinguishable, yet physically different images. In this paper, we propose our  NeuroFovea metamer model, a foveated generative model that is based on a mixture  of peripheral representations and style transfer forward-pass algorithms. Our  gradient-descent free model is parametrized by a foveated VGG19 encoder-decoder  which allows us to encode images in high dimensional space and interpolate  between the content and texture information with adaptive instance normalization  anywhere in the visual field. Our contributions include: 1) A framework for computing metamers that resembles a noisy communication system via a foveated feed-forward encoder-decoder network – We observe that metamerism arises as a byproduct of noisy perturbations that partially lie in the perceptual null space; 2) A perceptual optimization scheme as a solution to the hyperparametric nature of our metamer model that requires tuning of the image-texture tradeoff coefficients everywhere in the visual field which are a consequence of internal noise; 3) An  ABX psychophysical evaluation of our metamers where we also find that the rate  of growth of the receptive fields in our model match V1 for reference metamers  and V2 between synthesized samples. Our model also renders metamers at roughly  a second, presenting a ×1000 speed-up compared to the previous work, which now  allows for tractable data-driven metamer experiments.", "target": ["我们引入了一种新的前馈框架来生成视觉metamer", "提出了一个NeuroFovea模型，通过编码器-解码器风格架构，使用风格转移方法，生成固定点metamer", "对同色异谱的分析和一个能够快速产生对实验心理物理学和其他领域有价值的metamer模型。", "该论文提出了一种快速的方法，用于通过注视点，快速，任意的样式转移来生成视觉同色异构体 - 物理上不同的图像，无法与原始图像分开。"]}
{"source": "Past works have shown that, somewhat surprisingly, over-parametrization can help generalization in neural networks. Towards explaining this phenomenon, we adopt a margin-based perspective. We establish: 1) for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks, 2) as a result, increasing the over-parametrization improves the normalized margin and generalization error bounds for deep networks. In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees. The typical infinite feature methods are kernel methods; we compare the neural net margin with that of kernel methods and construct natural instances where kernel methods have much weaker generalization guarantees. We validate this gap between the two approaches empirically. Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization. We show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time.", "target": ["我们表明，用弱的正则器训练前馈网络会产生最大的余量，并分析了这个结果的含义。", "研究神经集的边际理论，表明最大边际在网络规模中是单调增长的。", "本文研究了具有ReLU激活的两层网络的正则化交叉熵损失的最小化器的隐性偏差，得到了一个不随网络大小而增加的泛化上限。"]}
{"source": "We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.", "target": ["一个大规模的深度强化学习的分布式架构，使用并行数据生成来提高Arcade学习环境基准的技术水平，只需要以前方法的一小部分挂钟训练时间。", "研究了一个分布式深层RL系统，其中经验而不是梯度是在平行工作和集中学习者之间共享的。", "一种平行的DQN训练方法，基于让多个行为者平行收集数据，同时由一个学习者从中央重放存储器的经验采样来训练模型的想法。", "本文提出了大规模深度强化学习的分布式架构，重点是在优先体验回放框架中增加行为者算法的并行化。"]}
{"source": "Designing neural networks for continuous-time stochastic processes is challenging, especially when observations are made irregularly. In this article, we analyze neural networks from a frame theoretic perspective to identify the sufficient conditions that enable smoothly recoverable representations of signals in L^2(R). Moreover, we show that, under certain assumptions, these properties hold even when signals are irregularly observed. As we converge to the family of (convolutional) neural networks that satisfy these conditions, we show that we can optimize our convolution filters while constraining them so that they effectively compute a Discrete Wavelet Transform. Such a neural network can efficiently divide the time-axis of a signal into orthogonal sub-spaces of different temporal scale and localization. We evaluate the resulting neural network on an assortment of synthetic and real-world tasks: parsimonious auto-encoding, video classification, and financial forecasting.", "target": ["提供不规则观察信号表征的神经架构，可证明能够进行信号重建。", "证明具有Leaky ReLU激活函数的卷积神经网络是非线性框架，对非均匀采样的时间序列有类似的结果", "本文考虑了时间序列上的神经网络，并表明可以选择第一个卷积滤波器来表示离散小波变换。"]}
{"source": "Most state-of-the-art neural machine translation systems, despite being different  in architectural skeletons (e.g., recurrence, convolutional), share an indispensable  feature: the Attention. However, most existing attention methods are token-based  and ignore the importance of phrasal alignments, the key ingredient for the success  of phrase-based statistical machine translation. In this paper, we propose  novel phrase-based attention methods to model n-grams of tokens as attention  entities. We incorporate our phrase-based attentions into the recently proposed  Transformer network, and demonstrate that our approach yields improvements of  1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation  tasks, and 1.75 and 1.35 BLEU points in English-to-Russian and Russian-to-English translation tasks   on WMT newstest2014 using WMT’16 training data.", "target": ["基于短语的注意力机制，用于将注意力分配给短语，实现token到短语，短语到token，短语到短语的注意力对齐，以及现有的token到token注意。", "论文提出了一种注意力机制，它不仅可以计算单个token的加权和，还可以计算ngrams（短语）。"]}
{"source": "Intuitively, unfamiliarity should lead to lack of confidence. In reality, current algorithms often make highly confident yet wrong predictions when faced with unexpected test samples from an unknown distribution different from training. Unlike domain adaptation methods, we cannot gather an \"unexpected dataset\" prior to test, and unlike novelty detection methods, a best-effort original task prediction is still expected. We compare a number of methods from related fields such as calibration and epistemic uncertainty modeling, as well as two proposed methods that reduce overconfident errors of samples from an unknown novel distribution without drastically increasing evaluation time: (1) G-distillation, training an ensemble of classifiers and then distill into a single model using both labeled and unlabeled examples, or (2) NCR, reducing prediction confidence based on its novelty detection score. Experimentally, we investigate the overconfidence problem and evaluate our solution by creating \"familiar\" and \"novel\" test splits, where \"familiar\" are identically distributed with training and \"novel\" are not. We discover that calibrating using temperature scaling on familiar data is the best single-model method for improving novel confidence, followed by our proposed methods. In addition, some methods' NLL performance are roughly equivalent to a regularly trained model with certain degree of smoothing. Calibrating can also reduce confident errors, for example, in gender recognition by 95% on demographic groups different from the training data.", "target": ["在测试意外数据时，深度网络更有可能出现置信误差。我们提出了一种实验方法来研究这个问题，以及两种方法来减少未知输入分布上的置信误差。", "提出了两个减少过度置信的错误预测的想法。用额外的无监督数据对集合体进行 \"G-distillation\"，并利用新奇性检测器降低新奇性的置信度", "作者提出了两种方法来估计新的未见过的数据分布的分类置信度。第一个想法是使用集合方法作为基础方法来帮助识别不确定的情况，然后使用蒸馏方法将集合方法减少为模仿集合方法行为的单一模型。第二个想法是使用新颖性检测器分类器，并通过新颖性得分对网络输出进行加权。"]}
{"source": "Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.", "target": ["我们描述了一种实用的深度神经网络的优化算法，与广泛使用的算法相比，它工作得更快，生成的模型更好。", "提出了一种新的算法，他们声称隐含地使用了Hessian，并使用了幂级数的动机。", "提出了一种隐含使用曲率信息的新二阶算法，展示了算法中近似方案背后的直觉，并在各种实验中验证了启发式算法。"]}
{"source": "Recent work has shown that performing inference with fast, very-low-bitwidth  (e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurate  results. However, although 2-bit approximated networks have been shown to  be quite accurate, 1 bit approximations, which are twice as fast, have restrictively  low accuracy. We propose a method to train models whose weights are a mixture  of bitwidths, that allows us to more finely tune the accuracy/speed trade-off. We  present the “middle-out” criterion for determining the bitwidth for each value, and  show how to integrate it into training models with a desired mixture of bitwidths.  We evaluate several architectures and binarization techniques on the ImageNet  dataset. We show that our heterogeneous bitwidth approximation achieves superlinear  scaling of accuracy with bitwidth. Using an average of only 1.4 bits, we are  able to outperform state-of-the-art 2-bit architectures.", "target": ["我们引入了分数位宽近似，并表明它具有显著的优势。", "提出了一种在前向传播阶段改变神经网络中量化程度的方法", "在使用少于2位权重的情况下，保持2位网络字的准确性"]}
{"source": "Pruning units in a deep network can help speed up inference and training as well as reduce the size of the model. We show that bias propagation is a pruning technique which consistently outperforms the common approach of merely removing units,  regardless of the architecture and the dataset.   We also show how a simple adaptation to an existing scoring function allows us to select the best units to prune.   Finally,  we show that the units selected by the best performing scoring functions are somewhat consistent over the course of training, implying the dead parts of the network appear during the stages of training.", "target": ["平均替换是一种有效的方法，可以改善修剪后的损失，而基于泰勒近似的评分函数在绝对值方面效果更好。", "提出了对使用 \"平均替换 \"的单元修剪方法的简单改进。", "本文提出了一种平均替换的修剪策略，并利用绝对值泰勒扩展作为修剪的评分函数。"]}
{"source": "Due to the phenomenon of “posterior collapse,” current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective. We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information. Our proposed δ-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate our method’s efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 × 32.", "target": ["通过对速率的下限设定来避免后验塌陷。", "提出了一种通过将变分近似族限制为后部来防止VAE后验塌陷的方法", "本文介绍了对变分后验族的约束，使得KL项可以被控制以对抗深层生成模型（如VAEs）中的后后延塌陷。"]}
{"source": "Mini-batch gradient descent and its variants are commonly used in deep learning. The principle of mini-batch gradient descent is to use noisy gradient calculated on a batch to estimate the real gradient, thus balancing the computation cost per iteration and the uncertainty of noisy gradient. However, its batch size is a fixed hyper-parameter requiring manual setting before training the neural network. Yin et al. (2017) proposed a batch adaptive stochastic gradient descent (BA-SGD) that can dynamically choose a proper batch size as learning proceeds. We extend the BA-SGD to momentum algorithm and evaluate both the BA-SGD and the batch adaptive momentum (BA-Momentum) on two deep learning tasks from natural language processing to image classification. Experiments confirm that batch adaptive methods can achieve a lower loss compared with mini-batch methods after scanning the same epochs of data. Furthermore, our BA-Momentum is more robust against larger step sizes, in that it can dynamically enlarge the batch size to reduce the larger uncertainty brought by larger step sizes. We also identified an interesting phenomenon, batch size boom. The code implementing batch adaptive framework is now open source, applicable to any gradient-based optimization problems.", "target": ["我们开发了一种批处理自适应动量，与小批处理方法相比，在扫描相同的历时数据后，可以达到更低的损失，而且对大步长也更加稳健。", "本文解决了在深度学习训练过程中自动调整批次大小的问题，并声称将批次自适应SGD扩展到自适应动量，并将算法采用于复杂的神经网络问题。", "本文提出了一个算法的一般化，该算法通过向效用函数添加动量来执行具有自适应批次大小的SGD。"]}
{"source": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.   Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "target": ["我们使用元梯度来攻击图的深度神经网络的训练过程。", "研究学习一个更好的中毒图参数的问题，可以使图神经网络的损失最大化。", "一种通过增加/删除边缘来改变图结构的算法，从而降低节点分类的全局性能，以及使用元学习来解决双级优化问题的想法。"]}
{"source": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.", "target": ["我们表明，模块化结构模型在系统泛化方面是最好的，其端到端版本的泛化效果并不理想。", "本文通过引入一个新的空间推理数据集，评估了模块化神经网络和其他通用模型之间的系统泛化。", "对视觉推理模型的泛化进行了有针对性的实证评估，重点是在以字母和数字为特征的合成场景中识别（物体、关系、物体）三要素的问题。"]}
{"source": "The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them. Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments. Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents' behavior, and what events mediate the intensity and valence of social interactions. Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines.   As more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary. Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial.", "target": ["用于多代理学习的关系型前向模型能够准确预测代理的未来行为，它们产生了可解释的表征，并可在代理内部使用。", "一种通过具有显式模型（使用类似图形 conv 网络的体系结构）来减少模型自由学习中方差的方法，该模型包含其他代理将执行的操作。", "使用带有循环成分的关系型前向模型预测多代理行为，性能优于两个基线和两个消减。"]}
{"source": "We show that gradient descent on an unregularized logistic regression  problem, for almost all separable datasets, converges to the same direction as the max-margin solution. The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss. Furthermore,  we show this convergence is very slow, and only logarithmic in the  convergence of the loss itself. This can help explain the benefit  of continuing to optimize the logistic or cross-entropy loss even  after the training error is zero and the training loss is extremely  small, and, as we show, even if the validation loss increases. Our  methodology can also aid in understanding implicit regularization  in more complex models and with other optimization methods.", "target": ["梯度下降在逻辑回归上的归一化解（或类似的衰减损失）慢慢收敛到可分离数据上的L2最大边际解。", "本文提供了一个正式的证明，即在数据是线性可分的情况下，逻辑损失的梯度下降会非常缓慢地收敛到硬SVM的解决方案。", "本文着重描述了线性可分数据上对数损失最小化的行为，并表明用梯度下降法将对数损失最小化，会导致收敛到最大边际的解决方案。"]}
{"source": "Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier. Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals. To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image. Then we introduce two techniques for improving our networks' out-of-sample performance. The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable. The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's.  We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.", "target": ["在以前关于领域泛化的工作基础上，我们希望产生一个分类器，即使在训练期间没有领域标识符的情况下，也能泛化到以前未见过的领域。", "基于CNN和NGLCM输出层的线性投影方案，揭示语义信息的领域概括方法。", "本文提出了一种无监督的方法来识别对图像分类任务没有意义的图像特征"]}
{"source": "In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild. In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors. Our approach alternates between two threads. In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles. In the second, we minimize the approximated detection score by searching for the optimal camouflage. Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors.", "target": ["我们提出了一种学习物理车辆伪装的方法，以对抗性地攻击野外的物体检测器。我们发现我们的伪装是有效的和可转移的。", "作者研究了学习一种伪装模式的问题，这种模式在应用于模拟车辆时，将阻止物体探测器对其进行探测。", "本文以对抗性学习为目标，通过学习伪装模式进行干扰车检测"]}
{"source": "As deep learning-based classifiers are increasingly adopted in real-world applications, the importance of understanding how a particular label is chosen grows. Single decision trees are an example of a simple, interpretable classifier, but are unsuitable for use with complex, high-dimensional data. On the other hand, the variational autoencoder (VAE) is designed to learn a factored, low-dimensional representation of data, but typically encodes high-likelihood data in an intrinsically non-separable way.   We introduce the differentiable decision tree (DDT) as a modular component of deep networks and a simple, differentiable loss function that allows for end-to-end optimization of a deep network to compress high-dimensional data for classification by a single decision tree.   We also explore the power of labeled data in a  supervised VAE (SVAE) with a Gaussian mixture prior, which leverages label information to produce a high-quality generative model with improved bounds on log-likelihood.   We combine the SVAE with the DDT to get our classifier+VAE (C+VAE), which is competitive in both classification error and log-likelihood, despite optimizing both simultaneously and using a very simple encoder/decoder architecture.", "target": ["我们将可微分决策树与监督变分自动编码器相结合，以增强分类的可解释性。", "本文提出了一个由变分自动编码器与可微决策树组成的混合模型，以及一个配套的训练方案，实验证明了树的分类性能、负对数似然性能和潜在空间的可解释性。", "本文试图通过堆叠有监督的VAE和可微的决策树来建立一个可解释的和准确的分类器。"]}
{"source": "We propose Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain. We first estimate importance weights using labeled source data and unlabeled target data, and then train a classifier on the weighted source samples. We derive a generalization bound for the classifier on the target domain which is independent of the (ambient) data dimensions, and instead only depends on the complexity of the function class. To the best of our knowledge, this is the first generalization bound for the label-shift problem where the labels in the target domain are not available. Based on this bound, we propose a regularized estimator for the small-sample regime which accounts for the uncertainty in the estimated weights. Experiments on the CIFAR-10 and MNIST datasets show that RLLS improves classification accuracy, especially in the low sample and large-shift regimes, compared to previous methods.", "target": ["在源数据集和目标数据集之间存在标签转移的情况下，训练高效分类器的实用和可证明的保证方法", "作者提出了一种新的算法，以两步程序提高类重要性加权估计程序的稳定性。", "作者考虑了标签转移下的学习问题，即标签比例不同而条件相等，并提出了一个改进的正则化估计器。"]}
{"source": "The statistics of the real visual world presents a long-tailed distribution: a few classes have significantly more training instances than the remaining classes in a dataset. This is because the real visual world has a few classes that are common while others are rare. Unfortunately, the performance of a convolutional neural network is typically unsatisfactory when trained using a long-tailed dataset. To alleviate this issue, we propose a method that discriminatively learns an embedding in which a simple Bayesian classifier can balance the class-priors to generalize well for rare classes. To this end, the proposed approach uses a Gaussian mixture model to factor out class-likelihoods and class-priors in a long-tailed dataset. The proposed method is simple and easy-to-implement in existing deep learning frameworks. Experiments on publicly available datasets show that the proposed approach improves the performance on classes with few training instances, while maintaining a comparable performance to the state-of-the-art on classes with abundant training examples.", "target": ["提高尾部类分类准确性的方法。", "本文的主要目标是学习一个ConvNet分类器，该分类器对处于类发生率分布尾部的类表现得更好。", "建议采用高斯混合模型的贝叶斯框架来解决分类应用中的一个问题，即不同类别的训练数据的数量是不平衡的。"]}
{"source": "As deep reinforcement learning is being applied to more and more tasks, there is a growing need to better understand and probe the learned agents. Visualizing and understanding the decision making process can be very valuable to comprehend and identify problems in the learned behavior. However, this topic has been relatively under-explored in the reinforcement learning community. In this work we present a method for synthesizing states of interest for a trained agent. Such states could be situations (e.g. crashing or damaging a car) in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved (e.g. risky states) are often interesting to understand the situational awareness of the system. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insightful visualizations for a variety of environments and reinforcement learning methods. We explore these issues in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify significant decision scenarios with this technique, we believe this general approach could serve as an important tool for AI safety applications.", "target": ["我们提出了一种为强化学习代理合成感兴趣的状态的方法，以分析其行为。", "本文提出了一个RL中视觉观察的生成模型，它能够生成感兴趣的观察结果。", "一种可视化感兴趣状态的方法，涉及学习重建状态空间的变分自动编码器和查找条件参数以生成合成图像的优化步骤。"]}
{"source": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "target": ["一个用新的损失函数训练的深度弃权神经网络，学习何时弃权的表征，在不同类型的噪声存在下实现稳健学习。", "一种新的损失函数，用于训练深度神经网络，该神经网络可以弃权，从存在结构化噪声，存在非结构化噪声和开放世界检测的角度看待性能。", "本稿件介绍了深度弃权分类器，它用弃权损失修改了多类交叉熵损失，然后将其应用于扰动图像分类任务。"]}
{"source": "Temporal Difference learning with function approximation has been widely used recently and has led to several successful results.   However, compared with the original tabular-based methods, one major drawback of temporal difference learning with neural networks and other function approximators is that they tend to over-generalize across temporally successive states, resulting in slow convergence and even instability. In this work, we propose a novel TD learning method, Hadamard product Regularized TD (HR-TD), that reduces over-generalization and thus leads to faster convergence. This approach can be easily applied to both linear and nonlinear function approximators.   HR-TD is evaluated on several linear and nonlinear benchmark domains, where we show improvement in learning behavior and performance.", "target": ["一种用于TD学习的正则化技术，避免了时间上的过度泛化，尤其是在深度网络中。", "在函数近似的情况下，对时间差异学习的一种变体，试图解决跨时间连续状态的过度泛化问题。", "本文介绍了HR-TD，这是TD（0）算法的变体，旨在改善传统TD中的过度泛化问题。"]}
{"source": "We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals.   To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters. Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation. As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters. We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation. Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.", "target": ["我们提出了一个新的CNN内核，用于球形信号的非结构化网格，并在三维分类和全向图像分割等任务上显示出明显的准确性和参数效率的提高。", "一种高效的方法使球形数据的深度学习成为可能，与流行的方法相比，用更少的参数就能达到有竞争力/最先进的数字。", "本文提出了一种用于非结构化网格的CNN的新型卷积核，并通过微分算子的线性组合来表述卷积。"]}
{"source": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "target": ["在视觉预测任务中，让你的预测模型选择哪些时间进行预测有两点好处。(i)提高预测质量，(ii)导致语义上连贯的 \"瓶颈状态 \"预测，这对规划很有用。", "一种关于视频中帧预测的方法，该方法包括目标预测是浮动的，由预测误差的最小值解决。", "重新制定视频预测/插值的任务，使预测器不被强迫在固定的时间间隔内生成帧，而是被训练成在未来任何时间点发生的帧。"]}
{"source": "In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly. We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process. First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes. Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location. Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior. We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy.", "target": ["我们用一个LSTM来检测智能手机何时走进大楼。然后我们利用智能手机上的传感器的数据预测设备的楼层高度。", "本文介绍了一个系统，通过他们的移动设备的传感器数据，使用LSTM和气压的变化来估计一个楼层的高度。", "建议采用两步法来确定移动电话在高层建筑内的哪个楼层。"]}
{"source": "Sparse reward is one of the most challenging problems in reinforcement learning (RL). Hindsight Experience Replay (HER) attempts to address this issue by converting a failure experience to a successful one by relabeling the goals. Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation. We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation. We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn. We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons. We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, but we also found that little amount of hindsight advice is sufficient for the learning to take off, showing the practical aspect of the method.", "target": ["将语言目标表述与事后的经验复述结合起来。", "本文考虑了后知后觉的经验复述中隐含的假设，即可以获得从状态到目标的映射，并提出了一种自然语言的目标表述。", "本报告使用带有自然语言目标的Hindsight Experience Replay框架来提高指令遵循模型的样本效率。"]}
{"source": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "target": ["我们提出了一个联合编码本和因子化方案来改善二阶集合。", "本文提出了一种将现有的因子化二阶表征与编码本式的硬分配相结合的方法。", "提出了一种基于编码本模型的新型双线性表示法，以及一种有效的公式，其中基于编码本的投影通过共享投影进行因子化，以进一步减少参数大小。"]}
{"source": "Natural language understanding research has recently shifted towards complex Machine Learning and Deep Learning algorithms. Such models often outperform their simpler counterparts significantly. However, their performance relies on the availability of large amounts of labeled data, which are rarely available. To tackle this problem, we propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision. We apply this methodology on biomedical relation extraction, a task where training datasets are excessively time-consuming and expensive to create, yet has a major impact on downstream applications such as drug discovery. We demonstrate in two small-scale controlled experiments that our method consistently enhances the performance of an LSTM network, with performance improvements comparable to hand-labeled training data. Finally, we discuss the optimal setting for applying weak supervision using this methodology.", "target": ["我们提出并应用了一种基于弱监督的元学习方法，在生物医学关系提取的任务上将半监督和集合学习相结合。", "一种用于关系分类的半监督方法，它使用一个小的标记数据集训练多个基础学习者，并应用其中的一些来注释未标记的例子，以进行半监督学习。", "本文解决了生成生物关系提取的训练数据的问题，并使用由弱分类器标记的数据的预测作为元学习算法的额外训练数据。", "本文提出了半监督学习和集合学习相结合的信息提取方法，并在生物医学关系提取任务中进行了实验"]}
{"source": "We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations. CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly. Our approach offers two major advantages: (i) for each prediction, valid instance-specific explanations are generated with no computational overhead and (ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings. We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations. Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support.", "target": ["一类网络在即时生成简单的模型（称为解释），作为正则器，使模型的诊断和可解释性一致。", "作者称，以前的技术直接将神经网络作为组件整合到图形模型中，这使得模型无法被解释。", "通过使用深度神经网来预测图形模型的参数，提出了神经网和图形模型相结合的建议。"]}
{"source": "The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations. Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks. However, GAIL requires a huge number of interactions with environment during training. We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced. To this end, we propose a model free, off-policy IL algorithm for continuous control. The keys of our algorithm are two folds: 1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG), 2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations. Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.", "target": ["我们提出了一种无模型的模仿学习算法，与最先进的模仿学习算法即GAIL相比，该算法能够减少与环境的交互次数。", "提出扩展确定性策略梯度算法，从演示中学习，同时结合专家的一种密度估计。", "本文考虑了无模型模仿学习的问题，并提出了生成式对抗性模仿学习算法的扩展，即用确定性的策略取代学习者的随机策略。", "本文结合了IRL、对抗性训练和确定性政策梯度的思想，目的是降低样本的复杂性。"]}
{"source": "Convolution acts as a local feature extractor in convolutional neural networks (CNNs). However, the convolution operation is not applicable when the input data is supported on an irregular graph such as with social networks, citation networks, or knowledge graphs. This paper proposes the topology adaptive graph convolutional network (TAGCN), a novel graph convolutional network that generalizes CNN architectures to graph-structured data and provides a systematic way to design a set of fixed-size learnable filters to perform convolutions on graphs. The topologies of these filters are adaptive to the topology of the graph when they scan the graph to perform convolution, replacing the square filter for the grid-structured data in traditional CNNs. The outputs are the weighted sum of these filters’ outputs, extraction of both vertex features and strength of correlation between vertices. It  can be used with both directed and undirected graphs. The proposed TAGCN not only inherits the properties of convolutions in CNN for grid-structured data, but it is also consistent with convolution as defined in graph signal processing. Further, as no approximation to the convolution is needed, TAGCN exhibits better performance than existing graph-convolution-approximation methods on a number  of data sets. As only the polynomials of degree two of the adjacency matrix are used, TAGCN is also computationally simpler than other recent methods.", "target": ["计算复杂度低的图形CNN（无近似），分类精度更高", "提出了一种新的CNN图形分类方法，使用基于长度增加的外向行走的过滤器，在一个传播步骤中纳入来自更远的顶点的信息。", "提出了用于半监督图分类的新的神经网络结构，建立在图多项式过滤器的基础上，并将其用于具有ReLU激活函数的连续神经网络层。", "本文介绍了拓扑自适应GCN，将卷积网络推广到图形结构的数据中。"]}
{"source": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.", "target": ["我们表明，灾难性的遗忘发生在被认为是单一任务的范围内，并发现不容易遗忘的例子可以从训练集中删除而不损失泛化。", "研究了SGD过程中训练实例的遗忘行为，并表明在不同的网络结构中，神经网络训练存在 \"支持实例\"。", "本文分析了网络在多大程度上学会了对特定的例子进行正确分类，然后在训练过程中忘记这些例子。", "本文研究了训练神经网络中的一些例子是否比其他例子更难学习。这样的例子在学习中会被遗忘，并多次被重新学习。"]}
{"source": "Discovering objects and their attributes is of great importance for autonomous agents to effectively operate in human environments. This task is particularly challenging due to the ubiquitousness of objects and all their nuances in perceptual and semantic detail. In this paper we present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos. These continuous representations are not biased by or limited by a discrete set of labels determined by human labelers. The proposed representation is trained with a metric learning loss, where objects with homogeneous features are pushed together, while those with heterogeneous features are pulled apart. We show these unsupervised embeddings allow to discover object attributes and can enable robots to self-supervise in previously unseen environments. We quantitatively evaluate performance on a large-scale synthetic dataset with 12k object models, as well as on a real dataset collected by a robot and show that our unsupervised object understanding generalizes to previously unseen objects. Specifically, we demonstrate the effectiveness of our approach on robotic manipulation tasks, such as pointing at and grasping of objects. An interesting and perhaps surprising finding in this approach is that given a limited set of objects, object correspondences will naturally emerge when using metric learning without requiring explicit positive pairs.", "target": ["一种无监督的方法，完全从无标签的单眼视频中学习物体的分离表征。", "从不同视点拍摄的场景视频序列中设计出一个特征表征。", "提出了一种用于视觉输入的无监督表征学习方法，该方法包含了一种度量学习方法，在嵌入空间中将图像块的近邻对拉近，而将其他对推开。", "本文探讨了物体表征的自我监督学习，其主要思想是鼓励具有相似特征的物体进一步相互吸引。"]}
{"source": "Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions.   We introduce state aligned vector rewards, which are easily defined in metric state spaces and allow our deep reinforcement learning agent to tackle the curse of dimensionality.   Our agent learns to map from action distributions to state change distributions implicitly defined in a quantile function neural network.    We further introduce a new reinforcement learning technique inspired by quantile regression which does not limit agents to explicitly parameterized action distributions.   Our results in high dimensional state spaces show that training with vector rewards allows our agent to learn multiple times faster than an agent training with scalar rewards.", "target": ["我们用状态对齐的向量奖励来训练一个从行动分布中预测状态变化的代理，使用一种新的强化学习技术，其灵感来自于分位数回归。", "提出了旨在于奖励与状态空间对齐的情况下加速强化学习的算法。", "本文通过使用一个重新参数化的策略和一个新的基于向量的训练目标来解决连续行动空间中的RL。", "这项工作提出将分布式RL与负责以分位数的方式对世界的演变进行建模的网混合在一起，声称可以改善样本效率。"]}
{"source": "We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.", "target": ["我们提出了Episodic Backward Update，这是一种新颖的深度强化学习算法，可以逐集采样转换并以向后方式递归更新值，以实现快速稳定的学习。", "提出了一个新的DQN，在这个DQN中，目标是通过向后更新（从头到尾）在一个完整的集上计算的，以便在集结束时更快地传播奖赏。", "作者提出修改DQN算法，在有一定衰减的轨迹上递归地应用最大贝尔曼算子，以防止嵌套的最大误差积累。", "在deep-Q网络中，从集结束时开始更新Q值，以促进奖励沿集快速传播。"]}
{"source": "Survival Analysis (time-to-event analysis) in the presence of multiple possible adverse events, i.e., competing risks, is a challenging, yet very important problem in medicine, finance, manufacturing, etc. Extending classical survival analysis to competing risks is not trivial since only one event (e.g. one cause of death) is observed and hence, the incidence of an event of interest is often obscured by other related competing events. This leads to the nonidentifiability of the event times’ distribution parameters, which makes the problem significantly more challenging. In this work we introduce Siamese Survival Prognosis Network, a novel Siamese Deep Neural Network architecture that is able to effectively learn from data in the presence of multiple adverse events. The Siamese Survival Network is especially crafted to issue pairwise concordant time-dependent risks, in which longer event times are assigned lower risks. Furthermore, our architecture is able to directly optimize an approximation to the C-discrimination index, rather than relying on well-known metrics of cross-entropy etc., and which are not able to capture the unique requirements of survival analysis with competing risks. Our results show consistent performance improvements on a number of publicly available medical datasets over both statistical and deep learning state-of-the-art methods.", "target": ["在这项工作中，我们介绍了一种新的Siamese深度神经网络架构，该架构能够在存在多种不良事件的情况下有效地从数据中学习。", "本文通过直接对C指数进行优化，将siamese神经网络引入竞争性风险框架。", "作者讨论了在有竞争风险的生存分析环境中估计风险的问题，并建议使用siamese生存网络直接优化随时间变化的区分指数。"]}
{"source": "The digitization of data has resulted in making datasets available to millions of users in the form of relational databases and spreadsheet tables. However, a majority of these users come from diverse backgrounds and lack the programming expertise to query and analyze such tables. We present a system that allows for querying data tables using natural language questions, where the system translates the question into an executable SQL query. We use a deep sequence to sequence model in wich the decoder uses a simple type system of SQL expressions to structure the output prediction. Based on the type, the decoder either copies an output token from the input question using an attention-based copying mechanism or generates it from a fixed vocabulary. We also introduce a value-based loss function that transforms a distribution over locations to copy from into a distribution over the set of input tokens to improve training of our model. We evaluate our model on the recently released WikiSQL dataset and show that our model trained using only supervised learning significantly outperforms the current state-of-the-art Seq2SQL model that uses reinforcement learning.", "target": ["我们提出了一个基于类型的指针网络模型以及基于值的损失方法，以有效地训练一个神经模型，将自然语言翻译成SQL。", "本文声称开发了一种新颖的方法，通过使用语法来指导解码，并使用新的损失函数来指导指针/复制机制，将自然语言查询映射到SQL。"]}
{"source": "To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity. Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric \"self-control\" baseline function together with the REINFORCE estimator in that augmented space. Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers. Python code for reproducible research is publicly available.", "target": ["离散潜在变量模型的无偏低方差梯度估计器", "提出了一种新的方差减小技术，用于计算预期损失梯度，其中预期是关于独立二元随机变量的。", "在随机二元网络的特殊情况下，降低得分函数梯度估计器方差的Rao-Blackwellization和一般的随机数相结合的算法", "用于计算和反向传播二元神经网络梯度的无偏和低方差的增强-REINFORCE-合并（ARM）估计器"]}
{"source": "Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training. The scheme can reach a linear speed-up with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits. To overcome this communication bottleneck recent works propose to reduce the communication frequency. An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while. This scheme shows promising results in practice, but eluded thorough theoretical analysis.       We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size. The number of  communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD. This also holds for asynchronous implementations.   Local SGD can also be used for large scale training of deep learning models. The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.", "target": ["我们证明，并行本地SGD实现了线性加速，而通信量比并行小批量SGD少得多。", "为本地SGD提供了一个收敛证明，并证明了本地SGD可以提供与minibatch相同的加速收益，但可能能够显著减少通信。", "本文提出了对本地SGD的分析，以及对运行SGD得到的估计器需要多频繁地进行平均化才能产生线性并行化速度的界限。", "作者分析了本地SGD算法，其中SGD的$K$并行链被运行，迭代结果偶尔通过平均化在机器间同步。"]}
{"source": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "target": ["动态过程的紧凑感知", "研究用信息瓶颈法紧凑地表示复杂动态系统的模型，同时保留信息的问题。", "本文研究了高斯线性动态，并提出了一种计算信息瓶颈层次的算法（IBH）。"]}
{"source": "We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly. As the density of the connection increases, the number of paths through which the gradient flows can be increased. It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time. Larger gradients, however, can also cause exploding gradient problem. To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows. We describe the relation between the attention gate and the gradient flows by approximation. The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the model’s performance.", "target": ["密集的RNN，从每个隐藏状态到所有层的多个前面的隐藏状态有完全的连接。", "提出了一种新的RNN架构，可以更好地模拟长期依赖关系，可以学习顺序数据的多尺度表示，并通过使用参数化的门控单元避开梯度问题。", "本文提出了一种全连接的密集型RNN结构，该结构具有每一层的门控连接和前一层的连接，以及它在PTB字符级建模任务上的结果。"]}
{"source": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm’s ability to generate convincing, identity-matched photographs.", "target": ["SD-GAN根据数据集中的已知共性（例如，描绘同一个人的照片）来分解潜伏代码。", "本文研究了受控图像生成的问题，并提出了一种能产生一对相同身份的图像的算法。", "本文提出了，SD-GAN，一种训练GAN的方法，以分解潜伏向量输入Z中的身份和非身份信息。"]}
{"source": "The goal of unpaired cross-domain translation is to learn useful mappings between two domains, given unpaired sets of datapoints from these domains. While this formulation is highly underconstrained, recent work has shown that it is possible to learn mappings useful for downstream tasks by encouraging approximate cycle consistency in the mappings between the two domains [Zhu et al., 2017]. In this work, we propose AlignFlow, a framework for unpaired cross-domain translation that ensures exact cycle consistency in the learned mappings. Our framework uses a normalizing flow model to specify a single invertible mapping between the two domains. In contrast to prior works in cycle-consistent translations, we can learn AlignFlow via adversarial training, maximum likelihood estimation, or a hybrid of the two methods. Theoretically, we derive consistency results for AlignFlow which guarantee recovery of desirable mappings under suitable assumptions. Empirically, AlignFlow demonstrates significant improvements over relevant baselines on image-to-image translation and unsupervised domain adaptation tasks on benchmark datasets.", "target": ["我们提出了一个跨域翻译的学习框架，该框架完全是循环一致的，可以通过对抗性训练、最大似然估计或混合方式学习。", "提出了AlignFlow，这是一种利用可逆流实现周期一致性原则的有效方法。", "非配对图像到图像转换的流模型"]}
{"source": "Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, that maps the inputs to their corresponding outputs exactly. Due to its precise and combinatorial nature, it is commonly formulated as a constraint satisfaction problem, where input-output examples are expressed constraints, and solved with a constraint solver. A key challenge of this formulation is that of scalability: While constraint solvers work well with few well-chosen examples, constraining the entire set of example constitutes a significant overhead in both time and memory. In this paper we address this challenge by constructing a representative subset of examples that is both small and is able to constrain the solver sufficiently. We build the subset one example at a time, using a trained discriminator to predict the probability of unchosen input-output examples conditioned on the chosen input-output examples, adding the least probable example to the subset. Experiment on a diagram drawing domain shows our approach produces subset of examples that are small and representative for the constraint solver.", "target": ["在程序合成的背景下，输入一组例子，我们通过计算代表例子的子集来减少成本", "提出了一种确定程序合成的代表性例子的方法，以提高现有约束性编程解决方案的可扩展性。", "一种选择实例子集的方法，在此基础上运行约束解算器，以解决程序综合问题。", "本文提出了一种加快通用程序合成器的方法。"]}
{"source": "Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.", "target": ["我们介绍了循环关系网络，这是一个用于关系推理的强大而通用的神经网络模块，并利用它解决了96.6%最难的Sudokus和19/20 BaBi任务。", "引入了循环关系网络（RRN）,可以添加到任何神经网络中以增加关系推理能力。", "介绍一个用于结构化预测的深度神经网络，该网络在Soduku谜题和BaBi任务上取得了最先进的性能。", "本文介绍了一种称为关系网络的方法，为深度神经网络增加关系推理能力。"]}
{"source": "Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification. In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM. We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors. These two facts answer a fundamental question---what the minimal supervision is for training any binary classifier from only U data. Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent. Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data.", "target": ["仅从 U 数据训练深度模型所需的三个类先验就足够了，而任何两个都不够。", "提出了一个无偏估计器，允许在两个具有已知类先验的无标签数据集上用弱监督训练模型，并讨论了估计器的理论属性。", "一种仅从无标签数据中训练任何二元分类器的方法，以及一种针对两组无标签数据的经验性风险最小化方法，其中类别先验是给定的。"]}
{"source": "Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.", "target": ["从许多小图像块中聚集类证据足以解决ImageNet，产生更多可解释的模型，并能解释流行的DNN的决策方面。", "本文提出了一种新颖而紧凑的神经网络结构，它使用了词包特征中的信息。建议的算法只独立使用补丁信息，并使用独立分类的图像块进行多数投票。"]}
{"source": "Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) is an unmet challenge that is intractable with current state-of-the-art mutation calling methods. Specifically, the limit of VAF detection is closely related to the depth of coverage, due to the requirement of multiple supporting reads in extant methods, precluding the detection of mutations at VAFs that are orders of magnitude lower than the depth of coverage. Nevertheless, the ability to detect cancer-associated mutations in ultra low VAFs is a fundamental requirement for low-tumor burden cancer diagnostics applications such as early detection, monitoring, and therapy nomination using liquid biopsy methods (cell-free DNA). Here we defined a spatial representation of sequencing information adapted for convolutional architecture that enables variant detection at VAFs, in a manner independent of the depth of sequencing. This method enables the detection of cancer mutations even in VAFs as low as 10x-4^, >2 orders of magnitude below the current state-of-the-art. We validated our method on both simulated plasma and on clinical cfDNA plasma samples from cancer patients and non-cancer controls. This method introduces a new domain within bioinformatics and personalized medicine – somatic whole genome mutation calling for liquid biopsy.", "target": ["目前的体细胞突变方法对液体活检（即低覆盖率测序）不起作用，我们将一个CNN架构应用于读数的独特表示和其识别，我们显示在低频设置中比以前的方法有显著的改进。", "提出了一个基于CNN的解决方案，称为Kittyhawk，用于超低等位基因频率下的体细胞突变调用。", "一种从无细胞DNA测序中检测癌症突变的新算法，该算法将识别测序错误与真正突变的序列背景。", "本文提出了一种深度学习框架，用于预测在检测无细胞DNA肿瘤时发生的极低频率的体细胞突变。"]}
{"source": "This paper presents the formal release of {\\em MedMentions}, a new manually annotated resource for the recognition of biomedical concepts. What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines. In addition to the full corpus, a sub-corpus of MedMentions is also presented, comprising annotations for a subset of UMLS 2017 targeted towards document retrieval. To encourage research in Biomedical Named Entity Recognition and Linking, data splits for training and testing are included in the release, and a baseline model and its metrics for entity linking are also described.", "target": ["本文介绍了一个新的黄金标准的生物医学科学文献语料库，该语料库由UMLS概念提及的人工注释而成。", "详细介绍了一个涵盖生物医学概念的人工注释数据集的构建，该数据集比以前的数据集更大，并由更大的本体所覆盖。", "本文使用MedMentions，一个用于端到端概念识别的TaggerOne半马尔可夫模型，并在一组Pubmed摘要上链接，为论文贴上生物医学概念/实体标签。"]}
{"source": "In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm. It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder. A clustering network transforms the data into another space and then selects one of the clusters. Next, the autoencoder associated with this cluster is used to reconstruct the data-point. The clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders. The optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network. Unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.", "target": ["我们提出了一种深度聚类方法，其中每个聚类由一个自动编码器来代表，而不是中心点。", "提出了基于自动编码器混合物的深度聚类，如果用自动编码器网络来表示，数据点就会被分配到一个聚类中，而这个聚类是基于表示误差的。", "一种深度聚类方法，使用自动编码器框架同时学习数据的低维嵌入，同时使用深度神经网络进行数据聚类。", "一种深度聚类方法，用不同的自动编码器表示每个聚类，以端到端的方式工作，也可用于对新进入的数据进行聚类，而不需要重做整个聚类程序。"]}
{"source": "We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu. We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis. The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared. Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions. We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.", "target": ["我们定义了一个新的积分概率度量（Sobolev IPM），并展示了它如何被用于训练GAN的文本生成和半监督学习。", "提出了一种基于Sobolev范数的GAN的新型正则化方案，衡量导数的L2范数之间的偏差。", "作者提供了另一种类型的GAN，使用GAN的典型设置，但有不同的函数类，并制作了一个用这种函数类训练GAN的配方。", "该论文为GAN critics提出了一个不同的梯度惩罚，强制梯度的预期平方范数等于1"]}
{"source": "We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.", "target": ["我们提出了一种新的方法，用混合生成器训练GAN，以克服模式崩溃问题。", "利用生成器的受限混合分布和预测源混合成分的辅助分类器，解决GAN中的模式崩溃问题。", "本文提出了一种混合生成器来训练GAN，而没有额外的计算成本", "作者提出，使用MGAN，旨在通过混合生成器克服模型崩溃问题，取得了最先进的结果。"]}
{"source": "Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons.   Though, given  the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts. We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English. The platform also provides a hand-crafted bot agent, which simulates a human teacher.   We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties.", "target": ["我们提出了BabyAI平台，用于研究有人类参与的语言学习的数据效率。", "提出了一个研究平台，其中有一个机器人在循环中学习执行语言指令，其中语言有构成性结构", "介绍了一个基础语言学习的平台，该平台用启发式教师取代了循环中的任何人类，并使用映射到2D网格世界的合成语言。"]}
{"source": "Recently, there has been growing interest in methods that perform neural network compression, namely techniques that attempt to substantially reduce the size of a neural network without significant reduction in performance. However, most existing methods are post-processing approaches in that they take a learned neural network as input and output a compressed network by either forcing several parameters to take the same value (parameter tying via quantization) or pruning irrelevant edges (pruning) or both. In this paper, we propose a novel algorithm that jointly learns and compresses a neural network. The key idea in our approach is to change the optimization criteria by adding $k$ independent Gaussian priors over the parameters and a sparsity penalty. We show that our approach is easy to implement using existing neural network libraries, generalizes L1 and L2 regularization and elegantly enforces parameter tying as well as pruning constraints. Experimentally, we demonstrate that our new algorithm yields state-of-the-art compression on several standard benchmarks with minimal loss in accuracy while requiring little to no hyperparameter tuning as compared with related, competing approaches.", "target": ["k-means先验与L1正则化相结合，产生了最先进的压缩结果。", "本文探讨了DNN/CNN的软参数捆绑和压缩问题"]}
{"source": "The application of stochastic variance reduction to optimization has shown remarkable recent theoretical and practical success. The applicability of these techniques to the hard non-convex optimization problems encountered during training of modern deep neural networks is an open problem. We show that naive application of the SVRG technique and related approaches fail, and explore why.", "target": ["SVRG方法在现代深度学习问题上失败了", "本文对SVRG风格的方法进行了分析，显示出信息缺失、批量规范、数据增强（随机裁剪/轮换/翻译）往往会增加更新的偏差和/或变异。", "本文研究了SVGD对现代神经网络的适用性，并表明SVGD的天真应用通常会失败。"]}
{"source": "The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks. One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on non-euclidean surfaces. In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere. A cascade set of geodesic disk filters rotate on the 2-sphere and collect spherical patterns and so to extract geometric features for various 3D shape analysis tasks. We demonstrate theoretically and experimentally that our proposed method has the possibility to bridge the gap between 2D images and 3D shapes with the desired rotation equivariance/invariance, and its effectiveness is evaluated in applications of non-rigid/ rigid shape classification and shape retrieval.", "target": ["一种将深度学习应用于三维表面的方法，使用其球形描述符和2-球体上的alt-az各向异性卷积。", "通过用过滤器旋转代替过滤器平移，提出了单位球体上的极性各向异性卷积方案。", "本文探讨了使用alt-az各向异性2-球体卷积的3D形状的深度学习。"]}
{"source": "Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.   One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.", "target": ["利用CLT近似的局部重参数化训练二元/三元网络", "使用反向传播训练二元和三元权重分布网络，用重参数化技巧对神经元预激活进行采样", "本文建议使用随机参数与局部重参数技巧相结合来训练具有二元或三元权重的神经网络，从而获得最先进的结果。"]}
{"source": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.   Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "target": ["最佳完成度蒸馏法（Optimal Completion Distillation）是一种基于编辑距离的序列到序列模型的优化训练程序，它在端到端语音识别任务上达到了最先进的水平。", "使用动态程序训练seq2seq模型的替代方法来计算预测前缀的最佳延续性", "一种自动回归模型的训练算法，不需要任何MLE预训练，可以直接从采样中进行优化。", "本文考虑了使用最大似然估计训练的序列对序列模型的缺陷，并提出了一种基于编辑距离和训练期间隐含使用给定标签序列的方法。"]}
{"source": "As an emerging field, federated learning has recently attracted considerable attention.  Compared to distributed learning in the datacenter setting, federated learning  has more strict constraints on computate efficiency of the learned model and communication  cost during the training process. In this work, we propose an efficient  federated learning framework based on variational dropout. Our approach is able  to jointly learn a sparse model while reducing the amount of gradients exchanged  during the iterative training process. We demonstrate the superior performance  of our approach on achieving significant model compression and communication  reduction ratios with no accuracy loss.", "target": ["联合学习的联合模型和梯度稀疏化方法", "应用变分丢弃来减少神经网络分布式训练的通信成本，并在mnist、cifar10和svhn数据集上做了实验。", "作者提出了一种算法，通过将稀疏梯度从设备发送到服务器并返回，来减少联合学习中的通信成本。", "将分布式优化算法与变分丢弃相结合，使本地学习者发送给主服务器的梯度更加稀疏。"]}
{"source": "We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.   Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.   Our method only requires the relatively inexpensive sequential training of T \"shallow ResNets\". We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.   In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.   A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.", "target": ["我们证明了ResNet架构的多类提升理论，该理论同时创造了一种新的多类提升技术，并为ResNet风格的架构提供了一种新算法。", "提出了一种训练深度残差网络的提升式算法，对训练误差进行收敛分析，并对泛化能力进行了分析。", "一种使用提升框架的ResNet学习方法，分解了复杂网络的学习，并使用较少的计算成本。", "作者提出将深度ResNet作为一种提升算法，他们声称这比标准的端到端反向传播更有效。"]}
{"source": "We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously.  \t  Inspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t  1. All local minima of $G$ are also global minima.  2. All global minima of $G$ correspond to the ground truth parameters.  3. The value and gradient of $G$ can be estimated using samples. \t  With these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations.", "target": ["本文分析了单隐层神经网络的优化情况，并设计了一个新的目标，证明其没有虚假的局部最小值。", "本文研究了单隐层神经网络的学习问题，建立了最小二乘群体损失和Hermite多项式之间的联系，并提出了一个新的损失函数。", "一种张量因子化的方法，用于倾斜一个隐藏层的神经网络"]}
{"source": "Open information extraction (OIE) systems extract relations and their   arguments from natural language text in an unsupervised manner. The resulting   extractions are a valuable resource for downstream tasks such as knowledge   base construction, open question answering, or event schema induction. In this   paper, we release, describe, and analyze an OIE corpus called OPIEC, which was   extracted from the text of English Wikipedia. OPIEC complements the available   OIE resources: It is the largest OIE corpus publicly available to date (over   340M triples) and contains valuable metadata such as provenance information,   confidence scores, linguistic annotations, and semantic annotations including   spatial and temporal information. We analyze the OPIEC corpus by comparing its   content with knowledge bases such as DBpedia or YAGO, which are also based on   Wikipedia. We found that most of the facts between entities present in OPIEC   cannot be found in DBpedia and/or YAGO, that OIE facts    often differ in the level of specificity compared to knowledge base facts, and   that OIE open relations are generally highly polysemous. We believe that the   OPIEC corpus is a valuable resource for future research on automated knowledge   base construction.", "target": ["一个开放的信息提取语料库及其深入分析", "为信息提取建立一个新的语料库，这个语料库比之前的公共语料库要大，并且包含当前语料库中不存在的信息。", "介绍了在最近的提取系统的帮助下从维基百科上收集的开放式IE三要素的数据集。", "本文描述了通过自动方式在英语维基百科上创建开放IE语料库的过程。"]}
{"source": "The process of designing neural architectures requires expert knowledge and extensive trial and error.  While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.  We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.  The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.   Using two different candidate generation techniques, random search with a ranking function and reinforcement learning,  we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.  The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.", "target": ["我们为RNN架构的生成定义了一个灵活的DSL，允许不同大小和复杂程度的RNN，并提出了一个将RNN表示为递归神经网络的排名函数，模拟其性能以决定最有前途的架构。", "介绍了一种新的方法来生成RNN架构，使用一种特定领域的语言来生成两种类型的生成器（随机和基于RL的）以及一个排序函数和评估器。", "本文将寻找良好的RNN单元结构作为一个黑箱优化问题，其中例子被表示为运算符树，并根据学习到的函数或RL代理生成的函数进行评分。", "本文通过使用指定RNN循环操作的DSL，研究了RNN背景下自动架构搜索的元学习策略。"]}
{"source": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "target": ["我们在DNN中只应用低位宽整数的训练和推理", "一种叫做WAGE的方法，它将神经网络中的所有操作数和运算符，以减少网络中的表示位数。", "作者提出在神经网络的训练和测试时间内，对权重、激活、梯度和误差进行离散化处理。"]}
{"source": "Modern Convolutional Neural Networks (CNNs) are complex, encompassing millions of parameters. Their deployment exerts computational, storage and energy demands, particularly on embedded platforms. Existing approaches to prune or sparsify CNNs require retraining to maintain inference accuracy. Such retraining is not feasible in some contexts. In this paper, we explore the sparsification of CNNs by proposing three model-independent methods. Our methods are applied on-the-fly and require no retraining. We show that the state-of-the-art models' weights can be reduced by up to 73% (compression factor of 3.7x) without incurring more than 5% loss in Top-5 accuracy. Additional fine-tuning gains only 8% in sparsity, which indicates that our fast on-the-fly methods are effective.", "target": ["在本文中，我们开发了快速的无再训练的稀疏化方法，可以在许多工业背景下对CNN进行即时的稀疏化部署。", "本文通过引入三种方案来确定修剪权重的阈值，提出了无需再训练的CNN修剪方法。", "本文介绍了一种无需再训练的CNN稀疏化方法。"]}
{"source": "Curriculum learning and Self paced learning are popular topics in the machine learning that suggest to put the training samples in order by considering their difficulty levels. Studies in these topics show that starting with a small training set and adding new samples according to difficulty levels improves the learning performance. In this paper we experimented that we can also obtain good results by adding the samples randomly without a meaningful order. We compared our method with classical training, Curriculum learning, Self paced learning and their reverse ordered versions. Results of the statistical tests show that the proposed method is better than classical method and similar with the others. These results point a new training regime that removes the process of difficulty level determination in Curriculum and Self paced learning and as successful as these methods.", "target": ["我们提出，用成长集逐级训练为神经网络提供了一种优化。", "作者将课程学习与随机顺序的学习进行了比较，在之前随机构建的集合中加入了新的例子样本的阶段。", "本文研究了课程和自定进度学习中排序的影响，并表明在某种程度上，培训实例的排序并不重要。"]}
{"source": "We study the problem of learning to map, in an unsupervised way, between domains $A$ and $B$, such that the samples $\\vb \\in B$ contain all the information that exists in samples $\\va\\in A$ and some additional information. For example, ignoring occlusions, $B$ can be people with glasses, $A$ people without, and the glasses, would be the added information. When mapping a sample $\\va$ from the first domain to the other domain, the missing information is replicated from an independent reference sample $\\vb\\in B$. Thus, in the above example, we can create, for every person without glasses a version with the glasses observed in any face image.    Our solution employs a single two-pathway encoder and a single decoder for both domains. The common part of the two domains and the separate part are encoded as two vectors, and the separate part is fixed at zero for domain $A$. The loss terms are minimal and involve reconstruction losses for the two domains and a domain confusion term. Our analysis shows that under mild assumptions, this architecture, which is much simpler than the literature guided-translation methods, is enough to ensure disentanglement between the two domains. We present convincing results in a few visual domains, such as no-glasses to glasses, adding facial hair based on a reference image, etc.", "target": ["一种图像对图像的翻译方法，在一个图像中加入另一个图像的内容，从而形成一个新的图像。", "本文处理的是内容转移的任务，其新颖性在损失上。"]}
{"source": "Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test spits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.", "target": ["一个用于测试数学推理（和代数泛化）的数据集，以及关于当前序列到序列模型的结果。", "提出了一个新的合成数据集来评估序列对序列模型的数学推理能力，并利用它来评估多个模型。", "解决基本数学问题的模式。"]}
{"source": "Convolutional Neural Networks (CNNs) filter the input data using a series of spatial convolution operators with compactly supported stencils and point-wise nonlinearities.  Commonly, the convolution operators couple features from all channels.  For wide networks, this leads to immense computational cost in the training of and prediction with CNNs.  In this paper, we present novel ways to parameterize the convolution more efficiently, aiming to decrease the number of parameters in CNNs and their computational complexity.  We propose new architectures that use a sparser coupling between the channels and thereby reduce both the number of trainable weights and the computational cost of the CNN.  Our architectures arise as new types of residual neural network (ResNet) that can be seen as discretizations of a Partial Differential Equations (PDEs) and thus have predictable theoretical properties. Our first architecture involves a convolution operator with a special sparsity structure, and is applicable to a large class of CNNs. Next, we present an architecture that can be seen as a discretization of a diffusion reaction PDE, and use it with three different convolution operators. We outline in our experiments that the proposed architectures,  although considerably reducing the number of trainable weights, yield comparable accuracy to existing CNNs that are fully coupled in the channel dimension.", "target": ["本文介绍了以偏微分方程为动机的卷积神经网络的高效和经济参数化。", "介绍了标准卷积运算的四个 \"低成本 \"替代方案，可以用来代替标准卷积运算，以降低其计算复杂性。", "本文介绍了降低CNN实现的计算成本的方法，并介绍了限制参数耦合的类似CNN架构的新参数化。", "本文提出了一个基于PDE的视角来理解CNN并将其参数化"]}
{"source": "In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model? One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model. The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood. Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions. We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling.", "target": ["使用速率失真理论来约束潜在变量模型的改进程度", "通过提出基于负对数似然的下限的标准，解决了潜变量模型中先验的优化问题和似然函数的选择。", "提出了一个定理，给出了潜在变量建模的速率扭曲的负对数似然的下限", "作者认为，有损压缩的速率-失真理论为研究潜变量模型提供了一个自然的工具包，提出了一个下限。"]}
{"source": "Backprop is the primary learning algorithm used in many machine learning algorithms. In practice, however, Backprop in deep neural networks is a highly sensitive learning algorithm and successful learning depends on numerous conditions and constraints. One set of constraints is to avoid weights that lead to saturated units. The motivation for avoiding unit saturation is that gradients vanish and as a result learning comes to a halt. Careful weight initialization and re-scaling schemes such as batch normalization ensure that input activity to the neuron is within the linear regime where gradients are not vanished and can flow. Here we investigate backpropagating error terms only linearly. That is, we ignore the saturation that arise by ensuring gradients always flow. We refer to this learning rule as Linear Backprop since in the backward pass the network appears to be linear. In addition to ensuring persistent gradient flow, Linear Backprop is also favorable when computation is expensive since gradients are never computed. Our early results suggest that learning with Linear Backprop is competitive with Backprop and saves expensive gradient computations.", "target": ["我们忽略了非线性，在后向传递中不计算梯度，以节省计算量，并确保梯度始终流动。", "作者提出了线性反向传播算法，以确保反向传播过程中所有部分的梯度流动。"]}
{"source": "Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning  abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models,  however, despite several recent improvements, the training of discrete latent variable models has remained  challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete autoencoder with EM and combining it with sequence  level knowledge distillation alows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.", "target": ["利用EM系统地理解VQ-VAE离散自动编码器，并利用它来设计匹配强自回归基线的非自回归翻译模型。", "本文介绍了一种解释VQ-VAE的新方法，并提出了一种基于软EM聚类的新训练算法。", "本文对使用软电磁算法的VQ-VAE的训练程序提出了另一种观点"]}
{"source": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN  model also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "target": ["本文提出了一个在最佳边际分布方面嵌入损失函数的深度神经网络，在理论上和经验上缓解了过拟合问题。", "提出了边际损失的PAC-Bayesian界限"]}
{"source": "Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.   Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.   While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.   In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \\emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.  We propose a suite of triage methods and compare them on problem spaces of varying complexity.   We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.   Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.", "target": ["我们试图通过一种我们称之为深度网络分流的实验机制来了解压缩网络中的学习表征", "比较各种初始化和训练方法，将知识从VGG网络转移到一个较小的学生网络，用单层取代层块。", "本文介绍了五种为深度网络做分流或块层压缩的方法。", "本文提出了一种压缩NN中的层块的方法，评估了几种不同的子方法"]}
{"source": "In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.    The existence of super-convergence is relevant to understanding why deep networks generalize well.   One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.   Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.   We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.   The architectures to replicate this work will be made available upon publication.", "target": ["对一个新现象的经验证明需要新的理论见解，并与文献中关于SGD和理解泛化的积极讨论相关。", "本文讨论了一个现象，即在非常特定的环境中，神经网络训练可以从包括大的学习率的时间表中获益良多", "作者分析了使用大循环学习率训练残差网络的情况，并证明了循环学习率的快速收敛，以及大学习率作为正则化的证据。"]}
{"source": "Infinite-width neural networks have been extensively used to study the theoretical properties underlying the extraordinary empirical success of standard, finite-width neural networks. Nevertheless, until now, infinite-width networks have been limited to at most two hidden layers. To address this shortcoming, we study the initialisation requirements of these networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights. Based on these observations, we propose a principled approach to weight initialisation that correctly accounts for the functional nature of the hidden layer activations and facilitates the construction of arbitrarily many infinite-width layers, thus enabling the construction of arbitrarily deep infinite-width networks. The main idea of our approach is to iteratively reparametrise the hidden-layer activations into appropriately defined reproducing kernel Hilbert spaces and use the canonical way of constructing probability distributions over these spaces for specifying the required weight distributions in a principled way. Furthermore, we examine the practical implications of this construction for standard, finite-width networks. In particular, we derive a novel weight initialisation scheme for standard, finite-width networks that takes into account the structure of the data and information about the task at hand. We demonstrate the effectiveness of this weight initialisation approach on the MNIST, CIFAR-10 and Year Prediction MSD datasets.", "target": ["我们提出了一种构建任意深度的无限宽网络的方法，在此基础上，我们得出了一种新的有限宽网络的权重初始化方案，并证明了其具有竞争力的性能。", "提出了一种权重初始化方法，以实现无限深和无限宽的网络，并在小数据集上取得了实验结果。", "提出了无限宽度的深度神经网络。"]}
{"source": "Working memory requires information about external stimuli to be represented in the brain even after those stimuli go away. This information is encoded in the activities of neurons, and neural activities change over timescales of tens of milliseconds. Information in working memory, however, is retained for tens of seconds, suggesting the question of how time-varying neural activities maintain stable representations. Prior work shows that, if the neural dynamics are in the `  null space' of the representation - so that changes to neural activity do not affect the downstream read-out of stimulus information - then information can be retained for periods much longer than the time-scale of individual-neuronal activities. The prior work, however, requires precisely constructed synaptic connectivity matrices, without explaining how this would arise in a biological neural network. To identify mechanisms through which biological networks can self-organize to learn  memory function, we derived biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing. Networks implementing this plasticity rule can successfully learn to form memory representations even if only 10% of the synapses are plastic, they are robust to synaptic noise, and they can represent information about multiple stimuli.", "target": ["我们推导出了生物学上合理的突触可塑性学习规则，用于存储刺激表征的循环神经网络。", "一个由循环连接的神经元和一个或多个再输出组成的神经网络模型，其目的是在一段时间内保留一些输出。", "本文提出了一个神经模型中的自组织记忆机制，并引入了一个目标函数，使要记忆的信号的变化最小。"]}
{"source": "Generative Adversarial Networks (GANs) have been proposed as an approach to learning generative models. While GANs have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, neither in theory nor in practice. In particular, the work in this domain has been focused so far only on understanding the properties of the stationary solutions that this dynamics might converge to, and of the behavior of that dynamics in this solutions’ immediate neighborhood.   To address this issue, in this work we take a first step towards a principled study of the GAN dynamics itself. To this end, we propose a model that, on one hand, exhibits several of the common problematic convergence behaviors (e.g., vanishing gradient, mode collapse, diverging or oscillatory behavior), but on the other hand, is sufficiently simple to enable rigorous convergence analysis.   This methodology enables us to exhibit an interesting phenomena: a GAN with an optimal discriminator provably converges, while guiding the GAN training using only a first order approximation of the discriminator leads to unstable GAN dynamics and mode collapse. This suggests that such usage of the first order approximation of the discriminator, which is a de-facto standard in all the existing GAN dynamics, might be one of the factors that makes GAN training so challenging in practice. Additionally, our convergence result constitutes the first rigorous analysis of a dynamics of a concrete parametric GAN.", "target": ["为了理解GAN训练，我们定义了简单的GAN动力学，并展示了该模型中最优和一阶更新之间的定量差异。", "作者研究了GAN在以下情况下的影响：在每个迭代中，判别器训练到收敛，生成器用梯度步骤更新，或者对判别器和生成器做几个梯度步骤。", "本文研究了高斯混合模型上GAN的对抗性训练的动态情况"]}
{"source": "The machine learning and computer vision community is witnessing an unprecedented rate of new tasks being proposed and addressed, thanks to the power of deep convolutional networks to find complex mappings from X to Y. The advent of each task often accompanies the release of a large-scale human-labeled dataset, for supervised training of the deep network. However, it is expensive and time-consuming to manually label sufficient amount of training data. Therefore, it is important to develop algorithms that can leverage off-the-shelf labeled dataset to learn useful knowledge for the target task. While previous works mostly focus on transfer learning from a single source, we study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting. We propose GradMix, a model-agnostic method applicable to any model trained with gradient-based learning rule. GradMix transfers knowledge via gradient descent, by weighting and mixing the gradients from all sources during training. Our method follows a meta-learning objective, by assigning layer-wise weights to the source gradients, such that the combined gradient follows the direction that can minimize the loss for a small set of samples from the target dataset. In addition, we propose to adaptively adjust the learning rate for each mini-batch based on its importance to the target task, and a pseudo-labeling method to leverage the unlabeled samples in the target domain. We perform experiments on two MS-DTT tasks: digit recognition and action recognition, and demonstrate the advantageous performance of the proposed method against multiple baselines.", "target": ["我们提出了一种基于梯度的方法，在不同的领域和任务中从多个来源转移知识。", "本文提出结合源域的梯度来帮助目标域的学习。"]}
{"source": "Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates. In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis. We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions. We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models. We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference. Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling. Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods.", "target": ["系统发育推理的第一个变分贝叶斯公式，这是一个具有挑战性的推理问题，涉及离散和连续成分交织的结构。", "探讨了利用最近提出的子分裂贝叶斯网络和现代梯度估计器对系统发育树的贝叶斯推断问题的近似推断方案。", "提出了一种系统发育树的贝叶斯后验推断的变分方法。"]}
{"source": "This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive  models for raw audio waveform generation. As an example, we propose  a hybrid model that combines an autoregressive network named WaveNet and a  conventional LSTM model to address speech synthesis. Instead of generating  one sample per time-step, the proposed HybridNet generates multiple samples per  time-step by exploiting the long-term memory utilization property of LSTMs. In  the evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.  HybridNet achieves a 3.83 subjective 5-scale mean opinion score on  US English, largely outperforming the same size WaveNet in terms of naturalness  and provide 2x speed up at inference.", "target": ["它是一个混合神经架构，以加速自回归模型。", "结论是，为了在不增加推理时间的情况下扩大模型规模，进行顺序预测，要使用一次预测多个时间步长的模型。", "本文介绍了HybridNet，一个神经语音和其他音频合成系统，它将WaveNet模型与LSTM结合起来，目的是提供一个具有更快的推理时间的音频生成模型。"]}
{"source": "Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features. In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations. Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation. Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest.", "target": ["通过识别作为感兴趣的任务的指标的模型学习的特征进行解释。通过强调这些特征在测试数据中的反应来解释模型决策。用一个受控的数据集客观地评估解释。", "本文提出了一种为深度神经网络输出制作视觉解释的方法，并发布了一个新的合成数据集。", "一种用于深层神经网络的方法，可以自动识别类集的相关特征，支持解释和说明，而不依赖额外的注释。"]}
{"source": "In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.", "target": ["一个高效学习高质量句子表征的框架。", "提出了一种更快的算法，用于从有序句子的语料库中学习SkipThought式的句子表征，将词级解码器换成对比性分类损失。", "本文提出了一个无监督学习句子表征的框架，通过最大化真实上下文句子相对于随机候选句子的概率模型来实现"]}
{"source": "Many regularization methods have been proposed to prevent overfitting in neural networks. Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian. However, this method cannot be generalized to regular neural network architectures. We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework. Unlike in previous literature, it can be applied to any general neural network. We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification. We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.", "target": ["我们从信息瓶颈的角度推导出对神经网络输出的范数惩罚", "提出了激活范数惩罚，这是一种对激活的L_2类型的正则化，源于信息瓶颈原则", "本文利用变分丢弃框架在激活范数惩罚和信息瓶颈框架之间建立了一个映射。"]}
{"source": "Unsupervised learning of timeseries data is a challenging problem in machine learning. Here,  we propose a novel algorithm, Deep Temporal Clustering (DTC), a fully unsupervised method, to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework. The algorithm starts with an initial cluster estimates using an autoencoder for dimensionality reduction and a novel temporal clustering layer for cluster assignment. Then it jointly optimizes the clustering objective and the dimensionality reduction objective. Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric. Several similarity metrics are considered and compared.   To gain insight into features that the network has learned for its clustering, we apply a visualization method that generates a heat map of regions of interest in the timeseries. The viability of the algorithm is demonstrated using timeseries data from diverse domains, ranging from earthquakes to sensor data from spacecraft. In each case, we show that our algorithm outperforms traditional methods. This performance is attributed to fully integrated temporal dimensionality reduction and clustering criterion.", "target": ["一个完全无监督的方法，将降维和时间聚类自然地整合到一个单一的端到端学习框架中。", "提出了一种算法，利用适合时间序列数据的网络结构，将自动编码器与时间序列数据聚类结合起来。", "一种在深度学习背景下联合进行降维和时间聚类的算法，利用了自动编码器和聚类目标。", "作者提出了一种用深度神经网络构建的无监督时间序列聚类方法，并配备了编码器-解码器和聚类模式，以缩短时间序列，提取局部时间特征，并获得编码表征。"]}
{"source": "We study many-class few-shot (MCFS) problem in both supervised learning and meta-learning scenarios. Compared to the well-studied many-class many-shot and few-class few-shot problems, MCFS problem commonly occurs in practical applications but is rarely studied. MCFS brings new challenges because it needs to distinguish between many classes, but only a few samples per class are available for training. In this paper, we propose ``memory-augmented hierarchical-classification network (MahiNet)'' for MCFS learning. It addresses the ``many-class'' problem by exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain. MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a memory-augmented attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes. While the MLP extends the linear classifier, the attention module extends a KNN classifier, both together targeting the ''`few-shot'' problem. We design different training strategies of MahiNet for supervised learning and meta-learning. Moreover, we propose two novel benchmark datasets ''mcfsImageNet'' (as a subset of ImageNet) and ''mcfsOmniglot'' (re-splitted Omniglot) specifically for MCFS problem. In experiments, we show that MahiNet outperforms several state-of-the-art models on MCFS classification tasks in both supervised learning and meta-learning scenarios.", "target": ["一个记忆增强的神经网络，通过利用监督学习和元学习中的类层次，解决了多类少样本的问题。", "本文介绍了通过沿阶级层次从粗到细的预测和学习基于记忆的KNN分类器，在学习过程中跟踪错误标记的实例，向分类器添加归纳偏见的方法。", "本文从监督学习的角度和元学习的角度阐述了多类少样本分类问题。"]}
{"source": "Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years. In this work, we focus on learning a representation that would be useful in a clustering task. We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.", "target": ["一种新颖的损失成分，迫使网络在分类任务的训练中学习一种非常适合聚类的表征。", "本文提出了两个基于两个softmax归一化输入参数之间的KL发散的复合铰链损失的正则化项，以鼓励学习不相干的代表。", "提出了两个正则器，旨在使分类器倒数第二层学到的表征更符合数据的固有结构。"]}
{"source": "In high dimensions, the performance of nearest neighbor algorithms depends crucially on structure in the data.  While traditional nearest neighbor datasets consisted mostly of hand-crafted feature vectors, an increasing number of datasets comes from representations learned with neural networks.  We study the interaction between nearest neighbor algorithms and neural networks in more detail.  We find that the network architecture can significantly influence the efficacy of nearest neighbor algorithms even when the classification accuracy is unchanged.  Based on our experiments, we propose a number of training modifications that lead to significantly better datasets for nearest neighbor algorithms.  Our modifications lead to learned representations that can accelerate nearest neighbor queries by 5x.", "target": ["我们展示了如何从Simiarity Search的角度获得良好的表示。", "研究改变DNN上面的图像分类部分对用LSH或kd-tree算法来索引描述符的能力的影响。", "建议使用 softmax 交叉熵损失来学习一个网络，该网络试图在监督框架中使用减少输入和相应类向量之间的角度。"]}
{"source": "Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices. In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure. Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid. These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization. We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent. We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.", "target": ["我们引入了一种技术，可以对量化的神经网络进行基于梯度的训练。", "提出了一种统一的、通用的训练神经网络的方法，即降低精度的量化突触权重和激活。", "一种量化激活的新方法，在几个真实的图像问题上是最先进的或有竞争力的。", "一种通过随机量化数值并以连续松弛取代所产生的分类分布来学习具有量化权重和激活的神经网络的方法"]}
{"source": "In most current formulations of adversarial training, the discriminators can be expressed as single-input operators, that is, the mapping they define is separable over observations. In this work, we argue that this property might help explain the infamous mode collapse phenomenon in adversarially-trained generative models. Inspired by discrepancy measures and two-sample tests between probability distributions, we propose distributional adversaries that operate on samples, i.e., on sets of multiple points drawn from a distribution, rather than on single observations. We show how they can be easily implemented on top of existing models. Various experimental results show that generators trained in combination with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with observation-wise prediction discriminators. In addition, the application of our framework to domain adaptation results in strong improvement over recent state-of-the-art.", "target": ["我们表明，GAN中的模式崩溃问题可能是由训练批次中的观察值之间缺乏信息共享来解释的，并提出了一个基于分布的梯度间信息全局共享框架，导致更稳定和有效的对抗性训练。", "建议在对抗性训练中用明确操作实例分布的判别器取代单样本判别器。", "关于双样本检验和MMD的理论，以及如何将其有益地纳入GAN框架。"]}
{"source": "Chemical information extraction is to convert chemical knowledge in text into true chemical database, which is a text processing task heavily relying on chemical compound name identification and standardization. Once a systematic name for a chemical compound is given, it will naturally and much simply convert the name into the eventually required molecular formula. However, for many chemical substances, they have been shown in many other names besides their systematic names which poses a great challenge for this task. In this paper, we propose a framework to do the auto standardization from the non-systematic names to the corresponding systematic names by using the spelling error correction, byte pair encoding tokenization and neural sequence to sequence model. Our framework is trained end to end and is fully data-driven. Our standardization accuracy on the test dataset achieves 54.04% which has a great improvement compared to previous state-of-the-art result.", "target": ["我们设计了一个端到端的框架，使用序列到序列的模型来进行化学名称的标准化。", "通过建立非系统性名称和系统性名称的平行语料库并建立seq2seq模型，使化学信息提取中的非系统性名称标准化。", "这项工作提出了一种方法，利用各种机制的组合将化合物的非系统名称转化成其系统等价物"]}
{"source": "The training of deep neural networks with Stochastic Gradient Descent (SGD) with a large learning rate or a small batch-size typically ends in flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss. This was found to correlate with a good final generalization performance.   In this paper we extend previous work by investigating the curvature of the loss surface along the whole training trajectory, rather than only at the endpoint. We find that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD. At this peak value SGD starts to fail to minimize the loss along directions in the loss surface corresponding to the largest curvature (sharpest directions). To further investigate the effect of these dynamics in the training process, we study a variant of SGD using a reduced learning rate along the sharpest directions which we show can improve training speed while finding both sharper and better generalizing solution, compared to vanilla SGD. Overall, our results show that the SGD dynamics in the subspace of the sharpest directions influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model.", "target": ["在训练的早期，SGD被引导到一个区域，在这个区域里，它的步长与曲率相比太大，这影响了训练的其余部分。", "分析了收敛/泛化与DNN经验损失的Hessian的最大特征向量的更新之间的关系。", "这项工作研究了SGD步长和损失表面的曲率之间的关系"]}
{"source": "We introduce a new approach to estimate continuous actions using actor-critic algorithms for reinforcement learning problems. Policy gradient methods usually predict one continuous action estimate or parameters of a presumed distribution (most commonly Gaussian) for any given state which might not be optimal as it may not capture the complete description of the target distribution. Our approach instead predicts M actions with the policy network (actor) and then uniformly sample one action during training as well as testing at each state. This allows the agent to learn a simple stochastic policy that has an easy to compute expected return. In all experiments, this facilitates better exploration of the state space during training and converges to a better policy.", "target": ["我们引入了一种新的强化学习算法，该算法可以预测多个行动并从中取样。", "这项工作引入了确定性策略的统一混合物，并发现这种随机政策的参数化在几个OpenAI体育馆的基准上优于DDPG。", "作者研究了一种改善用DDPG训练的网络性能的方法，并显示了在大量标准连续控制环境下的性能改善。"]}
{"source": "Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks. DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse. However, like other CNN models, DenseNets also face overfitting problem if not severer. Existing dropout method can be applied but not as effective due to the introduced nonlinear connections. In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps. To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability. The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections. Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.", "target": ["意识到在DenseNet上应用原始丢弃时的缺点，我们从三个方面精心设计了丢弃方法，其理念也可以应用于其他CNN模型。", "应用不同的二元丢弃结构和时间表，具体目的是使DenseNet结构正规化。", "提出了一种用于densenet的预丢弃技术，该技术在非线性激活函数之前实现了丢弃。"]}
{"source": "While extremely successful in several applications, especially with low-level representations; sparse, noisy samples and structured domains (with multiple objects and interactions) are some of the open challenges in most deep models. Column Networks, a deep architecture, can succinctly capture such domain structure and interactions, but may still be prone to sub-optimal learning from sparse and noisy samples. Inspired by the success of human-advice guided learning in AI, especially in data-scarce domains, we propose Knowledge-augmented Column Networks that leverage human advice/knowledge for better learning with noisy/sparse samples. Our experiments demonstrate how our approach leads to either superior overall performance or faster convergence.", "target": ["引导关系感知的深度模型更好地学习人类知识。", "这项工作提出了一种基于注入人类指导的柱状网络的变体，通过修改网络中的计算结果。", "通过扩展列网络（一种用于集体分类的图形神经网络）将人类建议纳入深度学习的方法。"]}
{"source": "Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors. In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated good classification performance with BNNs, our work explains why these BNNs work in terms of HD geometry.   Furthermore, the results and analysis used on BNNs are shown to generalize to neural networks with ternary weights and activations. Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "target": ["最近二元神经网络的成功可以基于高维二元向量的几何学来理解", "从数字和理论上研究了二值化神经网络的经验成功背后的原因。", "本文分析了二元神经网络的有效性，以及为什么二元化能够保持模型性能。"]}
{"source": "In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR). In this paper, we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations. We show how a single neuron is able to provide the optimum solution for inverse problem, given a low resolution image dictionary as an operator. Introducing a new concept called Representation Dictionary Duality, we show that CNN elements (filters) are trained to be representation vectors and then, during reconstruction, used as dictionaries. In the light of theoretical work, we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and show that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.", "target": ["在证明了一个神经元作为超分辨率的逆向问题解决者和一个神经元网络保证提供解决方案后，我们提出了一个双网络架构，其性能比最先进的要快。", "讨论了使用神经网络进行超分辨率的问题", "一个用于解决图像超分辨率任务的新架构，以及旨在建立用于解决超分辨率的CNN与解决稀疏正则化反问题之间联系的分析。"]}
{"source": "We consider the learning of algorithmic tasks by mere observation of input-output  pairs. Rather than studying this as a black-box discrete regression problem with  no assumption whatsoever on the input-output mapping, we concentrate on tasks  that are amenable to the principle of divide and conquer, and study what are its  implications in terms of learning.  This principle creates a powerful inductive bias that we leverage with neural  architectures that are defined recursively and dynamically, by learning two scale-  invariant atomic operations: how to split a given input into smaller sets, and how  to merge two partially solved tasks into a larger partial solution. Our model can be  trained in weakly supervised environments, namely by just observing input-output  pairs, and in even weaker environments, using a non-differentiable reward signal.  Moreover, thanks to the dynamic aspect of our architecture, we can incorporate  the computational complexity as a regularization term that can be optimized by  backpropagation. We demonstrate the flexibility and efficiency of the Divide-  and-Conquer Network on several combinatorial and geometric tasks: convex hull,  clustering, knapsack and euclidean TSP. Thanks to the dynamic programming  nature of our model, we show significant improvements in terms of generalization  error and computational complexity.", "target": ["通过弱监督来学习分而治之策略的动态模型。", "提出通过使用分而治之的策略为神经网络架构增加新的归纳偏压。", "本文研究了可以用动态编程方法解决的问题，并提出了一种神经网络架构来解决这种问题，这种架构击败了序列的基线。", "本文提出了一个独特的网络架构，可以学习分而治之的策略来解决算法任务。"]}
{"source": "Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\\boldsymbol{\\gamma}$ for expectation-based objectives $\\mathbb{E}_{q_{\\boldsymbol{\\gamma}} (\\boldsymbol{y})} [f (\\boldsymbol{y}) ]$. Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick. To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick. We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired). Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables.", "target": ["对于不可重新参数化的连续/离散分布的类似Rep的梯度；进一步推广到深度概率模型，产生统计反传播", "提出了一个基于期望目标的梯度估计器，它是无偏的，具有低方差，并适用于连续和离散的随机变量。", "一种改进的计算期望值导数的方法，以及一种新的低方差梯度估计器，允许在观测或潜变量是离散的情况下训练生成模型。", "为与连续或离散随机变量相关的分布设计一个低方差梯度。"]}
{"source": "Quantum computers promise significant advantages over classical computers for a number of different applications. We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer. We demonstrate this explicitly for a binary neural network and, further, show how a quantum computer can train the network by manipulating this state using a well-known algorithm known as quantum amplitude amplification. We further show that with minor adaptation, this method can also represent the meta-loss landscape of a number of neural network architectures simultaneously. We search this meta-loss landscape with the same method to simultaneously train and design a binary neural network.", "target": ["我们表明，NN参数和超参数成本景观可以用一个单一的量子电路作为量子态生成，这些可以用于训练和元训练。", "描述了一种方法，即深度学习框架可以通过考虑布洛赫球体/量子比特的两种状态形式来进行量化，并创建一个量子二元神经网络。", "本文提出了量子振幅放大法，这是一种用于二元神经网络中训练和模型选择的新算法。", "提出了一个新颖的想法，即通过构建量子二元神经网络（QBNN），输出代表一个给定二元神经网络所有参数的完整成本景观的量子状态。"]}
{"source": "Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations. These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications. We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for tasks where some adversarial transformation are more important than others. We encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness. Our experiments on simple MNIST and CIFAR10 models with a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy.", "target": ["用于训练经过认证的成本敏感的稳健分类器的一般方法，以对抗对抗性扰动", "计算并将对抗性攻击的成本插入优化的目标中，得到一个对对抗性攻击具有成本敏感性的模型。", "在Dalvi等人的半年度工作的基础上，扩展了可认证稳健性的方法，用一个成本矩阵来指定每一对源-目标类别的模型是否应该对对抗性例子具有稳健性。"]}
{"source": "Retinal prostheses for treating incurable blindness are designed to electrically stimulate surviving retinal neurons,  causing them to send artificial visual signals to the brain. However, electrical stimulation generally cannot precisely reproduce  normal patterns of neural activity in the retina. Therefore, an electrical stimulus must be selected that produces a neural response as close as possible to the desired response. This requires a technique for computing a distance between the desired response and the achievable response that is meaningful in terms of the visual signal being conveyed. Here we propose a method to learn such a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells (RGCs) in the primate retina. The learned metric produces a measure of similarity of RGC population responses that accurately reflects the similarity of the visual input. Using data from electrical stimulation experiments, we demonstrate that this metric may improve the performance of a prosthesis.", "target": ["使用三联体来学习比较神经反应的指标并提高假体的性能。", "作者开发了新的尖峰训练距离度量，包括神经网络和二次元度量。这些指标被证明优于天真的汉明距离指标，并隐含地捕捉了神经代码中的一些结构。", "考虑到改进神经假体的应用，作者提议通过优化二次方形式或深度神经网络来学习神经反应之间的度量。"]}
{"source": "We introduce a novel workflow, QCue, for providing textual stimulation during mind-mapping. Mind-mapping is a powerful tool whose intent is to allow one to externalize ideas and their relationships surrounding a central problem. The key challenge in mind-mapping is the difficulty in balancing the exploration of different aspects of the problem (breadth) with a detailed exploration of each of those aspects (depth). Our idea behind QCue is based on two mechanisms: (1) computer-generated automatic cues to stimulate the user to explore the breadth of topics based on the temporal and topological evolution of a mind-map and (2) user-elicited queries for helping the user explore the depth for a given topic. We present a two-phase study wherein the first phase provided insights that led to the development of our work-flow for stimulating the user through cues and queries. In the second phase, we present a between-subjects evaluation comparing QCue with a digital mind-mapping work-flow without computer intervention. Finally, we present an expert rater evaluation of the mind-maps created by users in conjunction with user feedback.", "target": ["本文介绍了一种生成问题（线索）和查询（建议）以帮助用户进行思维导图的方法。", "提出了一个工具，通过与现有节点相关的建议背景，以及通过对不太发达的分支进行扩展的问题，来帮助思维导图。", "本文提出了一种帮助人们完成思维导图任务的方法，设计了一种界面和算法特征来支持思维导图，并提供了一项评估性研究。"]}
{"source": "The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks. In this paper, we show that a simple ensembling of first and second order deep feature statistics can be exploited to effectively differentiate in-distribution and out-of-distribution samples. Specifically, we observe that  the mean and standard deviation within feature maps  differs greatly between in-distribution and out-of-distribution samples. Based on this observation, we propose a simple and  efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model.   The proposed method outperforms the state-of-the-art by a large margin in all standard benchmarking tasks, while being much simpler to implement and execute. Notably, our method improves the true negative rate from 39.6% to 95.3% when 95% of in-distribution (CIFAR-100) are correctly detected using a DenseNet and the out-of-distribution dataset is TinyImageNet resize. The source code of our method will be made publicly available.", "target": ["在不需要改变底层DNN的情况下，通过使用低阶特征统计来检测分布外的样本。", "提出了一种算法，通过使用BatchNorm层内的均值和方差的运行估计来检测分布外的样本，以构建后来被送入线性分类器的特征表示。", "一种检测分布外样本的方法，作者提出在CNN的每个批次规范化层的简单统计上使用逻辑回归。", "该论文建议使用Z-cores来比较ID和OOD样本，以评估深层网络所要做的事情。"]}
{"source": "Due to the sharp increase in the severity of the threat imposed by software vulnerabilities, the detection of vulnerabilities in binary code has become an important concern in the software industry, such as the embedded systems industry, and in the field of computer security. However, most of the work in binary code vulnerability detection has relied on handcrafted features which are manually chosen by a select few, knowledgeable domain experts. In this paper, we attempt to alleviate this severe binary vulnerability detection bottleneck by leveraging recent advances in deep learning representations and propose the Maximal Divergence Sequential Auto-Encoder. In particular, latent codes representing vulnerable and non-vulnerable binaries are encouraged to be maximally divergent, while still being able to maintain crucial information from the original binaries. We conducted extensive experiments to compare and contrast our proposed methods with the baselines, and the results show that our proposed methods outperform the baselines in all performance measures of interest.", "target": ["我们提出了一种名为 \"最大发散顺序自动编码器 \"的新方法，该方法利用变分自动编码器的表示方法进行二进制代码漏洞检测。", "本文提出了一个基于变分自动编码器的架构，用于二进制软件漏洞检测的代码嵌入，与基线相比，学到的嵌入在区分易受攻击和非不易受攻击二进制代码方面更加有效。", "本文提出了一个模型，利用深度学习技术自动提取漏洞检测的特征。"]}
{"source": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.  We present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.   First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "target": ["基于后验分布的注意力计算会导致更有意义的注意力和更好的表现", "本文提出了一个序列到序列的模型，其中注意力被视为一个潜在的变量，并为这个模型推导出新颖的推理程序，在机器翻译和语态转折生成任务中获得改进。", "本文提出了一种新型的seq2seq问题的后发注意力模型"]}
{"source": "The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms. Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation. This is achieved by either eliminating components from the model, or penalizing complexity during training. While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions. In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function. In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it. We will show that using this technique, we can achieve competitive results.", "target": ["通过最小化其复杂性，同时约束其损失来压缩训练的DNN模型。", "本文提出了一种在精度约束下的深度神经网络压缩方法。", "本文提出了一种用于网络压缩的损失值约束的k-means编码方法，并开发了一种模型优化的迭代算法。"]}
{"source": "Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.", "target": ["我们开发了一种技术来可视化任意神经网络中的注意力机制。", "提出学习一个潜在注意力网络，可以帮助可视化深度神经网络的内部结构。", "本文作者提出了一个数据驱动的黑箱可视化方案。"]}
{"source": "The design of small molecules with bespoke properties is of central importance to drug discovery.   However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.   This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.   The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.   Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "target": ["我们研究了各种用于分子生成的RL算法，并定义了新的基准（将作为OpenAI Gym发布），发现PPO和一个爬坡MLE算法效果最好。", "通过提出19个基准，将小型数据集扩展到大型标准化数据集，并探讨如何将RL技术应用于分子设计，考虑了分子生成的模型评估。", "本文表明，在建模和合成分子时，最复杂的RL方法不如简单的爬坡技术有效，PPO是例外。"]}
{"source": "Analogical reasoning has been a principal focus of various waves of AI research. Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience. Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data. We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model. The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features. Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures.", "target": ["当网络通过对比其输入域中的抽象关系结构来学习类比时，会诱发最强大的类比推理能力。", "本文研究了神经网络学习类比的能力，表明一个简单的神经网络能够解决某些类比问题", "本文介绍了一种训练神经网络进行类比推理任务的方法，特别是考虑到视觉类比和符号类比。"]}
{"source": "Building chatbots that can accomplish goals such as booking a flight ticket is an unsolved problem in natural language understanding. Much progress has been made to build conversation models using techniques such as sequence2sequence modeling. One challenge in applying such techniques to building goal-oriented conversation models is that maximum likelihood-based models are not optimized toward accomplishing goals. Recently, many methods have been proposed to address this issue by optimizing a reward that contains task status or outcome. However, adding the reward optimization on the fly usually provides little guidance for language construction and the conversation model soon becomes decoupled from the language model. In this paper, we propose a new setting in goal-oriented dialogue system to tighten the gap between these two aspects by enforcing model level information isolation on individual models between two agents. Language construction now becomes an important part in reward optimization since it is the only way information can be exchanged. We experimented our models using self-play and results showed that our method not only beat the baseline sequence2sequence model in rewards but can also generate human-readable meaningful conversations of comparable quality.", "target": ["一个以目标为导向的自娱自乐的神经对话模型", "面向目标的对话生成的自娱自乐模型，旨在加强任务奖励和语言模型之间的耦合。", "本文描述了一种利用自娱自乐来改进目标导向的对话系统的方法。"]}
{"source": "Search engine users nowadays heavily depend on query completion and correction to shape their queries.  Typically, the completion is done by database lookup which does not understand the context and cannot generalize to prefixes not in the database . In the paper, we propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix .  We show how to address two main challenges that renders this method practical for large-scale deployment : 1) we propose a method for integrating error correction into the language model completion via a edit-distance potential and a variant of beam search that can exploit these potential functions; and 2) we show how to efficiently perform CPU-based computation to complete the queries, with error correction, in real time (generating top 10 completions within 16 ms). Experiments show that the method substantially increases hit rate over standard approaches, and is capable of handling tail queries.", "target": ["使用字符级LSTM语言模型的实时搜索查询完成", "本文介绍了包括前缀校正的查询完成方法，以及一些工程细节，以满足CPU上的特殊延迟要求。", "作者提出了一种解决带有纠错功能的查询完成问题的算法，并采用基于字符级的RNN建模，对推理部分进行了优化，以实时实现目标。"]}
{"source": "RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical convergence properties have remained unclear. Further, recent work has seemed to suggest that these algorithms have worse generalization properties when compared to carefully tuned stochastic gradient descent or its momentum variants. In this work, we make progress towards a deeper understanding of ADAM and RMSProp in two ways. First, we provide proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time.   Next we design experiments to empirically study the convergence and generalization properties of RMSProp and ADAM against Nesterov's Accelerated Gradient method on a variety of common autoencoder setups and on VGG-9 with CIFAR-10. Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter \\beta_1. We show that at very high values of the momentum parameter (\\beta_1 = 0.99) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses. On the other hand, NAG can sometimes do better when ADAM's \\beta_1 is set to the most commonly used value: \\beta_1 = 0.9, indicating the importance of tuning the hyperparameters of ADAM to get better generalization performance.   We also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms, and it also produces iterates which exhibit an increasing trend for the minimum eigenvalue of the Hessian of the loss function at the iterates.", "target": ["在本文中，我们证明了（随机的和确定的）RMSProp和确定的ADAM对于平滑非凸目标的收敛临界性，并且我们证明了ADAM在自动编码器上的一个有趣的beta_1敏感性。", "本文介绍了RMSProp和ADAM在光滑非凸函数情况下的收敛性分析"]}
{"source": "Recent advances in adversarial Deep Learning (DL) have opened up a new and largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. We introduce a novel automated countermeasure called Parallel Checkpointing Learners (PCL) to thwart the potential adversarial attacks and significantly improve the reliability (safety) of a victim DL model. The proposed PCL methodology is unsupervised, meaning that no adversarial sample is leveraged to build/train parallel checkpointing learners. We formalize the goal of preventing adversarial attacks as an optimization problem to minimize the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint checkpointing modules are trained and leveraged to validate the victim model execution in parallel. Each checkpointing learner explicitly characterizes the geometry of the input data and the corresponding high-level data abstractions within a particular DL layer. As such, the adversary is required to simultaneously deceive all the defender modules in order to succeed. We extensively evaluate the performance of the PCL methodology against the state-of-the-art attack scenarios, including Fast-Gradient-Sign (FGS), Jacobian Saliency Map Attack (JSMA), Deepfool, and Carlini&WagnerL2 algorithm. Extensive proof-of-concept evaluations for analyzing various data collections including MNIST, CIFAR10, and ImageNet corroborate the effectiveness of our proposed defense mechanism against adversarial samples.", "target": ["设计针对对抗性攻击的无监督防御机制，对于确保防御的通用性至关重要。", "本文介绍了一种在深度学习分类环境下检测对抗性例子的方法", "本文提出了一种无监督的方法来检测神经网络的对抗性例子。"]}
{"source": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "target": ["无代理的神经架构搜索，在大规模目标任务（ImageNet）上直接学习架构，同时将成本降低到与正常训练相同的水平。", "本文讨论了架构搜索的问题，并特别寻求在不需要训练 \"代理 \"任务的情况下做到这一点，在这些任务中，问题通过更有限的优化、架构复杂性或数据集大小得到简化。"]}
{"source": "With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications. However, deep neural networks are also known to have very little control over its uncertainty for test examples, which potentially causes very harmful and annoying consequences in practical scenarios. In this paper, we are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task proposed by~\\cite{hendrycks2016baseline}. Our method first assumes there exists a underlying higher-order distribution $\\mathcal{P}(z)$ , which generated label-wise distribution $\\mathcal{P}(y)$ over classes on the K-dimension simplex, and then approximate such higher-order distribution via parameterized posterior function $p_{\\theta}(z|x)$ under variational inference framework, finally we use the entropy of learned posterior distribution $p_{\\theta}(z|x)$ as uncertainty measure to detect out-of-distribution examples. However , we identify the overwhelming over-concentration issue in such a framework, which greatly hinders the detection performance. Therefore , we further design a log-smoothing function to alleviate such issue to greatly increase the robustness of the proposed entropy-based uncertainty measure. Through comprehensive experiments on various datasets and architectures, our proposed variational Dirichlet framework with entropy-based uncertainty measure is consistently observed to yield significant improvements over many baseline systems.", "target": ["基于变分推理的分布外检测新框架", "描述了一种在DNN分类任务中量化不确定性的概率方法，该方法在分发外检测的任务中优于其他SOTA方法。", "一个基于变分推理和先验狄利克雷分布的新的分布外检测框架，报告了几个数据集上的最新结果。", "通过一种新的方法对分类概率的置信度分布进行近似，利用狄利克雷分布的变异推理，进行分布外检测。"]}
{"source": "Intelligent agents can learn to represent the action spaces of other agents simply by observing them act. Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences. In this work, we address the problem of learning an agent’s action space purely from visual observation. We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content. We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions. We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP). We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings. When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels. Project website: https://daniilidis-group.github.io/learned_action_spaces", "target": ["我们从纯粹的视觉观察中学习一个代理行动空间的表示。我们使用了一种具有新的可组合性损失的循环潜变量方法。", "提出了一个组合式的潜在变量模型，以学习预测在行动标签不丰富的情况下接下来会发生什么的模型。", "一种基于变分IB的方法，直接从正在进行的动作视频中学习动作表征，实现了后续学习方法的更好效率，同时需要较少的动作标签视频。", "本文提出了一种视频预测的方法，它可以自主地找到一个动作空间，对后续帧之间的差异进行编码。"]}
{"source": "When autonomous agents interact in the same environment, they must often cooperate to achieve their goals. One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it. However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement. Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols. More general methods usually require human input or domain-specific data, and so do not scale. To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning. Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven. We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory. Additionally, we investigate how the physical location of agents influences negotiation outcomes.", "target": ["强化学习可用于训练代理在许多谈判协议中进行团队组建谈判", "本文研究了在所有代理必须合作完成任务的情况下的深度多代理RL（例如，搜索和救援，多人视频游戏），并使用简单的合作加权投票游戏来研究深度RL的功效，并将深度RL找到的解决方案与公平的解决方案进行比较。", "一种在合作博弈论环境下谈判联盟的强化学习方法，可用于无限制训练模拟的情况。"]}
{"source": "Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.", "target": ["寻找、分析和控制NMT中重要神经元的无监督方法", "本文介绍了发现神经机器翻译系统中重要神经元的无监督方法，并分析了由这些神经元控制的语言学特性。", "在机器翻译中对神经元进行排序的无监督方法，其中重要的神经元因此被识别并用于控制MT输出。"]}
{"source": "Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task. Hence, both environment and task specific knowledge are entangled into one framework. However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes. Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units. The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task. The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods.", "target": ["我们提出了一个DRL框架，该框架将任务和环境的具体知识分开。", "作者建议将强化学习分解为一个PATH函数和一个GOAL函数", "一个模块化的架构，目的是将特定环境的知识和特定任务的知识分离成不同的模块，与标准的A3C在广泛的任务中保持一致。"]}
{"source": "Modelling 3D scenes from 2D images is a long-standing problem in computer vision with implications in, e.g., simulation and robotics. We propose pix2scene, a deep generative-based approach that implicitly models the geometric properties of a scene from images. Our method learns the depth and orientation of scene points visible in images. Our model can then predict the structure of a scene from various, previously unseen view points. It relies on a bi-directional adversarial learning mechanism to generate scene representations from a latent code, inferring the 3D representation of the underlying scene geometry. We showcase a novel differentiable renderer to train the 3D model in an end-to-end fashion, using only images. We demonstrate the generative ability of our model qualitatively on both a custom dataset and on ShapeNet. Finally, we evaluate the effectiveness of the learned 3D scene representation in supporting a 3D spatial reasoning.", "target": ["pix2scene：一种基于深度生成的方法，用于从图像中隐含地模拟三维场景的几何属性", "探讨用神经识别模型中的surfels解释场景，并展示图像重建、合成和心理形状旋转的结果。", "作者介绍了一种方法，在给定二维图像和摄像机姿势的情况下，使用self-superfised模型创建三维场景模型。"]}
{"source": "Identifying the relations that connect words is an important step towards understanding human languages and is useful for various NLP tasks such as knowledge base completion and analogical reasoning. Simple unsupervised operators such as vector offset between two-word embeddings have shown to recover some specific relationships between those words, if any. Despite this, how to accurately learn generic relation representations from word representations remains unclear. We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations. We propose a method for learning relation representations using a feed-forward neural network that performs relation prediction. Our evaluations on two benchmark datasets reveal that the penultimate layer of the trained neural network-based relational predictor acts as a good representation for the relations between words.", "target": ["识别连接单词的关系对各种NLP任务都很重要。我们将关系表征建模为一个监督学习问题，并学习参数化操作符，将预先训练好的词嵌入映射到关系表征中。", "本文提出了一种新的方法，只用预先训练好的词嵌入和在成对的词上操作的新的损失函数，就可以把词汇关系表示为向量。", "当你已经有了预先训练好的词/实体嵌入，并且只对学习构成关系表征感兴趣时，对关系合成问题的一个新的解决方案。"]}
{"source": "Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.", "target": ["我们建议用不同的丢弃掩码训练两个相同的循环神经网络副本（共享参数），同时最大限度地减少它们（pre-softmax）预测的差异。", "介绍了Fraternal丢弃在收敛性方面对Expectation-linear丢弃的改进，并在一些任务和数据集上展示了Fraternal丢弃的效用。"]}
{"source": "We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields. Our method specifically targets the space-time representation of physical surfaces from liquid simulations. Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions. Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface. Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients. To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes. Our representation makes it possible to rapidly generate the desired implicit surfaces. We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach.", "target": ["学习加权和变形的时空数据集，以实现液体行为的高效近似。", "一个基于神经网络的模型被用来从密集注册的4D隐性表面对新的场景条件进行插值模拟，用于结构化场景。", "本文介绍了一种耦合的深度学习方法，用于生成对实时决策支持应用有用的现实液体模拟数据。", "本文介绍了一种用于物理模拟的深度学习方法，它结合了两个网络来合成代表三维物理模拟的4D数据"]}
{"source": "This is an empirical paper which constructs color invariant networks and evaluates their performances on a realistic data set. The paper studies the simplest possible case of color invariance: invariance under pixel-wise permutation of the color channels. Thus the network is aware not of the specific color object, but its colorfulness. The data set introduced in the paper consists of images showing crashed cars from which ten classes were extracted. An additional annotation was done which labeled whether the car shown was red or non-red.   The networks were evaluated by their performance on the classification task. With the color annotation we altered the color ratios  in the training data and analyzed the generalization capabilities of the networks on the unaltered test data. We further split the test data in red and non-red cars and did a similar evaluation. It is shown in the paper that an pixel-wise ordering of the rgb-values of the images performs better or at least similarly for small deviations from the true color ratios. The limits of these networks are also discussed.", "target": ["我们在一个新的现实数据集上构建并评估颜色不变的神经网络", "提出了一种使用于图像识别的神经网络具有颜色不变性的方法，并在cifar 10数据集上对其进行了评估。", "作者研究了一种修改过的输入层，它导致了颜色不变的网络，并表明某些颜色不变的输入层可以提高来自与训练图像不同颜色分布的测试图像的准确性。", "作者在图像上测试了一个CNN，该CNN的颜色通道被修改为不受排列组合的影响，性能没有下降太多。"]}
{"source": "Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of \"overlaps\" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).  To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well. Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.", "target": ["我们分析了卷积网络的感受野之间的重叠程度如何影响其表达能力。", "本文通过考虑带有乘积池的线性激活，研究了DNN卷积层中的 \"重叠 \"所提供的表达能力。", "本文分析了卷积算术电路的表现力，并表明需要指数级的大量非重叠的ConvAC来近似重叠的ConvAC的网格张量。"]}
{"source": "We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks. Our algorithm receives any parameter value and returns: local minimum, second-order stationary point, or a strict descent direction. The presence of M data points on the nondifferentiability of the ReLU divides the parameter space into at most 2^M regions, which makes analysis difficult. By exploiting polyhedral geometry, we reduce the total computation down to one convex quadratic program (QP) for each hidden node, O(M) (in)equality tests, and one (or a few) nonconvex QP. For the last QP, we show that our specific problem can be solved efficiently, in spite of nonconvexity. In the benign case, we solve one equality constrained QP, and we prove that projected gradient descent solves it exponentially fast. In the bad case, we have to solve a few more inequality constrained QPs, but we prove that the time complexity is exponential only in the number of inequality constraints. Our experiments show that either benign case or bad case with very few inequality constraints occurs, implying that our algorithm is efficient in most cases.", "target": ["检验局部最优性和提取单隐层ReLU网络的经验风险的不可微分点的下降方向的理论算法。", "提出了一种检查给定点是否是广义二阶静止点的算法。", "一种理论上的算法，涉及解决凸和非凸的二次方程序，用于在训练两层ReLU网络时检查局部最优性和摆脱鞍座。", "作者提出了一种方法来检查一个点是否是静止点，然后将静止点分类为局部最小点或二阶静止点。"]}
{"source": "We present a new technique for learning visual-semantic embeddings for cross-modal retrieval.   Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, we introduce a simple change to common loss functions used to learn multi-modal embeddings.   That, combined with fine-tuning and the use of augmented data, yields significant gains in retrieval performance.   We showcase our approach, dubbed VSE++, on the MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods.   On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval, and 11.3% in image retrieval (based on R@1).", "target": ["一种基于相对较硬的底片的新损失，在图像标题检索中取得了最先进的性能。", "使用适用于最难否定的三联体而不是所有三联体的平均数的三联体损失，学习句子和图像的联合嵌入"]}
{"source": "We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle. DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks. It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem. We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively. DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations. In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.", "target": ["我们利用交替最小化原理，提供了一种有效的新技术来训练深度自动编码器。", "训练自动编码器和编码器-解码器网络的交替最小化框架", "作者探索了一种训练自动编码器的交替优化方法，将每一层视为广义线性模型，并建议在每个阶段使用随机归一化GD作为最小化算法。"]}
{"source": "We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature. By taking advantage of transfer learning, we are able to efficiently use different data sources that are related to the same underlying causal mechanisms. We compare our algorithms with those in the extant literature using extensive simulation studies based on large-scale voter persuasion experiments and the MNIST database. Our methods can perform an order of magnitude better than existing benchmarks while using a fraction of the data.", "target": ["使用神经网络估计因果效应的转移学习。", "开发了在不同环境下通过辅助数据集估计条件平均治疗效果的算法，包括有和没有基础学习器。", "作者提出了解决估计CATE函数的转移学习的新任务的方法，并使用一个合成环境和一个真实世界的实验数据集对其进行了评估。", "使用神经网络回归和比较转移学习框架来估计字符串无视性假设下的条件平均处理效应"]}
{"source": "Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or \"motifs\", are thought to be building blocks of neural representations and information processing. We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology. Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction. Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used. An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance. In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations.", "target": ["我们提出了LeMoNADe，一种直接在钙成像视频上操作的端到端学习基序检测方法。", "本文提出了一个VAE式的模型，用于识别钙成像视频中的图案，依赖于伯努利变量，并需要Gumbel-softmax技巧进行推理。"]}
{"source": "A noisy and diverse demonstration set may hinder the performances of an agent aiming to acquire certain skills via imitation learning. However, state-of-the-art imitation learning algorithms often assume the optimality of the given demonstration set.  In this paper, we address such optimal assumption by learning only from the most suitable demonstrations in a given set. Suitability of a demonstration is estimated by whether imitating it produce desirable outcomes for achieving the goals of the tasks. For more efficient demonstration suitability assessments, the learning agent should be capable of imitating a demonstration as quick as possible, which shares similar spirit with fast adaptation in the meta-learning regime. Our framework, thus built on top of Model-Agnostic Meta-Learning, evaluates how desirable the imitated outcomes are, after adaptation to each demonstration in the set. The resulting assessments hence enable us to select suitable demonstration subsets for acquiring better imitated skills. The videos related to our experiments are available at: https://sites.google.com/view/deepdj", "target": ["我们提出了一个框架，通过元训练示范适用性评估器，从嘈杂的示范集中模仿学习一个好策略。", "贡献了一个基于MAML的模仿学习算法，该算法可自动确定所提供的示范是否 \"合适\"。", "一种从包括无用行为的演示集合中进行模仿学习的方法，该方法通过其在元训练时提供的性能收益来选择有用的演示。"]}
{"source": "We introduce causal implicit generative models (CiGMs): models that allow sampling from not only the true observational but also the true interventional distributions. We show that adversarial training can be used to learn a CiGM, if the generator architecture is structured based on a given causal graph. We consider the application of conditional and interventional sampling of face images with binary feature labels, such as mustache, young. We preserve the dependency structure between the labels with a given causal graph. We devise a two-stage procedure for learning a CiGM over the labels and the image. First we train a CiGM over the binary labels using a  Wasserstein GAN where the generator neural network is consistent with the causal graph between the labels. Later, we combine this with a conditional GAN to generate images conditioned on the binary labels. We propose two new conditional GAN architectures: CausalGAN and CausalBEGAN. We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels. The conditional GAN combined with a trained CiGM for the labels is then a CiGM over the labels and the generated image. We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset.", "target": ["我们引入了因果隐含生成模型，它可以从条件分布和干预分布中取样，还提出了两个新的条件GAN，我们用它来训练它们。", "一种结合随意图的方法，描述了标签的依赖结构和两个条件性GAN架构，该架构生成了以二进制标签为条件的图像。", "作者解决了在给定图像标签之间的因果结构时，从观察数据中学习图像变量和图像本身之间的因果模型的问题。"]}
{"source": "Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function. This property is useful to computationally-intensive neural network classifiers, as the cost of computing the partition function grows linearly with the number of classes and may become prohibitive. In particular, since neural language models may deal with up to millions of classes, their self-normalization properties received notable attention. Several  recent studies empirically found that language models, trained using Noise Contrastive Estimation (NCE), exhibit self-normalization, but could not explain why. In this study, we provide a theoretical justification to this property by viewing  NCE as a low-rank matrix approximation. Our empirical investigation compares NCE to the alternative explicit approach for self-normalizing language models. It also uncovers a surprising negative correlation between self-normalization and  perplexity, as well as some regularity in the observed errors that may potentially be used for improving self-normalization algorithms in the future.", "target": ["我们证明NCE是自我归一化的，并在数据集上证明了这一点。", "提出了NCE的自我归一化证明，因为它是归一化条件概率矩阵的低秩矩阵近似，是低秩近似的结果。", "本文考虑了自我归一化模型的问题，并通过用矩阵因式分解解释NCE来解释自我归一化机制。"]}
{"source": "Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings. Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge. Our work addresses the question: can affect lexica improve the word representations learnt from a corpus? In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach. We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus. Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection. We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.", "target": ["用情感信息来丰富词的嵌入，可以提高它们在情感预测任务上的表现。", "提议使用情感词库来改进词嵌入，使其优于标准的Word2vec和Glove。", "本文提出将量化词语影响的语义资源的信息整合到基于文本的词语嵌入算法中，以使语言模型更能反映语义和语用现象。", "本文介绍了对word2vec和GloVe损失函数的修改，以纳入情感词库，促进情感敏感词嵌入的学习。"]}
{"source": "Different kinds of representation learning techniques on graph have shown significant effect in downstream machine learning tasks. Recently, in order to inductively learn representations for graph structures that is unobservable during training, a general framework with sampling and aggregating (GraphSAGE) was proposed by Hamilton and Ying and had been proved more efficient than transductive methods on fileds like transfer learning or evolving dataset. However, GraphSAGE is uncapable of selective neighbor sampling and lack of memory of known nodes that've been trained. To address these problems, we present an unsupervised method that samples neighborhood information attended by co-occurring structures and optimizes a trainable global bias as a representation expectation for each node in the given graph. Experiments show that our approach outperforms the state-of-the-art inductive and unsupervised methods for representation learning on graphs.", "target": ["对于无监督和归纳式网络嵌入，我们提出了一种新的方法，通过利用双注意力架构和引入全局偏见，分别探索最相关的邻居和保留先前学习的节点知识。", "这就提出了对GraphSAGE的扩展，在局部聚合函数中使用全局嵌入偏置矩阵，并采用一种方法对感兴趣的节点进行采样。"]}
{"source": "Learning distributed representations for nodes in graphs is a crucial primitive in network analysis with a wide spectrum of applications. Linear graph embedding methods learn such representations by optimizing the likelihood of both positive and negative edges while constraining the dimension of the embedding vectors. We argue that the generalization performance of these methods is not due to the dimensionality constraint as commonly believed, but rather the small norm of embedding vectors. Both theoretical and empirical evidence are provided to support this argument: (a) we prove that the generalization error of these methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension; (b) we show that the generalization performance of linear graph embedding methods is correlated with the norm of embedding vectors, which is small due to the early stopping of SGD and the vanishing gradients. We performed extensive experiments to validate our analysis and showcased the importance of proper norm regularization in practice.", "target": ["我们认为，线性图嵌入的泛化并不是由于维度的约束，而是由于嵌入向量的小范数。", "作者表明，线性图嵌入方法的泛化误差受嵌入向量的范数约束，而不是维度约束。", "作者对学习图嵌入的泛化性能提出了一个理论约束，并认为坐标的范数决定了所学表征的成功。"]}
{"source": "Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover. Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE. We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers. Code is immediately available.", "target": ["混合普通SGD和动量（或用Adam做类似的事情），获得巨大利润。", "本文提出了对SGD和Adam的简单修改，称为QH变体，可以恢复父方法和其他一系列的优化技巧。", "经典动量的一个变体，它采取动量和梯度更新的加权平均，以及对其与其他基于动量的优化方案之间关系的评估。"]}
{"source": "Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function. lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads. Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.", "target": ["通过允许RL代理决定它要权衡n个步骤的每一个回报的程度来概括lambda-returns的一种新方法。", "扩展了带有lambda回报的A3C算法，并提出一种学习回报权重的方法。", "作者提出了基于置信度的自律回报，这是一种深度学习RL方法，用于调整TD(lambda)-like价值估计中的资格向量的权重，以有利于对状态的更稳定的估计。"]}
{"source": "Current end-to-end deep learning driving models have two problems: (1) Poor  generalization ability of unobserved driving environment when diversity of train-  ing driving dataset is limited (2) Lack of accident explanation ability when driving  models don’t work as expected. To tackle these two problems, rooted on the be-  lieve that knowledge of associated easy task is benificial for addressing difficult  task, we proposed a new driving model which is composed of perception module  for see and think and driving module for behave, and trained it with multi-task  perception-related basic knowledge and driving knowledge stepwisely.  Specifi-  cally segmentation map and depth map (pixel level understanding of images) were  considered as what & where and how far knowledge for tackling easier driving-  related perception problems before generating final control commands for difficult  driving task. The results of experiments demonstrated the effectiveness of multi-  task perception knowledge for better generalization and accident explanation abil-  ity. With our method the average sucess rate of finishing most difficult navigation  tasks in untrained city of CoRL test surpassed current benchmark method for 15  percent in trained weather and 20 percent in untrained weathers.", "target": ["我们提出了一个新的自驾游模型，该模型由观察和思考的感知模块和行为的驾驶模块组成，以获得更好的泛化和事故解释能力。", "提出了一个多任务学习架构，用于深度和分割图的估计以及使用感知模块和驾驶决策模块的驾驶预测。", "一种修改后的端到端结构的方法，具有更好的概括和解释能力，对不同的测试环境更加稳健，并且有解码器输出，可以帮助调试模型。", "作者提出了一个用于端到端驾驶的多任务卷积神经网络，并提供了用CARLA开源模拟器进行的评估，显示在新的驾驶条件下比基线有更好的泛化性能。"]}
{"source": "Recently there has been a surge of interest in designing graph embedding methods. Few, if any, can scale to a large-sized graph with millions of nodes due to both computational complexity and memory requirements. In this paper, we relax this limitation by introducing the MultI-Level Embedding (MILE) framework – a generic methodology allowing contemporary graph embedding methods to scale to large graphs. MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique to maintain the backbone structure of the graph. It then applies existing embedding methods on the coarsest graph and refines the embeddings to the original graph through a novel graph convolution neural network that it learns. The proposed MILE framework is agnostic to the underlying graph embedding techniques and can be applied to many existing graph embedding methods without modifying them. We employ our framework on several popular graph embedding techniques and conduct embedding for real-world graphs. Experimental results on five large-scale datasets demonstrate that MILE significantly boosts the speed (order of magnitude) of graph embedding while also often generating embeddings of better quality for the task of node classification. MILE can comfortably scale to a graph with 9 million nodes and 40 million edges, on which existing methods run out of memory or take too long to compute on a modern workstation.", "target": ["一个通用框架，将现有的图嵌入技术扩展到大型图。", "本文提出了一个多级嵌入框架，应用于现有的网络嵌入方法之上，以便以更快的速度扩展到大规模的网络。", "作者提出了一个提高嵌入质量的大规模图形嵌入的三阶段框架。"]}
{"source": "Anomaly detection discovers regular patterns in unlabeled data and identifies the non-conforming data points, which in some cases are the result of malicious attacks by adversaries. Learners such as One-Class Support Vector Machines (OCSVMs) have been successfully in anomaly detection, yet their performance may degrade significantly in the presence of sophisticated adversaries, who target the algorithm itself by compromising the integrity of the training data. With the rise in the use of machine learning in mission critical day-to-day activities where errors may have significant consequences, it is imperative that machine learning systems are made secure. To address this, we propose a defense mechanism that is based on a contraction of the data, and we test its effectiveness using OCSVMs. The proposed approach introduces a layer of uncertainty on top of the OCSVM learner, making it infeasible for the adversary to guess the specific configuration of the learner. We theoretically analyze the effects of adversarial perturbations on the separating margin of OCSVMs and provide empirical evidence on several benchmark datasets, which show that by carefully contracting the data in low dimensional spaces, we can successfully identify adversarial samples that would not have been identifiable in the original dimensional space. The numerical results show that the proposed method improves OCSVMs performance significantly (2-7%)", "target": ["通过对数据进行选择性的非线性转换，以较低的维度增加OCSVMs对目标性、完整性攻击的抵抗力的新方法。", "作者提出了对基于单类SVM的异常检测器的安全攻击的防御措施", "本文探讨了如何利用随机投影来使OCSVM对对抗性扰动的训练数据具有稳健性。"]}
{"source": "In this paper, we present a layer-wise learning of stochastic neural networks (SNNs) in an information-theoretic perspective. In each layer of an SNN, the compression and the relevance are defined to quantify the amount of information that the layer contains about the input space and the target space, respectively. We jointly optimize the compression and the relevance of all parameters in an SNN to better exploit the neural network's representation. Previously, the Information Bottleneck (IB) framework (\\cite{Tishby99}) extracts relevant information for a target variable. Here, we propose Parametric Information Bottleneck (PIB) for a neural network by utilizing (only) its model parameters explicitly to approximate the compression and the relevance. We show that, as compared to the maximum likelihood estimate (MLE) principle, PIBs : (i) improve the generalization of neural networks in classification tasks, (ii) push the representation of neural networks closer to the optimal information-theoretical representation in a faster manner.", "target": ["利用信息瓶颈原理学习更好的神经网络表征", "提出了一种基于信息瓶颈框架的学习方法，深层网络的隐藏层在压缩输入X的同时保持足够的信息来预测输出Y。", "本文提出了一种遵循类似于信息瓶颈的信息相关性/压缩框架来训练随机神经网络的新方法。"]}
{"source": "The maximum mean discrepancy (MMD) between two probability measures P  and Q is a metric that is zero if and only if all moments of the two measures  are equal, making it an appealing statistic for two-sample tests. Given i.i.d. samples  from P and Q, Gretton et al. (2012) show that we can construct an unbiased  estimator for the square of the MMD between the two distributions. If P is a  distribution of interest and Q is the distribution implied by a generative neural  network with stochastic inputs, we can use this estimator to train our neural network.  However, in practice we do not always have i.i.d. samples from our target  of interest. Data sets often exhibit biases—for example, under-representation of  certain demographics—and if we ignore this fact our machine learning algorithms  will propagate these biases. Alternatively, it may be useful to assume our data has  been gathered via a biased sample selection mechanism in order to manipulate  properties of the estimating distribution Q.  In this paper, we construct an estimator for the MMD between P and Q when we  only have access to P via some biased sample selection mechanism, and suggest  methods for estimating this sample selection mechanism when it is not already  known. We show that this estimator can be used to train generative neural networks  on a biased data sample, to give a simulator that reverses the effect of that  bias.", "target": ["我们提出了一个最大平均差异的估计器，适用于目标分布只能通过有偏见的样本选择程序获得的情况，并表明它可以用于生成网络以纠正这种偏见。", "提出了一个重要性加权的MMD估计器，以估计基于根据已知或估计的未知方案偏向的样本的分布之间的MMD。", "作者解决了MMD-GANs中的样本选择偏差问题，并提出使用加权最大平均差异来估计两个分布之间的MMD。", "本文介绍了对用于训练生成网络的目标的修改，该目标具有MMD对抗性"]}
{"source": "We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.", "target": ["使用贝叶斯回归来估计Q函数的后验，并将汤普森抽样作为一种有针对性的探索策略，有效地权衡探索和开发。", "作者提出了一种新的深层RL的探索算法，他们应用贝叶斯线性回归与DQN网络最后一层的特征来估计每个动作的Q函数。", "作者描述了如何使用带有汤普森采样的贝叶斯神经网络在q-learning中进行高效探索，并提出了一种优于ε-greedy探索方法的方法。"]}
{"source": "In this work, we propose the polynomial convolutional neural network (PolyCNN), as a new design of a weight-learning efficient variant of the traditional CNN. The biggest advantage of the PolyCNN is that at each convolutional layer, only one convolutional filter is needed for learning the weights, which we call the seed filter, and all the other convolutional filters are the polynomial transformations of the seed filter, which is termed as an early fan-out. Alternatively, we can also perform late fan-out on the seed filter response to create the number of response maps needed to be input into the next layer. Both early and late fan-out allow the PolyCNN to learn only one convolutional filter at each layer, which can dramatically reduce the model complexity by saving 10x to 50x parameters during learning. While being efficient during both training and testing, the PolyCNN does not suffer performance due to the non-linear polynomial expansion which translates to richer representational power within the convolutional layers. By allowing direct control over model complexity, PolyCNN provides a flexible trade-off between performance and efficiency. We have verified the on-par performance between the proposed PolyCNN and the standard CNN on several visual datasets, such as MNIST, CIFAR-10, SVHN, and ImageNet.", "target": ["PolyCNN只需要在每层学习一个种子卷积滤波器。这是传统CNN的一个有效变体，具有同等的性能。", "试图通过使用滤波器的多项式变换来减少CNN模型参数的数量，以创造放大滤波器的响应。", "作者提出了一种权重共享架构，用于减少带有种子滤波器的卷积神经网络参数数量"]}
{"source": "Detecting the emergence of abrupt property changes in time series is a challenging problem. Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches. However, selecting kernels is non-trivial in practice. Although kernel selection for the two-sample test has been studied, the insufficient samples in change point detection problem hinder the success of those developed kernel selection algorithms. In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model. With deep kernel parameterization, KL-CPD endows kernel two-sample test with the data-driven kernel to detect different types of change-points in real-world applications. The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies.", "target": ["在本文中，我们提出了KL-CPD，一个用于时间序列CPD的新型核学习框架，通过辅助生成模型作为异常分布的替代物，优化测试功率的下限。", "描述了一种新的方法来优化内核的选择，以提高测试能力，并显示出比其他方法更好。"]}
{"source": "Theories in cognitive psychology postulate that humans use similarity as a basis  for object categorization. However, work in image classification generally as-  sumes disjoint and equally dissimilar classes to achieve super-human levels of  performance on certain datasets. In our work, we adapt notions of similarity using  weak labels over multiple hierarchical levels to boost classification performance.  Instead of pitting clustering directly against classification, we use a warm-start  based evaluation to explicitly provide value to a clustering representation by its  ability to aid classification. We evaluate on CIFAR10 and a fine-grained classifi-  cation dataset to show improvements in performance with the procedural addition  of intermediate losses and weak labels based on multiple hierarchy levels. Further-  more, we show that pretraining AlexNet on hierarchical weak labels in conjunc-  tion with intermediate losses outperforms a classification baseline by over 17% on  a subset of Birdsnap dataset. Finally, we show improvement over AlexNet trained  using ImageNet pre-trained weights as initializations which further supports our   claim of the importance of similarity.", "target": ["先聚类再分类；使用弱标签来改进分类", "建议在深度网络的多个层次上使用基于聚类的损失函数，以及使用标签空间的分层结构来训练更好的表征。", "本文利用分层标签信息对神经网络训练中的中间表征施加额外损失。"]}
{"source": "Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.", "target": ["基于优势的遗憾最小化是一种新的深度强化学习算法，对部分可观察的任务特别有效，如《毁灭战士》和《Minecraft》中的第一人称导航。", "本文介绍了Deep RL领域的反事实遗憾最小化的概念和一种叫做ARM的算法，该算法可以更好地处理部分可观察性。", "本文基于反事实遗憾最小化的思想，提供了一个受博弈论启发的策略梯度算法的变体，并声称该方法可以比标准方法更好地处理部分可观察域。"]}
{"source": "Recent deep multi-task learning (MTL) has been witnessed its success in alleviating data scarcity of some task by utilizing domain-specific knowledge from related tasks. Nonetheless, several major issues of deep MTL, including the effectiveness of sharing mechanisms, the efficiency of model complexity and the flexibility of network architectures, still remain largely unaddressed. To this end, we propose a novel generalized latent-subspace based knowledge sharing mechanism for linking task-specific models, namely tensor ring multi-task learning (TRMTL). TRMTL has a highly compact representation, and it is very effective in transferring task-invariant knowledge while being super flexible in learning task-specific features, successfully mitigating the dilemma of both negative-transfer in lower layers and under-transfer in higher layers. Under our TRMTL, it is feasible for each task to have heterogenous input data dimensionality or distinct feature sizes at different hidden layers. Experiments on a variety of datasets demonstrate our model is capable of significantly improving each single task’s performance, particularly favourable in scenarios where some of the tasks have insufficient data.", "target": ["一个适应张量环状表征的深度多任务学习模型", "用于多任务学习的张量环公式的变体，通过共享一些TT核心来学习 \"共同任务\"，同时为每个独立任务学习单独的TT核心。"]}
{"source": "Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.", "target": ["通过将注意力纳入神经过程，建立了一个学习随机过程的条件分布的回归模型。", "提出通过在确定性路径中加入注意力机制来解决神经过程法中的欠拟合问题。", "对神经过程框架的扩展，增加了一个基于注意力的调节机制，使该模型能够更好地捕捉调节集的依赖性。", "作者通过纳入自我注意来丰富上下文点的特征，以及交叉注意来产生特定查询的表征来扩展神经过程。他们解决了NP的欠拟合问题，并表明ANP比NP收敛得更好、更快。"]}
{"source": "Deconvolutional layers have been widely used in a variety of deep  models for up-sampling, including encoder-decoder networks for  semantic segmentation and deep generative models for unsupervised  learning. One of the key limitations of deconvolutional operations  is that they result in the so-called checkerboard problem. This is  caused by the fact that no direct relationship exists among adjacent  pixels on the output feature map. To address this problem, we  propose the pixel deconvolutional layer (PixelDCL) to establish  direct relationships among adjacent pixels on the up-sampled feature  map. Our method is based on a fresh interpretation of the regular  deconvolution operation. The resulting PixelDCL can be used to  replace any deconvolutional layer in a plug-and-play manner without  compromising the fully trainable capabilities of original models.  The proposed PixelDCL may result in slight decrease in efficiency,  but this can be overcome by an implementation trick. Experimental  results on semantic segmentation demonstrate that PixelDCL can  consider spatial features such as edges and shapes and yields more  accurate segmentation outputs than deconvolutional layers. When used  in image generation tasks, our PixelDCL can largely overcome the  checkerboard problem suffered by regular deconvolution operations.", "target": ["通过建立像素间的依赖关系解决去反卷积层中的棋盘问题", "这项工作为卷积神经网络提出了像素去卷积层，作为缓解棋盘效应的一种方式。", "一种新的技术来概括标准CNN架构中使用的解卷积操作，它提出对相邻的像素特征进行顺序预测，从而为解卷积层带来更多的空间平滑输出。"]}
{"source": "In this paper, the preparation of a neural network for pruning and few-bit quantization is formulated as a variational inference problem. To this end, a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights, is introduced and a differentiable Kullback-Leibler divergence approximation for this prior is derived. After training with Variational Network Quantization, weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0). The method does not require fine-tuning after quantization. Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10).", "target": ["我们使用具有多模态、稀疏性诱导先验的变分贝叶斯推理对神经网络权重进行量化和修整。", "提出用连续穗状突变1/abs的混合物作为贝叶斯神经网络的先验，并证明了minist和cifar-10的相对稀疏化的凸网络的良好性能。", "本文提出了一种变分贝叶斯方法，以一种原则性的方式将神经网络权重量化为三元值后进行训练。"]}
{"source": "Deep neural networks (DNNs) although achieving human-level performance in many domains, have very large model size that hinders their broader applications on edge computing devices. Extensive research work have been conducted on DNN model compression or pruning. However, most of the previous work took heuristic approaches. This work proposes a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints. Motivated by dynamic programming, the proposed method reaches extremely high pruning rate by using partial prunings with moderate pruning rates. Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios. It achieves up to 34× pruning rate for ImageNet dataset and 167× pruning rate for MNIST dataset, significantly higher than those reached by the literature work. Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates. The codes and pruned DNN models are released in the anonymous link bit.ly/2zxdlss.", "target": ["我们实施了一种DNN权重修剪方法，实现了最高的修剪率。", "本文专注于神经网络压缩的权重修剪，实现了AlexNet的30倍压缩率和ImageNet的VGG。", "一种渐进式修剪技术，对权重参数施加结构稀疏性约束，并将优化改写为ADMM框架，实现了比投影梯度下降更高的精度。"]}
{"source": "In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.", "target": ["本文提出了一个新的深度学习架构，用于解决稀疏和不规则采样的多变量时间序列的监督学习问题。", "提出了一个对稀疏的、不规则采样的时间序列数据进行预测的框架，使用一个插值模块，在使用平滑插值、非平滑插值和强度中对缺失值进行建模。", "利用半参数插值网络和预测网络，解决了稀疏和不规则采样的多变量时间序列的监督学习问题。"]}
{"source": "We introduce an analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function as well as a regularizer for machine learning applications. We compare our novel construction to other point set distance functions and show proof of concept experiments for training neural networks end-to-end on point set prediction tasks such as object detection.", "target": ["点集预测的排列不变损失函数。", "提出了一种新的点注册损失（对准两个点集），具有较好的排列不变的特性。", "本文介绍了一个新的点集之间的距离函数，在一个端到端的物体检测任务中应用了另外两个排列距离，并表明在二维中，全息损失的所有局部最小值都是全局最小值。", "提出了取决于集合距离的排列不变的损失函数。"]}
{"source": "We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. Our method learns to assign graph operations to groups and to allocate those groups to available devices. The grouping and device allocations are learned jointly. The proposed method is trained with policy gradient and requires no human intervention. Experiments with widely-used  computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations. In addition, our approach outperforms placements by human  experts as well as a previous state-of-the-art placement method based on deep reinforcement learning. Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.", "target": ["我们引入了一个分层模型，用于将计算图高效地端对端放置在硬件设备上。", "建议共同学习操作符的分组，将学习到的分组放在设备上，通过强化学习分配操作，进行深度学习。", "作者的目的是用一个完全连接的网络来取代为加速TensorFlow模型的运行时间而提出的自动放置方法中的共同定位步骤。", "提出了一种设备放置算法，将tensorflow的操作放在设备上。"]}
{"source": "Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels. Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types. In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry. Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.", "target": ["我们提出了利用群体属性来学习没有标签的运动表征的方法，并展示了这种方法在代表二维和三维运动方面的应用。", "建议从图像序列的潜在表示中学习刚性运动组，而不需要明确的标签，并在MINST数字序列和KITTI数据集上实验证明了该方法。", "本文提出了一种以无监督的方式学习视频运动特征的方法，使用约束条件来优化神经网络，以产生可用于回归里程的特征。"]}
{"source": "This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general. Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases. We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS. Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters. This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity. Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate.", "target": ["本文提出了一种新颖的卷积层，它在一个连续的再现核希尔伯特空间中运行。", "将实例投射到RK希尔伯特空间，并对该空间进行卷积和过滤。", "本文提出了一种卷积神经网络的变体，它将激活和过滤器建模为由核基组成的连续函数。"]}
{"source": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.", "target": ["ImageNet训练的CNN偏向于物体纹理（而不是像人类一样偏向于形状）。克服人类和机器视觉之间的这一主要差异，可以产生更好的检测性能和以前没有见过的对图像失真的稳健性。", "使用图像风格来增强ImageNet训练的CNN的训练数据，使产生的网络看起来与人类的判断更加一致", "本文研究了AlexNet、VGG、GoogleNet和ResNet50等CNN，显示这些模型在ImageNet上训练时偏向于纹理，并提出了一个新的ImageNet数据集。"]}
{"source": "In this work, we exploited different strategies to provide prior knowledge to commonly used generative modeling approaches aiming to obtain speaker-dependent low dimensional representations from short-duration segments of speech data, making use of available information of speaker identities. Namely, convolutional variational autoencoders are employed, and statistics of its learned posterior distribution are used as low dimensional representations of fixed length short-duration utterances. In order to enforce speaker dependency in the latent layer, we introduced a variation of the commonly used prior within the variational autoencoders framework, i.e. the model is simultaneously trained for reconstruction of inputs along with a discriminative task performed on top of latent layers outputs. The effectiveness of both triplet loss minimization and speaker recognition are evaluated as implicit priors on the challenging cross-language NIST SRE 2016 setting and compared against fully supervised and unsupervised baselines.", "target": ["我们评估了在由变分自动编码器学习的后验分布的统计资料上执行辅助判别任务的有效性，以执行说话人的依赖性。", "提出一个自动编码器模型来学习使用短时分析窗口进行说话人验证的表示方法。", "解决短时段说话人识别问题的变分自动编码器模型的修正版"]}
{"source": "The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K). In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions. Based on this analysis we develop jackknife variational inference (JVI),  a family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders. We also report first results on applying JVI to learning variational autoencoders.   Our implementation is available at https://github.com/Microsoft/jackknife-variational-inference", "target": ["变分推理是有偏差的，让我们对它去除偏差。", "介绍了jackknife变分推理，这是一种对蒙特卡洛目标（如重要性加权的自动编码器）进行去重的方法。", "作者分析了IWAE约束的偏差和方差，并推导出一种估计矩的jacknife方法，作为有限重要性加权样本的debias IWAE的方式。"]}
{"source": "In this paper, we consider the problem of autonomous lane changing for self driving vehicles in a multi-lane, multi-agent setting. We present a framework that demonstrates a more structured and data efficient alternative to end-to-end complete policy learning on problems where the high-level policy is hard to formulate using traditional optimization or rule based methods but well designed low-level controllers are available. Our framework uses deep reinforcement learning solely to obtain a high-level policy for tactical decision making, while still maintaining a tight integration with the low-level controller, thus getting the best of both worlds. We accomplish this with Q-masking, a technique with which we are able to incorporate prior knowledge, constraints, and information from a low-level controller, directly in to the learning process thereby simplifying the reward function and making learning faster and data efficient. We provide preliminary results in a simulator and show our approach to be more efficient than a greedy baseline, and more successful and safer than human driving.", "target": ["一个框架，通过学习用深度强化学习做出高层次的战术决策，并与低层次的控制器保持紧密结合，采取低层次的行动，为自主改变车道提供政策。", "考虑了在多车道多代理老虎车设置中自驾车的自主变道问题，提出了一种新的学习策略Q-masking--将确定的低级控制器与高级战术决策策略相耦合。", "本文提出了一种使用 \"Q-masking \"的深度Q-learning方法来解决变道问题，该方法根据限制条件或先验知识来减少行动空间。", "作者提出了一种方法，该方法使用基于Q-learning的高级策略，该策略与来自安全约束和低级控制器的上下文掩码相结合，使某些行动在某些状态下不能被选择。"]}
{"source": "Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering. Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates. To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space. We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively. Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs. In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation. We show that NGE significantly outperforms previous methods by an order of magnitude. As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs. Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2  machine.", "target": ["用图神经网络自动搜索机器人设计", "提出了一种基于神经图进化的自动机器人设计方法。实验证明，同时优化控制器和硬件要比只优化控制器要好。", "作者提出了一个基于机器人结构的图表示的方案，以及作为控制器的图-神经-网络来优化机器人结构，并与他们的控制器相结合。"]}
{"source": "Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of conditional molecule generation.", "target": ["我们展示了一个图的自动编码器。", "学习使用深度学习方法 \"一次性 \"地生成图，直接输出节点和边的存在概率，以及节点属性向量。", "用于生成图形的变分自动编码器"]}
{"source": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.", "target": ["我们提出了一种新的LSTM训练算法，通过向二值门学习，我们表明它具有许多良好的特性。", "为LSTM提出一个新的 \"门 \"函数，使门的值朝向0或1。 ", "本文旨在通过采用最近的Gumbel-Softmax技巧来获得端到端的可训练分类分布，从而推动LSTM门的二元化。"]}
{"source": "We present a personalized recommender system using neural network for recommending  products, such as eBooks, audio-books, Mobile Apps, Video and Music.  It produces recommendations based on customer’s implicit feedback history such  as purchases, listens or watches. Our key contribution is to formulate recommendation  problem as a model that encodes historical behavior to predict the future  behavior using soft data split, combining predictor and auto-encoder models. We  introduce convolutional layer for learning the importance (time decay) of the purchases  depending on their purchase date and demonstrate that the shape of the time  decay function can be well approximated by a parametrical function. We present  offline experimental results showing that neural networks with two hidden layers  can capture seasonality changes, and at the same time outperform other modeling  techniques, including our recommender in production. Most importantly, we  demonstrate that our model can be scaled to all digital categories, and we observe  significant improvements in an online A/B test. We also discuss key enhancements  to the neural network model and describe our production pipeline. Finally  we open-sourced our deep learning library which supports multi-gpu model parallel  training. This is an important feature in building neural network based recommenders  with large dimensionality of input and output data.", "target": ["在一个零售网站的多个产品类别中使用神经网络的时间敏感模型来改进推荐。", "本文提出了一种新的基于神经网络的推荐方法。", "作者描述了从头开始建立他们的生产推荐系统的程序，并将购买的时间衰减纳入学习框架。"]}
{"source": "Deep Learning (DL) algorithms based on Generative Adversarial Network (GAN) have demonstrated great potentials in computer vision tasks such as image restoration. Despite the rapid development of image restoration algorithms using DL and GANs, image restoration for specific scenarios, such as medical image enhancement and super-resolved identity recognition, are still facing challenges. How to ensure visually realistic restoration while avoiding hallucination or mode- collapse? How to make sure the visually plausible results do not contain hallucinated features jeopardizing downstream tasks such as pathology identification and subject identification?  Here we propose to resolve these challenges by coupling the GAN based image restoration framework with another task-specific network. With medical imaging restoration as an example, the proposed model conducts additional pathology recognition/classification task to ensure the preservation of detailed structures that are important to this task. Validated on multiple medical datasets, we demonstrate the proposed method leads to improved deep learning based image restoration while preserving the detailed structure and diagnostic features. Additionally, the trained task network show potentials to achieve super-human level performance in identifying pathology and diagnosis.  Further validation on super-resolved identity recognition tasks also show that the proposed method can be generalized for diverse image restoration tasks.", "target": ["将基于GAN的图像修复框架与另一个特定任务的网络结合起来，在保留特定任务特征的同时生成真实的图像。", "一种新的图像耦合的任务--GAN方法，该方法将GAN和特定任务网络结合起来，缓解了避免幻觉或模式崩溃的情况。", "作者建议用另一个特定任务的分支，如分类任务，来增强基于GAN的图像修复，以进一步改进。"]}
{"source": "Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.", "target": ["一个端到端的训练有素的深度神经网络，利用高斯混合模型，在一个由深度自动编码器学习的低维空间中进行密度估计和无监督的异常检测。", "本文提出了一个降维-聚类的联合深度学习框架，导致了有竞争力的异常检测。", "一种新的异常检测技术，其中降维和密度估计步骤被联合优化。"]}
{"source": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "target": ["我们提出了投射子空间网络，用于少样本和半监督的少样本学习", "本文提出了一种新的基于嵌入的方法来解决少样本学习的问题，并将这种模式扩展到半监督的几率学习环境中。", "基于学习一般嵌入，然后为每个类别学习其子空间的完全和半监督的少样本分类的新方法"]}
{"source": "This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning. To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE). In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games. The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The learned contingency information is used as a part of the state representation for exploration purposes. We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards. For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data. Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations.", "target": ["我们研究了探索中的应急意识和可控制的方面，并在没有专家示范的情况下，在Montezuma's Revenge中取得了最先进的性能。", "本文研究了在面临稀疏奖励任务时，通过识别状态的可控（学习）特征，提取有意义的状态表征来帮助探索的问题。", "本文提出了在稀疏奖励强化学习任务中使用应急意识来帮助探索的新思路，获得了最先进的结果。"]}
{"source": "Disentangling factors of variation has always been a challenging problem in representation learning. Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, bad quality of generated images from encodings, lack of identity information, etc. In this paper, we proposed a supervised algorithm called DNA-GAN trying to disentangle different attributes of images. The latent representations of images are DNA-like, in which each individual piece represents an independent factor of variation. By annihilating the recessive piece and swapping a certain piece of two latent representations, we obtain another two different representations which could be decoded into images. In order to obtain realistic images and also disentangled representations, we introduced the discriminator for adversarial training. Experiments on Multi-PIE and CelebA datasets demonstrate the effectiveness of our method and the advantage of overcoming limitations existing in other methods.", "target": ["我们提出了一种有监督的算法，即DNA-GAN，来分解图像的多种属性。", "本文研究了使用生成式对抗网络生成属性条件下的图像问题，并提出从属性和潜伏代码中生成图像作为高层表征。", "本文提出了一种新的方法，使用新型DNA结构的GAN来分解图像的不同属性。"]}
{"source": "Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode. We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations). This approach to representation learning is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data). We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.", "target": ["本文提出了一种新型的潜变量生成模型技术，能够将全局信息表示为一个潜变量，将局部信息表示为另一个潜变量。", "本文提出了一种VAE，它使用标签将学到的表征分成不变部分和协变部分。"]}
{"source": "Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe;  training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a  very large collection (i.e. active learning).   Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.", "target": ["我们把主动学习的问题看作是一个核心集的选择问题，并表明这种方法在批量主动学习的设置中特别有用，这在训练CNN时是至关重要的。", "作者为多类分类提供了一种算法无关的主动学习算法", "本文提出了一种针对CNN的批处理模式主动学习算法，作为一个核心集问题，其性能优于随机抽样和不确定性抽样。", "研究卷积神经网络的主动学习，将主动学习问题表述为核心集选择，并提出了一种新的策略"]}
{"source": "Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps. We introduce a simple stochastic algorithm (\\textit{h}-detach) that is specific to LSTM optimization and targeted towards addressing this problem. Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them. Our algorithm\\footnote{Our code is available at https://github.com/bhargav104/h-detach. } prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better. We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets.", "target": ["改善LSTM中长期依赖性的优化和处理的简单算法", "本文介绍了一种名为h-detach的简单随机算法，该算法专门针对LSTM优化，并以解决这一问题为目标。", "提出了对LSTM训练过程的简单修改，以促进沿细胞状态的梯度传播，或 \"线性时间路径\""]}
{"source": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "target": ["用垃圾箱类适当地训练CNN，可以提高它们对对抗性攻击的稳健性，以及处理分布外样本的能力。", "本文提出在CNN模型中增加一个额外的标签来检测OOD样本和对抗性例子。", "本文提出了一个额外的类别，即在CNN中为对抗性样本和外分布样本纳入自然外分布图像和插值图像。"]}
{"source": "Modern deep artificial neural networks have achieved impressive results through models with very large capacity---compared to the number of training examples---that control overfitting with the help of different forms of regularization. Regularization can be implicit, as is the case of stochastic gradient descent or parameter sharing in convolutional layers, or explicit. Most common explicit regularization techniques, such as dropout and weight decay, reduce the effective capacity of the model and typically require the use of deeper and wider architectures to compensate for the reduced capacity. Although these techniques have been proven successful in terms of results, they seem to waste capacity. In contrast, data augmentation techniques reduce the generalization error by increasing the number of training examples and without reducing the effective capacity. In this paper we systematically analyze the effect of data augmentation on some popular architectures and conclude that data augmentation alone---without any other explicit regularization techniques---can achieve the same performance or higher as regularized models, especially when training with fewer examples.", "target": ["在一个用足够的数据增强水平训练的深度卷积神经网络中，通过SGD优化，明确的正则器（权重衰减和放弃）可能不会提供任何额外的泛化改进。", "本文提出了数据扩充作为常用正则化技术的替代方案，并表明对于少数参考模型/任务，仅使用数据扩充就能获得相同的泛化性能。", "本文对深度神经网络在图像分类中的数据增强进行了系统研究，认为数据增强可以复制一些常见的正则器，如权重衰减和丢弃。"]}
{"source": "Text editing on mobile devices can be a tedious process. To perform various editing operations, a user must repeatedly move his or her fingers between the text input area and the keyboard, making multiple round trips and breaking the flow of typing. In this work, we present Gedit, a system of on-keyboard gestures for convenient mobile text editing. Our design includes a ring gesture and flicks for cursor control, bezel gestures for mode switching, and four gesture shortcuts for copy, paste, cut, and undo. Variations of our gestures exist for one and two hands. We conducted an experiment to compare Gedit with the de facto touch+widget based editing interactions. Our results showed that Gedit’s gestures were easy to learn, 24% and 17% faster than the de facto interactions for one- and two-handed use, respectively, and preferred by participants.", "target": ["在这项工作中，我们提出了Gedit，一个用于方便移动文本编辑的键盘手势系统。", "报告了Gedit交互技术的设计和评估。", "提出了一套新的触摸手势，在移动设备中进行文本输入和文本编辑之间的无缝转换"]}
{"source": "Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.", "target": ["我们证明，DNN是最大熵原理的递归近似解。", "提出了一个推导，将DNN与最大熵模型拟合的递归应用联系起来。", "本文旨在从最大熵原理的角度提供一个深度学习的观点。"]}
{"source": "As people learn to navigate the world, autonomic nervous system (e.g., ``fight or flight) responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger. We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency. We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.", "target": ["我们提出了一种新的强化学习方法，利用与任务无关的内在奖励函数，在与人类自律神经系统反应相关的周边脉搏测量上进行训练。", "在自动驾驶的背景下，提出了一个基于人类情绪反应的强化学习框架。", "作者提议在RL框架内使用信号，如影响决策的基本自律内脏反应，通过直接从人类神经系统反应中学习的模型来增强RL奖励功能。", "提出利用生理信号来提高强化学习算法的性能，并通过测量心脏脉搏振幅来建立一个不那么稀疏的内在奖励函数。"]}
{"source": "Deep convolutional neural networks (CNNs) are known to be robust against label noise on extensive datasets. However, at the same time, CNNs are capable of memorizing all labels even if they are random, which means they can memorize corrupted labels. Are CNNs robust or fragile to label noise? Much of researches focusing on such memorization uses class-independent label noise to simulate label corruption, but this setting is simple and unrealistic. In this paper, we investigate the behavior of CNNs under class-dependently simulated label noise, which is generated based on the conceptual distance between classes of a large dataset (i.e., ImageNet-1k). Contrary to previous knowledge, we reveal CNNs are more robust to such class-dependent label noise than class-independent label noise. We also demonstrate the networks under class-dependent noise situations learn similar representation to the no noise situation, compared to class-independent noise situations.", "target": ["CNN对标签噪声是稳健的还是脆弱的？实际上，稳健。", "作者使用WordNet的ImageNet 1k树挑战CNN对标签噪音的稳健性。", "卷积神经网络模型在引入类相关和类独立噪声时的性能分析", "证明了CNN对类相关的标签噪声具有更强的稳健性，并认为真实世界的噪声应该是类相关的。"]}
{"source": "Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies  with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.", "target": ["用GANs进行高质量的音频合成", "提出了一种使用GAN框架的方法，通过在谱域中以足够的频率分辨率对对数幅度和瞬时频率进行建模来生成音频。", "用GAN从噪声中生成音频样本的策略，对架构和表示方法进行必要的改变，以生成包含可解释的潜伏代码的令人信服的音频。", "提出了一个简单的想法，以更好地代表音频数据，从而可以应用生成式对抗网络等卷积模型。"]}
{"source": "In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation. Next we build a neural network architecture compatible with our optimization approach and motivated by graph filtering in the vertex domain. We demonstrate that the learned graph has richer structure than often used nearest neighbors graphs constructed based on features similarity. Our experiments demonstrate that we can improve prediction quality for several convolution on graphs architectures, while others appeared to be insensitive to the input graph.", "target": ["顶点域中的信号过滤的图形优化。", "本文研究了使用投影子梯度下降算法学习具有非负边缘权重的稀疏连接无向图的邻接矩阵。", "开发了一种新的神经网络图的邻接矩阵的反推方案"]}
{"source": "The use of AR in an industrial context could help for the training of new operators. To be able to use an AR guidance system, we need a tool to quickly create a 3D representation of the assembly line and of its AR annotations. This tool should be very easy to use by an operator who is not an AR or VR specialist: typically the manager of the assembly line. This is why we proposed WAAT, a 3D authoring tool allowing user to quickly create 3D models of the workstations, and also test the AR guidance placement. WAAT makes on-site authoring possible, which should really help to have an accurate 3D representation of the assembly line. The verification of AR guidance should also be very useful to make sure everything is visible and doesn't interfere with technical tasks. In addition to these features, our future work will be directed in the deployment of WAAT into a real boiler assembly line to assess the usability of this solution.", "target": ["本文描述了一种用于在工业4.0的装配线上提供AR的3D创作工具。", "本文讨论了AR创作工具如何支持装配线系统的培训，并提出了一种方法", "一个用于工业装配线的AR指导系统，可以现场编写AR内容。", "提出了一个系统，允许工厂工人使用增强现实系统进行更有效的培训。"]}
{"source": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "target": ["我们表明，如果选择适当的步长，在训练GAN中广泛使用的一阶迭代算法实际上会以亚线性的速度收敛到静止的解决方案。", "本文使用GAN和多任务学习为某些最小最大值问题上的原始-双重算法提供了收敛保证。", "通过假设一类有限的模型，将问题表述为原始-双重优化问题，分析了GAN的学习动力学。"]}
{"source": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.", "target": ["我们展示了如何使用深度RL来构建能够解决矩阵游戏之外的社会困境的代理。", "学习玩有不完全信息的状态的双人一般和的游戏", "规定了一个触发策略（CCC）和相应的算法，证明了在社会困境中收敛到有效的结果，而不需要代理观察对方的行动。"]}
{"source": "In distributed training, the communication cost due to the transmission of gradients  or the parameters of the deep model is a major bottleneck in scaling up the number  of processing nodes. To address this issue, we propose dithered quantization for  the transmission of the stochastic gradients and show that training with Dithered  Quantized Stochastic Gradients (DQSG) is similar to the training with unquantized  SGs perturbed by an independent bounded uniform noise, in contrast to the other  quantization methods where the perturbation depends on the gradients and hence,  complicating the convergence analysis. We study the convergence of training  algorithms using DQSG and the trade off between the number of quantization  levels and the training time. Next, we observe that there is a correlation among the  SGs computed by workers that can be utilized to further reduce the communication  overhead without any performance loss. Hence, we develop a simple yet effective  quantization scheme, nested dithered quantized SG (NDQSG), that can reduce the  communication significantly without requiring the workers communicating extra  information to each other. We prove that although NDQSG requires significantly  less bits, it can achieve the same quantization variance bound as DQSG. Our  simulation results confirm the effectiveness of training using DQSG and NDQSG  in reducing the communication bits or the convergence time compared to the  existing methods without sacrificing the accuracy of the trained model.", "target": ["本文提出并分析了在分布式学习中交流随机梯度的两种量化方案，与目前的技术水平相比，这将减少通信成本，同时保持相同的精度。", "作者提出将抖动量化应用于通过训练过程计算的随机梯度，这改善了量化误差，与基线相比取得了卓越的结果，并提出了一个嵌套方案以减少通信成本。", "作者建立了分布式优化中的通信减少和抖动量化之间的联系，并开发了两种新的分布式训练算法，其中通信开销被大大减少。"]}
{"source": "Deep neural networks have been shown to perform well in many classical machine learning problems, especially in image classification tasks. However, researchers have found that neural networks can be easily fooled, and they are surprisingly sensitive to small perturbations imperceptible to humans.   Carefully crafted input images (adversarial examples) can force a well-trained neural network to provide arbitrary outputs.   Including adversarial examples during training is a popular defense mechanism against adversarial attacks. In this paper we propose a new defensive mechanism under the generative adversarial network~(GAN) framework. We model the adversarial noise using a generative network, trained jointly with a classification discriminative network as a minimax game. We show empirically that our adversarial network approach works well against black box attacks, with performance on par with state-of-art methods such as ensemble adversarial training and adversarial training with projected gradient descent.", "target": ["将对抗性噪声生成网络与分类网络联合训练，对对抗性攻击提供更好的稳健性。", "一个用于深度分类模型的GAN解决方案，面对白盒和黑盒攻击，能生成稳健的模型。", "本文提出了一种防御机制，利用GAN的生成扰动作为对抗性例子，并使用判别器来区分对抗性攻击。"]}
{"source": "Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians. Hence, in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.", "target": ["使用集成方法作为对深度神经网络的对抗性扰动的防御。", "本文提出将集成作为一种对抗性防御机制。", "在MNIST和CIFAR10这两个流行的数据集上，实证研究了不同的深度神经工程组合对FGSM和BIM这两类攻击的稳健性。"]}
{"source": "In this paper, we propose the Associative Conversation Model that generates visual information from textual information and uses it for generating sentences in order to utilize visual information in a dialogue system without image input. In research on Neural Machine Translation, there are studies that generate translated sentences using both images and sentences, and these studies show that visual information improves translation performance. However, it is not possible to use sentence generation algorithms using images for the dialogue systems since many text-based dialogue systems only accept text input. Our approach generates (associates) visual information from input text and generates response text using context vector  fusing associative visual information and sentence textual information. A comparative experiment between our proposed model and a model without association showed that our proposed model is generating useful sentences by associating visual information related to sentences. Furthermore, analysis experiment of visual association showed that our proposed model generates (associates) visual information effective for sentence generation.", "target": ["基于文本信息和与文本信息相关的视觉信息融合的句子生成方法的建议", "这项工作描述了一个用于对话系统的深度学习模型，它利用了视觉信息。", "本文提出了一个新的基础对话的数据集，并提出了一个计算上的观察，即即使在进行基于文本的对话时，它也可以帮助推理视觉。", "建议通过收集由文本和相关图像或视频组成的一堆数据，将视觉信息纳入其中，从而增强传统的基于文本的句子生成/对话方法。"]}
{"source": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "target": ["我们提出了一种新型的情景循环卷积网络，具有视觉学习的鲁棒性。", "本文介绍了反馈连接，通过纳入上下文信息来加强特征学习。", "本文提出在卷积网络中加入 \"循环\"连接，并采用门控机制。"]}
{"source": "Deep neural networks have led to a series of breakthroughs, dramatically improving the state-of-the-art in many domains. The techniques driving these advances, however, lack a formal method to account for model uncertainty. While the Bayesian approach to learning provides a solid theoretical framework to handle uncertainty, inference in Bayesian-inspired deep neural networks is difficult. In this paper, we provide a practical approach to Bayesian learning that relies on a regularization technique found in nearly every modern network, batch normalization. We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty. Using our approach, it is possible to make meaningful uncertainty estimates using conventional architectures without modifying the network or the training procedure. Our approach is thoroughly validated in a series of empirical experiments on different tasks and using various measures, showing it to outperform baselines on a majority of datasets with strong statistical significance.", "target": ["我们表明，使用批量归一化训练深度网络等同于贝叶斯模型的近似推理，我们还证明了这一发现如何让我们对传统网络的模型不确定性做出有用的估计。", "本文提出在测试时使用批量规范化来获得预测的不确定性，并表明在测试时使用批量规范的蒙特卡洛预测优于丢弃。", "提出被称为批量规范化的规范化程序可以被理解为进行近似的贝叶斯推理，就其产生的不确定性估计值而言，其表现与MC dropout相似。"]}
{"source": "Data-parallel neural network training is network-intensive, so gradient dropping was designed to exchange only large gradients.   However, gradient dropping has been shown to slow convergence.   We propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network. We empirically confirm with machine translation tasks that gradient dropping with local gradients approaches convergence 48% faster than non-compressed multi-node training and 28% faster compared to vanilla gradient dropping. We also show that gradient dropping with a local gradient update does not reduce the model's final quality.", "target": ["我们改进了梯度下降（一种在分布式训练中只交换大梯度的技术），在做参数更新时加入了局部梯度，以减少质量损失并进一步提高训练时间。", "本文提出了一种结合局部和全局梯度的3种模式，以更好地利用更多的计算节点", "研究减少实施分布式优化技术的通信要求问题，特别是SGD。"]}
{"source": "We establish the relation between Distributional RL and the Upper Confidence Bound (UCB) approach to exploration.      In this paper we show that the density of the Q function estimated by Distributional RL can be successfully used for the estimation of UCB. This approach does not require counting and, therefore, generalizes well to the Deep RL. We also point to the asymmetry of the empirical densities estimated by the Distributional RL algorithms like QR-DQN. This observation leads to the reexamination of the variance's performance in the UCB type approach to exploration. We introduce truncated variance as an alternative estimator of the UCB and a novel algorithm based on it. We empirically show that newly introduced algorithm achieves better performance in multi-armed bandits setting. Finally, we extend this approach to high-dimensional setting and test it on the Atari 2600 games. New approach achieves better performance compared to QR-DQN in 26 of games, 13 ties out of 49 games.", "target": ["使用分布式RL和截断方差进行的探索。", "提出了一种通过UCB技术管理探索-开发权衡的RL方法。", "一种使用分位数回归DQN学到的分布进行探索的方法，以取代通常的ε-greedy策略。", "提出了新的算法（QUCB和QUCB+）来处理Multi-Armed Bandit和更普遍的强化学习中的探索权衡。"]}
{"source": "Good representations facilitate transfer learning and few-shot learning. Motivated by theories of language and communication that explain why communities with large number of speakers have, on average, simpler languages with more regularity, we cast the representation learning problem in terms of learning to communicate. Our starting  point sees traditional autoencoders as  a single encoder with a fixed decoder partner that must learn to communicate. Generalizing from there, we introduce community-based autoencoders in which multiple encoders and decoders collectively learn representations by being randomly paired up on successive training iterations. Our experiments show that increasing community sizes reduce idiosyncrasies in the learned codes, resulting in more invariant representations with increased reusability and structure.", "target": ["在语言和通信理论的推动下，我们引入了基于社区的自动编码器，其中多个编码器和解码器共同学习结构化和可重复使用的表征。", "作者解决了表征学习的问题，旨在建立可重复使用和结构化的表征，认为传统AE中编码器和解码器之间的共同适应会产生糟糕的表征，并介绍了基于社区的自动编码器。", "本文提出了一个基于社区的自动编码器框架，以解决编码器和解码器的共同适应问题，旨在构建更好的表征。"]}
{"source": "Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt. Humans use this ability to quickly solve a  task instance, and to bootstrap learning of new tasks. Achieving these abilities in autonomous agents is an open problem. In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn both (i) policies for high-fidelity one-shot imitation of diverse novel skills, and (ii) policies that enable the agent to solve tasks more efficiently than the demonstrators. MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL. This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task.  The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.", "target": ["我们提出了MetaMimic算法，该算法将示范数据集作为输入，并输出（i）一次性高保真模仿策略和（ii）无条件的任务策略。", "本文研究了具有高模仿精度的一次性模仿问题，将DDPGfD扩展到只使用状态轨迹。", "本文提出了一种具有高精确度的一次性模仿方法，并解决了模仿学习中常见的探索问题。", "提出了一种从视频演示中学习的RL方法，不需要获得专家的行动。"]}
{"source": "Normalization methods are a central building block in the deep learning toolbox. They accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules. When learning from multi-modal distributions, the effectiveness of batch normalization (BN), arguably the most prominent normalization method, is reduced. As a remedy, we propose a more flexible approach: by extending the normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features. We demonstrate that our method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets.", "target": ["我们提出了一种新的深度神经网络的归一化方法，对中间特征分布中的多模态具有稳健性。", "在特征空间中学习多模态分布的归一化方法", "在单元激活在批处理和空间维度上的统计数字不是单峰的假设下，提出了批处理归一化的一般化。"]}
{"source": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.", "target": ["我们提出了一种基于知识提炼的方法来提高多语言神经机器翻译的准确性。", "一个多对一的多语言神经机器翻译模型，首先为每个语言对训练单独的模型，然后进行蒸馏。", "本文旨在通过用基于单个（单语言对）教师模型的提炼成分来增强标准的交叉熵损失，来训练机器翻译模型。"]}
{"source": "What makes humans so good at solving seemingly complex video games?   Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play.", "target": ["我们研究了有助于人类学习的各种先验知识，发现关于物体的一般先验知识在指导人类游戏方面发挥了最关键的作用。", "作者通过实验研究了人类先验的哪些方面对视频游戏中的强化学习是重要的。", "作者提出了一项关于人类在玩视频游戏时采用的先验因素的研究，并证明了存在一种分类法，这些特征在不同程度上影响着完成游戏中的任务的能力。"]}
{"source": "Driven by the need for parallelizable hyperparameter optimization methods, this paper studies \\emph{open loop} search methods: sequences that are predetermined and can be generated before a single configuration is evaluated. Examples include grid search, uniform random search, low discrepancy sequences, and other sampling distributions.  In particular, we propose the use of $k$-determinantal point processes in  hyperparameter optimization via random search. Compared to conventional uniform random search where hyperparameter settings are sampled independently, a $k$-DPP promotes diversity.   We describe an approach that transforms hyperparameter search spaces for efficient use with a $k$-DPP. In addition, we introduce a novel Metropolis-Hastings algorithm which can sample from $k$-DPPs defined over spaces with a mixture of discrete and continuous dimensions. Our experiments show significant benefits over uniform random search  in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel.", "target": ["在对可并行的、开环的超参数优化方法的需求驱动下，我们提出在通过随机搜索进行的超参数优化中使用k行列式的点过程。", "提议在超参数搜索中使用k-DPP来选择候选点。", "作者提出了k-DPP作为超参数优化的开环方法，并提供了其实证研究和与其他方法的比较。", "考虑使用行列式的点过程进行非顺序的和不知情的超参数搜索，这是在地面集的子集上的概率分布，其特性是具有更多 \"不同 \"元素的子集有更高的概率。"]}
{"source": "In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.  When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.  However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task.  In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.  We eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network.", "target": ["在归纳迁移学习中，对预先训练好的卷积网络进行微调，大大超过了从头训练的效果。", "解决了深度网络中的转移学习问题，并提出要有一个正则化项来惩罚与初始化的分歧。", "提出了对深度迁移学习的不同自适应正则化技术的分析，特别关注L@-SP条件的使用"]}
{"source": "Artificial neural networks have opened up a world of possibilities in data science and artificial intelligence, but neural networks are cumbersome tools that grow with the complexity of the learning problem. We make contributions to this issue by considering a modified version of the fully connected layer we call a block diagonal inner product layer. These modified layers have weight matrices that are block diagonal, turning a single fully connected layer into a set of densely connected neuron groups. This idea is a natural extension of group, or depthwise separable, convolutional layers applied to the fully connected layers. Block diagonal inner product layers can be achieved by either initializing a purely block diagonal weight matrix or by iteratively pruning off diagonal block entries. This method condenses network storage and speeds up the run time without significant adverse effect on the testing accuracy, thus offering a new approach to improve network computation efficiency.", "target": ["我们着眼于具有块状对角线内积层的神经网络，以提高效率。", "本文提出使神经网络中的内层为块状对角线，并讨论了块状对角线矩阵比修剪更有效，块状对角线层导致更有效的网络。", "用块状对角线全连接层取代全连接层"]}
{"source": "One of the challenges in the study of generative adversarial networks is the instability of its training.   In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.  Our new normalization technique is computationally light and easy to incorporate into existing implementations.   We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.", "target": ["我们提出了一种新的权重归一化技术，即光谱归一化，以稳定GAN的判别器的训练。", "本文使用光谱正则化来规范GAN目标，随之而来的GAN被称为SN-GAN，基本上保证了判别器的Lipschitz属性。", "本文提出了 \"光谱归一化\"，在改善GAN的训练方面向前迈出了一大步。"]}
{"source": "Humans acquire complex skills by exploiting previously learned skills and making transitions between them. To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at. We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills. The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments. We make our environments, primitive skills, and code public for further research at https://youngwoon.github.io/transition .", "target": ["过渡策略使代理人能够通过平稳地连接以前获得的原始技能来组成复杂的技能。", "提出了一种在连续域中执行给定选项时过渡到有利的分层状态的方案。这使用了两个同时进行的学习过程。", "提出了一种学习从一个任务过渡到另一个任务的策略的方法，目的是利用状态接近估计器对过渡策略进行奖励，完成复杂的任务。", "提出了一个新的训练方案，用学习到的辅助奖励函数来优化过渡策略，将前一个宏操作/选择的结束状态与后一个宏操作/选择的良好启动状态连接起来。"]}
{"source": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.  We found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "target": ["我们对一个和两个GRU单元能够和不能在连续时间内捕获的动态特征进行了分类，并通过k-step时间序列预测实验验证了我们的发现。", "作者将隐藏规模为1和2的GRU分析为连续时间动态系统，声称隐藏状态表示的表达能力可以提供关于GRU在给定数据集上表现如何的先验知识。", "本文从动态系统的角度分析了GRU，并表明2d GRU可以被训练成采用各种固定点，并可以近似于线吸引子，但不能模仿环形吸引子。", "将GRU方程转换为连续时间，利用理论和经验研究一维和二维GRU网络，并展示这些系统中的各种动态拓扑结构。"]}
{"source": "Stacked hourglass network has become an important model for Human pose estimation. The estimation of human body posture depends on the global information of the keypoints type and the local information of the keypoints location. The consistent processing of inputs and constraints makes it difficult to form differentiated and determined collaboration mechanisms for each stacked hourglass network. In this paper, we propose a Multi-Scale Stacked Hourglass (MSSH) network to high-light the differentiation capabilities of each Hourglass network for human pose estimation.   The pre-processing network forms feature maps of different scales,and dispatch them to various locations of the stack hourglass network, where the small-scale features reach the front of stacked hourglass network, and large-scale features reach the rear of stacked hourglass network.    And a new loss function is proposed for multi-scale stacked hourglass network.   Different keypoints have different weight coefficients of loss function at different scales, and the keypoints weight coefficients are dynamically adjusted from the top-level hourglass network to the bottom-level hourglass network.   Experimental results show that the pro-posed method is competitive with respect to the comparison algorithm on MPII and LSP datasets.", "target": ["差异化的输入会引起网络的功能差异化，网络之间损失函数的相互作用会影响优化过程。", "对原始沙漏网络的修改，用于单一姿势的估计，产生了比原始基线的改进。", "作者用inception-resnet-A模块扩展了一个堆叠的沙漏网络，并提出了一种多尺度的方法，用于静态RGB图像中的人体姿势估计。"]}
{"source": "We present a new unsupervised method for learning general-purpose sentence embeddings.  Unlike existing methods which rely on local contexts, such as words  inside the sentence or immediately neighboring sentences, our method selects, for  each target sentence, influential sentences in the entire document based on a document  structure. We identify a dependency structure of sentences using metadata  or text styles. Furthermore, we propose a novel out-of-vocabulary word handling  technique to model many domain-specific terms, which were mostly discarded by  existing sentence embedding methods. We validate our model on several tasks  showing 30% precision improvement in coreference resolution in a technical domain,  and 7.5% accuracy increase in paraphrase detection compared to baselines.", "target": ["为了使用技术文件训练句子嵌入，我们的方法考虑了文件结构，以寻找更广泛的上下文并处理词汇外的单词。", "提出了通过借鉴更多的语境来改善句子嵌入的想法。", "用句子的依赖性信息学习句子表征", "通过使用更广泛的证据集来形成句子的表征，扩展了SkipThough方法中使用的形成无监督的句子表征的想法"]}
{"source": "Neural network training relies on our ability to find ````````\"good\" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood.   In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple ``\"filter normalization\" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture effects the loss landscape, and how training parameters affect the shape of minimizers.", "target": ["我们使用一系列的可视化方法探索神经损失函数的结构，以及损失景观对泛化的影响。", "本文提出了一种将NN的损失函数可视化的方法，并对NN的可训练性和泛化提出了见解。", "研究了损失面和优化路径的非凸性。"]}
{"source": "Deep models are state-of-the-art for many computer vision tasks including image classification and object detection. However, it has been shown that deep models are vulnerable to adversarial examples. We highlight how one-hot encoding directly contributes to this vulnerability and propose breaking away from this widely-used, but highly-vulnerable mapping. We demonstrate that by leveraging a different output encoding, multi-way encoding, we can make models more robust. Our approach makes it more difficult for adversaries to find useful gradients for generating adversarial attacks. We present state-of-the-art robustness results for black-box, white-box attacks, and achieve higher clean accuracy on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN when combined with adversarial training. The strength of our approach is also presented in the form of an attack for model watermarking, raising challenges in detecting stolen models.", "target": ["我们证明，通过利用多向输出编码，而不是广泛使用的独热编码，我们可以使深度模型对对抗性攻击更加稳健。", "本文提出通过将每个标签编码为高维向量，并训练分类器以最小化与正确类别的编码的L2距离，来取代分类器中以一热标签训练的最终交叉熵层。", "作者提出了对抗对抗性攻击的新方法，与基线相比，显示出显著的收益。"]}
{"source": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.", "target": ["我们引入了第一个具有完全并行解码的NMT模型，将推理延迟降低了10倍。", "这项工作为编码器-解码器框架提出了非自回归解码器，其中生成单词的决定不取决于已生成的词的先前决定。", "本文介绍了一种对神经机器翻译进行非自回归解码的方法，有可能实现更多的并行解码，这可以使速度大大加快。", "建议引入一组潜变量来代表每个源词的生育率，使目标句子的生成非自回归。"]}
{"source": "While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.", "target": ["我们展示了一种可认证的、可训练的和可扩展的方法，用于防御对抗性实例。", "提出了一种新的防御神经网络安全攻击的方法，其攻击模型可以输出算法的安全证书。", "推导出对具有一个隐藏层的神经网络的对抗性扰动的上限"]}
{"source": "We formulate an information-based optimization problem for supervised classification. For invertible neural networks, the control of these information terms is passed down to the latent features and parameter matrix in the last fully connected layer, given that mutual information is invariant under invertible map.   We propose an objective function and prove that it solves the optimization problem. Our framework allows us to learn latent features in an more interpretable form while improving the classification performance. We perform extensive quantitative and qualitative experiments in comparison with the existing state-of-the-art classification models.", "target": ["我们提出了一种能提高神经网络分类性能的正则器", "作者建议从预测和真实输出之间的相互信息最大化的角度来训练模型，并在学习的同时将不相关的信息降到最低的正则化项。", "提出将参数分解为可逆的特征图F和最后一层的线性变换w，以使相互信息I(Y, hat{T})最大化，同时约束不相关信息"]}
{"source": "Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood. These models often suffer from poor use of their latent variable, with ad-hoc annealing factors used to encourage retention of information in the latent variable. We discuss an alternative and general approach to latent variable modelling, based on an objective that encourages a perfect reconstruction by tying a stochastic autoencoder with a variational autoencoder (VAE). This ensures by design that the latent variable captures information about the observations, whilst retaining the ability to generate well. Interestingly, although our model is fundamentally different to a VAE, the lower bound attained is identical to the standard VAE bound but with the addition of a simple pre-factor; thus, providing a formal interpretation of the commonly used, ad-hoc pre-factors in training VAEs.", "target": ["本文介绍了一个新的生成模型框架，该框架避免了潜在变量的崩溃，并阐明了在训练变分自动编码器时使用某些特殊的因素。", "本文提出解决关于变分自动编码器忽略潜在变量的问题。", "本文提出在原VAE模型中加入随机自动编码器，以解决语言模型的LSTM解码器可能过于强大而忽略潜变量信息的问题。", "本文介绍了AutoGen，它将生成式变分自动编码器与基于自动编码器的高保真重建模型结合起来，以更好地利用潜在的表示方法。"]}
{"source": "This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions. This problem exists in many previous recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL), where the testing instances come from either seen or unseen/novel classes with different probabilistic distributions. Previous works only calibrate the conﬁdent prediction of classiﬁers of seen classes (WSVM Scheirer et al. (2014)) or taking unseen classes as outliers Socher et al. (2013). In contrast, this paper proposes a probabilistic way of directly estimating and ﬁne-tuning the decision boundary between seen and unseen classes. In particular, we propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain. Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the ﬁrst time, are introduced to uncover and ﬁne-tune the decision boundary of each domain. Critically, the uncertain domain is newly introduced in our framework to adopt those instances whose domain labels cannot be predicted conﬁdently. Extensive experiments demonstrate that our approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks.", "target": ["本文通过分割来自不同概率分布的实例来研究领域划分的问题。", "本文论述了开放集学习和广义零样本学习中的新颖性识别问题，并提出了一种可能的解决方案", "一种基于引导的领域分离方法，以确定已知类别的相似性截止阈值，然后用Kolmogorov-Smirnoff测试来完善引导的内分布区。", "建议引入一个新的域，即不确定域，以更好地处理开放集和广义零样本学习中看到/看不到的域的划分。"]}
{"source": "Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such out-of-equilibrium behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.", "target": ["SGD隐含地执行了变分推理；梯度噪声是高度非各向异性的，因此SGD甚至不能收敛到原始损失的临界点。", "本文对SGD作为一个非平衡过程进行了变分分析。", "本文讨论了神经网络背景下标准SGD最小化的正则化目标函数，并利用Fokker-Planck方程提供了一个变分推理的视角。", "开发了一种理论，研究随机梯度噪声对SGD的影响，特别是对深度神经网络模型的影响。"]}
{"source": "The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.", "target": ["代理在训练时不需要任何形式的监督，从自己的经验中学习后，可以在测试时学习模仿单纯的视觉演示（没有动作）。", "本文提出了通过学习参数化的技能函数进行零样本视觉学习的方法。", "一篇关于模仿推理过程中刚刚提出的任务的论文，其中学习是以自我监督的方式进行的，在训练过程中，代理探索相关但不同的任务。", "提出了一种避开昂贵的专家示范问题的方法，即利用代理的随机探索来学习无需特定预训练就能应用的通用技能。"]}
{"source": "Distributional Semantics Models(DSM) derive word space from linguistic items  in context. Meaning is obtained by defining a distance measure between vectors  corresponding to lexical entities. Such vectors present several problems. This  work concentrates on quality of word embeddings, improvement of word embedding  vectors, applicability of a novel similarity metric used ‘on top’ of the  word embeddings. In this paper we provide comparison between two methods  for post process improvements to the baseline DSM vectors. The counter-fitting  method which enforces antonymy and synonymy constraints into the Paragram  vector space representations recently showed improvement in the vectors’ capability  for judging semantic similarity. The second method is our novel RESM  method applied to GloVe baseline vectors. By applying the hubness reduction  method, implementing relational knowledge into the model by retrofitting synonyms  and providing a new ranking similarity definition RESM that gives maximum  weight to the top vector component values we equal the results for the ESL  and TOEFL sets in comparison with our calculations using the Paragram and Paragram  + Counter-fitting methods. For SIMLEX-999 gold standard since we cannot  use the RESM the results using GloVe and PPDB are significantly worse compared  to Paragram. Apparently, counter-fitting corrects hubness. The Paragram  or our cosine retrofitting method are state-of-the-art results for the SIMLEX-999  gold standard. They are 0.2 better for SIMLEX-999 than word2vec with sense  de-conflation (that was announced to be state-of the-art method for less reliable  gold standards). Apparently relational knowledge and counter-fitting is more important  for judging semantic similarity than sense determination for words. It is to  be mentioned, though that Paragram hyperparameters are fitted to SIMLEX-999  results. The lesson is that many corrections to word embeddings are necessary  and methods with more parameters and hyperparameters perform better.", "target": ["论文描述了一个加强词向量空间模型的过程，并对Paragram和GloVe模型的相似度基准进行了评估。", "本文提出了一种新的算法，调整GloVe词向量，然后使用它们之间的非欧几里得相似度函数。", "作者提出了对现有向量空间模型弱点的看法，并列出了完善现有词汇向量的六步方法"]}
{"source": "Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.   We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and  ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration. Both results beat the exiting quantization works with large margins.   We extend our alternating quantization to image classification tasks. In both RNNs and feedforward neural networks, the method also achieves  excellent performance.", "target": ["我们提出了一种新的量化方法，并将其应用于RNN的量化，以实现压缩和加速。", "本文提出了一种用于循环神经网络的多位量化方法。", "一种量化神经网络权重矩阵的技术，以及一种交替优化过程，以估计最能代表原始向量的k个二进制向量和系数的集合。"]}
{"source": "The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset. The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression. We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.", "target": ["我们用多尺度纠缠重正化假定取代了神经网络的全连接层，这是一种描述长距离相关的量子操作。", "在这篇论文中，作者建议使用MERA张量技术来压缩神经网络。", "用于神经网络的线性映射的新参数化，使用线性图的分层因子化，减少了参数的数量，同时仍允许对相对复杂的相互作用进行建模。", "研究利用低秩张量分解压缩前馈层，探索树状分解的方法"]}
{"source": "Deep learning models have outperformed traditional methods in many fields such  as natural language processing and computer vision. However, despite their  tremendous success, the methods of designing optimal Convolutional Neural Networks  (CNNs) are still based on heuristics or grid search. The resulting networks  obtained using these techniques are often overparametrized with huge computational  and memory requirements. This paper focuses on a structured, explainable  approach towards optimal model design that maximizes accuracy while keeping  computational costs tractable. We propose a single-shot analysis of a trained CNN  that uses Principal Component Analysis (PCA) to determine the number of filters  that are doing significant transformations per layer, without the need for retraining.  It can be interpreted as identifying the dimensionality of the hypothesis space  under consideration. The proposed technique also helps estimate an optimal number  of layers by looking at the expansion of dimensions as the model gets deeper.  This analysis can be used to design an optimal structure of a given network on  a dataset, or help to adapt a predesigned network on a new dataset. We demonstrate  these techniques by optimizing VGG and AlexNet networks on CIFAR-10,  CIFAR-100 and ImageNet datasets.", "target": ["我们提出了对训练有素的神经网络的单次分析，以消除冗余并确定最佳网络结构", "本文提出了一套启发式方法，用于识别一个好的神经网络架构，基于数据集上的单元激活的PCA", "本文提出了一个通过识别跨层的冗余过滤器来优化神经网络结构的框架。"]}
{"source": "Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary’s capability to conduct attacks on black-box networks. This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels.   First, we define the threat model for these attacks:  our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim ’s deep learning  (DL) system is running and passively monitors the accesses of the target functions in the shared framework.   Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique. Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim’s entire network architecture.   In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having only observed one forward propagation. Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting. From this meta-model,  we evaluate the importance of the observed attributes in the fingerprinting process. Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker’s observations. Our empirical security analysis represents a step toward understanding the DNNs’ vulnerability to cache side-channel attacks.", "target": ["我们对利用缓存侧信道的DNN指纹攻击进行了首次深入的安全分析，这代表着向了解DNN对侧信道攻击的脆弱性迈出了一步。", "本文考虑了利用缓存侧信道对神经网络架构进行指纹识别的问题，并讨论了通过不安全防御的安全性。", "本文进行了缓存侧信道攻击，以提取受害者模型的属性并推断其架构，同时表明它们可以达到几乎完美的分类精度。"]}
{"source": "Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.", "target": ["我们提出了补充目标训练（COT），这是一种新的训练范式，可以优化主要目标和补充目标，以有效学习神经网络的参数。", "考虑用 \"补充 \"目标最大化来增强交叉熵目标，其目的是中和除地面真实标签之外的其他类别的预测概率。", "作者提出了一个基于评估从不正确的类中收集的信息的softmax最小化的次要目标，导致了一种新的训练方法。", "处理使用跨熵损失的分类或序列生成任务的神经网络的训练问题"]}
{"source": "We present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output. We extend softmax layer with an additional constant input. The corresponding additional output is able to represent the uncertainty of the network. The proposed method requires neither additional parameters nor multiple forward passes nor input preprocessing nor out-of-distribution datasets. We show that our method performs comparably to more computationally expensive methods and outperforms baselines on our experiments from image recognition and sentiment analysis domains.", "target": ["在没有额外的可学习参数的情况下，在单一的前向传递中进行不确定性估计。", "一种计算用于分类问题的DNN的输出不确定性估计的新方法，与最先进的不确定性估计方法相匹配，并在分布外检测任务中优于它们。", "作者提出了抑制性softmax，即通过增加一个常数激活对softmax进行修改，从而提供一个不确定性的衡量标准。"]}
{"source": "When deep learning is applied to sensitive data sets, many privacy-related implementation issues arise. These issues are especially evident in the healthcare, finance, law and government industries. Homomorphic encryption could allow a server to make inferences on inputs encrypted by a client, but to our best knowledge, there has been no complete implementation of common deep learning operations, for arbitrary model depths, using homomorphic encryption. This paper demonstrates a novel approach, efficiently implementing many deep learning functions with bootstrapped homomorphic encryption. As part of our implementation, we demonstrate Single and Multi-Layer Neural Networks, for the Wisconsin Breast Cancer dataset, as well as a Convolutional Neural Network for MNIST. Our results give promising directions for privacy-preserving representation learning, and the return of data control to users.", "target": ["我们做了一个功能丰富的深度学习系统，用加密的输入，产生加密的输出，保护了隐私。", "一个使用FHE方案的私有深度学习模型推理框架，支持快速引导，从而可以减少计算时间。", "本文提出了一种使用同态加密安全评估神经网络的方法。"]}
{"source": "In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.", "target": ["我们介绍了一个名为GamePad的系统，以探索机器学习方法在Coq证明助手中的定理证明的应用。", "本文描述了一个将机器学习应用于交互式定理证明的系统，重点讨论了战术预测和位置评估的任务，并表明神经模型在这两项任务上的表现优于SVM。", "建议使用机器学习技术来帮助在定理检验器Coq中构建证明。"]}
{"source": "Deep neural networks are usually huge, which significantly limits the deployment on low-end devices. In recent years, many  weight-quantized models have  been proposed. They have small storage and fast inference, but training can still be time-consuming. This can be improved with distributed learning. To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train deep networks with full-precision weights.   In this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence.  We show  that (i) weight-quantized models converge to an error related to the weight quantization resolution and weight dimension; (ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and (iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization. Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks.", "target": ["在本文中，我们从理论上和经验上研究了在分布式环境中对具有量化梯度的损失感知权重量化网络的高效训练。", "本文研究了分布式环境下不同梯度精度的损失意识权重量化的收敛特性，并提供了全精度、量化和量化剪切梯度的权重量化的收敛分析。", "作者提出分析在完全同步的分布式环境下训练参数化模型时，同时量化权重和梯度的效果。"]}
{"source": "Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them. To achieve Selfless Sequential Learning we study different regularization strategies and activation functions. We find that  imposing sparsity at the level of the representation (i.e. neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity. In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition. It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks. As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations,  our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain. We combine our novel regularizer with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network. We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement on diverse datasets.", "target": ["提高顺序学习性能的正则化策略", "一种新的、基于正则化的方法来解决顺序学习问题，使用固定大小的模型，为损失增加额外的项，鼓励表示的稀疏性和打击灾难性的遗忘。", "本文通过提出规范化的学习策略来处理终身学习中的灾难性遗忘问题"]}
{"source": "A Synaptic Neural Network (SynaNN) consists of synapses and neurons. Inspired by the synapse research of neuroscience, we built a synapse model with a nonlinear synapse function of excitatory and inhibitory channel probabilities. Introduced the concept of surprisal space and constructed a commutative diagram, we proved that the inhibitory probability function -log(1-exp(-x)) in surprisal space is the topologically conjugate function of the inhibitory complementary probability 1-x in probability space. Furthermore, we found that the derivative of the synapse over the parameter in the surprisal space is equal to the negative Bose-Einstein distribution. In addition, we constructed a fully connected synapse graph (tensor) as a synapse block of a synaptic neural network. Moreover, we proved the gradient formula of a cross-entropy loss function over parameters, so synapse learning can work with the gradient descent and backpropagation algorithms. In the proof-of-concept experiment, we performed an MNIST training and testing on the MLP model with synapse network as hidden layers.", "target": ["一个具有突触图和学习的突触神经网络，它具有拓扑共轭和玻色-爱因斯坦分布在surprisal空间的特点。", "作者提出了一种由突触图组成的混合神经工作，可以嵌入到标准神经网络中。", "提出了一个基于真实细胞膜的兴奋性和抑制性离子通道的生物启发的神经网络模型"]}
{"source": "Many types of relations in physical, biological, social and information systems can be modeled as homogeneous or heterogeneous concept graphs. Hence, learning from and with graph embeddings has drawn a great deal of research interest recently, but only ad hoc solutions have been obtained this far. In this paper, we conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms, and propose to extend this by introducing a multi-shot unsupervised learning framework. Empirical results on several real-world data set show that the proposed model consistently and significantly outperforms existing state-of-the-art approaches on knowledge base completion and graph based multi-label classification tasks.", "target": ["广义的图嵌入模型", "一种广义的知识图谱嵌入方法，基于三个不同的同时目标来学习嵌入，其性能与现有的最先进的方法相当，甚至超过了现有的方法。", "解决了使用神经网络学习多关系图的嵌入的任务", "提出了一种新的方法GEN，来计算多关系图的嵌入，特别是所谓的E-Cells和R-Cells可以回答(h,r,?),(?r,t)和(h,?,t)形式的查询"]}
{"source": "We introduce and study minimax curriculum learning (MCL), a new method for adaptively selecting a sequence of training subsets for a succession of stages in machine learning. The subsets are encouraged to be small and diverse early on, and then larger, harder, and allowably more homogeneous in later stages. At each stage, model weights and training sets are chosen by solving a joint continuous-discrete minimax optimization, whose objective is composed of a continuous loss (reflecting training set hardness) and a discrete submodular promoter of diversity for the chosen subset. MCL repeatedly solves a sequence of such optimizations with a schedule of increasing training set size and decreasing pressure on diversity encouragement. We reduce MCL to the minimization of a surrogate function handled by submodular maximization and continuous gradient methods. We show that MCL achieves better performance and, with a clustering trick, uses fewer labeled samples for both shallow and deep models while achieving the same performance. Our method involves repeatedly solving constrained submodular maximization of an only slowly varying function on the same ground set. Therefore, we develop a heuristic method that utilizes the previous submodular maximization solution as a warm start for the current submodular maximization process to reduce computation while still yielding a guarantee.", "target": ["Minimax Curriculum Learning是一种机器教学方法，涉及增加理想的硬度和预定减少多样性。", "一种课程学习方法，使用子模块函数来捕捉训练期间选择的例子的多样性。", "本文介绍了MiniMax Curriculum学习，作为一种通过为其提供不同的数据子集来适应性地训练模型的方法。"]}
{"source": "Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference. However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases. In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes? How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships? To address these challenges, we synthesize ideas from causality and modern probabilistic modeling. For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density. For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples. In experiments, we scale Bayesian inference on up to a billion genetic measurements. We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%.", "target": ["应用于因果关系和遗传学的隐性模型", "作者提出使用隐含模型来解决全基因组关联问题。", "本文针对全基因组关联研究中由于种群结构造成的混杂和基因组不同部分之间可能存在的非线性相互作用问题提出了解决方案，并在统计遗传学和ML之间建立了桥梁。", "提出了一个用于GWAS的非线性生成模型，该模型对种群结构进行建模，其中非线性的建模采用神经网络作为非线性函数近似器，推理采用无似然变分推理进行。"]}
{"source": "Few-shot learning trains image classifiers over datasets with few examples per category.   It poses challenges for the optimization algorithms, which typically require many examples to fine-tune the model parameters for new categories.   Distance-learning-based approaches avoid the optimization issue by embedding the images into a metric space and applying the nearest neighbor classifier for new categories. In this paper, we propose to exploit the object-level relation to learn the image relation feature, which is converted into a distance directly.  For a new category, even though its images are not seen by the model, some objects may appear in the training images. Hence, object-level relation is useful for inferring the relation of images from unseen categories. Consequently, our model generalizes well for new categories without fine-tuning.  Experimental results on benchmark datasets show that our approach outperforms state-of-the-art methods.", "target": ["通过利用对象级的关系来学习图像层面的关系（相似性），进行少样本学习", "本文通过提出一种基于嵌入的方法来处理少样本学习的问题，该方法可以学习比较支持和查询集例子之间的对象级特征。", "提出了一种基于近邻搜索，利用不同图像之间的对象级关系，将两张输入图像的特征图串联成一张特征图的少样本学习方法。"]}
{"source": "Word embeddings are widely used in machine learning based natural language processing systems. It is common to use pre-trained word embeddings which provide benefits such as reduced training time and improved overall performance. There has been a recent interest in applying natural language processing techniques to programming languages. However, none of this recent work uses pre-trained embeddings on code tokens. Using extreme summarization as the downstream task, we show that using pre-trained embeddings on code tokens provides the same benefits as it does to natural languages, achieving: over 1.9x speedup, 5\\% improvement in test loss, 4\\% improvement in F1 scores, and resistance to over-fitting. We also show that the choice of language used for the embeddings does not have to match that of the task to achieve these benefits and that even embeddings pre-trained on human languages provide these benefits to programming languages.", "target": ["探索应用于源代码的自然语言处理技术的研究人员没有使用任何形式的预训练嵌入，我们表明他们应该这样做。", "本文旨在了解通过使用类似NLP的语言模型对编程语言代码的词嵌入进行预训练是否会对极端的代码总结任务产生影响。", "这项工作显示了如何利用代码语料库预训练单词向量，从而获得比随机初始化和训练的表征更适合于函数/方法名称预测的表征。"]}
{"source": "Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data. These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network). In these two-player games, a reward is always received at the end of the game. However, the Rubik’s Cube has only a single solved state, and episodes are not guaranteed to terminate. This poses a major problem for these API algorithms since they rely on the reward received at the end of the game. We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away. Autodidactic Iteration is able to learn how to solve the Rubik’s Cube and the 15-puzzle without relying on human data. Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves — less than or equal to solvers that employ human domain knowledge.", "target": ["我们用纯强化学习解决Rubik's Cube问题", "使用强化学习（RL）与蒙特卡洛树搜索（MCTS）通过自律迭代解决Rubik cube问题。", "这项工作使用一种叫做Autodidactic迭代的近似策略迭代方法来解决Rubik's Cube ，通过创建自己的奖励系统来克服稀疏的奖励问题。", "介绍了一种解决Rubik's cube的深度RL算法，该算法可以处理Rubik's cube的巨大状态空间和非常稀疏的奖励。"]}
{"source": "Answering compositional questions requiring multi-step reasoning is challenging for current models. We introduce an end-to-end differentiable model for interpreting questions, which is inspired by formal approaches to semantics. Each span of text is represented by a denotation in a knowledge graph, together with a vector that captures ungrounded aspects of meaning. Learned composition modules recursively combine constituents, culminating in a grounding for the complete sentence which is an answer to the question. For example, to interpret ‘not green’, the model will represent ‘green’ as a set of entities, ‘not’ as a trainable ungrounded vector, and then use this vector to parametrize a composition function to perform a complement operation. For each sentence, we build a parse chart subsuming all possible parses, allowing the model to jointly learn both the composition operators and output structure by gradient descent. We show the model can learn to represent a variety of challenging semantic operators, such as quantifiers, negation, disjunctions and composed relations on a synthetic question answering task. The model also generalizes well to longer sentences than seen in its training data, in contrast to LSTM and RelNet baselines. We will release our code.", "target": ["我们描述了一个端到端的可微的问答模型，该模型通过学习构成的神经模块和句子的句法结构，学习将问题中的文本跨度表示为知识图谱中的指称。", "本文提出了一个用于视觉问答的模型，它可以学习模块化神经网络的参数和结构预测器，不需要监督结构或句法分析器的帮助。", "提出通过学习捕捉句法和学习词的语义的潜在树，从答案和KB中训练问答模型。"]}
{"source": "Deep learning software demands reliability and performance. However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter. We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM. Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity. With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.", "target": ["我们介绍了一种新颖的编译器基础设施，解决了现有深度学习框架的缺陷。", "提出在深度学习引擎中从临时性的代码生成转向编译器和语言的最佳实践。", "本文提出了一个编译器框架，允许为深度学习系统定义特定领域的语言，并定义了可以利用标准优化和神经网络的专门优化的编译阶段。", "本文介绍了一个DLVM，以利用张量编译器的编译器方面的优势"]}
{"source": "In this work, we focus on the problem of grounding language by training an agent  to follow a set of natural language instructions and navigate to a target object  in a 2D grid environment. The agent receives visual information through raw  pixels and a natural language instruction telling what task needs to be achieved.  Other than these two sources of information, our model does not have any prior  information of both the visual and textual modalities and is end-to-end trainable.  We develop an attention mechanism for multi-modal fusion of visual and textual  modalities that allows the agent to learn to complete the navigation tasks and also  achieve language grounding. Our experimental results show that our attention  mechanism outperforms the existing multi-modal fusion mechanisms proposed in  order to solve the above mentioned navigation task. We demonstrate through the  visualization of attention weights that our model learns to correlate attributes of  the object referred in the instruction with visual representations and also show  that the learnt textual representations are semantically meaningful as they follow  vector arithmetic and are also consistent enough to induce translation between instructions  in different natural languages. We also show that our model generalizes  effectively to unseen scenarios and exhibit zero-shot generalization capabilities.  In order to simulate the above described challenges, we introduce a new 2D environment  for an agent to jointly learn visual and textual modalities", "target": ["在一个新的可定制的二维网格环境中，通过强化学习来实现基于注意力的language grounding架构", "本文解决了给定指令的导航问题，并提出了一种通过注意力机制将文字和视觉信息结合起来的方法。", "本文考虑了在先验未知环境的第一人称视角下遵循自然语言指令的问题，并提出了一种神经结构方法。", "研究通过遵循给定的自然语言描述和接收原始像素的视觉信息，在二维网格环境中导航到目标对象的问题。"]}
{"source": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A architecture called QANet, which does not require recurrent networks:  Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model.   On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.", "target": ["一个由卷积和注意力组成的简单架构取得了与有记载的最好的循环模型相同的结果。", "基于数据增强的快速高性能释义方法和仅使用卷积和注意力的非循环阅读理解模型。", "本文提出应用CNN+自我注意模块代替LSTM，用神经释义模型产生的段落释义加强RC模型训练，以提高RC的性能。", "本文提出了一个使用卷积和注意力的阅读理解模型，并建议通过基于现成的神经机器翻译的释义来增加额外的训练数据。"]}
{"source": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.   In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.", "target": ["我们介绍了球形CNN，一种针对球形信号的卷积网络，并将其应用于三维模型识别和分子能量回归。", "本文提出了一个构建球形卷积网络的框架，该框架基于对几个现有概念的新颖综合", "本文重点讨论了如何扩展卷积神经网络，使其具有内置的球形不变性，并采用非阿贝尔谐波分析的工具来实现这一目标。", "作者开发了一种新颖的方案，从根本上代表球形数据"]}
{"source": "We propose a novel method that makes use of deep neural networks and gradient decent to perform automated design on complex real world engineering tasks. Our approach works by training a neural network to mimic the fitness function of a design optimization task and then, using the differential nature of the neural network, perform gradient decent to maximize the fitness. We demonstrate this methods effectiveness by designing an optimized heat sink and both 2D and 3D airfoils that maximize the lift drag ratio under steady state flow conditions. We highlight that our method has two distinct benefits over other automated design approaches. First, evaluating the neural networks prediction of fitness can be orders of magnitude faster then simulating the system of interest. Second, using gradient decent allows the design space to be searched much more efficiently then other gradient free methods. These two strengths work together to overcome some of the current shortcomings of automated design.", "target": ["一种利用神经网络和梯度下降法对现实世界的物体（如散热器和机翼）进行自动化设计的方法。", "神经网络（参数化和预测）和梯度下降（反向传播），为工程任务自动设计。", "本文介绍了使用深度网络来逼近复杂物理系统的行为，然后通过优化该网络的输入来设计最佳设备。"]}
{"source": "Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results. However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization. We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy. Our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max GAN-like objective and an MMD objective under the same restrictions. We test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits. In both cases, the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time.", "target": ["我们提出了一个用于特征对齐的逻辑对抗距离的双重版本，并表明它比最小最大目标产生更稳定的梯度步骤迭代。", "本文讨论了在计算层面固定GAN的问题", "本文研究了一种基于逻辑损失上限的对抗性损失的双重表述，并将对抗性训练的标准最小最大问题转化为单一的最小化问题。", "建议通过对正则化逻辑回归的最大似然目标进行二元化，将GAN的鞍点目标（用于逻辑回归判别器）重新表述为最小化问题。"]}
{"source": "There are many applications scenarios for which the computational   performance and memory footprint of the prediction phase of Deep   Neural Networks (DNNs) need to be optimized. Binary Deep Neural   Networks (BDNNs) have been shown to be an effective way of achieving   this objective. In this paper, we show how Convolutional Neural   Networks (CNNs) can be implemented using binary   representations. Espresso is a compact, yet powerful   library written in C/CUDA that features all the functionalities   required for the forward propagation of CNNs, in a binary file less   than 400KB, without any external dependencies. Although it is mainly   designed to take advantage of massive GPU parallelism, Espresso also   provides an equivalent CPU implementation for CNNs. Espresso   provides special convolutional and dense layers for BCNNs,   leveraging bit-packing and bit-wise computations   for efficient execution. These techniques provide a speed-up of   matrix-multiplication routines, and at the same time, reduce memory   usage when storing parameters and activations. We experimentally   show that Espresso is significantly faster than existing   implementations of optimized binary neural networks (~ 2   orders of magnitude). Espresso is released under the Apache 2.0   license and is available at http://github.com/organization/project.", "target": ["二元神经网络的最先进的计算性能实现", "本文介绍了一个用C/CUDA编写的库，它具有BCNN前向传播所需的所有功能。", "本文在Binary-NET的基础上，将其扩展到CNN架构，提供了提高前向传递速度的优化，并提供了Binary CNN的优化代码。"]}
{"source": "Optimal selection of a subset of items from a given set is a hard problem that requires combinatorial optimization. In this paper, we propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization. We focus on the task of identifying a relevant set of sentences for claim verification in the context of the FEVER task. Conventional methods for this task look at sentences on their individual merit and thus do not optimize the informativeness of sentences as a set. We show that our proposed method which builds on the idea of unfolding a greedy algorithm into a computational graph allows both interpretability and gradient based training. The proposed differentiable greedy network (DGN) outperforms discrete optimization algorithms as well as other baseline methods in terms of precision and recall.", "target": ["我们提出了一种子集选择算法，这种算法可以用基于梯度的方法进行训练，但通过子模态优化实现了接近最佳的性能。", "提出了一个基于神经网络的模型，该模型通过将基于梯度的优化技术与子模块框架相结合，整合了子模块函数，并命名为 \"可微分贪婪网络\"（DGN）。", "提出了一个旨在选择元素子集的神经网络（例如，从一组检索到的文档中选择与声明主要相关的K句子）。"]}
{"source": "The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years. In spite of the advance, clustering with representation learning has been limited to flat-level categories, which oftentimes involves cohesive clustering with a focus on instance relations. To overcome the limitations of flat clustering, we introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space. Specifically, we place a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies under the variational autoencoder framework, and to adopt the generative process of a hierarchical-versioned Gaussian mixture model. Compared with a few prior works focusing on unifying representation learning and hierarchical clustering, HCRL is the first model to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components. This generation process enables more meaningful separations and mergers of clusters via branches in a hierarchy. In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features. We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.", "target": ["我们引入了分层聚类表征学习（HCRL），它同时优化了表征学习和嵌入空间的分层聚类。", "本文提出将嵌套的CRP作为聚类模型，而不是主题模型。", "提出了一种新的嵌入空间上的分层聚类方法，嵌入空间和分层聚类都是同时学习的。"]}
{"source": "We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep (convolutional) neural networks into adversarially robust classifiers. Class-conditional probability densities based on Bayesian nonparametric mixtures of factor analyzers (BNP-MFA) over the input space are used to design soft decision labels for feature to label isometry. Classconditional distributions over features are also learned using BNP-MFA to develop plug-in maximum a posterior (MAP) classifiers to replace the traditional multinomial logistic softmax classification layers. This novel unsupervised augmented framework, which we call geometrically robust networks (GRN), is applied to CIFAR-10, CIFAR-100, and to Radio-ML (a time series dataset for radio modulation recognition). We demonstrate the robustness of GRN models to adversarial attacks from fast gradient sign method, Carlini-Wagner, and projected gradient descent.", "target": ["我们为深度神经网络开发了一个统计-几何无监督学习增强框架，使其对对抗性攻击具有稳健性。", "利用GRN将传统的深度神经网络转化为对抗性的稳健分析器", "提出了一种基于类别条件特征分布的防御措施，将深度神经网络变成稳健的分类器。"]}
{"source": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "target": ["利用具有深度分层策略的结构化探索，使大型状态-动作空间中的深度强化学习更有效率。", "一种通过使用具有共享潜伏结构的策略来协调代理行为的方法，一种优化协调策略的变分策略优化方法，以及作者的变分性、层次性更新的推导。", "本文提出了一种由分层潜变量组成的算法创新，用于多代理环境下的协调探索。"]}
{"source": "Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.", "target": ["我们从理论上可行的线性案例中提供了许多关于神经网络泛化的见解。", "作者研究了一个简单的线性网络模型，以了解泛化和迁移学习。"]}
{"source": "We conduct a mathematical analysis on the Batch normalization (BN) effect on gradient backpropagation in residual network training in this work, which is believed to play a critical role in addressing the gradient vanishing/explosion problem. Specifically, by analyzing the mean and variance behavior of the input and the gradient in the forward and backward passes through the BN and residual branches, respectively, we show that they work together to confine the gradient variance to a certain range across residual blocks in backpropagation. As a result, the gradient vanishing/explosion problem is avoided. Furthermore, we use the same analysis to discuss the tradeoff between depth and width of a residual network and demonstrate that shallower yet wider resnets have stronger learning performance than deeper yet thinner resnets.", "target": ["批量归一化在整个训练过程中保持梯度方差，从而稳定了优化。", "本文分析了批量归一化对残差网络中梯度反向传播的影响"]}
{"source": "To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories. These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments. Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes. We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task.", "target": ["人类的行为判断被用来获得对象的稀疏和可解释的表征，这些表征可以推广到其他任务中去", "本文描述了一个关于人类对象/语义表征的大规模实验以及这种表征的模型。", "本文通过对从人类对图像进行的独特判断中收集的数据进行训练，开发了一个新的对象表征系统。", "一种新的方法来学习稀疏的、积极的、可解释的语义空间，通过训练来专门最大化预测人类的相似性判断。"]}
{"source": "We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering.    We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer.    The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks.  We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.", "target": ["我们提出了一个位于用户和黑盒问题回答系统之间的代理，它可以学习重新表述问题以获得最佳答案。", "本文提出了通过强化学习的方法来主动回答问题，学习以提供最佳答案的方式来重新表述问题。", "清楚地描述了研究人员如何设计并积极训练在回答问题的过程中重新表述问题和选择答案的两个模型"]}
{"source": "Most deep latent factor models choose simple priors for simplicity, tractability  or not knowing what prior to use. Recent studies show that the choice of  the prior may have a profound effect on the expressiveness of the model,  especially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders  (AAEs). We introduce the notion of code generators to transform manually selected  simple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than  AAEs in both supervised and unsupervised settings. Lastly, we present its  ability to do cross-domain translation in a  text-to-image synthesis task.", "target": ["为对抗性自动编码器学习优先权", "提出了用于条件性图像生成的对抗性自动编码器的简单扩展。", "专注于对抗性自动编码器，并介绍了代码生成器网络，将简单的先验转化为与生成器一起能更好地适应数据分布的先验。"]}
{"source": "In the past few years, various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks (GANs). GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer. In the field of Natural Language Processing, word embeddings such as word2vec and GLoVe are state-of-the-art methods for applying neural network models on textual data. Attempts have been made for utilizing GANs with word embeddings for text generation. This work presents an approach to text generation using Skip-Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures. The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used.", "target": ["在生成式对抗网络的帮助下，利用来自Skip-Thought矢量的句子嵌入生成文本。", "描述了生成对抗网络的应用，在ski-thought向量的帮助下对文本数据进行建模，并对两个不同的数据集进行了不同风格的GAN实验。"]}
{"source": "The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.   Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.   On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.", "target": ["介绍了一种在线的、无偏的、易于实现的循环模型梯度估计。", "作者介绍了一种从长序列中在线学习循环神经网络参数的新方法，该方法克服了通过时间来模仿截断反向传播的做法。", "本文以一种原则性的方式来处理RNN的在线训练，并提出了对RTRL的修改和使用前向方法来计算梯度。"]}
{"source": "We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level (high-resolution) labels, given the joint distribution between those low- and high-resolution labels. This method involves a novel loss function that minimizes the distance between a distribution determined by a set of model outputs and the corresponding distribution given by low-resolution labels over the same set of outputs. This setup does not require that the high-resolution classes match the low-resolution classes and can be used in high-resolution semantic segmentation tasks where high-resolution labeled data is not available. Furthermore, our proposed method is able to utilize both data with low-resolution labels and any available high-resolution labels, which we show improves performance compared to a network trained only with the same amount of high-resolution data.  We test our proposed algorithm in a challenging land cover mapping task to super-resolve labels at a 30m resolution to a separate set of labels at a 1m resolution. We compare our algorithm with models that are trained on high-resolution data and show that 1) we can achieve similar performance using only low-resolution data; and 2) we can achieve better performance when we incorporate a small amount of high-resolution data in our training. We also test our approach on a medical imaging problem, resolving low-resolution probability maps into high-resolution segmentation of lymphocytes with accuracy equal to that of fully supervised models.", "target": ["将粗略的标签超分辨为像素级标签，应用于航空图像和医学扫描。", "如果低分辨率和高分辨率标签的联合分布是已知的，则有一种方法可以超分辨粗略的低分辨率分割标签。"]}
{"source": "We propose a novel framework for combining datasets via alignment of their associated intrinsic dimensions. Our approach assumes that the two datasets are sampled from a common latent space, i.e., they measure equivalent systems. Thus, we expect there to exist a natural (albeit unknown) alignment of the data manifolds associated with the intrinsic geometry of these datasets, which are perturbed by measurement artifacts in the sampling process. Importantly, we do not assume any individual correspondence (partial or complete) between data points. Instead, we rely on our assumption that a subset of data features have correspondence across datasets. We leverage this assumption to estimate relations between intrinsic manifold dimensions, which are given by diffusion map coordinates over each of the datasets. We compute a correlation matrix between diffusion coordinates of the datasets by considering graph (or manifold) Fourier coefficients of corresponding data features. We then orthogonalize this correlation matrix to form an isometric transformation between the diffusion maps of the datasets. Finally, we apply this transformation to the diffusion coordinates and construct a unified diffusion geometry of the datasets together. We show that this approach successfully corrects misalignment artifacts, and allows for integrated data.", "target": ["我们提出了一种使用谐波相关的方法来调整从不同数据集学到的潜在特征。", "提出利用特征对应关系在同一样本的各批数据之间预先进行流形对齐，以避免收集到噪声测量结果。"]}
{"source": "Reinforcement learning (RL) has proven to be a powerful paradigm for deriving complex behaviors from simple reward signals in a wide range of environments. When applying RL to continuous control agents in simulated physics environments, the body is usually considered to be part of the environment. However, during evolution the physical body of biological organisms and their controlling brains are co-evolved, thus exploring a much larger space of actuator/controller configurations. Put differently, the intelligence does not reside only in the agent's mind, but also in the design of their body.   We propose a method for uncovering strong agents, consisting of a good combination of a body and policy, based on combining RL with an evolutionary procedure. Given the resulting agent, we also propose an approach for identifying the body changes that contributed the most to the agent performance. We use the Shapley value from cooperative game theory to find the fair contribution of individual components, taking into account synergies between components.   We evaluate our methods in an environment similar to the the recently proposed Robo-Sumo task, where agents in a 3D environment with simulated physics compete in tipping over their opponent or pushing them out of the arena. Our results show that the proposed methods are indeed capable of generating strong agents, significantly outperforming baselines that focus on optimizing the agent policy alone.    A video is available at: www.youtube.com/watch?v=eei6Rgom3YY", "target": ["在RL控制的代理中进化身体的形状可以提高其性能（并帮助学习）。", "纳入Shapley值的PEOM算法，通过识别每个身体部分的贡献来加速进化。"]}
{"source": "Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never. Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe. This score acts as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions. Our analysis also shows robustness to classification errors. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.", "target": ["塑造具有内在动机的奖励，以避免灾难性的状态，减轻灾难性的遗忘。", "一种RL算法，将DQN算法与平行训练的恐惧模型相结合，以预测灾难性的状态。", "本文研究了RL中的灾难性遗忘，通过强调DQN能够学习避免灾难性事件的任务，只要它能避免遗忘。"]}
{"source": "Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere S^2 or a unit ball B^3---entails unique challenges. In this work, we propose a novel `\"volumetric convolution\" operation that can effectively convolve arbitrary functions in B^3. We develop a theoretical framework for \"volumetric convolution\" based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks. Furthermore, our formulation leads to derivation of a  novel formula to measure the symmetry of a function in B^3 around an arbitrary axis, that is useful in 3D shape analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task.", "target": ["一种新的卷积算子用于单位球内的自动表征学习", "这项工作与最近的球形CNN和SE(n)等值网络论文有关，并将以前的想法扩展到单位球的体积数据。", "提出在卷积网络上使用体积卷积，以学习单元球，并讨论了方法和过程的结果。"]}
{"source": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent’s learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.", "target": ["我们使用奖励增强、课程学习和元学习来训练强化学习策略，以成功浏览网页。", "基于将一条指令分解为多个子指令的理念，开发了一种课程学习方法，用于训练RL代理导航网络。"]}
{"source": "Labeled text classification datasets are typically only available in a few select languages. In order to train a model for e.g news categorization in a language $L_t$ without a suitable text classification dataset there are two options. The first option is to create a new labeled dataset by hand, and the second option is to transfer label information from an existing labeled dataset in a source language $L_s$ to the target language $L_t$. In this paper we propose a method for sharing label information across languages by means of a language independent text encoder. The encoder will give almost identical representations to multilingual versions of the same text. This means that labeled data in one language can be used to train a classifier that works for the rest of the languages. The encoder is trained independently of any concrete classification task and can therefore subsequently be used for any classification task.   We show that it is possible to obtain good performance even in the case where only a comparable corpus of texts is available.", "target": ["通过通用编码进行跨语言文本分类", "本文提出了一种通过使用可比语料库进行跨语言文本分类的方法。", "学习跨语言嵌入并使用源语言中的标记数据训练分类器，以解决在目标语言中没有标记信息的情况下学习跨语言文本分类器的问题"]}
{"source": "Syntax is a powerful abstraction for language understanding. Many downstream tasks require segmenting input text into meaningful constituent chunks (e.g., noun phrases or entities); more generally, models for learning semantic representations of text benefit from integrating syntax in the form of parse trees (e.g., tree-LSTMs). Supervised parsers have traditionally been used to obtain these trees, but lately interest has increased in unsupervised methods that induce syntactic representations directly from unlabeled text. To this end, we propose the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Unlike many prior approaches, DIORA does not rely on supervision from auxiliary downstream tasks and is thus not constrained to particular domains. Furthermore, competing approaches do not learn explicit phrase representations along with tree structures, which limits their applicability to phrase-based tasks. Extensive experiments on unsupervised parsing, segmentation, and phrase clustering demonstrate the efficacy of our method. DIORA achieves the state of the art in unsupervised parsing (46.9 F1) on the benchmark WSJ dataset.", "target": ["在这项工作中，我们提出了深度内-外递归自动编码器（DIORA），这是一种完全无监督的发现语法的方法，同时为发现的成分学习表征。", "一个用自动编码目标训练的神经潜树模型，在无监督的成分分析方面达到了先进水平，并比其他潜树模型更好地捕捉了句法结构。", "本文提出了一个无监督依赖性解析（潜树归纳）的模型，该模型是基于内-外算法与神经建模（递归自动编码器）的结合。"]}
{"source": "Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training. There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled. But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training. We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias. We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons. We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area. We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes.", "target": ["我们研究了短跨度元优化目标的偏差。", "本文提出了一个简化的模型和问题来证明学习率元优化的短跨度偏差。", "本文通过对一个玩具问题的一些实验，研究了截断反推法的元优化问题"]}
{"source": "Mainstream captioning models often follow a sequential structure to generate cap-  tions, leading to issues such as introduction of irrelevant semantics, lack of diversity  in the generated captions, and inadequate generalization performance. In this paper,  we present an alternative paradigm for image captioning, which factorizes the  captioning procedure into two stages: (1) extracting an explicit semantic represen-  tation from the given image; and (2) constructing the caption based on a recursive  compositional procedure in a bottom-up manner. Compared to conventional ones,  our paradigm better preserves the semantic content through an explicit factorization  of semantics and syntax. By using the compositional generation procedure, caption  construction follows a recursive structure, which naturally fits the properties of  human language. Moreover, the proposed compositional procedure requires less  data to train, generalizes better, and yields more diverse captions.", "target": ["以分层和组合的方式生成标题", "本文提出了一种更易解释的图像标题方法。"]}
{"source": "While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data. Measures for characterizing and monitoring structural properties, however, have not been developed. In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs. To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization. Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss.", "target": ["我们为深度神经网络开发了一种新的拓扑复杂性测量方法，并证明它能捕捉到其突出的特性。", "本文提出了神经持久性的概念，这是一个拓扑学上的衡量标准，用于给神经网络中的全连接层分配分数。", "论文提出利用神经网络的第0持久同源性来分析其复杂性。"]}
{"source": "Deep neural networks (DNNs) are vulnerable to adversarial examples, which are carefully crafted instances aiming to cause prediction errors for DNNs. Recent research on adversarial examples has examined local neighborhoods in the input space of DNN models. However, previous work has limited what regions to consider, focusing either on low-dimensional subspaces or small balls. In this paper, we argue that information from larger neighborhoods, such as from more directions and from greater distances, will better characterize the relationship between adversarial examples and the DNN models. First, we introduce an attack, OPTMARGIN, which generates adversarial examples robust to small perturbations. These examples successfully evade a defense that only considers a small ball around an input instance. Second, we analyze a larger neighborhood around input instances by looking at properties of surrounding decision boundaries, namely the distances to the boundaries and the adjacent classes. We find that the boundaries around these adversarial examples do not resemble the boundaries around benign examples. Finally, we show that, under scrutiny of the surrounding decision boundaries, our OPTMARGIN examples do not convincingly mimic benign examples. Although our experiments are limited to a few specific attacks, we hope these findings will motivate new, more evasive attacks and ultimately, effective defenses.", "target": ["观察一个输入周围的决策边界，比固定的小邻域提供更多信息", "作者提出了一种新的攻击方法，用于生成对抗性例子，他们攻击通过随机分类L2小扰动创建的分类器", "一种对神经网络产生对抗性攻击的新方法，以及一种保护神经网络免受这些攻击的方法。"]}
{"source": "Machine learning models are usually tuned by nesting optimization of model weights inside the optimization of hyperparameters.   We give a method to collapse this nested optimization into joint stochastic optimization of both weights and hyperparameters.   Our method trains a neural network to output approximately optimal weights as a function of hyperparameters.   We show that our method converges to locally optimal weights and hyperparameters for sufficiently large hypernets.   We compare this method to standard hyperparameter optimization strategies and demonstrate its effectiveness for tuning thousands of hyperparameters.", "target": ["我们训练一个神经网络来输出近似最佳的权重，作为超参数的函数。", "用于神经网络中超参数优化的超网络。"]}
{"source": "Estimating covariances between financial assets plays an important role in risk management. In practice, when the sample size is small compared to the number of variables, the empirical estimate is known to be very unstable. Here, we propose a novel covariance estimator based on the Gaussian Process Latent Variable Model (GP-LVM). Our estimator can be considered as a non-linear extension of standard factor models with readily interpretable parameters reminiscent of market betas. Furthermore, our Bayesian treatment naturally shrinks the sample covariance matrix towards a more structured matrix given by the prior and thereby systematically reduces estimation errors. Finally, we discuss some financial applications of the GP-LVM model.", "target": ["用高斯过程潜变量模型对金融资产进行协方差矩阵估计", "说明高斯过程潜变量模型（GP-LVM）如何取代经典的线性因子模型来估计投资组合优化问题中的协方差矩阵。", "本文使用标准的GPLVMs对S&P500金融时间序列的协方差结构和潜在空间表示进行建模，以优化投资组合和预测缺失值。", "本文建议使用GPLVM来建立金融收益模型"]}
{"source": "We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution. In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator. As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables. In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training. We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients.", "target": ["我们引入了元对抗学习，这是一种使GAN正则化的新技术，并提出了一种通过明确控制判别器输出分布的训练方法。", "本文提出了用于训练GAN的方差正则化对抗学习，以确保生成器的梯度不会消失。"]}
{"source": "We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent’s policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights.   NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and epsilon-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.", "target": ["一个深度强化学习代理，在其权重上添加了参数化的噪音，可以用来帮助高效的探索。", "本文介绍了NoisyNets，即参数被参数化噪声函数扰动的神经网络，与基线深度强化学习算法相比，它获得了实质性的性能改进。", "通过向深度网络的权重注入噪声，为深度RL提供新的探索方法，噪声有多种形式"]}
{"source": "Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.", "target": ["Active Neural Localizer，一个完全可微的神经网络，利用深度强化学习有效地学习定位。", "本文将使用信念网络在已知地图上进行定位的问题表述为一个RL问题，其中代理的目标是最小化自我定位的步骤数。", "这是一篇清晰而有趣的论文，它建立了一个参数化的网络，在模拟环境中为机器人选择行动。"]}
{"source": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "target": ["我们开发了一种非自回归机器翻译模型的训练算法，实现了与强自回归基线相当的准确性，但在推理中的速度要快一个数量级。", "从中间的隐藏状态和注意权重中提炼知识，以改善非自回归神经机器翻译。", "建议利用训练有素的自回归模型，为非自回归神经机器翻译模型的隐藏状态和单词对齐提供信息。"]}
{"source": "Artificial neural networks are built on the basic operation of linear combination and non-linear activation function. Theoretically this structure can approximate any continuous function with three layer architecture. But in practice learning  the parameters of such network can be hard. Also the choice of activation function can greatly impact the performance of the network. In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non-linear activation function. To this end we are proposing the use of elementary  morphological operations (dilation and erosion) as the basic operation in neurons. We show that these networks (Denoted as Morph-Net) with morphological operations can approximate any smooth function requiring less number of parameters than what is necessary for normal neural networks. The results show that our network perform favorably when compared with similar structured network. We have carried out our experiments on  MNIST, Fashion-MNIST, CIFAR10 and CIFAR100.", "target": ["通过使用形态学操作（扩张和侵蚀），我们定义了一类网络，它可以近似任何连续的函数。", "本文提出用扩张和侵蚀操作的组合取代标准的RELU/tanh单元，观察到新的算子创造了更多的超平面，并具有更强的表现力。", "作者介绍了Morph-Net，这是一个单层的神经网络，其中的映射是使用形态学的扩张和侵蚀进行的。"]}
{"source": "With the rapidly scaling up of deep neural networks (DNNs), extensive research studies on network model compression such as weight pruning have been performed for efficient deployment. This work aims to advance the compression beyond the weights to the activations of DNNs. We propose the Integral Pruning (IP) technique which integrates the activation pruning with the weight pruning. Through the learning on the different importance of neuron responses and connections, the generated network, namely IPnet, balances the sparsity between activations and weights and therefore further improves execution efficiency. The feasibility and effectiveness of IPnet are thoroughly evaluated through various network models with different activation functions and on different datasets. With <0.5% disturbance on the testing accuracy, IPnet saves 71.1% ~ 96.35% of computation cost, compared to the original dense models with up to 5.8x and 10x reductions in activation and weight numbers, respectively.", "target": ["这项工作通过整合激活修剪和权重修剪，将DNN的压缩范围从权重扩大到激活。", "一种整体模型压缩方法，同时处理权重和激活修剪，实现更有效的网络计算，并有效减少乘积的数量。", "本文提出了一种新的方法，通过将激活修剪与权重修剪相结合来降低深度神经网络的计算成本，并表明排他性的权重修剪的常用技术在ReLU后增加了非零激活的数量。"]}
{"source": "The Variational Auto Encoder (VAE) is a popular generative  latent variable model that is often  applied for representation learning.  Standard VAEs assume continuous valued  latent variables and are trained by maximization of the evidence lower bound (ELBO). Conventional methods obtain a  differentiable estimate of the ELBO with reparametrized sampling and optimize it with Stochastic Gradient Descend (SGD). However, this is not possible if  we want to train VAEs with discrete valued latent variables,  since reparametrized sampling is not possible. Till now, there exist no simple solutions to circumvent this problem.  In this paper, we propose an easy method to train VAEs  with binary or categorically valued latent representations. Therefore, we use a differentiable estimator for the ELBO which is based on importance sampling. In experiments, we verify the approach and train two different VAEs architectures with Bernoulli and  Categorically distributed latent representations on two different benchmark datasets.", "target": ["我们提出了一种简单的方法来训练具有离散潜在表征的变分自动编码器（VAE），使用重要性采样", "引入一个重要性抽样分布，并使用分布中的样本来计算梯度的重要性加权估计值", "本文提出使用重要抽样来优化具有离散潜变量的VAE。"]}
{"source": "Distributed computing can significantly reduce the training time of neural networks. Despite its potential, however, distributed training has not been widely adopted: scaling the training process is difficult, and existing SGD methods require substantial tuning of hyperparameters and learning schedules to achieve sufficient accuracy when increasing the number of workers. In practice, such tuning can be prohibitively expensive given the huge number of potential hyperparameter configurations and the effort required to test each one.       We propose DANA, a novel approach that scales out-of-the-box to large clusters using the same hyperparameters and learning schedule optimized for training on a single worker, while maintaining similar final accuracy without additional overhead. DANA estimates the future value of model parameters by adapting Nesterov Accelerated Gradient to a distributed setting, and so mitigates the effect of gradient staleness, one of the main difficulties in scaling SGD to more workers.   Evaluation on three state-of-the-art network architectures and three datasets shows that DANA scales as well as or better than existing work without having to tune any hyperparameters or tweak the learning schedule. For example, DANA achieves 75.73% accuracy on ImageNet when training ResNet-50 with 16 workers, similar to the non-distributed baseline.", "target": ["一种新的分布式异步SGD算法，在现有架构上达到了最先进的精度，而没有任何额外的调整或开销。", "提出了在中等规模下对现有ASGD方法的改进，即在分布式工人池中使用带有SGD的动量进行异步训练。", "本文解决了分布式深度学习训练中的梯度滞后性与并行性能问题，并提出了一种在从站估计未来模型参数以减少通信延迟影响的方法。"]}
{"source": "This paper proposes a novel approach to train deep neural networks by unlocking the layer-wise dependency of backpropagation training. The approach employs additional modules called local critic networks besides the main network model to be trained, which are used to obtain error gradients without complete feedforward and backward propagation processes. We propose a cascaded learning strategy for these local networks. In addition, the approach is also useful from multi-model perspectives, including structural optimization of neural networks, computationally efficient progressive inference, and ensemble classification for performance improvement. Experimental results show the effectiveness of the proposed approach and suggest guidelines for determining appropriate algorithm parameters.", "target": ["我们提出了一种新的深度神经网络的学习算法，它解开了反向传播的层级依赖性。", "DNI的另一种训练范式，其中辅助模块的训练直接近似于原始模型的最终输出，提供附带的好处。", "描述了一种没有更新锁定的神经网络训练方法。"]}
{"source": "\\emph{Truncated Backpropagation Through Time} (truncated BPTT, \\cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of \\emph{Backpropagation Through Time} (BPTT \\cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.   However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce \\emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling \\cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.", "target": ["通过对截断长度的采样和相应的重新加权，提供一个无偏的截断反向传播的版本。", "提出了通过时间反向传播中截断点的随机确定方法。", "通过时间对反向传播的新近似，以克服在必须从长序列中学习时产生的计算和内存负荷。"]}
{"source": "Graph convolutional networks (GCNs) have been widely used for classifying graph nodes in the semi-supervised setting.  Previous works have shown that GCNs are vulnerable to the perturbation on adjacency and feature matrices of existing nodes. However, it is unrealistic to change the connections of  existing nodes in many applications, such as existing users in social networks. In this paper, we investigate methods attacking GCNs by adding fake nodes. A greedy algorithm is proposed to generate adjacency and feature matrices of fake nodes, aiming to minimize the classification accuracy on the existing ones. In additional, we introduce a discriminator to classify fake nodes from real nodes, and propose a Greedy-GAN algorithm to simultaneously update the discriminator and the attacker, to make fake nodes indistinguishable to the real ones.   Our non-targeted attack decreases the accuracy of GCN down to 0.10, and our targeted attack reaches a success rate of 0.99 for attacking the whole datasets, and 0.94 on average for attacking a single node.", "target": ["通过添加假节点对GCN进行非目标和目标攻击", "作者提出了一种新的对抗性技术，增加 \"假 \"节点来欺骗基于GCN的分类器"]}
{"source": "Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain. Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer. RNN uses a chain of repeating cells to model the sequence data. However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling. Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain.   In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer. ART is in a recurrent manner that different cells share the same parameters. Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain. This strategy enables ART to capture the word collocation across domains in a more flexible way. We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis). ART outperforms the state-of-the-arts over all experiments.", "target": ["通过学习使各领域的细胞级信息保持一致，进行序列的迁移学习。", "该论文提出使用RNN/LSTM与搭配对齐作为NLP中迁移学习/领域适应的表征学习方法。"]}
{"source": "Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world. We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution. Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state. Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers.", "target": ["我们将强化学习中的模型不确定性表述为一个连续的贝叶斯自适应马尔科夫决策过程，并提出了一种实用的、可扩展的贝叶斯策略优化方法。", "使用贝叶斯方法，在RL中探索和开发之间有一个更好的权衡。"]}
{"source": "For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic.  We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model. In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between distributions. The resulting benchmarks cannot be ``won'' by training set memorization, while still being perceptually correlated and computable only from samples. We survey past work on using NNDs for evaluation, implement an example black-box metric based on these ideas, and validate experimentally that it can measure a notion of generalization.", "target": ["我们认为，GAN基准必须要求从模型中获得大量样本以惩罚记忆，并研究神经网络分歧是否具有这种特性。", "作者提出了评价生成式对抗网络所产生的样本质量的标准。"]}
{"source": "Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.", "target": ["用对话行为生成开放领域的对话", "作者使用远距离监督技术，在开放领域的对话中加入对话行为标签，作为生成响应的条件因素", "该论文描述了一种将对话行为纳入神经对话代理的技术"]}
{"source": "We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.   We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity. A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique. The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.   Various predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping. These are verified extensively. In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.", "target": ["我们的假设是，给定两个领域，具有低差异的最低复杂度映射接近于目标映射。", "本文讨论了在没有任何监督的情况下学习不同领域之间的映射的问题，陈述了三个猜想。", "证明了在非对齐数据的无监督学习中，只用GAN学习域间映射是可能的，没有重建损失。"]}
{"source": "We present a novel approach for the certification of neural networks against adversarial perturbations which combines scalable overapproximation methods with precise (mixed integer) linear programming. This results in significantly better precision than state-of-the-art verifiers on challenging feedforward and convolutional neural networks with piecewise linear activation functions.", "target": ["我们用MILP求解器完善了不完全验证器的超近似结果，以证明比最先进的稳健性属性。", "介绍了一种验证器，利用超参数化、混合整数线性规划和线性规划松弛，获得了对不完整验证器的精度和完整验证器的可扩展性的改进。", "一种混合策略，在具有分段线性激活函数的前馈神经网络的稳健性验证上获得更好的精度，实现了比不完全验证器更好的精度和比完全验证器更大的可扩展性。"]}
{"source": "A distinct commonality between HMMs and RNNs is that they both learn hidden representations for sequential data. In addition, it has been noted that the backward computation of the Baum-Welch algorithm for HMMs is a special case of the back-propagation algorithm used for neural networks (Eisner (2016)). Do these observations suggest that, despite their many apparent differences, HMMs are a special case of RNNs? In this paper, we show that that is indeed the case, and investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization. In particular, we investigate three key design factors—independence assumptions between the hidden states and the observation, the placement of softmaxes, and the use of non-linearities—in order to pin down their empirical effects. We present a comprehensive empirical study to provide insights into the interplay between expressivity and interpretability in this model family with respect to language modeling and parts-of-speech induction.", "target": ["HMM是RNN的一个特例吗？我们通过理论推导和经验杂交研究了HMM和RNN之间的一系列架构转换，并提供了新的见解。", "本文探讨了HMM是否是RNN的一个特例，使用语言建模和POS标签"]}
{"source": "Deep neural networks have been tremendously successful in a number of tasks.  One of the main reasons for this is their capability to automatically  learn representations of data in levels of abstraction,  increasingly disentangling the data as the internal transformations are applied.  In this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.  This makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.  The proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.  Our approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.", "target": ["我们提出了一种新的正则化方法，对网络中隐藏层的维度之间的协方差进行惩罚。", "本文提出了一种正则化机制，该机制对神经网络潜在表征中所有维度之间的协方差进行惩罚，以便拆分潜在表征"]}
{"source": "This report introduces a training and recognition scheme, in which classification is realized via class-wise discerning. Trained with datasets whose labels are randomly shuffled except for one class of interest, a neural network learns class-wise parameter values, and remolds itself from a feature sorter into feature filters, each of which discerns objects belonging to one of the classes only. Classification of an input can be inferred from the maximum response of the filters. A multiple check with multiple versions of filters can diminish fluctuation and yields better performance. This scheme of discerning, maximum response and multiple check is a method of general viability to improve performance of feedforward networks, and the filter training itself is a promising feature abstraction procedure. In contrast to the direct sorting, the scheme mimics the classification process mediated by a series of one component picking.", "target": ["所提出的方案模拟了由一系列单组分拣选所调解的分类过程。", "一种提高深度网在多类分类任务上的准确性的方法，似乎是通过将多类分类还原为二元分类。", "辨别、最大响应和多重检查的新型分类程序，以提高平庸网络的准确性并增强前馈网络。"]}
{"source": "A long-held conventional wisdom states that larger models train more slowly when using gradient descent. This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step. In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.   We design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal. Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.", "target": ["经验表明，较大的模型在较少的训练步骤中进行训练，因为权重空间遍历中的所有因素都得到了改善。", "本文显示，当应用于NLP问题时，更宽的RNN提高了收敛速度，并延伸出增加深度神经网络中的宽度对优化收敛的影响。", "本文描述了过度参数化对算法收敛所需迭代次数的影响，并对神经网络训练中过度参数化的影响做了进一步的实证观察。"]}
{"source": "Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research. Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes. In this work, we consider a recently identified class of bugs called variable-misuse bugs. The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction. We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs. We present multi-headed pointer networks for this purpose, with one head each for localization and repair. The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.", "target": ["用于联合学习的多头指针网络，以定位和修复可变误用的bug", "提出了一个基于LSTM的模型，用指针将VarMisuse的问题分解成多个步骤。", "本文提出了一个基于LSTM的模型，用于检测和修复VarMisuse的错误，并在几个数据集上展示了与之前的方法相比的显著改进。"]}
{"source": "Classification and clustering have been studied separately in machine learning and computer vision. Inspired by the recent success of deep learning models in solving various vision problems (e.g., object recognition, semantic segmentation) and the fact that humans serve as the gold standard in assessing clustering algorithms, here, we advocate for a unified treatment of the two problems and suggest that hierarchical frameworks that progressively build complex patterns on top of the simpler ones (e.g., convolutional neural networks) offer a promising solution. We do not dwell much on the learning mechanisms in these frameworks as they are still a matter of debate, with respect to biological constraints. Instead, we emphasize on the compositionality of the real world structures and objects. In particular, we show that CNNs, trained end to end using back propagation with noisy labels, are able to cluster data points belonging to several overlapping shapes, and do so much better than the state of the art algorithms. The main takeaway lesson from our study is that mechanisms of human vision, particularly the hierarchal organization of the visual ventral stream should be taken into account in clustering algorithms (e.g., for learning representations in an unsupervised manner or with minimum supervision) to reach human level clustering performance. This, by no means, suggests that other methods do not hold merits. For example, methods relying on pairwise affinities (e.g., spectral clustering) have been very successful in many cases but still fail in some cases (e.g., overlapping clusters).", "target": ["用CNN进行类似人类的聚类", "该论文验证了深度卷积神经网络可以比其他聚类方法更好地学习对输入数据进行聚类的观点，指出由于视野大，它们有能力解释每个输入点的背景。", "这项工作将深度学习的特征表示与类似人类的无监督分组的任务结合起来。"]}
{"source": "Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features.   This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models. We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization.   In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models.   We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory. We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation.", "target": ["我们开发了两种线性复杂度算法，用于基于Shapley值的模型诊断模型解释，在这种情况下，特征对目标的贡献被图结构的因子化很好地概括了。", "本文提出了用于生成可解释性特征分数的Shapley值的两种近似方法。", "本文提出了两种使用Shapely值进行实例重要性评分的方法，并提供了两种在存在已知的特征相关结构时计算近似Shapely值的有效方法。"]}
{"source": "According to parallel distributed processing (PDP) theory in psychology, neural networks (NN) learn distributed rather than interpretable localist representations. This view has been held so strongly that few researchers have analysed single units to determine if this assumption is correct. However, recent results from psychology, neuroscience and computer science have shown the occasional existence of local codes emerging in artificial and biological neural networks. In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network, using generated input and output data with known qualities. We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data. Using a 1-hot output code drastically decreases the number of local codes on the hidden layer. The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks. This data suggests that localist coding can emerge from feed-forward PDP networks and suggests some of the conditions that may lead to interpretable localist representations in the cortex. The findings highlight how local codes should not be dismissed out of hand.", "target": ["在前馈神经网络中已经发现了本地代码", "一种确定MLP隐藏层中的单个神经元在多大程度上编码局部主义代码的方法，该方法针对不同的输入表征进行了研究。", "研究在前馈神经网络的隐蔽层中的本地主义表征的发展。"]}
{"source": "Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches however primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in relational databases, such as text, images, and numerical values. In our approach, we propose a multimodal embedding using different neural encoders for this variety of data, and combine with existing models to learn embeddings of the entities. We extend existing datasets to create two novel benchmarks, YAGO-10-plus and MovieLens-100k-plus, that contain additional relations such as textual descriptions and images of the original entities. We demonstrate that our model utilizes the additional information effectively to provide further gains in accuracy. Moreover, we test our learned multimodal embeddings by using them to predict missing multimodal attributes.", "target": ["使用神经编码器扩展关系模型以支持多模态数据。", "本文提出通过用多模态信息补充原始实体来进行知识库中的链接预测，并提出了一个能够在对三联体进行评分时编码各种信息的模型。", "本文是关于将来自不同模式的信息纳入链接预测方法的研究"]}
{"source": "An ensemble of neural networks is known to be more robust and accurate than an individual network, however usually with linearly-increased cost in both training and testing.   In this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.  In the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters. In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.  In this way of learning SSEs with SG-MCMC and pruning, we not only achieve high prediction accuracy since SG-MCMC enhances exploration of the model-parameter space, but also reduce memory and computation cost significantly in both training and testing of NN ensembles.  This is thoroughly evaluated in the experiments of learning SSE ensembles of both FNNs and LSTMs.  For example, in LSTM based language modeling (LM), we obtain 21\\% relative reduction in LM perplexity by learning a SSE of 4 large LSTM models, which has only 30\\% of model parameters and 70\\% of computations in total, as compared to the baseline large LSTM LM.  To the best of our knowledge, this work represents the first methodology and empirical study of integrating SG-MCMC, group sparse prior and network pruning together for learning NN ensembles.", "target": ["提出一种新的方法，通过整合SG-MCMC采样、分组稀疏先验和网络修剪来学习稀疏结构化集合（SSE），与传统方法相比，性能提高，成本大幅降低。", "作者提出了一个生成稀疏结构模型集合的程序", "一个用于训练集合神经网络的新框架，在深度学习中使用SG-MCMC方法，然后通过分组稀疏+修剪提高计算效率。", "本文探讨了使用FNN和LSTM来使贝叶斯模型平均化在计算上更加可行，并提高平均模型性能。"]}
{"source": "This paper introduces a new framework for data efficient and versatile learning. Specifically:  1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods.   2) We introduce \\Versa{}, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. \\Versa{} substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training.  3) We evaluate \\Versa{} on benchmark datasets where the method sets new state-of-the-art results, and can handle arbitrary number of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.", "target": ["新的元学习框架，统一并扩展了广泛的现有少样本学习方法。在不需要迭代测试时间推导的情况下，在少数学习基准上取得了强大的性能。", "这项工作从概率推理的角度解决了少许学习的问题，尽管比许多竞争者的设置更简单，但仍达到了最先进的水平。"]}
{"source": "In recent years, softmax together with its fast approximations has become the de-facto loss function for deep neural networks with multiclass predictions. However, softmax is used in many problems that do not fully fit the multiclass framework and where the softmax assumption of mutually exclusive outcomes can lead to biased results. This is often the case for applications such as language modeling, next event prediction and matrix factorization, where many of the potential outcomes are not mutually exclusive, but are more likely to be independent conditionally on the state. To this end, for the set of problems with positive and unlabeled data, we propose a relaxation of the original softmax formulation, where, given the observed state, each of the outcomes are conditionally independent but share a common set of negatives. Since we operate in a regime where explicit negatives are missing, we create an adversarially-trained model of negatives and derive a new negative sampling and weighting scheme which we denote as Cooperative Importance Sampling (CIS). We show empirically the advantages of our newly introduced negative sampling scheme by pluging it in the Word2Vec algorithm and benching it extensively against other negative sampling schemes on both language modeling and matrix factorization tasks and show large lifts in performance.", "target": ["为正向数据定义部分互斥的softmax损失并实施基于合作的采样方案", "本文提出了合作重要性抽样，以解决传统softmax的互斥假设在没有明确定义负样本时有偏差的问题。", "本文提出了PMES方法来放松softmax loss中的排他性结果假设，证明了在改进word2vec类型的嵌入模型方面的实证价值。"]}
{"source": "Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention. Current models for these tasks compute an encoding of the video by treating it as a sequence of images and going over every image in the sequence, which becomes computationally expensive for longer videos. In this paper, we focus on the task of video classification and aim to reduce the computational cost by using the idea of distillation. Specifically, we propose a Teacher-Student network wherein the teacher looks at all the frames in the video but the student looks at only a small fraction of the frames in the video. The idea is to then train the student to minimize  (i)  the difference between the final representation computed by the student and the teacher and/or (ii) the difference between the distributions predicted by the teacher and the student. This smaller student network which involves fewer computations but still learns to mimic the teacher can then be employed at inference time for video classification. We experiment with the YouTube-8M dataset and show  that the proposed student network can reduce the inference time by upto 30% with a negligent drop in the performance.", "target": ["使用较少帧的高效视频分类的师生框架", "本文提出了一个想法，从一个完整的视频分类模型中提炼出一个只接收较少帧数的小型模型。", "作者提出了一个解决视频分类问题的师生网络，提出了旨在降低计算成本的串行和并行训练算法。"]}
{"source": "Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transfered techniques.", "target": ["深度生成模型大类的统一统计视图", "本文建立了一个框架，将GAN算法解释为对生成模型进行一种变分推理，重构一个样本是否来自生成数据分布的真实的指标变量。"]}
{"source": "Deep neural networks have demonstrated promising prediction and classification performance on many healthcare applications. However, the interpretability of those models are often lacking. On the other hand, classical interpretable models such as rule lists or decision trees do not lead to the same level of accuracy as deep neural networks and can often be too complex to interpret (due to the potentially large depth of rule lists). In this work, we present PEARL,  Prototype lEArning via Rule Lists, which iteratively uses rule lists to guide a neural network to learn representative data prototypes. The resulting prototype neural network provides  accurate prediction, and the prediction can be easily explained by  prototype and its guiding rule lists. Thanks to the prediction power of neural networks, the rule lists from\t\t\t\t prototypes are more concise and hence provide better interpretability. On two real-world electronic healthcare records (EHR) datasets, PEARL consistently outperforms all baselines across both datasets, especially achieving performance improvement over conventional rule learning by up to 28% and over prototype learning by up to 3%. Experimental results also show the resulting interpretation of PEARL is  simpler than the standard rule learning.", "target": ["一种结合规则列表学习和原型学习的方法", "提出了一个新的可解释性预测框架，它结合了基于规则的学习、原型学习和NN，特别适用于纵向数据。", "本文旨在解决深度学习模型缺乏可解释性的问题，并提出了通过规则列表的原型学习（PEARL），它结合了规则学习和原型学习，以实现更准确的分类，并使可解释性的任务更加简单。"]}
{"source": "Generative Adversarial Networks (GANs) are powerful tools for realistic image generation. However, a major drawback of GANs is that they are especially hard to train, often requiring large amounts of data and long training time. In this paper we propose the Deli-Fisher GAN, a GAN that generates photo-realistic images by enforcing structure on the latent generative space using similar approaches in \\cite{deligan}. The structure of the latent space we consider in this paper is modeled as a mixture of Gaussians, whose parameters are learned in the training process. Furthermore, to improve stability and efficiency, we use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence. We show by experiments that the Deli-Fisher GAN performs better than DCGAN, WGAN, and the Fisher GAN as measured by inception score.", "target": ["本文提出了一种新的生成对抗网络，它比现有的生成对抗网络更稳定、更有效，并能产生更好的图像。", "本文结合了Fisher-GAN和Deli-GAN", "本文结合了Deli-GAN和Fisher GAN，前者在潜空间中具有混合先验分布，后者使用Fisher IPM而不是JSD作为目标。"]}
{"source": "Recent work on encoder-decoder models for sequence-to-sequence mapping has shown that integrating both temporal and spatial attentional mechanisms into neural networks increases the performance of the system substantially. We report on a new modular network architecture that applies an attentional mechanism not on temporal and spatial regions of the input, but on sensor selection for multi-sensor setups. This network called the sensor transformation attention network (STAN) is evaluated in scenarios which include the presence of natural noise or synthetic dynamic noise. We demonstrate how the attentional signal responds dynamically to changing noise levels and sensor-specific noise, leading to reduced word error rates (WERs) on both audio and visual tasks using TIDIGITS and GRID; and also on CHiME-3, a multi-microphone real-world noisy dataset. The improvement grows as more channels are corrupted as demonstrated on the CHiME-3 dataset. Moreover, the proposed STAN architecture naturally introduces a number of advantages including ease of removing sensors from existing architectures, attentional interpretability, and increased robustness to a variety of noise environments.", "target": ["我们引入了一个带有注意力机制的模块化多传感器网络架构，它能够在CHiME-3的真实世界的噪声数据上进行动态传感器选择。", "一个通用的神经结构能够学习根据每个传感器的相对质量而必须对不同的输入通道给予的注意力。", "考虑使用注意力来选择传感器或通道，在TIDIGITS和GRID上的结果显示了注意力比特征串联的好处。"]}
{"source": "Massive data exist among user local platforms that usually cannot support deep neural network (DNN) training due to computation and storage resource constraints. Cloud-based training schemes provide beneficial services but suffer from potential privacy risks due to excessive user data collection. To enable cloud-based DNN training while protecting the data privacy simultaneously, we propose to leverage the intermediate representations of the data, which is achieved by splitting the DNNs and deploying them separately onto local platforms and the cloud. The local neural network (NN) is used to generate the feature representations. To avoid local training and protect data privacy, the local NN is derived from pre-trained NNs. The cloud NN is then trained based on the extracted intermediate representations for the target learning task. We validate the idea of DNN splitting by characterizing the dependency of privacy loss and classification accuracy on the local NN topology for a convolutional NN (CNN) based image classification task. Based on the characterization, we further propose PrivyNet to determine the local NN topology, which optimizes the accuracy of the target learning task under the constraints on privacy loss, local computation, and storage. The efficiency and effectiveness of PrivyNet are demonstrated with CIFAR-10 dataset.", "target": ["为了实现基于云的DNN训练，同时保护数据隐私，我们建议利用中间数据表征，这是通过拆分DNN并将其分别部署到本地平台和云端来实现的。", "本文提出了一种通过学习特征表示来实现数据私有化的技术，这种技术很难用于图像重建，但对图像分类有帮助。"]}
{"source": "Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from ‘serialised’ MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.", "target": ["用于实值医疗序列生成的条件循环GAN，显示了新的评估方法和实证性的隐私分析。", "建议使用GAN生成的合成数据来替代个人身份识别数据，用于训练对隐私敏感的应用的ML模型", "作者提出了一种新型的循环GAN架构，可以生成连续的领域序列，并在几个合成任务和一个ICU时间序列数据任务上对其进行了评估。", "提议使用RGAN和RCGAN来生成实际数据的合成序列。"]}
{"source": "Emphasis effects – visual changes that make certain elements more  prominent – are commonly used in information visualization to draw  the user’s attention or to indicate importance. Although theoretical  frameworks of emphasis exist (that link visually diverse emphasis  effects through the idea of visual prominence compared to background  elements), most metrics for predicting how emphasis effects  will be perceived by users come from abstract models of human  vision which may not apply to visualization design. In particular,  it is difficult for designers to know, when designing a visualization,  how different emphasis effects will compare and what level of one  effect is equivalent to what level of another. To address this gap,  we carried out two studies that provide empirical evidence about  how users perceive different emphasis effects, using three visual  variables (colour, size, and blur/focus) and eight strength levels.  Results from gaze tracking, mouse clicks, and subjective responses  show that there are significant differences between visual variables  and between levels, and allow us to develop an initial understanding  of perceptual equivalence. We developed a model from the data in  our first study, and used it to predict the results in the second; the  model was accurate, with high correlations between predictions and  real values. Our studies and empirical models provide valuable new  information for designers who want to understand and control how  emphasis effects will be perceived by users.", "target": ["我们的研究和实证模型为那些希望了解和控制强调效果如何被用户感知的设计者提供了宝贵的新信息。", "本文考虑了在数据可视化中哪种视觉高亮被认为是更快的，以及不同的高亮方法如何相互比较", "两项关于强调效果的研究，一项是评估有用的差异水平，一项是使用实际不同的可视化的更多应用，以进行更生态有效的调查。"]}
{"source": "Memory Network based models have shown a remarkable progress on the task of relational reasoning.  Recently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced.   Despite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.  We introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures.   We follow memory network's four components while each component operates similar to the relation network without taking a pair of objects.   As a result, our model is as simple as RN but the computational complexity is reduced to linear time.  It achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset.", "target": ["基于记忆网络（MemNN）和关系网络（RN）的简单推理架构，与RN相比，降低了时间复杂度，在基于bAbI故事的QA和bAbI对话上取得了最先进的成果。", "介绍了相关记忆网络（RRMN），这是关系网络（RN）的改进。"]}
{"source": "We investigate in this paper the architecture of deep convolutional networks. Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN. We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance. The use of branches brings an additional form of regularization. In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities. The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently. We refer to this branched architecture as \"coupled ensembles\". The approach is very generic and can be applied with almost any neural network architecture. With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks. For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.   With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.", "target": ["我们表明，将一个神经网络分割成平行分支可以提高性能，而分支的适当耦合可以进一步提高性能。", "这项工作提出了一个使用新的分支结构对现有的最先进的CNN模型进行重新配置，具有更好的性能。", "本文展示了耦合合集的参数节约优势。", "提出了一个深度网络架构，该架构使用多个平行分支处理数据，并结合这些分支的后验结果来计算最终得分。"]}
{"source": "Convolutional Neural Networks (CNN) are very popular in many fields including computer vision, speech recognition, natural language processing, to name a few. Though deep learning leads to groundbreaking performance in these domains, the networks used are very demanding computationally and are far from real-time even on a GPU, which is not power efficient and therefore does not suit low power systems such as mobile devices. To overcome this challenge, some solutions have been proposed for quantizing the weights and activations of these networks, which accelerate the runtime significantly. Yet, this acceleration comes at the cost of a larger error. The NICE method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy. This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and 3 -bit activations. We implement the proposed solution on an FPGA to demonstrate its applicability for low power real-time applications.", "target": ["结合噪声注入、渐进式量化和激活钳制学习，实现最先进的3、4和5位量化", "建议在训练过程中注入噪声，并在神经网络量化中钳制某一层的参数值以及激活输出。", "一种用于分类和回归的深度神经网络的量化方法，使用噪声注入、用学习到的最大激活量进行箝制，以及渐进式块状量化，使其性能与最先进的方法相当或更好。"]}
{"source": "In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. Approaches that transfer information contained only in the final parameters of a source model will therefore struggle. Instead, transfer learning at at higher level of abstraction is needed. We propose Leap, a framework that achieves this by transferring knowledge across learning processes. We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path. Our framework leverages only information obtained during training and can be computed on the fly at negligible cost. We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks. Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps.", "target": ["我们提出了Leap，一个通过最小化训练过程在任务损失表面上的预期距离来转移学习过程中的知识的框架。", "文章提出了一个新的元学习目标，旨在处理表现出大量任务间多样性的任务集合时，超越最先进的方法。"]}
{"source": "Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones. DANs preserve performance on the original task, require a fraction (typically 13%) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance. When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.", "target": ["一种替代迁移学习的方法，学习速度更快，需要的参数少得多（3-13%），通常能取得更好的结果，并且精确地保留了旧任务的性能。", "用于图像分类数据集的增量学习的控制器模块"]}
{"source": "High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including 1) channel-wise scale factors of weights, especially for depthwise convolution, 2) Winograd convolution, and 3) topology-wise 8-bit support. We experiment the techniques on top of a widely-used deep learning framework. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution. We show that the inference throughput  and latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline. We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks. All the code and models will be publicly available soon.", "target": ["我们提出了一种对卷积神经网络进行8位低精度推理的通用技术。", "本文设计了一个系统来自动量化CNN预训练的模型"]}
{"source": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "target": ["我们建议加入来自双曲几何的归纳偏差和操作，以改善神经网络的注意力机制。", "本文用负双曲线距离取代了注意力机制中使用的点积相似性，并将其应用于现有的Transformer模型、图形注意力网络和关系网络中。", "作者提出了一种新的方法，通过改变匹配和聚合函数，使用双曲线几何来改善关系-注意力。"]}
{"source": "We present a method for evaluating the sensitivity of deep reinforcement learning (RL) policies. We also formulate a zero-sum dynamic game for designing robust deep reinforcement learning policies. Our approach mitigates the brittleness of policies when agents are trained in a simulated environment and are later exposed to the real world where it is hazardous to employ RL policies. This framework for training deep RL policies involve a zero-sum  dynamic game against an adversarial agent, where the goal is to drive the system dynamics to a saddle region. Using a variant of the guided policy search algorithm, our agent learns to adopt robust policies that require less samples for learning the dynamics and performs better than the GPS algorithm. Without loss of generality, we demonstrate that deep RL policies trained in this fashion will be maximally robust to a ``worst\" possible adversarial disturbances.", "target": ["本文展示了H-无穷大控制理论如何帮助更好地为机器人电机设计鲁稳健的深度策略。", "建议将稳健控制的要素纳入指导性策略研究，以便设计出一种对扰动和模型不匹配具有弹性的方法。", "本文提出了一种评估深度RL策略的敏感性和稳健性的方法，并提出了一种学习稳健策略的动态博弈方法。"]}
{"source": "Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models). We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved. Under such conditions, we prove the existence of small universal perturbations. Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.", "target": ["分析分类器对普遍扰动的脆弱性以及与决策边界曲率的关系。", "本文提供了一个有趣的分析，将分类器决策边界的几何形状与小型通用对抗性扰动联系起来。", "本文讨论了通用扰动--如果将其添加到大多数输入数据点中，就会误导经过训练的分类器。", "本文开发了一些模型，试图解释愚弄神经网络的普遍扰动的存在。"]}
{"source": "Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.", "target": ["我们提出了一种元学习方法，用于用自然语言互动地纠正策略。", "本文提供了一个元学习框架，展示了如何在互动设置中学习新的任务。每个任务都是通过强化学习设置学习的，然后通过观察新的指令来更新任务。", "本文通过自然语言指令，在一个反复的过程中教代理完成任务。"]}
{"source": "Deep generative models such as Generative Adversarial Networks (GANs) and  Variational Auto-Encoders (VAEs) are important tools to capture and investigate  the properties of complex empirical data. However, the complexity of their inner  elements makes their functionment challenging to assess and modify. In this  respect, these architectures behave as black box models. In order to better  understand the function of such networks, we analyze their modularity based on  the counterfactual manipulation of their internal variables. Our experiments on the  generation of human faces with VAEs and GANs support that modularity between  activation maps distributed over channels of generator architectures is achieved  to some degree, can be used to better understand how these systems operate and allow meaningful transformations of the generated images without further training.  erate and edit the content of generated images.", "target": ["我们研究了深度生成模型的模块化。", "本文提供了一种研究深度生成模型的模块化结构的方法，其关键概念是分布在生成器架构的通道上。"]}
{"source": "Relational databases store a significant amount of the worlds data. However, accessing this data currently requires users to understand a query language such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model uses rewards from in the loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss. Moreover, Seq2SQL leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem. In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables fromWikipedia that is an order of magnitude larger than comparable datasets. By applying policy based reinforcement learning with a query execution environment to WikiSQL, Seq2SQL outperforms a state-of-the-art semantic parser, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.", "target": ["我们介绍了Seq2SQL，它利用在线查询执行的奖励将问题翻译成SQL查询，以及WikiSQL，一个比现有数据集大几个数量级的SQL表/问题/查询数据集。", "一个新的语义解析数据集，它侧重于使用基于强化学习的模型从自然语言中生成SQL。"]}
{"source": "We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks. We find that the implicit generative modeling of random noise with the same loss function used during posterior maximization, improves a model's understanding of the data manifold furthering adversarial robustness. We prove our approach's efficacy and provide a simplistic visualization tool for understanding adversarial data, using Principal Component Analysis. Our analysis reveals that adversarial robustness, in general, manifests in models with higher variance along the high-ranked principal components. We show that models learnt with our approach perform remarkably well against a wide-range of attacks. Furthermore, combining ExL with state-of-the-art adversarial training extends the robustness of a model, even beyond what it is adversarially trained for, in both white-box and black-box attack scenarios.", "target": ["鉴别性训练过程中的输入噪声建模提高了对抗性稳健性。提出基于PCA的对抗性稳健性评价指标", "本文提出了ExL，一种使用乘法噪声的对抗性训练方法，该方法被证明有助于防御三个数据集的黑箱攻击。", "本文在训练数据中包括乘法噪声N，以实现对抗性的稳健性，当对模型参数theta和噪声本身进行训练时。"]}
{"source": "We propose a method which can visually explain the classification decision of deep neural networks (DNNs). There are many proposed methods in machine learning and computer vision seeking to clarify the decision of machine learning black boxes, specifically DNNs.   All of these methods try to gain insight into why the network \"chose class A\" as an answer. Humans, when searching for explanations, ask two types of questions. The first question is, \"Why did you choose this answer? \" The second question asks, \"Why did you not choose answer B over A?\" The previously proposed methods are either not able to provide the latter directly or efficiently.   We introduce a method capable of answering the second question both directly and efficiently. In this work, we limit the inputs to be images. In general, the proposed method generates explanations in the input space of any model capable of efficient evaluation and gradient evaluation. We provide results, showing the superiority of this approach for gaining insight into the inner representation of machine learning models.", "target": ["回答 \"为什么不是B类？\"的方法来解释深度网络", "本文提出了一种为深度神经网络提供对比性视觉解释的方法。"]}
{"source": "We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping. We provide theoretical and numerical results on the inverse of ReLU-layers. First, we derive a necessary and sufficient condition on the existence of invariance that provides a geometric interpretation. Next, we move to robustness via analyzing local effects on the inverse. To conclude, we show how this reverse point of view not only provides insights into key effects, but also enables to view adversarial examples from different perspectives.", "target": ["我们通过研究ReLU-层的预成像和逆的稳定性来分析深度神经网络的可逆性。", "本文研究了ReLU网络在某一层激活的预成像量，它建立在ReLU网络的前向函数的分段线性上。", "本文对ReLU网络的逆不变性进行了分析，并提供了train网络的奇异值的上限。"]}
{"source": "While deep learning has led to remarkable results on a number of challenging problems, researchers have discovered a vulnerability of neural networks in adversarial settings, where small but carefully chosen perturbations to the input can make the models produce extremely inaccurate outputs. This makes these models particularly unsuitable for safety-critical application domains (e.g. self-driving cars) where robustness is extremely important. Recent work has shown that augmenting training with adversarially generated data provides some degree of robustness against test-time attacks. In this paper we investigate how this approach scales as we increase the computational budget given to the defender. We show that increasing the number of parameters in adversarially-trained models increases their robustness, and in particular that ensembling smaller models while adversarially training the entire ensemble as a single model is a more efficient way of spending said budget than simply using a larger single model. Crucially, we show that it is the adversarial training of the ensemble, rather than the ensembling of adversarially trained models, which provides robustness.", "target": ["集合体的对抗性训练提供了对对抗性例子的稳健性，超过了在对抗性训练的模型和独立训练的集合体中观察到的。", "建议联合训练一个集合模型，在每个时间步骤中，将一组对集合模型本身具有对抗性的例子纳入学习。"]}
{"source": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network – for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR100 (20 tasks) we obtain cross-stitch performance levels with an 85% average reduction in training time.", "target": ["路由网络：一种新的神经网络，它可以为多任务学习学习自适应地确定其输入路线", "该文件建议使用一个带有控制器的模块化网络，该控制器在每个时间步骤中对下一个应用的结核作出决定。", "本文提出了一种在多任务学习框架下，通过使用多代理强化学习寻找策略来学习神经网络的最佳架构的新公式，并显示出比共享层的硬编码架构的改进。"]}
{"source": "We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization. However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ regularized objective is differentiable with respect to the distribution parameters. We further propose the \\emph{hard concrete} distribution for the gates, which is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.", "target": ["我们展示了如何用梯度下降法优化参数模型的预期L_0范数，并介绍了一种有利于硬门控的新分布。", "作者介绍了一种基于梯度的方法来最小化带有L0稀疏惩罚的目标函数，以帮助学习稀疏神经网络"]}
{"source": "Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.", "target": ["我们提出了一种新型的基于注意力的可解释图神经网络架构，在标准基准数据集中的表现优于目前最先进的图神经网络。", "作者提出了GCN的两个扩展，即从GCN计算中去除中间的非线性，并在聚集层中增加一个注意力机制。", "本文提出了一种半监督的图节点分类学习算法，其灵感来自于图神经网络。"]}
{"source": "Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimensionality of data can be much lower than the ambient dimensionality. We argue that this discrepancy may contribute to the difficulties in training generative models. We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space. The resulting method, perceptual generative autoencoder (PGA), is then incorporated with maximum likelihood or variational autoencoder (VAE) objective to train the generative model. With maximum likelihood, PGA generalizes the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities. When combined with VAE, PGA can generate sharper samples than vanilla VAE.", "target": ["基于自动编码器的生成模型训练框架，具有非对抗性损失和非限制性神经网络架构。", "本文使用自动编码器来做高维空间的分布匹配。"]}
{"source": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.  Euclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.  We address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.  We introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.  Empirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.  We discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.  On a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6  points in Spearman rank correlation on similarity tasks  and 3.4 points on analogy accuracy.", "target": ["与各种结构的传统嵌入空间相比，具有异质曲率的积流形嵌入空间产生了更好的表述。", "提出了一种降维方法，将数据嵌入球形、欧几里得和双曲流形的积流形中。该算法的基础是将积流形上的测地距离与图的距离相匹配。"]}
{"source": "Synthesizing user-intended programs from a small number of input-output exam-  ples is a challenging problem with several important applications like spreadsheet  manipulation, data wrangling and code refactoring. Existing synthesis systems  either completely rely on deductive logic techniques that are extensively hand-  engineered or on purely statistical models that need massive amounts of data, and in  general fail to provide real-time synthesis on challenging benchmarks. In this work,  we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique  that combines the best of both symbolic logic techniques and statistical models.  Thus, it produces programs that satisfy the provided specifications by construction  and generalize well on unseen examples, similar to data-driven systems. Our  technique effectively utilizes the deductive search framework to reduce the learning  problem of the neural component to a simple supervised learning setup. Further,  this allows us to both train on sparingly available real-world data and still leverage  powerful recurrent neural network encoders. We demonstrate the effectiveness  of our method by evaluating on real-world customer scenarios by synthesizing  accurate programs with up to 12× speed-up compared to state-of-the-art systems.", "target": ["我们整合了符号（演绎）和统计（基于神经）方法，以实现实时程序合成，并从1个输入输出的例子中得到几乎完美的泛化。", "本文提出了一种分支和边界方法来学习好的程序，其中LSTM被用来预测搜索树中的哪些分支应该导致好的程序。", "提出了从一个例子中合成程序的系统，其泛化优于先前的最先进技术。"]}
{"source": "Variational auto-encoders  (VAEs) offer a tractable approach when performing approximate inference in otherwise intractable generative models. However, standard VAEs often produce latent codes that are disperse and lack interpretability, thus making the resulting representations unsuitable for auxiliary tasks (e.g. classiﬁcation) and human interpretation. We address these issues by merging ideas from variational auto-encoders and sparse coding, and propose to explicitly model sparsity in the latent space of a VAE with a Spike and Slab prior distribution. We derive the evidence lower bound using a discrete mixture recognition function thereby making approximate posterior inference as computational efﬁcient as in the standard VAE case. With the new approach, we are able to infer truly sparse representations with generally intractable non-linear probabilistic models. We show that these sparse representations are advantageous over standard VAE representations on two benchmark classiﬁcation tasks (MNIST and Fashion-MNIST) by demonstrating improved classiﬁcation accuracy and signiﬁcantly increased robustness to the number of latent dimensions. Furthermore, we demonstrate qualitatively that the sparse elements capture subjectively understandable sources of variation.", "target": ["我们探索VAE和稀疏编码的交叉点。", "本文提出了一个具有稀疏先验和后验的VAEs的扩展，以学习稀疏的可解释表征。"]}
{"source": "A widely observed phenomenon in deep learning is the degradation problem: increasing  the depth of a network leads to a decrease in performance on both test and training data. Novel architectures such as ResNets and Highway networks have addressed this issue by introducing various flavors of skip-connections or gating mechanisms. However, the degradation problem persists in the context of plain feed-forward networks. In this work we propose a simple method to address this issue. The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers. This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner. We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to greatly decrease the degradation effect (compared to plain networks) and is often competitive with ResNets.", "target": ["以一种有原则的方式逐步淘汰跳过连接，避免了深度前馈网络的退化。", "作者提出了一种新的训练策略，即VAN，用于训练没有跳过连接的非常深的前馈网络。", "本文介绍了一种在ResNets和vanilla深网之间线性插值的架构，而无需跳过连接。"]}
{"source": "Deep learning is becoming more widespread in its application due to its power in solving complex classification problems. However, deep learning models often require large memory and energy consumption, which may prevent them from being deployed effectively on embedded platforms, limiting their applications. This work addresses the problem by proposing methods {\\em Weight Reduction Quantisation} for compressing the memory footprint of the models, including reducing the number of weights and the number of bits to store each weight. Beside, applying with sparsity-inducing regularization, our work focuses on speeding up stochastic variance reduced gradients (SVRG) optimization on non-convex problem. Our method that mini-batch SVRG with $\\ell$1 regularization on non-convex problem has faster and smoother convergence rates than SGD by using adaptive learning rates. Experimental evaluation of our approach uses MNIST and CIFAR-10 datasets on LeNet-300-100 and LeNet-5 models, showing our approach can reduce the memory requirements both in the convolutional and fully connected layers by up to 60$\\times$ without affecting their test accuracy.", "target": ["部署在嵌入式设备上的深度神经网络的压缩。", "作者提出了一种基于l-1正则化的SVRG训练算法，能够强迫网络的许多权重为0。", "这项工作降低了对内存的要求。"]}
{"source": "It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment. For this purpose, it builds a model of the world around us and uses this model to infer the external stimulus. Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment. However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain. In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment. Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm. To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training. Despite the novel features of these objects the model is able to infer the latent representations for them. Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects.", "target": ["一种基于预测编码的学习算法，用于构建大脑的深度神经网络模型", "本文考虑了使用预测编码设置的生成性神经网络的学习。"]}
{"source": "In this paper, we propose deep convolutional generative adversarial networks (DCGAN) that learn to produce a 'mental image' of the input image as internal representation of a certain category of input data distribution.   This mental image is what the DCGAN 'imagines' that the input image might look like under ideal conditions.   The mental image contains a version of the input that is iconic, without any peculiarities that do not contribute to the ideal representation of the input data distribution within a category. A DCGAN learns this association by training an encoder to capture salient features from the original image and a decoder to convert salient features into its associated mental image representation.   Our new approach, which we refer to as a Mental Image DCGAN (MIDCGAN), learns features that are useful for recognizing entire classes of objects, and that this in turn has the benefit of helping single and zero shot recognition.   We demonstrate our approach on object instance recognition and handwritten digit recognition tasks.", "target": ["用对抗性自动编码器进行对象实例识别时，使用了一个新的 \"心理图像 \"目标，它是输入图像的典范表征。", "本文提出了一种学习物体识别特征的方法，该方法对物体的各种变换是不变的，最主要的是物体的姿势。", "本文研究了在给定输入图像的情况下，通过生成的心理图像作为中间表征的少样本识别任务。"]}
{"source": "An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large number of interactions with the environment in order to master a skill. The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task. We present a framework that combines techniques in \\textit{formal methods} with \\textit{hierarchical reinforcement learning} (HRL). The set of techniques we provide allows for the convenient specification of tasks with logical expressions, learns hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards using any RL methods and is able to construct new skills from existing ones without additional learning. We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.", "target": ["将时间逻辑与分层强化学习结合起来进行技能组合", "本文提供了一种从原始MDP和与LTL公式相关的自动机中构建积MDP的策略。", "提议将时间逻辑与分层强化学习结合起来，以简化技能构成。"]}
{"source": "The tremendous memory and computational complexity of Convolutional Neural Networks (CNNs) prevents the inference deployment on resource-constrained systems. As a result, recent research focused on CNN optimization techniques, in particular quantization, which allows weights and activations of layers to be represented with just a few bits while achieving impressive prediction performance. However, aggressive quantization techniques still fail to achieve full-precision prediction performance on state-of-the-art CNN architectures on large-scale classification tasks. In this work we propose a method for weight and activation quantization that is scalable in terms of quantization levels (n-ary representations) and easy to compute while maintaining the performance close to full-precision CNNs. Our weight quantization scheme is based on trainable scaling factors and a nested-means clustering strategy which is robust to weight updates and therefore exhibits good convergence properties. The flexibility of nested-means clustering enables exploration of various n-ary weight representations with the potential of high parameter compression. For activations, we propose a linear quantization strategy that takes the statistical properties of batch normalization into account. We demonstrate the effectiveness of our approach using state-of-the-art models on ImageNet.", "target": ["我们为深度神经网络的权重和激活提出了一个量化方案。这大大减少了内存占用并加速了推理。", "CNN模型压缩和使用量化的推理加速。"]}
{"source": "Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.", "target": ["当机器人被部署在一个人类一直在行动的环境中时，环境的状态已经被优化为人类想要的东西，我们可以利用这一点来推断出人类的偏好。", "作者建议用从初始状态和状态动态模型推断出的辅助奖励/成本来增强RL代理的明确说明的奖励函数。", "这项工作提出了一种利用IRL推断初始状态下的隐含信息，并将推断出的奖励与指定的奖励相结合的方法。"]}
{"source": "Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other. In our work we present a novel, systematic, unifying taxonomy to categorize existing methods. We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures. We identify the atomic building blocks of existing methods, and decouple the assumptions they enforce from the mathematical tools they rely on. We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories. This helps revealing links and fundamental similarities between them. Finally, we include practical recommendations both for users and for developers of new regularization methods.", "target": ["对深度学习的正则化方法进行系统分类，揭示其相似性。", "试图为深度学习中采用的正则化技术建立一个分类法。"]}
{"source": "Deep neural networks are surprisingly efficient at solving practical tasks,  but the theory behind this phenomenon is only starting to catch up with  the practice. Numerous works show that depth is the key to this efficiency.  A certain class of deep convolutional networks – namely those that correspond  to the Hierarchical Tucker (HT) tensor decomposition – has been  proven to have exponentially higher expressive power than shallow networks.  I.e. a shallow network of exponential width is required to realize  the same score function as computed by the deep architecture. In this paper,  we prove the expressive power theorem (an exponential lower bound on  the width of the equivalent shallow network) for a class of recurrent neural  networks – ones that correspond to the Tensor Train (TT) decomposition.  This means that even processing an image patch by patch with an RNN  can be exponentially more efficient than a (shallow) convolutional network  with one hidden layer. Using theoretical results on the relation between  the tensor decompositions we compare expressive powers of the HT- and  TT-Networks. We also implement the recurrent TT-Networks and provide  numerical evidence of their expressivity.", "target": ["我们证明了循环型神经网络超越浅层网络的指数效率。", "作者比较了张量训练网络与通过CP分解结构的网络的复杂性"]}
{"source": "Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN). In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior. Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference. Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution. Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs.", "target": ["具有理论保证的GAN的新型概率处理方法。", "本文提出了一种贝叶斯GAN，它在理论上保证了对真实分布的收敛性，并把似然放在生成器和判别器上，其对数与传统GAN目标函数成正比。"]}
{"source": "In the adversarial-perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\\bfx$ that $F$ classifies correctly, and applies a \\emph{small perturbation} to  $\\bfx$ to produce another point $\\bfx'$ that $F$ classifies \\emph{incorrectly}.  In this paper, we propose taking into account \\emph{the inherent confidence information} produced by models when studying adversarial perturbations, where a natural measure of ``confidence'' is \\|F(\\bfx)\\|_\\infty$ (i.e. how confident $F$ is about its prediction?) . Motivated by a thought experiment based on the manifold assumption, we propose a ``goodness property'' of models which states that \\emph{confident regions of a good model should be well separated}. We give formalizations of this property and examine existing robust training objectives in view of them. Interestingly, we find that a recent objective by Madry et al. encourages training a model that satisfies well our formal version of the goodness property, but has a weak control of points that are wrong but with low confidence. However, if Madry et al.'s model is indeed a good solution to their objective, then good and bad points are now distinguishable and we can try to embed uncertain points back to the closest confident region to get (hopefully) correct predictions. We thus propose embedding objectives and algorithms, and perform an empirical study using this method. Our experimental results are encouraging: Madry et al.'s model wrapped with our embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining good generalization behavior.", "target": ["抵御来自manifold假设的神经网络的对抗性扰动", "该手稿提出了两个基于manifold假设的目标函数，作为对对抗性例子的防御机制。", "防御基于自然数据的manifold假设的对抗性攻击"]}
{"source": "Recently Neural Architecture Search (NAS) has aroused great interest in both academia and industry, however it remains challenging because of its huge and non-continuous search space. Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method. In DSO-NAS, we provide a novel model pruning view to NAS problem. In specific, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations. Next, we impose sparse regularizations to prune useless connections in the architecture. Lastly, we derive an efficient and theoretically sound optimization method to solve it. Our method enjoys both advantages of differentiability and efficiency, therefore can be directly applied to large datasets like ImageNet. Particularly, On CIFAR-10 dataset, DSO-NAS achieves an average test error 2.84%, while on the ImageNet dataset DSO-NAS achieves 25.4% test error under 600M FLOPs with 8 GPUs in 18 hours.", "target": ["通过直接稀疏优化进行单次神经结构搜索", "提出了一种架构搜索方法，其中的连接是用稀疏的正则化来消除的。", "本文提出了直接稀疏优化，这是一种以合理的计算成本获得特定问题的神经架构的方法。", "本文提出了一种基于直接稀疏优化的神经结构搜索方法"]}
{"source": "Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.   We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.", "target": ["通过利用蒸馏技术，获得了最先进的量化、浅层网的准确性。", "通过结合蒸馏和量化为视觉和神经机器翻译实验提出了小型和低成本的模型", "本文在模型压缩的背景下，提出了一个利用教师模型来帮助深度学习模型压缩的框架。"]}
{"source": "Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.", "target": ["用潜在的树来改进NMT", "本文介绍了一种为神经机器翻译服务的诱导源端依赖结构的方法。"]}
{"source": "Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to improve the sample efficiency when we have access to demonstrations. Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency. This includes reward shaping, behavioral cloning, and reverse curriculum generation.", "target": ["通过从一个单一的示范，甚至是一个低效的示范向后学习，并逐步让代理自己做更多的解决方法。", "本文提出了一种通过专家论证的后向课程提高稀疏奖励RL方法效率的方法。", "本文提出了一种用RL解决稀疏奖励任务的策略，即从演示中抽取初始状态。"]}
{"source": "Episodic memory is a psychology term which refers to the ability to recall specific events from the past. We suggest one advantage of this particular type of memory is the ability to easily assign credit to a specific state when remembered information is found to be useful. Inspired by this idea, and the increasing popularity of external memory mechanisms to handle long-term dependencies in deep learning systems, we propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states. The algorithm allows a deep reinforcement learning agent to learn online to preferentially remember those states which are found to be useful to recall later on. Critically this method allows for efficient online computation of gradient estimates with respect to the write process of the external memory. Thus unlike most prior mechanisms for external memory it is feasible to use in an online reinforcement learning setting.", "target": ["基于估计梯度的在线强化学习的外部记忆在一个新的水库采样技术上。", "本文提出了一种改良的RL方法，即由代理保留一个额外的 \"事件记忆\"，并使用一个基于当前状态的 \"查询网络\"。"]}
{"source": "We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation. Our decomposition leads to an interesting phenomenon that the variance does not necessarily increase when more parameters are included in Boltzmann machines, while the bias always decreases. Our result gives a theoretical evidence of the generalization ability of deep learning architectures because it provides the possibility of increasing the representation power with avoiding the variance inflation.", "target": ["我们利用信息几何学的公式实现了对玻尔兹曼机的偏差-方差分解。", "本文的目标是通过对分级模型，特别是玻尔兹曼机的偏差-方差分解进行理论分析，来分析深度学习的有效性和可推广性。", "本文得出的主要结论是，在分层模型中，有可能同时减少偏差和方差。"]}
{"source": "Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network.   Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs.   We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout.   With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a density of 30%.   Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x, enabling larger networks to help advance the state-of-the-art.   We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.", "target": ["将网络修剪和持久性内核结合起来，形成一个实用、快速、准确的网络实现。", "本文介绍了稀疏持久性 RNN，这是一种在芯片上存储 RNN 权重的现有工作中添加修剪的机制。"]}
{"source": "Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy. Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources. Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists. In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed. In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm. The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process. Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate. Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet.", "target": ["我们提出了一种新的剪枝方法和稀疏矩阵格式，以实现高索引压缩率和并行索引解码过程。", "作者使用Viterbi编码极大地压缩了修剪过的网络的稀疏矩阵索引，减少了主要的内存开销之一，并加快了并行设置中的推理速度。"]}
{"source": "Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.", "target": ["一个新颖的分层政策网络，它可以通过发现技能之间的潜在关系，将以前学到的技能与新技能一起重用并作为新技能的子组件。", "本文旨在通过使用由随机时间语法调节的递归政策结构来学习分层策略。", "本文提出了一种在终身学习背景下学习分层策略的方法，通过堆叠策略，然后使用明确的 \"切换 \"策略。"]}
{"source": "Embeddings are a fundamental component of many modern machine learning and natural language processing models.  Understanding them and visualizing them is essential for gathering insights about the information they capture and the behavior of the models.  State of the art in analyzing embeddings consists in projecting them in two-dimensional planes without any interpretable semantics associated to the axes of the projection, which makes detailed analyses and comparison among multiple sets of embeddings challenging.  In this work, we propose to use explicit axes defined as algebraic formulae over embeddings to project them into a lower dimensional, but semantically meaningful subspace, as a simple yet effective analysis and visualization methodology.  This methodology assigns an interpretable semantics to the measures of variability and the axes of visualizations, allowing for both comparisons among different sets of embeddings and fine-grained inspection of the embedding spaces.  We demonstrate the power of the proposed methodology through a series of case studies that make use of visualizations constructed around the underlying methodology and through a user study. The results show how the methodology is effective at providing more profound insights than classical projection methods and how it is widely applicable to many other use cases.", "target": ["我们提出使用明确的向量代数公式投影作为可视化嵌入空间的另一种方式，专门为面向目标的分析任务定制，在我们的用户研究中，它优于t-SNE。", "以非参数的方式分析嵌入空间（基于实例的_方式"]}
{"source": "Learning deep networks which can resist large variations between training andtesting data is essential to build accurate and robust image classifiers.   Towardsthis end, a typical strategy is to apply data augmentation to enlarge the trainingset.    However,  standard  data  augmentation  is  essentially  a  brute-force  strategywhich is inefficient,  as it performs all the pre-defined transformations  to everytraining sample. In this paper, we propose a principled approach to train networkswith  significantly  improved  resistance  to  large  variations  between  training  andtesting data.   This is achieved by embedding a learnable transformation moduleinto the introspective networks (Jin et al., 2017; Lazarow et al., 2017; Lee et al.,2018), which is a convolutional neural network (CNN) classifier empowered withgenerative capabilities.   Our approach alternatively synthesizes pseudo-negativesamples with learned transformations and enhances the classifier by retraining itwith synthesized samples.   Experimental results verify that our approach signif-icantly improves the ability of deep networks to resist large variations betweentraining and testing data and achieves classification accuracy improvements onseveral benchmark datasets, including MNIST, affNIST, SVHN and CIFAR-10.", "target": ["我们提出了一种原则性的方法，赋予分类器以智能和高效的方式抵御训练和测试数据之间较大变化的能力。", "使用内省学习来处理测试时的数据变化", "本文建议使用学习到的转换网络，嵌入到内省网络中，以提高综合实例的分类性能。"]}
{"source": "It is well known that it is possible to construct \"adversarial examples\"  for neural networks: inputs which are misclassified by the network  yet indistinguishable from true data. We propose a simple  modification to standard neural network architectures, thermometer  encoding, which significantly increases the robustness of the network to  adversarial examples. We demonstrate this robustness with experiments  on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that  models with thermometer-encoded inputs consistently have higher accuracy  on adversarial examples, without decreasing generalization.  State-of-the-art accuracy under the strongest known white-box attack was   increased from 93.20% to 94.30% on MNIST and 50.00% to 79.16% on CIFAR-10.  We explore the properties of these networks, providing evidence  that thermometer encodings help neural networks to  find more-non-linear decision boundaries.", "target": ["输入离散化带来了对对抗性例子的稳健性", "作者介绍了对输入离散化/量化的深入研究，作为对对抗性例子的一种防御。"]}
{"source": "Low-precision training is a promising way of decreasing the time and energy cost of training machine learning models.  Previous work has analyzed low-precision training algorithms, such as low-precision stochastic gradient descent, and derived theoretical bounds on their convergence rates.  These bounds tend to depend on the dimension of the model $d$ in that the number of bits needed to achieve a particular error bound increases as $d$ increases.  This is undesirable because a motivating application for low-precision training is large-scale models, such as deep learning, where $d$ can be huge.  In this paper, we prove dimension-independent bounds for low-precision training algorithms that use fixed-point arithmetic, which lets us better understand what affects the convergence of these algorithms as parameters scale.  Our methods also generalize naturally to let us prove new convergence bounds on low-precision training with other quantization schemes, such as low-precision floating-point computation and logarithmic quantization.", "target": ["我们证明了低精度训练算法的维度无关边界", "本文讨论了具有低精度权重的训练模型的收敛性不依赖于模型维度的条件。"]}
{"source": "We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and ERL2. Results are presented on a novel environment we call 'Krazy World'  and a set of maze environments. We show E-MAML and ERL2 deliver better performance on tasks where exploration is important.", "target": ["对MAML和RL2的修改，应该允许更好的探索。", "本文提出了在最近的两种元RL算法的基础上，扩展目标函数以驱动元RL的探索的技巧"]}
{"source": "We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs, given a small annotated set of human programs. The key idea of our approach is to learn a rich latent space which effectively propagates program annotations from known questions to novel questions. We do this by formalizing prior work on VQA, called module networks (Andreas, 2016) as discrete, structured, latent variable models on the joint distribution over questions and answers given images, and devise a procedure to train the model effectively. Our results on a dataset of compositional questions about SHAPES (Andreas, 2016) show that our model generates more interpretable programs and obtains better accuracy on VQA in the low-data regime than prior work.", "target": ["一个具有潜在程序空间的概率神经符号模型，用于更多的可解释问题的回答", "本文提出了一种用于视觉问答的离散的、结构化的潜变量模型，该模型涉及组合式的泛化和推理，在性能和能力方面有明显的增益。"]}
{"source": "The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for training networks that are robust to such examples; and each time stronger attacks have been devised, demonstrating the shortcomings of existing defenses. This highlights a key difficulty in designing an effective defense: the inability to assess a network's robustness against future attacks. We propose to address this difficulty through formal verification techniques. We construct ground truths: adversarial examples with a provably-minimal distance from a given input point. We demonstrate how ground truths can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced by those attacks to the ground truths; and also of defense techniques, by computing the distance to the ground truths before and after the defense is applied, and measuring the improvement. We use this technique to assess recently suggested attack and defense techniques.", "target": ["我们使用形式验证来评估寻找对抗性例子或防御对抗性例子的技术的有效性。", "本文提出了一种计算与原始输入最小距离的对抗性例子的方法。", "作者提议采用可证明的最小距离的例子作为工具来评估训练过的网络的稳健性。", "本文介绍了一种生成对抗性例子的方法，这些例子与用于生成对抗性例子的训练例子的距离最小。"]}
{"source": "This paper introduces a new framework for open-domain question answering in which the retriever and the reader \\emph{iteratively interact} with each other. The framework is agnostic to the architecture of the machine reading model provided it has \\emph{access} to the token-level hidden representations of the reader. The retriever uses fast nearest neighbor search that allows it to scale to corpora containing millions of paragraphs. A gated recurrent unit updates the query at each step conditioned on the \\emph{state} of the reader and the \\emph{reformulated} query is used to re-rank the paragraphs by the retriever. We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus. Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures (\\drqa and \\bidaf) on various large open-domain datasets ---\\tqau, \\quasart, \\searchqa, and \\squado\\footnote{Code and pretrained models are available at \\url{https://github.com/rajarshd/Multi-Step-Reasoning}}.", "target": ["段落检索器和机器阅读器通过强化学习相互作用，在开放领域的数据集上产生巨大的改进。", "本文介绍了一个文档检索器和读者之间双向互动的新框架，用于开放领域的问题回答，并提出了从读者到检索器的 \"读者状态 \"概念。", "本文提出了一个由3个不同部分和一个算法组成的多文档抽取式机器阅读模型。"]}
{"source": "Many imaging tasks require global information about all pixels in an image. Conventional bottom-up classification networks globalize information by decreasing resolution; features are pooled and down-sampled into a single output. But for semantic segmentation and object detection tasks, a network must provide higher-resolution pixel-level outputs. To globalize information while preserving resolution, many researchers propose the inclusion of sophisticated auxiliary blocks, but these come at the cost of a considerable increase in network size and computational cost. This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution. SUNets leverage the information globalization power of u-nets in a deeper net- work architectures that is capable of handling the complexity of natural images. SUNets perform extremely well on semantic segmentation tasks using a small number of parameters.", "target": ["提出了新的架构，在更深的网络中利用u-net的信息全球化能力，并在各种任务中表现良好，没有任何花哨的东西。", "一种用于语义图像分割的网络架构，基于对基本的U-Net架构的组合，减少了参数数量并改善了结果。", "这提出了一个用于图像分割的堆叠式U-Net架构。"]}
{"source": "Asking questions is an important ability for a chatbot. This paper focuses on question generation. Although there are existing works on question generation based on a piece of descriptive text, it remains to be a very challenging problem. In the paper, we propose a new question generation problem, which also requires the input of a target topic in addition to a piece of descriptive text. The key reason for proposing the new problem is that in practical applications, we found that useful questions need to be targeted toward some relevant topics. One almost never asks a random question in a conversation. Due to the fact that given a descriptive text, it is often possible to ask many types of questions, generating a question without knowing what it is about is of limited use. To solve the problem, we propose a novel neural network that is able to generate topic-specific questions. One major advantage of this model is that it can be trained directly using a question-answering corpus without requiring any additional annotations like annotating topics in the questions or answers. Experimental results show that our model outperforms the state-of-the-art baseline.", "target": ["我们提出了一个能够生成特定主题问题的神经网络。", "提出了一种基于神经网络的方法来生成特定主题的问题，其动机是主题问题在实际应用中更有意义。", "提出了一种基于话题的生成方法，使用LSTM的两阶段编码技术来提取话题"]}
{"source": "Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable option  to restore voluntary movements after paralysis. These devices are based on the  ability to extract information about movement intent from neural signals recorded  using multi-electrode arrays chronically implanted in the motor cortices of the  brain. However, the inherent loss and turnover of recorded neurons requires repeated  recalibrations of the interface, which can potentially alter the day-to-day  user experience. The resulting need for continued user adaptation interferes with  the natural, subconscious use of the BMI. Here, we introduce a new computational  approach that decodes movement intent from a low-dimensional latent representation  of the neural data. We implement various domain adaptation methods  to stabilize the interface over significantly long times. This includes Canonical  Correlation Analysis used to align the latent variables across days; this method  requires prior point-to-point correspondence of the time series across domains.  Alternatively, we match the empirical probability distributions of the latent variables  across days through the minimization of their Kullback-Leibler divergence.  These two methods provide a significant and comparable improvement in the performance  of the interface. However, implementation of an Adversarial Domain  Adaptation Network trained to match the empirical probability distribution of the  residuals of the reconstructed neural signals outperforms the two methods based  on latent variables, while requiring remarkably few data points to solve the domain  adaptation problem.", "target": ["我们实现了一个对抗性领域适应网络，以稳定一个固定的脑机接口，对抗记录的神经信号的逐渐变化。", "描述了一种植入式脑机接口的新方法，以解决校准问题和协变量转移。", "作者定义了一个使用自动编码器的BMI，然后解决BMI中的数据漂移问题。"]}
{"source": "Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words. While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so. Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions. For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world. We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge. We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.", "target": ["分析和理解神经网络代理如何学习理解简单的基础语言", "作者将心理学实验方法与理解深度学习方法的黑箱如何解决问题联系起来。", "本文介绍了在一个简单的环境中通过强化学习来学习基础语言的代理的分析，该环境将口头指示与视觉信息相结合"]}
{"source": "Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.", "target": ["通过观察物体的运动方式来学习物体的部件、层次结构和动态变化", "提出了一个无监督学习模型，该模型可以学习将物体分解成若干部分，预测各部分的层次结构，并根据分解的部分和层次结构来预测运动。"]}
{"source": "A successful application of convolutional architectures is to increase the resolution of single low-resolution images -- a image restoration task called super-resolution (SR). Naturally, SR is of value to resource constrained devices like mobile phones, electronic photograph frames and televisions to enhance image quality. However, SR demands perhaps the most extreme amounts of memory and compute operations of any mainstream vision task known today, preventing SR from being deployed to devices that require them. In this paper, we perform a early systematic study of system resource efficiency for SR, within the context of a variety of architectural and low-precision approaches originally developed for discriminative neural networks. We present a rich set of insights, representative SR architectures, and efficiency trade-offs; for example, the prioritization of ways to compress models to reach a specific memory and computation target and techniques to compact SR models so that they are suitable for DSPs and FPGAs. As a result of doing so, we manage to achieve better and comparable performance with previous models in the existing literature, highlighting the practicality of using existing efficiency techniques in SR tasks. Collectively, we believe these results provides the foundation for further research into the little explored area of resource efficiency for SR.", "target": ["我们在超级分辨率上建立了对资源节约型技术的理解", "本文对各种卷积神经网络在超级分辨率问题上取得的权衡提出了详细的实证评估。", "本文提出要提高超分辨率网络的系统资源效率。"]}
{"source": "Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets easier with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.", "target": ["我们研究了傅里叶域中的ReLU网络，并展示了奇特的行为。", "对ReLU网络进行傅里叶分析，发现它们偏向于学习低频", "本文对神经网络的傅里叶系数课题有理论和实证上的贡献。"]}
{"source": "Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering. Many metric learning methods represent the input as a single point in the embedding space. Often the distance between points is used as a proxy for match confidence. However, this can fail to represent uncertainty which can arise when the input is ambiguous, e.g., due to occlusion or blurriness. This work addresses this issue and explicitly models the uncertainty by “hedging” the location of each input in the embedding space. We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle (Alemi et al., 2016; Achille & Soatto, 2018). Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of “hedging its bets” across the embedding space upon encountering ambiguous inputs. This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure which is correlated with downstream performance.", "target": ["本文提出在识别和验证等实例嵌入任务中使用概率分布而不是点。", "论文提出了当前点嵌入的替代方案和一种训练它们的技术。", "本文提出了一个使用不确定嵌入的模型来扩展深度学习在贝叶斯的应用"]}
{"source": "Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.   While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.   Despite its success, this approach has notable drawbacks. Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters.   We present two new techniques to address these problems.   First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network. Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.   Both the contraction and regression weights are learned end-to-end by backpropagation. By imposing low rank on both, we use significantly fewer parameters.   Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.", "target": ["我们提出了张量收缩和低秩张量回归层，以保留和利用整个网络的多线性结构，从而节省了大量空间，对性能几乎没有影响。", "本文提出了使用张量的低秩表示的神经网络的新层架构", "本文通过使用一个新的张量回归层，将张量分解和张量回归纳入CNN。"]}
{"source": "We explore ways of incorporating bilingual dictionaries to enable semi-supervised  neural machine translation. Conventional back-translation methods have shown  success in leveraging target side monolingual data. However, since the quality of  back-translation models is tied to the size of the available parallel corpora, this  could adversely impact the synthetically generated sentences in a low resource  setting. We propose a simple data augmentation technique to address both this  shortcoming. We incorporate widely available bilingual dictionaries that yield  word-by-word translations to generate synthetic sentences. This automatically  expands the vocabulary of the model while maintaining high quality content. Our  method shows an appreciable improvement in performance over strong baselines.", "target": ["我们使用双语词典为神经机器翻译提供数据增援", "本文研究了使用双语词典为目标端单语数据创建合成源，以改善用少量平行数据训练的NMT模型。"]}
{"source": "Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics. This allows us to overcome the known \"couch-potato\" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo. In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only. The code is available at https://github.com/google-research/episodic-curiosity/.", "target": ["我们提出了一个基于事件记忆和可及性思想的新颖的好奇心模型，这使我们能够克服先前工作中已知的 \"沙发土豆 \"问题。", "建议在RL算法中给予探索奖励，对环境步骤中的父亲的观察给予更大的奖金。", "作者提出了一个旨在帮助解决稀疏奖励RL问题的探索奖励，并考虑在复杂的三维环境中进行许多实验。"]}
{"source": "We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a ``convolution over possible worlds''. Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.", "target": ["我们引入了一个新的逻辑蕴含数据集，目的是衡量模型捕捉和利用逻辑表达式结构的能力，以完成蕴含预测任务。", "本文提出了一个新的模型，使用深度模型来检测作为可能世界上连续函数的乘积的逻辑蕴含。", "提出了一个为机器学习设计的新模型，并预测了逻辑上蕴含。"]}
{"source": "Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.   However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements. Also, previously seen training samples may not be available at the time of retraining. We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network. Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes. An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network. We evaluated the proposed scheme on several recognition applications. The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).", "target": ["本文是关于增量学习的一种新的节能方法。", "提出了作为迁移学习的增量学习的程序。", "本文提出了一种以增量方式训练深度卷积神经网络的方法，即在一段时间内以小批量的方式获得数据。", "提出了一种利用深度网络进行类增量学习的方法，在最终/最佳方法中提出了三种不同的学习策略。"]}
{"source": "Recurrent neural networks (RNNs) are widely used to model sequential data but  their non-linear dependencies between sequence elements prevent parallelizing  training over sequence length. We show the training of RNNs with only linear  sequential dependencies can be parallelized over the sequence length using the  parallel scan algorithm, leading to rapid training on long sequences even with  small minibatch size. We develop a parallel linear recurrence CUDA kernel and  show that it can be applied to immediately speed up training and inference of  several state of the art RNN architectures by up to 9x.  We abstract recent work  on linear RNNs into a new framework of linear surrogate RNNs and develop a  linear surrogate model for the long short-term memory unit, the GILR-LSTM, that  utilizes parallel linear recurrence.  We extend sequence learning to new  extremely long sequence regimes that were previously out of reach by  successfully training a GILR-LSTM on a synthetic sequence classification task  with a one million timestep dependency.", "target": ["使用并行扫描来并行化线性递归神经网络。 在长度为100万的依赖关系上训练模型。", "建议通过应用Blelloch的方法来加速RNN。", "作者提出了一种线性代理RNN的并行算法，它比现有的准RNN、SRU和LSTM的实现方式产生了加速。"]}
{"source": "Neural text generation models are often autoregressive language models or seq2seq models. Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality. Language models are typically trained via maximum likelihood and most often with teacher forcing. Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time. We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.", "target": ["用于填空的自然语言GAN", "本文提出使用GAN来生成文本。", "使用GAN和机制生成文本样本，以周围文本为条件填补缺失的单词"]}
{"source": "Parametric texture models have been applied successfully to synthesize artificial images. Psychophysical studies show that under defined conditions observers are unable to differentiate between model-generated and original natural textures. In industrial applications the reverse case is of interest: a texture analysis system should decide if human observers are able to discriminate between a reference and a novel texture. For example, in case of inspecting decorative surfaces the de- tection of visible texture anomalies without any prior knowledge is required. Here, we implemented a human-vision-inspired novelty detection approach. Assuming that the features used for texture synthesis are important for human texture percep- tion, we compare psychophysical as well as learnt texture representations based on activations of a pretrained CNN in a novelty detection scenario. Additionally, we introduce a novel objective function to train one-class neural networks for novelty detection and compare the results to standard one-class SVM approaches. Our experiments clearly show the differences between human-vision-inspired texture representations and learnt features in detecting visual anomalies. Based on a dig- ital print inspection scenario we show that psychophysical texture representations are able to outperform CNN-encoded features.", "target": ["心理物理学和CNN编码的纹理表征在单类神经网络新颖性检测应用中的比较。", "本文重点讨论了新颖性检测，并表明心理物理表征在这一任务的某些部分可以胜过VGG编码器特征。", "本文考虑检测纹理中的异常情况，并提出了原始损失函数。", "提出从三个不同的模型中训练两个异常检测器来检测视觉纹理中的感知异常。"]}
{"source": "In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues. To address these problems, we study a new type of regularization approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector. We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regularized SC. Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance.", "target": ["我们提出了一种新型的正则化方法，鼓励表征学习中的非重叠性，以提高可解释性并减少过拟合。", "本文引入了一个矩阵正则器来同时诱导稀疏性和近似正交性。", "本文研究了一种正则化方法，以促进稀疏性并减少所学表征中权重向量支持之间的重叠，从而提高可解释性并避免过度拟合。", "该论文提出了一种新的正则化方法，同时鼓励权重向量（W）是稀疏的，并且是相互正交的。"]}
{"source": "Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.   Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues. \t  We prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach. \t  This result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.", "target": ["证明了门控机制提供了对时间转换的不变性。从这一观点出发，介绍并测试了LSTM的新初始化。", "论文将循环网络的设计及其对网络对时间转换的反应的影响联系起来，并利用这一点开发了一个简单的偏差初始化方案。"]}
{"source": "Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations. A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students. In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \\emph{learning}. In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. We call this approach ``learning to teach''. In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model). The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution. To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).", "target": ["我们提出并验证了以学定教的有效性，这是一个自动指导机器学习过程的新框架。", "本文专注于 \"机器教学\"，并建议利用强化学习，将奖励定义为学习者的学习速度，并使用策略梯度来更新教师参数。", "作者定义了一个由四个部分组成的深度学习模型：学生模型、教师模型、损失函数和数据集。", "提出了一个 \"学习教学 \"的框架，对应于对呈现给学习者的数据的选择。"]}
{"source": "We present DL2, a system for training and querying neural networks with logical constraints. The key idea is to translate these constraints into a differentiable loss with desirable mathematical properties and to then either train with this loss in an iterative manner or to use the loss for querying the network for inputs subject to the constraints. We empirically demonstrate that DL2 is effective in both training and querying scenarios, across a range of constraints and data sets.", "target": ["用于训练和查询神经网络的逻辑约束的可微损失。", "将神经网络的参数和输入输出对的查询转化为可微的损失函数的框架，以及用于指定这些查询的相关声明性语言", "本文通过将逻辑公式转化为神经网络的非负损失函数，解决了逻辑方法与神经网络的结合问题。"]}
{"source": "Genetic algorithms have been widely used in many practical optimization problems.  Inspired by natural selection, operators, including mutation, crossover  and selection, provide effective heuristics for search and black-box optimization.  However, they have not been shown useful for deep reinforcement learning, possibly  due to the catastrophic consequence of parameter crossovers of neural networks.  Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm  for sample-efficient deep policy optimization. GPO uses imitation learning  for policy crossover in the state space and applies policy gradient methods for mutation.  Our experiments on MuJoCo tasks show that GPO as a genetic algorithm  is able to provide superior performance over the state-of-the-art policy gradient  methods and achieves comparable or higher sample efficiency.", "target": ["基于遗传算法的深度神经网络策略优化方法", "作者提出了一种训练政策网络组合的算法，该算法定期将组合中的不同政策混合在一起。", "本文提出了一种受遗传算法启发的策略优化方法，它模拟了策略网络上的突变和交叉运算。"]}
{"source": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.  Building upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov’s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "target": ["使用近端梯度法进行模型量化的原则性框架，并进行了实证评估和理论收敛分析。", "提出了ProxQuant方法来训练具有量化权重的神经网络。", "提出用近端梯度下降法求解二元网及其变体。"]}
{"source": "Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., “near” linear separability), or an unrealistically wide hidden layer with \\Omega\\(N) units.    Results: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss. We prove that, with high probability in the limit of N\\rightarrow\\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\\tilde{\\Omega}(\\sqrt{N}), and a more realistic number of d_1=\\tilde{\\Omega}(N/d_0) hidden units. We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.", "target": ["多层神经网中的 \"坏 \"局部最小值正在消失：用比以前更合理的假设进行证明", "在只有一个隐藏层的网络中，与全局最小值相比，次优局部最小值的数量呈指数级下降。", "本文旨在回答为什么基于神经网络的标准SGD算法会收敛到 \"好 \"的解决方案。"]}
{"source": "Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness, and several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we identify the attention shift phenomenon, which may hinder the transferability of adversarial examples to the defense models. It indicates that the defenses rely on different discriminative regions to make predictions compared with normally trained models. Therefore, we propose an attention-invariant attack method to generate more transferable adversarial examples. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the defense techniques.", "target": ["我们提出了一种注意力不变的攻击方法，为黑箱攻击产生更多可转移的对抗性例子，可以以高成功率骗过最先进的防御系统。", "本文提出了一种新的方法来克服对CNN的对抗性攻击的最先进的防御。", "本文认为，\"注意力转移 \"是对抗性攻击无法转移背后的一个关键属性，并提出一种注意力不变的攻击方法"]}
{"source": "We present Merged-Averaged Classifiers via Hashing (MACH) for $K$-classification with large $K$. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(d\\log{K})$ memory while only requiring $O(K\\log{K} + d\\log{K})$ operation for inference. MACH is the first generic $K$-classification algorithm, with provably theoretical guarantees, which requires $O(\\log{K})$ memory without any assumption on the relationship between classes. MACH uses universal hashing to reduce classification with a large number of classes to few independent classification task with very small (constant) number of classes. We provide theoretical quantification of accuracy-memory tradeoff by showing the first connection between extreme classification and heavy hitters. With MACH we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU (12GB), with the classification accuracy of 19.28\\%, which is the best-reported accuracy on this dataset. Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (320 GB model size) and achieves 9\\% accuracy.   In contrast, MACH can achieve 9\\% accuracy with 480x reduction in the model size (of mere 0.6GB). With MACH, we also demonstrate complete training of fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU.", "target": ["如何在单个GPU上训练100,000个类", "提出了一种高效的散列方法MACH，用于大输出空间背景下的softmax逼近，既节省内存又节省计算量。", "一种针对在 ODP 和 Imagenet-21K 数据集上演示的多类设置中涉及大量类的问题的分类方案方法", "本文提出了一种基于散列的方案，以减少K-way分类在K较大时的内存和计算时间。"]}
{"source": "Gradient-based optimization is the foundation of deep learning and reinforcement learning.  Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.  These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.", "target": ["我们提出了一种对随机变量的黑箱函数梯度进行无偏估计的一般方法。我们将此方法应用于离散变分推理和强化学习。", "提出了一种新的方法，为黑箱优化或训练离散潜变量模型执行梯度下降。"]}
{"source": "Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support. In other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.   (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.", "target": ["我们提出了一个GAN学习分布的支持度估计器，表明它们确实受到模式崩溃的影响，并且我们证明了编码器-解码器GAN也不能避免这个问题。", "本文试图通过实验来估计典型的GAN产生的解决方案的支持大小。", "本文提出了一个基于生日悖论的巧妙的新测试，用于测量生成样本的多样性，实验结果被解释为意味着模式崩溃在一些最先进的生成模型中很强。", "本文利用生日悖论表明，一些GAN架构产生的分布具有相当低的支持度。"]}
{"source": "Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.", "target": ["我们提出了一个框架，通过在潜在语义空间中寻找对手，为视觉和文本领域生成针对黑箱分类器的自然对手。", "提出了一种创建语义对抗例子的方法。", "提出了一个框架，通过在密集和连续数据表示的潜在空间中搜索对抗者来生成自然对抗的例子"]}
{"source": "Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017).   It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized. The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well. In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs. This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form. We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks.", "target": ["我们通过开发一个新的Fisher近似系列，将K-FAC方法扩展到RNN。", "作者将K-FAC方法扩展到RNN，并提出了3种近似F的方法，展示了3个数据集的优化结果，在更新次数和计算时间上都优于ADAM。", "建议将克朗克系数适当曲率优化方法扩展到循环神经网络的设置中。", "作者提出了一种专门为RNN设计的二阶方法"]}
{"source": "Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution. In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights. We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors. In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks.", "target": ["卷积神经网络核的生成模型，在新的数据集上训练时作为先验分布。", "一种使用贝叶斯方法为卷积神经网络建模的方法。", "提出了 \"深度权重先验\"：这个想法是在一个辅助数据集上引出一个先验，然后在CNN过滤器上使用该先验来快速启动对感兴趣的数据集的推理。", "本文探讨了通过使用自动编码器来获得对训练过的网络的过滤权重的表达性先验，从而为具有类似问题域的卷积神经网络模型学习信息性先验。"]}
{"source": "The high dimensionality of hyperspectral imaging forces unique challenges in scope, size and processing requirements.   Motivated by the potential for an in-the-field cell sorting detector, we examine a Synechocystis sp. PCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or deplete cultures.   We use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background. Further, we use the classification accuracy to guide a data-driven, iterative feature selection method, allowing the design neural networks requiring 90% fewer input features with little accuracy degradation.", "target": ["我们将深度学习技术应用于高光谱图像分割和迭代特征采样。", "提出了一个贪婪的方案，在分类任务中选择一个高度相关的光谱特征子集。", "本文探讨了使用神经网络对细胞的高光谱成像（HSI）进行分类和分割。", "基于深度学习技术对细胞进行分类并实现细胞分割，减少输入特征"]}
{"source": "Data Interpretation is an important part of Quantitative Aptitude exams and requires an individual to answer questions grounded in plots such as bar charts, line graphs, scatter plots, \\textit{etc}. Recently, there has been an increasing interest in building models which can perform this task by learning from datasets containing triplets of the form \\{plot, question, answer\\}. Two such datasets have been proposed in the recent past which contain plots generated from synthetic data with limited (i) $x-y$ axes variables (ii) question templates and (iii) answer vocabulary and hence do not adequately capture the challenges posed by this task. To overcome these limitations of existing datasets, we introduce a new dataset containing $9.7$ million question-answer pairs grounded over $270,000$ plots with three main differentiators. First, the plots in our dataset contain a wide variety of realistic $x$-$y$ variables such as CO2 emission, fertility rate, \\textit{etc. } extracted from  real word data sources such as World Bank, government sites, \\textit{etc}. Second, the questions in our dataset are more complex as they are based on templates extracted from interesting questions asked by a crowd of workers using a fraction of these plots. Lastly, the answers in our dataset are not restricted to a small vocabulary and a large fraction of the answers seen at test time are not present in the training vocabulary. As a result, existing models for Visual Question Answering which largely use end-to-end models in a multi-class classification framework cannot be used for this task. We establish initial results on this dataset and emphasize the complexity of the task using a multi-staged modular pipeline with various sub-components to (i) extract relevant data from the plot and convert it to a semi-structured table (ii) combine the question with this table and use compositional semantic parsing to arrive at a logical form from which the answer can be derived. We believe that such a modular framework is the best way to go forward as it would enable the research community to independently make progress on all the sub-tasks involved in plot question answering.", "target": ["我们创建了一个新的数据集，用于对图的数据解释，并提出了相同的基线。", "作者提出了一个解决DIP问题的管道，涉及从包含{plot, question, answer}形式的三联体的数据集中学习。", "提出了一种能够解释科学图谱中显示的数据的算法。"]}
{"source": "Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing. Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model. PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule. Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well. Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train. Orthogonal Random Features (ORFs) (Choromanski et al.) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation. Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting. In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs. In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed.", "target": ["通过正交随机特征改进预测状态的循环神经网络", "建议通过考虑正交随机特征来提高预测状态递归神经网络的性能。", "本文解决了训练预测状态循环神经网络的问题，并做出了两个贡献。"]}
{"source": "Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing. This creates a fundamental quality- versus-quantity trade-off in the learning process. Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data? We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds. To this end, we propose “fidelity-weighted learning” (FWL), a semi-supervised student- teacher approach for training deep neural networks using weakly-labeled data. FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels). Both student and teacher are learned from the data. We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.", "target": ["我们提出了保真度加权学习（Fidelity-weighted Learning），这是一种半监督的师生方法，用于使用弱标记的数据训练神经网络。", "本文通过使用一个干净的和一个有噪声的数据集，并假设一个教师和学生网络，提出了一种在弱监督下的学习方法。", "这篇论文试图用少数标记的训练样本来训练深度神经网络模型。", "作者提出了一种针对没有足够可靠的注释数据的情况下训练深度学习模型的方法。"]}
{"source": "Online learning has attracted great attention due to the increasing demand for systems that have the ability of learning and evolving. When the data to be processed is also high dimensional and dimension reduction is necessary for visualization or prediction enhancement, online dimension reduction will play an essential role. The purpose of this paper is to propose new online learning approaches for supervised dimension reduction. Our first algorithm is motivated by adapting the sliced inverse regression (SIR), a pioneer and effective algorithm for supervised dimension reduction, and making it implementable in an incremental manner. The new algorithm, called incremental sliced inverse regression (ISIR), is able to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in. We also refine the algorithm by using an overlapping technique  and develop an incremental overlapping sliced inverse regression (IOSIR) algorithm. We verify the effectiveness and efficiency of both algorithms by simulations and real data applications.", "target": ["我们提出了两种新的方法，增量切片反回归和增量重叠切片反回归，以在线学习的方式实现监督降维。", "研究充分降维问题，提出增量切片反回归算法。", "本文提出了一种用于监督降维的在线学习算法，称为增量切分反回归"]}
{"source": "This paper presents a storage-efficient learning model titled Recursive Binary Neural Networks for embedded and mobile devices having a limited amount of on-chip data storage such as hundreds of kilo-Bytes. The main idea of the proposed model is to recursively recycle data storage of weights (parameters) during training. This enables a device with a given storage constraint to train and instantiate a neural network classifier with a larger number of weights on a chip, achieving better classification accuracy. Such efficient use of on-chip storage reduces off-chip storage accesses, improving energy-efficiency and speed of training. We verified the proposed training model with deep and convolutional neural network classifiers on the MNIST and voice activity detection benchmarks. For the deep neural network, our model achieves data storage requirement of as low as 2 bits/weight, whereas the conventional binary neural network learning models require data storage of 8 to 32 bits/weight. With the same amount of data storage, our model can train a bigger network having more weights, achieving 1% less test error than the conventional binary neural network learning model. To achieve the similar classification error, the conventional binary neural network model requires 4× more data storage for weights than our proposed model. For the convolution neural network classifier, the proposed model achieves 2.4% less test error for the same on-chip storage or 6× storage savings to achieve the similar accuracy.", "target": ["我们提出了一个学习模型，使DNN仅用2位/权重进行学习，这对设备上的学习特别有用。", "提出了一种将NN逐步离散化的方法，以提高内存和性能。"]}
{"source": "Within-class variation in a high-dimensional dataset can be modeled as being on a low-dimensional manifold due to the constraints of the physical processes producing that variation (e.g., translation, illumination, etc.). We desire a method for learning a representation of the manifolds induced by identity-preserving transformations that can be used to increase robustness, reduce the training burden, and encourage interpretability in machine learning tasks. In particular, what is needed is a representation of the transformation manifold that can robustly capture the shape of the manifold from the input data, generate new points on the manifold, and extend transformations outside of the training domain without significantly increasing the error. Previous work has proposed algorithms to efficiently learn analytic operators (called transport operators) that define the process of transporting one data point on a manifold to another.   The main contribution of this paper is to define two transfer learning methods that use this generative manifold representation to learn natural transformations and incorporate them into new data. The first method uses this representation in a novel randomized approach to transfer learning that employs the learned generative model to map out unseen regions of the data space. These results are shown through demonstrations of transfer learning in a data augmentation task for few-shot image classification. The second method use of transport operators for injecting specific transformations into new data examples which allows for realistic image animation and informed data augmentation.   These results are shown on stylized constructions using the classic swiss roll data structure and in demonstrations of transfer learning in a data augmentation task for few-shot image classification. We also propose the use of transport operators for injecting transformations into new data examples which allows for realistic image animation.", "target": ["在流形上学习运输算子形成了一个有价值的表示，用于做迁移学习等任务。", "使用字典学习框架，在增强的USPS数字上学习多方面的运输算子。", "本文考虑了Culpepper和Olshausen（2009）的流形运输算子学习框架，并将其解释为获得概率生成模型下的MAP估计。"]}
{"source": "The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner. Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.", "target": ["我们提出了一个神经变分模型，用于学习语言指导下的构成性视觉概念。", "提出了一种新的神经网结构，通过结合β-VAE和SCAN来学习物体概念。", "本文介绍了一个基于VAE的模型，用于图像和文本之间的翻译，其潜伏表示非常适合应用符号运算，使其在从文本中抽取图像时有更多的表达语言。", "本文提出了一个名为SCAN（符号-概念关联网络）的新模型，用于层次化的概念学习，并允许使用逻辑运算符对由现有概念组成的新概念进行泛化。"]}
{"source": "Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.", "target": ["Latent Topic Conversational Model，是seq2seq和神经主题模型的混合体，以产生更多样化和有趣的反应。", "本文提出了话题模型和seq2seq对话模型的结合。", "通过结合seq2seq模型和神经主题模型，提出了一个带有主题信息的对话模型，并表明所提出的模型优于一些基线模型seq2seq和seq2seq的其他潜在变量模型变体。", "本文讨论了对话模型中持久的主题性问题，并提出了一个模型，它是神经主题模型和基于seq2seq的对话系统的结合。"]}
{"source": "Most of the existing Graph Neural Networks (GNNs) are the mere extension of the Convolutional Neural Networks (CNNs) to graphs. Generally, they consist of several steps of message passing between the nodes followed by a global indiscriminate feature pooling function. In many data-sets, however, the nodes are unlabeled or their labels provide no information about the similarity between the nodes and the locations of the nodes in the graph. Accordingly, message passing may not propagate helpful information throughout the graph. We show that this conventional approach can fail to learn to perform even simple graph classification tasks. We alleviate this serious shortcoming of the GNNs by making them a two step method. In the first of the proposed approach, a graph embedding algorithm is utilized to obtain a continuous feature vector for each node of the graph. The embedding algorithm represents the graph as a point-cloud in the embedding space. In the second step, the GNN is applied to the point-cloud representation of the graph provided by the embedding method. The GNN learns to perform the given task by inferring the topological structure of the graph encoded in the spatial distribution of the embedded vectors. In addition, we extend the proposed approach to the graph clustering problem and a new architecture for graph clustering is proposed. Moreover, the spatial representation of the graph is utilized to design a graph pooling algorithm. We turn the problem of graph down-sampling into a column sampling problem, i.e., the sampling algorithm selects a subset of the nodes whose feature vectors preserve the spatial distribution of all the feature vectors. We apply the proposed approach to several popular benchmark data-sets and it is shown that the proposed geometrical approach strongly improves the state-of-the-art result for several data-sets. For instance, for the PTC data-set, we improve the state-of-the-art result for more than 22 %.", "target": ["图形分析问题被转化为点云分析问题。", "提出了一个使用其自适应图池层的深度GNN网络用于图分类问题。", "作者提出了一种学习图的表征的方法"]}
{"source": "Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs. Such adversarial examples can mislead DNNs to produce adversary-selected results.  Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts.   In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances.   For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses.    We apply AdvGAN in both semi-whitebox and black-box attack settings. In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks. In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly.  Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks. Our attack  has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge.", "target": ["我们提议在半白盒和黑盒环境下，基于生成式对抗网络生成对抗性例子。", "描述了AdvGAN，一个有条件的GAN加对抗性损失，并在半白盒和黑盒设置上评估了AdvGAN，报告了最先进的结果。", "本文提出了一种生成欺骗分类系统的对抗性例子的方法，并赢得了MadryLab的mnist挑战。"]}
{"source": "This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set. Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training. We empirically demonstrate that: a) deep autoencoder models generalize much better than the shallow ones, b) non-linear activation functions with negative parts are crucial for training deep models, and c) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting. We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering. The new algorithm significantly speeds up training and improves model performance. Our code is publicly available.", "target": ["本文展示了如何在时间分割的Netflix数据集上端到端训练深度自动编码器以实现SoA结果。", "本文提出了一个用于评级预测的深度自动编码器模型，该模型在Netflix奖项数据集上的表现优于其他最先进的方法。", "建议在推荐系统中使用深度AE来做评级预测任务。", "作者提出了一个更准确的Netflix推荐模型，证明了深度自动编码器可以胜过有时间信息的更复杂的基于RNN的模型。"]}
{"source": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.", "target": ["我们介绍了Universal Transformer，这是一个自关注的并行时间循环序列模型，在广泛的序列到序列的任务中，包括机器翻译，它的性能优于Transformers和LSTMs。", "提出了一个新的模型UT，基于Transformer模型，增加了循环和动态停止循环的功能。", "本文通过递归地应用多头自我关注块来扩展Transformer，而不是在vanilla Transformer中叠加多个块。"]}
{"source": "We present a framework for interpretable continual learning (ICL). We show that explanations of previously performed tasks can be used to improve performance on future tasks. ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task. The ICL idea is general and may be applied to many continual learning approaches. Here we focus on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting. We use saliency maps to provide explanations of performed tasks and propose a new metric to assess their quality. Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms of its explanations, which are assessed qualitatively and quantitatively using the proposed metric.", "target": ["本文开发了一个可解释的持续学习框架，其中对已完成任务的解释被用来提高学习者在未来任务中的注意力，并且还提出了一个解释指标。", "作者提出了一个持续学习的框架，该框架基于对以前所学任务的执行分类的解释。", "本文提出了一个持续学习框架的扩展，使用现有的变分持续学习作为基础方法，并使用证据权重。"]}
{"source": "The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 Micikevicius et al. (2017). On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10). In this work, we train state-of-the-art visual understanding neural networks on the ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware. In particular, we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.We propose a shared exponent representation of tensors and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations. The nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator. We implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters and with a 1.8X improvement in end-to-end training throughput. To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision", "target": ["在通用硬件上使用16位整数的混合精度训练管道；ImageNet级CNN的SOTA精度；ImageNet-1K分类任务的最佳报告精度与任何降低精度训练。", "本文展示了混合精度动态定点计算的精心实现，可以使用16位整数表示的降低精度的深度学习模型来达到最先进的精度。", "提出了一种 \"动态定点 \"方案，分享张量的指数部分，开发了用这种格式进行NN计算的程序，并对有限精度训练进行了演示。"]}
{"source": "In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model. The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed. Key ingredients for the acoustic model are Gated Linear Units and high dropout. We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.", "target": ["一个基于字母的ConvNet声学模型带来了一个简单而有竞争力的语音识别管道。", "本文将门控卷积神经网络应用于语音识别，使用训练标准ASG。"]}
{"source": "Generative adversarial networks (GANs) are a powerful framework for generative tasks. However, they are difficult to train and tend to miss modes of the true data generation process. Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space. We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations. Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes. We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks.", "target": ["一个新的GAN框架，利用变换不变的特征来学习丰富的表示和强大的生成器。", "提出了一个修正的GAN目标，由一个经典的GAN项和一个不变的编码项组成。", "本文介绍了IVE-GAN，一个将编码器引入生成式对抗网络框架的模型。"]}
{"source": "We propose a method for learning the dependency structure between latent variables in deep latent variable models.   Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure.   The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective.   Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values.   We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10. Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model.", "target": ["我们提出了一种在变分自动编码器中学习潜在依赖结构的方法。", "使用二元随机变量矩阵来捕捉分层深度生成模型中潜在变量之间的依赖关系。", "本文提出了一种VAE方法，在训练过程中学习潜变量的依赖结构。", "作者建议用自动回归结构来增加VAE的潜空间，以提高推理网络和潜先验的表现力。"]}
{"source": "Many real-world time series, such as in activity recognition, finance, or climate science, have changepoints where the system's structure or parameters change. Detecting changes is important as they may indicate critical events. However, existing methods for changepoint detection face challenges when (1) the patterns of change cannot be modeled using simple and predefined metrics, and (2) changes can occur gradually, at multiple time-scales. To address this, we show how changepoint detection can be treated as a supervised learning problem, and propose a new deep neural network architecture that can efficiently identify both abrupt and gradual changes at multiple scales. Our proposed method, pyramid recurrent neural network (PRNN), is designed to be scale-invariant, by incorporating wavelets and pyramid analysis techniques from multi-scale signal processing. Through experiments on synthetic and real-world datasets, we show that PRNN can detect abrupt and gradual changes with higher accuracy than the state of the art and can extrapolate to detect changepoints at novel timescales that have not been seen in training.", "target": ["我们介绍了一种用于多变量时间序列中变化点检测的标度不变的神经网络结构。", "本文利用深度架构中的小波变换概念来解决变化点检测。", "本文提出了一个基于金字塔的神经网络，并将其应用于一维信号，其基本过程发生在不同的时间尺度上，其任务是变化点检测。"]}
{"source": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "target": ["RL为自动推理算法找到更好的启发式方法。", "旨在利用强化学习为回溯搜索算法学习启发式方法，并提出一个模型，利用图形神经网络产生字面和句子嵌入，并利用它们来预测每个字面的质量，以决定每个动作的概率。", "本文提出了一种利用深度学习为QBF自动学习变量选择启发式的方法"]}
{"source": "We consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalise beyond memorising the training data. We propose a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalisation is. Our approach involves using a test set to estimate the Wasserstein distance between the generative distribution produced by our procedure, and the underlying data distribution. We use this procedure to assess the performance of several modern generative adversarial network architectures. We find that this procedure is sensitive to the choice of ground metric on the underlying data space, and suggest a choice of ground metric that substantially improves performance.   We finally suggest that attending to the ground metric used in Wasserstein generative adversarial network training may be fruitful, and outline a concrete pathway towards doing so.", "target": ["评估您的GAN是否实际上在做一些事情，而不是记住训练数据。", "旨在为GAN提供一个质量衡量/测试，并提议通过使用由Diracs之和构成的两个分布之间的Wasserstein距离作为基线性能来评估GAN学习的分布的当前近似度。", "本文提出了一个通过重新考虑观察的关键来评估GAN性能的程序，使用该程序来测试和改进目前的GAN"]}
{"source": "The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.", "target": ["我们使用搜索技术来发现新的激活函数，我们发现的最佳激活函数f(x) = x * sigmoid(beta * x)，在ImageNet等一些具有挑战性的任务上优于ReLU。", "提出了一种基于强化学习的方法，通过从一组单数和双数运算符中搜索组合来寻找非线性。", "本文利用强化学习来搜索一组一元和二元函数的组合，从而得到一个新的激活函数", "作者利用强化学习，从丰富的可能候选者中找到新的潜在激活函数。"]}
{"source": "Successful training of convolutional neural networks is often associated with suffi-  ciently deep architectures composed of high amounts of features. These networks  typically rely on a variety of regularization and pruning techniques to converge  to less redundant states. We introduce a novel bottom-up approach to expand  representations in fixed-depth architectures. These architectures start from just a  single feature per layer and greedily increase width of individual layers to attain  effective representational capacities needed for a specific task. While network  growth can rely on a family of metrics, we propose a computationally efficient  version based on feature time evolution and demonstrate its potency in determin-  ing feature importance and a networks’ effective capacity. We demonstrate how  automatically expanded architectures converge to similar topologies that benefit  from lesser amount of parameters or improved accuracy and exhibit systematic  correspondence in representational complexity with the specified task. In contrast  to conventional design patterns with a typical monotonic increase in the amount of  features with increased depth, we observe that CNNs perform better when there is  more learnable parameters in intermediate, with falloffs to earlier and later layers.", "target": ["一种自下而上的算法，将每层只有一个特征的CNN扩展到具有足够表征能力的架构。", "提出动态调整全卷积神经网络的特征图深度，制定自我相似度的措施，提升性能。", "介绍了一个简单的基于相关性的指标来衡量神经网络中的过滤器是否被有效利用，作为有效容量的代理。", "旨在通过对网络中间层通道的增量添加和移除，解决深度学习架构的搜索问题。"]}
{"source": "Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation). Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way. In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient. Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network. In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation. This feed-forward network learns to approximate the state of the fixed-point using a local learning rule. After training, we can simply use this initializing network for inference, resulting in a learned feedforward network. Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation. This shows how we might go about training deep networks without using backpropagation.", "target": ["我们通过使用基于能量的模型来提供本地目标，训练一个没有反推的前馈网络", "本文旨在加快用平衡传播法（EP）训练的基于能量的模型的迭代推理程序，建议训练一个前馈网络来预测 \"平衡网络 \"的固定点。 ", "训练一个单独的网络来初始化用平衡传播法训练的循环网络"]}
{"source": "We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation. A single object may have many attributes which when altered do not change the identity of the object itself. Consider the human face; the identity of a particular person is independent of whether or not they happen to be wearing glasses. The attribute of wearing glasses can be changed without changing the identity of the person. However, the ability to manipulate and alter image attributes without altering the object identity is not a trivial task. Here, we are interested in learning a representation of the image that separates the identity of an object (such as a human face) from an attribute (such as 'wearing glasses'). We demonstrate the success of our factorization approach by using the learned representation to synthesize the same face with and without a chosen attribute. We refer to this specific synthesis process as image attribute manipulation. We further demonstrate that our model achieves competitive scores, with state of the art, on a facial attribute classification task.", "target": ["学习图像的表征，将单一属性的因素剔除。", "本文在条件性VAE GAN的基础上，允许在合成过程中进行属性操作。", "本文提出了一个生成模型来学习能够将对象的身份与属性分开的表征，并通过增加一个辅助网络来扩展自动编码器的对抗性。"]}
{"source": "Stochastic video prediction models take in a sequence of image frames, and generate a sequence of consecutive future image frames. These models typically generate future frames in an autoregressive fashion, which is slow and requires the input and output frames to be consecutive. We introduce a model that overcomes these drawbacks by generating a latent representation from an arbitrary set of frames that can then be used to simultaneously and efficiently sample temporally consistent frames at arbitrary time-points. For example, our model can \"jump\" and directly sample frames at the end of the video, without sampling intermediate frames. Synthetic video evaluations confirm substantial gains in speed and functionality without loss in fidelity. We also apply our framework to a 3D scene reconstruction dataset. Here, our model is conditioned on camera location and can sample consistent sets of images for what an occluded region of a 3D scene might look like, even if there are multiple possibilities for what that region might contain. Reconstructions and videos are available at https://bit.ly/2O4Pc4R.", "target": ["我们提出了一个用于一致的三维重建和跳跃式视频预测的模型，例如在不产生中间帧的情况下产生未来多个时间步长的图像帧。", "本文提出了一种索引数据建模的通用方法，将索引信息与观测信息一起编码到神经网络中，然后将观测条件解码到目标索引上。", "建议使用一种以排列不变的方式编码输入视频的VAE来预测视频的未来帧。"]}
{"source": "The ADAM optimizer is exceedingly popular in the deep learning community. Often it works very well, sometimes it doesn’t. Why? We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance. We  disentangle these two aspects and analyze them in isolation, shedding light on ADAM ’s inner workings. Transferring the \"variance adaptation” to momentum- SGD gives rise to a novel method, completing the practitioner’s toolbox for problems where ADAM fails.", "target": ["分析流行的Adam优化器", "本文试图通过提出两种算法来改进基于动量的方差适应的Adam。", "本文分析了Adam中使用的学习率的标度不变性和特殊形状，认为Adam的更新是签名更新和基于方差的学习率的结合。", "本文将ADAM算法分成两部分：梯度符号的随机方向和相对方差的自适应步进式，并提出两种算法分别进行测试。"]}
{"source": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "target": ["我们提出了一个新的框架，基于Rademacher复杂度约束，自适应调整深度神经网络的丢弃率。", "作者将丢弃参数与网络的Rademacher复杂性的边界联系起来", "将网络的可学习性的复杂性与反向传播的丢弃率联系起来。"]}
{"source": "Sensor fusion is a key technology that integrates various sensory inputs to allow for robust decision making in many applications such as autonomous driving and robot control. Deep neural networks have been adopted for sensor fusion in a body of recent studies. Among these, the so-called netgated architecture was proposed, which has demonstrated improved performances over the conventional convolu- tional neural networks (CNN). In this paper, we address several limitations of the baseline negated architecture by proposing two further optimized architectures: a coarser-grained gated architecture employing (feature) group-level fusion weights and a two-stage gated architectures leveraging both the group-level and feature- level fusion weights. Using driving mode prediction and human activity recogni- tion datasets, we demonstrate the significant performance improvements brought by the proposed gated architectures and also their robustness in the presence of sensor noise and failures.", "target": ["提出了用于传感器融合的优化门控深度学习架构。", "作者通过提出更粗粒度的门控融合架构和两阶段门控融合架构，改善了基线否定架构的几个局限性", "提出了两个用于传感器融合的门控深度学习架构，通过分组特征，展示了改进的性能，特别是在随机传感器噪声和故障的情况下。"]}
{"source": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "target": ["批量归一化导致vanilla前馈网络的梯度爆炸。", "在具有随机初始化权重的全连接网络中，发展了批量正常化（BN）的平均场理论。", "利用协方差矩阵与各层的演变，为深度神经网络提供了一个动态视角。"]}
{"source": "We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability.   Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.", "target": ["我们训练一个图网络来预测布尔可满足性，并表明它学会了搜索解决方案，而且它找到的解决方案可以从其激活中解码。", "本文描述了一个用于预测可满足性的通用神经网络架构", "本文介绍了NeuroSAT架构，它使用深度信息传递神经网来预测CNF实例的可满足性。"]}
{"source": "Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling. We evaluate the framework on two real-world large-scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines", "target": ["一个在有向图上学习预测的神经序列模型。", "本文提出了用于时空流量预测问题的扩散卷积递归神经网络架构", "建议利用卷积循环神经网络的扩散过程建立流量预测模型，以解决时空自相关的问题。"]}
{"source": "Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge. Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior. In particular, the common practice of a standard normal prior in weight space imposes only weak regularities, causing the function posterior to possibly generalize in unforeseen ways on inputs outside of the training distribution. We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates. The key idea is to train the model to output high uncertainty for data points outside of the training distribution. NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs. NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training. Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning. We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.", "target": ["我们训练神经网络对噪声输入的不确定性，以避免在训练分布之外的过度自信的预测。", "介绍了一种获得神经网络预测不确定性估计的方法，该方法在量化训练分布之外的点的预测不确定性时具有良好的性能。", "本文考虑了神经网络的不确定性估计问题，并提出使用贝叶斯方法与噪声对比性先验"]}
{"source": "Convolutional neural networks (CNNs) were inspired by human vision and, in some settings, achieve a performance comparable to human object recognition. This has lead to the speculation that both systems use similar mechanisms to perform recognition. In this study, we conducted a series of simulations that indicate that there is a fundamental difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias. We teased apart the type of features selected by the model by modifying the CIFAR-10 dataset so that, in addition to containing objects with shape, the images concurrently contained non-shape features, such as a noise-like mask. When trained on these modified set of images, the model did not show any bias towards selecting shapes as features. Instead it relied on whichever feature allowed it to perform the best prediction -- even when this feature was a noise-like mask or a single predictive pixel amongst 50176 pixels. We also found that regularisation methods, such as batch normalisation or Dropout, did not change this behaviour and neither did past or concurrent experience with images from other datasets.", "target": ["这项研究强调了人类视觉和CNN之间的一个关键区别：人类的物体识别依赖于对形状的分析，而CNN则没有这种形状偏差。", "试图通过一系列精心设计的实验来确定，为图像分类而训练的CNN并不像人类视觉那样编码形状偏差。", "本文强调了一个事实，即CNN不一定会学会根据物体的形状来识别物体，并表明它们会过度依赖基于噪声的特征。"]}
{"source": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize.", "target": ["我们描述了一个新颖的多视图生成模型，它可以生成同一物体的多个视图，或者同一视图中的多个物体，而不需要对视图进行标注。", "本文提出了一种基于GAN的图像生成方法，试图将描述图像内容的潜变量与描述视图属性的潜变量分开。", "本文提出了一个GAN架构，旨在将某一特定类别的基础分布分解为 \"内容 \"和 \"视图\"。", "提出了一种基于生成对抗网络（GAN）的新的生成模型，该模型在没有视图监督的情况下将对象的内容和视图分开，并将GMV扩展为一个条件生成模型，该模型接受一个输入图像并生成输入图像中对象的不同视图。"]}
{"source": "The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m > 2) quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.", "target": ["提出了一种直接考虑其对损失的影响的损失感知权重量化算法。", "提出了一种通过权重ternarization来压缩网络的方法。", "本文提出了一种新的方法来训练具有量化权重的DNN，方法是将量化作为近似准牛顿算法的一个约束条件，同时学习量化值的比例。", "本文将损失感知的权重二值化方案扩展到terarization和任意m位量化，并展示了其良好的性能。"]}
{"source": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.   However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.   In this work we develop a novel policy gradient method for the automatic learning of policies with options.   This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "target": ["我们开发了一种新的策略梯度方法，用于使用可微分推理步骤自动学习带有选项的策略。", "本文提出了一种新的学习选项策略梯度技术，即可以用一个样本来更新所有选项。", "提出了一种在复杂连续问题中学习选择的非策略性方法。"]}
{"source": "The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data. The proposed approach, called Locally Linear Unsupervised Feature Selection, relies on a dimensionality reduction method to characterize such patterns; each feature is thereafter assessed according to its compliance w.r.t. the local patterns, taking inspiration from Locally Linear Embedding (Roweis and Saul, 2000). The experimental validation of the approach on the scikit-feature benchmark suite demonstrates its effectiveness compared to the state of the art.", "target": ["通过捕捉数据的局部线性结构进行无监督的特征选择", "提出了局部线性无监督的特征选择。", "本文提出了LLUFS方法进行特征选择。"]}
{"source": "Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply \"mix-and-match\" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the \"dax\" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.", "target": ["使用一个简单的语言驱动的导航任务，我们研究了现代seq2seq循环网络的组成能力。", "本文重点讨论了现代序列到序列RNN的零样本学习构成能力，并暴露了目前seq2seq RNN架构的不足之处。", "本文分析了RNN的构成能力，特别是RNN在随机的SCAN命令子集上的泛化能力，在较长的SCAN命令上的泛化能力，以及对原始命令的构成能力。", "作者介绍了一个新的数据集，便于分析一个Seq2Seq学习案例"]}
{"source": "This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions. First, we demonstrate how  Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning. Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism. We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems. The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.", "target": ["我们解决了结构化对象的相似性学习问题，特别是在计算机安全方面的应用，并提出了一个新的模型图匹配网络，在这个任务上表现出色。", "作者介绍了一个图形匹配网络，用于检索和匹配图形结构的对象。", "作者通过提出一个图形嵌入网络的扩展来解决图形匹配的问题", "作者提出了两种学习图对之间相似性分数的方法，并展示了将图匹配的思想引入图神经网络的好处。"]}
{"source": "Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder, and we show that neither an autoregressive decoder nor an RNN decoder is required.   We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.", "target": ["我们提出了一个RNN-CNN编码器-解码器模型，用于快速无监督的句子表征学习。", "对学习句子嵌入的跳过思考框架的修改。", "本文提出了一种新的RNN编码器和CNN解码器混合设计，用于预训练，在预训练编码器时不需要自回归解码器。", "作者通过使用CNN解码器只对一个目标句子进行解码来扩展Skip-thought。"]}
{"source": "Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs). VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function. GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data. The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems. In this work, we show that an optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average sample likelihoods, an approach that VAEs are based on. In particular, our proof constructs an explicit probability model for GANs that can be used to compute likelihood statistics within GAN's framework. Our numerical results on several datasets demonstrate consistent trends with the proposed theory.", "target": ["计算生成式对抗网络中样本可能性的统计方法", "证明具有熵正则化的WGAN能使观察到的数据分布的可能性达到最大的下限。", "作者声称有可能利用熵正则化最优运输的上限，得出一个 \"样本可能性 \"的测量。"]}
{"source": "We introduce geomstats, a Python package for Riemannian modelization and optimization over manifolds such as hyperspheres, hyperbolic spaces, SPD matrices or Lie groups of transformations. Our contribution is threefold. First, geomstats allows the flexible modeling of many a machine learning problem through an efficient and extensively unit-tested implementations of these manifolds, as well as the set of useful Riemannian metrics, exponential and logarithm maps that we provide. Moreover, the wide choice of loss functions and our implementation of the corresponding gradients allow fast and easy optimization over manifolds. Finally, geomstats is the only package to provide a unified framework for Riemannian geometry, as the operations implemented in geomstats are available with different computing backends (numpy,tensorflow and keras), as well as with a GPU-enabled mode–-thus considerably facilitating the application of Riemannian geometry in machine learning. In this paper, we present geomstats through a review of the utility and advantages of manifolds in machine learning, using the concrete examples that they span to show the efficiency and practicality of their implementation using our package", "target": ["我们介绍geomstats，这是一个高效的Python包，用于流形上的黎曼建模和优化，与numpy和tensorflow都兼容。", "本文介绍了软件包geomstats，它提供了在机器学习模型中对黎曼流形和度量的简单使用。", "提出了一个用于黎曼流形上的优化和应用的Python包，并强调了Geomstats包与其他包的区别。", "介绍了一个几何工具箱Geomstats，用于黎曼流形上的机器学习。"]}
{"source": "We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference. The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices. However, most of the previous studies optimize for inference while neglect training or even complicate it. Training is far more intractable, since (i) the neurons dominate the memory cost rather than the weights in inference; (ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid; (iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity. To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimensionreduction search and obtains the BN compatibility via a double-mask selection. Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks.", "target": ["我们通过降维搜索构建动态稀疏图，以减少DNN训练和推理的计算和内存成本。", "作者提出使用动态稀疏计算图来减少深度神经网络（DNN）的计算内存和时间成本。", "本文提出了一种利用计算图的动态修剪来加快深度神经网络的训练和推理的方法。"]}
{"source": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.", "target": ["我们开发了强化学习的信息导向抽样的实际扩展，它考虑了参数的不确定性和回报分布的异方差，以进行探索。", "作者提出了一种将信息导向采样扩展到强化学习的方法，即结合两种不确定性，得到一种基于IDS的简单探索策略。", "本文研究了建立在信息直接取样和分布式强化学习基础上的强化学习的复杂探索方法。"]}
{"source": "We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.", "target": ["使用图谱神经网络对代理人的结构信息进行建模，以改善策略和可转移性", "一种使用图谱神经网络表示和学习连续控制任务的结构化策略的方法", "呈文提出将额外的结构纳入强化学习问题，特别是代理的形态结构", "提出图形神经网络在学习控制不同长度的 \"蜈蚣 \"机器人的策略方面的应用。"]}
{"source": "Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling.    In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy.   Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.", "target": ["本文提出了一个基于确定性选项策略和相互信息最大化的分层强化学习框架。", "提出了一种HRL算法，该算法试图学习在最优政策下与状态行动密度相互信息最大化的选项。", "本文提出了一个HRL系统，其中潜变量和状态-行动对的突变信息近似最大化。", "提出了一个旨在使选项和状态动作对之间的相互信息最大化的标准，并通过经验表明，学到的选项分解了状态动作空间，但没有分解状态空间。"]}
{"source": "Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.", "target": ["我们介绍并验证了分层局部解释，这是第一个自动搜索并显示LSTM和CNN所做的单个预测的重要交互作用的技术。", "通过学习输入特征组的分层表征及其对最终预测的贡献来解释神经网络预测的一种新方法", "将现有的LSTM的特征解释方法扩展到更通用的DNN，并引入了输入特征的分层聚类以及每个聚类对最终预测的贡献。", "本文提出了背景分解的分层扩展。"]}
{"source": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "target": ["我们提出了一种易于实现但有效的神经网络压缩方法。PFA利用网络层内滤波器响应之间的内在关联性来推荐一个更小的网络足迹。", "建议通过分析同一层的过滤器之间观察到的相关性，即通过其协方差矩阵的特征值谱表示的相关性，来修整卷积网络。", "本文介绍了一种压缩神经网络的方法，即通过两种策略考察每一层中滤波器响应的相关性。", "本文提出了一种基于频谱分析的压缩方法"]}
{"source": "We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering. In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer. Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set. Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data. We show that the improved performance is due to the increased diversity of reformulation strategies.", "target": ["用强化学习训练的多种多样的查询重组代理，以改善搜索引擎。", "在查询重构的重构学习中，集合方法的并行化，加快了训练速度，提高了学习到的自由格式的多样性。", "作者提议训练多个不同的代理，每个代理在训练集的不同子集上训练。", "作者提出了一种用于查询重构的集合方法"]}
{"source": "Network Embeddings (NEs) map the nodes of a given network into $d$-dimensional Euclidean space $\\mathbb{R}^d$. Ideally, this mapping is such that 'similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if 'similar' means being 'more likely to be connected') or classification (if 'similar' means 'being more likely to have the same label'). In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes.   A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc. To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \\emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.). We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently.   We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity. Finally, we illustrate the potential of CNE for network visualization.", "target": ["我们引入了一种网络嵌入方法，它考虑了网络的先验信息，产生了卓越的实证性能。", "该论文提出使用先验分布来约束网络嵌入，对于该表述，该论文使用了非常有限的高斯分布。", "提出了通过考虑网络的结构特性来学习无监督的节点嵌入。"]}
{"source": "This paper studies a class of adaptive gradient based momentum algorithms that update the  search directions and learning rates simultaneously using past gradients. This class, which we refer to as the ''``Adam-type'', includes the popular algorithms such as Adam, AMSGrad, AdaGrad. Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving  non-convex problems remains an open question. In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order   $O(\\log{T}/\\sqrt{T})$ for non-convex stochastic optimization. Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum). We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge. Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily  monitor the progress of algorithms and determine their convergence behavior.", "target": ["我们分析了亚当型算法的收敛性，并提供了保证其收敛性的温和的充分条件，我们还表明违反条件会使算法发生发散。", "介绍了在非凸环境下对一系列优化算法的收敛性分析。", "本文研究了亚当型优化器在无约束的非凸优化问题中的收敛条件。"]}
{"source": "This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data. These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data. The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required. This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture. Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.", "target": ["以abs函数为激活函数的捆绑权重自动编码器，由于特别定义的成本函数，学会了在前向进行分类，在后向进行回归。", "本文提出在自动编码器结构中使用绝对值激活函数，在目标函数中增加监督学习项", "本文介绍了一个以绝对值作为激活函数的可逆网络。"]}
{"source": "Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step. Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages. Similarly, pre-processing introduces an additional source of error. To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018]. Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions. TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task. TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively. Furthermore, we observe a significant increase in sample efficiency. With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset. We open-source our trained models, experiments, and source code.", "target": ["我们提出了一个基于Transformer的关系提取模型，该模型使用预先训练好的语言表征，而不是明确的语言学特征。", "提出了一个基于转化器的关系提取模型，该模型利用了对未标记文本的预训练，具有语言建模的目标。", "本文介绍了Transformer网络在关系提取方面的一种新的应用。", "本文提出了一个基于变形器的松弛提取架构，在两个数据集上进行了评估。"]}
{"source": "Neural networks have recently had a lot of success for many tasks. However, neural  network architectures that perform well are still typically designed manually  by experts in a cumbersome trial-and-error process. We propose a new method  to automatically search for well-performing CNN architectures based on a simple  hill climbing procedure whose operators apply network morphisms, followed  by short optimization runs by cosine annealing. Surprisingly, this simple method  yields competitive results, despite only requiring resources in the same order of  magnitude as training a single network. E.g., on CIFAR-10, our method designs  and trains networks with an error rate below 6% in only 12 hours on a single GPU;  training for one day reduces this error further, to almost 5%.", "target": ["我们提出了一种简单而有效的卷积神经网络架构搜索方法。", "提出了一种神经结构搜索方法，在CIFAR10上达到了接近最先进的准确度，而且占用的计算资源少得多。", "提出了一种在训练的同时搜索神经网络架构的方法，极大地节省了训练时间和架构搜索时间。", "提出了使用网络形态的神经架构搜索的变体，以定义使用CNN架构完成CIFAR图像分类任务的搜索空间"]}
{"source": "We propose GraphGAN - the first implicit generative model for graphs that enables to mimic real-world networks.  We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph.  Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction as well as promising results on node classification, even though not specifically trained for these tasks.", "target": ["使用GAN通过随机漫步生成图。", "作者提出了一个图上随机漫步的生成模型，该模型可以进行模型诊断学习、可控拟合、集合图生成", "提出了一个WGAN公式，用于生成基于随机漫步的图，使用节点嵌入和LSTM架构进行建模。"]}
{"source": "The ability of a classifier to recognize unknown inputs is important for many classification-based systems. We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes. We propose a method based on the Generative Adversarial Networks (GAN) framework. We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector. We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection. Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection.", "target": ["我们提议在GAN框架内解决一个同时进行分类和新奇性检测的问题。", "提出了一个GAN来统一分类和新颖性检测。", "本文提出了一种基于多类GAN的新颖性检测方法，该方法被训练为输出由名义分布和新颖分布的混合物生成的图像。", "本文提出了一个使用混合物生成器与特征匹配损失进行新颖性检测的GAN。"]}
{"source": "Verifying a person's identity based on their voice is a challenging, real-world problem in biometric security. A crucial requirement of such speaker verification systems is to be domain robust. Performance should not degrade even if speakers are talking in languages not seen during training. To this end, we present a flexible and interpretable framework for learning domain invariant speaker embeddings using Generative Adversarial Networks. We combine adversarial training with an angular margin loss function, which encourages the speaker embedding model to be discriminative by directly optimizing for cosine similarity between classes. We are able to beat a strong baseline system using a cosine distance classifier and a simple score-averaging strategy. Our results also show that models with adversarial adaptation perform significantly better than unadapted models. In an attempt to better understand this behavior, we quantitatively measure the degree of invariance induced by our proposed methods using Maximum Mean Discrepancy and Frechet distances. Our analysis shows that our proposed adversarial speaker embedding models significantly reduce the distance between source and target data distributions, while performing similarly on the former and better on the latter.", "target": ["通过使用生成对抗网络使模型适应域内数据，可以显著提高说话人的验证性能。此外，这种适应可以以无监督的方式进行。", "在领域不匹配条件下的说话人识别任务上提出了一些GAN变体。"]}
{"source": "Learning disentangling representations of the independent factors of variations that explain the data in an unsupervised setting is still a major challenge. In the following paper we address the task of disentanglement and introduce a new state-of-the-art approach called Non-synergistic variational Autoencoder (Non-Syn VAE). Our model draws inspiration from population coding, where the notion of synergy arises when we describe the encoded information by neurons in the form of responses from the stimuli. If those responses convey more information together than separate as independent sources of encoding information, they are acting synergetically. By penalizing the synergistic mutual information within the latents we encourage information independence and by doing that disentangle the latent factors. Notably, our approach could be added to the VAE framework easily, where the new ELBO function is still a lower bound on the log likelihood. In addition, we qualitatively compare our model with Factor VAE and show that this one implicitly minimises the synergy of the latents.", "target": ["使用VAE框架使潜在和数据内的协同互信息最小化，以完成拆分的任务。", "提出了一个新的目标函数，通过最小化所提供信息的协同作用，在变分框架中学习解缠表征。", "作者的目标是训练一个以 \"协同 \"的最大方式拆分潜伏表征的VAE。 ", "本文提出了一种新的方法来强制执行VAE中的disentanglement，使用的术语是惩罚潜变量之间的协同互信息。"]}
{"source": "Metric embeddings are   immensely useful representations of associations between entities   (images, users, search queries, words, and more).   Embeddings are learned by  optimizing a loss objective of the general form of a sum over example associations. Typically, the optimization uses stochastic gradient updates over minibatches of examples that are arranged  independently at random. In this work, we propose the use of {\\em structured arrangements} through randomized {\\em microbatches} of examples that are more likely to include similar ones. We make a principled argument for the properties of our arrangements  that accelerate the training and present efficient algorithms to generate microbatches that respect the marginal  distribution of training examples.   Finally, we observe experimentally that our structured arrangements accelerate training by 3-20\\%. Structured arrangements emerge as a powerful and novel performance knob for SGD that is independent and complementary to other SGD  hyperparameters and thus is a candidate for wide deployment.", "target": ["通过安排不同的例子来加速SGD", "本文提出了一种通过将相似的训练样本分组来提高随机梯度下降法学习嵌入的收敛率的方法。", "提出了一种非均匀抽样策略，在SGD中为学习对象关联的嵌入任务构建迷你批。"]}
{"source": "Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end deep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is  the large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.   We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags.", "target": ["结合预先建立的单词嵌入和特定任务的单词表示之间的信息，以解决词汇量不足的问题", "本文提出了一种改进词汇外嵌入预测的方法，用于模拟对话会话的任务，比基线有相当大的收益。", "建议将外部预训练的词嵌入和训练数据上的预训练词嵌入结合起来，将它们作为两个视图保留。", "提出了扩大预训练词嵌入覆盖范围的方法，以处理将其应用于对话数据集时出现的OOV问题，并将基于LSTM模型的新变体应用于对话建模中的反应选择任务。"]}
{"source": "Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned update functions may similarly outperform current hand-designed optimizers, especially for specific tasks. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process. The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. This allows us to train neural networks to perform optimization faster than well tuned first-order methods. Moreover, by training the optimizer against validation loss, as opposed to training loss, we are able to use it to train models which generalize better than those trained by first order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks in a fifth of the wall-clock time compared to tuned first-order methods, and with an improvement", "target": ["我们分析了训练学习型优化器时的问题，通过使用两个互补的梯度估计器的变分优化来解决这些问题，并训练出比基线优化器（如Adam）快5倍壁钟时间的优化器。", "本文使用非滚动优化来学习神经网络进行优化。", "本文解决了学习优化器的问题，特别是作者专注于从未滚动的训练程序中获得更清洁的梯度。", "提出了一种通过对 \"外部 \"优化器损失使用变分优化来 \"学习优化器 \"的方法，并提出了将变分目标的再参数化梯度和得分函数估计器结合起来的想法，并使用高斯乘积公式对均值进行加权。"]}
{"source": "Asynchronous distributed gradient descent algorithms for training of deep neural  networks are usually considered as inefficient, mainly because of the Gradient delay  problem. In this paper, we propose a novel asynchronous distributed algorithm  that tackles this limitation by well-thought-out averaging of model updates, computed  by workers. The algorithm allows computing gradients along the process  of gradient merge, thus, reducing or even completely eliminating worker idle time  due to communication overhead, which is a pitfall of existing asynchronous methods.  We provide theoretical analysis of the proposed asynchronous algorithm,  and show its regret bounds. According to our analysis, the crucial parameter for  keeping high convergence rate is the maximal discrepancy between local parameter  vectors of any pair of workers. As long as it is kept relatively small, the  convergence rate of the algorithm is shown to be the same as the one of a sequential  online learning. Furthermore, in our algorithm, this discrepancy is bounded  by an expression that involves the staleness parameter of the algorithm, and is  independent on the number of workers. This is the main differentiator between  our approach and other solutions, such as Elastic Asynchronous SGD or Downpour  SGD, in which that maximal discrepancy is bounded by an expression that  depends on the number of workers, due to gradient delay problem. To demonstrate  effectiveness of our approach, we conduct a series of experiments on image  classification task on a cluster with 4 machines, equipped with a commodity communication  switch and with a single GPU card per machine. Our experiments  show a linear scaling on 4-machine cluster without sacrificing the test accuracy,  while eliminating almost completely worker idle time. Since our method allows  using commodity communication switch, it paves a way for large scale distributed  training performed on commodity clusters.", "target": ["一种高效的深度学习模型异步分布式训练方法，以及理论上的遗憾界限。", "本文提出了一种限制异步SGD中滞后性的算法，并提供了理论分析", "提出了一种混合算法来消除异步方法的梯度延迟。"]}
{"source": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack.", "target": ["我们设计了一种新颖的量化方法，以共同优化深度学习模型的效率和稳健性。", "提出了一种正则化方案，利用内层输出的Lipschitz常数过滤来保护量化的神经网络免受对抗性攻击。"]}
{"source": "Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/.", "target": ["对现有的RNN架构进行修改，使其能够跳过状态更新，同时保留了原始架构的性能。", "提出了跳过RNN模型，该模型允许循环网络有选择地跳过更新某些输入的隐藏状态，从而减少测试时的计算。", "提出了一种新的RNN模型，其中循环单元的输入和状态更新在某些时间段内被自适应地跳过。"]}
{"source": "We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers. Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement. Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step. Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network. This estimate has the same size and is similar to the momentum variable that is commonly used in SGD. No estimate of the Hessian is maintained.  We first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle. We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning. We also show our optimiser's generality by testing on a large set of randomly-generated architectures.", "target": ["用于深度学习的快速二阶求解器，可用于ImageNet规模的问题，无需进行超参数调整", "通过使用单步梯度下降 \"朝向牛顿步 \"从最初的估计值选择方向，然后采取这个方向而不是原始梯度", "一种新的近似二阶优化方法，计算成本低，用单一梯度步骤和热启动策略取代了Hessian矩阵的计算。"]}
{"source": "The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.", "target": ["用REINFORCE训练的基于注意力的模型，以贪婪的推出为基线，学习启发式方法，在TSP和其他路由问题上取得有竞争力的结果。", "介绍了一种基于注意力的方法来学习解决TSP和其他路由型组合优化问题的策略。", "本文试图学习用于解决组合优化问题的启发式方法"]}
{"source": "We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters. While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks. Finally, we show its effectiveness on two distinct tasks.", "target": ["训练期间优化正则化超参数的算法", "本文提出了一种在每次更新lambda时重新初始化y的方法和y的剪切程序，以保持动态系统的稳定性。", "提出了一种超参数优化的算法，可以看作是Franceschi 2017年的扩展，是对一些估计值进行了热重启，以提高方法的稳定性。", "提出了对现有方法的扩展，以优化正则化超参数。"]}
{"source": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.", "target": ["表明LSTM对LM来说和最近的创新一样好，甚至更好，而模型评估往往是不可靠的。", "本文描述了对基于LSTM的单词和字符语言模型的全面验证，导致了语言建模的重大成果，也是深度学习的一个里程碑。"]}
{"source": "Residual and skip connections play an important role in many current   generative models. Although their theoretical and numerical advantages   are understood, their role in speech enhancement systems has not been   investigated so far. When performing spectral speech enhancement,   residual connections are very similar in nature to spectral subtraction,   which is the one of the most commonly employed speech enhancement approaches.    Highway networks, on the other hand, can be seen as a combination of spectral   masking and spectral subtraction. However, when using deep neural networks, such operations would   normally happen in a transformed spectral domain, as opposed to traditional speech   enhancement where all operations are often done directly on the spectrum.    In this paper, we aim to investigate the role of residual and highway   connections in deep neural networks for speech enhancement, and verify whether   or not they operate similarly to their traditional, digital signal processing   counterparts. We visualize the outputs of such connections, projected back to   the spectral domain, in models trained for speech denoising, and show that while   skip connections do not necessarily improve performance with regards to the   number of parameters, they make speech enhancement models more interpretable.", "target": ["我们展示了使用跳过连接可以使语音增强模型更具可解释性，因为它使模型使用了DSP文献中已经探索过的类似机制。", "作者提出在完全卷积管道内加入残差、Highway和Masking block，以了解在语音增强任务中如何进行输出和掩码的迭代推理。", "作者解释了highway, residual和masking的联系。", "作者通过人为地将噪音从一个完善的噪音数据集添加到一个不太为人所知的干净的语音数据集上，产生了他们自己的噪音语音。"]}
{"source": "Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.", "target": ["为有效训练贝叶斯神经网络而消除梯度方差和自动调整预案的方法", "提出了一种新的方法，通过近似层状矩，对具有特定非线性激活函数的前馈BNN进行确定性的变分推理。", "本文考虑了一种纯粹的确定性方法来学习贝叶斯神经网络的变分后验近似值。"]}
{"source": "Skills learned through (deep) reinforcement learning often generalizes poorly  across tasks and re-training is necessary when presented with a new task. We  present a framework that combines techniques in formal methods with reinforcement  learning (RL) that allows for the convenient specification of complex temporal  dependent tasks with logical expressions and construction of new skills from existing  ones with no additional exploration. We provide theoretical results for our  composition technique and evaluate on a simple grid world simulation as well as  a robotic manipulation task.", "target": ["强化学习任务中技能构成的正式方法方法", "本文通过设置scTLTL公式的自动化，将RL和由逻辑公式表达的约束结合起来。", "提出了一种有助于从学习到的子任务中构建策略的方法，其主题是将RL任务与线性时间逻辑公式相结合。"]}
{"source": "The application of multi-modal generative models by means of a Variational Auto Encoder (VAE) is an upcoming research topic for sensor fusion and bi-directional modality exchange.  This contribution gives insights into the learned joint latent representation and shows that expressiveness and coherence are decisive properties for multi-modal datasets.  Furthermore, we propose a multi-modal VAE derived from the full joint marginal log-likelihood that is able to learn the most meaningful representation for ambiguous observations.  Since the properties of multi-modal sensor setups are essential for our approach but hardly available, we also propose a technique to generate correlated datasets from uni-modal ones.", "target": ["从联合边际对数似然推导出多模态VAE的一般表述。", "提出了一个多模态的VAE，其变分约束来自于连锁规则。", "本文提出了一个针对多模态VAE的目标，即M^2VAE，它应该是学习一个更有意义的潜在空间表示。"]}
{"source": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "target": ["我们在自动编码顺序蒙特卡洛的基础上，获得了新的理论见解，并在这些见解的基础上开发了一个改进的训练程序。", "本文提出了一个IWAE式训练的版本，使用SMC代替经典的重要性采样。", "这项工作提出了自动编码的顺序蒙特卡洛（SMC），将VAE框架扩展到基于SMC的新蒙特卡洛目标。"]}
{"source": "A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control. Many of the algorithms for learning values, however, are designed for linear function approximation---with a fixed basis or fixed representation. Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning. In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale. The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates. We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation. We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control.", "target": ["我们提出了一个学习价值函数的架构，它允许使用任何线性策略评估算法与非线性特征学习相配合。", "本文提出了一个学习价值函数的双时间尺度框架，以及一个完全由非线性近似器构成的状态表征。", "本文提出了双时间尺度网络（TTNs），并使用来自两个时间尺度随机近似的方法证明了这种方法的收敛性。", "本文提出了一个双时间尺度网络（TTN），使线性方法可以用来学习数值。"]}
{"source": "Large-scale Long Short-Term Memory (LSTM) cells are often the building blocks of many state-of-the-art algorithms for tasks in Natural Language Processing (NLP). However, LSTMs are known to be computationally inefficient because the memory capacity of the models depends on the number of parameters, and the inherent recurrence that models the temporal dependency is not parallelizable. In this paper, we propose simple, but effective, low-rank matrix factorization (MF) algorithms to compress network parameters and significantly speed up LSTMs with almost no loss of performance (and sometimes even gain). To show the effectiveness of our method across different tasks, we examine two settings: 1) compressing core LSTM layers in Language Models, 2) compressing biLSTM layers of ELMo~\\citep{ELMo} and evaluate in three downstream NLP tasks (Sentiment Analysis, Textual Entailment, and Question Answering). The latter is particularly interesting as embeddings from large pre-trained biLSTM Language Models are often used as contextual word representations. Finally, we discover that matrix factorization performs better in general, additive recurrence is often more important than multiplicative recurrence, and we identify an interesting correlation between matrix norms and compression performance.", "target": ["我们提出了简单而有效的低秩矩阵分解（MF）算法，以加快运行时间，节省内存，并提高LSTM的性能。", "建议通过使用MF作为后处理压缩策略来加速LSTM，并进行了大量的实验来展示其性能。"]}
{"source": "Manipulation and re-use of images in scientific publications is a recurring problem, at present lacking a scalable solution.   Existing tools for detecting image duplication are mostly manual or semi-automated, despite the fact that generating data for a learning-based approach is straightforward, as we here illustrate. This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of certain geometric and statistical manipulations, e.g. copy, rotation, translation, scale, perspective transform, histogram adjustment, partial erasing, and compression artifacts. We propose a solution based on a 3-branch Siamese Convolutional Neural Network. The ConvNet model is trained to map images into a 128-dimensional space, where the Euclidean distance between duplicate (respectively, unique) images is no greater (respectively, greater) than 1. Our results suggest that such an approach can serve as tool to improve surveillance of the published and in-peer-review literature for image manipulation. We also show that as a byproduct the network learns useful representations for semantic segmentation, with performance comparable to that of domain-specific models.", "target": ["一个法医指标，以确定一个给定的图像是否是来自给定数据集的另一个图像的副本（可能被操纵）。", "介绍了识别重复和复制/修改图像的siamese网络，可用于改善对已发表和同行评议文献的监控。", "本文介绍了深度卷积网络在重复图像检测任务中的应用", "这项工作解决了从生物医学出版物中寻找重复/接近重复的图像的问题，提出了一个标准的CNN和损失函数，并将其应用于这个领域。"]}
{"source": "Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training.   Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We  demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.", "target": ["通过使用判别器阵列在高维度上进行稳定的GAN训练，每个判别器都有生成样本的低维视图", "本文提出通过使用鉴别器的集合来稳定GAN的训练，每个鉴别器在输入数据的随机投影上工作，为生成器模型提供训练信号。", "本文提出了一种提高训练稳定性的GAN训练方法。", "本文提出了一种新的GAN训练方法，它提供了稳定的梯度来训练发生器。"]}
{"source": "We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short). Inclusion relations among N-balls implicitly encode subordinate relations among categories. The similarity measurement in terms of the cosine function is enriched by category information. Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved. A new benchmark data set is created for validating the category of unknown words. Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words. Source codes and data-sets are free for public access \\url{https://github.com/gnodisnait/nball4tree.git} and \\url{https://github.com/gnodisnait/bp94nball.git}.", "target": ["我们展示了一种几何方法，将类别树信息完美地编码到预先训练好的词嵌入中。", "本文提出了分类数据的N球嵌入，其中N球是一对中心点向量和离中心点的半径。", "本文介绍了一种调整现有分类对象（如单词）的向量嵌入的方法，将其转换为遵循层次结构的球状嵌入。", "专注于调整预训练的词嵌入，使其通过适当的n-ball封装来尊重hypernymy/hyponymy关系。"]}
{"source": "For the challenging semantic image segmentation task the best performing models  have traditionally combined the structured modelling capabilities of Conditional  Random Fields (CRFs) with the feature extraction power of CNNs. In more recent  works however, CRF post-processing has fallen out of favour. We argue that this  is mainly due to the slow training and inference speeds of CRFs, as well as the  difficulty of learning the internal CRF parameters. To overcome both issues we  propose to add the assumption of conditional independence to the framework of  fully-connected CRFs. This allows us to reformulate the inference in terms of  convolutions, which can be implemented highly efficiently on GPUs.Doing so  speeds up inference and training by two orders of magnitude. All parameters of  the convolutional CRFs can easily be optimized using backpropagation. Towards  the goal of facilitating further CRF research we have made our implementations  publicly available.", "target": ["我们提出卷积CRF是全连接CRF的快速、强大和可训练的替代方案。", "作者用一个空间变化的卷积核取代了permutohedral网格中的大型过滤步骤，并表明推理更有效，训练更容易。", "建议在截断的高斯核CRF上进行消息传递，使用定义的内核和GPU上的并行化消息传递。"]}
{"source": "Deep Learning NLP domain lacks procedures for the analysis of model robustness. In this paper we propose a framework which validates robustness of any Question Answering model through model explainers. We propose that output of a robust model should be invariant to alterations that do not change its semantics. We test this property by manipulating question in two ways: swapping important question word for 1) its semantically correct synonym and 2) for word vector that is close in embedding space. We estimate importance of words in asked questions with Locally Interpretable Model Agnostic Explanations method (LIME). With these two steps we compare state-of-the-art Q&A models. We show that although accuracy of state-of-the-art models is high, they are very fragile to changes in the input. We can choose architecture that is more immune to attacks and thus more robust and stable in production environment. Morevoer, we propose 2 adversarial training scenarios which raise model sensitivity to true synonyms by up to 7% accuracy measure. Our findings help to understand which models are more stable and how they can be improved. In addition, we have created and published a new dataset that may be used for validation of robustness of a Q&A model.", "target": ["我们提出了一种与模型无关的方法来验证问答系统的稳健性，并在最先进的问答模型上展示结果。", "解决了问题回答中对对抗性信息的稳健性问题。", "提高机器理解/问题回答的稳健性。"]}
{"source": "In this paper, we propose a mix-generator generative adversarial networks (PGAN) model that works in parallel by mixing multiple disjoint generators to approximate a complex real distribution. In our model, we propose an adjustment component that collects all the generated data points from the generators, learns the boundary between each pair of generators, and provides error to separate the support of each of the generated distributions. To overcome the instability in a multiplayer game, a shrinkage adjustment component method is introduced to gradually reduce the boundary between generators during the training procedure. To address the linearly growing training time problem in a multiple generators model, we propose a method to train the generators in parallel. This means that our work can be scaled up to large parallel computation frameworks. We present an efficient loss function for the discriminator, an effective adjustment component, and a suitable generator. We also show how to introduce the decay factor to stabilize the training procedure. We have performed extensive experiments on synthetic datasets, MNIST, and CIFAR-10. These experiments reveal that the error provided by the adjustment component could successfully separate the generated distributions and each of the generators can stably learn a part of the real distribution even if only a few modes are contained in the real distribution.", "target": ["多发生器捕捉Pdata，解决竞争和一锤定音的问题", "提出了并行的GAN，通过多个弱生成器的组合来避免GAN的模式崩溃。"]}
{"source": "We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images. The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images. We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image. However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects. Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.", "target": ["利用图像的组成结构和生成模型进行弱监督的图像分割。", "本文创建了一个分层表示法，以便更好地从未标记的图像中学习分割。", "本文提出了一个基于GAN的生成模型，该模型将图像分解成多个层次，其中GAN的目标是将真实的图像和由各层组合形成的图像区分开来。", "本文围绕分层场景构成的理念提出了一个神经网络架构"]}
{"source": "Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.", "target": ["我们提出了一个证明鲁棒性保证的几何框架，并强调了在对抗性例子中余维度的重要性。", "本文对对抗性例子进行了理论分析，表明在不同的范数下存在稳健性的权衡，对抗性训练的样本效率很低，而最近的邻居分类器在某些条件下可以是稳健的。"]}
{"source": "Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.   Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.", "target": ["CharNMT是脆性的", "本文研究了字符级噪声对4种不同的神经机器翻译系统的影响", "本文实证研究了字符级NMT系统在面对字符级噪声时的表现，包括合成的和自然的。", "本文研究了噪声输入对机器翻译的影响，并测试了使NMT模型更加稳健的方法。"]}
{"source": "As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.", "target": ["我们通过使用组合优化设置隐藏单元目标和凸优化设置权重来学习硬阈值单元的深度网络，从而提高了在ImageNet上的性能。", "本文解释并概括了学习硬激活神经网的方法。", "本文研究了优化硬阈值单元的深度网络问题。", "本文讨论了具有硬阈值的神经网络的优化问题，并提出了一个具有启发式/近似式集合的新颖解决方案。"]}
{"source": "The robust and efficient recognition of visual relations in images is a hallmark of biological vision. Here, we argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity. We further show that another type of feedforward network, called a relational network (RN), which was shown to successfully solve seemingly difficult visual question answering (VQA) problems on the CLEVR datasets, suffers similar limitations. Motivated by the comparable success of biological vision, we argue that feedback mechanisms including working memory and attention are the key computational components underlying abstract visual reasoning.", "target": ["我们使用一个新的、受控的、视觉关系的挑战，表明相同-不同的任务严重地限制了CNN的能力；我们认为，视觉关系可以用注意力记忆策略更好地解决。", "通过在人工生成的视觉关系数据上训练网络，证明卷积和关系神经网络无法解决视觉关系问题。", "本文探讨了目前的CNN和关系网络如何无法识别图像中的视觉关系。"]}
{"source": "Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations. Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios. However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the tracker’s generalization on the unseen object moving patterns. To learn a robust tracker for VAT, in this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT. In AD-VAT, both the tracker and the target are approximated by end-to-end neural networks, and are trained via RL in a dueling/competitive manner: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker. They are asymmetric in that the target is aware of the tracker, but not vice versa. Specifically, besides its own observation, the target is fed with the tracker’s observation and action, and learns to predict the tracker’s reward as an auxiliary task. We show that such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker. To stabilize the training, we also propose a novel partial zero-sum reward for the tracker/target. The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios. For supplementary videos, see: https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS   The code is available at https://github.com/zfw1226/active_tracking_rl", "target": ["我们提出了AD-VAT，其中跟踪器和目标对象被看作是两个可学习的代理，它们是对手，在训练中可以相互增强。", "这项工作的目的是用一种训练机制来解决视觉主动跟踪问题，在这种机制中，跟踪者和目标作为相互的对手。", "本文提出了一个简单的多代理深度RL任务，一个移动的跟踪器试图跟踪一个移动的目标。", "提出了一个新的奖励函数--\"部分零和\"，它只在跟踪者和目标的竞争接近时鼓励他们，在他们太远时惩罚他们。"]}
{"source": "Identifying the hypernym relations that hold between words is a fundamental task in NLP. Word embedding methods have recently shown some capability to encode hypernymy. However, such methods tend not to explicitly encode the hypernym hierarchy that exists between words. In this paper, we propose a method to learn a hierarchical word embedding in a speciﬁc order to capture the hypernymy. To learn the word embeddings, the proposed method considers not only the hypernym relations that exists between words on a taxonomy, but also their contextual information in a large text corpus. The experimental results on a supervised hypernymy detection and a newly-proposed hierarchical path completion tasks show the ability of the proposed method to encode the hierarchy. Moreover, the proposed method outperforms previously proposed methods for learning word and hypernym-speciﬁc word embeddings on multiple benchmarks.", "target": ["我们提出了一种利用语料库和分类法联合学习层次词嵌入（HWE）的方法，用于识别词之间的超义关系。", "本文提出了一种利用共同发生统计学以及结合语义网络的层次信息来联合学习词嵌入的方法。", "本文提出了一种从原始文本和监督分类法数据中联合学习超词的方法。", "本文建议在GloVE目标中增加一个 \"分布性包容 \"差异的测量，以表示超词关系。"]}
{"source": "While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures. Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance. Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule. Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems.", "target": ["分层神经网络中自组织和监督学习的整合", "本文讨论了在一个有三层的神经网络中的学习，其中中间层是按拓扑组织的，并研究了生物背景下无监督学习和分层监督学习之间的相互作用。", "科霍宁自组织地图（SOM）的监督变体，但线性输出层被一个具有交叉熵的softmax层取代了平方误差。", "提出了一个使用具有自组织激活函数的隐性神经元的模型，其输出反馈给具有softmax输出函数的分类器。"]}
{"source": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "target": ["精密公路；4位以下量化的高精度信息流的一般概念", "研究神经网络量化问题，采用端到端的精度高速公路，减少累积量化误差，实现深度神经网络的超低精度。", "本文研究改善量化神经网络性能的方法", "本文提出在ResNet和LSTM这两种网络结构中保持高激活/梯度流。"]}
{"source": "The vast majority of natural sensory data is temporally redundant. For instance, video frames or audio samples which are sampled at nearby points in time tend to have similar values.   Typically, deep learning algorithms take no advantage of this redundancy to reduce computations.   This can be an obscene waste of energy.   We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data.   We do this by implementing a form of Predictive Coding wherein neurons communicate a combination of their state, and their temporal change in state, and quantize this signal using Sigma-Delta modulation.   Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a spike-timing-dependent weight-update similar to Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain.   We demonstrate that on MNIST, on a temporal variant of MNIST, and on Youtube-BB, a dataset with videos in the wild, our algorithm performs about as well as a standard deep network trained with backpropagation, despite only communicating discrete values between layers.", "target": ["一种在时间冗余数据上有效训练神经网络的算法。", "本文介绍了一种用于深度神经网络中基于尖峰学习的神经编码方案", "本文提出了一种基于尖峰的学习方法，旨在减少学习和测试过程中对时间冗余数据进行分类时所需要的计算。", "本文应用Sigma-Delta编码方案的预测编码版本来减少深度学习网络的计算负荷，以一种以前没有见过的方式将这三个部分结合起来。"]}
{"source": "Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. To do so, IB identifies an intermediate \"bottleneck\" variable T that has low mutual information I(X;T) and high mutual information I(Y;T). The \"IB curve\" characterizes the set of bottleneck variables that achieve maximal I(Y;T) for a given I(X;T), and is typically explored by maximizing the \"IB Lagrangian\", I(Y;T) - βI(X;T). In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of β; (2) there are \"uninteresting\" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal. We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way. To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases. We demonstrate the three caveats on the MNIST dataset.", "target": ["只要输出是输入的确定性函数，信息瓶颈就会以令人惊讶的方式表现出来。", "论证了大多数真实的分类问题在类别标签和输入X之间显示出这样一种确定性的关系，并探讨了由这种病理导致的几个问题。", "探讨将信息瓶颈概念应用于确定性的监督学习模型时出现的问题", "作者阐明了信息瓶颈法在确定性规则的监督学习中的几个反直觉行为。"]}
{"source": "We prove, under two sufficient conditions, that idealised models can have no adversarial examples. We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these. We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice. We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting. This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well. Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.", "target": ["我们证明了理想化的贝叶斯神经网络可以没有对抗性的例子，并且用现实世界的BNN给出了实证证据。", "该论文研究了贝叶斯分类器的对抗性稳健性，并陈述了两个条件，他们表明这两个条件足以证明 \"理想化模型 \"在 \"理想化数据集 \"上不存在对抗性实例", "论文提出了一类没有任何对抗性例子的判别性贝叶斯分类器。"]}
{"source": "Deep neural networks are susceptible to adversarial attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary—even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.", "target": ["我们介绍了对抗性攻击的第一个实例，即重新编程目标模型以执行攻击者选择的任务--而攻击者不需要为每个测试时间的输入指定或计算期望的输出。", "作者提出了一种新的对抗性攻击方案，即神经网被重新利用来完成与最初训练时不同的任务。", "本文提出了对训练有素的固定神经网络进行 \"对抗性重新编程\"，并表明对抗性重新编程对未训练的网络效果较差。", "本文将NN监督学习中的 \"对抗性攻击 \"的概念扩展到对训练过的网络的解决方案进行全面的重新利用。"]}
{"source": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).  Our measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "target": ["我们开发了一种新的方案来预测深度网络的泛化差距，并具有很高的准确性。", "作者建议使用几何边际和层间边际分布来预测泛化差距。", "经验表明，所提出的边际统计量和泛化差距之间存在有趣的联系，这可以用来为理解深度神经网的泛化提供一些规范性的见解。"]}
{"source": "We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned. Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks. Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems. We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.", "target": ["我们提出了一种可证明的恢复具有重叠图像块的卷积网络参数（卷积和输出权重）的算法。", "本文研究了单隐层卷积神经网的理论学习，得出了一个学习算法和使用该算法的可证明保证。", "本文给出了一种学习两层神经网络的新算法，该算法涉及单个卷积滤波器和不同位置的权重向量。"]}
{"source": "Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training. We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on several data sets.", "target": ["一个新的正则化术语可以改善你对Waserstein Gan的训练", "本文提出了一种基于放宽1的Lipschitz常数约束的Wasserstein GAN的正则化方案。", "文章讨论了基于L_1 Wasserstein度量的GAN拟合中的正则化/惩罚问题。"]}
{"source": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators.   The approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves.", "target": ["我们提出了Wasserstein近似方法来训练GAN。", "提出了一个新的GAN过程，考虑到前一次迭代中产生的点，并更新生成器为进行l次。", "考虑到GAN学习中的自然梯度学习，其中采用了由Wasserstein-2距离引起的黎曼结构。", "本文打算利用Wasserstein-2距离引起的自然梯度来训练GAN中的生成器，作者提出Wasserstein近似算子作为正则化。"]}
{"source": "Influence diagrams provide a modeling and inference framework for sequential decision problems, representing the probabilistic knowledge by a Bayesian network and the preferences of an agent by utility functions over the random variables and decision variables.  MDPs and POMDPS, widely used for planning under uncertainty can also be represented by influence diagrams.  The time and space complexity of computing the maximum expected utility (MEU) and its maximizing policy is exponential in the induced width of the underlying graphical model, which is often prohibitively large due to the growth of the information set under the sequence of decisions.  In this paper, we develop a weighted mini-bucket approach for bounding the MEU.  These bounds can be used as a stand-alone approximation that can be improved as a function of a controlling i-bound parameter . They can also be used as heuristic  functions to guide search, especially for planning   such as MDPs and POMDPs.  We evaluate the scheme empirically against state-of-the-art, thus illustrating its potential.", "target": ["本文介绍了一种基于排除法的启发式决策函数用于顺序决策，适用于指导解决影响图的AND/OR搜索算法。", "将minibuckets的推理启发式推广到影响图中。"]}
{"source": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.  In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.  Methods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.  The main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.  We evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "target": ["噪声输入/丢弃/不确定参数下NN输出的平均值和方差的近似。argmax、softmax和max层的分析性近似。", "作者专注于不确定性传播DNN的问题", "本文重新审视了神经元中均值和方差的前馈传播，解决了通过最大集合层和softmax传播不确定性的问题。"]}
{"source": "Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation.   However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel. Some of the most promising adversarial models today minimize a Wasserstein objective. It is smoother and more stable to optimize. In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties. By making the discriminator of a GAN robust to adversarial attacks we can turn any GAN objective into a smooth and stable loss. We experimentally show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively. The training additionally becomes more robust to suboptimal choices of hyperparameters, model architectures, or objective functions.", "target": ["一个不容易被对抗性例子愚弄的判别器使GAN训练更加稳健，并导致一个更平滑的目标。", "本文提出了一种新的方法来稳定GAN的训练过程，通过对判别器进行正则化，使其对对抗性例子具有鲁棒性。", "本文提出了一种系统化的带有稳健性正则化条款的GAN训练方式，使GAN的训练更加平滑。", "提出的观点是，使判别器对对抗性扰动具有稳健性，可以使GAN目标变得平滑，从而在视觉上和FID方面取得更好的结果。"]}
{"source": "We propose a method to learn stochastic activation functions for use in probabilistic neural networks.  First, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.  Second, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.  Third, we show how to apply variational Bayesian inference to regularize and efficiently train this model.  The resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.  Like a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.", "target": ["我们将每个神经元的激活函数建模为高斯过程，并通过变分推理法与权重一起学习。", "提出在神经网中每个激活函数的函数形式上放置高斯过程先验，以学习激活函数的形式。"]}
{"source": "Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice. In this paper, we bridge the gap between these good empirical results   and the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks. More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of  bounded width and small depth can approximate a deep ReLU network in which the dense matrices are  of low rank. Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks. We support our experimental results with thorough experiments on a large, real world video classification problem.", "target": ["我们对Deep circulant-diagonal ReLU网络的特性进行了理论研究，并证明它们是有界宽度的通用近似器。", "本文提出使用环形矩阵和对角矩阵来加快计算速度，减少神经网络的内存需求。", "本文证明了有界宽度的对角线-循环ReLU网络（DC-ReLU）是通用的近似器。"]}
{"source": "Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility. However, manual remote piloting of a drone is prone to errors. In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections. Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest. The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy. A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system.", "target": ["StarHopper是一个新颖的触摸屏界面，用于高效和灵活的以物体为中心的摄像无人机导航", "作者概述了他们开发的新的无人机控制界面StarHopper，将自动和手动驾驶结合到一个新的混合导航界面中，并通过使用一个额外的顶置摄像机摆脱了目标物体已经在无人机FOV中的假设。", "本文介绍了StarHopper，一个在远程检查背景下用于半自动无人机导航的系统。", "介绍了StarHopper，一个使用计算机视觉技术与触摸输入的应用程序，支持以物体为中心的无人机驾驶。"]}
{"source": "Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations. RNN can capture long-range dependency but is hard to parallelize and not time-efficient. CNN focuses on local dependency but does not perform well on some tasks. SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length. In this paper, we propose a model, called \"bi-directional block self-attention network (Bi-BloSAN)\", for RNN/CNN-free sequence encoding. It requires as little memory as RNN but with all the merits of SAN. Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency. Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required. Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information. On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.", "target": ["用于RNN/CNN-free序列编码的自我注意力网络，内存消耗小，计算高度可并行，在几个NLP任务上具有最先进的性能", "建议在两个层面上应用自我注意力，以限制基于注意力的模型的记忆要求，对速度的影响可以忽略不计。", "本文介绍了双向块状自我注意力模型，作为一种通用的编码器，用于NLP中的各种序列建模任务。"]}
{"source": "End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.", "target": ["使用粗粒度细粒度分层注意力的多证据问答的新的最先进的模型。", "提出了一种基于两个独立模块（粗粒度和细粒度模块）的多跳问答方法。", "本文提出了一个有趣的粗粒度细粒度的共同注意力网络架构，以解决多证据问题的回答。", "专注于多选问答，并提出了一个从粗到细的评分框架。"]}
{"source": "Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures. In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework. We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner. We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018). We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling.", "target": ["我们提出了使用生成式对抗网络进行不平衡最优运输的新方法。", "作者使用随机最小最大算法和局部缩放法考虑了两个总质量不同的措施之间的不平衡最优运输问题", "作者提出了一种估计采样措施之间的不平衡最优传输的方法，该方法在维度和采样数量上有很好的扩展性。", "本文通过同时学习运输图T和缩放因子xi，引入了非平衡最优运输的静态公式。"]}
{"source": "Extracting saliency maps, which indicate parts of the image important to classification, requires many tricks to achieve satisfactory performance when using classifier-dependent methods. Instead, we propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance. We observe that the proposed approach extracts higher quality saliency maps and outperforms existing weakly-supervised localization techniques, setting the new state of the art result on the ImageNet dataset.", "target": ["我们提出了一种新的显著性地图提取方法，它能提取出更高质量的地图。", "提出了一种与分类器无关的显著图提取方法。", "本文介绍了一种新的显著图提取器，似乎可以改善最先进的结果。", "作者认为，当提取的显著图直接依赖于一个模型时，那么它可能对不同的分类器没有用处，并提出了一个近似的解决方案。"]}
{"source": "Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs). However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, e.g., translating pants to skirts in fashion images. To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration. The proposed method translates both an image and the corresponding set of instance attributes while maintaining the permutation invariance property of the instances. To this end, we introduce a context preserving loss that encourages the network to learn the identity function outside of target instances. We also propose a sequential mini-batch inference/training technique that handles multiple instances with a limited GPU memory and enhances the network to generalize better for multiple instances. Our comparative evaluation demonstrates the effectiveness of the proposed method on different image datasets, in particular, in the aforementioned challenging cases. Code and results are available in https://github.com/sangwoomo/instagan", "target": ["我们提出了一种新颖的方法，将实例属性集纳入图像到图像的转换。", "本文提出了一种方法--InstaGAN--它建立在CycleGAN的基础上，以每个实例分割掩码的形式考虑到了实例信息，其结果与CycleGAN和其他基线相比更有优势。", "提出针对未配对的图像与图像之间的转换问题，增加实例感知的分割掩码。"]}
{"source": "Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime  where classical learning theory would instead predict that they would severely overfit.   While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit.   In this paper, we provide a new explanation. By applying a very general probability-complexity bound recently derived from  algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions. We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks trained on CIFAR10 and MNIST.  As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems.  This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than  the more conventional prior over parameter space.   If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets.   By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood,  we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks.", "target": ["深度网络的参数-函数图是有巨大偏差的；这可以解释为什么它们会泛化。我们使用PAC-Bayes和高斯过程来获得非空边界。", "本文在PAC-Bayesian学习理论和经验支持的直觉的帮助下，研究了深度神经网络的泛化能力。", "本文提出了对大型过度参数化神经网络泛化行为的解释，声称神经网络中的参数-函数图偏向于 \"简单 \"的函数，如果目标概念也是 \"简单 \"的，则泛化行为会很好。"]}
{"source": "We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.  While the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).   Learning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).  We choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,  the latent states are then   optimized according to a tractable free-energy objective . Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the  individuals as the (log) joint probabilities given by the used generative model.   As a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.", "target": ["我们提出了进化EM，作为一种新的算法，用于对具有二元潜变量的生成模型进行无监督训练，该算法将变异EM与进化优化紧密联系起来。", "本文提出了一种结合进化计算和变异EM的方法，适用于通过基于粒子的近似表示的二元潜变量模型。", "本文尝试将期望最大化训练算法与进化算法紧密结合。"]}
{"source": "While deep neural networks have achieved groundbreaking prediction results in many tasks, there is a class of data where existing architectures are not optimal -- sequences of probability distributions. Performing forward prediction on sequences of distributions has many important applications. However, there are two main challenges in designing a network model for this task. First, neural networks are unable to encode distributions compactly as each node encodes just a real value. A recent work of Distribution Regression Network (DRN) solved this problem with a novel network that encodes an entire distribution in a single node, resulting in improved accuracies while using much fewer parameters than neural networks. However, despite its compact distribution representation, DRN does not address the second challenge, which is the need to model time dependencies in a sequence of distributions. In this paper, we propose our Recurrent Distribution Regression Network (RDRN) which adopts a recurrent architecture for DRN. The combination of compact distribution representation and shared weights architecture across time steps makes RDRN suitable for modeling the time dependencies in a distribution sequence. Compared to neural networks and DRN, RDRN achieves the best prediction performance while keeping the network compact.", "target": ["我们提出了一个高效的循环网络模型，用于对时变分布进行前向预测。", "本文提出了一种创建神经网络的方法，将历史分布映射到分布上，并将该方法应用于几个分布预测任务。", "提出了一种循环分布回归网络，它在以前的模型分布回归网络的基础上使用了一个循环结构。", "本文通过研究循环神经网络环境下的时间变化分布，对概率分布进行回归。"]}
{"source": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).", "target": ["用神经网络处理图形结构数据的新方法，利用对节点邻域的关注。在归纳引文网络任务和归纳蛋白质-蛋白质相互作用任务上取得了最先进的结果。", "本文提出了一种对图的节点进行分类的新方法，该方法可用于半监督的场景和全新的图上。", "本文介绍了一个在图结构数据上操作的神经网络架构，名为图注意网络。", "对学习图的节点的向量表示的最新方法进行了公平和几乎全面的讨论。"]}
{"source": "While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model. In the second stage, another  recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.", "target": ["基于强化学习的新型方法，用知识蒸馏法压缩深度神经网络", "本文提出在知识蒸馏过程中使用强化学习而不是预先定义的启发式方法来确定压缩模型的结构", "介绍了一种网络对网络压缩的原则性方法，它使用策略梯度来优化两个策略，将一个强大的教师压缩成一个强大但较小的学生模型。"]}
{"source": "Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss. However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples. In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss. We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset. Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples.", "target": ["我们证明了条件型GAN的模式崩溃主要是由于重建损失和GAN损失之间的不匹配，并介绍了一套新的损失函数作为重建损失的替代品。", "本文提出了对传统的条件性GAN目标的修改，以促进图像的多样化、多模态生成。", "本文提出了一种替代L1/L2误差的方法，这些误差在训练条件GAN时被用来增加对抗性损失。"]}
{"source": "Generative models are important tools to capture and investigate the properties of complex empirical data. Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \\textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data. Does learning the parameters of both architectures obey the same rules? We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other. Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference. In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.", "target": ["我们使用因果推理来描述生成模型的结构特点", "本文研究了VAE的编码器和解码器以及GAN的发生器和判别器中卷积滤波器的性质。", "这项工作利用了因果关系原理来量化连续层的权重如何相互适应。"]}
{"source": "Many deep reinforcement learning approaches use graphical state representations,  this means visually distinct games that share the same underlying structure cannot  effectively share knowledge. This paper outlines a new approach for learning  underlying game state embeddings irrespective of the visual rendering of the game  state. We utilise approaches from multi-task learning and domain adaption in  order to place visually distinct game states on a shared embedding manifold. We  present our results in the context of deep reinforcement learning agents.", "target": ["一种学习视觉上不同的游戏之间共享嵌入空间的方法。", "一种学习视觉上不同的游戏底层结构的新方法，结合了处理输入图像的卷积层、用于深度强化学习的异步优势演员批判和强迫嵌入表示独立于游戏的视觉表示的对抗性方法。", "介绍了一种通过适应深度强化学习来学习视觉上不同游戏的策略的方法。", "本文讨论了一个代理架构，该架构使用一个共享的表示法来训练具有不同精灵级别视觉统计的多个任务"]}
{"source": "We study discrete time dynamical systems governed by the state equation $h_{t+1}=ϕ(Ah_t+Bu_t)$. Here A,B are weight matrices, ϕ is an activation function, and $u_t$ is the input data. This relation is the backbone of recurrent neural networks (e.g. LSTMs) which have broad applications in sequential learning tasks. We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$. We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size. Our results apply to increasing activations whose derivatives are bounded away from zero. The analysis is based on i) an SGD convergence result with nonlinear activations and ii) careful statistical characterization of the state vector. Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory.", "target": ["我们研究了一个循环神经网络的状态方程。我们表明，在适当的假设下，SGD可以有效地从少数输入/输出观测中学习未知的动态。", "本文研究了具有非线性状态方程的离散时间动力系统，证明了在固定长度的轨迹上运行SGD可以得到对数收敛。", "这项工作考虑了学习一个非线性动态系统的问题，其中输出等于状态。", "本文研究了SGD学习线性系统的动态和非线性激活的能力。"]}
{"source": "Although deep neural networks show their extraordinary power in various tasks, they are not feasible for deploying such large models on embedded systems due to high computational cost and storage space limitation. The recent work knowledge distillation (KD) aims at transferring model knowledge from a well-trained teacher model to a small and fast student model which can significantly help extending the usage of large deep neural networks on portable platform. In this paper, we show that, by properly defining the neuron manifold of deep neuron network (DNN), we can significantly improve the performance of student DNN networks through approximating neuron manifold of powerful teacher network. To make this, we propose several novel methods for learning neuron manifold from DNN model. Empowered with neuron manifold knowledge, our experiments show the great improvement across a variety of DNN architectures and training data. Compared with other KD methods, our Neuron Manifold Transfer (NMT) has best transfer ability of the learned features.", "target": ["一种用于迁移学习的新的知识蒸馏方法", "该工作介绍了一种使用所提出的神经元流形概念的知识蒸馏方法。", "提出了一种知识蒸馏的方法，其中神经流形被作为转移的知识。"]}
{"source": "We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization. At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models. Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively. We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters. Lastly we visualize and discuss the learned features of slimmable networks. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks", "target": ["我们提出了一种简单而通用的方法来训练一个可在不同宽度（一个层中的通道数）下执行的单一神经网络，允许在运行时进行即时和自适应的准确性-效率权衡。", "本文提出了将不同大小的模型组合成一个共享网的想法，大大提高了检测的性能。", "本文在不同的宽度下训练了一个单一的可执行网络。"]}
{"source": "Measuring visual (dis)similarity between two or more instances within a data distribution is a fundamental task in many applications, specially in image retrieval. Theoretically, non-metric distances are able to generate a more complex and accurate similarity model than metric distances, provided that the non-linear data distribution is precisely captured by the similarity model. In this work, we analyze a simple approach for deep learning networks to be used as an approximation of non-metric similarity functions and we study how these models generalize across different image retrieval datasets.", "target": ["学习一对图像之间的非度量视觉相似性估计的相似性网络", "作者提出了视觉相似性的学习相似性措施，并以此在非常著名的牛津和巴黎的图像检索数据集中获得了改进。", "本文认为，使用非公制距离而不是公制距离更合适。"]}
{"source": "Training a model to perform a task typically requires a large amount of data from the domains in which the task will be applied.  However, it is often the case that data are abundant in some domains but scarce in others. Domain adaptation deals with the challenge of adapting a model trained from a data-rich source domain to perform well in a data-poor target domain. In general, this requires learning plausible mappings between domains. CycleGAN is a powerful framework that efficiently learns to map inputs from one domain to another using adversarial training and a cycle-consistency constraint. However, the conventional approach of enforcing cycle-consistency via reconstruction may be overly restrictive in cases where one or more domains have limited training data. In this paper, we propose an augmented cyclic adversarial learning model that enforces the cycle-consistency constraint via an external task specific model, which encourages the preservation of task-relevant content as opposed to exact reconstruction. We explore digit classification in a low-resource setting in supervised, semi and unsupervised situation, as well as high resource unsupervised. In low-resource supervised setting, the results show that our approach improves absolute performance by 14% and 4% when adapting SVHN to MNIST and vice versa, respectively, which outperforms unsupervised domain adaptation methods that require high-resource unlabeled target domain.   Moreover, using only few unsupervised target data, our approach can still outperforms many high-resource unsupervised models. Our model also outperforms on USPS to MNIST and synthetic digit to SVHN for high resource unsupervised adaptation. In speech domains, we similarly adopt a speech recognition model from each domain as the task specific model. Our approach improves absolute performance of speech recognition by 2% for female speakers in the TIMIT dataset, where the majority of training samples are from male voices.", "target": ["一种新的循环对抗学习与辅助任务模型相结合，提高了低资源监督和无监督情况下的领域适应性能", "提出了循环一致的逆向适应方法的扩展，以解决有监督的目标数据有限时的领域适应。", "本文介绍了一种基于Cyclic GAN思想的领域适应方法，并提出了两种不同的算法。"]}
{"source": "Nodes residing in different parts of a graph can have similar structural roles within their local network topology. The identification of such roles provides key insight into the organization of networks and can also be used to inform machine learning on graphs. However, learning structural representations of nodes is a challenging unsupervised-learning task, which typically involves manually specifying and tailoring topological features for each node. Here we develop GraphWave, a method that represents each node’s local network neighborhood via a low-dimensional embedding by leveraging spectral graph wavelet diffusion patterns. We prove that nodes with similar local network neighborhoods will have similar GraphWave embeddings even though these nodes may reside in very different parts of the network. Our method scales linearly with the number of edges and does not require any hand-tailoring of topological features. We evaluate performance on both synthetic and real-world datasets, obtaining improvements of up to 71% over state-of-the-art baselines.", "target": ["我们开发了一种基于谱图小波扩散的学习网络结构特征的方法。", "利用节点局部近邻的谱图小波扩散模式将节点嵌入低维空间中", "本文基于图拉普拉斯算子的小波分析，得出了一种比较图中节点的方法。"]}
{"source": "Driving simulators play an important role in vehicle research. However, existing virtual reality simulators do not give users a true sense of presence. UniNet is our driving simulator, designed to allow users to interact with and visualize simulated traffic in mixed reality. It is powered by SUMO and Unity. UniNet's modular architecture allows us to investigate interdisciplinary research topics such as vehicular ad-hoc networks, human-computer interaction, and traffic management. We accomplish this by giving users the ability to observe and interact with simulated traffic in a high fidelity driving simulator. We present a user study that subjectively measures user's sense of presence in UniNet. Our findings suggest that our novel mixed reality system does increase this sensation.", "target": ["一个使用立体摄像机和穿透式VR的混合现实驾驶模拟器在24名参与者的用户研究中进行了评估。", "提出了一个复杂的驾驶模拟系统。", "本文介绍了一个混合现实驾驶模拟器的设置，以增强存在感。", "提出了一个混合现实驾驶模拟器，其中包括交通生成，并声称由于MR系统而增强了 \"存在感\"。"]}
{"source": "We consider the problem of improving kernel approximation via feature maps. These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets. We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods. Our approach allows to use information about the integrand to enhance approximation and facilitates fast computations. We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.", "target": ["内核近似的正交规则。", "本文提出通过使用正交规则（如随机球状辐射规则）来改进随机特征的核近似。", "作者提出了一种新型的随机特征图方法来近似解决大规模的核问题。", "本文表明，由于Genz & Monahan（1998）的技术可用于在随机傅里叶特征的框架下实现低内核近似误差，这是一种应用正交规则改善内核近似的新方法。"]}
{"source": "Human world knowledge is both structured and flexible. When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts. Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context. Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects. To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts. Using this dataset, we developed neural listener and speaker models with strong capacity for generalization. By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities. We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object.", "target": ["如何建立神经说话人/听众，从指代语言中学习三维物体的细微特征。", "作者提供了一项关于学习指代三维物体的研究，收集了一个指代表达的数据集，并通过试验一些建筑选择来训练几个模型"]}
{"source": "Object-based factorizations provide a useful level of abstraction for interacting with the world. Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice. We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties. Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels. For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics. After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.", "target": ["我们提出了一个学习以物体为中心的表征的框架，该框架适用于需要理解物理的任务中的计划。", "本文提出了一个预测物体在引力作用下相互作用的图像的平台。", "本文提出了一种方法，可以从给定的图像中学习复制 \"块状塔\"。", "提出了一种方法，可以学习推理不同物体的物理互动，而不需要监督物体的属性。"]}
{"source": "We study the error landscape of deep linear and nonlinear neural networks with the squared error loss. Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete. For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum. Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization. We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting.", "target": ["我们为深度线性神经网络的全局最优性提供了有效的可检查的必要条件和充分条件，并对非线性设置进行了一些初步扩展。", "本文给出了深度线性神经网络损失函数的全局最优性的条件", "本文给出了关于深度神经网络目标函数中存在局部最小值的理论结果。", "研究深度线性网络的一些理论特性。"]}
{"source": "Recurrent auto-encoder model can summarise sequential data through an encoder structure into a fixed-length vector and then reconstruct into its original sequential form through the decoder structure. The summarised information can be used to represent time series features. In this paper, we propose relaxing the dimensionality of the decoder output so that it performs partial reconstruction. The fixed-length vector can therefore represent features only in the selected dimensions. In addition, we propose using rolling fixed window approach to generate samples. The change of time series features over time can be summarised as a smooth trajectory path. The fixed-length vectors are further analysed through additional visualisation and unsupervised clustering techniques.    This proposed method can be applied in large-scale industrial processes for sensors signal analysis purpose where clusters of the vector representations can be used to reflect the operating states of selected aspects of the industrial system.", "target": ["使用循环自动编码器模型来提取多维时间序列特征", "本文介绍了循环自动编码器在多维时间序列分析中的应用。", "本文描述了一个序列到序列的自动编码器模型，该模型用于学习序列表征，表明对于它们的应用，当网络只被训练为重建数据测量的一个子集时，可以获得更好的性能。", "提出了一种受循环自动编码器模型启发的策略，这样就可以根据背景向量对多维时间序列数据进行聚类。"]}
{"source": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin.", "target": ["我们引入了一个图到图的编码器-解码器框架，用于学习多样化的图转换。", "在匹配分子对分析的启发下，提出了分子优化的图-图转换模型。", "通过增加潜伏变量来捕捉多模态和潜伏空间中的对抗性正则化，将JT-VAE扩展到图与图的转换场景中", "提出了一个相当复杂的系统，涉及许多不同的选择和组成部分，以便从一个给定的语料库开始获得具有改进性质的化学复合物。"]}
{"source": "Partial differential equations (PDEs) are widely used across the physical and computational sciences. Decades of research and engineering went into designing fast iterative solution methods. Existing solvers are general purpose, but may be sub-optimal for specific classes of problems. In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain. We achieve this goal by learning to modify the updates of an existing solver using a deep neural network. Crucially, our approach is proven to preserve strong correctness and convergence guarantees. After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.", "target": ["我们学习了一种具有收敛保证的PDEs的快速神经解算器。", "开发了一种加速解决PDEs的有限差分方法，并提出了离散化后固定点迭代的修订框架。", "作者提出了一种加快PDE求解器速度的线性方法。"]}
{"source": "Variational Bayesian neural networks (BNN) perform variational inference over weights, but it is difficult to specify meaningful priors and approximating posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes is equal to the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors which entail rich structure, including Gaussian processes and implicit stochastic processes. Empirically, we find that fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and can scale to large datasets.", "target": ["我们对贝叶斯神经网络定义的随机过程进行功能变分推理。", "拟合函数形式的变分贝叶斯神经网络近似，并考虑通过样本隐含地与随机过程先验匹配。", "提出了一种用于训练BNN的新型ELBO目标，允许在模型中编码更有意义的先验，而不是文献中信息量较小的权重先验特征。", "提出了一种新的贝叶斯神经网络模型的变分推理算法，在这种算法中，先验是以函数方式而不是通过权重的先验来指定的。"]}
{"source": "Words are not created equal. In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal. In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry. This connection allows us to introduce a novel principled hypernymy score for word embeddings. Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds. We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry. Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection. In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy.", "target": ["我们将词嵌入双曲空间，并与高斯词嵌入建立了联系。", "本文将Glove词嵌入改编为由Poincare半平面模型给出的双曲空间", "本文提出了一种实现基于GLOVE的双曲词嵌入模型的方法，它通过黎曼优化方法进行优化。"]}
{"source": "Answering questions about a text frequently requires aggregating information from multiple places in that text. End-to-end neural network models, the dominant approach in the current literature, can theoretically learn how to distill and manipulate representations of the text without explicit supervision about how to do so. We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings. In a simple synthetic setting, the path-finding task of the bAbI dataset, the model fails to learn the correct reasoning without additional supervision of its attention mechanism. However, with this supervision, it can perform well. On a real text dataset, WikiHop, the memory network gives nearly state-of-the-art performance, but does so without using its multi-hop capabilities. A tougher anonymized version of the WikiHop dataset is qualitatively similar to bAbI: the model fails to perform well unless it has additional supervision. We hypothesize that many \"multi-hop\" architectures do not truly learn this reasoning as advertised, though they could learn this reasoning if appropriately supervised.", "target": ["除非我们监督它们，否则记忆网络不会学习多跳推理。", "声称多跳推理不容易直接学习，需要直接监督，在WikiHop上做得很好并不一定意味着模型真的在学习跳跃。", "本文拟研究众所周知的记忆网络学习问题，更确切地说，是用这种模型进行注意力学习监督的难度。", "本文认为，记忆网络未能学会合理的多跳推理。"]}
{"source": "Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood. We compute deep convolutional network generators by inverting a fixed embedding operator. Therefore, they do not require to be optimized with a discriminator or an encoder. The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into deformations between output images. This embedding is computed with a wavelet Scattering transform. Numerical experiments demonstrate that the resulting Scattering generators have similar properties as GANs or VAEs, without learning a discriminative network or an encoder.", "target": ["我们引入了生成网络，它不需要用判别器或编码器来学习；它们是通过倒置一个由小波散射变换定义的特殊嵌入算子得到的。", "介绍散射变换作为生成对抗网络背景下的图像生成模型，并提出为什么它们可以被视为具有可控信息损失和可逆性的高斯化变换。", "本文提出了一种图像的生成模型，不需要学习判别器（如GAN）或学习嵌入。"]}
{"source": "Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token. Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance. Efforts to speed up this inference, known as ''neural speed reading'', either ignore or skim over part of the input. We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference. The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word.  A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that   Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text.", "target": ["我们提出了一个新的神经速读模型，利用文本固有的标点结构来定义有效的跳跃和跳过行为。", "本文提出了一个结构跳跃-LSTM模型，用两个代理而不是一个代理来加快机器阅读速度", "提出了一个新的神经速读模型，其中新读者有能力跳过一个词或一连串的词。", "本文提出了一种使用跳过和跳跃动作的快速阅读方法，表明所提出的方法与LSTM一样准确，但使用的计算量要少得多。"]}
{"source": "One of the key challenges of session-based recommender systems is to enhance users’ purchase intentions. In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning. Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN). Speciﬁcally, IRN enables the agent to explore its environment and learn predictive representations via three key components. The imagination core generates predicted trajectories, i.e., imagined items that users may purchase. The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards. To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms. Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines. Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can \"guess what you like\" via internal planning.", "target": ["我们提出了IRN架构，以增强基于会话的推荐的稀疏和延迟的购买奖励。", "本文提出通过使用想象力重构网络的强化学习来提高推荐系统的性能。", "本文通过关注用户的购买行为而不是点击量，提出了一种基于会话的推荐方法。"]}
{"source": "The question why deep learning algorithms generalize so well has attracted increasing  research interest. However, most of the well-established approaches,  such as hypothesis capacity, stability or sparseness, have not provided complete  explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus  on the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis  will not change much due to perturbations of its training examples, then it  will also generalize well. As most deep learning algorithms are stochastic (e.g.,  Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness  arguments of Xu & Mannor, and introduce a new approach – ensemble  robustness – that concerns the robustness of a population of hypotheses. Through  the lens of ensemble robustness, we reveal that a stochastic learning algorithm can  generalize well as long as its sensitiveness to adversarial perturbations is bounded  in average over training examples. Moreover, an algorithm may be sensitive to  some adversarial examples (Goodfellow et al., 2015) but still generalize well. To  support our claims, we provide extensive simulations for different deep learning  algorithms and different network architectures exhibiting a strong correlation between  ensemble robustness and the ability to generalize.", "target": ["通过集合稳健性从理论和经验上解释随机深度学习算法的泛化问题", "本文提出了对Xu&Mannor'12的算法稳健性的改编，并提出了学习边界和一个显示经验性集合稳健性和泛化误差之间相关性的实验。", "提出了对深度学习算法泛化能力的研究，使用了稳定性概念的扩展，称为集合稳健性，并给出了随机算法的泛化误差在稳定性参数方面的边界，并提供了试图将理论与实践联系起来的实证研究。", "该论文从稳健性的角度研究了深度学习背景下学习算法的泛化能力"]}
{"source": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.   However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.   In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.   Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.", "target": ["少样本的学习PixelCNN", "本文提出，当训练数据的可用性较低时，通过使用元学习模型来使用密度估计。", "本文考虑了单/少样本密度估计的问题，使用已经应用于单/少样本监督学习的金属学习技术。", "本文重点介绍了自回归密度估计的少样本学习，并利用神经注意力和元学习技术改进了PixelCNN。"]}
{"source": "Neural networks exhibit good generalization behavior in the  over-parameterized regime, where the number of network parameters  exceeds the number of observations. Nonetheless,  current generalization bounds for neural networks fail to explain this  phenomenon. In an attempt to bridge this gap, we study the problem of  learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function. In the case where the network has Leaky  ReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks.  Specifically, we prove convergence rates of SGD to a global  minimum and provide generalization guarantees for this global minimum  that are independent of the network size.   Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model. This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.", "target": ["我们表明，SGD可以学习具有Leaky ReLU激活的两层超参数神经网络，并且可以证明在线性可分离数据上的泛化。", "本文研究了超参数模型能够通过使用具有固定输出层的1个隐藏层网络来学习具有良好泛化的解决方案。", "本文表明，在线性可分离数据上，过参数化网络上的SGD仍能靠出一个可证明是泛化的分类器。"]}
{"source": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "target": ["用目标策略信息瓶颈训练代理，促进转移并产生强大的探索奖励", "提出了在多目标RL环境下，用负条件互信息对标准RL损失进行正则化的策略搜索。", "本文提出了决策状态的概念，并提出了KL发散正则化来学习任务的结构，以利用这些信息来鼓励策略访问决策状态。", "本文提出了一种用相互信息项来规范目标条件策略的方法。"]}
{"source": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "target": ["我们提出了一种优化方法，适用于只有有偏差的梯度的情况--我们为这种情况定义了一个新的梯度估计器，推导出这个估计器的偏差和方差，并将其应用于实例问题。", "作者提出了一种将随机搜索与代用梯度信息相结合的方法，并对方差-偏置的权衡进行了讨论，还对超参数优化进行了讨论。", "本文提出了一种通过建立前k个代用梯度的子空间来改进随机搜索的方法。", "本文试图通过引入一个具有I+UU^t形式的协方差矩阵的非等熵分布和外部信息（如代用梯度）来加速OpenAI类型的演化，以确定U"]}
{"source": "Point clouds are an important type of geometric data and have widespread use in computer graphics and vision. However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space. Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as classification or segmentation. This paper studies the unsupervised problem of a generative model exploiting graph convolution. We focus on the generator of a GAN and define methods for graph convolution when the graph is not known in advance as it is the very output of the generator. The proposed architecture learns to generate localized features that approximate graph embeddings of the output geometry. We also study the problem of defining an upsampling layer in the graph-convolutional generator, such that it learns to exploit a self-similarity prior on the data distribution to sample more effectively.", "target": ["一个使用图卷积操作的GAN，从隐藏的特征中动态地计算出图来", "本文提出了一个专门用于生成点云的GAN版本，该工作的核心贡献是上采样操作。", "本文提出了用于不规则三维点云的图形卷积GAN，它同时学习领域和特征。"]}
{"source": "Memorization in over-parameterized neural networks can severely hurt generalization in the presence of mislabeled examples. However, mislabeled examples are to hard avoid in extremely large datasets. We address this problem using the implicit regularization effect of stochastic gradient descent with large learning rates, which we find to be able to separate clean and mislabeled examples with remarkable success using loss statistics. We leverage this to identify and on-the-fly discard mislabeled examples using a threshold on their losses. This leads to On-the-fly Data Denoising (ODD), a simple yet effective algorithm that is robust to mislabeled examples, while introducing almost zero computational overhead. Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeled examples.", "target": ["我们介绍了一种快速且易于实施的算法，该算法对数据集的噪声具有稳健性。", "本文旨在通过丢弃训练过程中损失较大的例子来去除潜在的标签噪音。"]}
{"source": "Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency. Concurrently, it has been shown that neural networks may be overly sensitive to ``attacks\" -- tiny adversarial changes in the input -- which may be detrimental to their use in safety-critical domains. Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks.  The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks. In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization. We propose a Mixed Integer Linear Programming (MILP) formulation of the problem. While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow. To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems. Experimentally, we evaluate both proposed methods against the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to PGD, while scaling beyond the limits of the MILP.", "target": ["由于二值化神经网络的非差异性，基于梯度的攻击并不有效；我们的IPROP算法利用整数优化解决了这个问题。", "提出了一种新的目标传播式算法，以产生对二值化神经网络的强对抗性攻击。", "本文提出了一种基于二元神经网络的MILP的新攻击算法。", "本文提出了一种寻找二元神经网络对抗性攻击的算法，该算法从顶层到输入层逐层迭代寻找所需的表征，比解决完整的混合整数线性规划（MILP）求解器更有效率。"]}
{"source": "Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax. We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling. These results constitute a new state-of-the-art in their respective settings.", "target": ["使用预测的下一个token分布对上下文中的最后一个token进行解码，可以作为正则器，改善语言建模。", "作者介绍了过去解码的想法，目的是在Penn Treebank上进行正则化以改善困惑性。", "建议在训练LSTM LM时使用一个额外的损失项，并表明通过增加这个损失项，他们可以在一些LM的基准上实现SOTA的困惑性。", "提出了一种新的正则化技术，可以在Merity等人（2017）的AWD-LSTM中使用的正则化技术的基础上添加，而且开销很小。"]}
{"source": "The assumption that data samples are independently identically distributed is the backbone of many learning algorithms. Nevertheless, datasets often exhibit rich structures in practice, and we argue that there exist some unknown orders within the data instances. Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically. Specifically, we assume that the instances are sampled from a Markov chain. Our goal is to learn the transitional operator of the chain as well as the generation order by maximizing the generation probability under all possible data permutations. One of our key ideas is to use neural networks as a soft lookup table for approximating the possibly huge, but discrete transition matrix. This strategy allows us to amortize the space complexity with a single model and make the transitional operator generalizable to unseen instances. To ensure the learned Markov chain is ergodic, we propose a greedy batch-wise permutation scheme that allows fast training.   Empirically, we evaluate the learned Markov chain by showing that GMNs are able to discover orders among data instances and also perform comparably well to state-of-the-art methods on the one-shot recognition benchmark task.", "target": ["提出以生成模型的观点观察数据集中的隐性顺序。", "作者处理了数据集中的隐性排序问题和恢复它的挑战，并提议学习一个无距离计量的模型，该模型假定马尔可夫链为数据的生成机制。", "本文提出了生成马尔可夫网络--一种基于深度学习的方法，用于对序列进行建模并发现数据集中的顺序。", "提出了通过学习马尔可夫链来学习无序数据样本的顺序。"]}
{"source": "We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.", "target": ["通过Seq2Tree模型上的Tree-Beam搜索，从自然语言描述和输入/输出实例中进行程序合成", "提出了一个seq2Tree模型，将自然语言中的问题陈述翻译成DSL中的相应功能程序，这比seq2seq基线方法有了改进。", "本文解决了在给定问题描述和少量输入输出实例时进行程序综合的问题。", "本文介绍了一种程序合成的技术，涉及到一个限制性的问题语法，该语法使用注意力编码器-解码器网络进行束状搜索。"]}
{"source": "Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically  neural networks. The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable). In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions. This is a very mild condition satisfied even by neural networks with a single neuron. Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics. When evaluated with neural distance, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set. When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training. Our analysis sheds lights on understanding the practical performance of GANs.", "target": ["本文研究了当判别器集是一个像神经网络一样的限制性函数类时，GAN的判别和泛化特性。", "通过保证诱导的IPM是度量而非伪度量，平衡GAN中生成器和判别器类的能力", "本文对GAN中对手/判别器集的大小的作用进行了数学分析"]}
{"source": "Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.", "target": ["训练深层残差网络所需要的只是一个好的初始化；规范化层是不必要的。", "提出了一种用于深度残差网络的初始化和规范化的方法。这是基于对此类网络中前向和后向爆炸的观察。该方法的性能与其他具有更明确规范化的网络获得的最佳结果相当。", "作者提出了一种新的初始化残差网络的方法，其动机是为了避免梯度爆炸/消失的需要。", "提出了一种新的初始化方法，用于训练非常深的RedNets，而不使用批量规范。"]}
{"source": "Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.", "target": ["本文旨在为无监督学习（如文本生成）学习一个更好的衡量标准，并显示出对SeqGAN的显著改进。", "描述了一种通过学习状态-动作值来生成时间序列的方法，其中状态是到目前为止生成的序列，而动作是对下一个值的选择。", "本文考虑了通过学习更好的指标来改进序列生成的问题，特别是曝光偏差问题"]}
{"source": "One of the most successful techniques in generative models has been decomposing a complicated generation task into a series of simpler generation tasks.   For example, generating an image at a low resolution and then learning to refine that into a high resolution image often improves results substantially.   Here we explore a novel strategy for decomposing generation for complicated objects in which we first generate latent variables which describe a subset of the observed variables, and then map from these latent variables to the observed space.   We show that this allows us to achieve decoupled training of complicated generative models and present both theoretical and experimental results supporting the benefit of such an approach.", "target": ["将学习生成模型的任务分解为学习数据子集的分解潜因素，然后学习这些潜因素的联合。", "分层潜在变量生成模型的局部解缠因素，可以看作是逆向学习推理的分层变体", "本文研究了分层潜变量模型在生成图像和图像序列方面的潜力，并提议训练几个ALI模型叠加在一起，形成数据的分层表示。", "本文旨在直接学习层次优化计划中训练GAN的层次结构，而不是由人类来设计"]}
{"source": "Visual grounding of language is an active research field aiming at enriching text-based representations with visual information. In this paper, we propose a new way to leverage visual knowledge for sentence representations. Our approach transfers the structure of a visual representation space to the textual space by using two complementary sources of information: (1) the cluster information: the implicit knowledge that two sentences associated with the same visual content describe the same underlying reality and (2) the perceptual information contained within the structure of the visual space. We use a joint approach to encourage beneficial interactions during training between textual, perceptual, and cluster information. We demonstrate the quality of the learned representations on semantic relatedness, classification, and cross-modal retrieval tasks.", "target": ["我们提出一个联合模型，将视觉知识纳入句子表征中", "本文提出了一种使用视频与字幕配对的方法来改进句子嵌入的方法", "这份材料提出了一个基于相关视频数据的句子学习句子表征模型，该模型是有依据的。", "提出了一种通过多模态联合框架改善基于文本的句子嵌入的方法。"]}